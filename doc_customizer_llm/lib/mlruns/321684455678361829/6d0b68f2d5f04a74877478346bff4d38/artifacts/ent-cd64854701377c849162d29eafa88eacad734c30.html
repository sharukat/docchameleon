<!DOCTYPE html>
<html lang="en">
    <head>
        <title>displaCy</title>
    </head>

    <body style="font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr">
<figure style="margin-bottom: 6rem">
<div class="entities" style="line-height: 2.5; direction: ltr">The user is asking for examples on how to retrain a model that was saved using the `tf.saved_model.save()` function in 
<mark class="entity" style="background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    TensorFlow
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PRODUCT</span>
</mark>
. They have tried loading the saved model using `tf.saved_model.load()` but are unable to train it again as the loaded model lacks the necessary method `model.fit()`.<br><br>To address this issue, the documentation can provide multiple complete examples that showcase the usability of the `tf.saved_model.save()` function and how to retrain the saved model. These examples should include the necessary steps to load the saved model and perform training on new datasets.<br><br>Example 1: Retraining a saved model using `tf.saved_model.save()` and `tf.saved_model.load()`<br><br>```python<br># Training and saving the model<br>model = tf.keras.Sequential([...])  # Define your model architecture<br>model.compile([...])  # Compile the model with loss and optimizer<br>model.fit(train_data, epochs=10)  # Train the model<br>tf.saved_model.save(model, 'saved_model')  # Save the model<br><br># Loading the saved model and retraining<br>loaded_model = tf.saved_model.load('saved_model')  # Load the saved model<br>loaded_model.compile([...])  # Compile the loaded model with loss and optimizer<br>loaded_model.fit(new_train_data, epochs=5)  # Retrain the model on new dataset<br>```<br><br>Example 2: Retraining a saved model using `tf.saved_model.save()` and custom training loop<br><br>```python<br># Training and saving the model<br>model = tf.keras.Sequential([...])  # Define your model architecture<br>optimizer = tf.keras.optimizers.
<mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Adam
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PERSON</span>
</mark>
()<br>loss_fn = tf.keras.losses.
<mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    SparseCategoricalCrossentropy
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PERSON</span>
</mark>
()<br>for epoch in range(10):<br>    for batch in train_data:<br>        with tf.
<mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    GradientTape
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PERSON</span>
</mark>
() as tape:<br>            logits = model(batch['input'])<br>            loss_value = loss_fn(batch['target'], logits)<br>        grads = tape.gradient(loss_value, model.trainable_variables)<br>        optimizer.apply_gradients(zip(grads, model.trainable_variables))<br>tf.saved_model.save(model, 'saved_model')  # Save the model<br><br># Loading the saved model and retraining<br>loaded_model = tf.saved_model.load('saved_model')  # Load the saved model<br>optimizer = tf.keras.optimizers.
<mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    Adam
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PERSON</span>
</mark>
()<br>loss_fn = tf.keras.losses.
<mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    SparseCategoricalCrossentropy
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PERSON</span>
</mark>
()<br>for epoch in range(5):<br>    for batch in new_train_data:<br>        with tf.
<mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
    GradientTape
    <span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem">PERSON</span>
</mark>
() as tape:<br>            logits = loaded_model(batch['input'])<br>            loss_value = loss_fn(batch['target'], logits)<br>        grads = tape.gradient(loss_value, loaded_model.trainable_variables)<br>        optimizer.apply_gradients(zip(grads, loaded_model.trainable_variables))<br>```<br><br>These examples demonstrate how to save a model using `tf.saved_model.save()` and then load and retrain the saved model using `tf.saved_model.load()`. The examples include both the usage of `model.fit()` and a custom training loop to showcase different approaches to retraining the model.</div>
</figure>
</body>
</html>