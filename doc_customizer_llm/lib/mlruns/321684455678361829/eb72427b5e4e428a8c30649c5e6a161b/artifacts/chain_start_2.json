{
  "action": "on_chain_start",
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain",
    "chains",
    "llm",
    "LLMChain"
  ],
  "kwargs_llm_lc": 1,
  "kwargs_llm_type": "constructor",
  "kwargs_llm_id": [
    "langchain",
    "chat_models",
    "openai",
    "ChatOpenAI"
  ],
  "kwargs_llm_kwargs_model_name": "gpt-3.5-turbo",
  "kwargs_llm_kwargs_temperature": 0.0,
  "kwargs_llm_kwargs_verbose": true,
  "kwargs_llm_kwargs_openai_api_key_lc": 1,
  "kwargs_llm_kwargs_openai_api_key_type": "secret",
  "kwargs_llm_kwargs_openai_api_key_id": [
    "OPENAI_API_KEY"
  ],
  "kwargs_prompt_lc": 1,
  "kwargs_prompt_type": "constructor",
  "kwargs_prompt_id": [
    "langchain",
    "prompts",
    "prompt",
    "PromptTemplate"
  ],
  "kwargs_prompt_kwargs_template": "\nUse the following pieces of context to answer the question at the end.  If you do not know the answer, please think rationally and answer from your own knowledge base.\n\n{context}\n\n{issue_type} definition:\n{definition}\n\n\nBelow is a Stack Overflow question posted by a user related to documentation replication on other examples.\n\nQuestion Title: {title}\nQuestion Body: {body}\n\n{task}. And this customization will enhance the understandability of the documentation while avoiding any questions like this in future.\n",
  "kwargs_prompt_kwargs_input_variables": [
    "body",
    "context",
    "definition",
    "issue_type",
    "task",
    "title"
  ],
  "kwargs_prompt_kwargs_template_format": "f-string",
  "step": 3,
  "starts": 2,
  "ends": 0,
  "errors": 1,
  "text_ctr": 0,
  "chain_starts": 2,
  "chain_ends": 0,
  "llm_starts": 0,
  "llm_ends": 0,
  "llm_streams": 0,
  "tool_starts": 0,
  "tool_ends": 0,
  "agent_ends": 0,
  "retriever_starts": 0,
  "retriever_ends": 0,
  "inputs": "input_list=\nUse the following pieces of context to answer the question at the end.  If you do not know the answer, please think rationally and answer from your own knowledge base.\n\nTitle: how to load and use a saved model on tensorflow?\n\nBody: <p>I have found 2 ways to save a model in Tensorflow: <code>tf.train.Saver()</code> and <code>SavedModelBuilder</code>. However, <strong>I can't find documentation on using the model</strong> after it being loaded  the second way.</p>\nTitle: How to restore dangling tf.py_func within the tf.data.Dataset() with tf.saved_model API?\n\nBody: <p>After doing a research for restoring the <code>tf.py_func()</code> when using saved_model API in vain, I couldn't find other information than documented in <a href=\"https://www.tensorflow.org/api_docs/python/tf/py_func\" rel=\"nofollow noreferrer\">tensorflow</a>:</p>\nTitle: KeyError: u'NearestNeighbors' on loading saved model from tf.contrib.factorization.KMeansClustering\n\nBody: <p>I am trying to do the following:</p>\n\n<ul>\n<li>Run kmeans clustering using tensorflow (1.8.0)</li>\n<li>Save the model using <code>kmeans.export_savedmodel</code></li>\n<li>Use the model using <code>tf.saved_model.loader.load</code></li>\n</ul>\n\nInadequate Examples definition:\nIssues that mention the documentation has insufficient examples.\n\n\nBelow is a Stack Overflow question posted by a user related to documentation replication on other examples.\n\nQuestion Title: How to retrain a model that was saved using the tf.saved_model.save() function in Tensorflow\nQuestion Body: <p>I am building a Neural Machine Translator for English to Konkani (a local language) language using the Transformer architecture proposed by (Vaswani et, al. 2017). I am following the tutorial code from <a href=\"https://www.tensorflow.org/text/tutorials/transformer\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/text/tutorials/transformer</a>. I have trained the model and used the <code>tf.saved_model.save()</code> method to save the model files locally.</p>\n<p>I now want to retrain that saved model on a new dataset that I have gathered recently, but I've realised that after loading the model using the <code>tf.saved_model.load()</code> method, I am not able to train it again as the loaded model now lacks the necessary method <code>model.fit()</code> .</p>\n<p>Here is a part of the model training code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>class Transformer(tf.keras.Model):\n  def __init__(self, *, num_layers, d_model, num_heads, dff,\n               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n    super().__init__()\n    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=input_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n                           num_heads=num_heads, dff=dff,\n                           vocab_size=target_vocab_size,\n                           dropout_rate=dropout_rate)\n\n    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n  def call(self, inputs):\n    # To use a Keras model with `.fit` you must pass all your inputs in the\n    # first argument.\n    context, x  = inputs\n\n    context = self.encoder(context)  # (batch_size, context_len, d_model)\n\n    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n\n    # Final linear layer output.\n    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n\n    try:\n      # Drop the keras mask, so it doesn't scale the losses/metrics.\n      # b/250038731\n      del logits._keras_mask\n    except AttributeError:\n      pass\n\n    # Return the final output and the attention weights.\n    return logits\n#-----------------------------------------------------------------------\n\n#...&lt;code to define optimizers and loss functions&gt;...\n\n# This Class acts as an interface for the Transformer\nclass Translator(tf.Module):\n  def __init__(self, context_tokenizers, target_tokenizers, transformer):\n    self.context_tokenizers = context_tokenizers\n    self.target_tokenizers = target_tokenizers\n    self.transformer = transformer\n\n  def __call__(self, sentence, max_length=MAX_TOKENS): #max_length=MAX_TOKENS\n    assert isinstance(sentence, tf.Tensor)\n    if len(sentence.shape) == 0:\n      sentence = sentence[tf.newaxis]\n\n    sentence = tokenize(sentence,self.context_tokenizers).to_tensor()\n\n    encoder_input = sentence\n\n    # As the output language is English, initialize the output with the\n    # English `[START]` token.\n\n    start_end = tokenize('',self.target_tokenizers)[0]\n    start = start_end[0][tf.newaxis]\n    end = start_end[-1][tf.newaxis]\n\n    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n    output_array = output_array.write(0, start)\n\n    for i in tf.range(max_length):\n      output = tf.transpose(output_array.stack())\n      predictions = self.transformer([encoder_input, output], training=False)\n\n      # Select the last token from the `seq_len` dimension.\n      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n\n      predicted_id = tf.argmax(predictions, axis=-1)\n\n      # Concatenate the `predicted_id` to the output which is given to the\n      # decoder as its input.\n      output_array = output_array.write(i+1, predicted_id[0])\n\n      if predicted_id == end:\n        break\n\n    output = tf.transpose(output_array.stack())\n    # The output shape is `(1, tokens)`.\n\n    text = self.target_tokenizers.detokenize(output)\n\n    tokens = tf.gather(target_vocab, output)\n\n    # `tf.function` prevents us from using the attention_weights that were\n    # calculated on the last iteration of the loop.\n    # So, recalculate them outside the loop.\n    self.transformer([encoder_input, output[:,:-1]], training=False)\n    attention_weights = self.transformer.decoder.last_attn_scores\n\n    joined_text = tf.strings.reduce_join(text[0][1:-1], separator=' ', axis=-1)\n    return joined_text, tokens, attention_weights\n#-----------------------------------------------------------------------\n\nclass ExportTranslator(tf.Module):\n  def __init__(self, translator):\n    self.translator = translator\n\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n  def __call__(self, sentence):\n    (result,\n     tokens,\n     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n\n    return result\n#-----------------------------------------------------------------------\n\ntransformer = Transformer(\n    num_layers=num_layers,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    input_vocab_size=context_vocab_size,\n    target_vocab_size=target_vocab_size,\n    dropout_rate=dropout_rate)\n\ntransformer.compile(\n    loss=masked_loss,\n    optimizer=optimizer,\n    metrics=[masked_accuracy])\n\n# training the model on the training data for some epochs\ntransformer.fit(train_batches,\n                epochs=20,\n                validation_data=val_batches,\n                callbacks=[\n                  tf.keras.callbacks.EarlyStopping(patience=3)],\n                )\n\ntranslator = Translator(context_tokenizer, target_tokenizer, transformer)\n\nexp_translator = ExportTranslator(translator)\n\n#saving the model\ntf.saved_model.save(exp_translator, export_dir=MODEL_SAVED_FILES)\n\n#-----------------------------------------------------------------------\n\n#loading a saved model\nreloaded = tf.saved_model.load(MODEL_SAVED_FILES)\n</code></pre>\n<p>Here's the error I get when I try to retrain the model using the following code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>reloaded = tf.saved_model.load(MODEL_SAVED_FILES)\n\n#retraing the model on new dataset\nreloaded.translator.transformer.fit(train_batches,\n                epochs=20,\n                validation_data=val_batches,\n                callbacks=[\n                  tf.keras.callbacks.EarlyStopping(patience=3)],\n                )\n</code></pre>\n<p>The error:</p>\n<pre class=\"lang-py prettyprint-override\"><code>\n---------------------------------------------------------------------------\n\nAttributeError                            Traceback (most recent call last)\n\n&lt;ipython-input-41-ad1b625ff6c0&gt; in &lt;cell line: 2&gt;()\n      1 #retraing the model on new dataset\n----&gt; 2 reloaded.translator.transformer.fit(train_batches,\n      3                 epochs=20,\n      4                 validation_data=val_batches,\n      5                 callbacks=[\n\nAttributeError: '_UserObject' object has no attribute 'fit'\n</code></pre>\n<p>After reading the documentation I've realised that when saving the model in the above method, the <code>model.fit()</code> and other methods are not saved hence they are not callable.</p>\n<p>I need help in finding a way to retrain my saved model, It is not feasible for me to train a new model on a combined dataset as It will take up lot of time and I have very limited resources. I have been looking up on the web for days but couldn't find a solution. Any help in this regards will be appreciated!</p>\n\n\nProvide multiple complete examples to showcase its usability with respect to the question asked by the user.. And this customization will enhance the understandability of the documentation while avoiding any questions like this in future.\n"
}