QuestionID,API Name,Course_URLs,YT_URLs,SO_URLs,Docs
51586693,tf.gather,"{'https://www.edx.org/learn/tensorflow', 'https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187', 'https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/', 'https://www.udemy.com/course/modern-computer-vision/', 'https://www.udemy.com/course/complete-machine-learning-and-data-science-zero-to-mastery/', 'https://www.coursera.org/learn/getting-started-with-tensor-flow2', 'https://www.coursera.org/learn/building-deep-learning-models-with-tensorflow', 'https://www.udemy.com/course/deep-learning-tensorflow-2/', 'https://www.udemy.com/course/deep-learning-masterclass-with-tensorflow-2-over-15-projects/', 'https://www.coursera.org/learn/introduction-tensorflow/reviews'}",{'https://www.youtube.com/watch?v=W5TTgLqVVE8'},"{'https://stackoverflow.com/questions/55375665/how-to-reintroduce-none-batch-dimension-to-tensor-in-keras-tensorflow', 'https://stackoverflow.com/questions/34092850/how-do-i-fix-a-dimension-error-in-tensorflow', 'https://stackoverflow.com/questions/60486437/add-none-dimension-in-tensorflow-2-0', 'https://stackoverflow.com/questions/36966316/how-to-get-the-dimensions-of-a-tensor-in-tensorflow-at-graph-construction-time', 'https://stackoverflow.com/questions/34718736/dynamic-size-for-tf-zeros-for-use-with-placeholders-with-none-dimensions', 'https://stackoverflow.com/questions/59607850/is-there-a-way-to-apply-a-function-to-dimension-0-of-a-tensorflow-array-having-t'}","??[[Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nIs there a way to apply a function to dimension 0 of a tensorflow array having the shape (None, 2)\n\nAsked 4 years, 3 months ago\n\nModified 4 years, 3 months ago\n\nI have a very technical question related to TensorFlow.\n\nI have a TensorFlow matrix having a dimension of (None, 2). I need to apply a function, say some_function, only on Dimension 0 of the matrix i.e. over all rows. The issue is dimension 0 is a None type (it is dynamic as it depends on the input size being fed to the NN model), and it gives an error showing None is not an integer type. There are two tf functions: tf.map_fn and tf.scan to iterate over a Tensorflow array. But both won\'t work over a None dimension.\n\nMaybe you could check it by defining a test TensorFlow array of shape (None, 2) and try applying any test function to the first dimension. Any help/input would be appreciated!\n\nImprove this question\n\nedited Jan 6, 2020 at 7:12\n\n1,96455 gold badges2727 silver badges4141 bronze badges\n\nasked Jan 6, 2020 at 6:52\n\nManav MishraManav Mishra\n\n11322 silver badges77 bronze badges 6\n\nWhich TF version are you using?\n\n– thushv89 Jan 6, 2020 at 6:56\n\n– Manav Mishra Jan 6, 2020 at 6:57\n\nUsually None in this context means that that dimension is arbitrary. Often that dimension is reserved for the number of training examples, meaning that any function you apply is going to be applied to each element of your data. Perhaps some data preprocessing could help you deal with this?\n\n– chang_trenton Jan 6, 2020 at 6:57\n\n@ManavMishra, can you share code how you defined a Tensor with a None dimension? Because you cannot define tf.Tensor with None dimensions (you were able to use placeholders to achieve this in TF 1). Keras model output is the only instance I can think, where you\'d end up with a None dimension.\n\n– thushv89 Jan 6, 2020 at 6:59\n\nI have a NN model defined using tf.keras taking in 2 inputs and giving 2 outputs. So my model.output is a tensor with shape (None,2). You could think the outputs to be x and y, my goal is to apply a function f(x,y) that works over all the inputs.\n\n– Manav Mishra Jan 6, 2020 at 7:08\n\n | Show 1 more comment\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nSince this is a keras model output, if I try to do the following,\n\nres2 = tf.map_fn(lambda y: y*2, model.output)\n\nTypeError: \'Tensor\' object cannot be interpreted as an integer\n\nBut, the following would work,\n\n# Inital model that produces the output you want to map model = tf.keras.models.Sequential() model.add(tf.keras.layers.Dense(2, input_shape=(2,))) res = tf.keras.layers.Lambda(lambda x: tf.map_fn(lambda y: y*2, x))(model.output)\n\nThen you define a new model, and use that to get the result of the tf.map_fn.\n\nmodel2 = tf.keras.Model(inputs=model.inputs, outputs=res) print(model2.predict(np.array([[1,2],[3,4]])))\n\nPS: But this is nothing to do with the first dimension being None. tf.map_fn can deal with None dimension just fine. You can verify this by running tf.map_fn on a tf.placeholder([None,2]) in TF 1.x.\n\nBecause it is iteratively applying a function over that dimension and does not need to know the size to do so.\n\nedited Jan 6, 2020 at 7:46\n\nanswered Jan 6, 2020 at 7:40\n\n11.2k11 gold badge2828 silver badges4141 bronze badges 1\n\nI have two questions: 1. Shouldn\'t the inputs be model.output rather than model.input since I am using the NN output as function arguments? 2. My actual function is: def tf2(vec): [x,y] = vec; x1 = tf.math.atanh(x); y1 = tf.math.atanh(y); return tf.math.exp(-x1**2 + -y1**2)*(x1**2 + y1**2) And in this scenario, res = tf.keras.layers.Lambda(lambda x: tf.map_fn(lambda y: tf2(y), x))(model.output)ails. How do I fix this?\n\n– Manav Mishra Jan 6, 2020 at 12:19\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nneural-network or ask your own question.\n\nWant to be a great software engineer? Don’t be a jerk.\n\nClimbing the GenAI decision tree\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\nPausing the 1-rep voting experiment on Stack Overflow: reflecting on the...\n\n2 TypeError: \'Tensor\' object cannot be interpreted as an integer when using tf.map_fn()\n\n4 Apply a function to the 0-dimension of an ndarray\n\n1 How to omit zeroes in a 1-D tensorflow tensor\n\n3 Tensorflow: how to apply a function to the last dimension of a tensor\n\n1 How do you set numpy array elements to zero according to length vector\n\n0 How to set array entries to zero in Python\n\n1 Apply function to 3D tensor while ignoring zero rows and padding\n\n1 How tensorflow reshape a tensor with dimension of None, like this [None,2,2,1]?\n\n1 Generate a zero matrix from a vector with None Shape\n\n1 How to apply function to each element of a 2d tensor?\n\n1 tensorflow add \'None\' dimension to a tensor\n\nHot Network Questions\n\nDo faster responding low-pass filters exist compared to a traditional RC filter?\n\nDo you say ""my car is high on fuel"" as a counterpart of ""my car is low on fuel""?\n\nWhen teaching Computer Architecture, why are universities using obscure or even made-up CPUs? Why not x86, ARM or RISC-V?\n\nWhat\'s the name of the room where you watch a movie inside the movie theater?\n\nHow to make a Data Modifying CTE abort if an update only partially succeeded?\n\nFixed Repeating Output\n\nTried to use cig lighter in my 12 volt plug now charger won’t work\n\nColorbar to illustrate the change of a specific parameter\n\nTo what extent can citizens of democracies be held responsible for the acts of their governments?\n\nHow do I prompt GPT-4 to look at a PDF in Jupyter Notebook?\n\n\'I can consent to be coerced at a later time\' Is this a logical paradox?\n\nHow to equally split college fund between 2 children going to college 5 years apart?\n\nCould variation of the universal constants affect computation?\n\nCo-Existing with a Highly Extroverted Leader and ""Forced Fun""\n\nFirst mention of Einstein in Science Fiction?\n\nWhat bike should i buy if i want to fit a child seat and live in area with a lot of hills and\n\nThe dimension of the Clifford algebra for the Dirac equation\n\nHow important was the US steel industry to the allies during World War II?\n\nDo batteries have capacitance?\n\nHow do I properly exit a program and return to the CCP in CP/M?\n\nWhy does a goddess put a low limit on the number of priests?\n\nWhat\'s to stop domain registrars from price gouging renewals?\n\nAnother more sophisticated and/or elegant way of saying ""Sort/Kind of related/similar"" more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_0', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nIs there a way to apply a function to dimension 0 of a tensorflow array having the shape (None, 2)\n\nAsked 4 years, 3 months ago\n\nModified 4 years, 3 months ago\n\nI have a very technical question related to TensorFlow.\n\nI have a TensorFlow matrix having a dimension of (None, 2). I need to apply a function, say some_function, only on Dimension 0 of the matrix i.e. over all rows. The issue is dimension 0 is a None type (it is dynamic as it depends on the input size being fed to the NN model), and it gives an error showing None is not an integer type. There are two tf functions: tf.map_fn and tf.scan to iterate over a Tensorflow array. But both won\'t work over a None dimension.\n\nMaybe you could check it by defining a test TensorFlow array of shape (None, 2) and try applying any test function to the first dimension. Any help/input would be appreciated!\n\nImprove this question\n\nedited Jan 6, 2020 at 7:12\n\n1,96455 gold badges2727 silver badges4141 bronze badges\n\nasked Jan 6, 2020 at 6:52\n\nManav MishraManav Mishra\n\n11322 silver badges77 bronze badges 6\n\nWhich TF version are you using?\n\n– thushv89 Jan 6, 2020 at 6:56\n\n– Manav Mishra Jan 6, 2020 at 6:57\n\nUsually None in this context means that that dimension is arbitrary. Often that dimension is reserved for the number of training examples, meaning that any function you apply is going to be applied to each element of your data. Perhaps some data preprocessing could help you deal with this?\n\n– chang_trenton Jan 6, 2020 at 6:57\n\n@ManavMishra, can you share code how you defined a Tensor with a None dimension? Because you cannot define tf.Tensor with None dimensions (you were able to use placeholders to achieve this in TF 1). Keras model output is the only instance I can think, where you\'d end up with a None dimension.\n\n– thushv89 Jan 6, 2020 at 6:59\n\nI have a NN model defined using tf.keras taking in 2 inputs and giving 2 outputs. So my model.output is a tensor with shape (None,2). You could think the outputs to be x and y, my goal is to apply a function f(x,y) that works over all the inputs.\n\n– Manav Mishra Jan 6, 2020 at 7:08\n\n | Show 1 more comment\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nSince this is a keras model output, if I try to do the following,\n\nres2 = tf.map_fn(lambda y: y*2, model.output)\n\nTypeError: \'Tensor\' object cannot be interpreted as an integer\n\nBut, the following would work,\n\n# Inital model that produces the output you want to map model = tf.keras.models.Sequential() model.add(tf.keras.layers.Dense(2, input_shape=(2,))) res = tf.keras.layers.Lambda(lambda x: tf.map_fn(lambda y: y*2, x))(model.output)\n\nThen you define a new model, and use that to get the result of the tf.map_fn.\n\nmodel2 = tf.keras.Model(inputs=model.inputs, outputs=res) print(model2.predict(np.array([[1,2],[3,4]])))\n\nPS: But this is nothing to do with the first dimension being None. tf.map_fn can deal with None dimension just fine. You can verify this by running tf.map_fn on a tf.placeholder([None,2]) in TF 1.x.\n\nBecause it is iteratively applying a function over that dimension and does not need to know the size to do so.\n\nedited Jan 6, 2020 at 7:46\n\nanswered Jan 6, 2020 at 7:40\n\n11.2k11 gold badge2828 silver badges4141 bronze badges 1\n\nI have two questions: 1. Shouldn\'t the inputs be model.output rather than model.input since I am using the NN output as function arguments? 2. My actual function is: def tf2(vec): [x,y] = vec; x1 = tf.math.atanh(x); y1 = tf.math.atanh(y); return tf.math.exp(-x1**2 + -y1**2)*(x1**2 + y1**2) And in this scenario, res = tf.keras.layers.Lambda(lambda x: tf.map_fn(lambda y: tf2(y), x))(model.output)ails. How do I fix this?\n\n– Manav Mishra Jan 6, 2020 at 12:19\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nneural-network or ask your own question.\n\nWant to be a great software engineer? Don’t be a jerk.\n\nClimbing the GenAI decision tree\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\nPausing the 1-rep voting experiment on Stack Overflow: reflecting on the...\n\n2 TypeError: \'Tensor\' object cannot be interpreted as an integer when using tf.map_fn()\n\n4 Apply a function to the 0-dimension of an ndarray\n\n1 How to omit zeroes in a 1-D tensorflow tensor\n\n3 Tensorflow: how to apply a function to the last dimension of a tensor\n\n1 How do you set numpy array elements to zero according to length vector\n\n0 How to set array entries to zero in Python\n\n1 Apply function to 3D tensor while ignoring zero rows and padding\n\n1 How tensorflow reshape a tensor with dimension of None, like this [None,2,2,1]?\n\n1 Generate a zero matrix from a vector with None Shape\n\n1 How to apply function to each element of a 2d tensor?\n\n1 tensorflow add \'None\' dimension to a tensor\n\nHot Network Questions\n\nDo faster responding low-pass filters exist compared to a traditional RC filter?\n\nDo you say ""my car is high on fuel"" as a counterpart of ""my car is low on fuel""?\n\nWhen teaching Computer Architecture, why are universities using obscure or even made-up CPUs? Why not x86, ARM or RISC-V?\n\nWhat\'s the name of the room where you watch a movie inside the movie theater?\n\nHow to make a Data Modifying CTE abort if an update only partially succeeded?\n\nFixed Repeating Output\n\nTried to use cig lighter in my 12 volt plug now charger won’t work\n\nColorbar to illustrate the change of a specific parameter\n\nTo what extent can citizens of democracies be held responsible for the acts of their governments?\n\nHow do I prompt GPT-4 to look at a PDF in Jupyter Notebook?\n\n\'I can consent to be coerced at a later time\' Is this a logical paradox?\n\nHow to equally split college fund between 2 children going to college 5 years apart?\n\nCould variation of the universal constants affect computation?\n\nCo-Existing with a Highly Extroverted Leader and ""Forced Fun""\n\nFirst mention of Einstein in Science Fiction?\n\nWhat bike should i buy if i want to fit a child seat and live in area with a lot of hills and\n\nThe dimension of the Clifford algebra for the Dirac equation\n\nHow important was the US steel industry to the allies during World War II?\n\nDo batteries have capacitance?\n\nHow do I properly exit a program and return to the CCP in CP/M?\n\nWhy does a goddess put a low limit on the number of priests?\n\nWhat\'s to stop domain registrars from price gouging renewals?\n\nAnother more sophisticated and/or elegant way of saying ""Sort/Kind of related/similar"" more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-04-11T13:44:39', 'title': 'python - Is there a way to apply a function to dimension 0 of a tensorflow array having the shape (None, 2) - Stack Overflow', 'url': 'https://stackoverflow.com/questions/59607850/is-there-a-way-to-apply-a-function-to-dimension-0-of-a-tensorflow-array-having-t'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nDynamic size for tf.zeros() (for use with placeholders with None dimensions)\n\nAsked 8 years, 3 months ago\n\nModified 6 years, 6 months ago\n\nConsider the following code:\n\nx = tf.placeholder(""float"", shape=[42, 4]) y = tf.zeros([42, 4], ""float"") xy_stacked = tf.concat(1, [x, y]) print(x.get_shape()) print(y.get_shape()) print(xy_stacked.get_shape())\n\nThis will produce the following output, as expected:\n\nTensorShape([Dimension(42), Dimension(4)]) TensorShape([Dimension(42), Dimension(4)]) TensorShape([Dimension(42), Dimension(8)])\n\nHowever, what if the placeholder has a dynamic dimension that is determined at run-time by the value passed to feed_dict=, as placeholders often do:\n\nx = tf.placeholder(""float"", shape=[None, 4]) y = tf.zeros([None, 4], ""float"") xy_stacked = tf.concat(1, [x, y])\n\nThis will produce an error for tf.zeros([None, 4], ""float""). Apparently Dimension(None) is not allowed for tf.zeros:\n\nTypeError Traceback (most recent call last) <ipython-input-24-277eca38a392> in <module>() 2 3 x = tf.placeholder(""float"", shape=[None, 4]) ----> 4 y = tf.zeros([None, 4], ""float"") 5 xy_stacked = tf.concat(1, [x, y]) 6 [...] /usr/local/lib/python3.4/dist-packages/numpy/core/_methods.py in _prod(a, axis, dtype, out, keepdims) 33 34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False): ---> 35 return umr_prod(a, axis, dtype, out, keepdims) 36 37 def _any(a, axis=None, dtype=None, out=None, keepdims=False): TypeError: unsupported operand type(s) for *: \'NoneType\' and \'int\'\n\nI have figured out that it does not produce an error if I set the first dimension of my zeros tensor to non-None, such as 1:\n\nx = tf.placeholder(""float"", shape=[None, 4]) y = tf.zeros([1, 4], ""float"") xy_stacked = tf.concat(1, [x, y])\n\nbut then the resulting xy_stacked tensor is truncated to this size:\n\nTensorShape([Dimension(None), Dimension(4)]) TensorShape([Dimension(1), Dimension(4)]) TensorShape([Dimension(1), Dimension(8)])\n\nHow can I pad the placeholder tensor with zeros so I get a tensor of shape TensorShape([Dimension(None), Dimension(8)]) in this example?\n\nThe only ""solutions"" I found so far is either something like the following:\n\nx = tf.placeholder(""float"", shape=[None, 4]) y = 0 * x xy_stacked = tf.concat(1, [x, y])\n\nOr simply declaring y as a placeholder and always passing a zero array of the right size.\n\nBut neither looks like a clean solution to the problem and hacks like that get out of hand quickly in an application more complex than this simple example..\n\nI\'m using tensorflow-0.6.0-py3.\n\nImprove this question\n\nedited Jan 11, 2016 at 12:31\n\n24.8k88 gold badges7373 silver badges141141 bronze badges\n\nasked Jan 11, 2016 at 10:06\n\nCliffordViennaCliffordVienna\n\n8,11511 gold badge3838 silver badges5757 bronze badges 2\n\nwhat happens with y=tf.zeros_like(x)?\n\n– user728291 Jan 11, 2016 at 11:36\n\n@user728291 then I get something of shape TensorShape([Dimension(None), Dimension(None)]) for y and xy_stacked!? The 0 * x hack I mentioned in the question seems to makes more sense if x actually should have the same shape as y. In my real app everything is more complicated of course and I have to construct the zeros tensor by slicing, duplicating and concatenating the result of 0 * <placeholder> to get a tensor of the shape I need.\n\n– CliffordVienna Jan 11, 2016 at 11:55\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe recommended way to make a zero tensor with the same shape as another tensor is to use the tf.zeros_like() op:\n\nx = tf.placeholder(tf.float32, shape=[None, 4]) y = tf.zeros_like(x)\n\nThe resulting tensor y appears to have the shape [None, None] according to Tensor.get_shape(), but at runtime it will expand to the same shape as x:\n\nprint y.get_shape() # ==> TensorShape([Dimension(None), Dimension(None)]) sess = tf.Session() y_result = sess.run(y, feed_dict={x: np.random.rand(4, 4)}) print y_result.shape # ==> (4, 4)\n\nThe [None, None] static shape is returned because shape inference hasn\'t been specialized for tf.zeros_like(). I\'ve filed a GitHub issue for that and it should be fixed soon.\n\nEDIT: In your comment, you asked how to deal with the case where the zero tensor had a shape based on, but different from the original tensor. This is also possible, using tf.shape() and tf.stack() to build the dimensions, and tf.fill() to produce the zero tensor:\n\nx = tf.placeholder(tf.float32, shape=[None, 4]) # Use tf.shape() to get the runtime size of `x` in the 0th dimension. zeros_dims = tf.stack([tf.shape(x)[0], 7]) y = tf.fill(zeros_dims, 0.0) sess = tf.Session() y_result = sess.run(y, feed_dict={x: np.random.rand(4, 4)}) print y_result.shape # ==> (4, 7)\n\nedited Oct 16, 2017 at 1:17\n\nanswered Jan 11, 2016 at 15:40\n\n126k2626 gold badges401401 silver badges400400 bronze badges 3\n\nWhat do I do if the 2nd dimension does not match? E.g. I want x to have shape=[None, 4] and y to have shape=[None, 7]. (In my app the 2nd dimensions of those tensors are independent configuration parameters. I can\'t even tell in advance which one is larger.) Sorry for not mentioning that in the question in the first place..\n\n– CliffordVienna Jan 11, 2016 at 16:06\n\nUpdated my answer to cover your case.\n\n– mrry Jan 11, 2016 at 16:26\n\nNote that on tf 1.x+ tf.pack was renamed to tf.stack\n\n– Yuval Atzmon Oct 15, 2017 at 11:31\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nNet neutrality is in; TikTok and noncompetes are out\n\nUpcoming research at Stack Overflow\n\nTesting a new version of Stack Overflow Jobs\n\nPausing the 1-rep voting experiment on Stack Overflow: reflecting on the...\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n6 TensorFlow - Defining the shape of a variable dynamically, depending on the shape of another variable\n\n3 read and write tfrecords binary file (type missmatch)\n\n0 Tensorflow feed_dict with list of floats and InvalidArgumentError\n\n3 In Tensorflow, how do I generate a scalar summary?\n\n0 Error with feed values for placeholders when running the merged summary op\n\n1 How to use a ValidationMonitor for an Estimator in TensorFlow 1.0?\n\n0 2D sparse input tensorflow\n\n1 tf.gradients application on a function\n\n0 tensorflow error - you must feed a value for placeholder tensor \'in\'\n\nHot Network Questions\n\nDoes damage taken from Warding Bond count as magical?\n\nHow do you correctly compile the chained comparison operators like ones that exist in Python (`a < b < c`), if `b` might have side-effects?\n\nDoes Hebrews 13:2 teach that God\'s elect have become angels in disguise?\n\nEfficient method of storing energy in a near-future, semi-hard sci-fi game\n\nDetermining Jordan canonical form(JCF) of an operator given by complex differentiation.\n\nWhy is COALESCE not a function?\n\nCompany threatening me after I changed my mind on joining them after observing their work style\n\nMovie where three people get locked in a hotel\n\nCan a district attorney dismiss their own traffic ticket?\n\nHow do you balance and transport a container of concrete on a ridged roof?\n\nCan my step dad steal from my room when I’m gone\n\nIs there any potential ambiguity in this phrase from Xenophon?\n\nWho are the enslaved mutants shown working as laborers in Cable\'s dark future timeline in X-Men \'97?\n\nHow to solve water pooling in crawl space?\n\nHow do these trees not collapse?\n\nScratch or crack in alloy headtube?\n\nUsing smartphone boarding pass 15 hours before flight\n\nIs there a true one-dimensional object?\n\nShould I use an author\'s code made available on request to help retract their highly cited paper?\n\nStruggling with PhD in Switzerland: should I transfer to a more favorable climate?\n\nIs there any halachic basis for ""dairy out""?\n\nEye dialect in Chinese\n\nCan lattice vectors have negative components for DFT calculations?\n\nCompiler bug or am I am using constexpr wrongly? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_1', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nDynamic size for tf.zeros() (for use with placeholders with None dimensions)\n\nAsked 8 years, 3 months ago\n\nModified 6 years, 6 months ago\n\nConsider the following code:\n\nx = tf.placeholder(""float"", shape=[42, 4]) y = tf.zeros([42, 4], ""float"") xy_stacked = tf.concat(1, [x, y]) print(x.get_shape()) print(y.get_shape()) print(xy_stacked.get_shape())\n\nThis will produce the following output, as expected:\n\nTensorShape([Dimension(42), Dimension(4)]) TensorShape([Dimension(42), Dimension(4)]) TensorShape([Dimension(42), Dimension(8)])\n\nHowever, what if the placeholder has a dynamic dimension that is determined at run-time by the value passed to feed_dict=, as placeholders often do:\n\nx = tf.placeholder(""float"", shape=[None, 4]) y = tf.zeros([None, 4], ""float"") xy_stacked = tf.concat(1, [x, y])\n\nThis will produce an error for tf.zeros([None, 4], ""float""). Apparently Dimension(None) is not allowed for tf.zeros:\n\nTypeError Traceback (most recent call last) <ipython-input-24-277eca38a392> in <module>() 2 3 x = tf.placeholder(""float"", shape=[None, 4]) ----> 4 y = tf.zeros([None, 4], ""float"") 5 xy_stacked = tf.concat(1, [x, y]) 6 [...] /usr/local/lib/python3.4/dist-packages/numpy/core/_methods.py in _prod(a, axis, dtype, out, keepdims) 33 34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False): ---> 35 return umr_prod(a, axis, dtype, out, keepdims) 36 37 def _any(a, axis=None, dtype=None, out=None, keepdims=False): TypeError: unsupported operand type(s) for *: \'NoneType\' and \'int\'\n\nI have figured out that it does not produce an error if I set the first dimension of my zeros tensor to non-None, such as 1:\n\nx = tf.placeholder(""float"", shape=[None, 4]) y = tf.zeros([1, 4], ""float"") xy_stacked = tf.concat(1, [x, y])\n\nbut then the resulting xy_stacked tensor is truncated to this size:\n\nTensorShape([Dimension(None), Dimension(4)]) TensorShape([Dimension(1), Dimension(4)]) TensorShape([Dimension(1), Dimension(8)])\n\nHow can I pad the placeholder tensor with zeros so I get a tensor of shape TensorShape([Dimension(None), Dimension(8)]) in this example?\n\nThe only ""solutions"" I found so far is either something like the following:\n\nx = tf.placeholder(""float"", shape=[None, 4]) y = 0 * x xy_stacked = tf.concat(1, [x, y])\n\nOr simply declaring y as a placeholder and always passing a zero array of the right size.\n\nBut neither looks like a clean solution to the problem and hacks like that get out of hand quickly in an application more complex than this simple example..\n\nI\'m using tensorflow-0.6.0-py3.\n\nImprove this question\n\nedited Jan 11, 2016 at 12:31\n\n24.8k88 gold badges7373 silver badges141141 bronze badges\n\nasked Jan 11, 2016 at 10:06\n\nCliffordViennaCliffordVienna\n\n8,11511 gold badge3838 silver badges5757 bronze badges 2\n\nwhat happens with y=tf.zeros_like(x)?\n\n– user728291 Jan 11, 2016 at 11:36\n\n@user728291 then I get something of shape TensorShape([Dimension(None), Dimension(None)]) for y and xy_stacked!? The 0 * x hack I mentioned in the question seems to makes more sense if x actually should have the same shape as y. In my real app everything is more complicated of course and I have to construct the zeros tensor by slicing, duplicating and concatenating the result of 0 * <placeholder> to get a tensor of the shape I need.\n\n– CliffordVienna Jan 11, 2016 at 11:55\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe recommended way to make a zero tensor with the same shape as another tensor is to use the tf.zeros_like() op:\n\nx = tf.placeholder(tf.float32, shape=[None, 4]) y = tf.zeros_like(x)\n\nThe resulting tensor y appears to have the shape [None, None] according to Tensor.get_shape(), but at runtime it will expand to the same shape as x:\n\nprint y.get_shape() # ==> TensorShape([Dimension(None), Dimension(None)]) sess = tf.Session() y_result = sess.run(y, feed_dict={x: np.random.rand(4, 4)}) print y_result.shape # ==> (4, 4)\n\nThe [None, None] static shape is returned because shape inference hasn\'t been specialized for tf.zeros_like(). I\'ve filed a GitHub issue for that and it should be fixed soon.\n\nEDIT: In your comment, you asked how to deal with the case where the zero tensor had a shape based on, but different from the original tensor. This is also possible, using tf.shape() and tf.stack() to build the dimensions, and tf.fill() to produce the zero tensor:\n\nx = tf.placeholder(tf.float32, shape=[None, 4]) # Use tf.shape() to get the runtime size of `x` in the 0th dimension. zeros_dims = tf.stack([tf.shape(x)[0], 7]) y = tf.fill(zeros_dims, 0.0) sess = tf.Session() y_result = sess.run(y, feed_dict={x: np.random.rand(4, 4)}) print y_result.shape # ==> (4, 7)\n\nedited Oct 16, 2017 at 1:17\n\nanswered Jan 11, 2016 at 15:40\n\n126k2626 gold badges401401 silver badges400400 bronze badges 3\n\nWhat do I do if the 2nd dimension does not match? E.g. I want x to have shape=[None, 4] and y to have shape=[None, 7]. (In my app the 2nd dimensions of those tensors are independent configuration parameters. I can\'t even tell in advance which one is larger.) Sorry for not mentioning that in the question in the first place..\n\n– CliffordVienna Jan 11, 2016 at 16:06\n\nUpdated my answer to cover your case.\n\n– mrry Jan 11, 2016 at 16:26\n\nNote that on tf 1.x+ tf.pack was renamed to tf.stack\n\n– Yuval Atzmon Oct 15, 2017 at 11:31\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nNet neutrality is in; TikTok and noncompetes are out\n\nUpcoming research at Stack Overflow\n\nTesting a new version of Stack Overflow Jobs\n\nPausing the 1-rep voting experiment on Stack Overflow: reflecting on the...\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n6 TensorFlow - Defining the shape of a variable dynamically, depending on the shape of another variable\n\n3 read and write tfrecords binary file (type missmatch)\n\n0 Tensorflow feed_dict with list of floats and InvalidArgumentError\n\n3 In Tensorflow, how do I generate a scalar summary?\n\n0 Error with feed values for placeholders when running the merged summary op\n\n1 How to use a ValidationMonitor for an Estimator in TensorFlow 1.0?\n\n0 2D sparse input tensorflow\n\n1 tf.gradients application on a function\n\n0 tensorflow error - you must feed a value for placeholder tensor \'in\'\n\nHot Network Questions\n\nDoes damage taken from Warding Bond count as magical?\n\nHow do you correctly compile the chained comparison operators like ones that exist in Python (`a < b < c`), if `b` might have side-effects?\n\nDoes Hebrews 13:2 teach that God\'s elect have become angels in disguise?\n\nEfficient method of storing energy in a near-future, semi-hard sci-fi game\n\nDetermining Jordan canonical form(JCF) of an operator given by complex differentiation.\n\nWhy is COALESCE not a function?\n\nCompany threatening me after I changed my mind on joining them after observing their work style\n\nMovie where three people get locked in a hotel\n\nCan a district attorney dismiss their own traffic ticket?\n\nHow do you balance and transport a container of concrete on a ridged roof?\n\nCan my step dad steal from my room when I’m gone\n\nIs there any potential ambiguity in this phrase from Xenophon?\n\nWho are the enslaved mutants shown working as laborers in Cable\'s dark future timeline in X-Men \'97?\n\nHow to solve water pooling in crawl space?\n\nHow do these trees not collapse?\n\nScratch or crack in alloy headtube?\n\nUsing smartphone boarding pass 15 hours before flight\n\nIs there a true one-dimensional object?\n\nShould I use an author\'s code made available on request to help retract their highly cited paper?\n\nStruggling with PhD in Switzerland: should I transfer to a more favorable climate?\n\nIs there any halachic basis for ""dairy out""?\n\nEye dialect in Chinese\n\nCan lattice vectors have negative components for DFT calculations?\n\nCompiler bug or am I am using constexpr wrongly? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-05-01T18:13:36', 'title': 'tensorflow - Dynamic size for tf.zeros() (for use with placeholders with None dimensions) - Stack Overflow', 'url': 'https://stackoverflow.com/questions/34718736/dynamic-size-for-tf-zeros-for-use-with-placeholders-with-none-dimensions'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow to get the dimensions of a tensor (in TensorFlow) at graph construction time?\n\nAsked 8 years, 1 month ago\n\nModified 2 years, 3 months ago\n\nI am trying an Op that is not behaving as expected.\n\ngraph = tf.Graph() with graph.as_default(): train_dataset = tf.placeholder(tf.int32, shape=[128, 2]) embeddings = tf.Variable( tf.random_uniform([50000, 64], -1.0, 1.0)) embed = tf.nn.embedding_lookup(embeddings, train_dataset) embed = tf.reduce_sum(embed, reduction_indices=0)\n\nSo I need to know the dimensions of the Tensor embed. I know that it can be done at the run time but it\'s too much work for such a simple operation. What\'s the easier way to do it?\n\nedited Jan 18, 2018 at 20:25\n\n60k1515 gold badges168168 silver badges156156 bronze badges\n\nasked May 1, 2016 at 11:49\n\n9,22488 gold badges4444 silver badges5050 bronze badges 0\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nI see most people confused about tf.shape(tensor) and tensor.get_shape() Let\'s make it clear:\n\ntf.shape is used for dynamic shape. If your tensor\'s shape is changable, use it. An example: a input is an image with changable width and height, we want resize it to half of its size, then we can write something like: new_height = tf.shape(image)[0] / 2\n\ntensor.get_shape is used for fixed shapes, which means the tensor\'s shape can be deduced in the graph.\n\nConclusion: tf.shape can be used almost anywhere, but t.get_shape only for shapes can be deduced from graph.\n\nedited Apr 9, 2017 at 4:16\n\nanswered Apr 8, 2017 at 6:23\n\n75155 silver badges55 bronze badges\n\nTensor.get_shape() from this post.\n\nc = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) print(c.get_shape()) ==> TensorShape([Dimension(2), Dimension(3)])\n\nedited May 23, 2017 at 11:54\n\nanswered May 1, 2016 at 13:06\n\n9,22488 gold badges4444 silver badges5050 bronze badges 2\n\nIf anyone wonders: tf.shape(c) returns a 1-D integer tensor representing the shape of c. In the example given in this answer, tf.shape(c) returns Tensor(""Shape:0"", shape=(2,), dtype=int32)\n\n– Franck Dernoncourt Commented Feb 18, 2017 at 16:12\n\n@nobar if the dimension is None (i.e., if it is unspecified), you might need to use tf.shape(c). For example, if a = tf.placeholder(tf.int32, (None,2)), and you run tf.Session().run(tf.constant(a.get_shape().as_list()[0]), {a:[[1,2]]}) you will get error, but you can get the dimension by: tf.Session().run(tf.shape(a)[0], {a:[[1,2]]}).\n\n– Saeid BK Commented Apr 9, 2017 at 2:31\n\nA function to access the values:\n\ndef shape(tensor): s = tensor.get_shape() return tuple([s[i].value for i in range(0, len(s))])\n\nbatch_size, num_feats = shape(logits)\n\nanswered Jan 20, 2017 at 19:48\n\nColin SwaneyColin Swaney\n\n14111 silver badge22 bronze badges 1\n\nreturn tuple(tensor.get_shape().as_list()) if you want a tuple, or directly return the python list as in return tensor.get_shape().as_list()\n\n– Mr_and_Mrs_D Commented Mar 27, 2018 at 10:07\n\nJust print out the embed after construction graph (ops) without running:\n\nimport tensorflow as tf ... train_dataset = tf.placeholder(tf.int32, shape=[128, 2]) embeddings = tf.Variable( tf.random_uniform([50000, 64], -1.0, 1.0)) embed = tf.nn.embedding_lookup(embeddings, train_dataset) print (embed)\n\nThis will show the shape of the embed tensor:\n\nTensor(""embedding_lookup:0"", shape=(128, 2, 64), dtype=float32)\n\nUsually, it\'s good to check shapes of all tensors before training your models.\n\nanswered May 1, 2016 at 13:08\n\n8,49799 gold badges3535 silver badges4343 bronze badges 1\n\nWhile the answer I gave before you post yours was correct, your answer gives more information about the tensor than just its shape, hence, I accept it as the correct answer ;)\n\n– Thoran Commented May 1, 2016 at 13:13\n\nLet\'s make it simple as hell. If you want a single number for the number of dimensions like 2, 3, 4, etc., then just use tf.rank(). But, if you want the exact shape of the tensor then use tensor.get_shape()\n\nwith tf.Session() as sess: arr = tf.random_normal(shape=(10, 32, 32, 128)) a = tf.random_gamma(shape=(3, 3, 1), alpha=0.1) print(sess.run([tf.rank(arr), tf.rank(a)])) print(arr.get_shape(), "", "", a.get_shape()) # for tf.rank() [4, 3] # for tf.get_shape() Output: (10, 32, 32, 128) , (3, 3, 1)\n\nanswered Apr 8, 2017 at 16:57\n\n60k1515 gold badges168168 silver badges156156 bronze badges\n\nThe method tf.shape is a TensorFlow static method. However, there is also the method get_shape for the Tensor class. See\n\nhttps://www.tensorflow.org/api_docs/python/tf/Tensor#get_shape\n\nanswered Nov 9, 2017 at 19:38\n\n1122 bronze badges 1\n\nNone really - I was just trying to explain it as succinctly as possible ;-)\n\n– cliffberg Commented Nov 10, 2017 at 20:07\n\nTo create tensor in tensorflow using tf.constant()\n\nThis is to import the library\n\nimport tensorflow as tf\n\nThis will create the tensor\n\ntensor = tf.constant([[[2,4,5], [5,6,6]], [[9,7,8], [4,8,2]], [[7,1,3], [4,8,9]]])\n\nThis will show the tensor\n\nthis will show the number of dimension\n\nanswered Mar 28, 2022 at 14:33\n\nHappy N. MondayHappy N. Monday\n\n43111 gold badge44 silver badges88 bronze badges\n\ntensor = tf.constant([[[1, 2, 3], [3, 4, 5]], [[5, 6, 7], [8, 6, 9]], [[2, 1, 5], [5, 7, 8]]]) tensor\n\n<tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy= array([[[1, 2, 3],[3, 4, 5]], [[5, 6, 7], [8, 6, 9]], [[2, 1, 5], [5, 7, 8]]], dtype=int32)>\n\nanswered Mar 28, 2022 at 14:39\n\nGrace U. NnejiGrace U. Nneji\n\n35155 silver badges88 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensor or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nThe return of Staging Ground to Stack Overflow\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\n65 tf.shape() get wrong shape in tensorflow\n\n34 Tensor with unspecified dimension in tensorflow\n\n2 How to divide a tensor of shape of NoneType?\n\n1 Pad a variable size tensor to a specific size in a custom layer\n\n-1 What are the TensorFlow equivalents of these PyTorch functions?\n\n3889 How do I get the current time in Python?\n\n5488 How to access the index value in a \'for\' loop?\n\n4409 How to find the index for a given item in a list?\n\n2763 How do I get the last element of a list?\n\n3853 How to catch multiple exceptions in one line? (in the ""except"" block)\n\n3213 How do I print colored text to the terminal?\n\n3216 How do I change the size of figures drawn with Matplotlib?\n\n1770 How do I get time of a Python program\'s execution?\n\nHot Network Questions\n\nDoes Bluetooth not work on devices without GPS?\n\nWhat rights does an employee retain, if any, who does not consent to being monitored on a work IT system?\n\nIssues with my D&D group\n\nDo I need to staple cable for new wire run through a preexisting wall?\n\nParity of a number with missing digits\n\nAbstract symbols in an artistic grid… with a secret meaning\n\nWhy am I unable to distribute rotated text evenly in Adobe Illustrator 2024?\n\nWhy does a battery have a limit for current in amperes?\n\nHow to avoid pgfornament being clipped by its bounding box?\n\nHow can I enable read only mode in microSD card\n\nA TCP server which uses one thread to read while writing data with another thread\n\nTiny book about a planet full of man-eating sunflowers\n\nShort story about soldiers who are fighting against an enemy which turns out to be themselves\n\nC# Linked List implementation\n\nWhy does the Clausius inequality involve a single term/integral if we consider a body interacting with multiple heat sources/sinks?\n\nDerivative of the Score Function in Fisher Information\n\nAre there really half-a billion visible supernovae exploding all the time?\n\nIs the new series for 𝜋 a Big (or even Medium) Deal?\n\nHow to join two PCBs with a very small separation?\n\nRAW, do transparent obstacles generally grant Total Cover?\n\nHow is it possible to supply an LM833D with +12V and -8V supply?\n\nCreating a command to display blackboard font\n\nWould a PhD from Europe, Canada, Australia, or New Zealand be accepted in the US? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_2', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow to get the dimensions of a tensor (in TensorFlow) at graph construction time?\n\nAsked 8 years, 1 month ago\n\nModified 2 years, 3 months ago\n\nI am trying an Op that is not behaving as expected.\n\ngraph = tf.Graph() with graph.as_default(): train_dataset = tf.placeholder(tf.int32, shape=[128, 2]) embeddings = tf.Variable( tf.random_uniform([50000, 64], -1.0, 1.0)) embed = tf.nn.embedding_lookup(embeddings, train_dataset) embed = tf.reduce_sum(embed, reduction_indices=0)\n\nSo I need to know the dimensions of the Tensor embed. I know that it can be done at the run time but it\'s too much work for such a simple operation. What\'s the easier way to do it?\n\nedited Jan 18, 2018 at 20:25\n\n60k1515 gold badges168168 silver badges156156 bronze badges\n\nasked May 1, 2016 at 11:49\n\n9,22488 gold badges4444 silver badges5050 bronze badges 0\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nI see most people confused about tf.shape(tensor) and tensor.get_shape() Let\'s make it clear:\n\ntf.shape is used for dynamic shape. If your tensor\'s shape is changable, use it. An example: a input is an image with changable width and height, we want resize it to half of its size, then we can write something like: new_height = tf.shape(image)[0] / 2\n\ntensor.get_shape is used for fixed shapes, which means the tensor\'s shape can be deduced in the graph.\n\nConclusion: tf.shape can be used almost anywhere, but t.get_shape only for shapes can be deduced from graph.\n\nedited Apr 9, 2017 at 4:16\n\nanswered Apr 8, 2017 at 6:23\n\n75155 silver badges55 bronze badges\n\nTensor.get_shape() from this post.\n\nc = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) print(c.get_shape()) ==> TensorShape([Dimension(2), Dimension(3)])\n\nedited May 23, 2017 at 11:54\n\nanswered May 1, 2016 at 13:06\n\n9,22488 gold badges4444 silver badges5050 bronze badges 2\n\nIf anyone wonders: tf.shape(c) returns a 1-D integer tensor representing the shape of c. In the example given in this answer, tf.shape(c) returns Tensor(""Shape:0"", shape=(2,), dtype=int32)\n\n– Franck Dernoncourt Commented Feb 18, 2017 at 16:12\n\n@nobar if the dimension is None (i.e., if it is unspecified), you might need to use tf.shape(c). For example, if a = tf.placeholder(tf.int32, (None,2)), and you run tf.Session().run(tf.constant(a.get_shape().as_list()[0]), {a:[[1,2]]}) you will get error, but you can get the dimension by: tf.Session().run(tf.shape(a)[0], {a:[[1,2]]}).\n\n– Saeid BK Commented Apr 9, 2017 at 2:31\n\nA function to access the values:\n\ndef shape(tensor): s = tensor.get_shape() return tuple([s[i].value for i in range(0, len(s))])\n\nbatch_size, num_feats = shape(logits)\n\nanswered Jan 20, 2017 at 19:48\n\nColin SwaneyColin Swaney\n\n14111 silver badge22 bronze badges 1\n\nreturn tuple(tensor.get_shape().as_list()) if you want a tuple, or directly return the python list as in return tensor.get_shape().as_list()\n\n– Mr_and_Mrs_D Commented Mar 27, 2018 at 10:07\n\nJust print out the embed after construction graph (ops) without running:\n\nimport tensorflow as tf ... train_dataset = tf.placeholder(tf.int32, shape=[128, 2]) embeddings = tf.Variable( tf.random_uniform([50000, 64], -1.0, 1.0)) embed = tf.nn.embedding_lookup(embeddings, train_dataset) print (embed)\n\nThis will show the shape of the embed tensor:\n\nTensor(""embedding_lookup:0"", shape=(128, 2, 64), dtype=float32)\n\nUsually, it\'s good to check shapes of all tensors before training your models.\n\nanswered May 1, 2016 at 13:08\n\n8,49799 gold badges3535 silver badges4343 bronze badges 1\n\nWhile the answer I gave before you post yours was correct, your answer gives more information about the tensor than just its shape, hence, I accept it as the correct answer ;)\n\n– Thoran Commented May 1, 2016 at 13:13\n\nLet\'s make it simple as hell. If you want a single number for the number of dimensions like 2, 3, 4, etc., then just use tf.rank(). But, if you want the exact shape of the tensor then use tensor.get_shape()\n\nwith tf.Session() as sess: arr = tf.random_normal(shape=(10, 32, 32, 128)) a = tf.random_gamma(shape=(3, 3, 1), alpha=0.1) print(sess.run([tf.rank(arr), tf.rank(a)])) print(arr.get_shape(), "", "", a.get_shape()) # for tf.rank() [4, 3] # for tf.get_shape() Output: (10, 32, 32, 128) , (3, 3, 1)\n\nanswered Apr 8, 2017 at 16:57\n\n60k1515 gold badges168168 silver badges156156 bronze badges\n\nThe method tf.shape is a TensorFlow static method. However, there is also the method get_shape for the Tensor class. See\n\nhttps://www.tensorflow.org/api_docs/python/tf/Tensor#get_shape\n\nanswered Nov 9, 2017 at 19:38\n\n1122 bronze badges 1\n\nNone really - I was just trying to explain it as succinctly as possible ;-)\n\n– cliffberg Commented Nov 10, 2017 at 20:07\n\nTo create tensor in tensorflow using tf.constant()\n\nThis is to import the library\n\nimport tensorflow as tf\n\nThis will create the tensor\n\ntensor = tf.constant([[[2,4,5], [5,6,6]], [[9,7,8], [4,8,2]], [[7,1,3], [4,8,9]]])\n\nThis will show the tensor\n\nthis will show the number of dimension\n\nanswered Mar 28, 2022 at 14:33\n\nHappy N. MondayHappy N. Monday\n\n43111 gold badge44 silver badges88 bronze badges\n\ntensor = tf.constant([[[1, 2, 3], [3, 4, 5]], [[5, 6, 7], [8, 6, 9]], [[2, 1, 5], [5, 7, 8]]]) tensor\n\n<tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy= array([[[1, 2, 3],[3, 4, 5]], [[5, 6, 7], [8, 6, 9]], [[2, 1, 5], [5, 7, 8]]], dtype=int32)>\n\nanswered Mar 28, 2022 at 14:39\n\nGrace U. NnejiGrace U. Nneji\n\n35155 silver badges88 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensor or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nThe return of Staging Ground to Stack Overflow\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\n65 tf.shape() get wrong shape in tensorflow\n\n34 Tensor with unspecified dimension in tensorflow\n\n2 How to divide a tensor of shape of NoneType?\n\n1 Pad a variable size tensor to a specific size in a custom layer\n\n-1 What are the TensorFlow equivalents of these PyTorch functions?\n\n3889 How do I get the current time in Python?\n\n5488 How to access the index value in a \'for\' loop?\n\n4409 How to find the index for a given item in a list?\n\n2763 How do I get the last element of a list?\n\n3853 How to catch multiple exceptions in one line? (in the ""except"" block)\n\n3213 How do I print colored text to the terminal?\n\n3216 How do I change the size of figures drawn with Matplotlib?\n\n1770 How do I get time of a Python program\'s execution?\n\nHot Network Questions\n\nDoes Bluetooth not work on devices without GPS?\n\nWhat rights does an employee retain, if any, who does not consent to being monitored on a work IT system?\n\nIssues with my D&D group\n\nDo I need to staple cable for new wire run through a preexisting wall?\n\nParity of a number with missing digits\n\nAbstract symbols in an artistic grid… with a secret meaning\n\nWhy am I unable to distribute rotated text evenly in Adobe Illustrator 2024?\n\nWhy does a battery have a limit for current in amperes?\n\nHow to avoid pgfornament being clipped by its bounding box?\n\nHow can I enable read only mode in microSD card\n\nA TCP server which uses one thread to read while writing data with another thread\n\nTiny book about a planet full of man-eating sunflowers\n\nShort story about soldiers who are fighting against an enemy which turns out to be themselves\n\nC# Linked List implementation\n\nWhy does the Clausius inequality involve a single term/integral if we consider a body interacting with multiple heat sources/sinks?\n\nDerivative of the Score Function in Fisher Information\n\nAre there really half-a billion visible supernovae exploding all the time?\n\nIs the new series for 𝜋 a Big (or even Medium) Deal?\n\nHow to join two PCBs with a very small separation?\n\nRAW, do transparent obstacles generally grant Total Cover?\n\nHow is it possible to supply an LM833D with +12V and -8V supply?\n\nCreating a command to display blackboard font\n\nWould a PhD from Europe, Canada, Australia, or New Zealand be accepted in the US? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-06-26T14:10:26', 'title': 'python - How to get the dimensions of a tensor (in TensorFlow) at graph construction time? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/36966316/how-to-get-the-dimensions-of-a-tensor-in-tensorflow-at-graph-construction-time'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow to reintroduce (None, ) batch dimension to tensor in Keras / Tensorflow?\n\nAsked 5 years, 3 months ago\n\nModified 4 years, 5 months ago\n\nI am trying to create a tensorflow model using Keras that is compatible with Google’s Machine Learning Engine. I have an existing trained Keras model which takes a vector float input. I am introducing a string vector input layer to the front of the existing model. This would pass the string to be preprocessed. I am trying to preprocess image data using a Lambda layer. While preprocessing, in order to decode the string jpeg data, I would need to remove the batch dimension from the tensor. After preprocessing, I would need to reintroduce the “None” batch dimension. This is where I’m facing the issue. There seems to be no way to reintroduce “None” as the batch dimension. Google ML Engine requires that the batch dimension should be unknown all the way through the entire model.\n\nTensorflow version: 1.12 Keras version: 2.2.4 OS: Debian Linux (VM instance) Python version: 2.7\n\nI have tried: 1. Reshape() with both [None,299,299,3] as well as with [-1,299,299,3]. Both don’t work as required\n\ntf.reshape as above. Does not work.\n\nimg_height=299 img_width=299 inputs = Input(shape=[1],dtype=tf.string) inputs_inter1 = Lambda(preprocess_input, output_shape=(img_height,img_width,3))(inputs) print(inputs_inter1.shape) print(""Combining with string vector input"") combine_out = trainedmodel(inputs_inter1) Combinedmodel = Model(inputs,combine_out) input_tensor = Combinedmodel.inputs[0] output_tensor = Combinedmodel.outputs[0] print(""Inputs: ""+str(input_tensor)) print(""Outputs: ""+str(output_tensor))\n\ndef preprocess_input(x): import tensorflow as tf x=tf.reshape(x,()) x = tf.image.decode_jpeg(x,channels=3) x = tf.image.resize_images(x,(299,299)) x = tf.cast(x, tf.float32) x = tf.math.divide(x, 255.0) x = tf.math.subtract(x, 0.5) x = tf.math.multiply(x, 2.0) x = tf.expand_dims(x,0) return x\n\nInputs: Tensor(""input_1_1:0"", shape=(?, 1), dtype=string)\n\nOutputs: Tensor(""model_2/model_1/dense_2/Softmax:0"", shape=(?, 8), dtype=float32)\n\nInputs: Tensor(""input_1_1:0"", shape=(?, 1), dtype=string)\n\nOutputs: Tensor(""model_2/model_1/dense_2/Softmax:0"", shape=(1, 8), dtype=float32)\n\nasked Mar 27, 2019 at 11:04\n\n8111 silver badge88 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nAnswering my own question.\n\nThe trick is to create a new placeholder with the required dimensions [None,299,299,3], copy the preprocessed tensor into it and return that placeholder from the Lambda function/layer.\n\ndef preprocess_input(x): import tensorflow as tf x=tf.reshape(x,()) x = tf.image.decode_jpeg(x,channels=3) x = tf.image.resize_images(x,(299,299)) x = tf.cast(x, tf.float32) x = tf.math.divide(x, 255.0) x = tf.math.subtract(x, 0.5) x = tf.math.multiply(x, 2.0) x = tf.placeholder_with_default(x,[None,299,299,3]) return x\n\nThe usage of tf.placeholder_with_default can be found here: https://www.tensorflow.org/api_docs/python/tf/placeholder_with_default\n\nETA: Latest Tensorflow 2.0 has a backwards compatible placeholder with default function. Can be used in the short-term.\n\ntf.compat.v1.placeholder_with_default\n\nedited Jan 11, 2020 at 18:52\n\nanswered Mar 31, 2019 at 2:53\n\n8111 silver badge88 bronze badges 2\n\nAlas but tf. placeholder_with_default() is now deprecated for TF2.0.\n\n– Hephaestus Commented Jan 4, 2020 at 0:16\n\n@Hephaestus Thanks for the info. Edited the answer to include TF2.0 backwards compatibility function.\n\n– rahullak Commented Jan 11, 2020 at 18:53\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nThe return of Staging Ground to Stack Overflow\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nShould we burninate the [lib] tag?\n\n0 How to do broadcasting addition in Keras Functional API?\n\n1 Get batch input tensor based on batch number\n\n3 Keras: Construct a full tensor of the same batch size as a given tensor\n\n2 Keras repeat_elements with unknown batch_size\n\n2 Reshape a tensor with None for batch size\n\n3 How do I re-batch a tensor in Tensorflow?\n\n5 Preserving unknown batch dimension for custom static tensors in Tensorflow\n\n1 How to prevent Tensorflow Input from generating batch dimension\n\n0 How to expand the dimension of each batch in a tensorflow dataset\n\n1 Tensorflow with batch size and wrong dimesion\n\n0 Keep the batch dimension of Conv1D Variable\n\nHot Network Questions\n\nWhy did Geordi have his visor replaced with ocular implants between Generations and First Contact?\n\nHow to prevent Dataset to format list values?\n\nWhy does the Clausius inequality involve a single term/integral if we consider a body interacting with multiple heat sources/sinks?\n\nDetect the social network of a given URL\n\nIs it unethical to have a videogame open on a personal device and interact with it occasionally as part of taking breaks while working from home?\n\nShouldn\'t this est be a sunt in this sentence?\n\nParis Taxi with children seats (from and to airport)\n\nA puzzle from YOU to ME ;)\n\nIs FDISK /MBR really undocumented, and why?\n\nWhat is the mode of operation of a Hobb\'s meter?\n\nWhat is a quarter in 19th-century England converted to contemporary pints?\n\nIs it possible to animate a curve along it\'s own path?\n\nWhy does c show up in Schwarzschild\'s equation for the horizon radius?\n\nParity of a number with missing digits\n\nConstant curvature on a sphere?\n\nIs it legal to discriminate on marital status for car insurance/pensions etc.?\n\nDid James Madison say or write that the 10 Commandments are critical to the US nation?\n\nDo wererats take falling damage?\n\nHow to turn a desert into a fertile farmland with engineering?\n\nIs it possible to give an unambiguous definition to the concept of “information”?\n\nWhat kind of publications can I submit on my own without the need of supervisors approval?\n\nHistorically are there any documented attempts at finding algorithms that are asymptotically faster than the FFT for the Discrete Fourier Transform?\n\nWhy is SRAM/Quarq Drop-In BB Power Meter not Ai-Compatible?\n\nProof/Reference to a claim about AC and definable real numbers more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_3', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow to reintroduce (None, ) batch dimension to tensor in Keras / Tensorflow?\n\nAsked 5 years, 3 months ago\n\nModified 4 years, 5 months ago\n\nI am trying to create a tensorflow model using Keras that is compatible with Google’s Machine Learning Engine. I have an existing trained Keras model which takes a vector float input. I am introducing a string vector input layer to the front of the existing model. This would pass the string to be preprocessed. I am trying to preprocess image data using a Lambda layer. While preprocessing, in order to decode the string jpeg data, I would need to remove the batch dimension from the tensor. After preprocessing, I would need to reintroduce the “None” batch dimension. This is where I’m facing the issue. There seems to be no way to reintroduce “None” as the batch dimension. Google ML Engine requires that the batch dimension should be unknown all the way through the entire model.\n\nTensorflow version: 1.12 Keras version: 2.2.4 OS: Debian Linux (VM instance) Python version: 2.7\n\nI have tried: 1. Reshape() with both [None,299,299,3] as well as with [-1,299,299,3]. Both don’t work as required\n\ntf.reshape as above. Does not work.\n\nimg_height=299 img_width=299 inputs = Input(shape=[1],dtype=tf.string) inputs_inter1 = Lambda(preprocess_input, output_shape=(img_height,img_width,3))(inputs) print(inputs_inter1.shape) print(""Combining with string vector input"") combine_out = trainedmodel(inputs_inter1) Combinedmodel = Model(inputs,combine_out) input_tensor = Combinedmodel.inputs[0] output_tensor = Combinedmodel.outputs[0] print(""Inputs: ""+str(input_tensor)) print(""Outputs: ""+str(output_tensor))\n\ndef preprocess_input(x): import tensorflow as tf x=tf.reshape(x,()) x = tf.image.decode_jpeg(x,channels=3) x = tf.image.resize_images(x,(299,299)) x = tf.cast(x, tf.float32) x = tf.math.divide(x, 255.0) x = tf.math.subtract(x, 0.5) x = tf.math.multiply(x, 2.0) x = tf.expand_dims(x,0) return x\n\nInputs: Tensor(""input_1_1:0"", shape=(?, 1), dtype=string)\n\nOutputs: Tensor(""model_2/model_1/dense_2/Softmax:0"", shape=(?, 8), dtype=float32)\n\nInputs: Tensor(""input_1_1:0"", shape=(?, 1), dtype=string)\n\nOutputs: Tensor(""model_2/model_1/dense_2/Softmax:0"", shape=(1, 8), dtype=float32)\n\nasked Mar 27, 2019 at 11:04\n\n8111 silver badge88 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nAnswering my own question.\n\nThe trick is to create a new placeholder with the required dimensions [None,299,299,3], copy the preprocessed tensor into it and return that placeholder from the Lambda function/layer.\n\ndef preprocess_input(x): import tensorflow as tf x=tf.reshape(x,()) x = tf.image.decode_jpeg(x,channels=3) x = tf.image.resize_images(x,(299,299)) x = tf.cast(x, tf.float32) x = tf.math.divide(x, 255.0) x = tf.math.subtract(x, 0.5) x = tf.math.multiply(x, 2.0) x = tf.placeholder_with_default(x,[None,299,299,3]) return x\n\nThe usage of tf.placeholder_with_default can be found here: https://www.tensorflow.org/api_docs/python/tf/placeholder_with_default\n\nETA: Latest Tensorflow 2.0 has a backwards compatible placeholder with default function. Can be used in the short-term.\n\ntf.compat.v1.placeholder_with_default\n\nedited Jan 11, 2020 at 18:52\n\nanswered Mar 31, 2019 at 2:53\n\n8111 silver badge88 bronze badges 2\n\nAlas but tf. placeholder_with_default() is now deprecated for TF2.0.\n\n– Hephaestus Commented Jan 4, 2020 at 0:16\n\n@Hephaestus Thanks for the info. Edited the answer to include TF2.0 backwards compatibility function.\n\n– rahullak Commented Jan 11, 2020 at 18:53\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nThe return of Staging Ground to Stack Overflow\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nShould we burninate the [lib] tag?\n\n0 How to do broadcasting addition in Keras Functional API?\n\n1 Get batch input tensor based on batch number\n\n3 Keras: Construct a full tensor of the same batch size as a given tensor\n\n2 Keras repeat_elements with unknown batch_size\n\n2 Reshape a tensor with None for batch size\n\n3 How do I re-batch a tensor in Tensorflow?\n\n5 Preserving unknown batch dimension for custom static tensors in Tensorflow\n\n1 How to prevent Tensorflow Input from generating batch dimension\n\n0 How to expand the dimension of each batch in a tensorflow dataset\n\n1 Tensorflow with batch size and wrong dimesion\n\n0 Keep the batch dimension of Conv1D Variable\n\nHot Network Questions\n\nWhy did Geordi have his visor replaced with ocular implants between Generations and First Contact?\n\nHow to prevent Dataset to format list values?\n\nWhy does the Clausius inequality involve a single term/integral if we consider a body interacting with multiple heat sources/sinks?\n\nDetect the social network of a given URL\n\nIs it unethical to have a videogame open on a personal device and interact with it occasionally as part of taking breaks while working from home?\n\nShouldn\'t this est be a sunt in this sentence?\n\nParis Taxi with children seats (from and to airport)\n\nA puzzle from YOU to ME ;)\n\nIs FDISK /MBR really undocumented, and why?\n\nWhat is the mode of operation of a Hobb\'s meter?\n\nWhat is a quarter in 19th-century England converted to contemporary pints?\n\nIs it possible to animate a curve along it\'s own path?\n\nWhy does c show up in Schwarzschild\'s equation for the horizon radius?\n\nParity of a number with missing digits\n\nConstant curvature on a sphere?\n\nIs it legal to discriminate on marital status for car insurance/pensions etc.?\n\nDid James Madison say or write that the 10 Commandments are critical to the US nation?\n\nDo wererats take falling damage?\n\nHow to turn a desert into a fertile farmland with engineering?\n\nIs it possible to give an unambiguous definition to the concept of “information”?\n\nWhat kind of publications can I submit on my own without the need of supervisors approval?\n\nHistorically are there any documented attempts at finding algorithms that are asymptotically faster than the FFT for the Discrete Fourier Transform?\n\nWhy is SRAM/Quarq Drop-In BB Power Meter not Ai-Compatible?\n\nProof/Reference to a claim about AC and definable real numbers more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-06-26T22:11:38', 'title': 'How to reintroduce (None, ) batch dimension to tensor in Keras / Tensorflow? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/55375665/how-to-reintroduce-none-batch-dimension-to-tensor-in-keras-tensorflow'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nAdd None dimension in tensorflow 2.0\n\nAsked 4 years, 4 months ago\n\nModified 8 months ago\n\nI have a tensor xx with shape:\n\n>>> xx.shape TensorShape([32, 32, 256])\n\nHow can I add a leading None dimension to get:\n\n>>> xx.shape TensorShape([None, 32, 32, 256])\n\nI have seen many answers here but all are related to TF 1.x\n\nWhat is the straight forward way for TF 2.0?\n\nImprove this question\n\nedited Jul 8, 2021 at 9:41\n\n4,88022 gold badges2222 silver badges3535 bronze badges\n\nasked Mar 2, 2020 at 10:02\n\n2,05722 gold badges1919 silver badges3636 bronze badges 2\n\nDoes the first dimension have to be None? You can easily add a singleton dimension with xx[tf.newaxis] or tf.expand_dims(xx, 0). In general, there are not many ways of ""erasing"" known shape information, specially if you are in eager mode where tensors are not symbolic. If you need None, what is the reason?\n\n– javidcf Commented Mar 2, 2020 at 10:22\n\n@jdehesa this is requied during by TF graph creation, the first dimension is None since its represents batch size. I am trying to implement a layer by myself: For example I want to take a tensor shaped (None, 32,32, 512) and to sum every four channels into one channel yy[:,:,:,0] = sum(xx[:,:,:,0:16:4]), yy[:,:,:,1] = sum(xx[:,:,:,1:17:4])...\n\n– Benny K Commented Mar 2, 2020 at 10:30\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou can either use ""None"" or numpy\'s ""newaxis"" to create the new dimension.\n\nGeneral Tip: You can also use None in place of np.newaxis; These are in fact the same objects.\n\nBelow is the code that explains both the options.\n\ntry: %tensorflow_version 2.x except Exception: pass import tensorflow as tf print(tf.__version__) # TensorFlow and tf.keras from tensorflow import keras # Helper libraries import numpy as np #### Import the Fashion MNIST dataset fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() #Original Dimension print(train_images.shape) train_images1 = train_images[None,:,:,:] #Add Dimension using None print(train_images1.shape) train_images2 = train_images[np.newaxis is None,:,:,:] #Add dimension using np.newaxis print(train_images2.shape) #np.newaxis and none are same np.newaxis is None\n\nThe Output of the above code is\n\n2.1.0 (60000, 28, 28) (1, 60000, 28, 28) (1, 60000, 28, 28) True\n\nedited Mar 2, 2020 at 12:12\n\nanswered Mar 2, 2020 at 12:04\n\nuser8879803user8879803\n\nI\'m not sure why this was accepted: wasn\'t the question how to get a leading None dimension, rather than a leading 1 dimension? Is there a way to get a leading None instead of a 1? I am asking for the same reason that @Benny K states, that it\'s required for TF graph creation and represents arbitrary batch size.\n\n– garythegoat Commented Mar 9, 2022 at 18:57\n\nIn TF2 you can use tf.expand_dims:\n\nxx = tf.expand_dims(xx, 0) xx.shape > TensorShape([1, 32, 32, 256])\n\nanswered Dec 9, 2021 at 16:40\n\nGerard CarbóGerard Carbó\n\n1,8851919 silver badges1616 bronze badges\n\nI do not think it is possible to simply add a ""None"" dimension.\n\nHowever, assuming you are trying to prepend a variable-size batch dimension to your tensor, you can just tf.repeat your tensor using the tf.shape() of another tensor.\n\ny = tf.keras.layers.Input(shape=(32, 32, 3)) # Shape: [None, 32, 32, 3] ... batch_size = tf.shape(y)[0] # Will be None initially, but is mutable xx = tf.ones(shape=(32, 32, 356)) # Shape: [32, 32, 356] xx = tf.expand_dims(xx, 0) # Shape: [1, 32, 32, 356] xx = tf.repeat(xx, repeats=batch_size, axis=0) # Shape: [None, 32, 32, 356]\n\nThis will likely be more useful than just hard-coding the first dimension to None since what you probably actually want to be doing is copying it along that first dimension based on batch size.\n\nanswered Oct 14, 2023 at 3:39\n\nNathan Van WoudenbergNathan Van Woudenberg\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow2.0 or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n0 How to use TensorFlow RelativePositionEmbedding layers with batches?\n\n0 keep None dimension when slicing (tf.gather) a tensor\n\n0 How to transform tensor with shape (4498,) to (None, 4498)?\n\n34 Tensor with unspecified dimension in tensorflow\n\n8 How to add dimension to a tensor using Tensorflow\n\n1 How tensorflow reshape a tensor with dimension of None, like this [None,2,2,1]?\n\n0 TensorFlow: Add dimension (column) with constant value\n\n0 Tensorflow Extra None dimension required for model\n\n2 How to make tensor to have four dimension?\n\n1 Add a dimension to a tensorflow tensor\n\n1 Dealing with none dimension of symbolic tensor when creating a layer\n\n1 tensorflow add \'None\' dimension to a tensor\n\n1 Tensorflow Keras Tensor Multiplication with None as First Dimension\n\nHot Network Questions\n\nReproducing Ruth Vollmer\'s acrylic Steiner surface\n\nIf someone clearly believes that he has witnessed something extraordinary very clearly, why is it more reasonable to believe that they hallucinated?\n\nHow to POSIX-ly ignore ""warning: command substitution: ignored null byte in input""?\n\nHow can I search File Explorer for files only (i.e. exclude folders) in Windows 10?\n\nWhat\'s the meaning of ""lex fundamentum est libertatis, qua fruimur. legum omnes servi sumus, ut liberi esse pssimus""?\n\nWhat is the maximum velocity possible for autofocus (focusing speed) on a Canon 5D Mark II\n\nWould moving the equator to the Tropic of Cancer increase Earth\'s arable land?\n\nIdentify the story about an author whose work-in-progress is completed by a computer\n\nFill the grid subject to product, sum and knight move constraints\n\nIs it an option for the ls utility specified in POSIX.1-2017?\n\n向こう as a pronoun (""he/she/they"")?\n\nWhy does the length of an antenna matter when electromagnetic waves propagate perpendicular to the antenna?\n\nCan compositions ""da + preposition"" be used as a relative pronoun?\n\nRecommend an essay, article, entry, author, or branch of philosophy that addresses the futility of arguing for or against free will\n\nRadioactive rocks and an unusual track\n\nIs a ""single"" cpu safer than multiple cores?\n\nIs ElGamal homomorphic encryption using additive groups works only for Discrete Log ElGamal? What about EC ElGamal?\n\nHow can I watch a timelapse movie on the Nikon D7100?\n\nSingle-qubit quantum channel from the CNOT gate\n\nShould I apologise to a professor after a gift authorship attempt, which they refused?\n\nWhy is there not a test for diagonalizability of a matrix\n\nInfinity is not a number\n\nDoes the damage from Thunderwave occur before or after the target is moved\n\nMathematica integral wrong more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_4', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nAdd None dimension in tensorflow 2.0\n\nAsked 4 years, 4 months ago\n\nModified 8 months ago\n\nI have a tensor xx with shape:\n\n>>> xx.shape TensorShape([32, 32, 256])\n\nHow can I add a leading None dimension to get:\n\n>>> xx.shape TensorShape([None, 32, 32, 256])\n\nI have seen many answers here but all are related to TF 1.x\n\nWhat is the straight forward way for TF 2.0?\n\nImprove this question\n\nedited Jul 8, 2021 at 9:41\n\n4,88022 gold badges2222 silver badges3535 bronze badges\n\nasked Mar 2, 2020 at 10:02\n\n2,05722 gold badges1919 silver badges3636 bronze badges 2\n\nDoes the first dimension have to be None? You can easily add a singleton dimension with xx[tf.newaxis] or tf.expand_dims(xx, 0). In general, there are not many ways of ""erasing"" known shape information, specially if you are in eager mode where tensors are not symbolic. If you need None, what is the reason?\n\n– javidcf Commented Mar 2, 2020 at 10:22\n\n@jdehesa this is requied during by TF graph creation, the first dimension is None since its represents batch size. I am trying to implement a layer by myself: For example I want to take a tensor shaped (None, 32,32, 512) and to sum every four channels into one channel yy[:,:,:,0] = sum(xx[:,:,:,0:16:4]), yy[:,:,:,1] = sum(xx[:,:,:,1:17:4])...\n\n– Benny K Commented Mar 2, 2020 at 10:30\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou can either use ""None"" or numpy\'s ""newaxis"" to create the new dimension.\n\nGeneral Tip: You can also use None in place of np.newaxis; These are in fact the same objects.\n\nBelow is the code that explains both the options.\n\ntry: %tensorflow_version 2.x except Exception: pass import tensorflow as tf print(tf.__version__) # TensorFlow and tf.keras from tensorflow import keras # Helper libraries import numpy as np #### Import the Fashion MNIST dataset fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() #Original Dimension print(train_images.shape) train_images1 = train_images[None,:,:,:] #Add Dimension using None print(train_images1.shape) train_images2 = train_images[np.newaxis is None,:,:,:] #Add dimension using np.newaxis print(train_images2.shape) #np.newaxis and none are same np.newaxis is None\n\nThe Output of the above code is\n\n2.1.0 (60000, 28, 28) (1, 60000, 28, 28) (1, 60000, 28, 28) True\n\nedited Mar 2, 2020 at 12:12\n\nanswered Mar 2, 2020 at 12:04\n\nuser8879803user8879803\n\nI\'m not sure why this was accepted: wasn\'t the question how to get a leading None dimension, rather than a leading 1 dimension? Is there a way to get a leading None instead of a 1? I am asking for the same reason that @Benny K states, that it\'s required for TF graph creation and represents arbitrary batch size.\n\n– garythegoat Commented Mar 9, 2022 at 18:57\n\nIn TF2 you can use tf.expand_dims:\n\nxx = tf.expand_dims(xx, 0) xx.shape > TensorShape([1, 32, 32, 256])\n\nanswered Dec 9, 2021 at 16:40\n\nGerard CarbóGerard Carbó\n\n1,8851919 silver badges1616 bronze badges\n\nI do not think it is possible to simply add a ""None"" dimension.\n\nHowever, assuming you are trying to prepend a variable-size batch dimension to your tensor, you can just tf.repeat your tensor using the tf.shape() of another tensor.\n\ny = tf.keras.layers.Input(shape=(32, 32, 3)) # Shape: [None, 32, 32, 3] ... batch_size = tf.shape(y)[0] # Will be None initially, but is mutable xx = tf.ones(shape=(32, 32, 356)) # Shape: [32, 32, 356] xx = tf.expand_dims(xx, 0) # Shape: [1, 32, 32, 356] xx = tf.repeat(xx, repeats=batch_size, axis=0) # Shape: [None, 32, 32, 356]\n\nThis will likely be more useful than just hard-coding the first dimension to None since what you probably actually want to be doing is copying it along that first dimension based on batch size.\n\nanswered Oct 14, 2023 at 3:39\n\nNathan Van WoudenbergNathan Van Woudenberg\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow2.0 or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n0 How to use TensorFlow RelativePositionEmbedding layers with batches?\n\n0 keep None dimension when slicing (tf.gather) a tensor\n\n0 How to transform tensor with shape (4498,) to (None, 4498)?\n\n34 Tensor with unspecified dimension in tensorflow\n\n8 How to add dimension to a tensor using Tensorflow\n\n1 How tensorflow reshape a tensor with dimension of None, like this [None,2,2,1]?\n\n0 TensorFlow: Add dimension (column) with constant value\n\n0 Tensorflow Extra None dimension required for model\n\n2 How to make tensor to have four dimension?\n\n1 Add a dimension to a tensorflow tensor\n\n1 Dealing with none dimension of symbolic tensor when creating a layer\n\n1 tensorflow add \'None\' dimension to a tensor\n\n1 Tensorflow Keras Tensor Multiplication with None as First Dimension\n\nHot Network Questions\n\nReproducing Ruth Vollmer\'s acrylic Steiner surface\n\nIf someone clearly believes that he has witnessed something extraordinary very clearly, why is it more reasonable to believe that they hallucinated?\n\nHow to POSIX-ly ignore ""warning: command substitution: ignored null byte in input""?\n\nHow can I search File Explorer for files only (i.e. exclude folders) in Windows 10?\n\nWhat\'s the meaning of ""lex fundamentum est libertatis, qua fruimur. legum omnes servi sumus, ut liberi esse pssimus""?\n\nWhat is the maximum velocity possible for autofocus (focusing speed) on a Canon 5D Mark II\n\nWould moving the equator to the Tropic of Cancer increase Earth\'s arable land?\n\nIdentify the story about an author whose work-in-progress is completed by a computer\n\nFill the grid subject to product, sum and knight move constraints\n\nIs it an option for the ls utility specified in POSIX.1-2017?\n\n向こう as a pronoun (""he/she/they"")?\n\nWhy does the length of an antenna matter when electromagnetic waves propagate perpendicular to the antenna?\n\nCan compositions ""da + preposition"" be used as a relative pronoun?\n\nRecommend an essay, article, entry, author, or branch of philosophy that addresses the futility of arguing for or against free will\n\nRadioactive rocks and an unusual track\n\nIs a ""single"" cpu safer than multiple cores?\n\nIs ElGamal homomorphic encryption using additive groups works only for Discrete Log ElGamal? What about EC ElGamal?\n\nHow can I watch a timelapse movie on the Nikon D7100?\n\nSingle-qubit quantum channel from the CNOT gate\n\nShould I apologise to a professor after a gift authorship attempt, which they refused?\n\nWhy is there not a test for diagonalizability of a matrix\n\nInfinity is not a number\n\nDoes the damage from Thunderwave occur before or after the target is moved\n\nMathematica integral wrong more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-08T12:58:01', 'title': 'tensorflow2.0 - Add None dimension in tensorflow 2.0 - Stack Overflow', 'url': 'https://stackoverflow.com/questions/60486437/add-none-dimension-in-tensorflow-2-0'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow do I fix a dimension error in TensorFlow?\n\nAsked 8 years, 4 months ago\n\nModified 8 years, 4 months ago\n\nI\'m trying to apply the expert portion of the tutorial to my own data but I keep running into dimension errors. Here\'s the code leading up to the error.\n\ndef weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\') def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\') W_conv1 = weight_variable([1, 8, 1, 4]) b_conv1 = bias_variable([4]) x_image = tf.reshape(tf_in, [-1,2,8,1]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1)\n\nAnd then when I try to run this command:\n\nW_conv2 = weight_variable([1, 4, 4, 8]) b_conv2 = bias_variable([8]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2)\n\nI get the following errors:\n\nValueError Traceback (most recent call last) <ipython-input-41-7ab0d7765f8c> in <module>() 3 4 h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) ----> 5 h_pool2 = max_pool_2x2(h_conv2) ValueError: (\'filter must not be larger than the input: \', \'Filter: [\', Dimension(2), \'x\', Dimension(2), \'] \', \'Input: [\', Dimension(1), \'x\', Dimension(4), \'] \')\n\nJust for some background information, the data that I\'m dealing with is a CSV file where each row contains 10 features and 1 empty column that can be a 1 or a 0. What I\'m trying to get is a probability in the empty column that the column will equal a 1.\n\nImprove this question\n\nedited Dec 9, 2015 at 18:34\n\nasked Dec 4, 2015 at 16:24\n\n3,28977 gold badges4141 silver badges6969 bronze badges 2\n\nWhat is tf_in? I\'m assuming it is the original 1x8 input.\n\n– erickrf Dec 4, 2015 at 17:47\n\ndata = genfromtxt(\'cs-training.csv\',delimiter=\',\'). A=data.shape[1]-1. tf_in = tf.placeholder(""float"", [None, A]).\n\n– Ravaal Dec 4, 2015 at 18:40\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou have to shape the input so it is compatible with both the training tensor and the output. If you input is length 1, your output should be length 1 (length is substituted for dimension).\n\nWhen you\'re dealing with-\n\ndef conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\') def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1], padding=\'SAME\')\n\nNotice how I changed the strides and the ksize to [1, 1, 1, 1]. This will match an output to a 1 dimensional input and prevent errors down the road.\n\nWhen you\'re defining your weight variable (see code below)-\n\ndef weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)\n\nyou\'re going to have to make the first 2 numbers conform to the feature tensor that you are using to train your model, the last two numbers will be the dimension of the predicted output (same as the dimension of the input).\n\nW_conv1 = weight_variable([1, 10, 1, 1]) b_conv1 = bias_variable([1])\n\nNotice the [1, 10, in the beginning which signifies that the feature tensor is going to be a 1x10 feature tensor; the last two numbers 1, 1] correspond to the dimensions of the input and output tensors/predictors.\n\nWhen you reshape your x_foo tensor (I call it x_ [x prime]), you, for whatever reason, have to define it like so-\n\nx_ = tf.reshape(x, [-1,1,10,1])\n\nNotice the 1 and 10 in the middle- ...1,10,.... Once again, these numbers correspond to the dimension of your feature tensor.\n\nFor every bias variable, you choose the final number of the previously defined variable. For example, if W_conv1 = weight_variable([1, 10, 1, 1]) appears like so, you take the final number and put that into your bias variable so it can match the dimensions of the input. This is done like so- b_conv1 = bias_variable([1]).\n\nIf you need any more explanation please comment below.\n\nanswered Dec 9, 2015 at 18:18\n\n3,28977 gold badges4141 silver badges6969 bronze badges\n\nThe dimensions you are using for the filter are not matching the output of the hidden layer.\n\nLet me see if I understood you: your input is composed of 8 features, and you want to reshape it into a 2x4 matrix, right?\n\nThe weights you created with weight_variable([1, 8, 1, 4]) expect a 1x8 input, in one channel, and produce a 1x8 output in 4 channels (or hidden units). The filter you are using sweeps the input in 2x2 squares. However, since the result of the weights is 1x8, they won\'t match.\n\nYou should reshape the input as\n\nx_image = tf.reshape(tf_in, [-1,2,4,1])\n\nNow, your input is actually 2x4 instead of 1x8. Then you need to change the weight shape to (2, 4, 1, hidden_units) to deal with a 2x4 output. It will also produce a 2x4 output, and the 2x2 filter now can be applied.\n\nAfter that, the filter will match the output of the weights. Also note that you will have to change the shape of your second weight matrix to weight_variable([2, 4, hidden_units, hidden2_units])\n\nedited Dec 4, 2015 at 20:00\n\nanswered Dec 4, 2015 at 17:59\n\n2,08655 gold badges2222 silver badges4444 bronze badges 4\n\nok so what I did was W_conv1 = weight_variable([2, 4, 1, 8]) and b_conv1 = bias_variable([8]). Then I did this x_image = tf.reshape(tf_in, [-1,2,4,1]). Then I get to this one and get an error- W_conv2 = weight_variable([2, 4, 4, 8]), b_conv2 = bias_variable([8]). I don\'t understand what you mean by hidden_units and hidden_units2 at the end. I assumed I could just throw in as many as I wanted but the tutorials make it seem like you always double the previous number. Even then I keep getting this error- ValueError: Dimensions Dimension(8) and Dimension(4) are not compatible.\n\n– Ravaal Dec 4, 2015 at 18:36\n\nOoops, I edited my answer to correct that. The shape of the second weights must match the size of the previous one, so it is [2, 4, hidden_units, hidden2_units]. Hidden units are the number of units in each hidden layer, i.e., their size. They correspond to the number of channels you produce in an image. The optimum size is problem dependent.\n\n– erickrf Dec 4, 2015 at 20:04\n\nI\'m not using images though. I\'m using a csv file where there are 8 features per row and one row to be predicted or given a probability of there being a 1 instead of a 0. Can you use this information to clarify what you mean?\n\n– Ravaal Dec 4, 2015 at 20:13\n\nValueError: (\'filter must not be larger than the input: \', \'Filter: [\', Dimension(2), \'x\', Dimension(4), \'] \', \'Input: [\', Dimension(1), \'x\', Dimension(2), \'] \'). I just changed the code to W_conv1 = weight_variable([2, 4, 1, 2]), b_conv1 = bias_variable([2]), x_image = tf.reshape(tf_in, [-1,2,4,1]), W_conv2 = weight_variable([2, 4, 2, 4]), b_conv2 = bias_variable([4]).\n\n– Ravaal Dec 4, 2015 at 20:18\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nSupporting the world’s most-used database engine through 2050\n\nWhat language should beginning programmers choose?\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\nPausing the 1-rep voting experiment on Stack Overflow: reflecting on the...\n\n0 TensorFlow: dimension error. how to debug?\n\n0 Dimensional Error in CNN\n\n2 Dimension Error WIth Tensorflow 2D Convolutional Layer\n\n1 Error TensorFlow ""Dimensions must be equal, but...""\n\n2 Input dimension error in Keras\n\n1 Tensorflow Dimension Error on reshape\n\n0 Tensorflow : ValueError: Dimensions must be equal\n\n0 Tensorflow dimension issue: ValueError: expected min_ndim=4, found ndim=3. Full shape received: (None, 3, 1)\n\n2 Tensorflow Input Dimension mismatch problem\n\n4 Cannot set tensor: Dimension mismatch\n\nHot Network Questions\n\nWhy is recovery mode not read-only?\n\nWhat does ""as a person in Bath who drinks the water"" mean?\n\nUp-to-date java version for setting up a minecraft server on raspberry pi 4\n\nRelativity of Time from an Observer Perspective\n\nAngle between 3 points defined with coordinate\n\nWhy use a special ""Name"" class (instead of just a string) for representing object names in C++?\n\nCalculating spread on a par rate curve given bond’s coupon and yield\n\nInfer pluses and minuses\n\nWhat would you call the ground floor if you were in a country where it is the first floor?\n\nWhat is a safe percentage for battery Maximum Capacity in an iPhone?\n\nHow can I make this AC current switch turn on at 20mA instead of 50mA?\n\nLocally conformally flat\n\nIs there such a thing as a ""physical"" fractal?\n\n""on a farm"" vs ""on the farm""\n\nCan copy-pasting a word definition from a dictionary site cause a copyright issue?\n\nWays to refuel nuclear powered cars\n\nWhat idiom could describe bureaucratic inefficiency?\n\nWhy is the metallicity of dwarf galaxies low?\n\nHow to view operator norms on open-system representation of quantum channels\n\nPoisson equation and gravitational potential\n\nProblem 5C.3 Isaacs\' Finite Group Theory\n\nMy paper has been rejected for accusations of ""gift authorship"", what now?\n\nHow can I use GLSL shaders within Mathematica?\n\nWhen or where did Gustave Flaubert say that Alexander Pushkin\'s work was ""dull""? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_5', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow do I fix a dimension error in TensorFlow?\n\nAsked 8 years, 4 months ago\n\nModified 8 years, 4 months ago\n\nI\'m trying to apply the expert portion of the tutorial to my own data but I keep running into dimension errors. Here\'s the code leading up to the error.\n\ndef weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\') def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\') W_conv1 = weight_variable([1, 8, 1, 4]) b_conv1 = bias_variable([4]) x_image = tf.reshape(tf_in, [-1,2,8,1]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) h_pool1 = max_pool_2x2(h_conv1)\n\nAnd then when I try to run this command:\n\nW_conv2 = weight_variable([1, 4, 4, 8]) b_conv2 = bias_variable([8]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) h_pool2 = max_pool_2x2(h_conv2)\n\nI get the following errors:\n\nValueError Traceback (most recent call last) <ipython-input-41-7ab0d7765f8c> in <module>() 3 4 h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) ----> 5 h_pool2 = max_pool_2x2(h_conv2) ValueError: (\'filter must not be larger than the input: \', \'Filter: [\', Dimension(2), \'x\', Dimension(2), \'] \', \'Input: [\', Dimension(1), \'x\', Dimension(4), \'] \')\n\nJust for some background information, the data that I\'m dealing with is a CSV file where each row contains 10 features and 1 empty column that can be a 1 or a 0. What I\'m trying to get is a probability in the empty column that the column will equal a 1.\n\nImprove this question\n\nedited Dec 9, 2015 at 18:34\n\nasked Dec 4, 2015 at 16:24\n\n3,28977 gold badges4141 silver badges6969 bronze badges 2\n\nWhat is tf_in? I\'m assuming it is the original 1x8 input.\n\n– erickrf Dec 4, 2015 at 17:47\n\ndata = genfromtxt(\'cs-training.csv\',delimiter=\',\'). A=data.shape[1]-1. tf_in = tf.placeholder(""float"", [None, A]).\n\n– Ravaal Dec 4, 2015 at 18:40\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou have to shape the input so it is compatible with both the training tensor and the output. If you input is length 1, your output should be length 1 (length is substituted for dimension).\n\nWhen you\'re dealing with-\n\ndef conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\') def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1], padding=\'SAME\')\n\nNotice how I changed the strides and the ksize to [1, 1, 1, 1]. This will match an output to a 1 dimensional input and prevent errors down the road.\n\nWhen you\'re defining your weight variable (see code below)-\n\ndef weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)\n\nyou\'re going to have to make the first 2 numbers conform to the feature tensor that you are using to train your model, the last two numbers will be the dimension of the predicted output (same as the dimension of the input).\n\nW_conv1 = weight_variable([1, 10, 1, 1]) b_conv1 = bias_variable([1])\n\nNotice the [1, 10, in the beginning which signifies that the feature tensor is going to be a 1x10 feature tensor; the last two numbers 1, 1] correspond to the dimensions of the input and output tensors/predictors.\n\nWhen you reshape your x_foo tensor (I call it x_ [x prime]), you, for whatever reason, have to define it like so-\n\nx_ = tf.reshape(x, [-1,1,10,1])\n\nNotice the 1 and 10 in the middle- ...1,10,.... Once again, these numbers correspond to the dimension of your feature tensor.\n\nFor every bias variable, you choose the final number of the previously defined variable. For example, if W_conv1 = weight_variable([1, 10, 1, 1]) appears like so, you take the final number and put that into your bias variable so it can match the dimensions of the input. This is done like so- b_conv1 = bias_variable([1]).\n\nIf you need any more explanation please comment below.\n\nanswered Dec 9, 2015 at 18:18\n\n3,28977 gold badges4141 silver badges6969 bronze badges\n\nThe dimensions you are using for the filter are not matching the output of the hidden layer.\n\nLet me see if I understood you: your input is composed of 8 features, and you want to reshape it into a 2x4 matrix, right?\n\nThe weights you created with weight_variable([1, 8, 1, 4]) expect a 1x8 input, in one channel, and produce a 1x8 output in 4 channels (or hidden units). The filter you are using sweeps the input in 2x2 squares. However, since the result of the weights is 1x8, they won\'t match.\n\nYou should reshape the input as\n\nx_image = tf.reshape(tf_in, [-1,2,4,1])\n\nNow, your input is actually 2x4 instead of 1x8. Then you need to change the weight shape to (2, 4, 1, hidden_units) to deal with a 2x4 output. It will also produce a 2x4 output, and the 2x2 filter now can be applied.\n\nAfter that, the filter will match the output of the weights. Also note that you will have to change the shape of your second weight matrix to weight_variable([2, 4, hidden_units, hidden2_units])\n\nedited Dec 4, 2015 at 20:00\n\nanswered Dec 4, 2015 at 17:59\n\n2,08655 gold badges2222 silver badges4444 bronze badges 4\n\nok so what I did was W_conv1 = weight_variable([2, 4, 1, 8]) and b_conv1 = bias_variable([8]). Then I did this x_image = tf.reshape(tf_in, [-1,2,4,1]). Then I get to this one and get an error- W_conv2 = weight_variable([2, 4, 4, 8]), b_conv2 = bias_variable([8]). I don\'t understand what you mean by hidden_units and hidden_units2 at the end. I assumed I could just throw in as many as I wanted but the tutorials make it seem like you always double the previous number. Even then I keep getting this error- ValueError: Dimensions Dimension(8) and Dimension(4) are not compatible.\n\n– Ravaal Dec 4, 2015 at 18:36\n\nOoops, I edited my answer to correct that. The shape of the second weights must match the size of the previous one, so it is [2, 4, hidden_units, hidden2_units]. Hidden units are the number of units in each hidden layer, i.e., their size. They correspond to the number of channels you produce in an image. The optimum size is problem dependent.\n\n– erickrf Dec 4, 2015 at 20:04\n\nI\'m not using images though. I\'m using a csv file where there are 8 features per row and one row to be predicted or given a probability of there being a 1 instead of a 0. Can you use this information to clarify what you mean?\n\n– Ravaal Dec 4, 2015 at 20:13\n\nValueError: (\'filter must not be larger than the input: \', \'Filter: [\', Dimension(2), \'x\', Dimension(4), \'] \', \'Input: [\', Dimension(1), \'x\', Dimension(2), \'] \'). I just changed the code to W_conv1 = weight_variable([2, 4, 1, 2]), b_conv1 = bias_variable([2]), x_image = tf.reshape(tf_in, [-1,2,4,1]), W_conv2 = weight_variable([2, 4, 2, 4]), b_conv2 = bias_variable([4]).\n\n– Ravaal Dec 4, 2015 at 20:18\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nSupporting the world’s most-used database engine through 2050\n\nWhat language should beginning programmers choose?\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\nPausing the 1-rep voting experiment on Stack Overflow: reflecting on the...\n\n0 TensorFlow: dimension error. how to debug?\n\n0 Dimensional Error in CNN\n\n2 Dimension Error WIth Tensorflow 2D Convolutional Layer\n\n1 Error TensorFlow ""Dimensions must be equal, but...""\n\n2 Input dimension error in Keras\n\n1 Tensorflow Dimension Error on reshape\n\n0 Tensorflow : ValueError: Dimensions must be equal\n\n0 Tensorflow dimension issue: ValueError: expected min_ndim=4, found ndim=3. Full shape received: (None, 3, 1)\n\n2 Tensorflow Input Dimension mismatch problem\n\n4 Cannot set tensor: Dimension mismatch\n\nHot Network Questions\n\nWhy is recovery mode not read-only?\n\nWhat does ""as a person in Bath who drinks the water"" mean?\n\nUp-to-date java version for setting up a minecraft server on raspberry pi 4\n\nRelativity of Time from an Observer Perspective\n\nAngle between 3 points defined with coordinate\n\nWhy use a special ""Name"" class (instead of just a string) for representing object names in C++?\n\nCalculating spread on a par rate curve given bond’s coupon and yield\n\nInfer pluses and minuses\n\nWhat would you call the ground floor if you were in a country where it is the first floor?\n\nWhat is a safe percentage for battery Maximum Capacity in an iPhone?\n\nHow can I make this AC current switch turn on at 20mA instead of 50mA?\n\nLocally conformally flat\n\nIs there such a thing as a ""physical"" fractal?\n\n""on a farm"" vs ""on the farm""\n\nCan copy-pasting a word definition from a dictionary site cause a copyright issue?\n\nWays to refuel nuclear powered cars\n\nWhat idiom could describe bureaucratic inefficiency?\n\nWhy is the metallicity of dwarf galaxies low?\n\nHow to view operator norms on open-system representation of quantum channels\n\nPoisson equation and gravitational potential\n\nProblem 5C.3 Isaacs\' Finite Group Theory\n\nMy paper has been rejected for accusations of ""gift authorship"", what now?\n\nHow can I use GLSL shaders within Mathematica?\n\nWhen or where did Gustave Flaubert say that Alexander Pushkin\'s work was ""dull""? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-04-29T01:17:05', 'title': 'python - How do I fix a dimension error in TensorFlow? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/34092850/how-do-i-fix-a-dimension-error-in-tensorflow'})], [Document(page_content=""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nTensor dimension becomes None after using gather or boolean_mask in Tensorflow 2.0 #15714\n\nkupferb opened this issue\n\nNov 28, 2021 · 5 comments\n\nTensor dimension becomes None after using gather or boolean_mask in Tensorflow 2.0 #15714\n\nkupferb opened this issue\n\nNov 28, 2021 · 5 comments\n\nstale stat:awaiting response from contributor type:support\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n\nWindows 10 Home (19043.1348):\n\nTensorFlow installed from (source or binary): binary\n\nTensorFlow version (use command below): 2.7.0\n\nPython version: 3.8.10\n\nCUDA/cuDNN version: 11.2\n\nGPU model and memory: GTX 1080 Ti (11 GB)\n\nDescribe the current behavior\n\nFor some reason I am getting different tensor dimensions when using gather in TF 2:\n\nThe first dimension becomes None when I use tensor as an index vector\n\nThe first dimension becomes len(indices) (as it should) where 'indices' are regular Python list\n\nThis happens only in eager mode (e.g., inside a custom loss function)\n\n(Same happens when using boolean_mask)\n\nStandalone code to reproduce the issue\n\nimport tensorflow as tf from tensorflow import keras from tensorflow.keras import Model from tensorflow.keras.layers import Input, Dense, Reshape from tensorflow.keras.datasets import mnist def cutsom_gan_loss_env(model): def custom_loss(y_true,y_pred): ff = tf.where([True, True, False , False])[:, 0] with tf.GradientTape(persistent=True) as tape: tf.print(tf.gather(y_true, [0, 1], axis=0).shape) #prints 2,28,28 tf.print(tf.gather(y_true, ff, axis=0).shape)#prints None,28,28 tape.watch(y_true) yy = model(y_true) d_yy = tape.gradient(yy,y_true) des_loss = tf.reduce_mean(d_yy) return des_loss return custom_loss def main_(): n_hidden_units = 5 num_lay = 3 kernel_init = keras.initializers.RandomUniform(-0.1, 0.1) (x_train, y_train), _ = mnist.load_data() x_train = tf.cast(x_train,tf.float32)/255. inputs = Input(x_train.shape[1:]) x = Dense(n_hidden_units,kernel_initializer=kernel_init, activation='sigmoid' )(inputs) for _ in range(num_lay): x = Dense(n_hidden_units,kernel_initializer=kernel_init, activation='sigmoid', )(x) outputs =Reshape(x_train.shape[1:])(Dense(x_train.shape[1], kernel_initializer=kernel_init, activation='softmax')(x)) model = Model(inputs=inputs, outputs=outputs) model.summary() optimizer1 = keras.optimizers.Adam(beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True) model.compile(loss=cutsom_gan_loss_env(model), optimizer=optimizer1, metrics=None) model.fit(x_train, x_train , batch_size=1000, epochs=1, shuffle=False) if __name__=='__main__': main_()\n\nThe text was updated successfully, but these errors were encountered:\n\nchunduriv self-assigned this\n\nchunduriv added the type:support\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited. label\n\n@jvishnuvardhan, I was able able to reproduce the issue on Colab using TF-nightly(2.8.0-dev20211201). Please find the gist here for reference.Thanks!\n\nchunduriv assigned jvishnuvardhan and unassigned chunduriv\n\ngowthamkpr commented\n\n@kupferb Please go through the solution mentioned here and this should solve your issue. This is not an error, but rather the difference between tensor.shape and tf.shape. The latter will give you the dynamic shape of a tensor after an operation like tf.gather\n\ngowthamkpr added the stat:awaiting response from contributor label\n\ngowthamkpr assigned gowthamkpr and unassigned jvishnuvardhan\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you'd like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nstale stat:awaiting response from contributor type:support\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited.\n\nYou can’t perform that action at this time."", metadata={'id': 'web-search_0', 'snippet': ""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nTensor dimension becomes None after using gather or boolean_mask in Tensorflow 2.0 #15714\n\nkupferb opened this issue\n\nNov 28, 2021 · 5 comments\n\nTensor dimension becomes None after using gather or boolean_mask in Tensorflow 2.0 #15714\n\nkupferb opened this issue\n\nNov 28, 2021 · 5 comments\n\nstale stat:awaiting response from contributor type:support\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n\nWindows 10 Home (19043.1348):\n\nTensorFlow installed from (source or binary): binary\n\nTensorFlow version (use command below): 2.7.0\n\nPython version: 3.8.10\n\nCUDA/cuDNN version: 11.2\n\nGPU model and memory: GTX 1080 Ti (11 GB)\n\nDescribe the current behavior\n\nFor some reason I am getting different tensor dimensions when using gather in TF 2:\n\nThe first dimension becomes None when I use tensor as an index vector\n\nThe first dimension becomes len(indices) (as it should) where 'indices' are regular Python list\n\nThis happens only in eager mode (e.g., inside a custom loss function)\n\n(Same happens when using boolean_mask)\n\nStandalone code to reproduce the issue\n\nimport tensorflow as tf from tensorflow import keras from tensorflow.keras import Model from tensorflow.keras.layers import Input, Dense, Reshape from tensorflow.keras.datasets import mnist def cutsom_gan_loss_env(model): def custom_loss(y_true,y_pred): ff = tf.where([True, True, False , False])[:, 0] with tf.GradientTape(persistent=True) as tape: tf.print(tf.gather(y_true, [0, 1], axis=0).shape) #prints 2,28,28 tf.print(tf.gather(y_true, ff, axis=0).shape)#prints None,28,28 tape.watch(y_true) yy = model(y_true) d_yy = tape.gradient(yy,y_true) des_loss = tf.reduce_mean(d_yy) return des_loss return custom_loss def main_(): n_hidden_units = 5 num_lay = 3 kernel_init = keras.initializers.RandomUniform(-0.1, 0.1) (x_train, y_train), _ = mnist.load_data() x_train = tf.cast(x_train,tf.float32)/255. inputs = Input(x_train.shape[1:]) x = Dense(n_hidden_units,kernel_initializer=kernel_init, activation='sigmoid' )(inputs) for _ in range(num_lay): x = Dense(n_hidden_units,kernel_initializer=kernel_init, activation='sigmoid', )(x) outputs =Reshape(x_train.shape[1:])(Dense(x_train.shape[1], kernel_initializer=kernel_init, activation='softmax')(x)) model = Model(inputs=inputs, outputs=outputs) model.summary() optimizer1 = keras.optimizers.Adam(beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True) model.compile(loss=cutsom_gan_loss_env(model), optimizer=optimizer1, metrics=None) model.fit(x_train, x_train , batch_size=1000, epochs=1, shuffle=False) if __name__=='__main__': main_()\n\nThe text was updated successfully, but these errors were encountered:\n\nchunduriv self-assigned this\n\nchunduriv added the type:support\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited. label\n\n@jvishnuvardhan, I was able able to reproduce the issue on Colab using TF-nightly(2.8.0-dev20211201). Please find the gist here for reference.Thanks!\n\nchunduriv assigned jvishnuvardhan and unassigned chunduriv\n\ngowthamkpr commented\n\n@kupferb Please go through the solution mentioned here and this should solve your issue. This is not an error, but rather the difference between tensor.shape and tf.shape. The latter will give you the dynamic shape of a tensor after an operation like tf.gather\n\ngowthamkpr added the stat:awaiting response from contributor label\n\ngowthamkpr assigned gowthamkpr and unassigned jvishnuvardhan\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you'd like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nstale stat:awaiting response from contributor type:support\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited.\n\nYou can’t perform that action at this time."", 'timestamp': '2024-06-14T11:02:35', 'title': 'Tensor dimension becomes None after using gather or boolean_mask in Tensorflow 2.0 · Issue #15714 · keras-team/keras', 'url': 'https://github.com/keras-team/keras/issues/15714'})], [Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to Tensors\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport tensorflow as tf import numpy as np\n\n2023-10-28 01:21:58.219231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2023-10-28 01:21:58.219277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2023-10-28 01:21:58.220822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nTensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes at tf.dtypes.\n\nIf you\'re familiar with NumPy, tensors are (kind of) like np.arrays.\n\nAll tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.\n\nFirst, create some basic tensors.\n\nHere is a ""scalar"" or ""rank-0"" tensor . A scalar contains a single value, and no ""axes"".\n\n# This will be an int32 tensor by default; see ""dtypes"" below. rank_0_tensor = tf.constant(4) print(rank_0_tensor)\n\ntf.Tensor(4, shape=(), dtype=int32)\n\nA ""vector"" or ""rank-1"" tensor is like a list of values. A vector has one axis:\n\n# Let\'s make this a float tensor. rank_1_tensor = tf.constant([2.0, 3.0, 4.0]) print(rank_1_tensor)\n\ntf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n\nA ""matrix"" or ""rank-2"" tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time rank_2_tensor = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float16) print(rank_2_tensor)\n\ntf.Tensor( [[1. 2.] [3. 4.] [5. 6.]], shape=(3, 2), dtype=float16)\n\nA vector, shape: [3]\n\nA matrix, shape: [3, 2]\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of # axes (sometimes called ""dimensions"") rank_3_tensor = tf.constant([ [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]], [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]], [[20, 21, 22, 23, 24], [25, 26, 27, 28, 29]],]) print(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nThere are many ways you might visualize a tensor with more than two axes.\n\nA 3-axis tensor, shape: [3, 2, 5]\n\nYou can convert a tensor to a NumPy array either using np.array or the tensor.numpy method:\n\nnp.array(rank_2_tensor)\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nrank_2_tensor.numpy()\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nTensors often contain floats and ints, but have many other types, including:\n\nThe base tf.Tensor class requires tensors to be ""rectangular""---that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\n\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na = tf.constant([[1, 2], [3, 4]]) b = tf.constant([[1, 1], [1, 1]]) # Could have also said `tf.ones([2,2], dtype=tf.int32)` print(tf.add(a, b), ""\\n"") print(tf.multiply(a, b), ""\\n"") print(tf.matmul(a, b), ""\\n"")\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nprint(a + b, ""\\n"") # element-wise addition print(a * b, ""\\n"") # element-wise multiplication print(a @ b, ""\\n"") # matrix multiplication\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nTensors are used in all kinds of operations (or ""Ops"").\n\nc = tf.constant([[4.0, 5.0], [10.0, 1.0]]) # Find the largest value print(tf.reduce_max(c)) # Find the index of the largest value print(tf.math.argmax(c)) # Compute the softmax print(tf.nn.softmax(c))\n\ntf.Tensor(10.0, shape=(), dtype=float32) tf.Tensor([1 0], shape=(2,), dtype=int64) tf.Tensor( [[2.6894143e-01 7.3105854e-01] [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)\n\nNote: Typically, anywhere a TensorFlow function expects a Tensor as input, the function will also accept anything that can be converted to a Tensor using tf.convert_to_tensor. See below for an example.\n\ntf.convert_to_tensor([1,2,3])\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n\ntf.reduce_max([1,2,3])\n\n<tf.Tensor: shape=(), dtype=int32, numpy=3>\n\ntf.reduce_max(np.array([1,2,3]))\n\n<tf.Tensor: shape=(), dtype=int64, numpy=3>\n\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\n\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\n\nAxis or Dimension: A particular dimension of a tensor.\n\nSize: The total number of items in the tensor, the product of the shape vector\'s elements.\n\nNote: Although you may see reference to a ""tensor of two dimensions"", a rank-2 tensor does not usually describe a 2D space.\n\nTensors and tf.TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor = tf.zeros([3, 2, 4, 5])\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\nprint(""Type of every element:"", rank_4_tensor.dtype) print(""Number of axes:"", rank_4_tensor.ndim) print(""Shape of tensor:"", rank_4_tensor.shape) print(""Elements along axis 0 of tensor:"", rank_4_tensor.shape[0]) print(""Elements along the last axis of tensor:"", rank_4_tensor.shape[-1]) print(""Total number of elements (3*2*4*5): "", tf.size(rank_4_tensor).numpy())\n\nType of every element: <dtype: \'float32\'> Number of axes: 4 Shape of tensor: (3, 2, 4, 5) Elements along axis 0 of tensor: 3 Elements along the last axis of tensor: 5 Total number of elements (3*2*4*5): 120\n\nBut note that the Tensor.ndim and Tensor.shape attributes don\'t return Tensor objects. If you need a Tensor use the tf.rank or tf.shape function. This difference is subtle, but it can be important when building graphs (later).\n\ntf.rank(rank_4_tensor)\n\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n\ntf.shape(rank_4_tensor)\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 2, 4, 5], dtype=int32)>\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\nSingle-axis indexing\n\nTensorFlow follows standard Python indexing rules, similar to indexing a list or a string in Python, and the basic rules for NumPy indexing.\n\nnegative indices count backwards from the end\n\ncolons, :, are used for slices: start:stop:step\n\nrank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34]) print(rank_1_tensor.numpy())\n\n[ 0 1 1 2 3 5 8 13 21 34]\n\nIndexing with a scalar removes the axis:\n\nprint(""First:"", rank_1_tensor[0].numpy()) print(""Second:"", rank_1_tensor[1].numpy()) print(""Last:"", rank_1_tensor[-1].numpy())\n\nFirst: 0 Second: 1 Last: 34\n\nIndexing with a : slice keeps the axis:\n\nprint(""Everything:"", rank_1_tensor[:].numpy()) print(""Before 4:"", rank_1_tensor[:4].numpy()) print(""From 4 to the end:"", rank_1_tensor[4:].numpy()) print(""From 2, before 7:"", rank_1_tensor[2:7].numpy()) print(""Every other item:"", rank_1_tensor[::2].numpy()) print(""Reversed:"", rank_1_tensor[::-1].numpy())\n\nEverything: [ 0 1 1 2 3 5 8 13 21 34] Before 4: [0 1 1 2] From 4 to the end: [ 3 5 8 13 21 34] From 2, before 7: [1 2 3 5 8] Every other item: [ 0 1 3 8 21] Reversed: [34 21 13 8 5 3 2 1 1 0]\n\nHigher rank tensors are indexed by passing multiple indices.\n\nThe exact same rules as in the single-axis case apply to each axis independently.\n\nprint(rank_2_tensor.numpy())\n\n[[1. 2.] [3. 4.] [5. 6.]]\n\nPassing an integer for each index, the result is a scalar.\n\n# Pull out a single value from a 2-rank tensor print(rank_2_tensor[1, 1].numpy())\n\nYou can index using any combination of integers and slices:\n\n# Get row and column tensors print(""Second row:"", rank_2_tensor[1, :].numpy()) print(""Second column:"", rank_2_tensor[:, 1].numpy()) print(""Last row:"", rank_2_tensor[-1, :].numpy()) print(""First item in last column:"", rank_2_tensor[0, -1].numpy()) print(""Skip the first row:"") print(rank_2_tensor[1:, :].numpy(), ""\\n"")\n\nSecond row: [3. 4.] Second column: [2. 4. 6.] Last row: [5. 6.] First item in last column: 2.0 Skip the first row: [[3. 4.] [5. 6.]]\n\nHere is an example with a 3-axis tensor:\n\nprint(rank_3_tensor[:, :, 4])\n\ntf.Tensor( [[ 4 9] [14 19] [24 29]], shape=(3, 2), dtype=int32)\n\nSelecting the last feature across all locations in each example in the batch\n\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors.\n\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis x = tf.constant([[1], [2], [3]]) print(x.shape)\n\n# You can convert this object into a Python list, too print(x.shape.as_list())\n\nYou can reshape a tensor into a new shape. The tf.reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape. # Note that you\'re passing in a list reshaped = tf.reshape(x, [1, 3])\n\nprint(x.shape) print(reshaped.shape)\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style ""row-major"" memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nprint(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says ""Whatever fits"". print(tf.reshape(rank_3_tensor, [-1]))\n\ntf.Tensor( [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29], shape=(30,), dtype=int32)\n\nTypically the only reasonable use of tf.reshape is to combine or split adjacent axes (or add/remove 1s).\n\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\nprint(tf.reshape(rank_3_tensor, [3*2, 5]), ""\\n"") print(tf.reshape(rank_3_tensor, [3, -1]))\n\ntf.Tensor( [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]], shape=(6, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n\nReshaping will ""work"" for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\n\nSwapping axes in tf.reshape does not work; you need tf.transpose for that.\n\n# Bad examples: don\'t do this # You can\'t reorder axes with reshape. print(tf.reshape(rank_3_tensor, [2, 3, 5]), ""\\n"") # This is a mess print(tf.reshape(rank_3_tensor, [5, 6]), ""\\n"") # This doesn\'t work at all try: tf.reshape(rank_3_tensor, [7, -1]) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]] [[15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) InvalidArgumentError: { {function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0} } Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n\nYou may run across not-fully-specified shapes. Either the shape contains a None (an axis-length is unknown) or the whole shape is None (the rank of the tensor is unknown).\n\nExcept for tf.RaggedTensor, such shapes will only occur in the context of TensorFlow\'s symbolic, graph-building APIs:\n\nThe keras functional API.\n\nTo inspect a tf.Tensor\'s data type use the Tensor.dtype property.\n\nWhen creating a tf.Tensor from a Python object you may optionally specify the datatype.\n\nIf you don\'t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts Python integers to tf.int32 and Python floating point numbers to tf.float32. Otherwise TensorFlow uses the same rules NumPy uses when converting to arrays.\n\nYou can cast from type to type.\n\nthe_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64) the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16) # Now, cast to an uint8 and lose the decimal precision the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8) print(the_u8_tensor)\n\ntf.Tensor([2 3 4], shape=(3,), dtype=uint8)\n\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are ""stretched"" automatically to fit larger tensors when running combined operations on them.\n\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx = tf.constant([1, 2, 3]) y = tf.constant(2) z = tf.constant([2, 2, 2]) # All of these are the same computation print(tf.multiply(x, 2)) print(x * y) print(x * z)\n\ntf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\n\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations x = tf.reshape(x,[3,1]) y = tf.range(1, 5) print(x, ""\\n"") print(y, ""\\n"") print(tf.multiply(x, y))\n\ntf.Tensor( [[1] [2] [3]], shape=(3, 1), dtype=int32) tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) tf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\nHere is the same operation without broadcasting:\n\nx_stretch = tf.constant([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]) y_stretch = tf.constant([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]) print(x_stretch * y_stretch) # Again, operator overloading\n\ntf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\n\nYou see what broadcasting looks like using tf.broadcast_to.\n\nprint(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))\n\ntf.Tensor( [[1 2 3] [1 2 3] [1 2 3]], shape=(3, 3), dtype=int32)\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\n\nIt can get even more complicated. This section of Jake VanderPlas\'s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy).\n\ntf.convert_to_tensor\n\nMost ops, like tf.matmul and tf.reshape take arguments of class tf.Tensor. However, you\'ll notice in the above case, Python objects shaped like tensors are accepted.\n\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy\'s ndarray, TensorShape, Python lists, and tf.Variable will all convert automatically.\n\nSee tf.register_tensor_conversion_function for more details, and if you have your own type you\'d like to automatically convert to a tensor.\n\nA tensor with variable numbers of elements along some axis is called ""ragged"". Use tf.ragged.RaggedTensor for ragged data.\n\nFor example, This cannot be represented as a regular tensor:\n\nA tf.RaggedTensor, shape: [4, None]\n\nragged_list = [ [0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]\n\ntry: tensor = tf.constant(ragged_list) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\nValueError: Can\'t convert non-rectangular Python sequence to Tensor.\n\nInstead create a tf.RaggedTensor using tf.ragged.constant:\n\nragged_tensor = tf.ragged.constant(ragged_list) print(ragged_tensor)\n\n<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>\n\nThe shape of a tf.RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor.shape)\n\ntf.string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\n\nThe strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the axes of the tensor. See tf.strings for functions to manipulate them.\n\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string. scalar_string_tensor = tf.constant(""Gray wolf"") print(scalar_string_tensor)\n\ntf.Tensor(b\'Gray wolf\', shape=(), dtype=string)\n\nAnd a vector of strings:\n\nA vector of strings, shape: [3,]\n\n# If you have three string tensors of different lengths, this is OK. tensor_of_strings = tf.constant([""Gray wolf"", ""Quick brown fox"", ""Lazy dog""]) # Note that the shape is (3,). The string length is not included. print(tensor_of_strings)\n\ntf.Tensor([b\'Gray wolf\' b\'Quick brown fox\' b\'Lazy dog\'], shape=(3,), dtype=string)\n\nIn the above printout the b prefix indicates that tf.string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\n\nIf you pass unicode characters they are utf-8 encoded.\n\ntf.constant(""🥳👍"")\n\n<tf.Tensor: shape=(), dtype=string, numpy=b\'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d\'>\n\nSome basic functions with strings can be found in tf.strings, including tf.strings.split.\n\n# You can use split to split a string into a set of tensors print(tf.strings.split(scalar_string_tensor, sep="" ""))\n\ntf.Tensor([b\'Gray\' b\'wolf\'], shape=(2,), dtype=string)\n\n# ...but it turns into a `RaggedTensor` if you split up a tensor of strings, # as each string might be split into a different number of parts. print(tf.strings.split(tensor_of_strings))\n\n<tf.RaggedTensor [[b\'Gray\', b\'wolf\'], [b\'Quick\', b\'brown\', b\'fox\'], [b\'Lazy\', b\'dog\']]>\n\nThree strings split, shape: [3, None]\n\nAnd tf.strings.to_number:\n\ntext = tf.constant(""1 10 100"") print(tf.strings.to_number(tf.strings.split(text, "" "")))\n\ntf.Tensor([ 1. 10. 100.], shape=(3,), dtype=float32)\n\nAlthough you can\'t use tf.cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings = tf.strings.bytes_split(tf.constant(""Duck"")) byte_ints = tf.io.decode_raw(tf.constant(""Duck""), tf.uint8) print(""Byte strings:"", byte_strings) print(""Bytes:"", byte_ints)\n\nByte strings: tf.Tensor([b\'D\' b\'u\' b\'c\' b\'k\'], shape=(4,), dtype=string) Bytes: tf.Tensor([ 68 117 99 107], shape=(4,), dtype=uint8)\n\n# Or split it up as unicode and then decode it unicode_bytes = tf.constant(""アヒル 🦆"") unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, ""UTF-8"") unicode_values = tf.strings.unicode_decode(unicode_bytes, ""UTF-8"") print(""\\nUnicode bytes:"", unicode_bytes) print(""\\nUnicode chars:"", unicode_char_bytes) print(""\\nUnicode values:"", unicode_values)\n\nUnicode bytes: tf.Tensor(b\'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86\', shape=(), dtype=string) Unicode chars: tf.Tensor([b\'\\xe3\\x82\\xa2\' b\'\\xe3\\x83\\x92\' b\'\\xe3\\x83\\xab\' b\' \' b\'\\xf0\\x9f\\xa6\\x86\'], shape=(5,), dtype=string) Unicode values: tf.Tensor([ 12450 12498 12523 32 129414], shape=(5,), dtype=int32)\n\nThe tf.string dtype is used for all raw bytes data in TensorFlow. The tf.io module contains functions for converting data to and from bytes, including decoding images and parsing csv.\n\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf.sparse.SparseTensor and related operations to store sparse data efficiently.\n\nA tf.SparseTensor, shape: [3, 4]\n\n# Sparse tensors store values by index in a memory-efficient manner sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]) print(sparse_tensor, ""\\n"") # You can convert sparse tensors to dense print(tf.sparse.to_dense(sparse_tensor))\n\nSparseTensor(indices=tf.Tensor( [[0 0] [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) tf.Tensor( [[1 0 0 0] [0 0 2 0] [0 0 0 0]], shape=(3, 4), dtype=int32)\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-10-28 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_1', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to Tensors\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport tensorflow as tf import numpy as np\n\n2023-10-28 01:21:58.219231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2023-10-28 01:21:58.219277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2023-10-28 01:21:58.220822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nTensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes at tf.dtypes.\n\nIf you\'re familiar with NumPy, tensors are (kind of) like np.arrays.\n\nAll tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.\n\nFirst, create some basic tensors.\n\nHere is a ""scalar"" or ""rank-0"" tensor . A scalar contains a single value, and no ""axes"".\n\n# This will be an int32 tensor by default; see ""dtypes"" below. rank_0_tensor = tf.constant(4) print(rank_0_tensor)\n\ntf.Tensor(4, shape=(), dtype=int32)\n\nA ""vector"" or ""rank-1"" tensor is like a list of values. A vector has one axis:\n\n# Let\'s make this a float tensor. rank_1_tensor = tf.constant([2.0, 3.0, 4.0]) print(rank_1_tensor)\n\ntf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n\nA ""matrix"" or ""rank-2"" tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time rank_2_tensor = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float16) print(rank_2_tensor)\n\ntf.Tensor( [[1. 2.] [3. 4.] [5. 6.]], shape=(3, 2), dtype=float16)\n\nA vector, shape: [3]\n\nA matrix, shape: [3, 2]\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of # axes (sometimes called ""dimensions"") rank_3_tensor = tf.constant([ [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]], [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]], [[20, 21, 22, 23, 24], [25, 26, 27, 28, 29]],]) print(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nThere are many ways you might visualize a tensor with more than two axes.\n\nA 3-axis tensor, shape: [3, 2, 5]\n\nYou can convert a tensor to a NumPy array either using np.array or the tensor.numpy method:\n\nnp.array(rank_2_tensor)\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nrank_2_tensor.numpy()\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nTensors often contain floats and ints, but have many other types, including:\n\nThe base tf.Tensor class requires tensors to be ""rectangular""---that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\n\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na = tf.constant([[1, 2], [3, 4]]) b = tf.constant([[1, 1], [1, 1]]) # Could have also said `tf.ones([2,2], dtype=tf.int32)` print(tf.add(a, b), ""\\n"") print(tf.multiply(a, b), ""\\n"") print(tf.matmul(a, b), ""\\n"")\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nprint(a + b, ""\\n"") # element-wise addition print(a * b, ""\\n"") # element-wise multiplication print(a @ b, ""\\n"") # matrix multiplication\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nTensors are used in all kinds of operations (or ""Ops"").\n\nc = tf.constant([[4.0, 5.0], [10.0, 1.0]]) # Find the largest value print(tf.reduce_max(c)) # Find the index of the largest value print(tf.math.argmax(c)) # Compute the softmax print(tf.nn.softmax(c))\n\ntf.Tensor(10.0, shape=(), dtype=float32) tf.Tensor([1 0], shape=(2,), dtype=int64) tf.Tensor( [[2.6894143e-01 7.3105854e-01] [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)\n\nNote: Typically, anywhere a TensorFlow function expects a Tensor as input, the function will also accept anything that can be converted to a Tensor using tf.convert_to_tensor. See below for an example.\n\ntf.convert_to_tensor([1,2,3])\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n\ntf.reduce_max([1,2,3])\n\n<tf.Tensor: shape=(), dtype=int32, numpy=3>\n\ntf.reduce_max(np.array([1,2,3]))\n\n<tf.Tensor: shape=(), dtype=int64, numpy=3>\n\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\n\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\n\nAxis or Dimension: A particular dimension of a tensor.\n\nSize: The total number of items in the tensor, the product of the shape vector\'s elements.\n\nNote: Although you may see reference to a ""tensor of two dimensions"", a rank-2 tensor does not usually describe a 2D space.\n\nTensors and tf.TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor = tf.zeros([3, 2, 4, 5])\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\nprint(""Type of every element:"", rank_4_tensor.dtype) print(""Number of axes:"", rank_4_tensor.ndim) print(""Shape of tensor:"", rank_4_tensor.shape) print(""Elements along axis 0 of tensor:"", rank_4_tensor.shape[0]) print(""Elements along the last axis of tensor:"", rank_4_tensor.shape[-1]) print(""Total number of elements (3*2*4*5): "", tf.size(rank_4_tensor).numpy())\n\nType of every element: <dtype: \'float32\'> Number of axes: 4 Shape of tensor: (3, 2, 4, 5) Elements along axis 0 of tensor: 3 Elements along the last axis of tensor: 5 Total number of elements (3*2*4*5): 120\n\nBut note that the Tensor.ndim and Tensor.shape attributes don\'t return Tensor objects. If you need a Tensor use the tf.rank or tf.shape function. This difference is subtle, but it can be important when building graphs (later).\n\ntf.rank(rank_4_tensor)\n\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n\ntf.shape(rank_4_tensor)\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 2, 4, 5], dtype=int32)>\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\nSingle-axis indexing\n\nTensorFlow follows standard Python indexing rules, similar to indexing a list or a string in Python, and the basic rules for NumPy indexing.\n\nnegative indices count backwards from the end\n\ncolons, :, are used for slices: start:stop:step\n\nrank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34]) print(rank_1_tensor.numpy())\n\n[ 0 1 1 2 3 5 8 13 21 34]\n\nIndexing with a scalar removes the axis:\n\nprint(""First:"", rank_1_tensor[0].numpy()) print(""Second:"", rank_1_tensor[1].numpy()) print(""Last:"", rank_1_tensor[-1].numpy())\n\nFirst: 0 Second: 1 Last: 34\n\nIndexing with a : slice keeps the axis:\n\nprint(""Everything:"", rank_1_tensor[:].numpy()) print(""Before 4:"", rank_1_tensor[:4].numpy()) print(""From 4 to the end:"", rank_1_tensor[4:].numpy()) print(""From 2, before 7:"", rank_1_tensor[2:7].numpy()) print(""Every other item:"", rank_1_tensor[::2].numpy()) print(""Reversed:"", rank_1_tensor[::-1].numpy())\n\nEverything: [ 0 1 1 2 3 5 8 13 21 34] Before 4: [0 1 1 2] From 4 to the end: [ 3 5 8 13 21 34] From 2, before 7: [1 2 3 5 8] Every other item: [ 0 1 3 8 21] Reversed: [34 21 13 8 5 3 2 1 1 0]\n\nHigher rank tensors are indexed by passing multiple indices.\n\nThe exact same rules as in the single-axis case apply to each axis independently.\n\nprint(rank_2_tensor.numpy())\n\n[[1. 2.] [3. 4.] [5. 6.]]\n\nPassing an integer for each index, the result is a scalar.\n\n# Pull out a single value from a 2-rank tensor print(rank_2_tensor[1, 1].numpy())\n\nYou can index using any combination of integers and slices:\n\n# Get row and column tensors print(""Second row:"", rank_2_tensor[1, :].numpy()) print(""Second column:"", rank_2_tensor[:, 1].numpy()) print(""Last row:"", rank_2_tensor[-1, :].numpy()) print(""First item in last column:"", rank_2_tensor[0, -1].numpy()) print(""Skip the first row:"") print(rank_2_tensor[1:, :].numpy(), ""\\n"")\n\nSecond row: [3. 4.] Second column: [2. 4. 6.] Last row: [5. 6.] First item in last column: 2.0 Skip the first row: [[3. 4.] [5. 6.]]\n\nHere is an example with a 3-axis tensor:\n\nprint(rank_3_tensor[:, :, 4])\n\ntf.Tensor( [[ 4 9] [14 19] [24 29]], shape=(3, 2), dtype=int32)\n\nSelecting the last feature across all locations in each example in the batch\n\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors.\n\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis x = tf.constant([[1], [2], [3]]) print(x.shape)\n\n# You can convert this object into a Python list, too print(x.shape.as_list())\n\nYou can reshape a tensor into a new shape. The tf.reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape. # Note that you\'re passing in a list reshaped = tf.reshape(x, [1, 3])\n\nprint(x.shape) print(reshaped.shape)\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style ""row-major"" memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nprint(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says ""Whatever fits"". print(tf.reshape(rank_3_tensor, [-1]))\n\ntf.Tensor( [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29], shape=(30,), dtype=int32)\n\nTypically the only reasonable use of tf.reshape is to combine or split adjacent axes (or add/remove 1s).\n\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\nprint(tf.reshape(rank_3_tensor, [3*2, 5]), ""\\n"") print(tf.reshape(rank_3_tensor, [3, -1]))\n\ntf.Tensor( [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]], shape=(6, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n\nReshaping will ""work"" for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\n\nSwapping axes in tf.reshape does not work; you need tf.transpose for that.\n\n# Bad examples: don\'t do this # You can\'t reorder axes with reshape. print(tf.reshape(rank_3_tensor, [2, 3, 5]), ""\\n"") # This is a mess print(tf.reshape(rank_3_tensor, [5, 6]), ""\\n"") # This doesn\'t work at all try: tf.reshape(rank_3_tensor, [7, -1]) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]] [[15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) InvalidArgumentError: { {function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0} } Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n\nYou may run across not-fully-specified shapes. Either the shape contains a None (an axis-length is unknown) or the whole shape is None (the rank of the tensor is unknown).\n\nExcept for tf.RaggedTensor, such shapes will only occur in the context of TensorFlow\'s symbolic, graph-building APIs:\n\nThe keras functional API.\n\nTo inspect a tf.Tensor\'s data type use the Tensor.dtype property.\n\nWhen creating a tf.Tensor from a Python object you may optionally specify the datatype.\n\nIf you don\'t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts Python integers to tf.int32 and Python floating point numbers to tf.float32. Otherwise TensorFlow uses the same rules NumPy uses when converting to arrays.\n\nYou can cast from type to type.\n\nthe_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64) the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16) # Now, cast to an uint8 and lose the decimal precision the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8) print(the_u8_tensor)\n\ntf.Tensor([2 3 4], shape=(3,), dtype=uint8)\n\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are ""stretched"" automatically to fit larger tensors when running combined operations on them.\n\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx = tf.constant([1, 2, 3]) y = tf.constant(2) z = tf.constant([2, 2, 2]) # All of these are the same computation print(tf.multiply(x, 2)) print(x * y) print(x * z)\n\ntf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\n\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations x = tf.reshape(x,[3,1]) y = tf.range(1, 5) print(x, ""\\n"") print(y, ""\\n"") print(tf.multiply(x, y))\n\ntf.Tensor( [[1] [2] [3]], shape=(3, 1), dtype=int32) tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) tf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\nHere is the same operation without broadcasting:\n\nx_stretch = tf.constant([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]) y_stretch = tf.constant([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]) print(x_stretch * y_stretch) # Again, operator overloading\n\ntf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\n\nYou see what broadcasting looks like using tf.broadcast_to.\n\nprint(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))\n\ntf.Tensor( [[1 2 3] [1 2 3] [1 2 3]], shape=(3, 3), dtype=int32)\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\n\nIt can get even more complicated. This section of Jake VanderPlas\'s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy).\n\ntf.convert_to_tensor\n\nMost ops, like tf.matmul and tf.reshape take arguments of class tf.Tensor. However, you\'ll notice in the above case, Python objects shaped like tensors are accepted.\n\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy\'s ndarray, TensorShape, Python lists, and tf.Variable will all convert automatically.\n\nSee tf.register_tensor_conversion_function for more details, and if you have your own type you\'d like to automatically convert to a tensor.\n\nA tensor with variable numbers of elements along some axis is called ""ragged"". Use tf.ragged.RaggedTensor for ragged data.\n\nFor example, This cannot be represented as a regular tensor:\n\nA tf.RaggedTensor, shape: [4, None]\n\nragged_list = [ [0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]\n\ntry: tensor = tf.constant(ragged_list) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\nValueError: Can\'t convert non-rectangular Python sequence to Tensor.\n\nInstead create a tf.RaggedTensor using tf.ragged.constant:\n\nragged_tensor = tf.ragged.constant(ragged_list) print(ragged_tensor)\n\n<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>\n\nThe shape of a tf.RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor.shape)\n\ntf.string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\n\nThe strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the axes of the tensor. See tf.strings for functions to manipulate them.\n\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string. scalar_string_tensor = tf.constant(""Gray wolf"") print(scalar_string_tensor)\n\ntf.Tensor(b\'Gray wolf\', shape=(), dtype=string)\n\nAnd a vector of strings:\n\nA vector of strings, shape: [3,]\n\n# If you have three string tensors of different lengths, this is OK. tensor_of_strings = tf.constant([""Gray wolf"", ""Quick brown fox"", ""Lazy dog""]) # Note that the shape is (3,). The string length is not included. print(tensor_of_strings)\n\ntf.Tensor([b\'Gray wolf\' b\'Quick brown fox\' b\'Lazy dog\'], shape=(3,), dtype=string)\n\nIn the above printout the b prefix indicates that tf.string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\n\nIf you pass unicode characters they are utf-8 encoded.\n\ntf.constant(""🥳👍"")\n\n<tf.Tensor: shape=(), dtype=string, numpy=b\'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d\'>\n\nSome basic functions with strings can be found in tf.strings, including tf.strings.split.\n\n# You can use split to split a string into a set of tensors print(tf.strings.split(scalar_string_tensor, sep="" ""))\n\ntf.Tensor([b\'Gray\' b\'wolf\'], shape=(2,), dtype=string)\n\n# ...but it turns into a `RaggedTensor` if you split up a tensor of strings, # as each string might be split into a different number of parts. print(tf.strings.split(tensor_of_strings))\n\n<tf.RaggedTensor [[b\'Gray\', b\'wolf\'], [b\'Quick\', b\'brown\', b\'fox\'], [b\'Lazy\', b\'dog\']]>\n\nThree strings split, shape: [3, None]\n\nAnd tf.strings.to_number:\n\ntext = tf.constant(""1 10 100"") print(tf.strings.to_number(tf.strings.split(text, "" "")))\n\ntf.Tensor([ 1. 10. 100.], shape=(3,), dtype=float32)\n\nAlthough you can\'t use tf.cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings = tf.strings.bytes_split(tf.constant(""Duck"")) byte_ints = tf.io.decode_raw(tf.constant(""Duck""), tf.uint8) print(""Byte strings:"", byte_strings) print(""Bytes:"", byte_ints)\n\nByte strings: tf.Tensor([b\'D\' b\'u\' b\'c\' b\'k\'], shape=(4,), dtype=string) Bytes: tf.Tensor([ 68 117 99 107], shape=(4,), dtype=uint8)\n\n# Or split it up as unicode and then decode it unicode_bytes = tf.constant(""アヒル 🦆"") unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, ""UTF-8"") unicode_values = tf.strings.unicode_decode(unicode_bytes, ""UTF-8"") print(""\\nUnicode bytes:"", unicode_bytes) print(""\\nUnicode chars:"", unicode_char_bytes) print(""\\nUnicode values:"", unicode_values)\n\nUnicode bytes: tf.Tensor(b\'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86\', shape=(), dtype=string) Unicode chars: tf.Tensor([b\'\\xe3\\x82\\xa2\' b\'\\xe3\\x83\\x92\' b\'\\xe3\\x83\\xab\' b\' \' b\'\\xf0\\x9f\\xa6\\x86\'], shape=(5,), dtype=string) Unicode values: tf.Tensor([ 12450 12498 12523 32 129414], shape=(5,), dtype=int32)\n\nThe tf.string dtype is used for all raw bytes data in TensorFlow. The tf.io module contains functions for converting data to and from bytes, including decoding images and parsing csv.\n\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf.sparse.SparseTensor and related operations to store sparse data efficiently.\n\nA tf.SparseTensor, shape: [3, 4]\n\n# Sparse tensors store values by index in a memory-efficient manner sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]) print(sparse_tensor, ""\\n"") # You can convert sparse tensors to dense print(tf.sparse.to_dense(sparse_tensor))\n\nSparseTensor(indices=tf.Tensor( [[0 0] [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) tf.Tensor( [[1 0 0 0] [0 0 2 0] [0 0 0 0]], shape=(3, 4), dtype=int32)\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-10-28 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-06-24T03:57:03', 'title': 'Introduction to Tensors | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/tensor'})]]??"
51858970,tf.gradients,"{'https://www.udemy.com/course/data-science-deep-learning-in-theano-tensorflow/', 'https://www.udacity.com/course/intro-to-machine-learning-with-tensorflow-nanodegree--nd230'}","{'https://www.youtube.com/watch?v=us-5I52Lac8', 'https://www.youtube.com/watch?v=uPcU5ybq4Jk', 'https://www.youtube.com/watch?v=Xw5wsWDt1Cg'}",{'https://stackoverflow.com/questions/51858970/tf-gradients-sums-over-ys-does-it'},"??[[Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\ntf.gradients() sums over ys, does it?\n\nAsked 5 years, 9 months ago\n\nModified 5 years, 9 months ago\n\nhttps://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients\n\nIn the documentation for tf.gradients(ys, xs) it states that\n\nConstructs symbolic derivatives of sum of ys w.r.t. x in xs\n\nI am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example:\n\nx_dims = 3 batch_size = 4 x = tf.placeholder(tf.float32, (None, x_dims)) y = 2*(x**2) grads = tf.gradients(y,x) sess = tf.Session() x_val = np.random.randint(0, 10, (batch_size, x_dims)) y_val, grads_val = sess.run([y, grads], {x:x_val}) print(\'x = \\n\', x_val) print(\'y = \\n\', y_val) print(\'dy/dx = \\n\', grads_val[0])\n\nThis gives the following output:\n\nx = [[5 3 7] [2 2 5] [7 5 0] [3 7 6]] y = [[50. 18. 98.] [ 8. 8. 50.] [98. 50. 0.] [18. 98. 72.]] dy/dx = [[20. 12. 28.] [ 8. 8. 20.] [28. 20. 0.] [12. 28. 24.]]\n\nThis is the output I would expect, simply the derivative dy/dx for every element in the batch. I don\'t see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html). Why is this necessary?\n\nI am using Tensorflow 1.6 and Python 3.\n\nImprove this question\n\nasked Aug 15, 2018 at 12:45\n\n11911 silver badge1111 bronze badges 1\n\nTo give more insight over why you see the gradients summed over batch size in methods like DDPG: those gradients haven\'t come from computing gradients through a loss function which has already accounted for this (like tf.reduce_mean(...)). The gradients have been summed by tf.gradients, so dividing by the batch size gives a mean gradient for the batch when applied with apply_gradients\n\n– parrowdice Jan 28, 2020 at 0:01\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIf y and x have the same shape then the sum over the dy/dx is the sum over exactly one value. However, if you have more than one y for each x, then the gradients are summed.\n\nimport numpy as np import tensorflow as tf x_dims = 3 batch_size = 4 x = tf.placeholder(tf.float32, (None, x_dims)) y = 2*(x**2) z = tf.stack([y, y]) # There are twice as many z\'s as x\'s dy_dx = tf.gradients(y,x) dz_dx = tf.gradients(z,x) sess = tf.Session() x_val = np.random.randint(0, 10, (batch_size, x_dims)) y_val, z_val, dy_dx_val, dz_dx_val = sess.run([y, z, dy_dx, dz_dx], {x:x_val}) print(\'x.shape =\', x_val.shape) print(\'x = \\n\', x_val) print(\'y.shape = \', y_val.shape) print(\'y = \\n\', y_val) print(\'z.shape = \', z_val.shape) print(\'z = \\n\', z_val) print(\'dy/dx = \\n\', dy_dx_val[0]) print(\'dz/dx = \\n\', dz_dx_val[0])\n\nProduces the following output:\n\nx.shape = (4, 3) x = [[1 4 8] [0 2 8] [2 8 1] [4 5 2]] y.shape = (4, 3) y = [[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]] z.shape = (2, 4, 3) z = [[[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]] [[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]]] dy/dx = [[ 4. 16. 32.] [ 0. 8. 32.] [ 8. 32. 4.] [16. 20. 8.]] dz/dx = [[ 8. 32. 64.] [ 0. 16. 64.] [16. 64. 8.] [32. 40. 16.]]\n\nIn particular, notice that the values of dz/dx are twice those of dy/dz since they are summed over the inputs to the stack.\n\nanswered Aug 15, 2018 at 16:03\n\n60633 silver badges88 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ngradient-descent or ask your own question.\n\nAn open-source development paradigm\n\nDevelopers get by with a little help from AI: Stack Overflow Knows code...\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n8 Tensorflow gradients: without automatic implicit sum\n\n5 Where in tensorflow gradients is the sum over the elements of y made?\n\n2 How do tf.gradients() work?\n\n0 How does tf.gradients behave when passed a list of `ys` tensors?\n\n5 If you use plus sign instead of tf.add, will tensorflow still calculate gradients correctly?\n\n0 Understanding how tf.gradients evaluates\n\n0 Why does this TensorFlow example not have a summation before the activation function?\n\n8 How to accumulate gradients in tensorflow 2.0?\n\n2 How does tensorflow.GradientTape() accumulate operation within a loop?\n\n2 Is the gradient of the sum equal to the sum of the gradients for a neural network in pytorch?\n\nHot Network Questions\n\nDid Paul remain a Jew even after his conversion?\n\nIs Genesis 2:7 sufficiently ambiguous to support the interpretation that God spent millions of years forming Adam\'s body from dust?\n\nIs there a technical reason why Datel Action Replay on PC required a card?\n\nPhrase or expression for being on the second level in terms of worseness\n\nWhich invention from the last fifty years could have the biggest impact on history?\n\nWhat is the origin of the Deck of Many Things?\n\nThe principle of legality in criminal law, and the common law\n\n""Galois theory"" on graphs\n\nHelp with a Calculus exercise (indefinite integral)\n\nWhy don\'t Hikaru, Fabi & Nepo buy out Liren so the 3 of them have a playoff to face Gukesh?\n\nI am the person who uses the mouse with ""their"" right hand?\n\nHow to manage being excluded from the lab group?\n\nPurpose of async/await in web servers\n\nWhat is the maximum number of times a liquid rocket engine has detonated/ exploded during development?\n\nHow many plot twists are too many plot twists?\n\nHow many Streifenkarte stripes are needed to get from Munich airport to the city centre?\n\nWhy are my benchmark times not repeatable, even for a CPU-bound task?\n\nIf someone is stranded in a transfer-only station after the last train has left, what happens?\n\nHow to prick Black Eyed Susan?\n\nOrderings in Philosophy\n\nCompare two strings, with NaC\n\nIs it bad style to write x^2, 2^\\frac{1}{2} and 2^\\sqrt{2}?\n\nHow can the Bitcoin blockchain be forked? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_0', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\ntf.gradients() sums over ys, does it?\n\nAsked 5 years, 9 months ago\n\nModified 5 years, 9 months ago\n\nhttps://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients\n\nIn the documentation for tf.gradients(ys, xs) it states that\n\nConstructs symbolic derivatives of sum of ys w.r.t. x in xs\n\nI am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example:\n\nx_dims = 3 batch_size = 4 x = tf.placeholder(tf.float32, (None, x_dims)) y = 2*(x**2) grads = tf.gradients(y,x) sess = tf.Session() x_val = np.random.randint(0, 10, (batch_size, x_dims)) y_val, grads_val = sess.run([y, grads], {x:x_val}) print(\'x = \\n\', x_val) print(\'y = \\n\', y_val) print(\'dy/dx = \\n\', grads_val[0])\n\nThis gives the following output:\n\nx = [[5 3 7] [2 2 5] [7 5 0] [3 7 6]] y = [[50. 18. 98.] [ 8. 8. 50.] [98. 50. 0.] [18. 98. 72.]] dy/dx = [[20. 12. 28.] [ 8. 8. 20.] [28. 20. 0.] [12. 28. 24.]]\n\nThis is the output I would expect, simply the derivative dy/dx for every element in the batch. I don\'t see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html). Why is this necessary?\n\nI am using Tensorflow 1.6 and Python 3.\n\nImprove this question\n\nasked Aug 15, 2018 at 12:45\n\n11911 silver badge1111 bronze badges 1\n\nTo give more insight over why you see the gradients summed over batch size in methods like DDPG: those gradients haven\'t come from computing gradients through a loss function which has already accounted for this (like tf.reduce_mean(...)). The gradients have been summed by tf.gradients, so dividing by the batch size gives a mean gradient for the batch when applied with apply_gradients\n\n– parrowdice Jan 28, 2020 at 0:01\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIf y and x have the same shape then the sum over the dy/dx is the sum over exactly one value. However, if you have more than one y for each x, then the gradients are summed.\n\nimport numpy as np import tensorflow as tf x_dims = 3 batch_size = 4 x = tf.placeholder(tf.float32, (None, x_dims)) y = 2*(x**2) z = tf.stack([y, y]) # There are twice as many z\'s as x\'s dy_dx = tf.gradients(y,x) dz_dx = tf.gradients(z,x) sess = tf.Session() x_val = np.random.randint(0, 10, (batch_size, x_dims)) y_val, z_val, dy_dx_val, dz_dx_val = sess.run([y, z, dy_dx, dz_dx], {x:x_val}) print(\'x.shape =\', x_val.shape) print(\'x = \\n\', x_val) print(\'y.shape = \', y_val.shape) print(\'y = \\n\', y_val) print(\'z.shape = \', z_val.shape) print(\'z = \\n\', z_val) print(\'dy/dx = \\n\', dy_dx_val[0]) print(\'dz/dx = \\n\', dz_dx_val[0])\n\nProduces the following output:\n\nx.shape = (4, 3) x = [[1 4 8] [0 2 8] [2 8 1] [4 5 2]] y.shape = (4, 3) y = [[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]] z.shape = (2, 4, 3) z = [[[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]] [[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]]] dy/dx = [[ 4. 16. 32.] [ 0. 8. 32.] [ 8. 32. 4.] [16. 20. 8.]] dz/dx = [[ 8. 32. 64.] [ 0. 16. 64.] [16. 64. 8.] [32. 40. 16.]]\n\nIn particular, notice that the values of dz/dx are twice those of dy/dz since they are summed over the inputs to the stack.\n\nanswered Aug 15, 2018 at 16:03\n\n60633 silver badges88 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ngradient-descent or ask your own question.\n\nAn open-source development paradigm\n\nDevelopers get by with a little help from AI: Stack Overflow Knows code...\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n8 Tensorflow gradients: without automatic implicit sum\n\n5 Where in tensorflow gradients is the sum over the elements of y made?\n\n2 How do tf.gradients() work?\n\n0 How does tf.gradients behave when passed a list of `ys` tensors?\n\n5 If you use plus sign instead of tf.add, will tensorflow still calculate gradients correctly?\n\n0 Understanding how tf.gradients evaluates\n\n0 Why does this TensorFlow example not have a summation before the activation function?\n\n8 How to accumulate gradients in tensorflow 2.0?\n\n2 How does tensorflow.GradientTape() accumulate operation within a loop?\n\n2 Is the gradient of the sum equal to the sum of the gradients for a neural network in pytorch?\n\nHot Network Questions\n\nDid Paul remain a Jew even after his conversion?\n\nIs Genesis 2:7 sufficiently ambiguous to support the interpretation that God spent millions of years forming Adam\'s body from dust?\n\nIs there a technical reason why Datel Action Replay on PC required a card?\n\nPhrase or expression for being on the second level in terms of worseness\n\nWhich invention from the last fifty years could have the biggest impact on history?\n\nWhat is the origin of the Deck of Many Things?\n\nThe principle of legality in criminal law, and the common law\n\n""Galois theory"" on graphs\n\nHelp with a Calculus exercise (indefinite integral)\n\nWhy don\'t Hikaru, Fabi & Nepo buy out Liren so the 3 of them have a playoff to face Gukesh?\n\nI am the person who uses the mouse with ""their"" right hand?\n\nHow to manage being excluded from the lab group?\n\nPurpose of async/await in web servers\n\nWhat is the maximum number of times a liquid rocket engine has detonated/ exploded during development?\n\nHow many plot twists are too many plot twists?\n\nHow many Streifenkarte stripes are needed to get from Munich airport to the city centre?\n\nWhy are my benchmark times not repeatable, even for a CPU-bound task?\n\nIf someone is stranded in a transfer-only station after the last train has left, what happens?\n\nHow to prick Black Eyed Susan?\n\nOrderings in Philosophy\n\nCompare two strings, with NaC\n\nIs it bad style to write x^2, 2^\\frac{1}{2} and 2^\\sqrt{2}?\n\nHow can the Bitcoin blockchain be forked? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-05-31T17:47:56', 'title': 'python - tf.gradients() sums over ys, does it? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/51858970/tf-gradients-sums-over-ys-does-it'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\ntf.gradients() sums over ys, does it?\n\nAsked 5 years, 9 months ago\n\nModified 5 years, 9 months ago\n\nhttps://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients\n\nIn the documentation for tf.gradients(ys, xs) it states that\n\nConstructs symbolic derivatives of sum of ys w.r.t. x in xs\n\nI am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example:\n\nx_dims = 3 batch_size = 4 x = tf.placeholder(tf.float32, (None, x_dims)) y = 2*(x**2) grads = tf.gradients(y,x) sess = tf.Session() x_val = np.random.randint(0, 10, (batch_size, x_dims)) y_val, grads_val = sess.run([y, grads], {x:x_val}) print(\'x = \\n\', x_val) print(\'y = \\n\', y_val) print(\'dy/dx = \\n\', grads_val[0])\n\nThis gives the following output:\n\nx = [[5 3 7] [2 2 5] [7 5 0] [3 7 6]] y = [[50. 18. 98.] [ 8. 8. 50.] [98. 50. 0.] [18. 98. 72.]] dy/dx = [[20. 12. 28.] [ 8. 8. 20.] [28. 20. 0.] [12. 28. 24.]]\n\nThis is the output I would expect, simply the derivative dy/dx for every element in the batch. I don\'t see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html). Why is this necessary?\n\nI am using Tensorflow 1.6 and Python 3.\n\nImprove this question\n\nasked Aug 15, 2018 at 12:45\n\n11911 silver badge1111 bronze badges 1\n\nTo give more insight over why you see the gradients summed over batch size in methods like DDPG: those gradients haven\'t come from computing gradients through a loss function which has already accounted for this (like tf.reduce_mean(...)). The gradients have been summed by tf.gradients, so dividing by the batch size gives a mean gradient for the batch when applied with apply_gradients\n\n– parrowdice Jan 28, 2020 at 0:01\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIf y and x have the same shape then the sum over the dy/dx is the sum over exactly one value. However, if you have more than one y for each x, then the gradients are summed.\n\nimport numpy as np import tensorflow as tf x_dims = 3 batch_size = 4 x = tf.placeholder(tf.float32, (None, x_dims)) y = 2*(x**2) z = tf.stack([y, y]) # There are twice as many z\'s as x\'s dy_dx = tf.gradients(y,x) dz_dx = tf.gradients(z,x) sess = tf.Session() x_val = np.random.randint(0, 10, (batch_size, x_dims)) y_val, z_val, dy_dx_val, dz_dx_val = sess.run([y, z, dy_dx, dz_dx], {x:x_val}) print(\'x.shape =\', x_val.shape) print(\'x = \\n\', x_val) print(\'y.shape = \', y_val.shape) print(\'y = \\n\', y_val) print(\'z.shape = \', z_val.shape) print(\'z = \\n\', z_val) print(\'dy/dx = \\n\', dy_dx_val[0]) print(\'dz/dx = \\n\', dz_dx_val[0])\n\nProduces the following output:\n\nx.shape = (4, 3) x = [[1 4 8] [0 2 8] [2 8 1] [4 5 2]] y.shape = (4, 3) y = [[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]] z.shape = (2, 4, 3) z = [[[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]] [[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]]] dy/dx = [[ 4. 16. 32.] [ 0. 8. 32.] [ 8. 32. 4.] [16. 20. 8.]] dz/dx = [[ 8. 32. 64.] [ 0. 16. 64.] [16. 64. 8.] [32. 40. 16.]]\n\nIn particular, notice that the values of dz/dx are twice those of dy/dz since they are summed over the inputs to the stack.\n\nanswered Aug 15, 2018 at 16:03\n\n60633 silver badges88 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ngradient-descent or ask your own question.\n\nAn open-source development paradigm\n\nDevelopers get by with a little help from AI: Stack Overflow Knows code...\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n8 Tensorflow gradients: without automatic implicit sum\n\n5 Where in tensorflow gradients is the sum over the elements of y made?\n\n2 How do tf.gradients() work?\n\n0 How does tf.gradients behave when passed a list of `ys` tensors?\n\n5 If you use plus sign instead of tf.add, will tensorflow still calculate gradients correctly?\n\n0 Understanding how tf.gradients evaluates\n\n0 Why does this TensorFlow example not have a summation before the activation function?\n\n8 How to accumulate gradients in tensorflow 2.0?\n\n2 How does tensorflow.GradientTape() accumulate operation within a loop?\n\n2 Is the gradient of the sum equal to the sum of the gradients for a neural network in pytorch?\n\nHot Network Questions\n\nDid Paul remain a Jew even after his conversion?\n\nIs Genesis 2:7 sufficiently ambiguous to support the interpretation that God spent millions of years forming Adam\'s body from dust?\n\nIs there a technical reason why Datel Action Replay on PC required a card?\n\nPhrase or expression for being on the second level in terms of worseness\n\nWhich invention from the last fifty years could have the biggest impact on history?\n\nWhat is the origin of the Deck of Many Things?\n\nThe principle of legality in criminal law, and the common law\n\n""Galois theory"" on graphs\n\nHelp with a Calculus exercise (indefinite integral)\n\nWhy don\'t Hikaru, Fabi & Nepo buy out Liren so the 3 of them have a playoff to face Gukesh?\n\nI am the person who uses the mouse with ""their"" right hand?\n\nHow to manage being excluded from the lab group?\n\nPurpose of async/await in web servers\n\nWhat is the maximum number of times a liquid rocket engine has detonated/ exploded during development?\n\nHow many plot twists are too many plot twists?\n\nHow many Streifenkarte stripes are needed to get from Munich airport to the city centre?\n\nWhy are my benchmark times not repeatable, even for a CPU-bound task?\n\nIf someone is stranded in a transfer-only station after the last train has left, what happens?\n\nHow to prick Black Eyed Susan?\n\nOrderings in Philosophy\n\nCompare two strings, with NaC\n\nIs it bad style to write x^2, 2^\\frac{1}{2} and 2^\\sqrt{2}?\n\nHow can the Bitcoin blockchain be forked? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_4', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\ntf.gradients() sums over ys, does it?\n\nAsked 5 years, 9 months ago\n\nModified 5 years, 9 months ago\n\nhttps://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients\n\nIn the documentation for tf.gradients(ys, xs) it states that\n\nConstructs symbolic derivatives of sum of ys w.r.t. x in xs\n\nI am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example:\n\nx_dims = 3 batch_size = 4 x = tf.placeholder(tf.float32, (None, x_dims)) y = 2*(x**2) grads = tf.gradients(y,x) sess = tf.Session() x_val = np.random.randint(0, 10, (batch_size, x_dims)) y_val, grads_val = sess.run([y, grads], {x:x_val}) print(\'x = \\n\', x_val) print(\'y = \\n\', y_val) print(\'dy/dx = \\n\', grads_val[0])\n\nThis gives the following output:\n\nx = [[5 3 7] [2 2 5] [7 5 0] [3 7 6]] y = [[50. 18. 98.] [ 8. 8. 50.] [98. 50. 0.] [18. 98. 72.]] dy/dx = [[20. 12. 28.] [ 8. 8. 20.] [28. 20. 0.] [12. 28. 24.]]\n\nThis is the output I would expect, simply the derivative dy/dx for every element in the batch. I don\'t see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html). Why is this necessary?\n\nI am using Tensorflow 1.6 and Python 3.\n\nImprove this question\n\nasked Aug 15, 2018 at 12:45\n\n11911 silver badge1111 bronze badges 1\n\nTo give more insight over why you see the gradients summed over batch size in methods like DDPG: those gradients haven\'t come from computing gradients through a loss function which has already accounted for this (like tf.reduce_mean(...)). The gradients have been summed by tf.gradients, so dividing by the batch size gives a mean gradient for the batch when applied with apply_gradients\n\n– parrowdice Jan 28, 2020 at 0:01\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIf y and x have the same shape then the sum over the dy/dx is the sum over exactly one value. However, if you have more than one y for each x, then the gradients are summed.\n\nimport numpy as np import tensorflow as tf x_dims = 3 batch_size = 4 x = tf.placeholder(tf.float32, (None, x_dims)) y = 2*(x**2) z = tf.stack([y, y]) # There are twice as many z\'s as x\'s dy_dx = tf.gradients(y,x) dz_dx = tf.gradients(z,x) sess = tf.Session() x_val = np.random.randint(0, 10, (batch_size, x_dims)) y_val, z_val, dy_dx_val, dz_dx_val = sess.run([y, z, dy_dx, dz_dx], {x:x_val}) print(\'x.shape =\', x_val.shape) print(\'x = \\n\', x_val) print(\'y.shape = \', y_val.shape) print(\'y = \\n\', y_val) print(\'z.shape = \', z_val.shape) print(\'z = \\n\', z_val) print(\'dy/dx = \\n\', dy_dx_val[0]) print(\'dz/dx = \\n\', dz_dx_val[0])\n\nProduces the following output:\n\nx.shape = (4, 3) x = [[1 4 8] [0 2 8] [2 8 1] [4 5 2]] y.shape = (4, 3) y = [[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]] z.shape = (2, 4, 3) z = [[[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]] [[ 2. 32. 128.] [ 0. 8. 128.] [ 8. 128. 2.] [ 32. 50. 8.]]] dy/dx = [[ 4. 16. 32.] [ 0. 8. 32.] [ 8. 32. 4.] [16. 20. 8.]] dz/dx = [[ 8. 32. 64.] [ 0. 16. 64.] [16. 64. 8.] [32. 40. 16.]]\n\nIn particular, notice that the values of dz/dx are twice those of dy/dz since they are summed over the inputs to the stack.\n\nanswered Aug 15, 2018 at 16:03\n\n60633 silver badges88 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ngradient-descent or ask your own question.\n\nAn open-source development paradigm\n\nDevelopers get by with a little help from AI: Stack Overflow Knows code...\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n8 Tensorflow gradients: without automatic implicit sum\n\n5 Where in tensorflow gradients is the sum over the elements of y made?\n\n2 How do tf.gradients() work?\n\n0 How does tf.gradients behave when passed a list of `ys` tensors?\n\n5 If you use plus sign instead of tf.add, will tensorflow still calculate gradients correctly?\n\n0 Understanding how tf.gradients evaluates\n\n0 Why does this TensorFlow example not have a summation before the activation function?\n\n8 How to accumulate gradients in tensorflow 2.0?\n\n2 How does tensorflow.GradientTape() accumulate operation within a loop?\n\n2 Is the gradient of the sum equal to the sum of the gradients for a neural network in pytorch?\n\nHot Network Questions\n\nDid Paul remain a Jew even after his conversion?\n\nIs Genesis 2:7 sufficiently ambiguous to support the interpretation that God spent millions of years forming Adam\'s body from dust?\n\nIs there a technical reason why Datel Action Replay on PC required a card?\n\nPhrase or expression for being on the second level in terms of worseness\n\nWhich invention from the last fifty years could have the biggest impact on history?\n\nWhat is the origin of the Deck of Many Things?\n\nThe principle of legality in criminal law, and the common law\n\n""Galois theory"" on graphs\n\nHelp with a Calculus exercise (indefinite integral)\n\nWhy don\'t Hikaru, Fabi & Nepo buy out Liren so the 3 of them have a playoff to face Gukesh?\n\nI am the person who uses the mouse with ""their"" right hand?\n\nHow to manage being excluded from the lab group?\n\nPurpose of async/await in web servers\n\nWhat is the maximum number of times a liquid rocket engine has detonated/ exploded during development?\n\nHow many plot twists are too many plot twists?\n\nHow many Streifenkarte stripes are needed to get from Munich airport to the city centre?\n\nWhy are my benchmark times not repeatable, even for a CPU-bound task?\n\nIf someone is stranded in a transfer-only station after the last train has left, what happens?\n\nHow to prick Black Eyed Susan?\n\nOrderings in Philosophy\n\nCompare two strings, with NaC\n\nIs it bad style to write x^2, 2^\\frac{1}{2} and 2^\\sqrt{2}?\n\nHow can the Bitcoin blockchain be forked? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-05-31T17:47:56', 'title': 'python - tf.gradients() sums over ys, does it? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/51858970/tf-gradients-sums-over-ys-does-it'})], [Document(page_content=""William-Yin123 / tensorflow-docs Public\n\nSwitch branches/tags\n\nCould not load branches\n\n{{ refName }} default\n\n{{ refName }} default\n\nA tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?\n\ntensorflow-docs/tf/compat/v1/gradients.md Go to file\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\n Cannot retrieve contributors at this time\n\ntf.compat.v1.gradients Args: Returns: Raises:\n\n138 lines (102 sloc) 4.88 KB\n\nOpen in GitHub Desktop\n\nCopy raw contents Copy raw contents Copy raw contents Copy raw contents\n\ntf.compat.v1.gradients\n\nConstructs symbolic derivatives of sum of ys w.r.t. x in xs.\n\ntf.compat.v1.gradients( ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None, stop_gradients=None, unconnected_gradients=tf.UnconnectedGradients.NONE )\n\nys and xs are each a Tensor or a list of tensors. grad_ys is a list of Tensor, holding the gradients received by the ys. The list must be the same length as ys.\n\ngradients() adds ops to the graph to output the derivatives of ys with respect to xs. It returns a list of Tensor of length len(xs) where each tensor is the sum(dy/dx) for y in ys.\n\ngrad_ys is a list of tensors of the same length as ys that holds the initial gradients for each y in ys. When grad_ys is None, we fill in a tensor of '1's of the shape of y for each y in ys. A user can provide their own initial grad_ys to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).\n\nstop_gradients is a Tensor or a list of tensors to be considered constant with respect to all xs. These tensors will not be backpropagated through, as though they had been explicitly disconnected using stop_gradient. Among other things, this allows computation of partial derivatives as opposed to total derivatives. For example:\n\na = tf.constant(0.) b = 2 * a g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n\nHere the partial derivatives g evaluate to [1.0, 1.0], compared to the total derivatives tf.gradients(a + b, [a, b]), which take into account the influence of a on b and evaluate to [3.0, 1.0]. Note that the above is equivalent to:\n\na = tf.stop_gradient(tf.constant(0.)) b = tf.stop_gradient(2 * a) g = tf.gradients(a + b, [a, b])\n\nstop_gradients provides a way of stopping gradient after the graph has already been constructed, as compared to tf.stop_gradient which is used during graph construction. When the two approaches are combined, backpropagation stops at both tf.stop_gradient nodes and nodes in stop_gradients, whichever is encountered first.\n\nAll integer tensors are considered constant with respect to all xs, as if they were included in stop_gradients.\n\nunconnected_gradients determines the value returned for each x in xs if it is unconnected in the graph to ys. By default this is None to safeguard against errors. Mathematically these gradients are zero which can be requested using the 'zero' option. tf.UnconnectedGradients provides the following options and behaviors:\n\na = tf.ones([1, 2]) b = tf.ones([3, 1]) g1 = tf.gradients([b], [a], unconnected_gradients='none') sess.run(g1) # [None] g2 = tf.gradients([b], [a], unconnected_gradients='zero') sess.run(g2) # [array([[0., 0.]], dtype=float32)]\n\nys: A Tensor or list of tensors to be differentiated.\n\nxs: A Tensor or list of tensors to be used for differentiation.\n\ngrad_ys: Optional. A Tensor or list of tensors the same size as ys and holding the gradients computed for each y in ys.\n\nname: Optional name to use for grouping all the gradient ops together. defaults to 'gradients'.\n\ncolocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n\ngate_gradients: If True, add a tuple around the gradients returned for an operations. This avoids some race conditions.\n\naggregation_method: Specifies the method used to combine gradient terms. Accepted values are constants defined in the class AggregationMethod.\n\nstop_gradients: Optional. A Tensor or list of tensors not to differentiate through.\n\nunconnected_gradients: Optional. Specifies the gradient value returned when the given input tensors are unconnected. Accepted values are constants defined in the class tf.UnconnectedGradients and the default value is none.\n\nA list of sum(dy/dx) for each x in xs.\n\nLookupError: if one of the operations between x and y does not have a registered gradient function.\n\nValueError: if the arguments are invalid.\n\nRuntimeError: if called in Eager mode.\n\nYou can’t perform that action at this time.\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."", metadata={'id': 'web-search_5', 'snippet': ""William-Yin123 / tensorflow-docs Public\n\nSwitch branches/tags\n\nCould not load branches\n\n{{ refName }} default\n\n{{ refName }} default\n\nA tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?\n\ntensorflow-docs/tf/compat/v1/gradients.md Go to file\n\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n\n Cannot retrieve contributors at this time\n\ntf.compat.v1.gradients Args: Returns: Raises:\n\n138 lines (102 sloc) 4.88 KB\n\nOpen in GitHub Desktop\n\nCopy raw contents Copy raw contents Copy raw contents Copy raw contents\n\ntf.compat.v1.gradients\n\nConstructs symbolic derivatives of sum of ys w.r.t. x in xs.\n\ntf.compat.v1.gradients( ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None, stop_gradients=None, unconnected_gradients=tf.UnconnectedGradients.NONE )\n\nys and xs are each a Tensor or a list of tensors. grad_ys is a list of Tensor, holding the gradients received by the ys. The list must be the same length as ys.\n\ngradients() adds ops to the graph to output the derivatives of ys with respect to xs. It returns a list of Tensor of length len(xs) where each tensor is the sum(dy/dx) for y in ys.\n\ngrad_ys is a list of tensors of the same length as ys that holds the initial gradients for each y in ys. When grad_ys is None, we fill in a tensor of '1's of the shape of y for each y in ys. A user can provide their own initial grad_ys to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).\n\nstop_gradients is a Tensor or a list of tensors to be considered constant with respect to all xs. These tensors will not be backpropagated through, as though they had been explicitly disconnected using stop_gradient. Among other things, this allows computation of partial derivatives as opposed to total derivatives. For example:\n\na = tf.constant(0.) b = 2 * a g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n\nHere the partial derivatives g evaluate to [1.0, 1.0], compared to the total derivatives tf.gradients(a + b, [a, b]), which take into account the influence of a on b and evaluate to [3.0, 1.0]. Note that the above is equivalent to:\n\na = tf.stop_gradient(tf.constant(0.)) b = tf.stop_gradient(2 * a) g = tf.gradients(a + b, [a, b])\n\nstop_gradients provides a way of stopping gradient after the graph has already been constructed, as compared to tf.stop_gradient which is used during graph construction. When the two approaches are combined, backpropagation stops at both tf.stop_gradient nodes and nodes in stop_gradients, whichever is encountered first.\n\nAll integer tensors are considered constant with respect to all xs, as if they were included in stop_gradients.\n\nunconnected_gradients determines the value returned for each x in xs if it is unconnected in the graph to ys. By default this is None to safeguard against errors. Mathematically these gradients are zero which can be requested using the 'zero' option. tf.UnconnectedGradients provides the following options and behaviors:\n\na = tf.ones([1, 2]) b = tf.ones([3, 1]) g1 = tf.gradients([b], [a], unconnected_gradients='none') sess.run(g1) # [None] g2 = tf.gradients([b], [a], unconnected_gradients='zero') sess.run(g2) # [array([[0., 0.]], dtype=float32)]\n\nys: A Tensor or list of tensors to be differentiated.\n\nxs: A Tensor or list of tensors to be used for differentiation.\n\ngrad_ys: Optional. A Tensor or list of tensors the same size as ys and holding the gradients computed for each y in ys.\n\nname: Optional name to use for grouping all the gradient ops together. defaults to 'gradients'.\n\ncolocate_gradients_with_ops: If True, try colocating gradients with the corresponding op.\n\ngate_gradients: If True, add a tuple around the gradients returned for an operations. This avoids some race conditions.\n\naggregation_method: Specifies the method used to combine gradient terms. Accepted values are constants defined in the class AggregationMethod.\n\nstop_gradients: Optional. A Tensor or list of tensors not to differentiate through.\n\nunconnected_gradients: Optional. Specifies the gradient value returned when the given input tensors are unconnected. Accepted values are constants defined in the class tf.UnconnectedGradients and the default value is none.\n\nA list of sum(dy/dx) for each x in xs.\n\nLookupError: if one of the operations between x and y does not have a registered gradient function.\n\nValueError: if the arguments are invalid.\n\nRuntimeError: if called in Eager mode.\n\nYou can’t perform that action at this time.\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session."", 'timestamp': '2023-03-15T15:53:44', 'title': 'tensorflow-docs/gradients.md at master · William-Yin123/tensorflow-docs', 'url': 'https://github.com/William-Yin123/tensorflow-docs/blob/master/tf/compat/v1/gradients.md'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nGradients of non-scalars (higher rank Jacobians) #675\n\nzackchase opened this issue\n\nJan 4, 2016 · 68 comments\n\nGradients of non-scalars (higher rank Jacobians) #675\n\nzackchase opened this issue\n\nJan 4, 2016 · 68 comments\n\nCurrently if you call gradients(ys, xs), it will return the sum of dy/dx over all ys for each x in xs. I believe this doesn\'t accord with an a priori mathematical notion of the derivative of a vector. I\'d like the way to take the derivative of ys wrt xs where both are vectors and have a Jacobian matrix returned. By extension, I\'d like to take the derivative of a vector wrt a matrix and get back a 3-tensor. There doesn\'t seem to be a convenient tensorflow function to compute the Jacobian or higher order derivatives. Am I missing something or is this functionality that we could add?\n\nThe text was updated successfully, but these errors were encountered:\n\nzackchase@, you are right about the current gradients function. Currently, you can compute the Jacobian of, say, a vector, by calling gradients multiple times, one for every scalar component (obtained by slicing) of the original vector, and reassembling the results. Contributions are welcome to make this nicer and efficient.\n\nkeveman added the stat:contribution welcome\n\nStatus - Contributions welcome label\n\nIt\'d be pretty hard to support gradients of non-scalars with our current setup, since it would require every gradient function to handle extra rank input. The one possibility I could see would be if we add some sort of map facility to register how to add extra ranks to ops, then compute gradients with respect to extra rank by computing lower rank and calling the registered map transformations.\n\nSomeone asked for map a while back, so if anyone wanted to tackle this task that might be the way to go. Handling it at the gradient function level is probably bad, since it would add required complexity to an existing feature. Warning: This is a pretty large change, so a good deal of discussion would be in order before starting.\n\ngirving changed the title\n\nProblems Calculating the Jacobian\n\nGradients of non-scalars (higher rank Jacobians)\n\ngirving added the enhancement label\n\nHi Geoffrey, thanks for taking an interest in this issue. I was initially confused by the use of ""rank"" to describe the number of dimensions of the array. Should we avoid this name in the thread title and documentation to preempt confusion via overloading the linear algebra notion of rank?\n\nTensor rank is very standard terminology: http://mathworld.wolfram.com/TensorRank.html\n\nCool. The terminology gets funny when we talk about rank-R decompositions of tensors, meaning the tensor can be represented as a sum of R outer products of rank-1 tensors, but probably not a problem for us to solve here.\n\nOne thing I thought of is that I would like to compute the frobenius norm of the Jacobian of the log probabilities for use as a smoothness penalty much like the smoothness penalty used in a contractive autoencoder. In this case, as we only seek a scalar at the end, is there a more efficient method than separately calculating the derivative of each output with respect to the inputs?\n\nAre you saying your network has a bunch of outputs, and then you combine them into a single scalar that you are trying to optimize? In that case, you should differentiate with respect to that single scalar.\n\nNot exactly. I\'m saying if one wants to penalize the norm of the Jacobian of the mapping function. So optimization objective would be (pseudocode):\n\ncost(y,yhat, X) = loss(y,yhat) + norm(Jacobian(log(yhat), X))\n\nAh, sorry for not reading carefully. You\'re correct that (as far as I know) there\'s no easy way to do that in current tensorflow. According to someone more knowledgeable than I, people generally do such contractive autoencoders by writing out the first derivative manually. Also, they generally restrict to single layer at a time networks for speed issues, since doing the full Jacobian for a multilayer network is quite expensive.\n\nRegardless, it would be good to have a way to call derivatives of vectors and receive gradients of the expected shape.\n\nyaroslavvb commented\n\nDifferentiating with respect to one variable is similar to how it works in Theano. I agree it may be confusing when TensorFlow automatically turns many variables into one by taking the sum. An alternative would be to fail if there\'s more than 1 output variable specified, or have a wrapper that automatically calls existing gradient function on each output variable\n\nThe reason for ""one output variable at a time"" in TensorFlow (and Theano) is because we do reverse mode AD by default. In reverse AD you have a single target scalar quantity and you propagate sensitivities with respect to that quantity. In contrast, if you we did forward AD instead, we would naturally support multiple output variables, but only compute derivative with respect to one scalar variable at a time. Supporting mixed mode propagation to cover ""multiple inputs/multiple outputs"" case in the most efficient way could be a lot of extra plumbing.\n\nIf you have a small number of output variables but large number of input variables, standard thing to do is to apply reverse AD with respect to each variable in a loop. This is what Theano recommends to do for compute Hessian for instance: http://deeplearning.net/software/theano/tutorial/gradients.html#computing-the-hessian. If you have a small number of input variables but large number of output variables, then the most efficient thing to do would be to run forward-mode AD for all the input variables in a loop. Forward mode AD is not implemented and would require adding an equivalent of Theano\'s ""Rop"" operator to differentiable ops and some plumbing to call them instead of existing op ""gradient"" function (existing gradient function is an equivalent of Lop operation, or ""left multiply sensitivity vector by op\'s jacobian"" operation)\n\ntillahoffmann commented\n\nI was hoping to implement higher order derivatives using the map function but am getting an error message I can\'t quite get my head around. My implementation is (in pseudo code)\n\nparams = tf.Variable(""some initial value"") loss = some_function(params) grads = tf.gradients(loss, params)[0] hess = tf.map_fn(lambda grad: tf.gradients(grad, X)[0], grads)\n\nWhen I fetch the hessian, I get the error message\n\nInvalidArgumentError: All inputs to node map/while/gradients/map/TensorArrayUnpack_grad/TensorArrayGrad/TensorArrayGrad must be from the same frame.\n\nI assumed that tensorflow has an issue because it doesn\'t know about params in the loop (cf. non_sequences in theano scan), and extended map_fn to pass extra arguments to the loop. Unfortunately, the extra arguments get wrapped in an identity transformation and tf.gradients(params, tf.identity(params)) gives [None], which seems a bit unintuitive.\n\nLooping in python is of course fine but I\'d like to avoid introducing an extra node to the graph for every parameter. Any suggestions?\n\n@yuanbyu: Do you understand this issue with tf.map_fn?\n\ngirving added the triaged label\n\nNote for anyone who comes across this thread: tf.map_fn is an unrelated thing involving control flow, not something related to mapping over extra rank tensors.\n\ngirving mentioned this issue\n\nHessian (calling tf.gradients twice) of tf.scan fails #2598\n\naselle removed the triaged label\n\nWe don\'t support higher-order gradients for while_loop/map_fn/scan/fold. You should see an informative error message if you try to do that.\n\nyuanbyu closed this as completed\n\n@yaroslavvb Any plans on adding forward mode AD? I filed an issue on it a couple weeks ago but haven\'t heard back.\n\nyaroslavvb commented\n\n@vladfi1 I\'m no longer at Brain, so I wouldn\'t know. I would say it is unlikely to ever be part of core TensorFlow. There are >450 ops in TF, so Brain team would have to implement forward AD grad method for all of 450 ops and maintain them forever, or alternatively have to explain why someone\'s favorite op doesn\'t have forward AD support. It seems more realistic that someone would create a separately maintained library that does forward-AD, and utilizes TensorFlow as backend. Kind of like autograd but using TensorFlow instead of numpy as the backend.\n\ntillahoffmann mentioned this issue\n\nHessian with respect to one-dimensional tensors #5329\n\ncxfneo mentioned this issue\n\nIssues: self.action_grads = tf.gradients(self.out, self.action) pemami4911/deep-rl#3\n\naselle added type:feature\n\nFeature requests and removed enhancement labels\n\nIs tf.test.compute_gradient is some kind of function that we can get the Jacobian matrix (not as a tensor but as a numpy.ndarray) of a vector tensor y w.r.t. a vector tensor x?\n\nyaroslavvb commented\n\nFeb 20, 2017 via email\n\nThere are no built-in jacobians in TensorFlow, instead anything called \'grad\' or \'gradient\' computes Jacobian-vector product (also called LOp in theano), see https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation …\n\nOn Mon, Feb 20, 2017 at 4:04 AM, MYaooo ***@***.***> wrote: Is tf.test.compute_gradient is some kind of funtion that we can get the jacobian matrix (not as a tensor but as a numpy.ndarray) of a vector tensor y w.r.t. a vector tensor x? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#675 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AABaHO7dLBLvhJH5KoSiV8WcySnfR5cnks5reYFMgaJpZM4G9_8_> .\n\n@yaroslavvb Thanks for your reply. I see why it\'s expensive to do that. Do you have any suggestions if I want to get the numerical results of the Jacobian with certain inputs x. The only not so expensive workaround I can think of is to apply perturbation on each dim of x to get the approximate results.\n\n@marcociccone in TF 1.4 seems to be working fine, if this is an alternative for you.\n\nThis is indeed a bug in TF. It\'s caused by taking the gradient(y, x) inside a while loop wrt such that the computation of y from x goes through a different while loop, something like:\n\nx = ... y = while_loop(..., [x]) z = while_loop(..., tf.gradients(y, x), [y])\n\nSo in @mholzel\'s script, it\'s from passing the outcome of one jacobian call to the other jacobian call. (BTW, thanks very much for the easy repro.)\n\nUnfortunately this is quite tricky to fix. I\'ll try to take another look at it tomorrow and see if I can come up with something.\n\nskye assigned skye and unassigned agarwal-ashish\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nThis is actually extremely tricky to fix. I\'m not sure how this was working in 1.4, was it definitely giving the right answer?\n\nThe fundamental problem is that the second jacobian call (i.e. the hessian) is calling tf.gradients() inside a while loop, and that backprop calculation must go through the while loop from the first jacobian call. TF computes while loop gradients using stacks to store intermediate loop values, so if you\'re doing that calculation multiple times via another loop, we\'d have to somehow re-use the stack values on each iteration. This is conceptually possible but would be a pretty big change. I can at least try to improve the error message though.\n\nI have never seen the nested call work, but I did not try 1.4.\n\nskye added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nfKunstner mentioned this issue\n\n[feature request] Simple and Efficient way to get gradients of each element of a sum pytorch/pytorch#7786\n\nJoshuaalbert commented\n\n@mholzel for the double jacobian, suppose you just map to tf.hessian instead of tf.gradient. Doesn\'t solve to arbitrary order but it does get the hessian of a tensor wrt variables.\n\nagarwal-ashish commented\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nHarshini-Gadige added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nHarshini-Gadige unassigned skye\n\ntensorflowbutler assigned poxvoculi\n\npoxvoculi assigned agarwal-ashish and unassigned poxvoculi\n\nagarwal-ashish assigned saxenasaurabh and unassigned agarwal-ashish\n\nyifannieudem mentioned this issue\n\nCannot calculate tf.gradients wrt embedding_matrix #23033\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\nDid anyone compare the performance of the new functionality and the previous solution posted by @mholzel here? I am finding the new jacobian included in gradients.py way slower than @mholzel solution...\n\nagarwal-ashish commented\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\nDid anyone compare the performance of the new functionality and the previous solution posted by @mholzel here? I am finding the new jacobian included in gradients.py way slower than @mholzel solution...\n\ngradients_test.py in the same file has some benchmarks that show that pfor based jacobian generally works much faster than a while loop based one. Can you share your benchmark ?\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nI threw together this function, it uses tf.map_fn and assumes a batch style input, but at least it works for batches. It\'s pretty high level but seems to adress some of the ideas if the conversation above:\n\ndef Jacobian(X, y): J = tf.map_fn(lambda m: tf.gradients(y[:,:,m:m+1], X)[0], tf.range(tf.shape(y)[-1]), tf.float32) J = tf.transpose(tf.squeeze(J), perm = [1,0,2]) return J ```\n\njvishnuvardhan commented\n\n@zackchase Is this resolved? Please close If it was resolved already. Thanks!\n\nagarwal-ashish commented\n\ntf.GradientTape.jacobian and tf.GradientTape.batch_jacobian APIs have been added for computing Jacobians. These are based on experimental pfor logic which can also be disabled to fallback to loop based implementation.\n\nagarwal-ashish closed this as completed\n\nppwwyyxx mentioned this issue\n\nHow to calculate gradients of a list of y with respect to a single x? tensorpack/tensorpack#1281\n\ntsbertalan mentioned this issue\n\n[TF 2.0 API Docs] tf.custom_gradient #26270\n\nekuznetsov139 pushed a commit to ekuznetsov139/tensorflow that referenced this issue\n\nMerge pull request tensorflow#675 from ROCmSoftwarePlatform/r1.15-roc… …\n\n…m-pr-620 Cherry-pick PR 620\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_2', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nGradients of non-scalars (higher rank Jacobians) #675\n\nzackchase opened this issue\n\nJan 4, 2016 · 68 comments\n\nGradients of non-scalars (higher rank Jacobians) #675\n\nzackchase opened this issue\n\nJan 4, 2016 · 68 comments\n\nCurrently if you call gradients(ys, xs), it will return the sum of dy/dx over all ys for each x in xs. I believe this doesn\'t accord with an a priori mathematical notion of the derivative of a vector. I\'d like the way to take the derivative of ys wrt xs where both are vectors and have a Jacobian matrix returned. By extension, I\'d like to take the derivative of a vector wrt a matrix and get back a 3-tensor. There doesn\'t seem to be a convenient tensorflow function to compute the Jacobian or higher order derivatives. Am I missing something or is this functionality that we could add?\n\nThe text was updated successfully, but these errors were encountered:\n\nzackchase@, you are right about the current gradients function. Currently, you can compute the Jacobian of, say, a vector, by calling gradients multiple times, one for every scalar component (obtained by slicing) of the original vector, and reassembling the results. Contributions are welcome to make this nicer and efficient.\n\nkeveman added the stat:contribution welcome\n\nStatus - Contributions welcome label\n\nIt\'d be pretty hard to support gradients of non-scalars with our current setup, since it would require every gradient function to handle extra rank input. The one possibility I could see would be if we add some sort of map facility to register how to add extra ranks to ops, then compute gradients with respect to extra rank by computing lower rank and calling the registered map transformations.\n\nSomeone asked for map a while back, so if anyone wanted to tackle this task that might be the way to go. Handling it at the gradient function level is probably bad, since it would add required complexity to an existing feature. Warning: This is a pretty large change, so a good deal of discussion would be in order before starting.\n\ngirving changed the title\n\nProblems Calculating the Jacobian\n\nGradients of non-scalars (higher rank Jacobians)\n\ngirving added the enhancement label\n\nHi Geoffrey, thanks for taking an interest in this issue. I was initially confused by the use of ""rank"" to describe the number of dimensions of the array. Should we avoid this name in the thread title and documentation to preempt confusion via overloading the linear algebra notion of rank?\n\nTensor rank is very standard terminology: http://mathworld.wolfram.com/TensorRank.html\n\nCool. The terminology gets funny when we talk about rank-R decompositions of tensors, meaning the tensor can be represented as a sum of R outer products of rank-1 tensors, but probably not a problem for us to solve here.\n\nOne thing I thought of is that I would like to compute the frobenius norm of the Jacobian of the log probabilities for use as a smoothness penalty much like the smoothness penalty used in a contractive autoencoder. In this case, as we only seek a scalar at the end, is there a more efficient method than separately calculating the derivative of each output with respect to the inputs?\n\nAre you saying your network has a bunch of outputs, and then you combine them into a single scalar that you are trying to optimize? In that case, you should differentiate with respect to that single scalar.\n\nNot exactly. I\'m saying if one wants to penalize the norm of the Jacobian of the mapping function. So optimization objective would be (pseudocode):\n\ncost(y,yhat, X) = loss(y,yhat) + norm(Jacobian(log(yhat), X))\n\nAh, sorry for not reading carefully. You\'re correct that (as far as I know) there\'s no easy way to do that in current tensorflow. According to someone more knowledgeable than I, people generally do such contractive autoencoders by writing out the first derivative manually. Also, they generally restrict to single layer at a time networks for speed issues, since doing the full Jacobian for a multilayer network is quite expensive.\n\nRegardless, it would be good to have a way to call derivatives of vectors and receive gradients of the expected shape.\n\nyaroslavvb commented\n\nDifferentiating with respect to one variable is similar to how it works in Theano. I agree it may be confusing when TensorFlow automatically turns many variables into one by taking the sum. An alternative would be to fail if there\'s more than 1 output variable specified, or have a wrapper that automatically calls existing gradient function on each output variable\n\nThe reason for ""one output variable at a time"" in TensorFlow (and Theano) is because we do reverse mode AD by default. In reverse AD you have a single target scalar quantity and you propagate sensitivities with respect to that quantity. In contrast, if you we did forward AD instead, we would naturally support multiple output variables, but only compute derivative with respect to one scalar variable at a time. Supporting mixed mode propagation to cover ""multiple inputs/multiple outputs"" case in the most efficient way could be a lot of extra plumbing.\n\nIf you have a small number of output variables but large number of input variables, standard thing to do is to apply reverse AD with respect to each variable in a loop. This is what Theano recommends to do for compute Hessian for instance: http://deeplearning.net/software/theano/tutorial/gradients.html#computing-the-hessian. If you have a small number of input variables but large number of output variables, then the most efficient thing to do would be to run forward-mode AD for all the input variables in a loop. Forward mode AD is not implemented and would require adding an equivalent of Theano\'s ""Rop"" operator to differentiable ops and some plumbing to call them instead of existing op ""gradient"" function (existing gradient function is an equivalent of Lop operation, or ""left multiply sensitivity vector by op\'s jacobian"" operation)\n\ntillahoffmann commented\n\nI was hoping to implement higher order derivatives using the map function but am getting an error message I can\'t quite get my head around. My implementation is (in pseudo code)\n\nparams = tf.Variable(""some initial value"") loss = some_function(params) grads = tf.gradients(loss, params)[0] hess = tf.map_fn(lambda grad: tf.gradients(grad, X)[0], grads)\n\nWhen I fetch the hessian, I get the error message\n\nInvalidArgumentError: All inputs to node map/while/gradients/map/TensorArrayUnpack_grad/TensorArrayGrad/TensorArrayGrad must be from the same frame.\n\nI assumed that tensorflow has an issue because it doesn\'t know about params in the loop (cf. non_sequences in theano scan), and extended map_fn to pass extra arguments to the loop. Unfortunately, the extra arguments get wrapped in an identity transformation and tf.gradients(params, tf.identity(params)) gives [None], which seems a bit unintuitive.\n\nLooping in python is of course fine but I\'d like to avoid introducing an extra node to the graph for every parameter. Any suggestions?\n\n@yuanbyu: Do you understand this issue with tf.map_fn?\n\ngirving added the triaged label\n\nNote for anyone who comes across this thread: tf.map_fn is an unrelated thing involving control flow, not something related to mapping over extra rank tensors.\n\ngirving mentioned this issue\n\nHessian (calling tf.gradients twice) of tf.scan fails #2598\n\naselle removed the triaged label\n\nWe don\'t support higher-order gradients for while_loop/map_fn/scan/fold. You should see an informative error message if you try to do that.\n\nyuanbyu closed this as completed\n\n@yaroslavvb Any plans on adding forward mode AD? I filed an issue on it a couple weeks ago but haven\'t heard back.\n\nyaroslavvb commented\n\n@vladfi1 I\'m no longer at Brain, so I wouldn\'t know. I would say it is unlikely to ever be part of core TensorFlow. There are >450 ops in TF, so Brain team would have to implement forward AD grad method for all of 450 ops and maintain them forever, or alternatively have to explain why someone\'s favorite op doesn\'t have forward AD support. It seems more realistic that someone would create a separately maintained library that does forward-AD, and utilizes TensorFlow as backend. Kind of like autograd but using TensorFlow instead of numpy as the backend.\n\ntillahoffmann mentioned this issue\n\nHessian with respect to one-dimensional tensors #5329\n\ncxfneo mentioned this issue\n\nIssues: self.action_grads = tf.gradients(self.out, self.action) pemami4911/deep-rl#3\n\naselle added type:feature\n\nFeature requests and removed enhancement labels\n\nIs tf.test.compute_gradient is some kind of function that we can get the Jacobian matrix (not as a tensor but as a numpy.ndarray) of a vector tensor y w.r.t. a vector tensor x?\n\nyaroslavvb commented\n\nFeb 20, 2017 via email\n\nThere are no built-in jacobians in TensorFlow, instead anything called \'grad\' or \'gradient\' computes Jacobian-vector product (also called LOp in theano), see https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation …\n\nOn Mon, Feb 20, 2017 at 4:04 AM, MYaooo ***@***.***> wrote: Is tf.test.compute_gradient is some kind of funtion that we can get the jacobian matrix (not as a tensor but as a numpy.ndarray) of a vector tensor y w.r.t. a vector tensor x? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#675 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AABaHO7dLBLvhJH5KoSiV8WcySnfR5cnks5reYFMgaJpZM4G9_8_> .\n\n@yaroslavvb Thanks for your reply. I see why it\'s expensive to do that. Do you have any suggestions if I want to get the numerical results of the Jacobian with certain inputs x. The only not so expensive workaround I can think of is to apply perturbation on each dim of x to get the approximate results.\n\n@marcociccone in TF 1.4 seems to be working fine, if this is an alternative for you.\n\nThis is indeed a bug in TF. It\'s caused by taking the gradient(y, x) inside a while loop wrt such that the computation of y from x goes through a different while loop, something like:\n\nx = ... y = while_loop(..., [x]) z = while_loop(..., tf.gradients(y, x), [y])\n\nSo in @mholzel\'s script, it\'s from passing the outcome of one jacobian call to the other jacobian call. (BTW, thanks very much for the easy repro.)\n\nUnfortunately this is quite tricky to fix. I\'ll try to take another look at it tomorrow and see if I can come up with something.\n\nskye assigned skye and unassigned agarwal-ashish\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nThis is actually extremely tricky to fix. I\'m not sure how this was working in 1.4, was it definitely giving the right answer?\n\nThe fundamental problem is that the second jacobian call (i.e. the hessian) is calling tf.gradients() inside a while loop, and that backprop calculation must go through the while loop from the first jacobian call. TF computes while loop gradients using stacks to store intermediate loop values, so if you\'re doing that calculation multiple times via another loop, we\'d have to somehow re-use the stack values on each iteration. This is conceptually possible but would be a pretty big change. I can at least try to improve the error message though.\n\nI have never seen the nested call work, but I did not try 1.4.\n\nskye added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nfKunstner mentioned this issue\n\n[feature request] Simple and Efficient way to get gradients of each element of a sum pytorch/pytorch#7786\n\nJoshuaalbert commented\n\n@mholzel for the double jacobian, suppose you just map to tf.hessian instead of tf.gradient. Doesn\'t solve to arbitrary order but it does get the hessian of a tensor wrt variables.\n\nagarwal-ashish commented\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nHarshini-Gadige added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nHarshini-Gadige unassigned skye\n\ntensorflowbutler assigned poxvoculi\n\npoxvoculi assigned agarwal-ashish and unassigned poxvoculi\n\nagarwal-ashish assigned saxenasaurabh and unassigned agarwal-ashish\n\nyifannieudem mentioned this issue\n\nCannot calculate tf.gradients wrt embedding_matrix #23033\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\nDid anyone compare the performance of the new functionality and the previous solution posted by @mholzel here? I am finding the new jacobian included in gradients.py way slower than @mholzel solution...\n\nagarwal-ashish commented\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\nDid anyone compare the performance of the new functionality and the previous solution posted by @mholzel here? I am finding the new jacobian included in gradients.py way slower than @mholzel solution...\n\ngradients_test.py in the same file has some benchmarks that show that pfor based jacobian generally works much faster than a while loop based one. Can you share your benchmark ?\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nI threw together this function, it uses tf.map_fn and assumes a batch style input, but at least it works for batches. It\'s pretty high level but seems to adress some of the ideas if the conversation above:\n\ndef Jacobian(X, y): J = tf.map_fn(lambda m: tf.gradients(y[:,:,m:m+1], X)[0], tf.range(tf.shape(y)[-1]), tf.float32) J = tf.transpose(tf.squeeze(J), perm = [1,0,2]) return J ```\n\njvishnuvardhan commented\n\n@zackchase Is this resolved? Please close If it was resolved already. Thanks!\n\nagarwal-ashish commented\n\ntf.GradientTape.jacobian and tf.GradientTape.batch_jacobian APIs have been added for computing Jacobians. These are based on experimental pfor logic which can also be disabled to fallback to loop based implementation.\n\nagarwal-ashish closed this as completed\n\nppwwyyxx mentioned this issue\n\nHow to calculate gradients of a list of y with respect to a single x? tensorpack/tensorpack#1281\n\ntsbertalan mentioned this issue\n\n[TF 2.0 API Docs] tf.custom_gradient #26270\n\nekuznetsov139 pushed a commit to ekuznetsov139/tensorflow that referenced this issue\n\nMerge pull request tensorflow#675 from ROCmSoftwarePlatform/r1.15-roc… …\n\n…m-pr-620 Cherry-pick PR 620\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-28T12:44:36', 'title': 'Gradients of non-scalars (higher rank Jacobians) · Issue #675 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/675'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nGradients of non-scalars (higher rank Jacobians) #675\n\nzackchase opened this issue\n\nJan 4, 2016 · 68 comments\n\nGradients of non-scalars (higher rank Jacobians) #675\n\nzackchase opened this issue\n\nJan 4, 2016 · 68 comments\n\nCurrently if you call gradients(ys, xs), it will return the sum of dy/dx over all ys for each x in xs. I believe this doesn\'t accord with an a priori mathematical notion of the derivative of a vector. I\'d like the way to take the derivative of ys wrt xs where both are vectors and have a Jacobian matrix returned. By extension, I\'d like to take the derivative of a vector wrt a matrix and get back a 3-tensor. There doesn\'t seem to be a convenient tensorflow function to compute the Jacobian or higher order derivatives. Am I missing something or is this functionality that we could add?\n\nThe text was updated successfully, but these errors were encountered:\n\nzackchase@, you are right about the current gradients function. Currently, you can compute the Jacobian of, say, a vector, by calling gradients multiple times, one for every scalar component (obtained by slicing) of the original vector, and reassembling the results. Contributions are welcome to make this nicer and efficient.\n\nkeveman added the stat:contribution welcome\n\nStatus - Contributions welcome label\n\nIt\'d be pretty hard to support gradients of non-scalars with our current setup, since it would require every gradient function to handle extra rank input. The one possibility I could see would be if we add some sort of map facility to register how to add extra ranks to ops, then compute gradients with respect to extra rank by computing lower rank and calling the registered map transformations.\n\nSomeone asked for map a while back, so if anyone wanted to tackle this task that might be the way to go. Handling it at the gradient function level is probably bad, since it would add required complexity to an existing feature. Warning: This is a pretty large change, so a good deal of discussion would be in order before starting.\n\ngirving changed the title\n\nProblems Calculating the Jacobian\n\nGradients of non-scalars (higher rank Jacobians)\n\ngirving added the enhancement label\n\nHi Geoffrey, thanks for taking an interest in this issue. I was initially confused by the use of ""rank"" to describe the number of dimensions of the array. Should we avoid this name in the thread title and documentation to preempt confusion via overloading the linear algebra notion of rank?\n\nTensor rank is very standard terminology: http://mathworld.wolfram.com/TensorRank.html\n\nCool. The terminology gets funny when we talk about rank-R decompositions of tensors, meaning the tensor can be represented as a sum of R outer products of rank-1 tensors, but probably not a problem for us to solve here.\n\nOne thing I thought of is that I would like to compute the frobenius norm of the Jacobian of the log probabilities for use as a smoothness penalty much like the smoothness penalty used in a contractive autoencoder. In this case, as we only seek a scalar at the end, is there a more efficient method than separately calculating the derivative of each output with respect to the inputs?\n\nAre you saying your network has a bunch of outputs, and then you combine them into a single scalar that you are trying to optimize? In that case, you should differentiate with respect to that single scalar.\n\nNot exactly. I\'m saying if one wants to penalize the norm of the Jacobian of the mapping function. So optimization objective would be (pseudocode):\n\ncost(y,yhat, X) = loss(y,yhat) + norm(Jacobian(log(yhat), X))\n\nAh, sorry for not reading carefully. You\'re correct that (as far as I know) there\'s no easy way to do that in current tensorflow. According to someone more knowledgeable than I, people generally do such contractive autoencoders by writing out the first derivative manually. Also, they generally restrict to single layer at a time networks for speed issues, since doing the full Jacobian for a multilayer network is quite expensive.\n\nRegardless, it would be good to have a way to call derivatives of vectors and receive gradients of the expected shape.\n\nyaroslavvb commented\n\nDifferentiating with respect to one variable is similar to how it works in Theano. I agree it may be confusing when TensorFlow automatically turns many variables into one by taking the sum. An alternative would be to fail if there\'s more than 1 output variable specified, or have a wrapper that automatically calls existing gradient function on each output variable\n\nThe reason for ""one output variable at a time"" in TensorFlow (and Theano) is because we do reverse mode AD by default. In reverse AD you have a single target scalar quantity and you propagate sensitivities with respect to that quantity. In contrast, if you we did forward AD instead, we would naturally support multiple output variables, but only compute derivative with respect to one scalar variable at a time. Supporting mixed mode propagation to cover ""multiple inputs/multiple outputs"" case in the most efficient way could be a lot of extra plumbing.\n\nIf you have a small number of output variables but large number of input variables, standard thing to do is to apply reverse AD with respect to each variable in a loop. This is what Theano recommends to do for compute Hessian for instance: http://deeplearning.net/software/theano/tutorial/gradients.html#computing-the-hessian. If you have a small number of input variables but large number of output variables, then the most efficient thing to do would be to run forward-mode AD for all the input variables in a loop. Forward mode AD is not implemented and would require adding an equivalent of Theano\'s ""Rop"" operator to differentiable ops and some plumbing to call them instead of existing op ""gradient"" function (existing gradient function is an equivalent of Lop operation, or ""left multiply sensitivity vector by op\'s jacobian"" operation)\n\ntillahoffmann commented\n\nI was hoping to implement higher order derivatives using the map function but am getting an error message I can\'t quite get my head around. My implementation is (in pseudo code)\n\nparams = tf.Variable(""some initial value"") loss = some_function(params) grads = tf.gradients(loss, params)[0] hess = tf.map_fn(lambda grad: tf.gradients(grad, X)[0], grads)\n\nWhen I fetch the hessian, I get the error message\n\nInvalidArgumentError: All inputs to node map/while/gradients/map/TensorArrayUnpack_grad/TensorArrayGrad/TensorArrayGrad must be from the same frame.\n\nI assumed that tensorflow has an issue because it doesn\'t know about params in the loop (cf. non_sequences in theano scan), and extended map_fn to pass extra arguments to the loop. Unfortunately, the extra arguments get wrapped in an identity transformation and tf.gradients(params, tf.identity(params)) gives [None], which seems a bit unintuitive.\n\nLooping in python is of course fine but I\'d like to avoid introducing an extra node to the graph for every parameter. Any suggestions?\n\n@yuanbyu: Do you understand this issue with tf.map_fn?\n\ngirving added the triaged label\n\nNote for anyone who comes across this thread: tf.map_fn is an unrelated thing involving control flow, not something related to mapping over extra rank tensors.\n\ngirving mentioned this issue\n\nHessian (calling tf.gradients twice) of tf.scan fails #2598\n\naselle removed the triaged label\n\nWe don\'t support higher-order gradients for while_loop/map_fn/scan/fold. You should see an informative error message if you try to do that.\n\nyuanbyu closed this as completed\n\n@yaroslavvb Any plans on adding forward mode AD? I filed an issue on it a couple weeks ago but haven\'t heard back.\n\nyaroslavvb commented\n\n@vladfi1 I\'m no longer at Brain, so I wouldn\'t know. I would say it is unlikely to ever be part of core TensorFlow. There are >450 ops in TF, so Brain team would have to implement forward AD grad method for all of 450 ops and maintain them forever, or alternatively have to explain why someone\'s favorite op doesn\'t have forward AD support. It seems more realistic that someone would create a separately maintained library that does forward-AD, and utilizes TensorFlow as backend. Kind of like autograd but using TensorFlow instead of numpy as the backend.\n\ntillahoffmann mentioned this issue\n\nHessian with respect to one-dimensional tensors #5329\n\ncxfneo mentioned this issue\n\nIssues: self.action_grads = tf.gradients(self.out, self.action) pemami4911/deep-rl#3\n\naselle added type:feature\n\nFeature requests and removed enhancement labels\n\nIs tf.test.compute_gradient is some kind of function that we can get the Jacobian matrix (not as a tensor but as a numpy.ndarray) of a vector tensor y w.r.t. a vector tensor x?\n\nyaroslavvb commented\n\nFeb 20, 2017 via email\n\nThere are no built-in jacobians in TensorFlow, instead anything called \'grad\' or \'gradient\' computes Jacobian-vector product (also called LOp in theano), see https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation …\n\nOn Mon, Feb 20, 2017 at 4:04 AM, MYaooo ***@***.***> wrote: Is tf.test.compute_gradient is some kind of funtion that we can get the jacobian matrix (not as a tensor but as a numpy.ndarray) of a vector tensor y w.r.t. a vector tensor x? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#675 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AABaHO7dLBLvhJH5KoSiV8WcySnfR5cnks5reYFMgaJpZM4G9_8_> .\n\n@yaroslavvb Thanks for your reply. I see why it\'s expensive to do that. Do you have any suggestions if I want to get the numerical results of the Jacobian with certain inputs x. The only not so expensive workaround I can think of is to apply perturbation on each dim of x to get the approximate results.\n\n@marcociccone in TF 1.4 seems to be working fine, if this is an alternative for you.\n\nThis is indeed a bug in TF. It\'s caused by taking the gradient(y, x) inside a while loop wrt such that the computation of y from x goes through a different while loop, something like:\n\nx = ... y = while_loop(..., [x]) z = while_loop(..., tf.gradients(y, x), [y])\n\nSo in @mholzel\'s script, it\'s from passing the outcome of one jacobian call to the other jacobian call. (BTW, thanks very much for the easy repro.)\n\nUnfortunately this is quite tricky to fix. I\'ll try to take another look at it tomorrow and see if I can come up with something.\n\nskye assigned skye and unassigned agarwal-ashish\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nThis is actually extremely tricky to fix. I\'m not sure how this was working in 1.4, was it definitely giving the right answer?\n\nThe fundamental problem is that the second jacobian call (i.e. the hessian) is calling tf.gradients() inside a while loop, and that backprop calculation must go through the while loop from the first jacobian call. TF computes while loop gradients using stacks to store intermediate loop values, so if you\'re doing that calculation multiple times via another loop, we\'d have to somehow re-use the stack values on each iteration. This is conceptually possible but would be a pretty big change. I can at least try to improve the error message though.\n\nI have never seen the nested call work, but I did not try 1.4.\n\nskye added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nfKunstner mentioned this issue\n\n[feature request] Simple and Efficient way to get gradients of each element of a sum pytorch/pytorch#7786\n\nJoshuaalbert commented\n\n@mholzel for the double jacobian, suppose you just map to tf.hessian instead of tf.gradient. Doesn\'t solve to arbitrary order but it does get the hessian of a tensor wrt variables.\n\nagarwal-ashish commented\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nHarshini-Gadige added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nHarshini-Gadige unassigned skye\n\ntensorflowbutler assigned poxvoculi\n\npoxvoculi assigned agarwal-ashish and unassigned poxvoculi\n\nagarwal-ashish assigned saxenasaurabh and unassigned agarwal-ashish\n\nyifannieudem mentioned this issue\n\nCannot calculate tf.gradients wrt embedding_matrix #23033\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\nDid anyone compare the performance of the new functionality and the previous solution posted by @mholzel here? I am finding the new jacobian included in gradients.py way slower than @mholzel solution...\n\nagarwal-ashish commented\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\nDid anyone compare the performance of the new functionality and the previous solution posted by @mholzel here? I am finding the new jacobian included in gradients.py way slower than @mholzel solution...\n\ngradients_test.py in the same file has some benchmarks that show that pfor based jacobian generally works much faster than a while loop based one. Can you share your benchmark ?\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nI threw together this function, it uses tf.map_fn and assumes a batch style input, but at least it works for batches. It\'s pretty high level but seems to adress some of the ideas if the conversation above:\n\ndef Jacobian(X, y): J = tf.map_fn(lambda m: tf.gradients(y[:,:,m:m+1], X)[0], tf.range(tf.shape(y)[-1]), tf.float32) J = tf.transpose(tf.squeeze(J), perm = [1,0,2]) return J ```\n\njvishnuvardhan commented\n\n@zackchase Is this resolved? Please close If it was resolved already. Thanks!\n\nagarwal-ashish commented\n\ntf.GradientTape.jacobian and tf.GradientTape.batch_jacobian APIs have been added for computing Jacobians. These are based on experimental pfor logic which can also be disabled to fallback to loop based implementation.\n\nagarwal-ashish closed this as completed\n\nppwwyyxx mentioned this issue\n\nHow to calculate gradients of a list of y with respect to a single x? tensorpack/tensorpack#1281\n\ntsbertalan mentioned this issue\n\n[TF 2.0 API Docs] tf.custom_gradient #26270\n\nekuznetsov139 pushed a commit to ekuznetsov139/tensorflow that referenced this issue\n\nMerge pull request tensorflow#675 from ROCmSoftwarePlatform/r1.15-roc… …\n\n…m-pr-620 Cherry-pick PR 620\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_4', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nGradients of non-scalars (higher rank Jacobians) #675\n\nzackchase opened this issue\n\nJan 4, 2016 · 68 comments\n\nGradients of non-scalars (higher rank Jacobians) #675\n\nzackchase opened this issue\n\nJan 4, 2016 · 68 comments\n\nCurrently if you call gradients(ys, xs), it will return the sum of dy/dx over all ys for each x in xs. I believe this doesn\'t accord with an a priori mathematical notion of the derivative of a vector. I\'d like the way to take the derivative of ys wrt xs where both are vectors and have a Jacobian matrix returned. By extension, I\'d like to take the derivative of a vector wrt a matrix and get back a 3-tensor. There doesn\'t seem to be a convenient tensorflow function to compute the Jacobian or higher order derivatives. Am I missing something or is this functionality that we could add?\n\nThe text was updated successfully, but these errors were encountered:\n\nzackchase@, you are right about the current gradients function. Currently, you can compute the Jacobian of, say, a vector, by calling gradients multiple times, one for every scalar component (obtained by slicing) of the original vector, and reassembling the results. Contributions are welcome to make this nicer and efficient.\n\nkeveman added the stat:contribution welcome\n\nStatus - Contributions welcome label\n\nIt\'d be pretty hard to support gradients of non-scalars with our current setup, since it would require every gradient function to handle extra rank input. The one possibility I could see would be if we add some sort of map facility to register how to add extra ranks to ops, then compute gradients with respect to extra rank by computing lower rank and calling the registered map transformations.\n\nSomeone asked for map a while back, so if anyone wanted to tackle this task that might be the way to go. Handling it at the gradient function level is probably bad, since it would add required complexity to an existing feature. Warning: This is a pretty large change, so a good deal of discussion would be in order before starting.\n\ngirving changed the title\n\nProblems Calculating the Jacobian\n\nGradients of non-scalars (higher rank Jacobians)\n\ngirving added the enhancement label\n\nHi Geoffrey, thanks for taking an interest in this issue. I was initially confused by the use of ""rank"" to describe the number of dimensions of the array. Should we avoid this name in the thread title and documentation to preempt confusion via overloading the linear algebra notion of rank?\n\nTensor rank is very standard terminology: http://mathworld.wolfram.com/TensorRank.html\n\nCool. The terminology gets funny when we talk about rank-R decompositions of tensors, meaning the tensor can be represented as a sum of R outer products of rank-1 tensors, but probably not a problem for us to solve here.\n\nOne thing I thought of is that I would like to compute the frobenius norm of the Jacobian of the log probabilities for use as a smoothness penalty much like the smoothness penalty used in a contractive autoencoder. In this case, as we only seek a scalar at the end, is there a more efficient method than separately calculating the derivative of each output with respect to the inputs?\n\nAre you saying your network has a bunch of outputs, and then you combine them into a single scalar that you are trying to optimize? In that case, you should differentiate with respect to that single scalar.\n\nNot exactly. I\'m saying if one wants to penalize the norm of the Jacobian of the mapping function. So optimization objective would be (pseudocode):\n\ncost(y,yhat, X) = loss(y,yhat) + norm(Jacobian(log(yhat), X))\n\nAh, sorry for not reading carefully. You\'re correct that (as far as I know) there\'s no easy way to do that in current tensorflow. According to someone more knowledgeable than I, people generally do such contractive autoencoders by writing out the first derivative manually. Also, they generally restrict to single layer at a time networks for speed issues, since doing the full Jacobian for a multilayer network is quite expensive.\n\nRegardless, it would be good to have a way to call derivatives of vectors and receive gradients of the expected shape.\n\nyaroslavvb commented\n\nDifferentiating with respect to one variable is similar to how it works in Theano. I agree it may be confusing when TensorFlow automatically turns many variables into one by taking the sum. An alternative would be to fail if there\'s more than 1 output variable specified, or have a wrapper that automatically calls existing gradient function on each output variable\n\nThe reason for ""one output variable at a time"" in TensorFlow (and Theano) is because we do reverse mode AD by default. In reverse AD you have a single target scalar quantity and you propagate sensitivities with respect to that quantity. In contrast, if you we did forward AD instead, we would naturally support multiple output variables, but only compute derivative with respect to one scalar variable at a time. Supporting mixed mode propagation to cover ""multiple inputs/multiple outputs"" case in the most efficient way could be a lot of extra plumbing.\n\nIf you have a small number of output variables but large number of input variables, standard thing to do is to apply reverse AD with respect to each variable in a loop. This is what Theano recommends to do for compute Hessian for instance: http://deeplearning.net/software/theano/tutorial/gradients.html#computing-the-hessian. If you have a small number of input variables but large number of output variables, then the most efficient thing to do would be to run forward-mode AD for all the input variables in a loop. Forward mode AD is not implemented and would require adding an equivalent of Theano\'s ""Rop"" operator to differentiable ops and some plumbing to call them instead of existing op ""gradient"" function (existing gradient function is an equivalent of Lop operation, or ""left multiply sensitivity vector by op\'s jacobian"" operation)\n\ntillahoffmann commented\n\nI was hoping to implement higher order derivatives using the map function but am getting an error message I can\'t quite get my head around. My implementation is (in pseudo code)\n\nparams = tf.Variable(""some initial value"") loss = some_function(params) grads = tf.gradients(loss, params)[0] hess = tf.map_fn(lambda grad: tf.gradients(grad, X)[0], grads)\n\nWhen I fetch the hessian, I get the error message\n\nInvalidArgumentError: All inputs to node map/while/gradients/map/TensorArrayUnpack_grad/TensorArrayGrad/TensorArrayGrad must be from the same frame.\n\nI assumed that tensorflow has an issue because it doesn\'t know about params in the loop (cf. non_sequences in theano scan), and extended map_fn to pass extra arguments to the loop. Unfortunately, the extra arguments get wrapped in an identity transformation and tf.gradients(params, tf.identity(params)) gives [None], which seems a bit unintuitive.\n\nLooping in python is of course fine but I\'d like to avoid introducing an extra node to the graph for every parameter. Any suggestions?\n\n@yuanbyu: Do you understand this issue with tf.map_fn?\n\ngirving added the triaged label\n\nNote for anyone who comes across this thread: tf.map_fn is an unrelated thing involving control flow, not something related to mapping over extra rank tensors.\n\ngirving mentioned this issue\n\nHessian (calling tf.gradients twice) of tf.scan fails #2598\n\naselle removed the triaged label\n\nWe don\'t support higher-order gradients for while_loop/map_fn/scan/fold. You should see an informative error message if you try to do that.\n\nyuanbyu closed this as completed\n\n@yaroslavvb Any plans on adding forward mode AD? I filed an issue on it a couple weeks ago but haven\'t heard back.\n\nyaroslavvb commented\n\n@vladfi1 I\'m no longer at Brain, so I wouldn\'t know. I would say it is unlikely to ever be part of core TensorFlow. There are >450 ops in TF, so Brain team would have to implement forward AD grad method for all of 450 ops and maintain them forever, or alternatively have to explain why someone\'s favorite op doesn\'t have forward AD support. It seems more realistic that someone would create a separately maintained library that does forward-AD, and utilizes TensorFlow as backend. Kind of like autograd but using TensorFlow instead of numpy as the backend.\n\ntillahoffmann mentioned this issue\n\nHessian with respect to one-dimensional tensors #5329\n\ncxfneo mentioned this issue\n\nIssues: self.action_grads = tf.gradients(self.out, self.action) pemami4911/deep-rl#3\n\naselle added type:feature\n\nFeature requests and removed enhancement labels\n\nIs tf.test.compute_gradient is some kind of function that we can get the Jacobian matrix (not as a tensor but as a numpy.ndarray) of a vector tensor y w.r.t. a vector tensor x?\n\nyaroslavvb commented\n\nFeb 20, 2017 via email\n\nThere are no built-in jacobians in TensorFlow, instead anything called \'grad\' or \'gradient\' computes Jacobian-vector product (also called LOp in theano), see https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation …\n\nOn Mon, Feb 20, 2017 at 4:04 AM, MYaooo ***@***.***> wrote: Is tf.test.compute_gradient is some kind of funtion that we can get the jacobian matrix (not as a tensor but as a numpy.ndarray) of a vector tensor y w.r.t. a vector tensor x? — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#675 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AABaHO7dLBLvhJH5KoSiV8WcySnfR5cnks5reYFMgaJpZM4G9_8_> .\n\n@yaroslavvb Thanks for your reply. I see why it\'s expensive to do that. Do you have any suggestions if I want to get the numerical results of the Jacobian with certain inputs x. The only not so expensive workaround I can think of is to apply perturbation on each dim of x to get the approximate results.\n\n@marcociccone in TF 1.4 seems to be working fine, if this is an alternative for you.\n\nThis is indeed a bug in TF. It\'s caused by taking the gradient(y, x) inside a while loop wrt such that the computation of y from x goes through a different while loop, something like:\n\nx = ... y = while_loop(..., [x]) z = while_loop(..., tf.gradients(y, x), [y])\n\nSo in @mholzel\'s script, it\'s from passing the outcome of one jacobian call to the other jacobian call. (BTW, thanks very much for the easy repro.)\n\nUnfortunately this is quite tricky to fix. I\'ll try to take another look at it tomorrow and see if I can come up with something.\n\nskye assigned skye and unassigned agarwal-ashish\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nThis is actually extremely tricky to fix. I\'m not sure how this was working in 1.4, was it definitely giving the right answer?\n\nThe fundamental problem is that the second jacobian call (i.e. the hessian) is calling tf.gradients() inside a while loop, and that backprop calculation must go through the while loop from the first jacobian call. TF computes while loop gradients using stacks to store intermediate loop values, so if you\'re doing that calculation multiple times via another loop, we\'d have to somehow re-use the stack values on each iteration. This is conceptually possible but would be a pretty big change. I can at least try to improve the error message though.\n\nI have never seen the nested call work, but I did not try 1.4.\n\nskye added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nfKunstner mentioned this issue\n\n[feature request] Simple and Efficient way to get gradients of each element of a sum pytorch/pytorch#7786\n\nJoshuaalbert commented\n\n@mholzel for the double jacobian, suppose you just map to tf.hessian instead of tf.gradient. Doesn\'t solve to arbitrary order but it does get the hessian of a tensor wrt variables.\n\nagarwal-ashish commented\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nHarshini-Gadige added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nHarshini-Gadige unassigned skye\n\ntensorflowbutler assigned poxvoculi\n\npoxvoculi assigned agarwal-ashish and unassigned poxvoculi\n\nagarwal-ashish assigned saxenasaurabh and unassigned agarwal-ashish\n\nyifannieudem mentioned this issue\n\nCannot calculate tf.gradients wrt embedding_matrix #23033\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\nDid anyone compare the performance of the new functionality and the previous solution posted by @mholzel here? I am finding the new jacobian included in gradients.py way slower than @mholzel solution...\n\nagarwal-ashish commented\n\nThere is now an experimental new approach to doing Jacobians here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/gradients.py#L28\n\nDid anyone compare the performance of the new functionality and the previous solution posted by @mholzel here? I am finding the new jacobian included in gradients.py way slower than @mholzel solution...\n\ngradients_test.py in the same file has some benchmarks that show that pfor based jacobian generally works much faster than a while loop based one. Can you share your benchmark ?\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nI threw together this function, it uses tf.map_fn and assumes a batch style input, but at least it works for batches. It\'s pretty high level but seems to adress some of the ideas if the conversation above:\n\ndef Jacobian(X, y): J = tf.map_fn(lambda m: tf.gradients(y[:,:,m:m+1], X)[0], tf.range(tf.shape(y)[-1]), tf.float32) J = tf.transpose(tf.squeeze(J), perm = [1,0,2]) return J ```\n\njvishnuvardhan commented\n\n@zackchase Is this resolved? Please close If it was resolved already. Thanks!\n\nagarwal-ashish commented\n\ntf.GradientTape.jacobian and tf.GradientTape.batch_jacobian APIs have been added for computing Jacobians. These are based on experimental pfor logic which can also be disabled to fallback to loop based implementation.\n\nagarwal-ashish closed this as completed\n\nppwwyyxx mentioned this issue\n\nHow to calculate gradients of a list of y with respect to a single x? tensorpack/tensorpack#1281\n\ntsbertalan mentioned this issue\n\n[TF 2.0 API Docs] tf.custom_gradient #26270\n\nekuznetsov139 pushed a commit to ekuznetsov139/tensorflow that referenced this issue\n\nMerge pull request tensorflow#675 from ROCmSoftwarePlatform/r1.15-roc… …\n\n…m-pr-620 Cherry-pick PR 620\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-28T12:44:36', 'title': 'Gradients of non-scalars (higher rank Jacobians) · Issue #675 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/675'}), Document(page_content='# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper functions for computing gradients.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\ndef fwd_gradient(func_or_y, x, input_gradients=None, use_gradient_tape=False,\n                 unconnected_gradients=None,\n                 name=None):\n  """"""Computes forward mode gradient.\n\n  Implementation based on suggestions in\n  [this thread](https://github.com/tensorflow/tensorflow/issues/19361).\n\n  TensorFlow computes gradients using the reverse mode automatic\n  differentiation which is suitable for typical machine learning situations\n  where one has a scalar loss function that one wants to differentiate with\n  respect to the parameters. In some cases, one needs to be able to compute\n  directional derivatives of non-scalar functions. Suppose F is a function from\n  R^n to R^m and let u be a fixed vector in R^n, w a fixed vector in R^m and\n  x a variable taking values in R^n. Let J(F) denote the jacobian matrix of\n  F of shape [m, n] (i.e. J(F)[i, j] = dF_i / dx_j). Then the default\n  gradients function in TF computes the expression\n  w^T.J(F) (i.e. Sum[w_i dF_i / dx_j, 1 <= i <= m]).\n\n  On the other hand, one also often needs to compute the directional derivative\n  J(F).u (i.e. Sum[u_j dF_i / dx_j, 1 <= j <= n]). Unfortunately, TensorFlow\n  has no native support for accumulating this. Providing first class support\n  for forward mode differentiation requires some significant changes in the core\n  architecture of TF (including writing a directional derivative for each\n  op).\n\n  The following function sidesteps this by using two passes of reverse mode\n  differentiation. Mathematically, the idea is simple. If F: R^n -> R^m, then\n  w^T.J(F) seen as a function of w is a function from R^m to R^n (because\n  w is in R^m, and w^T.J(F) is in R^n). Hence a reverse mode differentiation\n  with respect to w should produce J(F).u.\n\n  This function provides only a small subset of the flexibility of\n  the tf.gradients function. This may be extended in the future.\n\n  Following example demonstrates the usage and the difference between this\n  op and the standard `tf.gradients`\n  ```python\n    t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]\n    def fn(t):\n      return tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]\n    # Produces shape [3, 2] with values [[1, 1], [2, 4], [3, 12]]\n    fwd_grad_y = fwd_gradient(fn, t)\n    # Produces shape [2] with values [6, 17].\n    bck_grad_y = tf.gradients(y, t)[0]\n  ```\n\n  Args:\n    func_or_y: Either a `Tensor` connected to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    x: A `Tensor` with respect to which the gradient is to be computed.\n    input_gradients: A `Tensor` of the same shape as `x`. The direction along\n      which the directional derivative is to be computed.\n      Default value: `None` which maps to a ones-like `Tensor` of `x`.\n    use_gradient_tape: Optional Python bool. Whether to use gradient tape even\n      when eager mode is not turned on.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` of the same shape as `func(x)`.\n\n  Raises:\n    ValueError: If `func_or_y` is not a callable and the output is eagerly\n      executed or when the `tf.GradientTape` is used.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  with tf.name_scope(name or ""gradients""):\n    f = _prepare_func(func_or_y)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(x)\n      w = tf.ones_like(y)\n      g = tf.gradients(y, x, grad_ys=w,\n                       unconnected_gradients=unconnected_gradients)\n      return tf.gradients(g, w, grad_ys=input_gradients,\n                          unconnected_gradients=unconnected_gradients)[0]\n    if not callable(func_or_y):\n      raise ValueError(""`func_or_y` should be a callable in eager mode or when ""\n                       ""`tf.GradientTape` is used."")\n    with tf.GradientTape() as outer_tape:\n      with tf.GradientTape() as inner_tape:\n        inner_tape.watch(x)\n        y = f(x)\n      w = tf.ones_like(y)\n      outer_tape.watch(w)\n      g = inner_tape.gradient(y, x, output_gradients=w,\n                              unconnected_gradients=unconnected_gradients)\n    return outer_tape.gradient(g, w, output_gradients=input_gradients,\n                               unconnected_gradients=unconnected_gradients)\n\n\ndef gradients(func_or_y, xs, output_gradients=None, use_gradient_tape=False,\n              unconnected_gradients=None,\n              name=None):\n  """"""Computes the gradients of `func_or_y` wrt to `*xs`.\n\n  Args:\n   func_or_y: Either a `Tensor` connected to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n      Default value: `None` which maps to a ones-like `Tensor` of `ys`.\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` with the gradient of `y` wrt each of `xs` or a list of `Tensor`s\n    if `xs` is a list.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  f = _prepare_func(func_or_y)\n  with tf.name_scope(name or ""gradients""):\n    xs, is_xs_list_like = _prepare_args(xs)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(*xs)\n      grad = tf.gradients(y, xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    else:\n      if not callable(func_or_y):\n        raise ValueError(""`func_or_y` should be a callable in eager mode or ""\n                         ""when `tf.GradientTape` is used."")\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return grad\n    else:\n      return grad[0]\n\n\ndef value_and_gradient(f,\n                       xs,\n                       output_gradients=None,\n                       use_gradient_tape=False,\n                       unconnected_gradients=None,\n                       name=None):\n  """"""Computes `f(*xs)` and its gradients wrt to `*xs`.\n\n  Args:\n    f: Python `callable` to be differentiated. If `f` returns a scalar, this\n      scalar will be differentiated. If `f` returns a tensor or list of tensors,\n      by default a scalar will be computed by adding all their values to produce\n      a single scalar. If desired, the tensors can be elementwise multiplied by\n      the tensors passed as the `dy` keyword argument to the returned gradient\n      function.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., `\'value_and_gradient\'`).\n\n  Returns:\n    A tuple of two elements. The first one is a `Tensor` representing the value\n    of the function at `xs` and the second one is either a `Tensor` or a list of\n    `Tensor`s representing the gradient of `f(*xs)` wrt `xs`.\n    y: `y = f(*xs)`.\n    dydx: Gradient of `y` wrt each of `xs`.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  xs, is_xs_list_like = _prepare_args(xs)\n  with tf.name_scope(name or ""value_and_gradient""):\n    if tf.executing_eagerly() or use_gradient_tape:\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    else:\n      y = f(*xs)\n      grad = tf.gradients(ys=y, xs=xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return y, grad\n    else:\n      return y, grad[0]\n\n\ndef make_val_and_grad_fn(value_fn):\n  """"""Function decorator to compute both function value and gradient.\n\n  ```\n  @tff.math.make_val_and_grad_fn\n  def quadratic(x):\n    return tf.reduce_sum(scales * (x - minimum) ** 2, axis=-1)\n  ```\n\n  Turns `quadratic` into a function that accepts a point as a `Tensor` as input\n  and returns a tuple of two `Tensor`s with the value and the gradient of the\n  defined quadratic function evaluated at the input point.\n\n  This is useful for constructing functions to optimize with tff.math.optimizer\n  methods.\n\n  Args:\n    value_fn: A python function to decorate.\n\n  Returns:\n    The decorated function.\n  """"""\n  @functools.wraps(value_fn)\n  def val_and_grad(x):\n    return value_and_gradient(value_fn, x)\n\n  return val_and_grad\n\n\ndef _prepare_func(func_or_y):\n  """"""Creates a function out of the input callable or `Tensor`.""""""\n  if callable(func_or_y):\n    return func_or_y\n  else:\n    return lambda *args: func_or_y\n\n\ndef _prepare_args(xs):\n  """"""Converts `xs` to a list if necessary.""""""\n  if isinstance(xs, (list, tuple)):\n    return xs, True\n  else:\n    return [xs], False', metadata={'id': 'web-search_0', 'snippet': '# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper functions for computing gradients.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\ndef fwd_gradient(func_or_y, x, input_gradients=None, use_gradient_tape=False,\n                 unconnected_gradients=None,\n                 name=None):\n  """"""Computes forward mode gradient.\n\n  Implementation based on suggestions in\n  [this thread](https://github.com/tensorflow/tensorflow/issues/19361).\n\n  TensorFlow computes gradients using the reverse mode automatic\n  differentiation which is suitable for typical machine learning situations\n  where one has a scalar loss function that one wants to differentiate with\n  respect to the parameters. In some cases, one needs to be able to compute\n  directional derivatives of non-scalar functions. Suppose F is a function from\n  R^n to R^m and let u be a fixed vector in R^n, w a fixed vector in R^m and\n  x a variable taking values in R^n. Let J(F) denote the jacobian matrix of\n  F of shape [m, n] (i.e. J(F)[i, j] = dF_i / dx_j). Then the default\n  gradients function in TF computes the expression\n  w^T.J(F) (i.e. Sum[w_i dF_i / dx_j, 1 <= i <= m]).\n\n  On the other hand, one also often needs to compute the directional derivative\n  J(F).u (i.e. Sum[u_j dF_i / dx_j, 1 <= j <= n]). Unfortunately, TensorFlow\n  has no native support for accumulating this. Providing first class support\n  for forward mode differentiation requires some significant changes in the core\n  architecture of TF (including writing a directional derivative for each\n  op).\n\n  The following function sidesteps this by using two passes of reverse mode\n  differentiation. Mathematically, the idea is simple. If F: R^n -> R^m, then\n  w^T.J(F) seen as a function of w is a function from R^m to R^n (because\n  w is in R^m, and w^T.J(F) is in R^n). Hence a reverse mode differentiation\n  with respect to w should produce J(F).u.\n\n  This function provides only a small subset of the flexibility of\n  the tf.gradients function. This may be extended in the future.\n\n  Following example demonstrates the usage and the difference between this\n  op and the standard `tf.gradients`\n  ```python\n    t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]\n    def fn(t):\n      return tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]\n    # Produces shape [3, 2] with values [[1, 1], [2, 4], [3, 12]]\n    fwd_grad_y = fwd_gradient(fn, t)\n    # Produces shape [2] with values [6, 17].\n    bck_grad_y = tf.gradients(y, t)[0]\n  ```\n\n  Args:\n    func_or_y: Either a `Tensor` connected to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    x: A `Tensor` with respect to which the gradient is to be computed.\n    input_gradients: A `Tensor` of the same shape as `x`. The direction along\n      which the directional derivative is to be computed.\n      Default value: `None` which maps to a ones-like `Tensor` of `x`.\n    use_gradient_tape: Optional Python bool. Whether to use gradient tape even\n      when eager mode is not turned on.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` of the same shape as `func(x)`.\n\n  Raises:\n    ValueError: If `func_or_y` is not a callable and the output is eagerly\n      executed or when the `tf.GradientTape` is used.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  with tf.name_scope(name or ""gradients""):\n    f = _prepare_func(func_or_y)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(x)\n      w = tf.ones_like(y)\n      g = tf.gradients(y, x, grad_ys=w,\n                       unconnected_gradients=unconnected_gradients)\n      return tf.gradients(g, w, grad_ys=input_gradients,\n                          unconnected_gradients=unconnected_gradients)[0]\n    if not callable(func_or_y):\n      raise ValueError(""`func_or_y` should be a callable in eager mode or when ""\n                       ""`tf.GradientTape` is used."")\n    with tf.GradientTape() as outer_tape:\n      with tf.GradientTape() as inner_tape:\n        inner_tape.watch(x)\n        y = f(x)\n      w = tf.ones_like(y)\n      outer_tape.watch(w)\n      g = inner_tape.gradient(y, x, output_gradients=w,\n                              unconnected_gradients=unconnected_gradients)\n    return outer_tape.gradient(g, w, output_gradients=input_gradients,\n                               unconnected_gradients=unconnected_gradients)\n\n\ndef gradients(func_or_y, xs, output_gradients=None, use_gradient_tape=False,\n              unconnected_gradients=None,\n              name=None):\n  """"""Computes the gradients of `func_or_y` wrt to `*xs`.\n\n  Args:\n   func_or_y: Either a `Tensor` connected to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n      Default value: `None` which maps to a ones-like `Tensor` of `ys`.\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` with the gradient of `y` wrt each of `xs` or a list of `Tensor`s\n    if `xs` is a list.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  f = _prepare_func(func_or_y)\n  with tf.name_scope(name or ""gradients""):\n    xs, is_xs_list_like = _prepare_args(xs)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(*xs)\n      grad = tf.gradients(y, xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    else:\n      if not callable(func_or_y):\n        raise ValueError(""`func_or_y` should be a callable in eager mode or ""\n                         ""when `tf.GradientTape` is used."")\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return grad\n    else:\n      return grad[0]\n\n\ndef value_and_gradient(f,\n                       xs,\n                       output_gradients=None,\n                       use_gradient_tape=False,\n                       unconnected_gradients=None,\n                       name=None):\n  """"""Computes `f(*xs)` and its gradients wrt to `*xs`.\n\n  Args:\n    f: Python `callable` to be differentiated. If `f` returns a scalar, this\n      scalar will be differentiated. If `f` returns a tensor or list of tensors,\n      by default a scalar will be computed by adding all their values to produce\n      a single scalar. If desired, the tensors can be elementwise multiplied by\n      the tensors passed as the `dy` keyword argument to the returned gradient\n      function.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., `\'value_and_gradient\'`).\n\n  Returns:\n    A tuple of two elements. The first one is a `Tensor` representing the value\n    of the function at `xs` and the second one is either a `Tensor` or a list of\n    `Tensor`s representing the gradient of `f(*xs)` wrt `xs`.\n    y: `y = f(*xs)`.\n    dydx: Gradient of `y` wrt each of `xs`.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  xs, is_xs_list_like = _prepare_args(xs)\n  with tf.name_scope(name or ""value_and_gradient""):\n    if tf.executing_eagerly() or use_gradient_tape:\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    else:\n      y = f(*xs)\n      grad = tf.gradients(ys=y, xs=xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return y, grad\n    else:\n      return y, grad[0]\n\n\ndef make_val_and_grad_fn(value_fn):\n  """"""Function decorator to compute both function value and gradient.\n\n  ```\n  @tff.math.make_val_and_grad_fn\n  def quadratic(x):\n    return tf.reduce_sum(scales * (x - minimum) ** 2, axis=-1)\n  ```\n\n  Turns `quadratic` into a function that accepts a point as a `Tensor` as input\n  and returns a tuple of two `Tensor`s with the value and the gradient of the\n  defined quadratic function evaluated at the input point.\n\n  This is useful for constructing functions to optimize with tff.math.optimizer\n  methods.\n\n  Args:\n    value_fn: A python function to decorate.\n\n  Returns:\n    The decorated function.\n  """"""\n  @functools.wraps(value_fn)\n  def val_and_grad(x):\n    return value_and_gradient(value_fn, x)\n\n  return val_and_grad\n\n\ndef _prepare_func(func_or_y):\n  """"""Creates a function out of the input callable or `Tensor`.""""""\n  if callable(func_or_y):\n    return func_or_y\n  else:\n    return lambda *args: func_or_y\n\n\ndef _prepare_args(xs):\n  """"""Converts `xs` to a list if necessary.""""""\n  if isinstance(xs, (list, tuple)):\n    return xs, True\n  else:\n    return [xs], False', 'timestamp': '2024-04-17T05:05:06', 'title': 'tf-quant-finance/tf_quant_finance/math/gradient.py at master · google/tf-quant-finance', 'url': 'https://github.com/google/tf-quant-finance/blob/master/tf_quant_finance/math/gradient.py'}), Document(page_content='Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nrenmengye / tensorflow-forward-ad Public\n\ngenerating forward-mode graphs by calling tf.gradients twice #2\n\nmattjj opened this issue\n\nJun 8, 2017 · 18 comments\n\ngenerating forward-mode graphs by calling tf.gradients twice #2\n\nmattjj opened this issue\n\nJun 8, 2017 · 18 comments\n\nBased on some autograd code from @j-towns, @alextp and I recently had this idea for getting forward-mode JVPs in TensorFlow by calling tf.gradients twice:\n\nimport numpy as np import numpy.random as npr import tensorflow as tf def fwd_gradients(ys, xs, d_xs): """"""Forward-mode pushforward analogous to the pullback defined by tf.gradients. With tf.gradients, grad_ys is the vector being pulled back, and here d_xs is the vector being pushed forward."""""" v = tf.placeholder(ys.dtype, shape=ys.get_shape()) # dummy variable g = tf.gradients(ys, xs, grad_ys=v) return tf.gradients(g, v, grad_ys=d_xs) A = tf.constant(npr.randn(5, 3), dtype=tf.float32) x = tf.placeholder(tf.float32, [1, 5]) y = tf.tanh(tf.matmul(x, A)) u = tf.placeholder(tf.float32, [1, 5]) jvp = fwd_gradients(y, x, u) x_val = npr.randn(1, 5) u_val = npr.randn(1, 5) init_op = tf.initialize_all_variables() with tf.Session() as sess: sess.run(init_op) print sess.run(jvp, feed_dict={x: x_val, u: u_val})\n\nThe idea is that when a is a linear function of b, calling tf.gradients(a, b, u) applies the adjoint of that linear function to u. The same idea lets us get the adjoint of a vector-Jacobian product operator produced by tf.gradients applied to a nonlinear function, yielding a Jacobian-vector product operator for that nonlinear function. Notice that there\'s extra work being done ahead-of-time (i.e. graph construction time), but not at runtime (i.e. graph evaluation time), since the extra nodes being created by the first call to tf.gradients aren\'t needed for evaluating the JVP graph that results.\n\nWe haven\'t worked out all the details yet (or even tested it beyond the above script) so there could be some subtleties here that I\'m missing, but I think in principle the TF graph that gets built by the fwd_gradients function above should be about as FLOP- and memory-efficient as possible. That statement has a lot of uncertainty on it at the moment, though!\n\nWe just wanted to give you a heads-up about this idea, and also to see if you had any thoughts.\n\nThe text was updated successfully, but these errors were encountered:\n\nThanks a lot for the post. I like your idea, which is way more elegant, in terms of using the built-in reverse-mode AD in TensorFlow (although from implementation perspective, forward-mode should be a lot easier than reverse-mode). I have tested your idea in my code base, it passes all the unit tests, except this one, which I reported as an issue to TF: tensorflow/tensorflow#9368.\n\nJun 8, 2017 via email\n\nNice! Are you interested in contributing the fix to sparse_softmax_cross_entropy_with_logits mentioned in that bug? …\n\nOn Thu, Jun 8, 2017 at 7:24 AM, Mengye Ren ***@***.***> wrote: Thanks a lot for the post. I like your idea, which is way more elegant, in terms of using the built-in reverse-mode AD in TensorFlow (although from implementation perspective, forward-mode should be a lot easier than reverse-mode). I have tested your idea in my code base, it passes all the unit tests, except this one, which I reported as an issue to TF: tensorflow/tensorflow#9368 <tensorflow/tensorflow#9368>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#2 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxX4BjKbIm5IwluNFC_f5U9d3qHHBks5sCAQKgaJpZM4NzmrK> .\n\nI can take a look, but this one of the core module of tensorflow, so better be changed by some of their core developers. In the mean time, you can use other versions of cross entropy which should be fine.\n\nMattShannon commented\n\nThis is a really neat trick! I believe this can be as time and memory efficient at runtime as an explicit implementation of the forward-mode computation, but it depends on the details of how the gradient of the gradient of each op involved is implemented. Mathematically a certain portion (grad_v below) of the gradient of the gradient always gives the desired forward-mode computation. However it is possible to implement the gradient of the gradient in a way that would result in extra computation, e.g. if the gradient of the gradient was written as a single monolithic op rather than decomposed into multiple ops.\n\nConsider the simple case of an op f with one input and one output, say y = f(x). Then grad_x = g(grad_y, x), where g would often consist of a single op, and that\'s reasonable. Let\'s change variable names and write u = g(v, x). Here g is linear in v. Then (grad_v, grad_x_more) = h(grad_u, v, x). Mathematically the first component grad_v of the output will be the desired forward-mode output when grad_u is the forward-mode input (and will not depend on v). However if h is implemented as a single op which computes both grad_v and grad_x_more, or several ops but where the parent ops of grad_v include calculations not required to compute grad_v, then extra computation will be performed. I have no idea how prevalent one case is versus the other for gradient of gradient ops in current tensorflow.\n\nUsing tf.placeholder as @mattjj suggested seems to offer some protection against the case where unnecessary computation is done, since it seems unlikely there are many cases where grad_x_more doesn\'t depend on v. If grad_x_more or similar is inadvertently being computed then the attempt to access v will result in a ""placeholder value must be specified"" error at runtime.\n\nMattShannon commented\n\nIndeed grad_x_more is linear in v, and the only way grad_x_more can be independent of v for all grad_u and x is if the second derivative of the original function f vanishes everywhere, i.e. f is a linear-affine function of x, i.e. f(x) = A x + b for some fixed A and b (not inputs to the op f). This could conceivably be true for some ops (e.g. an FFT op), but these cases seem likely to be few and far between, and even in these cases the gradient of the gradient may still be implemented in an efficient way.\n\nSo it seems like using tf.placeholder means it\'s pretty likely that any unnecessary computation in the forward-mode computation will result in a ""placeholder value must be specified"" error.\n\nreturn tf.gradients(g, v, grad_ys=d_xs)\n\nin @mattjj\'s code bove, you can see that we tell TF to take the grad of g w.r.t. v. I think the grad w.r.t. x won\'t be computed — if TF always took gradients with respect to every parent node in a graph that could be very inefficient.\n\nSo it seems like using tf.placeholder means it\'s pretty likely that any unnecessary computation in the forward-mode computation will result in a ""placeholder value must be specified"" error.\n\nThere may be cases where this would happen, although to my mind that seems unlikely. TF should know that g is linear as a function of v and sever any connections in the graph. This has also been discussed on Theano/Theano#6035 and HIPS/autograd#175 (comment).\n\nFor more detail see my blog post: https://j-towns.github.io/2017/06/12/A-new-trick.html.\n\nhughsalimbeni commented\n\nThanks for sharing this great trick!\n\nRegarding @j-towns\'s comment above, I have come across a case where tf complains about v being specified, but the value appears not to make a difference when I pass it in. The problem arises in using the cholesky (NB tensorflow/tensorflow@473a590 is required for this)\n\nfloat_type = tf.float64 float_type_np = np.float64 def fwd_gradients_with_v0(ys, xs, d_xs, v0): v = tf.placeholder_with_default(v0, shape=ys.get_shape()) g = tf.gradients(ys, xs, grad_ys=v) return tf.gradients(g, v, grad_ys=d_xs) def test_equivalence(f_tf, f_np, X_val, U_val): X = tf.placeholder(float_type, X_val.shape) U = tf.placeholder(float_type, U_val.shape) Y = f_tf(X) Y_val = f_np(X_val) v_zeros = np.zeros_like(Y_val, dtype=float_type_np) v_rand = np.array(npr.randn(*Y_val.shape), dtype=float_type_np) a_np = jacobian(f_np)(X_val) @ U_val # slow way with tf.Session() as sess: assert np.allclose(sess.run(Y, {X: X_val}), Y_val) # check np/tf agree a_tf_zeros, = sess.run(fwd_gradients_with_v0(Y, X, U, v_zeros), feed_dict={X: X_val, U: U_val}) a_tf_rand, = sess.run(fwd_gradients_with_v0(Y, X, U, v_rand), feed_dict={X: X_val, U: U_val}) print(\'zeros: {}\'.format(np.max(np.abs(a_np - a_tf_zeros)))) print(\'rand: {}\'.format(np.max(np.abs(a_np - a_tf_rand)))) try: a_tf, = sess.run(fwd_gradients(Y, X, U), feed_dict={X: X_val, U: U_val}) print(\'dummy: {}\'.format(np.max(np.abs(a_np - a_tf)))) except tf.errors.InvalidArgumentError: print(\'dummy method failed\')\n\nWith e.g. matrix inverse this all works fine:\n\nN = 3 L = npr.randn(N, N) X_val = (L @ L.T).flatten() U_val = npr.randn(N**2) f_tf = lambda X: tf.reshape(tf.matrix_inverse(tf.reshape(X, [N, N])), [N**2, ]) f_np = lambda X: np.linalg.inv(X.reshape(N, N)).flatten() test_equivalence(f_tf, f_np, X_val, U_val)\n\nzeros: 8.387360139749944e-09 rand: 8.387360139749944e-09 dummy: 8.387360139749944e-09\n\nBut with cholesky the dummy method fails:\n\nf_tf = lambda X: tf.reshape(tf.cholesky(tf.reshape(X, [N, N])), [N**2, ]) f_np = lambda X: np.linalg.cholesky(X.reshape(N, N)).flatten() test_equivalence(f_tf, f_np, X_val, U_val)\n\nzeros: 1.5365486660812167e-12 rand: 1.5365486660812167e-12 dummy method failed\n\nbut the other two methods give the same answer. Is something fishy going on, or is this all fine?\n\nEDIT: reading @MattShannon \'s comment further up I realise this is probably to do with the way cholesky grad is implemented, and that there is likely some redundant computation, but it shouldn’t effect the result. My mind is at rest. Annoying to have to pass v0, though...\n\n@hughsalimbeni I also met this problem where TF complains about placeholders when I have a list of variables for u - using tf.ones_like(ys) for v seems to work for me.\n\nbotev mentioned this issue\n\nGraph optimization and other features tensorflow/tensorflow#3610\n\nlawsonfulton commented\n\nI believe I have identified a bug where this trick does not work for the tf.nn.elu activation. Computing the jacobian-vector product at some x < 0 gives the incorrect result only for the ELU operation.\n\nConsider the following minimal example which computes the Jacobin numerically, with the vjp, and the jvp. Note that if you replace tf.nn.elu with my_relu, the results are correct.\n\nIf someone knows of a workaround to make this work for the built-in ELU I would greatly appreciate it, since using the my_elu function incurs a 2-3x reduction in performance.\n\nimport tensorflow as tf import numpy as np def fwd_gradients(ys, xs, d_xs): dummy = tf.zeros_like(ys) g = tf.gradients(ys, xs, grad_ys=dummy, name=""gradients"") return tf.gradients(g, dummy, grad_ys=d_xs, name=""jvp"") def my_elu(x): return tf.where(x >= 0.0, x, tf.exp(x) - 1.0) def main(): print(tf.__version__) sess = tf.InteractiveSession() init = tf.global_variables_initializer() activation = tf.nn.elu # Works correctly tf.nn.relu (or any other non-elu activation) x_size = 1 y_size = 1 # Single ELU or RELU op X = tf.placeholder(tf.float64, shape=[x_size]) # Input Y = activation(X) # Output # Define vjp and jvp Vx = tf.placeholder(tf.float64, shape=[x_size]) # Jac-vector product input V Vy = tf.placeholder(tf.float64, shape=[y_size]) # vector-jac product input V jvp = fwd_gradients(Y, X, d_xs=Vx) vjp = tf.gradients(Y, X, grad_ys=Vy) # Compute jacobians x = np.ones(x_size) - 1.5 # Bug only occurs in x < 0 region tf_jac, numeric_jac = tf.test.compute_gradient(X, [x_size], Y, [y_size], x_init_value=x) vjp_jac = np.array([sess.run(vjp, feed_dict={X: x, Vy: v})[0] for v in np.identity(y_size)]) jvp_jac = np.array([sess.run(jvp, feed_dict={X: x, Vx: v})[0] for v in np.identity(x_size)]) # Print results as maximum absolute error print(""Numeric jac:"", numeric_jac) print(""jvp jac:"", jvp_jac) print(""tf error:"", np.max(np.abs(numeric_jac - tf_jac))) # ~0.0 print(""vjp error:"", np.max(np.abs(numeric_jac - vjp_jac))) # ~0.0 print(""jvp error:"", np.max(np.abs(numeric_jac - jvp_jac))) # LARGE! for ELU sess.close() if __name__ == \'__main__\': main()\n\nTested on tensorflow 1.4.0, 1.8.0, and latest nightly 1.9.0-dev20180515\n\n@teaghan I guess if the inputs is 100-dim and output is 10-dim, it maybe more efficient just use back-propagation? Since for forward-mode, each input dimension needs a separate differentiation path.\n\n@renmengye by back-propagation do you mean just use the normal tf.gradients(y, x)? I have been doing this, but I was hoping for a faster method as this is quite expensive with a large model and a large number of inputs and outputs (the example shown is a simplification). Looking at @zero-impact\'s example, would running jvp_jac = np.array([sess.run(jvp, feed_dict={X: x, Vx: v})[0] for v in np.identity(x_size)[x_indices]) make any sense? where x_indices are my features of interest for the inputs.\n\nlawsonfulton mentioned this issue\n\nBug in EluGradGrad tensorflow/tensorflow#19333\n\n@teaghan If you know which input feature dimension you are interested in, then it makes sense to use forward mode. The u_val is the gradients towards the inputs. If you know which scalar of input is to take gradients, then you can leave u_val to be None. This is similar to, when you do backpropagation, the optional gradients to be fed on the output.\n\nmattjj mentioned this issue\n\nFeature request: forwards autodifferentiation for gradient computation tensorflow/tensorflow#19361\n\nrefraction-ray commented\n\nI believe there are many function primitives that the implementation using placeholder for v would fail. For example, by using the exactly the same code as shown by @mattjj with tf1.13, many functions fails complaining placeholder not specified. Pass cases include tanh (the original post), exp, sigmoid, pow while failing cases include tan, sin, cos, cosh, sinh, log and many more such as nn.elu as mentioned above. Therefore, the implementation using placeholder is not very robust unless you can make sure no primitive fails in your codebase.\n\nOf course, one can specify something like v=tf.ones_like(ys) and get the correct result for the gradients. But I guess in such cases, the two rounds of reverse AD are actually executed by tensorflow and thus nothing gains. I guess the reason that placeholder approach fails is because the linearity with v is only guaranteed in theory. In practice, Jacobian product is a complicated function and possibly for some of the function primitives, the expected linearity with v are disguised by their backprop function leaving tensorflow failing identifing the linearity between g and v and thus failing in optimizing the final graph.\n\ncooijmanstim mentioned this issue\n\nForward-mode-by-double-backprop fails on tf.square ops tensorflow/tensorflow#34554\n\njaweriaamjad commented\n\nWhy did @mattjj pass <npr.randn(1, 5)> for <grad_ys> in the in this function? As per tensorflow documentation ""grad_ys is a list of tensors of the same length as ys that holds the initial gradients for each y in ys. When grad_ys is None, we fill in a tensor of \'1\'s of the shape of y for each y in ys. A user can provide their own initial grad_ys to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).""\n\nCorrect me if I am wrong but I believe this means that if we pass a random vector instead of all 1s for <grad_ys>, it acts as a weighting function. I checked on a small example and it indeed is true:\n\ntf.reset_default_graph() A = tf.constant(npr.randn(5, 3), dtype=tf.float32) B = tf.constant(npr.randn(3, 3), dtype=tf.float32) x = tf.placeholder(tf.float32, [1, 5]) y = tf.nn.relu(tf.matmul(tf.tanh(tf.matmul(x, A)),B)) u = tf.placeholder(tf.float32, [1, 5]) jvp = fwd_gradients(y, x, u) x_val = npr.randn(1, 5) u_val = np.ones((1, 5)) init_op = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init_op) jac_man = (sess.run(jvp, feed_dict={x: x_val, u: u_val})) jac_jvp = (sess.run(tf.reduce_sum(jacobian(y, x),axis=2), feed_dict={x: x_val})) print(\'------------------------------------\') print(\'forward jacobian calculated manually is: {}\'.format(jac_man)) print(\'forward jacobian calculated using power iteration method is: {}\'.format(jac_jvp.T))\n\nHere jacobian(y,x) is a function that evaluates the jacobian matrix of a given function. In the code above, if I pass npr.randn(1,5) instead of np.ones((1,5)) for u, I get incorrect values for forward gradients.\n\n(although from implementation perspective, forward-mode should be a lot easier than reverse-mode)\n\nOnly because people have been thinking about AD in an awkward way (in terms of graphs and “tensors”), including seeing reverse mode as involving a forward pass, reverse pass, mutation, “tapes” etc. The simple essence of automatic differentiation shows that reverse mode AD is not only as simple as forward mode but that they’re both special cases of the same simple, general algorithm.\n\ncooijmanstim commented\n\n@jaweriaamjad it depends on what you want to compute. tf.gradients computes vector-Jacobian products u^T (dy/dx), and this trick computes Jacobian-vector products (dy/dx) u. The former is ""how does a change in x affect y in the direction of u"", the latter is ""how does a change in x in the direction of u affect y?"". You can set u to all ones, but there is nothing correct about that particular value. It just gets you the sum of the columns of the Jacobian.\n\nThe JVP/VJP parlance wasn\'t clear to me initially, but I think it\'s because we\'re so used to the typical use case of reverse-differentiating a scalar output. For example if y is a batch of losses, we tend to average it and then differentiate: tf.gradients(tf.reduce_mean(y), x). But you could equally well differentiate the batched losses and pass a uniform weighting vector for each of the losses, e.g. tf.gradients(y, x, grad_ys=tf.ones_like(y) / tf.size(y)). This is a more general view because it also applies the other way around (i.e. fwd_gradients above with the equivalent of a grad_xs argument).\n\njaweriaamjad commented\n\n@cooijmanstim so if I understand correctly, we will indeed pass all ones for <u> if we wish to calculate the sum of the columns of jacobian of loss functions during training (since usually we don\'t want a weighted sum). The purpose of <grad_ys> is to give us the freedom to calculate the weighted sum.\n\ncooijmanstim commented\n\n@jaweriaamjad right, but to be clear, summing the columns means summing across x, not across y. It will not correspond to summing losses but to summing parameters. The shape of the output will be the same as that of y. To sum across y you want to use reverse-mode.\n\nAlso, weighted sum is a particular case. More generally, grad_ys is used to implement the chain rule. Consider the ith layer of a feedforward neural net computes output h[i] from h[i-1], and further down the line this results in some loss L. Then if you have dL/dh[i] (call it dL_dh[i] to get a valid Python identifier), you can recursively go back, computing the gradient dL/dh[i-1] by dL_dh[i-1] = tf.gradients(h[i], h[i-1], grad_ys=dL_dh[i]). This is backprop.\n\nnish-ant mentioned this issue\n\nWhat exactly are dummy variables and fwd_gradients methods doing in discrete inference of Burgers Eq? maziarraissi/PINNs#25\n\nProGamerGov mentioned this issue\n\nOptim-wip: Add Activation Atlas tutorial & functions pytorch/captum#579\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_1', 'snippet': 'Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nrenmengye / tensorflow-forward-ad Public\n\ngenerating forward-mode graphs by calling tf.gradients twice #2\n\nmattjj opened this issue\n\nJun 8, 2017 · 18 comments\n\ngenerating forward-mode graphs by calling tf.gradients twice #2\n\nmattjj opened this issue\n\nJun 8, 2017 · 18 comments\n\nBased on some autograd code from @j-towns, @alextp and I recently had this idea for getting forward-mode JVPs in TensorFlow by calling tf.gradients twice:\n\nimport numpy as np import numpy.random as npr import tensorflow as tf def fwd_gradients(ys, xs, d_xs): """"""Forward-mode pushforward analogous to the pullback defined by tf.gradients. With tf.gradients, grad_ys is the vector being pulled back, and here d_xs is the vector being pushed forward."""""" v = tf.placeholder(ys.dtype, shape=ys.get_shape()) # dummy variable g = tf.gradients(ys, xs, grad_ys=v) return tf.gradients(g, v, grad_ys=d_xs) A = tf.constant(npr.randn(5, 3), dtype=tf.float32) x = tf.placeholder(tf.float32, [1, 5]) y = tf.tanh(tf.matmul(x, A)) u = tf.placeholder(tf.float32, [1, 5]) jvp = fwd_gradients(y, x, u) x_val = npr.randn(1, 5) u_val = npr.randn(1, 5) init_op = tf.initialize_all_variables() with tf.Session() as sess: sess.run(init_op) print sess.run(jvp, feed_dict={x: x_val, u: u_val})\n\nThe idea is that when a is a linear function of b, calling tf.gradients(a, b, u) applies the adjoint of that linear function to u. The same idea lets us get the adjoint of a vector-Jacobian product operator produced by tf.gradients applied to a nonlinear function, yielding a Jacobian-vector product operator for that nonlinear function. Notice that there\'s extra work being done ahead-of-time (i.e. graph construction time), but not at runtime (i.e. graph evaluation time), since the extra nodes being created by the first call to tf.gradients aren\'t needed for evaluating the JVP graph that results.\n\nWe haven\'t worked out all the details yet (or even tested it beyond the above script) so there could be some subtleties here that I\'m missing, but I think in principle the TF graph that gets built by the fwd_gradients function above should be about as FLOP- and memory-efficient as possible. That statement has a lot of uncertainty on it at the moment, though!\n\nWe just wanted to give you a heads-up about this idea, and also to see if you had any thoughts.\n\nThe text was updated successfully, but these errors were encountered:\n\nThanks a lot for the post. I like your idea, which is way more elegant, in terms of using the built-in reverse-mode AD in TensorFlow (although from implementation perspective, forward-mode should be a lot easier than reverse-mode). I have tested your idea in my code base, it passes all the unit tests, except this one, which I reported as an issue to TF: tensorflow/tensorflow#9368.\n\nJun 8, 2017 via email\n\nNice! Are you interested in contributing the fix to sparse_softmax_cross_entropy_with_logits mentioned in that bug? …\n\nOn Thu, Jun 8, 2017 at 7:24 AM, Mengye Ren ***@***.***> wrote: Thanks a lot for the post. I like your idea, which is way more elegant, in terms of using the built-in reverse-mode AD in TensorFlow (although from implementation perspective, forward-mode should be a lot easier than reverse-mode). I have tested your idea in my code base, it passes all the unit tests, except this one, which I reported as an issue to TF: tensorflow/tensorflow#9368 <tensorflow/tensorflow#9368>. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#2 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxX4BjKbIm5IwluNFC_f5U9d3qHHBks5sCAQKgaJpZM4NzmrK> .\n\nI can take a look, but this one of the core module of tensorflow, so better be changed by some of their core developers. In the mean time, you can use other versions of cross entropy which should be fine.\n\nMattShannon commented\n\nThis is a really neat trick! I believe this can be as time and memory efficient at runtime as an explicit implementation of the forward-mode computation, but it depends on the details of how the gradient of the gradient of each op involved is implemented. Mathematically a certain portion (grad_v below) of the gradient of the gradient always gives the desired forward-mode computation. However it is possible to implement the gradient of the gradient in a way that would result in extra computation, e.g. if the gradient of the gradient was written as a single monolithic op rather than decomposed into multiple ops.\n\nConsider the simple case of an op f with one input and one output, say y = f(x). Then grad_x = g(grad_y, x), where g would often consist of a single op, and that\'s reasonable. Let\'s change variable names and write u = g(v, x). Here g is linear in v. Then (grad_v, grad_x_more) = h(grad_u, v, x). Mathematically the first component grad_v of the output will be the desired forward-mode output when grad_u is the forward-mode input (and will not depend on v). However if h is implemented as a single op which computes both grad_v and grad_x_more, or several ops but where the parent ops of grad_v include calculations not required to compute grad_v, then extra computation will be performed. I have no idea how prevalent one case is versus the other for gradient of gradient ops in current tensorflow.\n\nUsing tf.placeholder as @mattjj suggested seems to offer some protection against the case where unnecessary computation is done, since it seems unlikely there are many cases where grad_x_more doesn\'t depend on v. If grad_x_more or similar is inadvertently being computed then the attempt to access v will result in a ""placeholder value must be specified"" error at runtime.\n\nMattShannon commented\n\nIndeed grad_x_more is linear in v, and the only way grad_x_more can be independent of v for all grad_u and x is if the second derivative of the original function f vanishes everywhere, i.e. f is a linear-affine function of x, i.e. f(x) = A x + b for some fixed A and b (not inputs to the op f). This could conceivably be true for some ops (e.g. an FFT op), but these cases seem likely to be few and far between, and even in these cases the gradient of the gradient may still be implemented in an efficient way.\n\nSo it seems like using tf.placeholder means it\'s pretty likely that any unnecessary computation in the forward-mode computation will result in a ""placeholder value must be specified"" error.\n\nreturn tf.gradients(g, v, grad_ys=d_xs)\n\nin @mattjj\'s code bove, you can see that we tell TF to take the grad of g w.r.t. v. I think the grad w.r.t. x won\'t be computed — if TF always took gradients with respect to every parent node in a graph that could be very inefficient.\n\nSo it seems like using tf.placeholder means it\'s pretty likely that any unnecessary computation in the forward-mode computation will result in a ""placeholder value must be specified"" error.\n\nThere may be cases where this would happen, although to my mind that seems unlikely. TF should know that g is linear as a function of v and sever any connections in the graph. This has also been discussed on Theano/Theano#6035 and HIPS/autograd#175 (comment).\n\nFor more detail see my blog post: https://j-towns.github.io/2017/06/12/A-new-trick.html.\n\nhughsalimbeni commented\n\nThanks for sharing this great trick!\n\nRegarding @j-towns\'s comment above, I have come across a case where tf complains about v being specified, but the value appears not to make a difference when I pass it in. The problem arises in using the cholesky (NB tensorflow/tensorflow@473a590 is required for this)\n\nfloat_type = tf.float64 float_type_np = np.float64 def fwd_gradients_with_v0(ys, xs, d_xs, v0): v = tf.placeholder_with_default(v0, shape=ys.get_shape()) g = tf.gradients(ys, xs, grad_ys=v) return tf.gradients(g, v, grad_ys=d_xs) def test_equivalence(f_tf, f_np, X_val, U_val): X = tf.placeholder(float_type, X_val.shape) U = tf.placeholder(float_type, U_val.shape) Y = f_tf(X) Y_val = f_np(X_val) v_zeros = np.zeros_like(Y_val, dtype=float_type_np) v_rand = np.array(npr.randn(*Y_val.shape), dtype=float_type_np) a_np = jacobian(f_np)(X_val) @ U_val # slow way with tf.Session() as sess: assert np.allclose(sess.run(Y, {X: X_val}), Y_val) # check np/tf agree a_tf_zeros, = sess.run(fwd_gradients_with_v0(Y, X, U, v_zeros), feed_dict={X: X_val, U: U_val}) a_tf_rand, = sess.run(fwd_gradients_with_v0(Y, X, U, v_rand), feed_dict={X: X_val, U: U_val}) print(\'zeros: {}\'.format(np.max(np.abs(a_np - a_tf_zeros)))) print(\'rand: {}\'.format(np.max(np.abs(a_np - a_tf_rand)))) try: a_tf, = sess.run(fwd_gradients(Y, X, U), feed_dict={X: X_val, U: U_val}) print(\'dummy: {}\'.format(np.max(np.abs(a_np - a_tf)))) except tf.errors.InvalidArgumentError: print(\'dummy method failed\')\n\nWith e.g. matrix inverse this all works fine:\n\nN = 3 L = npr.randn(N, N) X_val = (L @ L.T).flatten() U_val = npr.randn(N**2) f_tf = lambda X: tf.reshape(tf.matrix_inverse(tf.reshape(X, [N, N])), [N**2, ]) f_np = lambda X: np.linalg.inv(X.reshape(N, N)).flatten() test_equivalence(f_tf, f_np, X_val, U_val)\n\nzeros: 8.387360139749944e-09 rand: 8.387360139749944e-09 dummy: 8.387360139749944e-09\n\nBut with cholesky the dummy method fails:\n\nf_tf = lambda X: tf.reshape(tf.cholesky(tf.reshape(X, [N, N])), [N**2, ]) f_np = lambda X: np.linalg.cholesky(X.reshape(N, N)).flatten() test_equivalence(f_tf, f_np, X_val, U_val)\n\nzeros: 1.5365486660812167e-12 rand: 1.5365486660812167e-12 dummy method failed\n\nbut the other two methods give the same answer. Is something fishy going on, or is this all fine?\n\nEDIT: reading @MattShannon \'s comment further up I realise this is probably to do with the way cholesky grad is implemented, and that there is likely some redundant computation, but it shouldn’t effect the result. My mind is at rest. Annoying to have to pass v0, though...\n\n@hughsalimbeni I also met this problem where TF complains about placeholders when I have a list of variables for u - using tf.ones_like(ys) for v seems to work for me.\n\nbotev mentioned this issue\n\nGraph optimization and other features tensorflow/tensorflow#3610\n\nlawsonfulton commented\n\nI believe I have identified a bug where this trick does not work for the tf.nn.elu activation. Computing the jacobian-vector product at some x < 0 gives the incorrect result only for the ELU operation.\n\nConsider the following minimal example which computes the Jacobin numerically, with the vjp, and the jvp. Note that if you replace tf.nn.elu with my_relu, the results are correct.\n\nIf someone knows of a workaround to make this work for the built-in ELU I would greatly appreciate it, since using the my_elu function incurs a 2-3x reduction in performance.\n\nimport tensorflow as tf import numpy as np def fwd_gradients(ys, xs, d_xs): dummy = tf.zeros_like(ys) g = tf.gradients(ys, xs, grad_ys=dummy, name=""gradients"") return tf.gradients(g, dummy, grad_ys=d_xs, name=""jvp"") def my_elu(x): return tf.where(x >= 0.0, x, tf.exp(x) - 1.0) def main(): print(tf.__version__) sess = tf.InteractiveSession() init = tf.global_variables_initializer() activation = tf.nn.elu # Works correctly tf.nn.relu (or any other non-elu activation) x_size = 1 y_size = 1 # Single ELU or RELU op X = tf.placeholder(tf.float64, shape=[x_size]) # Input Y = activation(X) # Output # Define vjp and jvp Vx = tf.placeholder(tf.float64, shape=[x_size]) # Jac-vector product input V Vy = tf.placeholder(tf.float64, shape=[y_size]) # vector-jac product input V jvp = fwd_gradients(Y, X, d_xs=Vx) vjp = tf.gradients(Y, X, grad_ys=Vy) # Compute jacobians x = np.ones(x_size) - 1.5 # Bug only occurs in x < 0 region tf_jac, numeric_jac = tf.test.compute_gradient(X, [x_size], Y, [y_size], x_init_value=x) vjp_jac = np.array([sess.run(vjp, feed_dict={X: x, Vy: v})[0] for v in np.identity(y_size)]) jvp_jac = np.array([sess.run(jvp, feed_dict={X: x, Vx: v})[0] for v in np.identity(x_size)]) # Print results as maximum absolute error print(""Numeric jac:"", numeric_jac) print(""jvp jac:"", jvp_jac) print(""tf error:"", np.max(np.abs(numeric_jac - tf_jac))) # ~0.0 print(""vjp error:"", np.max(np.abs(numeric_jac - vjp_jac))) # ~0.0 print(""jvp error:"", np.max(np.abs(numeric_jac - jvp_jac))) # LARGE! for ELU sess.close() if __name__ == \'__main__\': main()\n\nTested on tensorflow 1.4.0, 1.8.0, and latest nightly 1.9.0-dev20180515\n\n@teaghan I guess if the inputs is 100-dim and output is 10-dim, it maybe more efficient just use back-propagation? Since for forward-mode, each input dimension needs a separate differentiation path.\n\n@renmengye by back-propagation do you mean just use the normal tf.gradients(y, x)? I have been doing this, but I was hoping for a faster method as this is quite expensive with a large model and a large number of inputs and outputs (the example shown is a simplification). Looking at @zero-impact\'s example, would running jvp_jac = np.array([sess.run(jvp, feed_dict={X: x, Vx: v})[0] for v in np.identity(x_size)[x_indices]) make any sense? where x_indices are my features of interest for the inputs.\n\nlawsonfulton mentioned this issue\n\nBug in EluGradGrad tensorflow/tensorflow#19333\n\n@teaghan If you know which input feature dimension you are interested in, then it makes sense to use forward mode. The u_val is the gradients towards the inputs. If you know which scalar of input is to take gradients, then you can leave u_val to be None. This is similar to, when you do backpropagation, the optional gradients to be fed on the output.\n\nmattjj mentioned this issue\n\nFeature request: forwards autodifferentiation for gradient computation tensorflow/tensorflow#19361\n\nrefraction-ray commented\n\nI believe there are many function primitives that the implementation using placeholder for v would fail. For example, by using the exactly the same code as shown by @mattjj with tf1.13, many functions fails complaining placeholder not specified. Pass cases include tanh (the original post), exp, sigmoid, pow while failing cases include tan, sin, cos, cosh, sinh, log and many more such as nn.elu as mentioned above. Therefore, the implementation using placeholder is not very robust unless you can make sure no primitive fails in your codebase.\n\nOf course, one can specify something like v=tf.ones_like(ys) and get the correct result for the gradients. But I guess in such cases, the two rounds of reverse AD are actually executed by tensorflow and thus nothing gains. I guess the reason that placeholder approach fails is because the linearity with v is only guaranteed in theory. In practice, Jacobian product is a complicated function and possibly for some of the function primitives, the expected linearity with v are disguised by their backprop function leaving tensorflow failing identifing the linearity between g and v and thus failing in optimizing the final graph.\n\ncooijmanstim mentioned this issue\n\nForward-mode-by-double-backprop fails on tf.square ops tensorflow/tensorflow#34554\n\njaweriaamjad commented\n\nWhy did @mattjj pass <npr.randn(1, 5)> for <grad_ys> in the in this function? As per tensorflow documentation ""grad_ys is a list of tensors of the same length as ys that holds the initial gradients for each y in ys. When grad_ys is None, we fill in a tensor of \'1\'s of the shape of y for each y in ys. A user can provide their own initial grad_ys to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).""\n\nCorrect me if I am wrong but I believe this means that if we pass a random vector instead of all 1s for <grad_ys>, it acts as a weighting function. I checked on a small example and it indeed is true:\n\ntf.reset_default_graph() A = tf.constant(npr.randn(5, 3), dtype=tf.float32) B = tf.constant(npr.randn(3, 3), dtype=tf.float32) x = tf.placeholder(tf.float32, [1, 5]) y = tf.nn.relu(tf.matmul(tf.tanh(tf.matmul(x, A)),B)) u = tf.placeholder(tf.float32, [1, 5]) jvp = fwd_gradients(y, x, u) x_val = npr.randn(1, 5) u_val = np.ones((1, 5)) init_op = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init_op) jac_man = (sess.run(jvp, feed_dict={x: x_val, u: u_val})) jac_jvp = (sess.run(tf.reduce_sum(jacobian(y, x),axis=2), feed_dict={x: x_val})) print(\'------------------------------------\') print(\'forward jacobian calculated manually is: {}\'.format(jac_man)) print(\'forward jacobian calculated using power iteration method is: {}\'.format(jac_jvp.T))\n\nHere jacobian(y,x) is a function that evaluates the jacobian matrix of a given function. In the code above, if I pass npr.randn(1,5) instead of np.ones((1,5)) for u, I get incorrect values for forward gradients.\n\n(although from implementation perspective, forward-mode should be a lot easier than reverse-mode)\n\nOnly because people have been thinking about AD in an awkward way (in terms of graphs and “tensors”), including seeing reverse mode as involving a forward pass, reverse pass, mutation, “tapes” etc. The simple essence of automatic differentiation shows that reverse mode AD is not only as simple as forward mode but that they’re both special cases of the same simple, general algorithm.\n\ncooijmanstim commented\n\n@jaweriaamjad it depends on what you want to compute. tf.gradients computes vector-Jacobian products u^T (dy/dx), and this trick computes Jacobian-vector products (dy/dx) u. The former is ""how does a change in x affect y in the direction of u"", the latter is ""how does a change in x in the direction of u affect y?"". You can set u to all ones, but there is nothing correct about that particular value. It just gets you the sum of the columns of the Jacobian.\n\nThe JVP/VJP parlance wasn\'t clear to me initially, but I think it\'s because we\'re so used to the typical use case of reverse-differentiating a scalar output. For example if y is a batch of losses, we tend to average it and then differentiate: tf.gradients(tf.reduce_mean(y), x). But you could equally well differentiate the batched losses and pass a uniform weighting vector for each of the losses, e.g. tf.gradients(y, x, grad_ys=tf.ones_like(y) / tf.size(y)). This is a more general view because it also applies the other way around (i.e. fwd_gradients above with the equivalent of a grad_xs argument).\n\njaweriaamjad commented\n\n@cooijmanstim so if I understand correctly, we will indeed pass all ones for <u> if we wish to calculate the sum of the columns of jacobian of loss functions during training (since usually we don\'t want a weighted sum). The purpose of <grad_ys> is to give us the freedom to calculate the weighted sum.\n\ncooijmanstim commented\n\n@jaweriaamjad right, but to be clear, summing the columns means summing across x, not across y. It will not correspond to summing losses but to summing parameters. The shape of the output will be the same as that of y. To sum across y you want to use reverse-mode.\n\nAlso, weighted sum is a particular case. More generally, grad_ys is used to implement the chain rule. Consider the ith layer of a feedforward neural net computes output h[i] from h[i-1], and further down the line this results in some loss L. Then if you have dL/dh[i] (call it dL_dh[i] to get a valid Python identifier), you can recursively go back, computing the gradient dL/dh[i-1] by dL_dh[i-1] = tf.gradients(h[i], h[i-1], grad_ys=dL_dh[i]). This is backprop.\n\nnish-ant mentioned this issue\n\nWhat exactly are dummy variables and fwd_gradients methods doing in discrete inference of Burgers Eq? maziarraissi/PINNs#25\n\nProGamerGov mentioned this issue\n\nOptim-wip: Add Activation Atlas tutorial & functions pytorch/captum#579\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2023-12-05T04:47:43', 'title': 'generating forward-mode graphs by calling tf.gradients twice · Issue #2 · renmengye/tensorflow-forward-ad', 'url': 'https://github.com/renmengye/tensorflow-forward-ad/issues/2'}), Document(page_content='# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper functions for computing gradients.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\ndef fwd_gradient(func_or_y, x, input_gradients=None, use_gradient_tape=False,\n                 unconnected_gradients=None,\n                 name=None):\n  """"""Computes forward mode gradient.\n\n  Implementation based on suggestions in\n  [this thread](https://github.com/tensorflow/tensorflow/issues/19361).\n\n  TensorFlow computes gradients using the reverse mode automatic\n  differentiation which is suitable for typical machine learning situations\n  where one has a scalar loss function that one wants to differentiate with\n  respect to the parameters. In some cases, one needs to be able to compute\n  directional derivatives of non-scalar functions. Suppose F is a function from\n  R^n to R^m and let u be a fixed vector in R^n, w a fixed vector in R^m and\n  x a variable taking values in R^n. Let J(F) denote the jacobian matrix of\n  F of shape [m, n] (i.e. J(F)[i, j] = dF_i / dx_j). Then the default\n  gradients function in TF computes the expression\n  w^T.J(F) (i.e. Sum[w_i dF_i / dx_j, 1 <= i <= m]).\n\n  On the other hand, one also often needs to compute the directional derivative\n  J(F).u (i.e. Sum[u_j dF_i / dx_j, 1 <= j <= n]). Unfortunately, TensorFlow\n  has no native support for accumulating this. Providing first class support\n  for forward mode differentiation requires some significant changes in the core\n  architecture of TF (including writing a directional derivative for each\n  op).\n\n  The following function sidesteps this by using two passes of reverse mode\n  differentiation. Mathematically, the idea is simple. If F: R^n -> R^m, then\n  w^T.J(F) seen as a function of w is a function from R^m to R^n (because\n  w is in R^m, and w^T.J(F) is in R^n). Hence a reverse mode differentiation\n  with respect to w should produce J(F).u.\n\n  This function provides only a small subset of the flexibility of\n  the tf.gradients function. This may be extended in the future.\n\n  Following example demonstrates the usage and the difference between this\n  op and the standard `tf.gradients`\n  ```python\n    t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]\n    def fn(t):\n      return tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]\n    # Produces shape [3, 2] with values [[1, 1], [2, 4], [3, 12]]\n    fwd_grad_y = fwd_gradient(fn, t)\n    # Produces shape [2] with values [6, 17].\n    bck_grad_y = tf.gradients(y, t)[0]\n  ```\n\n  Args:\n    func_or_y: Either a `Tensor` connected to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    x: A `Tensor` with respect to which the gradient is to be computed.\n    input_gradients: A `Tensor` of the same shape as `x`. The direction along\n      which the directional derivative is to be computed.\n      Default value: `None` which maps to a ones-like `Tensor` of `x`.\n    use_gradient_tape: Optional Python bool. Whether to use gradient tape even\n      when eager mode is not turned on.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` of the same shape as `func(x)`.\n\n  Raises:\n    ValueError: If `func_or_y` is not a callable and the output is eagerly\n      executed or when the `tf.GradientTape` is used.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  with tf.name_scope(name or ""gradients""):\n    f = _prepare_func(func_or_y)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(x)\n      w = tf.ones_like(y)\n      g = tf.gradients(y, x, grad_ys=w,\n                       unconnected_gradients=unconnected_gradients)\n      return tf.gradients(g, w, grad_ys=input_gradients,\n                          unconnected_gradients=unconnected_gradients)[0]\n    if not callable(func_or_y):\n      raise ValueError(""`func_or_y` should be a callable in eager mode or when ""\n                       ""`tf.GradientTape` is used."")\n    with tf.GradientTape() as outer_tape:\n      with tf.GradientTape() as inner_tape:\n        inner_tape.watch(x)\n        y = f(x)\n      w = tf.ones_like(y)\n      outer_tape.watch(w)\n      g = inner_tape.gradient(y, x, output_gradients=w,\n                              unconnected_gradients=unconnected_gradients)\n    return outer_tape.gradient(g, w, output_gradients=input_gradients,\n                               unconnected_gradients=unconnected_gradients)\n\n\ndef gradients(func_or_y, xs, output_gradients=None, use_gradient_tape=False,\n              unconnected_gradients=None,\n              name=None):\n  """"""Computes the gradients of `func_or_y` wrt to `*xs`.\n\n  Args:\n   func_or_y: Either a `Tensor` connected to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n      Default value: `None` which maps to a ones-like `Tensor` of `ys`.\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` with the gradient of `y` wrt each of `xs` or a list of `Tensor`s\n    if `xs` is a list.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  f = _prepare_func(func_or_y)\n  with tf.name_scope(name or ""gradients""):\n    xs, is_xs_list_like = _prepare_args(xs)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(*xs)\n      grad = tf.gradients(y, xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    else:\n      if not callable(func_or_y):\n        raise ValueError(""`func_or_y` should be a callable in eager mode or ""\n                         ""when `tf.GradientTape` is used."")\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return grad\n    else:\n      return grad[0]\n\n\ndef value_and_gradient(f,\n                       xs,\n                       output_gradients=None,\n                       use_gradient_tape=False,\n                       unconnected_gradients=None,\n                       name=None):\n  """"""Computes `f(*xs)` and its gradients wrt to `*xs`.\n\n  Args:\n    f: Python `callable` to be differentiated. If `f` returns a scalar, this\n      scalar will be differentiated. If `f` returns a tensor or list of tensors,\n      by default a scalar will be computed by adding all their values to produce\n      a single scalar. If desired, the tensors can be elementwise multiplied by\n      the tensors passed as the `dy` keyword argument to the returned gradient\n      function.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., `\'value_and_gradient\'`).\n\n  Returns:\n    A tuple of two elements. The first one is a `Tensor` representing the value\n    of the function at `xs` and the second one is either a `Tensor` or a list of\n    `Tensor`s representing the gradient of `f(*xs)` wrt `xs`.\n    y: `y = f(*xs)`.\n    dydx: Gradient of `y` wrt each of `xs`.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  xs, is_xs_list_like = _prepare_args(xs)\n  with tf.name_scope(name or ""value_and_gradient""):\n    if tf.executing_eagerly() or use_gradient_tape:\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    else:\n      y = f(*xs)\n      grad = tf.gradients(ys=y, xs=xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return y, grad\n    else:\n      return y, grad[0]\n\n\ndef make_val_and_grad_fn(value_fn):\n  """"""Function decorator to compute both function value and gradient.\n\n  ```\n  @tff.math.make_val_and_grad_fn\n  def quadratic(x):\n    return tf.reduce_sum(scales * (x - minimum) ** 2, axis=-1)\n  ```\n\n  Turns `quadratic` into a function that accepts a point as a `Tensor` as input\n  and returns a tuple of two `Tensor`s with the value and the gradient of the\n  defined quadratic function evaluated at the input point.\n\n  This is useful for constructing functions to optimize with tff.math.optimizer\n  methods.\n\n  Args:\n    value_fn: A python function to decorate.\n\n  Returns:\n    The decorated function.\n  """"""\n  @functools.wraps(value_fn)\n  def val_and_grad(x):\n    return value_and_gradient(value_fn, x)\n\n  return val_and_grad\n\n\ndef _prepare_func(func_or_y):\n  """"""Creates a function out of the input callable or `Tensor`.""""""\n  if callable(func_or_y):\n    return func_or_y\n  else:\n    return lambda *args: func_or_y\n\n\ndef _prepare_args(xs):\n  """"""Converts `xs` to a list if necessary.""""""\n  if isinstance(xs, (list, tuple)):\n    return xs, True\n  else:\n    return [xs], False', metadata={'id': 'web-search_3', 'snippet': '# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Helper functions for computing gradients.""""""\n\nimport tensorflow.compat.v2 as tf\n\n\ndef fwd_gradient(func_or_y, x, input_gradients=None, use_gradient_tape=False,\n                 unconnected_gradients=None,\n                 name=None):\n  """"""Computes forward mode gradient.\n\n  Implementation based on suggestions in\n  [this thread](https://github.com/tensorflow/tensorflow/issues/19361).\n\n  TensorFlow computes gradients using the reverse mode automatic\n  differentiation which is suitable for typical machine learning situations\n  where one has a scalar loss function that one wants to differentiate with\n  respect to the parameters. In some cases, one needs to be able to compute\n  directional derivatives of non-scalar functions. Suppose F is a function from\n  R^n to R^m and let u be a fixed vector in R^n, w a fixed vector in R^m and\n  x a variable taking values in R^n. Let J(F) denote the jacobian matrix of\n  F of shape [m, n] (i.e. J(F)[i, j] = dF_i / dx_j). Then the default\n  gradients function in TF computes the expression\n  w^T.J(F) (i.e. Sum[w_i dF_i / dx_j, 1 <= i <= m]).\n\n  On the other hand, one also often needs to compute the directional derivative\n  J(F).u (i.e. Sum[u_j dF_i / dx_j, 1 <= j <= n]). Unfortunately, TensorFlow\n  has no native support for accumulating this. Providing first class support\n  for forward mode differentiation requires some significant changes in the core\n  architecture of TF (including writing a directional derivative for each\n  op).\n\n  The following function sidesteps this by using two passes of reverse mode\n  differentiation. Mathematically, the idea is simple. If F: R^n -> R^m, then\n  w^T.J(F) seen as a function of w is a function from R^m to R^n (because\n  w is in R^m, and w^T.J(F) is in R^n). Hence a reverse mode differentiation\n  with respect to w should produce J(F).u.\n\n  This function provides only a small subset of the flexibility of\n  the tf.gradients function. This may be extended in the future.\n\n  Following example demonstrates the usage and the difference between this\n  op and the standard `tf.gradients`\n  ```python\n    t = tf.range(1, 3, dtype=tf.float32)  # Shape [2]\n    def fn(t):\n      return tf.stack([t, t ** 2, t ** 3], axis=0)  # Shape [3, 2]\n    # Produces shape [3, 2] with values [[1, 1], [2, 4], [3, 12]]\n    fwd_grad_y = fwd_gradient(fn, t)\n    # Produces shape [2] with values [6, 17].\n    bck_grad_y = tf.gradients(y, t)[0]\n  ```\n\n  Args:\n    func_or_y: Either a `Tensor` connected to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    x: A `Tensor` with respect to which the gradient is to be computed.\n    input_gradients: A `Tensor` of the same shape as `x`. The direction along\n      which the directional derivative is to be computed.\n      Default value: `None` which maps to a ones-like `Tensor` of `x`.\n    use_gradient_tape: Optional Python bool. Whether to use gradient tape even\n      when eager mode is not turned on.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` of the same shape as `func(x)`.\n\n  Raises:\n    ValueError: If `func_or_y` is not a callable and the output is eagerly\n      executed or when the `tf.GradientTape` is used.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  with tf.name_scope(name or ""gradients""):\n    f = _prepare_func(func_or_y)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(x)\n      w = tf.ones_like(y)\n      g = tf.gradients(y, x, grad_ys=w,\n                       unconnected_gradients=unconnected_gradients)\n      return tf.gradients(g, w, grad_ys=input_gradients,\n                          unconnected_gradients=unconnected_gradients)[0]\n    if not callable(func_or_y):\n      raise ValueError(""`func_or_y` should be a callable in eager mode or when ""\n                       ""`tf.GradientTape` is used."")\n    with tf.GradientTape() as outer_tape:\n      with tf.GradientTape() as inner_tape:\n        inner_tape.watch(x)\n        y = f(x)\n      w = tf.ones_like(y)\n      outer_tape.watch(w)\n      g = inner_tape.gradient(y, x, output_gradients=w,\n                              unconnected_gradients=unconnected_gradients)\n    return outer_tape.gradient(g, w, output_gradients=input_gradients,\n                               unconnected_gradients=unconnected_gradients)\n\n\ndef gradients(func_or_y, xs, output_gradients=None, use_gradient_tape=False,\n              unconnected_gradients=None,\n              name=None):\n  """"""Computes the gradients of `func_or_y` wrt to `*xs`.\n\n  Args:\n   func_or_y: Either a `Tensor` connected to the input `x` or a Python callable\n      accepting one `Tensor` of shape of `x` and returning a `Tensor` of any\n      shape. The function whose gradient is to be computed. If eagerly\n      executing, can only be a callable, i.e., one should not supply a Tensor\n      in eager mode.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n      Default value: `None` which maps to a ones-like `Tensor` of `ys`.\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., \'gradients\').\n\n  Returns:\n    A `Tensor` with the gradient of `y` wrt each of `xs` or a list of `Tensor`s\n    if `xs` is a list.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  f = _prepare_func(func_or_y)\n  with tf.name_scope(name or ""gradients""):\n    xs, is_xs_list_like = _prepare_args(xs)\n    if not tf.executing_eagerly() and not use_gradient_tape:\n      y = f(*xs)\n      grad = tf.gradients(y, xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    else:\n      if not callable(func_or_y):\n        raise ValueError(""`func_or_y` should be a callable in eager mode or ""\n                         ""when `tf.GradientTape` is used."")\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return grad\n    else:\n      return grad[0]\n\n\ndef value_and_gradient(f,\n                       xs,\n                       output_gradients=None,\n                       use_gradient_tape=False,\n                       unconnected_gradients=None,\n                       name=None):\n  """"""Computes `f(*xs)` and its gradients wrt to `*xs`.\n\n  Args:\n    f: Python `callable` to be differentiated. If `f` returns a scalar, this\n      scalar will be differentiated. If `f` returns a tensor or list of tensors,\n      by default a scalar will be computed by adding all their values to produce\n      a single scalar. If desired, the tensors can be elementwise multiplied by\n      the tensors passed as the `dy` keyword argument to the returned gradient\n      function.\n    xs: Python list of parameters of `f` for which to differentiate. (Can also\n      be single `Tensor`.)\n    output_gradients: A `Tensor` or list of `Tensor`s the same size as the\n      result `ys = f(*xs)` and holding the gradients computed for each `y` in\n      `ys`. This argument is forwarded to the underlying gradient implementation\n      (i.e., either the `grad_ys` argument of `tf.gradients` or the\n      `output_gradients` argument of `tf.GradientTape.gradient`).\n    use_gradient_tape: Python `bool` indicating that `tf.GradientTape` should be\n      used regardless of `tf.executing_eagerly()` status.\n      Default value: `False`.\n    unconnected_gradients: An enum `tf.UnconnectedGradients` which specifies the\n      gradient value returned when the given input tensors are unconnected.\n      Default value: `None`, which maps to `tf.UnconnectedGradients.NONE`.\n    name: Python `str` name prefixed to ops created by this function.\n      Default value: `None` (i.e., `\'value_and_gradient\'`).\n\n  Returns:\n    A tuple of two elements. The first one is a `Tensor` representing the value\n    of the function at `xs` and the second one is either a `Tensor` or a list of\n    `Tensor`s representing the gradient of `f(*xs)` wrt `xs`.\n    y: `y = f(*xs)`.\n    dydx: Gradient of `y` wrt each of `xs`.\n  """"""\n  unconnected_gradients = unconnected_gradients or tf.UnconnectedGradients.NONE\n  xs, is_xs_list_like = _prepare_args(xs)\n  with tf.name_scope(name or ""value_and_gradient""):\n    if tf.executing_eagerly() or use_gradient_tape:\n      with tf.GradientTape() as tape:\n        for x in xs:\n          tape.watch(x)\n        y = f(*xs)\n      grad = tape.gradient(y, xs, output_gradients=output_gradients,\n                           unconnected_gradients=unconnected_gradients)\n    else:\n      y = f(*xs)\n      grad = tf.gradients(ys=y, xs=xs, grad_ys=output_gradients,\n                          unconnected_gradients=unconnected_gradients)\n    if is_xs_list_like:\n      return y, grad\n    else:\n      return y, grad[0]\n\n\ndef make_val_and_grad_fn(value_fn):\n  """"""Function decorator to compute both function value and gradient.\n\n  ```\n  @tff.math.make_val_and_grad_fn\n  def quadratic(x):\n    return tf.reduce_sum(scales * (x - minimum) ** 2, axis=-1)\n  ```\n\n  Turns `quadratic` into a function that accepts a point as a `Tensor` as input\n  and returns a tuple of two `Tensor`s with the value and the gradient of the\n  defined quadratic function evaluated at the input point.\n\n  This is useful for constructing functions to optimize with tff.math.optimizer\n  methods.\n\n  Args:\n    value_fn: A python function to decorate.\n\n  Returns:\n    The decorated function.\n  """"""\n  @functools.wraps(value_fn)\n  def val_and_grad(x):\n    return value_and_gradient(value_fn, x)\n\n  return val_and_grad\n\n\ndef _prepare_func(func_or_y):\n  """"""Creates a function out of the input callable or `Tensor`.""""""\n  if callable(func_or_y):\n    return func_or_y\n  else:\n    return lambda *args: func_or_y\n\n\ndef _prepare_args(xs):\n  """"""Converts `xs` to a list if necessary.""""""\n  if isinstance(xs, (list, tuple)):\n    return xs, True\n  else:\n    return [xs], False', 'timestamp': '2024-04-17T05:05:06', 'title': 'tf-quant-finance/tf_quant_finance/math/gradient.py at master · google/tf-quant-finance', 'url': 'https://github.com/google/tf-quant-finance/blob/master/tf_quant_finance/math/gradient.py'})], [Document(page_content='中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', metadata={'id': 'web-search_0', 'snippet': '中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', 'timestamp': '2024-04-27T08:12:28', 'title': 'tf.gradients | TensorFlow v2.15.0.post1', 'url': 'https://www.tensorflow.org/api_docs/python/tf/gradients'}), Document(page_content='中文 – 简体 GitHub', metadata={'id': 'web-search_1', 'snippet': '中文 – 简体 GitHub', 'timestamp': '2024-04-11T21:14:32', 'title': 'tf.compat.v1.gradients | TensorFlow v2.14.0', 'url': 'https://www.tensorflow.org/api_docs/python/tf/compat/v1/gradients'}), Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to gradients and automatic differentiation\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nAutomatic Differentiation and Gradients\n\nAutomatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.\n\nIn this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution.\n\nimport numpy as np import matplotlib.pyplot as plt import tensorflow as tf\n\nTo differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients.\n\nTensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow ""records"" relevant operations executed inside the context of a tf.GradientTape onto a ""tape"". TensorFlow then uses that tape to compute the gradients of a ""recorded"" computation using reverse mode differentiation.\n\nHere is a simple example:\n\nx = tf.Variable(3.0) with tf.GradientTape() as tape: y = x**2\n\nOnce you\'ve recorded some operations, use GradientTape.gradient(target, sources) to calculate the gradient of some target (often a loss) relative to some source (often the model\'s variables):\n\n# dy = 2x * dx dy_dx = tape.gradient(y, x) dy_dx.numpy()\n\nThe above example uses scalars, but tf.GradientTape works as easily on any tensor:\n\nw = tf.Variable(tf.random.normal((3, 2)), name=\'w\') b = tf.Variable(tf.zeros(2, dtype=tf.float32), name=\'b\') x = [[1., 2., 3.]] with tf.GradientTape(persistent=True) as tape: y = x @ w + b loss = tf.reduce_mean(y**2)\n\nTo get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see tf.nest).\n\n[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n\nThe gradient with respect to each source has the shape of the source:\n\nprint(w.shape) print(dl_dw.shape)\n\nHere is the gradient calculation again, this time passing a dictionary of variables:\n\nmy_vars = { \'w\': w, \'b\': b } grad = tape.gradient(loss, my_vars) grad[\'b\']\n\nGradients with respect to a model\n\nIt\'s common to collect tf.Variables into a tf.Module or one of its subclasses (layers.Layer, keras.Model) for checkpointing and exporting.\n\nIn most cases, you will want to calculate gradients with respect to a model\'s trainable variables. Since all subclasses of tf.Module aggregate their variables in the Module.trainable_variables property, you can calculate these gradients in a few lines of code:\n\nlayer = tf.keras.layers.Dense(2, activation=\'relu\') x = tf.constant([[1., 2., 3.]]) with tf.GradientTape() as tape: # Forward pass y = layer(x) loss = tf.reduce_mean(y**2) # Calculate gradients with respect to every trainable variable grad = tape.gradient(loss, layer.trainable_variables)\n\nfor var, g in zip(layer.trainable_variables, grad): print(f\'{var.name}, shape: {g.shape}\')\n\nControlling what the tape watches\n\nThe default behavior is to record all operations after accessing a trainable tf.Variable. The reasons for this are:\n\nThe tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n\nThe tape holds references to intermediate outputs, so you don\'t want to record unnecessary operations.\n\nThe most common use case involves calculating the gradient of a loss with respect to all a model\'s trainable variables.\n\nFor example, the following fails to calculate a gradient because the tf.Tensor is not ""watched"" by default, and the tf.Variable is not trainable:\n\n# A trainable variable x0 = tf.Variable(3.0, name=\'x0\') # Not trainable x1 = tf.Variable(3.0, name=\'x1\', trainable=False) # Not a Variable: A variable + tensor returns a tensor. x2 = tf.Variable(2.0, name=\'x2\') + 1.0 # Not a variable x3 = tf.constant(3.0, name=\'x3\') with tf.GradientTape() as tape: y = (x0**2) + (x1**2) + (x2**2) grad = tape.gradient(y, [x0, x1, x2, x3]) for g in grad: print(g)\n\nYou can list the variables being watched by the tape using the GradientTape.watched_variables method:\n\n[var.name for var in tape.watched_variables()]\n\ntf.GradientTape provides hooks that give the user control over what is or is not watched.\n\nTo record gradients with respect to a tf.Tensor, you need to call GradientTape.watch(x):\n\nx = tf.constant(3.0) with tf.GradientTape() as tape: tape.watch(x) y = x**2 # dy = 2x * dx dy_dx = tape.gradient(y, x) print(dy_dx.numpy())\n\nConversely, to disable the default behavior of watching all tf.Variables, set watch_accessed_variables=False when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:\n\nx0 = tf.Variable(0.0) x1 = tf.Variable(10.0) with tf.GradientTape(watch_accessed_variables=False) as tape: tape.watch(x1) y0 = tf.math.sin(x0) y1 = tf.nn.softplus(x1) y = y0 + y1 ys = tf.reduce_sum(y)\n\nSince GradientTape.watch was not called on x0, no gradient is computed with respect to it:\n\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1) grad = tape.gradient(ys, {\'x0\': x0, \'x1\': x1}) print(\'dy/dx0:\', grad[\'x0\']) print(\'dy/dx1:\', grad[\'x1\'].numpy())\n\nIntermediate results\n\nYou can also request gradients of the output with respect to intermediate values computed inside the tf.GradientTape context.\n\nx = tf.constant(3.0) with tf.GradientTape() as tape: tape.watch(x) y = x * x z = y * y # Use the tape to compute the gradient of z with respect to the # intermediate value y. # dz_dy = 2 * y and y = x ** 2 = 9 print(tape.gradient(z, y).numpy())\n\nBy default, the resources held by a GradientTape are released as soon as the GradientTape.gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent=True. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:\n\nx = tf.constant([1, 3.0]) with tf.GradientTape(persistent=True) as tape: tape.watch(x) y = x * x z = y * y print(tape.gradient(z, x).numpy()) # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0]) print(tape.gradient(y, x).numpy()) # [2.0, 6.0] (2 * x at x = [1.0, 3.0])\n\ndel tape # Drop the reference to the tape\n\nNotes on performance\n\nThere is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n\nGradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n\nFor efficiency, some ops (like ReLU) don\'t need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent=True on your tape, nothing is discarded and your peak memory usage will be higher.\n\nGradients of non-scalar targets\n\nA gradient is fundamentally an operation on a scalar.\n\nx = tf.Variable(2.0) with tf.GradientTape(persistent=True) as tape: y0 = x**2 y1 = 1 / x print(tape.gradient(y0, x).numpy()) print(tape.gradient(y1, x).numpy())\n\nThus, if you ask for the gradient of multiple targets, the result for each source is:\n\nThe gradient of the sum of the targets, or equivalently\n\nThe sum of the gradients of each target.\n\nx = tf.Variable(2.0) with tf.GradientTape() as tape: y0 = x**2 y1 = 1 / x print(tape.gradient({\'y0\': y0, \'y1\': y1}, x).numpy())\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is calculated:\n\nx = tf.Variable(2.) with tf.GradientTape() as tape: y = x * [3., 4.] print(tape.gradient(y, x).numpy())\n\nThis makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n\nIf you need a separate gradient for each item, refer to Jacobians.\n\nIn some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:\n\nx = tf.linspace(-10.0, 10.0, 200+1) with tf.GradientTape() as tape: tape.watch(x) y = tf.nn.sigmoid(x) dy_dx = tape.gradient(y, x)\n\nplt.plot(x, y, label=\'y\') plt.plot(x, dy_dx, label=\'dy/dx\') plt.legend() _ = plt.xlabel(\'x\')\n\nBecause a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, if and while statements).\n\nHere a different variable is used on each branch of an if. The gradient only connects to the variable that was used:\n\nx = tf.constant(1.0) v0 = tf.Variable(2.0) v1 = tf.Variable(2.0) with tf.GradientTape(persistent=True) as tape: tape.watch(x) if x > 0.0: result = v0 else: result = v1**2 dv0, dv1 = tape.gradient(result, [v0, v1]) print(dv0) print(dv1)\n\nJust remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n\nDepending on the value of x in the above example, the tape either records result = v0 or result = v1**2. The gradient with respect to x is always None.\n\ndx = tape.gradient(result, x) print(dx)\n\nCases where gradient returns None\n\nWhen a target is not connected to a source, gradient will return None.\n\nx = tf.Variable(2.) y = tf.Variable(3.) with tf.GradientTape() as tape: z = y * y print(tape.gradient(z, x))\n\nHere z is obviously not connected to x, but there are several less-obvious ways that a gradient can be disconnected.\n\n1. Replaced a variable with a tensor\n\nIn the section on ""controlling what the tape watches"" you saw that the tape will automatically watch a tf.Variable but not a tf.Tensor.\n\nOne common error is to inadvertently replace a tf.Variable with a tf.Tensor, instead of using Variable.assign to update the tf.Variable. Here is an example:\n\nx = tf.Variable(2.0) for epoch in range(2): with tf.GradientTape() as tape: y = x+1 print(type(x).__name__, "":"", tape.gradient(y, x)) x = x + 1 # This should be `x.assign_add(1)`\n\n2. Did calculations outside of TensorFlow\n\nThe tape can\'t record the gradient path if the calculation exits TensorFlow. For example:\n\nx = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32) with tf.GradientTape() as tape: x2 = x**2 # This step is calculated with NumPy y = np.mean(x2, axis=0) # Like most ops, reduce_mean will cast the NumPy array to a constant tensor # using `tf.convert_to_tensor`. y = tf.reduce_mean(y, axis=0) print(tape.gradient(y, x))\n\n3. Took gradients through an integer or string\n\nIntegers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n\nNobody expects strings to be differentiable, but it\'s easy to accidentally create an int constant or variable if you don\'t specify the dtype.\n\nx = tf.constant(10) with tf.GradientTape() as g: g.watch(x) y = x * x print(g.gradient(y, x))\n\nTensorFlow doesn\'t automatically cast between types, so, in practice, you\'ll often get a type error instead of a missing gradient.\n\n4. Took gradients through a stateful object\n\nState stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n\nA tf.Tensor is immutable. You can\'t change a tensor once it\'s created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf.matmul only depends on its inputs.\n\nA tf.Variable has internal state—its value. When you use the variable, the state is read. It\'s normal to calculate a gradient with respect to a variable, but the variable\'s state blocks gradient calculations from going farther back. For example:\n\nx0 = tf.Variable(3.0) x1 = tf.Variable(0.0) with tf.GradientTape() as tape: # Update x1 = x1 + x0. x1.assign_add(x0) # The tape starts recording from x1. y = x1**2 # y = (x1 + x0)**2 # This doesn\'t work. print(tape.gradient(y, x0)) #dy/dx0 = 2*(x1 + x0)\n\nSimilarly, tf.data.Dataset iterators and tf.queues are stateful, and will stop all gradients on tensors that pass through them.\n\nNo gradient registered\n\nSome tf.Operations are registered as being non-differentiable and will return None. Others have no gradient registered.\n\nThe tf.raw_ops page shows which low-level ops have gradients registered.\n\nIf you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning None. This way you know something has gone wrong.\n\nFor example, the tf.image.adjust_contrast function wraps raw_ops.AdjustContrastv2, which could have a gradient but the gradient is not implemented:\n\nimage = tf.Variable([[[0.5, 0.0, 0.0]]]) delta = tf.Variable(0.1) with tf.GradientTape() as tape: new_image = tf.image.adjust_contrast(image, delta) try: print(tape.gradient(new_image, [image, delta])) assert False # This should not happen. except LookupError as e: print(f\'{type(e).__name__}: {e}\')\n\nIf you need to differentiate through this op, you\'ll either need to implement the gradient and register it (using tf.RegisterGradient) or re-implement the function using other ops.\n\nZeros instead of None\n\nIn some cases it would be convenient to get 0 instead of None for unconnected gradients. You can decide what to return when you have unconnected gradients using the unconnected_gradients argument:\n\nx = tf.Variable([2., 2.]) y = tf.Variable(3.) with tf.GradientTape() as tape: z = y**2 print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-03-23 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_2', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to gradients and automatic differentiation\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nAutomatic Differentiation and Gradients\n\nAutomatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.\n\nIn this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution.\n\nimport numpy as np import matplotlib.pyplot as plt import tensorflow as tf\n\nTo differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients.\n\nTensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow ""records"" relevant operations executed inside the context of a tf.GradientTape onto a ""tape"". TensorFlow then uses that tape to compute the gradients of a ""recorded"" computation using reverse mode differentiation.\n\nHere is a simple example:\n\nx = tf.Variable(3.0) with tf.GradientTape() as tape: y = x**2\n\nOnce you\'ve recorded some operations, use GradientTape.gradient(target, sources) to calculate the gradient of some target (often a loss) relative to some source (often the model\'s variables):\n\n# dy = 2x * dx dy_dx = tape.gradient(y, x) dy_dx.numpy()\n\nThe above example uses scalars, but tf.GradientTape works as easily on any tensor:\n\nw = tf.Variable(tf.random.normal((3, 2)), name=\'w\') b = tf.Variable(tf.zeros(2, dtype=tf.float32), name=\'b\') x = [[1., 2., 3.]] with tf.GradientTape(persistent=True) as tape: y = x @ w + b loss = tf.reduce_mean(y**2)\n\nTo get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see tf.nest).\n\n[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n\nThe gradient with respect to each source has the shape of the source:\n\nprint(w.shape) print(dl_dw.shape)\n\nHere is the gradient calculation again, this time passing a dictionary of variables:\n\nmy_vars = { \'w\': w, \'b\': b } grad = tape.gradient(loss, my_vars) grad[\'b\']\n\nGradients with respect to a model\n\nIt\'s common to collect tf.Variables into a tf.Module or one of its subclasses (layers.Layer, keras.Model) for checkpointing and exporting.\n\nIn most cases, you will want to calculate gradients with respect to a model\'s trainable variables. Since all subclasses of tf.Module aggregate their variables in the Module.trainable_variables property, you can calculate these gradients in a few lines of code:\n\nlayer = tf.keras.layers.Dense(2, activation=\'relu\') x = tf.constant([[1., 2., 3.]]) with tf.GradientTape() as tape: # Forward pass y = layer(x) loss = tf.reduce_mean(y**2) # Calculate gradients with respect to every trainable variable grad = tape.gradient(loss, layer.trainable_variables)\n\nfor var, g in zip(layer.trainable_variables, grad): print(f\'{var.name}, shape: {g.shape}\')\n\nControlling what the tape watches\n\nThe default behavior is to record all operations after accessing a trainable tf.Variable. The reasons for this are:\n\nThe tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n\nThe tape holds references to intermediate outputs, so you don\'t want to record unnecessary operations.\n\nThe most common use case involves calculating the gradient of a loss with respect to all a model\'s trainable variables.\n\nFor example, the following fails to calculate a gradient because the tf.Tensor is not ""watched"" by default, and the tf.Variable is not trainable:\n\n# A trainable variable x0 = tf.Variable(3.0, name=\'x0\') # Not trainable x1 = tf.Variable(3.0, name=\'x1\', trainable=False) # Not a Variable: A variable + tensor returns a tensor. x2 = tf.Variable(2.0, name=\'x2\') + 1.0 # Not a variable x3 = tf.constant(3.0, name=\'x3\') with tf.GradientTape() as tape: y = (x0**2) + (x1**2) + (x2**2) grad = tape.gradient(y, [x0, x1, x2, x3]) for g in grad: print(g)\n\nYou can list the variables being watched by the tape using the GradientTape.watched_variables method:\n\n[var.name for var in tape.watched_variables()]\n\ntf.GradientTape provides hooks that give the user control over what is or is not watched.\n\nTo record gradients with respect to a tf.Tensor, you need to call GradientTape.watch(x):\n\nx = tf.constant(3.0) with tf.GradientTape() as tape: tape.watch(x) y = x**2 # dy = 2x * dx dy_dx = tape.gradient(y, x) print(dy_dx.numpy())\n\nConversely, to disable the default behavior of watching all tf.Variables, set watch_accessed_variables=False when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:\n\nx0 = tf.Variable(0.0) x1 = tf.Variable(10.0) with tf.GradientTape(watch_accessed_variables=False) as tape: tape.watch(x1) y0 = tf.math.sin(x0) y1 = tf.nn.softplus(x1) y = y0 + y1 ys = tf.reduce_sum(y)\n\nSince GradientTape.watch was not called on x0, no gradient is computed with respect to it:\n\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1) grad = tape.gradient(ys, {\'x0\': x0, \'x1\': x1}) print(\'dy/dx0:\', grad[\'x0\']) print(\'dy/dx1:\', grad[\'x1\'].numpy())\n\nIntermediate results\n\nYou can also request gradients of the output with respect to intermediate values computed inside the tf.GradientTape context.\n\nx = tf.constant(3.0) with tf.GradientTape() as tape: tape.watch(x) y = x * x z = y * y # Use the tape to compute the gradient of z with respect to the # intermediate value y. # dz_dy = 2 * y and y = x ** 2 = 9 print(tape.gradient(z, y).numpy())\n\nBy default, the resources held by a GradientTape are released as soon as the GradientTape.gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent=True. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:\n\nx = tf.constant([1, 3.0]) with tf.GradientTape(persistent=True) as tape: tape.watch(x) y = x * x z = y * y print(tape.gradient(z, x).numpy()) # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0]) print(tape.gradient(y, x).numpy()) # [2.0, 6.0] (2 * x at x = [1.0, 3.0])\n\ndel tape # Drop the reference to the tape\n\nNotes on performance\n\nThere is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n\nGradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n\nFor efficiency, some ops (like ReLU) don\'t need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent=True on your tape, nothing is discarded and your peak memory usage will be higher.\n\nGradients of non-scalar targets\n\nA gradient is fundamentally an operation on a scalar.\n\nx = tf.Variable(2.0) with tf.GradientTape(persistent=True) as tape: y0 = x**2 y1 = 1 / x print(tape.gradient(y0, x).numpy()) print(tape.gradient(y1, x).numpy())\n\nThus, if you ask for the gradient of multiple targets, the result for each source is:\n\nThe gradient of the sum of the targets, or equivalently\n\nThe sum of the gradients of each target.\n\nx = tf.Variable(2.0) with tf.GradientTape() as tape: y0 = x**2 y1 = 1 / x print(tape.gradient({\'y0\': y0, \'y1\': y1}, x).numpy())\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is calculated:\n\nx = tf.Variable(2.) with tf.GradientTape() as tape: y = x * [3., 4.] print(tape.gradient(y, x).numpy())\n\nThis makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n\nIf you need a separate gradient for each item, refer to Jacobians.\n\nIn some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:\n\nx = tf.linspace(-10.0, 10.0, 200+1) with tf.GradientTape() as tape: tape.watch(x) y = tf.nn.sigmoid(x) dy_dx = tape.gradient(y, x)\n\nplt.plot(x, y, label=\'y\') plt.plot(x, dy_dx, label=\'dy/dx\') plt.legend() _ = plt.xlabel(\'x\')\n\nBecause a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, if and while statements).\n\nHere a different variable is used on each branch of an if. The gradient only connects to the variable that was used:\n\nx = tf.constant(1.0) v0 = tf.Variable(2.0) v1 = tf.Variable(2.0) with tf.GradientTape(persistent=True) as tape: tape.watch(x) if x > 0.0: result = v0 else: result = v1**2 dv0, dv1 = tape.gradient(result, [v0, v1]) print(dv0) print(dv1)\n\nJust remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n\nDepending on the value of x in the above example, the tape either records result = v0 or result = v1**2. The gradient with respect to x is always None.\n\ndx = tape.gradient(result, x) print(dx)\n\nCases where gradient returns None\n\nWhen a target is not connected to a source, gradient will return None.\n\nx = tf.Variable(2.) y = tf.Variable(3.) with tf.GradientTape() as tape: z = y * y print(tape.gradient(z, x))\n\nHere z is obviously not connected to x, but there are several less-obvious ways that a gradient can be disconnected.\n\n1. Replaced a variable with a tensor\n\nIn the section on ""controlling what the tape watches"" you saw that the tape will automatically watch a tf.Variable but not a tf.Tensor.\n\nOne common error is to inadvertently replace a tf.Variable with a tf.Tensor, instead of using Variable.assign to update the tf.Variable. Here is an example:\n\nx = tf.Variable(2.0) for epoch in range(2): with tf.GradientTape() as tape: y = x+1 print(type(x).__name__, "":"", tape.gradient(y, x)) x = x + 1 # This should be `x.assign_add(1)`\n\n2. Did calculations outside of TensorFlow\n\nThe tape can\'t record the gradient path if the calculation exits TensorFlow. For example:\n\nx = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32) with tf.GradientTape() as tape: x2 = x**2 # This step is calculated with NumPy y = np.mean(x2, axis=0) # Like most ops, reduce_mean will cast the NumPy array to a constant tensor # using `tf.convert_to_tensor`. y = tf.reduce_mean(y, axis=0) print(tape.gradient(y, x))\n\n3. Took gradients through an integer or string\n\nIntegers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n\nNobody expects strings to be differentiable, but it\'s easy to accidentally create an int constant or variable if you don\'t specify the dtype.\n\nx = tf.constant(10) with tf.GradientTape() as g: g.watch(x) y = x * x print(g.gradient(y, x))\n\nTensorFlow doesn\'t automatically cast between types, so, in practice, you\'ll often get a type error instead of a missing gradient.\n\n4. Took gradients through a stateful object\n\nState stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n\nA tf.Tensor is immutable. You can\'t change a tensor once it\'s created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf.matmul only depends on its inputs.\n\nA tf.Variable has internal state—its value. When you use the variable, the state is read. It\'s normal to calculate a gradient with respect to a variable, but the variable\'s state blocks gradient calculations from going farther back. For example:\n\nx0 = tf.Variable(3.0) x1 = tf.Variable(0.0) with tf.GradientTape() as tape: # Update x1 = x1 + x0. x1.assign_add(x0) # The tape starts recording from x1. y = x1**2 # y = (x1 + x0)**2 # This doesn\'t work. print(tape.gradient(y, x0)) #dy/dx0 = 2*(x1 + x0)\n\nSimilarly, tf.data.Dataset iterators and tf.queues are stateful, and will stop all gradients on tensors that pass through them.\n\nNo gradient registered\n\nSome tf.Operations are registered as being non-differentiable and will return None. Others have no gradient registered.\n\nThe tf.raw_ops page shows which low-level ops have gradients registered.\n\nIf you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning None. This way you know something has gone wrong.\n\nFor example, the tf.image.adjust_contrast function wraps raw_ops.AdjustContrastv2, which could have a gradient but the gradient is not implemented:\n\nimage = tf.Variable([[[0.5, 0.0, 0.0]]]) delta = tf.Variable(0.1) with tf.GradientTape() as tape: new_image = tf.image.adjust_contrast(image, delta) try: print(tape.gradient(new_image, [image, delta])) assert False # This should not happen. except LookupError as e: print(f\'{type(e).__name__}: {e}\')\n\nIf you need to differentiate through this op, you\'ll either need to implement the gradient and register it (using tf.RegisterGradient) or re-implement the function using other ops.\n\nZeros instead of None\n\nIn some cases it would be convenient to get 0 instead of None for unconnected gradients. You can decide what to return when you have unconnected gradients using the unconnected_gradients argument:\n\nx = tf.Variable([2., 2.]) y = tf.Variable(3.) with tf.GradientTape() as tape: z = y**2 print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-03-23 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-07-06T14:22:34', 'title': 'Introduction to gradients and automatic differentiation | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/autodiff'}), Document(page_content='中文 – 简体 GitHub\n\ntfp.math.value_and_gradient\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView source on GitHub\n\nComputes f(*args, **kwargs) and its gradients wrt to args, kwargs.\n\ntfp.math.value_and_gradient( f, *args, output_gradients=None, use_gradient_tape=False, auto_unpack_single_arg=True, has_aux=False, name=None, **kwargs )\n\nUsed in the notebooks\n\nUsed in the tutorials\n\nBayesian Modeling with Joint Distribution\n\nOptimizers in TensorFlow Probability\n\nTFP Release Notes notebook (0.12.1)\n\nThe function f is invoked according to one of the following rules:\n\nIf f is a function of no arguments then it is called as f().\n\nIf len(args) == 1, len(kwargs) == 0, auto_unpack_single_arg == True and isinstance(args[0], (list, tuple)) then args is presumed to be a packed sequence of args, i.e., the function is called as f(*args[0]).\n\nOtherwise, the function is called as f(*args, **kwargs).\n\nRegardless of how f is called, gradients are computed with respect to args and kwargs.\n\ntfd = tfp.distributions tfm = tfp.math # Case 1: argless `f`. x = tf.constant(2.) tfm.value_and_gradient(lambda: tf.math.log(x), x) # ==> [log(2.), 0.5] # Case 2: packed arguments. tfm.value_and_gradient(lambda x, y: x * tf.math.log(y), [2., 3.]) # ==> [2. * np.log(3.), (np.log(3.), 2. / 3)] # Case 3: default. tfm.value_and_gradient(tf.math.log, [1., 2., 3.], auto_unpack_single_arg=False) # ==> [(log(1.), log(2.), log(3.)), (1., 0.5, 0.333)]\n\nPython callable to be differentiated. If f returns a scalar, this scalar will be differentiated. If f returns a tensor or list of tensors, the gradient will be the sum of the gradients of each part. If desired the sum can be weighted by output_gradients (see below).\n\nArguments as in f(*args, **kwargs) and basis for gradient.\n\nA Tensor or structure of Tensors the same size as the result ys = f(*args, **kwargs) and holding the gradients computed for each y in ys. This argument is forwarded to the underlying gradient implementation (i.e., either the grad_ys argument of tf.gradients or the output_gradients argument of tf.GradientTape.gradient). Default value: None.\n\nPython bool indicating that tf.GradientTape should be used rather than tf.gradient and regardless of tf.executing_eagerly(). (It is only possible to use tf.gradient when not use_gradient_tape and not tf.executing_eagerly().) Default value: False.\n\nauto_unpack_single_arg\n\nPython bool which when False means the single arg case will not be interpreted as a list of arguments. (See case 2.) Default value: True.\n\nWhether f(*args, **kwargs) actually returns two outputs, the first being y and the second being an auxiliary output that does not get gradients computed.\n\nPython str name prefixed to ops created by this function. Default value: None (i.e., \'value_and_gradient\').\n\nNamed arguments as in f(*args, **kwargs) and basis for gradient.\n\nIf has_aux is False: y: y = f(*args, **kwargs). dydx: Gradients of y with respect to each of args and kwargs.\n\nA tuple ((y, aux), dydx), where y, aux = f(*args, **kwargs) and dydx are the gradients of y with respect to each of args and kwargs.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-11-21 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_3', 'snippet': '中文 – 简体 GitHub\n\ntfp.math.value_and_gradient\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView source on GitHub\n\nComputes f(*args, **kwargs) and its gradients wrt to args, kwargs.\n\ntfp.math.value_and_gradient( f, *args, output_gradients=None, use_gradient_tape=False, auto_unpack_single_arg=True, has_aux=False, name=None, **kwargs )\n\nUsed in the notebooks\n\nUsed in the tutorials\n\nBayesian Modeling with Joint Distribution\n\nOptimizers in TensorFlow Probability\n\nTFP Release Notes notebook (0.12.1)\n\nThe function f is invoked according to one of the following rules:\n\nIf f is a function of no arguments then it is called as f().\n\nIf len(args) == 1, len(kwargs) == 0, auto_unpack_single_arg == True and isinstance(args[0], (list, tuple)) then args is presumed to be a packed sequence of args, i.e., the function is called as f(*args[0]).\n\nOtherwise, the function is called as f(*args, **kwargs).\n\nRegardless of how f is called, gradients are computed with respect to args and kwargs.\n\ntfd = tfp.distributions tfm = tfp.math # Case 1: argless `f`. x = tf.constant(2.) tfm.value_and_gradient(lambda: tf.math.log(x), x) # ==> [log(2.), 0.5] # Case 2: packed arguments. tfm.value_and_gradient(lambda x, y: x * tf.math.log(y), [2., 3.]) # ==> [2. * np.log(3.), (np.log(3.), 2. / 3)] # Case 3: default. tfm.value_and_gradient(tf.math.log, [1., 2., 3.], auto_unpack_single_arg=False) # ==> [(log(1.), log(2.), log(3.)), (1., 0.5, 0.333)]\n\nPython callable to be differentiated. If f returns a scalar, this scalar will be differentiated. If f returns a tensor or list of tensors, the gradient will be the sum of the gradients of each part. If desired the sum can be weighted by output_gradients (see below).\n\nArguments as in f(*args, **kwargs) and basis for gradient.\n\nA Tensor or structure of Tensors the same size as the result ys = f(*args, **kwargs) and holding the gradients computed for each y in ys. This argument is forwarded to the underlying gradient implementation (i.e., either the grad_ys argument of tf.gradients or the output_gradients argument of tf.GradientTape.gradient). Default value: None.\n\nPython bool indicating that tf.GradientTape should be used rather than tf.gradient and regardless of tf.executing_eagerly(). (It is only possible to use tf.gradient when not use_gradient_tape and not tf.executing_eagerly().) Default value: False.\n\nauto_unpack_single_arg\n\nPython bool which when False means the single arg case will not be interpreted as a list of arguments. (See case 2.) Default value: True.\n\nWhether f(*args, **kwargs) actually returns two outputs, the first being y and the second being an auxiliary output that does not get gradients computed.\n\nPython str name prefixed to ops created by this function. Default value: None (i.e., \'value_and_gradient\').\n\nNamed arguments as in f(*args, **kwargs) and basis for gradient.\n\nIf has_aux is False: y: y = f(*args, **kwargs). dydx: Gradients of y with respect to each of args and kwargs.\n\nA tuple ((y, aux), dydx), where y, aux = f(*args, **kwargs) and dydx are the gradients of y with respect to each of args and kwargs.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-11-21 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-04-17T13:42:00', 'title': 'tfp.math.value_and_gradient | TensorFlow Probability', 'url': 'https://www.tensorflow.org/probability/api_docs/python/tfp/math/value_and_gradient'})]]??"
54047604,tf.custom_gradient,"{'https://www.coursera.org/learn/custom-distributed-training-with-tensorflow', 'https://www.udemy.com/course/yolov4-and-tf20/', 'https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/', 'https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187', 'https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/', 'https://www.udemy.com/course/deep-learning-tensorflow-2/', 'https://www.udemy.com/course/deep-learning-masterclass-with-tensorflow-2-over-15-projects/', 'https://www.udemy.com/course/data-science-deep-learning-in-theano-tensorflow/'}","{'https://www.youtube.com/watch?v=kq6mpyjSQ3w', 'https://www.youtube.com/watch?v=zVHV-rj3g38', 'https://www.youtube.com/watch?v=ODPjd05FmkI', 'https://www.youtube.com/watch?v=l-MGydWW-UE', 'https://www.youtube.com/watch?v=us-5I52Lac8', 'https://www.youtube.com/watch?v=boIOgsu-Q8E'}","{'https://stackoverflow.com/questions/54047604/how-to-assign-custom-gradient-to-tensorflow-op-with-multiple-inputs', 'https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs', 'https://stackoverflow.com/questions/52604879/how-excute-custom-gradient-with-tf-multiply'}","??[[Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\ntf.custom_gradient with multiple inputs\n\nAsked 5 years, 7 months ago\n\nModified 5 years ago\n\ntf.custom_gradient accepts only one Tensor x, what if this op needs more than one inputs?\n\nFor example, to define the gradient of Softmax which needs input x and label?\n\nThanks for the suggestion from @AllenLavoie, I use a Python list as input.\n\ndef self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(input_): x = input_[0] label = input_[2] def grad(dy): return [dy, dy] return x - label, grad x = tf.range(10, dtype=tf.float32) y = tf.range(10, dtype=tf.int32) loss = loss_func([x, y]) if __name__ == \'__main__\': self_define_op_multiple_inputs()\n\nIt seems that it will convert the Python list to a Tensor. The snippet above will raise a TypeError: TypeError: Cannot convert a list containing a tensor of dtype <dtype: \'int32\'> to <dtype: \'float32\'> (Tensor is: <tf.Tensor \'range_1:0\' shape=(10,) dtype=int32>)\n\nImprove this question\n\nedited Aug 16, 2018 at 2:19\n\nasked Aug 14, 2018 at 7:46\n\nhuangbiubiuhuangbiubiu\n\n1,2612020 silver badges3838 bronze badges 6\n\nThe documentation says x and y can both either be Tensors or sequences of Tensors. Did this not work for you?\n\n– Allen Lavoie Aug 14, 2018 at 16:32\n\n@AllenLavoie Actually this is exactly what confused me. I don\'t understand what\'s sequences of Tensors, does it mean a Python list of Tensor?\n\n– huangbiubiu Aug 15, 2018 at 2:37\n\nMy interpretation is Python list (or tuple, etc.). So len(x) is the number of inputs to the operation, len(y) is the number of outputs. Then the gradient function takes len(y) Tensor argument and returns len(x) Tensors.\n\n– Allen Lavoie Aug 15, 2018 at 18:33\n\n@AllenLavoie I tried to use list but it seems like a list will be converted as a Tensor, which will cause an error if there are multiple inputs with different type and matched shape. The question has been updated.\n\n– huangbiubiu Aug 16, 2018 at 2:23\n\n@AllenLavoie I created an issue on github\n\n– huangbiubiu Aug 21, 2018 at 10:09\n\n | Show 1 more comment\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nI ran into a similar problem yesterday and found this post, and I believe I know what you are running into. Problem is that while using @tf.custom_gradient, the function that it decorates can have multiple inputs (instead of a list of tensors). Look at the following code(note that it\'s just a test code with no actual meaning):\n\n@tf.custom_gradient def loop1(x,a): def grad(dy): return dy*3,dy*2 n = tf.multiply(x,a) return n,grad\n\nBy using two inputs x and a, you have to return two gradients respectively in the grad function. dy*3 corresponds to the gradient of x and dy*2 corresponds to the gradient of a.\n\nI think in this function the documents make people very confusing, but you can still use multiple inputs, just make sure that you also have the same number of gradients, or else you will run into errors.\n\nanswered Mar 13, 2019 at 2:10\n\n16311 silver badge99 bronze badges 1\n\nCan we return None as gradients for unused terms ?\n\n– pateheo Nov 20, 2022 at 16:31\n\nI believe you need something like this a tf Graph input:+ n_input is the input number\n\nx = tf.placeholder(""float"", [None, n_input]) y = tf.placeholder(""float"", [None])\n\nDoes this answer your question ?\n\nanswered Aug 14, 2018 at 7:52\n\n491212 bronze badges 1\n\nThanks for your help. But it seems that you didn\'t understand what I am talking about. tf.custom_gradient is not a computational graph. You can read docs for more details.\n\n– huangbiubiu Aug 14, 2018 at 8:00\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nBuilding GenAI features in practice with Intuit Mailchimp\n\nA leading ML educator on what you need to know about LLMs\n\n2024 Community Moderator Election\n\nOur partnership with Google and commitment to socially responsible AI\n\nShifting the data dump schedule: A proposal\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n11 How to register a custom gradient for a operation composed of tf operations\n\n1 TensorFlow custom gradients\n\n0 Using op inputs when defining custom gradients in TensorFlow\n\n7 How to provide custom gradient in TensorFlow\n\n1 Using tf.custom_gradient in tensorflow r1.8\n\n1 How excute custom gradient with tf.multiply?\n\n1 How to assign custom gradient to TensorFlow op with multiple inputs\n\n3 Correct way to apply gradients in TF2 custom training loop with multiple Keras models\n\n7 Taking gradients when using tf.function\n\n0 Tensorflow: Custom Layer/Gradient result in OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed\n\nHot Network Questions\n\nWhere exactly does the root directory exist in the linux filesystem?\n\nmultipy earths orbital speed with Square root of 2\n\nWhy do they fight with knives and not with assault rifles in Dune?\n\nPTIJ: Can a Cohen go into a room with 60 or more sleeping people?\n\nMaking Nuclear weapons almost Impossible to build\n\nIs there an archive of photos shot by astronauts on the ISS?\n\npresent simple with how long\n\nIs it safe to connect AC earth to DC negative?\n\nGive two points with the most similar distances to the origin\n\nDoes the existence of humans increase the plausibility of other kinds of designers?\n\nAre Faerûnian tieflings a people unto themselves?\n\nWhy is my hydrogen energy not equal to -0.5 hartrees?\n\nHow to calculate the true distance of the sun and the earth using skyfield?\n\nActivities besides tv\n\nWhere exactly is the “Wilhelm Scream” in “A Star is Born” (1954)?\n\nHow to make this complex gradient shape?\n\nIs there any version of an ion drive, fictional or not, that would be able to create torchship-like acceleration for interplanetary travel?\n\nWhy is ""programming"" so much prioritized in a computer science degree?\n\n\\NewDocumentCommand with delimited arguments\n\nFor countries without written constitutions, what power does the judiciary have to control laws passed by government?\n\nHow can you animate a roll of tablecloth flying out of the backpack?\n\nUse \\partial symbol in Stix2 Font\n\nHow large is large for direct solvers? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_0', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\ntf.custom_gradient with multiple inputs\n\nAsked 5 years, 7 months ago\n\nModified 5 years ago\n\ntf.custom_gradient accepts only one Tensor x, what if this op needs more than one inputs?\n\nFor example, to define the gradient of Softmax which needs input x and label?\n\nThanks for the suggestion from @AllenLavoie, I use a Python list as input.\n\ndef self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(input_): x = input_[0] label = input_[2] def grad(dy): return [dy, dy] return x - label, grad x = tf.range(10, dtype=tf.float32) y = tf.range(10, dtype=tf.int32) loss = loss_func([x, y]) if __name__ == \'__main__\': self_define_op_multiple_inputs()\n\nIt seems that it will convert the Python list to a Tensor. The snippet above will raise a TypeError: TypeError: Cannot convert a list containing a tensor of dtype <dtype: \'int32\'> to <dtype: \'float32\'> (Tensor is: <tf.Tensor \'range_1:0\' shape=(10,) dtype=int32>)\n\nImprove this question\n\nedited Aug 16, 2018 at 2:19\n\nasked Aug 14, 2018 at 7:46\n\nhuangbiubiuhuangbiubiu\n\n1,2612020 silver badges3838 bronze badges 6\n\nThe documentation says x and y can both either be Tensors or sequences of Tensors. Did this not work for you?\n\n– Allen Lavoie Aug 14, 2018 at 16:32\n\n@AllenLavoie Actually this is exactly what confused me. I don\'t understand what\'s sequences of Tensors, does it mean a Python list of Tensor?\n\n– huangbiubiu Aug 15, 2018 at 2:37\n\nMy interpretation is Python list (or tuple, etc.). So len(x) is the number of inputs to the operation, len(y) is the number of outputs. Then the gradient function takes len(y) Tensor argument and returns len(x) Tensors.\n\n– Allen Lavoie Aug 15, 2018 at 18:33\n\n@AllenLavoie I tried to use list but it seems like a list will be converted as a Tensor, which will cause an error if there are multiple inputs with different type and matched shape. The question has been updated.\n\n– huangbiubiu Aug 16, 2018 at 2:23\n\n@AllenLavoie I created an issue on github\n\n– huangbiubiu Aug 21, 2018 at 10:09\n\n | Show 1 more comment\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nI ran into a similar problem yesterday and found this post, and I believe I know what you are running into. Problem is that while using @tf.custom_gradient, the function that it decorates can have multiple inputs (instead of a list of tensors). Look at the following code(note that it\'s just a test code with no actual meaning):\n\n@tf.custom_gradient def loop1(x,a): def grad(dy): return dy*3,dy*2 n = tf.multiply(x,a) return n,grad\n\nBy using two inputs x and a, you have to return two gradients respectively in the grad function. dy*3 corresponds to the gradient of x and dy*2 corresponds to the gradient of a.\n\nI think in this function the documents make people very confusing, but you can still use multiple inputs, just make sure that you also have the same number of gradients, or else you will run into errors.\n\nanswered Mar 13, 2019 at 2:10\n\n16311 silver badge99 bronze badges 1\n\nCan we return None as gradients for unused terms ?\n\n– pateheo Nov 20, 2022 at 16:31\n\nI believe you need something like this a tf Graph input:+ n_input is the input number\n\nx = tf.placeholder(""float"", [None, n_input]) y = tf.placeholder(""float"", [None])\n\nDoes this answer your question ?\n\nanswered Aug 14, 2018 at 7:52\n\n491212 bronze badges 1\n\nThanks for your help. But it seems that you didn\'t understand what I am talking about. tf.custom_gradient is not a computational graph. You can read docs for more details.\n\n– huangbiubiu Aug 14, 2018 at 8:00\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nBuilding GenAI features in practice with Intuit Mailchimp\n\nA leading ML educator on what you need to know about LLMs\n\n2024 Community Moderator Election\n\nOur partnership with Google and commitment to socially responsible AI\n\nShifting the data dump schedule: A proposal\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n11 How to register a custom gradient for a operation composed of tf operations\n\n1 TensorFlow custom gradients\n\n0 Using op inputs when defining custom gradients in TensorFlow\n\n7 How to provide custom gradient in TensorFlow\n\n1 Using tf.custom_gradient in tensorflow r1.8\n\n1 How excute custom gradient with tf.multiply?\n\n1 How to assign custom gradient to TensorFlow op with multiple inputs\n\n3 Correct way to apply gradients in TF2 custom training loop with multiple Keras models\n\n7 Taking gradients when using tf.function\n\n0 Tensorflow: Custom Layer/Gradient result in OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed\n\nHot Network Questions\n\nWhere exactly does the root directory exist in the linux filesystem?\n\nmultipy earths orbital speed with Square root of 2\n\nWhy do they fight with knives and not with assault rifles in Dune?\n\nPTIJ: Can a Cohen go into a room with 60 or more sleeping people?\n\nMaking Nuclear weapons almost Impossible to build\n\nIs there an archive of photos shot by astronauts on the ISS?\n\npresent simple with how long\n\nIs it safe to connect AC earth to DC negative?\n\nGive two points with the most similar distances to the origin\n\nDoes the existence of humans increase the plausibility of other kinds of designers?\n\nAre Faerûnian tieflings a people unto themselves?\n\nWhy is my hydrogen energy not equal to -0.5 hartrees?\n\nHow to calculate the true distance of the sun and the earth using skyfield?\n\nActivities besides tv\n\nWhere exactly is the “Wilhelm Scream” in “A Star is Born” (1954)?\n\nHow to make this complex gradient shape?\n\nIs there any version of an ion drive, fictional or not, that would be able to create torchship-like acceleration for interplanetary travel?\n\nWhy is ""programming"" so much prioritized in a computer science degree?\n\n\\NewDocumentCommand with delimited arguments\n\nFor countries without written constitutions, what power does the judiciary have to control laws passed by government?\n\nHow can you animate a roll of tablecloth flying out of the backpack?\n\nUse \\partial symbol in Stix2 Font\n\nHow large is large for direct solvers? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-03-11T09:45:16', 'title': 'python - tf.custom_gradient with multiple inputs - Stack Overflow', 'url': 'https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nHow are we doing? Please help us improve Stack Overflow. Take our short survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nTake our short survey\n\nHow excute custom gradient with tf.multiply?\n\nAsked 5 years, 9 months ago\n\nModified 5 years, 9 months ago\n\nI define custom gradient mapper with tensorflow package.\n\nWhen i use tf.multiply with custom gradient, it did not work .\n\nimport tensorflow as tf @tf.RegisterGradient(""MyopGrad"") def frop_grad(op, grad): x = op.inputs[0] return 1000.0 * x input = tf.Variable([4.0], dtype=tf.float32) x = tf.constant(5.0) g = tf.get_default_graph() with g.gradient_override_map({""Multiply"": ""MyopGrad""}): output1 = tf.multiply(input, x , name = \'multiply\') grad1 = tf.gradients(output1, input) # output without gradient clipping in the backwards pass for comparison: output1_ori = tf.multiply(input , x) grad1_ori = tf.gradients(output1_ori, input) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(""with custom:"", sess.run(grad1)[0]) print(""without custom:"", sess.run(grad1_ori)[0])\n\nImprove this question\n\nasked Oct 2, 2018 at 8:43\n\n57855 silver badges1818 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe TensorFlow op name for tf.multiply is just Mul, not Multiply. Also, tf.multiply has two inputs, so its gradients should have two outputs. So your code could look something like this:\n\nimport tensorflow as tf @tf.RegisterGradient(""MyopGrad"") def frop_grad(op, grad): x = op.inputs[0] y = op.inputs[1] return 1000.0 * x, 1000.0 * y input = tf.Variable([4.0], dtype=tf.float32) x = tf.constant(5.0) g = tf.get_default_graph() with g.gradient_override_map({""Mul"": ""MyopGrad""}): output1 = tf.multiply(input, x , name = \'multiply\') grad1 = tf.gradients(output1, input) # output without gradient clipping in the backwards pass for comparison: output1_ori = tf.multiply(input , x) grad1_ori = tf.gradients(output1_ori, input) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(""with custom:"", sess.run(grad1)[0]) print(""without custom:"", sess.run(grad1_ori)[0])\n\nwith custom: [4000.] without custom: [5.]\n\nanswered Oct 2, 2018 at 9:23\n\n59.2k77 gold badges8181 silver badges126126 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ngradient or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n10 How to register a custom gradient for a operation composed of tf operations\n\n0 TensorFlow: how to do python function with custom gradients without eval?\n\n7 How to provide custom gradient in TensorFlow\n\n1 Using tf.custom_gradient in tensorflow r1.8\n\n2 Compute Gradients in Tensorflow\n\n0 tf.custom_gradient with multiple inputs\n\n1 How to assign custom gradient to TensorFlow op with multiple inputs\n\n2 Converting tf.gradients to tensor type\n\n0 Missing gradient when using tf.function\n\n7 Taking gradients when using tf.function\n\nHot Network Questions\n\nCan non-admins create new domain on local DNS from a client computer?\n\nWhat caused the builder to change plans midstream on this 1905 library in New England?\n\nWhat scientifically plausible apocalypse scenario, if any, meets my criteria?\n\nI want to pick my flight route. I can either get 2 round trip tickets (SRC<->MID, MID<->DST) or 3rd party booking (SRC<->DST). Which is more reliable?\n\nWhen, if ever, is bribery legal?\n\nDoes the damage from Thunderwave occur before or after the target is moved\n\nMysterious creaking on (mainly) the right pedal stroke even after replacing usual suspects\n\nHow can I search File Explorer for files only (i.e. exclude folders) in Windows 10?\n\nIs the variance of the mean of a set of possibly dependent random variables less than the average of their respective variances?\n\nIs it a security issue to expose PII on any publically accessible URL?\n\n""More beautiful than ever twisted braid..."" in John Keats\'s ""Lamia""\n\nSci fi book that has a tunnel. Depending on where you go through wall, go to different planet\n\n.Net 8 XUnit: Should my tests use real data or how to mock MySql with CQRS?\n\nIs this a Hadamard matrix?\n\nIntelligence vs Wisdom in D&D\n\nHow to turn a sum into an integral?\n\nHow to handle my player\'s use of Greater Camouflage Dye?\n\nReal-life problems involving solving triangles\n\nWhen is it appropriate to report mistakes in a paper?\n\nAccomodating whiteboard glare for low-vision student\n\nWhy does King Aegon speak to his dragon in the Common Tongue (English)?\n\nHow are GameManagers created in Unity?\n\nWhy do jet aircraft need chocks when they have parking brakes? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_2', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nHow are we doing? Please help us improve Stack Overflow. Take our short survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nTake our short survey\n\nHow excute custom gradient with tf.multiply?\n\nAsked 5 years, 9 months ago\n\nModified 5 years, 9 months ago\n\nI define custom gradient mapper with tensorflow package.\n\nWhen i use tf.multiply with custom gradient, it did not work .\n\nimport tensorflow as tf @tf.RegisterGradient(""MyopGrad"") def frop_grad(op, grad): x = op.inputs[0] return 1000.0 * x input = tf.Variable([4.0], dtype=tf.float32) x = tf.constant(5.0) g = tf.get_default_graph() with g.gradient_override_map({""Multiply"": ""MyopGrad""}): output1 = tf.multiply(input, x , name = \'multiply\') grad1 = tf.gradients(output1, input) # output without gradient clipping in the backwards pass for comparison: output1_ori = tf.multiply(input , x) grad1_ori = tf.gradients(output1_ori, input) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(""with custom:"", sess.run(grad1)[0]) print(""without custom:"", sess.run(grad1_ori)[0])\n\nImprove this question\n\nasked Oct 2, 2018 at 8:43\n\n57855 silver badges1818 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe TensorFlow op name for tf.multiply is just Mul, not Multiply. Also, tf.multiply has two inputs, so its gradients should have two outputs. So your code could look something like this:\n\nimport tensorflow as tf @tf.RegisterGradient(""MyopGrad"") def frop_grad(op, grad): x = op.inputs[0] y = op.inputs[1] return 1000.0 * x, 1000.0 * y input = tf.Variable([4.0], dtype=tf.float32) x = tf.constant(5.0) g = tf.get_default_graph() with g.gradient_override_map({""Mul"": ""MyopGrad""}): output1 = tf.multiply(input, x , name = \'multiply\') grad1 = tf.gradients(output1, input) # output without gradient clipping in the backwards pass for comparison: output1_ori = tf.multiply(input , x) grad1_ori = tf.gradients(output1_ori, input) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(""with custom:"", sess.run(grad1)[0]) print(""without custom:"", sess.run(grad1_ori)[0])\n\nwith custom: [4000.] without custom: [5.]\n\nanswered Oct 2, 2018 at 9:23\n\n59.2k77 gold badges8181 silver badges126126 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ngradient or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n10 How to register a custom gradient for a operation composed of tf operations\n\n0 TensorFlow: how to do python function with custom gradients without eval?\n\n7 How to provide custom gradient in TensorFlow\n\n1 Using tf.custom_gradient in tensorflow r1.8\n\n2 Compute Gradients in Tensorflow\n\n0 tf.custom_gradient with multiple inputs\n\n1 How to assign custom gradient to TensorFlow op with multiple inputs\n\n2 Converting tf.gradients to tensor type\n\n0 Missing gradient when using tf.function\n\n7 Taking gradients when using tf.function\n\nHot Network Questions\n\nCan non-admins create new domain on local DNS from a client computer?\n\nWhat caused the builder to change plans midstream on this 1905 library in New England?\n\nWhat scientifically plausible apocalypse scenario, if any, meets my criteria?\n\nI want to pick my flight route. I can either get 2 round trip tickets (SRC<->MID, MID<->DST) or 3rd party booking (SRC<->DST). Which is more reliable?\n\nWhen, if ever, is bribery legal?\n\nDoes the damage from Thunderwave occur before or after the target is moved\n\nMysterious creaking on (mainly) the right pedal stroke even after replacing usual suspects\n\nHow can I search File Explorer for files only (i.e. exclude folders) in Windows 10?\n\nIs the variance of the mean of a set of possibly dependent random variables less than the average of their respective variances?\n\nIs it a security issue to expose PII on any publically accessible URL?\n\n""More beautiful than ever twisted braid..."" in John Keats\'s ""Lamia""\n\nSci fi book that has a tunnel. Depending on where you go through wall, go to different planet\n\n.Net 8 XUnit: Should my tests use real data or how to mock MySql with CQRS?\n\nIs this a Hadamard matrix?\n\nIntelligence vs Wisdom in D&D\n\nHow to turn a sum into an integral?\n\nHow to handle my player\'s use of Greater Camouflage Dye?\n\nReal-life problems involving solving triangles\n\nWhen is it appropriate to report mistakes in a paper?\n\nAccomodating whiteboard glare for low-vision student\n\nWhy does King Aegon speak to his dragon in the Common Tongue (English)?\n\nHow are GameManagers created in Unity?\n\nWhy do jet aircraft need chocks when they have parking brakes? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-10T02:08:03', 'title': 'python - How excute custom gradient with tf.multiply? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/52604879/how-excute-custom-gradient-with-tf-multiply'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nHow to assign custom gradient to TensorFlow op with multiple inputs\n\nAsked 4 years, 5 months ago\n\nModified 3 years, 10 months ago\n\nI\'m trying to use TensorFlow\'s @tf.custom_gradient functionality to assign a custom gradient to a function with multiple inputs. I can put together a working setup for only one input, but not for two or more.\n\nI\'ve based my code on TensorFlow\'s custom_gradient documentation, which works just fine for one input, as in this example:\n\nimport tensorflow as tf import os # Suppress Tensorflow startup info os.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\' # Custom gradient decorator on a function, # as described in documentation @tf.custom_gradient def my_identity(x): # The custom gradient def grad(dy): return dy # Return the result AND the gradient return tf.identity(x), grad # Make a variable, run it through the custom op x = tf.get_variable(\'x\', initializer=1.) y = my_identity(x) # Calculate loss, make an optimizer, train the variable loss = tf.abs(y) opt = tf.train.GradientDescentOptimizer(learning_rate=0.001) train = opt.minimize(loss) # Start a TensorFlow session, initialize variables, train with tf.Session() as sess: sess.run(tf.global_variables_initializer()) sess.run(train)\n\nThis example runs silently, then closes. No issues, no errors. The variable optimizes as expected. However, in my application, I need to do such a calculation with multiple inputs, so something of this form:\n\n@tf.custom_gradient def my_identity(x, z): def grad(dy): return dy return tf.identity(x*z), grad\n\nRunning this in place of the example (and adding another variable input to the call of my_identify) results in the following error output. Best as I can tell, the last parts of the error are from the dynamic generation of the op -- the information format matches the C++ formatting required in the op establishment (though that\'s about all I know about it).\n\nTraceback (most recent call last): File ""testing.py"", line 27, in <module> train = opt.minimize(loss) File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 400, in minimize grad_loss=grad_loss) File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 519, in compute_gradients colocate_gradients_with_ops=colocate_gradients_with_ops) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 630, in gradients gate_gradients, aggregation_method, stop_gradients) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 821, in _GradientsHelper _VerifyGeneratedGradients(in_grads, op) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 323, in _VerifyGeneratedGradients ""inputs %d"" % (len(grads), op.node_def, len(op.inputs))) ValueError: Num gradients 2 generated for op name: ""IdentityN"" op: ""IdentityN"" input: ""Identity"" input: ""x/read"" input: ""y/read"" attr { key: ""T"" value { list { type: DT_FLOAT type: DT_FLOAT type: DT_FLOAT } } } attr { key: ""_gradient_op_type"" value { s: ""CustomGradient-9"" } } do not match num inputs 3\n\nBased on other custom gradient options, I surmised that the issue was a lack of supplied gradient for the second input argument. So, I changed my function to this:\n\n@tf.custom_gradient def my_identity(x, z): def grad(dy): return dy return tf.identity(x*z), grad, grad\n\nThis results in the following more familiar error:\n\nTraceback (most recent call last): File ""testing.py"", line 22, in <module> y = my_identity(x, z) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated return _graph_mode_decorator(f, *args, **kwargs) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 132, in _graph_mode_decorator result, grad_fn = f(*args) ValueError: too many values to unpack (expected 2)\n\nThe @custom_gradient decorator is only identifying the last returned element as a gradient. So, I tried putting the two gradients into a tuple as (grad, grad) such that there would only be ""two"" outputs for the function. TensorFlow rejected this too, this time because it can\'t call a tuple like it would a Tensor -- entirely reasonable, in hindsight.\n\nI\'ve fussed around with the example some more, but to no avail. No matter what I try, I can\'t get the custom-defined gradient to deal with multiple inputs. I\'m hoping that somebody with more knowledge than I regarding custom ops and gradients will have a better idea on this -- thanks in advance for the help!\n\nImprove this question\n\nasked Jan 4, 2019 at 23:45\n\naedificatoriaedificatori\n\n11911 silver badge1111 bronze badges 1\n\nIs this still a problem?\n\n– gab Oct 23, 2019 at 20:03\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIf we use multiple variables as input, the number of gradients return from ""grad"" function should be equals to number of input variables, though we maybe don\'t care about some of them.\n\n@tf.custom_gradient def my_multiple(x,z): def grad(dy): # return two gradients, one for \'x\' and one for \'z\' return (dy*z, dy*x) return tf.identity(x*z), grad\n\nNote that the second output of ""my_multiple"" is a function, not a gradient tensor.\n\nedited Apr 29, 2019 at 12:07\n\nanswered Feb 28, 2019 at 1:48\n\n5633 bronze badges 2\n\nAs input to my_multiple you have (x,z).. but the gradient returns (dy*z, dy*x), where you have reversed the order. Is that okay for tensorflow? How does it know which term belongs where..?\n\n– zwep Dec 3, 2019 at 14:07\n\n@zwep the order is not reversed. The gradient of (x*z) with respect to x is z, and vice-versa.\n\n– wimi Dec 6, 2019 at 13:50\n\nI ran into a similar problem some time ago and I think the documentation is not very clear on this. In general, the code should be something like:\n\n@tf.custom_gradient def custom_operation(x, y, scope=\'custom_op\'): # define the gradient def grad(g): return g, g # define the forward pass (a multiplication, in this example) with tf.variable_scope(scope): forward_pass = x * y return forward_pass, grad\n\nIn practice, your internal grad function should return the gradient N times, where N is the number of argument that the custom_operation takes as input (apart from the scope). By using two inputs (x and y), the grad function must return the gradients twice (once for x and once for y). In general, you could also make the grad() function return g1 != g2 instead of g for both the inputs. So, in your example it becomes:\n\n@tf.custom_gradient def my_identity(x, z): def grad(dy): return dy, dy return tf.identity(x*z), grad\n\nedited Aug 7, 2019 at 13:04\n\nanswered Aug 6, 2019 at 15:59\n\n78211 gold badge1010 silver badges3434 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow-gradient or ask your own question.\n\nThe cofounder of Chef is cooking up a less painful DevOps (Ep. 584)\n\nImproving the developer experience in the energy sector\n\nStatement from SO: June 5, 2023 Moderator Action\n\nStarting the Prompt Design Site: A New Home in our Stack Exchange Neighborhood\n\nDoes the policy change for AI-generated content affect users who (want to)...\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n10 Tensorflow: How to write op with gradient in python?\n\n0 Tensorflow -- how can I process in numpy op outputs in py_func gradient?\n\n11 How to register a custom gradient for a operation composed of tf operations\n\n1 How do I define a gradient for a custom op working on complex tensors in tensorflow?\n\n1 Tensorflow op with two inputs, return one of the two and override gradient\n\n0 Using op inputs when defining custom gradients in TensorFlow\n\n7 How to provide custom gradient in TensorFlow\n\n0 Apply tensorflow gradients to specific inputs\n\n0 tf.custom_gradient with multiple inputs\n\n1 How excute custom gradient with tf.multiply?\n\nHot Network Questions\n\nDell suggests lowest of the low Max ram\n\nHow to avoid underflow in Plot?\n\nWhat\'s the correct translation of Galatians 5:17\n\nHow does ""safely"" function in ""...a daydream safely beyond human possibility""?\n\ndeclval<_Xp(&)()>()() - what does this mean in the below context?\n\nWrite Query to get \'x\' number of rows in SQL Server\n\nWho wrote the short story ""Quietly""?\n\nExploiting the potential of RAM in a computer with a large amount of it\n\nUS citizen, with a clean record, needs license for armored car with 3 inch cannon\n\nR5 Carbon Fiber Seat Stay Tire Rub Damage\n\nTeach beginner staff notation\n\nCombining every 3 lines together starting on the second line, and removing first column from second and third line being combined\n\nWhere in the Andean Road System was this picture taken? What are the white formations?\n\nShort story in which a scout on a colony ship learns there are no habitable worlds\n\nHow to properly align two numbered equations?\n\nHow to exactly find shift beween two functions?\n\nIs a naval blockade considered a de-jure or a de-facto declaration of war?\n\nHow did the OS/360 link editor achieve overlay structuring at linkage time without annotations in the source code?\n\nCryptic Acrostic 6: Spill the Tea\n\nChoosing SMD size for passive components\n\nDoes Pre-Print compromise anonymity for a later peer-review?\n\nIf a GPS displays the correct time, can I trust the calculated position? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', metadata={'id': 'web-search_1', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nHow to assign custom gradient to TensorFlow op with multiple inputs\n\nAsked 4 years, 5 months ago\n\nModified 3 years, 10 months ago\n\nI\'m trying to use TensorFlow\'s @tf.custom_gradient functionality to assign a custom gradient to a function with multiple inputs. I can put together a working setup for only one input, but not for two or more.\n\nI\'ve based my code on TensorFlow\'s custom_gradient documentation, which works just fine for one input, as in this example:\n\nimport tensorflow as tf import os # Suppress Tensorflow startup info os.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\' # Custom gradient decorator on a function, # as described in documentation @tf.custom_gradient def my_identity(x): # The custom gradient def grad(dy): return dy # Return the result AND the gradient return tf.identity(x), grad # Make a variable, run it through the custom op x = tf.get_variable(\'x\', initializer=1.) y = my_identity(x) # Calculate loss, make an optimizer, train the variable loss = tf.abs(y) opt = tf.train.GradientDescentOptimizer(learning_rate=0.001) train = opt.minimize(loss) # Start a TensorFlow session, initialize variables, train with tf.Session() as sess: sess.run(tf.global_variables_initializer()) sess.run(train)\n\nThis example runs silently, then closes. No issues, no errors. The variable optimizes as expected. However, in my application, I need to do such a calculation with multiple inputs, so something of this form:\n\n@tf.custom_gradient def my_identity(x, z): def grad(dy): return dy return tf.identity(x*z), grad\n\nRunning this in place of the example (and adding another variable input to the call of my_identify) results in the following error output. Best as I can tell, the last parts of the error are from the dynamic generation of the op -- the information format matches the C++ formatting required in the op establishment (though that\'s about all I know about it).\n\nTraceback (most recent call last): File ""testing.py"", line 27, in <module> train = opt.minimize(loss) File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 400, in minimize grad_loss=grad_loss) File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 519, in compute_gradients colocate_gradients_with_ops=colocate_gradients_with_ops) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 630, in gradients gate_gradients, aggregation_method, stop_gradients) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 821, in _GradientsHelper _VerifyGeneratedGradients(in_grads, op) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 323, in _VerifyGeneratedGradients ""inputs %d"" % (len(grads), op.node_def, len(op.inputs))) ValueError: Num gradients 2 generated for op name: ""IdentityN"" op: ""IdentityN"" input: ""Identity"" input: ""x/read"" input: ""y/read"" attr { key: ""T"" value { list { type: DT_FLOAT type: DT_FLOAT type: DT_FLOAT } } } attr { key: ""_gradient_op_type"" value { s: ""CustomGradient-9"" } } do not match num inputs 3\n\nBased on other custom gradient options, I surmised that the issue was a lack of supplied gradient for the second input argument. So, I changed my function to this:\n\n@tf.custom_gradient def my_identity(x, z): def grad(dy): return dy return tf.identity(x*z), grad, grad\n\nThis results in the following more familiar error:\n\nTraceback (most recent call last): File ""testing.py"", line 22, in <module> y = my_identity(x, z) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated return _graph_mode_decorator(f, *args, **kwargs) File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 132, in _graph_mode_decorator result, grad_fn = f(*args) ValueError: too many values to unpack (expected 2)\n\nThe @custom_gradient decorator is only identifying the last returned element as a gradient. So, I tried putting the two gradients into a tuple as (grad, grad) such that there would only be ""two"" outputs for the function. TensorFlow rejected this too, this time because it can\'t call a tuple like it would a Tensor -- entirely reasonable, in hindsight.\n\nI\'ve fussed around with the example some more, but to no avail. No matter what I try, I can\'t get the custom-defined gradient to deal with multiple inputs. I\'m hoping that somebody with more knowledge than I regarding custom ops and gradients will have a better idea on this -- thanks in advance for the help!\n\nImprove this question\n\nasked Jan 4, 2019 at 23:45\n\naedificatoriaedificatori\n\n11911 silver badge1111 bronze badges 1\n\nIs this still a problem?\n\n– gab Oct 23, 2019 at 20:03\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIf we use multiple variables as input, the number of gradients return from ""grad"" function should be equals to number of input variables, though we maybe don\'t care about some of them.\n\n@tf.custom_gradient def my_multiple(x,z): def grad(dy): # return two gradients, one for \'x\' and one for \'z\' return (dy*z, dy*x) return tf.identity(x*z), grad\n\nNote that the second output of ""my_multiple"" is a function, not a gradient tensor.\n\nedited Apr 29, 2019 at 12:07\n\nanswered Feb 28, 2019 at 1:48\n\n5633 bronze badges 2\n\nAs input to my_multiple you have (x,z).. but the gradient returns (dy*z, dy*x), where you have reversed the order. Is that okay for tensorflow? How does it know which term belongs where..?\n\n– zwep Dec 3, 2019 at 14:07\n\n@zwep the order is not reversed. The gradient of (x*z) with respect to x is z, and vice-versa.\n\n– wimi Dec 6, 2019 at 13:50\n\nI ran into a similar problem some time ago and I think the documentation is not very clear on this. In general, the code should be something like:\n\n@tf.custom_gradient def custom_operation(x, y, scope=\'custom_op\'): # define the gradient def grad(g): return g, g # define the forward pass (a multiplication, in this example) with tf.variable_scope(scope): forward_pass = x * y return forward_pass, grad\n\nIn practice, your internal grad function should return the gradient N times, where N is the number of argument that the custom_operation takes as input (apart from the scope). By using two inputs (x and y), the grad function must return the gradients twice (once for x and once for y). In general, you could also make the grad() function return g1 != g2 instead of g for both the inputs. So, in your example it becomes:\n\n@tf.custom_gradient def my_identity(x, z): def grad(dy): return dy, dy return tf.identity(x*z), grad\n\nedited Aug 7, 2019 at 13:04\n\nanswered Aug 6, 2019 at 15:59\n\n78211 gold badge1010 silver badges3434 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow-gradient or ask your own question.\n\nThe cofounder of Chef is cooking up a less painful DevOps (Ep. 584)\n\nImproving the developer experience in the energy sector\n\nStatement from SO: June 5, 2023 Moderator Action\n\nStarting the Prompt Design Site: A New Home in our Stack Exchange Neighborhood\n\nDoes the policy change for AI-generated content affect users who (want to)...\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n10 Tensorflow: How to write op with gradient in python?\n\n0 Tensorflow -- how can I process in numpy op outputs in py_func gradient?\n\n11 How to register a custom gradient for a operation composed of tf operations\n\n1 How do I define a gradient for a custom op working on complex tensors in tensorflow?\n\n1 Tensorflow op with two inputs, return one of the two and override gradient\n\n0 Using op inputs when defining custom gradients in TensorFlow\n\n7 How to provide custom gradient in TensorFlow\n\n0 Apply tensorflow gradients to specific inputs\n\n0 tf.custom_gradient with multiple inputs\n\n1 How excute custom gradient with tf.multiply?\n\nHot Network Questions\n\nDell suggests lowest of the low Max ram\n\nHow to avoid underflow in Plot?\n\nWhat\'s the correct translation of Galatians 5:17\n\nHow does ""safely"" function in ""...a daydream safely beyond human possibility""?\n\ndeclval<_Xp(&)()>()() - what does this mean in the below context?\n\nWrite Query to get \'x\' number of rows in SQL Server\n\nWho wrote the short story ""Quietly""?\n\nExploiting the potential of RAM in a computer with a large amount of it\n\nUS citizen, with a clean record, needs license for armored car with 3 inch cannon\n\nR5 Carbon Fiber Seat Stay Tire Rub Damage\n\nTeach beginner staff notation\n\nCombining every 3 lines together starting on the second line, and removing first column from second and third line being combined\n\nWhere in the Andean Road System was this picture taken? What are the white formations?\n\nShort story in which a scout on a colony ship learns there are no habitable worlds\n\nHow to properly align two numbered equations?\n\nHow to exactly find shift beween two functions?\n\nIs a naval blockade considered a de-jure or a de-facto declaration of war?\n\nHow did the OS/360 link editor achieve overlay structuring at linkage time without annotations in the source code?\n\nCryptic Acrostic 6: Spill the Tea\n\nChoosing SMD size for passive components\n\nDoes Pre-Print compromise anonymity for a later peer-review?\n\nIf a GPS displays the correct time, can I trust the calculated position? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', 'timestamp': '2023-06-28T09:23:42', 'title': 'python - How to assign custom gradient to TensorFlow op with multiple inputs - Stack Overflow', 'url': 'https://stackoverflow.com/questions/54047604/how-to-assign-custom-gradient-to-tensorflow-op-with-multiple-inputs'})], [Document(page_content='# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Decorator to overrides the gradient for a function.""""""\n\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import record\nfrom tensorflow.python.framework import composite_tensor_gradient\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import handle_data_util\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import op_selector\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops.unconnected_gradients import UnconnectedGradients\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util import tf_decorator\nfrom tensorflow.python.util import tf_inspect\nfrom tensorflow.python.util import variable_utils\nfrom tensorflow.python.util.tf_export import tf_export\n\n\nVAR_OP_TYPES = [\n    ""VariableV2"",\n    ""VarHandleOp"",\n]\n\n\n@tf_export(""custom_gradient"")\ndef custom_gradient(f=None):\n  """"""Decorator to define a function with a custom gradient.\n\n  This decorator allows fine grained control over the gradients of a sequence\n  for operations.  This may be useful for multiple reasons, including providing\n  a more efficient or numerically stable gradient for a sequence of operations.\n\n  For example, consider the following function that commonly occurs in the\n  computation of cross entropy and log likelihoods:\n\n  ```python\n  def log1pexp(x):\n    return tf.math.log(1 + tf.exp(x))\n  ```\n\n  Due to numerical instability, the gradient of this function evaluated at x=100\n  is NaN.  For example:\n\n  ```python\n  with tf.GradientTape() as tape:\n    tape.watch(x)\n    y=log1pexp(x)\n  dy_dx = tape.gradient(y, x) # Will be NaN when evaluated.\n  ```\n\n  The gradient expression can be analytically simplified to provide numerical\n  stability:\n\n  ```python\n  @tf.custom_gradient\n  def log1pexp(x):\n    e = tf.exp(x)\n    def grad(upstream):\n      return upstream * (1 - 1 / (1 + e))\n    return tf.math.log(1 + e), grad\n  ```\n\n  With this definition, the gradient `dy_dx` at `x = 100` will be correctly\n  evaluated as 1.0.\n\n  The variable `upstream` is defined as the upstream gradient. i.e. the gradient\n  from all the layers or functions originating from this layer. The above\n  example has no upstream functions, therefore `upstream = dy/dy = 1.0`.\n\n  Assume that `x_i` is `log1pexp` in the forward pass `x_1 = x_1(x_0)`,\n  `x_2 = x_2(x_1)`, ..., `x_i = x_i(x_i-1)`, ..., `x_n = x_n(x_n-1)`. By\n  chain rule we know that `dx_n/dx_0 = dx_n/dx_n-1 * dx_n-1/dx_n-2 * ... *\n  dx_i/dx_i-1 * ... * dx_1/dx_0`.\n\n  In this case the gradient of our current function defined as\n  `dx_i/dx_i-1 = (exp(x_i) / (1 + exp(x_i))) = (1 - 1 / (1 + exp(x_i)))`. The\n  upstream gradient `upstream` would be `dx_n/dx_n-1 * dx_n-1/dx_n-2 * ... *\n  dx_i+1/dx_i`. The upstream gradient multiplied by the current gradient is\n  then passed downstream.\n\n  In case the function takes multiple variables as input, the `grad`\n  function must also return  the same number of variables.\n  We take the function `z = x * y` as an example.\n\n  >>> @tf.custom_gradient\n  ... def bar(x, y):\n  ...   def grad(upstream):\n  ...     dz_dx = y\n  ...     dz_dy = x\n  ...     return upstream * dz_dx, upstream * dz_dy\n  ...   z = x * y\n  ...   return z, grad\n  >>> x = tf.constant(2.0, dtype=tf.float32)\n  >>> y = tf.constant(3.0, dtype=tf.float32)\n  >>> with tf.GradientTape(persistent=True) as tape:\n  ...   tape.watch(x)\n  ...   tape.watch(y)\n  ...   z = bar(x, y)\n  >>> z\n  <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n  >>> tape.gradient(z, x)\n  <tf.Tensor: shape=(), dtype=float32, numpy=3.0>\n  >>> tape.gradient(z, y)\n  <tf.Tensor: shape=(), dtype=float32, numpy=2.0>\n\n  Nesting custom gradients can lead to unintuitive results. The default\n  behavior does not correspond to n-th order derivatives. For example\n\n  ```python\n  @tf.custom_gradient\n  def op(x):\n    y = op1(x)\n    @tf.custom_gradient\n    def grad_fn(dy):\n      gdy = op2(x, y, dy)\n      def grad_grad_fn(ddy):  # Not the 2nd order gradient of op w.r.t. x.\n        return op3(x, y, dy, ddy)\n      return gdy, grad_grad_fn\n    return y, grad_fn\n  ```\n\n  The function `grad_grad_fn` will be calculating the first order gradient\n  of `grad_fn` with respect to `dy`, which is used to generate forward-mode\n  gradient graphs from backward-mode gradient graphs, but is not the same as\n  the second order gradient of `op` with respect to `x`.\n\n  Instead, wrap nested `@tf.custom_gradients` in another function:\n\n  ```python\n  @tf.custom_gradient\n  def op_with_fused_backprop(x):\n    y, x_grad = fused_op(x)\n    def first_order_gradient(dy):\n      @tf.custom_gradient\n      def first_order_custom(unused_x):\n        def second_order_and_transpose(ddy):\n          return second_order_for_x(...), gradient_wrt_dy(...)\n        return x_grad, second_order_and_transpose\n      return dy * first_order_custom(x)\n    return y, first_order_gradient\n  ```\n\n  Additional arguments to the inner `@tf.custom_gradient`-decorated function\n  control the expected return values of the innermost function.\n\n  The examples above illustrate how to specify custom gradients for functions\n  which do not read from variables. The following example uses variables, which\n  require special handling because they are effectively inputs of the forward\n  function.\n\n  >>> weights = tf.Variable(tf.ones([2]))  # Trainable variable weights\n  >>> @tf.custom_gradient\n  ... def linear_poly(x):\n  ...   # Creating polynomial\n  ...   poly = weights[1] * x + weights[0]\n  ...\n  ...   def grad_fn(dpoly, variables):\n  ...     # dy/dx = weights[1] and we need to left multiply dpoly\n  ...     grad_xs = dpoly * weights[1]  # Scalar gradient\n  ...\n  ...     grad_vars = []  # To store gradients of passed variables\n  ...     assert variables is not None\n  ...     assert len(variables) == 1\n  ...     assert variables[0] is weights\n  ...     # Manually computing dy/dweights\n  ...     dy_dw = dpoly * tf.stack([x ** 1, x ** 0])\n  ...     grad_vars.append(\n  ...         tf.reduce_sum(tf.reshape(dy_dw, [2, -1]), axis=1)\n  ...     )\n  ...     return grad_xs, grad_vars\n  ...   return poly, grad_fn\n  >>> x = tf.constant([1., 2., 3.])\n  >>> with tf.GradientTape(persistent=True) as tape:\n  ...   tape.watch(x)\n  ...   poly = linear_poly(x)\n  >>> poly # poly = x + 1\n  <tf.Tensor: shape=(3,),\n    dtype=float32,\n    numpy=array([2., 3., 4.], dtype=float32)>\n  >>> tape.gradient(poly, x)  # conventional scalar gradient dy/dx\n  <tf.Tensor: shape=(3,),\n    dtype=float32,\n    numpy=array([1., 1., 1.], dtype=float32)>\n  >>> tape.gradient(poly, weights)\n  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 3.], dtype=float32)>\n\n  Above example illustrates usage of trainable variable `weights`.\n  In the example, the inner `grad_fn` accepts an extra `variables` input\n  parameter and also returns an extra `grad_vars` output. That extra argument\n  is passed if the forward function reads any variables. You need to\n  compute the gradient w.r.t. each of those `variables` and output it as a list\n  of `grad_vars`. Note here that default value of `variables` is set to `None`\n  when no variables are used in the forward function.\n\n  It should be noted `tf.GradientTape` is still watching the forward pass of a\n  `tf.custom_gradient`, and will use the ops it watches. As a consequence,\n  calling `tf.function` while the tape is still watching leads\n  to a gradient graph being built. If an op is used in `tf.function` without\n  registered gradient, a `LookupError` will be raised.\n\n  Users can insert `tf.stop_gradient` to customize this behavior. This\n  is demonstrated in the example below. `tf.random.shuffle` does not have a\n  registered gradient. As a result `tf.stop_gradient` is used to avoid the\n  `LookupError`.\n\n  ```python\n  x = tf.constant([0.3, 0.5], dtype=tf.float32)\n\n  @tf.custom_gradient\n  def test_func_with_stop_grad(x):\n    @tf.function\n    def _inner_func():\n      # Avoid exception during the forward pass\n      return tf.stop_gradient(tf.random.shuffle(x))\n      # return tf.random.shuffle(x)  # This will raise\n\n    res = _inner_func()\n    def grad(upstream):\n      return upstream  # Arbitrarily defined custom gradient\n    return res, grad\n\n  with tf.GradientTape() as g:\n    g.watch(x)\n    res = test_func_with_stop_grad(x)\n\n  g.gradient(res, x)\n  ```\n\n  See also `tf.RegisterGradient` which registers a gradient function for a\n  primitive TensorFlow operation. `tf.custom_gradient` on the other hand allows\n  for fine grained control over the gradient computation of a sequence of\n  operations.\n\n  Note that if the decorated function uses `Variable`s, the enclosing variable\n  scope must be using\n  [ResourceVariables](https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables).\n\n  Args:\n    f: function `f(*x)` that returns a tuple `(y, grad_fn)` where: - `x` is a\n      sequence of (nested structures of) `Tensor` inputs to the function. - `y`\n      is a (nested structure of) `Tensor` outputs of applying TensorFlow\n      operations in `f` to `x`. - `grad_fn` is a function with the signature\n      `g(*grad_ys)` which returns a list of `Tensor`s the same size as\n      (flattened) `x` - the derivatives of `Tensor`s in `y` with respect to the\n      `Tensor`s in `x`.  `grad_ys` is a sequence of `Tensor`s the same size as\n      (flattened) `y` holding the initial value gradients for each `Tensor` in\n      `y`.  In a pure mathematical sense, a vector-argument vector-valued\n      function `f`\'s derivatives should be its Jacobian matrix `J`. Here we are\n      expressing the Jacobian `J` as a function `grad_fn` which defines how `J`\n      will transform a vector `grad_ys` when left-multiplied with it (`grad_ys *\n      J`, the vector-Jacobian product, or VJP). This functional representation\n      of a matrix is convenient to use for chain-rule calculation (in e.g. the\n      back-propagation algorithm).  If `f` uses `Variable`s (that are not part\n      of the inputs), i.e. through `get_variable`, then `grad_fn` should have\n      signature `g(*grad_ys, variables=None)`, where `variables` is a list of\n      the `Variable`s, and return a 2-tuple `(grad_xs, grad_vars)`, where\n      `grad_xs` is the same as above, and `grad_vars` is a `list<Tensor>` with\n      the derivatives of `Tensor`s in `y` with respect to the variables (that\n      is, grad_vars has one Tensor per variable in variables).\n\n  Returns:\n    A function `h(x)` which returns the same value as `f(x)[0]` and whose\n    gradient (as calculated by `tf.gradients`) is determined by `f(x)[1]`.\n  """"""\n\n  if f is None:\n    return lambda f: custom_gradient(f=f)\n\n  @Bind.decorator\n  def decorated(wrapped, args, kwargs):\n    """"""Decorated function with custom gradient.""""""\n    if context.executing_eagerly():\n      return _eager_mode_decorator(wrapped, args, kwargs)\n    else:\n      return _graph_mode_decorator(wrapped, args, kwargs)\n\n  return tf_decorator.make_decorator(f, decorated(f))  # pylint: disable=no-value-for-parameter\n\n\nclass Bind:\n  """"""When called evaluates `d(f, args, kwargs)` but supports binding `f`.\n\n  >>> @Bind.decorator\n  ... def my_decorator(f, args, kwargs):\n  ...   print(""my_decorator called with"", args, kwargs)\n  ...   return f(*args, **kwargs)\n\n  >>> class Foo:\n  ...   @my_decorator\n  ...   def bar(self, a, b, c):\n  ...     return a * b * c\n\n  >>> Foo.bar(None, 1, 2, c=3)\n  my_decorator called with (None, 1, 2) {\'c\': 3}\n  6\n\n  >>> foo = Foo()\n  >>> foo.bar(1, 2, c=3)\n  my_decorator called with (1, 2) {\'c\': 3}\n  6\n  """"""\n\n  @classmethod\n  def decorator(cls, d):\n    return lambda f: Bind(f, d)\n\n  def __init__(self, f, d):\n    self._f = f\n    self._d = d\n\n  def __get__(self, instance, owner):\n    if instance is not None:\n      f = self._f.__get__(instance, owner)\n      return tf_decorator.make_decorator(f, Bind(f, self._d))\n    else:\n      return self\n\n  def __call__(self, *a, **k):\n    return self._d(self._f, a, k)\n\n\ndef get_variable_by_name(var_name):\n  """"""Given a variable name, retrieves a handle on the tensorflow Variable.""""""\n  global_vars = ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)\n\n  def _filter_fn(item):\n    try:\n      return var_name == item.op.name\n    except AttributeError:\n      # Collection items without operation are ignored.\n      return False\n\n  candidate_vars = list(filter(_filter_fn, global_vars))\n\n  if len(candidate_vars) >= 1:\n    # Filter out non-trainable variables.\n    candidate_vars = [v for v in candidate_vars if v.trainable]\n  else:\n    raise ValueError(""Unsuccessful at finding variable {}."".format(var_name))\n\n  if len(candidate_vars) == 1:\n    return candidate_vars[0]\n  elif len(candidate_vars) > 1:\n    raise ValueError(\n        ""Unsuccessful at finding trainable variable {}. ""\n        ""Number of candidates: {}. ""\n        ""Candidates: {}"".format(var_name, len(candidate_vars), candidate_vars))\n  else:\n    # The variable is not trainable.\n    return None\n\n\ndef _get_dependent_variables(input_ops, output_ops):\n  """"""Finds variables involved in the subgraph between input_ops and output_ops.\n\n  Args:\n    input_ops: Flattened list of input ops\n    output_ops: Flattened list of output ops\n\n  Returns:\n    A list of variables\n  """"""\n\n  # avoids the edge-case when input_ops == output_ops.\n  output_ops = nest.map_structure(gen_array_ops.identity, output_ops)\n  inbetween_ops = op_selector.get_backward_walk_ops(\n      seed_ops=output_ops,\n      stop_at_ts=input_ops,\n      inclusive=False,\n      only_differentiable=True)\n  var_ops = (op for op in inbetween_ops if op.type in VAR_OP_TYPES)\n  var_names = (op.name for op in var_ops)\n  tf_vars = (get_variable_by_name(var_name) for var_name in var_names)\n  tf_vars = [v for v in tf_vars if v is not None]\n  return tf_vars\n\n\ndef generate_name():\n  return ""CustomGradient-%s"" % ops.uid()\n\n\ndef _graph_mode_decorator(f, args, kwargs):\n  """"""Implement custom gradient decorator for graph mode.""""""\n  # TODO(rsepassi): Add support for kwargs\n  if kwargs:\n    raise ValueError(\n        ""The custom_gradient decorator currently supports keywords ""\n        ""arguments only when eager execution is enabled."")\n  name = generate_name()\n  args = variable_utils.convert_variables_to_tensors(args)\n  args = nest.map_structure(ops.convert_to_tensor, args, expand_composites=True)\n\n  # Checking global and local variables attempts to ensure that no non-resource\n  # Variables are added to the graph.\n  current_var_scope = variable_scope.get_variable_scope()\n  before_vars = set([\n      v.ref() for v in current_var_scope.global_variables() +\n      current_var_scope.local_variables()\n  ])\n  with record.VariableWatcher() as variable_watcher:\n    result, grad_fn = f(*args)\n\n  flat_args = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(args))\n  flat_result = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(result))\n  flat_result_len = len(flat_result)\n\n  after_vars = set([\n      v.ref() for v in current_var_scope.global_variables() +\n      current_var_scope.local_variables()\n  ])\n  new_vars = after_vars - before_vars\n  new_vars_list = [v.deref() for v in new_vars]\n  for v in new_vars_list:\n    if not resource_variable_ops.is_resource_variable(v):\n      raise TypeError(\n          ""All variables used by a function wrapped with @custom_gradient must ""\n          ""be `ResourceVariable`s. Ensure that no `variable_scope` is created ""\n          ""with `use_resource=False`."")\n\n  # The variables that grad_fn needs to return gradients for are the set of\n  # variables used that are *not* part of the inputs.\n  variables_in_tape = frozenset([\n      v.ref() for v in variable_watcher.watched_variables()\n  ])\n\n  graphs = {getattr(o, ""graph"", None) for o in flat_result}\n  # Not all results may be tensors. However, we want to ensure all tensor\n  # outputs are from the same graph and get a list of captured inputs for\n  # variable search\n  graphs.discard(None)  # Discard non-graph outputs\n  if graphs:\n    if len(graphs) > 1:\n      raise ValueError(\n          ""All custom_gradient outputs should be from the same graph"")\n    output_graph = graphs.pop()\n    filtered_input_tensors = []\n    for i in flat_args:\n      if i.graph == output_graph:\n        filtered_input_tensors.append(i)\n  else:\n    filtered_input_tensors = flat_args\n\n  variables_in_subgraph = frozenset([\n      v.ref() for v in _get_dependent_variables(\n          input_ops=filtered_input_tensors, output_ops=flat_result)\n  ])\n  variables = sorted(\n      [v.deref() for v in variables_in_subgraph.union(variables_in_tape)],\n      key=lambda v: v.name)\n\n  grad_argspec = tf_inspect.getfullargspec(grad_fn)\n  variables_in_signature = (""variables"" in grad_argspec.args or\n                            ""variables"" in grad_argspec.kwonlyargs or\n                            grad_argspec.varkw)\n  if variables and not variables_in_signature:\n    raise TypeError(\n        ""@tf.custom_gradient grad_fn must accept keyword argument \'variables\', ""\n        ""since function uses variables: {}"".format(variables))\n  if variables_in_signature and not variables:\n    # User seems to intend to use variables but none were captured.\n    logging.vlog(\n        1, ""@custom_gradient grad_fn has \'variables\' in signature, ""\n        ""but no ResourceVariables were used on the forward pass."")\n\n  all_tensors = flat_result + flat_args + variables\n\n  def tape_grad_fn(*result_grad_components):\n    """"""Custom grad fn wrapper.""""""\n    result_grads = composite_tensor_gradient.replace_flat_tensors_for_gradients(\n        nest.flatten(result), result_grad_components[:flat_result_len])\n    if not isinstance(result_grads, (list, tuple)):\n      result_grads = [result_grads]\n\n    if variables:\n      input_grads, variable_grads = grad_fn(*result_grads, variables=variables)\n      if len(variable_grads) != len(variables):\n        raise ValueError(""Must return gradient for each variable from ""\n                         ""@custom_gradient grad_fn."")\n    else:\n      input_grads = grad_fn(*result_grads)\n      variable_grads = []\n\n    # Need to return one value per input to the IdentityN, so pad the\n    # gradients of the inputs of the custom_gradient function with the\n    # gradients of the outputs as well.\n    input_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(\n        nest.flatten(input_grads))\n    return ([None] * flat_result_len) + input_grads + variable_grads\n\n  @ops.RegisterGradient(name)\n  def internal_grad_fn(unused_op, *result_grads):  # pylint: disable=unused-variable\n    """"""Custom grad fn wrapper.""""""\n    return tape_grad_fn(*result_grads)\n\n  original_tensors = all_tensors\n  with ops.get_default_graph().gradient_override_map({""IdentityN"": name}):\n    all_tensors = array_ops.identity_n(all_tensors)\n\n  original_tensors = [ops.convert_to_tensor(x) for x in original_tensors]\n\n  # Propagate handle data for happier shape inference for resource variables.\n  for i, t in enumerate(original_tensors):\n    if t.dtype == dtypes.resource and hasattr(t, ""_handle_data""):\n      all_tensors[i]._handle_data = t._handle_data  # pylint: disable=protected-access\n  record.record_operation(\n      f.__name__, all_tensors, original_tensors, tape_grad_fn)\n  for ot, t in zip(original_tensors, all_tensors):\n    handle_data_util.copy_handle_data(ot, t)\n  flat_result = composite_tensor_gradient.replace_flat_tensors_for_gradients(\n      nest.flatten(result), all_tensors[:flat_result_len])\n  return nest.pack_sequence_as(result, flat_result)\n\n\ndef _eager_mode_decorator(f, args, kwargs):\n  """"""Implement custom gradient decorator for eager mode.""""""\n  with record.VariableWatcher() as variable_watcher:\n    result, grad_fn = f(*args, **kwargs)\n  flat_args = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(args))\n  flat_kwargs = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(kwargs))\n  all_inputs = flat_args + flat_kwargs\n  # The variables that grad_fn needs to return gradients for are the set of\n  # variables used that are *not* part of the inputs.\n  variables = [\n      v.deref()  # pylint: disable=g-complex-comprehension\n      for v in set(v.ref() for v in variable_watcher.watched_variables())\n      if all(v.deref() is not i for i in all_inputs)\n  ]\n  grad_argspec = tf_inspect.getfullargspec(grad_fn)\n  if (variables and (""variables"" not in grad_argspec.args) and\n      (""variables"" not in grad_argspec.kwonlyargs) and\n      not grad_argspec.varkw):\n    raise TypeError(\n        ""@tf.custom_gradient grad_fn must accept keyword argument \'variables\', ""\n        ""since function uses variables: {}"".format(variables))\n  flat_result = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(result))\n  # TODO(apassos) consider removing the identity below.\n  flat_result = [gen_array_ops.identity(x) for x in flat_result]\n\n  input_tensors = [\n      ops.convert_to_tensor(x) for x in flat_args + list(variables)]\n\n  recorded_inputs = input_tensors\n  arg_count = len(flat_args)\n\n  def actual_grad_fn(*result_grad_components):\n    """"""Custom grad fn wrapper.""""""\n    result_grads = composite_tensor_gradient.replace_flat_tensors_for_gradients(\n        nest.flatten(result), result_grad_components)\n    if not isinstance(result_grads, (list, tuple)):\n      result_grads = [result_grads]\n\n    if variables:\n      input_grads, variable_grads = grad_fn(*result_grads, variables=variables)\n      if len(variable_grads) != len(variables):\n        raise ValueError(""Must return gradient for each variable from ""\n                         ""@custom_gradient grad_fn."")\n    else:\n      input_grads = grad_fn(*result_grads)\n      variable_grads = []\n    flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(\n        nest.flatten(input_grads))\n    if len(flat_grads) != arg_count:\n      raise ValueError(\n          f""custom_gradient function expected to return {arg_count} ""\n          f""gradients, but returned {len(flat_grads)} instead."")\n    return flat_grads + variable_grads\n\n  record.record_operation(f.__name__, flat_result, recorded_inputs,\n                          actual_grad_fn)\n  flat_result = composite_tensor_gradient.replace_flat_tensors_for_gradients(\n      nest.flatten(result), flat_result)\n  return nest.pack_sequence_as(result, flat_result)\n\n\n@tf_export(""recompute_grad"")\ndef recompute_grad(f):\n  """"""Defines a function as a recompute-checkpoint for the tape auto-diff.\n\n  Tape checkpointing is a technique to reduce the memory consumption of the\n  auto-diff tape:\n\n  - Without tape checkpointing operations and intermediate values are\n  recorded to the tape for use in the backward pass.\n\n  - With tape checkpointing, only the function call and its inputs are\n  recorded. During back-propagation the `recompute_grad` custom gradient\n  (`tf.custom_gradient`) recomputes the function under a localized Tape object.\n  This recomputation of the function during backpropagation performs redundant\n  calculation, but reduces the overall memory usage of the Tape.\n\n  >>> y = tf.Variable(1.0)\n\n  >>> def my_function(x):\n  ...   tf.print(\'running\')\n  ...   z = x*y\n  ...   return z\n\n  >>> my_function_recompute = tf.recompute_grad(my_function)\n\n  >>> with tf.GradientTape() as tape:\n  ...   r = tf.constant(1.0)\n  ...   for i in range(4):\n  ...     r = my_function_recompute(r)\n  running\n  running\n  running\n  running\n\n  >>> grad = tape.gradient(r, [y])\n  running\n  running\n  running\n  running\n\n  Without `recompute_grad`, the tape contains all intermitate steps, and no\n  recomputation is performed.\n\n  >>> with tf.GradientTape() as tape:\n  ...   r = tf.constant(1.0)\n  ...   for i in range(4):\n  ...     r = my_function(r)\n  running\n  running\n  running\n  running\n\n  >>> grad = tape.gradient(r, [y])\n\n\n  If `f` was a `tf.keras` `Model` or `Layer` object, methods and attributes\n  such as `f.variables` are not available on the returned function `g`.\n  Either keep a reference of `f` , or use `g.__wrapped__` for accessing\n  these variables and methods.\n\n\n  >>> def print_running_and_return(x):\n  ...   tf.print(""running"")\n  ...   return x\n\n  >>> model = tf.keras.Sequential([\n  ...   tf.keras.layers.Lambda(print_running_and_return),\n  ...   tf.keras.layers.Dense(2)\n  ... ])\n\n  >>> model_recompute = tf.recompute_grad(model)\n\n  >>> with tf.GradientTape(persistent=True) as tape:\n  ...   r = tf.constant([[1,2]])\n  ...   for i in range(4):\n  ...     r = model_recompute(r)\n  running\n  running\n  running\n  running\n\n  >>> grad = tape.gradient(r, model.variables)\n  running\n  running\n  running\n  running\n\n  Alternatively, use the `__wrapped__` attribute to access the original\n  model object.\n\n  >>> grad = tape.gradient(r, model_recompute.__wrapped__.variables)\n  running\n  running\n  running\n  running\n\n\n  Args:\n    f: function `f(*x)` that returns a `Tensor` or sequence of `Tensor` outputs.\n\n  Returns:\n    A function `g` wrapping `f` that defines a custom gradient, which recomputes\n    `f` on the backwards pass of a gradient call.\n  """"""\n  # TODO(cdfreeman) Add is_recomputing functionality from graph mode version\n\n  @custom_gradient\n  def inner(*args, **kwargs):\n    """"""Inner function closure for calculating gradients.""""""\n    current_var_scope = variable_scope.get_variable_scope()\n    with record.stop_recording():\n      result = f(*args, **kwargs)\n\n    def grad_wrapper(*wrapper_args, variables=None):\n      """"""Wrapper function to accomodate lack of kwargs in graph mode custom_gradient.""""""\n\n      @custom_gradient\n      def inner_recompute_grad(*dresult):\n        """"""Nested custom gradient function for computing grads in reverse and forward mode autodiff.""""""\n        # Gradient calculation for reverse mode autodiff.\n        with backprop.GradientTape() as t:\n          id_args = nest.map_structure(gen_array_ops.identity, args)\n          # Tuple `dresult` should contain at least one tensor.\n          assert len(dresult) >= 1\n\n          if not context.executing_eagerly():\n            # XLA doesn\'t respect `tf.control_dependencies`. The code block\n            # below manually adds a data dependency to `dresult` to ensure\n            # recomputation of `f(*args, **kwargs)` happens after `dresult`.\n\n            # This works even if `dresult[0]` is a size 0 tensor as reduce_max\n            # of a size 0 tensor returns -inf. Use reshape here to avoid reading\n            # the entire `dresult[0]`.\n            elem = math_ops.reduce_max(array_ops.reshape(dresult[0], [-1])[:1])\n            # Cast elem to bool in case elem is NaN.\n            elem_bool = math_ops.cast(elem, dtypes.bool)\n            dresult_dep = array_ops.where_v2(\n                elem_bool == elem_bool, 0., float(""nan""))  # pylint: disable=comparison-with-itself\n            id_args = nest.map_structure(\n                lambda x: x + math_ops.cast(dresult_dep, x.dtype), id_args)\n\n          t.watch(id_args)\n          if variables is not None:\n            t.watch(variables)\n          with variable_scope.variable_scope(current_var_scope):\n            recomputed_result = f(*id_args, **kwargs)\n        kw_vars = []\n        if variables is not None:\n          kw_vars = list(variables)\n        grads = t.gradient(\n            recomputed_result,\n            list(id_args) + kw_vars,\n            output_gradients=dresult,\n            unconnected_gradients=UnconnectedGradients.ZERO)\n\n        def transpose(*t_args, **t_kwargs):\n          """"""Gradient function calculation for forward mode autodiff.""""""\n          # Just throw an error since gradients / activations are not stored on\n          # tape for recompute.\n          raise NotImplementedError(\n              ""recompute_grad tried to transpose grad of {}. ""\n              ""Consider not using recompute_grad in forward mode""\n              ""autodiff"".format(f.__name__))\n\n        return (grads[:len(id_args)], grads[len(id_args):]), transpose\n\n      return inner_recompute_grad(*wrapper_args)\n\n    return result, grad_wrapper\n\n  return tf_decorator.make_decorator(f, inner)\n\n\n@tf_export(""grad_pass_through"")\ndef grad_pass_through(f):\n  """"""Creates a grad-pass-through op with the forward behavior provided in f.\n\n  Use this function to wrap any op, maintaining its behavior in the forward\n  pass, but replacing the original op in the backward graph with an identity.\n  For example:\n\n  ```python\n  x = tf.Variable(1.0, name=""x"")\n  z = tf.Variable(3.0, name=""z"")\n\n  with tf.GradientTape() as tape:\n    # y will evaluate to 9.0\n    y = tf.grad_pass_through(x.assign)(z**2)\n  # grads will evaluate to 6.0\n  grads = tape.gradient(y, z)\n  ```\n\n  Another example is a \'differentiable\' moving average approximation, where\n  gradients are allowed to flow into the last value fed to the moving average,\n  but the moving average is still used for the forward pass:\n\n  ```python\n  x = ... # Some scalar value\n  # A moving average object, we don\'t need to know how this is implemented\n  moving_average = MovingAverage()\n  with backprop.GradientTape() as tape:\n    # mavg_x will evaluate to the current running average value\n    mavg_x = tf.grad_pass_through(moving_average)(x)\n  grads = tape.gradient(mavg_x, x) # grads will evaluate to 1.0\n  ```\n\n  Args:\n    f: function `f(*x)` that returns a `Tensor` or nested structure of `Tensor`\n      outputs.\n\n  Returns:\n    A function `h(x)` which returns the same values as `f(x)` and whose\n    gradients are the same as those of an identity function.\n  """"""\n  @custom_gradient\n  def _grad_pass_through_op(*args, **kwargs):\n    def grad(*args, **kwargs):\n      variables = kwargs.get(""variables"")\n      if variables is not None:\n        # Variables involved in the wrapped op will not receive gradients.\n        return args, [None] * len(variables)\n      return args\n    return f(*args, **kwargs), grad\n  return tf_decorator.make_decorator(f, _grad_pass_through_op)', metadata={'id': 'web-search_3', 'snippet': '# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Decorator to overrides the gradient for a function.""""""\n\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import record\nfrom tensorflow.python.framework import composite_tensor_gradient\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import handle_data_util\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import op_selector\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops.unconnected_gradients import UnconnectedGradients\nfrom tensorflow.python.platform import tf_logging as logging\nfrom tensorflow.python.util import nest\nfrom tensorflow.python.util import tf_decorator\nfrom tensorflow.python.util import tf_inspect\nfrom tensorflow.python.util import variable_utils\nfrom tensorflow.python.util.tf_export import tf_export\n\n\nVAR_OP_TYPES = [\n    ""VariableV2"",\n    ""VarHandleOp"",\n]\n\n\n@tf_export(""custom_gradient"")\ndef custom_gradient(f=None):\n  """"""Decorator to define a function with a custom gradient.\n\n  This decorator allows fine grained control over the gradients of a sequence\n  for operations.  This may be useful for multiple reasons, including providing\n  a more efficient or numerically stable gradient for a sequence of operations.\n\n  For example, consider the following function that commonly occurs in the\n  computation of cross entropy and log likelihoods:\n\n  ```python\n  def log1pexp(x):\n    return tf.math.log(1 + tf.exp(x))\n  ```\n\n  Due to numerical instability, the gradient of this function evaluated at x=100\n  is NaN.  For example:\n\n  ```python\n  with tf.GradientTape() as tape:\n    tape.watch(x)\n    y=log1pexp(x)\n  dy_dx = tape.gradient(y, x) # Will be NaN when evaluated.\n  ```\n\n  The gradient expression can be analytically simplified to provide numerical\n  stability:\n\n  ```python\n  @tf.custom_gradient\n  def log1pexp(x):\n    e = tf.exp(x)\n    def grad(upstream):\n      return upstream * (1 - 1 / (1 + e))\n    return tf.math.log(1 + e), grad\n  ```\n\n  With this definition, the gradient `dy_dx` at `x = 100` will be correctly\n  evaluated as 1.0.\n\n  The variable `upstream` is defined as the upstream gradient. i.e. the gradient\n  from all the layers or functions originating from this layer. The above\n  example has no upstream functions, therefore `upstream = dy/dy = 1.0`.\n\n  Assume that `x_i` is `log1pexp` in the forward pass `x_1 = x_1(x_0)`,\n  `x_2 = x_2(x_1)`, ..., `x_i = x_i(x_i-1)`, ..., `x_n = x_n(x_n-1)`. By\n  chain rule we know that `dx_n/dx_0 = dx_n/dx_n-1 * dx_n-1/dx_n-2 * ... *\n  dx_i/dx_i-1 * ... * dx_1/dx_0`.\n\n  In this case the gradient of our current function defined as\n  `dx_i/dx_i-1 = (exp(x_i) / (1 + exp(x_i))) = (1 - 1 / (1 + exp(x_i)))`. The\n  upstream gradient `upstream` would be `dx_n/dx_n-1 * dx_n-1/dx_n-2 * ... *\n  dx_i+1/dx_i`. The upstream gradient multiplied by the current gradient is\n  then passed downstream.\n\n  In case the function takes multiple variables as input, the `grad`\n  function must also return  the same number of variables.\n  We take the function `z = x * y` as an example.\n\n  >>> @tf.custom_gradient\n  ... def bar(x, y):\n  ...   def grad(upstream):\n  ...     dz_dx = y\n  ...     dz_dy = x\n  ...     return upstream * dz_dx, upstream * dz_dy\n  ...   z = x * y\n  ...   return z, grad\n  >>> x = tf.constant(2.0, dtype=tf.float32)\n  >>> y = tf.constant(3.0, dtype=tf.float32)\n  >>> with tf.GradientTape(persistent=True) as tape:\n  ...   tape.watch(x)\n  ...   tape.watch(y)\n  ...   z = bar(x, y)\n  >>> z\n  <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n  >>> tape.gradient(z, x)\n  <tf.Tensor: shape=(), dtype=float32, numpy=3.0>\n  >>> tape.gradient(z, y)\n  <tf.Tensor: shape=(), dtype=float32, numpy=2.0>\n\n  Nesting custom gradients can lead to unintuitive results. The default\n  behavior does not correspond to n-th order derivatives. For example\n\n  ```python\n  @tf.custom_gradient\n  def op(x):\n    y = op1(x)\n    @tf.custom_gradient\n    def grad_fn(dy):\n      gdy = op2(x, y, dy)\n      def grad_grad_fn(ddy):  # Not the 2nd order gradient of op w.r.t. x.\n        return op3(x, y, dy, ddy)\n      return gdy, grad_grad_fn\n    return y, grad_fn\n  ```\n\n  The function `grad_grad_fn` will be calculating the first order gradient\n  of `grad_fn` with respect to `dy`, which is used to generate forward-mode\n  gradient graphs from backward-mode gradient graphs, but is not the same as\n  the second order gradient of `op` with respect to `x`.\n\n  Instead, wrap nested `@tf.custom_gradients` in another function:\n\n  ```python\n  @tf.custom_gradient\n  def op_with_fused_backprop(x):\n    y, x_grad = fused_op(x)\n    def first_order_gradient(dy):\n      @tf.custom_gradient\n      def first_order_custom(unused_x):\n        def second_order_and_transpose(ddy):\n          return second_order_for_x(...), gradient_wrt_dy(...)\n        return x_grad, second_order_and_transpose\n      return dy * first_order_custom(x)\n    return y, first_order_gradient\n  ```\n\n  Additional arguments to the inner `@tf.custom_gradient`-decorated function\n  control the expected return values of the innermost function.\n\n  The examples above illustrate how to specify custom gradients for functions\n  which do not read from variables. The following example uses variables, which\n  require special handling because they are effectively inputs of the forward\n  function.\n\n  >>> weights = tf.Variable(tf.ones([2]))  # Trainable variable weights\n  >>> @tf.custom_gradient\n  ... def linear_poly(x):\n  ...   # Creating polynomial\n  ...   poly = weights[1] * x + weights[0]\n  ...\n  ...   def grad_fn(dpoly, variables):\n  ...     # dy/dx = weights[1] and we need to left multiply dpoly\n  ...     grad_xs = dpoly * weights[1]  # Scalar gradient\n  ...\n  ...     grad_vars = []  # To store gradients of passed variables\n  ...     assert variables is not None\n  ...     assert len(variables) == 1\n  ...     assert variables[0] is weights\n  ...     # Manually computing dy/dweights\n  ...     dy_dw = dpoly * tf.stack([x ** 1, x ** 0])\n  ...     grad_vars.append(\n  ...         tf.reduce_sum(tf.reshape(dy_dw, [2, -1]), axis=1)\n  ...     )\n  ...     return grad_xs, grad_vars\n  ...   return poly, grad_fn\n  >>> x = tf.constant([1., 2., 3.])\n  >>> with tf.GradientTape(persistent=True) as tape:\n  ...   tape.watch(x)\n  ...   poly = linear_poly(x)\n  >>> poly # poly = x + 1\n  <tf.Tensor: shape=(3,),\n    dtype=float32,\n    numpy=array([2., 3., 4.], dtype=float32)>\n  >>> tape.gradient(poly, x)  # conventional scalar gradient dy/dx\n  <tf.Tensor: shape=(3,),\n    dtype=float32,\n    numpy=array([1., 1., 1.], dtype=float32)>\n  >>> tape.gradient(poly, weights)\n  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 3.], dtype=float32)>\n\n  Above example illustrates usage of trainable variable `weights`.\n  In the example, the inner `grad_fn` accepts an extra `variables` input\n  parameter and also returns an extra `grad_vars` output. That extra argument\n  is passed if the forward function reads any variables. You need to\n  compute the gradient w.r.t. each of those `variables` and output it as a list\n  of `grad_vars`. Note here that default value of `variables` is set to `None`\n  when no variables are used in the forward function.\n\n  It should be noted `tf.GradientTape` is still watching the forward pass of a\n  `tf.custom_gradient`, and will use the ops it watches. As a consequence,\n  calling `tf.function` while the tape is still watching leads\n  to a gradient graph being built. If an op is used in `tf.function` without\n  registered gradient, a `LookupError` will be raised.\n\n  Users can insert `tf.stop_gradient` to customize this behavior. This\n  is demonstrated in the example below. `tf.random.shuffle` does not have a\n  registered gradient. As a result `tf.stop_gradient` is used to avoid the\n  `LookupError`.\n\n  ```python\n  x = tf.constant([0.3, 0.5], dtype=tf.float32)\n\n  @tf.custom_gradient\n  def test_func_with_stop_grad(x):\n    @tf.function\n    def _inner_func():\n      # Avoid exception during the forward pass\n      return tf.stop_gradient(tf.random.shuffle(x))\n      # return tf.random.shuffle(x)  # This will raise\n\n    res = _inner_func()\n    def grad(upstream):\n      return upstream  # Arbitrarily defined custom gradient\n    return res, grad\n\n  with tf.GradientTape() as g:\n    g.watch(x)\n    res = test_func_with_stop_grad(x)\n\n  g.gradient(res, x)\n  ```\n\n  See also `tf.RegisterGradient` which registers a gradient function for a\n  primitive TensorFlow operation. `tf.custom_gradient` on the other hand allows\n  for fine grained control over the gradient computation of a sequence of\n  operations.\n\n  Note that if the decorated function uses `Variable`s, the enclosing variable\n  scope must be using\n  [ResourceVariables](https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables).\n\n  Args:\n    f: function `f(*x)` that returns a tuple `(y, grad_fn)` where: - `x` is a\n      sequence of (nested structures of) `Tensor` inputs to the function. - `y`\n      is a (nested structure of) `Tensor` outputs of applying TensorFlow\n      operations in `f` to `x`. - `grad_fn` is a function with the signature\n      `g(*grad_ys)` which returns a list of `Tensor`s the same size as\n      (flattened) `x` - the derivatives of `Tensor`s in `y` with respect to the\n      `Tensor`s in `x`.  `grad_ys` is a sequence of `Tensor`s the same size as\n      (flattened) `y` holding the initial value gradients for each `Tensor` in\n      `y`.  In a pure mathematical sense, a vector-argument vector-valued\n      function `f`\'s derivatives should be its Jacobian matrix `J`. Here we are\n      expressing the Jacobian `J` as a function `grad_fn` which defines how `J`\n      will transform a vector `grad_ys` when left-multiplied with it (`grad_ys *\n      J`, the vector-Jacobian product, or VJP). This functional representation\n      of a matrix is convenient to use for chain-rule calculation (in e.g. the\n      back-propagation algorithm).  If `f` uses `Variable`s (that are not part\n      of the inputs), i.e. through `get_variable`, then `grad_fn` should have\n      signature `g(*grad_ys, variables=None)`, where `variables` is a list of\n      the `Variable`s, and return a 2-tuple `(grad_xs, grad_vars)`, where\n      `grad_xs` is the same as above, and `grad_vars` is a `list<Tensor>` with\n      the derivatives of `Tensor`s in `y` with respect to the variables (that\n      is, grad_vars has one Tensor per variable in variables).\n\n  Returns:\n    A function `h(x)` which returns the same value as `f(x)[0]` and whose\n    gradient (as calculated by `tf.gradients`) is determined by `f(x)[1]`.\n  """"""\n\n  if f is None:\n    return lambda f: custom_gradient(f=f)\n\n  @Bind.decorator\n  def decorated(wrapped, args, kwargs):\n    """"""Decorated function with custom gradient.""""""\n    if context.executing_eagerly():\n      return _eager_mode_decorator(wrapped, args, kwargs)\n    else:\n      return _graph_mode_decorator(wrapped, args, kwargs)\n\n  return tf_decorator.make_decorator(f, decorated(f))  # pylint: disable=no-value-for-parameter\n\n\nclass Bind:\n  """"""When called evaluates `d(f, args, kwargs)` but supports binding `f`.\n\n  >>> @Bind.decorator\n  ... def my_decorator(f, args, kwargs):\n  ...   print(""my_decorator called with"", args, kwargs)\n  ...   return f(*args, **kwargs)\n\n  >>> class Foo:\n  ...   @my_decorator\n  ...   def bar(self, a, b, c):\n  ...     return a * b * c\n\n  >>> Foo.bar(None, 1, 2, c=3)\n  my_decorator called with (None, 1, 2) {\'c\': 3}\n  6\n\n  >>> foo = Foo()\n  >>> foo.bar(1, 2, c=3)\n  my_decorator called with (1, 2) {\'c\': 3}\n  6\n  """"""\n\n  @classmethod\n  def decorator(cls, d):\n    return lambda f: Bind(f, d)\n\n  def __init__(self, f, d):\n    self._f = f\n    self._d = d\n\n  def __get__(self, instance, owner):\n    if instance is not None:\n      f = self._f.__get__(instance, owner)\n      return tf_decorator.make_decorator(f, Bind(f, self._d))\n    else:\n      return self\n\n  def __call__(self, *a, **k):\n    return self._d(self._f, a, k)\n\n\ndef get_variable_by_name(var_name):\n  """"""Given a variable name, retrieves a handle on the tensorflow Variable.""""""\n  global_vars = ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)\n\n  def _filter_fn(item):\n    try:\n      return var_name == item.op.name\n    except AttributeError:\n      # Collection items without operation are ignored.\n      return False\n\n  candidate_vars = list(filter(_filter_fn, global_vars))\n\n  if len(candidate_vars) >= 1:\n    # Filter out non-trainable variables.\n    candidate_vars = [v for v in candidate_vars if v.trainable]\n  else:\n    raise ValueError(""Unsuccessful at finding variable {}."".format(var_name))\n\n  if len(candidate_vars) == 1:\n    return candidate_vars[0]\n  elif len(candidate_vars) > 1:\n    raise ValueError(\n        ""Unsuccessful at finding trainable variable {}. ""\n        ""Number of candidates: {}. ""\n        ""Candidates: {}"".format(var_name, len(candidate_vars), candidate_vars))\n  else:\n    # The variable is not trainable.\n    return None\n\n\ndef _get_dependent_variables(input_ops, output_ops):\n  """"""Finds variables involved in the subgraph between input_ops and output_ops.\n\n  Args:\n    input_ops: Flattened list of input ops\n    output_ops: Flattened list of output ops\n\n  Returns:\n    A list of variables\n  """"""\n\n  # avoids the edge-case when input_ops == output_ops.\n  output_ops = nest.map_structure(gen_array_ops.identity, output_ops)\n  inbetween_ops = op_selector.get_backward_walk_ops(\n      seed_ops=output_ops,\n      stop_at_ts=input_ops,\n      inclusive=False,\n      only_differentiable=True)\n  var_ops = (op for op in inbetween_ops if op.type in VAR_OP_TYPES)\n  var_names = (op.name for op in var_ops)\n  tf_vars = (get_variable_by_name(var_name) for var_name in var_names)\n  tf_vars = [v for v in tf_vars if v is not None]\n  return tf_vars\n\n\ndef generate_name():\n  return ""CustomGradient-%s"" % ops.uid()\n\n\ndef _graph_mode_decorator(f, args, kwargs):\n  """"""Implement custom gradient decorator for graph mode.""""""\n  # TODO(rsepassi): Add support for kwargs\n  if kwargs:\n    raise ValueError(\n        ""The custom_gradient decorator currently supports keywords ""\n        ""arguments only when eager execution is enabled."")\n  name = generate_name()\n  args = variable_utils.convert_variables_to_tensors(args)\n  args = nest.map_structure(ops.convert_to_tensor, args, expand_composites=True)\n\n  # Checking global and local variables attempts to ensure that no non-resource\n  # Variables are added to the graph.\n  current_var_scope = variable_scope.get_variable_scope()\n  before_vars = set([\n      v.ref() for v in current_var_scope.global_variables() +\n      current_var_scope.local_variables()\n  ])\n  with record.VariableWatcher() as variable_watcher:\n    result, grad_fn = f(*args)\n\n  flat_args = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(args))\n  flat_result = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(result))\n  flat_result_len = len(flat_result)\n\n  after_vars = set([\n      v.ref() for v in current_var_scope.global_variables() +\n      current_var_scope.local_variables()\n  ])\n  new_vars = after_vars - before_vars\n  new_vars_list = [v.deref() for v in new_vars]\n  for v in new_vars_list:\n    if not resource_variable_ops.is_resource_variable(v):\n      raise TypeError(\n          ""All variables used by a function wrapped with @custom_gradient must ""\n          ""be `ResourceVariable`s. Ensure that no `variable_scope` is created ""\n          ""with `use_resource=False`."")\n\n  # The variables that grad_fn needs to return gradients for are the set of\n  # variables used that are *not* part of the inputs.\n  variables_in_tape = frozenset([\n      v.ref() for v in variable_watcher.watched_variables()\n  ])\n\n  graphs = {getattr(o, ""graph"", None) for o in flat_result}\n  # Not all results may be tensors. However, we want to ensure all tensor\n  # outputs are from the same graph and get a list of captured inputs for\n  # variable search\n  graphs.discard(None)  # Discard non-graph outputs\n  if graphs:\n    if len(graphs) > 1:\n      raise ValueError(\n          ""All custom_gradient outputs should be from the same graph"")\n    output_graph = graphs.pop()\n    filtered_input_tensors = []\n    for i in flat_args:\n      if i.graph == output_graph:\n        filtered_input_tensors.append(i)\n  else:\n    filtered_input_tensors = flat_args\n\n  variables_in_subgraph = frozenset([\n      v.ref() for v in _get_dependent_variables(\n          input_ops=filtered_input_tensors, output_ops=flat_result)\n  ])\n  variables = sorted(\n      [v.deref() for v in variables_in_subgraph.union(variables_in_tape)],\n      key=lambda v: v.name)\n\n  grad_argspec = tf_inspect.getfullargspec(grad_fn)\n  variables_in_signature = (""variables"" in grad_argspec.args or\n                            ""variables"" in grad_argspec.kwonlyargs or\n                            grad_argspec.varkw)\n  if variables and not variables_in_signature:\n    raise TypeError(\n        ""@tf.custom_gradient grad_fn must accept keyword argument \'variables\', ""\n        ""since function uses variables: {}"".format(variables))\n  if variables_in_signature and not variables:\n    # User seems to intend to use variables but none were captured.\n    logging.vlog(\n        1, ""@custom_gradient grad_fn has \'variables\' in signature, ""\n        ""but no ResourceVariables were used on the forward pass."")\n\n  all_tensors = flat_result + flat_args + variables\n\n  def tape_grad_fn(*result_grad_components):\n    """"""Custom grad fn wrapper.""""""\n    result_grads = composite_tensor_gradient.replace_flat_tensors_for_gradients(\n        nest.flatten(result), result_grad_components[:flat_result_len])\n    if not isinstance(result_grads, (list, tuple)):\n      result_grads = [result_grads]\n\n    if variables:\n      input_grads, variable_grads = grad_fn(*result_grads, variables=variables)\n      if len(variable_grads) != len(variables):\n        raise ValueError(""Must return gradient for each variable from ""\n                         ""@custom_gradient grad_fn."")\n    else:\n      input_grads = grad_fn(*result_grads)\n      variable_grads = []\n\n    # Need to return one value per input to the IdentityN, so pad the\n    # gradients of the inputs of the custom_gradient function with the\n    # gradients of the outputs as well.\n    input_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(\n        nest.flatten(input_grads))\n    return ([None] * flat_result_len) + input_grads + variable_grads\n\n  @ops.RegisterGradient(name)\n  def internal_grad_fn(unused_op, *result_grads):  # pylint: disable=unused-variable\n    """"""Custom grad fn wrapper.""""""\n    return tape_grad_fn(*result_grads)\n\n  original_tensors = all_tensors\n  with ops.get_default_graph().gradient_override_map({""IdentityN"": name}):\n    all_tensors = array_ops.identity_n(all_tensors)\n\n  original_tensors = [ops.convert_to_tensor(x) for x in original_tensors]\n\n  # Propagate handle data for happier shape inference for resource variables.\n  for i, t in enumerate(original_tensors):\n    if t.dtype == dtypes.resource and hasattr(t, ""_handle_data""):\n      all_tensors[i]._handle_data = t._handle_data  # pylint: disable=protected-access\n  record.record_operation(\n      f.__name__, all_tensors, original_tensors, tape_grad_fn)\n  for ot, t in zip(original_tensors, all_tensors):\n    handle_data_util.copy_handle_data(ot, t)\n  flat_result = composite_tensor_gradient.replace_flat_tensors_for_gradients(\n      nest.flatten(result), all_tensors[:flat_result_len])\n  return nest.pack_sequence_as(result, flat_result)\n\n\ndef _eager_mode_decorator(f, args, kwargs):\n  """"""Implement custom gradient decorator for eager mode.""""""\n  with record.VariableWatcher() as variable_watcher:\n    result, grad_fn = f(*args, **kwargs)\n  flat_args = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(args))\n  flat_kwargs = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(kwargs))\n  all_inputs = flat_args + flat_kwargs\n  # The variables that grad_fn needs to return gradients for are the set of\n  # variables used that are *not* part of the inputs.\n  variables = [\n      v.deref()  # pylint: disable=g-complex-comprehension\n      for v in set(v.ref() for v in variable_watcher.watched_variables())\n      if all(v.deref() is not i for i in all_inputs)\n  ]\n  grad_argspec = tf_inspect.getfullargspec(grad_fn)\n  if (variables and (""variables"" not in grad_argspec.args) and\n      (""variables"" not in grad_argspec.kwonlyargs) and\n      not grad_argspec.varkw):\n    raise TypeError(\n        ""@tf.custom_gradient grad_fn must accept keyword argument \'variables\', ""\n        ""since function uses variables: {}"".format(variables))\n  flat_result = composite_tensor_gradient.get_flat_tensors_for_gradients(\n      nest.flatten(result))\n  # TODO(apassos) consider removing the identity below.\n  flat_result = [gen_array_ops.identity(x) for x in flat_result]\n\n  input_tensors = [\n      ops.convert_to_tensor(x) for x in flat_args + list(variables)]\n\n  recorded_inputs = input_tensors\n  arg_count = len(flat_args)\n\n  def actual_grad_fn(*result_grad_components):\n    """"""Custom grad fn wrapper.""""""\n    result_grads = composite_tensor_gradient.replace_flat_tensors_for_gradients(\n        nest.flatten(result), result_grad_components)\n    if not isinstance(result_grads, (list, tuple)):\n      result_grads = [result_grads]\n\n    if variables:\n      input_grads, variable_grads = grad_fn(*result_grads, variables=variables)\n      if len(variable_grads) != len(variables):\n        raise ValueError(""Must return gradient for each variable from ""\n                         ""@custom_gradient grad_fn."")\n    else:\n      input_grads = grad_fn(*result_grads)\n      variable_grads = []\n    flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(\n        nest.flatten(input_grads))\n    if len(flat_grads) != arg_count:\n      raise ValueError(\n          f""custom_gradient function expected to return {arg_count} ""\n          f""gradients, but returned {len(flat_grads)} instead."")\n    return flat_grads + variable_grads\n\n  record.record_operation(f.__name__, flat_result, recorded_inputs,\n                          actual_grad_fn)\n  flat_result = composite_tensor_gradient.replace_flat_tensors_for_gradients(\n      nest.flatten(result), flat_result)\n  return nest.pack_sequence_as(result, flat_result)\n\n\n@tf_export(""recompute_grad"")\ndef recompute_grad(f):\n  """"""Defines a function as a recompute-checkpoint for the tape auto-diff.\n\n  Tape checkpointing is a technique to reduce the memory consumption of the\n  auto-diff tape:\n\n  - Without tape checkpointing operations and intermediate values are\n  recorded to the tape for use in the backward pass.\n\n  - With tape checkpointing, only the function call and its inputs are\n  recorded. During back-propagation the `recompute_grad` custom gradient\n  (`tf.custom_gradient`) recomputes the function under a localized Tape object.\n  This recomputation of the function during backpropagation performs redundant\n  calculation, but reduces the overall memory usage of the Tape.\n\n  >>> y = tf.Variable(1.0)\n\n  >>> def my_function(x):\n  ...   tf.print(\'running\')\n  ...   z = x*y\n  ...   return z\n\n  >>> my_function_recompute = tf.recompute_grad(my_function)\n\n  >>> with tf.GradientTape() as tape:\n  ...   r = tf.constant(1.0)\n  ...   for i in range(4):\n  ...     r = my_function_recompute(r)\n  running\n  running\n  running\n  running\n\n  >>> grad = tape.gradient(r, [y])\n  running\n  running\n  running\n  running\n\n  Without `recompute_grad`, the tape contains all intermitate steps, and no\n  recomputation is performed.\n\n  >>> with tf.GradientTape() as tape:\n  ...   r = tf.constant(1.0)\n  ...   for i in range(4):\n  ...     r = my_function(r)\n  running\n  running\n  running\n  running\n\n  >>> grad = tape.gradient(r, [y])\n\n\n  If `f` was a `tf.keras` `Model` or `Layer` object, methods and attributes\n  such as `f.variables` are not available on the returned function `g`.\n  Either keep a reference of `f` , or use `g.__wrapped__` for accessing\n  these variables and methods.\n\n\n  >>> def print_running_and_return(x):\n  ...   tf.print(""running"")\n  ...   return x\n\n  >>> model = tf.keras.Sequential([\n  ...   tf.keras.layers.Lambda(print_running_and_return),\n  ...   tf.keras.layers.Dense(2)\n  ... ])\n\n  >>> model_recompute = tf.recompute_grad(model)\n\n  >>> with tf.GradientTape(persistent=True) as tape:\n  ...   r = tf.constant([[1,2]])\n  ...   for i in range(4):\n  ...     r = model_recompute(r)\n  running\n  running\n  running\n  running\n\n  >>> grad = tape.gradient(r, model.variables)\n  running\n  running\n  running\n  running\n\n  Alternatively, use the `__wrapped__` attribute to access the original\n  model object.\n\n  >>> grad = tape.gradient(r, model_recompute.__wrapped__.variables)\n  running\n  running\n  running\n  running\n\n\n  Args:\n    f: function `f(*x)` that returns a `Tensor` or sequence of `Tensor` outputs.\n\n  Returns:\n    A function `g` wrapping `f` that defines a custom gradient, which recomputes\n    `f` on the backwards pass of a gradient call.\n  """"""\n  # TODO(cdfreeman) Add is_recomputing functionality from graph mode version\n\n  @custom_gradient\n  def inner(*args, **kwargs):\n    """"""Inner function closure for calculating gradients.""""""\n    current_var_scope = variable_scope.get_variable_scope()\n    with record.stop_recording():\n      result = f(*args, **kwargs)\n\n    def grad_wrapper(*wrapper_args, variables=None):\n      """"""Wrapper function to accomodate lack of kwargs in graph mode custom_gradient.""""""\n\n      @custom_gradient\n      def inner_recompute_grad(*dresult):\n        """"""Nested custom gradient function for computing grads in reverse and forward mode autodiff.""""""\n        # Gradient calculation for reverse mode autodiff.\n        with backprop.GradientTape() as t:\n          id_args = nest.map_structure(gen_array_ops.identity, args)\n          # Tuple `dresult` should contain at least one tensor.\n          assert len(dresult) >= 1\n\n          if not context.executing_eagerly():\n            # XLA doesn\'t respect `tf.control_dependencies`. The code block\n            # below manually adds a data dependency to `dresult` to ensure\n            # recomputation of `f(*args, **kwargs)` happens after `dresult`.\n\n            # This works even if `dresult[0]` is a size 0 tensor as reduce_max\n            # of a size 0 tensor returns -inf. Use reshape here to avoid reading\n            # the entire `dresult[0]`.\n            elem = math_ops.reduce_max(array_ops.reshape(dresult[0], [-1])[:1])\n            # Cast elem to bool in case elem is NaN.\n            elem_bool = math_ops.cast(elem, dtypes.bool)\n            dresult_dep = array_ops.where_v2(\n                elem_bool == elem_bool, 0., float(""nan""))  # pylint: disable=comparison-with-itself\n            id_args = nest.map_structure(\n                lambda x: x + math_ops.cast(dresult_dep, x.dtype), id_args)\n\n          t.watch(id_args)\n          if variables is not None:\n            t.watch(variables)\n          with variable_scope.variable_scope(current_var_scope):\n            recomputed_result = f(*id_args, **kwargs)\n        kw_vars = []\n        if variables is not None:\n          kw_vars = list(variables)\n        grads = t.gradient(\n            recomputed_result,\n            list(id_args) + kw_vars,\n            output_gradients=dresult,\n            unconnected_gradients=UnconnectedGradients.ZERO)\n\n        def transpose(*t_args, **t_kwargs):\n          """"""Gradient function calculation for forward mode autodiff.""""""\n          # Just throw an error since gradients / activations are not stored on\n          # tape for recompute.\n          raise NotImplementedError(\n              ""recompute_grad tried to transpose grad of {}. ""\n              ""Consider not using recompute_grad in forward mode""\n              ""autodiff"".format(f.__name__))\n\n        return (grads[:len(id_args)], grads[len(id_args):]), transpose\n\n      return inner_recompute_grad(*wrapper_args)\n\n    return result, grad_wrapper\n\n  return tf_decorator.make_decorator(f, inner)\n\n\n@tf_export(""grad_pass_through"")\ndef grad_pass_through(f):\n  """"""Creates a grad-pass-through op with the forward behavior provided in f.\n\n  Use this function to wrap any op, maintaining its behavior in the forward\n  pass, but replacing the original op in the backward graph with an identity.\n  For example:\n\n  ```python\n  x = tf.Variable(1.0, name=""x"")\n  z = tf.Variable(3.0, name=""z"")\n\n  with tf.GradientTape() as tape:\n    # y will evaluate to 9.0\n    y = tf.grad_pass_through(x.assign)(z**2)\n  # grads will evaluate to 6.0\n  grads = tape.gradient(y, z)\n  ```\n\n  Another example is a \'differentiable\' moving average approximation, where\n  gradients are allowed to flow into the last value fed to the moving average,\n  but the moving average is still used for the forward pass:\n\n  ```python\n  x = ... # Some scalar value\n  # A moving average object, we don\'t need to know how this is implemented\n  moving_average = MovingAverage()\n  with backprop.GradientTape() as tape:\n    # mavg_x will evaluate to the current running average value\n    mavg_x = tf.grad_pass_through(moving_average)(x)\n  grads = tape.gradient(mavg_x, x) # grads will evaluate to 1.0\n  ```\n\n  Args:\n    f: function `f(*x)` that returns a `Tensor` or nested structure of `Tensor`\n      outputs.\n\n  Returns:\n    A function `h(x)` which returns the same values as `f(x)` and whose\n    gradients are the same as those of an identity function.\n  """"""\n  @custom_gradient\n  def _grad_pass_through_op(*args, **kwargs):\n    def grad(*args, **kwargs):\n      variables = kwargs.get(""variables"")\n      if variables is not None:\n        # Variables involved in the wrapped op will not receive gradients.\n        return args, [None] * len(variables)\n      return args\n    return f(*args, **kwargs), grad\n  return tf_decorator.make_decorator(f, _grad_pass_through_op)', 'timestamp': '2024-06-21T20:28:04', 'title': 'tensorflow/tensorflow/python/ops/custom_gradient.py at master · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/custom_gradient.py'}), Document(page_content='Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ninput and output of @tf.custom_gradient #21756\n\nhuangbiubiu opened this issue\n\nAug 21, 2018 · 5 comments\n\ninput and output of @tf.custom_gradient #21756\n\nhuangbiubiu opened this issue\n\nAug 21, 2018 · 5 comments\n\nhuangbiubiu commented\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\n\nTensorFlow installed from (source or binary): pip\n\nTensorFlow version (use command below): TensorFlow 1.10\n\nPython version: Python 3.6.5 by Anaconda\n\nBazel version (if compiling from source): N/A\n\nGCC/Compiler version (if compiling from source): N/A\n\nCUDA/cuDNN version: CUDA 9.0/ cuDNN 7.1\n\nGPU model and memory: NVIDIA GeForce GTX 1080Ti 11G\n\nExact command to reproduce: N/A\n\nDescribe the problem\n\nI am confusing about the input and output of tf.custom_gradient.\n\nx is a Tensor or sequence of Tensor inputs to the function. But with multiple inputs, instead of taking a sequence of Tensors, function f takes N positional arguments. I think this is a mistake in documentation. A sequence of Tensors can\'t be passed to f which can be reproduced by code below:\n\ndef self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(input_): x = input_[0] label = input_[2] def grad(dy): return [dy, dy] return x - label, grad x = tf.range(10, dtype=tf.float32) y = tf.range(10, dtype=tf.int32) loss = loss_func([x, y]) if __name__ == \'__main__\': self_define_op_multiple_inputs()\n\nIt will try to convert [x, y] to a single Tensor and raises a error:\n\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Traceback (most recent call last): File ""/home/hyh/projects/benchmark/test.py"", line 280, in <module> self_define_op_multiple_inputs() File ""/home/hyh/projects/benchmark/test.py"", line 276, in self_define_op_multiple_inputs loss = loss_func([x, y]) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated return _graph_mode_decorator(f, *args, **kwargs) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 124, in _graph_mode_decorator args = [ops.convert_to_tensor(x) for x in args] File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 124, in <listcomp> args = [ops.convert_to_tensor(x) for x in args] File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 998, in convert_to_tensor as_ref=False) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1094, in internal_convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 961, in _autopacking_conversion_function return _autopacking_helper(v, inferred_dtype, name or ""packed"") File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 903, in _autopacking_helper elem)) TypeError: Cannot convert a list containing a tensor of dtype <dtype: \'int32\'> to <dtype: \'float32\'> (Tensor is: <tf.Tensor \'range_1:0\' shape=(10,) dtype=int32>)\n\nWhile change to positional arguments can fix the bug:\n\n@tf.custom_gradient def loss_func(x, label): def grad(dy): return [dy, dy]\n\nRelated discussion can be found at https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs.\n\nThis is the problem about the output of grad_fn. In doc, grad_vars is a list<Tensor> with the derivatives of Tensors in y with respect to the variables, and signature is g(*grad_ys, variables=None).\n\nIs variables is original variables or the gradient of variables like grad_ys?\n\nReturn grad_vars as a list<Tensor> will raise an error:\n\ndef self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(x): w = tf.get_variable(""margin_inner_product_layer/W"", shape=(1,), dtype=tf.float32, initializer=tf.constant_initializer([10]), use_resource=True) def grad(dy, variables=None): return dy, [variables] # just for testing return tf.multiply(x, w), grad x = tf.constant([5], dtype=tf.float32, shape=(1,)) loss = loss_func(x) dl = tf.gradients(loss, x) with tf.Session(config=config) as sess: derivative = sess.run(dl) print(derivative) if __name__ == \'__main__\': self_define_op_multiple_inputs()\n\nIt seems like it handles grad_vars as a Tensor:\n\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Traceback (most recent call last): File ""/home/hyh/projects/benchmark/test.py"", line 259, in <module> self_define_op_multiple_inputs() File ""/home/hyh/projects/benchmark/test.py"", line 251, in self_define_op_multiple_inputs dl = tf.gradients(loss, x) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 596, in gradients gate_gradients, aggregation_method, stop_gradients) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 795, in _GradientsHelper _LogOpGradients(op, out_grads, in_grads) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 945, in _LogOpGradients "", "".join([x.name for x in in_grads if _FilterGrad(x)])) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 945, in <listcomp> "", "".join([x.name for x in in_grads if _FilterGrad(x)])) AttributeError: \'list\' object has no attribute \'name\'\n\nChange grad_vars to Tensor doesn\'t work either:\n\ndef self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(x): w = tf.get_variable(""margin_inner_product_layer/W"", shape=(1,), dtype=tf.float32, initializer=tf.constant_initializer([10]), use_resource=True) def grad(dy, variables=None): return dy, variables # just for testing return tf.multiply(x, w), grad x = tf.constant([5], dtype=tf.float32, shape=(1,)) loss = loss_func(x) dl = tf.gradients(loss, x) with tf.Session(config=config) as sess: derivative = sess.run(dl) print(derivative) if __name__ == \'__main__\': self_define_op_multiple_inputs()\n\nThe text was updated successfully, but these errors were encountered:\n\ntensorflowbutler assigned cy89\n\n@andydavis1 would you PTAL, or reassign to someone who knows the custom gradients code?\n\ncy89 assigned andydavis1\n\nandydavis1 assigned asimshankar and unassigned andydavis1\n\nasimshankar assigned alextp\n\nasimshankar commented\n\nThe documentation could be improved here. Saying ""x can be a list of Tensors"" is confusing. What we really wanted to convey was that f can be a function with multiple arguments, not just a single Tensor.\n\n@DSRYhh - do you have suggestions for better phrasing?\n\nI\'m preparing a PR which removes ""list"" from the documentation, fixing the issue you saw there.\n\nIn your last example the correct way to do this is\n\n@tf.custom_gradient def loss_func(x): w = tf.get_variable(""margin_inner_product_layer/W"", shape=(1,), dtype=tf.float32, initializer=tf.constant_initializer([10]), use_resource=True) def grad(dy, variables=None): return dy, [dy for v in variables] return tf.multiply(x, w), grad\n\nas in, the second return value when variables is not None should be a list with one element per variable in variables. I\'ll clarify the documentation there too.\n\ntensorflow-copybara closed this as completed in a3ef081\n\nhuangbiubiu commented\n\n@alextp To be more clear, for the second parameter (and the second return value), grad_fn accepts original variables (not the gradient of variables) and return the gradient of variables, is that correct?\n\nIf that\'s correct, why not grad_fn accepts gradient of variables instead of original variables ( in order to be consistent with grad_ys (the first parameter))? In that case, we can use the derivates of variables by automatic differentiation instead of writing the derivates of variables manually.\n\nSep 11, 2018 via email\n\ngrad_ys is the ""downstream"" gradient of the outputs of your function; since the variables are not outputs there is no gradient already computed wrt them. If you want you can call tf.gradients or use the tf.GradientTape yourself to compute the gradient wrt the variables to then modify it, but we don\'t force you to do that since it would waste computation in eager execution. …\n\nOn Tue, Sep 11, 2018 at 3:56 AM Huang Yuheng ***@***.***> wrote: @alextp <https://github.com/alextp> To be more clear, for the second parameter (and the second return value), grad_fn accepts *original* variables (not the *gradient* of variables) and return the *gradient* of variables, is that correct? If that\'s correct, why not grad_fn accepts *gradient* of variables instead of *original* variables ( in order to be consistent with grad_ys (the first parameter))? In that case, we can use the derivates of variables by automatic differentiation instead of writing the derivates of variables manually. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#21756 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxetzCj348nk4KLUjm3FAJNNXJqR3ks5uZ5b1gaJpZM4WFeuE> .\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_0', 'snippet': 'Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ninput and output of @tf.custom_gradient #21756\n\nhuangbiubiu opened this issue\n\nAug 21, 2018 · 5 comments\n\ninput and output of @tf.custom_gradient #21756\n\nhuangbiubiu opened this issue\n\nAug 21, 2018 · 5 comments\n\nhuangbiubiu commented\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\n\nTensorFlow installed from (source or binary): pip\n\nTensorFlow version (use command below): TensorFlow 1.10\n\nPython version: Python 3.6.5 by Anaconda\n\nBazel version (if compiling from source): N/A\n\nGCC/Compiler version (if compiling from source): N/A\n\nCUDA/cuDNN version: CUDA 9.0/ cuDNN 7.1\n\nGPU model and memory: NVIDIA GeForce GTX 1080Ti 11G\n\nExact command to reproduce: N/A\n\nDescribe the problem\n\nI am confusing about the input and output of tf.custom_gradient.\n\nx is a Tensor or sequence of Tensor inputs to the function. But with multiple inputs, instead of taking a sequence of Tensors, function f takes N positional arguments. I think this is a mistake in documentation. A sequence of Tensors can\'t be passed to f which can be reproduced by code below:\n\ndef self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(input_): x = input_[0] label = input_[2] def grad(dy): return [dy, dy] return x - label, grad x = tf.range(10, dtype=tf.float32) y = tf.range(10, dtype=tf.int32) loss = loss_func([x, y]) if __name__ == \'__main__\': self_define_op_multiple_inputs()\n\nIt will try to convert [x, y] to a single Tensor and raises a error:\n\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Traceback (most recent call last): File ""/home/hyh/projects/benchmark/test.py"", line 280, in <module> self_define_op_multiple_inputs() File ""/home/hyh/projects/benchmark/test.py"", line 276, in self_define_op_multiple_inputs loss = loss_func([x, y]) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated return _graph_mode_decorator(f, *args, **kwargs) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 124, in _graph_mode_decorator args = [ops.convert_to_tensor(x) for x in args] File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/custom_gradient.py"", line 124, in <listcomp> args = [ops.convert_to_tensor(x) for x in args] File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 998, in convert_to_tensor as_ref=False) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1094, in internal_convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 961, in _autopacking_conversion_function return _autopacking_helper(v, inferred_dtype, name or ""packed"") File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 903, in _autopacking_helper elem)) TypeError: Cannot convert a list containing a tensor of dtype <dtype: \'int32\'> to <dtype: \'float32\'> (Tensor is: <tf.Tensor \'range_1:0\' shape=(10,) dtype=int32>)\n\nWhile change to positional arguments can fix the bug:\n\n@tf.custom_gradient def loss_func(x, label): def grad(dy): return [dy, dy]\n\nRelated discussion can be found at https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs.\n\nThis is the problem about the output of grad_fn. In doc, grad_vars is a list<Tensor> with the derivatives of Tensors in y with respect to the variables, and signature is g(*grad_ys, variables=None).\n\nIs variables is original variables or the gradient of variables like grad_ys?\n\nReturn grad_vars as a list<Tensor> will raise an error:\n\ndef self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(x): w = tf.get_variable(""margin_inner_product_layer/W"", shape=(1,), dtype=tf.float32, initializer=tf.constant_initializer([10]), use_resource=True) def grad(dy, variables=None): return dy, [variables] # just for testing return tf.multiply(x, w), grad x = tf.constant([5], dtype=tf.float32, shape=(1,)) loss = loss_func(x) dl = tf.gradients(loss, x) with tf.Session(config=config) as sess: derivative = sess.run(dl) print(derivative) if __name__ == \'__main__\': self_define_op_multiple_inputs()\n\nIt seems like it handles grad_vars as a Tensor:\n\n/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Traceback (most recent call last): File ""/home/hyh/projects/benchmark/test.py"", line 259, in <module> self_define_op_multiple_inputs() File ""/home/hyh/projects/benchmark/test.py"", line 251, in self_define_op_multiple_inputs dl = tf.gradients(loss, x) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 596, in gradients gate_gradients, aggregation_method, stop_gradients) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 795, in _GradientsHelper _LogOpGradients(op, out_grads, in_grads) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 945, in _LogOpGradients "", "".join([x.name for x in in_grads if _FilterGrad(x)])) File ""/home/hyh/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 945, in <listcomp> "", "".join([x.name for x in in_grads if _FilterGrad(x)])) AttributeError: \'list\' object has no attribute \'name\'\n\nChange grad_vars to Tensor doesn\'t work either:\n\ndef self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(x): w = tf.get_variable(""margin_inner_product_layer/W"", shape=(1,), dtype=tf.float32, initializer=tf.constant_initializer([10]), use_resource=True) def grad(dy, variables=None): return dy, variables # just for testing return tf.multiply(x, w), grad x = tf.constant([5], dtype=tf.float32, shape=(1,)) loss = loss_func(x) dl = tf.gradients(loss, x) with tf.Session(config=config) as sess: derivative = sess.run(dl) print(derivative) if __name__ == \'__main__\': self_define_op_multiple_inputs()\n\nThe text was updated successfully, but these errors were encountered:\n\ntensorflowbutler assigned cy89\n\n@andydavis1 would you PTAL, or reassign to someone who knows the custom gradients code?\n\ncy89 assigned andydavis1\n\nandydavis1 assigned asimshankar and unassigned andydavis1\n\nasimshankar assigned alextp\n\nasimshankar commented\n\nThe documentation could be improved here. Saying ""x can be a list of Tensors"" is confusing. What we really wanted to convey was that f can be a function with multiple arguments, not just a single Tensor.\n\n@DSRYhh - do you have suggestions for better phrasing?\n\nI\'m preparing a PR which removes ""list"" from the documentation, fixing the issue you saw there.\n\nIn your last example the correct way to do this is\n\n@tf.custom_gradient def loss_func(x): w = tf.get_variable(""margin_inner_product_layer/W"", shape=(1,), dtype=tf.float32, initializer=tf.constant_initializer([10]), use_resource=True) def grad(dy, variables=None): return dy, [dy for v in variables] return tf.multiply(x, w), grad\n\nas in, the second return value when variables is not None should be a list with one element per variable in variables. I\'ll clarify the documentation there too.\n\ntensorflow-copybara closed this as completed in a3ef081\n\nhuangbiubiu commented\n\n@alextp To be more clear, for the second parameter (and the second return value), grad_fn accepts original variables (not the gradient of variables) and return the gradient of variables, is that correct?\n\nIf that\'s correct, why not grad_fn accepts gradient of variables instead of original variables ( in order to be consistent with grad_ys (the first parameter))? In that case, we can use the derivates of variables by automatic differentiation instead of writing the derivates of variables manually.\n\nSep 11, 2018 via email\n\ngrad_ys is the ""downstream"" gradient of the outputs of your function; since the variables are not outputs there is no gradient already computed wrt them. If you want you can call tf.gradients or use the tf.GradientTape yourself to compute the gradient wrt the variables to then modify it, but we don\'t force you to do that since it would waste computation in eager execution. …\n\nOn Tue, Sep 11, 2018 at 3:56 AM Huang Yuheng ***@***.***> wrote: @alextp <https://github.com/alextp> To be more clear, for the second parameter (and the second return value), grad_fn accepts *original* variables (not the *gradient* of variables) and return the *gradient* of variables, is that correct? If that\'s correct, why not grad_fn accepts *gradient* of variables instead of *original* variables ( in order to be consistent with grad_ys (the first parameter))? In that case, we can use the derivates of variables by automatic differentiation instead of writing the derivates of variables manually. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#21756 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxetzCj348nk4KLUjm3FAJNNXJqR3ks5uZ5b1gaJpZM4WFeuE> .\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-03-11T09:03:39', 'title': 'input and output of @tf.custom_gradient · Issue #21756 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/21756'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ntf.custom_gradient with multiple input and output #58941\n\njesgosi opened this issue\n\nDec 19, 2022 · 7 comments\n\ntf.custom_gradient with multiple input and output #58941\n\njesgosi opened this issue\n\nDec 19, 2022 · 7 comments\n\nOPs related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author TF 2.9\n\nIssues found in the TF 2.9 release (or RCs) type:support\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device:\n\nTensorFlow installed from (source or binary):\n\nTensorFlow version (use command below): 2.9.2\n\nBazel version (if compiling from source):\n\nGCC/Compiler version (if compiling from source):\n\nGPU model and memory:\n\nExact command to reproduce:\n\nDescribe the problem\n\nI have a function with 4 inputs (x1, x2, x3, x4) and 2 outputs (y1, y2) using Tensorflow. I would like to specify the gradients, since I perform some non-autodiff operations inside the function.\n\nI need to specify the derivatives of the outputs with respect to the inputs. We can see these derivatives as a Jacobian of size (2,4). Regarding this, I have 8 derivatives: dy1_dx1, dy1_dx2, dy1_dx3, dy1_dx4, dy2_dx1, dy2_dx2, dy2_dx3 and dy2_dx4.\n\nHowever, the grad function used in this tf.custom.gradient needs to have the same length as the inputs, this is 4. So, I do not know how Tensorflow handles with the introduction of the 8 derivatives using just 4 elements. I tried to include them as lists, but it gives the error. Here is a general code to reproduce the error:\n\nimport tensorflow as tf @tf.custom_gradient def bar(x1, x2, x3, x4): def grad(dy1, dy2): dy1_dx1 = x2**2 * x3**3 * x4**4 #360000 dy1_dx2 = x1 * 2*x2 * x3**3 * x4**4 #480000 dy1_dx3 = x1 * x2**2 * 3*x3**2 * x4**4 #540000 dy1_dx4 = x1 * x2**2 * x3**3 * 4*x4**3 #576000 dy2_dx1 = x2**2 + x3**3 + x4**4 #698 dy2_dx2 = x1 + 2*x2 + x3**3 + x4**4 #697 dy2_dx3 = x1 + x2**2 + 3*x3**2 + x4**4 #684 dy2_dx4 = x1 + x2**2 + x3**3 + 4*x4**3 #575 return [dy1_dx1, dy2_dx1], [dy1_dx2, dy2_dx2], [dy1_dx3, dy2_dx3], [dy1_dx4, dy2_dx4] y1 = x1 * x2**2 * x3**3 * x4**4 y2 = x1 + x2**2 + x3**3 + x4**4 return [y1, y2], grad x1 = tf.constant(2.0, dtype=tf.float32) x2 = tf.constant(3.0, dtype=tf.float32) x3 = tf.constant(4.0, dtype=tf.float32) x4 = tf.constant(5.0, dtype=tf.float32) with tf.GradientTape(persistent=True) as tape: tape.watch(x1) tape.watch(x2) tape.watch(x3) tape.watch(x4) z = bar(x1, x2, x3, x4) print(tape.gradient(z, x1)) #[dy1_dx1, dy2_dx1] print(tape.gradient(z, x2)) #[dy1_dx2, dy2_dx2] print(tape.gradient(z, x3)) #[dy1_dx3, dy2_dx3] print(tape.gradient(z, x4)) #[dy1_dx4, dy2_dx4]\n\nThe error says: ""custom_gradient function expected to return 4 gradients, but returned 8 instead"".\n\nI expect someway to specify the correspondent 8 derivatives. Thank you in advance!\n\nThe text was updated successfully, but these errors were encountered:\n\ngoogle-ml-butler bot assigned tilakrayal\n\ntilakrayal added TF 2.9\n\nIssues found in the TF 2.9 release (or RCs) comp:ops\n\nOPs related issues type:support\n\nSupport issues labels\n\ntilakrayal commented\n\n@jesgosi, I was facing a different issue while executing the mentioned code. Kindly find the gist of it here and provide the dependencies to debug the issue.\n\ntilakrayal commented\n\nAlso When we are using the function that takes multiple variables as input, the grad function must also return the same number of variables. Please take a look at this official doc link of tf.custom_gradient for the reference. Thank you!\n\ntilakrayal added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nAlso When we are using the function that takes multiple variables as input, the grad function must also return the same number of variables. Please take a look at this official doc link of tf.custom_gradient for the reference. Thank you!\n\nThan you for your response. I updated my question and the gist in order to better reproduce the error. I have already read the reference and I know I must return the same number of variables, but mathematically, I should return 8 gradients instead of 4, so I do not know how tensorflow makes it possible.\n\ngoogle-ml-butler bot removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ntilakrayal commented\n\n@jesgosi, This type of questions are better asked on TensorFlow Forum since it is not a bug or feature request. There is also a larger community that reads and responds to the questions there. Thank you!\n\ntilakrayal added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you\'d like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nOPs related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author TF 2.9\n\nIssues found in the TF 2.9 release (or RCs) type:support\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_1', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ntf.custom_gradient with multiple input and output #58941\n\njesgosi opened this issue\n\nDec 19, 2022 · 7 comments\n\ntf.custom_gradient with multiple input and output #58941\n\njesgosi opened this issue\n\nDec 19, 2022 · 7 comments\n\nOPs related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author TF 2.9\n\nIssues found in the TF 2.9 release (or RCs) type:support\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device:\n\nTensorFlow installed from (source or binary):\n\nTensorFlow version (use command below): 2.9.2\n\nBazel version (if compiling from source):\n\nGCC/Compiler version (if compiling from source):\n\nGPU model and memory:\n\nExact command to reproduce:\n\nDescribe the problem\n\nI have a function with 4 inputs (x1, x2, x3, x4) and 2 outputs (y1, y2) using Tensorflow. I would like to specify the gradients, since I perform some non-autodiff operations inside the function.\n\nI need to specify the derivatives of the outputs with respect to the inputs. We can see these derivatives as a Jacobian of size (2,4). Regarding this, I have 8 derivatives: dy1_dx1, dy1_dx2, dy1_dx3, dy1_dx4, dy2_dx1, dy2_dx2, dy2_dx3 and dy2_dx4.\n\nHowever, the grad function used in this tf.custom.gradient needs to have the same length as the inputs, this is 4. So, I do not know how Tensorflow handles with the introduction of the 8 derivatives using just 4 elements. I tried to include them as lists, but it gives the error. Here is a general code to reproduce the error:\n\nimport tensorflow as tf @tf.custom_gradient def bar(x1, x2, x3, x4): def grad(dy1, dy2): dy1_dx1 = x2**2 * x3**3 * x4**4 #360000 dy1_dx2 = x1 * 2*x2 * x3**3 * x4**4 #480000 dy1_dx3 = x1 * x2**2 * 3*x3**2 * x4**4 #540000 dy1_dx4 = x1 * x2**2 * x3**3 * 4*x4**3 #576000 dy2_dx1 = x2**2 + x3**3 + x4**4 #698 dy2_dx2 = x1 + 2*x2 + x3**3 + x4**4 #697 dy2_dx3 = x1 + x2**2 + 3*x3**2 + x4**4 #684 dy2_dx4 = x1 + x2**2 + x3**3 + 4*x4**3 #575 return [dy1_dx1, dy2_dx1], [dy1_dx2, dy2_dx2], [dy1_dx3, dy2_dx3], [dy1_dx4, dy2_dx4] y1 = x1 * x2**2 * x3**3 * x4**4 y2 = x1 + x2**2 + x3**3 + x4**4 return [y1, y2], grad x1 = tf.constant(2.0, dtype=tf.float32) x2 = tf.constant(3.0, dtype=tf.float32) x3 = tf.constant(4.0, dtype=tf.float32) x4 = tf.constant(5.0, dtype=tf.float32) with tf.GradientTape(persistent=True) as tape: tape.watch(x1) tape.watch(x2) tape.watch(x3) tape.watch(x4) z = bar(x1, x2, x3, x4) print(tape.gradient(z, x1)) #[dy1_dx1, dy2_dx1] print(tape.gradient(z, x2)) #[dy1_dx2, dy2_dx2] print(tape.gradient(z, x3)) #[dy1_dx3, dy2_dx3] print(tape.gradient(z, x4)) #[dy1_dx4, dy2_dx4]\n\nThe error says: ""custom_gradient function expected to return 4 gradients, but returned 8 instead"".\n\nI expect someway to specify the correspondent 8 derivatives. Thank you in advance!\n\nThe text was updated successfully, but these errors were encountered:\n\ngoogle-ml-butler bot assigned tilakrayal\n\ntilakrayal added TF 2.9\n\nIssues found in the TF 2.9 release (or RCs) comp:ops\n\nOPs related issues type:support\n\nSupport issues labels\n\ntilakrayal commented\n\n@jesgosi, I was facing a different issue while executing the mentioned code. Kindly find the gist of it here and provide the dependencies to debug the issue.\n\ntilakrayal commented\n\nAlso When we are using the function that takes multiple variables as input, the grad function must also return the same number of variables. Please take a look at this official doc link of tf.custom_gradient for the reference. Thank you!\n\ntilakrayal added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nAlso When we are using the function that takes multiple variables as input, the grad function must also return the same number of variables. Please take a look at this official doc link of tf.custom_gradient for the reference. Thank you!\n\nThan you for your response. I updated my question and the gist in order to better reproduce the error. I have already read the reference and I know I must return the same number of variables, but mathematically, I should return 8 gradients instead of 4, so I do not know how tensorflow makes it possible.\n\ngoogle-ml-butler bot removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ntilakrayal commented\n\n@jesgosi, This type of questions are better asked on TensorFlow Forum since it is not a bug or feature request. There is also a larger community that reads and responds to the questions there. Thank you!\n\ntilakrayal added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you\'d like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nOPs related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author TF 2.9\n\nIssues found in the TF 2.9 release (or RCs) type:support\n\nYou can’t perform that action at this time.', 'timestamp': '2024-04-25T18:27:01', 'title': 'tf.custom_gradient with multiple input and output · Issue #58941 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/58941'})], [Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to gradients and automatic differentiation\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nAutomatic Differentiation and Gradients\n\nAutomatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.\n\nIn this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution.\n\nimport numpy as np import matplotlib.pyplot as plt import tensorflow as tf\n\nTo differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients.\n\nTensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow ""records"" relevant operations executed inside the context of a tf.GradientTape onto a ""tape"". TensorFlow then uses that tape to compute the gradients of a ""recorded"" computation using reverse mode differentiation.\n\nHere is a simple example:\n\nx = tf.Variable(3.0) with tf.GradientTape() as tape: y = x**2\n\nOnce you\'ve recorded some operations, use GradientTape.gradient(target, sources) to calculate the gradient of some target (often a loss) relative to some source (often the model\'s variables):\n\n# dy = 2x * dx dy_dx = tape.gradient(y, x) dy_dx.numpy()\n\nThe above example uses scalars, but tf.GradientTape works as easily on any tensor:\n\nw = tf.Variable(tf.random.normal((3, 2)), name=\'w\') b = tf.Variable(tf.zeros(2, dtype=tf.float32), name=\'b\') x = [[1., 2., 3.]] with tf.GradientTape(persistent=True) as tape: y = x @ w + b loss = tf.reduce_mean(y**2)\n\nTo get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see tf.nest).\n\n[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n\nThe gradient with respect to each source has the shape of the source:\n\nprint(w.shape) print(dl_dw.shape)\n\nHere is the gradient calculation again, this time passing a dictionary of variables:\n\nmy_vars = { \'w\': w, \'b\': b } grad = tape.gradient(loss, my_vars) grad[\'b\']\n\nGradients with respect to a model\n\nIt\'s common to collect tf.Variables into a tf.Module or one of its subclasses (layers.Layer, keras.Model) for checkpointing and exporting.\n\nIn most cases, you will want to calculate gradients with respect to a model\'s trainable variables. Since all subclasses of tf.Module aggregate their variables in the Module.trainable_variables property, you can calculate these gradients in a few lines of code:\n\nlayer = tf.keras.layers.Dense(2, activation=\'relu\') x = tf.constant([[1., 2., 3.]]) with tf.GradientTape() as tape: # Forward pass y = layer(x) loss = tf.reduce_mean(y**2) # Calculate gradients with respect to every trainable variable grad = tape.gradient(loss, layer.trainable_variables)\n\nfor var, g in zip(layer.trainable_variables, grad): print(f\'{var.name}, shape: {g.shape}\')\n\nControlling what the tape watches\n\nThe default behavior is to record all operations after accessing a trainable tf.Variable. The reasons for this are:\n\nThe tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n\nThe tape holds references to intermediate outputs, so you don\'t want to record unnecessary operations.\n\nThe most common use case involves calculating the gradient of a loss with respect to all a model\'s trainable variables.\n\nFor example, the following fails to calculate a gradient because the tf.Tensor is not ""watched"" by default, and the tf.Variable is not trainable:\n\n# A trainable variable x0 = tf.Variable(3.0, name=\'x0\') # Not trainable x1 = tf.Variable(3.0, name=\'x1\', trainable=False) # Not a Variable: A variable + tensor returns a tensor. x2 = tf.Variable(2.0, name=\'x2\') + 1.0 # Not a variable x3 = tf.constant(3.0, name=\'x3\') with tf.GradientTape() as tape: y = (x0**2) + (x1**2) + (x2**2) grad = tape.gradient(y, [x0, x1, x2, x3]) for g in grad: print(g)\n\nYou can list the variables being watched by the tape using the GradientTape.watched_variables method:\n\n[var.name for var in tape.watched_variables()]\n\ntf.GradientTape provides hooks that give the user control over what is or is not watched.\n\nTo record gradients with respect to a tf.Tensor, you need to call GradientTape.watch(x):\n\nx = tf.constant(3.0) with tf.GradientTape() as tape: tape.watch(x) y = x**2 # dy = 2x * dx dy_dx = tape.gradient(y, x) print(dy_dx.numpy())\n\nConversely, to disable the default behavior of watching all tf.Variables, set watch_accessed_variables=False when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:\n\nx0 = tf.Variable(0.0) x1 = tf.Variable(10.0) with tf.GradientTape(watch_accessed_variables=False) as tape: tape.watch(x1) y0 = tf.math.sin(x0) y1 = tf.nn.softplus(x1) y = y0 + y1 ys = tf.reduce_sum(y)\n\nSince GradientTape.watch was not called on x0, no gradient is computed with respect to it:\n\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1) grad = tape.gradient(ys, {\'x0\': x0, \'x1\': x1}) print(\'dy/dx0:\', grad[\'x0\']) print(\'dy/dx1:\', grad[\'x1\'].numpy())\n\nIntermediate results\n\nYou can also request gradients of the output with respect to intermediate values computed inside the tf.GradientTape context.\n\nx = tf.constant(3.0) with tf.GradientTape() as tape: tape.watch(x) y = x * x z = y * y # Use the tape to compute the gradient of z with respect to the # intermediate value y. # dz_dy = 2 * y and y = x ** 2 = 9 print(tape.gradient(z, y).numpy())\n\nBy default, the resources held by a GradientTape are released as soon as the GradientTape.gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent=True. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:\n\nx = tf.constant([1, 3.0]) with tf.GradientTape(persistent=True) as tape: tape.watch(x) y = x * x z = y * y print(tape.gradient(z, x).numpy()) # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0]) print(tape.gradient(y, x).numpy()) # [2.0, 6.0] (2 * x at x = [1.0, 3.0])\n\ndel tape # Drop the reference to the tape\n\nNotes on performance\n\nThere is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n\nGradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n\nFor efficiency, some ops (like ReLU) don\'t need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent=True on your tape, nothing is discarded and your peak memory usage will be higher.\n\nGradients of non-scalar targets\n\nA gradient is fundamentally an operation on a scalar.\n\nx = tf.Variable(2.0) with tf.GradientTape(persistent=True) as tape: y0 = x**2 y1 = 1 / x print(tape.gradient(y0, x).numpy()) print(tape.gradient(y1, x).numpy())\n\nThus, if you ask for the gradient of multiple targets, the result for each source is:\n\nThe gradient of the sum of the targets, or equivalently\n\nThe sum of the gradients of each target.\n\nx = tf.Variable(2.0) with tf.GradientTape() as tape: y0 = x**2 y1 = 1 / x print(tape.gradient({\'y0\': y0, \'y1\': y1}, x).numpy())\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is calculated:\n\nx = tf.Variable(2.) with tf.GradientTape() as tape: y = x * [3., 4.] print(tape.gradient(y, x).numpy())\n\nThis makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n\nIf you need a separate gradient for each item, refer to Jacobians.\n\nIn some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:\n\nx = tf.linspace(-10.0, 10.0, 200+1) with tf.GradientTape() as tape: tape.watch(x) y = tf.nn.sigmoid(x) dy_dx = tape.gradient(y, x)\n\nplt.plot(x, y, label=\'y\') plt.plot(x, dy_dx, label=\'dy/dx\') plt.legend() _ = plt.xlabel(\'x\')\n\nBecause a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, if and while statements).\n\nHere a different variable is used on each branch of an if. The gradient only connects to the variable that was used:\n\nx = tf.constant(1.0) v0 = tf.Variable(2.0) v1 = tf.Variable(2.0) with tf.GradientTape(persistent=True) as tape: tape.watch(x) if x > 0.0: result = v0 else: result = v1**2 dv0, dv1 = tape.gradient(result, [v0, v1]) print(dv0) print(dv1)\n\nJust remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n\nDepending on the value of x in the above example, the tape either records result = v0 or result = v1**2. The gradient with respect to x is always None.\n\ndx = tape.gradient(result, x) print(dx)\n\nCases where gradient returns None\n\nWhen a target is not connected to a source, gradient will return None.\n\nx = tf.Variable(2.) y = tf.Variable(3.) with tf.GradientTape() as tape: z = y * y print(tape.gradient(z, x))\n\nHere z is obviously not connected to x, but there are several less-obvious ways that a gradient can be disconnected.\n\n1. Replaced a variable with a tensor\n\nIn the section on ""controlling what the tape watches"" you saw that the tape will automatically watch a tf.Variable but not a tf.Tensor.\n\nOne common error is to inadvertently replace a tf.Variable with a tf.Tensor, instead of using Variable.assign to update the tf.Variable. Here is an example:\n\nx = tf.Variable(2.0) for epoch in range(2): with tf.GradientTape() as tape: y = x+1 print(type(x).__name__, "":"", tape.gradient(y, x)) x = x + 1 # This should be `x.assign_add(1)`\n\n2. Did calculations outside of TensorFlow\n\nThe tape can\'t record the gradient path if the calculation exits TensorFlow. For example:\n\nx = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32) with tf.GradientTape() as tape: x2 = x**2 # This step is calculated with NumPy y = np.mean(x2, axis=0) # Like most ops, reduce_mean will cast the NumPy array to a constant tensor # using `tf.convert_to_tensor`. y = tf.reduce_mean(y, axis=0) print(tape.gradient(y, x))\n\n3. Took gradients through an integer or string\n\nIntegers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n\nNobody expects strings to be differentiable, but it\'s easy to accidentally create an int constant or variable if you don\'t specify the dtype.\n\nx = tf.constant(10) with tf.GradientTape() as g: g.watch(x) y = x * x print(g.gradient(y, x))\n\nTensorFlow doesn\'t automatically cast between types, so, in practice, you\'ll often get a type error instead of a missing gradient.\n\n4. Took gradients through a stateful object\n\nState stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n\nA tf.Tensor is immutable. You can\'t change a tensor once it\'s created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf.matmul only depends on its inputs.\n\nA tf.Variable has internal state—its value. When you use the variable, the state is read. It\'s normal to calculate a gradient with respect to a variable, but the variable\'s state blocks gradient calculations from going farther back. For example:\n\nx0 = tf.Variable(3.0) x1 = tf.Variable(0.0) with tf.GradientTape() as tape: # Update x1 = x1 + x0. x1.assign_add(x0) # The tape starts recording from x1. y = x1**2 # y = (x1 + x0)**2 # This doesn\'t work. print(tape.gradient(y, x0)) #dy/dx0 = 2*(x1 + x0)\n\nSimilarly, tf.data.Dataset iterators and tf.queues are stateful, and will stop all gradients on tensors that pass through them.\n\nNo gradient registered\n\nSome tf.Operations are registered as being non-differentiable and will return None. Others have no gradient registered.\n\nThe tf.raw_ops page shows which low-level ops have gradients registered.\n\nIf you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning None. This way you know something has gone wrong.\n\nFor example, the tf.image.adjust_contrast function wraps raw_ops.AdjustContrastv2, which could have a gradient but the gradient is not implemented:\n\nimage = tf.Variable([[[0.5, 0.0, 0.0]]]) delta = tf.Variable(0.1) with tf.GradientTape() as tape: new_image = tf.image.adjust_contrast(image, delta) try: print(tape.gradient(new_image, [image, delta])) assert False # This should not happen. except LookupError as e: print(f\'{type(e).__name__}: {e}\')\n\nIf you need to differentiate through this op, you\'ll either need to implement the gradient and register it (using tf.RegisterGradient) or re-implement the function using other ops.\n\nZeros instead of None\n\nIn some cases it would be convenient to get 0 instead of None for unconnected gradients. You can decide what to return when you have unconnected gradients using the unconnected_gradients argument:\n\nx = tf.Variable([2., 2.]) y = tf.Variable(3.) with tf.GradientTape() as tape: z = y**2 print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-03-23 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_1', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to gradients and automatic differentiation\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nAutomatic Differentiation and Gradients\n\nAutomatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks.\n\nIn this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution.\n\nimport numpy as np import matplotlib.pyplot as plt import tensorflow as tf\n\nTo differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients.\n\nTensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually tf.Variables. TensorFlow ""records"" relevant operations executed inside the context of a tf.GradientTape onto a ""tape"". TensorFlow then uses that tape to compute the gradients of a ""recorded"" computation using reverse mode differentiation.\n\nHere is a simple example:\n\nx = tf.Variable(3.0) with tf.GradientTape() as tape: y = x**2\n\nOnce you\'ve recorded some operations, use GradientTape.gradient(target, sources) to calculate the gradient of some target (often a loss) relative to some source (often the model\'s variables):\n\n# dy = 2x * dx dy_dx = tape.gradient(y, x) dy_dx.numpy()\n\nThe above example uses scalars, but tf.GradientTape works as easily on any tensor:\n\nw = tf.Variable(tf.random.normal((3, 2)), name=\'w\') b = tf.Variable(tf.zeros(2, dtype=tf.float32), name=\'b\') x = [[1., 2., 3.]] with tf.GradientTape(persistent=True) as tape: y = x @ w + b loss = tf.reduce_mean(y**2)\n\nTo get the gradient of loss with respect to both variables, you can pass both as sources to the gradient method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see tf.nest).\n\n[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n\nThe gradient with respect to each source has the shape of the source:\n\nprint(w.shape) print(dl_dw.shape)\n\nHere is the gradient calculation again, this time passing a dictionary of variables:\n\nmy_vars = { \'w\': w, \'b\': b } grad = tape.gradient(loss, my_vars) grad[\'b\']\n\nGradients with respect to a model\n\nIt\'s common to collect tf.Variables into a tf.Module or one of its subclasses (layers.Layer, keras.Model) for checkpointing and exporting.\n\nIn most cases, you will want to calculate gradients with respect to a model\'s trainable variables. Since all subclasses of tf.Module aggregate their variables in the Module.trainable_variables property, you can calculate these gradients in a few lines of code:\n\nlayer = tf.keras.layers.Dense(2, activation=\'relu\') x = tf.constant([[1., 2., 3.]]) with tf.GradientTape() as tape: # Forward pass y = layer(x) loss = tf.reduce_mean(y**2) # Calculate gradients with respect to every trainable variable grad = tape.gradient(loss, layer.trainable_variables)\n\nfor var, g in zip(layer.trainable_variables, grad): print(f\'{var.name}, shape: {g.shape}\')\n\nControlling what the tape watches\n\nThe default behavior is to record all operations after accessing a trainable tf.Variable. The reasons for this are:\n\nThe tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n\nThe tape holds references to intermediate outputs, so you don\'t want to record unnecessary operations.\n\nThe most common use case involves calculating the gradient of a loss with respect to all a model\'s trainable variables.\n\nFor example, the following fails to calculate a gradient because the tf.Tensor is not ""watched"" by default, and the tf.Variable is not trainable:\n\n# A trainable variable x0 = tf.Variable(3.0, name=\'x0\') # Not trainable x1 = tf.Variable(3.0, name=\'x1\', trainable=False) # Not a Variable: A variable + tensor returns a tensor. x2 = tf.Variable(2.0, name=\'x2\') + 1.0 # Not a variable x3 = tf.constant(3.0, name=\'x3\') with tf.GradientTape() as tape: y = (x0**2) + (x1**2) + (x2**2) grad = tape.gradient(y, [x0, x1, x2, x3]) for g in grad: print(g)\n\nYou can list the variables being watched by the tape using the GradientTape.watched_variables method:\n\n[var.name for var in tape.watched_variables()]\n\ntf.GradientTape provides hooks that give the user control over what is or is not watched.\n\nTo record gradients with respect to a tf.Tensor, you need to call GradientTape.watch(x):\n\nx = tf.constant(3.0) with tf.GradientTape() as tape: tape.watch(x) y = x**2 # dy = 2x * dx dy_dx = tape.gradient(y, x) print(dy_dx.numpy())\n\nConversely, to disable the default behavior of watching all tf.Variables, set watch_accessed_variables=False when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:\n\nx0 = tf.Variable(0.0) x1 = tf.Variable(10.0) with tf.GradientTape(watch_accessed_variables=False) as tape: tape.watch(x1) y0 = tf.math.sin(x0) y1 = tf.nn.softplus(x1) y = y0 + y1 ys = tf.reduce_sum(y)\n\nSince GradientTape.watch was not called on x0, no gradient is computed with respect to it:\n\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1) grad = tape.gradient(ys, {\'x0\': x0, \'x1\': x1}) print(\'dy/dx0:\', grad[\'x0\']) print(\'dy/dx1:\', grad[\'x1\'].numpy())\n\nIntermediate results\n\nYou can also request gradients of the output with respect to intermediate values computed inside the tf.GradientTape context.\n\nx = tf.constant(3.0) with tf.GradientTape() as tape: tape.watch(x) y = x * x z = y * y # Use the tape to compute the gradient of z with respect to the # intermediate value y. # dz_dy = 2 * y and y = x ** 2 = 9 print(tape.gradient(z, y).numpy())\n\nBy default, the resources held by a GradientTape are released as soon as the GradientTape.gradient method is called. To compute multiple gradients over the same computation, create a gradient tape with persistent=True. This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:\n\nx = tf.constant([1, 3.0]) with tf.GradientTape(persistent=True) as tape: tape.watch(x) y = x * x z = y * y print(tape.gradient(z, x).numpy()) # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0]) print(tape.gradient(y, x).numpy()) # [2.0, 6.0] (2 * x at x = [1.0, 3.0])\n\ndel tape # Drop the reference to the tape\n\nNotes on performance\n\nThere is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n\nGradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n\nFor efficiency, some ops (like ReLU) don\'t need to keep their intermediate results and they are pruned during the forward pass. However, if you use persistent=True on your tape, nothing is discarded and your peak memory usage will be higher.\n\nGradients of non-scalar targets\n\nA gradient is fundamentally an operation on a scalar.\n\nx = tf.Variable(2.0) with tf.GradientTape(persistent=True) as tape: y0 = x**2 y1 = 1 / x print(tape.gradient(y0, x).numpy()) print(tape.gradient(y1, x).numpy())\n\nThus, if you ask for the gradient of multiple targets, the result for each source is:\n\nThe gradient of the sum of the targets, or equivalently\n\nThe sum of the gradients of each target.\n\nx = tf.Variable(2.0) with tf.GradientTape() as tape: y0 = x**2 y1 = 1 / x print(tape.gradient({\'y0\': y0, \'y1\': y1}, x).numpy())\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is calculated:\n\nx = tf.Variable(2.) with tf.GradientTape() as tape: y = x * [3., 4.] print(tape.gradient(y, x).numpy())\n\nThis makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n\nIf you need a separate gradient for each item, refer to Jacobians.\n\nIn some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:\n\nx = tf.linspace(-10.0, 10.0, 200+1) with tf.GradientTape() as tape: tape.watch(x) y = tf.nn.sigmoid(x) dy_dx = tape.gradient(y, x)\n\nplt.plot(x, y, label=\'y\') plt.plot(x, dy_dx, label=\'dy/dx\') plt.legend() _ = plt.xlabel(\'x\')\n\nBecause a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, if and while statements).\n\nHere a different variable is used on each branch of an if. The gradient only connects to the variable that was used:\n\nx = tf.constant(1.0) v0 = tf.Variable(2.0) v1 = tf.Variable(2.0) with tf.GradientTape(persistent=True) as tape: tape.watch(x) if x > 0.0: result = v0 else: result = v1**2 dv0, dv1 = tape.gradient(result, [v0, v1]) print(dv0) print(dv1)\n\nJust remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n\nDepending on the value of x in the above example, the tape either records result = v0 or result = v1**2. The gradient with respect to x is always None.\n\ndx = tape.gradient(result, x) print(dx)\n\nCases where gradient returns None\n\nWhen a target is not connected to a source, gradient will return None.\n\nx = tf.Variable(2.) y = tf.Variable(3.) with tf.GradientTape() as tape: z = y * y print(tape.gradient(z, x))\n\nHere z is obviously not connected to x, but there are several less-obvious ways that a gradient can be disconnected.\n\n1. Replaced a variable with a tensor\n\nIn the section on ""controlling what the tape watches"" you saw that the tape will automatically watch a tf.Variable but not a tf.Tensor.\n\nOne common error is to inadvertently replace a tf.Variable with a tf.Tensor, instead of using Variable.assign to update the tf.Variable. Here is an example:\n\nx = tf.Variable(2.0) for epoch in range(2): with tf.GradientTape() as tape: y = x+1 print(type(x).__name__, "":"", tape.gradient(y, x)) x = x + 1 # This should be `x.assign_add(1)`\n\n2. Did calculations outside of TensorFlow\n\nThe tape can\'t record the gradient path if the calculation exits TensorFlow. For example:\n\nx = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32) with tf.GradientTape() as tape: x2 = x**2 # This step is calculated with NumPy y = np.mean(x2, axis=0) # Like most ops, reduce_mean will cast the NumPy array to a constant tensor # using `tf.convert_to_tensor`. y = tf.reduce_mean(y, axis=0) print(tape.gradient(y, x))\n\n3. Took gradients through an integer or string\n\nIntegers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n\nNobody expects strings to be differentiable, but it\'s easy to accidentally create an int constant or variable if you don\'t specify the dtype.\n\nx = tf.constant(10) with tf.GradientTape() as g: g.watch(x) y = x * x print(g.gradient(y, x))\n\nTensorFlow doesn\'t automatically cast between types, so, in practice, you\'ll often get a type error instead of a missing gradient.\n\n4. Took gradients through a stateful object\n\nState stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n\nA tf.Tensor is immutable. You can\'t change a tensor once it\'s created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf.matmul only depends on its inputs.\n\nA tf.Variable has internal state—its value. When you use the variable, the state is read. It\'s normal to calculate a gradient with respect to a variable, but the variable\'s state blocks gradient calculations from going farther back. For example:\n\nx0 = tf.Variable(3.0) x1 = tf.Variable(0.0) with tf.GradientTape() as tape: # Update x1 = x1 + x0. x1.assign_add(x0) # The tape starts recording from x1. y = x1**2 # y = (x1 + x0)**2 # This doesn\'t work. print(tape.gradient(y, x0)) #dy/dx0 = 2*(x1 + x0)\n\nSimilarly, tf.data.Dataset iterators and tf.queues are stateful, and will stop all gradients on tensors that pass through them.\n\nNo gradient registered\n\nSome tf.Operations are registered as being non-differentiable and will return None. Others have no gradient registered.\n\nThe tf.raw_ops page shows which low-level ops have gradients registered.\n\nIf you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning None. This way you know something has gone wrong.\n\nFor example, the tf.image.adjust_contrast function wraps raw_ops.AdjustContrastv2, which could have a gradient but the gradient is not implemented:\n\nimage = tf.Variable([[[0.5, 0.0, 0.0]]]) delta = tf.Variable(0.1) with tf.GradientTape() as tape: new_image = tf.image.adjust_contrast(image, delta) try: print(tape.gradient(new_image, [image, delta])) assert False # This should not happen. except LookupError as e: print(f\'{type(e).__name__}: {e}\')\n\nIf you need to differentiate through this op, you\'ll either need to implement the gradient and register it (using tf.RegisterGradient) or re-implement the function using other ops.\n\nZeros instead of None\n\nIn some cases it would be convenient to get 0 instead of None for unconnected gradients. You can decide what to return when you have unconnected gradients using the unconnected_gradients argument:\n\nx = tf.Variable([2., 2.]) y = tf.Variable(3.) with tf.GradientTape() as tape: z = y**2 print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-03-23 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-07-06T14:22:34', 'title': 'Introduction to gradients and automatic differentiation | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/autodiff'}), Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nAdvanced automatic differentiation\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nThe Introduction to gradients and automatic differentiation guide includes everything required to calculate gradients in TensorFlow. This guide focuses on deeper, less common features of the tf.GradientTape API.\n\nimport tensorflow as tf import matplotlib as mpl import matplotlib.pyplot as plt mpl.rcParams[\'figure.figsize\'] = (8, 6)\n\nControlling gradient recording\n\nIn the automatic differentiation guide you saw how to control which variables and tensors are watched by the tape while building the gradient calculation.\n\nThe tape also has methods to manipulate the recording.\n\nIf you wish to stop recording gradients, you can use tf.GradientTape.stop_recording to temporarily suspend recording.\n\nThis may be useful to reduce overhead if you do not wish to differentiate a complicated operation in the middle of your model. This could include calculating a metric or an intermediate result:\n\nx = tf.Variable(2.0) y = tf.Variable(3.0) with tf.GradientTape() as t: x_sq = x * x with t.stop_recording(): y_sq = y * y z = x_sq + y_sq grad = t.gradient(z, {\'x\': x, \'y\': y}) print(\'dz/dx:\', grad[\'x\']) # 2*x => 4 print(\'dz/dy:\', grad[\'y\'])\n\nReset/start recording from scratch\n\nIf you wish to start over entirely, use tf.GradientTape.reset. Simply exiting the gradient tape block and restarting is usually easier to read, but you can use the reset method when exiting the tape block is difficult or impossible.\n\nx = tf.Variable(2.0) y = tf.Variable(3.0) reset = True with tf.GradientTape() as t: y_sq = y * y if reset: # Throw out all the tape recorded so far. t.reset() z = x * x + y_sq grad = t.gradient(z, {\'x\': x, \'y\': y}) print(\'dz/dx:\', grad[\'x\']) # 2*x => 4 print(\'dz/dy:\', grad[\'y\'])\n\nStop gradient flow with precision\n\nIn contrast to the global tape controls above, the tf.stop_gradient function is much more precise. It can be used to stop gradients from flowing along a particular path, without needing access to the tape itself:\n\nx = tf.Variable(2.0) y = tf.Variable(3.0) with tf.GradientTape() as t: y_sq = y**2 z = x**2 + tf.stop_gradient(y_sq) grad = t.gradient(z, {\'x\': x, \'y\': y}) print(\'dz/dx:\', grad[\'x\']) # 2*x => 4 print(\'dz/dy:\', grad[\'y\'])\n\nIn some cases, you may want to control exactly how gradients are calculated rather than using the default. These situations include:\n\nThere is no defined gradient for a new op you are writing.\n\nThe default calculations are numerically unstable.\n\nYou wish to cache an expensive computation from the forward pass.\n\nYou want to modify a value (for example, using tf.clip_by_value or tf.math.round) without modifying the gradient.\n\nFor the first case, to write a new op you can use tf.RegisterGradient to set up your own (refer to the API docs for details). (Note that the gradient registry is global, so change it with caution.)\n\nFor the latter three cases, you can use tf.custom_gradient.\n\nHere is an example that applies tf.clip_by_norm to the intermediate gradient:\n\n# Establish an identity operation, but clip during the gradient pass. @tf.custom_gradient def clip_gradients(y): def backward(dy): return tf.clip_by_norm(dy, 0.5) return y, backward v = tf.Variable(2.0) with tf.GradientTape() as t: output = clip_gradients(v * v) print(t.gradient(output, v)) # calls ""backward"", which clips 4 to 2\n\nRefer to the tf.custom_gradient decorator API docs for more details.\n\nCustom gradients in SavedModel\n\nNote: This feature is available from TensorFlow 2.6.\n\nCustom gradients can be saved to SavedModel by using the option tf.saved_model.SaveOptions(experimental_custom_gradients=True).\n\nTo be saved into the SavedModel, the gradient function must be traceable (to learn more, check out the Better performance with tf.function guide).\n\nclass MyModule(tf.Module): @tf.function(input_signature=[tf.TensorSpec(None)]) def call_custom_grad(self, x): return clip_gradients(x) model = MyModule()\n\ntf.saved_model.save( model, \'saved_model\', options=tf.saved_model.SaveOptions(experimental_custom_gradients=True)) # The loaded gradients will be the same as the above example. v = tf.Variable(2.0) loaded = tf.saved_model.load(\'saved_model\') with tf.GradientTape() as t: output = loaded.call_custom_grad(v * v) print(t.gradient(output, v))\n\nA note about the above example: If you try replacing the above code with tf.saved_model.SaveOptions(experimental_custom_gradients=False), the gradient will still produce the same result on loading. The reason is that the gradient registry still contains the custom gradient used in the function call_custom_op. However, if you restart the runtime after saving without custom gradients, running the loaded model under the tf.GradientTape will throw the error: LookupError: No gradient defined for operation \'IdentityN\' (op type: IdentityN).\n\nMultiple tapes interact seamlessly.\n\nFor example, here each tape watches a different set of tensors:\n\nx0 = tf.constant(0.0) x1 = tf.constant(0.0) with tf.GradientTape() as tape0, tf.GradientTape() as tape1: tape0.watch(x0) tape1.watch(x1) y0 = tf.math.sin(x0) y1 = tf.nn.sigmoid(x1) y = y0 + y1 ys = tf.reduce_sum(y)\n\ntape0.gradient(ys, x0).numpy() # cos(x) => 1.0\n\ntape1.gradient(ys, x1).numpy() # sigmoid(x1)*(1-sigmoid(x1)) => 0.25\n\nHigher-order gradients\n\nOperations inside of the tf.GradientTape context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well.\n\nx = tf.Variable(1.0) # Create a Tensorflow variable initialized to 1.0 with tf.GradientTape() as t2: with tf.GradientTape() as t1: y = x * x * x # Compute the gradient inside the outer `t2` context manager # which means the gradient computation is differentiable as well. dy_dx = t1.gradient(y, x) d2y_dx2 = t2.gradient(dy_dx, x) print(\'dy_dx:\', dy_dx.numpy()) # 3 * x**2 => 3.0 print(\'d2y_dx2:\', d2y_dx2.numpy()) # 6 * x => 6.0\n\nWhile that does give you the second derivative of a scalar function, this pattern does not generalize to produce a Hessian matrix, since tf.GradientTape.gradient only computes the gradient of a scalar. To construct a Hessian matrix, go to the Hessian example under the Jacobian section.\n\n""Nested calls to tf.GradientTape.gradient"" is a good pattern when you are calculating a scalar from a gradient, and then the resulting scalar acts as a source for a second gradient calculation, as in the following example.\n\nExample: Input gradient regularization\n\nMany models are susceptible to ""adversarial examples"". This collection of techniques modifies the model\'s input to confuse the model\'s output. The simplest implementation—such as the Adversarial example using the Fast Gradient Signed Method attack—takes a single step along the gradient of the output with respect to the input; the ""input gradient"".\n\nOne technique to increase robustness to adversarial examples is input gradient regularization (Finlay & Oberman, 2019), which attempts to minimize the magnitude of the input gradient. If the input gradient is small, then the change in the output should be small too.\n\nBelow is a naive implementation of input gradient regularization. The implementation is:\n\nCalculate the gradient of the output with respect to the input using an inner tape.\n\nCalculate the magnitude of that input gradient.\n\nCalculate the gradient of that magnitude with respect to the model.\n\nx = tf.random.normal([7, 5]) layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n\nwith tf.GradientTape() as t2: # The inner tape only takes the gradient with respect to the input, # not the variables. with tf.GradientTape(watch_accessed_variables=False) as t1: t1.watch(x) y = layer(x) out = tf.reduce_sum(layer(x)**2) # 1. Calculate the input gradient. g1 = t1.gradient(out, x) # 2. Calculate the magnitude of the input gradient. g1_mag = tf.norm(g1) # 3. Calculate the gradient of the magnitude with respect to the model. dg1_mag = t2.gradient(g1_mag, layer.trainable_variables)\n\n[var.shape for var in dg1_mag]\n\nAll the previous examples took the gradients of a scalar target with respect to some source tensor(s).\n\nThe Jacobian matrix represents the gradients of a vector valued function. Each row contains the gradient of one of the vector\'s elements.\n\nThe tf.GradientTape.jacobian method allows you to efficiently calculate a Jacobian matrix.\n\nLike gradient: The sources argument can be a tensor or a container of tensors.\n\nUnlike gradient: The target tensor must be a single tensor.\n\nAs a first example, here is the Jacobian of a vector-target with respect to a scalar-source.\n\nx = tf.linspace(-10.0, 10.0, 200+1) delta = tf.Variable(0.0) with tf.GradientTape() as tape: y = tf.nn.sigmoid(x+delta) dy_dx = tape.jacobian(y, delta)\n\nWhen you take the Jacobian with respect to a scalar the result has the shape of the target, and gives the gradient of the each element with respect to the source:\n\nprint(y.shape) print(dy_dx.shape)\n\nplt.plot(x.numpy(), y, label=\'y\') plt.plot(x.numpy(), dy_dx, label=\'dy/dx\') plt.legend() _ = plt.xlabel(\'x\')\n\nWhether the input is scalar or tensor, tf.GradientTape.jacobian efficiently calculates the gradient of each element of the source with respect to each element of the target(s).\n\nFor example, the output of this layer has a shape of (10, 7):\n\nx = tf.random.normal([7, 5]) layer = tf.keras.layers.Dense(10, activation=tf.nn.relu) with tf.GradientTape(persistent=True) as tape: y = layer(x) y.shape\n\nAnd the layer\'s kernel\'s shape is (5, 10):\n\nThe shape of the Jacobian of the output with respect to the kernel is those two shapes concatenated together:\n\nj = tape.jacobian(y, layer.kernel) j.shape\n\nIf you sum over the target\'s dimensions, you\'re left with the gradient of the sum that would have been calculated by tf.GradientTape.gradient:\n\ng = tape.gradient(y, layer.kernel) print(\'g.shape:\', g.shape) j_sum = tf.reduce_sum(j, axis=[0, 1]) delta = tf.reduce_max(abs(g - j_sum)).numpy() assert delta < 1e-3 print(\'delta:\', delta)\n\nWhile tf.GradientTape doesn\'t give an explicit method for constructing a Hessian matrix it\'s possible to build one using the tf.GradientTape.jacobian method.\n\nNote: The Hessian matrix contains N**2 parameters. For this and other reasons it is not practical for most models. This example is included more as a demonstration of how to use the tf.GradientTape.jacobian method, and is not an endorsement of direct Hessian-based optimization. A Hessian-vector product can be calculated efficiently with nested tapes, and is a much more efficient approach to second-order optimization.\n\nx = tf.random.normal([7, 5]) layer1 = tf.keras.layers.Dense(8, activation=tf.nn.relu) layer2 = tf.keras.layers.Dense(6, activation=tf.nn.relu) with tf.GradientTape() as t2: with tf.GradientTape() as t1: x = layer1(x) x = layer2(x) loss = tf.reduce_mean(x**2) g = t1.gradient(loss, layer1.kernel) h = t2.jacobian(g, layer1.kernel)\n\nprint(f\'layer.kernel.shape: {layer1.kernel.shape}\') print(f\'h.shape: {h.shape}\')\n\nTo use this Hessian for a Newton\'s method step, you would first flatten out its axes into a matrix, and flatten out the gradient into a vector:\n\nn_params = tf.reduce_prod(layer1.kernel.shape) g_vec = tf.reshape(g, [n_params, 1]) h_mat = tf.reshape(h, [n_params, n_params])\n\nThe Hessian matrix should be symmetric:\n\ndef imshow_zero_center(image, **kwargs): lim = tf.reduce_max(abs(image)) plt.imshow(image, vmin=-lim, vmax=lim, cmap=\'seismic\', **kwargs) plt.colorbar()\n\nimshow_zero_center(h_mat)\n\nThe Newton\'s method update step is shown below:\n\neps = 1e-3 eye_eps = tf.eye(h_mat.shape[0])*eps\n\nNote: Don\'t actually invert the matrix.\n\n# X(k+1) = X(k) - (∇²f(X(k)))^-1 @ ∇f(X(k)) # h_mat = ∇²f(X(k)) # g_vec = ∇f(X(k)) update = tf.linalg.solve(h_mat + eye_eps, g_vec) # Reshape the update and apply it to the variable. _ = layer1.kernel.assign_sub(tf.reshape(update, layer1.kernel.shape))\n\nWhile this is relatively simple for a single tf.Variable, applying this to a non-trivial model would require careful concatenation and slicing to produce a full Hessian across multiple variables.\n\nIn some cases, you want to take the Jacobian of each of a stack of targets with respect to a stack of sources, where the Jacobians for each target-source pair are independent.\n\nFor example, here the input x is shaped (batch, ins) and the output y is shaped (batch, outs):\n\nx = tf.random.normal([7, 5]) layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu) layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu) with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape: tape.watch(x) y = layer1(x) y = layer2(y) y.shape\n\nThe full Jacobian of y with respect to x has a shape of (batch, ins, batch, outs), even if you only want (batch, ins, outs):\n\nj = tape.jacobian(y, x) j.shape\n\nIf the gradients of each item in the stack are independent, then every (batch, batch) slice of this tensor is a diagonal matrix:\n\nimshow_zero_center(j[:, 0, :, 0]) _ = plt.title(\'A (batch, batch) slice\')\n\ndef plot_as_patches(j): # Reorder axes so the diagonals will each form a contiguous patch. j = tf.transpose(j, [1, 0, 3, 2]) # Pad in between each patch. lim = tf.reduce_max(abs(j)) j = tf.pad(j, [[0, 0], [1, 1], [0, 0], [1, 1]], constant_values=-lim) # Reshape to form a single image. s = j.shape j = tf.reshape(j, [s[0]*s[1], s[2]*s[3]]) imshow_zero_center(j, extent=[-0.5, s[2]-0.5, s[0]-0.5, -0.5]) plot_as_patches(j) _ = plt.title(\'All (batch, batch) slices are diagonal\')\n\nTo get the desired result, you can sum over the duplicate batch dimension, or else select the diagonals using tf.einsum:\n\nj_sum = tf.reduce_sum(j, axis=2) print(j_sum.shape) j_select = tf.einsum(\'bxby->bxy\', j) print(j_select.shape)\n\nIt would be much more efficient to do the calculation without the extra dimension in the first place. The tf.GradientTape.batch_jacobian method does exactly that:\n\njb = tape.batch_jacobian(y, x) jb.shape\n\nerror = tf.reduce_max(abs(jb - j_sum)) assert error < 1e-3 print(error.numpy())\n\nCaution: tf.GradientTape.batch_jacobian only verifies that the first dimension of the source and target match. It doesn\'t check that the gradients are actually independent. It\'s up to you to make sure you only use batch_jacobian where it makes sense. For example, adding a tf.keras.layers.BatchNormalization destroys the independence, since it normalizes across the batch dimension:\n\nx = tf.random.normal([7, 5]) layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu) bn = tf.keras.layers.BatchNormalization() layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu) with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape: tape.watch(x) y = layer1(x) y = bn(y, training=True) y = layer2(y) j = tape.jacobian(y, x) print(f\'j.shape: {j.shape}\')\n\nplot_as_patches(j) _ = plt.title(\'These slices are not diagonal\') _ = plt.xlabel(""Don\'t use `batch_jacobian`"")\n\nIn this case, batch_jacobian still runs and returns something with the expected shape, but its contents have an unclear meaning:\n\njb = tape.batch_jacobian(y, x) print(f\'jb.shape: {jb.shape}\')\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-06-07 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_3', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nAdvanced automatic differentiation\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nThe Introduction to gradients and automatic differentiation guide includes everything required to calculate gradients in TensorFlow. This guide focuses on deeper, less common features of the tf.GradientTape API.\n\nimport tensorflow as tf import matplotlib as mpl import matplotlib.pyplot as plt mpl.rcParams[\'figure.figsize\'] = (8, 6)\n\nControlling gradient recording\n\nIn the automatic differentiation guide you saw how to control which variables and tensors are watched by the tape while building the gradient calculation.\n\nThe tape also has methods to manipulate the recording.\n\nIf you wish to stop recording gradients, you can use tf.GradientTape.stop_recording to temporarily suspend recording.\n\nThis may be useful to reduce overhead if you do not wish to differentiate a complicated operation in the middle of your model. This could include calculating a metric or an intermediate result:\n\nx = tf.Variable(2.0) y = tf.Variable(3.0) with tf.GradientTape() as t: x_sq = x * x with t.stop_recording(): y_sq = y * y z = x_sq + y_sq grad = t.gradient(z, {\'x\': x, \'y\': y}) print(\'dz/dx:\', grad[\'x\']) # 2*x => 4 print(\'dz/dy:\', grad[\'y\'])\n\nReset/start recording from scratch\n\nIf you wish to start over entirely, use tf.GradientTape.reset. Simply exiting the gradient tape block and restarting is usually easier to read, but you can use the reset method when exiting the tape block is difficult or impossible.\n\nx = tf.Variable(2.0) y = tf.Variable(3.0) reset = True with tf.GradientTape() as t: y_sq = y * y if reset: # Throw out all the tape recorded so far. t.reset() z = x * x + y_sq grad = t.gradient(z, {\'x\': x, \'y\': y}) print(\'dz/dx:\', grad[\'x\']) # 2*x => 4 print(\'dz/dy:\', grad[\'y\'])\n\nStop gradient flow with precision\n\nIn contrast to the global tape controls above, the tf.stop_gradient function is much more precise. It can be used to stop gradients from flowing along a particular path, without needing access to the tape itself:\n\nx = tf.Variable(2.0) y = tf.Variable(3.0) with tf.GradientTape() as t: y_sq = y**2 z = x**2 + tf.stop_gradient(y_sq) grad = t.gradient(z, {\'x\': x, \'y\': y}) print(\'dz/dx:\', grad[\'x\']) # 2*x => 4 print(\'dz/dy:\', grad[\'y\'])\n\nIn some cases, you may want to control exactly how gradients are calculated rather than using the default. These situations include:\n\nThere is no defined gradient for a new op you are writing.\n\nThe default calculations are numerically unstable.\n\nYou wish to cache an expensive computation from the forward pass.\n\nYou want to modify a value (for example, using tf.clip_by_value or tf.math.round) without modifying the gradient.\n\nFor the first case, to write a new op you can use tf.RegisterGradient to set up your own (refer to the API docs for details). (Note that the gradient registry is global, so change it with caution.)\n\nFor the latter three cases, you can use tf.custom_gradient.\n\nHere is an example that applies tf.clip_by_norm to the intermediate gradient:\n\n# Establish an identity operation, but clip during the gradient pass. @tf.custom_gradient def clip_gradients(y): def backward(dy): return tf.clip_by_norm(dy, 0.5) return y, backward v = tf.Variable(2.0) with tf.GradientTape() as t: output = clip_gradients(v * v) print(t.gradient(output, v)) # calls ""backward"", which clips 4 to 2\n\nRefer to the tf.custom_gradient decorator API docs for more details.\n\nCustom gradients in SavedModel\n\nNote: This feature is available from TensorFlow 2.6.\n\nCustom gradients can be saved to SavedModel by using the option tf.saved_model.SaveOptions(experimental_custom_gradients=True).\n\nTo be saved into the SavedModel, the gradient function must be traceable (to learn more, check out the Better performance with tf.function guide).\n\nclass MyModule(tf.Module): @tf.function(input_signature=[tf.TensorSpec(None)]) def call_custom_grad(self, x): return clip_gradients(x) model = MyModule()\n\ntf.saved_model.save( model, \'saved_model\', options=tf.saved_model.SaveOptions(experimental_custom_gradients=True)) # The loaded gradients will be the same as the above example. v = tf.Variable(2.0) loaded = tf.saved_model.load(\'saved_model\') with tf.GradientTape() as t: output = loaded.call_custom_grad(v * v) print(t.gradient(output, v))\n\nA note about the above example: If you try replacing the above code with tf.saved_model.SaveOptions(experimental_custom_gradients=False), the gradient will still produce the same result on loading. The reason is that the gradient registry still contains the custom gradient used in the function call_custom_op. However, if you restart the runtime after saving without custom gradients, running the loaded model under the tf.GradientTape will throw the error: LookupError: No gradient defined for operation \'IdentityN\' (op type: IdentityN).\n\nMultiple tapes interact seamlessly.\n\nFor example, here each tape watches a different set of tensors:\n\nx0 = tf.constant(0.0) x1 = tf.constant(0.0) with tf.GradientTape() as tape0, tf.GradientTape() as tape1: tape0.watch(x0) tape1.watch(x1) y0 = tf.math.sin(x0) y1 = tf.nn.sigmoid(x1) y = y0 + y1 ys = tf.reduce_sum(y)\n\ntape0.gradient(ys, x0).numpy() # cos(x) => 1.0\n\ntape1.gradient(ys, x1).numpy() # sigmoid(x1)*(1-sigmoid(x1)) => 0.25\n\nHigher-order gradients\n\nOperations inside of the tf.GradientTape context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well.\n\nx = tf.Variable(1.0) # Create a Tensorflow variable initialized to 1.0 with tf.GradientTape() as t2: with tf.GradientTape() as t1: y = x * x * x # Compute the gradient inside the outer `t2` context manager # which means the gradient computation is differentiable as well. dy_dx = t1.gradient(y, x) d2y_dx2 = t2.gradient(dy_dx, x) print(\'dy_dx:\', dy_dx.numpy()) # 3 * x**2 => 3.0 print(\'d2y_dx2:\', d2y_dx2.numpy()) # 6 * x => 6.0\n\nWhile that does give you the second derivative of a scalar function, this pattern does not generalize to produce a Hessian matrix, since tf.GradientTape.gradient only computes the gradient of a scalar. To construct a Hessian matrix, go to the Hessian example under the Jacobian section.\n\n""Nested calls to tf.GradientTape.gradient"" is a good pattern when you are calculating a scalar from a gradient, and then the resulting scalar acts as a source for a second gradient calculation, as in the following example.\n\nExample: Input gradient regularization\n\nMany models are susceptible to ""adversarial examples"". This collection of techniques modifies the model\'s input to confuse the model\'s output. The simplest implementation—such as the Adversarial example using the Fast Gradient Signed Method attack—takes a single step along the gradient of the output with respect to the input; the ""input gradient"".\n\nOne technique to increase robustness to adversarial examples is input gradient regularization (Finlay & Oberman, 2019), which attempts to minimize the magnitude of the input gradient. If the input gradient is small, then the change in the output should be small too.\n\nBelow is a naive implementation of input gradient regularization. The implementation is:\n\nCalculate the gradient of the output with respect to the input using an inner tape.\n\nCalculate the magnitude of that input gradient.\n\nCalculate the gradient of that magnitude with respect to the model.\n\nx = tf.random.normal([7, 5]) layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n\nwith tf.GradientTape() as t2: # The inner tape only takes the gradient with respect to the input, # not the variables. with tf.GradientTape(watch_accessed_variables=False) as t1: t1.watch(x) y = layer(x) out = tf.reduce_sum(layer(x)**2) # 1. Calculate the input gradient. g1 = t1.gradient(out, x) # 2. Calculate the magnitude of the input gradient. g1_mag = tf.norm(g1) # 3. Calculate the gradient of the magnitude with respect to the model. dg1_mag = t2.gradient(g1_mag, layer.trainable_variables)\n\n[var.shape for var in dg1_mag]\n\nAll the previous examples took the gradients of a scalar target with respect to some source tensor(s).\n\nThe Jacobian matrix represents the gradients of a vector valued function. Each row contains the gradient of one of the vector\'s elements.\n\nThe tf.GradientTape.jacobian method allows you to efficiently calculate a Jacobian matrix.\n\nLike gradient: The sources argument can be a tensor or a container of tensors.\n\nUnlike gradient: The target tensor must be a single tensor.\n\nAs a first example, here is the Jacobian of a vector-target with respect to a scalar-source.\n\nx = tf.linspace(-10.0, 10.0, 200+1) delta = tf.Variable(0.0) with tf.GradientTape() as tape: y = tf.nn.sigmoid(x+delta) dy_dx = tape.jacobian(y, delta)\n\nWhen you take the Jacobian with respect to a scalar the result has the shape of the target, and gives the gradient of the each element with respect to the source:\n\nprint(y.shape) print(dy_dx.shape)\n\nplt.plot(x.numpy(), y, label=\'y\') plt.plot(x.numpy(), dy_dx, label=\'dy/dx\') plt.legend() _ = plt.xlabel(\'x\')\n\nWhether the input is scalar or tensor, tf.GradientTape.jacobian efficiently calculates the gradient of each element of the source with respect to each element of the target(s).\n\nFor example, the output of this layer has a shape of (10, 7):\n\nx = tf.random.normal([7, 5]) layer = tf.keras.layers.Dense(10, activation=tf.nn.relu) with tf.GradientTape(persistent=True) as tape: y = layer(x) y.shape\n\nAnd the layer\'s kernel\'s shape is (5, 10):\n\nThe shape of the Jacobian of the output with respect to the kernel is those two shapes concatenated together:\n\nj = tape.jacobian(y, layer.kernel) j.shape\n\nIf you sum over the target\'s dimensions, you\'re left with the gradient of the sum that would have been calculated by tf.GradientTape.gradient:\n\ng = tape.gradient(y, layer.kernel) print(\'g.shape:\', g.shape) j_sum = tf.reduce_sum(j, axis=[0, 1]) delta = tf.reduce_max(abs(g - j_sum)).numpy() assert delta < 1e-3 print(\'delta:\', delta)\n\nWhile tf.GradientTape doesn\'t give an explicit method for constructing a Hessian matrix it\'s possible to build one using the tf.GradientTape.jacobian method.\n\nNote: The Hessian matrix contains N**2 parameters. For this and other reasons it is not practical for most models. This example is included more as a demonstration of how to use the tf.GradientTape.jacobian method, and is not an endorsement of direct Hessian-based optimization. A Hessian-vector product can be calculated efficiently with nested tapes, and is a much more efficient approach to second-order optimization.\n\nx = tf.random.normal([7, 5]) layer1 = tf.keras.layers.Dense(8, activation=tf.nn.relu) layer2 = tf.keras.layers.Dense(6, activation=tf.nn.relu) with tf.GradientTape() as t2: with tf.GradientTape() as t1: x = layer1(x) x = layer2(x) loss = tf.reduce_mean(x**2) g = t1.gradient(loss, layer1.kernel) h = t2.jacobian(g, layer1.kernel)\n\nprint(f\'layer.kernel.shape: {layer1.kernel.shape}\') print(f\'h.shape: {h.shape}\')\n\nTo use this Hessian for a Newton\'s method step, you would first flatten out its axes into a matrix, and flatten out the gradient into a vector:\n\nn_params = tf.reduce_prod(layer1.kernel.shape) g_vec = tf.reshape(g, [n_params, 1]) h_mat = tf.reshape(h, [n_params, n_params])\n\nThe Hessian matrix should be symmetric:\n\ndef imshow_zero_center(image, **kwargs): lim = tf.reduce_max(abs(image)) plt.imshow(image, vmin=-lim, vmax=lim, cmap=\'seismic\', **kwargs) plt.colorbar()\n\nimshow_zero_center(h_mat)\n\nThe Newton\'s method update step is shown below:\n\neps = 1e-3 eye_eps = tf.eye(h_mat.shape[0])*eps\n\nNote: Don\'t actually invert the matrix.\n\n# X(k+1) = X(k) - (∇²f(X(k)))^-1 @ ∇f(X(k)) # h_mat = ∇²f(X(k)) # g_vec = ∇f(X(k)) update = tf.linalg.solve(h_mat + eye_eps, g_vec) # Reshape the update and apply it to the variable. _ = layer1.kernel.assign_sub(tf.reshape(update, layer1.kernel.shape))\n\nWhile this is relatively simple for a single tf.Variable, applying this to a non-trivial model would require careful concatenation and slicing to produce a full Hessian across multiple variables.\n\nIn some cases, you want to take the Jacobian of each of a stack of targets with respect to a stack of sources, where the Jacobians for each target-source pair are independent.\n\nFor example, here the input x is shaped (batch, ins) and the output y is shaped (batch, outs):\n\nx = tf.random.normal([7, 5]) layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu) layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu) with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape: tape.watch(x) y = layer1(x) y = layer2(y) y.shape\n\nThe full Jacobian of y with respect to x has a shape of (batch, ins, batch, outs), even if you only want (batch, ins, outs):\n\nj = tape.jacobian(y, x) j.shape\n\nIf the gradients of each item in the stack are independent, then every (batch, batch) slice of this tensor is a diagonal matrix:\n\nimshow_zero_center(j[:, 0, :, 0]) _ = plt.title(\'A (batch, batch) slice\')\n\ndef plot_as_patches(j): # Reorder axes so the diagonals will each form a contiguous patch. j = tf.transpose(j, [1, 0, 3, 2]) # Pad in between each patch. lim = tf.reduce_max(abs(j)) j = tf.pad(j, [[0, 0], [1, 1], [0, 0], [1, 1]], constant_values=-lim) # Reshape to form a single image. s = j.shape j = tf.reshape(j, [s[0]*s[1], s[2]*s[3]]) imshow_zero_center(j, extent=[-0.5, s[2]-0.5, s[0]-0.5, -0.5]) plot_as_patches(j) _ = plt.title(\'All (batch, batch) slices are diagonal\')\n\nTo get the desired result, you can sum over the duplicate batch dimension, or else select the diagonals using tf.einsum:\n\nj_sum = tf.reduce_sum(j, axis=2) print(j_sum.shape) j_select = tf.einsum(\'bxby->bxy\', j) print(j_select.shape)\n\nIt would be much more efficient to do the calculation without the extra dimension in the first place. The tf.GradientTape.batch_jacobian method does exactly that:\n\njb = tape.batch_jacobian(y, x) jb.shape\n\nerror = tf.reduce_max(abs(jb - j_sum)) assert error < 1e-3 print(error.numpy())\n\nCaution: tf.GradientTape.batch_jacobian only verifies that the first dimension of the source and target match. It doesn\'t check that the gradients are actually independent. It\'s up to you to make sure you only use batch_jacobian where it makes sense. For example, adding a tf.keras.layers.BatchNormalization destroys the independence, since it normalizes across the batch dimension:\n\nx = tf.random.normal([7, 5]) layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu) bn = tf.keras.layers.BatchNormalization() layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu) with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape: tape.watch(x) y = layer1(x) y = bn(y, training=True) y = layer2(y) j = tape.jacobian(y, x) print(f\'j.shape: {j.shape}\')\n\nplot_as_patches(j) _ = plt.title(\'These slices are not diagonal\') _ = plt.xlabel(""Don\'t use `batch_jacobian`"")\n\nIn this case, batch_jacobian still runs and returns something with the expected shape, but its contents have an unclear meaning:\n\njb = tape.batch_jacobian(y, x) print(f\'jb.shape: {jb.shape}\')\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-06-07 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-04-17T22:02:21', 'title': 'Advanced automatic differentiation | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/advanced_autodiff'})]]??"
59555206,tf.keras,"{'https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187', 'https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/', 'https://www.udemy.com/course/deep-learning-by-tensorflow-tfkeras-keras-using-python/', 'https://www.udacity.com/course/intro-to-machine-learning-with-tensorflow-nanodegree--nd230', 'https://www.udemy.com/course/deep-learning-tensorflow-2/', 'https://www.udacity.com/course/multi-backend-deep-learning-with-keras--cd13370', 'https://www.coursera.org/learn/intro-tensorflow', 'https://www.edx.org/learn/keras', 'https://www.udemy.com/course/cours-complet-de-deep-learning-avec-tensorflow-et-keras/', 'https://www.coursera.org/learn/custom-models-layers-loss-functions-with-tensorflow'}",{'https://www.youtube.com/watch?v=oJ1i2c1KxKk'},"{'https://stackoverflow.com/questions/56918388/error-valueerror-the-last-dimension-of-the-inputs-to-dense-should-be-defined', 'https://stackoverflow.com/questions/71414627/the-last-dimension-of-the-inputs-to-a-dense-layer-should-be-defined-found-none', 'https://stackoverflow.com/questions/72927645/the-last-dimension-of-the-inputs-to-a-dense-layer-should-be-defined-found-none'}","??[[Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nError: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\nAsked 4 years, 10 months ago\n\nModified 1 year, 11 months ago\n\nI\'m trying to build a lstm model for text classification and I\'m receiving an error. This is my entire code that I\'ve tried.\n\nPlease let me know what\'s the reason behind the error and how to fix it.\n\ninput1.shape # text data integer coded (37788, 130) input2.shape # multiple category columns(one hot encoded) concatenated together (37788, 104) train_data = [input1, input2] # this is the train data. i1 = Input(shape=(130,), name=\'input\') embeddings = Embedding(input_dim=20000, output_dim=100, input_length=130)(i1) lstm = LSTM(100)(embeddings) flatten = Flatten()(lstm) i2 = Input(shape=(None, 104)) c1 = Conv1D(64, 2, padding=\'same\', activation=\'relu\', kernel_initializer=\'he_uniform\')(i2) c2 = Conv1D(32, kernel_size=3, activation=\'relu\', kernel_initializer=\'he_uniform\')(c1) flatten1 = Flatten()(c2) concat = concatenate([flatten, flatten1]) dense1 = Dense(32, \'relu\', kernel_initializer=\'he_uniform\')(concat)\n\nI tried to print shape of conv1d layers and I was getting None for flatten layer. I think it might be the reason for the error.\n\nTensor(""conv1d_81/Identity:0"", shape=(None, None, 64), dtype=float32) Tensor(""conv1d_82/Identity:0"", shape=(None, None, 32), dtype=float32) Tensor(""flatten_106/Identity:0"", shape=(None, None), dtype=float32)\n\nThis is the error I\'m getting. How to fix it?\n\n--------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-531-31a53fbf3d37> in <module> 14 concat = concatenate([flatten, flatten1]) ---> 15 dense1 = Dense(32, \'relu\', kernel_initializer=\'he_uniform\')(concat) 16 drop = Dropout(0.5)(dense1) ~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs) 614 # Build layer if applicable (if the `build` method has been 615 # overridden). --> 616 self._maybe_build(inputs) 617 618 # Wrapping `call` function in autograph to allow for dynamic control ~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _maybe_build(self, inputs) 1964 # operations. 1965 with tf_utils.maybe_init_scope(self): -> 1966 self.build(input_shapes) 1967 # We must set self.built since user defined build functions are not 1968 # constrained to set self.built. ~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py in build(self, input_shape) 1003 input_shape = tensor_shape.TensorShape(input_shape) 1004 if tensor_shape.dimension_value(input_shape[-1]) is None: -> 1005 raise ValueError(\'The last dimension of the inputs to `Dense` \' 1006 \'should be defined. Found `None`.\') 1007 last_dim = tensor_shape.dimension_value(input_shape[-1]) ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\n\nImprove this question\n\nasked Jul 6, 2019 at 23:42\n\n2,29655 gold badges2626 silver badges6161 bronze badges 4\n\nif Dense is first layer then you have to set shape/size of input data. You can see it even in example in Dense documentation\n\n– furas Jul 6, 2019 at 23:53\n\n@furas But I have few other layers right at the front like LSTM, Embedding as you can see in my code above I also have two conv1d layers and two flatten layers before I come to Dense layer. Right?\n\n– Jeeth Jul 6, 2019 at 23:58\n\nwhen you use Dense() in Sequential() then it automatically gets shape from previous Dense() and only first Dense() needs shape - like in example in documentation - but you may have to set this shape manually Dense(... input_shape=...)\n\n– furas Jul 7, 2019 at 0:02\n\n@furas How can I know what will be the shape after it passes through all the before layers like embedding layer, lstm, flatten, conv1d, flatten, concatenate layer? It is not my first layer. I\'m using keras functional api and It also automatically gets shape from previous layers just like sequential model.You just have to pass it like I did. Dense(100)(x) here x refer to previous layer. I can\'t figure out why i\'m getting an error here.\n\n– Jeeth Jul 7, 2019 at 0:10\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou have None in the length of the sequence in the second model.\n\ni2 = Input(shape=(None, 104))\n\nYou can\'t flatten a variable length and have a known size. You need a known size for Dense.\n\nEither you use a fixed length instead of None, or you use a GlobalMaxPooling1D or a GlobalAveragePooling1D instead of Flatten.\n\nanswered Jul 7, 2019 at 0:21\n\nDaniel MöllerDaniel Möller\n\n86k2121 gold badges199199 silver badges218218 bronze badges 13\n\nWhat should I fill in place of None here? Is it my input2 shape?\n\n– Jeeth Jul 7, 2019 at 0:53\n\nYes. It\'s input2 shape.\n\n– Daniel Möller Jul 7, 2019 at 0:53\n\nNow I get this error during model.fit ValueError: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (37788, 104) Why does it expect my input 1 to have 3 dimensions?\n\n– Jeeth Jul 7, 2019 at 1:06\n\nI combined both input1 and input2 as train_data = [input1, input2] and passed it as x to model.fit\n\n– Jeeth Jul 7, 2019 at 1:07\n\nThat\'s because Conv1D needs 3d arrays (samples, length, features), not 2d. I don\'t know what you want to achieve, but you must understand your data and what you want to extract from it.\n\n– Daniel Möller Jul 7, 2019 at 2:30\n\n | Show 8 more comments\n\nFor me the problem was that i did not reshape the Tensor before use in the input function\n\nimage = tf.reshape(image, [400,400,3])\n\nanswered Jun 9, 2020 at 12:13\n\nDavid BaceljDavid Bacelj\n\n17211 silver badge77 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nNet neutrality is in; TikTok and noncompetes are out\n\nUpcoming research at Stack Overflow\n\nTesting a new version of Stack Overflow Jobs\n\nPausing the 1-rep voting experiment on Stack Overflow: reflecting on the...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\n-1 Tensorflow: How to convert string inputs to numbers on the fly while training the model using a custom string_to_number function?\n\n10 Keras error: expected dense_input_1 to have 3 dimensions\n\n7 ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\n0 ValueError: Error when checking input: expected dense_151_input to have 3 dimensions, but got array with shape (2, 2100)\n\n0 Error when checking input: expected dense_input to have 3 dimensions\n\n6 Tensorflow: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\n3 ValueError: No data provided for ""dense_input""\n\n1 ValueError: Error when checking input: expected dense_1_input to have 2 dimensions\n\n1 ValueError: Error when checking input: expected dense_1_input to have 3 dimensions, but got array with shape (5, 1)\n\n5 ValueError: dimension of the inputs to `Dense` should be defined. Found `None`\n\n0 Keras Tensor Flow Error : ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\nHot Network Questions\n\nWhat skills do algebra teachers wish their students had mastered before taking algebra?\n\nIf I give my daughter $50k for her wedding and she elects to use the money to pay down a house, can I sue her?\n\nMaking an exoskeleton for combat\n\nWhy is intensity additive?\n\nCan a pilot tell a controller they don\'t need IFR clearance?\n\nHow do these trees not collapse?\n\nHow did the ancient cultures determine that the year was actually a fraction of an extra day beyond 365 days?\n\nHow quickly can a percussionist switch from vibraphone to marimba?\n\nWhy do scientific laws persist?\n\nHorror/Monster movie shown on TV in Basic Instinct\n\nWhy does it take longer to generate suitably large primes for Diffie-Hellman key exchange as opposed to for RSA encryption / decryption?\n\nWhat is the DC of a Long Jump?\n\nCan my step dad steal from my room when I’m gone\n\nDoes Sola Scriptura imply that one should expect no personal spiritual experience of the Gospel?\n\nWith modern technology would it be possible to build an HG Wells style Martian tripod?\n\nWhy don\'t room temperature superconductors exist?\n\nDetermining Jordan canonical form(JCF) of an operator given by complex differentiation.\n\nWhat is the difference of these MOSFET symbols?\n\nTrying to create syntax checking for Perl\n\nMovie where three people get locked in a hotel\n\nCompany threatening me after I changed my mind on joining them after observing their work style\n\nHow do you balance and transport a container of concrete on a ridged roof?\n\nHow can I express these two sets and find their intersection by Mathematica? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_3', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nError: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\nAsked 4 years, 10 months ago\n\nModified 1 year, 11 months ago\n\nI\'m trying to build a lstm model for text classification and I\'m receiving an error. This is my entire code that I\'ve tried.\n\nPlease let me know what\'s the reason behind the error and how to fix it.\n\ninput1.shape # text data integer coded (37788, 130) input2.shape # multiple category columns(one hot encoded) concatenated together (37788, 104) train_data = [input1, input2] # this is the train data. i1 = Input(shape=(130,), name=\'input\') embeddings = Embedding(input_dim=20000, output_dim=100, input_length=130)(i1) lstm = LSTM(100)(embeddings) flatten = Flatten()(lstm) i2 = Input(shape=(None, 104)) c1 = Conv1D(64, 2, padding=\'same\', activation=\'relu\', kernel_initializer=\'he_uniform\')(i2) c2 = Conv1D(32, kernel_size=3, activation=\'relu\', kernel_initializer=\'he_uniform\')(c1) flatten1 = Flatten()(c2) concat = concatenate([flatten, flatten1]) dense1 = Dense(32, \'relu\', kernel_initializer=\'he_uniform\')(concat)\n\nI tried to print shape of conv1d layers and I was getting None for flatten layer. I think it might be the reason for the error.\n\nTensor(""conv1d_81/Identity:0"", shape=(None, None, 64), dtype=float32) Tensor(""conv1d_82/Identity:0"", shape=(None, None, 32), dtype=float32) Tensor(""flatten_106/Identity:0"", shape=(None, None), dtype=float32)\n\nThis is the error I\'m getting. How to fix it?\n\n--------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-531-31a53fbf3d37> in <module> 14 concat = concatenate([flatten, flatten1]) ---> 15 dense1 = Dense(32, \'relu\', kernel_initializer=\'he_uniform\')(concat) 16 drop = Dropout(0.5)(dense1) ~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in __call__(self, inputs, *args, **kwargs) 614 # Build layer if applicable (if the `build` method has been 615 # overridden). --> 616 self._maybe_build(inputs) 617 618 # Wrapping `call` function in autograph to allow for dynamic control ~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py in _maybe_build(self, inputs) 1964 # operations. 1965 with tf_utils.maybe_init_scope(self): -> 1966 self.build(input_shapes) 1967 # We must set self.built since user defined build functions are not 1968 # constrained to set self.built. ~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py in build(self, input_shape) 1003 input_shape = tensor_shape.TensorShape(input_shape) 1004 if tensor_shape.dimension_value(input_shape[-1]) is None: -> 1005 raise ValueError(\'The last dimension of the inputs to `Dense` \' 1006 \'should be defined. Found `None`.\') 1007 last_dim = tensor_shape.dimension_value(input_shape[-1]) ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\n\nImprove this question\n\nasked Jul 6, 2019 at 23:42\n\n2,29655 gold badges2626 silver badges6161 bronze badges 4\n\nif Dense is first layer then you have to set shape/size of input data. You can see it even in example in Dense documentation\n\n– furas Jul 6, 2019 at 23:53\n\n@furas But I have few other layers right at the front like LSTM, Embedding as you can see in my code above I also have two conv1d layers and two flatten layers before I come to Dense layer. Right?\n\n– Jeeth Jul 6, 2019 at 23:58\n\nwhen you use Dense() in Sequential() then it automatically gets shape from previous Dense() and only first Dense() needs shape - like in example in documentation - but you may have to set this shape manually Dense(... input_shape=...)\n\n– furas Jul 7, 2019 at 0:02\n\n@furas How can I know what will be the shape after it passes through all the before layers like embedding layer, lstm, flatten, conv1d, flatten, concatenate layer? It is not my first layer. I\'m using keras functional api and It also automatically gets shape from previous layers just like sequential model.You just have to pass it like I did. Dense(100)(x) here x refer to previous layer. I can\'t figure out why i\'m getting an error here.\n\n– Jeeth Jul 7, 2019 at 0:10\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou have None in the length of the sequence in the second model.\n\ni2 = Input(shape=(None, 104))\n\nYou can\'t flatten a variable length and have a known size. You need a known size for Dense.\n\nEither you use a fixed length instead of None, or you use a GlobalMaxPooling1D or a GlobalAveragePooling1D instead of Flatten.\n\nanswered Jul 7, 2019 at 0:21\n\nDaniel MöllerDaniel Möller\n\n86k2121 gold badges199199 silver badges218218 bronze badges 13\n\nWhat should I fill in place of None here? Is it my input2 shape?\n\n– Jeeth Jul 7, 2019 at 0:53\n\nYes. It\'s input2 shape.\n\n– Daniel Möller Jul 7, 2019 at 0:53\n\nNow I get this error during model.fit ValueError: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (37788, 104) Why does it expect my input 1 to have 3 dimensions?\n\n– Jeeth Jul 7, 2019 at 1:06\n\nI combined both input1 and input2 as train_data = [input1, input2] and passed it as x to model.fit\n\n– Jeeth Jul 7, 2019 at 1:07\n\nThat\'s because Conv1D needs 3d arrays (samples, length, features), not 2d. I don\'t know what you want to achieve, but you must understand your data and what you want to extract from it.\n\n– Daniel Möller Jul 7, 2019 at 2:30\n\n | Show 8 more comments\n\nFor me the problem was that i did not reshape the Tensor before use in the input function\n\nimage = tf.reshape(image, [400,400,3])\n\nanswered Jun 9, 2020 at 12:13\n\nDavid BaceljDavid Bacelj\n\n17211 silver badge77 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nNet neutrality is in; TikTok and noncompetes are out\n\nUpcoming research at Stack Overflow\n\nTesting a new version of Stack Overflow Jobs\n\nPausing the 1-rep voting experiment on Stack Overflow: reflecting on the...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\n-1 Tensorflow: How to convert string inputs to numbers on the fly while training the model using a custom string_to_number function?\n\n10 Keras error: expected dense_input_1 to have 3 dimensions\n\n7 ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\n0 ValueError: Error when checking input: expected dense_151_input to have 3 dimensions, but got array with shape (2, 2100)\n\n0 Error when checking input: expected dense_input to have 3 dimensions\n\n6 Tensorflow: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\n3 ValueError: No data provided for ""dense_input""\n\n1 ValueError: Error when checking input: expected dense_1_input to have 2 dimensions\n\n1 ValueError: Error when checking input: expected dense_1_input to have 3 dimensions, but got array with shape (5, 1)\n\n5 ValueError: dimension of the inputs to `Dense` should be defined. Found `None`\n\n0 Keras Tensor Flow Error : ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\nHot Network Questions\n\nWhat skills do algebra teachers wish their students had mastered before taking algebra?\n\nIf I give my daughter $50k for her wedding and she elects to use the money to pay down a house, can I sue her?\n\nMaking an exoskeleton for combat\n\nWhy is intensity additive?\n\nCan a pilot tell a controller they don\'t need IFR clearance?\n\nHow do these trees not collapse?\n\nHow did the ancient cultures determine that the year was actually a fraction of an extra day beyond 365 days?\n\nHow quickly can a percussionist switch from vibraphone to marimba?\n\nWhy do scientific laws persist?\n\nHorror/Monster movie shown on TV in Basic Instinct\n\nWhy does it take longer to generate suitably large primes for Diffie-Hellman key exchange as opposed to for RSA encryption / decryption?\n\nWhat is the DC of a Long Jump?\n\nCan my step dad steal from my room when I’m gone\n\nDoes Sola Scriptura imply that one should expect no personal spiritual experience of the Gospel?\n\nWith modern technology would it be possible to build an HG Wells style Martian tripod?\n\nWhy don\'t room temperature superconductors exist?\n\nDetermining Jordan canonical form(JCF) of an operator given by complex differentiation.\n\nWhat is the difference of these MOSFET symbols?\n\nTrying to create syntax checking for Perl\n\nMovie where three people get locked in a hotel\n\nCompany threatening me after I changed my mind on joining them after observing their work style\n\nHow do you balance and transport a container of concrete on a ridged roof?\n\nHow can I express these two sets and find their intersection by Mathematica? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-05-01T19:28:10', 'title': 'python - Error: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None` - Stack Overflow', 'url': 'https://stackoverflow.com/questions/56918388/error-valueerror-the-last-dimension-of-the-inputs-to-dense-should-be-defined'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nThe last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: <unknown>\n\nModified 2 years ago\n\nPart of NLP Collective\n\nI am having trouble when switching a model from some local dummy data to using a TF dataset.\n\nSorry for the long model code, I have tried to shorten it as much as possible.\n\nThe following works fine:\n\nimport tensorflow as tf import tensorflow_recommenders as tfrs from transformers import AutoTokenizer, TFAutoModel MODEL_PATH = \'sentence-transformers/all-MiniLM-L6-v2\' tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) model = TFAutoModel.from_pretrained(MODEL_PATH, from_pt=True) class SBert(tf.keras.layers.Layer): def __init__(self, tokenizer, model): super(SBert, self).__init__() self.tokenizer = tokenizer self.model = model def tf_encode(self, inputs): def encode(inputs): inputs = [x[0].decode(""utf-8"") for x in inputs.numpy()] outputs = self.tokenizer(inputs, padding=True, truncation=True, return_tensors=\'tf\') return outputs[\'input_ids\'], outputs[\'token_type_ids\'], outputs[\'attention_mask\'] return tf.py_function(func=encode, inp=[inputs], Tout=[tf.int32, tf.int32, tf.int32]) def process(self, i, t, a): def __call(i, t, a): model_output = self.model( {\'input_ids\': i.numpy(), \'token_type_ids\': t.numpy(), \'attention_mask\': a.numpy()} ) return model_output[0] return tf.py_function(func=__call, inp=[i, t, a], Tout=[tf.float32]) def mean_pooling(self, model_output, attention_mask): token_embeddings = tf.squeeze(tf.stack(model_output), axis=0) input_mask_expanded = tf.cast( tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)), tf.float32 ) a = tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1) b = tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max) embeddings = a / b embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1) return embeddings def call(self, inputs): input_ids, token_type_ids, attention_mask = self.tf_encode(inputs) model_output = self.process(input_ids, token_type_ids, attention_mask) embeddings = self.mean_pooling(model_output, attention_mask) return embeddings sbert = SBert(tokenizer, model) inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string) outputs = sbert(inputs) model = tf.keras.Model(inputs, outputs) model(tf.constant([\'some text\', \'more text\']))\n\nThe call to the model outputs tensors - yipee :)\n\nNow I want to use this layer inside of a larger two tower model:\n\nclass Encoder(tf.keras.Model): def __init__(self): super().__init__() self.text_embedding = self._build_text_embedding() def _build_text_embedding(self): sbert = SBert(tokenizer, model) inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string) outputs = sbert(inputs) return tf.keras.Model(inputs, outputs) def call(self, inputs): return self.text_embedding(inputs) class RecModel(tfrs.models.Model): def __init__(self): super().__init__() self.query_model = tf.keras.Sequential([ Encoder(), tf.keras.layers.Dense(32) ]) self.candidate_model = tf.keras.Sequential([ Encoder(), tf.keras.layers.Dense(32) ]) self.retrieval_task = tfrs.tasks.Retrieval( metrics=tfrs.metrics.FactorizedTopK( candidates=tf.data.Dataset.from_tensor_slices( data[\'text\'] ).batch(1).map(self.candidate_model), ), batch_metrics=[ tf.keras.metrics.TopKCategoricalAccuracy(k=5) ] ) def call(self, features): query_embeddings = self.query_model(features[\'query\']) candidate_embeddings = self.candidate_model(features[\'text\']) return ( query_embeddings, candidate_embeddings, ) def compute_loss(self, features, training=False): query_embeddings, candidate_embeddings = self(features) retrieval_loss = self.retrieval_task(query_embeddings, candidate_embeddings) return retrieval_loss\n\nCreate a small dummy dataset:\n\ndata = { \'query\': [\'blue\', \'cat\', \'football\'], \'text\': [\'a nice colour\', \'a type of animal\', \'a sport\'] } ds = tf.data.Dataset.from_tensor_slices(data).batch(1)\n\nmodel = RecModel() model.compile(optimizer=tf.keras.optimizers.Adagrad())\n\nAnd we hit the following error:\n\n--------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-11-df4cc46e0307> in <module> ----> 1 model = RecModel() 2 model.compile(optimizer=tf.keras.optimizers.Adagrad()) <ipython-input-8-a774041744b9> in __init__(self) 33 candidates=tf.data.Dataset.from_tensor_slices( 34 data[\'text\'] ---> 35 ).batch(1).map(self.candidate_model), 36 ), 37 batch_metrics=[ ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic, name) 2014 warnings.warn(""The `deterministic` argument has no effect unless the "" 2015 ""`num_parallel_calls` argument is specified."") -> 2016 return MapDataset(self, map_func, preserve_cardinality=True, name=name) 2017 else: 2018 return ParallelMapDataset( ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name) 5193 self._transformation_name(), 5194 dataset=input_dataset, -> 5195 use_legacy_function=use_legacy_function) 5196 self._metadata = dataset_metadata_pb2.Metadata() 5197 if name: ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs) 269 fn_factory = trace_tf_function(defun_kwargs) 270 --> 271 self._function = fn_factory() 272 # There is no graph to add in eager mode. 273 add_to_graph &= not context.executing_eagerly() ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs) 3069 """""" 3070 graph_function = self._get_concrete_function_garbage_collected( -> 3071 *args, **kwargs) 3072 graph_function._garbage_collector.release() # pylint: disable=protected-access 3073 return graph_function ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs) 3034 args, kwargs = None, None 3035 with self._lock: -> 3036 graph_function, _ = self._maybe_define_function(args, kwargs) 3037 seen_names = set() 3038 captured = object_identity.ObjectIdentitySet( ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs) 3290 3291 self._function_cache.add_call_context(cache_key.call_context) -> 3292 graph_function = self._create_graph_function(args, kwargs) 3293 self._function_cache.add(cache_key, cache_key_deletion_observer, 3294 graph_function) ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes) 3138 arg_names=arg_names, 3139 override_flat_arg_shapes=override_flat_arg_shapes, -> 3140 capture_by_value=self._capture_by_value), 3141 self._function_attributes, 3142 function_spec=self.function_spec, ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses) 1159 _, original_func = tf_decorator.unwrap(python_func) 1160 -> 1161 func_outputs = python_func(*func_args, **func_kwargs) 1162 1163 # invariant: `func_outputs` contains only Tensors, CompositeTensors, ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py in wrapped_fn(*args) 246 attributes=defun_kwargs) 247 def wrapped_fn(*args): # pylint: disable=missing-docstring --> 248 ret = wrapper_helper(*args) 249 ret = structure.to_tensor_list(self._output_structure, ret) 250 return [ops.convert_to_tensor(t) for t in ret] ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py in wrapper_helper(*args) 175 if not _should_unpack(nested_args): 176 nested_args = (nested_args,) --> 177 ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args) 178 if _should_pack(ret): 179 ret = tuple(ret) ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs) 687 try: 688 with conversion_ctx: --> 689 return converted_call(f, args, kwargs, options=options) 690 except Exception as e: # pylint:disable=broad-except 691 if hasattr(e, \'ag_error_metadata\'): ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options) 375 376 if not options.user_requested and conversion.is_allowlisted(f): --> 377 return _call_unconverted(f, args, kwargs, options) 378 379 # internal_convert_user_code is for example turned off when issuing a dynamic ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache) 456 457 if kwargs is not None: --> 458 return f(*args, **kwargs) 459 return f(*args) 460 ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs) 65 except Exception as e: # pylint: disable=broad-except 66 filtered_tb = _process_traceback_frames(e.__traceback__) ---> 67 raise e.with_traceback(filtered_tb) from None 68 finally: 69 del filtered_tb ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/keras/layers/core/dense.py in build(self, input_shape) 137 last_dim = tf.compat.dimension_value(input_shape[-1]) 138 if last_dim is None: --> 139 raise ValueError(\'The last dimension of the inputs to a Dense layer \' 140 \'should be defined. Found None. \' 141 f\'Full input shape received: {input_shape}\') ValueError: Exception encountered when calling layer ""sequential_5"" (type Sequential). The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: <unknown> Call arguments received: • inputs=tf.Tensor(shape=(None,), dtype=string) • training=None • mask=None\n\nI am not quite sure where I should set the shape - as using regular tensors and not TF dataset works ok.\n\nhuggingface-transformers\n\nsentence-transformers\n\nImprove this question\n\nasked Mar 9, 2022 at 18:53\n\n3,13655 gold badges2626 silver badges6767 bronze badges 3\n\nCan you add the model.fit(…) part to your question?\n\n– AloneTogether Mar 9, 2022 at 21:49\n\n@AloneTogether the error occurs before that, so I never call model.fit because we map over all the unique elements of the data in the compilation of the model in the retrieval_task.\n\n– dendog Mar 10, 2022 at 8:50\n\nHmm, yeah I understand..the output shape is unknown..\n\n– AloneTogether Mar 10, 2022 at 9:13\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou will have to explicitly set the shapes of the tensors coming from tf.py_functions. Using None will allow variable input lengths. The Bert output dimension (384,) is, however, necessary:\n\nimport tensorflow as tf from transformers import AutoTokenizer, TFAutoModel MODEL_PATH = \'sentence-transformers/all-MiniLM-L6-v2\' tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) model = TFAutoModel.from_pretrained(MODEL_PATH, from_pt=True) class SBert(tf.keras.layers.Layer): def __init__(self, tokenizer, model): super(SBert, self).__init__() self.tokenizer = tokenizer self.model = model def tf_encode(self, inputs): def encode(inputs): inputs = [x[0].decode(""utf-8"") for x in inputs.numpy()] outputs = self.tokenizer(inputs, padding=True, truncation=True, return_tensors=\'tf\') return outputs[\'input_ids\'], outputs[\'token_type_ids\'], outputs[\'attention_mask\'] return tf.py_function(func=encode, inp=[inputs], Tout=[tf.int32, tf.int32, tf.int32]) def process(self, i, t, a): def __call(i, t, a): model_output = self.model({\'input_ids\': i.numpy(), \'token_type_ids\': t.numpy(), \'attention_mask\': a.numpy()}) return model_output[0] return tf.py_function(func=__call, inp=[i, t, a], Tout=[tf.float32]) def mean_pooling(self, model_output, attention_mask): token_embeddings = tf.squeeze(tf.stack(model_output), axis=0) input_mask_expanded = tf.cast( tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)), tf.float32 ) a = tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1) b = tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max) embeddings = a / b embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1) return embeddings def call(self, inputs): input_ids, token_type_ids, attention_mask = self.tf_encode(inputs) input_ids.set_shape(tf.TensorShape((None, None))) token_type_ids.set_shape(tf.TensorShape((None, None))) attention_mask.set_shape(tf.TensorShape((None, None))) model_output = self.process(input_ids, token_type_ids, attention_mask) model_output[0].set_shape(tf.TensorShape((None, None, 384))) embeddings = self.mean_pooling(model_output, attention_mask) return embeddings sbert = SBert(tokenizer, model) inputs = tf.keras.layers.Input((1,), dtype=tf.string) outputs = sbert(inputs) outputs = tf.keras.layers.Dense(32)(outputs) model = tf.keras.Model(inputs, outputs) print(model(tf.constant([\'some text\', \'more text\']))) print(model.summary())\n\ntf.Tensor( [[-0.06719425 -0.02954631 -0.05811356 -0.1456391 -0.13001677 0.00145465 0.0401044 0.05949172 -0.02589339 0.07255618 -0.00958113 0.01159782 0.02508018 0.03075579 -0.01910635 -0.03231853 0.00875124 0.01143366 -0.04365401 -0.02090197 0.07030752 -0.02872834 0.10535908 0.05691438 -0.017165 -0.02044982 0.02580127 -0.04564123 -0.0631128 -0.00303708 0.00133517 0.01613527] [-0.11922387 0.02304137 -0.02670465 -0.13117084 -0.11492493 0.03961402 0.08129141 -0.05999354 0.0039564 0.02892766 0.00493046 0.00440936 -0.07966737 0.11354238 0.03141225 0.00048972 0.04658606 -0.03658888 -0.05292419 -0.04639702 0.08445395 0.00522146 0.04359548 0.0290177 -0.02171512 -0.03399373 -0.00418095 -0.04019783 -0.04733383 -0.03972956 0.01890458 -0.03927581]], shape=(2, 32), dtype=float32) Model: ""model_12"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_18 (InputLayer) [(None, 1)] 0 s_bert_17 (SBert) (None, 384) 22713216 dense_78 (Dense) (None, 32) 12320 ================================================================= Total params: 22,725,536 Trainable params: 22,725,536 Non-trainable params: 0 _________________________________________________________________ None\n\nanswered Mar 10, 2022 at 8:57\n\nAloneTogetherAloneTogether\n\n26.3k55 gold badges2020 silver badges4040 bronze badges 2\n\nIt does, thank you so much! I do find it strange that this task has so many nuances, it seems like a fairly trivial model / use-case...\n\n– dendog Mar 10, 2022 at 11:51\n\nIntegrating the Bert model is just not so trivial, since it does not work entirely in Graph mode.\n\n– AloneTogether Mar 10, 2022 at 11:53\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nhuggingface-transformers\n\nsentence-transformers or ask your own question.\n\nNLP Collective Join the discussion\n\nThis question is in a collective: a subcommunity defined by tags with relevant content and experts.\n\nCommunity products: Reflections & looking ahead\n\nControlling cloud costs: Where to start, and where to go from there\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n1 Use `sentence-transformers` inside of a keras model\n\n6 Tensorflow: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\n5 ValueError: Input 0 of layer dense is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\n\n16 Error: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\n5 ValueError: dimension of the inputs to `Dense` should be defined. Found `None`\n\n13 ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: [None, 2584]\n\n4 Input 0 of layer sequential is incompatible with the layer expected ndim=3, found ndim=2. Full shape received: [None, 1]\n\n0 Input dense is incompatible with the layer invalid shape\n\n0 ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (10, 30))\n\n2 ValueError: The last dimension of the inputs to a Dense layer should be defined. Found None\n\n0 Input 0 of layer ""dense"" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (32,)\n\nHot Network Questions\n\nWhy so much kinetic energy inside a proton?\n\nWhat is the difference between \'withdrawn\' and \'retracted\'?\n\nMacro to convert military time to standard time\n\nHow to build a defensive wall that Stone Mages can\'t easily deconstruct?\n\nPentatonic and Chinese tradition - what I am getting wrong?\n\nCould a planet orbit a gravitational wave concentration\n\nShort Story. Super intelligent telepathic unborn child. Communicates telepathically with his mother. Writes a well received book\n\nSwifty Argument Labels\n\nDid Peter Pan, in the novel, kill every one of the lost boys as they showed signs of starting to grow up?\n\nHow to use existing curtain rod bracket with a flat hook\n\nShould a footnote number be placed after a plural\'s S?\n\nWhy is Dawkins not respected amongst philosophers?\n\nCan we know that something exists even if we can\'t explain it or define it?\n\nCan you use inductors for emitter degeneration?\n\nHow to get SPF alignment to pass DMARC for a subdomain?\n\nWhy is the bilinearity of an elliptic curve pairing shown as multiplicative rather than additive?\n\nAre millennials and generation Z more politically conscious than previous generations?\n\nConvert integer to its ASCII character\n\nMoon calendar with a permanent full Moon\n\nUsing a planetary flyby to reduce speed\n\nHow to validate a float input to 2 precision in panel?\n\nHow to create sublists of a list based on the relation between their elements?\n\nShould a cs teacher teach non-scientific methods?\n\nBuilding a ""hello, world!"" program using MPW more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_4', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nThe last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: <unknown>\n\nModified 2 years ago\n\nPart of NLP Collective\n\nI am having trouble when switching a model from some local dummy data to using a TF dataset.\n\nSorry for the long model code, I have tried to shorten it as much as possible.\n\nThe following works fine:\n\nimport tensorflow as tf import tensorflow_recommenders as tfrs from transformers import AutoTokenizer, TFAutoModel MODEL_PATH = \'sentence-transformers/all-MiniLM-L6-v2\' tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) model = TFAutoModel.from_pretrained(MODEL_PATH, from_pt=True) class SBert(tf.keras.layers.Layer): def __init__(self, tokenizer, model): super(SBert, self).__init__() self.tokenizer = tokenizer self.model = model def tf_encode(self, inputs): def encode(inputs): inputs = [x[0].decode(""utf-8"") for x in inputs.numpy()] outputs = self.tokenizer(inputs, padding=True, truncation=True, return_tensors=\'tf\') return outputs[\'input_ids\'], outputs[\'token_type_ids\'], outputs[\'attention_mask\'] return tf.py_function(func=encode, inp=[inputs], Tout=[tf.int32, tf.int32, tf.int32]) def process(self, i, t, a): def __call(i, t, a): model_output = self.model( {\'input_ids\': i.numpy(), \'token_type_ids\': t.numpy(), \'attention_mask\': a.numpy()} ) return model_output[0] return tf.py_function(func=__call, inp=[i, t, a], Tout=[tf.float32]) def mean_pooling(self, model_output, attention_mask): token_embeddings = tf.squeeze(tf.stack(model_output), axis=0) input_mask_expanded = tf.cast( tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)), tf.float32 ) a = tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1) b = tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max) embeddings = a / b embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1) return embeddings def call(self, inputs): input_ids, token_type_ids, attention_mask = self.tf_encode(inputs) model_output = self.process(input_ids, token_type_ids, attention_mask) embeddings = self.mean_pooling(model_output, attention_mask) return embeddings sbert = SBert(tokenizer, model) inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string) outputs = sbert(inputs) model = tf.keras.Model(inputs, outputs) model(tf.constant([\'some text\', \'more text\']))\n\nThe call to the model outputs tensors - yipee :)\n\nNow I want to use this layer inside of a larger two tower model:\n\nclass Encoder(tf.keras.Model): def __init__(self): super().__init__() self.text_embedding = self._build_text_embedding() def _build_text_embedding(self): sbert = SBert(tokenizer, model) inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string) outputs = sbert(inputs) return tf.keras.Model(inputs, outputs) def call(self, inputs): return self.text_embedding(inputs) class RecModel(tfrs.models.Model): def __init__(self): super().__init__() self.query_model = tf.keras.Sequential([ Encoder(), tf.keras.layers.Dense(32) ]) self.candidate_model = tf.keras.Sequential([ Encoder(), tf.keras.layers.Dense(32) ]) self.retrieval_task = tfrs.tasks.Retrieval( metrics=tfrs.metrics.FactorizedTopK( candidates=tf.data.Dataset.from_tensor_slices( data[\'text\'] ).batch(1).map(self.candidate_model), ), batch_metrics=[ tf.keras.metrics.TopKCategoricalAccuracy(k=5) ] ) def call(self, features): query_embeddings = self.query_model(features[\'query\']) candidate_embeddings = self.candidate_model(features[\'text\']) return ( query_embeddings, candidate_embeddings, ) def compute_loss(self, features, training=False): query_embeddings, candidate_embeddings = self(features) retrieval_loss = self.retrieval_task(query_embeddings, candidate_embeddings) return retrieval_loss\n\nCreate a small dummy dataset:\n\ndata = { \'query\': [\'blue\', \'cat\', \'football\'], \'text\': [\'a nice colour\', \'a type of animal\', \'a sport\'] } ds = tf.data.Dataset.from_tensor_slices(data).batch(1)\n\nmodel = RecModel() model.compile(optimizer=tf.keras.optimizers.Adagrad())\n\nAnd we hit the following error:\n\n--------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-11-df4cc46e0307> in <module> ----> 1 model = RecModel() 2 model.compile(optimizer=tf.keras.optimizers.Adagrad()) <ipython-input-8-a774041744b9> in __init__(self) 33 candidates=tf.data.Dataset.from_tensor_slices( 34 data[\'text\'] ---> 35 ).batch(1).map(self.candidate_model), 36 ), 37 batch_metrics=[ ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic, name) 2014 warnings.warn(""The `deterministic` argument has no effect unless the "" 2015 ""`num_parallel_calls` argument is specified."") -> 2016 return MapDataset(self, map_func, preserve_cardinality=True, name=name) 2017 else: 2018 return ParallelMapDataset( ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name) 5193 self._transformation_name(), 5194 dataset=input_dataset, -> 5195 use_legacy_function=use_legacy_function) 5196 self._metadata = dataset_metadata_pb2.Metadata() 5197 if name: ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs) 269 fn_factory = trace_tf_function(defun_kwargs) 270 --> 271 self._function = fn_factory() 272 # There is no graph to add in eager mode. 273 add_to_graph &= not context.executing_eagerly() ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs) 3069 """""" 3070 graph_function = self._get_concrete_function_garbage_collected( -> 3071 *args, **kwargs) 3072 graph_function._garbage_collector.release() # pylint: disable=protected-access 3073 return graph_function ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs) 3034 args, kwargs = None, None 3035 with self._lock: -> 3036 graph_function, _ = self._maybe_define_function(args, kwargs) 3037 seen_names = set() 3038 captured = object_identity.ObjectIdentitySet( ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs) 3290 3291 self._function_cache.add_call_context(cache_key.call_context) -> 3292 graph_function = self._create_graph_function(args, kwargs) 3293 self._function_cache.add(cache_key, cache_key_deletion_observer, 3294 graph_function) ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes) 3138 arg_names=arg_names, 3139 override_flat_arg_shapes=override_flat_arg_shapes, -> 3140 capture_by_value=self._capture_by_value), 3141 self._function_attributes, 3142 function_spec=self.function_spec, ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses) 1159 _, original_func = tf_decorator.unwrap(python_func) 1160 -> 1161 func_outputs = python_func(*func_args, **func_kwargs) 1162 1163 # invariant: `func_outputs` contains only Tensors, CompositeTensors, ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py in wrapped_fn(*args) 246 attributes=defun_kwargs) 247 def wrapped_fn(*args): # pylint: disable=missing-docstring --> 248 ret = wrapper_helper(*args) 249 ret = structure.to_tensor_list(self._output_structure, ret) 250 return [ops.convert_to_tensor(t) for t in ret] ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py in wrapper_helper(*args) 175 if not _should_unpack(nested_args): 176 nested_args = (nested_args,) --> 177 ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args) 178 if _should_pack(ret): 179 ret = tuple(ret) ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs) 687 try: 688 with conversion_ctx: --> 689 return converted_call(f, args, kwargs, options=options) 690 except Exception as e: # pylint:disable=broad-except 691 if hasattr(e, \'ag_error_metadata\'): ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options) 375 376 if not options.user_requested and conversion.is_allowlisted(f): --> 377 return _call_unconverted(f, args, kwargs, options) 378 379 # internal_convert_user_code is for example turned off when issuing a dynamic ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache) 456 457 if kwargs is not None: --> 458 return f(*args, **kwargs) 459 return f(*args) 460 ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs) 65 except Exception as e: # pylint: disable=broad-except 66 filtered_tb = _process_traceback_frames(e.__traceback__) ---> 67 raise e.with_traceback(filtered_tb) from None 68 finally: 69 del filtered_tb ~/.pyenv/versions/3.7.8/lib/python3.7/site-packages/keras/layers/core/dense.py in build(self, input_shape) 137 last_dim = tf.compat.dimension_value(input_shape[-1]) 138 if last_dim is None: --> 139 raise ValueError(\'The last dimension of the inputs to a Dense layer \' 140 \'should be defined. Found None. \' 141 f\'Full input shape received: {input_shape}\') ValueError: Exception encountered when calling layer ""sequential_5"" (type Sequential). The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: <unknown> Call arguments received: • inputs=tf.Tensor(shape=(None,), dtype=string) • training=None • mask=None\n\nI am not quite sure where I should set the shape - as using regular tensors and not TF dataset works ok.\n\nhuggingface-transformers\n\nsentence-transformers\n\nImprove this question\n\nasked Mar 9, 2022 at 18:53\n\n3,13655 gold badges2626 silver badges6767 bronze badges 3\n\nCan you add the model.fit(…) part to your question?\n\n– AloneTogether Mar 9, 2022 at 21:49\n\n@AloneTogether the error occurs before that, so I never call model.fit because we map over all the unique elements of the data in the compilation of the model in the retrieval_task.\n\n– dendog Mar 10, 2022 at 8:50\n\nHmm, yeah I understand..the output shape is unknown..\n\n– AloneTogether Mar 10, 2022 at 9:13\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou will have to explicitly set the shapes of the tensors coming from tf.py_functions. Using None will allow variable input lengths. The Bert output dimension (384,) is, however, necessary:\n\nimport tensorflow as tf from transformers import AutoTokenizer, TFAutoModel MODEL_PATH = \'sentence-transformers/all-MiniLM-L6-v2\' tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) model = TFAutoModel.from_pretrained(MODEL_PATH, from_pt=True) class SBert(tf.keras.layers.Layer): def __init__(self, tokenizer, model): super(SBert, self).__init__() self.tokenizer = tokenizer self.model = model def tf_encode(self, inputs): def encode(inputs): inputs = [x[0].decode(""utf-8"") for x in inputs.numpy()] outputs = self.tokenizer(inputs, padding=True, truncation=True, return_tensors=\'tf\') return outputs[\'input_ids\'], outputs[\'token_type_ids\'], outputs[\'attention_mask\'] return tf.py_function(func=encode, inp=[inputs], Tout=[tf.int32, tf.int32, tf.int32]) def process(self, i, t, a): def __call(i, t, a): model_output = self.model({\'input_ids\': i.numpy(), \'token_type_ids\': t.numpy(), \'attention_mask\': a.numpy()}) return model_output[0] return tf.py_function(func=__call, inp=[i, t, a], Tout=[tf.float32]) def mean_pooling(self, model_output, attention_mask): token_embeddings = tf.squeeze(tf.stack(model_output), axis=0) input_mask_expanded = tf.cast( tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)), tf.float32 ) a = tf.math.reduce_sum(token_embeddings * input_mask_expanded, axis=1) b = tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max) embeddings = a / b embeddings, _ = tf.linalg.normalize(embeddings, 2, axis=1) return embeddings def call(self, inputs): input_ids, token_type_ids, attention_mask = self.tf_encode(inputs) input_ids.set_shape(tf.TensorShape((None, None))) token_type_ids.set_shape(tf.TensorShape((None, None))) attention_mask.set_shape(tf.TensorShape((None, None))) model_output = self.process(input_ids, token_type_ids, attention_mask) model_output[0].set_shape(tf.TensorShape((None, None, 384))) embeddings = self.mean_pooling(model_output, attention_mask) return embeddings sbert = SBert(tokenizer, model) inputs = tf.keras.layers.Input((1,), dtype=tf.string) outputs = sbert(inputs) outputs = tf.keras.layers.Dense(32)(outputs) model = tf.keras.Model(inputs, outputs) print(model(tf.constant([\'some text\', \'more text\']))) print(model.summary())\n\ntf.Tensor( [[-0.06719425 -0.02954631 -0.05811356 -0.1456391 -0.13001677 0.00145465 0.0401044 0.05949172 -0.02589339 0.07255618 -0.00958113 0.01159782 0.02508018 0.03075579 -0.01910635 -0.03231853 0.00875124 0.01143366 -0.04365401 -0.02090197 0.07030752 -0.02872834 0.10535908 0.05691438 -0.017165 -0.02044982 0.02580127 -0.04564123 -0.0631128 -0.00303708 0.00133517 0.01613527] [-0.11922387 0.02304137 -0.02670465 -0.13117084 -0.11492493 0.03961402 0.08129141 -0.05999354 0.0039564 0.02892766 0.00493046 0.00440936 -0.07966737 0.11354238 0.03141225 0.00048972 0.04658606 -0.03658888 -0.05292419 -0.04639702 0.08445395 0.00522146 0.04359548 0.0290177 -0.02171512 -0.03399373 -0.00418095 -0.04019783 -0.04733383 -0.03972956 0.01890458 -0.03927581]], shape=(2, 32), dtype=float32) Model: ""model_12"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_18 (InputLayer) [(None, 1)] 0 s_bert_17 (SBert) (None, 384) 22713216 dense_78 (Dense) (None, 32) 12320 ================================================================= Total params: 22,725,536 Trainable params: 22,725,536 Non-trainable params: 0 _________________________________________________________________ None\n\nanswered Mar 10, 2022 at 8:57\n\nAloneTogetherAloneTogether\n\n26.3k55 gold badges2020 silver badges4040 bronze badges 2\n\nIt does, thank you so much! I do find it strange that this task has so many nuances, it seems like a fairly trivial model / use-case...\n\n– dendog Mar 10, 2022 at 11:51\n\nIntegrating the Bert model is just not so trivial, since it does not work entirely in Graph mode.\n\n– AloneTogether Mar 10, 2022 at 11:53\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nhuggingface-transformers\n\nsentence-transformers or ask your own question.\n\nNLP Collective Join the discussion\n\nThis question is in a collective: a subcommunity defined by tags with relevant content and experts.\n\nCommunity products: Reflections & looking ahead\n\nControlling cloud costs: Where to start, and where to go from there\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n1 Use `sentence-transformers` inside of a keras model\n\n6 Tensorflow: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\n5 ValueError: Input 0 of layer dense is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\n\n16 Error: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`\n\n5 ValueError: dimension of the inputs to `Dense` should be defined. Found `None`\n\n13 ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: [None, 2584]\n\n4 Input 0 of layer sequential is incompatible with the layer expected ndim=3, found ndim=2. Full shape received: [None, 1]\n\n0 Input dense is incompatible with the layer invalid shape\n\n0 ValueError: Shape mismatch: The shape of labels (received (1,)) should equal the shape of logits except for the last dimension (received (10, 30))\n\n2 ValueError: The last dimension of the inputs to a Dense layer should be defined. Found None\n\n0 Input 0 of layer ""dense"" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (32,)\n\nHot Network Questions\n\nWhy so much kinetic energy inside a proton?\n\nWhat is the difference between \'withdrawn\' and \'retracted\'?\n\nMacro to convert military time to standard time\n\nHow to build a defensive wall that Stone Mages can\'t easily deconstruct?\n\nPentatonic and Chinese tradition - what I am getting wrong?\n\nCould a planet orbit a gravitational wave concentration\n\nShort Story. Super intelligent telepathic unborn child. Communicates telepathically with his mother. Writes a well received book\n\nSwifty Argument Labels\n\nDid Peter Pan, in the novel, kill every one of the lost boys as they showed signs of starting to grow up?\n\nHow to use existing curtain rod bracket with a flat hook\n\nShould a footnote number be placed after a plural\'s S?\n\nWhy is Dawkins not respected amongst philosophers?\n\nCan we know that something exists even if we can\'t explain it or define it?\n\nCan you use inductors for emitter degeneration?\n\nHow to get SPF alignment to pass DMARC for a subdomain?\n\nWhy is the bilinearity of an elliptic curve pairing shown as multiplicative rather than additive?\n\nAre millennials and generation Z more politically conscious than previous generations?\n\nConvert integer to its ASCII character\n\nMoon calendar with a permanent full Moon\n\nUsing a planetary flyby to reduce speed\n\nHow to validate a float input to 2 precision in panel?\n\nHow to create sublists of a list based on the relation between their elements?\n\nShould a cs teacher teach non-scientific methods?\n\nBuilding a ""hello, world!"" program using MPW more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-03-28T17:37:03', 'title': 'python - The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: - Stack Overflow', 'url': 'https://stackoverflow.com/questions/71414627/the-last-dimension-of-the-inputs-to-a-dense-layer-should-be-defined-found-none'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nThe last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)\n\nAsked 1 year, 8 months ago\n\nModified 1 year, 8 months ago\n\nI am trying to build a binary Prediction model through Keras TensorFlow. And I am having trouble when I add data augmentation inside. This is my code.\n\ntrain_datagen = ImageDataGenerator(rescale=1./255) test_datagen = ImageDataGenerator(rescale=1./255) train_dir = \'C:/train\' test_dir = \'C:/test\' train_data = train_datagen.flow_from_directory(train_dir,target_size=(224,224),class_mode=\'binary\',seed=42) test_data = test_datagen.flow_from_directory(test_dir,target_size=(224,224),class_mode=\'binary\',seed=42) tf.random.set_seed(42) from tensorflow.keras.layers.experimental import preprocessing data_augmentation = keras.Sequential([ preprocessing.RandomFlip(""horizontal""), preprocessing.RandomZoom(0.2), preprocessing.RandomRotation(0.2), preprocessing.RandomHeight(0.2), preprocessing.RandomWidth(0.2), ], name=\'data_augmentation\') model_1 = tf.keras.Sequential([ tf.keras.layers.Input(shape=(224,224,3),name=\'input_layer\'), data_augmentation, tf.keras.layers.Conv2D(20,3,activation=\'relu\'), tf.keras.layers.MaxPool2D(pool_size=2), tf.keras.layers.Conv2D(20,3,activation=\'relu\'), tf.keras.layers.MaxPool2D(pool_size=2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(1,activation=\'sigmoid\') ]) model_1.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=\'Adam\', metrics=[\'accuracy\']) model_1.fit(train_data,epochs=10,validation_data=test_data)\n\nI ready tried this way but error again\n\ninputs = tf.keras.layers.Input(shape=(224,224,3),name=\'input_layer\') x = data_augmentation(inputs) x = tf.keras.layers.Conv2D(20,3,activation=\'relu\')(x) x = tf.keras.layers.Conv2D(20,3,activation=\'relu\')(x) x = tf.keras.layers.MaxPool2D(pool_size=2)(x) x = tf.keras.layers.Flatten()(x) outputs = tf.keras.layers.Dense(1,activation=\'sigmoid\')(x) model_1 = tf.keras.Model(inputs,outputs)\n\nand this is error message:\n\nTraceback (most recent call last): File ""C:/Users/pondy/PycharmProjects/pythonProject2/main.py"", line 60, in <module> model_1 = tf.keras.Sequential([ File ""C:\\Users\\pondy\\PycharmProjects\\pythonProject2\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py"", line 530, in _method_wrapper result = method(self, *args, **kwargs) File ""C:\\Users\\pondy\\PycharmProjects\\pythonProject2\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py"", line 67, in error_handler raise e.with_traceback(filtered_tb) from None File ""C:\\Users\\pondy\\PycharmProjects\\pythonProject2\\venv\\lib\\site-packages\\keras\\layers\\core\\dense.py"", line 139, in build raise ValueError(\'The last dimension of the inputs to a Dense layer \' ValueError: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None) Process finished with exit code 1\n\nif didn\'t add data_augmentation inside it\'s not error thank you for help<3\n\nImprove this question\n\nasked Jul 10, 2022 at 10:00\n\nVarintorn SithisintVarintorn Sithisint\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nyour code will work if you remove\n\npreprocessing.RandomHeight(0.2), preprocessing.RandomWidth(0.2),\n\nanswered Jul 10, 2022 at 23:30\n\n7,90233 gold badges1414 silver badges2222 bronze badges 2\n\nThank you. It work!! but that because preprocessing.RandomHeight(0.2), preprocessing.RandomWidth(0.2), will change size of image right?\n\n– Varintorn Sithisint Jul 11, 2022 at 4:30\n\nyes I believe that is what causes the problem\n\n– Gerry P Jul 11, 2022 at 14:58\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ndata-augmentation or ask your own question.\n\nCommunity products: Reflections & looking ahead\n\nControlling cloud costs: Where to start, and where to go from there\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n5 ValueError: Input 0 of layer dense is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\n\n22 ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [8, 28, 28]\n\n13 ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: [None, 2584]\n\n4 Input 0 of layer sequential is incompatible with the layer expected ndim=3, found ndim=2. Full shape received: [None, 1]\n\n0 ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [None, 32, 32]\n\n0 My Input shape is correct but I stil l get the following err.: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4,\n\n1 Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape (None, 1)\n\n1 ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 28, 28)\n\n3 The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: <unknown>\n\n0 why I\'m getting this error ""Input 0 of layer ""dense_30"" is incompatible with the layer:expected min_ndim=2,found ndim=1. Full shape received: (None,)""\n\nHot Network Questions\n\nHow to disclose previous job being terminated without cause (because of high level security clearance)\n\nwhy does assigning a var with command substitution then echoing that var always fail?\n\nAuto Loan Bankruptcy Question\n\nWas the MS-DOS window in Windows 95 an emulator like current DOSBox?\n\nimprove speed and climbing Cannondale CAAD8 Ultra\n\nWhy are most philosophers non-theists and most non-philosophers theists?\n\nStandards for data availability for internal employees\n\nHow to get SPF alignment to pass DMARC for a subdomain?\n\nCan we know that something exists even if we can\'t explain it or define it?\n\n75 integers are squared or cubed: minimum distinct results?\n\nCan I bring 500 grams of gold while travelling to Canada for 2 weeks on a visit visa?\n\nWhy is the Swiss Political Model not Replicated / Proposed Elsewhere?\n\nLooking for Esek Hashchita\n\nHow are these two conditions equivalent?\n\nIs it decidable whether two real algebraic irrationals generate the same extension of the rationals?\n\nWhy might an auction go 1NT-3NT?\n\nReconstruct list from its PositionIndex\n\nMacro to convert military time to standard time\n\nWhat is this celestial phenomenon in the video?\n\nConvert integer to its ASCII character\n\nCan you use inductors for emitter degeneration?\n\nAttribute nouns without an article in programmer-speak\n\nIn The Shadow of the Torturer, the torturers live in a tower made from metal. Is it ever stated that this tower is a long disused spaceship?\n\nHow to validate a float input to 2 precision in panel? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_5', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nThe last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)\n\nAsked 1 year, 8 months ago\n\nModified 1 year, 8 months ago\n\nI am trying to build a binary Prediction model through Keras TensorFlow. And I am having trouble when I add data augmentation inside. This is my code.\n\ntrain_datagen = ImageDataGenerator(rescale=1./255) test_datagen = ImageDataGenerator(rescale=1./255) train_dir = \'C:/train\' test_dir = \'C:/test\' train_data = train_datagen.flow_from_directory(train_dir,target_size=(224,224),class_mode=\'binary\',seed=42) test_data = test_datagen.flow_from_directory(test_dir,target_size=(224,224),class_mode=\'binary\',seed=42) tf.random.set_seed(42) from tensorflow.keras.layers.experimental import preprocessing data_augmentation = keras.Sequential([ preprocessing.RandomFlip(""horizontal""), preprocessing.RandomZoom(0.2), preprocessing.RandomRotation(0.2), preprocessing.RandomHeight(0.2), preprocessing.RandomWidth(0.2), ], name=\'data_augmentation\') model_1 = tf.keras.Sequential([ tf.keras.layers.Input(shape=(224,224,3),name=\'input_layer\'), data_augmentation, tf.keras.layers.Conv2D(20,3,activation=\'relu\'), tf.keras.layers.MaxPool2D(pool_size=2), tf.keras.layers.Conv2D(20,3,activation=\'relu\'), tf.keras.layers.MaxPool2D(pool_size=2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(1,activation=\'sigmoid\') ]) model_1.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=\'Adam\', metrics=[\'accuracy\']) model_1.fit(train_data,epochs=10,validation_data=test_data)\n\nI ready tried this way but error again\n\ninputs = tf.keras.layers.Input(shape=(224,224,3),name=\'input_layer\') x = data_augmentation(inputs) x = tf.keras.layers.Conv2D(20,3,activation=\'relu\')(x) x = tf.keras.layers.Conv2D(20,3,activation=\'relu\')(x) x = tf.keras.layers.MaxPool2D(pool_size=2)(x) x = tf.keras.layers.Flatten()(x) outputs = tf.keras.layers.Dense(1,activation=\'sigmoid\')(x) model_1 = tf.keras.Model(inputs,outputs)\n\nand this is error message:\n\nTraceback (most recent call last): File ""C:/Users/pondy/PycharmProjects/pythonProject2/main.py"", line 60, in <module> model_1 = tf.keras.Sequential([ File ""C:\\Users\\pondy\\PycharmProjects\\pythonProject2\\venv\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py"", line 530, in _method_wrapper result = method(self, *args, **kwargs) File ""C:\\Users\\pondy\\PycharmProjects\\pythonProject2\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py"", line 67, in error_handler raise e.with_traceback(filtered_tb) from None File ""C:\\Users\\pondy\\PycharmProjects\\pythonProject2\\venv\\lib\\site-packages\\keras\\layers\\core\\dense.py"", line 139, in build raise ValueError(\'The last dimension of the inputs to a Dense layer \' ValueError: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None) Process finished with exit code 1\n\nif didn\'t add data_augmentation inside it\'s not error thank you for help<3\n\nImprove this question\n\nasked Jul 10, 2022 at 10:00\n\nVarintorn SithisintVarintorn Sithisint\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nyour code will work if you remove\n\npreprocessing.RandomHeight(0.2), preprocessing.RandomWidth(0.2),\n\nanswered Jul 10, 2022 at 23:30\n\n7,90233 gold badges1414 silver badges2222 bronze badges 2\n\nThank you. It work!! but that because preprocessing.RandomHeight(0.2), preprocessing.RandomWidth(0.2), will change size of image right?\n\n– Varintorn Sithisint Jul 11, 2022 at 4:30\n\nyes I believe that is what causes the problem\n\n– Gerry P Jul 11, 2022 at 14:58\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ndata-augmentation or ask your own question.\n\nCommunity products: Reflections & looking ahead\n\nControlling cloud costs: Where to start, and where to go from there\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n5 ValueError: Input 0 of layer dense is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\n\n22 ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [8, 28, 28]\n\n13 ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=2. Full shape received: [None, 2584]\n\n4 Input 0 of layer sequential is incompatible with the layer expected ndim=3, found ndim=2. Full shape received: [None, 1]\n\n0 ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: [None, 32, 32]\n\n0 My Input shape is correct but I stil l get the following err.: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4,\n\n1 Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape (None, 1)\n\n1 ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: (None, 28, 28)\n\n3 The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: <unknown>\n\n0 why I\'m getting this error ""Input 0 of layer ""dense_30"" is incompatible with the layer:expected min_ndim=2,found ndim=1. Full shape received: (None,)""\n\nHot Network Questions\n\nHow to disclose previous job being terminated without cause (because of high level security clearance)\n\nwhy does assigning a var with command substitution then echoing that var always fail?\n\nAuto Loan Bankruptcy Question\n\nWas the MS-DOS window in Windows 95 an emulator like current DOSBox?\n\nimprove speed and climbing Cannondale CAAD8 Ultra\n\nWhy are most philosophers non-theists and most non-philosophers theists?\n\nStandards for data availability for internal employees\n\nHow to get SPF alignment to pass DMARC for a subdomain?\n\nCan we know that something exists even if we can\'t explain it or define it?\n\n75 integers are squared or cubed: minimum distinct results?\n\nCan I bring 500 grams of gold while travelling to Canada for 2 weeks on a visit visa?\n\nWhy is the Swiss Political Model not Replicated / Proposed Elsewhere?\n\nLooking for Esek Hashchita\n\nHow are these two conditions equivalent?\n\nIs it decidable whether two real algebraic irrationals generate the same extension of the rationals?\n\nWhy might an auction go 1NT-3NT?\n\nReconstruct list from its PositionIndex\n\nMacro to convert military time to standard time\n\nWhat is this celestial phenomenon in the video?\n\nConvert integer to its ASCII character\n\nCan you use inductors for emitter degeneration?\n\nAttribute nouns without an article in programmer-speak\n\nIn The Shadow of the Torturer, the torturers live in a tower made from metal. Is it ever stated that this tower is a long disused spaceship?\n\nHow to validate a float input to 2 precision in panel? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-03-28T17:37:01', 'title': 'tensorflow - The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None) - Stack Overflow', 'url': 'https://stackoverflow.com/questions/72927645/the-last-dimension-of-the-inputs-to-a-dense-layer-should-be-defined-found-none'})], [Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\ntensorflow.keras Dense layers complain if the input is a sparse Input layer. #23748\n\nJamesGlooTeam opened this issue\n\nNov 14, 2018 · 22 comments\n\ntensorflow.keras Dense layers complain if the input is a sparse Input layer. #23748\n\nJamesGlooTeam opened this issue\n\nNov 14, 2018 · 22 comments\n\nKeras related issues type:bug\n\nJamesGlooTeam commented\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave 10.14.1\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: na\n\nTensorFlow installed from (source or binary): binary\n\nTensorFlow version (use command below): 1.10.0 (v1.10.0-rc1-19-g656e7a2b34)\n\nBazel version (if compiling from source): NA\n\nGCC/Compiler version (if compiling from source):\n\nGPU model and memory:\n\nYou can collect some of this information using our environment capture script You can also obtain the TensorFlow version with python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""\n\nDescribe the current behavior See below, which I ran on a OSX Mojave Macbook Pro (Early 2015), ipython running python 3.5, tensorflow 1.10.0:\n\nIn [1]: from tensorflow.keras.models import Model //anaconda/envs/dssm/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module \'tensorflow.python.framework.fast_tensor_util\' does not match runtime version 3.5 return f(*args, **kwds) In [2]: from tensorflow.keras.layers import Input, Dense In [3]: i = Input((4,), sparse=True) In [4]: d = Dense(4)(i) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-4-0fb73bda26dc> in <module> ----> 1 d = Dense(4)(i) //anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs) 718 719 # Check input assumptions set before layer building, e.g. input rank. --> 720 self._assert_input_compatibility(inputs) 721 if input_list and self._dtype is None: 722 try: //anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in _assert_input_compatibility(self, inputs) 1408 spec.min_ndim is not None or 1409 spec.max_ndim is not None): -> 1410 if x.shape.ndims is None: 1411 raise ValueError(\'Input \' + str(input_index) + \' of layer \' + 1412 self.name + \' is incompatible with the layer: \' AttributeError: \'SparseTensor\' object has no attribute \'shape\'\n\nDescribe the expected behavior\n\nIf I were using normal Keras, I\'d expect no errors trying to do the above and for the model to compile subsequently without issue.\n\nCode to reproduce the issue See the code in my snippet above.\n\nThe text was updated successfully, but these errors were encountered:\n\nHarshini-Gadige assigned karmel\n\n@omalleyt12 -- I recall you worked on something similar recently; can you comment on what is expected here?\n\nNote that in 1.12 the above gives a different error:\n\n--------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-4-0347ad7938f4> in <module>() 2 from tensorflow.keras.layers import Input, Dense 3 i = Input(shape=(4,), sparse=True) ----> 4 d = Dense(4)(i) google3/third_party/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs) 532 if not self.built: 533 # Build layer if applicable (if the `build` method has been overridden). --> 534 self._maybe_build(inputs) 535 # We must set self.built since user defined build functions are not 536 # constrained to set self.built. google3/third_party/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs) 1592 # Only call `build` if the user has manually overridden the build method. 1593 if not hasattr(self.build, \'_is_default\'): -> 1594 self.build(input_shapes) 1595 1596 google3/third_party/tensorflow/python/keras/layers/core.py in build(self, input_shape) 928 input_shape = tensor_shape.TensorShape(input_shape) 929 if tensor_shape.dimension_value(input_shape[-1]) is None: --> 930 raise ValueError(\'The last dimension of the inputs to `Dense` \' 931 \'should be defined. Found `None`.\') 932 last_dim = tensor_shape.dimension_value(input_shape[-1]) ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\n\n@karmel @omalleyt12 I looked into this issue last week. There are several things that need to be fixed. One is that sparse.placeholder could not recognize (None, 4). It will always converts to (None, None) so that the error of:\n\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\n\nI created a PR #24048 to fix the above shape inforamtion issue first.\n\nThere might be some other places that need to be fixed to make @JamesGlooTeam example work though.\n\n(Note: as pointed out by @omalleyt12, the error is based in 1.12 and is different from the original error posted by @JamesGlooTeam )\n\nomalleyt12 commented\n\nThis is because for SparseTensors, the shape is actually a Tensor. Tensors can\'t have None values.\n\nYou can get around this on the Input layer by defining your batch_size:\n\nx = keras.Input(batch_size=10, shape=(4,), sparse=True)\n\nHowever, Dense layers (and most layers in general it seems) don\'t support sparse inputs, so you would need to subclass Layer in order to call tf.sparse.sparse_dense_matmul on your inputs, or create a Lambda layer to convert your sparse inputs to dense.\n\nomalleyt12 commented\n\nIdeally, the shape of a SparseTensor would probably be a TensorShape, but I think that would be a pretty substantial rewrite of a lot of the sparse ops in order to allow None values to flow through\n\n@omalleyt12 The PR #24048 propose to convert (None, 4) to (-1, 4) for the shape tensor then the information could be preserved without making drastic changes.\n\nHi, I got the same issue, if the input is sparse then even when I apply the Dense layer I got an error. That\'s too bad because to train a neural network, specifying sparse=True in the input layer makes the learning phase 5 times faster ...\n\nAnyone has a solution for that ?\n\nheydatatalks commented\n\n@omalleyt12 -- I recall you worked on something similar recently; can you comment on what is expected here?\n\nNote that in 1.12 the above gives a different error:\n\n--------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-4-0347ad7938f4> in <module>() 2 from tensorflow.keras.layers import Input, Dense 3 i = Input(shape=(4,), sparse=True) ----> 4 d = Dense(4)(i) google3/third_party/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs) 532 if not self.built: 533 # Build layer if applicable (if the `build` method has been overridden). --> 534 self._maybe_build(inputs) 535 # We must set self.built since user defined build functions are not 536 # constrained to set self.built. google3/third_party/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs) 1592 # Only call `build` if the user has manually overridden the build method. 1593 if not hasattr(self.build, \'_is_default\'): -> 1594 self.build(input_shapes) 1595 1596 google3/third_party/tensorflow/python/keras/layers/core.py in build(self, input_shape) 928 input_shape = tensor_shape.TensorShape(input_shape) 929 if tensor_shape.dimension_value(input_shape[-1]) is None: --> 930 raise ValueError(\'The last dimension of the inputs to `Dense` \' 931 \'should be defined. Found `None`.\') 932 last_dim = tensor_shape.dimension_value(input_shape[-1]) ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\n\ni got the same error in tf1.13. Anyones has a quick local fix solution? Thanks a lot!\n\nomalleyt12 commented\n\nSparseTensors aren\'t supported in Dense layer currently, although this is something we are considering. You can implement a custom layer that calls tf.sparse.sparse_dense_matmul on your SparseTensor\n\n@omalleyt12 Is there some progress on this issue?\n\nI\'d like to build a large sparse logistic regression model with Keras and having a dense layer supporting sparse input in Keras would be quite cool.\n\nShould I wait for such a feature landing in Keras or should I implement my own layer?\n\nIs there already an existing snippet with such a layer somewhere?\n\nSharoneDayan commented\n\nMaybe, the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out : https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1\n\nMaybe, the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out : https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1\n\nOn TensorFlow 2.0.0-rc0 I get ""ValueError: The two structures don\'t have the same nested structure."" trying your DenseLayerForSparse layer. Using sparse inputs as to regular Dense gives the ""ValueError: The last dimension of the inputs to Dense should be defined. Found None."" There doesn\'t seem to be any simple fix to this that doesn\'t involve in-depth understanding of TF2 internals..\n\nFixing the batch_size to all inputs as @omalleyt12 suggests gives the same ""two structures don\'t have the same nested structure"" error, and it seems that SparseTensorSpec gets the training dataset size as its first axis, whereas the SparseTensor -structure gets the batch_size as its first axis. Same when trying the DenseLayerSparse workaround.\n\nSparse inputs have worked out of the box for Keras so far, as in every other DL backend. These are a common use case with document and graph data, so it\'s strange that this feature is not working so close to TF2 full release, especially since SparseTensors are one of the advertised new features.\n\njvishnuvardhan mentioned this issue\n\ntf.keras.layers.Input has undefined shape when setting sparse=True, making it impossible to use in a Model #32737\n\ndanielegrattarola commented\n\nCan confirm that updating to >=1.14 breaks most code dealing with sparse data, which is especially relevant for graphs and text as @anttttti mentioned.\n\nThis does not seem to be an issue that can be easily worked around, and the only stable solution is to use Keras 2.2.5 (not 2.3, because that\'s broken too since it was made to be similar to tf.keras). This seems like a huge step back and effectively makes it impossible to use sparse tensors in a Keras model.\n\nFor tensorflow 2.0 (not rc):\n\nx = Input(shape=(32,), sparse=True) y = Dense(1, activation=\'sigmoid\')(x)\n\nValueError: The last dimension of the inputs to Dense should be defined. Found None.\n\nAnyone know how to resolve?\n\nquangkevin commented\n\nHi @kechan, did you figure out the solution? I\'m running into the same issue too.\n\n@quangkevin No. I haven\'t looked into it again. I thought i just use Masking for my particular model and move on. But try install tf.nightly and see if the issue is still there. it appears this ""bug"" has been opened for a long while.\n\nmmichaelzhang commented\n\nThis is because for SparseTensors, the shape is actually a Tensor. Tensors can\'t have None values.\n\nYou can get around this on the Input layer by defining your batch_size:\n\nx = keras.Input(batch_size=10, shape=(4,), sparse=True)\n\nHowever, Dense layers (and most layers in general it seems) don\'t support sparse inputs, so you would need to subclass Layer in order to call tf.sparse.sparse_dense_matmul on your inputs, or create a Lambda layer to convert your sparse inputs to dense.\n\nI have tried using the tf.sparse.sparse_dense_matmul() function to convert my sparse tensor into a dense one. But it still cannot fit into a dense layer:\n\nfor p, dim in zip(adjacency_powers, dim_per_power): net_p = adj_times_x(sparse_adjacency, x, p)\n\nwith tf.variable_scope(\'r%i_l%i_p%s\' % (replica, layer_id, str(p))): layer = tf.layers.Dense( dim, kernel_regularizer=kernel_regularizer, activation=None, use_bias=False) net_p = layer.apply(net_p)\n\ndef adj_times_x(adj, x, adj_pow=1): """"""Multiplies (adj^adj_pow)*x."""""" for i in range(adj_pow): x = tf.sparse_tensor_dense_matmul(adj, x) return x\n\nand the error is still ""ValueError: The last dimension of the inputs to Dense should be defined. Found None.""\n\nAny suggestions regarding this?\n\nThis is not fixed in 2.1.0 :( This is a major blocker for the Spektral library. Can we get some attention on this issue?\n\nkarmel assigned tanzhenyu and unassigned karmel\n\nymodak added comp:keras\n\nKeras related issues type:bug\n\nMarkDaoust self-assigned this\n\n@cgarciae -- can you file a new bug with a minimal repro in 2.x? This bug is very old, and it\'s hard to tell exactly what condition set doesn\'t work here.\n\nkarmel closed this as completed\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nMarkDaoust commented\n\nI just submitted the fix for this. It should be synced out shortly.\n\nMarkDaoust reopened this\n\ntensorflow-copybara closed this as completed in 74195b5\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSaduf2019 mentioned this issue\n\nragged.boolean_mask() gives a different result when executed in a model #41917\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues type:bug\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_0', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\ntensorflow.keras Dense layers complain if the input is a sparse Input layer. #23748\n\nJamesGlooTeam opened this issue\n\nNov 14, 2018 · 22 comments\n\ntensorflow.keras Dense layers complain if the input is a sparse Input layer. #23748\n\nJamesGlooTeam opened this issue\n\nNov 14, 2018 · 22 comments\n\nKeras related issues type:bug\n\nJamesGlooTeam commented\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave 10.14.1\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: na\n\nTensorFlow installed from (source or binary): binary\n\nTensorFlow version (use command below): 1.10.0 (v1.10.0-rc1-19-g656e7a2b34)\n\nBazel version (if compiling from source): NA\n\nGCC/Compiler version (if compiling from source):\n\nGPU model and memory:\n\nYou can collect some of this information using our environment capture script You can also obtain the TensorFlow version with python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""\n\nDescribe the current behavior See below, which I ran on a OSX Mojave Macbook Pro (Early 2015), ipython running python 3.5, tensorflow 1.10.0:\n\nIn [1]: from tensorflow.keras.models import Model //anaconda/envs/dssm/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module \'tensorflow.python.framework.fast_tensor_util\' does not match runtime version 3.5 return f(*args, **kwds) In [2]: from tensorflow.keras.layers import Input, Dense In [3]: i = Input((4,), sparse=True) In [4]: d = Dense(4)(i) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-4-0fb73bda26dc> in <module> ----> 1 d = Dense(4)(i) //anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs) 718 719 # Check input assumptions set before layer building, e.g. input rank. --> 720 self._assert_input_compatibility(inputs) 721 if input_list and self._dtype is None: 722 try: //anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in _assert_input_compatibility(self, inputs) 1408 spec.min_ndim is not None or 1409 spec.max_ndim is not None): -> 1410 if x.shape.ndims is None: 1411 raise ValueError(\'Input \' + str(input_index) + \' of layer \' + 1412 self.name + \' is incompatible with the layer: \' AttributeError: \'SparseTensor\' object has no attribute \'shape\'\n\nDescribe the expected behavior\n\nIf I were using normal Keras, I\'d expect no errors trying to do the above and for the model to compile subsequently without issue.\n\nCode to reproduce the issue See the code in my snippet above.\n\nThe text was updated successfully, but these errors were encountered:\n\nHarshini-Gadige assigned karmel\n\n@omalleyt12 -- I recall you worked on something similar recently; can you comment on what is expected here?\n\nNote that in 1.12 the above gives a different error:\n\n--------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-4-0347ad7938f4> in <module>() 2 from tensorflow.keras.layers import Input, Dense 3 i = Input(shape=(4,), sparse=True) ----> 4 d = Dense(4)(i) google3/third_party/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs) 532 if not self.built: 533 # Build layer if applicable (if the `build` method has been overridden). --> 534 self._maybe_build(inputs) 535 # We must set self.built since user defined build functions are not 536 # constrained to set self.built. google3/third_party/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs) 1592 # Only call `build` if the user has manually overridden the build method. 1593 if not hasattr(self.build, \'_is_default\'): -> 1594 self.build(input_shapes) 1595 1596 google3/third_party/tensorflow/python/keras/layers/core.py in build(self, input_shape) 928 input_shape = tensor_shape.TensorShape(input_shape) 929 if tensor_shape.dimension_value(input_shape[-1]) is None: --> 930 raise ValueError(\'The last dimension of the inputs to `Dense` \' 931 \'should be defined. Found `None`.\') 932 last_dim = tensor_shape.dimension_value(input_shape[-1]) ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\n\n@karmel @omalleyt12 I looked into this issue last week. There are several things that need to be fixed. One is that sparse.placeholder could not recognize (None, 4). It will always converts to (None, None) so that the error of:\n\nValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\n\nI created a PR #24048 to fix the above shape inforamtion issue first.\n\nThere might be some other places that need to be fixed to make @JamesGlooTeam example work though.\n\n(Note: as pointed out by @omalleyt12, the error is based in 1.12 and is different from the original error posted by @JamesGlooTeam )\n\nomalleyt12 commented\n\nThis is because for SparseTensors, the shape is actually a Tensor. Tensors can\'t have None values.\n\nYou can get around this on the Input layer by defining your batch_size:\n\nx = keras.Input(batch_size=10, shape=(4,), sparse=True)\n\nHowever, Dense layers (and most layers in general it seems) don\'t support sparse inputs, so you would need to subclass Layer in order to call tf.sparse.sparse_dense_matmul on your inputs, or create a Lambda layer to convert your sparse inputs to dense.\n\nomalleyt12 commented\n\nIdeally, the shape of a SparseTensor would probably be a TensorShape, but I think that would be a pretty substantial rewrite of a lot of the sparse ops in order to allow None values to flow through\n\n@omalleyt12 The PR #24048 propose to convert (None, 4) to (-1, 4) for the shape tensor then the information could be preserved without making drastic changes.\n\nHi, I got the same issue, if the input is sparse then even when I apply the Dense layer I got an error. That\'s too bad because to train a neural network, specifying sparse=True in the input layer makes the learning phase 5 times faster ...\n\nAnyone has a solution for that ?\n\nheydatatalks commented\n\n@omalleyt12 -- I recall you worked on something similar recently; can you comment on what is expected here?\n\nNote that in 1.12 the above gives a different error:\n\n--------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-4-0347ad7938f4> in <module>() 2 from tensorflow.keras.layers import Input, Dense 3 i = Input(shape=(4,), sparse=True) ----> 4 d = Dense(4)(i) google3/third_party/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs) 532 if not self.built: 533 # Build layer if applicable (if the `build` method has been overridden). --> 534 self._maybe_build(inputs) 535 # We must set self.built since user defined build functions are not 536 # constrained to set self.built. google3/third_party/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs) 1592 # Only call `build` if the user has manually overridden the build method. 1593 if not hasattr(self.build, \'_is_default\'): -> 1594 self.build(input_shapes) 1595 1596 google3/third_party/tensorflow/python/keras/layers/core.py in build(self, input_shape) 928 input_shape = tensor_shape.TensorShape(input_shape) 929 if tensor_shape.dimension_value(input_shape[-1]) is None: --> 930 raise ValueError(\'The last dimension of the inputs to `Dense` \' 931 \'should be defined. Found `None`.\') 932 last_dim = tensor_shape.dimension_value(input_shape[-1]) ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.\n\ni got the same error in tf1.13. Anyones has a quick local fix solution? Thanks a lot!\n\nomalleyt12 commented\n\nSparseTensors aren\'t supported in Dense layer currently, although this is something we are considering. You can implement a custom layer that calls tf.sparse.sparse_dense_matmul on your SparseTensor\n\n@omalleyt12 Is there some progress on this issue?\n\nI\'d like to build a large sparse logistic regression model with Keras and having a dense layer supporting sparse input in Keras would be quite cool.\n\nShould I wait for such a feature landing in Keras or should I implement my own layer?\n\nIs there already an existing snippet with such a layer somewhere?\n\nSharoneDayan commented\n\nMaybe, the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out : https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1\n\nMaybe, the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out : https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1\n\nOn TensorFlow 2.0.0-rc0 I get ""ValueError: The two structures don\'t have the same nested structure."" trying your DenseLayerForSparse layer. Using sparse inputs as to regular Dense gives the ""ValueError: The last dimension of the inputs to Dense should be defined. Found None."" There doesn\'t seem to be any simple fix to this that doesn\'t involve in-depth understanding of TF2 internals..\n\nFixing the batch_size to all inputs as @omalleyt12 suggests gives the same ""two structures don\'t have the same nested structure"" error, and it seems that SparseTensorSpec gets the training dataset size as its first axis, whereas the SparseTensor -structure gets the batch_size as its first axis. Same when trying the DenseLayerSparse workaround.\n\nSparse inputs have worked out of the box for Keras so far, as in every other DL backend. These are a common use case with document and graph data, so it\'s strange that this feature is not working so close to TF2 full release, especially since SparseTensors are one of the advertised new features.\n\njvishnuvardhan mentioned this issue\n\ntf.keras.layers.Input has undefined shape when setting sparse=True, making it impossible to use in a Model #32737\n\ndanielegrattarola commented\n\nCan confirm that updating to >=1.14 breaks most code dealing with sparse data, which is especially relevant for graphs and text as @anttttti mentioned.\n\nThis does not seem to be an issue that can be easily worked around, and the only stable solution is to use Keras 2.2.5 (not 2.3, because that\'s broken too since it was made to be similar to tf.keras). This seems like a huge step back and effectively makes it impossible to use sparse tensors in a Keras model.\n\nFor tensorflow 2.0 (not rc):\n\nx = Input(shape=(32,), sparse=True) y = Dense(1, activation=\'sigmoid\')(x)\n\nValueError: The last dimension of the inputs to Dense should be defined. Found None.\n\nAnyone know how to resolve?\n\nquangkevin commented\n\nHi @kechan, did you figure out the solution? I\'m running into the same issue too.\n\n@quangkevin No. I haven\'t looked into it again. I thought i just use Masking for my particular model and move on. But try install tf.nightly and see if the issue is still there. it appears this ""bug"" has been opened for a long while.\n\nmmichaelzhang commented\n\nThis is because for SparseTensors, the shape is actually a Tensor. Tensors can\'t have None values.\n\nYou can get around this on the Input layer by defining your batch_size:\n\nx = keras.Input(batch_size=10, shape=(4,), sparse=True)\n\nHowever, Dense layers (and most layers in general it seems) don\'t support sparse inputs, so you would need to subclass Layer in order to call tf.sparse.sparse_dense_matmul on your inputs, or create a Lambda layer to convert your sparse inputs to dense.\n\nI have tried using the tf.sparse.sparse_dense_matmul() function to convert my sparse tensor into a dense one. But it still cannot fit into a dense layer:\n\nfor p, dim in zip(adjacency_powers, dim_per_power): net_p = adj_times_x(sparse_adjacency, x, p)\n\nwith tf.variable_scope(\'r%i_l%i_p%s\' % (replica, layer_id, str(p))): layer = tf.layers.Dense( dim, kernel_regularizer=kernel_regularizer, activation=None, use_bias=False) net_p = layer.apply(net_p)\n\ndef adj_times_x(adj, x, adj_pow=1): """"""Multiplies (adj^adj_pow)*x."""""" for i in range(adj_pow): x = tf.sparse_tensor_dense_matmul(adj, x) return x\n\nand the error is still ""ValueError: The last dimension of the inputs to Dense should be defined. Found None.""\n\nAny suggestions regarding this?\n\nThis is not fixed in 2.1.0 :( This is a major blocker for the Spektral library. Can we get some attention on this issue?\n\nkarmel assigned tanzhenyu and unassigned karmel\n\nymodak added comp:keras\n\nKeras related issues type:bug\n\nMarkDaoust self-assigned this\n\n@cgarciae -- can you file a new bug with a minimal repro in 2.x? This bug is very old, and it\'s hard to tell exactly what condition set doesn\'t work here.\n\nkarmel closed this as completed\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nMarkDaoust commented\n\nI just submitted the fix for this. It should be synced out shortly.\n\nMarkDaoust reopened this\n\ntensorflow-copybara closed this as completed in 74195b5\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSaduf2019 mentioned this issue\n\nragged.boolean_mask() gives a different result when executed in a model #41917\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues type:bug\n\nYou can’t perform that action at this time.', 'timestamp': '2024-07-05T20:40:25', 'title': 'tensorflow.keras Dense layers complain if the input is a sparse Input layer. · Issue #23748 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/23748'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\ncompute_output_shape() Not Working For Custom Layer #19961\n\nChasearmer opened this issue\n\nJun 13, 2018 · 5 comments\n\ncompute_output_shape() Not Working For Custom Layer #19961\n\nChasearmer opened this issue\n\nJun 13, 2018 · 5 comments\n\nChasearmer commented\n\nI have created a custom layer (called GraphGather) in Keras, yet the output tensor prints as :\n\nTensor(""graph_gather/Tanh:0"", shape=(?, ?), dtype=float32)\n\nFor some reason the shape is being returned as (?,?), which is causing the next dense layer to raise the following error:\n\nValueError: The last dimension of the inputs to Dense should be defined. Found None.\n\nThe GraphGather layer code is as follows:\n\nclass GraphGather(tf.keras.layers.Layer): def __init__(self, batch_size, num_mols_in_batch, activation_fn=None, **kwargs): self.batch_size = batch_size self.num_mols_in_batch = num_mols_in_batch self.activation_fn = activation_fn super(GraphGather, self).__init__(**kwargs) def build(self, input_shape): super(GraphGather, self).build(input_shape) def call(self, x, **kwargs): # some operations (most of def call omitted) out_tensor = result_of_operations() # this line is pseudo code if self.activation_fn is not None: out_tensor = self.activation_fn(out_tensor) out_tensor = out_tensor return out_tensor def compute_output_shape(self, input_shape): return (self.num_mols_in_batch, 2 * input_shape[0][-1])\n\nI have also tried hardcoding compute_output_shape to be:\n\ndef compute_output_shape(self, input_shape): return (64, 150)\n\nYet the output tensor when printed is still\n\nTensor(""graph_gather/Tanh:0"", shape=(?, ?), dtype=float32)\n\nwhich causes the ValueError written above.\n\nHave written custom code\n\n*OS Platform and Distribution: Linux Ubuntu 16.04\n\nTensorFlow version (use command below): 1.5.0\n\nPython version: 3.5.5\n\nThe text was updated successfully, but these errors were encountered:\n\ntensorflowbutler assigned angerson\n\ntensorflowbutler added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ntensorflowbutler commented\n\nThank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks. Have I written custom code TensorFlow installed from Bazel version CUDA/cuDNN version GPU model and memory Exact command to reproduce\n\nChasearmer commented\n\nHave I written custom code: Yes TensorFlow installed from: Conda Bazel version: NA CUDA/cuDNN version: NA GPU model and memory: NA Exact command to reproduce: Written above\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there.\n\nIf you think we\'ve misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the issue template. Thanks!\n\nangerson closed this as completed\n\nChasearmer commented\n\n@angersson Are you positive this is not a bug? I have brought this up on Stack Overflow and hopefully that will shed light on any user error on my behalf, but if not then from my understanding this still may be a bug.\n\nhttps://stackoverflow.com/questions/51028861/tensorflow-compute-output-shape-not-working-for-custom-layer\n\nkiflowb777 mentioned this issue\n\nThe inheriting keras.layers.Layer does not call a compute_output_shape after switching to tf.keras from keras #33785\n\nkbrose mentioned this issue\n\ntf.keras custom layer does not use ""compute_output_shape"" #38296\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_2', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\ncompute_output_shape() Not Working For Custom Layer #19961\n\nChasearmer opened this issue\n\nJun 13, 2018 · 5 comments\n\ncompute_output_shape() Not Working For Custom Layer #19961\n\nChasearmer opened this issue\n\nJun 13, 2018 · 5 comments\n\nChasearmer commented\n\nI have created a custom layer (called GraphGather) in Keras, yet the output tensor prints as :\n\nTensor(""graph_gather/Tanh:0"", shape=(?, ?), dtype=float32)\n\nFor some reason the shape is being returned as (?,?), which is causing the next dense layer to raise the following error:\n\nValueError: The last dimension of the inputs to Dense should be defined. Found None.\n\nThe GraphGather layer code is as follows:\n\nclass GraphGather(tf.keras.layers.Layer): def __init__(self, batch_size, num_mols_in_batch, activation_fn=None, **kwargs): self.batch_size = batch_size self.num_mols_in_batch = num_mols_in_batch self.activation_fn = activation_fn super(GraphGather, self).__init__(**kwargs) def build(self, input_shape): super(GraphGather, self).build(input_shape) def call(self, x, **kwargs): # some operations (most of def call omitted) out_tensor = result_of_operations() # this line is pseudo code if self.activation_fn is not None: out_tensor = self.activation_fn(out_tensor) out_tensor = out_tensor return out_tensor def compute_output_shape(self, input_shape): return (self.num_mols_in_batch, 2 * input_shape[0][-1])\n\nI have also tried hardcoding compute_output_shape to be:\n\ndef compute_output_shape(self, input_shape): return (64, 150)\n\nYet the output tensor when printed is still\n\nTensor(""graph_gather/Tanh:0"", shape=(?, ?), dtype=float32)\n\nwhich causes the ValueError written above.\n\nHave written custom code\n\n*OS Platform and Distribution: Linux Ubuntu 16.04\n\nTensorFlow version (use command below): 1.5.0\n\nPython version: 3.5.5\n\nThe text was updated successfully, but these errors were encountered:\n\ntensorflowbutler assigned angerson\n\ntensorflowbutler added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ntensorflowbutler commented\n\nThank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks. Have I written custom code TensorFlow installed from Bazel version CUDA/cuDNN version GPU model and memory Exact command to reproduce\n\nChasearmer commented\n\nHave I written custom code: Yes TensorFlow installed from: Conda Bazel version: NA CUDA/cuDNN version: NA GPU model and memory: NA Exact command to reproduce: Written above\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there.\n\nIf you think we\'ve misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the issue template. Thanks!\n\nangerson closed this as completed\n\nChasearmer commented\n\n@angersson Are you positive this is not a bug? I have brought this up on Stack Overflow and hopefully that will shed light on any user error on my behalf, but if not then from my understanding this still may be a bug.\n\nhttps://stackoverflow.com/questions/51028861/tensorflow-compute-output-shape-not-working-for-custom-layer\n\nkiflowb777 mentioned this issue\n\nThe inheriting keras.layers.Layer does not call a compute_output_shape after switching to tf.keras from keras #33785\n\nkbrose mentioned this issue\n\ntf.keras custom layer does not use ""compute_output_shape"" #38296\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-16T00:02:38', 'title': 'compute_output_shape() Not Working For Custom Layer · Issue #19961 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/19961'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / hub Public\n\nYou must be signed in to change notification settings\n\nConvert dynamic shapes in hub.KerasLayer #808\n\nRishit-dagli opened this issue\n\nSep 28, 2021 · 8 comments\n\nConvert dynamic shapes in hub.KerasLayer #808\n\nRishit-dagli opened this issue\n\nSep 28, 2021 · 8 comments\n\nstat:awaiting response type:bug\n\nRishit-dagli commented\n\nI was trying to fine-tune a SavedModel, here\'s some minimal code on how I create a model to go about fine-tuning:\n\ndef get_model(num_classes=5): hub_layer = hub.KerasLayer(saved_model_dir, signature=""serving_default"", output_key=""output"") model = keras.Sequential( [ keras.layers.InputLayer((384, 384, 3)), hub_layer, keras.layers.Dense(num_classes, activation=""softmax"", input_shape = (None, 1024)), ] ) return model\n\nHere are the signatures for my SavedModel:\n\nsignature_def[\'serving_default\']: The given SavedModel SignatureDef contains the following input(s): inputs[\'input\'] tensor_info: dtype: DT_FLOAT shape: (-1, 384, 384, 3) name: serving_default_input:0 The given SavedModel SignatureDef contains the following output(s): outputs[\'output\'] tensor_info: dtype: DT_FLOAT shape: (-1, -1) name: StatefulPartitionedCall:0\n\nThis always returns outputs as (-1, 1024) but having a dynamic shape in the last axis seems to create problems with adding a Dense layer thus giving an error:\n\nValueError: The last dimension of the inputs to Dense should be defined. Found None.\n\nCould I modify this hub layer to not use dynamic outputs in the last axes which I think might fix the problem or would there be a better way?\n\nThe text was updated successfully, but these errors were encountered:\n\nCan you try setting the output_shape parameter when you create the KerasLayer? Eg:\n\nhub.KerasLayer(..., output_shape=[1024])\n\nNote that the batch size is intentionally not part of output_shape.\n\nRishit-dagli commented\n\nCan you try setting the output_shape parameter when you create the KerasLayer? Eg:\n\nhub.KerasLayer(..., output_shape=[1024])\n\nNote that the batch size is intentionally not part of output_shape.\n\n@MorganR I tried that as well, and it doesn\'t affect the error.\n\nIf the model really always outputs the same size final dimension, then the ideal solution is for the model to be updated with a more accurate signature.\n\nCan you share which model this is so we can debug further and look for other solutions?\n\nRishit-dagli commented\n\nSure thing @MorganR . Here is a minimalistic Colab Notebook of what I\'m trying and also includes a direct link to the SavedModel we created in the prequel notebooks (where it seems to work really well as a classifier, we just removed the classification head for this model): https://colab.research.google.com/drive/1EJGzibnljqxFyEeAc15sa5wAVJsVAyji?usp=sharing\n\nUsharaniPagadala assigned UsharaniPagadala and MorganR and unassigned UsharaniPagadala\n\nUsharaniPagadala added stat:awaiting tensorflower type:bug labels\n\nRishit-dagli commented\n\nAlso, until then I tried using a workaround, manually updating the signatures of the model which seemed to work at the moment; here\'s some minimal code of what I did:\n\n@tf.function() def my_predict(input): inputs = { ""my_serving_input"": input, } return { ""output"": tf.reshape( model.signatures[""serving_default""](input)[""output""], [-1, 1024] ) } my_signatures = my_predict.get_concrete_function( input=tf.TensorSpec([None, 384, 384, 3], dtype=tf.dtypes.float32, name=""input"") ) tf.saved_model.save(model, export_dir=saved_model_dir, signatures=my_signatures)\n\nI don\'t have bandwidth to look into this at the moment, so unassigning.\n\nMorganR removed their assignment\n\nUsharaniPagadala assigned akhorlin\n\nThe model in question doesn\'t seem to be hosted on tfhub.dev. The ideal solution as mentioned by MorganR is to update how the model was exported to fix the dimensions of the output tensor. If this is not possible, then calling tf.reshape() to fix up the dimensions makes sense. This what one would do when using TF directly (w/o Keras). There might be some nicer Keras specific ways of achieving this. One option is to ask a question on the TF/Keras\'s forums.\n\nakhorlin added stat:awaiting response and removed stat:awaiting tensorflower labels\n\nRishit-dagli commented\n\nGot it, thanks @akhorlin . I will continue using tf.reshape() thanks for confirming. I guess a nice Keras way to do the same would be to add a Reshape layer after the hub layer, but I will try exploring a bit more.\n\nRishit-dagli closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nstat:awaiting response type:bug\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_3', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / hub Public\n\nYou must be signed in to change notification settings\n\nConvert dynamic shapes in hub.KerasLayer #808\n\nRishit-dagli opened this issue\n\nSep 28, 2021 · 8 comments\n\nConvert dynamic shapes in hub.KerasLayer #808\n\nRishit-dagli opened this issue\n\nSep 28, 2021 · 8 comments\n\nstat:awaiting response type:bug\n\nRishit-dagli commented\n\nI was trying to fine-tune a SavedModel, here\'s some minimal code on how I create a model to go about fine-tuning:\n\ndef get_model(num_classes=5): hub_layer = hub.KerasLayer(saved_model_dir, signature=""serving_default"", output_key=""output"") model = keras.Sequential( [ keras.layers.InputLayer((384, 384, 3)), hub_layer, keras.layers.Dense(num_classes, activation=""softmax"", input_shape = (None, 1024)), ] ) return model\n\nHere are the signatures for my SavedModel:\n\nsignature_def[\'serving_default\']: The given SavedModel SignatureDef contains the following input(s): inputs[\'input\'] tensor_info: dtype: DT_FLOAT shape: (-1, 384, 384, 3) name: serving_default_input:0 The given SavedModel SignatureDef contains the following output(s): outputs[\'output\'] tensor_info: dtype: DT_FLOAT shape: (-1, -1) name: StatefulPartitionedCall:0\n\nThis always returns outputs as (-1, 1024) but having a dynamic shape in the last axis seems to create problems with adding a Dense layer thus giving an error:\n\nValueError: The last dimension of the inputs to Dense should be defined. Found None.\n\nCould I modify this hub layer to not use dynamic outputs in the last axes which I think might fix the problem or would there be a better way?\n\nThe text was updated successfully, but these errors were encountered:\n\nCan you try setting the output_shape parameter when you create the KerasLayer? Eg:\n\nhub.KerasLayer(..., output_shape=[1024])\n\nNote that the batch size is intentionally not part of output_shape.\n\nRishit-dagli commented\n\nCan you try setting the output_shape parameter when you create the KerasLayer? Eg:\n\nhub.KerasLayer(..., output_shape=[1024])\n\nNote that the batch size is intentionally not part of output_shape.\n\n@MorganR I tried that as well, and it doesn\'t affect the error.\n\nIf the model really always outputs the same size final dimension, then the ideal solution is for the model to be updated with a more accurate signature.\n\nCan you share which model this is so we can debug further and look for other solutions?\n\nRishit-dagli commented\n\nSure thing @MorganR . Here is a minimalistic Colab Notebook of what I\'m trying and also includes a direct link to the SavedModel we created in the prequel notebooks (where it seems to work really well as a classifier, we just removed the classification head for this model): https://colab.research.google.com/drive/1EJGzibnljqxFyEeAc15sa5wAVJsVAyji?usp=sharing\n\nUsharaniPagadala assigned UsharaniPagadala and MorganR and unassigned UsharaniPagadala\n\nUsharaniPagadala added stat:awaiting tensorflower type:bug labels\n\nRishit-dagli commented\n\nAlso, until then I tried using a workaround, manually updating the signatures of the model which seemed to work at the moment; here\'s some minimal code of what I did:\n\n@tf.function() def my_predict(input): inputs = { ""my_serving_input"": input, } return { ""output"": tf.reshape( model.signatures[""serving_default""](input)[""output""], [-1, 1024] ) } my_signatures = my_predict.get_concrete_function( input=tf.TensorSpec([None, 384, 384, 3], dtype=tf.dtypes.float32, name=""input"") ) tf.saved_model.save(model, export_dir=saved_model_dir, signatures=my_signatures)\n\nI don\'t have bandwidth to look into this at the moment, so unassigning.\n\nMorganR removed their assignment\n\nUsharaniPagadala assigned akhorlin\n\nThe model in question doesn\'t seem to be hosted on tfhub.dev. The ideal solution as mentioned by MorganR is to update how the model was exported to fix the dimensions of the output tensor. If this is not possible, then calling tf.reshape() to fix up the dimensions makes sense. This what one would do when using TF directly (w/o Keras). There might be some nicer Keras specific ways of achieving this. One option is to ask a question on the TF/Keras\'s forums.\n\nakhorlin added stat:awaiting response and removed stat:awaiting tensorflower labels\n\nRishit-dagli commented\n\nGot it, thanks @akhorlin . I will continue using tf.reshape() thanks for confirming. I guess a nice Keras way to do the same would be to add a Reshape layer after the hub layer, but I will try exploring a bit more.\n\nRishit-dagli closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nstat:awaiting response type:bug\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-29T06:32:04', 'title': 'Convert dynamic shapes in hub.KerasLayer · Issue #808 · tensorflow/hub', 'url': 'https://github.com/tensorflow/hub/issues/808'})], [Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nNew to machine learning? Watch a video course to get practical working knowledge of ML using web technologiesView series\n\nImporting a Keras model into TensorFlow.js\n\nStay organized with collections Save and categorize content based on your preferences.\n\nKeras models (typically created via the Python API) may be saved in one of several formats. The ""whole model"" format can be converted to TensorFlow.js Layers format, which can be loaded directly into TensorFlow.js for inference or for further training.\n\nThe target TensorFlow.js Layers format is a directory containing a model.json file and a set of sharded weight files in binary format. The model.json file contains both the model topology (aka ""architecture"" or ""graph"": a description of the layers and how they are connected) and a manifest of the weight files.\n\nThe conversion procedure requires a Python environment; you may want to keep an isolated one using pipenv or virtualenv. To install the converter, use pip install tensorflowjs.\n\nImporting a Keras model into TensorFlow.js is a two-step process. First, convert an existing Keras model to TF.js Layers format, and then load it into TensorFlow.js.\n\nStep 1. Convert an existing Keras model to TF.js Layers format\n\nKeras models are usually saved via model.save(filepath), which produces a single HDF5 (.h5) file containing both the model topology and the weights. To convert such a file to TF.js Layers format, run the following command, where path/to/my_model.h5 is the source Keras .h5 file and path/to/tfjs_target_dir is the target output directory for the TF.js files:\n\n# bash tensorflowjs_converter --input_format keras \\ path/to/my_model.h5 \\ path/to/tfjs_target_dir\n\nAlternative: Use the Python API to export directly to TF.js Layers format\n\nIf you have a Keras model in Python, you can export it directly to the TensorFlow.js Layers format as follows:\n\n# Python import tensorflowjs as tfjs def train(...): model = keras.models.Sequential() # for example ... model.compile(...) model.fit(...) tfjs.converters.save_keras_model(model, tfjs_target_dir)\n\nStep 2: Load the model into TensorFlow.js\n\nUse a web server to serve the converted model files you generated in Step 1. Note that you may need to configure your server to allow Cross-Origin Resource Sharing (CORS), in order to allow fetching the files in JavaScript.\n\nThen load the model into TensorFlow.js by providing the URL to the model.json file:\n\n// JavaScript import * as tf from \'@tensorflow/tfjs\'; const model = await tf.loadLayersModel(\'https://foo.bar/tfjs_artifacts/model.json\');\n\nNow the model is ready for inference, evaluation, or re-training. For instance, the loaded model can be immediately used to make a prediction:\n\n// JavaScript const example = tf.fromPixels(webcamElement); // for example const prediction = model.predict(example);\n\nMany of the TensorFlow.js Examples take this approach, using pretrained models that have been converted and hosted on Google Cloud Storage.\n\nNote that you refer to the entire model using the model.json filename. loadModel(...) fetches model.json, and then makes additional HTTP(S) requests to obtain the sharded weight files referenced in the model.json weight manifest. This approach allows all of these files to be cached by the browser (and perhaps by additional caching servers on the internet), because the model.json and the weight shards are each smaller than the typical cache file size limit. Thus a model is likely to load more quickly on subsequent occasions.\n\nTensorFlow.js Layers currently only supports Keras models using standard Keras constructs. Models using unsupported ops or layers—e.g. custom layers, Lambda layers, custom losses, or custom metrics—cannot be automatically imported, because they depend on Python code that cannot be reliably translated into JavaScript.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2022-11-01 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_3', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nNew to machine learning? Watch a video course to get practical working knowledge of ML using web technologiesView series\n\nImporting a Keras model into TensorFlow.js\n\nStay organized with collections Save and categorize content based on your preferences.\n\nKeras models (typically created via the Python API) may be saved in one of several formats. The ""whole model"" format can be converted to TensorFlow.js Layers format, which can be loaded directly into TensorFlow.js for inference or for further training.\n\nThe target TensorFlow.js Layers format is a directory containing a model.json file and a set of sharded weight files in binary format. The model.json file contains both the model topology (aka ""architecture"" or ""graph"": a description of the layers and how they are connected) and a manifest of the weight files.\n\nThe conversion procedure requires a Python environment; you may want to keep an isolated one using pipenv or virtualenv. To install the converter, use pip install tensorflowjs.\n\nImporting a Keras model into TensorFlow.js is a two-step process. First, convert an existing Keras model to TF.js Layers format, and then load it into TensorFlow.js.\n\nStep 1. Convert an existing Keras model to TF.js Layers format\n\nKeras models are usually saved via model.save(filepath), which produces a single HDF5 (.h5) file containing both the model topology and the weights. To convert such a file to TF.js Layers format, run the following command, where path/to/my_model.h5 is the source Keras .h5 file and path/to/tfjs_target_dir is the target output directory for the TF.js files:\n\n# bash tensorflowjs_converter --input_format keras \\ path/to/my_model.h5 \\ path/to/tfjs_target_dir\n\nAlternative: Use the Python API to export directly to TF.js Layers format\n\nIf you have a Keras model in Python, you can export it directly to the TensorFlow.js Layers format as follows:\n\n# Python import tensorflowjs as tfjs def train(...): model = keras.models.Sequential() # for example ... model.compile(...) model.fit(...) tfjs.converters.save_keras_model(model, tfjs_target_dir)\n\nStep 2: Load the model into TensorFlow.js\n\nUse a web server to serve the converted model files you generated in Step 1. Note that you may need to configure your server to allow Cross-Origin Resource Sharing (CORS), in order to allow fetching the files in JavaScript.\n\nThen load the model into TensorFlow.js by providing the URL to the model.json file:\n\n// JavaScript import * as tf from \'@tensorflow/tfjs\'; const model = await tf.loadLayersModel(\'https://foo.bar/tfjs_artifacts/model.json\');\n\nNow the model is ready for inference, evaluation, or re-training. For instance, the loaded model can be immediately used to make a prediction:\n\n// JavaScript const example = tf.fromPixels(webcamElement); // for example const prediction = model.predict(example);\n\nMany of the TensorFlow.js Examples take this approach, using pretrained models that have been converted and hosted on Google Cloud Storage.\n\nNote that you refer to the entire model using the model.json filename. loadModel(...) fetches model.json, and then makes additional HTTP(S) requests to obtain the sharded weight files referenced in the model.json weight manifest. This approach allows all of these files to be cached by the browser (and perhaps by additional caching servers on the internet), because the model.json and the weight shards are each smaller than the typical cache file size limit. Thus a model is likely to load more quickly on subsequent occasions.\n\nTensorFlow.js Layers currently only supports Keras models using standard Keras constructs. Models using unsupported ops or layers—e.g. custom layers, Lambda layers, custom losses, or custom metrics—cannot be automatically imported, because they depend on Python code that cannot be reliably translated into JavaScript.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2022-11-01 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-07-04T14:48:23', 'title': 'Importing a Keras model into TensorFlow.js', 'url': 'https://www.tensorflow.org/js/tutorials/conversion/import_keras'}), Document(page_content='中文 – 简体 GitHub', metadata={'id': 'web-search_4', 'snippet': '中文 – 简体 GitHub', 'timestamp': '2024-07-06T09:44:54', 'title': 'tf.keras.Model | TensorFlow v2.16.1', 'url': 'https://www.tensorflow.org/api_docs/python/tf/keras/Model'}), Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nConvert TensorFlow models\n\nStay organized with collections Save and categorize content based on your preferences.\n\nThis page describes how to convert a TensorFlow model to a TensorFlow Lite model (an optimized FlatBuffer format identified by the .tflite file extension) using the TensorFlow Lite converter.\n\nNote: This guide assumes you\'ve both installed TensorFlow 2.x and trained models in TensorFlow 2.x. If your model is trained in TensorFlow 1.x, considering migrating to TensorFlow 2.x. To identify the installed TensorFlow version, run print(tf.__version__).\n\nThe diagram below illustrations the high-level workflow for converting your model:\n\nFigure 1. Converter workflow.\n\nYou can convert your model using one of the following options:\n\nPython API (recommended): This allows you to integrate the conversion into your development pipeline, apply optimizations, add metadata and many other tasks that simplify the conversion process.\n\nCommand line: This only supports basic model conversion.\n\nNote: In case you encounter any issues during model conversion, create a GitHub issue.\n\nHelper code: To learn more about the TensorFlow Lite converter API, run print(help(tf.lite.TFLiteConverter)).\n\nConvert a TensorFlow model using tf.lite.TFLiteConverter. A TensorFlow model is stored using the SavedModel format and is generated either using the high-level tf.keras.* APIs (a Keras model) or the low-level tf.* APIs (from which you generate concrete functions). As a result, you have the following three options (examples are in the next few sections):\n\ntf.lite.TFLiteConverter.from_saved_model() (recommended): Converts a SavedModel.\n\ntf.lite.TFLiteConverter.from_keras_model(): Converts a Keras model.\n\ntf.lite.TFLiteConverter.from_concrete_functions(): Converts concrete functions.\n\nConvert a SavedModel (recommended)\n\nThe following example shows how to convert a SavedModel into a TensorFlow Lite model.\n\nimport tensorflow as tf # Convert the model converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory tflite_model = converter.convert() # Save the model. with open(\'model.tflite\', \'wb\') as f: f.write(tflite_model)\n\nConvert a Keras model\n\nThe following example shows how to convert a Keras model into a TensorFlow Lite model.\n\nimport tensorflow as tf # Create a model using high-level tf.keras.* APIs model = tf.keras.models.Sequential([ tf.keras.layers.Dense(units=1, input_shape=[1]), tf.keras.layers.Dense(units=16, activation=\'relu\'), tf.keras.layers.Dense(units=1) ]) model.compile(optimizer=\'sgd\', loss=\'mean_squared_error\') # compile the model model.fit(x=[-1, 0, 1], y=[-3, -1, 1], epochs=5) # train the model # (to generate a SavedModel) tf.saved_model.save(model, ""saved_model_keras_dir"") # Convert the model. converter = tf.lite.TFLiteConverter.from_keras_model(model) tflite_model = converter.convert() # Save the model. with open(\'model.tflite\', \'wb\') as f: f.write(tflite_model)\n\nConvert concrete functions\n\nThe following example shows how to convert concrete functions into a TensorFlow Lite model.\n\nimport tensorflow as tf # Create a model using low-level tf.* APIs class Squared(tf.Module): @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)]) def __call__(self, x): return tf.square(x) model = Squared() # (ro run your model) result = Squared(5.0) # This prints ""25.0"" # (to generate a SavedModel) tf.saved_model.save(model, ""saved_model_tf_dir"") concrete_func = model.__call__.get_concrete_function() # Convert the model. converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func], model) tflite_model = converter.convert() # Save the model. with open(\'model.tflite\', \'wb\') as f: f.write(tflite_model)\n\nApply optimizations. A common optimization used is post training quantization, which can further reduce your model latency and size with minimal loss in accuracy.\n\nAdd metadata, which makes it easier to create platform specific wrapper code when deploying models on devices.\n\nThe following are common conversion errors and their solutions:\n\nError: Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: <a href=""https://www.tensorflow.org/lite/guide/ops_select"">https://www.tensorflow.org/lite/guide/ops_select</a> TF Select ops: ..., .., ...\n\nSolution: The error occurs as your model has TF ops that don\'t have a corresponding TFLite implementation. You can resolve this by using the TF op in the TFLite model (recommended). If you want to generate a model with TFLite ops only, you can either add a request for the missing TFLite op in Github issue #21526 (leave a comment if your request hasn’t already been mentioned) or create the TFLite op yourself.\n\nError: .. is neither a custom op nor a flex op\n\nSolution: If this TF op is:\n\nSupported in TF: The error occurs because the TF op is missing from the allowlist (an exhaustive list of TF ops supported by TFLite). You can resolve this as follows:\n\nAdd missing ops to the allowlist.\n\nConvert the TF model to a TFLite model and run inference.\n\nUnsupported in TF: The error occurs because TFLite is unaware of the custom TF operator defined by you. You can resolve this as follows:\n\nConvert the TF model to a TFLite model.\n\nCreate the TFLite op and run inference by linking it to the TFLite runtime.\n\nNote: It is highly recommended that you use the Python API listed above instead, if possible.\n\nIf you\'ve installed TensorFlow 2.x from pip, use the tflite_convert command. To view all the available flags, use the following command:\n\n$ tflite_convert --help `--output_file`. Type: string. Full path of the output file. `--saved_model_dir`. Type: string. Full path to the SavedModel directory. `--keras_model_file`. Type: string. Full path to the Keras H5 model file. `--enable_v1_converter`. Type: bool. (default False) Enables the converter and flags used in TF 1.x instead of TF 2.x. You are required to provide the `--output_file` flag and either the `--saved_model_dir` or `--keras_model_file` flag.\n\nIf you have the TensorFlow 2.x source donwloaded and want to run the converter from that source without building and installing the package, you can replace \'tflite_convert\' with \'bazel run tensorflow/lite/python:tflite_convert --\' in the command.\n\nConverting a SavedModel\n\ntflite_convert \\ --saved_model_dir=/tmp/mobilenet_saved_model \\ --output_file=/tmp/mobilenet.tflite\n\nConverting a Keras H5 model\n\ntflite_convert \\ --keras_model_file=/tmp/mobilenet_keras_model.h5 \\ --output_file=/tmp/mobilenet.tflite\n\nUse the TensorFlow Lite interpreter to run inference on a client device (e.g. mobile, embedded).\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2022-06-11 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_5', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nConvert TensorFlow models\n\nStay organized with collections Save and categorize content based on your preferences.\n\nThis page describes how to convert a TensorFlow model to a TensorFlow Lite model (an optimized FlatBuffer format identified by the .tflite file extension) using the TensorFlow Lite converter.\n\nNote: This guide assumes you\'ve both installed TensorFlow 2.x and trained models in TensorFlow 2.x. If your model is trained in TensorFlow 1.x, considering migrating to TensorFlow 2.x. To identify the installed TensorFlow version, run print(tf.__version__).\n\nThe diagram below illustrations the high-level workflow for converting your model:\n\nFigure 1. Converter workflow.\n\nYou can convert your model using one of the following options:\n\nPython API (recommended): This allows you to integrate the conversion into your development pipeline, apply optimizations, add metadata and many other tasks that simplify the conversion process.\n\nCommand line: This only supports basic model conversion.\n\nNote: In case you encounter any issues during model conversion, create a GitHub issue.\n\nHelper code: To learn more about the TensorFlow Lite converter API, run print(help(tf.lite.TFLiteConverter)).\n\nConvert a TensorFlow model using tf.lite.TFLiteConverter. A TensorFlow model is stored using the SavedModel format and is generated either using the high-level tf.keras.* APIs (a Keras model) or the low-level tf.* APIs (from which you generate concrete functions). As a result, you have the following three options (examples are in the next few sections):\n\ntf.lite.TFLiteConverter.from_saved_model() (recommended): Converts a SavedModel.\n\ntf.lite.TFLiteConverter.from_keras_model(): Converts a Keras model.\n\ntf.lite.TFLiteConverter.from_concrete_functions(): Converts concrete functions.\n\nConvert a SavedModel (recommended)\n\nThe following example shows how to convert a SavedModel into a TensorFlow Lite model.\n\nimport tensorflow as tf # Convert the model converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory tflite_model = converter.convert() # Save the model. with open(\'model.tflite\', \'wb\') as f: f.write(tflite_model)\n\nConvert a Keras model\n\nThe following example shows how to convert a Keras model into a TensorFlow Lite model.\n\nimport tensorflow as tf # Create a model using high-level tf.keras.* APIs model = tf.keras.models.Sequential([ tf.keras.layers.Dense(units=1, input_shape=[1]), tf.keras.layers.Dense(units=16, activation=\'relu\'), tf.keras.layers.Dense(units=1) ]) model.compile(optimizer=\'sgd\', loss=\'mean_squared_error\') # compile the model model.fit(x=[-1, 0, 1], y=[-3, -1, 1], epochs=5) # train the model # (to generate a SavedModel) tf.saved_model.save(model, ""saved_model_keras_dir"") # Convert the model. converter = tf.lite.TFLiteConverter.from_keras_model(model) tflite_model = converter.convert() # Save the model. with open(\'model.tflite\', \'wb\') as f: f.write(tflite_model)\n\nConvert concrete functions\n\nThe following example shows how to convert concrete functions into a TensorFlow Lite model.\n\nimport tensorflow as tf # Create a model using low-level tf.* APIs class Squared(tf.Module): @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)]) def __call__(self, x): return tf.square(x) model = Squared() # (ro run your model) result = Squared(5.0) # This prints ""25.0"" # (to generate a SavedModel) tf.saved_model.save(model, ""saved_model_tf_dir"") concrete_func = model.__call__.get_concrete_function() # Convert the model. converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func], model) tflite_model = converter.convert() # Save the model. with open(\'model.tflite\', \'wb\') as f: f.write(tflite_model)\n\nApply optimizations. A common optimization used is post training quantization, which can further reduce your model latency and size with minimal loss in accuracy.\n\nAdd metadata, which makes it easier to create platform specific wrapper code when deploying models on devices.\n\nThe following are common conversion errors and their solutions:\n\nError: Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: <a href=""https://www.tensorflow.org/lite/guide/ops_select"">https://www.tensorflow.org/lite/guide/ops_select</a> TF Select ops: ..., .., ...\n\nSolution: The error occurs as your model has TF ops that don\'t have a corresponding TFLite implementation. You can resolve this by using the TF op in the TFLite model (recommended). If you want to generate a model with TFLite ops only, you can either add a request for the missing TFLite op in Github issue #21526 (leave a comment if your request hasn’t already been mentioned) or create the TFLite op yourself.\n\nError: .. is neither a custom op nor a flex op\n\nSolution: If this TF op is:\n\nSupported in TF: The error occurs because the TF op is missing from the allowlist (an exhaustive list of TF ops supported by TFLite). You can resolve this as follows:\n\nAdd missing ops to the allowlist.\n\nConvert the TF model to a TFLite model and run inference.\n\nUnsupported in TF: The error occurs because TFLite is unaware of the custom TF operator defined by you. You can resolve this as follows:\n\nConvert the TF model to a TFLite model.\n\nCreate the TFLite op and run inference by linking it to the TFLite runtime.\n\nNote: It is highly recommended that you use the Python API listed above instead, if possible.\n\nIf you\'ve installed TensorFlow 2.x from pip, use the tflite_convert command. To view all the available flags, use the following command:\n\n$ tflite_convert --help `--output_file`. Type: string. Full path of the output file. `--saved_model_dir`. Type: string. Full path to the SavedModel directory. `--keras_model_file`. Type: string. Full path to the Keras H5 model file. `--enable_v1_converter`. Type: bool. (default False) Enables the converter and flags used in TF 1.x instead of TF 2.x. You are required to provide the `--output_file` flag and either the `--saved_model_dir` or `--keras_model_file` flag.\n\nIf you have the TensorFlow 2.x source donwloaded and want to run the converter from that source without building and installing the package, you can replace \'tflite_convert\' with \'bazel run tensorflow/lite/python:tflite_convert --\' in the command.\n\nConverting a SavedModel\n\ntflite_convert \\ --saved_model_dir=/tmp/mobilenet_saved_model \\ --output_file=/tmp/mobilenet.tflite\n\nConverting a Keras H5 model\n\ntflite_convert \\ --keras_model_file=/tmp/mobilenet_keras_model.h5 \\ --output_file=/tmp/mobilenet.tflite\n\nUse the TensorFlow Lite interpreter to run inference on a client device (e.g. mobile, embedded).\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2022-06-11 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-04-19T08:57:52', 'title': 'Convert TensorFlow models | TensorFlow Lite', 'url': 'https://www.tensorflow.org/lite/models/convert/convert_models'})]]??"
76324368,tf.keras.layers.Dense,"{'https://www.edx.org/learn/tensorflow', 'https://www.udemy.com/course/deep-learning-with-python-and-keras/', 'https://www.udemy.com/course/deep-learning-neural-networks-python-keras-for-dummies/', 'https://www.udemy.com/course/building-a-deep-learning-model-and-neural-network-with-keras/', 'https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/', 'https://www.coursera.org/learn/introduction-to-deep-learning-with-keras', 'https://www.udemy.com/course/deep-learning-computer-vision-using-keras-dummies-guide/', 'https://www.udemy.com/course/deep-learning-with-keras-and-tensorflow-in-python-and-r/', 'https://www.edx.org/learn/keras', 'https://www.coursera.org/learn/custom-models-layers-loss-functions-with-tensorflow'}","{'https://www.youtube.com/watch?v=lor2LnEVn8M', 'https://www.youtube.com/watch?v=49IOTCzoWQg'}","{'https://stackoverflow.com/questions/43755293/what-does-dense-do', 'https://stackoverflow.com/questions/62703012/question-on-tensorflow-dense-layer-implementation', 'https://stackoverflow.com/questions/40866124/difference-between-dense-and-activation-layer-in-keras', 'https://stackoverflow.com/questions/66626700/difference-between-tensorflows-tf-keras-layers-dense-and-pytorchs-torch-nn-lin', 'https://stackoverflow.com/questions/76324368/understanding-tf-keras-layers-dense'}","??[[Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nUnderstanding tf.keras.layers.Dense()\n\nAsked 1 year, 1 month ago\n\nModified 6 months ago\n\nI am trying to understand why there is a difference between calculating a dense layer operation directly and using the keras implementation.\n\nFollowing the documentation (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) tf.keras.layers.Dense() should implement the operation output = activation(dot(input, kernel) + bias) but result and result1 below are not the same.\n\ntf.random.set_seed(1) bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32) kernel = tf.Variable(tf.random.uniform(shape=(5,10)), dtype=tf.float32) x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32)) result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x) + bias) tf.print(result) test = tf.keras.layers.Dense(units = 5, activation = \'relu\', use_bias = True, kernel_initializer = tf.keras.initializers.Constant(value=kernel), bias_initializer = tf.keras.initializers.Constant(value=bias), dtype=tf.float32) result1 = test(tf.transpose(x)) print() tf.print(result1)\n\n[[2.87080455] [3.25458574] [3.28776264] [3.14319134] [2.04760242]] [[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]\n\nUsing test.get_weights() I can see that the kernel and bias (b) are getting set to the correct values. I am using TF version 2.12.0.\n\nImprove this question\n\nedited Jan 7 at 7:48\n\n12.5k77 gold badges7676 silver badges112112 bronze badges\n\nasked May 24, 2023 at 14:00\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nAfter some experimentation I realized that the kernel for the dense layer needs to be of shape=(10,5) as apposed to (5,10) as in the code from the original question above. This is implicit because units=5 so a vector of size 10 needs to be passed (hence why input_shape=(10,) is commented out as a reminder). Below is the corrected code:\n\ntf.random.set_seed(1) bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32) kernel = tf.Variable(tf.random.uniform(shape=(10,5)), dtype=tf.float32) x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32)) result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x, transpose_a=True) + bias) tf.print(result) test = tf.keras.layers.Dense(units = 5, # input_shape=(10,), activation = \'relu\', use_bias = True, kernel_initializer = tf.keras.initializers.Constant(value=kernel), bias_initializer = tf.keras.initializers.Constant(value=bias), dtype=tf.float32) result1 = test(tf.transpose(x)) print() tf.print(result1)\n\n[[2.38769] [3.63470697] [2.62423944] [3.31286287] [2.91121125]] [[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]\n\nUltimately, I am not entirely sure what was happening under the hood and why keras did not raise an error. I will check with the tf.keras.layers.Dense() implementation but any thoughts or suggestions by someone who knows the code already are highly appreciated!\n\nedited Jan 7 at 7:47\n\n12.5k77 gold badges7676 silver badges112112 bronze badges\n\nanswered May 25, 2023 at 12:15\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras-layer or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n0 Why I cannot run a tensor and got error ""the variable was uninitialized""\n\n1 what exactly is Dense in LSTM model description?\n\n9 How does tf.layers.dense() interact with inputs of higher dim?\n\n0 TFE_Py_RecordGradient error using Keras with Tensorflow back end\n\n411 Understanding Keras LSTMs\n\nHot Network Questions\n\nHow to choose between 3/4 and 6/8 time?\n\nWhy does King Aegon speak to his dragon in the Common Tongue (English)?\n\nWhy do jet aircraft need chocks when they have parking brakes?\n\nAn adjective for something peaceful but sad?\n\nIdentify the story about an author whose work-in-progress is completed by a computer\n\nIntelligence vs Wisdom in D&D\n\nLooking for two tables to start at the same height\n\nFind unknown primes from two RSA modulus\n\nIf someone clearly believes that he has witnessed something extraordinary very clearly, why is it more reasonable to believe that they hallucinated?\n\nHelp understanding the implications of Security Breach section of an NDA\n\nHow can I sort all levels except the innermost level?\n\nWhy is this transformer placed on rails?\n\nWhy are metal ores dredged from coastal lagoons rather than being extracted directly from the mother lode?\n\nBreaking down a command to install Ros2\n\nCoping with consequences of a dog bite before buying a puppy\n\nHow can I search File Explorer for files only (i.e. exclude folders) in Windows 10?\n\nAre there other proposed translations of ""aelfheres"" in Beowulf than a name?\n\nFilled in \\diamond without XeLaTeX\n\nSearch and replace multiple characters simultaneously\n\nCombinatoric Problem in Stardew Valley about Keg Layout\n\nCan non-admins create new domain on local DNS from a client computer?\n\nIs prescreening not detrimental for paid surveys?\n\nIs this a Hadamard matrix? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_0', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nUnderstanding tf.keras.layers.Dense()\n\nAsked 1 year, 1 month ago\n\nModified 6 months ago\n\nI am trying to understand why there is a difference between calculating a dense layer operation directly and using the keras implementation.\n\nFollowing the documentation (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) tf.keras.layers.Dense() should implement the operation output = activation(dot(input, kernel) + bias) but result and result1 below are not the same.\n\ntf.random.set_seed(1) bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32) kernel = tf.Variable(tf.random.uniform(shape=(5,10)), dtype=tf.float32) x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32)) result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x) + bias) tf.print(result) test = tf.keras.layers.Dense(units = 5, activation = \'relu\', use_bias = True, kernel_initializer = tf.keras.initializers.Constant(value=kernel), bias_initializer = tf.keras.initializers.Constant(value=bias), dtype=tf.float32) result1 = test(tf.transpose(x)) print() tf.print(result1)\n\n[[2.87080455] [3.25458574] [3.28776264] [3.14319134] [2.04760242]] [[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]\n\nUsing test.get_weights() I can see that the kernel and bias (b) are getting set to the correct values. I am using TF version 2.12.0.\n\nImprove this question\n\nedited Jan 7 at 7:48\n\n12.5k77 gold badges7676 silver badges112112 bronze badges\n\nasked May 24, 2023 at 14:00\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nAfter some experimentation I realized that the kernel for the dense layer needs to be of shape=(10,5) as apposed to (5,10) as in the code from the original question above. This is implicit because units=5 so a vector of size 10 needs to be passed (hence why input_shape=(10,) is commented out as a reminder). Below is the corrected code:\n\ntf.random.set_seed(1) bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32) kernel = tf.Variable(tf.random.uniform(shape=(10,5)), dtype=tf.float32) x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32)) result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x, transpose_a=True) + bias) tf.print(result) test = tf.keras.layers.Dense(units = 5, # input_shape=(10,), activation = \'relu\', use_bias = True, kernel_initializer = tf.keras.initializers.Constant(value=kernel), bias_initializer = tf.keras.initializers.Constant(value=bias), dtype=tf.float32) result1 = test(tf.transpose(x)) print() tf.print(result1)\n\n[[2.38769] [3.63470697] [2.62423944] [3.31286287] [2.91121125]] [[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]\n\nUltimately, I am not entirely sure what was happening under the hood and why keras did not raise an error. I will check with the tf.keras.layers.Dense() implementation but any thoughts or suggestions by someone who knows the code already are highly appreciated!\n\nedited Jan 7 at 7:47\n\n12.5k77 gold badges7676 silver badges112112 bronze badges\n\nanswered May 25, 2023 at 12:15\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras-layer or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n0 Why I cannot run a tensor and got error ""the variable was uninitialized""\n\n1 what exactly is Dense in LSTM model description?\n\n9 How does tf.layers.dense() interact with inputs of higher dim?\n\n0 TFE_Py_RecordGradient error using Keras with Tensorflow back end\n\n411 Understanding Keras LSTMs\n\nHot Network Questions\n\nHow to choose between 3/4 and 6/8 time?\n\nWhy does King Aegon speak to his dragon in the Common Tongue (English)?\n\nWhy do jet aircraft need chocks when they have parking brakes?\n\nAn adjective for something peaceful but sad?\n\nIdentify the story about an author whose work-in-progress is completed by a computer\n\nIntelligence vs Wisdom in D&D\n\nLooking for two tables to start at the same height\n\nFind unknown primes from two RSA modulus\n\nIf someone clearly believes that he has witnessed something extraordinary very clearly, why is it more reasonable to believe that they hallucinated?\n\nHelp understanding the implications of Security Breach section of an NDA\n\nHow can I sort all levels except the innermost level?\n\nWhy is this transformer placed on rails?\n\nWhy are metal ores dredged from coastal lagoons rather than being extracted directly from the mother lode?\n\nBreaking down a command to install Ros2\n\nCoping with consequences of a dog bite before buying a puppy\n\nHow can I search File Explorer for files only (i.e. exclude folders) in Windows 10?\n\nAre there other proposed translations of ""aelfheres"" in Beowulf than a name?\n\nFilled in \\diamond without XeLaTeX\n\nSearch and replace multiple characters simultaneously\n\nCombinatoric Problem in Stardew Valley about Keg Layout\n\nCan non-admins create new domain on local DNS from a client computer?\n\nIs prescreening not detrimental for paid surveys?\n\nIs this a Hadamard matrix? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-10T10:18:02', 'title': 'python - Understanding tf.keras.layers.Dense() - Stack Overflow', 'url': 'https://stackoverflow.com/questions/76324368/understanding-tf-keras-layers-dense'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\n2024 Developer survey is here and we would like to hear from you! Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nAsked 7 years, 1 month ago\n\nModified 3 years, 3 months ago\n\nWhat is the meaning of the two Dense in this code?\n\nself.model.add(Flatten()) self.model.add(Dense(512)) self.model.add(Activation(\'relu\')) self.model.add(Dropout(0.5)) self.model.add(Dense(10)) self.model.add(Activation(\'softmax\')) self.model.summary()\n\nImprove this question\n\nedited Aug 23, 2020 at 14:54\n\n8,6972323 gold badges7777 silver badges100100 bronze badges\n\nasked May 3, 2017 at 8:47\n\n7111 gold badge11 silver badge33 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nDense is the only actual network layer in that model.\n\nA Dense layer feeds all outputs from the previous layer to all its neurons, each neuron providing one output to the next layer.\n\nIt\'s the most basic layer in neural networks.\n\nA Dense(10) has ten neurons. A Dense(512) has 512 neurons.\n\nanswered May 3, 2017 at 12:42\n\nDaniel MöllerDaniel Möller\n\n86.1k2222 gold badges199199 silver badges219219 bronze badges\n\nFurthermore, a dense layers applies the a non-linear transform:\n\nAs to the effect, well in the case that W and X are a 2D tensor W.X + b is a vector and f is a element wise non-linearity like tanh, so the result is just a vector of size in the numbers of neurons\n\nFrom the keras docs:\n\nDense implements the operation: output = activation(dot(input, kernel)\n\nbias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n\nedited Mar 12, 2021 at 15:27\n\nanswered May 3, 2017 at 12:57\n\n8,06833 gold badges3030 silver badges3232 bronze badges 1\n\nIn this context what is tf.keras.layers.DenseFeatures. Does DenseFeatures too apply non linearity?\n\n– sakeesh Commented Dec 9, 2021 at 1:54\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras-layer or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n3 keras dense input layer\n\n9 Python keras how to transform a dense layer into a convolutional layer\n\n32 Difference between Dense and Activation layer in Keras\n\n0 Where is the code for the ""Dense"" function in keras?\n\n1 what does dense_[number] mean in model.summary in keras\n\n1 what exactly is Dense in LSTM model description?\n\n4 Understanding output of Dense layer for higher dimension\n\n7 What exactly does tf.keras.layers.Dense do?\n\n2 usage of tf.keras.layers.DenseFeatures\n\n1 Result of Dense layer in keras\n\nHot Network Questions\n\nDo AOSP users have ""usernames""?\n\nIs there any piece that can take multiple pieces at once?\n\nCan I enter France on an expired EU passport?\n\nLimitations on pain and pleasure, and the ratio of extremes assuming perfect technology\n\nWill it break Counterspell if the trigger becomes ""perceive"" instead of ""see""?\n\n3 doors guarded by 1 knight and 2 knaves\n\nGeographic distribution of Bitcoin users\n\nCloning with stolen DNA\n\nHow to draw this abstract curve design\n\nWhy are amber bottles used to store Water for HPLC?\n\nHow do regression loss functions like MAE and MSE work although they remove the plus/minus sign?\n\nIs intrinsic spin a quantum or/and a relativistic phenomenon?\n\nCan sacrificing a Queen be considered a brilliant move?\n\nWhat is the difference between ""боля"" and ""болея""?\n\nHow to handle arguments in an efficient and flexible way?\n\nWhat would happen to \'politicians\' in this world?\n\nProbably a nit: ""openssl x509"" displays the serial number sometimes as octet string, sometimes as integer\n\nTruncating seconds on label\n\nReplacing grass against side of house\n\nIs there any reason to keep old checks?\n\nAre many figures in a paper considered bad practice?\n\nWhat is the meaning of "" I object to rows""?\n\nTo whom the neogrammarians reacted?\n\nBest practices for relicensing what was once a derivative work more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_2', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\n2024 Developer survey is here and we would like to hear from you! Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nAsked 7 years, 1 month ago\n\nModified 3 years, 3 months ago\n\nWhat is the meaning of the two Dense in this code?\n\nself.model.add(Flatten()) self.model.add(Dense(512)) self.model.add(Activation(\'relu\')) self.model.add(Dropout(0.5)) self.model.add(Dense(10)) self.model.add(Activation(\'softmax\')) self.model.summary()\n\nImprove this question\n\nedited Aug 23, 2020 at 14:54\n\n8,6972323 gold badges7777 silver badges100100 bronze badges\n\nasked May 3, 2017 at 8:47\n\n7111 gold badge11 silver badge33 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nDense is the only actual network layer in that model.\n\nA Dense layer feeds all outputs from the previous layer to all its neurons, each neuron providing one output to the next layer.\n\nIt\'s the most basic layer in neural networks.\n\nA Dense(10) has ten neurons. A Dense(512) has 512 neurons.\n\nanswered May 3, 2017 at 12:42\n\nDaniel MöllerDaniel Möller\n\n86.1k2222 gold badges199199 silver badges219219 bronze badges\n\nFurthermore, a dense layers applies the a non-linear transform:\n\nAs to the effect, well in the case that W and X are a 2D tensor W.X + b is a vector and f is a element wise non-linearity like tanh, so the result is just a vector of size in the numbers of neurons\n\nFrom the keras docs:\n\nDense implements the operation: output = activation(dot(input, kernel)\n\nbias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n\nedited Mar 12, 2021 at 15:27\n\nanswered May 3, 2017 at 12:57\n\n8,06833 gold badges3030 silver badges3232 bronze badges 1\n\nIn this context what is tf.keras.layers.DenseFeatures. Does DenseFeatures too apply non linearity?\n\n– sakeesh Commented Dec 9, 2021 at 1:54\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras-layer or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n3 keras dense input layer\n\n9 Python keras how to transform a dense layer into a convolutional layer\n\n32 Difference between Dense and Activation layer in Keras\n\n0 Where is the code for the ""Dense"" function in keras?\n\n1 what does dense_[number] mean in model.summary in keras\n\n1 what exactly is Dense in LSTM model description?\n\n4 Understanding output of Dense layer for higher dimension\n\n7 What exactly does tf.keras.layers.Dense do?\n\n2 usage of tf.keras.layers.DenseFeatures\n\n1 Result of Dense layer in keras\n\nHot Network Questions\n\nDo AOSP users have ""usernames""?\n\nIs there any piece that can take multiple pieces at once?\n\nCan I enter France on an expired EU passport?\n\nLimitations on pain and pleasure, and the ratio of extremes assuming perfect technology\n\nWill it break Counterspell if the trigger becomes ""perceive"" instead of ""see""?\n\n3 doors guarded by 1 knight and 2 knaves\n\nGeographic distribution of Bitcoin users\n\nCloning with stolen DNA\n\nHow to draw this abstract curve design\n\nWhy are amber bottles used to store Water for HPLC?\n\nHow do regression loss functions like MAE and MSE work although they remove the plus/minus sign?\n\nIs intrinsic spin a quantum or/and a relativistic phenomenon?\n\nCan sacrificing a Queen be considered a brilliant move?\n\nWhat is the difference between ""боля"" and ""болея""?\n\nHow to handle arguments in an efficient and flexible way?\n\nWhat would happen to \'politicians\' in this world?\n\nProbably a nit: ""openssl x509"" displays the serial number sometimes as octet string, sometimes as integer\n\nTruncating seconds on label\n\nReplacing grass against side of house\n\nIs there any reason to keep old checks?\n\nAre many figures in a paper considered bad practice?\n\nWhat is the meaning of "" I object to rows""?\n\nTo whom the neogrammarians reacted?\n\nBest practices for relicensing what was once a derivative work more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-06-18T19:22:53', 'title': 'keras - What does Dense do? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/43755293/what-does-dense-do'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nDifference between Dense and Activation layer in Keras\n\nAsked 7 years, 6 months ago\n\nModified 3 years, 7 months ago\n\nI was wondering what was the difference between Activation Layer and Dense layer in Keras.\n\nSince Activation Layer seems to be a fully connected layer, and Dense have a parameter to pass an activation function, what is the best practice ?\n\nLet\'s imagine a fictionnal network like this : Input -> Dense -> Dropout -> Final Layer Final Layer should be : Dense(activation=softmax) or Activation(softmax) ? What is the cleanest and why ?\n\nImprove this question\n\nedited Jul 4, 2017 at 10:38\n\n40.1k1010 gold badges111111 silver badges121121 bronze badges\n\nasked Nov 29, 2016 at 12:37\n\nPusheen_the_devPusheen_the_dev\n\n2,14744 gold badges1818 silver badges3333 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nUsing Dense(activation=softmax) is computationally equivalent to first add Dense and then add Activation(softmax). However there is one advantage of the second approach - you could retrieve the outputs of the last layer (before activation) out of such defined model. In the first approach - it\'s impossible.\n\nedited Dec 4, 2017 at 17:58\n\n27.8k4444 gold badges124124 silver badges181181 bronze badges\n\nanswered Nov 29, 2016 at 15:52\n\nMarcin MożejkoMarcin Możejko\n\n40.1k1010 gold badges111111 silver badges121121 bronze badges 4\n\ncan you spot any advantages of using the first one?\n\n– lejlot Nov 29, 2016 at 20:37\n\nModel definition is shorter and more compact.\n\n– Marcin Możejko Nov 29, 2016 at 21:49\n\nSo in Keras if I have to fine tune an InceptionV3 network on 20 classes, do I take the base_model.output where the base_model is InceptionV3(), and then add a Dense(1024, activation+""relu"")(base_model.output followed by Dense(20, activation=\'softmax\')? Is this the right way to do it in Keras?\n\n– London guy Dec 4, 2017 at 18:00\n\nyes from the Keras documentation (keras.io/activations), it is equivalent\n\n– ihebiheb Jun 10, 2018 at 11:08\n\nAs @MarcinMożejko said, it is equivalent. I just want to explain why. If you look at the Dense Keras documentation page, you\'ll see that the default activation function is None.\n\nA dense layer mathematically is:\n\nwhere g an activation function. When using Dense(units=k, activation=softmax), it is computing all the quantities in one shot. When doing Dense(units=k) and then Activation(\'softmax), it first calculates the quantity, W.T*a_prev+b (because the default activation function is None) and then applying the activation function specified as input to the Activation layer to the calculated quantity.\n\nanswered Oct 3, 2020 at 17:03\n\n8,9901616 gold badges8484 silver badges130130 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nWould you board a plane safety-tested by GenAI?\n\nAn open-source development paradigm\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe 2024 Developer Survey Is Live\n\n6 Difference between local and dense layers in CNNs\n\n0 Difference Between keras.layer.Dense(32) and keras.layer.SimpleRNN(32)?\n\n7 What does Dense do?\n\n21 Difference between Dense(2) and Dense(1) as the final layer of a binary classification CNN?\n\n2 What is the difference between keras.activations.softmax and keras.layers.Softmax?\n\n13 What is the difference between a layer with a linear activation and a layer without activation?\n\n8 what is the difference between using softmax as a sequential layer in tf.keras and softmax as an activation function for a dense layer?\n\n2 Is there a difference between Keras Dense layer and Pytorch\'s nn.linear layer?\n\n0 What is the difference between Activation layer and activation keyword argument\n\n1 In neural networks, activation is applied by a function or layer?\n\nHot Network Questions\n\nCan a crater form inside another crater\n\nStop electric motor when toy elevator reaches end of travel\n\nEnter Malaysia on dual citizenship (NOT including Malaysian)\n\nKeys and Locks Puzzle\n\nParade of planets - planetary alignment of 6 planets on 03.06.2024 - observation possibility from Prague\n\nAre horseback vigilantes, mud wrestling with dump teens, & using blue makeup to draw logos on faces in Unbeatable Squirrel Girl references to a work?\n\nWhich signals (Wi-Fi, mobile phone, and GPS) can reliably be blocked by aluminum foil?\n\nAm I a football enthusiast if I don\'t play football? Also, is there any field/subject/activity that is weird to use the word ""enthusiast"" with?\n\nWhat\'s wrong with my new environment?\n\nLTspice third party model gives problem with transient analsys\n\nLooking for a short story about a chemistry box for making humans\n\nNeed to create a math symbol\n\nWhy do I get binary solution even when I solve a LP problem with continuous variables?\n\nHow can I learn the intuition behind the proofs of theorems in Graph Theory? They all seem like random algorithms that just happen to work\n\nAre penalty functions still ""necessary""?\n\nHow to change the axis orientation of the cylinder in `ContourPlot3D`?\n\nA word for something that is, ""Bizarre"" but, ""Beautiful""\n\nMy Use Case Diagram is a mess, what can i do?\n\nWhat was the exposure duration for the Euclid images released on May 23, 2024?\n\nWhy is ""dict[int, int]"" incompatible with ""dict[int, int | str]""?\n\nWhat flag had a black cross blue field 1589?\n\nWhat is the cause of the re-emergence of premillennialism?\n\nCan you use a coin to restore a health segment after you\'ve reached 0?\n\nSitecore 10.3 and 10.4 Solr 9.x compatibility more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_5', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nDifference between Dense and Activation layer in Keras\n\nAsked 7 years, 6 months ago\n\nModified 3 years, 7 months ago\n\nI was wondering what was the difference between Activation Layer and Dense layer in Keras.\n\nSince Activation Layer seems to be a fully connected layer, and Dense have a parameter to pass an activation function, what is the best practice ?\n\nLet\'s imagine a fictionnal network like this : Input -> Dense -> Dropout -> Final Layer Final Layer should be : Dense(activation=softmax) or Activation(softmax) ? What is the cleanest and why ?\n\nImprove this question\n\nedited Jul 4, 2017 at 10:38\n\n40.1k1010 gold badges111111 silver badges121121 bronze badges\n\nasked Nov 29, 2016 at 12:37\n\nPusheen_the_devPusheen_the_dev\n\n2,14744 gold badges1818 silver badges3333 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nUsing Dense(activation=softmax) is computationally equivalent to first add Dense and then add Activation(softmax). However there is one advantage of the second approach - you could retrieve the outputs of the last layer (before activation) out of such defined model. In the first approach - it\'s impossible.\n\nedited Dec 4, 2017 at 17:58\n\n27.8k4444 gold badges124124 silver badges181181 bronze badges\n\nanswered Nov 29, 2016 at 15:52\n\nMarcin MożejkoMarcin Możejko\n\n40.1k1010 gold badges111111 silver badges121121 bronze badges 4\n\ncan you spot any advantages of using the first one?\n\n– lejlot Nov 29, 2016 at 20:37\n\nModel definition is shorter and more compact.\n\n– Marcin Możejko Nov 29, 2016 at 21:49\n\nSo in Keras if I have to fine tune an InceptionV3 network on 20 classes, do I take the base_model.output where the base_model is InceptionV3(), and then add a Dense(1024, activation+""relu"")(base_model.output followed by Dense(20, activation=\'softmax\')? Is this the right way to do it in Keras?\n\n– London guy Dec 4, 2017 at 18:00\n\nyes from the Keras documentation (keras.io/activations), it is equivalent\n\n– ihebiheb Jun 10, 2018 at 11:08\n\nAs @MarcinMożejko said, it is equivalent. I just want to explain why. If you look at the Dense Keras documentation page, you\'ll see that the default activation function is None.\n\nA dense layer mathematically is:\n\nwhere g an activation function. When using Dense(units=k, activation=softmax), it is computing all the quantities in one shot. When doing Dense(units=k) and then Activation(\'softmax), it first calculates the quantity, W.T*a_prev+b (because the default activation function is None) and then applying the activation function specified as input to the Activation layer to the calculated quantity.\n\nanswered Oct 3, 2020 at 17:03\n\n8,9901616 gold badges8484 silver badges130130 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nWould you board a plane safety-tested by GenAI?\n\nAn open-source development paradigm\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe 2024 Developer Survey Is Live\n\n6 Difference between local and dense layers in CNNs\n\n0 Difference Between keras.layer.Dense(32) and keras.layer.SimpleRNN(32)?\n\n7 What does Dense do?\n\n21 Difference between Dense(2) and Dense(1) as the final layer of a binary classification CNN?\n\n2 What is the difference between keras.activations.softmax and keras.layers.Softmax?\n\n13 What is the difference between a layer with a linear activation and a layer without activation?\n\n8 what is the difference between using softmax as a sequential layer in tf.keras and softmax as an activation function for a dense layer?\n\n2 Is there a difference between Keras Dense layer and Pytorch\'s nn.linear layer?\n\n0 What is the difference between Activation layer and activation keyword argument\n\n1 In neural networks, activation is applied by a function or layer?\n\nHot Network Questions\n\nCan a crater form inside another crater\n\nStop electric motor when toy elevator reaches end of travel\n\nEnter Malaysia on dual citizenship (NOT including Malaysian)\n\nKeys and Locks Puzzle\n\nParade of planets - planetary alignment of 6 planets on 03.06.2024 - observation possibility from Prague\n\nAre horseback vigilantes, mud wrestling with dump teens, & using blue makeup to draw logos on faces in Unbeatable Squirrel Girl references to a work?\n\nWhich signals (Wi-Fi, mobile phone, and GPS) can reliably be blocked by aluminum foil?\n\nAm I a football enthusiast if I don\'t play football? Also, is there any field/subject/activity that is weird to use the word ""enthusiast"" with?\n\nWhat\'s wrong with my new environment?\n\nLTspice third party model gives problem with transient analsys\n\nLooking for a short story about a chemistry box for making humans\n\nNeed to create a math symbol\n\nWhy do I get binary solution even when I solve a LP problem with continuous variables?\n\nHow can I learn the intuition behind the proofs of theorems in Graph Theory? They all seem like random algorithms that just happen to work\n\nAre penalty functions still ""necessary""?\n\nHow to change the axis orientation of the cylinder in `ContourPlot3D`?\n\nA word for something that is, ""Bizarre"" but, ""Beautiful""\n\nMy Use Case Diagram is a mess, what can i do?\n\nWhat was the exposure duration for the Euclid images released on May 23, 2024?\n\nWhy is ""dict[int, int]"" incompatible with ""dict[int, int | str]""?\n\nWhat flag had a black cross blue field 1589?\n\nWhat is the cause of the re-emergence of premillennialism?\n\nCan you use a coin to restore a health segment after you\'ve reached 0?\n\nSitecore 10.3 and 10.4 Solr 9.x compatibility more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-05-28T09:28:48', 'title': 'python - Difference between Dense and Activation layer in Keras - Stack Overflow', 'url': 'https://stackoverflow.com/questions/40866124/difference-between-dense-and-activation-layer-in-keras'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nDifference between Tensorflow\'s tf.keras.layers.Dense and PyTorch\'s torch.nn.Linear?\n\nAsked 3 years, 3 months ago\n\nModified 3 years, 3 months ago\n\nI have a quick (and possibly silly) question about how Tensorflow defines its Linear layer. Within PyTorch, a Linear (or Dense) layer is defined as, y = x A^T + b where A and b are the weight matrix and bias vector for a Linear layer (see here).\n\nHowever, I can\'t precisely find an equivalent equation for Tensorflow! Is it the same as PyTorch or is it just y = x A + b ?\n\nThank you in advance!\n\nImprove this question\n\nasked Mar 14, 2021 at 16:11\n\nAlphaBetaGamma96AlphaBetaGamma96\n\n67733 gold badges77 silver badges2222 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIf we set activation to None in the dense layer in keras API, then they are technically equivalent.\n\ntf.keras.layers.Dense(..., activation=None)\n\nAccording to the doc, more study here.\n\nactivation: Activation function to use. If you don\'t specify anything, no activation is applied (ie. ""linear"" activation: a(x) = x).\n\nAnd in PyTorch\'s src.\n\nThey are now equal at this point. A linear transformation to the incoming data: y = x*W^T + b. See the following more concrete equivalent implementation of these two. In PyTorch, we do\n\nclass Network(torch.nn.Module): def __init__(self): super(Network, self).__init__() self.fc1 = torch.nn.Linear(5, 30) def forward(self, state): return self.fc1(state)\n\ntrd = torch.nn.Linear(in_features = 3, out_features = 30) y = trd(torch.ones(5, 3)) print(y.size()) # torch.Size([5, 30])\n\nIts equivalent tf implementation would be\n\nmodel = tf.keras.models.Sequential() model.add(tf.keras.layers.Dense(30, input_shape=(5,), activation=None))\n\ntfd = tf.keras.layers.Dense(30, input_shape=(3,), activation=None) x = tfd(tf.ones(shape=(5, 3))) print(x.shape) # (5, 30)\n\nedited Mar 14, 2021 at 17:36\n\nanswered Mar 14, 2021 at 16:43\n\n17k66 gold badges5757 silver badges111111 bronze badges 3\n\nWhat I mean, in terms of difference, is purely with regards to the Linear Algebra of the operation. Rather than just the shape of the output Tensor. For example, it seems that Tensorflow\'s Dense layer is either y = xA + b or y = Ax + b and PyTorch\'s Linear layer is y = xA^T + b. Although, these give out the same shape their derivatives are different.\n\n– AlphaBetaGamma96 Commented Mar 14, 2021 at 18:43\n\nOh, I see. The question title confused me. It seems like you wanted to know how they\'re calculated behind. Alex\'s answer should ok for you.\n\n– Innat Commented Mar 14, 2021 at 19:08\n\nYes, I could\'ve phrased it slightly better. But regardless, I just needed clarification of the Linear Algebra itself and how these Layers differ between Linears. The derivative is one example in how they differ ( as they\'re different linear algrabra operations ). I did state a \'guess\' of what TF\'s Dense layer is and if it\'s the same. So, I might make an edit to the title if other people think it\'ll help with clarification! Thank you for the explanation of the TF\'s Dense layer though! :D\n\n– AlphaBetaGamma96 Commented Mar 14, 2021 at 19:18\n\ntf.keras.layers.Dense is defined here in the tensorflow source code:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py#L1081\n\nIf you follow the references in its call function, it leads you to the definition of the operation used here, which is indeed a matrix multiplication of the inputs and weights plus a bias vector as expected:\n\nhttps://github.com/tensorflow/tensorflow/blob/a68c6117a1a53431e739752bd2ab8654dbe2534a/tensorflow/python/keras/layers/ops/core.py#L74\n\noutputs = gen_math_ops.MatMul(a=inputs, b=kernel) ... outputs = nn_ops.bias_add(outputs, bias)\n\nanswered Mar 14, 2021 at 16:28\n\n23111 silver badge55 bronze badges 4\n\nSo, the definition of the kernel variable is a weight matrix but of the opposite dimension of PyTorch\'s weight matrix? So, let\'s say I have A input features of batch size N, and B output features. The dimensionality of kernel would be [A, B] whereas in the case of PyTorch it\'d be [B, A] (because there\'s a transpose applied to it?)\n\n– AlphaBetaGamma96 Commented Mar 14, 2021 at 16:34\n\nYes they store the weights slightly differently (W.T vs W) but the result is still the same. PyTorch\'s representation is closer to the notation in text books. You can check this quickly by printing out the shape of the Linear/Dense weights in torch and tf.\n\n– Alex Commented Mar 14, 2021 at 17:09\n\nIn line 1192 of the first link to the TF source code above, the weights in are initialised with shape=[last_dim, self.units] (N_feats, N_out) and in PyTorch (source code link), the weights are initialised with Parameter(torch.Tensor(out_features, in_features)) (N_out, N_feats)\n\n– Alex Commented Mar 14, 2021 at 17:13\n\nAlright, so for a linear layer of input x of shape (N_samp, N_feats) the output for TF would be matmul(x, A) + b where A is (N_feats, N_out) and b is (N_out, ) and for PyTorch it\'s matmul(x, A^T) + b where A is now (N_out, N_feats) and b is (N_out, ). Alright, so it seems that the 2 libraries define their Linear layers differently! Thank you @Alex!\n\n– AlphaBetaGamma96 Commented Mar 14, 2021 at 18:47\n\nNot the answer you\'re looking for? Browse other questions tagged\n\npytorch or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n66 PyTorch memory model: ""torch.from_numpy()"" vs ""torch.Tensor()""\n\n65 What is the difference between torch.tensor and torch.Tensor?\n\n10 Differences between `torch.Tensor` and `torch.cuda.Tensor`\n\n2 What\'s different between convolution layer in `Torch`(i.e `nn.SpatialConvolution`) and convolution layer in `Pytorch`(i.e `torch.nn.Conv2d`)\n\n0 Keras vs Pytorch NN code small differences, need clarification\n\n2 what is torch\'s unsqueeze equivalence with tensorflow?\n\n2 TensorFlow vs PyTorch convolution confusion\n\n17 Difference between torch.flatten() and nn.Flatten()\n\n1 PyTorch vs Tensorflow gives different results\n\n2 Is there a difference between Keras Dense layer and Pytorch\'s nn.linear layer?\n\nHot Network Questions\n\nWhy does independent research from people without formal academic qualifications generally turn out to be a complete waste of time?\n\nCan the US president legally kill at will?\n\nim2double and im2uint8 Functions Implementation for Image in C++\n\nDo thermodynamic cycles occur only in human-made machines?\n\nDoes Justice Sotomayor\'s ""Seal Team 6"" example, in and of itself, explicitly give the President the authority to execute opponents? If not, why not?\n\nWhat did Plautus mean by ""intervelli""?\n\nPath integral at large time\n\nfirefox returns odd results for file:/// or file:///tmp\n\nRead an article about connecting a hot tub to a heat pump, looking for details\n\nHourly pay rate calculation between Recruiting and Payroll Systems\n\nSum of the vectors to each of the vertices of the polygon\n\nAction of symmetric group on polynomial ring\n\nStrange Interaction with Professor\n\nEverything has a tiny nuclear reactor in it. How much of a concern are illegal nuclear bombs?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nCan a country refuse to deliver a person accused of attempted murder?\n\nOptimizing Pi Estimation Code\n\nWe are getting some place\n\nWhat spells can I cast while swallowed?\n\n为什么字形不同的两个字「骨」的编码相同（从而是一个字）？\n\nOld SF story about someone who detonated an atomic bomb, sacrificing self to save society from an evil government\n\nHow do I drill a 60cm hole in a tree stump, 4.4 cm wide?\n\nInteresting NT Question With AP and GCD.\n\nIs intuitionistic mathematics situated in time? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_1', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nDifference between Tensorflow\'s tf.keras.layers.Dense and PyTorch\'s torch.nn.Linear?\n\nAsked 3 years, 3 months ago\n\nModified 3 years, 3 months ago\n\nI have a quick (and possibly silly) question about how Tensorflow defines its Linear layer. Within PyTorch, a Linear (or Dense) layer is defined as, y = x A^T + b where A and b are the weight matrix and bias vector for a Linear layer (see here).\n\nHowever, I can\'t precisely find an equivalent equation for Tensorflow! Is it the same as PyTorch or is it just y = x A + b ?\n\nThank you in advance!\n\nImprove this question\n\nasked Mar 14, 2021 at 16:11\n\nAlphaBetaGamma96AlphaBetaGamma96\n\n67733 gold badges77 silver badges2222 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIf we set activation to None in the dense layer in keras API, then they are technically equivalent.\n\ntf.keras.layers.Dense(..., activation=None)\n\nAccording to the doc, more study here.\n\nactivation: Activation function to use. If you don\'t specify anything, no activation is applied (ie. ""linear"" activation: a(x) = x).\n\nAnd in PyTorch\'s src.\n\nThey are now equal at this point. A linear transformation to the incoming data: y = x*W^T + b. See the following more concrete equivalent implementation of these two. In PyTorch, we do\n\nclass Network(torch.nn.Module): def __init__(self): super(Network, self).__init__() self.fc1 = torch.nn.Linear(5, 30) def forward(self, state): return self.fc1(state)\n\ntrd = torch.nn.Linear(in_features = 3, out_features = 30) y = trd(torch.ones(5, 3)) print(y.size()) # torch.Size([5, 30])\n\nIts equivalent tf implementation would be\n\nmodel = tf.keras.models.Sequential() model.add(tf.keras.layers.Dense(30, input_shape=(5,), activation=None))\n\ntfd = tf.keras.layers.Dense(30, input_shape=(3,), activation=None) x = tfd(tf.ones(shape=(5, 3))) print(x.shape) # (5, 30)\n\nedited Mar 14, 2021 at 17:36\n\nanswered Mar 14, 2021 at 16:43\n\n17k66 gold badges5757 silver badges111111 bronze badges 3\n\nWhat I mean, in terms of difference, is purely with regards to the Linear Algebra of the operation. Rather than just the shape of the output Tensor. For example, it seems that Tensorflow\'s Dense layer is either y = xA + b or y = Ax + b and PyTorch\'s Linear layer is y = xA^T + b. Although, these give out the same shape their derivatives are different.\n\n– AlphaBetaGamma96 Commented Mar 14, 2021 at 18:43\n\nOh, I see. The question title confused me. It seems like you wanted to know how they\'re calculated behind. Alex\'s answer should ok for you.\n\n– Innat Commented Mar 14, 2021 at 19:08\n\nYes, I could\'ve phrased it slightly better. But regardless, I just needed clarification of the Linear Algebra itself and how these Layers differ between Linears. The derivative is one example in how they differ ( as they\'re different linear algrabra operations ). I did state a \'guess\' of what TF\'s Dense layer is and if it\'s the same. So, I might make an edit to the title if other people think it\'ll help with clarification! Thank you for the explanation of the TF\'s Dense layer though! :D\n\n– AlphaBetaGamma96 Commented Mar 14, 2021 at 19:18\n\ntf.keras.layers.Dense is defined here in the tensorflow source code:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py#L1081\n\nIf you follow the references in its call function, it leads you to the definition of the operation used here, which is indeed a matrix multiplication of the inputs and weights plus a bias vector as expected:\n\nhttps://github.com/tensorflow/tensorflow/blob/a68c6117a1a53431e739752bd2ab8654dbe2534a/tensorflow/python/keras/layers/ops/core.py#L74\n\noutputs = gen_math_ops.MatMul(a=inputs, b=kernel) ... outputs = nn_ops.bias_add(outputs, bias)\n\nanswered Mar 14, 2021 at 16:28\n\n23111 silver badge55 bronze badges 4\n\nSo, the definition of the kernel variable is a weight matrix but of the opposite dimension of PyTorch\'s weight matrix? So, let\'s say I have A input features of batch size N, and B output features. The dimensionality of kernel would be [A, B] whereas in the case of PyTorch it\'d be [B, A] (because there\'s a transpose applied to it?)\n\n– AlphaBetaGamma96 Commented Mar 14, 2021 at 16:34\n\nYes they store the weights slightly differently (W.T vs W) but the result is still the same. PyTorch\'s representation is closer to the notation in text books. You can check this quickly by printing out the shape of the Linear/Dense weights in torch and tf.\n\n– Alex Commented Mar 14, 2021 at 17:09\n\nIn line 1192 of the first link to the TF source code above, the weights in are initialised with shape=[last_dim, self.units] (N_feats, N_out) and in PyTorch (source code link), the weights are initialised with Parameter(torch.Tensor(out_features, in_features)) (N_out, N_feats)\n\n– Alex Commented Mar 14, 2021 at 17:13\n\nAlright, so for a linear layer of input x of shape (N_samp, N_feats) the output for TF would be matmul(x, A) + b where A is (N_feats, N_out) and b is (N_out, ) and for PyTorch it\'s matmul(x, A^T) + b where A is now (N_out, N_feats) and b is (N_out, ). Alright, so it seems that the 2 libraries define their Linear layers differently! Thank you @Alex!\n\n– AlphaBetaGamma96 Commented Mar 14, 2021 at 18:47\n\nNot the answer you\'re looking for? Browse other questions tagged\n\npytorch or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n66 PyTorch memory model: ""torch.from_numpy()"" vs ""torch.Tensor()""\n\n65 What is the difference between torch.tensor and torch.Tensor?\n\n10 Differences between `torch.Tensor` and `torch.cuda.Tensor`\n\n2 What\'s different between convolution layer in `Torch`(i.e `nn.SpatialConvolution`) and convolution layer in `Pytorch`(i.e `torch.nn.Conv2d`)\n\n0 Keras vs Pytorch NN code small differences, need clarification\n\n2 what is torch\'s unsqueeze equivalence with tensorflow?\n\n2 TensorFlow vs PyTorch convolution confusion\n\n17 Difference between torch.flatten() and nn.Flatten()\n\n1 PyTorch vs Tensorflow gives different results\n\n2 Is there a difference between Keras Dense layer and Pytorch\'s nn.linear layer?\n\nHot Network Questions\n\nWhy does independent research from people without formal academic qualifications generally turn out to be a complete waste of time?\n\nCan the US president legally kill at will?\n\nim2double and im2uint8 Functions Implementation for Image in C++\n\nDo thermodynamic cycles occur only in human-made machines?\n\nDoes Justice Sotomayor\'s ""Seal Team 6"" example, in and of itself, explicitly give the President the authority to execute opponents? If not, why not?\n\nWhat did Plautus mean by ""intervelli""?\n\nPath integral at large time\n\nfirefox returns odd results for file:/// or file:///tmp\n\nRead an article about connecting a hot tub to a heat pump, looking for details\n\nHourly pay rate calculation between Recruiting and Payroll Systems\n\nSum of the vectors to each of the vertices of the polygon\n\nAction of symmetric group on polynomial ring\n\nStrange Interaction with Professor\n\nEverything has a tiny nuclear reactor in it. How much of a concern are illegal nuclear bombs?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nCan a country refuse to deliver a person accused of attempted murder?\n\nOptimizing Pi Estimation Code\n\nWe are getting some place\n\nWhat spells can I cast while swallowed?\n\n为什么字形不同的两个字「骨」的编码相同（从而是一个字）？\n\nOld SF story about someone who detonated an atomic bomb, sacrificing self to save society from an evil government\n\nHow do I drill a 60cm hole in a tree stump, 4.4 cm wide?\n\nInteresting NT Question With AP and GCD.\n\nIs intuitionistic mathematics situated in time? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-05T07:03:17', 'title': ""Difference between Tensorflow's tf.keras.layers.Dense and PyTorch's torch.nn.Linear? - Stack Overflow"", 'url': 'https://stackoverflow.com/questions/66626700/difference-between-tensorflows-tf-keras-layers-dense-and-pytorchs-torch-nn-lin'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nQuestion on Tensorflow Dense Layer Implementation\n\nAsked 3 years, 9 months ago\n\nModified 3 years, 9 months ago\n\nFrom https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense (*Note Section)\n\nFor an input of (batch_size, d0, d1) why is the same (d1, units) kernel used for every sub-tensor (1, 1, d1)?\n\nAdditionally, why is the higher dimension dense layer operation broken down to work on subsets of input nodes, instead of having a weight from all d0xd1 inputs to an output node?\n\nI apologize if I am missing something obvious and thank you for any help!\n\nImprove this question\n\nedited Jul 2, 2020 at 20:56\n\nasked Jul 2, 2020 at 18:40\n\nSpaceDandySpaceDandy\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nSo I dug through the source code and found Tensorflow made a change in how they implement the dense operation and I asked my boss at uni why they made this change.\n\nIn tf1 for input > rank 2 they flattened the input and just did a regular 1-D dense operation.\n\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/layers/core.py\n\nIn tf2 for input > rank 2 they use the tensordot operation. This uses a smaller kernel and shares it for all input sub-tensors. This has the effect of sharing the learned channel-wise information.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/ops/core.py\n\nanswered Jul 4, 2020 at 22:25\n\nSpaceDandySpaceDandy\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nDevelopers with AI assistants need to follow the pair programming model\n\nHow do mixture-of-experts layers affect transformer models?\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\nWe will launch the 1-rep voting experiment on Stack Overflow for 4–6 weeks,...\n\n9 How to understand the ""Densely Connected Layer"" section in tensorflow tutorial\n\n65 Tensorflow dense gradient explanation?\n\n20 Is tf.layers.dense a single layer?\n\n3 Tensorflow dense layer operation\n\n1 DenseNet in Tensorflow\n\n0 Questions on tf.layers .dense\n\n0 Confusion on the \'linear\' activation in tf.keras.layers.Dense()\n\n1 Tensorflow - building LSTM model - need for tf.keras.layers.Dense()\n\n2 usage of tf.keras.layers.DenseFeatures\n\n1 Result of Dense layer in keras\n\nHot Network Questions\n\nWhat happens to an ongoing electrolysis when you turn off the battery?\n\nDoes changing your Mac name affect your SSH key?\n\nRadio Drama \'free range\' clones of rich people are grown so replacement organs will be available for them\n\nUK visitor visa for an OCI card holder with US passport\n\nWhat kind of chart is this and how to read it?\n\nWhat do you call a specific \'snapshot\' of the way a group of levers, knobs, and buttons are set?\n\nAre theorems in mathematics that have only been proved by contradiction applicable anywhere outside of mathematics?\n\nStaying out of the blind spot of a truck that doesn\'t have mirrors?\n\nHow might a Christian persuade a naturalist non-theist that the universe cannot be a brute fact?\n\nDonated $500 to a charity but didn\'t get an official receipt\n\nWhat does ""zigs when others zag"" mean?\n\nCharacteristic impedance of a BNC jack to jack adapter\n\nIs Gravitational Constant really constant over our Universe\n\nIf my 00:22 Deutsche Bahn train is cancelled and the following train is inconvenient, can I leave earlier?\n\nAn Arena/Bump Allocator in C\n\nRegarding phase space diagram/trajectory\n\nWill it be possible to prevent cancer by introducing ECC on genome?\n\nMultiple MOSFETs in parallel to switch a large load\n\nCan Neutron Star merger remnants solve the black hole mass gap? Why can\'t we observe them?\n\nAdvice on Picking Grad School with Goal of Getting Academic Job at Small College\n\nIs touring the Basque region in Spain by bike safe?\n\nTerm for a single piece of jargon\n\nWhy doesn\'t ""selecting similar faces by area"" work in my case?\n\nDo languages have any sort of upper limit for how long it takes to say things? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_3', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nQuestion on Tensorflow Dense Layer Implementation\n\nAsked 3 years, 9 months ago\n\nModified 3 years, 9 months ago\n\nFrom https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense (*Note Section)\n\nFor an input of (batch_size, d0, d1) why is the same (d1, units) kernel used for every sub-tensor (1, 1, d1)?\n\nAdditionally, why is the higher dimension dense layer operation broken down to work on subsets of input nodes, instead of having a weight from all d0xd1 inputs to an output node?\n\nI apologize if I am missing something obvious and thank you for any help!\n\nImprove this question\n\nedited Jul 2, 2020 at 20:56\n\nasked Jul 2, 2020 at 18:40\n\nSpaceDandySpaceDandy\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nSo I dug through the source code and found Tensorflow made a change in how they implement the dense operation and I asked my boss at uni why they made this change.\n\nIn tf1 for input > rank 2 they flattened the input and just did a regular 1-D dense operation.\n\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/python/keras/layers/core.py\n\nIn tf2 for input > rank 2 they use the tensordot operation. This uses a smaller kernel and shares it for all input sub-tensors. This has the effect of sharing the learned channel-wise information.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/ops/core.py\n\nanswered Jul 4, 2020 at 22:25\n\nSpaceDandySpaceDandy\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nDevelopers with AI assistants need to follow the pair programming model\n\nHow do mixture-of-experts layers affect transformer models?\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\nWe will launch the 1-rep voting experiment on Stack Overflow for 4–6 weeks,...\n\n9 How to understand the ""Densely Connected Layer"" section in tensorflow tutorial\n\n65 Tensorflow dense gradient explanation?\n\n20 Is tf.layers.dense a single layer?\n\n3 Tensorflow dense layer operation\n\n1 DenseNet in Tensorflow\n\n0 Questions on tf.layers .dense\n\n0 Confusion on the \'linear\' activation in tf.keras.layers.Dense()\n\n1 Tensorflow - building LSTM model - need for tf.keras.layers.Dense()\n\n2 usage of tf.keras.layers.DenseFeatures\n\n1 Result of Dense layer in keras\n\nHot Network Questions\n\nWhat happens to an ongoing electrolysis when you turn off the battery?\n\nDoes changing your Mac name affect your SSH key?\n\nRadio Drama \'free range\' clones of rich people are grown so replacement organs will be available for them\n\nUK visitor visa for an OCI card holder with US passport\n\nWhat kind of chart is this and how to read it?\n\nWhat do you call a specific \'snapshot\' of the way a group of levers, knobs, and buttons are set?\n\nAre theorems in mathematics that have only been proved by contradiction applicable anywhere outside of mathematics?\n\nStaying out of the blind spot of a truck that doesn\'t have mirrors?\n\nHow might a Christian persuade a naturalist non-theist that the universe cannot be a brute fact?\n\nDonated $500 to a charity but didn\'t get an official receipt\n\nWhat does ""zigs when others zag"" mean?\n\nCharacteristic impedance of a BNC jack to jack adapter\n\nIs Gravitational Constant really constant over our Universe\n\nIf my 00:22 Deutsche Bahn train is cancelled and the following train is inconvenient, can I leave earlier?\n\nAn Arena/Bump Allocator in C\n\nRegarding phase space diagram/trajectory\n\nWill it be possible to prevent cancer by introducing ECC on genome?\n\nMultiple MOSFETs in parallel to switch a large load\n\nCan Neutron Star merger remnants solve the black hole mass gap? Why can\'t we observe them?\n\nAdvice on Picking Grad School with Goal of Getting Academic Job at Small College\n\nIs touring the Basque region in Spain by bike safe?\n\nTerm for a single piece of jargon\n\nWhy doesn\'t ""selecting similar faces by area"" work in my case?\n\nDo languages have any sort of upper limit for how long it takes to say things? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-04-04T19:42:59', 'title': 'Question on Tensorflow Dense Layer Implementation - Stack Overflow', 'url': 'https://stackoverflow.com/questions/62703012/question-on-tensorflow-dense-layer-implementation'})], [Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nWhat is the difference b.t. tf.layers.dense and tf.layers.Dense #22020\n\nBNAadministrator3 opened this issue\n\nSep 3, 2018 · 1 comment\n\nWhat is the difference b.t. tf.layers.dense and tf.layers.Dense #22020\n\nBNAadministrator3 opened this issue\n\nSep 3, 2018 · 1 comment\n\nBNAadministrator3 commented\n\nJust as the title. I tried to search the Stack Overflow but I can\'t figure it out. I mean, how to choose the layer b.t. dense and Dense when I construct a neural network?\n\nPlease go to Stack Overflow for help and support:\n\nhttps://stackoverflow.com/questions/tagged/tensorflow\n\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\n\nThe form below must be filled out.\n\nIt shouldn\'t be a TensorBoard issue. Those go here.\n\nHere\'s why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n\nTensorFlow installed from (source or binary):\n\nTensorFlow version (use command below):\n\nBazel version (if compiling from source):\n\nGCC/Compiler version (if compiling from source):\n\nGPU model and memory:\n\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\n\nYou can obtain the TensorFlow version with\n\npython -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""\n\nDescribe the problem\n\nDescribe the problem clearly here. Be sure to convey here why it\'s a bug in TensorFlow or a feature request.\n\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\n\nThe text was updated successfully, but these errors were encountered:\n\nBNAadministrator3 closed this as completed\n\ncsukuangfj commented\n\ntensorflow/tensorflow/python/layers/core.py\n\nLines 32 to 33 in 9fa3d27\n\n@tf_export(v1=[\'layers.Dense\'])\n\nclass Dense(keras_layers.Dense, base.Layer):\n\ntf.layers.Dense returns an instance of class Dense.\n\ntensorflow/tensorflow/python/layers/core.py\n\nLines 116 to 117 in 9fa3d27\n\n@tf_export(v1=[\'layers.dense\'])\n\ntensorflow/tensorflow/python/layers/core.py\n\nLines 174 to 188 in 9fa3d27\n\nlayer = Dense(units,\n\nactivation=activation,\n\nkernel_initializer=kernel_initializer,\n\nbias_initializer=bias_initializer,\n\nkernel_regularizer=kernel_regularizer,\n\nbias_regularizer=bias_regularizer,\n\nactivity_regularizer=activity_regularizer,\n\nkernel_constraint=kernel_constraint,\n\nbias_constraint=bias_constraint,\n\ntrainable=trainable,\n\nreturn layer.apply(inputs)\n\ntf.layers.dense first creates an instance of tf.layers.Dense, then calls return layer.apply(inputs), which returns a tensor.\n\ntf.layers.Dropout and tf.layers.dropout\n\ntf.layers.Flatten and tf.layers.flatten\n\ntf.layers.AveragePooling1D and tf.layers.average_pooling1d\n\nand many more ... (refer to the code)\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_2', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nWhat is the difference b.t. tf.layers.dense and tf.layers.Dense #22020\n\nBNAadministrator3 opened this issue\n\nSep 3, 2018 · 1 comment\n\nWhat is the difference b.t. tf.layers.dense and tf.layers.Dense #22020\n\nBNAadministrator3 opened this issue\n\nSep 3, 2018 · 1 comment\n\nBNAadministrator3 commented\n\nJust as the title. I tried to search the Stack Overflow but I can\'t figure it out. I mean, how to choose the layer b.t. dense and Dense when I construct a neural network?\n\nPlease go to Stack Overflow for help and support:\n\nhttps://stackoverflow.com/questions/tagged/tensorflow\n\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\n\nThe form below must be filled out.\n\nIt shouldn\'t be a TensorBoard issue. Those go here.\n\nHere\'s why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n\nTensorFlow installed from (source or binary):\n\nTensorFlow version (use command below):\n\nBazel version (if compiling from source):\n\nGCC/Compiler version (if compiling from source):\n\nGPU model and memory:\n\nExact command to reproduce:\n\nYou can collect some of this information using our environment capture script:\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\n\nYou can obtain the TensorFlow version with\n\npython -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""\n\nDescribe the problem\n\nDescribe the problem clearly here. Be sure to convey here why it\'s a bug in TensorFlow or a feature request.\n\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\n\nThe text was updated successfully, but these errors were encountered:\n\nBNAadministrator3 closed this as completed\n\ncsukuangfj commented\n\ntensorflow/tensorflow/python/layers/core.py\n\nLines 32 to 33 in 9fa3d27\n\n@tf_export(v1=[\'layers.Dense\'])\n\nclass Dense(keras_layers.Dense, base.Layer):\n\ntf.layers.Dense returns an instance of class Dense.\n\ntensorflow/tensorflow/python/layers/core.py\n\nLines 116 to 117 in 9fa3d27\n\n@tf_export(v1=[\'layers.dense\'])\n\ntensorflow/tensorflow/python/layers/core.py\n\nLines 174 to 188 in 9fa3d27\n\nlayer = Dense(units,\n\nactivation=activation,\n\nkernel_initializer=kernel_initializer,\n\nbias_initializer=bias_initializer,\n\nkernel_regularizer=kernel_regularizer,\n\nbias_regularizer=bias_regularizer,\n\nactivity_regularizer=activity_regularizer,\n\nkernel_constraint=kernel_constraint,\n\nbias_constraint=bias_constraint,\n\ntrainable=trainable,\n\nreturn layer.apply(inputs)\n\ntf.layers.dense first creates an instance of tf.layers.Dense, then calls return layer.apply(inputs), which returns a tensor.\n\ntf.layers.Dropout and tf.layers.dropout\n\ntf.layers.Flatten and tf.layers.flatten\n\ntf.layers.AveragePooling1D and tf.layers.average_pooling1d\n\nand many more ... (refer to the code)\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-11T13:58:19', 'title': 'What is the difference b.t. tf.layers.dense and tf.layers.Dense · Issue #22020 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/22020'}), Document(page_content=""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nSupport for directly importing Dense layer from keras API #46407\n\nzyberg2091 opened this issue\n\nJan 13, 2021 · 6 comments\n\nSupport for directly importing Dense layer from keras API #46407\n\nzyberg2091 opened this issue\n\nJan 13, 2021 · 6 comments\n\nKeras related issues type:feature\n\nzyberg2091 commented\n\nTensorFlow version (you are using): TF 4.0\n\nAre you willing to contribute it (Yes/No): No\n\nDescribe the feature and the current behavior/state.\n\nWhile i was writing code i noticed Dense layer is not supported in importing directly in keras but input layer is there. In that case anyhow we have to mention full path dependency because this layer is the most basic layer and frequently used while building the architecture even if not initially used then at least last few layers are dense layers only mostly. It can be used as feed forward neural network as well as in the output layer.so I think it will be good if this layer can be directly imported from keras without any path dependencies,\n\nfrom tensorflow.python.keras.engine.input_layer import Input\n\nJust adding Dense here like this will do\n\nWill this change the current api? How?\n\nNothing is going to change in api. This will be just an additional functionality for the users to serve their purpose easily.\n\nThe text was updated successfully, but these errors were encountered:\n\nzyberg2091 added the type:feature\n\nFeature requests label\n\ngoogle-ml-butler bot assigned ravikyram\n\nravikyram added the comp:keras\n\nKeras related issues label\n\nravikyram assigned jvishnuvardhan and unassigned ravikyram\n\nanilkumarKanasani commented\n\n@zyberg2091 , Can we work on this issue ? Is it assigned to any one ?\n\njvishnuvardhan commented\n\n@zyberg2091 Just want to understand your use case. Currently we import Input and Dense layers as follows\n\nfrom tensorflow.keras import Input from tensorflow.keras.layers import Dense\n\nConsidering a simple example model,\n\nmodel = tf.keras.models.Sequential([ Dense(512, activation='relu', input_shape=(784,)), Dense(10) ])\n\nIn the above model, we are using Dense layer as simply Dense.\n\nIf I understand correctly, you want to import Dense directly as\n\nfrom tensorflow.keras import Dense\n\nCurrently Dense layer is part of layers API. Whether Dense is part of Layers or part of keras (similar to Input layer), the model building part of the code is same.\n\nI want to understand what do we accomplish from this requested feature (by moving Dense layer ). Is there anything missing from the current implementation that cannot work with your use-case? If we simply move out Dense, it may confuse the large community of existing users. Please let us know your use case with little more details. Thanks!\n\njvishnuvardhan added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nzyberg2091 commented\n\n@jvishnuvardhan considering your simple sequential API model. i can just import those layers in just one 1 line from keras engine.These is preventing me from just writing extra useless line.well all of us are aware that these two layers have to be imported in most use cases because these two layers are basic layers.for my project i just needed these two layers and if i have to use another layer other than input than i have to write like this-\n\nfrom tensorflow.keras.layers import Input,Dense\n\nThen i don't understand this approach-\n\nfrom tensorflow.keras import Input from tensorflow.keras.layers Import Dense\n\nUsers may have different use cases according to their needs and so importing layers become specific.Therefore i understand other layers might be imported from keras layers API but if we are able to import Input layer directly why can't we do this to dense layer which is equally basic as Input layer.\n\nThis also creates confusion and i am unable to think where can we use this From tensorflow.keras import Input alone\n\nMost simple use case can be -\n\nfrom tensorflow.keras import Input,Dense for importing layers from keras engine\n\njvishnuvardhan added stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower and removed stat:awaiting response\n\nStatus - Awaiting response from author labels\n\nTo clarify, you want us to export tf.keras.layers.Dense with an alias tf.keras.Dense?\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nzyberg2091 commented\n\nNotes from our internal discussion about this. We don't want to blur layer/non-layer api boundaries in Keras like this.\n\nkeras.Input is not a layer, it is a symbolic input object that can be passed to layers. keras.layers.InputLayer is the corresponding input layer, which is not aliased under keras directly.\n\n(from tensorflow.python.keras.engine.input_layer import Input might refer to the internal file location of keras.Input, but it's not a layer itself and as such we don't expose it under keras.layers.)\n\ntomerk closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues type:feature\n\nYou can’t perform that action at this time."", metadata={'id': 'web-search_4', 'snippet': ""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nSupport for directly importing Dense layer from keras API #46407\n\nzyberg2091 opened this issue\n\nJan 13, 2021 · 6 comments\n\nSupport for directly importing Dense layer from keras API #46407\n\nzyberg2091 opened this issue\n\nJan 13, 2021 · 6 comments\n\nKeras related issues type:feature\n\nzyberg2091 commented\n\nTensorFlow version (you are using): TF 4.0\n\nAre you willing to contribute it (Yes/No): No\n\nDescribe the feature and the current behavior/state.\n\nWhile i was writing code i noticed Dense layer is not supported in importing directly in keras but input layer is there. In that case anyhow we have to mention full path dependency because this layer is the most basic layer and frequently used while building the architecture even if not initially used then at least last few layers are dense layers only mostly. It can be used as feed forward neural network as well as in the output layer.so I think it will be good if this layer can be directly imported from keras without any path dependencies,\n\nfrom tensorflow.python.keras.engine.input_layer import Input\n\nJust adding Dense here like this will do\n\nWill this change the current api? How?\n\nNothing is going to change in api. This will be just an additional functionality for the users to serve their purpose easily.\n\nThe text was updated successfully, but these errors were encountered:\n\nzyberg2091 added the type:feature\n\nFeature requests label\n\ngoogle-ml-butler bot assigned ravikyram\n\nravikyram added the comp:keras\n\nKeras related issues label\n\nravikyram assigned jvishnuvardhan and unassigned ravikyram\n\nanilkumarKanasani commented\n\n@zyberg2091 , Can we work on this issue ? Is it assigned to any one ?\n\njvishnuvardhan commented\n\n@zyberg2091 Just want to understand your use case. Currently we import Input and Dense layers as follows\n\nfrom tensorflow.keras import Input from tensorflow.keras.layers import Dense\n\nConsidering a simple example model,\n\nmodel = tf.keras.models.Sequential([ Dense(512, activation='relu', input_shape=(784,)), Dense(10) ])\n\nIn the above model, we are using Dense layer as simply Dense.\n\nIf I understand correctly, you want to import Dense directly as\n\nfrom tensorflow.keras import Dense\n\nCurrently Dense layer is part of layers API. Whether Dense is part of Layers or part of keras (similar to Input layer), the model building part of the code is same.\n\nI want to understand what do we accomplish from this requested feature (by moving Dense layer ). Is there anything missing from the current implementation that cannot work with your use-case? If we simply move out Dense, it may confuse the large community of existing users. Please let us know your use case with little more details. Thanks!\n\njvishnuvardhan added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nzyberg2091 commented\n\n@jvishnuvardhan considering your simple sequential API model. i can just import those layers in just one 1 line from keras engine.These is preventing me from just writing extra useless line.well all of us are aware that these two layers have to be imported in most use cases because these two layers are basic layers.for my project i just needed these two layers and if i have to use another layer other than input than i have to write like this-\n\nfrom tensorflow.keras.layers import Input,Dense\n\nThen i don't understand this approach-\n\nfrom tensorflow.keras import Input from tensorflow.keras.layers Import Dense\n\nUsers may have different use cases according to their needs and so importing layers become specific.Therefore i understand other layers might be imported from keras layers API but if we are able to import Input layer directly why can't we do this to dense layer which is equally basic as Input layer.\n\nThis also creates confusion and i am unable to think where can we use this From tensorflow.keras import Input alone\n\nMost simple use case can be -\n\nfrom tensorflow.keras import Input,Dense for importing layers from keras engine\n\njvishnuvardhan added stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower and removed stat:awaiting response\n\nStatus - Awaiting response from author labels\n\nTo clarify, you want us to export tf.keras.layers.Dense with an alias tf.keras.Dense?\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nzyberg2091 commented\n\nNotes from our internal discussion about this. We don't want to blur layer/non-layer api boundaries in Keras like this.\n\nkeras.Input is not a layer, it is a symbolic input object that can be passed to layers. keras.layers.InputLayer is the corresponding input layer, which is not aliased under keras directly.\n\n(from tensorflow.python.keras.engine.input_layer import Input might refer to the internal file location of keras.Input, but it's not a layer itself and as such we don't expose it under keras.layers.)\n\ntomerk closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues type:feature\n\nYou can’t perform that action at this time."", 'timestamp': '2024-06-09T17:26:38', 'title': 'Support for directly importing Dense layer from keras API · Issue #46407 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/46407'})], [Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to modules, layers, and models\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nTo do machine learning in TensorFlow, you are likely to need to define, save, and restore a model.\n\nA model is, abstractly:\n\nA function that computes something on tensors (a forward pass)\n\nSome variables that can be updated in response to training\n\nIn this guide, you will go below the surface of Keras to see how TensorFlow models are defined. This looks at how TensorFlow collects variables and models, as well as how they are saved and restored.\n\nNote: If you instead want to immediately get started with Keras, please see the collection of Keras guides.\n\nimport tensorflow as tf import keras from datetime import datetime %load_ext tensorboard\n\n2023-10-18 01:21:05.536666: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2023-10-18 01:21:05.536712: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2023-10-18 01:21:05.536766: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nMost models are made of layers. Layers are functions with a known mathematical structure that can be reused and have trainable variables. In TensorFlow, most high-level implementations of layers and models, such as Keras or Sonnet, are built on the same foundational class: tf.Module.\n\nHere\'s an example of a very simple tf.Module that operates on a scalar tensor:\n\nclass SimpleModule(tf.Module): def __init__(self, name=None): super().__init__(name=name) self.a_variable = tf.Variable(5.0, name=""train_me"") self.non_trainable_variable = tf.Variable(5.0, trainable=False, name=""do_not_train_me"") def __call__(self, x): return self.a_variable * x + self.non_trainable_variable simple_module = SimpleModule(name=""simple"") simple_module(tf.constant(5.0))\n\n2023-10-18 01:21:08.181350: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... <tf.Tensor: shape=(), dtype=float32, numpy=30.0>\n\nModules and, by extension, layers are deep-learning terminology for ""objects"": they have internal state, and methods that use that state.\n\nThere is nothing special about __call__ except to act like a Python callable; you can invoke your models with whatever functions you wish.\n\nYou can set the trainability of variables on and off for any reason, including freezing layers and variables during fine-tuning.\n\nNote: tf.Module is the base class for both tf.keras.layers.Layer and tf.keras.Model, so everything you come across here also applies in Keras. For historical compatibility reasons Keras layers do not collect variables from modules, so your models should use only modules or only Keras layers. However, the methods shown below for inspecting variables are the same in either case.\n\nBy subclassing tf.Module, any tf.Variable or tf.Module instances assigned to this object\'s properties are automatically collected. This allows you to save and load variables, and also create collections of tf.Modules.\n\n# All trainable variables print(""trainable variables:"", simple_module.trainable_variables) # Every variable print(""all variables:"", simple_module.variables)\n\ntrainable variables: (<tf.Variable \'train_me:0\' shape=() dtype=float32, numpy=5.0>,) all variables: (<tf.Variable \'train_me:0\' shape=() dtype=float32, numpy=5.0>, <tf.Variable \'do_not_train_me:0\' shape=() dtype=float32, numpy=5.0>)\n\nThis is an example of a two-layer linear layer model made out of modules.\n\nFirst a dense (linear) layer:\n\nclass Dense(tf.Module): def __init__(self, in_features, out_features, name=None): super().__init__(name=name) self.w = tf.Variable( tf.random.normal([in_features, out_features]), name=\'w\') self.b = tf.Variable(tf.zeros([out_features]), name=\'b\') def __call__(self, x): y = tf.matmul(x, self.w) + self.b return tf.nn.relu(y)\n\nAnd then the complete model, which makes two layer instances and applies them:\n\nclass SequentialModule(tf.Module): def __init__(self, name=None): super().__init__(name=name) self.dense_1 = Dense(in_features=3, out_features=3) self.dense_2 = Dense(in_features=3, out_features=2) def __call__(self, x): x = self.dense_1(x) return self.dense_2(x) # You have made a model! my_model = SequentialModule(name=""the_model"") # Call it, with random results print(""Model results:"", my_model(tf.constant([[2.0, 2.0, 2.0]])))\n\nModel results: tf.Tensor([[0. 3.415034]], shape=(1, 2), dtype=float32)\n\ntf.Module instances will automatically collect, recursively, any tf.Variable or tf.Module instances assigned to it. This allows you to manage collections of tf.Modules with a single model instance, and save and load whole models.\n\nprint(""Submodules:"", my_model.submodules)\n\nSubmodules: (<__main__.Dense object at 0x7f7931aea250>, <__main__.Dense object at 0x7f77ed5b8a00>)\n\nfor var in my_model.variables: print(var, ""\\n"")\n\n<tf.Variable \'b:0\' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)> <tf.Variable \'w:0\' shape=(3, 3) dtype=float32, numpy= array([[-2.8161757, -2.6065955, 1.9061812], [-0.9430401, -0.4624743, -0.4531979], [-1.3428234, 0.7062293, 0.7874674]], dtype=float32)> <tf.Variable \'b:0\' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> <tf.Variable \'w:0\' shape=(3, 2) dtype=float32, numpy= array([[ 1.0474309 , -0.6920227 ], [ 1.2405277 , 0.36411622], [-1.6990206 , 0.762131 ]], dtype=float32)>\n\nWaiting to create variables\n\nYou may have noticed here that you have to define both input and output sizes to the layer. This is so the w variable has a known shape and can be allocated.\n\nBy deferring variable creation to the first time the module is called with a specific input shape, you do not need specify the input size up front.\n\nclass FlexibleDenseModule(tf.Module): # Note: No need for `in_features` def __init__(self, out_features, name=None): super().__init__(name=name) self.is_built = False self.out_features = out_features def __call__(self, x): # Create variables on first call. if not self.is_built: self.w = tf.Variable( tf.random.normal([x.shape[-1], self.out_features]), name=\'w\') self.b = tf.Variable(tf.zeros([self.out_features]), name=\'b\') self.is_built = True y = tf.matmul(x, self.w) + self.b return tf.nn.relu(y)\n\n# Used in a module class MySequentialModule(tf.Module): def __init__(self, name=None): super().__init__(name=name) self.dense_1 = FlexibleDenseModule(out_features=3) self.dense_2 = FlexibleDenseModule(out_features=2) def __call__(self, x): x = self.dense_1(x) return self.dense_2(x) my_model = MySequentialModule(name=""the_model"") print(""Model results:"", my_model(tf.constant([[2.0, 2.0, 2.0]])))\n\nModel results: tf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)\n\nThis flexibility is why TensorFlow layers often only need to specify the shape of their outputs, such as in tf.keras.layers.Dense, rather than both the input and output size.\n\nYou can save a tf.Module as both a checkpoint and a SavedModel.\n\nCheckpoints are just the weights (that is, the values of the set of variables inside the module and its submodules):\n\nchkp_path = ""my_checkpoint"" checkpoint = tf.train.Checkpoint(model=my_model) checkpoint.write(chkp_path)\n\nCheckpoints consist of two kinds of files: the data itself and an index file for metadata. The index file keeps track of what is actually saved and the numbering of checkpoints, while the checkpoint data contains the variable values and their attribute lookup paths.\n\nmy_checkpoint.data-00000-of-00001 my_checkpoint.index\n\nYou can look inside a checkpoint to be sure the whole collection of variables is saved, sorted by the Python object that contains them.\n\ntf.train.list_variables(chkp_path)\n\n[(\'_CHECKPOINTABLE_OBJECT_GRAPH\', []), (\'model/dense_1/b/.ATTRIBUTES/VARIABLE_VALUE\', [3]), (\'model/dense_1/w/.ATTRIBUTES/VARIABLE_VALUE\', [3, 3]), (\'model/dense_2/b/.ATTRIBUTES/VARIABLE_VALUE\', [2]), (\'model/dense_2/w/.ATTRIBUTES/VARIABLE_VALUE\', [3, 2])]\n\nDuring distributed (multi-machine) training they can be sharded, which is why they are numbered (e.g., \'00000-of-00001\'). In this case, though, there is only one shard.\n\nWhen you load models back in, you overwrite the values in your Python object.\n\nnew_model = MySequentialModule() new_checkpoint = tf.train.Checkpoint(model=new_model) new_checkpoint.restore(""my_checkpoint"") # Should be the same result as above new_model(tf.constant([[2.0, 2.0, 2.0]]))\n\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0., 0.]], dtype=float32)>\n\nNote: As checkpoints are at the heart of long training workflows tf.checkpoint.CheckpointManager is a helper class that makes checkpoint management much easier. Refer to the Training checkpoints guide for more details.\n\nTensorFlow can run models without the original Python objects, as demonstrated by TensorFlow Serving and TensorFlow Lite, even when you download a trained model from TensorFlow Hub.\n\nTensorFlow needs to know how to do the computations described in Python, but without the original code. To do this, you can make a graph, which is described in the Introduction to graphs and functions guide.\n\nThis graph contains operations, or ops, that implement the function.\n\nYou can define a graph in the model above by adding the @tf.function decorator to indicate that this code should run as a graph.\n\nclass MySequentialModule(tf.Module): def __init__(self, name=None): super().__init__(name=name) self.dense_1 = Dense(in_features=3, out_features=3) self.dense_2 = Dense(in_features=3, out_features=2) @tf.function def __call__(self, x): x = self.dense_1(x) return self.dense_2(x) # You have made a model with a graph! my_model = MySequentialModule(name=""the_model"")\n\nThe module you have made works exactly the same as before. Each unique signature passed into the function creates a separate graph. Check the Introduction to graphs and functions guide for details.\n\nprint(my_model([[2.0, 2.0, 2.0]])) print(my_model([[[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]))\n\ntf.Tensor([[0.31593648 0. ]], shape=(1, 2), dtype=float32) tf.Tensor( [[[0.31593648 0. ] [0.31593648 0. ]]], shape=(1, 2, 2), dtype=float32)\n\nYou can visualize the graph by tracing it within a TensorBoard summary.\n\n# Set up logging. stamp = datetime.now().strftime(""%Y%m%d-%H%M%S"") logdir = ""logs/func/%s"" % stamp writer = tf.summary.create_file_writer(logdir) # Create a new model to get a fresh trace # Otherwise the summary will not see the graph. new_model = MySequentialModule() # Bracket the function call with # tf.summary.trace_on() and tf.summary.trace_export(). tf.summary.trace_on(graph=True) tf.profiler.experimental.start(logdir) # Call only one tf.function when tracing. z = print(new_model(tf.constant([[2.0, 2.0, 2.0]]))) with writer.as_default(): tf.summary.trace_export( name=""my_func_trace"", step=0, profiler_outdir=logdir)\n\ntf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)\n\nLaunch TensorBoard to view the resulting trace:\n\n#docs_infra: no_execute %tensorboard --logdir logs/func\n\nCreating a SavedModel\n\nThe recommended way of sharing completely trained models is to use SavedModel. SavedModel contains both a collection of functions and a collection of weights.\n\nYou can save the model you have just trained as follows:\n\ntf.saved_model.save(my_model, ""the_saved_model"")\n\nINFO:tensorflow:Assets written to: the_saved_model/assets\n\n# Inspect the SavedModel in the directory ls -l the_saved_model\n\ntotal 32 drwxr-sr-x 2 kbuilder kokoro 4096 Oct 18 01:21 assets -rw-rw-r-- 1 kbuilder kokoro 58 Oct 18 01:21 fingerprint.pb -rw-rw-r-- 1 kbuilder kokoro 17704 Oct 18 01:21 saved_model.pb drwxr-sr-x 2 kbuilder kokoro 4096 Oct 18 01:21 variables\n\n# The variables/ directory contains a checkpoint of the variables ls -l the_saved_model/variables\n\ntotal 8 -rw-rw-r-- 1 kbuilder kokoro 490 Oct 18 01:21 variables.data-00000-of-00001 -rw-rw-r-- 1 kbuilder kokoro 356 Oct 18 01:21 variables.index\n\nThe saved_model.pb file is a protocol buffer describing the functional tf.Graph.\n\nModels and layers can be loaded from this representation without actually making an instance of the class that created it. This is desired in situations where you do not have (or want) a Python interpreter, such as serving at scale or on an edge device, or in situations where the original Python code is not available or practical to use.\n\nYou can load the model as new object:\n\nnew_model = tf.saved_model.load(""the_saved_model"")\n\nnew_model, created from loading a saved model, is an internal TensorFlow user object without any of the class knowledge. It is not of type SequentialModule.\n\nisinstance(new_model, SequentialModule)\n\nThis new model works on the already-defined input signatures. You can\'t add more signatures to a model restored like this.\n\nprint(my_model([[2.0, 2.0, 2.0]])) print(my_model([[[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]))\n\ntf.Tensor([[0.31593648 0. ]], shape=(1, 2), dtype=float32) tf.Tensor( [[[0.31593648 0. ] [0.31593648 0. ]]], shape=(1, 2, 2), dtype=float32)\n\nThus, using SavedModel, you are able to save TensorFlow weights and graphs using tf.Module, and then load them again.\n\nKeras models and layers\n\nNote that up until this point, there is no mention of Keras. You can build your own high-level API on top of tf.Module, and people have.\n\nIn this section, you will examine how Keras uses tf.Module. A complete user guide to Keras models can be found in the Keras guide.\n\nKeras layers and models have a lot more extra features including:\n\nBuilt-in support for an optional training argument to differentiate between training and inference use\n\nSaving and restoring python objects instead of just black-box functions\n\nget_config and from_config methods that allow you to accurately store configurations to allow model cloning in Python\n\nThese features allow for far more complex models through subclassing, such as a custom GAN or a Variational AutoEncoder (VAE) model. Read about them in the full guide to custom layers and models.\n\nKeras models also come with extra functionality that makes them easy to train, evaluate, load, save, and even train on multiple machines.\n\ntf.keras.layers.Layer is the base class of all Keras layers, and it inherits from tf.Module.\n\nYou can convert a module into a Keras layer just by swapping out the parent and then changing __call__ to call:\n\nclass MyDense(tf.keras.layers.Layer): # Adding **kwargs to support base Keras layer arguments def __init__(self, in_features, out_features, **kwargs): super().__init__(**kwargs) # This will soon move to the build step; see below self.w = tf.Variable( tf.random.normal([in_features, out_features]), name=\'w\') self.b = tf.Variable(tf.zeros([out_features]), name=\'b\') def call(self, x): y = tf.matmul(x, self.w) + self.b return tf.nn.relu(y) simple_layer = MyDense(name=""simple"", in_features=3, out_features=3)\n\nKeras layers have their own __call__ that does some bookkeeping described in the next section and then calls call(). You should notice no change in functionality.\n\nsimple_layer([[2.0, 2.0, 2.0]])\n\n<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1.1688161, 0. , 0. ]], dtype=float32)>\n\nAs noted, it\'s convenient in many cases to wait to create variables until you are sure of the input shape.\n\nKeras layers come with an extra lifecycle step that allows you more flexibility in how you define your layers. This is defined in the build function.\n\nbuild is called exactly once, and it is called with the shape of the input. It\'s usually used to create variables (weights).\n\nYou can rewrite MyDense layer above to be flexible to the size of its inputs:\n\nclass FlexibleDense(tf.keras.layers.Layer): # Note the added `**kwargs`, as Keras supports many arguments def __init__(self, out_features, **kwargs): super().__init__(**kwargs) self.out_features = out_features def build(self, input_shape): # Create the state of the layer (weights) self.w = tf.Variable( tf.random.normal([input_shape[-1], self.out_features]), name=\'w\') self.b = tf.Variable(tf.zeros([self.out_features]), name=\'b\') def call(self, inputs): # Defines the computation from inputs to outputs return tf.matmul(inputs, self.w) + self.b # Create the instance of the layer flexible_dense = FlexibleDense(out_features=3)\n\nAt this point, the model has not been built, so there are no variables:\n\nflexible_dense.variables\n\nCalling the function allocates appropriately-sized variables:\n\n# Call it, with predictably random results print(""Model results:"", flexible_dense(tf.constant([[2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])))\n\nModel results: tf.Tensor( [[-2.531786 -5.5550847 -0.4248762] [-3.7976792 -8.332626 -0.6373143]], shape=(2, 3), dtype=float32)\n\nflexible_dense.variables\n\n[<tf.Variable \'flexible_dense/w:0\' shape=(3, 3) dtype=float32, numpy= array([[-0.77719826, -1.9281565 , 0.82326293], [ 0.85628736, -0.31845194, 0.10916236], [-1.3449821 , -0.5309338 , -1.1448634 ]], dtype=float32)>, <tf.Variable \'flexible_dense/b:0\' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n\nSince build is only called once, inputs will be rejected if the input shape is not compatible with the layer\'s variables:\n\ntry: print(""Model results:"", flexible_dense(tf.constant([[2.0, 2.0, 2.0, 2.0]]))) except tf.errors.InvalidArgumentError as e: print(""Failed:"", e)\n\nFailed: Exception encountered when calling layer \'flexible_dense\' (type FlexibleDense). { {function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0} } Matrix size-incompatible: In[0]: [1,4], In[1]: [3,3] [Op:MatMul] name: Call arguments received by layer \'flexible_dense\' (type FlexibleDense): • inputs=tf.Tensor(shape=(1, 4), dtype=float32)\n\nYou can define your model as nested Keras layers.\n\nHowever, Keras also provides a full-featured model class called tf.keras.Model. It inherits from tf.keras.layers.Layer, so a Keras model can be used and nested in the same way as Keras layers. Keras models come with extra functionality that makes them easy to train, evaluate, load, save, and even train on multiple machines.\n\nYou can define the SequentialModule from above with nearly identical code, again converting __call__ to call() and changing the parent:\n\n@keras.saving.register_keras_serializable() class MySequentialModel(tf.keras.Model): def __init__(self, name=None, **kwargs): super().__init__(**kwargs) self.dense_1 = FlexibleDense(out_features=3) self.dense_2 = FlexibleDense(out_features=2) def call(self, x): x = self.dense_1(x) return self.dense_2(x) # You have made a Keras model! my_sequential_model = MySequentialModel(name=""the_model"") # Call it on a tensor, with random results print(""Model results:"", my_sequential_model(tf.constant([[2.0, 2.0, 2.0]])))\n\nModel results: tf.Tensor([[ 0.26034355 16.431221 ]], shape=(1, 2), dtype=float32)\n\nAll the same features are available, including tracking variables and submodules.\n\nNote: A raw tf.Module nested inside a Keras layer or model will not get its variables collected for training or saving. Instead, nest Keras layers inside of Keras layers.\n\nmy_sequential_model.variables\n\n[<tf.Variable \'my_sequential_model/flexible_dense_1/w:0\' shape=(3, 3) dtype=float32, numpy= array([[ 1.4749854 , 0.16090827, 2.2669017 ], [ 1.6850946 , 1.1545411 , 0.1707306 ], [ 0.8753734 , -0.13549292, 0.08751986]], dtype=float32)>, <tf.Variable \'my_sequential_model/flexible_dense_1/b:0\' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Variable \'my_sequential_model/flexible_dense_2/w:0\' shape=(3, 2) dtype=float32, numpy= array([[-0.8022977 , 1.9773549 ], [-0.76657015, -0.8485579 ], [ 1.6919082 , 0.49000967]], dtype=float32)>, <tf.Variable \'my_sequential_model/flexible_dense_2/b:0\' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n\nmy_sequential_model.submodules\n\n(<__main__.FlexibleDense at 0x7f790c7e0e80>, <__main__.FlexibleDense at 0x7f790c7e6940>)\n\nOverriding tf.keras.Model is a very Pythonic approach to building TensorFlow models. If you are migrating models from other frameworks, this can be very straightforward.\n\nIf you are constructing models that are simple assemblages of existing layers and inputs, you can save time and space by using the functional API, which comes with additional features around model reconstruction and architecture.\n\nHere is the same model with the functional API:\n\ninputs = tf.keras.Input(shape=[3,]) x = FlexibleDense(3)(inputs) x = FlexibleDense(2)(x) my_functional_model = tf.keras.Model(inputs=inputs, outputs=x) my_functional_model.summary()\n\nModel: ""model"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 3)] 0 flexible_dense_3 (Flexible (None, 3) 12 Dense) flexible_dense_4 (Flexible (None, 2) 8 Dense) ================================================================= Total params: 20 (80.00 Byte) Trainable params: 20 (80.00 Byte) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nmy_functional_model(tf.constant([[2.0, 2.0, 2.0]]))\n\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[3.4276495, 2.937252 ]], dtype=float32)>\n\nThe major difference here is that the input shape is specified up front as part of the functional construction process. The input_shape argument in this case does not have to be completely specified; you can leave some dimensions as None.\n\nNote: You do not need to specify input_shape or an InputLayer in a subclassed model; these arguments and layers will be ignored.\n\nKeras models have their own specialized zip archive saving format, marked by the .keras extension. When calling tf.keras.Model.save, add a .keras extension to the filename. For example:\n\nmy_sequential_model.save(""exname_of_file.keras"")\n\nJust as easily, they can be loaded back in:\n\nreconstructed_model = tf.keras.models.load_model(""exname_of_file.keras"")\n\nKeras zip archives — .keras files — also save metric, loss, and optimizer states.\n\nThis reconstructed model can be used and will produce the same result when called on the same data:\n\nreconstructed_model(tf.constant([[2.0, 2.0, 2.0]]))\n\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.26034355, 16.431221 ]], dtype=float32)>\n\nCheckpointing Keras models\n\nKeras models can also be checkpointed, and that will look the same as tf.Module.\n\nThere is more to know about saving and serialization of Keras models, including providing configuration methods for custom layers for feature support. Check out the guide to saving and serialization.\n\nIf you want to know more details about Keras, you can follow the existing Keras guides here.\n\nAnother example of a high-level API built on tf.module is Sonnet from DeepMind, which is covered on their site.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-03-23 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_3', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to modules, layers, and models\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nTo do machine learning in TensorFlow, you are likely to need to define, save, and restore a model.\n\nA model is, abstractly:\n\nA function that computes something on tensors (a forward pass)\n\nSome variables that can be updated in response to training\n\nIn this guide, you will go below the surface of Keras to see how TensorFlow models are defined. This looks at how TensorFlow collects variables and models, as well as how they are saved and restored.\n\nNote: If you instead want to immediately get started with Keras, please see the collection of Keras guides.\n\nimport tensorflow as tf import keras from datetime import datetime %load_ext tensorboard\n\n2023-10-18 01:21:05.536666: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2023-10-18 01:21:05.536712: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2023-10-18 01:21:05.536766: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nMost models are made of layers. Layers are functions with a known mathematical structure that can be reused and have trainable variables. In TensorFlow, most high-level implementations of layers and models, such as Keras or Sonnet, are built on the same foundational class: tf.Module.\n\nHere\'s an example of a very simple tf.Module that operates on a scalar tensor:\n\nclass SimpleModule(tf.Module): def __init__(self, name=None): super().__init__(name=name) self.a_variable = tf.Variable(5.0, name=""train_me"") self.non_trainable_variable = tf.Variable(5.0, trainable=False, name=""do_not_train_me"") def __call__(self, x): return self.a_variable * x + self.non_trainable_variable simple_module = SimpleModule(name=""simple"") simple_module(tf.constant(5.0))\n\n2023-10-18 01:21:08.181350: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... <tf.Tensor: shape=(), dtype=float32, numpy=30.0>\n\nModules and, by extension, layers are deep-learning terminology for ""objects"": they have internal state, and methods that use that state.\n\nThere is nothing special about __call__ except to act like a Python callable; you can invoke your models with whatever functions you wish.\n\nYou can set the trainability of variables on and off for any reason, including freezing layers and variables during fine-tuning.\n\nNote: tf.Module is the base class for both tf.keras.layers.Layer and tf.keras.Model, so everything you come across here also applies in Keras. For historical compatibility reasons Keras layers do not collect variables from modules, so your models should use only modules or only Keras layers. However, the methods shown below for inspecting variables are the same in either case.\n\nBy subclassing tf.Module, any tf.Variable or tf.Module instances assigned to this object\'s properties are automatically collected. This allows you to save and load variables, and also create collections of tf.Modules.\n\n# All trainable variables print(""trainable variables:"", simple_module.trainable_variables) # Every variable print(""all variables:"", simple_module.variables)\n\ntrainable variables: (<tf.Variable \'train_me:0\' shape=() dtype=float32, numpy=5.0>,) all variables: (<tf.Variable \'train_me:0\' shape=() dtype=float32, numpy=5.0>, <tf.Variable \'do_not_train_me:0\' shape=() dtype=float32, numpy=5.0>)\n\nThis is an example of a two-layer linear layer model made out of modules.\n\nFirst a dense (linear) layer:\n\nclass Dense(tf.Module): def __init__(self, in_features, out_features, name=None): super().__init__(name=name) self.w = tf.Variable( tf.random.normal([in_features, out_features]), name=\'w\') self.b = tf.Variable(tf.zeros([out_features]), name=\'b\') def __call__(self, x): y = tf.matmul(x, self.w) + self.b return tf.nn.relu(y)\n\nAnd then the complete model, which makes two layer instances and applies them:\n\nclass SequentialModule(tf.Module): def __init__(self, name=None): super().__init__(name=name) self.dense_1 = Dense(in_features=3, out_features=3) self.dense_2 = Dense(in_features=3, out_features=2) def __call__(self, x): x = self.dense_1(x) return self.dense_2(x) # You have made a model! my_model = SequentialModule(name=""the_model"") # Call it, with random results print(""Model results:"", my_model(tf.constant([[2.0, 2.0, 2.0]])))\n\nModel results: tf.Tensor([[0. 3.415034]], shape=(1, 2), dtype=float32)\n\ntf.Module instances will automatically collect, recursively, any tf.Variable or tf.Module instances assigned to it. This allows you to manage collections of tf.Modules with a single model instance, and save and load whole models.\n\nprint(""Submodules:"", my_model.submodules)\n\nSubmodules: (<__main__.Dense object at 0x7f7931aea250>, <__main__.Dense object at 0x7f77ed5b8a00>)\n\nfor var in my_model.variables: print(var, ""\\n"")\n\n<tf.Variable \'b:0\' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)> <tf.Variable \'w:0\' shape=(3, 3) dtype=float32, numpy= array([[-2.8161757, -2.6065955, 1.9061812], [-0.9430401, -0.4624743, -0.4531979], [-1.3428234, 0.7062293, 0.7874674]], dtype=float32)> <tf.Variable \'b:0\' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> <tf.Variable \'w:0\' shape=(3, 2) dtype=float32, numpy= array([[ 1.0474309 , -0.6920227 ], [ 1.2405277 , 0.36411622], [-1.6990206 , 0.762131 ]], dtype=float32)>\n\nWaiting to create variables\n\nYou may have noticed here that you have to define both input and output sizes to the layer. This is so the w variable has a known shape and can be allocated.\n\nBy deferring variable creation to the first time the module is called with a specific input shape, you do not need specify the input size up front.\n\nclass FlexibleDenseModule(tf.Module): # Note: No need for `in_features` def __init__(self, out_features, name=None): super().__init__(name=name) self.is_built = False self.out_features = out_features def __call__(self, x): # Create variables on first call. if not self.is_built: self.w = tf.Variable( tf.random.normal([x.shape[-1], self.out_features]), name=\'w\') self.b = tf.Variable(tf.zeros([self.out_features]), name=\'b\') self.is_built = True y = tf.matmul(x, self.w) + self.b return tf.nn.relu(y)\n\n# Used in a module class MySequentialModule(tf.Module): def __init__(self, name=None): super().__init__(name=name) self.dense_1 = FlexibleDenseModule(out_features=3) self.dense_2 = FlexibleDenseModule(out_features=2) def __call__(self, x): x = self.dense_1(x) return self.dense_2(x) my_model = MySequentialModule(name=""the_model"") print(""Model results:"", my_model(tf.constant([[2.0, 2.0, 2.0]])))\n\nModel results: tf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)\n\nThis flexibility is why TensorFlow layers often only need to specify the shape of their outputs, such as in tf.keras.layers.Dense, rather than both the input and output size.\n\nYou can save a tf.Module as both a checkpoint and a SavedModel.\n\nCheckpoints are just the weights (that is, the values of the set of variables inside the module and its submodules):\n\nchkp_path = ""my_checkpoint"" checkpoint = tf.train.Checkpoint(model=my_model) checkpoint.write(chkp_path)\n\nCheckpoints consist of two kinds of files: the data itself and an index file for metadata. The index file keeps track of what is actually saved and the numbering of checkpoints, while the checkpoint data contains the variable values and their attribute lookup paths.\n\nmy_checkpoint.data-00000-of-00001 my_checkpoint.index\n\nYou can look inside a checkpoint to be sure the whole collection of variables is saved, sorted by the Python object that contains them.\n\ntf.train.list_variables(chkp_path)\n\n[(\'_CHECKPOINTABLE_OBJECT_GRAPH\', []), (\'model/dense_1/b/.ATTRIBUTES/VARIABLE_VALUE\', [3]), (\'model/dense_1/w/.ATTRIBUTES/VARIABLE_VALUE\', [3, 3]), (\'model/dense_2/b/.ATTRIBUTES/VARIABLE_VALUE\', [2]), (\'model/dense_2/w/.ATTRIBUTES/VARIABLE_VALUE\', [3, 2])]\n\nDuring distributed (multi-machine) training they can be sharded, which is why they are numbered (e.g., \'00000-of-00001\'). In this case, though, there is only one shard.\n\nWhen you load models back in, you overwrite the values in your Python object.\n\nnew_model = MySequentialModule() new_checkpoint = tf.train.Checkpoint(model=new_model) new_checkpoint.restore(""my_checkpoint"") # Should be the same result as above new_model(tf.constant([[2.0, 2.0, 2.0]]))\n\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0., 0.]], dtype=float32)>\n\nNote: As checkpoints are at the heart of long training workflows tf.checkpoint.CheckpointManager is a helper class that makes checkpoint management much easier. Refer to the Training checkpoints guide for more details.\n\nTensorFlow can run models without the original Python objects, as demonstrated by TensorFlow Serving and TensorFlow Lite, even when you download a trained model from TensorFlow Hub.\n\nTensorFlow needs to know how to do the computations described in Python, but without the original code. To do this, you can make a graph, which is described in the Introduction to graphs and functions guide.\n\nThis graph contains operations, or ops, that implement the function.\n\nYou can define a graph in the model above by adding the @tf.function decorator to indicate that this code should run as a graph.\n\nclass MySequentialModule(tf.Module): def __init__(self, name=None): super().__init__(name=name) self.dense_1 = Dense(in_features=3, out_features=3) self.dense_2 = Dense(in_features=3, out_features=2) @tf.function def __call__(self, x): x = self.dense_1(x) return self.dense_2(x) # You have made a model with a graph! my_model = MySequentialModule(name=""the_model"")\n\nThe module you have made works exactly the same as before. Each unique signature passed into the function creates a separate graph. Check the Introduction to graphs and functions guide for details.\n\nprint(my_model([[2.0, 2.0, 2.0]])) print(my_model([[[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]))\n\ntf.Tensor([[0.31593648 0. ]], shape=(1, 2), dtype=float32) tf.Tensor( [[[0.31593648 0. ] [0.31593648 0. ]]], shape=(1, 2, 2), dtype=float32)\n\nYou can visualize the graph by tracing it within a TensorBoard summary.\n\n# Set up logging. stamp = datetime.now().strftime(""%Y%m%d-%H%M%S"") logdir = ""logs/func/%s"" % stamp writer = tf.summary.create_file_writer(logdir) # Create a new model to get a fresh trace # Otherwise the summary will not see the graph. new_model = MySequentialModule() # Bracket the function call with # tf.summary.trace_on() and tf.summary.trace_export(). tf.summary.trace_on(graph=True) tf.profiler.experimental.start(logdir) # Call only one tf.function when tracing. z = print(new_model(tf.constant([[2.0, 2.0, 2.0]]))) with writer.as_default(): tf.summary.trace_export( name=""my_func_trace"", step=0, profiler_outdir=logdir)\n\ntf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)\n\nLaunch TensorBoard to view the resulting trace:\n\n#docs_infra: no_execute %tensorboard --logdir logs/func\n\nCreating a SavedModel\n\nThe recommended way of sharing completely trained models is to use SavedModel. SavedModel contains both a collection of functions and a collection of weights.\n\nYou can save the model you have just trained as follows:\n\ntf.saved_model.save(my_model, ""the_saved_model"")\n\nINFO:tensorflow:Assets written to: the_saved_model/assets\n\n# Inspect the SavedModel in the directory ls -l the_saved_model\n\ntotal 32 drwxr-sr-x 2 kbuilder kokoro 4096 Oct 18 01:21 assets -rw-rw-r-- 1 kbuilder kokoro 58 Oct 18 01:21 fingerprint.pb -rw-rw-r-- 1 kbuilder kokoro 17704 Oct 18 01:21 saved_model.pb drwxr-sr-x 2 kbuilder kokoro 4096 Oct 18 01:21 variables\n\n# The variables/ directory contains a checkpoint of the variables ls -l the_saved_model/variables\n\ntotal 8 -rw-rw-r-- 1 kbuilder kokoro 490 Oct 18 01:21 variables.data-00000-of-00001 -rw-rw-r-- 1 kbuilder kokoro 356 Oct 18 01:21 variables.index\n\nThe saved_model.pb file is a protocol buffer describing the functional tf.Graph.\n\nModels and layers can be loaded from this representation without actually making an instance of the class that created it. This is desired in situations where you do not have (or want) a Python interpreter, such as serving at scale or on an edge device, or in situations where the original Python code is not available or practical to use.\n\nYou can load the model as new object:\n\nnew_model = tf.saved_model.load(""the_saved_model"")\n\nnew_model, created from loading a saved model, is an internal TensorFlow user object without any of the class knowledge. It is not of type SequentialModule.\n\nisinstance(new_model, SequentialModule)\n\nThis new model works on the already-defined input signatures. You can\'t add more signatures to a model restored like this.\n\nprint(my_model([[2.0, 2.0, 2.0]])) print(my_model([[[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]]))\n\ntf.Tensor([[0.31593648 0. ]], shape=(1, 2), dtype=float32) tf.Tensor( [[[0.31593648 0. ] [0.31593648 0. ]]], shape=(1, 2, 2), dtype=float32)\n\nThus, using SavedModel, you are able to save TensorFlow weights and graphs using tf.Module, and then load them again.\n\nKeras models and layers\n\nNote that up until this point, there is no mention of Keras. You can build your own high-level API on top of tf.Module, and people have.\n\nIn this section, you will examine how Keras uses tf.Module. A complete user guide to Keras models can be found in the Keras guide.\n\nKeras layers and models have a lot more extra features including:\n\nBuilt-in support for an optional training argument to differentiate between training and inference use\n\nSaving and restoring python objects instead of just black-box functions\n\nget_config and from_config methods that allow you to accurately store configurations to allow model cloning in Python\n\nThese features allow for far more complex models through subclassing, such as a custom GAN or a Variational AutoEncoder (VAE) model. Read about them in the full guide to custom layers and models.\n\nKeras models also come with extra functionality that makes them easy to train, evaluate, load, save, and even train on multiple machines.\n\ntf.keras.layers.Layer is the base class of all Keras layers, and it inherits from tf.Module.\n\nYou can convert a module into a Keras layer just by swapping out the parent and then changing __call__ to call:\n\nclass MyDense(tf.keras.layers.Layer): # Adding **kwargs to support base Keras layer arguments def __init__(self, in_features, out_features, **kwargs): super().__init__(**kwargs) # This will soon move to the build step; see below self.w = tf.Variable( tf.random.normal([in_features, out_features]), name=\'w\') self.b = tf.Variable(tf.zeros([out_features]), name=\'b\') def call(self, x): y = tf.matmul(x, self.w) + self.b return tf.nn.relu(y) simple_layer = MyDense(name=""simple"", in_features=3, out_features=3)\n\nKeras layers have their own __call__ that does some bookkeeping described in the next section and then calls call(). You should notice no change in functionality.\n\nsimple_layer([[2.0, 2.0, 2.0]])\n\n<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1.1688161, 0. , 0. ]], dtype=float32)>\n\nAs noted, it\'s convenient in many cases to wait to create variables until you are sure of the input shape.\n\nKeras layers come with an extra lifecycle step that allows you more flexibility in how you define your layers. This is defined in the build function.\n\nbuild is called exactly once, and it is called with the shape of the input. It\'s usually used to create variables (weights).\n\nYou can rewrite MyDense layer above to be flexible to the size of its inputs:\n\nclass FlexibleDense(tf.keras.layers.Layer): # Note the added `**kwargs`, as Keras supports many arguments def __init__(self, out_features, **kwargs): super().__init__(**kwargs) self.out_features = out_features def build(self, input_shape): # Create the state of the layer (weights) self.w = tf.Variable( tf.random.normal([input_shape[-1], self.out_features]), name=\'w\') self.b = tf.Variable(tf.zeros([self.out_features]), name=\'b\') def call(self, inputs): # Defines the computation from inputs to outputs return tf.matmul(inputs, self.w) + self.b # Create the instance of the layer flexible_dense = FlexibleDense(out_features=3)\n\nAt this point, the model has not been built, so there are no variables:\n\nflexible_dense.variables\n\nCalling the function allocates appropriately-sized variables:\n\n# Call it, with predictably random results print(""Model results:"", flexible_dense(tf.constant([[2.0, 2.0, 2.0], [3.0, 3.0, 3.0]])))\n\nModel results: tf.Tensor( [[-2.531786 -5.5550847 -0.4248762] [-3.7976792 -8.332626 -0.6373143]], shape=(2, 3), dtype=float32)\n\nflexible_dense.variables\n\n[<tf.Variable \'flexible_dense/w:0\' shape=(3, 3) dtype=float32, numpy= array([[-0.77719826, -1.9281565 , 0.82326293], [ 0.85628736, -0.31845194, 0.10916236], [-1.3449821 , -0.5309338 , -1.1448634 ]], dtype=float32)>, <tf.Variable \'flexible_dense/b:0\' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n\nSince build is only called once, inputs will be rejected if the input shape is not compatible with the layer\'s variables:\n\ntry: print(""Model results:"", flexible_dense(tf.constant([[2.0, 2.0, 2.0, 2.0]]))) except tf.errors.InvalidArgumentError as e: print(""Failed:"", e)\n\nFailed: Exception encountered when calling layer \'flexible_dense\' (type FlexibleDense). { {function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0} } Matrix size-incompatible: In[0]: [1,4], In[1]: [3,3] [Op:MatMul] name: Call arguments received by layer \'flexible_dense\' (type FlexibleDense): • inputs=tf.Tensor(shape=(1, 4), dtype=float32)\n\nYou can define your model as nested Keras layers.\n\nHowever, Keras also provides a full-featured model class called tf.keras.Model. It inherits from tf.keras.layers.Layer, so a Keras model can be used and nested in the same way as Keras layers. Keras models come with extra functionality that makes them easy to train, evaluate, load, save, and even train on multiple machines.\n\nYou can define the SequentialModule from above with nearly identical code, again converting __call__ to call() and changing the parent:\n\n@keras.saving.register_keras_serializable() class MySequentialModel(tf.keras.Model): def __init__(self, name=None, **kwargs): super().__init__(**kwargs) self.dense_1 = FlexibleDense(out_features=3) self.dense_2 = FlexibleDense(out_features=2) def call(self, x): x = self.dense_1(x) return self.dense_2(x) # You have made a Keras model! my_sequential_model = MySequentialModel(name=""the_model"") # Call it on a tensor, with random results print(""Model results:"", my_sequential_model(tf.constant([[2.0, 2.0, 2.0]])))\n\nModel results: tf.Tensor([[ 0.26034355 16.431221 ]], shape=(1, 2), dtype=float32)\n\nAll the same features are available, including tracking variables and submodules.\n\nNote: A raw tf.Module nested inside a Keras layer or model will not get its variables collected for training or saving. Instead, nest Keras layers inside of Keras layers.\n\nmy_sequential_model.variables\n\n[<tf.Variable \'my_sequential_model/flexible_dense_1/w:0\' shape=(3, 3) dtype=float32, numpy= array([[ 1.4749854 , 0.16090827, 2.2669017 ], [ 1.6850946 , 1.1545411 , 0.1707306 ], [ 0.8753734 , -0.13549292, 0.08751986]], dtype=float32)>, <tf.Variable \'my_sequential_model/flexible_dense_1/b:0\' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>, <tf.Variable \'my_sequential_model/flexible_dense_2/w:0\' shape=(3, 2) dtype=float32, numpy= array([[-0.8022977 , 1.9773549 ], [-0.76657015, -0.8485579 ], [ 1.6919082 , 0.49000967]], dtype=float32)>, <tf.Variable \'my_sequential_model/flexible_dense_2/b:0\' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n\nmy_sequential_model.submodules\n\n(<__main__.FlexibleDense at 0x7f790c7e0e80>, <__main__.FlexibleDense at 0x7f790c7e6940>)\n\nOverriding tf.keras.Model is a very Pythonic approach to building TensorFlow models. If you are migrating models from other frameworks, this can be very straightforward.\n\nIf you are constructing models that are simple assemblages of existing layers and inputs, you can save time and space by using the functional API, which comes with additional features around model reconstruction and architecture.\n\nHere is the same model with the functional API:\n\ninputs = tf.keras.Input(shape=[3,]) x = FlexibleDense(3)(inputs) x = FlexibleDense(2)(x) my_functional_model = tf.keras.Model(inputs=inputs, outputs=x) my_functional_model.summary()\n\nModel: ""model"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 3)] 0 flexible_dense_3 (Flexible (None, 3) 12 Dense) flexible_dense_4 (Flexible (None, 2) 8 Dense) ================================================================= Total params: 20 (80.00 Byte) Trainable params: 20 (80.00 Byte) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nmy_functional_model(tf.constant([[2.0, 2.0, 2.0]]))\n\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[3.4276495, 2.937252 ]], dtype=float32)>\n\nThe major difference here is that the input shape is specified up front as part of the functional construction process. The input_shape argument in this case does not have to be completely specified; you can leave some dimensions as None.\n\nNote: You do not need to specify input_shape or an InputLayer in a subclassed model; these arguments and layers will be ignored.\n\nKeras models have their own specialized zip archive saving format, marked by the .keras extension. When calling tf.keras.Model.save, add a .keras extension to the filename. For example:\n\nmy_sequential_model.save(""exname_of_file.keras"")\n\nJust as easily, they can be loaded back in:\n\nreconstructed_model = tf.keras.models.load_model(""exname_of_file.keras"")\n\nKeras zip archives — .keras files — also save metric, loss, and optimizer states.\n\nThis reconstructed model can be used and will produce the same result when called on the same data:\n\nreconstructed_model(tf.constant([[2.0, 2.0, 2.0]]))\n\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.26034355, 16.431221 ]], dtype=float32)>\n\nCheckpointing Keras models\n\nKeras models can also be checkpointed, and that will look the same as tf.Module.\n\nThere is more to know about saving and serialization of Keras models, including providing configuration methods for custom layers for feature support. Check out the guide to saving and serialization.\n\nIf you want to know more details about Keras, you can follow the existing Keras guides here.\n\nAnother example of a high-level API built on tf.module is Sonnet from DeepMind, which is covered on their site.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-03-23 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-06-26T08:11:06', 'title': 'Introduction to modules, layers, and models | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/intro_to_modules'})]]??"
68984841,tf.keras.layers.Dense,"{'https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187', 'https://www.udemy.com/course/tensorflow-2/', 'https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/', 'https://www.udemy.com/course/tensorflow-2-practical-advanced/', 'https://www.udemy.com/course/deep-learning-by-tensorflow-tfkeras-keras-using-python/', 'https://www.udacity.com/course/intro-to-machine-learning-with-tensorflow-nanodegree--nd230', 'https://www.udemy.com/course/deep-learning-tensorflow-2/', 'https://www.edx.org/learn/keras', 'https://www.coursera.org/learn/custom-models-layers-loss-functions-with-tensorflow'}","{'https://www.youtube.com/watch?v=oJ1i2c1KxKk', 'https://www.youtube.com/watch?v=lor2LnEVn8M', 'https://www.youtube.com/watch?v=oXMEeGrAuk0'}","{'https://stackoverflow.com/questions/60783216/what-exactly-does-tf-keras-layers-dense-do', 'https://stackoverflow.com/questions/68984841/how-can-i-understand-the-kernel-of-tf-keras-layers-dense-for-rank-2'}","??[[Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCalling all who code. Take the 2023 Developer Survey.\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2?\n\nAsked 1 year, 8 months ago\n\nModified 1 year, 8 months ago\n\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2?\n\nThe official API doc states that:\n\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 0 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n\nMy understanding is that for a rank larger than 2 (for example rank 3) only one kernel is created and thus the same kernel is applied on all slices of the second dimension, like above. That would consequently mean that the outputs for different indices of the second dimension are not independent of each other (especially during training).\n\nIs my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?\n\nImprove this question\n\nedited Sep 2, 2021 at 1:16\n\n10.9k2020 gold badges4242 silver badges7070 bronze badges\n\nasked Aug 30, 2021 at 13:16\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYes, your understanding is correct.\n\nTo achieve what you want, you need to define a custom keras layer. Let\'s suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original Dense layer (link: github), except that\n\nIn the build function, the shape of self.kernel is (d0, i0, units) instead. You can get the value of d0 as well as i0 from input_shape.\n\nIn the call function, to do the specified tensor multiplication between inputs and self.kernel, use tf.einsum with this equation: tf.einsum(\'abc,bcg->abg\', inputs, self.kernel)\n\nanswered Aug 31, 2021 at 15:15\n\nLaplace RickyLaplace Ricky\n\n1,52088 silver badges66 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nStories from our survey: Salary in the time of pandemic\n\nGreat code isn’t enough. Developers need to brag about it (Ep. 571)\n\nNew blog post from our CEO Prashanth: Community is the future of AI\n\nWe are updating our Code of Conduct and we would like your feedback\n\nTemporary policy: ChatGPT is banned\n\nThe [connect] tag is being burninated\n\nStack Overflow will be testing a title-drafting assistant, and we’d like your...\n\nWe are graduating the ""Related questions using Machine Learning"" experiment\n\n5500 How do I create a directory, and any missing parent directories?\n\n3063 How do I print colored text to the terminal?\n\n3135 How do I change the size of figures drawn with Matplotlib?\n\n3744 How do I get the current time?\n\n5139 Accessing the index in \'for\' loops\n\n1551 How can I check for NaN values?\n\n3461 How can I add new keys to a dictionary?\n\n3086 How can I access environment variables in Python?\n\n3220 How can I delete a file or folder in Python?\n\n2787 How can I remove a key from a Python dictionary?\n\nHot Network Questions\n\nGuitar amp is making bad noise when I plug in a jack only\n\nMilitary space short story, 1960s to early 1980s, that appeared in an anthology pre-1984\n\nSpectroscopy: Yellow vs Magenta\n\nCompute cumulative means efficiently\n\nHow to remove all files with a pattern except the recently created file\n\nRadius of the 45 degree elbow piece\n\nIs there a maximum time one can hold postdoc positions between the PhD and the more permanent positions in Mathematics?\n\nBinary Elliptic Curves Point Doubling Formula - Calculate Lambda from P3\n\nAs a novice, how can I improve at defending my pieces other than what I\'m already doing?\n\nWhy couldn\'t the dwarves beat/kill the Balrog?\n\nA strange construction of locks\n\nHow to solve this trigonometric function equation?\n\nCosmic circles and the celestial key\n\nIs the Big Bang a theory or a model?\n\nHow can I alias `...` to `../..` in Bash?\n\nGreen Triangles Puzzle\n\nFinding a minimum cut with an upper bound on the set sizes\n\nIs it a Contactor or is it a Relay?\n\nBorders of a Rectangular Matrix\n\nHow to load font for emacs on Arch Linux?\n\nWhat does the dot product of two vectors actually represent intuitively? What is its true meaning conceptually?\n\nWhat is a good second hand option for someone who doesn\'t use shields? And probably uses most bonus actions?\n\nA numerical matrix of power sum polynomials\n\nBeginner friendly pocket cuts more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', metadata={'id': 'web-search_0', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCalling all who code. Take the 2023 Developer Survey.\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2?\n\nAsked 1 year, 8 months ago\n\nModified 1 year, 8 months ago\n\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2?\n\nThe official API doc states that:\n\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 0 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n\nMy understanding is that for a rank larger than 2 (for example rank 3) only one kernel is created and thus the same kernel is applied on all slices of the second dimension, like above. That would consequently mean that the outputs for different indices of the second dimension are not independent of each other (especially during training).\n\nIs my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?\n\nImprove this question\n\nedited Sep 2, 2021 at 1:16\n\n10.9k2020 gold badges4242 silver badges7070 bronze badges\n\nasked Aug 30, 2021 at 13:16\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYes, your understanding is correct.\n\nTo achieve what you want, you need to define a custom keras layer. Let\'s suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original Dense layer (link: github), except that\n\nIn the build function, the shape of self.kernel is (d0, i0, units) instead. You can get the value of d0 as well as i0 from input_shape.\n\nIn the call function, to do the specified tensor multiplication between inputs and self.kernel, use tf.einsum with this equation: tf.einsum(\'abc,bcg->abg\', inputs, self.kernel)\n\nanswered Aug 31, 2021 at 15:15\n\nLaplace RickyLaplace Ricky\n\n1,52088 silver badges66 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nStories from our survey: Salary in the time of pandemic\n\nGreat code isn’t enough. Developers need to brag about it (Ep. 571)\n\nNew blog post from our CEO Prashanth: Community is the future of AI\n\nWe are updating our Code of Conduct and we would like your feedback\n\nTemporary policy: ChatGPT is banned\n\nThe [connect] tag is being burninated\n\nStack Overflow will be testing a title-drafting assistant, and we’d like your...\n\nWe are graduating the ""Related questions using Machine Learning"" experiment\n\n5500 How do I create a directory, and any missing parent directories?\n\n3063 How do I print colored text to the terminal?\n\n3135 How do I change the size of figures drawn with Matplotlib?\n\n3744 How do I get the current time?\n\n5139 Accessing the index in \'for\' loops\n\n1551 How can I check for NaN values?\n\n3461 How can I add new keys to a dictionary?\n\n3086 How can I access environment variables in Python?\n\n3220 How can I delete a file or folder in Python?\n\n2787 How can I remove a key from a Python dictionary?\n\nHot Network Questions\n\nGuitar amp is making bad noise when I plug in a jack only\n\nMilitary space short story, 1960s to early 1980s, that appeared in an anthology pre-1984\n\nSpectroscopy: Yellow vs Magenta\n\nCompute cumulative means efficiently\n\nHow to remove all files with a pattern except the recently created file\n\nRadius of the 45 degree elbow piece\n\nIs there a maximum time one can hold postdoc positions between the PhD and the more permanent positions in Mathematics?\n\nBinary Elliptic Curves Point Doubling Formula - Calculate Lambda from P3\n\nAs a novice, how can I improve at defending my pieces other than what I\'m already doing?\n\nWhy couldn\'t the dwarves beat/kill the Balrog?\n\nA strange construction of locks\n\nHow to solve this trigonometric function equation?\n\nCosmic circles and the celestial key\n\nIs the Big Bang a theory or a model?\n\nHow can I alias `...` to `../..` in Bash?\n\nGreen Triangles Puzzle\n\nFinding a minimum cut with an upper bound on the set sizes\n\nIs it a Contactor or is it a Relay?\n\nBorders of a Rectangular Matrix\n\nHow to load font for emacs on Arch Linux?\n\nWhat does the dot product of two vectors actually represent intuitively? What is its true meaning conceptually?\n\nWhat is a good second hand option for someone who doesn\'t use shields? And probably uses most bonus actions?\n\nA numerical matrix of power sum polynomials\n\nBeginner friendly pocket cuts more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', 'timestamp': '2024-02-15T16:48:37', 'title': 'python - How can I understand the kernel of tf.keras.layers.Dense for rank >2? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/68984841/how-can-i-understand-the-kernel-of-tf-keras-layers-dense-for-rank-2'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCalling all who code. Take the 2023 Developer Survey.\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2?\n\nAsked 1 year, 8 months ago\n\nModified 1 year, 8 months ago\n\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2?\n\nThe official API doc states that:\n\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 0 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n\nMy understanding is that for a rank larger than 2 (for example rank 3) only one kernel is created and thus the same kernel is applied on all slices of the second dimension, like above. That would consequently mean that the outputs for different indices of the second dimension are not independent of each other (especially during training).\n\nIs my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?\n\nImprove this question\n\nedited Sep 2, 2021 at 1:16\n\n10.9k2020 gold badges4242 silver badges7070 bronze badges\n\nasked Aug 30, 2021 at 13:16\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYes, your understanding is correct.\n\nTo achieve what you want, you need to define a custom keras layer. Let\'s suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original Dense layer (link: github), except that\n\nIn the build function, the shape of self.kernel is (d0, i0, units) instead. You can get the value of d0 as well as i0 from input_shape.\n\nIn the call function, to do the specified tensor multiplication between inputs and self.kernel, use tf.einsum with this equation: tf.einsum(\'abc,bcg->abg\', inputs, self.kernel)\n\nanswered Aug 31, 2021 at 15:15\n\nLaplace RickyLaplace Ricky\n\n1,52088 silver badges66 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nStories from our survey: Salary in the time of pandemic\n\nGreat code isn’t enough. Developers need to brag about it (Ep. 571)\n\nNew blog post from our CEO Prashanth: Community is the future of AI\n\nWe are updating our Code of Conduct and we would like your feedback\n\nTemporary policy: ChatGPT is banned\n\nThe [connect] tag is being burninated\n\nStack Overflow will be testing a title-drafting assistant, and we’d like your...\n\nWe are graduating the ""Related questions using Machine Learning"" experiment\n\n5500 How do I create a directory, and any missing parent directories?\n\n3063 How do I print colored text to the terminal?\n\n3135 How do I change the size of figures drawn with Matplotlib?\n\n3744 How do I get the current time?\n\n5139 Accessing the index in \'for\' loops\n\n1551 How can I check for NaN values?\n\n3461 How can I add new keys to a dictionary?\n\n3086 How can I access environment variables in Python?\n\n3220 How can I delete a file or folder in Python?\n\n2787 How can I remove a key from a Python dictionary?\n\nHot Network Questions\n\nGuitar amp is making bad noise when I plug in a jack only\n\nMilitary space short story, 1960s to early 1980s, that appeared in an anthology pre-1984\n\nSpectroscopy: Yellow vs Magenta\n\nCompute cumulative means efficiently\n\nHow to remove all files with a pattern except the recently created file\n\nRadius of the 45 degree elbow piece\n\nIs there a maximum time one can hold postdoc positions between the PhD and the more permanent positions in Mathematics?\n\nBinary Elliptic Curves Point Doubling Formula - Calculate Lambda from P3\n\nAs a novice, how can I improve at defending my pieces other than what I\'m already doing?\n\nWhy couldn\'t the dwarves beat/kill the Balrog?\n\nA strange construction of locks\n\nHow to solve this trigonometric function equation?\n\nCosmic circles and the celestial key\n\nIs the Big Bang a theory or a model?\n\nHow can I alias `...` to `../..` in Bash?\n\nGreen Triangles Puzzle\n\nFinding a minimum cut with an upper bound on the set sizes\n\nIs it a Contactor or is it a Relay?\n\nBorders of a Rectangular Matrix\n\nHow to load font for emacs on Arch Linux?\n\nWhat does the dot product of two vectors actually represent intuitively? What is its true meaning conceptually?\n\nWhat is a good second hand option for someone who doesn\'t use shields? And probably uses most bonus actions?\n\nA numerical matrix of power sum polynomials\n\nBeginner friendly pocket cuts more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', metadata={'id': 'web-search_3', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCalling all who code. Take the 2023 Developer Survey.\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2?\n\nAsked 1 year, 8 months ago\n\nModified 1 year, 8 months ago\n\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2?\n\nThe official API doc states that:\n\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 0 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n\nMy understanding is that for a rank larger than 2 (for example rank 3) only one kernel is created and thus the same kernel is applied on all slices of the second dimension, like above. That would consequently mean that the outputs for different indices of the second dimension are not independent of each other (especially during training).\n\nIs my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?\n\nImprove this question\n\nedited Sep 2, 2021 at 1:16\n\n10.9k2020 gold badges4242 silver badges7070 bronze badges\n\nasked Aug 30, 2021 at 13:16\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYes, your understanding is correct.\n\nTo achieve what you want, you need to define a custom keras layer. Let\'s suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original Dense layer (link: github), except that\n\nIn the build function, the shape of self.kernel is (d0, i0, units) instead. You can get the value of d0 as well as i0 from input_shape.\n\nIn the call function, to do the specified tensor multiplication between inputs and self.kernel, use tf.einsum with this equation: tf.einsum(\'abc,bcg->abg\', inputs, self.kernel)\n\nanswered Aug 31, 2021 at 15:15\n\nLaplace RickyLaplace Ricky\n\n1,52088 silver badges66 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nStories from our survey: Salary in the time of pandemic\n\nGreat code isn’t enough. Developers need to brag about it (Ep. 571)\n\nNew blog post from our CEO Prashanth: Community is the future of AI\n\nWe are updating our Code of Conduct and we would like your feedback\n\nTemporary policy: ChatGPT is banned\n\nThe [connect] tag is being burninated\n\nStack Overflow will be testing a title-drafting assistant, and we’d like your...\n\nWe are graduating the ""Related questions using Machine Learning"" experiment\n\n5500 How do I create a directory, and any missing parent directories?\n\n3063 How do I print colored text to the terminal?\n\n3135 How do I change the size of figures drawn with Matplotlib?\n\n3744 How do I get the current time?\n\n5139 Accessing the index in \'for\' loops\n\n1551 How can I check for NaN values?\n\n3461 How can I add new keys to a dictionary?\n\n3086 How can I access environment variables in Python?\n\n3220 How can I delete a file or folder in Python?\n\n2787 How can I remove a key from a Python dictionary?\n\nHot Network Questions\n\nGuitar amp is making bad noise when I plug in a jack only\n\nMilitary space short story, 1960s to early 1980s, that appeared in an anthology pre-1984\n\nSpectroscopy: Yellow vs Magenta\n\nCompute cumulative means efficiently\n\nHow to remove all files with a pattern except the recently created file\n\nRadius of the 45 degree elbow piece\n\nIs there a maximum time one can hold postdoc positions between the PhD and the more permanent positions in Mathematics?\n\nBinary Elliptic Curves Point Doubling Formula - Calculate Lambda from P3\n\nAs a novice, how can I improve at defending my pieces other than what I\'m already doing?\n\nWhy couldn\'t the dwarves beat/kill the Balrog?\n\nA strange construction of locks\n\nHow to solve this trigonometric function equation?\n\nCosmic circles and the celestial key\n\nIs the Big Bang a theory or a model?\n\nHow can I alias `...` to `../..` in Bash?\n\nGreen Triangles Puzzle\n\nFinding a minimum cut with an upper bound on the set sizes\n\nIs it a Contactor or is it a Relay?\n\nBorders of a Rectangular Matrix\n\nHow to load font for emacs on Arch Linux?\n\nWhat does the dot product of two vectors actually represent intuitively? What is its true meaning conceptually?\n\nWhat is a good second hand option for someone who doesn\'t use shields? And probably uses most bonus actions?\n\nA numerical matrix of power sum polynomials\n\nBeginner friendly pocket cuts more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', 'timestamp': '2024-02-15T16:48:37', 'title': 'python - How can I understand the kernel of tf.keras.layers.Dense for rank >2? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/68984841/how-can-i-understand-the-kernel-of-tf-keras-layers-dense-for-rank-2'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nWhat exactly does tf.keras.layers.Dense do?\n\nAsked 4 years, 3 months ago\n\nModified 4 years, 3 months ago\n\nI\'m using the Keras to build a convolutional neural network. I ran across the following:\n\nmodel = tf.keras.Sequential() model.add(layers.Dense(10*10*256, use_bias=False, input_shape=(100,)))\n\nI\'m curious - what exactly mathematically is going on here?\n\nMy guess is that for input of size [100,N], the network will be evaluated N times, once for each training example. The Dense layer created by layers.Dense contains (10*10*256) * (100) parameters that will be updated during backpropagation.\n\nImprove this question\n\nasked Mar 21, 2020 at 0:14\n\nwheresmycookiewheresmycookie\n\n75333 gold badges1818 silver badges4141 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nDense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n\nNote: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\n# as first layer in a sequential model: model = Sequential() model.add(Dense(32, input_shape=(16,))) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don\'t need to specify # the size of the input anymore: model.add(Dense(32))\n\n> units: Positive integer, dimensionality of the output space. > activation: Activation function to use. If you don\'t specify anything, > no activation is applied (ie. ""linear"" activation: a(x) = x). > use_bias: Boolean, whether the layer uses a bias vector. > kernel_initializer: Initializer for the kernel weights matrix. > bias_initializer: Initializer for the bias vector. >kernel_regularizer:Regularizer function applied to the kernel weights matrix. > bias_regularizer: Regularizer function applied to the bias vector. > activity_regularizer: Regularizer function applied to the output of the layer (its ""activation"").. >kernel_constraint: Constraint function applied to the kernel weights matrix. >bias_constraint: Constraint function applied to the bias vector.\n\nN-D tensor with shape: (batch_size, ..., input_dim). The most common situation would be a 2D input with shape (batch_size, input_dim).\n\nN-D tensor with shape: (batch_size, ..., units). For instance, for a 2D input with shape (batch_size, input_dim), the output would have shape (batch_size, units).\n\nanswered Mar 21, 2020 at 0:51\n\nAmar KumarAmar Kumar\n\n2,60022 gold badges2828 silver badges3434 bronze badges 1\n\nAfter reading your answer, what should have been obvious to me is now more obvious. I was somehow thinking that each 100-element example had to be run individually, but now I realize that that\'s dumb. Thanks for the clear explanation :)\n\n– wheresmycookie Commented Mar 21, 2020 at 0:57\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nThe return of Staging Ground to Stack Overflow\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\n32 Difference between Dense and Activation layer in Keras\n\n7 What does Dense do?\n\n20 Is tf.layers.dense a single layer?\n\n1 what does dense_[number] mean in model.summary in keras\n\n3 Does Tensorflows tf.layers.dense flatten input dimensions?\n\n0 Questions on tf.layers .dense\n\n0 Confusion on the \'linear\' activation in tf.keras.layers.Dense()\n\n2 usage of tf.keras.layers.DenseFeatures\n\n1 Result of Dense layer in keras\n\n0 What does applying a layer on a model do?\n\nHot Network Questions\n\nWhy would anyone kill a dragon rather than subdue it in OD&D?\n\nBTC Wallet with Seeed but without publickey\n\nwhich one is correct, ""looking forward to hear"" OR ""looking forward to hearing""\n\nWould a PhD from Europe, Canada, Australia, or New Zealand be accepted in the US?\n\nIs ""parse out"" actually a phrasal verb, and in what context do you use ""parse""\n\nWhat is a quarter in 19th-century England converted to contemporary pints?\n\nAre close states still close after measurement (regarding trace distance)?\n\nDoes hydrocal plaster work twice?\n\nWhat happened to Slic3r?\n\nDerivative of the Score Function in Fisher Information\n\nIdiom for a situation where a problem has two simultaneous but unrelated causes?\n\nCould Kessler Syndrome be used to control the temperature of the Earth?\n\nProper way to write C code that injects message into /var/log/messages?\n\nLiquid exited the jar during canning, is this safe to use\n\nA class for students who want to get better at a subject, aside from their public education\n\nDo I need to staple cable for new wire run through a preexisting wall?\n\nLD_DEBUG_OUTPUT does not work\n\nAre 1/20 undocumented immigrants married to American citizens?\n\nCould we control our personal reality\n\nWhy does Aemond focus on a coin in his room in ""House of the Dragon"" S02E02?\n\nShort story about soldiers who are fighting against an enemy which turns out to be themselves\n\nBenefit of splitting and routing/encrypting file packets separately?\n\nForcing QGIS to export using left hand rule more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_1', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nWhat exactly does tf.keras.layers.Dense do?\n\nAsked 4 years, 3 months ago\n\nModified 4 years, 3 months ago\n\nI\'m using the Keras to build a convolutional neural network. I ran across the following:\n\nmodel = tf.keras.Sequential() model.add(layers.Dense(10*10*256, use_bias=False, input_shape=(100,)))\n\nI\'m curious - what exactly mathematically is going on here?\n\nMy guess is that for input of size [100,N], the network will be evaluated N times, once for each training example. The Dense layer created by layers.Dense contains (10*10*256) * (100) parameters that will be updated during backpropagation.\n\nImprove this question\n\nasked Mar 21, 2020 at 0:14\n\nwheresmycookiewheresmycookie\n\n75333 gold badges1818 silver badges4141 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nDense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n\nNote: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\n# as first layer in a sequential model: model = Sequential() model.add(Dense(32, input_shape=(16,))) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don\'t need to specify # the size of the input anymore: model.add(Dense(32))\n\n> units: Positive integer, dimensionality of the output space. > activation: Activation function to use. If you don\'t specify anything, > no activation is applied (ie. ""linear"" activation: a(x) = x). > use_bias: Boolean, whether the layer uses a bias vector. > kernel_initializer: Initializer for the kernel weights matrix. > bias_initializer: Initializer for the bias vector. >kernel_regularizer:Regularizer function applied to the kernel weights matrix. > bias_regularizer: Regularizer function applied to the bias vector. > activity_regularizer: Regularizer function applied to the output of the layer (its ""activation"").. >kernel_constraint: Constraint function applied to the kernel weights matrix. >bias_constraint: Constraint function applied to the bias vector.\n\nN-D tensor with shape: (batch_size, ..., input_dim). The most common situation would be a 2D input with shape (batch_size, input_dim).\n\nN-D tensor with shape: (batch_size, ..., units). For instance, for a 2D input with shape (batch_size, input_dim), the output would have shape (batch_size, units).\n\nanswered Mar 21, 2020 at 0:51\n\nAmar KumarAmar Kumar\n\n2,60022 gold badges2828 silver badges3434 bronze badges 1\n\nAfter reading your answer, what should have been obvious to me is now more obvious. I was somehow thinking that each 100-element example had to be run individually, but now I realize that that\'s dumb. Thanks for the clear explanation :)\n\n– wheresmycookie Commented Mar 21, 2020 at 0:57\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nThe return of Staging Ground to Stack Overflow\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\n32 Difference between Dense and Activation layer in Keras\n\n7 What does Dense do?\n\n20 Is tf.layers.dense a single layer?\n\n1 what does dense_[number] mean in model.summary in keras\n\n3 Does Tensorflows tf.layers.dense flatten input dimensions?\n\n0 Questions on tf.layers .dense\n\n0 Confusion on the \'linear\' activation in tf.keras.layers.Dense()\n\n2 usage of tf.keras.layers.DenseFeatures\n\n1 Result of Dense layer in keras\n\n0 What does applying a layer on a model do?\n\nHot Network Questions\n\nWhy would anyone kill a dragon rather than subdue it in OD&D?\n\nBTC Wallet with Seeed but without publickey\n\nwhich one is correct, ""looking forward to hear"" OR ""looking forward to hearing""\n\nWould a PhD from Europe, Canada, Australia, or New Zealand be accepted in the US?\n\nIs ""parse out"" actually a phrasal verb, and in what context do you use ""parse""\n\nWhat is a quarter in 19th-century England converted to contemporary pints?\n\nAre close states still close after measurement (regarding trace distance)?\n\nDoes hydrocal plaster work twice?\n\nWhat happened to Slic3r?\n\nDerivative of the Score Function in Fisher Information\n\nIdiom for a situation where a problem has two simultaneous but unrelated causes?\n\nCould Kessler Syndrome be used to control the temperature of the Earth?\n\nProper way to write C code that injects message into /var/log/messages?\n\nLiquid exited the jar during canning, is this safe to use\n\nA class for students who want to get better at a subject, aside from their public education\n\nDo I need to staple cable for new wire run through a preexisting wall?\n\nLD_DEBUG_OUTPUT does not work\n\nAre 1/20 undocumented immigrants married to American citizens?\n\nCould we control our personal reality\n\nWhy does Aemond focus on a coin in his room in ""House of the Dragon"" S02E02?\n\nShort story about soldiers who are fighting against an enemy which turns out to be themselves\n\nBenefit of splitting and routing/encrypting file packets separately?\n\nForcing QGIS to export using left hand rule more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-06-26T19:19:26', 'title': 'What exactly does tf.keras.layers.Dense do? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/60783216/what-exactly-does-tf-keras-layers-dense-do'})], [Document(page_content=""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nWrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. #25780\n\nfredo994 opened this issue\n\nFeb 15, 2019 · 6 comments\n\nWrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. #25780\n\nfredo994 opened this issue\n\nFeb 15, 2019 · 6 comments\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author type:bug\n\nOS Platform and Distribution: MacOs High Sierra\n\nTensorFlow installed from: installed from pip\n\nTensorFlow version: v1.12.0-0-ga6d8ffae09 1.12.0\n\nProblem I think that tf.python.keras.Dense layer is not implemented correctly. The documentation states:\n\nNote: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nMy interpretation of this claim is: if we have input tensor with shape (None, 10, 10) then dense layer with 20 hidden units should produce output with a shape (None, 20) and the kernel should have a shape of (10 * 10, 20), and input tensor should be flattened to shape (None, 100) before product with kernel.\n\nBut, implementation is different.\n\nKernel is initialized to shape (10, 20) (for the example above)\n\nself.kernel = self.add_weight( # method Dense#build 'kernel', shape=[input_shape[-1].value, self.units], initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, dtype=self.dtype, trainable=True)\n\nThe output is computed as tensordot with summation over innermost dimensions\n\n# Broadcasting is required for the inputs. outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]]) # method Dense#call\n\nThis means that for x.shape == (None, 10, 10) and units == 20 the output shape will be (None, 10, 20), and that kernel is broadcasted for all other dimensions (this means weights are shared).\n\nCode to reproduce the issue\n\nimport tensorflow as tf tf.enable_eager_execution() x = tf.random.normal((10, 10, 10)) y = tf.layers.dense(x, 20) print(y.shape) # Produces (10, 10, 20) not (10, 20) as expected.\n\nDiscussion Three possible things are happening here:\n\nDocumentation is wrong and implementation is correct, and a dense layer is meant to accept only rank 2-dimensional input (batchSize, a dimension of input vector) and if we pass it higher rank it treats it as a stack of input vectors. The solution is to change the documentation (specifically the note part), it should state something like numpy's documentation for matmul:\n\nIf either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly\n\nImplementation is wrong and there should be flatten before tensordot (no need for tensordot, we can now use matmul)\n\nI do not understand what does dense layer should do (unlikely because I asked my colleagues for their opinion and they agree with my view of what kind of operation does dense layer perform)\n\nI've ordered this list from most to least likely (in my opinion).\n\nThe text was updated successfully, but these errors were encountered:\n\ngowthamkpr assigned gowthamkpr and unassigned gowthamkpr\n\njvishnuvardhan self-assigned this\n\njvishnuvardhan added comp:keras\n\nKeras related issues type:bug\n\njvishnuvardhan assigned caisq and unassigned jvishnuvardhan\n\njvishnuvardhan added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nImmediately build tf.keras.layers.Layer if input_dim or input_shape are specified #27050\n\nComment added in keras/layers/core.py #27057\n\ngshashank84 commented\n\nI have added an comment for the working of tensordot function here, however this does not state that the Problem has been solved.\n\nThe size of the specified input_shape still does not raise error if the shape of the inputs (called by Dense) is not equal to input_shape.\n\njvishnuvardhan assigned tanzhenyu and unassigned caisq\n\n@fredo994 I agree with your solution n°1 (remove or update the 'Note' part). I'm not an GitHub/PR expert, would you be able to open a PR with your proposition of changing the docstring?\n\nrmothukuru self-assigned this\n\nrmothukuru added stat:awaiting response\n\nStatus - Awaiting response from author and removed stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower labels\n\nrmothukuru commented\n\n@fredo994, A Note, as shown below, has been added in the Documentation of Dense Layer:\n\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 1 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n\nCan you please confirm if we can close this issue, as it has been resolved? Thanks!\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you'd like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author type:bug\n\nYou can’t perform that action at this time."", metadata={'id': 'web-search_0', 'snippet': ""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nWrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. #25780\n\nfredo994 opened this issue\n\nFeb 15, 2019 · 6 comments\n\nWrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. #25780\n\nfredo994 opened this issue\n\nFeb 15, 2019 · 6 comments\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author type:bug\n\nOS Platform and Distribution: MacOs High Sierra\n\nTensorFlow installed from: installed from pip\n\nTensorFlow version: v1.12.0-0-ga6d8ffae09 1.12.0\n\nProblem I think that tf.python.keras.Dense layer is not implemented correctly. The documentation states:\n\nNote: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nMy interpretation of this claim is: if we have input tensor with shape (None, 10, 10) then dense layer with 20 hidden units should produce output with a shape (None, 20) and the kernel should have a shape of (10 * 10, 20), and input tensor should be flattened to shape (None, 100) before product with kernel.\n\nBut, implementation is different.\n\nKernel is initialized to shape (10, 20) (for the example above)\n\nself.kernel = self.add_weight( # method Dense#build 'kernel', shape=[input_shape[-1].value, self.units], initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, dtype=self.dtype, trainable=True)\n\nThe output is computed as tensordot with summation over innermost dimensions\n\n# Broadcasting is required for the inputs. outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]]) # method Dense#call\n\nThis means that for x.shape == (None, 10, 10) and units == 20 the output shape will be (None, 10, 20), and that kernel is broadcasted for all other dimensions (this means weights are shared).\n\nCode to reproduce the issue\n\nimport tensorflow as tf tf.enable_eager_execution() x = tf.random.normal((10, 10, 10)) y = tf.layers.dense(x, 20) print(y.shape) # Produces (10, 10, 20) not (10, 20) as expected.\n\nDiscussion Three possible things are happening here:\n\nDocumentation is wrong and implementation is correct, and a dense layer is meant to accept only rank 2-dimensional input (batchSize, a dimension of input vector) and if we pass it higher rank it treats it as a stack of input vectors. The solution is to change the documentation (specifically the note part), it should state something like numpy's documentation for matmul:\n\nIf either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly\n\nImplementation is wrong and there should be flatten before tensordot (no need for tensordot, we can now use matmul)\n\nI do not understand what does dense layer should do (unlikely because I asked my colleagues for their opinion and they agree with my view of what kind of operation does dense layer perform)\n\nI've ordered this list from most to least likely (in my opinion).\n\nThe text was updated successfully, but these errors were encountered:\n\ngowthamkpr assigned gowthamkpr and unassigned gowthamkpr\n\njvishnuvardhan self-assigned this\n\njvishnuvardhan added comp:keras\n\nKeras related issues type:bug\n\njvishnuvardhan assigned caisq and unassigned jvishnuvardhan\n\njvishnuvardhan added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nImmediately build tf.keras.layers.Layer if input_dim or input_shape are specified #27050\n\nComment added in keras/layers/core.py #27057\n\ngshashank84 commented\n\nI have added an comment for the working of tensordot function here, however this does not state that the Problem has been solved.\n\nThe size of the specified input_shape still does not raise error if the shape of the inputs (called by Dense) is not equal to input_shape.\n\njvishnuvardhan assigned tanzhenyu and unassigned caisq\n\n@fredo994 I agree with your solution n°1 (remove or update the 'Note' part). I'm not an GitHub/PR expert, would you be able to open a PR with your proposition of changing the docstring?\n\nrmothukuru self-assigned this\n\nrmothukuru added stat:awaiting response\n\nStatus - Awaiting response from author and removed stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower labels\n\nrmothukuru commented\n\n@fredo994, A Note, as shown below, has been added in the Documentation of Dense Layer:\n\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 1 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n\nCan you please confirm if we can close this issue, as it has been resolved? Thanks!\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you'd like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author type:bug\n\nYou can’t perform that action at this time."", 'timestamp': '2024-06-02T09:18:54', 'title': 'Wrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. · Issue #25780 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/25780'}), Document(page_content=""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nWrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. #25780\n\nfredo994 opened this issue\n\nFeb 15, 2019 · 6 comments\n\nWrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. #25780\n\nfredo994 opened this issue\n\nFeb 15, 2019 · 6 comments\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author type:bug\n\nOS Platform and Distribution: MacOs High Sierra\n\nTensorFlow installed from: installed from pip\n\nTensorFlow version: v1.12.0-0-ga6d8ffae09 1.12.0\n\nProblem I think that tf.python.keras.Dense layer is not implemented correctly. The documentation states:\n\nNote: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nMy interpretation of this claim is: if we have input tensor with shape (None, 10, 10) then dense layer with 20 hidden units should produce output with a shape (None, 20) and the kernel should have a shape of (10 * 10, 20), and input tensor should be flattened to shape (None, 100) before product with kernel.\n\nBut, implementation is different.\n\nKernel is initialized to shape (10, 20) (for the example above)\n\nself.kernel = self.add_weight( # method Dense#build 'kernel', shape=[input_shape[-1].value, self.units], initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, dtype=self.dtype, trainable=True)\n\nThe output is computed as tensordot with summation over innermost dimensions\n\n# Broadcasting is required for the inputs. outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]]) # method Dense#call\n\nThis means that for x.shape == (None, 10, 10) and units == 20 the output shape will be (None, 10, 20), and that kernel is broadcasted for all other dimensions (this means weights are shared).\n\nCode to reproduce the issue\n\nimport tensorflow as tf tf.enable_eager_execution() x = tf.random.normal((10, 10, 10)) y = tf.layers.dense(x, 20) print(y.shape) # Produces (10, 10, 20) not (10, 20) as expected.\n\nDiscussion Three possible things are happening here:\n\nDocumentation is wrong and implementation is correct, and a dense layer is meant to accept only rank 2-dimensional input (batchSize, a dimension of input vector) and if we pass it higher rank it treats it as a stack of input vectors. The solution is to change the documentation (specifically the note part), it should state something like numpy's documentation for matmul:\n\nIf either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly\n\nImplementation is wrong and there should be flatten before tensordot (no need for tensordot, we can now use matmul)\n\nI do not understand what does dense layer should do (unlikely because I asked my colleagues for their opinion and they agree with my view of what kind of operation does dense layer perform)\n\nI've ordered this list from most to least likely (in my opinion).\n\nThe text was updated successfully, but these errors were encountered:\n\ngowthamkpr assigned gowthamkpr and unassigned gowthamkpr\n\njvishnuvardhan self-assigned this\n\njvishnuvardhan added comp:keras\n\nKeras related issues type:bug\n\njvishnuvardhan assigned caisq and unassigned jvishnuvardhan\n\njvishnuvardhan added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nImmediately build tf.keras.layers.Layer if input_dim or input_shape are specified #27050\n\nComment added in keras/layers/core.py #27057\n\ngshashank84 commented\n\nI have added an comment for the working of tensordot function here, however this does not state that the Problem has been solved.\n\nThe size of the specified input_shape still does not raise error if the shape of the inputs (called by Dense) is not equal to input_shape.\n\njvishnuvardhan assigned tanzhenyu and unassigned caisq\n\n@fredo994 I agree with your solution n°1 (remove or update the 'Note' part). I'm not an GitHub/PR expert, would you be able to open a PR with your proposition of changing the docstring?\n\nrmothukuru self-assigned this\n\nrmothukuru added stat:awaiting response\n\nStatus - Awaiting response from author and removed stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower labels\n\nrmothukuru commented\n\n@fredo994, A Note, as shown below, has been added in the Documentation of Dense Layer:\n\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 1 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n\nCan you please confirm if we can close this issue, as it has been resolved? Thanks!\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you'd like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author type:bug\n\nYou can’t perform that action at this time."", metadata={'id': 'web-search_3', 'snippet': ""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nWrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. #25780\n\nfredo994 opened this issue\n\nFeb 15, 2019 · 6 comments\n\nWrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. #25780\n\nfredo994 opened this issue\n\nFeb 15, 2019 · 6 comments\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author type:bug\n\nOS Platform and Distribution: MacOs High Sierra\n\nTensorFlow installed from: installed from pip\n\nTensorFlow version: v1.12.0-0-ga6d8ffae09 1.12.0\n\nProblem I think that tf.python.keras.Dense layer is not implemented correctly. The documentation states:\n\nNote: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nMy interpretation of this claim is: if we have input tensor with shape (None, 10, 10) then dense layer with 20 hidden units should produce output with a shape (None, 20) and the kernel should have a shape of (10 * 10, 20), and input tensor should be flattened to shape (None, 100) before product with kernel.\n\nBut, implementation is different.\n\nKernel is initialized to shape (10, 20) (for the example above)\n\nself.kernel = self.add_weight( # method Dense#build 'kernel', shape=[input_shape[-1].value, self.units], initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, dtype=self.dtype, trainable=True)\n\nThe output is computed as tensordot with summation over innermost dimensions\n\n# Broadcasting is required for the inputs. outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]]) # method Dense#call\n\nThis means that for x.shape == (None, 10, 10) and units == 20 the output shape will be (None, 10, 20), and that kernel is broadcasted for all other dimensions (this means weights are shared).\n\nCode to reproduce the issue\n\nimport tensorflow as tf tf.enable_eager_execution() x = tf.random.normal((10, 10, 10)) y = tf.layers.dense(x, 20) print(y.shape) # Produces (10, 10, 20) not (10, 20) as expected.\n\nDiscussion Three possible things are happening here:\n\nDocumentation is wrong and implementation is correct, and a dense layer is meant to accept only rank 2-dimensional input (batchSize, a dimension of input vector) and if we pass it higher rank it treats it as a stack of input vectors. The solution is to change the documentation (specifically the note part), it should state something like numpy's documentation for matmul:\n\nIf either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly\n\nImplementation is wrong and there should be flatten before tensordot (no need for tensordot, we can now use matmul)\n\nI do not understand what does dense layer should do (unlikely because I asked my colleagues for their opinion and they agree with my view of what kind of operation does dense layer perform)\n\nI've ordered this list from most to least likely (in my opinion).\n\nThe text was updated successfully, but these errors were encountered:\n\ngowthamkpr assigned gowthamkpr and unassigned gowthamkpr\n\njvishnuvardhan self-assigned this\n\njvishnuvardhan added comp:keras\n\nKeras related issues type:bug\n\njvishnuvardhan assigned caisq and unassigned jvishnuvardhan\n\njvishnuvardhan added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nImmediately build tf.keras.layers.Layer if input_dim or input_shape are specified #27050\n\nComment added in keras/layers/core.py #27057\n\ngshashank84 commented\n\nI have added an comment for the working of tensordot function here, however this does not state that the Problem has been solved.\n\nThe size of the specified input_shape still does not raise error if the shape of the inputs (called by Dense) is not equal to input_shape.\n\njvishnuvardhan assigned tanzhenyu and unassigned caisq\n\n@fredo994 I agree with your solution n°1 (remove or update the 'Note' part). I'm not an GitHub/PR expert, would you be able to open a PR with your proposition of changing the docstring?\n\nrmothukuru self-assigned this\n\nrmothukuru added stat:awaiting response\n\nStatus - Awaiting response from author and removed stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower labels\n\nrmothukuru commented\n\n@fredo994, A Note, as shown below, has been added in the Documentation of Dense Layer:\n\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 1 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n\nCan you please confirm if we can close this issue, as it has been resolved? Thanks!\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you'd like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author type:bug\n\nYou can’t perform that action at this time."", 'timestamp': '2024-06-02T09:18:54', 'title': 'Wrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2. · Issue #25780 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/25780'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\n""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 #10736\n\nstiffme opened this issue\n\nJul 20, 2018 · 3 comments\n\n""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 #10736\n\nstiffme opened this issue\n\nJul 20, 2018 · 3 comments\n\ntype:bug/performance type:docs\n\nNeed to modify the documentation\n\nKeras : 2.2.0 Tensorflow-GPU: 1.8.0/ 1.9.0 (Both versions have the same issue)\n\nProblem: When using an input with rank greater than 2 in a Dense layer, ""Dimensions must be equal"" exception will be thrown from Tensorflow.\n\nIf using use_bias=False in Dense layer, it is OK. If using Reshape before supply to Dense layer, it is OK.\n\nWith above symptons , it seems something is wrong when handling the bias with an input of rank size over 2. According to the documents: Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nHere is the short script to reproduce the issue:\n\nfrom keras import Input, Model from keras.layers import Dense, Reshape def build_model(): X = Input(shape=(30, 40)) output = Dense(10, activation=\'tanh\')(X) model = Model(inputs=X, outputs=output) def main(): build_model() if __name__ == ""__main__"": main()\n\nHere is the exception:\n\nUsing TensorFlow backend. Traceback (most recent call last): File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1567, in _create_c_op c_op = c_api.TF_FinishOperation(op_desc) tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 30 and 10 for \'dense_1/add\' (op: \'Add\') with input shapes: [?,30,10], [1,10,1]. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 15, in <module> main() File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 11, in main build_model() File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 6, in build_model output = Dense(10, activation=\'tanh\')(X) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/engine/topology.py"", line 619, in __call__ output = self.call(inputs, **kwargs) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/layers/core.py"", line 879, in call output = K.bias_add(output, self.bias) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 3781, in bias_add x += reshape(bias, (1, bias_shape[0], 1)) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 979, in binary_op_wrapper return func(x, y, name=name) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 297, in add ""Add"", x=x, y=y, name=name) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper op_def=op_def) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op op_def=op_def) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1734, in __init__ control_input_ops) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1570, in _create_c_op raise ValueError(str(e)) ValueError: Dimensions must be equal, but are 30 and 10 for \'dense_1/add\' (op: \'Add\') with input shapes: [?,30,10], [1,10,1].\n\nThe text was updated successfully, but these errors were encountered:\n\nContrary to the documentation, we don\'t actually flatten it. It\'s applied on the last axis independently.\n\nWe have a couple of options, but they some are breaking changes.\n\nUpdate the doc, making it clear that it\'s not actually flatten\n\nDon\'t allow this behavior (Deprecate it), forcing the user to use TimeDistributed\n\nActually Flatten as the doc says, but it\'s a breaking change.\n\nThis is an API breaking change so what do you think @fchollet?\n\nDref360 added type:bug/performance type:docs\n\nNeed to modify the documentation labels\n\ntRosenflanz commented\n\nSo is it true that current behaviour of Dense layer is identical to TimeDistributed(Dense) for rank 3 tensors but is different for rank 4+ ? Although it is up-to François, I as a user could get behind 2nd option especially if TimeDistributed wrapper could be reworked to have axis argument that defaults to 1 (won\'t break existing behaviour of TimeDistributed wrapper) but can be changed to higher values to match current Dense behaviour\n\nSriRangaTarun mentioned this issue\n\nUpdate Dense Layer documentation in core.py #12111\n\nfchollet closed this as completed\n\nalex-lt-kong commented\n\nHi @Dref360 , I encountered the same issue and after reading what you said I am still a bit confused. By saying It\'s applied on the last axis independently., my understanding is this:\n\nSuppose my samples are all 5x2 matrices and I have a Dense layer with 8 neurons：\n\nthe layer will be applied to each sample 5 times;\n\neach time, the layer is applied to a 1x2 vector, i.e., it fully connects the 2 input nodes to 8 output nodes;\n\nthe output from each application is a 1x8 vector;\n\nthe final output is a 5x8 matrix/tensor\n\nIs this understanding correct?\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\ntype:bug/performance type:docs\n\nNeed to modify the documentation\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_1', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\n""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 #10736\n\nstiffme opened this issue\n\nJul 20, 2018 · 3 comments\n\n""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 #10736\n\nstiffme opened this issue\n\nJul 20, 2018 · 3 comments\n\ntype:bug/performance type:docs\n\nNeed to modify the documentation\n\nKeras : 2.2.0 Tensorflow-GPU: 1.8.0/ 1.9.0 (Both versions have the same issue)\n\nProblem: When using an input with rank greater than 2 in a Dense layer, ""Dimensions must be equal"" exception will be thrown from Tensorflow.\n\nIf using use_bias=False in Dense layer, it is OK. If using Reshape before supply to Dense layer, it is OK.\n\nWith above symptons , it seems something is wrong when handling the bias with an input of rank size over 2. According to the documents: Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nHere is the short script to reproduce the issue:\n\nfrom keras import Input, Model from keras.layers import Dense, Reshape def build_model(): X = Input(shape=(30, 40)) output = Dense(10, activation=\'tanh\')(X) model = Model(inputs=X, outputs=output) def main(): build_model() if __name__ == ""__main__"": main()\n\nHere is the exception:\n\nUsing TensorFlow backend. Traceback (most recent call last): File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1567, in _create_c_op c_op = c_api.TF_FinishOperation(op_desc) tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 30 and 10 for \'dense_1/add\' (op: \'Add\') with input shapes: [?,30,10], [1,10,1]. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 15, in <module> main() File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 11, in main build_model() File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 6, in build_model output = Dense(10, activation=\'tanh\')(X) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/engine/topology.py"", line 619, in __call__ output = self.call(inputs, **kwargs) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/layers/core.py"", line 879, in call output = K.bias_add(output, self.bias) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 3781, in bias_add x += reshape(bias, (1, bias_shape[0], 1)) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 979, in binary_op_wrapper return func(x, y, name=name) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 297, in add ""Add"", x=x, y=y, name=name) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper op_def=op_def) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op op_def=op_def) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1734, in __init__ control_input_ops) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1570, in _create_c_op raise ValueError(str(e)) ValueError: Dimensions must be equal, but are 30 and 10 for \'dense_1/add\' (op: \'Add\') with input shapes: [?,30,10], [1,10,1].\n\nThe text was updated successfully, but these errors were encountered:\n\nContrary to the documentation, we don\'t actually flatten it. It\'s applied on the last axis independently.\n\nWe have a couple of options, but they some are breaking changes.\n\nUpdate the doc, making it clear that it\'s not actually flatten\n\nDon\'t allow this behavior (Deprecate it), forcing the user to use TimeDistributed\n\nActually Flatten as the doc says, but it\'s a breaking change.\n\nThis is an API breaking change so what do you think @fchollet?\n\nDref360 added type:bug/performance type:docs\n\nNeed to modify the documentation labels\n\ntRosenflanz commented\n\nSo is it true that current behaviour of Dense layer is identical to TimeDistributed(Dense) for rank 3 tensors but is different for rank 4+ ? Although it is up-to François, I as a user could get behind 2nd option especially if TimeDistributed wrapper could be reworked to have axis argument that defaults to 1 (won\'t break existing behaviour of TimeDistributed wrapper) but can be changed to higher values to match current Dense behaviour\n\nSriRangaTarun mentioned this issue\n\nUpdate Dense Layer documentation in core.py #12111\n\nfchollet closed this as completed\n\nalex-lt-kong commented\n\nHi @Dref360 , I encountered the same issue and after reading what you said I am still a bit confused. By saying It\'s applied on the last axis independently., my understanding is this:\n\nSuppose my samples are all 5x2 matrices and I have a Dense layer with 8 neurons：\n\nthe layer will be applied to each sample 5 times;\n\neach time, the layer is applied to a 1x2 vector, i.e., it fully connects the 2 input nodes to 8 output nodes;\n\nthe output from each application is a 1x8 vector;\n\nthe final output is a 5x8 matrix/tensor\n\nIs this understanding correct?\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\ntype:bug/performance type:docs\n\nNeed to modify the documentation\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-02T09:18:55', 'title': '""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 · Issue #10736 · keras-team/keras', 'url': 'https://github.com/keras-team/keras/issues/10736'}), Document(page_content=""Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nDense does not flatten inputs with rank >2 and behaves exactly like TimeDistributed(Dense) #30882\n\ndurandg12 opened this issue\n\nJul 19, 2019 · 9 comments\n\nDense does not flatten inputs with rank >2 and behaves exactly like TimeDistributed(Dense) #30882\n\ndurandg12 opened this issue\n\nJul 19, 2019 · 9 comments\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\n\nTensorFlow installed from (source or binary): from pip install\n\nTensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\n\nPython version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\n\nDescribe the current behavior A note in Dense documentation says that\n\nNote: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nI don't see this happening in real life. Instead, Dense behaves on a 3-rank tensor as it would behave if it was wrapped in a TimeDistributed layer, making me question the utility of TimeDistributed at all.\n\nDescribe the expected behavior Dense should flatten its input like the documentation says. In the first example bellow, the shape of the kernel weights of dense should be (5 * 3, 2) = (15, 2) instead of (3, 2), which is the shape of dense2 (as expected in the case of dense2).\n\nCode to reproduce the issue\n\nimport tensorflow as tf import numpy as np print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION)) tf.random.set_seed(12) np.random.seed(12) init = tf.keras.initializers.GlorotUniform(seed=12) inp = tf.constant(np.random.normal(0, 1, (1, 5, 6))) inp = tf.cast(inp, dtype=tf.float32) gru = tf.keras.layers.GRU(3, return_sequences=True)(inp) print(gru.shape) #(1, 5, 3) dense = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init) print(dense(gru)) #tf.Tensor( #[[[ 1.5456871 -0.5280464 ] # [ 0.11647969 -0.20553198] # [ 0.58126366 -0.16031623] # [-0.22882831 -0.22649539] # [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32) for w in dense.weights: print(w.shape) #(3, 2) instead of (5 * 3, 2) if Dense indeed flattened its input #(2,) tddense = tf.keras.layers.TimeDistributed(dense) print(tddense(gru)) #tf.Tensor( #[[[ 1.5456871 -0.5280464 ] # [ 0.11647969 -0.20553198] # [ 0.58126366 -0.16031623] # [-0.22882831 -0.22649539] # [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32) # if Dense kernel had shape (15, 2), this should result in the following error: # InvalidArgumentError: Matrix size-incompatible: In[0]: [5,3], In[1]: [15,2] [Op:MatMul] # but instead what we get is the same output # than without TimeDistributed, without error dense2 = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init) tddense = tf.keras.layers.TimeDistributed(dense2) print(tddense(gru)) #tf.Tensor( #[[[ 1.5456871 -0.5280464 ] # [ 0.11647969 -0.20553198] # [ 0.58126366 -0.16031623] # [-0.22882831 -0.22649539] # [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32) for w in dense2.weights: print(w.shape) #(3, 2) as expected #(2,)\n\nSecond example, with a rank even larger than 3:\n\nimport tensorflow as tf print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION)) inp = tf.keras.Input(shape=(10, 25, 25, 3)) dense_layer1 = tf.keras.layers.Dense(78) x = dense_layer1(inp) print('Output shape without TimeDistributed:') print(x.shape) dense_layer2 = tf.keras.layers.Dense(78) y=tf.keras.layers.TimeDistributed(dense_layer2)(inp) print('Output shape with TimeDistributed:') print(y.shape) print('Weight shapes without TimeDistributed:') for weight in dense_layer1.trainable_weights: if len(weight.shape) == 2: print(' kernel shape:') else: print(' bias shape:') print(weight.shape) print('Weight shapes with TimeDistributed:') for weight in dense_layer2.trainable_weights: if len(weight.shape) == 2: print(' kernel shape:') else: print(' bias shape:') print(weight.shape)\n\nUsing Tensorflow version 2.0.0-beta1 (git version v2.0.0-beta0-16-g1d91213fe7) Output shape without TimeDistributed: (None, 10, 25, 25, 78) Output shape with TimeDistributed: (None, 10, 25, 25, 78) Weight shapes without TimeDistributed: kernel shape: (3, 78) bias shape: (78,) Weight shapes with TimeDistributed: kernel shape: (3, 78) bias shape: (78,)\n\nWe see, in this example, that Dense and TimeDistributed(Dense) behave the same in that they only touch to the last dimension of the input.\n\nThe text was updated successfully, but these errors were encountered:\n\noanush self-assigned this\n\noanush added 2.0.0-beta0 comp:keras\n\nKeras related issues type:bug\n\nIssue replicating with TF version-2.0.0beta1, please find the gist of collab. Thanks!\n\noanush assigned ymodak and unassigned oanush\n\n@anush-o It says access denied. Please change your settings to enable it.\n\nymodak assigned pavithrasv and unassigned ymodak\n\nThe issue is still here in tf2.0.0.\n\njvishnuvardhan added TF 2.0\n\nIssues relating to TensorFlow 2.0 and removed TF 2.0.0-beta0 labels\n\npavithrasv commented\n\nThe note is incorrect, we will update the note to reflect the code behavior.\n\npavithrasv commented\n\nCommit 2e6a3c5#diff-5fb1fa5fa46d0ec9a01d5a60b7d8acc8\n\npavithrasv closed this as completed\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nI appreciate how the new note is comprehensive @pavithrasv .\n\nBut now I don't get the purpose of TimeDistributed anymore, as I don't see the difference between Dense and TimeDistributed(Dense).\n\npavithrasv commented\n\nThey are the same, TimeDistributed doesn't just apply to Dense layers but i see that the main example in the TimeDistributed docs is using Dense layer. i'll update that.\n\npavithrasv commented\n\nThe change to TimeDistributed docs have also been submitted. Thank you!\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nYou can’t perform that action at this time."", metadata={'id': 'web-search_2', 'snippet': ""Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nDense does not flatten inputs with rank >2 and behaves exactly like TimeDistributed(Dense) #30882\n\ndurandg12 opened this issue\n\nJul 19, 2019 · 9 comments\n\nDense does not flatten inputs with rank >2 and behaves exactly like TimeDistributed(Dense) #30882\n\ndurandg12 opened this issue\n\nJul 19, 2019 · 9 comments\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\n\nTensorFlow installed from (source or binary): from pip install\n\nTensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\n\nPython version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\n\nDescribe the current behavior A note in Dense documentation says that\n\nNote: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nI don't see this happening in real life. Instead, Dense behaves on a 3-rank tensor as it would behave if it was wrapped in a TimeDistributed layer, making me question the utility of TimeDistributed at all.\n\nDescribe the expected behavior Dense should flatten its input like the documentation says. In the first example bellow, the shape of the kernel weights of dense should be (5 * 3, 2) = (15, 2) instead of (3, 2), which is the shape of dense2 (as expected in the case of dense2).\n\nCode to reproduce the issue\n\nimport tensorflow as tf import numpy as np print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION)) tf.random.set_seed(12) np.random.seed(12) init = tf.keras.initializers.GlorotUniform(seed=12) inp = tf.constant(np.random.normal(0, 1, (1, 5, 6))) inp = tf.cast(inp, dtype=tf.float32) gru = tf.keras.layers.GRU(3, return_sequences=True)(inp) print(gru.shape) #(1, 5, 3) dense = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init) print(dense(gru)) #tf.Tensor( #[[[ 1.5456871 -0.5280464 ] # [ 0.11647969 -0.20553198] # [ 0.58126366 -0.16031623] # [-0.22882831 -0.22649539] # [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32) for w in dense.weights: print(w.shape) #(3, 2) instead of (5 * 3, 2) if Dense indeed flattened its input #(2,) tddense = tf.keras.layers.TimeDistributed(dense) print(tddense(gru)) #tf.Tensor( #[[[ 1.5456871 -0.5280464 ] # [ 0.11647969 -0.20553198] # [ 0.58126366 -0.16031623] # [-0.22882831 -0.22649539] # [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32) # if Dense kernel had shape (15, 2), this should result in the following error: # InvalidArgumentError: Matrix size-incompatible: In[0]: [5,3], In[1]: [15,2] [Op:MatMul] # but instead what we get is the same output # than without TimeDistributed, without error dense2 = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init) tddense = tf.keras.layers.TimeDistributed(dense2) print(tddense(gru)) #tf.Tensor( #[[[ 1.5456871 -0.5280464 ] # [ 0.11647969 -0.20553198] # [ 0.58126366 -0.16031623] # [-0.22882831 -0.22649539] # [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32) for w in dense2.weights: print(w.shape) #(3, 2) as expected #(2,)\n\nSecond example, with a rank even larger than 3:\n\nimport tensorflow as tf print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION)) inp = tf.keras.Input(shape=(10, 25, 25, 3)) dense_layer1 = tf.keras.layers.Dense(78) x = dense_layer1(inp) print('Output shape without TimeDistributed:') print(x.shape) dense_layer2 = tf.keras.layers.Dense(78) y=tf.keras.layers.TimeDistributed(dense_layer2)(inp) print('Output shape with TimeDistributed:') print(y.shape) print('Weight shapes without TimeDistributed:') for weight in dense_layer1.trainable_weights: if len(weight.shape) == 2: print(' kernel shape:') else: print(' bias shape:') print(weight.shape) print('Weight shapes with TimeDistributed:') for weight in dense_layer2.trainable_weights: if len(weight.shape) == 2: print(' kernel shape:') else: print(' bias shape:') print(weight.shape)\n\nUsing Tensorflow version 2.0.0-beta1 (git version v2.0.0-beta0-16-g1d91213fe7) Output shape without TimeDistributed: (None, 10, 25, 25, 78) Output shape with TimeDistributed: (None, 10, 25, 25, 78) Weight shapes without TimeDistributed: kernel shape: (3, 78) bias shape: (78,) Weight shapes with TimeDistributed: kernel shape: (3, 78) bias shape: (78,)\n\nWe see, in this example, that Dense and TimeDistributed(Dense) behave the same in that they only touch to the last dimension of the input.\n\nThe text was updated successfully, but these errors were encountered:\n\noanush self-assigned this\n\noanush added 2.0.0-beta0 comp:keras\n\nKeras related issues type:bug\n\nIssue replicating with TF version-2.0.0beta1, please find the gist of collab. Thanks!\n\noanush assigned ymodak and unassigned oanush\n\n@anush-o It says access denied. Please change your settings to enable it.\n\nymodak assigned pavithrasv and unassigned ymodak\n\nThe issue is still here in tf2.0.0.\n\njvishnuvardhan added TF 2.0\n\nIssues relating to TensorFlow 2.0 and removed TF 2.0.0-beta0 labels\n\npavithrasv commented\n\nThe note is incorrect, we will update the note to reflect the code behavior.\n\npavithrasv commented\n\nCommit 2e6a3c5#diff-5fb1fa5fa46d0ec9a01d5a60b7d8acc8\n\npavithrasv closed this as completed\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nI appreciate how the new note is comprehensive @pavithrasv .\n\nBut now I don't get the purpose of TimeDistributed anymore, as I don't see the difference between Dense and TimeDistributed(Dense).\n\npavithrasv commented\n\nThey are the same, TimeDistributed doesn't just apply to Dense layers but i see that the main example in the TimeDistributed docs is using Dense layer. i'll update that.\n\npavithrasv commented\n\nThe change to TimeDistributed docs have also been submitted. Thank you!\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nYou can’t perform that action at this time."", 'timestamp': '2024-03-21T16:20:20', 'title': 'Dense does not flatten inputs with rank >2 and behaves exactly like TimeDistributed(Dense) · Issue #30882 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/30882'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\n""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 #10736\n\nstiffme opened this issue\n\nJul 20, 2018 · 3 comments\n\n""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 #10736\n\nstiffme opened this issue\n\nJul 20, 2018 · 3 comments\n\ntype:bug/performance type:docs\n\nNeed to modify the documentation\n\nKeras : 2.2.0 Tensorflow-GPU: 1.8.0/ 1.9.0 (Both versions have the same issue)\n\nProblem: When using an input with rank greater than 2 in a Dense layer, ""Dimensions must be equal"" exception will be thrown from Tensorflow.\n\nIf using use_bias=False in Dense layer, it is OK. If using Reshape before supply to Dense layer, it is OK.\n\nWith above symptons , it seems something is wrong when handling the bias with an input of rank size over 2. According to the documents: Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nHere is the short script to reproduce the issue:\n\nfrom keras import Input, Model from keras.layers import Dense, Reshape def build_model(): X = Input(shape=(30, 40)) output = Dense(10, activation=\'tanh\')(X) model = Model(inputs=X, outputs=output) def main(): build_model() if __name__ == ""__main__"": main()\n\nHere is the exception:\n\nUsing TensorFlow backend. Traceback (most recent call last): File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1567, in _create_c_op c_op = c_api.TF_FinishOperation(op_desc) tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 30 and 10 for \'dense_1/add\' (op: \'Add\') with input shapes: [?,30,10], [1,10,1]. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 15, in <module> main() File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 11, in main build_model() File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 6, in build_model output = Dense(10, activation=\'tanh\')(X) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/engine/topology.py"", line 619, in __call__ output = self.call(inputs, **kwargs) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/layers/core.py"", line 879, in call output = K.bias_add(output, self.bias) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 3781, in bias_add x += reshape(bias, (1, bias_shape[0], 1)) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 979, in binary_op_wrapper return func(x, y, name=name) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 297, in add ""Add"", x=x, y=y, name=name) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper op_def=op_def) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op op_def=op_def) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1734, in __init__ control_input_ops) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1570, in _create_c_op raise ValueError(str(e)) ValueError: Dimensions must be equal, but are 30 and 10 for \'dense_1/add\' (op: \'Add\') with input shapes: [?,30,10], [1,10,1].\n\nThe text was updated successfully, but these errors were encountered:\n\nContrary to the documentation, we don\'t actually flatten it. It\'s applied on the last axis independently.\n\nWe have a couple of options, but they some are breaking changes.\n\nUpdate the doc, making it clear that it\'s not actually flatten\n\nDon\'t allow this behavior (Deprecate it), forcing the user to use TimeDistributed\n\nActually Flatten as the doc says, but it\'s a breaking change.\n\nThis is an API breaking change so what do you think @fchollet?\n\nDref360 added type:bug/performance type:docs\n\nNeed to modify the documentation labels\n\ntRosenflanz commented\n\nSo is it true that current behaviour of Dense layer is identical to TimeDistributed(Dense) for rank 3 tensors but is different for rank 4+ ? Although it is up-to François, I as a user could get behind 2nd option especially if TimeDistributed wrapper could be reworked to have axis argument that defaults to 1 (won\'t break existing behaviour of TimeDistributed wrapper) but can be changed to higher values to match current Dense behaviour\n\nSriRangaTarun mentioned this issue\n\nUpdate Dense Layer documentation in core.py #12111\n\nfchollet closed this as completed\n\nalex-lt-kong commented\n\nHi @Dref360 , I encountered the same issue and after reading what you said I am still a bit confused. By saying It\'s applied on the last axis independently., my understanding is this:\n\nSuppose my samples are all 5x2 matrices and I have a Dense layer with 8 neurons：\n\nthe layer will be applied to each sample 5 times;\n\neach time, the layer is applied to a 1x2 vector, i.e., it fully connects the 2 input nodes to 8 output nodes;\n\nthe output from each application is a 1x8 vector;\n\nthe final output is a 5x8 matrix/tensor\n\nIs this understanding correct?\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\ntype:bug/performance type:docs\n\nNeed to modify the documentation\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_4', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\n""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 #10736\n\nstiffme opened this issue\n\nJul 20, 2018 · 3 comments\n\n""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 #10736\n\nstiffme opened this issue\n\nJul 20, 2018 · 3 comments\n\ntype:bug/performance type:docs\n\nNeed to modify the documentation\n\nKeras : 2.2.0 Tensorflow-GPU: 1.8.0/ 1.9.0 (Both versions have the same issue)\n\nProblem: When using an input with rank greater than 2 in a Dense layer, ""Dimensions must be equal"" exception will be thrown from Tensorflow.\n\nIf using use_bias=False in Dense layer, it is OK. If using Reshape before supply to Dense layer, it is OK.\n\nWith above symptons , it seems something is wrong when handling the bias with an input of rank size over 2. According to the documents: Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\n\nHere is the short script to reproduce the issue:\n\nfrom keras import Input, Model from keras.layers import Dense, Reshape def build_model(): X = Input(shape=(30, 40)) output = Dense(10, activation=\'tanh\')(X) model = Model(inputs=X, outputs=output) def main(): build_model() if __name__ == ""__main__"": main()\n\nHere is the exception:\n\nUsing TensorFlow backend. Traceback (most recent call last): File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1567, in _create_c_op c_op = c_api.TF_FinishOperation(op_desc) tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 30 and 10 for \'dense_1/add\' (op: \'Add\') with input shapes: [?,30,10], [1,10,1]. During handling of the above exception, another exception occurred: Traceback (most recent call last): File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 15, in <module> main() File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 11, in main build_model() File ""/home/stiffme/PycharmProjects/Dense_Concate/main.py"", line 6, in build_model output = Dense(10, activation=\'tanh\')(X) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/engine/topology.py"", line 619, in __call__ output = self.call(inputs, **kwargs) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/layers/core.py"", line 879, in call output = K.bias_add(output, self.bias) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 3781, in bias_add x += reshape(bias, (1, bias_shape[0], 1)) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 979, in binary_op_wrapper return func(x, y, name=name) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 297, in add ""Add"", x=x, y=y, name=name) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper op_def=op_def) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op op_def=op_def) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1734, in __init__ control_input_ops) File ""/home/stiffme/PycharmProjects/neural_network_transfer/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1570, in _create_c_op raise ValueError(str(e)) ValueError: Dimensions must be equal, but are 30 and 10 for \'dense_1/add\' (op: \'Add\') with input shapes: [?,30,10], [1,10,1].\n\nThe text was updated successfully, but these errors were encountered:\n\nContrary to the documentation, we don\'t actually flatten it. It\'s applied on the last axis independently.\n\nWe have a couple of options, but they some are breaking changes.\n\nUpdate the doc, making it clear that it\'s not actually flatten\n\nDon\'t allow this behavior (Deprecate it), forcing the user to use TimeDistributed\n\nActually Flatten as the doc says, but it\'s a breaking change.\n\nThis is an API breaking change so what do you think @fchollet?\n\nDref360 added type:bug/performance type:docs\n\nNeed to modify the documentation labels\n\ntRosenflanz commented\n\nSo is it true that current behaviour of Dense layer is identical to TimeDistributed(Dense) for rank 3 tensors but is different for rank 4+ ? Although it is up-to François, I as a user could get behind 2nd option especially if TimeDistributed wrapper could be reworked to have axis argument that defaults to 1 (won\'t break existing behaviour of TimeDistributed wrapper) but can be changed to higher values to match current Dense behaviour\n\nSriRangaTarun mentioned this issue\n\nUpdate Dense Layer documentation in core.py #12111\n\nfchollet closed this as completed\n\nalex-lt-kong commented\n\nHi @Dref360 , I encountered the same issue and after reading what you said I am still a bit confused. By saying It\'s applied on the last axis independently., my understanding is this:\n\nSuppose my samples are all 5x2 matrices and I have a Dense layer with 8 neurons：\n\nthe layer will be applied to each sample 5 times;\n\neach time, the layer is applied to a 1x2 vector, i.e., it fully connects the 2 input nodes to 8 output nodes;\n\nthe output from each application is a 1x8 vector;\n\nthe final output is a 5x8 matrix/tensor\n\nIs this understanding correct?\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\ntype:bug/performance type:docs\n\nNeed to modify the documentation\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-02T09:18:55', 'title': '""Dimensions must be equal"" exception when using Dense layer on input with a rank greater than 2 · Issue #10736 · keras-team/keras', 'url': 'https://github.com/keras-team/keras/issues/10736'})], [Document(page_content='中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', metadata={'id': 'web-search_1', 'snippet': '中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', 'timestamp': '2024-07-09T08:43:35', 'title': 'tf.keras.layers.Dense | TensorFlow v2.15.0.post1', 'url': 'https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense'}), Document(page_content='中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', metadata={'id': 'web-search_4', 'snippet': '中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', 'timestamp': '2024-07-09T08:43:35', 'title': 'tf.keras.layers.Dense | TensorFlow v2.15.0.post1', 'url': 'https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense'}), Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nThe Sequential model\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport tensorflow as tf import keras from keras import layers\n\nWhen to use a Sequential model\n\nA Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nSchematically, the following Sequential model:\n\n# Define Sequential model with 3 layers model = keras.Sequential( [ layers.Dense(2, activation=""relu"", name=""layer1""), layers.Dense(3, activation=""relu"", name=""layer2""), layers.Dense(4, name=""layer3""), ] ) # Call model on a test input x = tf.ones((3, 3)) y = model(x)\n\nis equivalent to this function:\n\n# Create 3 layers layer1 = layers.Dense(2, activation=""relu"", name=""layer1"") layer2 = layers.Dense(3, activation=""relu"", name=""layer2"") layer3 = layers.Dense(4, name=""layer3"") # Call layers on a test input x = tf.ones((3, 3)) y = layer3(layer2(layer1(x)))\n\nA Sequential model is not appropriate when:\n\nYour model has multiple inputs or multiple outputs\n\nAny of your layers has multiple inputs or multiple outputs\n\nYou need to do layer sharing\n\nYou want non-linear topology (e.g. a residual connection, a multi-branch model)\n\nCreating a Sequential model\n\nYou can create a Sequential model by passing a list of layers to the Sequential constructor:\n\nmodel = keras.Sequential( [ layers.Dense(2, activation=""relu""), layers.Dense(3, activation=""relu""), layers.Dense(4), ] )\n\nIts layers are accessible via the layers attribute:\n\n[<keras.src.layers.core.dense.Dense at 0x7fa3c8de0100>, <keras.src.layers.core.dense.Dense at 0x7fa3c8de09a0>, <keras.src.layers.core.dense.Dense at 0x7fa5181b5c10>]\n\nYou can also create a Sequential model incrementally via the add() method:\n\nmodel = keras.Sequential() model.add(layers.Dense(2, activation=""relu"")) model.add(layers.Dense(3, activation=""relu"")) model.add(layers.Dense(4))\n\nNote that there\'s also a corresponding pop() method to remove layers: a Sequential model behaves very much like a list of layers.\n\nmodel.pop() print(len(model.layers)) # 2\n\nAlso note that the Sequential constructor accepts a name argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nmodel = keras.Sequential(name=""my_sequential"") model.add(layers.Dense(2, activation=""relu"", name=""layer1"")) model.add(layers.Dense(3, activation=""relu"", name=""layer2"")) model.add(layers.Dense(4, name=""layer3""))\n\nSpecifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nlayer = layers.Dense(3) layer.weights # Empty\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\n# Call layer on a test input x = tf.ones((1, 4)) y = layer(x) layer.weights # Now it has weights, of shape (4, 3) and (3,)\n\n[<tf.Variable \'dense_6/kernel:0\' shape=(4, 3) dtype=float32, numpy= array([[ 0.1752373 , 0.47623062, 0.24374962], [-0.0298934 , 0.50255656, 0.78478384], [-0.58323103, -0.56861055, -0.7190975 ], [-0.3191281 , -0.23635858, -0.8841506 ]], dtype=float32)>, <tf.Variable \'dense_6/bias:0\' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn\'t ""built"": it has no weights (and calling model.weights results in an error stating just this). The weights are created when the model first sees some input data:\n\nmodel = keras.Sequential( [ layers.Dense(2, activation=""relu""), layers.Dense(3, activation=""relu""), layers.Dense(4), ] ) # No weights at this stage! # At this point, you can\'t do this: # model.weights # You also can\'t do this: # model.summary() # Call the model on a test input x = tf.ones((1, 4)) y = model(x) print(""Number of weights after calling the model:"", len(model.weights)) # 6\n\nNumber of weights after calling the model: 6\n\nOnce a model is ""built"", you can call its summary() method to display its contents:\n\nModel: ""sequential_3"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_7 (Dense) (1, 2) 10 dense_8 (Dense) (1, 3) 9 dense_9 (Dense) (1, 4) 16 ================================================================= Total params: 35 (140.00 Byte) Trainable params: 35 (140.00 Byte) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an Input object to your model, so that it knows its input shape from the start:\n\nmodel = keras.Sequential() model.add(keras.Input(shape=(4,))) model.add(layers.Dense(2, activation=""relu"")) model.summary()\n\nModel: ""sequential_4"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 2) 10 ================================================================= Total params: 10 (40.00 Byte) Trainable params: 10 (40.00 Byte) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nNote that the Input object is not displayed as part of model.layers, since it isn\'t a layer:\n\n[<keras.src.layers.core.dense.Dense at 0x7fa3bc0ba820>]\n\nA simple alternative is to just pass an input_shape argument to your first layer:\n\nmodel = keras.Sequential() model.add(layers.Dense(2, activation=""relu"", input_shape=(4,))) model.summary()\n\nModel: ""sequential_5"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_11 (Dense) (None, 2) 10 ================================================================= Total params: 10 (40.00 Byte) Trainable params: 10 (40.00 Byte) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\n\nIn general, it\'s a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.\n\nA common debugging workflow: add() + summary()\n\nWhen building a new Sequential architecture, it\'s useful to incrementally stack layers with add() and frequently print model summaries. For instance, this enables you to monitor how a stack of Conv2D and MaxPooling2D layers is downsampling image feature maps:\n\nmodel = keras.Sequential() model.add(keras.Input(shape=(250, 250, 3))) # 250x250 RGB images model.add(layers.Conv2D(32, 5, strides=2, activation=""relu"")) model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.MaxPooling2D(3)) # Can you guess what the current output shape is at this point? Probably not. # Let\'s just print it: model.summary() # The answer was: (40, 40, 32), so we can keep downsampling... model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.MaxPooling2D(3)) model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.MaxPooling2D(2)) # And now? model.summary() # Now that we have 4x4 feature maps, time to apply global max pooling. model.add(layers.GlobalMaxPooling2D()) # Finally, we add a classification layer. model.add(layers.Dense(10))\n\nModel: ""sequential_6"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 123, 123, 32) 2432 conv2d_1 (Conv2D) (None, 121, 121, 32) 9248 max_pooling2d (MaxPooling2 (None, 40, 40, 32) 0 D) ================================================================= Total params: 11680 (45.62 KB) Trainable params: 11680 (45.62 KB) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________ Model: ""sequential_6"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 123, 123, 32) 2432 conv2d_1 (Conv2D) (None, 121, 121, 32) 9248 max_pooling2d (MaxPooling2 (None, 40, 40, 32) 0 D) conv2d_2 (Conv2D) (None, 38, 38, 32) 9248 conv2d_3 (Conv2D) (None, 36, 36, 32) 9248 max_pooling2d_1 (MaxPoolin (None, 12, 12, 32) 0 g2D) conv2d_4 (Conv2D) (None, 10, 10, 32) 9248 conv2d_5 (Conv2D) (None, 8, 8, 32) 9248 max_pooling2d_2 (MaxPoolin (None, 4, 4, 32) 0 g2D) ================================================================= Total params: 48672 (190.12 KB) Trainable params: 48672 (190.12 KB) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nVery practical, right?\n\nWhat to do once you have a model\n\nOnce your model architecture is ready, you will want to:\n\nTrain your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\n\nSave your model to disk and restore it. See our guide to serialization & saving.\n\nSpeed up model training by leveraging multiple GPUs. See our guide to multi-GPU and distributed training.\n\nFeature extraction with a Sequential model\n\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an input and output attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\ninitial_model = keras.Sequential( [ keras.Input(shape=(250, 250, 3)), layers.Conv2D(32, 5, strides=2, activation=""relu""), layers.Conv2D(32, 3, activation=""relu""), layers.Conv2D(32, 3, activation=""relu""), ] ) feature_extractor = keras.Model( inputs=initial_model.inputs, outputs=[layer.output for layer in initial_model.layers], ) # Call feature extractor on test input. x = tf.ones((1, 250, 250, 3)) features = feature_extractor(x)\n\nHere\'s a similar example that only extract features from one layer:\n\ninitial_model = keras.Sequential( [ keras.Input(shape=(250, 250, 3)), layers.Conv2D(32, 5, strides=2, activation=""relu""), layers.Conv2D(32, 3, activation=""relu"", name=""my_intermediate_layer""), layers.Conv2D(32, 3, activation=""relu""), ] ) feature_extractor = keras.Model( inputs=initial_model.inputs, outputs=initial_model.get_layer(name=""my_intermediate_layer"").output, ) # Call feature extractor on test input. x = tf.ones((1, 250, 250, 3)) features = feature_extractor(x)\n\nTransfer learning with a Sequential model\n\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren\'t familiar with it, make sure to read our guide to transfer learning.\n\nHere are two common transfer learning blueprint involving Sequential models.\n\nFirst, let\'s say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over model.layers and set layer.trainable = False on each layer, except the last one. Like this:\n\nmodel = keras.Sequential([ keras.Input(shape=(784)), layers.Dense(32, activation=\'relu\'), layers.Dense(32, activation=\'relu\'), layers.Dense(32, activation=\'relu\'), layers.Dense(10), ]) # Presumably you would want to first load pre-trained weights. model.load_weights(...) # Freeze all layers except the last one. for layer in model.layers[:-1]: layer.trainable = False # Recompile and train (this will only update the weights of the last layer). model.compile(...) model.fit(...)\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:\n\n# Load a convolutional base with pre-trained weights base_model = keras.applications.Xception( weights=\'imagenet\', include_top=False, pooling=\'avg\') # Freeze the base model base_model.trainable = False # Use a Sequential model to add a trainable classifier on top model = keras.Sequential([ base_model, layers.Dense(1000), ]) # Compile & train model.compile(...) model.fit(...)\n\nIf you do transfer learning, you will probably find yourself frequently using these two patterns.\n\nThat\'s about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\n\nGuide to the Functional API\n\nGuide to making new Layers & Models via subclassing\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-07-24 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_5', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nThe Sequential model\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport tensorflow as tf import keras from keras import layers\n\nWhen to use a Sequential model\n\nA Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nSchematically, the following Sequential model:\n\n# Define Sequential model with 3 layers model = keras.Sequential( [ layers.Dense(2, activation=""relu"", name=""layer1""), layers.Dense(3, activation=""relu"", name=""layer2""), layers.Dense(4, name=""layer3""), ] ) # Call model on a test input x = tf.ones((3, 3)) y = model(x)\n\nis equivalent to this function:\n\n# Create 3 layers layer1 = layers.Dense(2, activation=""relu"", name=""layer1"") layer2 = layers.Dense(3, activation=""relu"", name=""layer2"") layer3 = layers.Dense(4, name=""layer3"") # Call layers on a test input x = tf.ones((3, 3)) y = layer3(layer2(layer1(x)))\n\nA Sequential model is not appropriate when:\n\nYour model has multiple inputs or multiple outputs\n\nAny of your layers has multiple inputs or multiple outputs\n\nYou need to do layer sharing\n\nYou want non-linear topology (e.g. a residual connection, a multi-branch model)\n\nCreating a Sequential model\n\nYou can create a Sequential model by passing a list of layers to the Sequential constructor:\n\nmodel = keras.Sequential( [ layers.Dense(2, activation=""relu""), layers.Dense(3, activation=""relu""), layers.Dense(4), ] )\n\nIts layers are accessible via the layers attribute:\n\n[<keras.src.layers.core.dense.Dense at 0x7fa3c8de0100>, <keras.src.layers.core.dense.Dense at 0x7fa3c8de09a0>, <keras.src.layers.core.dense.Dense at 0x7fa5181b5c10>]\n\nYou can also create a Sequential model incrementally via the add() method:\n\nmodel = keras.Sequential() model.add(layers.Dense(2, activation=""relu"")) model.add(layers.Dense(3, activation=""relu"")) model.add(layers.Dense(4))\n\nNote that there\'s also a corresponding pop() method to remove layers: a Sequential model behaves very much like a list of layers.\n\nmodel.pop() print(len(model.layers)) # 2\n\nAlso note that the Sequential constructor accepts a name argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nmodel = keras.Sequential(name=""my_sequential"") model.add(layers.Dense(2, activation=""relu"", name=""layer1"")) model.add(layers.Dense(3, activation=""relu"", name=""layer2"")) model.add(layers.Dense(4, name=""layer3""))\n\nSpecifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nlayer = layers.Dense(3) layer.weights # Empty\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\n# Call layer on a test input x = tf.ones((1, 4)) y = layer(x) layer.weights # Now it has weights, of shape (4, 3) and (3,)\n\n[<tf.Variable \'dense_6/kernel:0\' shape=(4, 3) dtype=float32, numpy= array([[ 0.1752373 , 0.47623062, 0.24374962], [-0.0298934 , 0.50255656, 0.78478384], [-0.58323103, -0.56861055, -0.7190975 ], [-0.3191281 , -0.23635858, -0.8841506 ]], dtype=float32)>, <tf.Variable \'dense_6/bias:0\' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn\'t ""built"": it has no weights (and calling model.weights results in an error stating just this). The weights are created when the model first sees some input data:\n\nmodel = keras.Sequential( [ layers.Dense(2, activation=""relu""), layers.Dense(3, activation=""relu""), layers.Dense(4), ] ) # No weights at this stage! # At this point, you can\'t do this: # model.weights # You also can\'t do this: # model.summary() # Call the model on a test input x = tf.ones((1, 4)) y = model(x) print(""Number of weights after calling the model:"", len(model.weights)) # 6\n\nNumber of weights after calling the model: 6\n\nOnce a model is ""built"", you can call its summary() method to display its contents:\n\nModel: ""sequential_3"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_7 (Dense) (1, 2) 10 dense_8 (Dense) (1, 3) 9 dense_9 (Dense) (1, 4) 16 ================================================================= Total params: 35 (140.00 Byte) Trainable params: 35 (140.00 Byte) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an Input object to your model, so that it knows its input shape from the start:\n\nmodel = keras.Sequential() model.add(keras.Input(shape=(4,))) model.add(layers.Dense(2, activation=""relu"")) model.summary()\n\nModel: ""sequential_4"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 2) 10 ================================================================= Total params: 10 (40.00 Byte) Trainable params: 10 (40.00 Byte) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nNote that the Input object is not displayed as part of model.layers, since it isn\'t a layer:\n\n[<keras.src.layers.core.dense.Dense at 0x7fa3bc0ba820>]\n\nA simple alternative is to just pass an input_shape argument to your first layer:\n\nmodel = keras.Sequential() model.add(layers.Dense(2, activation=""relu"", input_shape=(4,))) model.summary()\n\nModel: ""sequential_5"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_11 (Dense) (None, 2) 10 ================================================================= Total params: 10 (40.00 Byte) Trainable params: 10 (40.00 Byte) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\n\nIn general, it\'s a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.\n\nA common debugging workflow: add() + summary()\n\nWhen building a new Sequential architecture, it\'s useful to incrementally stack layers with add() and frequently print model summaries. For instance, this enables you to monitor how a stack of Conv2D and MaxPooling2D layers is downsampling image feature maps:\n\nmodel = keras.Sequential() model.add(keras.Input(shape=(250, 250, 3))) # 250x250 RGB images model.add(layers.Conv2D(32, 5, strides=2, activation=""relu"")) model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.MaxPooling2D(3)) # Can you guess what the current output shape is at this point? Probably not. # Let\'s just print it: model.summary() # The answer was: (40, 40, 32), so we can keep downsampling... model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.MaxPooling2D(3)) model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.Conv2D(32, 3, activation=""relu"")) model.add(layers.MaxPooling2D(2)) # And now? model.summary() # Now that we have 4x4 feature maps, time to apply global max pooling. model.add(layers.GlobalMaxPooling2D()) # Finally, we add a classification layer. model.add(layers.Dense(10))\n\nModel: ""sequential_6"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 123, 123, 32) 2432 conv2d_1 (Conv2D) (None, 121, 121, 32) 9248 max_pooling2d (MaxPooling2 (None, 40, 40, 32) 0 D) ================================================================= Total params: 11680 (45.62 KB) Trainable params: 11680 (45.62 KB) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________ Model: ""sequential_6"" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 123, 123, 32) 2432 conv2d_1 (Conv2D) (None, 121, 121, 32) 9248 max_pooling2d (MaxPooling2 (None, 40, 40, 32) 0 D) conv2d_2 (Conv2D) (None, 38, 38, 32) 9248 conv2d_3 (Conv2D) (None, 36, 36, 32) 9248 max_pooling2d_1 (MaxPoolin (None, 12, 12, 32) 0 g2D) conv2d_4 (Conv2D) (None, 10, 10, 32) 9248 conv2d_5 (Conv2D) (None, 8, 8, 32) 9248 max_pooling2d_2 (MaxPoolin (None, 4, 4, 32) 0 g2D) ================================================================= Total params: 48672 (190.12 KB) Trainable params: 48672 (190.12 KB) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________\n\nVery practical, right?\n\nWhat to do once you have a model\n\nOnce your model architecture is ready, you will want to:\n\nTrain your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\n\nSave your model to disk and restore it. See our guide to serialization & saving.\n\nSpeed up model training by leveraging multiple GPUs. See our guide to multi-GPU and distributed training.\n\nFeature extraction with a Sequential model\n\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an input and output attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\ninitial_model = keras.Sequential( [ keras.Input(shape=(250, 250, 3)), layers.Conv2D(32, 5, strides=2, activation=""relu""), layers.Conv2D(32, 3, activation=""relu""), layers.Conv2D(32, 3, activation=""relu""), ] ) feature_extractor = keras.Model( inputs=initial_model.inputs, outputs=[layer.output for layer in initial_model.layers], ) # Call feature extractor on test input. x = tf.ones((1, 250, 250, 3)) features = feature_extractor(x)\n\nHere\'s a similar example that only extract features from one layer:\n\ninitial_model = keras.Sequential( [ keras.Input(shape=(250, 250, 3)), layers.Conv2D(32, 5, strides=2, activation=""relu""), layers.Conv2D(32, 3, activation=""relu"", name=""my_intermediate_layer""), layers.Conv2D(32, 3, activation=""relu""), ] ) feature_extractor = keras.Model( inputs=initial_model.inputs, outputs=initial_model.get_layer(name=""my_intermediate_layer"").output, ) # Call feature extractor on test input. x = tf.ones((1, 250, 250, 3)) features = feature_extractor(x)\n\nTransfer learning with a Sequential model\n\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren\'t familiar with it, make sure to read our guide to transfer learning.\n\nHere are two common transfer learning blueprint involving Sequential models.\n\nFirst, let\'s say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over model.layers and set layer.trainable = False on each layer, except the last one. Like this:\n\nmodel = keras.Sequential([ keras.Input(shape=(784)), layers.Dense(32, activation=\'relu\'), layers.Dense(32, activation=\'relu\'), layers.Dense(32, activation=\'relu\'), layers.Dense(10), ]) # Presumably you would want to first load pre-trained weights. model.load_weights(...) # Freeze all layers except the last one. for layer in model.layers[:-1]: layer.trainable = False # Recompile and train (this will only update the weights of the last layer). model.compile(...) model.fit(...)\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:\n\n# Load a convolutional base with pre-trained weights base_model = keras.applications.Xception( weights=\'imagenet\', include_top=False, pooling=\'avg\') # Freeze the base model base_model.trainable = False # Use a Sequential model to add a trainable classifier on top model = keras.Sequential([ base_model, layers.Dense(1000), ]) # Compile & train model.compile(...) model.fit(...)\n\nIf you do transfer learning, you will probably find yourself frequently using these two patterns.\n\nThat\'s about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\n\nGuide to the Functional API\n\nGuide to making new Layers & Models via subclassing\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-07-24 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-07-04T12:25:46', 'title': 'The Sequential model | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/keras/sequential_model'})]]??"
53079436,tf.cond,"{'https://www.coursera.org/learn/intro-tensorflow', 'https://www.coursera.org/learn/neural-networks-random-forests', 'https://www.coursera.org/learn/probabilistic-deep-learning-with-tensorflow2', 'https://www.coursera.org/learn/introduction-tensorflow'}","{'https://www.youtube.com/watch?v=YAJIIfW_OtA', 'https://www.youtube.com/watch?v=7SycWyHk_Bw', 'https://www.youtube.com/watch?v=DKQDvnKlwig', 'https://www.youtube.com/watch?v=WImsHaQci70'}",{'https://stackoverflow.com/questions/53079436/tensorflow-tf-cond-giving-unexpected-output'},"??[[Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\ntensorflow Tf.cond giving unexpected output\n\nAsked 4 years, 5 months ago\n\nModified 4 years, 5 months ago\n\nI seem to be having a misunderstanding on how tf.cond works. In the tensorflow documentation, it gives the following example:\n\nz = tf.multiply(a, b) result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))\n\nThe result of the example, if x<y is True is tf.add(x,z) else tf.square(y)\n\nFollowing this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation.\n\nin my example, deterministic_action = 4, random_action = 11, chose_random=False. The stochastic_action should be 4, instead it is 1. Where did the value 1 come from?\n\n#!/usr/bin/env python3 import tensorflow as tf import numpy as np with tf.Graph().as_default(): with tf.device(\'/cpu:0\'): stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"") eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0)) with tf.variable_scope(\'test_cond\') as sc: deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4 random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11 chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) < eps # False because eps = 0 stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1 #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action) init = tf.global_variables_initializer() sess = tf.Session() sess.run(init, feed_dict={stochastic_ph: True}) print (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) r_action = sess.run(random_action) print (""rand_action= "", r_action) e = sess.run(eps) c_action = sess.run(chose_random) print (""chose_rand= "", c_action) s_action = sess.run(stochastic_action) print (""s_action= "", s_action) #output = sess.run(output_action)\n\npython random_vec.py 2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA s_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 1\n\nImprove this question\n\nedited Oct 31, 2018 at 9:15\n\nasked Oct 31, 2018 at 8:46\n\n2,04044 gold badges2525 silver badges5050 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThis is because you are evaluating again in a new sess.run. Since you are generating a random number for deterministic_action, the result turns out to be the next random number after 4, which is 1. Here is the result of your code, when I extract the value of deterministic_action as well in the last step.\n\nprint (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) r_action = sess.run(random_action) print (""rand_action= "", r_action) e = sess.run(eps) c_action = sess.run(chose_random) print (""chose_rand= "", c_action) s_action, d_action = sess.run([stochastic_action, deterministic_action]) print (""s_action= "", s_action) print (""det_action= "", d_action)\n\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 1 det_action= 1\n\nNow all you need to do is run everything in one sess.run\n\nd_action, r_action, e, c_action, s_action = sess.run([deterministic_action, random_action, eps, chose_random, stochastic_action]) print (""det_action= "", d_action) print (""rand_action= "", r_action) print (""chose_rand= "", c_action) print (""s_action= "", s_action)\n\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 4\n\nI was not clear on why the random_uniform generates different values when seed is set. This is because the code is running with the same session object that it initialized the variables with. Modifying the code with a new session object, this is what happens:\n\nprint (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) sess.close() sess = tf.Session() sess.run(init, feed_dict={stochastic_ph: True}) s_action = sess.run(stochastic_action) print (""s_action= "", s_action)\n\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 s_action= 4\n\nedited Nov 1, 2018 at 13:17\n\nanswered Oct 31, 2018 at 13:27\n\n99211 gold badge88 silver badges1212 bronze badges 4\n\nThe seed is fixed. So, irrespective of the session you run in, the random value should always be the same.\n\n– tandem Oct 31, 2018 at 14:50\n\n@tandem the sequence of numbers you get would be the same, but not the value every time. If you notice my first result, where I\'m just reading out the value of deterministic_action along with stochastic_action that you are getting in your last step. Your deterministic_action is 1, which means your result is also 1. When I put all the evaluations together in a single session run, your deterministic_action is 4, and hence your result is also 4.\n\n– kvish Oct 31, 2018 at 14:58\n\nSeed should always give the same random number. as long as it is set. That explanation doesnt still make sense\n\n– tandem Nov 1, 2018 at 6:53\n\n@tandem tensorflow computation graphs depend on your initialization. In the above example, you are still using the session object that is initialized at the beginning. This means tensorflow is keeping a track of the state of its variables in the session object, and thus does not reinitialize the random_uniform. Close the session object with sess.close() and then open a new session object and run your stochastic_action variable, it will produce the result 4 as expected.\n\n– kvish Nov 1, 2018 at 13:06\n\nNot the answer you\'re looking for? Browse other questions tagged\n\npython-3.6 or ask your own question.\n\nGoing stateless with authorization-as-a-service (Ep. 553)\n\nAre meetings making you less productive?\n\nImproving the copy in the close modal and post notices - 2023 edition\n\nPlagiarism flag and moderator tooling has launched to Stack Overflow!\n\nTemporary policy: ChatGPT is banned\n\nShould we burninate the [protection] tag?\n\nDo you observe increased relevance of Related Questions with our Machine...\n\n337 Disable Tensorflow debugging information\n\n297 What\'s the difference of name scope and a variable scope in tensorflow?\n\n428 What is the difference between \'SAME\' and \'VALID\' padding in tf.nn.max_pool of tensorflow?\n\n702 TensorFlow not found using pip\n\n6 Simple Feedforward Neural Network with TensorFlow won\'t learn\n\n1 TensorFlow multi-GPU InvalidArgumentError : cifar10_multi_gpu.py\n\n768 Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\n\n2 TF/keras subclass works perfectly in Eager Execution, and throws a massive untraceable error without it?\n\nHot Network Questions\n\nDid/do the dinosaurs in Jurassic Park reproduce asexually or did some turn into males?\n\nHow can I find the nature of a turning point?\n\nCan I tell DeleteCases not to delete function arguments?\n\nHard sci fi novel that ends with vast civilization ships all cruising in a line toward the same destination in the galaxy\n\nDoes my passport need to be stamped while re-entering Schengen area?\n\nStop stdin while command is running\n\nUnreadable Russian TeX files\n\nWhy are there such low rates of acceptance in AI/ML conferences?\n\nWould it benefit me to file taxes this year?\n\nDoes the computational theory of mind explain anything?\n\nWhat kind of fallacy is it to say if abolition of something isn\'t possible, we shouldn\'t attempt to address it at all?\n\n""Why"" do animals excrete excess nitrogen instead of recycling it?\n\nPID output at 0 error\n\nHow to draw a diagram without using graphics\n\nCreating a Table on Tex Stack Exchange Question Editor\n\nDecline promotion because of teaching load\n\nGood / recommended way to archive fastq and bam files?\n\nMeaning of ""water, the weight of which is one-eighth hydrogen""\n\nHow to analyze this circuit with an op-amp and positive feedback?\n\nWhere do I send a nomination for the Presidential Medal of Freedom?\n\nWhy do we insist that the electron be a point particle when calculation shows it creates an electrostatic field of infinite energy?\n\nC++ Binary Mathematics Class\n\nHow can any light get past a polarizer?\n\nPhD supervisor calls me a retard in my face more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', metadata={'id': 'web-search_0', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\ntensorflow Tf.cond giving unexpected output\n\nAsked 4 years, 5 months ago\n\nModified 4 years, 5 months ago\n\nI seem to be having a misunderstanding on how tf.cond works. In the tensorflow documentation, it gives the following example:\n\nz = tf.multiply(a, b) result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))\n\nThe result of the example, if x<y is True is tf.add(x,z) else tf.square(y)\n\nFollowing this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation.\n\nin my example, deterministic_action = 4, random_action = 11, chose_random=False. The stochastic_action should be 4, instead it is 1. Where did the value 1 come from?\n\n#!/usr/bin/env python3 import tensorflow as tf import numpy as np with tf.Graph().as_default(): with tf.device(\'/cpu:0\'): stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"") eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0)) with tf.variable_scope(\'test_cond\') as sc: deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4 random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11 chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) < eps # False because eps = 0 stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1 #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action) init = tf.global_variables_initializer() sess = tf.Session() sess.run(init, feed_dict={stochastic_ph: True}) print (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) r_action = sess.run(random_action) print (""rand_action= "", r_action) e = sess.run(eps) c_action = sess.run(chose_random) print (""chose_rand= "", c_action) s_action = sess.run(stochastic_action) print (""s_action= "", s_action) #output = sess.run(output_action)\n\npython random_vec.py 2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA s_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 1\n\nImprove this question\n\nedited Oct 31, 2018 at 9:15\n\nasked Oct 31, 2018 at 8:46\n\n2,04044 gold badges2525 silver badges5050 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThis is because you are evaluating again in a new sess.run. Since you are generating a random number for deterministic_action, the result turns out to be the next random number after 4, which is 1. Here is the result of your code, when I extract the value of deterministic_action as well in the last step.\n\nprint (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) r_action = sess.run(random_action) print (""rand_action= "", r_action) e = sess.run(eps) c_action = sess.run(chose_random) print (""chose_rand= "", c_action) s_action, d_action = sess.run([stochastic_action, deterministic_action]) print (""s_action= "", s_action) print (""det_action= "", d_action)\n\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 1 det_action= 1\n\nNow all you need to do is run everything in one sess.run\n\nd_action, r_action, e, c_action, s_action = sess.run([deterministic_action, random_action, eps, chose_random, stochastic_action]) print (""det_action= "", d_action) print (""rand_action= "", r_action) print (""chose_rand= "", c_action) print (""s_action= "", s_action)\n\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 4\n\nI was not clear on why the random_uniform generates different values when seed is set. This is because the code is running with the same session object that it initialized the variables with. Modifying the code with a new session object, this is what happens:\n\nprint (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) sess.close() sess = tf.Session() sess.run(init, feed_dict={stochastic_ph: True}) s_action = sess.run(stochastic_action) print (""s_action= "", s_action)\n\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 s_action= 4\n\nedited Nov 1, 2018 at 13:17\n\nanswered Oct 31, 2018 at 13:27\n\n99211 gold badge88 silver badges1212 bronze badges 4\n\nThe seed is fixed. So, irrespective of the session you run in, the random value should always be the same.\n\n– tandem Oct 31, 2018 at 14:50\n\n@tandem the sequence of numbers you get would be the same, but not the value every time. If you notice my first result, where I\'m just reading out the value of deterministic_action along with stochastic_action that you are getting in your last step. Your deterministic_action is 1, which means your result is also 1. When I put all the evaluations together in a single session run, your deterministic_action is 4, and hence your result is also 4.\n\n– kvish Oct 31, 2018 at 14:58\n\nSeed should always give the same random number. as long as it is set. That explanation doesnt still make sense\n\n– tandem Nov 1, 2018 at 6:53\n\n@tandem tensorflow computation graphs depend on your initialization. In the above example, you are still using the session object that is initialized at the beginning. This means tensorflow is keeping a track of the state of its variables in the session object, and thus does not reinitialize the random_uniform. Close the session object with sess.close() and then open a new session object and run your stochastic_action variable, it will produce the result 4 as expected.\n\n– kvish Nov 1, 2018 at 13:06\n\nNot the answer you\'re looking for? Browse other questions tagged\n\npython-3.6 or ask your own question.\n\nGoing stateless with authorization-as-a-service (Ep. 553)\n\nAre meetings making you less productive?\n\nImproving the copy in the close modal and post notices - 2023 edition\n\nPlagiarism flag and moderator tooling has launched to Stack Overflow!\n\nTemporary policy: ChatGPT is banned\n\nShould we burninate the [protection] tag?\n\nDo you observe increased relevance of Related Questions with our Machine...\n\n337 Disable Tensorflow debugging information\n\n297 What\'s the difference of name scope and a variable scope in tensorflow?\n\n428 What is the difference between \'SAME\' and \'VALID\' padding in tf.nn.max_pool of tensorflow?\n\n702 TensorFlow not found using pip\n\n6 Simple Feedforward Neural Network with TensorFlow won\'t learn\n\n1 TensorFlow multi-GPU InvalidArgumentError : cifar10_multi_gpu.py\n\n768 Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\n\n2 TF/keras subclass works perfectly in Eager Execution, and throws a massive untraceable error without it?\n\nHot Network Questions\n\nDid/do the dinosaurs in Jurassic Park reproduce asexually or did some turn into males?\n\nHow can I find the nature of a turning point?\n\nCan I tell DeleteCases not to delete function arguments?\n\nHard sci fi novel that ends with vast civilization ships all cruising in a line toward the same destination in the galaxy\n\nDoes my passport need to be stamped while re-entering Schengen area?\n\nStop stdin while command is running\n\nUnreadable Russian TeX files\n\nWhy are there such low rates of acceptance in AI/ML conferences?\n\nWould it benefit me to file taxes this year?\n\nDoes the computational theory of mind explain anything?\n\nWhat kind of fallacy is it to say if abolition of something isn\'t possible, we shouldn\'t attempt to address it at all?\n\n""Why"" do animals excrete excess nitrogen instead of recycling it?\n\nPID output at 0 error\n\nHow to draw a diagram without using graphics\n\nCreating a Table on Tex Stack Exchange Question Editor\n\nDecline promotion because of teaching load\n\nGood / recommended way to archive fastq and bam files?\n\nMeaning of ""water, the weight of which is one-eighth hydrogen""\n\nHow to analyze this circuit with an op-amp and positive feedback?\n\nWhere do I send a nomination for the Presidential Medal of Freedom?\n\nWhy do we insist that the electron be a point particle when calculation shows it creates an electrostatic field of infinite energy?\n\nC++ Binary Mathematics Class\n\nHow can any light get past a polarizer?\n\nPhD supervisor calls me a retard in my face more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', 'timestamp': '2023-04-12T14:13:39', 'title': 'python 3.6 - tensorflow Tf.cond giving unexpected output - Stack Overflow', 'url': 'https://stackoverflow.com/questions/53079436/tensorflow-tf-cond-giving-unexpected-output'})], [Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ntf.cond and tf.case execute all branches #3287\n\nalexatknit opened this issue\n\nJul 13, 2016 · 7 comments\n\ntf.cond and tf.case execute all branches #3287\n\nalexatknit opened this issue\n\nJul 13, 2016 · 7 comments\n\nalexatknit commented\n\nimport tensorflow as tf X1 = tf.Variable(1.) X2 = tf.Variable(1.) cond_value = tf.Variable(True) assign_1 = tf.assign(X1, 2.) assign_2 = tf.assign(X2, 2.) with tf.control_dependencies([assign_1]): result_1 = tf.identity(X1) with tf.control_dependencies([assign_2]): result_2 = tf.identity(X2) cond_result = tf.cond(cond_value, lambda: result_1, lambda: result_2) with tf.Session() as sesh: sesh.run(tf.initialize_all_variables()) sesh.run(cond_result) print(sesh.run(X1), sesh.run(X2))\n\nmy expected output is: 2.0 1.0 but my actual output is 2.0 2.0. I need control flow in my graph that actually controls which nodes get executed, not just selects from a list of tensors. Am I missing some sort of operation here?\n\nThe text was updated successfully, but these errors were encountered:\n\nvrv assigned yuanbyu\n\nAssigning to Yuan -- this is expected behavior, but we still need more documentation on this.\n\nI believe if you define your control dependencies as a function, and the lambda calls the function, it will work.\n\ndef fn1(): with tf.control_dependencies.... cond_result = tf.cond(cond_value, lambda: fn1(), lambda: fn2())\n\nor something like that. Essentially the ops have to be defined in the function passed to the lambda for them to not be executed.\n\nvrv added type:docs-bug\n\nDocument issues triaged labels\n\nalexatknit commented\n\nah, I see. This may require a large amount of refactoring on my part\n\nalexatknit commented\n\nthe assigns need to be in the method as well:\n\nimport tensorflow as tf X1 = tf.Variable(1.) X2 = tf.Variable(1.) cond_value = tf.Variable(True) def result_1(): assign_1 = tf.assign(X1, 2.) with tf.control_dependencies([assign_1]): return tf.identity(X1) def result_2(): assign_2 = tf.assign(X2, 2.) with tf.control_dependencies([assign_2]): return tf.identity(X2) cond_result = tf.cond(cond_value, result_1, result_2) with tf.Session() as sesh: sesh.run(tf.initialize_all_variables()) sesh.run(cond_result) print(sesh.run(X1), sesh.run(X2))\n\nalexatknit commented\n\nLifting this requirement would allow me to have more modular code when designing resnet structures:\n\n... with tf.variable_scope(\'resnet1\'): for i in range(5): with tf.variable_scope(str(i)): X = self.conv(elu(X1), 3, 3, 16, name=\'conv1\') # 128 -> 128 X = self.conv(elu(X), 3, 3, 16, name=\'conv2\') # 128 -> 128 X1 = self.feedback(X, X1, dropout=0.7) ...\n\nIn this case, feedback will add the two tensors with a 70% chance or simply pass X1 with a 30% chance.\n\nFor your simple example, I believe that you could simply do:\n\nX1 = tf.Variable(1.) X2 = tf.Variable(1.) cond_value = tf.Variable(True) cond_result = tf.cond(cond_value, lambda: tf.assign(X1, 2.), lambda: tf.assign(X2, 2.)) sess.run(tf.initialize_all_variables()) sess.run(cond_result) print(sess.run(X1), sess.run(X2))\n\nalexatknit commented\n\nsure, I get that, I threw it together to test the unexpected behavior I was seeing running rnns, wasn\'t supposed to be a polished example. Now I\'m getting some weird behavior when converting my trainable variables to constants. It might be a separate issue but the stack looks like:\n\nTraceback (most recent call last): File ""/usr/local/bin/finalize_graph"", line 9, in <module> load_entry_point(\'KnitNN==0.5.3\', \'console_scripts\', \'finalize_graph\')() File ""/usr/local/lib/python3.5/site-packages/KnitNN-0.5.3-py3.5.egg/nn/tools/finalize_graph.py"", line 59, in finalize_graph variable_names_whitelist=variables) File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/graph_util.py"", line 224, in convert_variables_to_constants returned_variables = sess.run(variable_names) File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 382, in run run_metadata_ptr) File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 598, in _run processed_fetches = self._process_fetches(fetches) File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 553, in _process_fetches \'Tensor. (%s)\' % (subfetch, fetch, str(e))) ValueError: Fetch argument \'base_model/cond/resnet1/Assign/Switch:1:0\' of \'base_model/cond/resnet1/Assign/Switch:1:0\' cannot be interpreted as a Tensor. (The name \'base_model/cond/resnet1/Assign/Switch:1:0\' looks a like a Tensor name, but is not a valid one. Tensor names must be of the form ""<op_name>:<output_index>"".)\n\naselle removed the triaged label\n\nyuanbyu closed this as completed\n\nMicaelCarvalho mentioned this issue\n\ntf.case evaluating all outputs when using batches (?) #8168\n\naselle mentioned this issue\n\ntf.cond should be evaluated lazily #10800\n\ngokceneraslan mentioned this issue\n\nConditional variable updates via K.switch() are error-prone on TF backend keras-team/keras#6621\n\ntomjur mentioned this issue\n\nmove training steps to lambda ashual/style-transfer#23\n\ngcp mentioned this issue\n\nImplement real validation split for TF. leela-zero/leela-zero#747\n\n@vrv @yuanbyu is there other way to avoid evaluating all branches other than put the operator creation inside the lambda? There are many cases like:\n\n# Complex code creating two subgraphs tf.case(predicate, true_fn=lambda: subgraph_a, false_fn=lambda: subgraph_b)\n\nIt would be hard to refactor the creation of subgraph_a and subgraph_b into the lambda for true_fn and false_fn. Maybe add an additional argument that avoids evaluating all branches? This feels like a limitation of the expressiveness of TF Python API, rather than an intended feature.\n\nhelinwang unassigned yuanbyu\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_4', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ntf.cond and tf.case execute all branches #3287\n\nalexatknit opened this issue\n\nJul 13, 2016 · 7 comments\n\ntf.cond and tf.case execute all branches #3287\n\nalexatknit opened this issue\n\nJul 13, 2016 · 7 comments\n\nalexatknit commented\n\nimport tensorflow as tf X1 = tf.Variable(1.) X2 = tf.Variable(1.) cond_value = tf.Variable(True) assign_1 = tf.assign(X1, 2.) assign_2 = tf.assign(X2, 2.) with tf.control_dependencies([assign_1]): result_1 = tf.identity(X1) with tf.control_dependencies([assign_2]): result_2 = tf.identity(X2) cond_result = tf.cond(cond_value, lambda: result_1, lambda: result_2) with tf.Session() as sesh: sesh.run(tf.initialize_all_variables()) sesh.run(cond_result) print(sesh.run(X1), sesh.run(X2))\n\nmy expected output is: 2.0 1.0 but my actual output is 2.0 2.0. I need control flow in my graph that actually controls which nodes get executed, not just selects from a list of tensors. Am I missing some sort of operation here?\n\nThe text was updated successfully, but these errors were encountered:\n\nvrv assigned yuanbyu\n\nAssigning to Yuan -- this is expected behavior, but we still need more documentation on this.\n\nI believe if you define your control dependencies as a function, and the lambda calls the function, it will work.\n\ndef fn1(): with tf.control_dependencies.... cond_result = tf.cond(cond_value, lambda: fn1(), lambda: fn2())\n\nor something like that. Essentially the ops have to be defined in the function passed to the lambda for them to not be executed.\n\nvrv added type:docs-bug\n\nDocument issues triaged labels\n\nalexatknit commented\n\nah, I see. This may require a large amount of refactoring on my part\n\nalexatknit commented\n\nthe assigns need to be in the method as well:\n\nimport tensorflow as tf X1 = tf.Variable(1.) X2 = tf.Variable(1.) cond_value = tf.Variable(True) def result_1(): assign_1 = tf.assign(X1, 2.) with tf.control_dependencies([assign_1]): return tf.identity(X1) def result_2(): assign_2 = tf.assign(X2, 2.) with tf.control_dependencies([assign_2]): return tf.identity(X2) cond_result = tf.cond(cond_value, result_1, result_2) with tf.Session() as sesh: sesh.run(tf.initialize_all_variables()) sesh.run(cond_result) print(sesh.run(X1), sesh.run(X2))\n\nalexatknit commented\n\nLifting this requirement would allow me to have more modular code when designing resnet structures:\n\n... with tf.variable_scope(\'resnet1\'): for i in range(5): with tf.variable_scope(str(i)): X = self.conv(elu(X1), 3, 3, 16, name=\'conv1\') # 128 -> 128 X = self.conv(elu(X), 3, 3, 16, name=\'conv2\') # 128 -> 128 X1 = self.feedback(X, X1, dropout=0.7) ...\n\nIn this case, feedback will add the two tensors with a 70% chance or simply pass X1 with a 30% chance.\n\nFor your simple example, I believe that you could simply do:\n\nX1 = tf.Variable(1.) X2 = tf.Variable(1.) cond_value = tf.Variable(True) cond_result = tf.cond(cond_value, lambda: tf.assign(X1, 2.), lambda: tf.assign(X2, 2.)) sess.run(tf.initialize_all_variables()) sess.run(cond_result) print(sess.run(X1), sess.run(X2))\n\nalexatknit commented\n\nsure, I get that, I threw it together to test the unexpected behavior I was seeing running rnns, wasn\'t supposed to be a polished example. Now I\'m getting some weird behavior when converting my trainable variables to constants. It might be a separate issue but the stack looks like:\n\nTraceback (most recent call last): File ""/usr/local/bin/finalize_graph"", line 9, in <module> load_entry_point(\'KnitNN==0.5.3\', \'console_scripts\', \'finalize_graph\')() File ""/usr/local/lib/python3.5/site-packages/KnitNN-0.5.3-py3.5.egg/nn/tools/finalize_graph.py"", line 59, in finalize_graph variable_names_whitelist=variables) File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/graph_util.py"", line 224, in convert_variables_to_constants returned_variables = sess.run(variable_names) File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 382, in run run_metadata_ptr) File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 598, in _run processed_fetches = self._process_fetches(fetches) File ""/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 553, in _process_fetches \'Tensor. (%s)\' % (subfetch, fetch, str(e))) ValueError: Fetch argument \'base_model/cond/resnet1/Assign/Switch:1:0\' of \'base_model/cond/resnet1/Assign/Switch:1:0\' cannot be interpreted as a Tensor. (The name \'base_model/cond/resnet1/Assign/Switch:1:0\' looks a like a Tensor name, but is not a valid one. Tensor names must be of the form ""<op_name>:<output_index>"".)\n\naselle removed the triaged label\n\nyuanbyu closed this as completed\n\nMicaelCarvalho mentioned this issue\n\ntf.case evaluating all outputs when using batches (?) #8168\n\naselle mentioned this issue\n\ntf.cond should be evaluated lazily #10800\n\ngokceneraslan mentioned this issue\n\nConditional variable updates via K.switch() are error-prone on TF backend keras-team/keras#6621\n\ntomjur mentioned this issue\n\nmove training steps to lambda ashual/style-transfer#23\n\ngcp mentioned this issue\n\nImplement real validation split for TF. leela-zero/leela-zero#747\n\n@vrv @yuanbyu is there other way to avoid evaluating all branches other than put the operator creation inside the lambda? There are many cases like:\n\n# Complex code creating two subgraphs tf.case(predicate, true_fn=lambda: subgraph_a, false_fn=lambda: subgraph_b)\n\nIt would be hard to refactor the creation of subgraph_a and subgraph_b into the lambda for true_fn and false_fn. Maybe add an additional argument that avoids evaluating all branches? This feels like a limitation of the expressiveness of TF Python API, rather than an intended feature.\n\nhelinwang unassigned yuanbyu\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-05-26T02:11:56', 'title': 'tf.cond and tf.case execute all branches · Issue #3287 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/3287'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ntf.random.uniform() always picking the same value when metal plugin is installed #56122\n\nyunhao-qian opened this issue\n\nMay 16, 2022 · 4 comments\n\ntf.random.uniform() always picking the same value when metal plugin is installed #56122\n\nyunhao-qian opened this issue\n\nMay 16, 2022 · 4 comments\n\nOPs related issues TF 2.8 type:bug\n\nyunhao-qian commented\n\nOS: macOS Montererey 12.4\n\nTensorflow installed from: pip commands inside a conda environment\n\nTensorflow version: tensorflow-macos 2.8.0\n\nMetal plugin version: tensorflow-metal 0.4.0\n\nPython version: 3.9.12\n\nGPU model and memory: Apple M1 Pro with 16 GB memory\n\nExact command to reproduce: see below\n\nDescribe the problem\n\nWhen the tensorflow-metal plugin is installed, the tf.random.uniform() function always picks the same value on every call. Other random functions like tf.random.normal() do not have this problem. Neither does this problem occur when the Metal plugin is not installed.\n\nyunhao@Yunhaos-MBP ~ % conda create --name tf-rand-test python=3.9 six=1.15 Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /Users/yunhao/miniforge3/envs/tf-rand-test added / updated specs: - python=3.9 - six=1.15 The following NEW packages will be INSTALLED: bzip2 conda-forge/osx-arm64::bzip2-1.0.8-h3422bc3_4 ca-certificates conda-forge/osx-arm64::ca-certificates-2021.10.8-h4653dfc_0 libffi conda-forge/osx-arm64::libffi-3.4.2-h3422bc3_5 libzlib conda-forge/osx-arm64::libzlib-1.2.11-h90dfc92_1014 ncurses conda-forge/osx-arm64::ncurses-6.3-h07bb92c_1 openssl conda-forge/osx-arm64::openssl-3.0.3-ha287fd2_0 pip conda-forge/noarch::pip-22.1-pyhd8ed1ab_0 python conda-forge/osx-arm64::python-3.9.12-h14b404e_1_cpython python_abi conda-forge/osx-arm64::python_abi-3.9-2_cp39 readline conda-forge/osx-arm64::readline-8.1-hedafd6a_0 setuptools conda-forge/osx-arm64::setuptools-62.2.0-py39h2804cbe_0 six conda-forge/noarch::six-1.15.0-pyh9f0ad1d_0 sqlite conda-forge/osx-arm64::sqlite-3.38.5-h40dfcc0_0 tk conda-forge/osx-arm64::tk-8.6.12-he1e0b03_0 tzdata conda-forge/noarch::tzdata-2022a-h191b570_0 wheel conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0 xz conda-forge/osx-arm64::xz-5.2.5-h642e427_1 zlib conda-forge/osx-arm64::zlib-1.2.11-h90dfc92_1014 Proceed ([y]/n)? Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use # # $ conda activate tf-rand-test # # To deactivate an active environment, use # # $ conda deactivate yunhao@Yunhaos-MBP ~ % conda activate tf-rand-test (tf-rand-test) yunhao@Yunhaos-MBP ~ % conda install -c apple tensorflow-deps Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /Users/yunhao/miniforge3/envs/tf-rand-test added / updated specs: - tensorflow-deps The following NEW packages will be INSTALLED: c-ares conda-forge/osx-arm64::c-ares-1.18.1-h3422bc3_0 cached-property conda-forge/noarch::cached-property-1.5.2-hd8ed1ab_1 cached_property conda-forge/noarch::cached_property-1.5.2-pyha770c72_1 grpcio conda-forge/osx-arm64::grpcio-1.46.1-py39h365d37b_0 h5py conda-forge/osx-arm64::h5py-3.6.0-nompi_py39hd982b79_100 hdf5 conda-forge/osx-arm64::hdf5-1.12.1-nompi_hd9dbc9e_104 krb5 conda-forge/osx-arm64::krb5-1.19.3-he492e65_0 libblas conda-forge/osx-arm64::libblas-3.9.0-14_osxarm64_openblas libcblas conda-forge/osx-arm64::libcblas-3.9.0-14_osxarm64_openblas libcurl conda-forge/osx-arm64::libcurl-7.83.1-h7965298_0 libcxx conda-forge/osx-arm64::libcxx-14.0.3-h6a5c8ee_0 libedit conda-forge/osx-arm64::libedit-3.1.20191231-hc8eb9b7_2 libev conda-forge/osx-arm64::libev-4.33-h642e427_1 libgfortran conda-forge/osx-arm64::libgfortran-5.0.0.dev0-11_0_1_hf114ba7_23 libgfortran5 conda-forge/osx-arm64::libgfortran5-11.0.1.dev0-hf114ba7_23 liblapack conda-forge/osx-arm64::liblapack-3.9.0-14_osxarm64_openblas libnghttp2 conda-forge/osx-arm64::libnghttp2-1.47.0-hf30690b_0 libopenblas conda-forge/osx-arm64::libopenblas-0.3.20-openmp_h2209c59_0 libssh2 conda-forge/osx-arm64::libssh2-1.10.0-h7a5bd25_2 llvm-openmp conda-forge/osx-arm64::llvm-openmp-14.0.3-hd125106_0 numpy conda-forge/osx-arm64::numpy-1.21.6-py39h690d673_0 tensorflow-deps apple/osx-arm64::tensorflow-deps-2.8.0-0 Proceed ([y]/n)? Preparing transaction: done Verifying transaction: done Executing transaction: done (tf-rand-test) yunhao@Yunhaos-MBP ~ % python -m pip install tensorflow-macos Collecting tensorflow-macos Using cached tensorflow_macos-2.8.0-cp39-cp39-macosx_11_0_arm64.whl (190.1 MB) Collecting tensorboard<2.9,>=2.8 Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB) Collecting absl-py>=0.4.0 Using cached absl_py-1.0.0-py3-none-any.whl (126 kB) Collecting libclang>=9.0.1 Using cached libclang-14.0.1-py2.py3-none-macosx_11_0_arm64.whl (11.8 MB) Collecting termcolor>=1.1.0 Using cached termcolor-1.1.0-py3-none-any.whl Collecting protobuf>=3.9.2 Using cached protobuf-3.20.1-py2.py3-none-any.whl (162 kB) Collecting opt-einsum>=2.3.2 Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB) Collecting keras-preprocessing>=1.1.1 Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB) Collecting flatbuffers>=1.12 Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB) Requirement already satisfied: six>=1.12.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (1.15.0) Collecting keras<2.9,>=2.8.0rc0 Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB) Requirement already satisfied: setuptools in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (62.2.0) Collecting astunparse>=1.6.0 Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB) Collecting wrapt>=1.11.0 Using cached wrapt-1.14.1-cp39-cp39-macosx_11_0_arm64.whl (35 kB) Collecting gast>=0.2.1 Using cached gast-0.5.3-py3-none-any.whl (19 kB) Collecting google-pasta>=0.1.1 Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB) Collecting typing-extensions>=3.6.6 Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB) Requirement already satisfied: numpy>=1.20 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (1.21.6) Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (1.46.1) Collecting tf-estimator-nightly==2.8.0.dev2021122109 Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB) Requirement already satisfied: h5py>=2.9.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (3.6.0) Requirement already satisfied: wheel<1.0,>=0.23.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos) (0.37.1) Collecting google-auth<3,>=1.6.3 Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB) Collecting requests<3,>=2.21.0 Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB) Collecting tensorboard-plugin-wit>=1.6.0 Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB) Collecting werkzeug>=0.11.15 Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB) Collecting markdown>=2.6.8 Using cached Markdown-3.3.7-py3-none-any.whl (97 kB) Collecting tensorboard-data-server<0.7.0,>=0.6.0 Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB) Collecting google-auth-oauthlib<0.5,>=0.4.1 Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB) Collecting pyasn1-modules>=0.2.1 Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB) Collecting cachetools<6.0,>=2.0.0 Using cached cachetools-5.1.0-py3-none-any.whl (9.2 kB) Collecting rsa<5,>=3.1.4 Using cached rsa-4.8-py3-none-any.whl (39 kB) Collecting requests-oauthlib>=0.7.0 Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB) Collecting importlib-metadata>=4.4 Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB) Collecting idna<4,>=2.5 Using cached idna-3.3-py3-none-any.whl (61 kB) Collecting certifi>=2017.4.17 Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB) Collecting urllib3<1.27,>=1.21.1 Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB) Collecting charset-normalizer~=2.0.0 Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB) Collecting zipp>=0.5 Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB) Collecting pyasn1<0.5.0,>=0.4.6 Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB) Collecting oauthlib>=3.0.0 Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB) Installing collected packages: tf-estimator-nightly, termcolor, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, certifi, zipp, wrapt, werkzeug, urllib3, typing-extensions, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, keras-preprocessing, idna, google-pasta, gast, charset-normalizer, cachetools, astunparse, absl-py, requests, importlib-metadata, google-auth, requests-oauthlib, markdown, google-auth-oauthlib, tensorboard, tensorflow-macos Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.1.0 certifi-2021.10.8 charset-normalizer-2.0.12 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 idna-3.3 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-macos-2.8.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.2 wrapt-1.14.1 zipp-3.8.0 (tf-rand-test) yunhao@Yunhaos-MBP ~ % python Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:14) [Clang 12.0.1 ] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf >>> for _ in range(10): ... print(tf.random.uniform([])) ... tf.Tensor(0.43182516, shape=(), dtype=float32) tf.Tensor(0.73127687, shape=(), dtype=float32) tf.Tensor(0.8234819, shape=(), dtype=float32) tf.Tensor(0.7472857, shape=(), dtype=float32) tf.Tensor(0.06519365, shape=(), dtype=float32) tf.Tensor(0.46567404, shape=(), dtype=float32) tf.Tensor(0.081846, shape=(), dtype=float32) tf.Tensor(0.26130438, shape=(), dtype=float32) tf.Tensor(0.53803635, shape=(), dtype=float32) tf.Tensor(0.98602235, shape=(), dtype=float32) >>> exit() (tf-rand-test) yunhao@Yunhaos-MBP ~ % python -m pip install tensorflow-metal Collecting tensorflow-metal Using cached tensorflow_metal-0.4.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB) Requirement already satisfied: wheel~=0.35 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-metal) (0.37.1) Requirement already satisfied: six~=1.15.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-metal) (1.15.0) Installing collected packages: tensorflow-metal Successfully installed tensorflow-metal-0.4.0 (tf-rand-test) yunhao@Yunhaos-MBP ~ % python Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:14) [Clang 12.0.1 ] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf >>> for _ in range(10): ... print(tf.random.uniform([])) ... Metal device set to: Apple M1 Pro systemMemory: 16.00 GB maxCacheSize: 5.33 GB 2022-05-16 15:21:55.560387: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 2022-05-16 15:21:55.560711: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) >>> for _ in range(10): ... print(tf.random.uniform([])) ... tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) >>> exit() (tf-rand-test) yunhao@Yunhaos-MBP ~ %\n\nThe text was updated successfully, but these errors were encountered:\n\ngoogle-ml-butler bot assigned tilakrayal\n\ntilakrayal added TF 2.8 comp:ops\n\nOPs related issues type:bug\n\ntilakrayal assigned sachinprasadhs and unassigned tilakrayal\n\nsachinprasadhs commented\n\nIt is because the random.uniform in apple metal is not able to initialize the op seed or global seed, when both of these seeds values are set to None, you\'ll get the same number generated. To avoid this you can enable tf.config.experimental.enable_op_determinism() and set the random_seed to generate different results. Refer the below code.\n\nimport tensorflow as tf tf.keras.utils.set_random_seed(1) tf.config.experimental.enable_op_determinism() for _ in range(10): print(tf.random.uniform([]))\n\nsachinprasadhs added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nyunhao-qian commented\n\nThank you! 😊 After the global seed is set, tf.random.uniform() behaves as expected.\n\nWhen I read the documentation of tf.random.set_seed(), I found the following sentence which complements your explanation:\n\nIf neither the global seed nor the operation seed is set: A randomly picked seed is used for this op.\n\nSo here, how the seed is ""randomly picked"" is not clearly specified. Should I understand it as: since Metal is unable to initialize the global seed or the operation seed, the current behaviour (without seeding) is an expected ""sharp edge"" instead of a bug? This can be very surprising for many people like me, so maybe it should be documented somewhere more explicitly.\n\nThank you again for your help. You can close this issue if you think no further action is needed.\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nsachinprasadhs commented\n\nYes, you can consider it as a edge case, random seed works on a kernel level and maintains the counter in the kernel level itself. apple metal fails to initialize the random seed, tensorflow-macos is not officially supported by Tensorflow to document this behavior. Closing the issue as per your above comment. Thanks!\n\nsachinprasadhs closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nOPs related issues TF 2.8 type:bug\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_0', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ntf.random.uniform() always picking the same value when metal plugin is installed #56122\n\nyunhao-qian opened this issue\n\nMay 16, 2022 · 4 comments\n\ntf.random.uniform() always picking the same value when metal plugin is installed #56122\n\nyunhao-qian opened this issue\n\nMay 16, 2022 · 4 comments\n\nOPs related issues TF 2.8 type:bug\n\nyunhao-qian commented\n\nOS: macOS Montererey 12.4\n\nTensorflow installed from: pip commands inside a conda environment\n\nTensorflow version: tensorflow-macos 2.8.0\n\nMetal plugin version: tensorflow-metal 0.4.0\n\nPython version: 3.9.12\n\nGPU model and memory: Apple M1 Pro with 16 GB memory\n\nExact command to reproduce: see below\n\nDescribe the problem\n\nWhen the tensorflow-metal plugin is installed, the tf.random.uniform() function always picks the same value on every call. Other random functions like tf.random.normal() do not have this problem. Neither does this problem occur when the Metal plugin is not installed.\n\nyunhao@Yunhaos-MBP ~ % conda create --name tf-rand-test python=3.9 six=1.15 Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /Users/yunhao/miniforge3/envs/tf-rand-test added / updated specs: - python=3.9 - six=1.15 The following NEW packages will be INSTALLED: bzip2 conda-forge/osx-arm64::bzip2-1.0.8-h3422bc3_4 ca-certificates conda-forge/osx-arm64::ca-certificates-2021.10.8-h4653dfc_0 libffi conda-forge/osx-arm64::libffi-3.4.2-h3422bc3_5 libzlib conda-forge/osx-arm64::libzlib-1.2.11-h90dfc92_1014 ncurses conda-forge/osx-arm64::ncurses-6.3-h07bb92c_1 openssl conda-forge/osx-arm64::openssl-3.0.3-ha287fd2_0 pip conda-forge/noarch::pip-22.1-pyhd8ed1ab_0 python conda-forge/osx-arm64::python-3.9.12-h14b404e_1_cpython python_abi conda-forge/osx-arm64::python_abi-3.9-2_cp39 readline conda-forge/osx-arm64::readline-8.1-hedafd6a_0 setuptools conda-forge/osx-arm64::setuptools-62.2.0-py39h2804cbe_0 six conda-forge/noarch::six-1.15.0-pyh9f0ad1d_0 sqlite conda-forge/osx-arm64::sqlite-3.38.5-h40dfcc0_0 tk conda-forge/osx-arm64::tk-8.6.12-he1e0b03_0 tzdata conda-forge/noarch::tzdata-2022a-h191b570_0 wheel conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0 xz conda-forge/osx-arm64::xz-5.2.5-h642e427_1 zlib conda-forge/osx-arm64::zlib-1.2.11-h90dfc92_1014 Proceed ([y]/n)? Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use # # $ conda activate tf-rand-test # # To deactivate an active environment, use # # $ conda deactivate yunhao@Yunhaos-MBP ~ % conda activate tf-rand-test (tf-rand-test) yunhao@Yunhaos-MBP ~ % conda install -c apple tensorflow-deps Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /Users/yunhao/miniforge3/envs/tf-rand-test added / updated specs: - tensorflow-deps The following NEW packages will be INSTALLED: c-ares conda-forge/osx-arm64::c-ares-1.18.1-h3422bc3_0 cached-property conda-forge/noarch::cached-property-1.5.2-hd8ed1ab_1 cached_property conda-forge/noarch::cached_property-1.5.2-pyha770c72_1 grpcio conda-forge/osx-arm64::grpcio-1.46.1-py39h365d37b_0 h5py conda-forge/osx-arm64::h5py-3.6.0-nompi_py39hd982b79_100 hdf5 conda-forge/osx-arm64::hdf5-1.12.1-nompi_hd9dbc9e_104 krb5 conda-forge/osx-arm64::krb5-1.19.3-he492e65_0 libblas conda-forge/osx-arm64::libblas-3.9.0-14_osxarm64_openblas libcblas conda-forge/osx-arm64::libcblas-3.9.0-14_osxarm64_openblas libcurl conda-forge/osx-arm64::libcurl-7.83.1-h7965298_0 libcxx conda-forge/osx-arm64::libcxx-14.0.3-h6a5c8ee_0 libedit conda-forge/osx-arm64::libedit-3.1.20191231-hc8eb9b7_2 libev conda-forge/osx-arm64::libev-4.33-h642e427_1 libgfortran conda-forge/osx-arm64::libgfortran-5.0.0.dev0-11_0_1_hf114ba7_23 libgfortran5 conda-forge/osx-arm64::libgfortran5-11.0.1.dev0-hf114ba7_23 liblapack conda-forge/osx-arm64::liblapack-3.9.0-14_osxarm64_openblas libnghttp2 conda-forge/osx-arm64::libnghttp2-1.47.0-hf30690b_0 libopenblas conda-forge/osx-arm64::libopenblas-0.3.20-openmp_h2209c59_0 libssh2 conda-forge/osx-arm64::libssh2-1.10.0-h7a5bd25_2 llvm-openmp conda-forge/osx-arm64::llvm-openmp-14.0.3-hd125106_0 numpy conda-forge/osx-arm64::numpy-1.21.6-py39h690d673_0 tensorflow-deps apple/osx-arm64::tensorflow-deps-2.8.0-0 Proceed ([y]/n)? Preparing transaction: done Verifying transaction: done Executing transaction: done (tf-rand-test) yunhao@Yunhaos-MBP ~ % python -m pip install tensorflow-macos Collecting tensorflow-macos Using cached tensorflow_macos-2.8.0-cp39-cp39-macosx_11_0_arm64.whl (190.1 MB) Collecting tensorboard<2.9,>=2.8 Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB) Collecting absl-py>=0.4.0 Using cached absl_py-1.0.0-py3-none-any.whl (126 kB) Collecting libclang>=9.0.1 Using cached libclang-14.0.1-py2.py3-none-macosx_11_0_arm64.whl (11.8 MB) Collecting termcolor>=1.1.0 Using cached termcolor-1.1.0-py3-none-any.whl Collecting protobuf>=3.9.2 Using cached protobuf-3.20.1-py2.py3-none-any.whl (162 kB) Collecting opt-einsum>=2.3.2 Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB) Collecting keras-preprocessing>=1.1.1 Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB) Collecting flatbuffers>=1.12 Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB) Requirement already satisfied: six>=1.12.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (1.15.0) Collecting keras<2.9,>=2.8.0rc0 Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB) Requirement already satisfied: setuptools in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (62.2.0) Collecting astunparse>=1.6.0 Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB) Collecting wrapt>=1.11.0 Using cached wrapt-1.14.1-cp39-cp39-macosx_11_0_arm64.whl (35 kB) Collecting gast>=0.2.1 Using cached gast-0.5.3-py3-none-any.whl (19 kB) Collecting google-pasta>=0.1.1 Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB) Collecting typing-extensions>=3.6.6 Using cached typing_extensions-4.2.0-py3-none-any.whl (24 kB) Requirement already satisfied: numpy>=1.20 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (1.21.6) Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (1.46.1) Collecting tf-estimator-nightly==2.8.0.dev2021122109 Using cached tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB) Requirement already satisfied: h5py>=2.9.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-macos) (3.6.0) Requirement already satisfied: wheel<1.0,>=0.23.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos) (0.37.1) Collecting google-auth<3,>=1.6.3 Using cached google_auth-2.6.6-py2.py3-none-any.whl (156 kB) Collecting requests<3,>=2.21.0 Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB) Collecting tensorboard-plugin-wit>=1.6.0 Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB) Collecting werkzeug>=0.11.15 Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB) Collecting markdown>=2.6.8 Using cached Markdown-3.3.7-py3-none-any.whl (97 kB) Collecting tensorboard-data-server<0.7.0,>=0.6.0 Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB) Collecting google-auth-oauthlib<0.5,>=0.4.1 Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB) Collecting pyasn1-modules>=0.2.1 Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB) Collecting cachetools<6.0,>=2.0.0 Using cached cachetools-5.1.0-py3-none-any.whl (9.2 kB) Collecting rsa<5,>=3.1.4 Using cached rsa-4.8-py3-none-any.whl (39 kB) Collecting requests-oauthlib>=0.7.0 Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB) Collecting importlib-metadata>=4.4 Using cached importlib_metadata-4.11.3-py3-none-any.whl (18 kB) Collecting idna<4,>=2.5 Using cached idna-3.3-py3-none-any.whl (61 kB) Collecting certifi>=2017.4.17 Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB) Collecting urllib3<1.27,>=1.21.1 Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB) Collecting charset-normalizer~=2.0.0 Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB) Collecting zipp>=0.5 Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB) Collecting pyasn1<0.5.0,>=0.4.6 Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB) Collecting oauthlib>=3.0.0 Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB) Installing collected packages: tf-estimator-nightly, termcolor, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, certifi, zipp, wrapt, werkzeug, urllib3, typing-extensions, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, keras-preprocessing, idna, google-pasta, gast, charset-normalizer, cachetools, astunparse, absl-py, requests, importlib-metadata, google-auth, requests-oauthlib, markdown, google-auth-oauthlib, tensorboard, tensorflow-macos Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.1.0 certifi-2021.10.8 charset-normalizer-2.0.12 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 idna-3.3 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-macos-2.8.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.2.0 urllib3-1.26.9 werkzeug-2.1.2 wrapt-1.14.1 zipp-3.8.0 (tf-rand-test) yunhao@Yunhaos-MBP ~ % python Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:14) [Clang 12.0.1 ] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf >>> for _ in range(10): ... print(tf.random.uniform([])) ... tf.Tensor(0.43182516, shape=(), dtype=float32) tf.Tensor(0.73127687, shape=(), dtype=float32) tf.Tensor(0.8234819, shape=(), dtype=float32) tf.Tensor(0.7472857, shape=(), dtype=float32) tf.Tensor(0.06519365, shape=(), dtype=float32) tf.Tensor(0.46567404, shape=(), dtype=float32) tf.Tensor(0.081846, shape=(), dtype=float32) tf.Tensor(0.26130438, shape=(), dtype=float32) tf.Tensor(0.53803635, shape=(), dtype=float32) tf.Tensor(0.98602235, shape=(), dtype=float32) >>> exit() (tf-rand-test) yunhao@Yunhaos-MBP ~ % python -m pip install tensorflow-metal Collecting tensorflow-metal Using cached tensorflow_metal-0.4.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB) Requirement already satisfied: wheel~=0.35 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-metal) (0.37.1) Requirement already satisfied: six~=1.15.0 in ./miniforge3/envs/tf-rand-test/lib/python3.9/site-packages (from tensorflow-metal) (1.15.0) Installing collected packages: tensorflow-metal Successfully installed tensorflow-metal-0.4.0 (tf-rand-test) yunhao@Yunhaos-MBP ~ % python Python 3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:14) [Clang 12.0.1 ] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import tensorflow as tf >>> for _ in range(10): ... print(tf.random.uniform([])) ... Metal device set to: Apple M1 Pro systemMemory: 16.00 GB maxCacheSize: 5.33 GB 2022-05-16 15:21:55.560387: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 2022-05-16 15:21:55.560711: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) >>> for _ in range(10): ... print(tf.random.uniform([])) ... tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) tf.Tensor(0.9149647, shape=(), dtype=float32) >>> exit() (tf-rand-test) yunhao@Yunhaos-MBP ~ %\n\nThe text was updated successfully, but these errors were encountered:\n\ngoogle-ml-butler bot assigned tilakrayal\n\ntilakrayal added TF 2.8 comp:ops\n\nOPs related issues type:bug\n\ntilakrayal assigned sachinprasadhs and unassigned tilakrayal\n\nsachinprasadhs commented\n\nIt is because the random.uniform in apple metal is not able to initialize the op seed or global seed, when both of these seeds values are set to None, you\'ll get the same number generated. To avoid this you can enable tf.config.experimental.enable_op_determinism() and set the random_seed to generate different results. Refer the below code.\n\nimport tensorflow as tf tf.keras.utils.set_random_seed(1) tf.config.experimental.enable_op_determinism() for _ in range(10): print(tf.random.uniform([]))\n\nsachinprasadhs added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nyunhao-qian commented\n\nThank you! 😊 After the global seed is set, tf.random.uniform() behaves as expected.\n\nWhen I read the documentation of tf.random.set_seed(), I found the following sentence which complements your explanation:\n\nIf neither the global seed nor the operation seed is set: A randomly picked seed is used for this op.\n\nSo here, how the seed is ""randomly picked"" is not clearly specified. Should I understand it as: since Metal is unable to initialize the global seed or the operation seed, the current behaviour (without seeding) is an expected ""sharp edge"" instead of a bug? This can be very surprising for many people like me, so maybe it should be documented somewhere more explicitly.\n\nThank you again for your help. You can close this issue if you think no further action is needed.\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nsachinprasadhs commented\n\nYes, you can consider it as a edge case, random seed works on a kernel level and maintains the counter in the kernel level itself. apple metal fails to initialize the random seed, tensorflow-macos is not officially supported by Tensorflow to document this behavior. Closing the issue as per your above comment. Thanks!\n\nsachinprasadhs closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nOPs related issues TF 2.8 type:bug\n\nYou can’t perform that action at this time.', 'timestamp': '2024-05-18T10:01:42', 'title': '`tf.random.uniform()` always picking the same value when metal plugin is installed · Issue #56122 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/56122'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nNon-determinism from tf.data.Dataset.map with random ops #13932\n\ndusenberrymw opened this issue\n\nOct 24, 2017 · 9 comments\n\nNon-determinism from tf.data.Dataset.map with random ops #13932\n\ndusenberrymw opened this issue\n\nOct 24, 2017 · 9 comments\n\nstat:awaiting response\n\nStatus - Awaiting response from author\n\ndusenberrymw commented\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes -- please see the minimal reproducible example script below.\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.12, Linux CentOS 7 (4.6.6-300.el7.centos.x86_64)\n\nTensorFlow installed from (source or binary): pip3 install tf-nightly (also happens when built from source)\n\nTensorFlow version (use command below): v1.3.0-rc1-3690-g9b9cbbe 1.5.0-dev20171023\n\nPython version: 3.6.3\n\nBazel version (if compiling from source): N/A since nightly build reproduces the issue (but when built from source, I use 0.6.1-homebrew)\n\nCUDA/cuDNN version: a GPU is not needed to reproduce the issue (however, it has also been tested with CUDA 8.0.61 / cuDNN 7.0.1)\n\nGPU model and memory: N/A -- a GPU is not needed to reproduce the issue (however, it has also been tested with Tesla K80s)\n\nExact command to reproduce: See minimal reproducible example below\n\nDescribe the problem\n\nThe new tf.data.Dataset API contains a map function with a num_parallel_calls parameter, which allows elements to be processed in parallel by multiple threads. Although not explicitly mentioned in the API docs, prior discussions (such as a comment from today) have indicated that the map function should be deterministic (w.r.t. the graph seed) even if num_parallel_calls > 1. I have observed that if the function being mapped contains only non-random ops, then this determinism is observed (see step 2 below). However, if the the function being mapped contains a random op, the results become non-deterministic for all values of num_parallel_calls > 1. This is unexpected, and prevents training experiments from being reproducible, unless num_parallel_calls == 1. Also, please note that the example below serves as a minimal example to reproduce the issue. The real scenario involves running data augmentation during training.\n\npip3 install tf-nightly\n\nRun the following code to observe that map functions with only non-random ops are deterministic for all values of num_parallel_calls, which is the expected behavior:\n\nimport numpy as np import tensorflow as tf def test(threads): np.random.seed(42) tf.set_random_seed(42) images = np.random.rand(100, 64, 64, 3).astype(np.float32) def get_data(): dataset = tf.data.Dataset.from_tensor_slices(images) # some initial dataset dataset = dataset.map(lambda x: x * 2, num_parallel_calls=threads) # this works fine always dataset = dataset.batch(32) x = dataset.make_one_shot_iterator().get_next() return x # execution 1 x = get_data() with tf.Session() as sess: x_batch1 = sess.run(x) # clear out everything tf.reset_default_graph() # execution 2 x = get_data() with tf.Session() as sess: x_batch2 = sess.run(x) # results should be equivalent assert np.allclose(x_batch1, x_batch2) test(1) # works with 1 thread! test(15) # works with >1 threads!\n\nRun the following code to observe that map functions with random ops are deterministic if num_parallel_calls == 1, but are non-deterministic for values of num_parallel_calls > 1, which seems to me to be an unexpected behavior:\n\nimport numpy as np import tensorflow as tf def test(threads): np.random.seed(42) tf.set_random_seed(42) images = np.random.rand(100, 64, 64, 3).astype(np.float32) def get_data(): dataset = tf.data.Dataset.from_tensor_slices(images) # some initial dataset # ONLY DIFFERENCE IS THE BELOW LINE: dataset = dataset.map(lambda image: tf.image.random_hue(image, 0.04, seed=42), num_parallel_calls=threads) # ONLY DIFFERENCE IS THE ABOVE LINE ^^^: dataset = dataset.batch(32) x = dataset.make_one_shot_iterator().get_next() return x # execution 1 x = get_data() with tf.Session() as sess: x_batch1 = sess.run(x) # clear out everything tf.reset_default_graph() # execution 2 x = get_data() with tf.Session() as sess: x_batch2 = sess.run(x) # results should be equivalent assert np.allclose(x_batch1, x_batch2) test(1) # works with 1 thread! test(15) # fails with >1 threads!\n\nObserve that swapping out the map line above with an entirely different random op such as dataset = dataset.map(lambda x: x * tf.random_normal([64, 64, 3], seed=42), num_parallel_calls=threads) is also non-deterministic for values of num_parallel_calls > 1.\n\nThe text was updated successfully, but these errors were encountered:\n\nskye added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\ndusenberrymw commented\n\nAdditionally, I would like to note that for steps 3 and 4, an op-level seed must be set on the random ops used within the map function, regardless of whether or not a graph-level seed is set. This appears to be an inconsistent behavior with that of the documentation for tf.set_random_seed():\n\nIf the graph-level seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the graph-level seed so that it gets a unique random sequence.\n\nI\'m not familiar with tensorflow codes, but I tried to trace this. Looks like if we can\'t assign the exact thread in thread pool to run for each input element, we can\'t make sure the parallel map functions with random ops are deterministic. However, assigning thread sounds counterintuitive to the nature of thread pool.\n\nUnfortunately, this is ""expected behavior"" due to the way tf.random_uniform() (used inside tf.image.random_hue()) and the other RNG ops are implemented. The parallel invocations of map will race to access the mutable RNG state inside the op, and different invocations will see a non-deterministically chosen element of the same sequence. Currently, the only way to ensure deterministic results from Dataset.map() that contains an RNG op is to set num_parallel_calls=1.\n\nIn principle, you could slice your map() function so that the random number generation in a serial fashion, and the compute-intensive part of the op in a parallel map. For example, it\'s possible to do this manually for tf.image.random_hue(), because it is simply a composition of tf.adjust_hue(..., tf.random_uniform(...)):\n\nimport numpy as np import tensorflow as tf def test(threads): np.random.seed(42) tf.set_random_seed(42) images = np.random.rand(100, 64, 64, 3).astype(np.float32) def get_data(): dataset = tf.data.Dataset.from_tensor_slices(images) # Perform the random number generation in a single-threaded map(). dataset = dataset.map( lambda image: (image, tf.random_uniform([], -0.04, 0.04, seed=42)), num_parallel_calls=1) # Perform the compute-intensive hue adjustment in a multi-threaded map(). dataset = dataset.map( lambda image, adjustment: tf.image.adjust_hue(image, adjustment), num_parallel_calls=threads) dataset = dataset.batch(32) x = dataset.make_one_shot_iterator().get_next() return x # execution 1 x = get_data() with tf.Session() as sess: x_batch1 = sess.run(x) # clear out everything tf.reset_default_graph() # execution 2 x = get_data() with tf.Session() as sess: x_batch2 = sess.run(x) # results should be equivalent assert np.allclose(x_batch1, x_batch2) test(1) # works with 1 thread! test(15) # works with >1 threads!\n\nHowever, this manual approach might not scale to a real program. In our CNN benchmarks, we\'ve been using a sequence number to deterministically map ""random"" perturbations onto input images. In future we might consider doing this kind of slicing automatically, but that\'s probably some way off.\n\nHope this helps though!\n\nmrry added stat:awaiting response\n\nStatus - Awaiting response from author and removed stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower labels\n\ntensorflowbutler commented\n\nIt has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.\n\ntensorflowbutler commented\n\nIt has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.\n\nmrry closed this as completed\n\nwoodshop mentioned this issue\n\nDataset.map() with random_shuffle() and num_parallel_calls=1 has non-deterministic result #23789\n\nTimZaman mentioned this issue\n\nRandom seed not set in graph context of Dataset#map #29101\n\nremydubois mentioned this issue\n\nNon-deterministic access to Random Number Generator in tf.data.Dataset.map with num_parallel_calls > 1 #34909\n\nzaccharieramzi commented\n\nI just stumbled on this behaviour. I wanted to understand whether this was something that could (and would) be fixed in the future?\n\nIf not, I think it would be nice to have a warning in the docs, especially since there is a deterministic keyword in the docs of map. I could submit a PR for that if needed.\n\n@zaccharieramzi @mrry I encountered this nondeterminism in map as well, when it is used with the random augmentation function like tf.image.random_brightness and num_parallel_calls > 1.\n\nI tried setting deterministic = True but it didn\'t work.\n\nBy the way, I\'ve already called a function for deterministic results as below.\n\ndef seed_everything(seed_value): tf.random.set_seed(seed_value) os.environ[\'TF_DETERMINISTIC_OPS\'] = \'1\'\n\nAnd the tf version I\'m using is tf-nightly-gpu 2.5.0-dev20201130.\n\nduncanriach mentioned this issue\n\nUsage of numpy.random.Generator to deal with Data-Loader Parallelism NVIDIA/framework-reproducibility#36\n\nduncanriach commented\n\nThe work-around suggested by @mrry can be extended using the stateless random image ops. For example, an early stage in your tf.data.Dataloader pipeline could append a (deterministic) random seed to each example using a single-threaded (num_parallel_calls=1) map. Then, any subsequent stateless random image op in a parallel stage (num_parallel_calls > 1) could use the seed associated with the example. This would require you replacing tf.image.random_brightness with tf.image.stateless_random_brightness in your example.\n\nThe advantage of using the relatively newly added stateless random image ops in this way is that you only have to inject one random number per-example into the pipeline and that one random number can be used for all the stateless random image ops (as the op\'s seed parameter).\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nstat:awaiting response\n\nStatus - Awaiting response from author\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_2', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nNon-determinism from tf.data.Dataset.map with random ops #13932\n\ndusenberrymw opened this issue\n\nOct 24, 2017 · 9 comments\n\nNon-determinism from tf.data.Dataset.map with random ops #13932\n\ndusenberrymw opened this issue\n\nOct 24, 2017 · 9 comments\n\nstat:awaiting response\n\nStatus - Awaiting response from author\n\ndusenberrymw commented\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes -- please see the minimal reproducible example script below.\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.12, Linux CentOS 7 (4.6.6-300.el7.centos.x86_64)\n\nTensorFlow installed from (source or binary): pip3 install tf-nightly (also happens when built from source)\n\nTensorFlow version (use command below): v1.3.0-rc1-3690-g9b9cbbe 1.5.0-dev20171023\n\nPython version: 3.6.3\n\nBazel version (if compiling from source): N/A since nightly build reproduces the issue (but when built from source, I use 0.6.1-homebrew)\n\nCUDA/cuDNN version: a GPU is not needed to reproduce the issue (however, it has also been tested with CUDA 8.0.61 / cuDNN 7.0.1)\n\nGPU model and memory: N/A -- a GPU is not needed to reproduce the issue (however, it has also been tested with Tesla K80s)\n\nExact command to reproduce: See minimal reproducible example below\n\nDescribe the problem\n\nThe new tf.data.Dataset API contains a map function with a num_parallel_calls parameter, which allows elements to be processed in parallel by multiple threads. Although not explicitly mentioned in the API docs, prior discussions (such as a comment from today) have indicated that the map function should be deterministic (w.r.t. the graph seed) even if num_parallel_calls > 1. I have observed that if the function being mapped contains only non-random ops, then this determinism is observed (see step 2 below). However, if the the function being mapped contains a random op, the results become non-deterministic for all values of num_parallel_calls > 1. This is unexpected, and prevents training experiments from being reproducible, unless num_parallel_calls == 1. Also, please note that the example below serves as a minimal example to reproduce the issue. The real scenario involves running data augmentation during training.\n\npip3 install tf-nightly\n\nRun the following code to observe that map functions with only non-random ops are deterministic for all values of num_parallel_calls, which is the expected behavior:\n\nimport numpy as np import tensorflow as tf def test(threads): np.random.seed(42) tf.set_random_seed(42) images = np.random.rand(100, 64, 64, 3).astype(np.float32) def get_data(): dataset = tf.data.Dataset.from_tensor_slices(images) # some initial dataset dataset = dataset.map(lambda x: x * 2, num_parallel_calls=threads) # this works fine always dataset = dataset.batch(32) x = dataset.make_one_shot_iterator().get_next() return x # execution 1 x = get_data() with tf.Session() as sess: x_batch1 = sess.run(x) # clear out everything tf.reset_default_graph() # execution 2 x = get_data() with tf.Session() as sess: x_batch2 = sess.run(x) # results should be equivalent assert np.allclose(x_batch1, x_batch2) test(1) # works with 1 thread! test(15) # works with >1 threads!\n\nRun the following code to observe that map functions with random ops are deterministic if num_parallel_calls == 1, but are non-deterministic for values of num_parallel_calls > 1, which seems to me to be an unexpected behavior:\n\nimport numpy as np import tensorflow as tf def test(threads): np.random.seed(42) tf.set_random_seed(42) images = np.random.rand(100, 64, 64, 3).astype(np.float32) def get_data(): dataset = tf.data.Dataset.from_tensor_slices(images) # some initial dataset # ONLY DIFFERENCE IS THE BELOW LINE: dataset = dataset.map(lambda image: tf.image.random_hue(image, 0.04, seed=42), num_parallel_calls=threads) # ONLY DIFFERENCE IS THE ABOVE LINE ^^^: dataset = dataset.batch(32) x = dataset.make_one_shot_iterator().get_next() return x # execution 1 x = get_data() with tf.Session() as sess: x_batch1 = sess.run(x) # clear out everything tf.reset_default_graph() # execution 2 x = get_data() with tf.Session() as sess: x_batch2 = sess.run(x) # results should be equivalent assert np.allclose(x_batch1, x_batch2) test(1) # works with 1 thread! test(15) # fails with >1 threads!\n\nObserve that swapping out the map line above with an entirely different random op such as dataset = dataset.map(lambda x: x * tf.random_normal([64, 64, 3], seed=42), num_parallel_calls=threads) is also non-deterministic for values of num_parallel_calls > 1.\n\nThe text was updated successfully, but these errors were encountered:\n\nskye added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\ndusenberrymw commented\n\nAdditionally, I would like to note that for steps 3 and 4, an op-level seed must be set on the random ops used within the map function, regardless of whether or not a graph-level seed is set. This appears to be an inconsistent behavior with that of the documentation for tf.set_random_seed():\n\nIf the graph-level seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the graph-level seed so that it gets a unique random sequence.\n\nI\'m not familiar with tensorflow codes, but I tried to trace this. Looks like if we can\'t assign the exact thread in thread pool to run for each input element, we can\'t make sure the parallel map functions with random ops are deterministic. However, assigning thread sounds counterintuitive to the nature of thread pool.\n\nUnfortunately, this is ""expected behavior"" due to the way tf.random_uniform() (used inside tf.image.random_hue()) and the other RNG ops are implemented. The parallel invocations of map will race to access the mutable RNG state inside the op, and different invocations will see a non-deterministically chosen element of the same sequence. Currently, the only way to ensure deterministic results from Dataset.map() that contains an RNG op is to set num_parallel_calls=1.\n\nIn principle, you could slice your map() function so that the random number generation in a serial fashion, and the compute-intensive part of the op in a parallel map. For example, it\'s possible to do this manually for tf.image.random_hue(), because it is simply a composition of tf.adjust_hue(..., tf.random_uniform(...)):\n\nimport numpy as np import tensorflow as tf def test(threads): np.random.seed(42) tf.set_random_seed(42) images = np.random.rand(100, 64, 64, 3).astype(np.float32) def get_data(): dataset = tf.data.Dataset.from_tensor_slices(images) # Perform the random number generation in a single-threaded map(). dataset = dataset.map( lambda image: (image, tf.random_uniform([], -0.04, 0.04, seed=42)), num_parallel_calls=1) # Perform the compute-intensive hue adjustment in a multi-threaded map(). dataset = dataset.map( lambda image, adjustment: tf.image.adjust_hue(image, adjustment), num_parallel_calls=threads) dataset = dataset.batch(32) x = dataset.make_one_shot_iterator().get_next() return x # execution 1 x = get_data() with tf.Session() as sess: x_batch1 = sess.run(x) # clear out everything tf.reset_default_graph() # execution 2 x = get_data() with tf.Session() as sess: x_batch2 = sess.run(x) # results should be equivalent assert np.allclose(x_batch1, x_batch2) test(1) # works with 1 thread! test(15) # works with >1 threads!\n\nHowever, this manual approach might not scale to a real program. In our CNN benchmarks, we\'ve been using a sequence number to deterministically map ""random"" perturbations onto input images. In future we might consider doing this kind of slicing automatically, but that\'s probably some way off.\n\nHope this helps though!\n\nmrry added stat:awaiting response\n\nStatus - Awaiting response from author and removed stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower labels\n\ntensorflowbutler commented\n\nIt has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.\n\ntensorflowbutler commented\n\nIt has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.\n\nmrry closed this as completed\n\nwoodshop mentioned this issue\n\nDataset.map() with random_shuffle() and num_parallel_calls=1 has non-deterministic result #23789\n\nTimZaman mentioned this issue\n\nRandom seed not set in graph context of Dataset#map #29101\n\nremydubois mentioned this issue\n\nNon-deterministic access to Random Number Generator in tf.data.Dataset.map with num_parallel_calls > 1 #34909\n\nzaccharieramzi commented\n\nI just stumbled on this behaviour. I wanted to understand whether this was something that could (and would) be fixed in the future?\n\nIf not, I think it would be nice to have a warning in the docs, especially since there is a deterministic keyword in the docs of map. I could submit a PR for that if needed.\n\n@zaccharieramzi @mrry I encountered this nondeterminism in map as well, when it is used with the random augmentation function like tf.image.random_brightness and num_parallel_calls > 1.\n\nI tried setting deterministic = True but it didn\'t work.\n\nBy the way, I\'ve already called a function for deterministic results as below.\n\ndef seed_everything(seed_value): tf.random.set_seed(seed_value) os.environ[\'TF_DETERMINISTIC_OPS\'] = \'1\'\n\nAnd the tf version I\'m using is tf-nightly-gpu 2.5.0-dev20201130.\n\nduncanriach mentioned this issue\n\nUsage of numpy.random.Generator to deal with Data-Loader Parallelism NVIDIA/framework-reproducibility#36\n\nduncanriach commented\n\nThe work-around suggested by @mrry can be extended using the stateless random image ops. For example, an early stage in your tf.data.Dataloader pipeline could append a (deterministic) random seed to each example using a single-threaded (num_parallel_calls=1) map. Then, any subsequent stateless random image op in a parallel stage (num_parallel_calls > 1) could use the seed associated with the example. This would require you replacing tf.image.random_brightness with tf.image.stateless_random_brightness in your example.\n\nThe advantage of using the relatively newly added stateless random image ops in this way is that you only have to inject one random number per-example into the pipeline and that one random number can be used for all the stateless random image ops (as the op\'s seed parameter).\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nstat:awaiting response\n\nStatus - Awaiting response from author\n\nYou can’t perform that action at this time.', 'timestamp': '2024-05-31T21:52:21', 'title': 'Non-determinism from `tf.data.Dataset.map` with random ops · Issue #13932 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/13932'})], [Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nRandom number generation\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nTensorFlow provides a set of pseudo-random number generators (RNG), in the tf.random module. This document describes how you can control the random number generators, and how these generators interact with other tensorflow sub-systems.\n\nNote: The random numbers are not guaranteed to be consistent across TensorFlow versions. See: Version Compatibility\n\nTensorFlow provides two approaches for controlling the random number generation process:\n\nThrough the explicit use of tf.random.Generator objects. Each such object maintains a state (in tf.Variable) that will be changed after each number generation.\n\nThrough the purely-functional stateless random functions like tf.random.stateless_uniform. Calling these functions with the same arguments (which include the seed) and on the same device will always produce the same results.\n\nWarning: The old RNGs from TF 1.x such as tf.random.uniform and tf.random.normal are not yet deprecated but strongly discouraged.\n\nimport tensorflow as tf # Creates some virtual devices (cpu:0, cpu:1, etc.) for using distribution strategy physical_devices = tf.config.list_physical_devices(""CPU"") tf.config.experimental.set_virtual_device_configuration( physical_devices[0], [ tf.config.experimental.VirtualDeviceConfiguration(), tf.config.experimental.VirtualDeviceConfiguration(), tf.config.experimental.VirtualDeviceConfiguration() ])\n\n2024-01-17 02:22:51.386100: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2024-01-17 02:22:51.386148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2024-01-17 02:22:51.387696: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nThe tf.random.Generator class\n\nThe tf.random.Generator class is used in cases where you want each RNG call to produce different results. It maintains an internal state (managed by a tf.Variable object) which will be updated every time random numbers are generated. Because the state is managed by tf.Variable, it enjoys all facilities provided by tf.Variable such as easy checkpointing, automatic control-dependency and thread safety.\n\nYou can get a tf.random.Generator by manually creating an object of the class or call tf.random.get_global_generator() to get the default global generator:\n\ng1 = tf.random.Generator.from_seed(1) print(g1.normal(shape=[2, 3])) g2 = tf.random.get_global_generator() print(g2.normal(shape=[2, 3]))\n\ntf.Tensor( [[ 0.43842277 -0.53439844 -0.07710262] [ 1.5658045 -0.1012345 -0.2744976 ]], shape=(2, 3), dtype=float32) tf.Tensor( [[ 0.24077678 0.39891425 0.03557164] [-0.15206331 -0.7270625 1.8158559 ]], shape=(2, 3), dtype=float32)\n\nThere are multiple ways to create a generator object. The easiest is Generator.from_seed, as shown above, that creates a generator from a seed. A seed is any non-negative integer. from_seed also takes an optional argument alg which is the RNG algorithm that will be used by this generator:\n\ng1 = tf.random.Generator.from_seed(1, alg=\'philox\') print(g1.normal(shape=[2, 3]))\n\ntf.Tensor( [[ 0.43842277 -0.53439844 -0.07710262] [ 1.5658045 -0.1012345 -0.2744976 ]], shape=(2, 3), dtype=float32)\n\nSee the Algorithms section below for more information about it.\n\nAnother way to create a generator is with Generator.from_non_deterministic_state. A generator created this way will start from a non-deterministic state, depending on e.g., time and OS.\n\ng = tf.random.Generator.from_non_deterministic_state() print(g.normal(shape=[2, 3]))\n\ntf.Tensor( [[-0.8503367 -0.8919918 0.688985 ] [-0.51400167 0.57703274 -0.5177701 ]], shape=(2, 3), dtype=float32)\n\nThere are yet other ways to create generators, such as from explicit states, which are not covered by this guide.\n\nWhen using tf.random.get_global_generator to get the global generator, you need to be careful about device placement. The global generator is created (from a non-deterministic state) at the first time tf.random.get_global_generator is called, and placed on the default device at that call. So, for example, if the first site you call tf.random.get_global_generator is within a tf.device(""gpu"") scope, the global generator will be placed on the GPU, and using the global generator later on from the CPU will incur a GPU-to-CPU copy.\n\nThere is also a function tf.random.set_global_generator for replacing the global generator with another generator object. This function should be used with caution though, because the old global generator may have been captured by a tf.function (as a weak reference), and replacing it will cause it to be garbage collected, breaking the tf.function. A better way to reset the global generator is to use one of the ""reset"" functions such as Generator.reset_from_seed, which won\'t create new generator objects.\n\ng = tf.random.Generator.from_seed(1) print(g.normal([])) print(g.normal([])) g.reset_from_seed(1) print(g.normal([]))\n\ntf.Tensor(0.43842277, shape=(), dtype=float32) tf.Tensor(1.6272374, shape=(), dtype=float32) tf.Tensor(0.43842277, shape=(), dtype=float32)\n\nCreating independent random-number streams\n\nIn many applications one needs multiple independent random-number streams, independent in the sense that they won\'t overlap and won\'t have any statistically detectable correlations. This is achieved by using Generator.split to create multiple generators that are guaranteed to be independent of each other (i.e. generating independent streams).\n\ng = tf.random.Generator.from_seed(1) print(g.normal([])) new_gs = g.split(3) for new_g in new_gs: print(new_g.normal([])) print(g.normal([]))\n\ntf.Tensor(0.43842277, shape=(), dtype=float32) tf.Tensor(2.536413, shape=(), dtype=float32) tf.Tensor(0.33186463, shape=(), dtype=float32) tf.Tensor(-0.07144657, shape=(), dtype=float32) tf.Tensor(-0.79253083, shape=(), dtype=float32)\n\nsplit will change the state of the generator on which it is called (g in the above example), similar to an RNG method such as normal. In addition to being independent of each other, the new generators (new_gs) are also guaranteed to be independent of the old one (g).\n\nSpawning new generators is also useful when you want to make sure the generator you use is on the same device as other computations, to avoid the overhead of cross-device copy. For example:\n\nwith tf.device(""cpu""): # change ""cpu"" to the device you want g = tf.random.get_global_generator().split(1)[0] print(g.normal([])) # use of g won\'t cause cross-device copy, unlike the global generator\n\ntf.Tensor(0.4637335, shape=(), dtype=float32)\n\nNote: In theory, you can use constructors such as from_seed instead of split here to obtain a new generator, but by doing so you lose the guarantee that the new generator is independent of the global generator. You will also run the risk that you may accidentally create two generators with the same seed or with seeds that lead to overlapping random-number streams.\n\nYou can do splitting recursively, calling split on split generators. There are no limits (barring integer overflow) on the depth of recursions.\n\nInteraction with tf.function\n\ntf.random.Generator obeys the same rules as tf.Variable when used with tf.function. This includes three aspects.\n\nCreating generators outside tf.function\n\ntf.function can use a generator created outside of it.\n\ng = tf.random.Generator.from_seed(1) @tf.function def foo(): return g.normal([]) print(foo())\n\ntf.Tensor(0.43842277, shape=(), dtype=float32)\n\nThe user needs to make sure that the generator object is still alive (not garbage-collected) when the function is called.\n\nCreating generators inside tf.function\n\nCreation of generators inside a tf.function can only happened during the first run of the function.\n\ng = None @tf.function def foo(): global g if g is None: g = tf.random.Generator.from_seed(1) return g.normal([]) print(foo()) print(foo())\n\ntf.Tensor(0.43842277, shape=(), dtype=float32) tf.Tensor(1.6272374, shape=(), dtype=float32)\n\nPassing generators as arguments to tf.function\n\nWhen used as an argument to a tf.function, different generator objects will cause retracing of the tf.function.\n\nnum_traces = 0 @tf.function def foo(g): global num_traces num_traces += 1 return g.normal([]) foo(tf.random.Generator.from_seed(1)) foo(tf.random.Generator.from_seed(2)) print(num_traces)\n\nNote that this retracing behavior is consistent with tf.Variable:\n\nnum_traces = 0 @tf.function def foo(v): global num_traces num_traces += 1 return v.read_value() foo(tf.Variable(1)) foo(tf.Variable(2)) print(num_traces)\n\nInteraction with distribution strategies\n\nThere are two ways in which Generator interacts with distribution strategies.\n\nCreating generators outside distribution strategies\n\nIf a generator is created outside strategy scopes, all replicas’ access to the generator will be serialized, and hence the replicas will get different random numbers.\n\ng = tf.random.Generator.from_seed(1) strat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): def f(): print(g.normal([])) results = strat.run(f)\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. tf.Tensor(0.43842274, shape=(), dtype=float32) tf.Tensor(1.6272374, shape=(), dtype=float32)\n\nNote that this usage may have performance issues because the generator\'s device is different from the replicas.\n\nCreating generators inside distribution strategies\n\nIf a generator is created inside a strategy scope, each replica will get a different and independent stream of random numbers.\n\nstrat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): g = tf.random.Generator.from_seed(1) print(strat.run(lambda: g.normal([]))) print(strat.run(lambda: g.normal([])))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. PerReplica:{ 0: tf.Tensor(-0.87930447, shape=(), dtype=float32), 1: tf.Tensor(0.020661574, shape=(), dtype=float32) } WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32) }\n\nNote: Currently tf.random.Generator doesn\'t provide an option to let different replicas get identical (instead of different) streams (which is technically not hard). If you have a use case for this feature, please let the TensorFlow developers know.\n\nIf the generator is seeded (e.g. created by Generator.from_seed), the random numbers are determined by the seed, even though different replicas get different and uncorrelated numbers. One can think of a random number generated on a replica as a hash of the replica ID and a ""primary"" random number that is common to all replicas. Hence, the whole system is still deterministic.\n\ntf.random.Generator can also be created inside Strategy.run:\n\nstrat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): def f(): g = tf.random.Generator.from_seed(1) a = g.normal([]) b = g.normal([]) return tf.stack([a, b]) print(strat.run(f)) print(strat.run(f))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. PerReplica:{ 0: tf.Tensor([-0.87930447 -1.5822568 ], shape=(2,), dtype=float32), 1: tf.Tensor([0.02066157 0.77539235], shape=(2,), dtype=float32) } WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. PerReplica:{ 0: tf.Tensor([-0.87930447 -1.5822568 ], shape=(2,), dtype=float32), 1: tf.Tensor([0.02066157 0.77539235], shape=(2,), dtype=float32) }\n\nWe no longer recommend passing tf.random.Generator as arguments to Strategy.run, because Strategy.run generally expects the arguments to be tensors, not generators.\n\nGenerally for saving or serializing you can handle a tf.random.Generator the same way you would handle a tf.Variable or a tf.Module (or its subclasses). In TF there are two mechanisms for serialization: Checkpoint and SavedModel.\n\nGenerators can be freely saved and restored using tf.train.Checkpoint. The random-number stream from the restoring point will be the same as that from the saving point.\n\nfilename = ""./checkpoint"" g = tf.random.Generator.from_seed(1) cp = tf.train.Checkpoint(generator=g) print(g.normal([]))\n\ntf.Tensor(0.43842277, shape=(), dtype=float32)\n\ncp.write(filename) print(""RNG stream from saving point:"") print(g.normal([])) print(g.normal([]))\n\nRNG stream from saving point: tf.Tensor(1.6272374, shape=(), dtype=float32) tf.Tensor(1.6307176, shape=(), dtype=float32)\n\ncp.restore(filename) print(""RNG stream from restoring point:"") print(g.normal([])) print(g.normal([]))\n\nRNG stream from restoring point: tf.Tensor(1.6272374, shape=(), dtype=float32) tf.Tensor(1.6307176, shape=(), dtype=float32)\n\nYou can also save and restore within a distribution strategy:\n\nfilename = ""./checkpoint"" strat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): g = tf.random.Generator.from_seed(1) cp = tf.train.Checkpoint(my_generator=g) print(strat.run(lambda: g.normal([])))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') PerReplica:{ 0: tf.Tensor(-0.87930447, shape=(), dtype=float32), 1: tf.Tensor(0.020661574, shape=(), dtype=float32) }\n\nwith strat.scope(): cp.write(filename) print(""RNG stream from saving point:"") print(strat.run(lambda: g.normal([]))) print(strat.run(lambda: g.normal([])))\n\nRNG stream from saving point: PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32) } PerReplica:{ 0: tf.Tensor(-0.5039703, shape=(), dtype=float32), 1: tf.Tensor(0.1251838, shape=(), dtype=float32) }\n\nwith strat.scope(): cp.restore(filename) print(""RNG stream from restoring point:"") print(strat.run(lambda: g.normal([]))) print(strat.run(lambda: g.normal([])))\n\nRNG stream from restoring point: PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32) } PerReplica:{ 0: tf.Tensor(-0.5039703, shape=(), dtype=float32), 1: tf.Tensor(0.1251838, shape=(), dtype=float32) }\n\nYou should make sure that the replicas don\'t diverge in their RNG call history (e.g. one replica makes one RNG call while another makes two RNG calls) before saving. Otherwise, their internal RNG states will diverge and tf.train.Checkpoint (which only saves the first replica\'s state) won\'t properly restore all the replicas.\n\nYou can also restore a saved checkpoint to a different distribution strategy with a different number of replicas. Because a tf.random.Generator object created in a strategy can only be used in the same strategy, to restore to a different strategy, you have to create a new tf.random.Generator in the target strategy and a new tf.train.Checkpoint for it, as shown in this example:\n\nfilename = ""./checkpoint"" strat1 = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat1.scope(): g1 = tf.random.Generator.from_seed(1) cp1 = tf.train.Checkpoint(my_generator=g1) print(strat1.run(lambda: g1.normal([])))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') PerReplica:{ 0: tf.Tensor(-0.87930447, shape=(), dtype=float32), 1: tf.Tensor(0.020661574, shape=(), dtype=float32) }\n\nwith strat1.scope(): cp1.write(filename) print(""RNG stream from saving point:"") print(strat1.run(lambda: g1.normal([]))) print(strat1.run(lambda: g1.normal([])))\n\nRNG stream from saving point: PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32) } PerReplica:{ 0: tf.Tensor(-0.5039703, shape=(), dtype=float32), 1: tf.Tensor(0.1251838, shape=(), dtype=float32) }\n\nstrat2 = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1"", ""cpu:2""]) with strat2.scope(): g2 = tf.random.Generator.from_seed(1) cp2 = tf.train.Checkpoint(my_generator=g2) cp2.restore(filename) print(""RNG stream from restoring point:"") print(strat2.run(lambda: g2.normal([]))) print(strat2.run(lambda: g2.normal([])))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\', \'/job:localhost/replica:0/task:0/device:CPU:2\') RNG stream from restoring point: PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32), 2: tf.Tensor(0.6851049, shape=(), dtype=float32) } PerReplica:{ 0: tf.Tensor(-0.5039703, shape=(), dtype=float32), 1: tf.Tensor(0.1251838, shape=(), dtype=float32), 2: tf.Tensor(-0.58519536, shape=(), dtype=float32) }\n\nAlthough g1 and cp1 are different objects from g2 and cp2, they are linked via the common checkpoint file filename and object name my_generator. Overlapping replicas between strategies (e.g. cpu:0 and cpu:1 above) will have their RNG streams properly restored like in previous examples. This guarantee doesn\'t cover the case when a generator is saved in a strategy scope and restored outside of any strategy scope or vice versa, because a device outside strategies is treated as different from any replica in a strategy.\n\ntf.random.Generator can be saved to a SavedModel. The generator can be created within a strategy scope. The saving can also happen within a strategy scope.\n\nfilename = ""./saved_model"" class MyModule(tf.Module): def __init__(self): super(MyModule, self).__init__() self.g = tf.random.Generator.from_seed(0) @tf.function def __call__(self): return self.g.normal([]) @tf.function def state(self): return self.g.state strat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): m = MyModule() print(strat.run(m)) print(""state:"", m.state())\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') PerReplica:{ 0: tf.Tensor(-1.4154755, shape=(), dtype=float32), 1: tf.Tensor(-0.11388441, shape=(), dtype=float32) } state: tf.Tensor([256 0 0], shape=(3,), dtype=int64)\n\nwith strat.scope(): tf.saved_model.save(m, filename) print(""RNG stream from saving point:"") print(strat.run(m)) print(""state:"", m.state()) print(strat.run(m)) print(""state:"", m.state())\n\nINFO:tensorflow:Assets written to: ./saved_model/assets RNG stream from saving point: PerReplica:{ 0: tf.Tensor(-0.68758255, shape=(), dtype=float32), 1: tf.Tensor(0.8084062, shape=(), dtype=float32) } state: tf.Tensor([512 0 0], shape=(3,), dtype=int64) PerReplica:{ 0: tf.Tensor(-0.27342677, shape=(), dtype=float32), 1: tf.Tensor(-0.53093255, shape=(), dtype=float32) } state: tf.Tensor([768 0 0], shape=(3,), dtype=int64)\n\nimported = tf.saved_model.load(filename) print(""RNG stream from loading point:"") print(""state:"", imported.state()) print(imported()) print(""state:"", imported.state()) print(imported()) print(""state:"", imported.state())\n\nRNG stream from loading point: state: tf.Tensor([256 0 0], shape=(3,), dtype=int64) tf.Tensor(-1.0359411, shape=(), dtype=float32) state: tf.Tensor([512 0 0], shape=(3,), dtype=int64) tf.Tensor(-0.06425078, shape=(), dtype=float32) state: tf.Tensor([768 0 0], shape=(3,), dtype=int64)\n\nLoading a SavedModel containing tf.random.Generator into a distribution strategy is not recommended because the replicas will all generate the same random-number stream (which is because replica ID is frozen in SavedModel\'s graph).\n\nLoading a distributed tf.random.Generator (a generator created within a distribution strategy) into a non-strategy environment, like the above example, also has a caveat. The RNG state will be properly restored, but the random numbers generated will be different from the original generator in its strategy (again because a device outside strategies is treated as different from any replica in a strategy).\n\nUsage of stateless RNGs is simple. Since they are just pure functions, there is no state or side effect involved.\n\nprint(tf.random.stateless_normal(shape=[2, 3], seed=[1, 2])) print(tf.random.stateless_normal(shape=[2, 3], seed=[1, 2]))\n\ntf.Tensor( [[ 0.5441101 0.20738031 0.07356433] [ 0.04643455 -1.30159 -0.95385665]], shape=(2, 3), dtype=float32) tf.Tensor( [[ 0.5441101 0.20738031 0.07356433] [ 0.04643455 -1.30159 -0.95385665]], shape=(2, 3), dtype=float32)\n\nEvery stateless RNG requires a seed argument, which needs to be an integer Tensor of shape [2]. The results of the op are fully determined by this seed.\n\nThe RNG algorithm used by stateless RNGs is device-dependent, meaning the same op running on a different device may produce different outputs.\n\nBoth the tf.random.Generator class and the stateless functions support the Philox algorithm (written as ""philox"" or tf.random.Algorithm.PHILOX) on all devices.\n\nDifferent devices will generate the same integer numbers, if using the same algorithm and starting from the same state. They will also generate ""almost the same"" float-point numbers, though there may be small numerical discrepancies caused by the different ways the devices carry out the float-point computation (e.g. reduction order).\n\nOn XLA-driven devices (such as TPU, and also CPU/GPU when XLA is enabled) the ThreeFry algorithm (written as ""threefry"" or tf.random.Algorithm.THREEFRY) is also supported. This algorithm is fast on TPU but slow on CPU/GPU compared to Philox.\n\nSee paper \'Parallel Random Numbers: As Easy as 1, 2, 3\' for more details about these algorithms.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-01-17 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_5', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nRandom number generation\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nTensorFlow provides a set of pseudo-random number generators (RNG), in the tf.random module. This document describes how you can control the random number generators, and how these generators interact with other tensorflow sub-systems.\n\nNote: The random numbers are not guaranteed to be consistent across TensorFlow versions. See: Version Compatibility\n\nTensorFlow provides two approaches for controlling the random number generation process:\n\nThrough the explicit use of tf.random.Generator objects. Each such object maintains a state (in tf.Variable) that will be changed after each number generation.\n\nThrough the purely-functional stateless random functions like tf.random.stateless_uniform. Calling these functions with the same arguments (which include the seed) and on the same device will always produce the same results.\n\nWarning: The old RNGs from TF 1.x such as tf.random.uniform and tf.random.normal are not yet deprecated but strongly discouraged.\n\nimport tensorflow as tf # Creates some virtual devices (cpu:0, cpu:1, etc.) for using distribution strategy physical_devices = tf.config.list_physical_devices(""CPU"") tf.config.experimental.set_virtual_device_configuration( physical_devices[0], [ tf.config.experimental.VirtualDeviceConfiguration(), tf.config.experimental.VirtualDeviceConfiguration(), tf.config.experimental.VirtualDeviceConfiguration() ])\n\n2024-01-17 02:22:51.386100: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2024-01-17 02:22:51.386148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2024-01-17 02:22:51.387696: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nThe tf.random.Generator class\n\nThe tf.random.Generator class is used in cases where you want each RNG call to produce different results. It maintains an internal state (managed by a tf.Variable object) which will be updated every time random numbers are generated. Because the state is managed by tf.Variable, it enjoys all facilities provided by tf.Variable such as easy checkpointing, automatic control-dependency and thread safety.\n\nYou can get a tf.random.Generator by manually creating an object of the class or call tf.random.get_global_generator() to get the default global generator:\n\ng1 = tf.random.Generator.from_seed(1) print(g1.normal(shape=[2, 3])) g2 = tf.random.get_global_generator() print(g2.normal(shape=[2, 3]))\n\ntf.Tensor( [[ 0.43842277 -0.53439844 -0.07710262] [ 1.5658045 -0.1012345 -0.2744976 ]], shape=(2, 3), dtype=float32) tf.Tensor( [[ 0.24077678 0.39891425 0.03557164] [-0.15206331 -0.7270625 1.8158559 ]], shape=(2, 3), dtype=float32)\n\nThere are multiple ways to create a generator object. The easiest is Generator.from_seed, as shown above, that creates a generator from a seed. A seed is any non-negative integer. from_seed also takes an optional argument alg which is the RNG algorithm that will be used by this generator:\n\ng1 = tf.random.Generator.from_seed(1, alg=\'philox\') print(g1.normal(shape=[2, 3]))\n\ntf.Tensor( [[ 0.43842277 -0.53439844 -0.07710262] [ 1.5658045 -0.1012345 -0.2744976 ]], shape=(2, 3), dtype=float32)\n\nSee the Algorithms section below for more information about it.\n\nAnother way to create a generator is with Generator.from_non_deterministic_state. A generator created this way will start from a non-deterministic state, depending on e.g., time and OS.\n\ng = tf.random.Generator.from_non_deterministic_state() print(g.normal(shape=[2, 3]))\n\ntf.Tensor( [[-0.8503367 -0.8919918 0.688985 ] [-0.51400167 0.57703274 -0.5177701 ]], shape=(2, 3), dtype=float32)\n\nThere are yet other ways to create generators, such as from explicit states, which are not covered by this guide.\n\nWhen using tf.random.get_global_generator to get the global generator, you need to be careful about device placement. The global generator is created (from a non-deterministic state) at the first time tf.random.get_global_generator is called, and placed on the default device at that call. So, for example, if the first site you call tf.random.get_global_generator is within a tf.device(""gpu"") scope, the global generator will be placed on the GPU, and using the global generator later on from the CPU will incur a GPU-to-CPU copy.\n\nThere is also a function tf.random.set_global_generator for replacing the global generator with another generator object. This function should be used with caution though, because the old global generator may have been captured by a tf.function (as a weak reference), and replacing it will cause it to be garbage collected, breaking the tf.function. A better way to reset the global generator is to use one of the ""reset"" functions such as Generator.reset_from_seed, which won\'t create new generator objects.\n\ng = tf.random.Generator.from_seed(1) print(g.normal([])) print(g.normal([])) g.reset_from_seed(1) print(g.normal([]))\n\ntf.Tensor(0.43842277, shape=(), dtype=float32) tf.Tensor(1.6272374, shape=(), dtype=float32) tf.Tensor(0.43842277, shape=(), dtype=float32)\n\nCreating independent random-number streams\n\nIn many applications one needs multiple independent random-number streams, independent in the sense that they won\'t overlap and won\'t have any statistically detectable correlations. This is achieved by using Generator.split to create multiple generators that are guaranteed to be independent of each other (i.e. generating independent streams).\n\ng = tf.random.Generator.from_seed(1) print(g.normal([])) new_gs = g.split(3) for new_g in new_gs: print(new_g.normal([])) print(g.normal([]))\n\ntf.Tensor(0.43842277, shape=(), dtype=float32) tf.Tensor(2.536413, shape=(), dtype=float32) tf.Tensor(0.33186463, shape=(), dtype=float32) tf.Tensor(-0.07144657, shape=(), dtype=float32) tf.Tensor(-0.79253083, shape=(), dtype=float32)\n\nsplit will change the state of the generator on which it is called (g in the above example), similar to an RNG method such as normal. In addition to being independent of each other, the new generators (new_gs) are also guaranteed to be independent of the old one (g).\n\nSpawning new generators is also useful when you want to make sure the generator you use is on the same device as other computations, to avoid the overhead of cross-device copy. For example:\n\nwith tf.device(""cpu""): # change ""cpu"" to the device you want g = tf.random.get_global_generator().split(1)[0] print(g.normal([])) # use of g won\'t cause cross-device copy, unlike the global generator\n\ntf.Tensor(0.4637335, shape=(), dtype=float32)\n\nNote: In theory, you can use constructors such as from_seed instead of split here to obtain a new generator, but by doing so you lose the guarantee that the new generator is independent of the global generator. You will also run the risk that you may accidentally create two generators with the same seed or with seeds that lead to overlapping random-number streams.\n\nYou can do splitting recursively, calling split on split generators. There are no limits (barring integer overflow) on the depth of recursions.\n\nInteraction with tf.function\n\ntf.random.Generator obeys the same rules as tf.Variable when used with tf.function. This includes three aspects.\n\nCreating generators outside tf.function\n\ntf.function can use a generator created outside of it.\n\ng = tf.random.Generator.from_seed(1) @tf.function def foo(): return g.normal([]) print(foo())\n\ntf.Tensor(0.43842277, shape=(), dtype=float32)\n\nThe user needs to make sure that the generator object is still alive (not garbage-collected) when the function is called.\n\nCreating generators inside tf.function\n\nCreation of generators inside a tf.function can only happened during the first run of the function.\n\ng = None @tf.function def foo(): global g if g is None: g = tf.random.Generator.from_seed(1) return g.normal([]) print(foo()) print(foo())\n\ntf.Tensor(0.43842277, shape=(), dtype=float32) tf.Tensor(1.6272374, shape=(), dtype=float32)\n\nPassing generators as arguments to tf.function\n\nWhen used as an argument to a tf.function, different generator objects will cause retracing of the tf.function.\n\nnum_traces = 0 @tf.function def foo(g): global num_traces num_traces += 1 return g.normal([]) foo(tf.random.Generator.from_seed(1)) foo(tf.random.Generator.from_seed(2)) print(num_traces)\n\nNote that this retracing behavior is consistent with tf.Variable:\n\nnum_traces = 0 @tf.function def foo(v): global num_traces num_traces += 1 return v.read_value() foo(tf.Variable(1)) foo(tf.Variable(2)) print(num_traces)\n\nInteraction with distribution strategies\n\nThere are two ways in which Generator interacts with distribution strategies.\n\nCreating generators outside distribution strategies\n\nIf a generator is created outside strategy scopes, all replicas’ access to the generator will be serialized, and hence the replicas will get different random numbers.\n\ng = tf.random.Generator.from_seed(1) strat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): def f(): print(g.normal([])) results = strat.run(f)\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. tf.Tensor(0.43842274, shape=(), dtype=float32) tf.Tensor(1.6272374, shape=(), dtype=float32)\n\nNote that this usage may have performance issues because the generator\'s device is different from the replicas.\n\nCreating generators inside distribution strategies\n\nIf a generator is created inside a strategy scope, each replica will get a different and independent stream of random numbers.\n\nstrat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): g = tf.random.Generator.from_seed(1) print(strat.run(lambda: g.normal([]))) print(strat.run(lambda: g.normal([])))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. PerReplica:{ 0: tf.Tensor(-0.87930447, shape=(), dtype=float32), 1: tf.Tensor(0.020661574, shape=(), dtype=float32) } WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32) }\n\nNote: Currently tf.random.Generator doesn\'t provide an option to let different replicas get identical (instead of different) streams (which is technically not hard). If you have a use case for this feature, please let the TensorFlow developers know.\n\nIf the generator is seeded (e.g. created by Generator.from_seed), the random numbers are determined by the seed, even though different replicas get different and uncorrelated numbers. One can think of a random number generated on a replica as a hash of the replica ID and a ""primary"" random number that is common to all replicas. Hence, the whole system is still deterministic.\n\ntf.random.Generator can also be created inside Strategy.run:\n\nstrat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): def f(): g = tf.random.Generator.from_seed(1) a = g.normal([]) b = g.normal([]) return tf.stack([a, b]) print(strat.run(f)) print(strat.run(f))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. PerReplica:{ 0: tf.Tensor([-0.87930447 -1.5822568 ], shape=(2,), dtype=float32), 1: tf.Tensor([0.02066157 0.77539235], shape=(2,), dtype=float32) } WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance. PerReplica:{ 0: tf.Tensor([-0.87930447 -1.5822568 ], shape=(2,), dtype=float32), 1: tf.Tensor([0.02066157 0.77539235], shape=(2,), dtype=float32) }\n\nWe no longer recommend passing tf.random.Generator as arguments to Strategy.run, because Strategy.run generally expects the arguments to be tensors, not generators.\n\nGenerally for saving or serializing you can handle a tf.random.Generator the same way you would handle a tf.Variable or a tf.Module (or its subclasses). In TF there are two mechanisms for serialization: Checkpoint and SavedModel.\n\nGenerators can be freely saved and restored using tf.train.Checkpoint. The random-number stream from the restoring point will be the same as that from the saving point.\n\nfilename = ""./checkpoint"" g = tf.random.Generator.from_seed(1) cp = tf.train.Checkpoint(generator=g) print(g.normal([]))\n\ntf.Tensor(0.43842277, shape=(), dtype=float32)\n\ncp.write(filename) print(""RNG stream from saving point:"") print(g.normal([])) print(g.normal([]))\n\nRNG stream from saving point: tf.Tensor(1.6272374, shape=(), dtype=float32) tf.Tensor(1.6307176, shape=(), dtype=float32)\n\ncp.restore(filename) print(""RNG stream from restoring point:"") print(g.normal([])) print(g.normal([]))\n\nRNG stream from restoring point: tf.Tensor(1.6272374, shape=(), dtype=float32) tf.Tensor(1.6307176, shape=(), dtype=float32)\n\nYou can also save and restore within a distribution strategy:\n\nfilename = ""./checkpoint"" strat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): g = tf.random.Generator.from_seed(1) cp = tf.train.Checkpoint(my_generator=g) print(strat.run(lambda: g.normal([])))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') PerReplica:{ 0: tf.Tensor(-0.87930447, shape=(), dtype=float32), 1: tf.Tensor(0.020661574, shape=(), dtype=float32) }\n\nwith strat.scope(): cp.write(filename) print(""RNG stream from saving point:"") print(strat.run(lambda: g.normal([]))) print(strat.run(lambda: g.normal([])))\n\nRNG stream from saving point: PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32) } PerReplica:{ 0: tf.Tensor(-0.5039703, shape=(), dtype=float32), 1: tf.Tensor(0.1251838, shape=(), dtype=float32) }\n\nwith strat.scope(): cp.restore(filename) print(""RNG stream from restoring point:"") print(strat.run(lambda: g.normal([]))) print(strat.run(lambda: g.normal([])))\n\nRNG stream from restoring point: PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32) } PerReplica:{ 0: tf.Tensor(-0.5039703, shape=(), dtype=float32), 1: tf.Tensor(0.1251838, shape=(), dtype=float32) }\n\nYou should make sure that the replicas don\'t diverge in their RNG call history (e.g. one replica makes one RNG call while another makes two RNG calls) before saving. Otherwise, their internal RNG states will diverge and tf.train.Checkpoint (which only saves the first replica\'s state) won\'t properly restore all the replicas.\n\nYou can also restore a saved checkpoint to a different distribution strategy with a different number of replicas. Because a tf.random.Generator object created in a strategy can only be used in the same strategy, to restore to a different strategy, you have to create a new tf.random.Generator in the target strategy and a new tf.train.Checkpoint for it, as shown in this example:\n\nfilename = ""./checkpoint"" strat1 = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat1.scope(): g1 = tf.random.Generator.from_seed(1) cp1 = tf.train.Checkpoint(my_generator=g1) print(strat1.run(lambda: g1.normal([])))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') PerReplica:{ 0: tf.Tensor(-0.87930447, shape=(), dtype=float32), 1: tf.Tensor(0.020661574, shape=(), dtype=float32) }\n\nwith strat1.scope(): cp1.write(filename) print(""RNG stream from saving point:"") print(strat1.run(lambda: g1.normal([]))) print(strat1.run(lambda: g1.normal([])))\n\nRNG stream from saving point: PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32) } PerReplica:{ 0: tf.Tensor(-0.5039703, shape=(), dtype=float32), 1: tf.Tensor(0.1251838, shape=(), dtype=float32) }\n\nstrat2 = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1"", ""cpu:2""]) with strat2.scope(): g2 = tf.random.Generator.from_seed(1) cp2 = tf.train.Checkpoint(my_generator=g2) cp2.restore(filename) print(""RNG stream from restoring point:"") print(strat2.run(lambda: g2.normal([]))) print(strat2.run(lambda: g2.normal([])))\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\', \'/job:localhost/replica:0/task:0/device:CPU:2\') RNG stream from restoring point: PerReplica:{ 0: tf.Tensor(-1.5822568, shape=(), dtype=float32), 1: tf.Tensor(0.77539235, shape=(), dtype=float32), 2: tf.Tensor(0.6851049, shape=(), dtype=float32) } PerReplica:{ 0: tf.Tensor(-0.5039703, shape=(), dtype=float32), 1: tf.Tensor(0.1251838, shape=(), dtype=float32), 2: tf.Tensor(-0.58519536, shape=(), dtype=float32) }\n\nAlthough g1 and cp1 are different objects from g2 and cp2, they are linked via the common checkpoint file filename and object name my_generator. Overlapping replicas between strategies (e.g. cpu:0 and cpu:1 above) will have their RNG streams properly restored like in previous examples. This guarantee doesn\'t cover the case when a generator is saved in a strategy scope and restored outside of any strategy scope or vice versa, because a device outside strategies is treated as different from any replica in a strategy.\n\ntf.random.Generator can be saved to a SavedModel. The generator can be created within a strategy scope. The saving can also happen within a strategy scope.\n\nfilename = ""./saved_model"" class MyModule(tf.Module): def __init__(self): super(MyModule, self).__init__() self.g = tf.random.Generator.from_seed(0) @tf.function def __call__(self): return self.g.normal([]) @tf.function def state(self): return self.g.state strat = tf.distribute.MirroredStrategy(devices=[""cpu:0"", ""cpu:1""]) with strat.scope(): m = MyModule() print(strat.run(m)) print(""state:"", m.state())\n\nINFO:tensorflow:Using MirroredStrategy with devices (\'/job:localhost/replica:0/task:0/device:CPU:0\', \'/job:localhost/replica:0/task:0/device:CPU:1\') PerReplica:{ 0: tf.Tensor(-1.4154755, shape=(), dtype=float32), 1: tf.Tensor(-0.11388441, shape=(), dtype=float32) } state: tf.Tensor([256 0 0], shape=(3,), dtype=int64)\n\nwith strat.scope(): tf.saved_model.save(m, filename) print(""RNG stream from saving point:"") print(strat.run(m)) print(""state:"", m.state()) print(strat.run(m)) print(""state:"", m.state())\n\nINFO:tensorflow:Assets written to: ./saved_model/assets RNG stream from saving point: PerReplica:{ 0: tf.Tensor(-0.68758255, shape=(), dtype=float32), 1: tf.Tensor(0.8084062, shape=(), dtype=float32) } state: tf.Tensor([512 0 0], shape=(3,), dtype=int64) PerReplica:{ 0: tf.Tensor(-0.27342677, shape=(), dtype=float32), 1: tf.Tensor(-0.53093255, shape=(), dtype=float32) } state: tf.Tensor([768 0 0], shape=(3,), dtype=int64)\n\nimported = tf.saved_model.load(filename) print(""RNG stream from loading point:"") print(""state:"", imported.state()) print(imported()) print(""state:"", imported.state()) print(imported()) print(""state:"", imported.state())\n\nRNG stream from loading point: state: tf.Tensor([256 0 0], shape=(3,), dtype=int64) tf.Tensor(-1.0359411, shape=(), dtype=float32) state: tf.Tensor([512 0 0], shape=(3,), dtype=int64) tf.Tensor(-0.06425078, shape=(), dtype=float32) state: tf.Tensor([768 0 0], shape=(3,), dtype=int64)\n\nLoading a SavedModel containing tf.random.Generator into a distribution strategy is not recommended because the replicas will all generate the same random-number stream (which is because replica ID is frozen in SavedModel\'s graph).\n\nLoading a distributed tf.random.Generator (a generator created within a distribution strategy) into a non-strategy environment, like the above example, also has a caveat. The RNG state will be properly restored, but the random numbers generated will be different from the original generator in its strategy (again because a device outside strategies is treated as different from any replica in a strategy).\n\nUsage of stateless RNGs is simple. Since they are just pure functions, there is no state or side effect involved.\n\nprint(tf.random.stateless_normal(shape=[2, 3], seed=[1, 2])) print(tf.random.stateless_normal(shape=[2, 3], seed=[1, 2]))\n\ntf.Tensor( [[ 0.5441101 0.20738031 0.07356433] [ 0.04643455 -1.30159 -0.95385665]], shape=(2, 3), dtype=float32) tf.Tensor( [[ 0.5441101 0.20738031 0.07356433] [ 0.04643455 -1.30159 -0.95385665]], shape=(2, 3), dtype=float32)\n\nEvery stateless RNG requires a seed argument, which needs to be an integer Tensor of shape [2]. The results of the op are fully determined by this seed.\n\nThe RNG algorithm used by stateless RNGs is device-dependent, meaning the same op running on a different device may produce different outputs.\n\nBoth the tf.random.Generator class and the stateless functions support the Philox algorithm (written as ""philox"" or tf.random.Algorithm.PHILOX) on all devices.\n\nDifferent devices will generate the same integer numbers, if using the same algorithm and starting from the same state. They will also generate ""almost the same"" float-point numbers, though there may be small numerical discrepancies caused by the different ways the devices carry out the float-point computation (e.g. reduction order).\n\nOn XLA-driven devices (such as TPU, and also CPU/GPU when XLA is enabled) the ThreeFry algorithm (written as ""threefry"" or tf.random.Algorithm.THREEFRY) is also supported. This algorithm is fast on TPU but slow on CPU/GPU compared to Philox.\n\nSee paper \'Parallel Random Numbers: As Easy as 1, 2, 3\' for more details about these algorithms.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-01-17 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-04-12T18:54:22', 'title': 'Random number generation | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/random_numbers'})]]??"
55560676,tf.while_loop,"{'https://www.coursera.org/learn/get-started-with-python', 'https://www.coursera.org/learn/linear-algebra-machine-learning'}","{'https://www.youtube.com/watch?v=GSZhXVjuLgA', 'https://www.youtube.com/watch?v=_hKneRZRKrI', 'https://www.youtube.com/watch?v=IobOuSyfExc', 'https://www.youtube.com/watch?v=EY-oLHxpKrI', 'https://www.youtube.com/watch?v=QemYvk0lT7s', 'https://www.youtube.com/watch?v=t6NqtwCWK0E'}","{'https://stackoverflow.com/questions/64298298/type-hinting-callable-with-no-parameters', 'https://stackoverflow.com/questions/4534438/typeerror-module-object-is-not-callable', 'https://stackoverflow.com/questions/9768865/python-nonetype-object-is-not-callable-beginner', 'https://stackoverflow.com/questions/111234/what-is-a-callable', 'https://stackoverflow.com/questions/70967266/what-exactly-is-python-typing-callable'}","??[[Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nWhat is a ""callable""?\n\nAsked 15 years, 9 months ago\n\nModified 1 year, 7 months ago\n\nNow that it\'s clear what a metaclass is, there is an associated concept that I use all the time without knowing what it really means.\n\nI suppose everybody made once a mistake with parenthesis, resulting in an ""object is not callable"" exception. What\'s more, using __init__ and __new__ lead to wonder what this bloody __call__ can be used for.\n\nCould you give me some explanations, including examples with the magic method ?\n\nedited Nov 26, 2022 at 22:34\n\n1.1m313313 gold badges4.2k4.2k silver badges3.4k3.4k bronze badges\n\nasked Sep 21, 2008 at 15:34\n\n590k116116 gold badges308308 silver badges334334 bronze badges 1\n\nrelated: Python internals: how callables work\n\n– jfs Commented Mar 23, 2012 at 14:46\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nA callable is anything that can be called.\n\nThe built-in callable (PyCallable_Check in objects.c) checks if the argument is either:\n\nan instance of a class with a __call__ method or\n\nis of a type that has a non null tp_call (c struct) member which indicates callability otherwise (such as in functions, methods etc.)\n\nThe method named __call__ is (according to the documentation)\n\nCalled when the instance is \'\'called\'\' as a function\n\nclass Foo: def __call__(self): print \'called\' foo_instance = Foo() foo_instance() #this is calling the __call__ method\n\nedited Aug 12, 2019 at 13:57\n\n11122 silver badges1717 bronze badges\n\nanswered Sep 21, 2008 at 15:44\n\nFlorian BöschFlorian Bösch\n\n27.7k1212 gold badges4949 silver badges5353 bronze badges 8\n\nNote that the builtin callable is being removed in Python 3.0 in favor of checking for call\n\n– Eli Courtwright Commented Sep 22, 2008 at 0:31\n\n@Eli: Hmm that sounds like a very bad move. callable actually tells you if something is callable or not, while checking for __call__ tells you nothing; If an object o provides __getattribute__ or __getattr__, hasattr(o, \'__call__\') may return True, yet o will still not be callable because Python skips __getattribute__ and __getattr__ for calls. The only real way left to check if something is callable is thus EAFP.\n\n– L̲̳o̲̳̳n̲̳̳g̲̳̳p̲̳o̲̳̳k̲̳̳e̲̳̳ Commented Jul 1, 2010 at 23:03\n\n@Longpoke: Just for the record, please see the documentation for callable() in Python 3.x: ""This function was first removed in Python 3.0 and then brought back in Python 3.2."".\n\n– Tadeck Commented May 8, 2013 at 4:41\n\nIt seems in python 3.8 only the presence of tp_call is checked. See implementation of PyCallable_Check, it\'s 3 lines.\n\n– Michele Piccolini Commented May 28, 2020 at 13:38\n\n@MichelePiccolini It\'s been that way for over a decade actually, but it still works to do what it\'s documented to do, which is to check if something is callable or not. When Python 3 was still young they changed the implementation, and now objects with a __call__ method just always have tp_call set as well. I\'m not sure when ""all callables have tp_call"" was implemented, but the PyCallable_Check change happened in back in August 2006: github.com/python/cpython/commit/…\n\n– mtraceur Commented Jan 29, 2021 at 19:11\n\n | Show 3 more comments\n\nFrom Python\'s sources object.c:\n\n/* Test whether an object can be called */ int PyCallable_Check(PyObject *x) { if (x == NULL) return 0; if (PyInstance_Check(x)) { PyObject *call = PyObject_GetAttrString(x, ""__call__""); if (call == NULL) { PyErr_Clear(); return 0; } /* Could test recursively but don\'t, for fear of endless recursion if some joker sets self.__call__ = self */ Py_DECREF(call); return 1; } else { return x->ob_type->tp_call != NULL; } }\n\nIf an object is an instance of some class then it is callable iff it has __call__ attribute.\n\nElse the object x is callable iff x->ob_type->tp_call != NULL\n\nDesciption of tp_call field:\n\nternaryfunc tp_call An optional pointer to a function that implements calling the object. This should be NULL if the object is not callable. The signature is the same as for PyObject_Call(). This field is inherited by subtypes.\n\nYou can always use built-in callable function to determine whether given object is callable or not; or better yet just call it and catch TypeError later. callable is removed in Python 3.0 and 3.1, use callable = lambda o: hasattr(o, \'__call__\') or isinstance(o, collections.Callable).\n\nExample, a simplistic cache implementation:\n\nclass Cached: def __init__(self, function): self.function = function self.cache = {} def __call__(self, *args): try: return self.cache[args] except KeyError: ret = self.cache[args] = self.function(*args) return ret\n\n@Cached def ack(x, y): return ack(x-1, ack(x, y-1)) if x*y else (x + y + 1)\n\nExample from standard library, file site.py, definition of built-in exit() and quit() functions:\n\nclass Quitter(object): def __init__(self, name): self.name = name def __repr__(self): return \'Use %s() or %s to exit\' % (self.name, eof) def __call__(self, code=None): # Shells like IDLE catch the SystemExit, but listen when their # stdin wrapper is closed. try: sys.stdin.close() except: pass raise SystemExit(code) __builtin__.quit = Quitter(\'quit\') __builtin__.exit = Quitter(\'exit\')\n\nedited Jan 2, 2014 at 3:22\n\n1,50833 gold badges1111 silver badges2828 bronze badges\n\nanswered Sep 22, 2008 at 15:04\n\n410k200200 gold badges1k1k silver badges1.7k1.7k bronze badges 13\n\nI find the example for the call method highly missleading because it mixes it with a recipe for caching and decorators, which add nothing to the understanding of call\n\n– Florian Bösch Commented Sep 26, 2008 at 16:13\n\nJ.F. Sebastian, also piling more examples you copy&pasted from somewhere else that are not minimal doesn\'t help.\n\n– Florian Bösch Commented Sep 27, 2008 at 13:10\n\n@J.F. Sebastian: It\'s BS that more life-like examples are better. I could show you life-like code that would make you weep as an example. Simple examples work too, and they work better to illustrate something because they don\'t distract.\n\n– Florian Bösch Commented Sep 28, 2008 at 22:32\n\nYou are explaining what\'s a callable, but you gave an example how to use callable objects to define a decorator. I know it\'s a typical usage of callable but this can confuse readers who just want to know what is callable and how to use callable. I\'d prefer @Florian Bösch\'s answer.\n\n– KFL Commented Mar 6, 2012 at 19:01\n\n@Kay: I also like the @Florian Bösch\'s answer (in its current form). btw, a decorator is not a typical usage of a ""callable"". The most typical ""callables"" are functions/methods such as def f(): ..., and class objects such as class C: ... i.e., f, \'\'.strip, len, and C all are callable. Instances that have a __call__() method in their class are relatively rare.\n\n– jfs Commented Mar 6, 2012 at 20:30\n\n | Show 8 more comments\n\nA callable is an object allows you to use round parenthesis ( ) and eventually pass some parameters, just like functions.\n\nEvery time you define a function python creates a callable object. In example, you could define the function func in these ways (it\'s the same):\n\nclass a(object): def __call__(self, *args): print \'Hello\' func = a() # or ... def func(*args): print \'Hello\'\n\nYou could use this method instead of methods like doit or run, I think it\'s just more clear to see obj() than obj.doit()\n\nedited Sep 26, 2008 at 13:31\n\nanswered Sep 26, 2008 at 13:22\n\nAndrea AmbuAndrea Ambu\n\n39k1414 gold badges5555 silver badges7777 bronze badges 0\n\nLet me explain backwards:\n\n... as syntactic sugar for:\n\nWhere foo can be any object that responds to __call__. When I say any object, I mean it: built-in types, your own classes and their instances.\n\nIn the case of built-in types, when you write:\n\nint(\'10\') unicode(10)\n\nYou\'re essentially doing:\n\nint.__call__(\'10\') unicode.__call__(10)\n\nThat\'s also why you don\'t have foo = new int in Python: you just make the class object return an instance of it on __call__. The way Python solves this is very elegant in my opinion.\n\nanswered Mar 22, 2013 at 23:38\n\n2,28411 gold badge2222 silver badges1717 bronze badges 2\n\nYou\'re essentially doing type(int).__call__(int, \'10\') and type(unicode).__call__(unicode, \'10\'). Dunders are always called on their class, not through the instance. And they never go through the metaclass either. For most cases that\'s just a nitpick, but it matters sometimes.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:24\n\nBuilt-in types work in special ways in the reference C implementation, although what happens is essentially equivalent to this, yes. For user-defined types, writing MyClass() attempts to call the class, by looking for a __call__ method on MyClass \'s class, i.e. it skips doing attribute lookup within MyClass (otherwise it could find a user-defined __call__ intended for the instances!) and finds type.__call__ - which then evaluates to a bound method on MyClass via the usual mechanisms, which is then called.\n\n– Karl Knechtel Commented Jul 5, 2022 at 5:35\n\n__call__ makes any object be callable as a function.\n\nThis example will output 8:\n\nclass Adder(object): def __init__(self, val): self.val = val def __call__(self, val): return self.val + val func = Adder(5) print func(3)\n\nanswered Sep 21, 2008 at 15:49\n\n23.1k99 gold badges6969 silver badges9696 bronze badges\n\nA Callable is an object that has the __call__ method. This means you can fake callable functions or do neat things like Partial Function Application where you take a function and add something that enhances it or fills in some of the parameters, returning something that can be called in turn (known as Currying in functional programming circles).\n\nCertain typographic errors will have the interpreter attempting to call something you did not intend, such as (for example) a string. This can produce errors where the interpreter attempts to execute a non-callable application. You can see this happening in a python interpreter by doing something like the transcript below.\n\n[nigel@k9 ~]$ python Python 2.5 (r25:51908, Nov 6 2007, 15:55:44) [GCC 4.1.2 20070925 (Red Hat 4.1.2-27)] on linux2 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> \'aaa\'() # <== Here we attempt to call a string. Traceback (most recent call last): File ""<stdin>"", line 1, in <module> TypeError: \'str\' object is not callable >>>\n\nedited Sep 21, 2008 at 15:51\n\nanswered Sep 21, 2008 at 15:43\n\nConcernedOfTunbridgeWellsConcernedOfTunbridgeWells\n\n65.8k1515 gold badges145145 silver badges198198 bronze badges\n\nQuite simply, a ""callable"" is something that can be called like a method. The built in function ""callable()"" will tell you whether something appears to be callable, as will checking for a call property. Functions are callable as are classes, class instances can be callable. See more about this here and here.\n\nanswered Sep 21, 2008 at 15:43\n\n14.9k55 gold badges3737 silver badges3939 bronze badges 1\n\nTo warp the readers\' mind a bit: classes are callable because they are instances of a class that has callable instances. You may have heard of it: it\'s called type. This object, uniquely, is its own type, and an instance of itself. It defines a __call__ method so that classes can be callable; that\'s where the implementation-specific magic is invoked to actually allocate memory, as well as potentially using the class\' __new__ hook and then calling __init__ on the resultant object.\n\n– Karl Knechtel Commented Jul 5, 2022 at 5:39\n\nIn Python a callable is an object which type has a __call__ method:\n\n>>> class Foo: ... pass ... >>> class Bar(object): ... pass ... >>> type(Foo).__call__(Foo) <__main__.Foo instance at 0x711440> >>> type(Bar).__call__(Bar) <__main__.Bar object at 0x712110> >>> def foo(bar): ... return bar ... >>> type(foo).__call__(foo, 42) 42\n\nAs simple as that :)\n\nThis of course can be overloaded:\n\n>>> class Foo(object): ... def __call__(self): ... return 42 ... >>> f = Foo() >>> f() 42\n\nanswered Sep 21, 2008 at 16:37\n\nArmin RonacherArmin Ronacher\n\n32.3k1414 gold badges6666 silver badges6969 bronze badges\n\nIt\'s something you can put ""(args)"" after and expect it to work. A callable is usually a method or a class. Methods get called, classes get instantiated.\n\nanswered Sep 21, 2008 at 15:43\n\n8,70655 gold badges4444 silver badges5151 bronze badges\n\nTo check function or method of class is callable or not that means we can call that function.\n\nClass A: def __init__(self,val): self.val = val def bar(self): print ""bar"" obj = A() callable(obj.bar) True callable(obj.__init___) False def foo(): return ""s"" callable(foo) True callable(foo()) False\n\nedited Sep 20, 2016 at 11:12\n\n2,10144 gold badges1919 silver badges2626 bronze badges\n\nanswered Sep 20, 2016 at 10:03\n\nRavi SinghRavi Singh\n\nAre you sure callable(obj.__init___) doesn\'t have an extra underscore (as in AttributeError)? If it doesn\'t, are you sure the answer isn\'t True for that one?\n\n– Mad Physicist Commented Sep 19, 2018 at 3:34\n\ncallables implement the __call__ special method so any object with such a method is callable.\n\nanswered May 7, 2012 at 21:40\n\n7,2111212 gold badges4040 silver badges6262 bronze badges 1\n\nAn instance on which you define __call__ won\'t be callable if the class doesn\'t define such a method.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:35\n\nCallable is a type or class of ""Build-in function or Method"" with a method call\n\n>>> type(callable) <class \'builtin_function_or_method\'> >>>\n\nExample: print is a callable object. With a build-in function call When you invoke the print function, Python creates an object of type print and invokes its method call passing the parameters if any.\n\n>>> type(print) <class \'builtin_function_or_method\'> >>> print.__call__(10) 10 >>> print(10) 10 >>>\n\nedited Feb 23, 2022 at 7:20\n\n12.3k2020 gold badges5050 silver badges8484 bronze badges\n\nanswered Apr 7, 2017 at 7:10\n\n73877 silver badges88 bronze badges 1\n\nSome of the info here is straight up wrong. E.g. ""When you invoke the print function, Python creates an object of type print and invokes its method __call__"". Python does not create a print object. It just calls something equivalent to type(print).__call__(print, *args, **kwargs). And the first sentence doesn\'t make much sense. You appear to be confusing a callable object and ""callable"" the function.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:30\n\nA class, function, method and object which has __call__() are callable.\n\nYou can check if callable with callable() which returns True if callable and returns False if not callable as shown below:\n\nclass Class1: def __call__(self): print(""__call__"") class Class2: pass def func(): pass print(callable(Class1)) # Class1 print(callable(Class2)) # Class2 print(callable(Class1())) # Class1 object print(callable(Class2())) # Class2 object print(callable(func)) # func\n\nThen, only Class2 object which doesn\'t have __call__() is not callable returning False as shown below:\n\nTrue # Class1 True # Class2 True # Class1 object False # Class2 object True # func\n\nIn addition, all of them below are not callable returning False as shown below:\n\nprint(callable(""Hello"")) # ""str"" type print(callable(100)) # ""int"" type print(callable(100.23)) # ""float"" type print(callable(100 + 2j)) # ""complex"" type print(callable(True)) # ""bool"" type print(callable(None)) # ""NoneType"" print(callable([])) # ""list"" type print(callable(())) # ""tuple"" type print(callable({})) # ""dict"" type print(callable({""""})) # ""set"" type\n\nFalse # ""str"" type False # ""int"" type False # ""float"" type False # ""complex"" type False # ""bool"" type False # ""NoneType"" False # ""list"" type False # ""tuple"" type False # ""dict"" type False # ""set"" type\n\nedited Nov 24, 2022 at 3:03\n\nanswered Nov 23, 2022 at 15:26\n\nSuper Kai - Kazuya ItoSuper Kai - Kazuya Ito\n\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ncallable or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n4 self() as function within class, what does it do?\n\n6 What is self() in python?\n\n-2 What does s() mean?\n\n-2 Why isn\'t my program interpreting the user\'s input correctly?\n\n0 What are the different types of callables in Python?\n\n-2 What does it do when we call self() inside a class in Python?\n\n0 Why does pytorch treat network classes like functions?\n\n7401 What are metaclasses in Python?\n\n820 Collections.defaultdict difference with normal dict\n\n178 Python __call__ special method practical example\n\nSee more linked questions\n\n13 Why do we have callable objects in python?\n\n12 alternative to callable(),for use in Python 3\n\n12 Python functions and their __call__ attribute\n\n20 Do all callables have __name__?\n\n18 Functions, Callable Objects, and how both are created in Python\n\n1 statement ""foo is callable"" return false (with foo is a function)\n\n4 Function call and the __call__ attribute\n\n5 What is the definition of a callable type?\n\n5 What exactly is a caller in python?\n\n0 Reading class method definition - what is Callable?\n\nHot Network Questions\n\nHow to maintain dependencies shared among microservices?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nWhy danach instead of darüber?\n\nAre US enlisted personnel (as opposed to officers) required, or allowed, to disobey unlawful orders?\n\nType inference with type classes in Coq\n\nWhy does the voltage double at the end of line in a open transmission line (physical explanation)\n\nEvil God Challenge: What if an evil god is just trolling humanity and that explains why there\'s good in the world?\n\nDoes antenna arraying for deep space communications affect the CMB contribution?\n\nHow to read chainline specs for crankset and bottom bracket compatibility?\n\nWere there any stone vessels made in Paleolithic?\n\nCan the US president kill at will?\n\nRaid 0+1 Failure Cases Vs. Raid 1+0\n\nSwitch loop light convert to ceiling fan\n\nIs a desert planet with a small habitable area possible?\n\nWhat is the ""closest approximation"" to fields by an equational variety?\n\nSegments of a string, doubling in length\n\nDo thermodynamic cycles occur only in human-made machines?\n\nGuessing whether the revealed number is higher\n\nWhen do you know things are actually going poorly in graduate school?\n\nWhat type of interaction in a π-complex?\n\nSpace Invasion story. People get a blister, or pimple on their arm, treated, they are fine, untreated they die\n\nWhy didn\'t Smith give Atlas painkillers during the surgery?\n\n为什么字形不同的两个字「骨」的编码相同（从而是一个字）？ more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_2', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nWhat is a ""callable""?\n\nAsked 15 years, 9 months ago\n\nModified 1 year, 7 months ago\n\nNow that it\'s clear what a metaclass is, there is an associated concept that I use all the time without knowing what it really means.\n\nI suppose everybody made once a mistake with parenthesis, resulting in an ""object is not callable"" exception. What\'s more, using __init__ and __new__ lead to wonder what this bloody __call__ can be used for.\n\nCould you give me some explanations, including examples with the magic method ?\n\nedited Nov 26, 2022 at 22:34\n\n1.1m313313 gold badges4.2k4.2k silver badges3.4k3.4k bronze badges\n\nasked Sep 21, 2008 at 15:34\n\n590k116116 gold badges308308 silver badges334334 bronze badges 1\n\nrelated: Python internals: how callables work\n\n– jfs Commented Mar 23, 2012 at 14:46\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nA callable is anything that can be called.\n\nThe built-in callable (PyCallable_Check in objects.c) checks if the argument is either:\n\nan instance of a class with a __call__ method or\n\nis of a type that has a non null tp_call (c struct) member which indicates callability otherwise (such as in functions, methods etc.)\n\nThe method named __call__ is (according to the documentation)\n\nCalled when the instance is \'\'called\'\' as a function\n\nclass Foo: def __call__(self): print \'called\' foo_instance = Foo() foo_instance() #this is calling the __call__ method\n\nedited Aug 12, 2019 at 13:57\n\n11122 silver badges1717 bronze badges\n\nanswered Sep 21, 2008 at 15:44\n\nFlorian BöschFlorian Bösch\n\n27.7k1212 gold badges4949 silver badges5353 bronze badges 8\n\nNote that the builtin callable is being removed in Python 3.0 in favor of checking for call\n\n– Eli Courtwright Commented Sep 22, 2008 at 0:31\n\n@Eli: Hmm that sounds like a very bad move. callable actually tells you if something is callable or not, while checking for __call__ tells you nothing; If an object o provides __getattribute__ or __getattr__, hasattr(o, \'__call__\') may return True, yet o will still not be callable because Python skips __getattribute__ and __getattr__ for calls. The only real way left to check if something is callable is thus EAFP.\n\n– L̲̳o̲̳̳n̲̳̳g̲̳̳p̲̳o̲̳̳k̲̳̳e̲̳̳ Commented Jul 1, 2010 at 23:03\n\n@Longpoke: Just for the record, please see the documentation for callable() in Python 3.x: ""This function was first removed in Python 3.0 and then brought back in Python 3.2."".\n\n– Tadeck Commented May 8, 2013 at 4:41\n\nIt seems in python 3.8 only the presence of tp_call is checked. See implementation of PyCallable_Check, it\'s 3 lines.\n\n– Michele Piccolini Commented May 28, 2020 at 13:38\n\n@MichelePiccolini It\'s been that way for over a decade actually, but it still works to do what it\'s documented to do, which is to check if something is callable or not. When Python 3 was still young they changed the implementation, and now objects with a __call__ method just always have tp_call set as well. I\'m not sure when ""all callables have tp_call"" was implemented, but the PyCallable_Check change happened in back in August 2006: github.com/python/cpython/commit/…\n\n– mtraceur Commented Jan 29, 2021 at 19:11\n\n | Show 3 more comments\n\nFrom Python\'s sources object.c:\n\n/* Test whether an object can be called */ int PyCallable_Check(PyObject *x) { if (x == NULL) return 0; if (PyInstance_Check(x)) { PyObject *call = PyObject_GetAttrString(x, ""__call__""); if (call == NULL) { PyErr_Clear(); return 0; } /* Could test recursively but don\'t, for fear of endless recursion if some joker sets self.__call__ = self */ Py_DECREF(call); return 1; } else { return x->ob_type->tp_call != NULL; } }\n\nIf an object is an instance of some class then it is callable iff it has __call__ attribute.\n\nElse the object x is callable iff x->ob_type->tp_call != NULL\n\nDesciption of tp_call field:\n\nternaryfunc tp_call An optional pointer to a function that implements calling the object. This should be NULL if the object is not callable. The signature is the same as for PyObject_Call(). This field is inherited by subtypes.\n\nYou can always use built-in callable function to determine whether given object is callable or not; or better yet just call it and catch TypeError later. callable is removed in Python 3.0 and 3.1, use callable = lambda o: hasattr(o, \'__call__\') or isinstance(o, collections.Callable).\n\nExample, a simplistic cache implementation:\n\nclass Cached: def __init__(self, function): self.function = function self.cache = {} def __call__(self, *args): try: return self.cache[args] except KeyError: ret = self.cache[args] = self.function(*args) return ret\n\n@Cached def ack(x, y): return ack(x-1, ack(x, y-1)) if x*y else (x + y + 1)\n\nExample from standard library, file site.py, definition of built-in exit() and quit() functions:\n\nclass Quitter(object): def __init__(self, name): self.name = name def __repr__(self): return \'Use %s() or %s to exit\' % (self.name, eof) def __call__(self, code=None): # Shells like IDLE catch the SystemExit, but listen when their # stdin wrapper is closed. try: sys.stdin.close() except: pass raise SystemExit(code) __builtin__.quit = Quitter(\'quit\') __builtin__.exit = Quitter(\'exit\')\n\nedited Jan 2, 2014 at 3:22\n\n1,50833 gold badges1111 silver badges2828 bronze badges\n\nanswered Sep 22, 2008 at 15:04\n\n410k200200 gold badges1k1k silver badges1.7k1.7k bronze badges 13\n\nI find the example for the call method highly missleading because it mixes it with a recipe for caching and decorators, which add nothing to the understanding of call\n\n– Florian Bösch Commented Sep 26, 2008 at 16:13\n\nJ.F. Sebastian, also piling more examples you copy&pasted from somewhere else that are not minimal doesn\'t help.\n\n– Florian Bösch Commented Sep 27, 2008 at 13:10\n\n@J.F. Sebastian: It\'s BS that more life-like examples are better. I could show you life-like code that would make you weep as an example. Simple examples work too, and they work better to illustrate something because they don\'t distract.\n\n– Florian Bösch Commented Sep 28, 2008 at 22:32\n\nYou are explaining what\'s a callable, but you gave an example how to use callable objects to define a decorator. I know it\'s a typical usage of callable but this can confuse readers who just want to know what is callable and how to use callable. I\'d prefer @Florian Bösch\'s answer.\n\n– KFL Commented Mar 6, 2012 at 19:01\n\n@Kay: I also like the @Florian Bösch\'s answer (in its current form). btw, a decorator is not a typical usage of a ""callable"". The most typical ""callables"" are functions/methods such as def f(): ..., and class objects such as class C: ... i.e., f, \'\'.strip, len, and C all are callable. Instances that have a __call__() method in their class are relatively rare.\n\n– jfs Commented Mar 6, 2012 at 20:30\n\n | Show 8 more comments\n\nA callable is an object allows you to use round parenthesis ( ) and eventually pass some parameters, just like functions.\n\nEvery time you define a function python creates a callable object. In example, you could define the function func in these ways (it\'s the same):\n\nclass a(object): def __call__(self, *args): print \'Hello\' func = a() # or ... def func(*args): print \'Hello\'\n\nYou could use this method instead of methods like doit or run, I think it\'s just more clear to see obj() than obj.doit()\n\nedited Sep 26, 2008 at 13:31\n\nanswered Sep 26, 2008 at 13:22\n\nAndrea AmbuAndrea Ambu\n\n39k1414 gold badges5555 silver badges7777 bronze badges 0\n\nLet me explain backwards:\n\n... as syntactic sugar for:\n\nWhere foo can be any object that responds to __call__. When I say any object, I mean it: built-in types, your own classes and their instances.\n\nIn the case of built-in types, when you write:\n\nint(\'10\') unicode(10)\n\nYou\'re essentially doing:\n\nint.__call__(\'10\') unicode.__call__(10)\n\nThat\'s also why you don\'t have foo = new int in Python: you just make the class object return an instance of it on __call__. The way Python solves this is very elegant in my opinion.\n\nanswered Mar 22, 2013 at 23:38\n\n2,28411 gold badge2222 silver badges1717 bronze badges 2\n\nYou\'re essentially doing type(int).__call__(int, \'10\') and type(unicode).__call__(unicode, \'10\'). Dunders are always called on their class, not through the instance. And they never go through the metaclass either. For most cases that\'s just a nitpick, but it matters sometimes.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:24\n\nBuilt-in types work in special ways in the reference C implementation, although what happens is essentially equivalent to this, yes. For user-defined types, writing MyClass() attempts to call the class, by looking for a __call__ method on MyClass \'s class, i.e. it skips doing attribute lookup within MyClass (otherwise it could find a user-defined __call__ intended for the instances!) and finds type.__call__ - which then evaluates to a bound method on MyClass via the usual mechanisms, which is then called.\n\n– Karl Knechtel Commented Jul 5, 2022 at 5:35\n\n__call__ makes any object be callable as a function.\n\nThis example will output 8:\n\nclass Adder(object): def __init__(self, val): self.val = val def __call__(self, val): return self.val + val func = Adder(5) print func(3)\n\nanswered Sep 21, 2008 at 15:49\n\n23.1k99 gold badges6969 silver badges9696 bronze badges\n\nA Callable is an object that has the __call__ method. This means you can fake callable functions or do neat things like Partial Function Application where you take a function and add something that enhances it or fills in some of the parameters, returning something that can be called in turn (known as Currying in functional programming circles).\n\nCertain typographic errors will have the interpreter attempting to call something you did not intend, such as (for example) a string. This can produce errors where the interpreter attempts to execute a non-callable application. You can see this happening in a python interpreter by doing something like the transcript below.\n\n[nigel@k9 ~]$ python Python 2.5 (r25:51908, Nov 6 2007, 15:55:44) [GCC 4.1.2 20070925 (Red Hat 4.1.2-27)] on linux2 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> \'aaa\'() # <== Here we attempt to call a string. Traceback (most recent call last): File ""<stdin>"", line 1, in <module> TypeError: \'str\' object is not callable >>>\n\nedited Sep 21, 2008 at 15:51\n\nanswered Sep 21, 2008 at 15:43\n\nConcernedOfTunbridgeWellsConcernedOfTunbridgeWells\n\n65.8k1515 gold badges145145 silver badges198198 bronze badges\n\nQuite simply, a ""callable"" is something that can be called like a method. The built in function ""callable()"" will tell you whether something appears to be callable, as will checking for a call property. Functions are callable as are classes, class instances can be callable. See more about this here and here.\n\nanswered Sep 21, 2008 at 15:43\n\n14.9k55 gold badges3737 silver badges3939 bronze badges 1\n\nTo warp the readers\' mind a bit: classes are callable because they are instances of a class that has callable instances. You may have heard of it: it\'s called type. This object, uniquely, is its own type, and an instance of itself. It defines a __call__ method so that classes can be callable; that\'s where the implementation-specific magic is invoked to actually allocate memory, as well as potentially using the class\' __new__ hook and then calling __init__ on the resultant object.\n\n– Karl Knechtel Commented Jul 5, 2022 at 5:39\n\nIn Python a callable is an object which type has a __call__ method:\n\n>>> class Foo: ... pass ... >>> class Bar(object): ... pass ... >>> type(Foo).__call__(Foo) <__main__.Foo instance at 0x711440> >>> type(Bar).__call__(Bar) <__main__.Bar object at 0x712110> >>> def foo(bar): ... return bar ... >>> type(foo).__call__(foo, 42) 42\n\nAs simple as that :)\n\nThis of course can be overloaded:\n\n>>> class Foo(object): ... def __call__(self): ... return 42 ... >>> f = Foo() >>> f() 42\n\nanswered Sep 21, 2008 at 16:37\n\nArmin RonacherArmin Ronacher\n\n32.3k1414 gold badges6666 silver badges6969 bronze badges\n\nIt\'s something you can put ""(args)"" after and expect it to work. A callable is usually a method or a class. Methods get called, classes get instantiated.\n\nanswered Sep 21, 2008 at 15:43\n\n8,70655 gold badges4444 silver badges5151 bronze badges\n\nTo check function or method of class is callable or not that means we can call that function.\n\nClass A: def __init__(self,val): self.val = val def bar(self): print ""bar"" obj = A() callable(obj.bar) True callable(obj.__init___) False def foo(): return ""s"" callable(foo) True callable(foo()) False\n\nedited Sep 20, 2016 at 11:12\n\n2,10144 gold badges1919 silver badges2626 bronze badges\n\nanswered Sep 20, 2016 at 10:03\n\nRavi SinghRavi Singh\n\nAre you sure callable(obj.__init___) doesn\'t have an extra underscore (as in AttributeError)? If it doesn\'t, are you sure the answer isn\'t True for that one?\n\n– Mad Physicist Commented Sep 19, 2018 at 3:34\n\ncallables implement the __call__ special method so any object with such a method is callable.\n\nanswered May 7, 2012 at 21:40\n\n7,2111212 gold badges4040 silver badges6262 bronze badges 1\n\nAn instance on which you define __call__ won\'t be callable if the class doesn\'t define such a method.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:35\n\nCallable is a type or class of ""Build-in function or Method"" with a method call\n\n>>> type(callable) <class \'builtin_function_or_method\'> >>>\n\nExample: print is a callable object. With a build-in function call When you invoke the print function, Python creates an object of type print and invokes its method call passing the parameters if any.\n\n>>> type(print) <class \'builtin_function_or_method\'> >>> print.__call__(10) 10 >>> print(10) 10 >>>\n\nedited Feb 23, 2022 at 7:20\n\n12.3k2020 gold badges5050 silver badges8484 bronze badges\n\nanswered Apr 7, 2017 at 7:10\n\n73877 silver badges88 bronze badges 1\n\nSome of the info here is straight up wrong. E.g. ""When you invoke the print function, Python creates an object of type print and invokes its method __call__"". Python does not create a print object. It just calls something equivalent to type(print).__call__(print, *args, **kwargs). And the first sentence doesn\'t make much sense. You appear to be confusing a callable object and ""callable"" the function.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:30\n\nA class, function, method and object which has __call__() are callable.\n\nYou can check if callable with callable() which returns True if callable and returns False if not callable as shown below:\n\nclass Class1: def __call__(self): print(""__call__"") class Class2: pass def func(): pass print(callable(Class1)) # Class1 print(callable(Class2)) # Class2 print(callable(Class1())) # Class1 object print(callable(Class2())) # Class2 object print(callable(func)) # func\n\nThen, only Class2 object which doesn\'t have __call__() is not callable returning False as shown below:\n\nTrue # Class1 True # Class2 True # Class1 object False # Class2 object True # func\n\nIn addition, all of them below are not callable returning False as shown below:\n\nprint(callable(""Hello"")) # ""str"" type print(callable(100)) # ""int"" type print(callable(100.23)) # ""float"" type print(callable(100 + 2j)) # ""complex"" type print(callable(True)) # ""bool"" type print(callable(None)) # ""NoneType"" print(callable([])) # ""list"" type print(callable(())) # ""tuple"" type print(callable({})) # ""dict"" type print(callable({""""})) # ""set"" type\n\nFalse # ""str"" type False # ""int"" type False # ""float"" type False # ""complex"" type False # ""bool"" type False # ""NoneType"" False # ""list"" type False # ""tuple"" type False # ""dict"" type False # ""set"" type\n\nedited Nov 24, 2022 at 3:03\n\nanswered Nov 23, 2022 at 15:26\n\nSuper Kai - Kazuya ItoSuper Kai - Kazuya Ito\n\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ncallable or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n4 self() as function within class, what does it do?\n\n6 What is self() in python?\n\n-2 What does s() mean?\n\n-2 Why isn\'t my program interpreting the user\'s input correctly?\n\n0 What are the different types of callables in Python?\n\n-2 What does it do when we call self() inside a class in Python?\n\n0 Why does pytorch treat network classes like functions?\n\n7401 What are metaclasses in Python?\n\n820 Collections.defaultdict difference with normal dict\n\n178 Python __call__ special method practical example\n\nSee more linked questions\n\n13 Why do we have callable objects in python?\n\n12 alternative to callable(),for use in Python 3\n\n12 Python functions and their __call__ attribute\n\n20 Do all callables have __name__?\n\n18 Functions, Callable Objects, and how both are created in Python\n\n1 statement ""foo is callable"" return false (with foo is a function)\n\n4 Function call and the __call__ attribute\n\n5 What is the definition of a callable type?\n\n5 What exactly is a caller in python?\n\n0 Reading class method definition - what is Callable?\n\nHot Network Questions\n\nHow to maintain dependencies shared among microservices?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nWhy danach instead of darüber?\n\nAre US enlisted personnel (as opposed to officers) required, or allowed, to disobey unlawful orders?\n\nType inference with type classes in Coq\n\nWhy does the voltage double at the end of line in a open transmission line (physical explanation)\n\nEvil God Challenge: What if an evil god is just trolling humanity and that explains why there\'s good in the world?\n\nDoes antenna arraying for deep space communications affect the CMB contribution?\n\nHow to read chainline specs for crankset and bottom bracket compatibility?\n\nWere there any stone vessels made in Paleolithic?\n\nCan the US president kill at will?\n\nRaid 0+1 Failure Cases Vs. Raid 1+0\n\nSwitch loop light convert to ceiling fan\n\nIs a desert planet with a small habitable area possible?\n\nWhat is the ""closest approximation"" to fields by an equational variety?\n\nSegments of a string, doubling in length\n\nDo thermodynamic cycles occur only in human-made machines?\n\nGuessing whether the revealed number is higher\n\nWhen do you know things are actually going poorly in graduate school?\n\nWhat type of interaction in a π-complex?\n\nSpace Invasion story. People get a blister, or pimple on their arm, treated, they are fine, untreated they die\n\nWhy didn\'t Smith give Atlas painkillers during the surgery?\n\n为什么字形不同的两个字「骨」的编码相同（从而是一个字）？ more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-04T15:07:45', 'title': 'python - What is a ""callable""? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/111234/what-is-a-callable'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nWhat is a ""callable""?\n\nAsked 15 years, 9 months ago\n\nModified 1 year, 7 months ago\n\nNow that it\'s clear what a metaclass is, there is an associated concept that I use all the time without knowing what it really means.\n\nI suppose everybody made once a mistake with parenthesis, resulting in an ""object is not callable"" exception. What\'s more, using __init__ and __new__ lead to wonder what this bloody __call__ can be used for.\n\nCould you give me some explanations, including examples with the magic method ?\n\nedited Nov 26, 2022 at 22:34\n\n1.1m313313 gold badges4.2k4.2k silver badges3.4k3.4k bronze badges\n\nasked Sep 21, 2008 at 15:34\n\n590k116116 gold badges308308 silver badges334334 bronze badges 1\n\nrelated: Python internals: how callables work\n\n– jfs Commented Mar 23, 2012 at 14:46\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nA callable is anything that can be called.\n\nThe built-in callable (PyCallable_Check in objects.c) checks if the argument is either:\n\nan instance of a class with a __call__ method or\n\nis of a type that has a non null tp_call (c struct) member which indicates callability otherwise (such as in functions, methods etc.)\n\nThe method named __call__ is (according to the documentation)\n\nCalled when the instance is \'\'called\'\' as a function\n\nclass Foo: def __call__(self): print \'called\' foo_instance = Foo() foo_instance() #this is calling the __call__ method\n\nedited Aug 12, 2019 at 13:57\n\n11122 silver badges1717 bronze badges\n\nanswered Sep 21, 2008 at 15:44\n\nFlorian BöschFlorian Bösch\n\n27.7k1212 gold badges4949 silver badges5353 bronze badges 8\n\nNote that the builtin callable is being removed in Python 3.0 in favor of checking for call\n\n– Eli Courtwright Commented Sep 22, 2008 at 0:31\n\n@Eli: Hmm that sounds like a very bad move. callable actually tells you if something is callable or not, while checking for __call__ tells you nothing; If an object o provides __getattribute__ or __getattr__, hasattr(o, \'__call__\') may return True, yet o will still not be callable because Python skips __getattribute__ and __getattr__ for calls. The only real way left to check if something is callable is thus EAFP.\n\n– L̲̳o̲̳̳n̲̳̳g̲̳̳p̲̳o̲̳̳k̲̳̳e̲̳̳ Commented Jul 1, 2010 at 23:03\n\n@Longpoke: Just for the record, please see the documentation for callable() in Python 3.x: ""This function was first removed in Python 3.0 and then brought back in Python 3.2."".\n\n– Tadeck Commented May 8, 2013 at 4:41\n\nIt seems in python 3.8 only the presence of tp_call is checked. See implementation of PyCallable_Check, it\'s 3 lines.\n\n– Michele Piccolini Commented May 28, 2020 at 13:38\n\n@MichelePiccolini It\'s been that way for over a decade actually, but it still works to do what it\'s documented to do, which is to check if something is callable or not. When Python 3 was still young they changed the implementation, and now objects with a __call__ method just always have tp_call set as well. I\'m not sure when ""all callables have tp_call"" was implemented, but the PyCallable_Check change happened in back in August 2006: github.com/python/cpython/commit/…\n\n– mtraceur Commented Jan 29, 2021 at 19:11\n\n | Show 3 more comments\n\nFrom Python\'s sources object.c:\n\n/* Test whether an object can be called */ int PyCallable_Check(PyObject *x) { if (x == NULL) return 0; if (PyInstance_Check(x)) { PyObject *call = PyObject_GetAttrString(x, ""__call__""); if (call == NULL) { PyErr_Clear(); return 0; } /* Could test recursively but don\'t, for fear of endless recursion if some joker sets self.__call__ = self */ Py_DECREF(call); return 1; } else { return x->ob_type->tp_call != NULL; } }\n\nIf an object is an instance of some class then it is callable iff it has __call__ attribute.\n\nElse the object x is callable iff x->ob_type->tp_call != NULL\n\nDesciption of tp_call field:\n\nternaryfunc tp_call An optional pointer to a function that implements calling the object. This should be NULL if the object is not callable. The signature is the same as for PyObject_Call(). This field is inherited by subtypes.\n\nYou can always use built-in callable function to determine whether given object is callable or not; or better yet just call it and catch TypeError later. callable is removed in Python 3.0 and 3.1, use callable = lambda o: hasattr(o, \'__call__\') or isinstance(o, collections.Callable).\n\nExample, a simplistic cache implementation:\n\nclass Cached: def __init__(self, function): self.function = function self.cache = {} def __call__(self, *args): try: return self.cache[args] except KeyError: ret = self.cache[args] = self.function(*args) return ret\n\n@Cached def ack(x, y): return ack(x-1, ack(x, y-1)) if x*y else (x + y + 1)\n\nExample from standard library, file site.py, definition of built-in exit() and quit() functions:\n\nclass Quitter(object): def __init__(self, name): self.name = name def __repr__(self): return \'Use %s() or %s to exit\' % (self.name, eof) def __call__(self, code=None): # Shells like IDLE catch the SystemExit, but listen when their # stdin wrapper is closed. try: sys.stdin.close() except: pass raise SystemExit(code) __builtin__.quit = Quitter(\'quit\') __builtin__.exit = Quitter(\'exit\')\n\nedited Jan 2, 2014 at 3:22\n\n1,50833 gold badges1111 silver badges2828 bronze badges\n\nanswered Sep 22, 2008 at 15:04\n\n410k200200 gold badges1k1k silver badges1.7k1.7k bronze badges 13\n\nI find the example for the call method highly missleading because it mixes it with a recipe for caching and decorators, which add nothing to the understanding of call\n\n– Florian Bösch Commented Sep 26, 2008 at 16:13\n\nJ.F. Sebastian, also piling more examples you copy&pasted from somewhere else that are not minimal doesn\'t help.\n\n– Florian Bösch Commented Sep 27, 2008 at 13:10\n\n@J.F. Sebastian: It\'s BS that more life-like examples are better. I could show you life-like code that would make you weep as an example. Simple examples work too, and they work better to illustrate something because they don\'t distract.\n\n– Florian Bösch Commented Sep 28, 2008 at 22:32\n\nYou are explaining what\'s a callable, but you gave an example how to use callable objects to define a decorator. I know it\'s a typical usage of callable but this can confuse readers who just want to know what is callable and how to use callable. I\'d prefer @Florian Bösch\'s answer.\n\n– KFL Commented Mar 6, 2012 at 19:01\n\n@Kay: I also like the @Florian Bösch\'s answer (in its current form). btw, a decorator is not a typical usage of a ""callable"". The most typical ""callables"" are functions/methods such as def f(): ..., and class objects such as class C: ... i.e., f, \'\'.strip, len, and C all are callable. Instances that have a __call__() method in their class are relatively rare.\n\n– jfs Commented Mar 6, 2012 at 20:30\n\n | Show 8 more comments\n\nA callable is an object allows you to use round parenthesis ( ) and eventually pass some parameters, just like functions.\n\nEvery time you define a function python creates a callable object. In example, you could define the function func in these ways (it\'s the same):\n\nclass a(object): def __call__(self, *args): print \'Hello\' func = a() # or ... def func(*args): print \'Hello\'\n\nYou could use this method instead of methods like doit or run, I think it\'s just more clear to see obj() than obj.doit()\n\nedited Sep 26, 2008 at 13:31\n\nanswered Sep 26, 2008 at 13:22\n\nAndrea AmbuAndrea Ambu\n\n39k1414 gold badges5555 silver badges7777 bronze badges 0\n\nLet me explain backwards:\n\n... as syntactic sugar for:\n\nWhere foo can be any object that responds to __call__. When I say any object, I mean it: built-in types, your own classes and their instances.\n\nIn the case of built-in types, when you write:\n\nint(\'10\') unicode(10)\n\nYou\'re essentially doing:\n\nint.__call__(\'10\') unicode.__call__(10)\n\nThat\'s also why you don\'t have foo = new int in Python: you just make the class object return an instance of it on __call__. The way Python solves this is very elegant in my opinion.\n\nanswered Mar 22, 2013 at 23:38\n\n2,28411 gold badge2222 silver badges1717 bronze badges 2\n\nYou\'re essentially doing type(int).__call__(int, \'10\') and type(unicode).__call__(unicode, \'10\'). Dunders are always called on their class, not through the instance. And they never go through the metaclass either. For most cases that\'s just a nitpick, but it matters sometimes.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:24\n\nBuilt-in types work in special ways in the reference C implementation, although what happens is essentially equivalent to this, yes. For user-defined types, writing MyClass() attempts to call the class, by looking for a __call__ method on MyClass \'s class, i.e. it skips doing attribute lookup within MyClass (otherwise it could find a user-defined __call__ intended for the instances!) and finds type.__call__ - which then evaluates to a bound method on MyClass via the usual mechanisms, which is then called.\n\n– Karl Knechtel Commented Jul 5, 2022 at 5:35\n\n__call__ makes any object be callable as a function.\n\nThis example will output 8:\n\nclass Adder(object): def __init__(self, val): self.val = val def __call__(self, val): return self.val + val func = Adder(5) print func(3)\n\nanswered Sep 21, 2008 at 15:49\n\n23.1k99 gold badges6969 silver badges9696 bronze badges\n\nA Callable is an object that has the __call__ method. This means you can fake callable functions or do neat things like Partial Function Application where you take a function and add something that enhances it or fills in some of the parameters, returning something that can be called in turn (known as Currying in functional programming circles).\n\nCertain typographic errors will have the interpreter attempting to call something you did not intend, such as (for example) a string. This can produce errors where the interpreter attempts to execute a non-callable application. You can see this happening in a python interpreter by doing something like the transcript below.\n\n[nigel@k9 ~]$ python Python 2.5 (r25:51908, Nov 6 2007, 15:55:44) [GCC 4.1.2 20070925 (Red Hat 4.1.2-27)] on linux2 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> \'aaa\'() # <== Here we attempt to call a string. Traceback (most recent call last): File ""<stdin>"", line 1, in <module> TypeError: \'str\' object is not callable >>>\n\nedited Sep 21, 2008 at 15:51\n\nanswered Sep 21, 2008 at 15:43\n\nConcernedOfTunbridgeWellsConcernedOfTunbridgeWells\n\n65.8k1515 gold badges145145 silver badges198198 bronze badges\n\nQuite simply, a ""callable"" is something that can be called like a method. The built in function ""callable()"" will tell you whether something appears to be callable, as will checking for a call property. Functions are callable as are classes, class instances can be callable. See more about this here and here.\n\nanswered Sep 21, 2008 at 15:43\n\n14.9k55 gold badges3737 silver badges3939 bronze badges 1\n\nTo warp the readers\' mind a bit: classes are callable because they are instances of a class that has callable instances. You may have heard of it: it\'s called type. This object, uniquely, is its own type, and an instance of itself. It defines a __call__ method so that classes can be callable; that\'s where the implementation-specific magic is invoked to actually allocate memory, as well as potentially using the class\' __new__ hook and then calling __init__ on the resultant object.\n\n– Karl Knechtel Commented Jul 5, 2022 at 5:39\n\nIn Python a callable is an object which type has a __call__ method:\n\n>>> class Foo: ... pass ... >>> class Bar(object): ... pass ... >>> type(Foo).__call__(Foo) <__main__.Foo instance at 0x711440> >>> type(Bar).__call__(Bar) <__main__.Bar object at 0x712110> >>> def foo(bar): ... return bar ... >>> type(foo).__call__(foo, 42) 42\n\nAs simple as that :)\n\nThis of course can be overloaded:\n\n>>> class Foo(object): ... def __call__(self): ... return 42 ... >>> f = Foo() >>> f() 42\n\nanswered Sep 21, 2008 at 16:37\n\nArmin RonacherArmin Ronacher\n\n32.3k1414 gold badges6666 silver badges6969 bronze badges\n\nIt\'s something you can put ""(args)"" after and expect it to work. A callable is usually a method or a class. Methods get called, classes get instantiated.\n\nanswered Sep 21, 2008 at 15:43\n\n8,70655 gold badges4444 silver badges5151 bronze badges\n\nTo check function or method of class is callable or not that means we can call that function.\n\nClass A: def __init__(self,val): self.val = val def bar(self): print ""bar"" obj = A() callable(obj.bar) True callable(obj.__init___) False def foo(): return ""s"" callable(foo) True callable(foo()) False\n\nedited Sep 20, 2016 at 11:12\n\n2,10144 gold badges1919 silver badges2626 bronze badges\n\nanswered Sep 20, 2016 at 10:03\n\nRavi SinghRavi Singh\n\nAre you sure callable(obj.__init___) doesn\'t have an extra underscore (as in AttributeError)? If it doesn\'t, are you sure the answer isn\'t True for that one?\n\n– Mad Physicist Commented Sep 19, 2018 at 3:34\n\ncallables implement the __call__ special method so any object with such a method is callable.\n\nanswered May 7, 2012 at 21:40\n\n7,2111212 gold badges4040 silver badges6262 bronze badges 1\n\nAn instance on which you define __call__ won\'t be callable if the class doesn\'t define such a method.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:35\n\nCallable is a type or class of ""Build-in function or Method"" with a method call\n\n>>> type(callable) <class \'builtin_function_or_method\'> >>>\n\nExample: print is a callable object. With a build-in function call When you invoke the print function, Python creates an object of type print and invokes its method call passing the parameters if any.\n\n>>> type(print) <class \'builtin_function_or_method\'> >>> print.__call__(10) 10 >>> print(10) 10 >>>\n\nedited Feb 23, 2022 at 7:20\n\n12.3k2020 gold badges5050 silver badges8484 bronze badges\n\nanswered Apr 7, 2017 at 7:10\n\n73877 silver badges88 bronze badges 1\n\nSome of the info here is straight up wrong. E.g. ""When you invoke the print function, Python creates an object of type print and invokes its method __call__"". Python does not create a print object. It just calls something equivalent to type(print).__call__(print, *args, **kwargs). And the first sentence doesn\'t make much sense. You appear to be confusing a callable object and ""callable"" the function.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:30\n\nA class, function, method and object which has __call__() are callable.\n\nYou can check if callable with callable() which returns True if callable and returns False if not callable as shown below:\n\nclass Class1: def __call__(self): print(""__call__"") class Class2: pass def func(): pass print(callable(Class1)) # Class1 print(callable(Class2)) # Class2 print(callable(Class1())) # Class1 object print(callable(Class2())) # Class2 object print(callable(func)) # func\n\nThen, only Class2 object which doesn\'t have __call__() is not callable returning False as shown below:\n\nTrue # Class1 True # Class2 True # Class1 object False # Class2 object True # func\n\nIn addition, all of them below are not callable returning False as shown below:\n\nprint(callable(""Hello"")) # ""str"" type print(callable(100)) # ""int"" type print(callable(100.23)) # ""float"" type print(callable(100 + 2j)) # ""complex"" type print(callable(True)) # ""bool"" type print(callable(None)) # ""NoneType"" print(callable([])) # ""list"" type print(callable(())) # ""tuple"" type print(callable({})) # ""dict"" type print(callable({""""})) # ""set"" type\n\nFalse # ""str"" type False # ""int"" type False # ""float"" type False # ""complex"" type False # ""bool"" type False # ""NoneType"" False # ""list"" type False # ""tuple"" type False # ""dict"" type False # ""set"" type\n\nedited Nov 24, 2022 at 3:03\n\nanswered Nov 23, 2022 at 15:26\n\nSuper Kai - Kazuya ItoSuper Kai - Kazuya Ito\n\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ncallable or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n4 self() as function within class, what does it do?\n\n6 What is self() in python?\n\n-2 What does s() mean?\n\n-2 Why isn\'t my program interpreting the user\'s input correctly?\n\n0 What are the different types of callables in Python?\n\n-2 What does it do when we call self() inside a class in Python?\n\n0 Why does pytorch treat network classes like functions?\n\n7401 What are metaclasses in Python?\n\n820 Collections.defaultdict difference with normal dict\n\n178 Python __call__ special method practical example\n\nSee more linked questions\n\n13 Why do we have callable objects in python?\n\n12 alternative to callable(),for use in Python 3\n\n12 Python functions and their __call__ attribute\n\n20 Do all callables have __name__?\n\n18 Functions, Callable Objects, and how both are created in Python\n\n1 statement ""foo is callable"" return false (with foo is a function)\n\n4 Function call and the __call__ attribute\n\n5 What is the definition of a callable type?\n\n5 What exactly is a caller in python?\n\n0 Reading class method definition - what is Callable?\n\nHot Network Questions\n\nHow to maintain dependencies shared among microservices?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nWhy danach instead of darüber?\n\nAre US enlisted personnel (as opposed to officers) required, or allowed, to disobey unlawful orders?\n\nType inference with type classes in Coq\n\nWhy does the voltage double at the end of line in a open transmission line (physical explanation)\n\nEvil God Challenge: What if an evil god is just trolling humanity and that explains why there\'s good in the world?\n\nDoes antenna arraying for deep space communications affect the CMB contribution?\n\nHow to read chainline specs for crankset and bottom bracket compatibility?\n\nWere there any stone vessels made in Paleolithic?\n\nCan the US president kill at will?\n\nRaid 0+1 Failure Cases Vs. Raid 1+0\n\nSwitch loop light convert to ceiling fan\n\nIs a desert planet with a small habitable area possible?\n\nWhat is the ""closest approximation"" to fields by an equational variety?\n\nSegments of a string, doubling in length\n\nDo thermodynamic cycles occur only in human-made machines?\n\nGuessing whether the revealed number is higher\n\nWhen do you know things are actually going poorly in graduate school?\n\nWhat type of interaction in a π-complex?\n\nSpace Invasion story. People get a blister, or pimple on their arm, treated, they are fine, untreated they die\n\nWhy didn\'t Smith give Atlas painkillers during the surgery?\n\n为什么字形不同的两个字「骨」的编码相同（从而是一个字）？ more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_3', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nWhat is a ""callable""?\n\nAsked 15 years, 9 months ago\n\nModified 1 year, 7 months ago\n\nNow that it\'s clear what a metaclass is, there is an associated concept that I use all the time without knowing what it really means.\n\nI suppose everybody made once a mistake with parenthesis, resulting in an ""object is not callable"" exception. What\'s more, using __init__ and __new__ lead to wonder what this bloody __call__ can be used for.\n\nCould you give me some explanations, including examples with the magic method ?\n\nedited Nov 26, 2022 at 22:34\n\n1.1m313313 gold badges4.2k4.2k silver badges3.4k3.4k bronze badges\n\nasked Sep 21, 2008 at 15:34\n\n590k116116 gold badges308308 silver badges334334 bronze badges 1\n\nrelated: Python internals: how callables work\n\n– jfs Commented Mar 23, 2012 at 14:46\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nA callable is anything that can be called.\n\nThe built-in callable (PyCallable_Check in objects.c) checks if the argument is either:\n\nan instance of a class with a __call__ method or\n\nis of a type that has a non null tp_call (c struct) member which indicates callability otherwise (such as in functions, methods etc.)\n\nThe method named __call__ is (according to the documentation)\n\nCalled when the instance is \'\'called\'\' as a function\n\nclass Foo: def __call__(self): print \'called\' foo_instance = Foo() foo_instance() #this is calling the __call__ method\n\nedited Aug 12, 2019 at 13:57\n\n11122 silver badges1717 bronze badges\n\nanswered Sep 21, 2008 at 15:44\n\nFlorian BöschFlorian Bösch\n\n27.7k1212 gold badges4949 silver badges5353 bronze badges 8\n\nNote that the builtin callable is being removed in Python 3.0 in favor of checking for call\n\n– Eli Courtwright Commented Sep 22, 2008 at 0:31\n\n@Eli: Hmm that sounds like a very bad move. callable actually tells you if something is callable or not, while checking for __call__ tells you nothing; If an object o provides __getattribute__ or __getattr__, hasattr(o, \'__call__\') may return True, yet o will still not be callable because Python skips __getattribute__ and __getattr__ for calls. The only real way left to check if something is callable is thus EAFP.\n\n– L̲̳o̲̳̳n̲̳̳g̲̳̳p̲̳o̲̳̳k̲̳̳e̲̳̳ Commented Jul 1, 2010 at 23:03\n\n@Longpoke: Just for the record, please see the documentation for callable() in Python 3.x: ""This function was first removed in Python 3.0 and then brought back in Python 3.2."".\n\n– Tadeck Commented May 8, 2013 at 4:41\n\nIt seems in python 3.8 only the presence of tp_call is checked. See implementation of PyCallable_Check, it\'s 3 lines.\n\n– Michele Piccolini Commented May 28, 2020 at 13:38\n\n@MichelePiccolini It\'s been that way for over a decade actually, but it still works to do what it\'s documented to do, which is to check if something is callable or not. When Python 3 was still young they changed the implementation, and now objects with a __call__ method just always have tp_call set as well. I\'m not sure when ""all callables have tp_call"" was implemented, but the PyCallable_Check change happened in back in August 2006: github.com/python/cpython/commit/…\n\n– mtraceur Commented Jan 29, 2021 at 19:11\n\n | Show 3 more comments\n\nFrom Python\'s sources object.c:\n\n/* Test whether an object can be called */ int PyCallable_Check(PyObject *x) { if (x == NULL) return 0; if (PyInstance_Check(x)) { PyObject *call = PyObject_GetAttrString(x, ""__call__""); if (call == NULL) { PyErr_Clear(); return 0; } /* Could test recursively but don\'t, for fear of endless recursion if some joker sets self.__call__ = self */ Py_DECREF(call); return 1; } else { return x->ob_type->tp_call != NULL; } }\n\nIf an object is an instance of some class then it is callable iff it has __call__ attribute.\n\nElse the object x is callable iff x->ob_type->tp_call != NULL\n\nDesciption of tp_call field:\n\nternaryfunc tp_call An optional pointer to a function that implements calling the object. This should be NULL if the object is not callable. The signature is the same as for PyObject_Call(). This field is inherited by subtypes.\n\nYou can always use built-in callable function to determine whether given object is callable or not; or better yet just call it and catch TypeError later. callable is removed in Python 3.0 and 3.1, use callable = lambda o: hasattr(o, \'__call__\') or isinstance(o, collections.Callable).\n\nExample, a simplistic cache implementation:\n\nclass Cached: def __init__(self, function): self.function = function self.cache = {} def __call__(self, *args): try: return self.cache[args] except KeyError: ret = self.cache[args] = self.function(*args) return ret\n\n@Cached def ack(x, y): return ack(x-1, ack(x, y-1)) if x*y else (x + y + 1)\n\nExample from standard library, file site.py, definition of built-in exit() and quit() functions:\n\nclass Quitter(object): def __init__(self, name): self.name = name def __repr__(self): return \'Use %s() or %s to exit\' % (self.name, eof) def __call__(self, code=None): # Shells like IDLE catch the SystemExit, but listen when their # stdin wrapper is closed. try: sys.stdin.close() except: pass raise SystemExit(code) __builtin__.quit = Quitter(\'quit\') __builtin__.exit = Quitter(\'exit\')\n\nedited Jan 2, 2014 at 3:22\n\n1,50833 gold badges1111 silver badges2828 bronze badges\n\nanswered Sep 22, 2008 at 15:04\n\n410k200200 gold badges1k1k silver badges1.7k1.7k bronze badges 13\n\nI find the example for the call method highly missleading because it mixes it with a recipe for caching and decorators, which add nothing to the understanding of call\n\n– Florian Bösch Commented Sep 26, 2008 at 16:13\n\nJ.F. Sebastian, also piling more examples you copy&pasted from somewhere else that are not minimal doesn\'t help.\n\n– Florian Bösch Commented Sep 27, 2008 at 13:10\n\n@J.F. Sebastian: It\'s BS that more life-like examples are better. I could show you life-like code that would make you weep as an example. Simple examples work too, and they work better to illustrate something because they don\'t distract.\n\n– Florian Bösch Commented Sep 28, 2008 at 22:32\n\nYou are explaining what\'s a callable, but you gave an example how to use callable objects to define a decorator. I know it\'s a typical usage of callable but this can confuse readers who just want to know what is callable and how to use callable. I\'d prefer @Florian Bösch\'s answer.\n\n– KFL Commented Mar 6, 2012 at 19:01\n\n@Kay: I also like the @Florian Bösch\'s answer (in its current form). btw, a decorator is not a typical usage of a ""callable"". The most typical ""callables"" are functions/methods such as def f(): ..., and class objects such as class C: ... i.e., f, \'\'.strip, len, and C all are callable. Instances that have a __call__() method in their class are relatively rare.\n\n– jfs Commented Mar 6, 2012 at 20:30\n\n | Show 8 more comments\n\nA callable is an object allows you to use round parenthesis ( ) and eventually pass some parameters, just like functions.\n\nEvery time you define a function python creates a callable object. In example, you could define the function func in these ways (it\'s the same):\n\nclass a(object): def __call__(self, *args): print \'Hello\' func = a() # or ... def func(*args): print \'Hello\'\n\nYou could use this method instead of methods like doit or run, I think it\'s just more clear to see obj() than obj.doit()\n\nedited Sep 26, 2008 at 13:31\n\nanswered Sep 26, 2008 at 13:22\n\nAndrea AmbuAndrea Ambu\n\n39k1414 gold badges5555 silver badges7777 bronze badges 0\n\nLet me explain backwards:\n\n... as syntactic sugar for:\n\nWhere foo can be any object that responds to __call__. When I say any object, I mean it: built-in types, your own classes and their instances.\n\nIn the case of built-in types, when you write:\n\nint(\'10\') unicode(10)\n\nYou\'re essentially doing:\n\nint.__call__(\'10\') unicode.__call__(10)\n\nThat\'s also why you don\'t have foo = new int in Python: you just make the class object return an instance of it on __call__. The way Python solves this is very elegant in my opinion.\n\nanswered Mar 22, 2013 at 23:38\n\n2,28411 gold badge2222 silver badges1717 bronze badges 2\n\nYou\'re essentially doing type(int).__call__(int, \'10\') and type(unicode).__call__(unicode, \'10\'). Dunders are always called on their class, not through the instance. And they never go through the metaclass either. For most cases that\'s just a nitpick, but it matters sometimes.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:24\n\nBuilt-in types work in special ways in the reference C implementation, although what happens is essentially equivalent to this, yes. For user-defined types, writing MyClass() attempts to call the class, by looking for a __call__ method on MyClass \'s class, i.e. it skips doing attribute lookup within MyClass (otherwise it could find a user-defined __call__ intended for the instances!) and finds type.__call__ - which then evaluates to a bound method on MyClass via the usual mechanisms, which is then called.\n\n– Karl Knechtel Commented Jul 5, 2022 at 5:35\n\n__call__ makes any object be callable as a function.\n\nThis example will output 8:\n\nclass Adder(object): def __init__(self, val): self.val = val def __call__(self, val): return self.val + val func = Adder(5) print func(3)\n\nanswered Sep 21, 2008 at 15:49\n\n23.1k99 gold badges6969 silver badges9696 bronze badges\n\nA Callable is an object that has the __call__ method. This means you can fake callable functions or do neat things like Partial Function Application where you take a function and add something that enhances it or fills in some of the parameters, returning something that can be called in turn (known as Currying in functional programming circles).\n\nCertain typographic errors will have the interpreter attempting to call something you did not intend, such as (for example) a string. This can produce errors where the interpreter attempts to execute a non-callable application. You can see this happening in a python interpreter by doing something like the transcript below.\n\n[nigel@k9 ~]$ python Python 2.5 (r25:51908, Nov 6 2007, 15:55:44) [GCC 4.1.2 20070925 (Red Hat 4.1.2-27)] on linux2 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> \'aaa\'() # <== Here we attempt to call a string. Traceback (most recent call last): File ""<stdin>"", line 1, in <module> TypeError: \'str\' object is not callable >>>\n\nedited Sep 21, 2008 at 15:51\n\nanswered Sep 21, 2008 at 15:43\n\nConcernedOfTunbridgeWellsConcernedOfTunbridgeWells\n\n65.8k1515 gold badges145145 silver badges198198 bronze badges\n\nQuite simply, a ""callable"" is something that can be called like a method. The built in function ""callable()"" will tell you whether something appears to be callable, as will checking for a call property. Functions are callable as are classes, class instances can be callable. See more about this here and here.\n\nanswered Sep 21, 2008 at 15:43\n\n14.9k55 gold badges3737 silver badges3939 bronze badges 1\n\nTo warp the readers\' mind a bit: classes are callable because they are instances of a class that has callable instances. You may have heard of it: it\'s called type. This object, uniquely, is its own type, and an instance of itself. It defines a __call__ method so that classes can be callable; that\'s where the implementation-specific magic is invoked to actually allocate memory, as well as potentially using the class\' __new__ hook and then calling __init__ on the resultant object.\n\n– Karl Knechtel Commented Jul 5, 2022 at 5:39\n\nIn Python a callable is an object which type has a __call__ method:\n\n>>> class Foo: ... pass ... >>> class Bar(object): ... pass ... >>> type(Foo).__call__(Foo) <__main__.Foo instance at 0x711440> >>> type(Bar).__call__(Bar) <__main__.Bar object at 0x712110> >>> def foo(bar): ... return bar ... >>> type(foo).__call__(foo, 42) 42\n\nAs simple as that :)\n\nThis of course can be overloaded:\n\n>>> class Foo(object): ... def __call__(self): ... return 42 ... >>> f = Foo() >>> f() 42\n\nanswered Sep 21, 2008 at 16:37\n\nArmin RonacherArmin Ronacher\n\n32.3k1414 gold badges6666 silver badges6969 bronze badges\n\nIt\'s something you can put ""(args)"" after and expect it to work. A callable is usually a method or a class. Methods get called, classes get instantiated.\n\nanswered Sep 21, 2008 at 15:43\n\n8,70655 gold badges4444 silver badges5151 bronze badges\n\nTo check function or method of class is callable or not that means we can call that function.\n\nClass A: def __init__(self,val): self.val = val def bar(self): print ""bar"" obj = A() callable(obj.bar) True callable(obj.__init___) False def foo(): return ""s"" callable(foo) True callable(foo()) False\n\nedited Sep 20, 2016 at 11:12\n\n2,10144 gold badges1919 silver badges2626 bronze badges\n\nanswered Sep 20, 2016 at 10:03\n\nRavi SinghRavi Singh\n\nAre you sure callable(obj.__init___) doesn\'t have an extra underscore (as in AttributeError)? If it doesn\'t, are you sure the answer isn\'t True for that one?\n\n– Mad Physicist Commented Sep 19, 2018 at 3:34\n\ncallables implement the __call__ special method so any object with such a method is callable.\n\nanswered May 7, 2012 at 21:40\n\n7,2111212 gold badges4040 silver badges6262 bronze badges 1\n\nAn instance on which you define __call__ won\'t be callable if the class doesn\'t define such a method.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:35\n\nCallable is a type or class of ""Build-in function or Method"" with a method call\n\n>>> type(callable) <class \'builtin_function_or_method\'> >>>\n\nExample: print is a callable object. With a build-in function call When you invoke the print function, Python creates an object of type print and invokes its method call passing the parameters if any.\n\n>>> type(print) <class \'builtin_function_or_method\'> >>> print.__call__(10) 10 >>> print(10) 10 >>>\n\nedited Feb 23, 2022 at 7:20\n\n12.3k2020 gold badges5050 silver badges8484 bronze badges\n\nanswered Apr 7, 2017 at 7:10\n\n73877 silver badges88 bronze badges 1\n\nSome of the info here is straight up wrong. E.g. ""When you invoke the print function, Python creates an object of type print and invokes its method __call__"". Python does not create a print object. It just calls something equivalent to type(print).__call__(print, *args, **kwargs). And the first sentence doesn\'t make much sense. You appear to be confusing a callable object and ""callable"" the function.\n\n– Mad Physicist Commented Sep 19, 2018 at 3:30\n\nA class, function, method and object which has __call__() are callable.\n\nYou can check if callable with callable() which returns True if callable and returns False if not callable as shown below:\n\nclass Class1: def __call__(self): print(""__call__"") class Class2: pass def func(): pass print(callable(Class1)) # Class1 print(callable(Class2)) # Class2 print(callable(Class1())) # Class1 object print(callable(Class2())) # Class2 object print(callable(func)) # func\n\nThen, only Class2 object which doesn\'t have __call__() is not callable returning False as shown below:\n\nTrue # Class1 True # Class2 True # Class1 object False # Class2 object True # func\n\nIn addition, all of them below are not callable returning False as shown below:\n\nprint(callable(""Hello"")) # ""str"" type print(callable(100)) # ""int"" type print(callable(100.23)) # ""float"" type print(callable(100 + 2j)) # ""complex"" type print(callable(True)) # ""bool"" type print(callable(None)) # ""NoneType"" print(callable([])) # ""list"" type print(callable(())) # ""tuple"" type print(callable({})) # ""dict"" type print(callable({""""})) # ""set"" type\n\nFalse # ""str"" type False # ""int"" type False # ""float"" type False # ""complex"" type False # ""bool"" type False # ""NoneType"" False # ""list"" type False # ""tuple"" type False # ""dict"" type False # ""set"" type\n\nedited Nov 24, 2022 at 3:03\n\nanswered Nov 23, 2022 at 15:26\n\nSuper Kai - Kazuya ItoSuper Kai - Kazuya Ito\n\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ncallable or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n4 self() as function within class, what does it do?\n\n6 What is self() in python?\n\n-2 What does s() mean?\n\n-2 Why isn\'t my program interpreting the user\'s input correctly?\n\n0 What are the different types of callables in Python?\n\n-2 What does it do when we call self() inside a class in Python?\n\n0 Why does pytorch treat network classes like functions?\n\n7401 What are metaclasses in Python?\n\n820 Collections.defaultdict difference with normal dict\n\n178 Python __call__ special method practical example\n\nSee more linked questions\n\n13 Why do we have callable objects in python?\n\n12 alternative to callable(),for use in Python 3\n\n12 Python functions and their __call__ attribute\n\n20 Do all callables have __name__?\n\n18 Functions, Callable Objects, and how both are created in Python\n\n1 statement ""foo is callable"" return false (with foo is a function)\n\n4 Function call and the __call__ attribute\n\n5 What is the definition of a callable type?\n\n5 What exactly is a caller in python?\n\n0 Reading class method definition - what is Callable?\n\nHot Network Questions\n\nHow to maintain dependencies shared among microservices?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nWhy danach instead of darüber?\n\nAre US enlisted personnel (as opposed to officers) required, or allowed, to disobey unlawful orders?\n\nType inference with type classes in Coq\n\nWhy does the voltage double at the end of line in a open transmission line (physical explanation)\n\nEvil God Challenge: What if an evil god is just trolling humanity and that explains why there\'s good in the world?\n\nDoes antenna arraying for deep space communications affect the CMB contribution?\n\nHow to read chainline specs for crankset and bottom bracket compatibility?\n\nWere there any stone vessels made in Paleolithic?\n\nCan the US president kill at will?\n\nRaid 0+1 Failure Cases Vs. Raid 1+0\n\nSwitch loop light convert to ceiling fan\n\nIs a desert planet with a small habitable area possible?\n\nWhat is the ""closest approximation"" to fields by an equational variety?\n\nSegments of a string, doubling in length\n\nDo thermodynamic cycles occur only in human-made machines?\n\nGuessing whether the revealed number is higher\n\nWhen do you know things are actually going poorly in graduate school?\n\nWhat type of interaction in a π-complex?\n\nSpace Invasion story. People get a blister, or pimple on their arm, treated, they are fine, untreated they die\n\nWhy didn\'t Smith give Atlas painkillers during the surgery?\n\n为什么字形不同的两个字「骨」的编码相同（从而是一个字）？ more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-04T15:07:45', 'title': 'python - What is a ""callable""? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/111234/what-is-a-callable'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nwhat exactly is python typing.Callable?\n\nAsked 2 years, 5 months ago\n\nModified 2 years, 4 months ago\n\nI have seen typing.Callable, but I didn\'t find any useful docs about it. What exactly is typing.Callable?\n\nasked Feb 3, 2022 at 7:24\n\nuser14901770user14901770\n\n50711 gold badge44 silver badges1010 bronze badges 5\n\ndocs.python.org/3/library/typing.html#typing.Callable\n\n– Chris Commented Feb 3, 2022 at 7:30\n\nSomething that you can call as a function. So Callable[[int, list[int]], bool] would be a function that takes two arguments, an int and a list of integers, and returns a boolean. Python 3.10 adds the ability to do more complicated things with Callable, but most users won\'t need that.\n\n– Frank Yellin Commented Feb 3, 2022 at 7:32\n\n@Chris. The docs only work well if you already know what a callable is.\n\n– Mad Physicist Commented Feb 3, 2022 at 7:35\n\n@FrankYellin. And PEP 677 removes the need to use Callable explicitly at all. Your example could be written as (int, list[int]) -> bool\n\n– Mad Physicist Commented Feb 3, 2022 at 7:36\n\nFYI: PEP 677 as linked above was rejected, so that syntax doesn\'t work. (but happened 7 days after the above comment was written)\n\n– Profilüfter Commented Jul 23, 2022 at 22:49\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\ntyping.Callable is the type you use to indicate a callable. Most python types that support the () operator are of the type collections.abc.Callable. Examples include functions, classmethods, staticmethods, bound methods and lambdas.\n\nIn summary, anything with a __call__ method (which is how () is implemented), is a callable.\n\nPEP 677 attempted to introduce implicit tuple-with-arrow syntax, so that something like Callable[[int, str], list[float]] could be expressed much more intuitively as (int, str) -> list[float]. The PEP was rejected because the benefits of the new syntax were not deemed sufficient given the added maintenance burden and possible room for confusion.\n\nedited Feb 14, 2022 at 23:44\n\nanswered Feb 3, 2022 at 7:34\n\nMad PhysicistMad Physicist\n\n112k2828 gold badges191191 silver badges274274 bronze badges 4\n\nOh that\'s great, I hadn\'t seen PEP 677!\n\n– juanpa.arrivillaga Commented Feb 3, 2022 at 8:20\n\n@juanpa.arrivillaga. I just found it totally by accident because of this question. I\'ve never really used typing much to begin with, and it won\'t come out until Python 3.11\n\n– Mad Physicist Commented Feb 3, 2022 at 8:23\n\nNote: PEP 677 has been rejected. Could you please update your answer?\n\n– decorator-factory Commented Feb 14, 2022 at 21:20\n\n@decorator-factory. Updated. Thanks for the catch\n\n– Mad Physicist Commented Feb 14, 2022 at 23:44\n\nThe typing module is used for type hints:\n\nThis module provides runtime support for type hints.\n\nWhat are type hints?\n\nThe documentation provides this example:\n\ndef greeting(name: str) -> str: return \'Hello \' + name\n\nIn the function greeting, the argument name is expected to be of type str and the return type str. Subtypes are accepted as arguments.\n\nHow to use typing.Callable\n\nAssume you want to define a function that takes two integers and performs some kind of operation on them that returns another integer:\n\ndef apply_func(a: int, b: int, func) -> int: return func(a, b)\n\nSo the expected type of the func argument in apply_func is ""something that can be called (e.g. a function) that takes two integer arguments and returns an integer"":\n\ntyping.Callable[[int, int], int]\n\nWhy bother with type hints in the first place?\n\nUsing type hints enables you to perform type checking. If you use an IDE like PyCharm or Visual Studio Code, you\'ll get visual feedback if you\'re using unexpected types:\n\nanswered Feb 3, 2022 at 7:49\n\nAnton Yang-WälderAnton Yang-Wälder\n\n88455 silver badges1515 bronze badges\n\nI would like to add to other answers that starting with Python 3.9, typing.Callable is deprecated. You should use collections.abc.Callable instead.\n\nFor more details and the rationale, you can take a look at PEP 585\n\nanswered Feb 14, 2022 at 21:22\n\ndecorator-factorydecorator-factory\n\n2,99511 gold badge1414 silver badges3030 bronze badges 3\n\nI don\'t understand why Callable is moved to collections.abc though. It is a valid way to type functions which has nothing to do with collections at all!\n\n– Sri Harsha Chilakapati Commented Jul 6, 2022 at 14:10\n\nIt appears that collections.Callabable also works in Python 3.9.\n\n– Paul Wintz Commented Oct 22, 2022 at 7:24\n\n@PaulWintz It\'s deprecated though, and in 3.10 it\'s removed\n\n– decorator-factory Commented Oct 25, 2022 at 19:44\n\nSee the documentation here.\n\nShort version, Callable is a type hint that indicates a function or other object which can be called.\n\nConsider a simple example below. The bar parameter is a callable object that takes two ints as parameters and returns an int.\n\n>>> def foo(bar: Callable[[int, int], int], a: int, b: int) -> int: ... return bar(a, b) ... >>> foo(int.__add__, 4, 5) 9 >>> class A: ... def __call__(self, a, b): ... return a * b ... >>> foo(A(), 6, 7) 42 >>> foo(lambda x, y: x - y, 8, 3) 5 >>>\n\nanswered Feb 3, 2022 at 7:37\n\n33.5k55 gold badges3030 silver badges4848 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntyping or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n2 what exactly are built-in types in Python?\n\n5 What is the definition of a callable type?\n\n87 python typing signature (typing.Callable) for function with kwargs\n\n5 Python type hints: What does Callable followed by a TypeVar mean?\n\n3 In Python, Is type implemented as a callable class?\n\n1 How is the typing.NewType syntax working?\n\n3 Python Typing for a Callable Class\n\n2 Python types: multiple unpackings in first argument to Callable\n\n0 Typing a Generic Callable Bug\n\n2 What does this type hint do?\n\nHot Network Questions\n\nIf a lambda is declared as a default argument, is it different for each call site?\n\nDid any 8-bit machine select palette by character name instead of color memory?\n\nWill 2.1"" schwalbe MTB tire in 25mm rim become wider that 2.25"" in 19mm rim?\n\nConflict between `\\setmainfont` and `\\mainmatter`\n\nWhy are 16th note apoggiaturas not written as normal 16th notes?\n\nAs an advisor, how can I help students with time management and procrastination?\n\nJava: Benchmark findFirst() and findAny() methods on non-parallel streams\n\nMaking a node in TikZ occupy no space - at least as far as centering the diagram on the page is concerned\n\nHow far back in time have historians estimated the rate of economic growth and the economic power of various empires?\n\nWhy didn\'t Jimmy Neutron realize immediately when he read the note on the refrigerator that the note is phony, as the note says ""son or daughter...""?\n\nHow can you identify VDP on Prescott ILS 21L without DME?\n\nKiCAD symbols for JST connectors\n\nDon\'t make noise. OR Don\'t make a noise\n\nWhy does the Egyptian Hieroglyph M8 (pool with lotus flowers) phonetically correspnd to \'Sh\' sound?\n\nWhy does independent research from people without formal academic qualifications generally turn out to be a complete waste of time?\n\nWhy would a plane be allowed to fly to LAX but not Maui?\n\nWhat is the value of air anisotropy?\n\nAre all Starship/Super Heavy ""cylinders"" 4mm thick?\n\nDoes the Grimme D3 correction improve band gaps of vdW heterostructures?\n\nBaffled by connection issue\n\nHow to maintain dependencies shared among microservices?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nGeometry question about a six-pack of beer\n\nWhy should I meet my advisor even if I have nothing to report? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_4', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nwhat exactly is python typing.Callable?\n\nAsked 2 years, 5 months ago\n\nModified 2 years, 4 months ago\n\nI have seen typing.Callable, but I didn\'t find any useful docs about it. What exactly is typing.Callable?\n\nasked Feb 3, 2022 at 7:24\n\nuser14901770user14901770\n\n50711 gold badge44 silver badges1010 bronze badges 5\n\ndocs.python.org/3/library/typing.html#typing.Callable\n\n– Chris Commented Feb 3, 2022 at 7:30\n\nSomething that you can call as a function. So Callable[[int, list[int]], bool] would be a function that takes two arguments, an int and a list of integers, and returns a boolean. Python 3.10 adds the ability to do more complicated things with Callable, but most users won\'t need that.\n\n– Frank Yellin Commented Feb 3, 2022 at 7:32\n\n@Chris. The docs only work well if you already know what a callable is.\n\n– Mad Physicist Commented Feb 3, 2022 at 7:35\n\n@FrankYellin. And PEP 677 removes the need to use Callable explicitly at all. Your example could be written as (int, list[int]) -> bool\n\n– Mad Physicist Commented Feb 3, 2022 at 7:36\n\nFYI: PEP 677 as linked above was rejected, so that syntax doesn\'t work. (but happened 7 days after the above comment was written)\n\n– Profilüfter Commented Jul 23, 2022 at 22:49\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\ntyping.Callable is the type you use to indicate a callable. Most python types that support the () operator are of the type collections.abc.Callable. Examples include functions, classmethods, staticmethods, bound methods and lambdas.\n\nIn summary, anything with a __call__ method (which is how () is implemented), is a callable.\n\nPEP 677 attempted to introduce implicit tuple-with-arrow syntax, so that something like Callable[[int, str], list[float]] could be expressed much more intuitively as (int, str) -> list[float]. The PEP was rejected because the benefits of the new syntax were not deemed sufficient given the added maintenance burden and possible room for confusion.\n\nedited Feb 14, 2022 at 23:44\n\nanswered Feb 3, 2022 at 7:34\n\nMad PhysicistMad Physicist\n\n112k2828 gold badges191191 silver badges274274 bronze badges 4\n\nOh that\'s great, I hadn\'t seen PEP 677!\n\n– juanpa.arrivillaga Commented Feb 3, 2022 at 8:20\n\n@juanpa.arrivillaga. I just found it totally by accident because of this question. I\'ve never really used typing much to begin with, and it won\'t come out until Python 3.11\n\n– Mad Physicist Commented Feb 3, 2022 at 8:23\n\nNote: PEP 677 has been rejected. Could you please update your answer?\n\n– decorator-factory Commented Feb 14, 2022 at 21:20\n\n@decorator-factory. Updated. Thanks for the catch\n\n– Mad Physicist Commented Feb 14, 2022 at 23:44\n\nThe typing module is used for type hints:\n\nThis module provides runtime support for type hints.\n\nWhat are type hints?\n\nThe documentation provides this example:\n\ndef greeting(name: str) -> str: return \'Hello \' + name\n\nIn the function greeting, the argument name is expected to be of type str and the return type str. Subtypes are accepted as arguments.\n\nHow to use typing.Callable\n\nAssume you want to define a function that takes two integers and performs some kind of operation on them that returns another integer:\n\ndef apply_func(a: int, b: int, func) -> int: return func(a, b)\n\nSo the expected type of the func argument in apply_func is ""something that can be called (e.g. a function) that takes two integer arguments and returns an integer"":\n\ntyping.Callable[[int, int], int]\n\nWhy bother with type hints in the first place?\n\nUsing type hints enables you to perform type checking. If you use an IDE like PyCharm or Visual Studio Code, you\'ll get visual feedback if you\'re using unexpected types:\n\nanswered Feb 3, 2022 at 7:49\n\nAnton Yang-WälderAnton Yang-Wälder\n\n88455 silver badges1515 bronze badges\n\nI would like to add to other answers that starting with Python 3.9, typing.Callable is deprecated. You should use collections.abc.Callable instead.\n\nFor more details and the rationale, you can take a look at PEP 585\n\nanswered Feb 14, 2022 at 21:22\n\ndecorator-factorydecorator-factory\n\n2,99511 gold badge1414 silver badges3030 bronze badges 3\n\nI don\'t understand why Callable is moved to collections.abc though. It is a valid way to type functions which has nothing to do with collections at all!\n\n– Sri Harsha Chilakapati Commented Jul 6, 2022 at 14:10\n\nIt appears that collections.Callabable also works in Python 3.9.\n\n– Paul Wintz Commented Oct 22, 2022 at 7:24\n\n@PaulWintz It\'s deprecated though, and in 3.10 it\'s removed\n\n– decorator-factory Commented Oct 25, 2022 at 19:44\n\nSee the documentation here.\n\nShort version, Callable is a type hint that indicates a function or other object which can be called.\n\nConsider a simple example below. The bar parameter is a callable object that takes two ints as parameters and returns an int.\n\n>>> def foo(bar: Callable[[int, int], int], a: int, b: int) -> int: ... return bar(a, b) ... >>> foo(int.__add__, 4, 5) 9 >>> class A: ... def __call__(self, a, b): ... return a * b ... >>> foo(A(), 6, 7) 42 >>> foo(lambda x, y: x - y, 8, 3) 5 >>>\n\nanswered Feb 3, 2022 at 7:37\n\n33.5k55 gold badges3030 silver badges4848 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntyping or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n2 what exactly are built-in types in Python?\n\n5 What is the definition of a callable type?\n\n87 python typing signature (typing.Callable) for function with kwargs\n\n5 Python type hints: What does Callable followed by a TypeVar mean?\n\n3 In Python, Is type implemented as a callable class?\n\n1 How is the typing.NewType syntax working?\n\n3 Python Typing for a Callable Class\n\n2 Python types: multiple unpackings in first argument to Callable\n\n0 Typing a Generic Callable Bug\n\n2 What does this type hint do?\n\nHot Network Questions\n\nIf a lambda is declared as a default argument, is it different for each call site?\n\nDid any 8-bit machine select palette by character name instead of color memory?\n\nWill 2.1"" schwalbe MTB tire in 25mm rim become wider that 2.25"" in 19mm rim?\n\nConflict between `\\setmainfont` and `\\mainmatter`\n\nWhy are 16th note apoggiaturas not written as normal 16th notes?\n\nAs an advisor, how can I help students with time management and procrastination?\n\nJava: Benchmark findFirst() and findAny() methods on non-parallel streams\n\nMaking a node in TikZ occupy no space - at least as far as centering the diagram on the page is concerned\n\nHow far back in time have historians estimated the rate of economic growth and the economic power of various empires?\n\nWhy didn\'t Jimmy Neutron realize immediately when he read the note on the refrigerator that the note is phony, as the note says ""son or daughter...""?\n\nHow can you identify VDP on Prescott ILS 21L without DME?\n\nKiCAD symbols for JST connectors\n\nDon\'t make noise. OR Don\'t make a noise\n\nWhy does the Egyptian Hieroglyph M8 (pool with lotus flowers) phonetically correspnd to \'Sh\' sound?\n\nWhy does independent research from people without formal academic qualifications generally turn out to be a complete waste of time?\n\nWhy would a plane be allowed to fly to LAX but not Maui?\n\nWhat is the value of air anisotropy?\n\nAre all Starship/Super Heavy ""cylinders"" 4mm thick?\n\nDoes the Grimme D3 correction improve band gaps of vdW heterostructures?\n\nBaffled by connection issue\n\nHow to maintain dependencies shared among microservices?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nGeometry question about a six-pack of beer\n\nWhy should I meet my advisor even if I have nothing to report? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-03T10:48:03', 'title': 'what exactly is python typing.Callable? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/70967266/what-exactly-is-python-typing-callable'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nTypeError: \'module\' object is not callable\n\nAsked 13 years, 6 months ago\n\nModified 6 months ago\n\nFile ""C:\\Users\\Administrator\\Documents\\Mibot\\oops\\blinkserv.py"", line 82, in __init__ self.serv = socket(AF_INET,SOCK_STREAM) TypeError: \'module\' object is not callable\n\nWhy am I getting this error? I\'m confused.\n\nHow can I solve this error?\n\nedited Dec 17, 2022 at 18:22\n\nSuper Kai - Kazuya Ito\n\nasked Dec 26, 2010 at 15:56\n\nuser551717user551717\n\n8,51355 gold badges1919 silver badges1010 bronze badges 2\n\nI once got this error because I had both a (global) variable and a function with the same name.\n\n– remustata Commented Jun 13, 2017 at 12:35\n\nI got this error with file name pointed to random.py in the same folder where I had a previously worked ipynb file. I changed the name of the file to random_function.py and relaunched the jupyter notebook. The error disappeared. To test if the name random.py was the problem, I changed the file random_function.py back to random.py. The problem came back. Dont name your file after name of a python library.\n\n– seakyourpeak Commented Jan 2, 2022 at 16:59\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nsocket is a module, containing the class socket.\n\nYou need to do socket.socket(...) or from socket import socket:\n\n>>> import socket >>> socket <module \'socket\' from \'C:\\Python27\\lib\\socket.pyc\'> >>> socket.socket <class \'socket._socketobject\'> >>> >>> from socket import socket >>> socket <class \'socket._socketobject\'>\n\nThis is what the error message means: It says module object is not callable, because your code is calling a module object. A module object is the type of thing you get when you import a module. What you were trying to do is to call a class object within the module object that happens to have the same name as the module that contains it.\n\nHere is a way to logically break down this sort of error:\n\n""module object is not callable. Python is telling me my code trying to call something that cannot be called. What is my code trying to call?""\n\n""The code is trying to call on socket. That should be callable! Is the variable socket is what I think it is?`\n\nI should print out what socket is and check print(socket)\n\nedited Sep 15, 2022 at 3:43\n\n3,6372626 silver badges3636 bronze badges\n\nanswered Dec 26, 2010 at 15:59\n\n123k1919 gold badges137137 silver badges170170 bronze badges 13\n\nI also changed it to from socket import socket and I\'m still getting the same error.\n\n– user551717 Commented Dec 26, 2010 at 16:07\n\nOhh I get it. The socket.socket was a little confusing. I simply did import write_to_file and then, since the method I was using inside of write_to_file.py is named writeToTextFile I simply rand write_to_file.writeToTextFile\n\n– maudulus Commented Jul 30, 2014 at 21:26\n\nIt\'s worth noting that this wasn\'t obvious to at least 133 people who took time to up vote (myself included) who didn\'t understand this. Now, it\'s obvious, and next time I reach in my toolbox, I will find this tool when a module is reported as ""not callable"". Getting started with a new language is the toughest part.\n\n– jamesmortensen Commented Sep 27, 2014 at 21:04\n\n@maudulus I don\'t find Python very user-friendly in these respects.\n\n– Snowcrash Commented Aug 6, 2020 at 11:19\n\nWhy on earth someone would ever want to have a ""handle"" to the module itself (which is just a source code file), rather than the class defined in it, in the first place? What is the use case for that?\n\n– Anton Samsonov Commented Oct 31, 2021 at 10:30\n\n | Show 8 more comments\n\nAssume that the content of YourClass.py is:\n\nclass YourClass: # ......\n\nfrom YourClassParentDir import YourClass # means YourClass.py\n\nIn this way, you will get TypeError: \'module\' object is not callable if you then tried to call YourClass().\n\nfrom YourClassParentDir.YourClass import YourClass # means Class YourClass\n\nor use YourClass.YourClass(), it works.\n\nedited Mar 31, 2021 at 6:37\n\nanswered Feb 22, 2017 at 7:19\n\n4,05611 gold badge2424 silver badges2525 bronze badges 3\n\nclass = YourClass.YourClass()\n\n– KunMing Xie Commented Oct 30, 2017 at 8:57\n\nI solved this issue by using from yourClass import *\n\n– Keith Commented Oct 18, 2018 at 5:44\n\nBest answer, i need to look more into the namespaces imports in python\n\n– Dimitris Thomas Commented Mar 28, 2021 at 15:34\n\nAdd to the main __init__.py in YourClassParentDir, e.g.:\n\nfrom .YourClass import YourClass\n\nThen, you will have an instance of your class ready when you import it into another script:\n\nfrom YourClassParentDir import YourClass\n\nedited Jan 8, 2019 at 13:04\n\n1,5591616 silver badges2626 bronze badges\n\nanswered Nov 14, 2014 at 12:05\n\nJose AlbanJose Alban\n\n7,68622 gold badges3737 silver badges1919 bronze badges 1\n\nShouldn\'t it be from .YourClass import YourClass in the __init__.py file ?\n\n– Nicolas Seiller Commented Apr 2, 2018 at 10:09\n\nShort answer: You are calling a file/directory as a function instead of real function\n\nThis kind of error happens when you import module thinking it as function and call it. So in python module is a .py file. Packages(directories) can also be considered as modules. Let\'s say I have a create.py file. In that file I have a function like this:\n\n#inside create.py def create(): pass\n\nNow, in another code file if I do like this:\n\n#inside main.py file import create create() #here create refers to create.py , so create.create() would work here\n\nIt gives this error as am calling the create.py file as a function. so I gotta do this:\n\nfrom create import create create() #now it works.\n\nedited Mar 4, 2022 at 19:41\n\n61.3k1818 gold badges179179 silver badges153153 bronze badges\n\nanswered Jul 15, 2020 at 7:42\n\nDeekshith AnandDeekshith Anand\n\n2,53122 gold badges2323 silver badges2424 bronze badges 0\n\nHere is another gotcha, that took me awhile to see even after reading these posts. I was setting up a script to call my python bin scripts. I was getting the module not callable too.\n\nMy zig was that I was doing the following:\n\nfrom mypackage.bin import myscript ... myscript(...)\n\nwhen my zag needed to do the following:\n\nfrom mypackage.bin.myscript import myscript ... myscript(...)\n\nIn summary, double check your package and module nesting.\n\nWhat I am trying to do is have a scripts directory that does not have the *.py extension, and still have the \'bin\' modules to be in mypackage/bin and these have my *.py extension. I am new to packaging, and trying to follow the standards as I am interpreting them. So, I have at the setup root:\n\nsetup.py scripts/ script1 mypackage/ bin/ script1.py subpackage1/ subpackage_etc/\n\nIf this is not compliant with standard, please let me know.\n\nanswered Jan 9, 2013 at 23:07\n\n1,7552222 silver badges3333 bronze badges\n\nIt seems like what you\'ve done is imported the socket module as import socket. Therefore socket is the module. You either need to change that line to self.serv = socket.socket(socket.AF_INET, socket.SOCK_STREAM), as well as every other use of the socket module, or change the import statement to from socket import socket.\n\nOr you\'ve got an import socket after your from socket import *:\n\n>>> from socket import * >>> serv = socket(AF_INET,SOCK_STREAM) >>> import socket >>> serv = socket(AF_INET,SOCK_STREAM) Traceback (most recent call last): File ""<input>"", line 1, in <module> TypeError: \'module\' object is not callable\n\nanswered Dec 26, 2010 at 15:59\n\n137k4545 gold badges192192 silver badges216216 bronze badges 3\n\nI\'ve imported socket as: from socket import * I can change it, but it\'ll take a while, so I\'m reluctant to.\n\n– user551717 Commented Dec 26, 2010 at 16:00\n\n@user You\'ve probably later somewhere got an import socket, which will import the module socket overriding the class socket. See code snippet in edit.\n\n– moinudin Commented Dec 26, 2010 at 16:02\n\n@user: you should change it. The reason from <...> import * imports are bad, bad, bad is more or less this: normally you know exactly what\'s in the global namespace, because it\'s exactly what you\'ve put there. But when you import *, you fill that namespace with all sorts of stuff that other modules define. In this case, it\'s unclear where the name socket came from -- is it the module or something defined in that module? If you always use import socket or from socket import socket, you will never have this problem, since you can see exactly what names are in use.\n\n– Katriel Commented Dec 26, 2010 at 16:06\n\nI know this thread is a year old, but the real problem is in your working directory.\n\nI believe that the working directory is C:\\Users\\Administrator\\Documents\\Mibot\\oops\\. Please check for the file named socket.py in this directory. Once you find it, rename or move it. When you import socket, socket.py from the current directory is used instead of the socket.py from Python\'s directory. Hope this helped. :)\n\nNote: Never use the file names from Python\'s directory to save your program\'s file name; it will conflict with your program(s).\n\nedited Nov 4, 2016 at 14:25\n\n1,40211 gold badge1616 silver badges2626 bronze badges\n\nanswered Jan 13, 2012 at 6:06\n\n18211 silver badge1010 bronze badges 1\n\nThis is definitely worth noting. I was just trying a quick check with sockets so I simply named the file socket.py. Well, that was causing this exact same error message. This page put me on the right track: python-notes.curiousefficiency.org/en/latest/python_concepts/…\n\n– Czechnology Commented May 26, 2016 at 22:04\n\nHere\'s a possible extra edge case that I stumbled upon and was puzzled by for a while, hope it helps someone:\n\nIn some_module/a.py:\n\nIn some_module/b.py:\n\nfrom . import a def b(): a()\n\nIn some_module/__init__.py:\n\nfrom .b import b from .a import a\n\nfrom some_module import b b()\n\nThen because when main.py loads b, it goes via __init__.py which tries to load b.py before a.py. This means when b.py tries to load a it gets the module rather than the function - meaning you\'ll get the error message module object is not callable\n\nThe solution here is to swap the order in some_module/__init__.py:\n\nfrom .a import a from .b import b\n\nOr, if this would create a circular dependency, change your file names to not match the functions, and load directly from the files rather than relying on __init__.py\n\nanswered Dec 13, 2021 at 18:00\n\nStuart MooreStuart Moore\n\n75166 silver badges3333 bronze badges\n\nI got the same error below:\n\nTypeError: \'module\' object is not callable\n\nWhen calling time() to print as shown below:\n\nimport time print(time()) # Here\n\nSo to solve the error, I called time.time() as shown below:\n\nimport time print(time.time()) # Here\n\nOr, I imported time from time as shown below:\n\nfrom time import time # Here print(time())\n\nAlso, I got the same error in Django View because I use @transaction as shown below:\n\n# ""views.py"" from django.http import HttpResponse from django.db import transaction # ↓ Here ↓ @transaction def test(request): return HttpResponse(""Test"")\n\nSo to solve the error, I use @transaction.atomic as shown below:\n\n# ""views.py"" from django.http import HttpResponse from django.db import transaction # ↓ Here ↓ @transaction.atomic def test(request): return HttpResponse(""Test"")\n\nedited May 11, 2023 at 22:16\n\nanswered Dec 17, 2022 at 18:19\n\nSuper Kai - Kazuya ItoSuper Kai - Kazuya Ito\n\nWhen configuring an console_scripts entrypoint in setup.py I found this issue existed when the endpoint was a module or package rather than a function within the module.\n\nTraceback (most recent call last): File ""/Users/ubuntu/.virtualenvs/virtualenv/bin/mycli"", line 11, in <module> load_entry_point(\'my-package\', \'console_scripts\', \'mycli\')() TypeError: \'module\' object is not callable\n\nfrom setuptools import setup setup ( # ... entry_points = { \'console_scripts\': [mycli=package.module.submodule] }, # ... )\n\nfrom setuptools import setup setup ( # ... entry_points = { \'console_scripts\': [mycli=package.module.submodule:main] }, # ... )\n\nSo that it would refer to a callable function rather than the module itself. It seems to make no difference if the module has a if __name__ == \'__main__\': block. This will not make the module callable.\n\nanswered Jun 22, 2017 at 20:33\n\nLuke ExtonLuke Exton\n\n3,66622 gold badges2121 silver badges3333 bronze badges 1\n\nan alternative is to use the scripts parameter instead of entry_points\n\n– Aaron F Commented Mar 22, 2022 at 11:12\n\nI faced the same problem. then I tried not using from YourClass import YourClass\n\nI just copied the whole code of YourClass.py and run it on the main code (or current code).it solved the error\n\nanswered May 4, 2021 at 18:29\n\nNazmus Sakib AkashNazmus Sakib Akash\n\n32722 silver badges33 bronze badges 1\n\nWell, that does work... However, you have just partially replaced a compiler (technically an interpreter) while cluttering up your codebase... Also, I\'m not sure this would work for binary-coded files (at least, not without some extra work)...\n\n– Stev Commented Feb 9 at 6:02\n\nyou are using the name of a module instead of the name of the class use\n\nits a weird thing with the module, but you can also use something like\n\nimport socket as sock\n\nanswered Nov 23, 2022 at 15:00\n\n2933 bronze badges 1\n\nThat answer was already given and highly upvoted 11 years ago. You can upvote one of the existing similar answer if you want to confirm it. But please, when answering to old question, be sure to bring new information that was not already given in existing answers.\n\n– chrslg Commented Nov 29, 2022 at 0:12\n\nI guess you have overridden the builtin function/variable or something else ""module"" by setting the global variable ""module"". just print the module see whats in it.\n\nanswered Mar 31, 2018 at 14:00\n\n40066 silver badges1919 bronze badges\n\nI had this error when I was trying to use optuna (a library for hyperparameter tuning) with LightGBM. After an hour struggle I realized that I was importing class directly and that was creating an issue.\n\nimport lightgbm as lgb def LGB_Objective(trial): parameters = { \'objective_type\': \'regression\', \'max_depth\': trial.suggest_int(\'max_depth\', 10, 60), \'boosting\': trial.suggest_categorical(\'boosting\', [\'gbdt\', \'rf\', \'dart\']), \'data_sample_strategy\': \'bagging\', \'num_iterations\': trial.suggest_int(\'num_iterations\', 50, 250), \'learning_rate\': trial.suggest_float(\'learning_rate\', 0.01, 1.0), \'reg_alpha\': trial.suggest_float(\'reg_alpha\', 0.01, 1.0), \'reg_lambda\': trial.suggest_float(\'reg_lambda\', 0.01, 1.0) } \'\'\'.....LightGBM model....\'\'\' model_lgb = lgb(**parameters) model_lgb.fit(x_train, y_train) y_pred = model_lgb.predict(x_test) return mse(y_test, y_pred, squared=True) study_lgb = optuna.create_study(direction=\'minimize\', study_name=\'lgb_regression\') study_lgb.optimize(LGB_Objective, n_trials=200)\n\nHere, the line model_lgb = lgb(**parameters) was trying to call the cLass itself. When I digged into the __init__.py file in site_packages folder of LGB installation as below, I identified the module which was fit to me (I was working on regression problem). I therefore imported LGBMRegressor and replaced lgb in my code with LGBMRegressor and it started working.\n\nYou can check in your code if you are importing the entire class/directory (by mistake) or the target module within the class.\n\nfrom lightgbm import LGBMRegressor def LGB_Objective(trial): parameters = { \'objective_type\': \'regression\', \'max_depth\': trial.suggest_int(\'max_depth\', 10, 60), \'boosting\': trial.suggest_categorical(\'boosting\', [\'gbdt\', \'rf\', \'dart\']), \'data_sample_strategy\': \'bagging\', \'num_iterations\': trial.suggest_int(\'num_iterations\', 50, 250), \'learning_rate\': trial.suggest_float(\'learning_rate\', 0.01, 1.0), \'reg_alpha\': trial.suggest_float(\'reg_alpha\', 0.01, 1.0), \'reg_lambda\': trial.suggest_float(\'reg_lambda\', 0.01, 1.0) } \'\'\'.....LightGBM model....\'\'\' model_lgb = LGBMRegressor(**parameters) #here I\'ve changed lgb to LGBMRegressor model_lgb.fit(x_train, y_train) y_pred = model_lgb.predict(x_test) return mse(y_test, y_pred, squared=True) study_lgb = optuna.create_study(direction=\'minimize\', study_name=\'lgb_regression\') study_lgb.optimize(LGB_Objective, n_trials=200)\n\nedited Jan 24, 2023 at 7:54\n\nanswered Jan 24, 2023 at 7:47\n\nBhanu ChanderBhanu Chander\n\n46011 gold badge77 silver badges1818 bronze badges\n\nfrom socket import socket\n\n#in a separate file you define a class like: class one: def __init__(self, name, value): self.name = name self.value = value pass #and in other file you define a other class. #and you want use defined class inside it.like: from one import one as clsOne #this model of import a class solve the error class two: item = clsOne(""A"", 22) print(item.name) pass\n\nedited Feb 23, 2023 at 16:16\n\nanswered Feb 23, 2023 at 16:08\n\nA simple way to solve this problem is export thePYTHONPATH variable enviroment. For example, for Python 2.6 in Debian/GNU Linux:\n\nexport PYTHONPATH=/usr/lib/python2.6`\n\nIn other operating systems, you would first find the location of this module or the socket.py file.\n\nedited Nov 4, 2016 at 14:56\n\n1,40211 gold badge1616 silver badges2626 bronze badges\n\nanswered Dec 30, 2012 at 15:02\n\nOscar ArdilaOscar Ardila\n\n6711 silver badge44 bronze badges\n\ncheck the import statements since a module is not callable. In Python, everything (including functions, methods, modules, classes etc.) is an object.\n\nanswered May 14, 2020 at 18:59\n\nAyesha SiddiqaAyesha Siddiqa\n\n34522 silver badges33 bronze badges 1\n\n""Check the import statements"" - Could you please be any bit more specific? At all? Please?\n\n– Stev Commented Feb 9 at 6:04\n\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ncallable or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n4 TypeError: \'module\' object is not callable - using datetime\n\n5 python \'module\' object is not callable when calling a function\n\n3 TypeError: \'module\' object is not callable Python3\n\n0 TypeError: \'Module\' object is not callable Help please\n\n1 Why is \'module\' object not callable?\n\n0 SQLAlchemy \'module\' not callable\n\n-1 How do you make a instance of an external class in python\n\n-1 How do I solve a TypeError module\n\n0 How do I run a Python script through a webpage?\n\n0 Module object is not callable when using Glob\n\nSee more linked questions\n\n0 TypeError when calling socket.connect(...)\n\n8 Socket.error: Invalid Argument supplied\n\n1 AttributeError: \'module\' object has no attribute\n\n3 import module from socket error python\n\n0 Python 3.4.2 - Socket module is not callable\n\n5 Can not find _socket module\n\n0 How come my code says that the module socket is not callable?\n\n3 AttributeError: type object \'socket\' has no attribute \'socket\'\n\n0 Python socket error: An object that is not a socket\n\n3 Getting ""TypeError: \'module\' object is not callable"" with `socket.socket()`\n\nHot Network Questions\n\nWhy does Google Maps only rotate 90º but not 180º when I rotate my iPhone?\n\nWhen do people say ""Toiletten"" in the plural?\n\nLooking for the title of a short story for my students to read about a woman searching for the last man alive\n\nA manifold whose tangent space of a sum of line bundles and higher rank vector bundles\n\nIn a sum of high-variance lognormals, what fraction comes from the first term?\n\nDominate Person/Dominate Monster and action order for the dominated creature\n\nAttaching foam to the bottom of a PCB\n\nHow to POSIX-ly ignore ""warning: command substitution: ignored null byte in input""?\n\nSubscripts in fractions on an exponent look terrible\n\nHow can I export my Location History now that this data is only stored locally on the phone?\n\nClass Echo Control Tower Procedures\n\nWhy does black have a higher win rate here?\n\nDirections of puff pastry folds\n\n""Though Fancy\'s casket were unlock\'d to choose"" in John Keats\'s ""Lamia""\n\nDid Zapata ask a young revolutionary, ""What is your name?"" and then write that man\'s name on a piece of paper?\n\nThe rise and fall of oval chainrings?\n\nHow can I power both sides of breaker box with two 120 volt battery backups?\n\nRecommend an essay, article, entry, author, or branch of philosophy that addresses the futility of arguing for or against free will\n\nHow do we define addition?\n\nOther than approximating the total energy of the system, what other information does the Hartree-Fock method provide?\n\nWhy not build smaller Ringworlds?\n\nTwo Sinus Multiply and Add\n\nHow is 11:22 four minutes slow if it\'s actually 11:29?\n\nHow to arrange three identical habitable planets in one solar system in similar or, if possible, same orbit? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_5', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nTypeError: \'module\' object is not callable\n\nAsked 13 years, 6 months ago\n\nModified 6 months ago\n\nFile ""C:\\Users\\Administrator\\Documents\\Mibot\\oops\\blinkserv.py"", line 82, in __init__ self.serv = socket(AF_INET,SOCK_STREAM) TypeError: \'module\' object is not callable\n\nWhy am I getting this error? I\'m confused.\n\nHow can I solve this error?\n\nedited Dec 17, 2022 at 18:22\n\nSuper Kai - Kazuya Ito\n\nasked Dec 26, 2010 at 15:56\n\nuser551717user551717\n\n8,51355 gold badges1919 silver badges1010 bronze badges 2\n\nI once got this error because I had both a (global) variable and a function with the same name.\n\n– remustata Commented Jun 13, 2017 at 12:35\n\nI got this error with file name pointed to random.py in the same folder where I had a previously worked ipynb file. I changed the name of the file to random_function.py and relaunched the jupyter notebook. The error disappeared. To test if the name random.py was the problem, I changed the file random_function.py back to random.py. The problem came back. Dont name your file after name of a python library.\n\n– seakyourpeak Commented Jan 2, 2022 at 16:59\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nsocket is a module, containing the class socket.\n\nYou need to do socket.socket(...) or from socket import socket:\n\n>>> import socket >>> socket <module \'socket\' from \'C:\\Python27\\lib\\socket.pyc\'> >>> socket.socket <class \'socket._socketobject\'> >>> >>> from socket import socket >>> socket <class \'socket._socketobject\'>\n\nThis is what the error message means: It says module object is not callable, because your code is calling a module object. A module object is the type of thing you get when you import a module. What you were trying to do is to call a class object within the module object that happens to have the same name as the module that contains it.\n\nHere is a way to logically break down this sort of error:\n\n""module object is not callable. Python is telling me my code trying to call something that cannot be called. What is my code trying to call?""\n\n""The code is trying to call on socket. That should be callable! Is the variable socket is what I think it is?`\n\nI should print out what socket is and check print(socket)\n\nedited Sep 15, 2022 at 3:43\n\n3,6372626 silver badges3636 bronze badges\n\nanswered Dec 26, 2010 at 15:59\n\n123k1919 gold badges137137 silver badges170170 bronze badges 13\n\nI also changed it to from socket import socket and I\'m still getting the same error.\n\n– user551717 Commented Dec 26, 2010 at 16:07\n\nOhh I get it. The socket.socket was a little confusing. I simply did import write_to_file and then, since the method I was using inside of write_to_file.py is named writeToTextFile I simply rand write_to_file.writeToTextFile\n\n– maudulus Commented Jul 30, 2014 at 21:26\n\nIt\'s worth noting that this wasn\'t obvious to at least 133 people who took time to up vote (myself included) who didn\'t understand this. Now, it\'s obvious, and next time I reach in my toolbox, I will find this tool when a module is reported as ""not callable"". Getting started with a new language is the toughest part.\n\n– jamesmortensen Commented Sep 27, 2014 at 21:04\n\n@maudulus I don\'t find Python very user-friendly in these respects.\n\n– Snowcrash Commented Aug 6, 2020 at 11:19\n\nWhy on earth someone would ever want to have a ""handle"" to the module itself (which is just a source code file), rather than the class defined in it, in the first place? What is the use case for that?\n\n– Anton Samsonov Commented Oct 31, 2021 at 10:30\n\n | Show 8 more comments\n\nAssume that the content of YourClass.py is:\n\nclass YourClass: # ......\n\nfrom YourClassParentDir import YourClass # means YourClass.py\n\nIn this way, you will get TypeError: \'module\' object is not callable if you then tried to call YourClass().\n\nfrom YourClassParentDir.YourClass import YourClass # means Class YourClass\n\nor use YourClass.YourClass(), it works.\n\nedited Mar 31, 2021 at 6:37\n\nanswered Feb 22, 2017 at 7:19\n\n4,05611 gold badge2424 silver badges2525 bronze badges 3\n\nclass = YourClass.YourClass()\n\n– KunMing Xie Commented Oct 30, 2017 at 8:57\n\nI solved this issue by using from yourClass import *\n\n– Keith Commented Oct 18, 2018 at 5:44\n\nBest answer, i need to look more into the namespaces imports in python\n\n– Dimitris Thomas Commented Mar 28, 2021 at 15:34\n\nAdd to the main __init__.py in YourClassParentDir, e.g.:\n\nfrom .YourClass import YourClass\n\nThen, you will have an instance of your class ready when you import it into another script:\n\nfrom YourClassParentDir import YourClass\n\nedited Jan 8, 2019 at 13:04\n\n1,5591616 silver badges2626 bronze badges\n\nanswered Nov 14, 2014 at 12:05\n\nJose AlbanJose Alban\n\n7,68622 gold badges3737 silver badges1919 bronze badges 1\n\nShouldn\'t it be from .YourClass import YourClass in the __init__.py file ?\n\n– Nicolas Seiller Commented Apr 2, 2018 at 10:09\n\nShort answer: You are calling a file/directory as a function instead of real function\n\nThis kind of error happens when you import module thinking it as function and call it. So in python module is a .py file. Packages(directories) can also be considered as modules. Let\'s say I have a create.py file. In that file I have a function like this:\n\n#inside create.py def create(): pass\n\nNow, in another code file if I do like this:\n\n#inside main.py file import create create() #here create refers to create.py , so create.create() would work here\n\nIt gives this error as am calling the create.py file as a function. so I gotta do this:\n\nfrom create import create create() #now it works.\n\nedited Mar 4, 2022 at 19:41\n\n61.3k1818 gold badges179179 silver badges153153 bronze badges\n\nanswered Jul 15, 2020 at 7:42\n\nDeekshith AnandDeekshith Anand\n\n2,53122 gold badges2323 silver badges2424 bronze badges 0\n\nHere is another gotcha, that took me awhile to see even after reading these posts. I was setting up a script to call my python bin scripts. I was getting the module not callable too.\n\nMy zig was that I was doing the following:\n\nfrom mypackage.bin import myscript ... myscript(...)\n\nwhen my zag needed to do the following:\n\nfrom mypackage.bin.myscript import myscript ... myscript(...)\n\nIn summary, double check your package and module nesting.\n\nWhat I am trying to do is have a scripts directory that does not have the *.py extension, and still have the \'bin\' modules to be in mypackage/bin and these have my *.py extension. I am new to packaging, and trying to follow the standards as I am interpreting them. So, I have at the setup root:\n\nsetup.py scripts/ script1 mypackage/ bin/ script1.py subpackage1/ subpackage_etc/\n\nIf this is not compliant with standard, please let me know.\n\nanswered Jan 9, 2013 at 23:07\n\n1,7552222 silver badges3333 bronze badges\n\nIt seems like what you\'ve done is imported the socket module as import socket. Therefore socket is the module. You either need to change that line to self.serv = socket.socket(socket.AF_INET, socket.SOCK_STREAM), as well as every other use of the socket module, or change the import statement to from socket import socket.\n\nOr you\'ve got an import socket after your from socket import *:\n\n>>> from socket import * >>> serv = socket(AF_INET,SOCK_STREAM) >>> import socket >>> serv = socket(AF_INET,SOCK_STREAM) Traceback (most recent call last): File ""<input>"", line 1, in <module> TypeError: \'module\' object is not callable\n\nanswered Dec 26, 2010 at 15:59\n\n137k4545 gold badges192192 silver badges216216 bronze badges 3\n\nI\'ve imported socket as: from socket import * I can change it, but it\'ll take a while, so I\'m reluctant to.\n\n– user551717 Commented Dec 26, 2010 at 16:00\n\n@user You\'ve probably later somewhere got an import socket, which will import the module socket overriding the class socket. See code snippet in edit.\n\n– moinudin Commented Dec 26, 2010 at 16:02\n\n@user: you should change it. The reason from <...> import * imports are bad, bad, bad is more or less this: normally you know exactly what\'s in the global namespace, because it\'s exactly what you\'ve put there. But when you import *, you fill that namespace with all sorts of stuff that other modules define. In this case, it\'s unclear where the name socket came from -- is it the module or something defined in that module? If you always use import socket or from socket import socket, you will never have this problem, since you can see exactly what names are in use.\n\n– Katriel Commented Dec 26, 2010 at 16:06\n\nI know this thread is a year old, but the real problem is in your working directory.\n\nI believe that the working directory is C:\\Users\\Administrator\\Documents\\Mibot\\oops\\. Please check for the file named socket.py in this directory. Once you find it, rename or move it. When you import socket, socket.py from the current directory is used instead of the socket.py from Python\'s directory. Hope this helped. :)\n\nNote: Never use the file names from Python\'s directory to save your program\'s file name; it will conflict with your program(s).\n\nedited Nov 4, 2016 at 14:25\n\n1,40211 gold badge1616 silver badges2626 bronze badges\n\nanswered Jan 13, 2012 at 6:06\n\n18211 silver badge1010 bronze badges 1\n\nThis is definitely worth noting. I was just trying a quick check with sockets so I simply named the file socket.py. Well, that was causing this exact same error message. This page put me on the right track: python-notes.curiousefficiency.org/en/latest/python_concepts/…\n\n– Czechnology Commented May 26, 2016 at 22:04\n\nHere\'s a possible extra edge case that I stumbled upon and was puzzled by for a while, hope it helps someone:\n\nIn some_module/a.py:\n\nIn some_module/b.py:\n\nfrom . import a def b(): a()\n\nIn some_module/__init__.py:\n\nfrom .b import b from .a import a\n\nfrom some_module import b b()\n\nThen because when main.py loads b, it goes via __init__.py which tries to load b.py before a.py. This means when b.py tries to load a it gets the module rather than the function - meaning you\'ll get the error message module object is not callable\n\nThe solution here is to swap the order in some_module/__init__.py:\n\nfrom .a import a from .b import b\n\nOr, if this would create a circular dependency, change your file names to not match the functions, and load directly from the files rather than relying on __init__.py\n\nanswered Dec 13, 2021 at 18:00\n\nStuart MooreStuart Moore\n\n75166 silver badges3333 bronze badges\n\nI got the same error below:\n\nTypeError: \'module\' object is not callable\n\nWhen calling time() to print as shown below:\n\nimport time print(time()) # Here\n\nSo to solve the error, I called time.time() as shown below:\n\nimport time print(time.time()) # Here\n\nOr, I imported time from time as shown below:\n\nfrom time import time # Here print(time())\n\nAlso, I got the same error in Django View because I use @transaction as shown below:\n\n# ""views.py"" from django.http import HttpResponse from django.db import transaction # ↓ Here ↓ @transaction def test(request): return HttpResponse(""Test"")\n\nSo to solve the error, I use @transaction.atomic as shown below:\n\n# ""views.py"" from django.http import HttpResponse from django.db import transaction # ↓ Here ↓ @transaction.atomic def test(request): return HttpResponse(""Test"")\n\nedited May 11, 2023 at 22:16\n\nanswered Dec 17, 2022 at 18:19\n\nSuper Kai - Kazuya ItoSuper Kai - Kazuya Ito\n\nWhen configuring an console_scripts entrypoint in setup.py I found this issue existed when the endpoint was a module or package rather than a function within the module.\n\nTraceback (most recent call last): File ""/Users/ubuntu/.virtualenvs/virtualenv/bin/mycli"", line 11, in <module> load_entry_point(\'my-package\', \'console_scripts\', \'mycli\')() TypeError: \'module\' object is not callable\n\nfrom setuptools import setup setup ( # ... entry_points = { \'console_scripts\': [mycli=package.module.submodule] }, # ... )\n\nfrom setuptools import setup setup ( # ... entry_points = { \'console_scripts\': [mycli=package.module.submodule:main] }, # ... )\n\nSo that it would refer to a callable function rather than the module itself. It seems to make no difference if the module has a if __name__ == \'__main__\': block. This will not make the module callable.\n\nanswered Jun 22, 2017 at 20:33\n\nLuke ExtonLuke Exton\n\n3,66622 gold badges2121 silver badges3333 bronze badges 1\n\nan alternative is to use the scripts parameter instead of entry_points\n\n– Aaron F Commented Mar 22, 2022 at 11:12\n\nI faced the same problem. then I tried not using from YourClass import YourClass\n\nI just copied the whole code of YourClass.py and run it on the main code (or current code).it solved the error\n\nanswered May 4, 2021 at 18:29\n\nNazmus Sakib AkashNazmus Sakib Akash\n\n32722 silver badges33 bronze badges 1\n\nWell, that does work... However, you have just partially replaced a compiler (technically an interpreter) while cluttering up your codebase... Also, I\'m not sure this would work for binary-coded files (at least, not without some extra work)...\n\n– Stev Commented Feb 9 at 6:02\n\nyou are using the name of a module instead of the name of the class use\n\nits a weird thing with the module, but you can also use something like\n\nimport socket as sock\n\nanswered Nov 23, 2022 at 15:00\n\n2933 bronze badges 1\n\nThat answer was already given and highly upvoted 11 years ago. You can upvote one of the existing similar answer if you want to confirm it. But please, when answering to old question, be sure to bring new information that was not already given in existing answers.\n\n– chrslg Commented Nov 29, 2022 at 0:12\n\nI guess you have overridden the builtin function/variable or something else ""module"" by setting the global variable ""module"". just print the module see whats in it.\n\nanswered Mar 31, 2018 at 14:00\n\n40066 silver badges1919 bronze badges\n\nI had this error when I was trying to use optuna (a library for hyperparameter tuning) with LightGBM. After an hour struggle I realized that I was importing class directly and that was creating an issue.\n\nimport lightgbm as lgb def LGB_Objective(trial): parameters = { \'objective_type\': \'regression\', \'max_depth\': trial.suggest_int(\'max_depth\', 10, 60), \'boosting\': trial.suggest_categorical(\'boosting\', [\'gbdt\', \'rf\', \'dart\']), \'data_sample_strategy\': \'bagging\', \'num_iterations\': trial.suggest_int(\'num_iterations\', 50, 250), \'learning_rate\': trial.suggest_float(\'learning_rate\', 0.01, 1.0), \'reg_alpha\': trial.suggest_float(\'reg_alpha\', 0.01, 1.0), \'reg_lambda\': trial.suggest_float(\'reg_lambda\', 0.01, 1.0) } \'\'\'.....LightGBM model....\'\'\' model_lgb = lgb(**parameters) model_lgb.fit(x_train, y_train) y_pred = model_lgb.predict(x_test) return mse(y_test, y_pred, squared=True) study_lgb = optuna.create_study(direction=\'minimize\', study_name=\'lgb_regression\') study_lgb.optimize(LGB_Objective, n_trials=200)\n\nHere, the line model_lgb = lgb(**parameters) was trying to call the cLass itself. When I digged into the __init__.py file in site_packages folder of LGB installation as below, I identified the module which was fit to me (I was working on regression problem). I therefore imported LGBMRegressor and replaced lgb in my code with LGBMRegressor and it started working.\n\nYou can check in your code if you are importing the entire class/directory (by mistake) or the target module within the class.\n\nfrom lightgbm import LGBMRegressor def LGB_Objective(trial): parameters = { \'objective_type\': \'regression\', \'max_depth\': trial.suggest_int(\'max_depth\', 10, 60), \'boosting\': trial.suggest_categorical(\'boosting\', [\'gbdt\', \'rf\', \'dart\']), \'data_sample_strategy\': \'bagging\', \'num_iterations\': trial.suggest_int(\'num_iterations\', 50, 250), \'learning_rate\': trial.suggest_float(\'learning_rate\', 0.01, 1.0), \'reg_alpha\': trial.suggest_float(\'reg_alpha\', 0.01, 1.0), \'reg_lambda\': trial.suggest_float(\'reg_lambda\', 0.01, 1.0) } \'\'\'.....LightGBM model....\'\'\' model_lgb = LGBMRegressor(**parameters) #here I\'ve changed lgb to LGBMRegressor model_lgb.fit(x_train, y_train) y_pred = model_lgb.predict(x_test) return mse(y_test, y_pred, squared=True) study_lgb = optuna.create_study(direction=\'minimize\', study_name=\'lgb_regression\') study_lgb.optimize(LGB_Objective, n_trials=200)\n\nedited Jan 24, 2023 at 7:54\n\nanswered Jan 24, 2023 at 7:47\n\nBhanu ChanderBhanu Chander\n\n46011 gold badge77 silver badges1818 bronze badges\n\nfrom socket import socket\n\n#in a separate file you define a class like: class one: def __init__(self, name, value): self.name = name self.value = value pass #and in other file you define a other class. #and you want use defined class inside it.like: from one import one as clsOne #this model of import a class solve the error class two: item = clsOne(""A"", 22) print(item.name) pass\n\nedited Feb 23, 2023 at 16:16\n\nanswered Feb 23, 2023 at 16:08\n\nA simple way to solve this problem is export thePYTHONPATH variable enviroment. For example, for Python 2.6 in Debian/GNU Linux:\n\nexport PYTHONPATH=/usr/lib/python2.6`\n\nIn other operating systems, you would first find the location of this module or the socket.py file.\n\nedited Nov 4, 2016 at 14:56\n\n1,40211 gold badge1616 silver badges2626 bronze badges\n\nanswered Dec 30, 2012 at 15:02\n\nOscar ArdilaOscar Ardila\n\n6711 silver badge44 bronze badges\n\ncheck the import statements since a module is not callable. In Python, everything (including functions, methods, modules, classes etc.) is an object.\n\nanswered May 14, 2020 at 18:59\n\nAyesha SiddiqaAyesha Siddiqa\n\n34522 silver badges33 bronze badges 1\n\n""Check the import statements"" - Could you please be any bit more specific? At all? Please?\n\n– Stev Commented Feb 9 at 6:04\n\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ncallable or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n4 TypeError: \'module\' object is not callable - using datetime\n\n5 python \'module\' object is not callable when calling a function\n\n3 TypeError: \'module\' object is not callable Python3\n\n0 TypeError: \'Module\' object is not callable Help please\n\n1 Why is \'module\' object not callable?\n\n0 SQLAlchemy \'module\' not callable\n\n-1 How do you make a instance of an external class in python\n\n-1 How do I solve a TypeError module\n\n0 How do I run a Python script through a webpage?\n\n0 Module object is not callable when using Glob\n\nSee more linked questions\n\n0 TypeError when calling socket.connect(...)\n\n8 Socket.error: Invalid Argument supplied\n\n1 AttributeError: \'module\' object has no attribute\n\n3 import module from socket error python\n\n0 Python 3.4.2 - Socket module is not callable\n\n5 Can not find _socket module\n\n0 How come my code says that the module socket is not callable?\n\n3 AttributeError: type object \'socket\' has no attribute \'socket\'\n\n0 Python socket error: An object that is not a socket\n\n3 Getting ""TypeError: \'module\' object is not callable"" with `socket.socket()`\n\nHot Network Questions\n\nWhy does Google Maps only rotate 90º but not 180º when I rotate my iPhone?\n\nWhen do people say ""Toiletten"" in the plural?\n\nLooking for the title of a short story for my students to read about a woman searching for the last man alive\n\nA manifold whose tangent space of a sum of line bundles and higher rank vector bundles\n\nIn a sum of high-variance lognormals, what fraction comes from the first term?\n\nDominate Person/Dominate Monster and action order for the dominated creature\n\nAttaching foam to the bottom of a PCB\n\nHow to POSIX-ly ignore ""warning: command substitution: ignored null byte in input""?\n\nSubscripts in fractions on an exponent look terrible\n\nHow can I export my Location History now that this data is only stored locally on the phone?\n\nClass Echo Control Tower Procedures\n\nWhy does black have a higher win rate here?\n\nDirections of puff pastry folds\n\n""Though Fancy\'s casket were unlock\'d to choose"" in John Keats\'s ""Lamia""\n\nDid Zapata ask a young revolutionary, ""What is your name?"" and then write that man\'s name on a piece of paper?\n\nThe rise and fall of oval chainrings?\n\nHow can I power both sides of breaker box with two 120 volt battery backups?\n\nRecommend an essay, article, entry, author, or branch of philosophy that addresses the futility of arguing for or against free will\n\nHow do we define addition?\n\nOther than approximating the total energy of the system, what other information does the Hartree-Fock method provide?\n\nWhy not build smaller Ringworlds?\n\nTwo Sinus Multiply and Add\n\nHow is 11:22 four minutes slow if it\'s actually 11:29?\n\nHow to arrange three identical habitable planets in one solar system in similar or, if possible, same orbit? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-07T16:33:11', 'title': ""python - TypeError: 'module' object is not callable - Stack Overflow"", 'url': 'https://stackoverflow.com/questions/4534438/typeerror-module-object-is-not-callable'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nPython NoneType object is not callable (beginner) [duplicate]\n\nAsked 12 years, 3 months ago\n\nModified 1 year, 3 months ago\n\nThis question already has answers here:\n\nHow does using a function (callback) as an argument to another function work in Python? (11 answers)\n\nIs it possible to make a `for` loop without an iterator variable? (How can I make code loop a set number of times?) (16 answers)\n\nIt tells me line 1 and line 5 (new to debugging/programming, not sure if that helps)\n\ndef hi(): print(\'hi\') def loop(f, n): # f repeats n times if n <= 0: return else: f() loop(f, n-1)\n\n>>> loop(hi(), 5) hi f() TypeError: \'NoneType\' object is not callable\n\nWhy does it give me this error?\n\nImprove this question\n\nedited Apr 13, 2020 at 20:36\n\n4,36711 gold badge2020 silver badges2525 bronze badges\n\nasked Mar 19, 2012 at 10:54\n\n1,97622 gold badges1717 silver badges3232 bronze badges 0\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou want to pass the function object hi to your loop() function, not the result of a call to hi() (which is None since hi() doesn\'t return anything).\n\n>>> loop(hi, 5) hi hi hi hi hi\n\nPerhaps this will help you understand better:\n\n>>> print hi() hi None >>> print hi <function hi at 0x0000000002422648>\n\nanswered Mar 19, 2012 at 10:56\n\nTim PietzckerTim Pietzcker\n\n334k5858 gold badges514514 silver badges567567 bronze badges 2\n\nYou\'re welcome. Also, you might want to call your function recurse or something similar since it doesn\'t actually loop...\n\n– Tim Pietzcker Commented Mar 19, 2012 at 11:09\n\nFor anyone in the future seeing this after decorating a class/function: make sure you return something from the implementation of the decorator!\n\n– ijustlovemath Commented Jun 12, 2023 at 3:26\n\nWhy does it give me that error?\n\nBecause your first parameter you pass to the loop function is None but your function is expecting an callable object, which None object isn\'t.\n\nTherefore you have to pass the callable-object which is in your case the hi function object.\n\ndef hi(): print \'hi\' def loop(f, n): #f repeats n times if n<=0: return else: f() loop(f, n-1) loop(hi, 5)\n\nanswered Mar 19, 2012 at 11:10\n\nNicola CorettiNicola Coretti\n\n2,69333 gold badges2121 silver badges2323 bronze badges 3\n\nWhat if the hi function took an argument, e.g. text, and printed the string passed as the variable text. How would that be handled?\n\n– Olivier Commented Jun 20, 2018 at 9:56\n\ntry lambda arg: hi(arg)\n\n– Ajax Commented Dec 17, 2018 at 19:55\n\nWith hi taking an argument, loop(hi, 5) still runs, outputting the expected string, eg \'hi\'+<your passed text>. @OlivierdeBroqueville\n\n– Treefish Zhang Commented Feb 12, 2022 at 23:06\n\nYou should not pass the call function hi() to the loop() function, This will give the result.\n\ndef hi(): print(\'hi\') def loop(f, n): #f repeats n times if n<=0: return else: f() loop(f, n-1) loop(hi, 5) # Do not use hi() function inside loop() function\n\nanswered Feb 9, 2018 at 4:55\n\n1621313 bronze badges\n\nI faced the error ""TypeError: \'NoneType\' object is not callable "" but for a different issue. With the above clues, i was able to debug and got it right! The issue that i faced was : I had the custome Library written and my file wasnt recognizing it although i had mentioned it\n\nexample: Library ../../../libraries/customlibraries/ExtendedWaitKeywords.py the keywords from my custom library were recognized and that error was resolved only after specifying the complete path, as it was not getting the callable function.\n\nanswered Oct 26, 2018 at 8:17\n\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nnonetype or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n244 Is it possible to make a `for` loop without an iterator variable? (How can I make code loop a set number of times?)\n\n140 How does using a function (callback) as an argument to another function work in Python?\n\n9 Why does the print function return None?\n\n3 TypeError: ""NoneType"" object is not callable in Google Colab\n\n2 JWT token generator with Authlib 0.11\n\n-2 Right Click using a hotkey in python\n\n-3 What\'s the difference between the two code snippets below. To me, they look almost identical, but they behave completely different\n\n4 Python ""NoneType is not callable"" error\n\n7 Python old-style class __getattr__: ""TypeError: \'NoneType\' object is not callable""\n\n2 TypeError: \'NoneType\' object is not callable, but not sure why. Can someone please explain?\n\n3 TypeError: NoneType object is not callable\n\n0 python error ""typeerror: nonetype object not callable""\n\n0 TypeError: \'NoneType\' object is not callable (Python)\n\n0 Python NoneType error when attempting to access object functions\n\n0 TypeError: \'NoneType\' object is not callable but object is not nonetype\n\n1 Getting "" \'NoneType\' object is not callable"" Error While Using Classes in Python\n\nHot Network Questions\n\nEverything has a tiny nuclear reactor in it. How much of a concern are illegal nuclear bombs?\n\nCapture multiple errors before raising an exception\n\nim2double and im2uint8 Functions Implementation for Image in C++\n\nWould this telescope be capable to detect Middle Ages Civilization?\n\nWhat did Plautus mean by ""intervelli""?\n\nIs a desert planet with a small habitable area possible?\n\nHourly pay rate calculation between Recruiting and Payroll Systems\n\nDo thermodynamic cycles occur only in human-made machines?\n\nReversing vowels in a string\n\nAny alternative to lockdown browser?\n\nconfidence intervals for proportions containing a theoretically impossible value (zero)\n\nMysterious plumbing piece\n\nWhat is the mean of random effects?\n\nMy lemon tree was in a hail storm\n\nA check given by castling: is it a discovered check or a special case?\n\nCan a country refuse to deliver a person accused of attempted murder?\n\nIs intuitionistic mathematics situated in time?\n\nDo you always experience the gravitational influence of other mass as you see them in your frame?\n\n130 TIF DEM file (total size 3 GB) become 7.4 GB TIF file after merging. Why?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nPregnancy in a hibernated state\n\npdfgrep How to locate the pages that contain multiple strings and print the page numbers?\n\nWhat are the `estimates` returned by `avg_slopes()` in modelsummary?\n\nfirefox returns odd results for file:/// or file:///tmp more hot questions', metadata={'id': 'web-search_0', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nPython NoneType object is not callable (beginner) [duplicate]\n\nAsked 12 years, 3 months ago\n\nModified 1 year, 3 months ago\n\nThis question already has answers here:\n\nHow does using a function (callback) as an argument to another function work in Python? (11 answers)\n\nIs it possible to make a `for` loop without an iterator variable? (How can I make code loop a set number of times?) (16 answers)\n\nIt tells me line 1 and line 5 (new to debugging/programming, not sure if that helps)\n\ndef hi(): print(\'hi\') def loop(f, n): # f repeats n times if n <= 0: return else: f() loop(f, n-1)\n\n>>> loop(hi(), 5) hi f() TypeError: \'NoneType\' object is not callable\n\nWhy does it give me this error?\n\nImprove this question\n\nedited Apr 13, 2020 at 20:36\n\n4,36711 gold badge2020 silver badges2525 bronze badges\n\nasked Mar 19, 2012 at 10:54\n\n1,97622 gold badges1717 silver badges3232 bronze badges 0\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou want to pass the function object hi to your loop() function, not the result of a call to hi() (which is None since hi() doesn\'t return anything).\n\n>>> loop(hi, 5) hi hi hi hi hi\n\nPerhaps this will help you understand better:\n\n>>> print hi() hi None >>> print hi <function hi at 0x0000000002422648>\n\nanswered Mar 19, 2012 at 10:56\n\nTim PietzckerTim Pietzcker\n\n334k5858 gold badges514514 silver badges567567 bronze badges 2\n\nYou\'re welcome. Also, you might want to call your function recurse or something similar since it doesn\'t actually loop...\n\n– Tim Pietzcker Commented Mar 19, 2012 at 11:09\n\nFor anyone in the future seeing this after decorating a class/function: make sure you return something from the implementation of the decorator!\n\n– ijustlovemath Commented Jun 12, 2023 at 3:26\n\nWhy does it give me that error?\n\nBecause your first parameter you pass to the loop function is None but your function is expecting an callable object, which None object isn\'t.\n\nTherefore you have to pass the callable-object which is in your case the hi function object.\n\ndef hi(): print \'hi\' def loop(f, n): #f repeats n times if n<=0: return else: f() loop(f, n-1) loop(hi, 5)\n\nanswered Mar 19, 2012 at 11:10\n\nNicola CorettiNicola Coretti\n\n2,69333 gold badges2121 silver badges2323 bronze badges 3\n\nWhat if the hi function took an argument, e.g. text, and printed the string passed as the variable text. How would that be handled?\n\n– Olivier Commented Jun 20, 2018 at 9:56\n\ntry lambda arg: hi(arg)\n\n– Ajax Commented Dec 17, 2018 at 19:55\n\nWith hi taking an argument, loop(hi, 5) still runs, outputting the expected string, eg \'hi\'+<your passed text>. @OlivierdeBroqueville\n\n– Treefish Zhang Commented Feb 12, 2022 at 23:06\n\nYou should not pass the call function hi() to the loop() function, This will give the result.\n\ndef hi(): print(\'hi\') def loop(f, n): #f repeats n times if n<=0: return else: f() loop(f, n-1) loop(hi, 5) # Do not use hi() function inside loop() function\n\nanswered Feb 9, 2018 at 4:55\n\n1621313 bronze badges\n\nI faced the error ""TypeError: \'NoneType\' object is not callable "" but for a different issue. With the above clues, i was able to debug and got it right! The issue that i faced was : I had the custome Library written and my file wasnt recognizing it although i had mentioned it\n\nexample: Library ../../../libraries/customlibraries/ExtendedWaitKeywords.py the keywords from my custom library were recognized and that error was resolved only after specifying the complete path, as it was not getting the callable function.\n\nanswered Oct 26, 2018 at 8:17\n\nHighly active question. Earn 10 reputation (not counting the association bonus) in order to answer this question. The reputation requirement helps protect this question from spam and non-answer activity.\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nnonetype or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n244 Is it possible to make a `for` loop without an iterator variable? (How can I make code loop a set number of times?)\n\n140 How does using a function (callback) as an argument to another function work in Python?\n\n9 Why does the print function return None?\n\n3 TypeError: ""NoneType"" object is not callable in Google Colab\n\n2 JWT token generator with Authlib 0.11\n\n-2 Right Click using a hotkey in python\n\n-3 What\'s the difference between the two code snippets below. To me, they look almost identical, but they behave completely different\n\n4 Python ""NoneType is not callable"" error\n\n7 Python old-style class __getattr__: ""TypeError: \'NoneType\' object is not callable""\n\n2 TypeError: \'NoneType\' object is not callable, but not sure why. Can someone please explain?\n\n3 TypeError: NoneType object is not callable\n\n0 python error ""typeerror: nonetype object not callable""\n\n0 TypeError: \'NoneType\' object is not callable (Python)\n\n0 Python NoneType error when attempting to access object functions\n\n0 TypeError: \'NoneType\' object is not callable but object is not nonetype\n\n1 Getting "" \'NoneType\' object is not callable"" Error While Using Classes in Python\n\nHot Network Questions\n\nEverything has a tiny nuclear reactor in it. How much of a concern are illegal nuclear bombs?\n\nCapture multiple errors before raising an exception\n\nim2double and im2uint8 Functions Implementation for Image in C++\n\nWould this telescope be capable to detect Middle Ages Civilization?\n\nWhat did Plautus mean by ""intervelli""?\n\nIs a desert planet with a small habitable area possible?\n\nHourly pay rate calculation between Recruiting and Payroll Systems\n\nDo thermodynamic cycles occur only in human-made machines?\n\nReversing vowels in a string\n\nAny alternative to lockdown browser?\n\nconfidence intervals for proportions containing a theoretically impossible value (zero)\n\nMysterious plumbing piece\n\nWhat is the mean of random effects?\n\nMy lemon tree was in a hail storm\n\nA check given by castling: is it a discovered check or a special case?\n\nCan a country refuse to deliver a person accused of attempted murder?\n\nIs intuitionistic mathematics situated in time?\n\nDo you always experience the gravitational influence of other mass as you see them in your frame?\n\n130 TIF DEM file (total size 3 GB) become 7.4 GB TIF file after merging. Why?\n\nMeasure by mass vs. \'Spooned and Leveled\'\n\nPregnancy in a hibernated state\n\npdfgrep How to locate the pages that contain multiple strings and print the page numbers?\n\nWhat are the `estimates` returned by `avg_slopes()` in modelsummary?\n\nfirefox returns odd results for file:/// or file:///tmp more hot questions', 'timestamp': '2024-07-05T10:23:24', 'title': 'Python NoneType object is not callable (beginner) - Stack Overflow', 'url': 'https://stackoverflow.com/questions/9768865/python-nonetype-object-is-not-callable-beginner'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nType hinting Callable with no parameters\n\nAsked 3 years, 9 months ago\n\nModified 3 years, 9 months ago\n\nI want to use type hinting for a function with no parameters\n\nfrom typing import Callable def no_parameters_returns_int() -> int: return 7 def get_int_returns_int(a: int) -> int: return a def call_function(next_method: Callable[[], int]): print(next_method()) call_function(no_parameters_returns_int) # no indication of error from IDE expected call_function(get_int_returns_int) # expected an indication of error from IDE\n\nI expected PyCharm to mark the line when I pass a function that does take parameters. Also tried Callabale[[None], int] and Callabale[[...], int]. However the first one hinting the passed function to receive a None type argument, second one hinting the passed function to receive at least one argument.\n\nIs it possible to hint that the passed function receives no arguments?\n\nImprove this question\n\nedited Oct 11, 2020 at 7:14\n\n12.3k2020 gold badges5050 silver badges8484 bronze badges\n\nasked Oct 10, 2020 at 21:05\n\nEliy ArlevEliy Arlev\n\n56611 gold badge44 silver badges1616 bronze badges 4\n\nMyPy detects it correctly: mypy-play.net/…\n\n– jonrsharpe Commented Oct 10, 2020 at 21:17\n\n@jonrsharpe thanks, I switched to Callable[..., int] and got no errors from hinting, however I expected an error at runtime\n\n– Eliy Arlev Commented Oct 10, 2020 at 21:28\n\nWhat do you mean at runtime? Python doesn\'t type check at runtime, I assumed you were asking about PyCharm\'s failure to warn.\n\n– jonrsharpe Commented Oct 10, 2020 at 21:29\n\n@jonrsharpe I meant that when python interpreter executes the line hof(one_arg) it should result in an error. However I opened your code on mypy and changed Callable[[], int] to Callable[..., int] and now there are no errors. Does mypy-play just tests type checking?\n\n– Eliy Arlev Commented Oct 10, 2020 at 21:37\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIs it possible to hint that the passed function receives no arguments?\n\nThe correct way to type hint a Callable without arguments is stated in:\n\n""Fundamental building blocks"", PEP 483\n\nCallable[[t1, t2, ..., tn], tr]. A function with positional argument types t1 etc., and return type tr. The argument list may be empty n==0.\n\nAn explicit example is given in:\n\n""Covariance and Contravariance"", PEP 483\n\n- Callable[[], int] is a subtype of Callable[[], float]. - Callable[[], Manager] is a subtype of Callable[[], Employee].\n\nfrom typing import Callable def feeder(get_next_item: Callable[[], str]) -> None: # Body\n\nThe built-in name None should be distinguished from the type None (the first is used to access the second):\n\n3.2. The standard type hierarchy, Data Model\n\nThis type has a single value. There is a single object with this value. This object is accessed through the built-in name None.\n\nThe syntax and meaning of the built-in name None used as a type hint is a special case:\n\n""Using None"", PEP 484\n\nWhen used in a type hint, the expression None is considered equivalent to type(None).\n\nConsidering the above, it\'s less of a surprise the following two ways -of trying to write a Callable type hint of a function without arguments- are wrong:\n\nCallable[[None], tr] Callable[[type(None)], tr]\n\nThe Ellipsis in a Callable type hint simply means:\n\nNote that there are no square brackets around the ellipsis. The arguments of the callback are completely unconstrained in this case (and keyword arguments are acceptable).\n\nSince it is ""unconstrained"" the following is unlikely to cause the static type checker to issue any warnings because of arguments:\n\nWorth noting, the relation between Callable, Any and ... (Ellipsis).\n\n""The Any type"", PEP 484\n\nAs well, a bare Callable in an annotation is equivalent to Callable[..., Any]\n\nFinally, if you run your code through MyPy the expected warning is in fact issued:\n\nmain.py:13: error: Argument 1 to ""call_function"" has incompatible type ""Callable[[int], int]""; expected ""Callable[[], int]"" Found 1 error in 1 file (checked 1 source file)\n\nI checked your example in PyCharm 2020.2 Pro and the IDE does not issue the above warning. Notice that PyCharm uses its own implementation of PEP 484, and their static type checker has been know to have bugs.\n\nI think you found a bug...\n\nFinal Note: Running type(None) gives NoneType. In Python 3 NoneType isn\'t exposed for import although in Python 2 it was importable.\n\nEDIT: For some reason Python 3.10 is reintroducing types.NoneType.\n\nedited Oct 11, 2020 at 16:31\n\nanswered Oct 11, 2020 at 7:11\n\n12.3k2020 gold badges5050 silver badges8484 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nnonetype or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n41 Where is the NoneType located?\n\n175 How can I tell PyCharm what type a parameter is expected to be?\n\n140 Python 3 type hinting for None?\n\n18 Python type hints and `*args`\n\n5 Python typing: declare type of callable when give it instance method\n\n4 Python 2.7 type hinting callable types in PyCharm\n\n3 How to use a function parameter as a type hint?\n\n2 Python Type Hints ignored\n\n2 Typehints for python class\n\n2 Why are type hints for variables not handled as type hints for function parameters?\n\n2 How to write a Python type hint that specifies a Callable that takes certain parameters or 0 parameters?\n\nHot Network Questions\n\nAn adjective for something peaceful but sad?\n\nWhat\'s the shape of the Control Weather spell? (if any)\n\nHow to handle my player\'s use of Greater Camouflage Dye?\n\nError concerning projectile motion in respected textbook?\n\nAre there other proposed translations of ""aelfheres"" in Beowulf than a name?\n\nEquivalence of first/second choice with naive probability - I don\'t buy it\n\nAlternatives to iterrow loops in python pandas dataframes\n\nWhat does ""master o\' the Tiger"" mean?\n\nAre any Population III stars red dwarfs?\n\nHow to turn a sum into an integral?\n\nFilled in \\diamond without XeLaTeX\n\nCan a MicroSD card help speed up my Mini PC?\n\nAre there any reasons I shouldn\'t remove this odd nook from a basement room?\n\nHow should I deal with curves in new deck boards during installation?\n\nNilpotency of generalized cohomology\n\nWhat enforcement exists for medical informed consent?\n\nQuantum: why linear combination of vectors (superposition) is described as ""both at the same time""?\n\nWhich interpreter for ""Unicode text, UTF-8 text executable""\n\nHelp understanding the implications of Security Breach section of an NDA\n\nSchreier-Sims algorithm for solving Rubik\'s cube\n\nNegative kinetic energy on a step potential\n\nWhy is this transformer placed on rails?\n\nHow can I sort all levels except the innermost level?\n\niMac 21"" i3 very slow even on clean install of OS more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_1', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nType hinting Callable with no parameters\n\nAsked 3 years, 9 months ago\n\nModified 3 years, 9 months ago\n\nI want to use type hinting for a function with no parameters\n\nfrom typing import Callable def no_parameters_returns_int() -> int: return 7 def get_int_returns_int(a: int) -> int: return a def call_function(next_method: Callable[[], int]): print(next_method()) call_function(no_parameters_returns_int) # no indication of error from IDE expected call_function(get_int_returns_int) # expected an indication of error from IDE\n\nI expected PyCharm to mark the line when I pass a function that does take parameters. Also tried Callabale[[None], int] and Callabale[[...], int]. However the first one hinting the passed function to receive a None type argument, second one hinting the passed function to receive at least one argument.\n\nIs it possible to hint that the passed function receives no arguments?\n\nImprove this question\n\nedited Oct 11, 2020 at 7:14\n\n12.3k2020 gold badges5050 silver badges8484 bronze badges\n\nasked Oct 10, 2020 at 21:05\n\nEliy ArlevEliy Arlev\n\n56611 gold badge44 silver badges1616 bronze badges 4\n\nMyPy detects it correctly: mypy-play.net/…\n\n– jonrsharpe Commented Oct 10, 2020 at 21:17\n\n@jonrsharpe thanks, I switched to Callable[..., int] and got no errors from hinting, however I expected an error at runtime\n\n– Eliy Arlev Commented Oct 10, 2020 at 21:28\n\nWhat do you mean at runtime? Python doesn\'t type check at runtime, I assumed you were asking about PyCharm\'s failure to warn.\n\n– jonrsharpe Commented Oct 10, 2020 at 21:29\n\n@jonrsharpe I meant that when python interpreter executes the line hof(one_arg) it should result in an error. However I opened your code on mypy and changed Callable[[], int] to Callable[..., int] and now there are no errors. Does mypy-play just tests type checking?\n\n– Eliy Arlev Commented Oct 10, 2020 at 21:37\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIs it possible to hint that the passed function receives no arguments?\n\nThe correct way to type hint a Callable without arguments is stated in:\n\n""Fundamental building blocks"", PEP 483\n\nCallable[[t1, t2, ..., tn], tr]. A function with positional argument types t1 etc., and return type tr. The argument list may be empty n==0.\n\nAn explicit example is given in:\n\n""Covariance and Contravariance"", PEP 483\n\n- Callable[[], int] is a subtype of Callable[[], float]. - Callable[[], Manager] is a subtype of Callable[[], Employee].\n\nfrom typing import Callable def feeder(get_next_item: Callable[[], str]) -> None: # Body\n\nThe built-in name None should be distinguished from the type None (the first is used to access the second):\n\n3.2. The standard type hierarchy, Data Model\n\nThis type has a single value. There is a single object with this value. This object is accessed through the built-in name None.\n\nThe syntax and meaning of the built-in name None used as a type hint is a special case:\n\n""Using None"", PEP 484\n\nWhen used in a type hint, the expression None is considered equivalent to type(None).\n\nConsidering the above, it\'s less of a surprise the following two ways -of trying to write a Callable type hint of a function without arguments- are wrong:\n\nCallable[[None], tr] Callable[[type(None)], tr]\n\nThe Ellipsis in a Callable type hint simply means:\n\nNote that there are no square brackets around the ellipsis. The arguments of the callback are completely unconstrained in this case (and keyword arguments are acceptable).\n\nSince it is ""unconstrained"" the following is unlikely to cause the static type checker to issue any warnings because of arguments:\n\nWorth noting, the relation between Callable, Any and ... (Ellipsis).\n\n""The Any type"", PEP 484\n\nAs well, a bare Callable in an annotation is equivalent to Callable[..., Any]\n\nFinally, if you run your code through MyPy the expected warning is in fact issued:\n\nmain.py:13: error: Argument 1 to ""call_function"" has incompatible type ""Callable[[int], int]""; expected ""Callable[[], int]"" Found 1 error in 1 file (checked 1 source file)\n\nI checked your example in PyCharm 2020.2 Pro and the IDE does not issue the above warning. Notice that PyCharm uses its own implementation of PEP 484, and their static type checker has been know to have bugs.\n\nI think you found a bug...\n\nFinal Note: Running type(None) gives NoneType. In Python 3 NoneType isn\'t exposed for import although in Python 2 it was importable.\n\nEDIT: For some reason Python 3.10 is reintroducing types.NoneType.\n\nedited Oct 11, 2020 at 16:31\n\nanswered Oct 11, 2020 at 7:11\n\n12.3k2020 gold badges5050 silver badges8484 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nnonetype or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n41 Where is the NoneType located?\n\n175 How can I tell PyCharm what type a parameter is expected to be?\n\n140 Python 3 type hinting for None?\n\n18 Python type hints and `*args`\n\n5 Python typing: declare type of callable when give it instance method\n\n4 Python 2.7 type hinting callable types in PyCharm\n\n3 How to use a function parameter as a type hint?\n\n2 Python Type Hints ignored\n\n2 Typehints for python class\n\n2 Why are type hints for variables not handled as type hints for function parameters?\n\n2 How to write a Python type hint that specifies a Callable that takes certain parameters or 0 parameters?\n\nHot Network Questions\n\nAn adjective for something peaceful but sad?\n\nWhat\'s the shape of the Control Weather spell? (if any)\n\nHow to handle my player\'s use of Greater Camouflage Dye?\n\nError concerning projectile motion in respected textbook?\n\nAre there other proposed translations of ""aelfheres"" in Beowulf than a name?\n\nEquivalence of first/second choice with naive probability - I don\'t buy it\n\nAlternatives to iterrow loops in python pandas dataframes\n\nWhat does ""master o\' the Tiger"" mean?\n\nAre any Population III stars red dwarfs?\n\nHow to turn a sum into an integral?\n\nFilled in \\diamond without XeLaTeX\n\nCan a MicroSD card help speed up my Mini PC?\n\nAre there any reasons I shouldn\'t remove this odd nook from a basement room?\n\nHow should I deal with curves in new deck boards during installation?\n\nNilpotency of generalized cohomology\n\nWhat enforcement exists for medical informed consent?\n\nQuantum: why linear combination of vectors (superposition) is described as ""both at the same time""?\n\nWhich interpreter for ""Unicode text, UTF-8 text executable""\n\nHelp understanding the implications of Security Breach section of an NDA\n\nSchreier-Sims algorithm for solving Rubik\'s cube\n\nNegative kinetic energy on a step potential\n\nWhy is this transformer placed on rails?\n\nHow can I sort all levels except the innermost level?\n\niMac 21"" i3 very slow even on clean install of OS more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-10T09:52:22', 'title': 'python - Type hinting Callable with no parameters - Stack Overflow', 'url': 'https://stackoverflow.com/questions/64298298/type-hinting-callable-with-no-parameters'})], [Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. #51376\n\nJJKK1313 opened this issue\n\nAug 8, 2021 · 8 comments\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. #51376\n\nJJKK1313 opened this issue\n\nAug 8, 2021 · 8 comments\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author TF 2.4\n\nfor issues related to TF 2.4 type:bug\n\nI\'m Trying to write my own RandomHSV custom layer to create faster augmentations to images (and later on just remove it from the model). the code is as follows:\n\nclass RandomHSV(Layer): """"""Adding Random Noise to HSV image. output is RGB Input shape: Arbitrary. Output shape: Same as input. Arguments: hsv_max_amp: list or tuple of the maximum amplitudes of the noise in range of [0, 1] name: A string, the name of the layer. NOTE: MAKE SURE INPUTS LAYERS HAS THE BATCH SIZE FIGURED BEFORE USING THIS LAYER. """""" def __init__(self, hsv_max_amp=(0, 0, 0), batch_size=None, name=None, **kwargs): super(RandomHSV, self).__init__(name=name, **kwargs) # self.rand_generator = tf.random.Generator.from_seed(seed=int(time.time())) self.hsv_max_amp = np.array(list(hsv_max_amp), dtype=\'float32\') def build(self, input_shape): # self.batch_size = input_shape[0] super(RandomHSV, self).build(input_shape) # Be sure to call this at the end def call(self, inputs, training=True, **kwargs): def hsv_noise(): hsv = tf.image.rgb_to_hsv(inputs) # the random noise is a random matrix in shape (batch_size, img_w, img_h, depth) # after creating the random matrix, multiply it (element wise) by the self.hsv_max_amp. # that gets multiplied by random enabler (np.random.randint(0, 2, 3) -> 3 items, 0 or 1) # then removing an offset. random_noise = (np.random.ranf(inputs.shape) * ( np.random.random(1) * self.hsv_max_amp * np.random.randint(0, 2, 3)) - self.hsv_max_amp / 2) * 2 # those lines will cut any number which goes above 1 or goes below 0 (round it to 1 or 0 respectively). hsv = tf.minimum(1., tf.maximum(0., tf.add(hsv, random_noise))) batch = tf.image.hsv_to_rgb(hsv) batch.set_shape(inputs.shape) return batch # applying hsv_noise if Training. if Testing then just passing batch forward unchanged return control_flow_util.smart_cond(pred=training, true_fn=hsv_noise, false_fn=lambda: inputs) def compute_output_shape(self, input_shape): return input_shape def get_config(self): config = { \'hsv_max_amp\': self.hsv_max_amp, \'batch_size\': self.batch_size } base_config = super(RandomHSV, self).get_config() return dict(list(base_config.items()) + list(config.items()))\n\nWhen the whole model is built (or trying to build it self), I get the error:\n\nTypeError: \'NoneType\' object cannot be interpreted as an integer\n\nor when I use the tf.random.Generator() class:\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor.\n\nthis is because I don\'t tell the model\'s Inputs layers the batch size before I run it. I tried to figure out how to work around it, but the only solution is to let the model know the batch size (Input(batch_size=BATCH_SIZE) layer is mandatory) on other layers (such as Dense, Conv2D and so on...) its not mandatory to name the batch_size. Is there any idea how could I do it on my layer too?\n\nThe text was updated successfully, but these errors were encountered:\n\nJJKK1313 added the type:bug\n\nkumariko self-assigned this\n\n@JJKK1313 Can you please fill the issue template to expedite the troubleshooting process. Thanks!\n\nkumariko added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\n@kumariko I did, its that post. There is a specific template I should use? @kumariko #\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\n@JJKK1313 In order to reproduce the issue reported here, could you please provide the complete code and the dataset , tensorflow version you are using. Thanks!\n\nkumariko added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nThe version if Tensorflow is 2.4.1 The dataset is LFW dataset. The missing part from the code:\n\ndef get_siamese_model(input_shape, conv2d_filts): # Define the tensors for the two input images # ================================= THE INNER MODEL ================================= augmentations = Sequential( [ tf.keras.layers.experimental.preprocessing.RandomContrast(factor=0.70), RandomBrightnessLayer(max_delta=0.1, name=\'RandomBrightness\'), RandomHSVLayer(hsv_max_amp=[0.05, 0.25, 0], name=\'RandomHSVPreprocessor\'), ], name=configurations.AUGMENTATIONS_LAYER_NAME ) # ================================= THE INNER MODEL ================================= # THE PROBLEM IS HERE, WHEN NOT SPECIFING \'batch_size\' TO INPUT LAYER. left_input = Input(input_shape, name=""Input1"") right_input = Input(input_shape, name=""Input2"") left_input_augmented = augmentations(left_input) right_input_augmented = augmentations(right_input) # Generate the encodings (feature vectors) for the two images body = build_body(input_shape=input_shape, conv2d_filts=conv2d_filts) encoded_l = body(left_input_augmented) encoded_r = body(right_input_augmented) distance = Lambda(lambda embeds: euclidean_distance(embeds), name=\'Distance\')([encoded_l, encoded_r]) # normed_layer = BatchNormalization()(distance) # making sure the distances wont be all over the place. distance = Dense(1, activation=\'sigmoid\', name=\'Prediction\')(distance) # Connect the inputs with the outputs siamese_net = Model(inputs=[left_input, right_input], outputs=distance) return siamese_net # DataFrameGeneratorClass is a custom pair image generator, since nither Keras or Tensorflow has one. train_gen, test_gen = DataFrameGeneratorClass.create_train_test_generators( csv_path=\'data.csv\', validation_split=0.1, shuffle=True, batch_size=32, rescale=1. / 255., img_size=(128, 128), ) siamese_model = get_siamese_model(IMG_SIZE, conv2d_filts=CONV2D_FILTERS) siamese_model.summary() # siamese_model.load_weights(\'check_points/29-07-21_034351/\') optimizer = Adam() siamese_model.compile(loss=\'binary_crossentropy\', optimizer=optimizer, metrics=[\'accuracy\', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]) history = siamese_model.fit(train_gen, epochs=10, validation_data=test_gen)\n\nIf you want the code for the generators:\n\nclass PairDataGenerator(tf.keras.utils.Sequence): """""" NOTE: ON model.fit(SHUFFLE=FALSE) -> MUST BE FALSE! """""" def __init__(self, df_similar: pd.DataFrame, df_dissimilar: pd.DataFrame, batch_size=256, shuffle=True, rescale: {float, None} = 1. / 255., target_img_size=(128, 128), preprocess_function=None, rand_preproc_single: {ImageDataGenerator, dict} = None, rand_preproc_batch: list = None): # self.batch_counter = 0 self.last_batch_index = 0 self.df_similar = df_similar.sample(frac=1).reset_index(drop=True) self.df_dissimilar = df_dissimilar.sample(frac=1).reset_index(drop=True) self.preprocess_function = preprocess_function self.rescale = rescale self.target_img_size = target_img_size # rounding up batch size to be an even number. self.batch_size = batch_size + (batch_size % 2 == 1) self.shuffle = shuffle # indexes of rows. every batch we draw 2 samples. 1 similar and 1 dissimilar assert batch_size <= len(self.df_similar) + len( self.df_dissimilar), f""Cannot create batch of size {batch_size} when there "" \\ f""are only {len(self.df_similar)} samples"" self.indexes_similar = np.arange(len(self.df_similar)) self.similar_max_idx = len(self.indexes_similar) // self.batch_size self.indexes_dissimilar = np.arange(len(self.df_dissimilar)) self.dissimilar_max_idx = len(self.indexes_dissimilar) // self.batch_size if self.shuffle: np.random.shuffle(self.indexes_dissimilar) np.random.shuffle(self.indexes_similar) self.rand_preproc_single = rand_preproc_single self.rand_preproc_batch = rand_preproc_batch def __len__(self): """"""Denotes the number of batches per epoch"""""" return (len(self.df_similar) + len(self.df_dissimilar)) // self.batch_size def __getitem__(self, index): """"""Generate one batch of data"""""" # Generate indexes of the batch batch_idx_sim = index % self.similar_max_idx indexes_sim = self.indexes_similar[batch_idx_sim * (self.batch_size // 2): (batch_idx_sim + 1) * (self.batch_size // 2)] batch_idx_dissim = index % self.dissimilar_max_idx indexes_dissim = self.indexes_dissimilar[batch_idx_dissim * (self.batch_size // 2): (batch_idx_dissim + 1) * (self.batch_size // 2)] self.last_batch_index = index img1 = [] img2 = [] labels = [0, 1] * (self.batch_size // 2) # creating labels list np.random.shuffle(labels) same_counter = 0 diff_counter = 0 for idx, label in enumerate(labels): if label == configurations.LABELS[\'same\']: img1_path, img2_path, _ = self.df_similar.iloc[indexes_sim[same_counter]] same_counter += 1 else: img1_path, img2_path, _ = self.df_dissimilar.iloc[indexes_dissim[diff_counter]] diff_counter += 1 img1.append(self.load_image(img1_path)) img2.append(self.load_image(img2_path)) img1 = np.array(img1, dtype=\'float32\') img2 = np.array(img2, dtype=\'float32\') labels = np.array(labels, dtype=\'float32\') if self.rand_preproc_batch is not None: for func in self.rand_preproc_batch: img1 = func(img1) img2 = func(img2) return [img1, img2], labels def on_epoch_end(self): """"""Updates indexes after each epoch"""""" if (self.last_batch_index + 1) % self.dissimilar_max_idx == 0 and self.shuffle: self.indexes_dissimilar = np.arange(len(self.df_dissimilar)) np.random.shuffle(self.indexes_dissimilar) if (self.last_batch_index + 1) % self.similar_max_idx == 0 and self.shuffle: self.indexes_similar = np.arange(len(self.df_similar)) np.random.shuffle(self.indexes_similar) def load_image(self, path): """""" loads an image using tensorflow tools :param path: absolute path (refers to the project\'s folder) to the image :return: an image array. """""" if self.rand_preproc_single is not None: if isinstance(self.rand_preproc_single, ImageDataGenerator): img_arr = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB) img_arr = self.rand_preproc_single.random_transform(img_arr) img_arr = cv2.resize(img_arr, self.target_img_size) else: img_arr = my_utils.image_augmentations(path, **self.rand_preproc_single) else: img_arr = cv2.imread(path) img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB) img_arr = cv2.resize(img_arr, self.target_img_size) if self.preprocess_function is not None: img_arr = self.preprocess_function(img_arr) elif self.rescale is not None: img_arr = img_arr * self.rescale return img_arr\n\nand the function you saw on main:\n\ndef create_train_test_generators(csv_path: str, pair_gen: bool = True, validation_split: float = 0.1, shuffle: bool = True, batch_size: int = 256, rescale: {float, None} = 1. / 255., img_size: tuple = (128, 128), preprocess_func=None, rand_preproc_single: {ImageDataGenerator, dict} = None, rand_preproc_batch: list = None, ): """""" Initialization :param rand_preproc_batch: list of functions which augments a whole batch. :param rand_preproc_single: an ImageDataGenerator Instance or dictionary for my_utils.image_augmentation function. :param pair_gen: boolean. True = Pair Gen. False = Triplets Gen. :param rescale: rescaling factor :param preprocess_func: a preprocessing function for the network\'s inputs. :param img_size: the size of the output image. :param batch_size: batch size :param shuffle: whether to shuffle the data before casting to Train Test. :param csv_path: the path to the csv file which we create DataFrame object from. :param validation_split: how much from the whole data goes to validation. """""" from sklearn.model_selection import train_test_split # read all the csv file df = pd.read_csv(csv_path, index_col=False) params = dict(batch_size=batch_size, shuffle=shuffle, rescale=rescale, target_img_size=img_size, preprocess_function=preprocess_func, rand_preproc_single=rand_preproc_single, rand_preproc_batch=rand_preproc_batch) if pair_gen: # split the rows to similar and dissimilar by label column df_similar = df.where(df[\'labels\'] == 1.0).dropna().reset_index(drop=True) df_dissimilar = df.where(df[\'labels\'] == 0.0).dropna().reset_index(drop=True) print(len(df_dissimilar), len(df_similar)) print(f""Found {len(df)} pairs."") # split similar and dissimilar to train and test (4 groups) df_train_similar, df_test_similar = train_test_split(df_similar, test_size=validation_split, shuffle=shuffle) df_train_dissimilar, df_test_dissimilar = train_test_split(df_dissimilar, test_size=validation_split, shuffle=shuffle) # drop the index column, no need of that. df_train_similar = df_train_similar.reset_index(drop=True) df_test_similar = df_test_similar.reset_index(drop=True) df_train_dissimilar = df_train_dissimilar.reset_index(drop=True) df_test_similar = df_test_similar.reset_index(drop=True) # print(len(pd.merge(df_train_similar, df_train_dissimilar, how=\'inner\', on=[\'img1_p\', \'img2_p\', \'labels\']))) print(f""Total={len(df)}"", f""Train={len(df_train_similar)} + {len(df_train_dissimilar)}"", f""Test={len(df_test_similar)} + {len(df_test_dissimilar)}"", sep=\'\\n\') return PairDataGenerator(df_similar=df_train_similar, df_dissimilar=df_train_dissimilar, **params), \\ PairDataGenerator(df_similar=df_test_similar, df_dissimilar=df_test_dissimilar, **params) else: print(f""Found {len(df)} triplets."") # split similar and dissimilar to train and test (4 groups) df_train, df_test = train_test_split(df, test_size=validation_split, shuffle=shuffle) # drop the index column, no need of that. df_train = df_train.reset_index(drop=True) df_test = df_test.reset_index(drop=True) # print(len(pd.merge(df_train_similar, df_train_dissimilar, how=\'inner\', on=[\'img1_p\', \'img2_p\', \'labels\']))) print(f""Total={len(df)}"", f""Train={len(df_train)}"", f""Test={len(df_test)}"", sep=\'\\n\') return TripletDataGenerator(df=df_train, **params), \\ TripletDataGenerator(df=df_test, **params)\n\nNotice that the else part isn\'t relevant here so I didn\'t add it. If you still want it, let me know.\n\nkumariko added TF 2.4\n\nfor issues related to TF 2.4 comp:keras\n\nKeras related issues and removed stat:awaiting response\n\nStatus - Awaiting response from author labels\n\n@JJKK1313 Thank you for the response ! We see that this issue is more related to Keras .Please post this issue on keras-team/keras repo. To know more see; https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999 .Thanks!\n\nkumariko added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you\'d like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author TF 2.4\n\nfor issues related to TF 2.4 type:bug\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_0', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. #51376\n\nJJKK1313 opened this issue\n\nAug 8, 2021 · 8 comments\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. #51376\n\nJJKK1313 opened this issue\n\nAug 8, 2021 · 8 comments\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author TF 2.4\n\nfor issues related to TF 2.4 type:bug\n\nI\'m Trying to write my own RandomHSV custom layer to create faster augmentations to images (and later on just remove it from the model). the code is as follows:\n\nclass RandomHSV(Layer): """"""Adding Random Noise to HSV image. output is RGB Input shape: Arbitrary. Output shape: Same as input. Arguments: hsv_max_amp: list or tuple of the maximum amplitudes of the noise in range of [0, 1] name: A string, the name of the layer. NOTE: MAKE SURE INPUTS LAYERS HAS THE BATCH SIZE FIGURED BEFORE USING THIS LAYER. """""" def __init__(self, hsv_max_amp=(0, 0, 0), batch_size=None, name=None, **kwargs): super(RandomHSV, self).__init__(name=name, **kwargs) # self.rand_generator = tf.random.Generator.from_seed(seed=int(time.time())) self.hsv_max_amp = np.array(list(hsv_max_amp), dtype=\'float32\') def build(self, input_shape): # self.batch_size = input_shape[0] super(RandomHSV, self).build(input_shape) # Be sure to call this at the end def call(self, inputs, training=True, **kwargs): def hsv_noise(): hsv = tf.image.rgb_to_hsv(inputs) # the random noise is a random matrix in shape (batch_size, img_w, img_h, depth) # after creating the random matrix, multiply it (element wise) by the self.hsv_max_amp. # that gets multiplied by random enabler (np.random.randint(0, 2, 3) -> 3 items, 0 or 1) # then removing an offset. random_noise = (np.random.ranf(inputs.shape) * ( np.random.random(1) * self.hsv_max_amp * np.random.randint(0, 2, 3)) - self.hsv_max_amp / 2) * 2 # those lines will cut any number which goes above 1 or goes below 0 (round it to 1 or 0 respectively). hsv = tf.minimum(1., tf.maximum(0., tf.add(hsv, random_noise))) batch = tf.image.hsv_to_rgb(hsv) batch.set_shape(inputs.shape) return batch # applying hsv_noise if Training. if Testing then just passing batch forward unchanged return control_flow_util.smart_cond(pred=training, true_fn=hsv_noise, false_fn=lambda: inputs) def compute_output_shape(self, input_shape): return input_shape def get_config(self): config = { \'hsv_max_amp\': self.hsv_max_amp, \'batch_size\': self.batch_size } base_config = super(RandomHSV, self).get_config() return dict(list(base_config.items()) + list(config.items()))\n\nWhen the whole model is built (or trying to build it self), I get the error:\n\nTypeError: \'NoneType\' object cannot be interpreted as an integer\n\nor when I use the tf.random.Generator() class:\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor.\n\nthis is because I don\'t tell the model\'s Inputs layers the batch size before I run it. I tried to figure out how to work around it, but the only solution is to let the model know the batch size (Input(batch_size=BATCH_SIZE) layer is mandatory) on other layers (such as Dense, Conv2D and so on...) its not mandatory to name the batch_size. Is there any idea how could I do it on my layer too?\n\nThe text was updated successfully, but these errors were encountered:\n\nJJKK1313 added the type:bug\n\nkumariko self-assigned this\n\n@JJKK1313 Can you please fill the issue template to expedite the troubleshooting process. Thanks!\n\nkumariko added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\n@kumariko I did, its that post. There is a specific template I should use? @kumariko #\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\n@JJKK1313 In order to reproduce the issue reported here, could you please provide the complete code and the dataset , tensorflow version you are using. Thanks!\n\nkumariko added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nThe version if Tensorflow is 2.4.1 The dataset is LFW dataset. The missing part from the code:\n\ndef get_siamese_model(input_shape, conv2d_filts): # Define the tensors for the two input images # ================================= THE INNER MODEL ================================= augmentations = Sequential( [ tf.keras.layers.experimental.preprocessing.RandomContrast(factor=0.70), RandomBrightnessLayer(max_delta=0.1, name=\'RandomBrightness\'), RandomHSVLayer(hsv_max_amp=[0.05, 0.25, 0], name=\'RandomHSVPreprocessor\'), ], name=configurations.AUGMENTATIONS_LAYER_NAME ) # ================================= THE INNER MODEL ================================= # THE PROBLEM IS HERE, WHEN NOT SPECIFING \'batch_size\' TO INPUT LAYER. left_input = Input(input_shape, name=""Input1"") right_input = Input(input_shape, name=""Input2"") left_input_augmented = augmentations(left_input) right_input_augmented = augmentations(right_input) # Generate the encodings (feature vectors) for the two images body = build_body(input_shape=input_shape, conv2d_filts=conv2d_filts) encoded_l = body(left_input_augmented) encoded_r = body(right_input_augmented) distance = Lambda(lambda embeds: euclidean_distance(embeds), name=\'Distance\')([encoded_l, encoded_r]) # normed_layer = BatchNormalization()(distance) # making sure the distances wont be all over the place. distance = Dense(1, activation=\'sigmoid\', name=\'Prediction\')(distance) # Connect the inputs with the outputs siamese_net = Model(inputs=[left_input, right_input], outputs=distance) return siamese_net # DataFrameGeneratorClass is a custom pair image generator, since nither Keras or Tensorflow has one. train_gen, test_gen = DataFrameGeneratorClass.create_train_test_generators( csv_path=\'data.csv\', validation_split=0.1, shuffle=True, batch_size=32, rescale=1. / 255., img_size=(128, 128), ) siamese_model = get_siamese_model(IMG_SIZE, conv2d_filts=CONV2D_FILTERS) siamese_model.summary() # siamese_model.load_weights(\'check_points/29-07-21_034351/\') optimizer = Adam() siamese_model.compile(loss=\'binary_crossentropy\', optimizer=optimizer, metrics=[\'accuracy\', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]) history = siamese_model.fit(train_gen, epochs=10, validation_data=test_gen)\n\nIf you want the code for the generators:\n\nclass PairDataGenerator(tf.keras.utils.Sequence): """""" NOTE: ON model.fit(SHUFFLE=FALSE) -> MUST BE FALSE! """""" def __init__(self, df_similar: pd.DataFrame, df_dissimilar: pd.DataFrame, batch_size=256, shuffle=True, rescale: {float, None} = 1. / 255., target_img_size=(128, 128), preprocess_function=None, rand_preproc_single: {ImageDataGenerator, dict} = None, rand_preproc_batch: list = None): # self.batch_counter = 0 self.last_batch_index = 0 self.df_similar = df_similar.sample(frac=1).reset_index(drop=True) self.df_dissimilar = df_dissimilar.sample(frac=1).reset_index(drop=True) self.preprocess_function = preprocess_function self.rescale = rescale self.target_img_size = target_img_size # rounding up batch size to be an even number. self.batch_size = batch_size + (batch_size % 2 == 1) self.shuffle = shuffle # indexes of rows. every batch we draw 2 samples. 1 similar and 1 dissimilar assert batch_size <= len(self.df_similar) + len( self.df_dissimilar), f""Cannot create batch of size {batch_size} when there "" \\ f""are only {len(self.df_similar)} samples"" self.indexes_similar = np.arange(len(self.df_similar)) self.similar_max_idx = len(self.indexes_similar) // self.batch_size self.indexes_dissimilar = np.arange(len(self.df_dissimilar)) self.dissimilar_max_idx = len(self.indexes_dissimilar) // self.batch_size if self.shuffle: np.random.shuffle(self.indexes_dissimilar) np.random.shuffle(self.indexes_similar) self.rand_preproc_single = rand_preproc_single self.rand_preproc_batch = rand_preproc_batch def __len__(self): """"""Denotes the number of batches per epoch"""""" return (len(self.df_similar) + len(self.df_dissimilar)) // self.batch_size def __getitem__(self, index): """"""Generate one batch of data"""""" # Generate indexes of the batch batch_idx_sim = index % self.similar_max_idx indexes_sim = self.indexes_similar[batch_idx_sim * (self.batch_size // 2): (batch_idx_sim + 1) * (self.batch_size // 2)] batch_idx_dissim = index % self.dissimilar_max_idx indexes_dissim = self.indexes_dissimilar[batch_idx_dissim * (self.batch_size // 2): (batch_idx_dissim + 1) * (self.batch_size // 2)] self.last_batch_index = index img1 = [] img2 = [] labels = [0, 1] * (self.batch_size // 2) # creating labels list np.random.shuffle(labels) same_counter = 0 diff_counter = 0 for idx, label in enumerate(labels): if label == configurations.LABELS[\'same\']: img1_path, img2_path, _ = self.df_similar.iloc[indexes_sim[same_counter]] same_counter += 1 else: img1_path, img2_path, _ = self.df_dissimilar.iloc[indexes_dissim[diff_counter]] diff_counter += 1 img1.append(self.load_image(img1_path)) img2.append(self.load_image(img2_path)) img1 = np.array(img1, dtype=\'float32\') img2 = np.array(img2, dtype=\'float32\') labels = np.array(labels, dtype=\'float32\') if self.rand_preproc_batch is not None: for func in self.rand_preproc_batch: img1 = func(img1) img2 = func(img2) return [img1, img2], labels def on_epoch_end(self): """"""Updates indexes after each epoch"""""" if (self.last_batch_index + 1) % self.dissimilar_max_idx == 0 and self.shuffle: self.indexes_dissimilar = np.arange(len(self.df_dissimilar)) np.random.shuffle(self.indexes_dissimilar) if (self.last_batch_index + 1) % self.similar_max_idx == 0 and self.shuffle: self.indexes_similar = np.arange(len(self.df_similar)) np.random.shuffle(self.indexes_similar) def load_image(self, path): """""" loads an image using tensorflow tools :param path: absolute path (refers to the project\'s folder) to the image :return: an image array. """""" if self.rand_preproc_single is not None: if isinstance(self.rand_preproc_single, ImageDataGenerator): img_arr = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB) img_arr = self.rand_preproc_single.random_transform(img_arr) img_arr = cv2.resize(img_arr, self.target_img_size) else: img_arr = my_utils.image_augmentations(path, **self.rand_preproc_single) else: img_arr = cv2.imread(path) img_arr = cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB) img_arr = cv2.resize(img_arr, self.target_img_size) if self.preprocess_function is not None: img_arr = self.preprocess_function(img_arr) elif self.rescale is not None: img_arr = img_arr * self.rescale return img_arr\n\nand the function you saw on main:\n\ndef create_train_test_generators(csv_path: str, pair_gen: bool = True, validation_split: float = 0.1, shuffle: bool = True, batch_size: int = 256, rescale: {float, None} = 1. / 255., img_size: tuple = (128, 128), preprocess_func=None, rand_preproc_single: {ImageDataGenerator, dict} = None, rand_preproc_batch: list = None, ): """""" Initialization :param rand_preproc_batch: list of functions which augments a whole batch. :param rand_preproc_single: an ImageDataGenerator Instance or dictionary for my_utils.image_augmentation function. :param pair_gen: boolean. True = Pair Gen. False = Triplets Gen. :param rescale: rescaling factor :param preprocess_func: a preprocessing function for the network\'s inputs. :param img_size: the size of the output image. :param batch_size: batch size :param shuffle: whether to shuffle the data before casting to Train Test. :param csv_path: the path to the csv file which we create DataFrame object from. :param validation_split: how much from the whole data goes to validation. """""" from sklearn.model_selection import train_test_split # read all the csv file df = pd.read_csv(csv_path, index_col=False) params = dict(batch_size=batch_size, shuffle=shuffle, rescale=rescale, target_img_size=img_size, preprocess_function=preprocess_func, rand_preproc_single=rand_preproc_single, rand_preproc_batch=rand_preproc_batch) if pair_gen: # split the rows to similar and dissimilar by label column df_similar = df.where(df[\'labels\'] == 1.0).dropna().reset_index(drop=True) df_dissimilar = df.where(df[\'labels\'] == 0.0).dropna().reset_index(drop=True) print(len(df_dissimilar), len(df_similar)) print(f""Found {len(df)} pairs."") # split similar and dissimilar to train and test (4 groups) df_train_similar, df_test_similar = train_test_split(df_similar, test_size=validation_split, shuffle=shuffle) df_train_dissimilar, df_test_dissimilar = train_test_split(df_dissimilar, test_size=validation_split, shuffle=shuffle) # drop the index column, no need of that. df_train_similar = df_train_similar.reset_index(drop=True) df_test_similar = df_test_similar.reset_index(drop=True) df_train_dissimilar = df_train_dissimilar.reset_index(drop=True) df_test_similar = df_test_similar.reset_index(drop=True) # print(len(pd.merge(df_train_similar, df_train_dissimilar, how=\'inner\', on=[\'img1_p\', \'img2_p\', \'labels\']))) print(f""Total={len(df)}"", f""Train={len(df_train_similar)} + {len(df_train_dissimilar)}"", f""Test={len(df_test_similar)} + {len(df_test_dissimilar)}"", sep=\'\\n\') return PairDataGenerator(df_similar=df_train_similar, df_dissimilar=df_train_dissimilar, **params), \\ PairDataGenerator(df_similar=df_test_similar, df_dissimilar=df_test_dissimilar, **params) else: print(f""Found {len(df)} triplets."") # split similar and dissimilar to train and test (4 groups) df_train, df_test = train_test_split(df, test_size=validation_split, shuffle=shuffle) # drop the index column, no need of that. df_train = df_train.reset_index(drop=True) df_test = df_test.reset_index(drop=True) # print(len(pd.merge(df_train_similar, df_train_dissimilar, how=\'inner\', on=[\'img1_p\', \'img2_p\', \'labels\']))) print(f""Total={len(df)}"", f""Train={len(df_train)}"", f""Test={len(df_test)}"", sep=\'\\n\') return TripletDataGenerator(df=df_train, **params), \\ TripletDataGenerator(df=df_test, **params)\n\nNotice that the else part isn\'t relevant here so I didn\'t add it. If you still want it, let me know.\n\nkumariko added TF 2.4\n\nfor issues related to TF 2.4 comp:keras\n\nKeras related issues and removed stat:awaiting response\n\nStatus - Awaiting response from author labels\n\n@JJKK1313 Thank you for the response ! We see that this issue is more related to Keras .Please post this issue on keras-team/keras repo. To know more see; https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999 .Thanks!\n\nkumariko added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ngoogle-ml-butler bot commented\n\nThis issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n\ngoogle-ml-butler bot added the stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity label\n\ngoogle-ml-butler bot commented\n\nClosing as stale. Please reopen if you\'d like to work on this further.\n\ngoogle-ml-butler bot closed this as completed\n\ngoogle-ml-butler bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues stale\n\nThis label marks the issue/pr stale - to be closed automatically if no activity stat:awaiting response\n\nStatus - Awaiting response from author TF 2.4\n\nfor issues related to TF 2.4 type:bug\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-17T05:55:10', 'title': ""ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor. · Issue #51376 · tensorflow/tensorflow"", 'url': 'https://github.com/tensorflow/tensorflow/issues/51376'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nAttempt to convert a value (1.0) with an unsupported type (<class \'numpy.float32\'>) to a Tensor. #30120\n\nguyko81 opened this issue\n\nJun 25, 2019 · 6 comments\n\nAttempt to convert a value (1.0) with an unsupported type (<class \'numpy.float32\'>) to a Tensor. #30120\n\nguyko81 opened this issue\n\nJun 25, 2019 · 6 comments\n\nOPs related issues stat:awaiting response\n\nStatus - Awaiting response from author TF 2.0\n\nIssues relating to TensorFlow 2.0 type:support\n\nPlease make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No, I have used the example script from https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n\nTensorFlow installed from (source or binary): pip install (binary)\n\nTensorFlow version (use command below): 2.0.0-beta1\n\nPython version: 3.6.6\n\nBazel version (if compiling from source):\n\nGCC/Compiler version (if compiling from source):\n\nCUDA/cuDNN version: 10.0\n\nGPU model and memory: GeForce GTX 1050, 4GB\n\nYou can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" 2. TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""\n\nDescribe the current behavior I was trying to convert a numpy array to a tensorflow tensor as in the example code. Describe the expected behavior Should give back tensor (as it works properly in earlier versions) Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. `import numpy as np\n\ndef my_func(arg): arg = tf.convert_to_tensor(arg, dtype=tf.float32) return tf.matmul(arg, arg) + arg\n\n#The following calls are equivalent. `import numpy as np\n\ndef my_func(arg): arg = tf.convert_to_tensor(arg, dtype=tf.float32) return tf.matmul(arg, arg) + arg\n\n#The following calls are equivalent. value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]])) value_2 = my_func([[1.0, 2.0], [3.0, 4.0]]) value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)) `\n\nOther info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. `--------------------------------------------------------------------------- ValueError Traceback (most recent call last) in 9 value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]])) 10 value_2 = my_func([[1.0, 2.0], [3.0, 4.0]]) ---> 11 value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\n\nin my_func(arg) 3 4 def my_func(arg): ----> 5 arg = tf.convert_to_tensor(arg, dtype=tf.float32) 6 return tf.matmul(arg, arg) + arg 7\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name) 1156 name=name, 1157 preferred_dtype=dtype_hint, -> 1158 as_ref=False) 1159 1160\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors) 1235 1236 if ret is None: -> 1237 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) 1238 1239 if ret is NotImplemented:\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref) 303 as_ref=False): 304 _ = as_ref --> 305 return constant(v, dtype=dtype, name=name) 306 307\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name) 244 """""" 245 return _constant_impl(value, dtype, shape, name, verify_shape=False, --> 246 allow_broadcast=True) 247 248\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast) 252 ctx = context.context() 253 if ctx.executing_eagerly(): --> 254 t = convert_to_eager_tensor(value, ctx, dtype) 255 if shape is None: 256 return t\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in convert_to_eager_tensor(value, ctx, dtype) 113 return t 114 else: --> 115 return ops.EagerTensor(value, handle, device, dtype) 116 117\n\nValueError: Attempt to convert a value (1.0) with an unsupported type (<class \'numpy.float32\'>) to a Tensor. `\n\nThe text was updated successfully, but these errors were encountered:\n\nachandraa self-assigned this\n\nachandraa added 2.0.0-beta0 comp:ops\n\nOPs related issues labels\n\nI have tried the code snippet in Colab with TF GPU version 2.0beta1 and it executed without error. Please help us to reproduce the issue. Thanks!\n\nachandraa added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nThanks for the reply, I have re-installed the tensorflow 2.0beta1, and now the problem is not present anymore. Don\'t know the reasons why, sorry.\n\nguyko81 closed this as completed\n\nGreat to know the issue is resolved. Thanks!\n\nachandraa added the type:support\n\nSupport issues label\n\nmichalisfrangos commented\n\nI also came across this error.\n\nI believe there is an issue with compatibility with latest version of numpy (1.17.3)\n\nSOLUTION : install previous version of numpy 1.16.4\n\ntensorlfow version : 2.1.0-dev20191023\n\nnumpy: 1.17.3 (install the previous version 1.16.4)\n\nmichalisfrangos mentioned this issue\n\ntensorflow + numpy compatibility? #31249\n\nInstalling numpy 1.16.4 worked for me. Thanks!\n\nedwardpassagi commented\n\nreverting to numpy 1.16.4 fixes the problem for me, thanks.\n\nlvenugopalan added the TF 2.0\n\nIssues relating to TensorFlow 2.0 label\n\ntirthasheshpatel mentioned this issue\n\nENH: add MarginalGP model and fixes in covariance functions API pymc-devs/pymc4#309\n\nSaduf2019 mentioned this issue\n\nBetter Instruction/Insight as to how models.load_model() custom objects are loaded (doc/feature request/bug?) #42830\n\nSaduf2019 mentioned this issue\n\nTensorflow 2+ framework class exception #44641\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nOPs related issues stat:awaiting response\n\nStatus - Awaiting response from author TF 2.0\n\nIssues relating to TensorFlow 2.0 type:support\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_1', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nAttempt to convert a value (1.0) with an unsupported type (<class \'numpy.float32\'>) to a Tensor. #30120\n\nguyko81 opened this issue\n\nJun 25, 2019 · 6 comments\n\nAttempt to convert a value (1.0) with an unsupported type (<class \'numpy.float32\'>) to a Tensor. #30120\n\nguyko81 opened this issue\n\nJun 25, 2019 · 6 comments\n\nOPs related issues stat:awaiting response\n\nStatus - Awaiting response from author TF 2.0\n\nIssues relating to TensorFlow 2.0 type:support\n\nPlease make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No, I have used the example script from https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n\nTensorFlow installed from (source or binary): pip install (binary)\n\nTensorFlow version (use command below): 2.0.0-beta1\n\nPython version: 3.6.6\n\nBazel version (if compiling from source):\n\nGCC/Compiler version (if compiling from source):\n\nCUDA/cuDNN version: 10.0\n\nGPU model and memory: GeForce GTX 1050, 4GB\n\nYou can collect some of this information using our environment capture script You can also obtain the TensorFlow version with: 1. TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" 2. TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""\n\nDescribe the current behavior I was trying to convert a numpy array to a tensorflow tensor as in the example code. Describe the expected behavior Should give back tensor (as it works properly in earlier versions) Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. `import numpy as np\n\ndef my_func(arg): arg = tf.convert_to_tensor(arg, dtype=tf.float32) return tf.matmul(arg, arg) + arg\n\n#The following calls are equivalent. `import numpy as np\n\ndef my_func(arg): arg = tf.convert_to_tensor(arg, dtype=tf.float32) return tf.matmul(arg, arg) + arg\n\n#The following calls are equivalent. value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]])) value_2 = my_func([[1.0, 2.0], [3.0, 4.0]]) value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)) `\n\nOther info / logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. `--------------------------------------------------------------------------- ValueError Traceback (most recent call last) in 9 value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]])) 10 value_2 = my_func([[1.0, 2.0], [3.0, 4.0]]) ---> 11 value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\n\nin my_func(arg) 3 4 def my_func(arg): ----> 5 arg = tf.convert_to_tensor(arg, dtype=tf.float32) 6 return tf.matmul(arg, arg) + arg 7\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name) 1156 name=name, 1157 preferred_dtype=dtype_hint, -> 1158 as_ref=False) 1159 1160\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors) 1235 1236 if ret is None: -> 1237 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) 1238 1239 if ret is NotImplemented:\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref) 303 as_ref=False): 304 _ = as_ref --> 305 return constant(v, dtype=dtype, name=name) 306 307\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name) 244 """""" 245 return _constant_impl(value, dtype, shape, name, verify_shape=False, --> 246 allow_broadcast=True) 247 248\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast) 252 ctx = context.context() 253 if ctx.executing_eagerly(): --> 254 t = convert_to_eager_tensor(value, ctx, dtype) 255 if shape is None: 256 return t\n\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in convert_to_eager_tensor(value, ctx, dtype) 113 return t 114 else: --> 115 return ops.EagerTensor(value, handle, device, dtype) 116 117\n\nValueError: Attempt to convert a value (1.0) with an unsupported type (<class \'numpy.float32\'>) to a Tensor. `\n\nThe text was updated successfully, but these errors were encountered:\n\nachandraa self-assigned this\n\nachandraa added 2.0.0-beta0 comp:ops\n\nOPs related issues labels\n\nI have tried the code snippet in Colab with TF GPU version 2.0beta1 and it executed without error. Please help us to reproduce the issue. Thanks!\n\nachandraa added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nThanks for the reply, I have re-installed the tensorflow 2.0beta1, and now the problem is not present anymore. Don\'t know the reasons why, sorry.\n\nguyko81 closed this as completed\n\nGreat to know the issue is resolved. Thanks!\n\nachandraa added the type:support\n\nSupport issues label\n\nmichalisfrangos commented\n\nI also came across this error.\n\nI believe there is an issue with compatibility with latest version of numpy (1.17.3)\n\nSOLUTION : install previous version of numpy 1.16.4\n\ntensorlfow version : 2.1.0-dev20191023\n\nnumpy: 1.17.3 (install the previous version 1.16.4)\n\nmichalisfrangos mentioned this issue\n\ntensorflow + numpy compatibility? #31249\n\nInstalling numpy 1.16.4 worked for me. Thanks!\n\nedwardpassagi commented\n\nreverting to numpy 1.16.4 fixes the problem for me, thanks.\n\nlvenugopalan added the TF 2.0\n\nIssues relating to TensorFlow 2.0 label\n\ntirthasheshpatel mentioned this issue\n\nENH: add MarginalGP model and fixes in covariance functions API pymc-devs/pymc4#309\n\nSaduf2019 mentioned this issue\n\nBetter Instruction/Insight as to how models.load_model() custom objects are loaded (doc/feature request/bug?) #42830\n\nSaduf2019 mentioned this issue\n\nTensorflow 2+ framework class exception #44641\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nOPs related issues stat:awaiting response\n\nStatus - Awaiting response from author TF 2.0\n\nIssues relating to TensorFlow 2.0 type:support\n\nYou can’t perform that action at this time.', 'timestamp': '2024-05-08T13:08:57', 'title': ""Attempt to convert a value (1.0) with an unsupported type (<class 'numpy.float32'>) to a Tensor. · Issue #30120 · tensorflow/tensorflow"", 'url': 'https://github.com/tensorflow/tensorflow/issues/30120'}), Document(page_content='HasnainRaz / SemSegPipeline Public\n\nINVALID_ARGUMENT: ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor #13\n\npanovr opened this issue\n\nApr 24, 2022 · 1 comment\n\nINVALID_ARGUMENT: ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor #13\n\npanovr opened this issue\n\nApr 24, 2022 · 1 comment\n\ntrain_image_paths = [os.path.join(train_img_dir, x) for x in os.listdir(train_img_dir) if x.endswith(\'.png\')] train_mask_paths = [os.path.join(train_mask_dir, x) for x in os.listdir(train_mask_dir) if x.endswith(\'.png\')] print(train_image_paths[0:3]) print(train_mask_paths[0:3])\n\n[\'dataset/train_dir/uav_aerials/97.png\', \'dataset/train_dir/uav_aerials/76.png\', \'dataset/train_dir/uav_aerials/136.png\'] [\'dataset/train_dir/uav_masks/97.png\', \'dataset/train_dir/uav_masks/76.png\', \'dataset/train_dir/uav_masks/136.png\']\n\nSo I think the image and mask paths are correct.\n\nMy mask image is in RGB channel format, and the foreground color is (128,0,0)\n\ncolor_list = [[0, 0, 0], [128, 0, 0]] train_dl = DataLoader(image_paths=train_image_paths, mask_paths=train_mask_paths, image_size=(256,256), channels=(3,3), augment=True, one_hot_encoding=True, palette=color_list) train_ds = train_dl.data_batch(batch_size=BATCH_SIZE,shuffle=True) for image, mask in train_ds.take(1): print(image,mask)\n\n2022-04-24 09:40:55.352224: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. Traceback (most recent call last): File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 269, in __call__ return func(device, token, args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 147, in __call__ outputs = self._call(device, args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 154, in _call ret = self._func(*args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 642, in wrapper return func(*args, **kwargs) File ""/tmp/__autograph_generated_filenbagilbr.py"", line 57, in _augmentation_func ag__.if_stmt(ag__.ld(self).augment, if_body_1, else_body_1, get_state_1, set_state_1, (\'image_f\', \'mask_f\'), 2) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1321, in if_stmt _py_if_stmt(cond, body, orelse) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1374, in _py_if_stmt return body() if cond else orelse() File ""/tmp/__autograph_generated_filenbagilbr.py"", line 50, in if_body_1 ag__.if_stmt(ag__.ld(self).compose, if_body, else_body, get_state, set_state, (\'image_f\', \'mask_f\'), 2) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1321, in if_stmt _py_if_stmt(cond, body, orelse) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1374, in _py_if_stmt return body() if cond else orelse() File ""/tmp/__autograph_generated_filenbagilbr.py"", line 47, in else_body (image_f, mask_f) = ag__.converted_call(ag__.ld(augment_func), (ag__.ld(image_f), ag__.ld(mask_f)), None, fscope_1) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 335, in converted_call return _call_unconverted(f, args, kwargs, options, False) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 459, in _call_unconverted return f(*args) File ""/home/ylzhao/project/potsdam_seg/dataloader.py"", line 95, in _crop_random h = tf.cast(shape[0] * self.crop_percent, tf.int32) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler raise e.with_traceback(filtered_tb) from None File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in convert_to_eager_tensor return ops.EagerTensor(value, ctx.device_name, dtype) ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. 2022-04-24 09:40:55.369289: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Tensor conversion requested dtype uint8 for Tensor with dtype float32: <tf.Tensor: shape=(256, 256, 2), dtype=float32, numpy= array([[[1., 0.], [1., 0.], [1., 0.], ..., [1., 0.], [1., 0.], [1., 0.]], [[1., 0.], [1., 0.], [1., 0.], ..., [1., 0.], [1., 0.], [1., 0.]], [[1., 0.], [1., 0.], [1., 0.], ..., [1., 0.], [1., 0.], [1., 0.]], ..., [[1., 0.], [1., 0.], [1., 0.], ..., [0., 1.], [0., 1.], [0., 1.]], [[1., 0.], [1., 0.], [1., 0.], ..., [0., 1.], [0., 1.], [0., 1.]], [[1., 0.], [1., 0.], [1., 0.], ..., [0., 1.], [0., 1.], [0., 1.]]], dtype=float32)> Traceback (most recent call last): File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 269, in __call__ return func(device, token, args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 147, in __call__ outputs = self._call(device, args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 163, in _call outputs = [ File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 164, in <listcomp> _maybe_copy_to_context_device(self._convert(x, dtype=dtype), File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 131, in _convert return ops.convert_to_tensor(value, dtype=dtype) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped return func(*args, **kwargs) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 1662, in convert_to_tensor raise ValueError(\n\nThe text was updated successfully, but these errors were encountered:\n\nHasnainRaz commented\n\nIt\'s a bug when crop_percent argument is not specified, I will push a fix for it.\n\nIn the meantime you can try with:\n\ntrain_dl = DataLoader(image_paths=train_image_paths, mask_paths=train_mask_paths, image_size=(256,256), channels=(3,3), augment=True, one_hot_encoding=True, crop_percent=0.9, palette=color_list)\n\nHasnainRaz closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session.', metadata={'id': 'web-search_2', 'snippet': 'HasnainRaz / SemSegPipeline Public\n\nINVALID_ARGUMENT: ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor #13\n\npanovr opened this issue\n\nApr 24, 2022 · 1 comment\n\nINVALID_ARGUMENT: ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor #13\n\npanovr opened this issue\n\nApr 24, 2022 · 1 comment\n\ntrain_image_paths = [os.path.join(train_img_dir, x) for x in os.listdir(train_img_dir) if x.endswith(\'.png\')] train_mask_paths = [os.path.join(train_mask_dir, x) for x in os.listdir(train_mask_dir) if x.endswith(\'.png\')] print(train_image_paths[0:3]) print(train_mask_paths[0:3])\n\n[\'dataset/train_dir/uav_aerials/97.png\', \'dataset/train_dir/uav_aerials/76.png\', \'dataset/train_dir/uav_aerials/136.png\'] [\'dataset/train_dir/uav_masks/97.png\', \'dataset/train_dir/uav_masks/76.png\', \'dataset/train_dir/uav_masks/136.png\']\n\nSo I think the image and mask paths are correct.\n\nMy mask image is in RGB channel format, and the foreground color is (128,0,0)\n\ncolor_list = [[0, 0, 0], [128, 0, 0]] train_dl = DataLoader(image_paths=train_image_paths, mask_paths=train_mask_paths, image_size=(256,256), channels=(3,3), augment=True, one_hot_encoding=True, palette=color_list) train_ds = train_dl.data_batch(batch_size=BATCH_SIZE,shuffle=True) for image, mask in train_ds.take(1): print(image,mask)\n\n2022-04-24 09:40:55.352224: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. Traceback (most recent call last): File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 269, in __call__ return func(device, token, args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 147, in __call__ outputs = self._call(device, args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 154, in _call ret = self._func(*args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 642, in wrapper return func(*args, **kwargs) File ""/tmp/__autograph_generated_filenbagilbr.py"", line 57, in _augmentation_func ag__.if_stmt(ag__.ld(self).augment, if_body_1, else_body_1, get_state_1, set_state_1, (\'image_f\', \'mask_f\'), 2) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1321, in if_stmt _py_if_stmt(cond, body, orelse) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1374, in _py_if_stmt return body() if cond else orelse() File ""/tmp/__autograph_generated_filenbagilbr.py"", line 50, in if_body_1 ag__.if_stmt(ag__.ld(self).compose, if_body, else_body, get_state, set_state, (\'image_f\', \'mask_f\'), 2) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1321, in if_stmt _py_if_stmt(cond, body, orelse) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1374, in _py_if_stmt return body() if cond else orelse() File ""/tmp/__autograph_generated_filenbagilbr.py"", line 47, in else_body (image_f, mask_f) = ag__.converted_call(ag__.ld(augment_func), (ag__.ld(image_f), ag__.ld(mask_f)), None, fscope_1) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 335, in converted_call return _call_unconverted(f, args, kwargs, options, False) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 459, in _call_unconverted return f(*args) File ""/home/ylzhao/project/potsdam_seg/dataloader.py"", line 95, in _crop_random h = tf.cast(shape[0] * self.crop_percent, tf.int32) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler raise e.with_traceback(filtered_tb) from None File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in convert_to_eager_tensor return ops.EagerTensor(value, ctx.device_name, dtype) ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. 2022-04-24 09:40:55.369289: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: ValueError: Tensor conversion requested dtype uint8 for Tensor with dtype float32: <tf.Tensor: shape=(256, 256, 2), dtype=float32, numpy= array([[[1., 0.], [1., 0.], [1., 0.], ..., [1., 0.], [1., 0.], [1., 0.]], [[1., 0.], [1., 0.], [1., 0.], ..., [1., 0.], [1., 0.], [1., 0.]], [[1., 0.], [1., 0.], [1., 0.], ..., [1., 0.], [1., 0.], [1., 0.]], ..., [[1., 0.], [1., 0.], [1., 0.], ..., [0., 1.], [0., 1.], [0., 1.]], [[1., 0.], [1., 0.], [1., 0.], ..., [0., 1.], [0., 1.], [0., 1.]], [[1., 0.], [1., 0.], [1., 0.], ..., [0., 1.], [0., 1.], [0., 1.]]], dtype=float32)> Traceback (most recent call last): File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 269, in __call__ return func(device, token, args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 147, in __call__ outputs = self._call(device, args) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 163, in _call outputs = [ File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 164, in <listcomp> _maybe_copy_to_context_device(self._convert(x, dtype=dtype), File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 131, in _convert return ops.convert_to_tensor(value, dtype=dtype) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped return func(*args, **kwargs) File ""/home/ylzhao/anaconda3/envs/airbus/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 1662, in convert_to_tensor raise ValueError(\n\nThe text was updated successfully, but these errors were encountered:\n\nHasnainRaz commented\n\nIt\'s a bug when crop_percent argument is not specified, I will push a fix for it.\n\nIn the meantime you can try with:\n\ntrain_dl = DataLoader(image_paths=train_image_paths, mask_paths=train_mask_paths, image_size=(256,256), channels=(3,3), augment=True, one_hot_encoding=True, crop_percent=0.9, palette=color_list)\n\nHasnainRaz closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session.', 'timestamp': '2023-02-17T19:53:33', 'title': ""INVALID_ARGUMENT: ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor · Issue #13 · ..."", 'url': 'https://github.com/HasnainRaz/SemSegPipeline/issues/13'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list). #13940\n\nbigboy32 opened this issue\n\nApr 6, 2020 · 8 comments\n\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list). #13940\n\nbigboy32 opened this issue\n\nApr 6, 2020 · 8 comments\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited.\n\nHello, I have problems using Tensorflow:\n\nNvidia RTX 2080 with 8 or 6 GB VRAM\n\nI was using a text file and got data out of it. The data was a text for a chatbot. I then created a Dictionary with all the words of the text and gave them numbers. Then i replaced the words with numbers.\n\nIt gave me an error saying : ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).\n\nOf course, i converted the Labels (X) and the features (Y) to np.arrays()\n\nfrom tensorflow.keras.layers import * from tensorflow.keras.models import Sequential\n\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\n\nLearnedWordsIndex = 0\n\ndef CreateLearnedWords(X, Y, outWordDict, outWordIndex):\n\nfor sent in X: for word in sent: outWordDict[word] = outWordIndex outWordIndex += 1 for sent in Y: for word in sent: outWordDict[word] = outWordIndex outWordIndex += 1\n\nX = pickle.load(open(""X.pkl"", ""rb"").read()) Y = pickle.load(open(""Y.pkl"", ""rb"").read()) Z = pickle.sayhello\n\nwith open(""Datasets/conversation1.txt"", \'r\') as file: for item in file: if item == """" or item == ""\\n"": pass elif "":"" not in item and rn == 1: Y.append(item) elif rn == 0: X.append(item.split(\': \')) rn = 1 else: Y.append(item.split("": "")) rn = 0 if rn == 1: rn = 0 with open(""Datasets/conversation2.txt"", \'r\') as file: for item in file: if item == """" or item == ""\\n"": pass elif "":"" not in item and rn == 1: Y.append(item) elif rn == 0: X.append(item.split(\': \')) rn = 1 else: Y.append(item.split("": "")) rn = 0 if rn == 1: rn = 0 with open(""Datasets/conversation3.txt"", \'r\') as file: for item in file: if item == """" or item == ""\\n"": pass elif "":"" not in item and rn == 1: Y.append(item) elif rn == 0: X.append(item.split(\': \')) rn = 1 else: Y.append(item.split("": "")) rn = 0 del(rn) TokenizedY = [] TokenizedX = [] for item in Y: tokY = word_tokenize(str(item)) words = [word for word in tokY if word.isalpha()] TokenizedY.append(words) for item in X: tokX = word_tokenize(str(item)) words = [word for word in tokX if word.isalpha()] TokenizedX.append(words) Y = TokenizedY X = TokenizedX CreateLearnedWords(X, Y, LearnedWords, LearnedWordsIndex) for sent in X: for word in sent: sent[sent.index(word)] = LearnedWords[word] for sent in Y: for word in sent: sent[sent.index(word)] = LearnedWords[word] print(X) pickle.dump(X, open(""X.pkl"", ""wb"")) pickle.dump(Y, open(""Y.pkl"", ""wb""))\n\ntb = tf.keras.callbacks.TensorBoard(log_dir=""logs/"")\n\nmodel = Sequential()\n\nmodel.add(Embedding(1000, 10))\n\nmodel.add(GlobalAveragePooling1D())\n\nmodel.add(Dense(16)) model.add(Activation(""relu""))\n\nmodel.add(Dense(1)) model.add(Activation(""sigmoid""))\n\nmodel.compile(loss=""categorical_hinge"", optimizer=""rmsprop"")\n\nmodel.fit(X,Y , epochs=1000)\n\ntb = tf.keras.callbacks.TensorBoard(logdir=""logs/"")`\n\nSorry if the code is long, i really don\'t know where the error is coming from.\n\nThanks for all answers!!\n\nThe text was updated successfully, but these errors were encountered:\n\ngoogle-ml-butler bot added the backend:tensorflow label\n\nsaikumarchalla added the type:support\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited. label\n\nI have the same problem\n\nRavitejaBadugu commented\n\nDid you got the solution?\n\ndinhngoc267 commented\n\nAbhijeet-Katiyar commented\n\ntry X=np.asarray(X).astype(np.float32) and Y=np.asarray(Y).astype(np.float32) before passing X and Y to model\n\nbigboy32 closed this as completed\n\nAkankhya123 commented\n\nX=np.asarray(X).astype(np.float32) and Y=np.asarray(Y).astype(np.float32)\n\nThank you so much this worked!\n\nI get this same error but I am using a TimeseriesGenerator so i am passing that object for training using the model.fit_generator() method. Any idea what it could be?\n\nubiratanfilho commented\n\nHaving the same error with a TimeSeriesGenerator. @Samir221 have you found a solution?\n\nX=np.asarray(X).astype(np.float32)\n\nthis doesn\'t work for mine x_train is text data whereas y train is its classification\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited.\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_3', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list). #13940\n\nbigboy32 opened this issue\n\nApr 6, 2020 · 8 comments\n\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list). #13940\n\nbigboy32 opened this issue\n\nApr 6, 2020 · 8 comments\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited.\n\nHello, I have problems using Tensorflow:\n\nNvidia RTX 2080 with 8 or 6 GB VRAM\n\nI was using a text file and got data out of it. The data was a text for a chatbot. I then created a Dictionary with all the words of the text and gave them numbers. Then i replaced the words with numbers.\n\nIt gave me an error saying : ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).\n\nOf course, i converted the Labels (X) and the features (Y) to np.arrays()\n\nfrom tensorflow.keras.layers import * from tensorflow.keras.models import Sequential\n\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\n\nLearnedWordsIndex = 0\n\ndef CreateLearnedWords(X, Y, outWordDict, outWordIndex):\n\nfor sent in X: for word in sent: outWordDict[word] = outWordIndex outWordIndex += 1 for sent in Y: for word in sent: outWordDict[word] = outWordIndex outWordIndex += 1\n\nX = pickle.load(open(""X.pkl"", ""rb"").read()) Y = pickle.load(open(""Y.pkl"", ""rb"").read()) Z = pickle.sayhello\n\nwith open(""Datasets/conversation1.txt"", \'r\') as file: for item in file: if item == """" or item == ""\\n"": pass elif "":"" not in item and rn == 1: Y.append(item) elif rn == 0: X.append(item.split(\': \')) rn = 1 else: Y.append(item.split("": "")) rn = 0 if rn == 1: rn = 0 with open(""Datasets/conversation2.txt"", \'r\') as file: for item in file: if item == """" or item == ""\\n"": pass elif "":"" not in item and rn == 1: Y.append(item) elif rn == 0: X.append(item.split(\': \')) rn = 1 else: Y.append(item.split("": "")) rn = 0 if rn == 1: rn = 0 with open(""Datasets/conversation3.txt"", \'r\') as file: for item in file: if item == """" or item == ""\\n"": pass elif "":"" not in item and rn == 1: Y.append(item) elif rn == 0: X.append(item.split(\': \')) rn = 1 else: Y.append(item.split("": "")) rn = 0 del(rn) TokenizedY = [] TokenizedX = [] for item in Y: tokY = word_tokenize(str(item)) words = [word for word in tokY if word.isalpha()] TokenizedY.append(words) for item in X: tokX = word_tokenize(str(item)) words = [word for word in tokX if word.isalpha()] TokenizedX.append(words) Y = TokenizedY X = TokenizedX CreateLearnedWords(X, Y, LearnedWords, LearnedWordsIndex) for sent in X: for word in sent: sent[sent.index(word)] = LearnedWords[word] for sent in Y: for word in sent: sent[sent.index(word)] = LearnedWords[word] print(X) pickle.dump(X, open(""X.pkl"", ""wb"")) pickle.dump(Y, open(""Y.pkl"", ""wb""))\n\ntb = tf.keras.callbacks.TensorBoard(log_dir=""logs/"")\n\nmodel = Sequential()\n\nmodel.add(Embedding(1000, 10))\n\nmodel.add(GlobalAveragePooling1D())\n\nmodel.add(Dense(16)) model.add(Activation(""relu""))\n\nmodel.add(Dense(1)) model.add(Activation(""sigmoid""))\n\nmodel.compile(loss=""categorical_hinge"", optimizer=""rmsprop"")\n\nmodel.fit(X,Y , epochs=1000)\n\ntb = tf.keras.callbacks.TensorBoard(logdir=""logs/"")`\n\nSorry if the code is long, i really don\'t know where the error is coming from.\n\nThanks for all answers!!\n\nThe text was updated successfully, but these errors were encountered:\n\ngoogle-ml-butler bot added the backend:tensorflow label\n\nsaikumarchalla added the type:support\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited. label\n\nI have the same problem\n\nRavitejaBadugu commented\n\nDid you got the solution?\n\ndinhngoc267 commented\n\nAbhijeet-Katiyar commented\n\ntry X=np.asarray(X).astype(np.float32) and Y=np.asarray(Y).astype(np.float32) before passing X and Y to model\n\nbigboy32 closed this as completed\n\nAkankhya123 commented\n\nX=np.asarray(X).astype(np.float32) and Y=np.asarray(Y).astype(np.float32)\n\nThank you so much this worked!\n\nI get this same error but I am using a TimeseriesGenerator so i am passing that object for training using the model.fit_generator() method. Any idea what it could be?\n\nubiratanfilho commented\n\nHaving the same error with a TimeSeriesGenerator. @Samir221 have you found a solution?\n\nX=np.asarray(X).astype(np.float32)\n\nthis doesn\'t work for mine x_train is text data whereas y train is its classification\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nUser is asking for help / asking an implementation question. Stackoverflow would be better suited.\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-27T07:02:25', 'title': 'ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list). · Issue #13940 · keras-team/keras', 'url': 'https://github.com/keras-team/keras/issues/13940'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ngooglecolab / colabtools Public\n\nTensorFlow ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list) #1515\n\nmitramir55 opened this issue\n\nAug 20, 2020 · 3 comments\n\nTensorFlow ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list) #1515\n\nmitramir55 opened this issue\n\nAug 20, 2020 · 3 comments\n\nmitramir55 commented\n\nHi, I\'m trying to write this code into colab. Interestingly, I was running the same code in colab a few days ago but now it doesn\'t work. the code also works in kaggle kernel. Why do you think this error occurs?\n\nI tried using np.array and tf.convert_to_tensor() for the model\'s input and also changed the version of TensorFlow and TensorFlow-hub but none worked. This is the colab notebook if you need more info. Thanks in advance!\n\nthe code for short is using this class:\n\nclass DisasterDetector: def __init__(self, tokenizer, bert_layer, max_len =30, lr = 0.0001, epochs = 15, batch_size = 32, dtype = tf.int32 , activation = \'sigmoid\', optimizer = \'SGD\', beta_1=0.9, beta_2=0.999, epsilon=1e-07, metrics = \'accuracy\', loss = \'binary_crossentropy\'): self.lr = lr self.epochs = epochs self.max_len = max_len self.batch_size = batch_size self.tokenizer = tokenizer self.bert_layer = bert_layer self.models = [] self.activation = activation self.optimizer = optimizer self.dtype = dtype self.beta_1 = beta_1 self.beta_2 = beta_2 self.epsilon =epsilon self.metrics = metrics self.loss = loss def encode(self, texts): all_tokens = [] masks = [] segments = [] for text in texts: tokenized = self.tokenizer.convert_tokens_to_ids([\'[CLS]\'] + self.tokenizer.tokenize(text) + [\'[SEP]\']) len_zeros = self.max_len - len(tokenized) padded = tokenized + [0] * len_zeros mask = [1] * len(tokenized) + [0] * len_zeros segment = [0] * self.max_len all_tokens.append(padded) masks.append(mask) segments.append(segment) print(len(all_tokens[0])) return np.array(all_tokens), np.array(masks), np.array(segments) def make_model(self): input_word_ids = Input(shape = (self.max_len, ), dtype=tf.int32, name = \'input_word_ids\') input_mask = Input(shape = (self.max_len, ), dtype=tf.int32, name = \'input_mask\') segment_ids = Input(shape = (self.max_len, ), dtype=tf.int32, name = \'segment_ids\') pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids]) clf_output = sequence_output[:, 0, :] out = tf.keras.layers.Dense(1, activation = self.activation)(clf_output) model = Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = out) if self.optimizer is \'SGD\': optimizer = SGD(learning_rate = self.lr) elif self.optimizer is \'Adam\': optimizer = Adam(learning_rate = self.lr, beta_1=self.beta_1, beta_2=self.beta_2, epsilon=self.epsilon) model.compile(loss = self.loss, optimizer = self.optimizer, metrics = [self.metrics]) return model def train(self, x, k = 3): kfold = StratifiedKFold(n_splits = k, shuffle = True) for fold, (train_idx, val_idx) in enumerate(kfold.split(x[\'cleaned_text\'], x[\'target\'])): print(\'fold: \', fold) x_trn = self.encode(x.loc[train_idx, \'cleaned_text\']) x_val = self.encode(x.loc[val_idx, \'cleaned_text\']) y_trn = np.array(x.loc[train_idx, \'target\'], dtype = np.uint8) y_val = np.array(x.loc[val_idx, \'target\'], dtype = np.uint8) model = self.make_model() print(\'model made.\') model.fit(x_trn, tf.convert_to_tensor(y_trn), validation_data = (x_val, tf.convert_to_tensor(y_val)), batch_size=self.batch_size, epochs = self.epochs) self.models.append(model)\n\nAnd then we define the object:\n\nclassifier = DisasterDetector(tokenizer = tokenizer, bert_layer = bert_layer, max_len = max_len, lr = 0.0001, epochs = 10, activation = \'sigmoid\', batch_size = 32,optimizer = \'SGD\', beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n\nThen I want to train it: classifier.train(train_cleaned)\n\nValueError Traceback (most recent call last) <ipython-input-9-106c756f2e47> in <module>() ----> 1 classifier.train(train_cleaned) <ipython-input-7-2c77803eea5f> in train(self, x, k) 109 model.fit(x_trn, y_trn, 110 validation_data = (x_val, y_val), --> 111 batch_size=self.batch_size, epochs = self.epochs) 112 113 self.models.append(model) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs) 106 def _method_wrapper(self, *args, **kwargs): 107 if not self._in_multi_worker_mode(): # pylint: disable=protected-access --> 108 return method(self, *args, **kwargs) 109 110 # Running inside `run_distribute_coordinator` already. /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 1061 use_multiprocessing=use_multiprocessing, 1062 model=self, -> 1063 steps_per_execution=self._steps_per_execution) 1064 1065 # Container that configures and calls `tf.keras.Callback`s. /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution) 1115 use_multiprocessing=use_multiprocessing, 1116 distribution_strategy=ds_context.get_strategy(), -> 1117 model=model) 1118 1119 strategy = ds_context.get_strategy() /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs) 263 **kwargs): 264 super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs) --> 265 x, y, sample_weights = _process_tensorlike((x, y, sample_weights)) 266 sample_weight_modes = broadcast_sample_weight_modes( 267 sample_weights, sample_weight_modes) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _process_tensorlike(inputs) 1019 return x 1020 -> 1021 inputs = nest.map_structure(_convert_numpy_and_scipy, inputs) 1022 return nest.list_to_tuple(inputs) 1023 /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs) 633 634 return pack_sequence_as( --> 635 structure[0], [func(*x) for x in entries], 636 expand_composites=expand_composites) 637 /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0) 633 634 return pack_sequence_as( --> 635 structure[0], [func(*x) for x in entries], 636 expand_composites=expand_composites) 637 /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _convert_numpy_and_scipy(x) 1014 if issubclass(x.dtype.type, np.floating): 1015 dtype = backend.floatx() -> 1016 return ops.convert_to_tensor(x, dtype=dtype) 1017 elif scipy_sparse and scipy_sparse.issparse(x): 1018 return _scipy_sparse_to_sparse_tensor(x) /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types) 1497 1498 if ret is None: -> 1499 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) 1500 1501 if ret is NotImplemented: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***) 50 def _default_conversion_function(value, dtype, name, as_ref): 51 del as_ref # Unused. ---> 52 return constant_op.constant(value, dtype, name=name) 53 54 /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name) 262 """""" 263 return _constant_impl(value, dtype, shape, name, verify_shape=False, --> 264 allow_broadcast=True) 265 266 /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast) 273 with trace.Trace(""tf.constant""): 274 return _constant_eager_impl(ctx, value, dtype, shape, verify_shape) --> 275 return _constant_eager_impl(ctx, value, dtype, shape, verify_shape) 276 277 g = ops.get_default_graph() /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape) 298 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape): 299 """"""Implementation of eager constant."""""" --> 300 t = convert_to_eager_tensor(value, ctx, dtype) 301 if shape is None: 302 return t /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype) 96 dtype = dtypes.as_dtype(dtype).as_datatype_enum 97 ctx.ensure_initialized() ---> 98 return ops.EagerTensor(value, ctx.device_name, dtype) 99 100 ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).\n\nThe text was updated successfully, but these errors were encountered:\n\ncraigcitro commented\n\nI suspect the issue is to do with the TensorFlow version -- colab is on 2.3, and IIRC Kaggle is on 2.2.\n\nYou can try installing a different version (eg !pip install tensorflow==2.2) and restarting your kernel; if you can confirm it\'s to do with the TF version, I\'d try asking on the TF issue tracker or Stack Overflow.\n\ncraigcitro closed this as completed\n\nmitramir55 commented\n\n@craigcitro Thank you very much for answering. It turns out that I\'ve been giving a small max_len for the bert layer inputs and because of that the sentences couldn\'t completely fit into the input vectors. The error was misleading.\n\ncraigcitro commented\n\nAwesome, glad you got unblocked. 👍\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_4', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ngooglecolab / colabtools Public\n\nTensorFlow ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list) #1515\n\nmitramir55 opened this issue\n\nAug 20, 2020 · 3 comments\n\nTensorFlow ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list) #1515\n\nmitramir55 opened this issue\n\nAug 20, 2020 · 3 comments\n\nmitramir55 commented\n\nHi, I\'m trying to write this code into colab. Interestingly, I was running the same code in colab a few days ago but now it doesn\'t work. the code also works in kaggle kernel. Why do you think this error occurs?\n\nI tried using np.array and tf.convert_to_tensor() for the model\'s input and also changed the version of TensorFlow and TensorFlow-hub but none worked. This is the colab notebook if you need more info. Thanks in advance!\n\nthe code for short is using this class:\n\nclass DisasterDetector: def __init__(self, tokenizer, bert_layer, max_len =30, lr = 0.0001, epochs = 15, batch_size = 32, dtype = tf.int32 , activation = \'sigmoid\', optimizer = \'SGD\', beta_1=0.9, beta_2=0.999, epsilon=1e-07, metrics = \'accuracy\', loss = \'binary_crossentropy\'): self.lr = lr self.epochs = epochs self.max_len = max_len self.batch_size = batch_size self.tokenizer = tokenizer self.bert_layer = bert_layer self.models = [] self.activation = activation self.optimizer = optimizer self.dtype = dtype self.beta_1 = beta_1 self.beta_2 = beta_2 self.epsilon =epsilon self.metrics = metrics self.loss = loss def encode(self, texts): all_tokens = [] masks = [] segments = [] for text in texts: tokenized = self.tokenizer.convert_tokens_to_ids([\'[CLS]\'] + self.tokenizer.tokenize(text) + [\'[SEP]\']) len_zeros = self.max_len - len(tokenized) padded = tokenized + [0] * len_zeros mask = [1] * len(tokenized) + [0] * len_zeros segment = [0] * self.max_len all_tokens.append(padded) masks.append(mask) segments.append(segment) print(len(all_tokens[0])) return np.array(all_tokens), np.array(masks), np.array(segments) def make_model(self): input_word_ids = Input(shape = (self.max_len, ), dtype=tf.int32, name = \'input_word_ids\') input_mask = Input(shape = (self.max_len, ), dtype=tf.int32, name = \'input_mask\') segment_ids = Input(shape = (self.max_len, ), dtype=tf.int32, name = \'segment_ids\') pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids]) clf_output = sequence_output[:, 0, :] out = tf.keras.layers.Dense(1, activation = self.activation)(clf_output) model = Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = out) if self.optimizer is \'SGD\': optimizer = SGD(learning_rate = self.lr) elif self.optimizer is \'Adam\': optimizer = Adam(learning_rate = self.lr, beta_1=self.beta_1, beta_2=self.beta_2, epsilon=self.epsilon) model.compile(loss = self.loss, optimizer = self.optimizer, metrics = [self.metrics]) return model def train(self, x, k = 3): kfold = StratifiedKFold(n_splits = k, shuffle = True) for fold, (train_idx, val_idx) in enumerate(kfold.split(x[\'cleaned_text\'], x[\'target\'])): print(\'fold: \', fold) x_trn = self.encode(x.loc[train_idx, \'cleaned_text\']) x_val = self.encode(x.loc[val_idx, \'cleaned_text\']) y_trn = np.array(x.loc[train_idx, \'target\'], dtype = np.uint8) y_val = np.array(x.loc[val_idx, \'target\'], dtype = np.uint8) model = self.make_model() print(\'model made.\') model.fit(x_trn, tf.convert_to_tensor(y_trn), validation_data = (x_val, tf.convert_to_tensor(y_val)), batch_size=self.batch_size, epochs = self.epochs) self.models.append(model)\n\nAnd then we define the object:\n\nclassifier = DisasterDetector(tokenizer = tokenizer, bert_layer = bert_layer, max_len = max_len, lr = 0.0001, epochs = 10, activation = \'sigmoid\', batch_size = 32,optimizer = \'SGD\', beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n\nThen I want to train it: classifier.train(train_cleaned)\n\nValueError Traceback (most recent call last) <ipython-input-9-106c756f2e47> in <module>() ----> 1 classifier.train(train_cleaned) <ipython-input-7-2c77803eea5f> in train(self, x, k) 109 model.fit(x_trn, y_trn, 110 validation_data = (x_val, y_val), --> 111 batch_size=self.batch_size, epochs = self.epochs) 112 113 self.models.append(model) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs) 106 def _method_wrapper(self, *args, **kwargs): 107 if not self._in_multi_worker_mode(): # pylint: disable=protected-access --> 108 return method(self, *args, **kwargs) 109 110 # Running inside `run_distribute_coordinator` already. /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 1061 use_multiprocessing=use_multiprocessing, 1062 model=self, -> 1063 steps_per_execution=self._steps_per_execution) 1064 1065 # Container that configures and calls `tf.keras.Callback`s. /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution) 1115 use_multiprocessing=use_multiprocessing, 1116 distribution_strategy=ds_context.get_strategy(), -> 1117 model=model) 1118 1119 strategy = ds_context.get_strategy() /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs) 263 **kwargs): 264 super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs) --> 265 x, y, sample_weights = _process_tensorlike((x, y, sample_weights)) 266 sample_weight_modes = broadcast_sample_weight_modes( 267 sample_weights, sample_weight_modes) /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _process_tensorlike(inputs) 1019 return x 1020 -> 1021 inputs = nest.map_structure(_convert_numpy_and_scipy, inputs) 1022 return nest.list_to_tuple(inputs) 1023 /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs) 633 634 return pack_sequence_as( --> 635 structure[0], [func(*x) for x in entries], 636 expand_composites=expand_composites) 637 /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0) 633 634 return pack_sequence_as( --> 635 structure[0], [func(*x) for x in entries], 636 expand_composites=expand_composites) 637 /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _convert_numpy_and_scipy(x) 1014 if issubclass(x.dtype.type, np.floating): 1015 dtype = backend.floatx() -> 1016 return ops.convert_to_tensor(x, dtype=dtype) 1017 elif scipy_sparse and scipy_sparse.issparse(x): 1018 return _scipy_sparse_to_sparse_tensor(x) /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types) 1497 1498 if ret is None: -> 1499 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) 1500 1501 if ret is NotImplemented: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***) 50 def _default_conversion_function(value, dtype, name, as_ref): 51 del as_ref # Unused. ---> 52 return constant_op.constant(value, dtype, name=name) 53 54 /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name) 262 """""" 263 return _constant_impl(value, dtype, shape, name, verify_shape=False, --> 264 allow_broadcast=True) 265 266 /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast) 273 with trace.Trace(""tf.constant""): 274 return _constant_eager_impl(ctx, value, dtype, shape, verify_shape) --> 275 return _constant_eager_impl(ctx, value, dtype, shape, verify_shape) 276 277 g = ops.get_default_graph() /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape) 298 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape): 299 """"""Implementation of eager constant."""""" --> 300 t = convert_to_eager_tensor(value, ctx, dtype) 301 if shape is None: 302 return t /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype) 96 dtype = dtypes.as_dtype(dtype).as_datatype_enum 97 ctx.ensure_initialized() ---> 98 return ops.EagerTensor(value, ctx.device_name, dtype) 99 100 ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).\n\nThe text was updated successfully, but these errors were encountered:\n\ncraigcitro commented\n\nI suspect the issue is to do with the TensorFlow version -- colab is on 2.3, and IIRC Kaggle is on 2.2.\n\nYou can try installing a different version (eg !pip install tensorflow==2.2) and restarting your kernel; if you can confirm it\'s to do with the TF version, I\'d try asking on the TF issue tracker or Stack Overflow.\n\ncraigcitro closed this as completed\n\nmitramir55 commented\n\n@craigcitro Thank you very much for answering. It turns out that I\'ve been giving a small max_len for the bert layer inputs and because of that the sentences couldn\'t completely fit into the input vectors. The error was misleading.\n\ncraigcitro commented\n\nAwesome, glad you got unblocked. 👍\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-05-18T04:46:45', 'title': 'TensorFlow ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list) · Issue #1515 · googlecolab/colabtools', 'url': 'https://github.com/googlecolab/colabtools/issues/1515'}), Document(page_content='Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nhuggingface / transformers Public\n\nmT5 TensorFlow error - Attempt to convert a value (None) with an unsupported type #13821\n\ngcervantes8 opened this issue\n\nSep 30, 2021 · 12 comments\n\nmT5 TensorFlow error - Attempt to convert a value (None) with an unsupported type #13821\n\ngcervantes8 opened this issue\n\nSep 30, 2021 · 12 comments\n\ngcervantes8 commented\n\ntransformers version: 4.11.2\n\nPlatform: Linux-5.11.0-37-generic-x86_64-with-glibc2.29\n\nPython version: 3.8.10\n\nPyTorch version (GPU?): not installed (NA)\n\nTensorflow version (GPU?): 2.6.0 (False)\n\nFlax version (CPU?/GPU?/TPU?): not installed (NA)\n\nJax version: not installed\n\nJaxLib version: not installed\n\nUsing GPU in script?: No\n\nUsing distributed or parallel set-up in script?: No\n\nModel I am using is mT5:\n\nThe problem arises when running the transformers/examples/tensorflow/translation/run_translation.py file.\n\nI made this modification to get the machine translation to run:\n\nsource_lang = data_args.source_lang.split(""_"")[0] target_lang = data_args.target_lang.split(""_"")[0]\n\nsource_lang = data_args.source_lang target_lang = data_args.target_lang\n\nAnd then I ran the script with these parameters: --do_train True --model_name_or_path google/mt5-base --tokenizer_name google/mt5-base --output_dir output --dataset_name ccaligned_multilingual --dataset_config_name sentences-ak_GH --source_lang en_XX --target_lang ak_GH\n\nAnd I ran into the error: ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor.\n\nChanging the model and tokenizer from google/mt5-base to t5-base will fix the error I am getting, so I think it\'s specific to the mT5 model.\n\nI appreciate any help or advice, I really like this library so far!\n\n/home/gcervantes/Desktop/work/python_envs/huggingface/bin/python /home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py --do_train True --model_name_or_path google/mt5-base --tokenizer_name google/mt5-base --output_dir output --dataset_name ccaligned_multilingual --dataset_config_name sentences-ak_GH --source_lang en_XX --target_lang ak_GH 2021-09-30 17:00:16.749595: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcudart.so.11.0\'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory 2021-09-30 17:00:16.749613: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 09/30/2021 17:00:17 - INFO - __main__ - Training/evaluation parameters TFTrainingArguments( _n_gpu=-1, adafactor=False, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, dataloader_drop_last=False, dataloader_num_workers=0, dataloader_pin_memory=True, ddp_find_unused_parameters=None, debug=[], deepspeed=None, disable_tqdm=False, do_eval=False, do_predict=False, do_train=True, eval_accumulation_steps=None, eval_steps=None, evaluation_strategy=IntervalStrategy.NO, fp16=False, fp16_backend=auto, fp16_full_eval=False, fp16_opt_level=O1, gcp_project=None, gradient_accumulation_steps=1, gradient_checkpointing=False, greater_is_better=None, group_by_length=False, hub_model_id=None, hub_strategy=HubStrategy.EVERY_SAVE, hub_token=None, ignore_data_skip=False, label_names=None, label_smoothing_factor=0.0, learning_rate=5e-05, length_column_name=length, load_best_model_at_end=False, local_rank=-1, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir=output/runs/Sep30_17-00-17_nb24862, logging_first_step=False, logging_nan_inf_filter=True, logging_steps=500, logging_strategy=IntervalStrategy.STEPS, lr_scheduler_type=SchedulerType.LINEAR, max_grad_norm=1.0, max_steps=-1, metric_for_best_model=None, mp_parameters=, no_cuda=False, num_train_epochs=3.0, output_dir=output, overwrite_output_dir=False, past_index=-1, per_device_eval_batch_size=8, per_device_train_batch_size=8, poly_power=1.0, prediction_loss_only=False, push_to_hub=False, push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, remove_unused_columns=True, report_to=[\'tensorboard\'], resume_from_checkpoint=None, run_name=output, save_on_each_node=False, save_steps=500, save_strategy=IntervalStrategy.STEPS, save_total_limit=None, seed=42, sharded_ddp=[], skip_memory_metrics=True, tpu_metrics_debug=False, tpu_name=None, tpu_num_cores=None, tpu_zone=None, use_legacy_prediction_loop=False, warmup_ratio=0.0, warmup_steps=0, weight_decay=0.0, xla=False, ) 09/30/2021 17:00:18 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/ccaligned_multilingual.py at /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual 09/30/2021 17:00:18 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/ccaligned_multilingual.py at /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36 09/30/2021 17:00:18 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/ccaligned_multilingual.py to /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36/ccaligned_multilingual.py 09/30/2021 17:00:18 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/dataset_infos.json to /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36/dataset_infos.json 09/30/2021 17:00:18 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/ccaligned_multilingual.py at /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36/ccaligned_multilingual.json 09/30/2021 17:00:18 - INFO - datasets.info - Loading Dataset Infos from /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36 09/30/2021 17:00:18 - INFO - datasets.builder - Overwrite dataset info from restored data version. 09/30/2021 17:00:18 - INFO - datasets.info - Loading Dataset info from /home/gcervantes/.cache/huggingface/datasets/ccaligned_multilingual/sentences-ak_GH/1.0.0/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36 09/30/2021 17:00:18 - WARNING - datasets.builder - Reusing dataset ccaligned_multilingual (/home/gcervantes/.cache/huggingface/datasets/ccaligned_multilingual/sentences-ak_GH/1.0.0/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36) 09/30/2021 17:00:18 - INFO - datasets.info - Loading Dataset info from /home/gcervantes/.cache/huggingface/datasets/ccaligned_multilingual/sentences-ak_GH/1.0.0/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36 100%|██████████| 1/1 [00:00<00:00, 899.29it/s] loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /home/gcervantes/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de Model config MT5Config { ""_name_or_path"": ""/home/patrick/hugging_face/t5/mt5-base"", ""architectures"": [ ""MT5ForConditionalGeneration"" ], ""d_ff"": 2048, ""d_kv"": 64, ""d_model"": 768, ""decoder_start_token_id"": 0, ""dropout_rate"": 0.1, ""eos_token_id"": 1, ""feed_forward_proj"": ""gated-gelu"", ""initializer_factor"": 1.0, ""is_encoder_decoder"": true, ""layer_norm_epsilon"": 1e-06, ""model_type"": ""mt5"", ""num_decoder_layers"": 12, ""num_heads"": 12, ""num_layers"": 12, ""output_past"": true, ""pad_token_id"": 0, ""relative_attention_num_buckets"": 32, ""tie_word_embeddings"": false, ""tokenizer_class"": ""T5Tokenizer"", ""transformers_version"": ""4.11.0.dev0"", ""use_cache"": true, ""vocab_size"": 250112 } loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /home/gcervantes/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de Model config MT5Config { ""_name_or_path"": ""/home/patrick/hugging_face/t5/mt5-base"", ""architectures"": [ ""MT5ForConditionalGeneration"" ], ""d_ff"": 2048, ""d_kv"": 64, ""d_model"": 768, ""decoder_start_token_id"": 0, ""dropout_rate"": 0.1, ""eos_token_id"": 1, ""feed_forward_proj"": ""gated-gelu"", ""initializer_factor"": 1.0, ""is_encoder_decoder"": true, ""layer_norm_epsilon"": 1e-06, ""model_type"": ""mt5"", ""num_decoder_layers"": 12, ""num_heads"": 12, ""num_layers"": 12, ""output_past"": true, ""pad_token_id"": 0, ""relative_attention_num_buckets"": 32, ""tie_word_embeddings"": false, ""tokenizer_class"": ""T5Tokenizer"", ""transformers_version"": ""4.11.0.dev0"", ""use_cache"": true, ""vocab_size"": 250112 } loading file https://huggingface.co/google/mt5-base/resolve/main/spiece.model from cache at /home/gcervantes/.cache/huggingface/transformers/4764ec347af4d2d6286acbe1d9d630ac0afd8554a4c4a64170e0b663fd2e2412.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2 loading file https://huggingface.co/google/mt5-base/resolve/main/tokenizer.json from cache at None loading file https://huggingface.co/google/mt5-base/resolve/main/added_tokens.json from cache at None loading file https://huggingface.co/google/mt5-base/resolve/main/special_tokens_map.json from cache at /home/gcervantes/.cache/huggingface/transformers/0d7d5b3fc19bf58d4b274990c8bcf5e307726bc18d95f40a1436dfb6a0892f85.294ebaa4cd17bb284635004c92d2c4d522ec488c828dcce0c2471b6f28e3fe82 loading file https://huggingface.co/google/mt5-base/resolve/main/tokenizer_config.json from cache at /home/gcervantes/.cache/huggingface/transformers/afba33be693521ccefbde6d03b93b5c517d7108ba31f6c08000ed52c2cea45c9.28bbf90ae7962b1b7211c0ce8b2006f968c82439ec9c47e0847ba63642f9435a loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /home/gcervantes/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de Model config MT5Config { ""_name_or_path"": ""/home/patrick/hugging_face/t5/mt5-base"", ""architectures"": [ ""MT5ForConditionalGeneration"" ], ""d_ff"": 2048, ""d_kv"": 64, ""d_model"": 768, ""decoder_start_token_id"": 0, ""dropout_rate"": 0.1, ""eos_token_id"": 1, ""feed_forward_proj"": ""gated-gelu"", ""initializer_factor"": 1.0, ""is_encoder_decoder"": true, ""layer_norm_epsilon"": 1e-06, ""model_type"": ""mt5"", ""num_decoder_layers"": 12, ""num_heads"": 12, ""num_layers"": 12, ""output_past"": true, ""pad_token_id"": 0, ""relative_attention_num_buckets"": 32, ""tie_word_embeddings"": false, ""tokenizer_class"": ""T5Tokenizer"", ""transformers_version"": ""4.11.0.dev0"", ""use_cache"": true, ""vocab_size"": 250112 } loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /home/gcervantes/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de Model config MT5Config { ""_name_or_path"": ""/home/patrick/hugging_face/t5/mt5-base"", ""architectures"": [ ""MT5ForConditionalGeneration"" ], ""d_ff"": 2048, ""d_kv"": 64, ""d_model"": 768, ""decoder_start_token_id"": 0, ""dropout_rate"": 0.1, ""eos_token_id"": 1, ""feed_forward_proj"": ""gated-gelu"", ""initializer_factor"": 1.0, ""is_encoder_decoder"": true, ""layer_norm_epsilon"": 1e-06, ""model_type"": ""mt5"", ""num_decoder_layers"": 12, ""num_heads"": 12, ""num_layers"": 12, ""output_past"": true, ""pad_token_id"": 0, ""relative_attention_num_buckets"": 32, ""tie_word_embeddings"": false, ""tokenizer_class"": ""T5Tokenizer"", ""transformers_version"": ""4.11.0.dev0"", ""use_cache"": true, ""vocab_size"": 250112 } 09/30/2021 17:00:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/gcervantes/.cache/huggingface/datasets/ccaligned_multilingual/sentences-ak_GH/1.0.0/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36/cache-d7a5cf279d2e727e.arrow Tensorflow: setting up strategy 2021-09-30 17:00:22.340094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2021-09-30 17:00:22.340493: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcudart.so.11.0\'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340535: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcublas.so.11\'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340572: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcublasLt.so.11\'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340609: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcufft.so.10\'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340646: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcurand.so.10\'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340682: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcusolver.so.11\'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340718: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcusparse.so.11\'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340754: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcudnn.so.8\'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340762: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... 2021-09-30 17:00:22.341064: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. loading weights file https://huggingface.co/google/mt5-base/resolve/main/tf_model.h5 from cache at /home/gcervantes/.cache/huggingface/transformers/41c2fc682e5acee0c74105c9950da8f133eef8879ef0e2e2edd37c4d237da2ee.ffac6e54739b6e6cd3d9e8b6671a9514d3b1b755459a51fdc1749d110e5a5a1d.h5 2021-09-30 17:00:22.636446: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. All model checkpoint layers were used when initializing TFMT5ForConditionalGeneration. All the layers of TFMT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-base. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMT5ForConditionalGeneration for predictions without further training. Traceback (most recent call last): File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 622, in <module> main() File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 493, in main model.resize_token_embeddings(len(tokenizer)) File ""/home/gcervantes/Desktop/work/Code/transformers/src/transformers/modeling_tf_utils.py"", line 856, in resize_token_embeddings model_embeds = self._resize_token_embeddings(new_num_tokens) File ""/home/gcervantes/Desktop/work/Code/transformers/src/transformers/modeling_tf_utils.py"", line 901, in _resize_token_embeddings new_lm_head_decoder = self._get_resized_lm_head_decoder(old_lm_head_decoder, new_num_tokens) File ""/home/gcervantes/Desktop/work/Code/transformers/src/transformers/modeling_tf_utils.py"", line 981, in _get_resized_lm_head_decoder self._get_word_embedding_weight(self.get_input_embeddings()) == old_lm_head_decoder File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 1092, in __eq__ return gen_math_ops.equal(self, other, incompatible_shape_error=False) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 3208, in equal return equal_eager_fallback( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 3237, in equal_eager_fallback _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, []) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 273, in args_to_matching_eager tensor = ops.convert_to_tensor( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py"", line 163, in wrapped return func(*args, **kwargs) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1566, in convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 346, in _constant_tensor_conversion_function return constant(v, dtype=dtype, name=name) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 271, in constant return _constant_impl(value, dtype, shape, name, verify_shape=False, File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 283, in _constant_impl return _constant_eager_impl(ctx, value, dtype, shape, verify_shape) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 308, in _constant_eager_impl t = convert_to_eager_tensor(value, ctx, dtype) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 106, in convert_to_eager_tensor return ops.EagerTensor(value, ctx.device_name, dtype) ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. Process finished with exit code 1\n\nThe text was updated successfully, but these errors were encountered:\n\npatrickvonplaten commented\n\nI don\'t really understand why this change is needed:\n\nsource_lang = data_args.source_lang.split(""_"")[0] target_lang = data_args.target_lang.split(""_"")[0]\n\nWhat error do you get without making this change? Can you maybe copy-paste the command you run that gives you an error without having made the above changes\n\ngcervantes8 commented\n\nHey @patrickvonplaten thanks for the help!\n\nWithout making the change I get this error:\n\nTraceback (most recent call last): File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 620, in <module> main() File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 450, in main train_dataset = train_dataset.map( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1686, in map return self._map_single( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 185, in wrapper out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/fingerprint.py"", line 398, in wrapper out = func(self, *args, **kwargs) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2048, in _map_single batch = apply_function_on_filtered_inputs( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1939, in apply_function_on_filtered_inputs function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs) File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 424, in preprocess_function inputs = [ex[source_lang] for ex in examples[""translation""]] File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 424, in <listcomp> inputs = [ex[source_lang] for ex in examples[""translation""]] KeyError: \'en\' Process finished with exit code 1\n\nI do this because in the ccaligned_multilingual data, the keys used in the JSON file are ak_GH and en_XX. The original code modifies the language code so that it only uses the characters before the _, in this example it would give en and ak giving me a key error.\n\ngcervantes8 commented\n\nWithout making any changes to the code, running transformers/examples/tensorflow/translation/run_translation.py with these arguments also gives the same error.\n\n--do_train True --model_name_or_path google/mt5-base --tokenizer_name google/mt5-base --output_dir output --dataset_name opus_euconst --dataset_config_name cs-en --source_lang cs --target_lang en\n\nChanging the model and tokenizer to google/byt5-base gives no error.\n\npatrickvonplaten commented\n\nThanks for the answer! @Rocketknight1 - could you maybe give this a look? I think you\'ve recently worked with the TF translation script no? :-)\n\nRocketknight1 commented\n\n@patrickvonplaten On my list, I\'ll try to investigate this today or tomorrow!\n\nRocketknight1 commented\n\nHi @gcervantes8, sorry to be annoying, but can I ask you to test this with the TF translation notebook too? Just swap in the mT5 model and your dataset there, and then if you encounter the same issue, you can save and upload the notebook with your changes and the error outputs. I know it\'s a bit lazy of me, but it\'ll make it much easier for me to reproduce and locate the problem!\n\ngcervantes8 commented\n\nHey @Rocketknight1 thanks for the help! So I tried running the model with the TF translation notebook, but I didn\'t encounter the issue strangely enough.\n\nThese are the changes I made to the TF Notebook. I changed the model. model_checkpoint = ""google/mt5-base"" Changed the dataset raw_datasets = load_dataset(""opus_euconst"", ""cs-da"") I modified the source language and the target language specified before the preprocess function\n\nsource_lang = ""cs"" target_lang = ""da""\n\nI modified the batch size because I was getting out of memory errors batch_size = 1\n\nAnd I also had to remove the validation_dataset.\n\nSo this might be specific to the transformers/examples/tensorflow/translation/run_translation.py script\n\nRocketknight1 commented\n\nHm, that\'s quite unusual because the scripts should be similar. I\'ll try to reproduce with the example script here in the next couple of days and let you know what I find.\n\ngcervantes8 commented\n\nI looked into it more and it seems that the resize_token_embeddings function in src/transformers/modeling_tf_utils.py expects the get_output_embeddings function in src/transformers/models/t5/modeling_tf_t5.py to return an object with the attribute weight or decoder.\n\nThe model works for T5 because in the get_output_embeddings T5 function, self.config.tie_word_embeddings is True and it doesn\'t go to the else part of the if statement which only returns the Tensor.\n\nI\'m not really sure how the best way for this to be fixed is. @patrickvonplaten what do you think?\n\ngithub-actions bot commented\n\nThis issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the contributing guidelines are likely to be ignored.\n\ngcervantes8 commented\n\nI think this issue still needs to be addressed, I\'m still receiving the error retraining a mT5 Model using TensorFlow.\n\ngcervantes8 commented\n\nIt seems this issue is the same (or maybe just similar?) as issue #13839 And it looks like #14329 will probably fix it, so I\'ll close this issue.\n\ngcervantes8 closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_5', 'snippet': 'Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nhuggingface / transformers Public\n\nmT5 TensorFlow error - Attempt to convert a value (None) with an unsupported type #13821\n\ngcervantes8 opened this issue\n\nSep 30, 2021 · 12 comments\n\nmT5 TensorFlow error - Attempt to convert a value (None) with an unsupported type #13821\n\ngcervantes8 opened this issue\n\nSep 30, 2021 · 12 comments\n\ngcervantes8 commented\n\ntransformers version: 4.11.2\n\nPlatform: Linux-5.11.0-37-generic-x86_64-with-glibc2.29\n\nPython version: 3.8.10\n\nPyTorch version (GPU?): not installed (NA)\n\nTensorflow version (GPU?): 2.6.0 (False)\n\nFlax version (CPU?/GPU?/TPU?): not installed (NA)\n\nJax version: not installed\n\nJaxLib version: not installed\n\nUsing GPU in script?: No\n\nUsing distributed or parallel set-up in script?: No\n\nModel I am using is mT5:\n\nThe problem arises when running the transformers/examples/tensorflow/translation/run_translation.py file.\n\nI made this modification to get the machine translation to run:\n\nsource_lang = data_args.source_lang.split(""_"")[0] target_lang = data_args.target_lang.split(""_"")[0]\n\nsource_lang = data_args.source_lang target_lang = data_args.target_lang\n\nAnd then I ran the script with these parameters: --do_train True --model_name_or_path google/mt5-base --tokenizer_name google/mt5-base --output_dir output --dataset_name ccaligned_multilingual --dataset_config_name sentences-ak_GH --source_lang en_XX --target_lang ak_GH\n\nAnd I ran into the error: ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor.\n\nChanging the model and tokenizer from google/mt5-base to t5-base will fix the error I am getting, so I think it\'s specific to the mT5 model.\n\nI appreciate any help or advice, I really like this library so far!\n\n/home/gcervantes/Desktop/work/python_envs/huggingface/bin/python /home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py --do_train True --model_name_or_path google/mt5-base --tokenizer_name google/mt5-base --output_dir output --dataset_name ccaligned_multilingual --dataset_config_name sentences-ak_GH --source_lang en_XX --target_lang ak_GH 2021-09-30 17:00:16.749595: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcudart.so.11.0\'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory 2021-09-30 17:00:16.749613: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. 09/30/2021 17:00:17 - INFO - __main__ - Training/evaluation parameters TFTrainingArguments( _n_gpu=-1, adafactor=False, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, dataloader_drop_last=False, dataloader_num_workers=0, dataloader_pin_memory=True, ddp_find_unused_parameters=None, debug=[], deepspeed=None, disable_tqdm=False, do_eval=False, do_predict=False, do_train=True, eval_accumulation_steps=None, eval_steps=None, evaluation_strategy=IntervalStrategy.NO, fp16=False, fp16_backend=auto, fp16_full_eval=False, fp16_opt_level=O1, gcp_project=None, gradient_accumulation_steps=1, gradient_checkpointing=False, greater_is_better=None, group_by_length=False, hub_model_id=None, hub_strategy=HubStrategy.EVERY_SAVE, hub_token=None, ignore_data_skip=False, label_names=None, label_smoothing_factor=0.0, learning_rate=5e-05, length_column_name=length, load_best_model_at_end=False, local_rank=-1, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir=output/runs/Sep30_17-00-17_nb24862, logging_first_step=False, logging_nan_inf_filter=True, logging_steps=500, logging_strategy=IntervalStrategy.STEPS, lr_scheduler_type=SchedulerType.LINEAR, max_grad_norm=1.0, max_steps=-1, metric_for_best_model=None, mp_parameters=, no_cuda=False, num_train_epochs=3.0, output_dir=output, overwrite_output_dir=False, past_index=-1, per_device_eval_batch_size=8, per_device_train_batch_size=8, poly_power=1.0, prediction_loss_only=False, push_to_hub=False, push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, remove_unused_columns=True, report_to=[\'tensorboard\'], resume_from_checkpoint=None, run_name=output, save_on_each_node=False, save_steps=500, save_strategy=IntervalStrategy.STEPS, save_total_limit=None, seed=42, sharded_ddp=[], skip_memory_metrics=True, tpu_metrics_debug=False, tpu_name=None, tpu_num_cores=None, tpu_zone=None, use_legacy_prediction_loop=False, warmup_ratio=0.0, warmup_steps=0, weight_decay=0.0, xla=False, ) 09/30/2021 17:00:18 - INFO - datasets.load - Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/ccaligned_multilingual.py at /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual 09/30/2021 17:00:18 - INFO - datasets.load - Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/ccaligned_multilingual.py at /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36 09/30/2021 17:00:18 - INFO - datasets.load - Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/ccaligned_multilingual.py to /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36/ccaligned_multilingual.py 09/30/2021 17:00:18 - INFO - datasets.load - Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/dataset_infos.json to /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36/dataset_infos.json 09/30/2021 17:00:18 - INFO - datasets.load - Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.12.1/datasets/ccaligned_multilingual/ccaligned_multilingual.py at /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36/ccaligned_multilingual.json 09/30/2021 17:00:18 - INFO - datasets.info - Loading Dataset Infos from /home/gcervantes/.cache/huggingface/modules/datasets_modules/datasets/ccaligned_multilingual/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36 09/30/2021 17:00:18 - INFO - datasets.builder - Overwrite dataset info from restored data version. 09/30/2021 17:00:18 - INFO - datasets.info - Loading Dataset info from /home/gcervantes/.cache/huggingface/datasets/ccaligned_multilingual/sentences-ak_GH/1.0.0/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36 09/30/2021 17:00:18 - WARNING - datasets.builder - Reusing dataset ccaligned_multilingual (/home/gcervantes/.cache/huggingface/datasets/ccaligned_multilingual/sentences-ak_GH/1.0.0/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36) 09/30/2021 17:00:18 - INFO - datasets.info - Loading Dataset info from /home/gcervantes/.cache/huggingface/datasets/ccaligned_multilingual/sentences-ak_GH/1.0.0/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36 100%|██████████| 1/1 [00:00<00:00, 899.29it/s] loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /home/gcervantes/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de Model config MT5Config { ""_name_or_path"": ""/home/patrick/hugging_face/t5/mt5-base"", ""architectures"": [ ""MT5ForConditionalGeneration"" ], ""d_ff"": 2048, ""d_kv"": 64, ""d_model"": 768, ""decoder_start_token_id"": 0, ""dropout_rate"": 0.1, ""eos_token_id"": 1, ""feed_forward_proj"": ""gated-gelu"", ""initializer_factor"": 1.0, ""is_encoder_decoder"": true, ""layer_norm_epsilon"": 1e-06, ""model_type"": ""mt5"", ""num_decoder_layers"": 12, ""num_heads"": 12, ""num_layers"": 12, ""output_past"": true, ""pad_token_id"": 0, ""relative_attention_num_buckets"": 32, ""tie_word_embeddings"": false, ""tokenizer_class"": ""T5Tokenizer"", ""transformers_version"": ""4.11.0.dev0"", ""use_cache"": true, ""vocab_size"": 250112 } loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /home/gcervantes/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de Model config MT5Config { ""_name_or_path"": ""/home/patrick/hugging_face/t5/mt5-base"", ""architectures"": [ ""MT5ForConditionalGeneration"" ], ""d_ff"": 2048, ""d_kv"": 64, ""d_model"": 768, ""decoder_start_token_id"": 0, ""dropout_rate"": 0.1, ""eos_token_id"": 1, ""feed_forward_proj"": ""gated-gelu"", ""initializer_factor"": 1.0, ""is_encoder_decoder"": true, ""layer_norm_epsilon"": 1e-06, ""model_type"": ""mt5"", ""num_decoder_layers"": 12, ""num_heads"": 12, ""num_layers"": 12, ""output_past"": true, ""pad_token_id"": 0, ""relative_attention_num_buckets"": 32, ""tie_word_embeddings"": false, ""tokenizer_class"": ""T5Tokenizer"", ""transformers_version"": ""4.11.0.dev0"", ""use_cache"": true, ""vocab_size"": 250112 } loading file https://huggingface.co/google/mt5-base/resolve/main/spiece.model from cache at /home/gcervantes/.cache/huggingface/transformers/4764ec347af4d2d6286acbe1d9d630ac0afd8554a4c4a64170e0b663fd2e2412.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2 loading file https://huggingface.co/google/mt5-base/resolve/main/tokenizer.json from cache at None loading file https://huggingface.co/google/mt5-base/resolve/main/added_tokens.json from cache at None loading file https://huggingface.co/google/mt5-base/resolve/main/special_tokens_map.json from cache at /home/gcervantes/.cache/huggingface/transformers/0d7d5b3fc19bf58d4b274990c8bcf5e307726bc18d95f40a1436dfb6a0892f85.294ebaa4cd17bb284635004c92d2c4d522ec488c828dcce0c2471b6f28e3fe82 loading file https://huggingface.co/google/mt5-base/resolve/main/tokenizer_config.json from cache at /home/gcervantes/.cache/huggingface/transformers/afba33be693521ccefbde6d03b93b5c517d7108ba31f6c08000ed52c2cea45c9.28bbf90ae7962b1b7211c0ce8b2006f968c82439ec9c47e0847ba63642f9435a loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /home/gcervantes/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de Model config MT5Config { ""_name_or_path"": ""/home/patrick/hugging_face/t5/mt5-base"", ""architectures"": [ ""MT5ForConditionalGeneration"" ], ""d_ff"": 2048, ""d_kv"": 64, ""d_model"": 768, ""decoder_start_token_id"": 0, ""dropout_rate"": 0.1, ""eos_token_id"": 1, ""feed_forward_proj"": ""gated-gelu"", ""initializer_factor"": 1.0, ""is_encoder_decoder"": true, ""layer_norm_epsilon"": 1e-06, ""model_type"": ""mt5"", ""num_decoder_layers"": 12, ""num_heads"": 12, ""num_layers"": 12, ""output_past"": true, ""pad_token_id"": 0, ""relative_attention_num_buckets"": 32, ""tie_word_embeddings"": false, ""tokenizer_class"": ""T5Tokenizer"", ""transformers_version"": ""4.11.0.dev0"", ""use_cache"": true, ""vocab_size"": 250112 } loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /home/gcervantes/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de Model config MT5Config { ""_name_or_path"": ""/home/patrick/hugging_face/t5/mt5-base"", ""architectures"": [ ""MT5ForConditionalGeneration"" ], ""d_ff"": 2048, ""d_kv"": 64, ""d_model"": 768, ""decoder_start_token_id"": 0, ""dropout_rate"": 0.1, ""eos_token_id"": 1, ""feed_forward_proj"": ""gated-gelu"", ""initializer_factor"": 1.0, ""is_encoder_decoder"": true, ""layer_norm_epsilon"": 1e-06, ""model_type"": ""mt5"", ""num_decoder_layers"": 12, ""num_heads"": 12, ""num_layers"": 12, ""output_past"": true, ""pad_token_id"": 0, ""relative_attention_num_buckets"": 32, ""tie_word_embeddings"": false, ""tokenizer_class"": ""T5Tokenizer"", ""transformers_version"": ""4.11.0.dev0"", ""use_cache"": true, ""vocab_size"": 250112 } 09/30/2021 17:00:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/gcervantes/.cache/huggingface/datasets/ccaligned_multilingual/sentences-ak_GH/1.0.0/ecebf2fba25342d63934850b389502a24fb3d61845e74643a416e06c773ffa36/cache-d7a5cf279d2e727e.arrow Tensorflow: setting up strategy 2021-09-30 17:00:22.340094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero 2021-09-30 17:00:22.340493: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcudart.so.11.0\'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340535: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcublas.so.11\'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340572: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcublasLt.so.11\'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340609: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcufft.so.10\'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340646: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcurand.so.10\'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340682: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcusolver.so.11\'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340718: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcusparse.so.11\'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340754: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library \'libcudnn.so.8\'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory 2021-09-30 17:00:22.340762: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices... 2021-09-30 17:00:22.341064: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. loading weights file https://huggingface.co/google/mt5-base/resolve/main/tf_model.h5 from cache at /home/gcervantes/.cache/huggingface/transformers/41c2fc682e5acee0c74105c9950da8f133eef8879ef0e2e2edd37c4d237da2ee.ffac6e54739b6e6cd3d9e8b6671a9514d3b1b755459a51fdc1749d110e5a5a1d.h5 2021-09-30 17:00:22.636446: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them. All model checkpoint layers were used when initializing TFMT5ForConditionalGeneration. All the layers of TFMT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-base. If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMT5ForConditionalGeneration for predictions without further training. Traceback (most recent call last): File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 622, in <module> main() File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 493, in main model.resize_token_embeddings(len(tokenizer)) File ""/home/gcervantes/Desktop/work/Code/transformers/src/transformers/modeling_tf_utils.py"", line 856, in resize_token_embeddings model_embeds = self._resize_token_embeddings(new_num_tokens) File ""/home/gcervantes/Desktop/work/Code/transformers/src/transformers/modeling_tf_utils.py"", line 901, in _resize_token_embeddings new_lm_head_decoder = self._get_resized_lm_head_decoder(old_lm_head_decoder, new_num_tokens) File ""/home/gcervantes/Desktop/work/Code/transformers/src/transformers/modeling_tf_utils.py"", line 981, in _get_resized_lm_head_decoder self._get_word_embedding_weight(self.get_input_embeddings()) == old_lm_head_decoder File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 1092, in __eq__ return gen_math_ops.equal(self, other, incompatible_shape_error=False) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 3208, in equal return equal_eager_fallback( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 3237, in equal_eager_fallback _attr_T, _inputs_T = _execute.args_to_matching_eager([x, y], ctx, []) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 273, in args_to_matching_eager tensor = ops.convert_to_tensor( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py"", line 163, in wrapped return func(*args, **kwargs) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1566, in convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 346, in _constant_tensor_conversion_function return constant(v, dtype=dtype, name=name) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 271, in constant return _constant_impl(value, dtype, shape, name, verify_shape=False, File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 283, in _constant_impl return _constant_eager_impl(ctx, value, dtype, shape, verify_shape) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 308, in _constant_eager_impl t = convert_to_eager_tensor(value, ctx, dtype) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py"", line 106, in convert_to_eager_tensor return ops.EagerTensor(value, ctx.device_name, dtype) ValueError: Attempt to convert a value (None) with an unsupported type (<class \'NoneType\'>) to a Tensor. Process finished with exit code 1\n\nThe text was updated successfully, but these errors were encountered:\n\npatrickvonplaten commented\n\nI don\'t really understand why this change is needed:\n\nsource_lang = data_args.source_lang.split(""_"")[0] target_lang = data_args.target_lang.split(""_"")[0]\n\nWhat error do you get without making this change? Can you maybe copy-paste the command you run that gives you an error without having made the above changes\n\ngcervantes8 commented\n\nHey @patrickvonplaten thanks for the help!\n\nWithout making the change I get this error:\n\nTraceback (most recent call last): File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 620, in <module> main() File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 450, in main train_dataset = train_dataset.map( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1686, in map return self._map_single( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 185, in wrapper out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/fingerprint.py"", line 398, in wrapper out = func(self, *args, **kwargs) File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2048, in _map_single batch = apply_function_on_filtered_inputs( File ""/home/gcervantes/Desktop/work/python_envs/huggingface/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1939, in apply_function_on_filtered_inputs function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs) File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 424, in preprocess_function inputs = [ex[source_lang] for ex in examples[""translation""]] File ""/home/gcervantes/Desktop/work/Code/transformers/examples/tensorflow/translation/run_translation.py"", line 424, in <listcomp> inputs = [ex[source_lang] for ex in examples[""translation""]] KeyError: \'en\' Process finished with exit code 1\n\nI do this because in the ccaligned_multilingual data, the keys used in the JSON file are ak_GH and en_XX. The original code modifies the language code so that it only uses the characters before the _, in this example it would give en and ak giving me a key error.\n\ngcervantes8 commented\n\nWithout making any changes to the code, running transformers/examples/tensorflow/translation/run_translation.py with these arguments also gives the same error.\n\n--do_train True --model_name_or_path google/mt5-base --tokenizer_name google/mt5-base --output_dir output --dataset_name opus_euconst --dataset_config_name cs-en --source_lang cs --target_lang en\n\nChanging the model and tokenizer to google/byt5-base gives no error.\n\npatrickvonplaten commented\n\nThanks for the answer! @Rocketknight1 - could you maybe give this a look? I think you\'ve recently worked with the TF translation script no? :-)\n\nRocketknight1 commented\n\n@patrickvonplaten On my list, I\'ll try to investigate this today or tomorrow!\n\nRocketknight1 commented\n\nHi @gcervantes8, sorry to be annoying, but can I ask you to test this with the TF translation notebook too? Just swap in the mT5 model and your dataset there, and then if you encounter the same issue, you can save and upload the notebook with your changes and the error outputs. I know it\'s a bit lazy of me, but it\'ll make it much easier for me to reproduce and locate the problem!\n\ngcervantes8 commented\n\nHey @Rocketknight1 thanks for the help! So I tried running the model with the TF translation notebook, but I didn\'t encounter the issue strangely enough.\n\nThese are the changes I made to the TF Notebook. I changed the model. model_checkpoint = ""google/mt5-base"" Changed the dataset raw_datasets = load_dataset(""opus_euconst"", ""cs-da"") I modified the source language and the target language specified before the preprocess function\n\nsource_lang = ""cs"" target_lang = ""da""\n\nI modified the batch size because I was getting out of memory errors batch_size = 1\n\nAnd I also had to remove the validation_dataset.\n\nSo this might be specific to the transformers/examples/tensorflow/translation/run_translation.py script\n\nRocketknight1 commented\n\nHm, that\'s quite unusual because the scripts should be similar. I\'ll try to reproduce with the example script here in the next couple of days and let you know what I find.\n\ngcervantes8 commented\n\nI looked into it more and it seems that the resize_token_embeddings function in src/transformers/modeling_tf_utils.py expects the get_output_embeddings function in src/transformers/models/t5/modeling_tf_t5.py to return an object with the attribute weight or decoder.\n\nThe model works for T5 because in the get_output_embeddings T5 function, self.config.tie_word_embeddings is True and it doesn\'t go to the else part of the if statement which only returns the Tensor.\n\nI\'m not really sure how the best way for this to be fixed is. @patrickvonplaten what do you think?\n\ngithub-actions bot commented\n\nThis issue has been automatically marked as stale because it has not had recent activity. If you think this still needs to be addressed please comment on this thread.\n\nPlease note that issues that do not follow the contributing guidelines are likely to be ignored.\n\ngcervantes8 commented\n\nI think this issue still needs to be addressed, I\'m still receiving the error retraining a mT5 Model using TensorFlow.\n\ngcervantes8 commented\n\nIt seems this issue is the same (or maybe just similar?) as issue #13839 And it looks like #14329 will probably fix it, so I\'ll close this issue.\n\ngcervantes8 closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-02-26T04:42:58', 'title': 'mT5 TensorFlow error - Attempt to convert a value (None) with an unsupported type · Issue #13821 · huggingface/transformers', 'url': 'https://github.com/huggingface/transformers/issues/13821'})], []]??"
64826405,tf.tensordot,"{'https://www.edx.org/learn/tensorflow', 'https://www.coursera.org/learn/linear-algebra-machine-learning', 'https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187', 'https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/', 'https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/', 'https://www.udacity.com/course/intro-to-tensorflow-lite--ud190', 'https://www.coursera.org/learn/introduction-tensorflow', 'https://www.udacity.com/course/intro-to-machine-learning-with-tensorflow-nanodegree--nd230', 'https://www.udemy.com/course/deep-learning-tensorflow-2/', 'https://www.udemy.com/course/mastrecam-tip-create-and-modify-geometry/'}","{'https://www.youtube.com/watch?v=0PGnW7ZN3sg', 'https://www.youtube.com/watch?v=Rj1SI6kwxR8'}","{'https://stackoverflow.com/questions/51266507/product-of-pytorch-tensors-along-arbitrary-axes-%C3%A0-la-numpys-tensordot', 'https://stackoverflow.com/questions/64826405/tensorflow-axis-argument-in-dot-product', 'https://stackoverflow.com/questions/41870228/understanding-tensordot', 'https://stackoverflow.com/questions/51989572/how-does-numpy-tensordot-function-works-step-by-step', 'https://stackoverflow.com/questions/70907083/numpy-einsum-tensordot-with-shared-non-contracted-axis'}","??[[Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nUnderstanding tensordot\n\nAsked 7 years, 5 months ago\n\nAfter I learned how to use einsum, I am now trying to understand how np.tensordot works.\n\nHowever, I am a little bit lost especially regarding the various possibilities for the parameter axes.\n\nTo understand it, as I have never practiced tensor calculus, I use the following example:\n\nA = np.random.randint(2, size=(2, 3, 5)) B = np.random.randint(2, size=(3, 2, 4))\n\nIn this case, what are the different possible np.tensordot and how would you compute it manually?\n\nImprove this question\n\nedited Dec 26, 2017 at 8:47\n\n60k1515 gold badges168168 silver badges156156 bronze badges\n\nasked Jan 26, 2017 at 9:22\n\n2,30122 gold badges2424 silver badges4646 bronze badges 0\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe idea with tensordot is pretty simple - We input the arrays and the respective axes along which the sum-reductions are intended. The axes that take part in sum-reduction are removed in the output and all of the remaining axes from the input arrays are spread-out as different axes in the output keeping the order in which the input arrays are fed.\n\nLet\'s look at few sample cases with one and two axes of sum-reductions and also swap the input places and see how the order is kept in the output.\n\nI. One axis of sum-reduction\n\nIn [7]: A = np.random.randint(2, size=(2, 6, 5)) ...: B = np.random.randint(2, size=(3, 2, 4)) ...:\n\nIn [9]: np.tensordot(A, B, axes=((0),(1))).shape Out[9]: (6, 5, 3, 4) A : (2, 6, 5) -> reduction of axis=0 B : (3, 2, 4) -> reduction of axis=1 Output : `(2, 6, 5)`, `(3, 2, 4)` ===(2 gone)==> `(6,5)` + `(3,4)` => `(6,5,3,4)`\n\nCase #2 (same as case #1 but the inputs are fed swapped):\n\nIn [8]: np.tensordot(B, A, axes=((1),(0))).shape Out[8]: (3, 4, 6, 5) B : (3, 2, 4) -> reduction of axis=1 A : (2, 6, 5) -> reduction of axis=0 Output : `(3, 2, 4)`, `(2, 6, 5)` ===(2 gone)==> `(3,4)` + `(6,5)` => `(3,4,6,5)`.\n\nII. Two axes of sum-reduction\n\nIn [11]: A = np.random.randint(2, size=(2, 3, 5)) ...: B = np.random.randint(2, size=(3, 2, 4)) ...:\n\nIn [12]: np.tensordot(A, B, axes=((0,1),(1,0))).shape Out[12]: (5, 4) A : (2, 3, 5) -> reduction of axis=(0,1) B : (3, 2, 4) -> reduction of axis=(1,0) Output : `(2, 3, 5)`, `(3, 2, 4)` ===(2,3 gone)==> `(5)` + `(4)` => `(5,4)`\n\nIn [14]: np.tensordot(B, A, axes=((1,0),(0,1))).shape Out[14]: (4, 5) B : (3, 2, 4) -> reduction of axis=(1,0) A : (2, 3, 5) -> reduction of axis=(0,1) Output : `(3, 2, 4)`, `(2, 3, 5)` ===(2,3 gone)==> `(4)` + `(5)` => `(4,5)`\n\nWe can extend this to as many axes as possible.\n\nanswered Jan 26, 2017 at 10:26\n\n221k1919 gold badges265265 silver badges364364 bronze badges 21\n\nWhat do you exactly mean by sum-reduction?\n\n– floflo29 Commented Jan 26, 2017 at 11:56\n\n@floflo29 Well you might know that matrix-multiplication involves elementwise multiplication keeping an axis aligned and then summation of elements along that common aligned axis. With that summation, we are losing that common axis, which is termed as reduction, so in short sum-reduction.\n\n– Divakar Commented Jan 26, 2017 at 12:05\n\n@BryanHead The only way to reorder the output axes using np.tensordot is to swap the inputs. If it doesn\'t get you your desired one, transpose would be the way to go.\n\n– Divakar Commented May 19, 2017 at 17:47\n\nWould have been better if @Divakar have added the example starting from 1-D tensor along with how each entry is computed. E.g. t1=K.variable([[1,2],[2,3]] ) t2=K.variable([2,3]) print(K.eval(tf.tensordot(t1,t2,axes=0))) output: [[[2. 3.] [4. 6.]] [[4. 6.] [6. 9.]]] Not sure how the output shape is 2x2x2.\n\n– CKM Commented Aug 20, 2019 at 16:31\n\n@dereks The sum-reduction term used in this post is an umbrella term for element-wise multiplication and then sum-reduction. In the context of dot/tensordot, I assumed it would be safe to put it that way. Apologies if that was confusing. Now, with matrix-multiplication you have one axis of sum-reduction (second axis of first array against first axis of second array), whereas in tensordot more than one axes of sum-reduction. The examples presented show how axes are aligned in the input arrays and how the output axes are obtained from those.\n\n– Divakar Commented Nov 21, 2019 at 15:25\n\n | Show 16 more comments\n\ntensordot swaps axes and reshapes the inputs so it can apply np.dot to 2 2d arrays. It then swaps and reshapes back to the target. It may be easier to experiment than to explain. There\'s no special tensor math going on, just extending dot to work in higher dimensions. tensor just means arrays with more than 2d. If you are already comfortable with einsum then it will be simplest compare the results to that.\n\nA sample test, summing on 1 pair of axes\n\nIn [823]: np.tensordot(A,B,[0,1]).shape Out[823]: (3, 5, 3, 4) In [824]: np.einsum(\'ijk,lim\',A,B).shape Out[824]: (3, 5, 3, 4) In [825]: np.allclose(np.einsum(\'ijk,lim\',A,B),np.tensordot(A,B,[0,1])) Out[825]: True\n\nanother, summing on two.\n\nIn [826]: np.tensordot(A,B,[(0,1),(1,0)]).shape Out[826]: (5, 4) In [827]: np.einsum(\'ijk,jim\',A,B).shape Out[827]: (5, 4) In [828]: np.allclose(np.einsum(\'ijk,jim\',A,B),np.tensordot(A,B,[(0,1),(1,0)])) Out[828]: True\n\nWe could do same with the (1,0) pair. Given the mix of dimension I don\'t think there\'s another combination.\n\nedited Jan 26, 2017 at 10:11\n\nanswered Jan 26, 2017 at 10:04\n\n228k1414 gold badges249249 silver badges373373 bronze badges 3\n\nI still don\'t fully grasp it :(. In the 1st example from the docs they are multiplying element-wise 2 arrays with shape (4,3) and then doing sum over those 2 axes. How could you get that same result using a dot product?\n\n– Brenlla Commented Apr 22, 2019 at 12:30\n\nThe way I could reproduce the 1st result from the docs is by using np.dot on flattened 2-D arrays: for aa in a.T: for bb in b.T: print(aa.ravel().dot(bb.T.ravel()))\n\n– Brenlla Commented Apr 22, 2019 at 13:12\n\nThe einsum equivalent of tensordot with axes=([1,0],[0,1]), is np.einsum(\'ijk,jil->kl\',a,b). This dot also does it: a.T.reshape(5,12).dot(b.reshape(12,2)). The dot is between a (5,12) and (12,2). The a.T puts the 5 first, and also swaps the (3,4) to match b.\n\n– hpaulj Commented Apr 23, 2019 at 2:43\n\nThe answers above are great and helped me a lot in understanding tensordot. But they don\'t show actual math behind operations. That\'s why I did equivalent operations in TF 2 for myself and decided to share them here:\n\na = tf.constant([1,2.]) b = tf.constant([2,3.]) print(f""{tf.tensordot(a, b, 0)}\\t tf.einsum(\'i,j\', a, b)\\t\\t- ((the last 0 axes of a), (the first 0 axes of b))"") print(f""{tf.tensordot(a, b, ((),()))}\\t tf.einsum(\'i,j\', a, b)\\t\\t- ((() axis of a), (() axis of b))"") print(f""{tf.tensordot(b, a, 0)}\\t tf.einsum(\'i,j->ji\', a, b)\\t- ((the last 0 axes of b), (the first 0 axes of a))"") print(f""{tf.tensordot(a, b, 1)}\\t\\t tf.einsum(\'i,i\', a, b)\\t\\t- ((the last 1 axes of a), (the first 1 axes of b))"") print(f""{tf.tensordot(a, b, ((0,), (0,)))}\\t\\t tf.einsum(\'i,i\', a, b)\\t\\t- ((0th axis of a), (0th axis of b))"") print(f""{tf.tensordot(a, b, (0,0))}\\t\\t tf.einsum(\'i,i\', a, b)\\t\\t- ((0th axis of a), (0th axis of b))"") [[2. 3.] [4. 6.]] tf.einsum(\'i,j\', a, b) - ((the last 0 axes of a), (the first 0 axes of b)) [[2. 3.] [4. 6.]] tf.einsum(\'i,j\', a, b) - ((() axis of a), (() axis of b)) [[2. 4.] [3. 6.]] tf.einsum(\'i,j->ji\', a, b) - ((the last 0 axes of b), (the first 0 axes of a)) 8.0 tf.einsum(\'i,i\', a, b) - ((the last 1 axes of a), (the first 1 axes of b)) 8.0 tf.einsum(\'i,i\', a, b) - ((0th axis of a), (0th axis of b)) 8.0 tf.einsum(\'i,i\', a, b) - ((0th axis of a), (0th axis of b))\n\nAnd for (2,2) shape:\n\na = tf.constant([[1,2], [-2,3.]]) b = tf.constant([[-2,3], [0,4.]]) print(f""{tf.tensordot(a, b, 0)}\\t tf.einsum(\'ij,kl\', a, b)\\t- ((the last 0 axes of a), (the first 0 axes of b))"") print(f""{tf.tensordot(a, b, (0,0))}\\t tf.einsum(\'ij,ik\', a, b)\\t- ((0th axis of a), (0th axis of b))"") print(f""{tf.tensordot(a, b, (0,1))}\\t tf.einsum(\'ij,ki\', a, b)\\t- ((0th axis of a), (1st axis of b))"") print(f""{tf.tensordot(a, b, 1)}\\t tf.matmul(a, b)\\t\\t- ((the last 1 axes of a), (the first 1 axes of b))"") print(f""{tf.tensordot(a, b, ((1,), (0,)))}\\t tf.einsum(\'ij,jk\', a, b)\\t- ((1st axis of a), (0th axis of b))"") print(f""{tf.tensordot(a, b, (1, 0))}\\t tf.matmul(a, b)\\t\\t- ((1st axis of a), (0th axis of b))"") print(f""{tf.tensordot(a, b, 2)}\\t tf.reduce_sum(tf.multiply(a, b))\\t- ((the last 2 axes of a), (the first 2 axes of b))"") print(f""{tf.tensordot(a, b, ((0,1), (0,1)))}\\t tf.einsum(\'ij,ij->\', a, b)\\t\\t- ((0th axis of a, 1st axis of a), (0th axis of b, 1st axis of b))"") [[[[-2. 3.] [ 0. 4.]] [[-4. 6.] [ 0. 8.]]] [[[ 4. -6.] [-0. -8.]] [[-6. 9.] [ 0. 12.]]]] tf.einsum(\'ij,kl\', a, b) - ((the last 0 axes of a), (the first 0 axes of b)) [[-2. -5.] [-4. 18.]] tf.einsum(\'ij,ik\', a, b) - ((0th axis of a), (0th axis of b)) [[-8. -8.] [ 5. 12.]] tf.einsum(\'ij,ki\', a, b) - ((0th axis of a), (1st axis of b)) [[-2. 11.] [ 4. 6.]] tf.matmul(a, b) - ((the last 1 axes of a), (the first 1 axes of b)) [[-2. 11.] [ 4. 6.]] tf.einsum(\'ij,jk\', a, b) - ((1st axis of a), (0th axis of b)) [[-2. 11.] [ 4. 6.]] tf.matmul(a, b) - ((1st axis of a), (0th axis of b)) 16.0 tf.reduce_sum(tf.multiply(a, b)) - ((the last 2 axes of a), (the first 2 axes of b)) 16.0 tf.einsum(\'ij,ij->\', a, b) - ((0th axis of a, 1st axis of a), (0th axis of b, 1st axis of b))\n\nedited Nov 22, 2019 at 19:09\n\nanswered Nov 22, 2019 at 19:02\n\n55411 gold badge1111 silver badges2626 bronze badges\n\nBesides the above answers, it would be easier to understand np.tensordot if breaking it down into nested loops:\n\nimport numpy as np a = np.arange(24).reshape(2,3,4) b = np.arange(30).reshape(3,5,2)\n\narray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]])\n\narray([[[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9]], [[10, 11], [12, 13], [14, 15], [16, 17], [18, 19]], [[20, 21], [22, 23], [24, 25], [26, 27], [28, 29]]])\n\nc = np.tensordot(a, b, axes=([0,1],[2,0]))\n\nwhich is equivalent to\n\nc = np.zeros((4,5)) for i in range(4): for j in range(5): for p in range(2): for q in range(3): c[i,j] += a[p,q,i] * b[q,j,p]\n\nThe axes having the same dimensions (here are 2 and 3) in both tensors can be reduced by the sum over them. And the parameter axes=([0,1],[2,0]) is same as axes=([1,0],[0,2]).\n\narray([[ 808, 928, 1048, 1168, 1288], [ 871, 1003, 1135, 1267, 1399], [ 934, 1078, 1222, 1366, 1510], [ 997, 1153, 1309, 1465, 1621]])\n\nanswered Jun 24, 2023 at 20:03\n\n67166 silver badges66 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ndot-product or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n5 How does numpy.tensordot function works step-by-step?\n\n5 Matrix multiplication on 4D numpy arrays\n\n4 Numpy - multiple 3d array with a 2d array\n\n4 How to avoid Kronecker product in NumPy computations\n\n4 Looping over np.einsum many times... Is there a faster way?\n\n2 Sum of dot products\n\n3 Compute weighted sum of 3D and 2D NumPy arrays\n\n2 numpy - matrix multiple 3x3 and 100x100x3 arrays?\n\n2 Broadcasting np.dot vs tf.matmul for tensor-matrix multiplication (Shape must be rank 2 but is rank 3 error)\n\n2 Dot product broadcasting on a 3D grid with numpy\n\nSee more linked questions\n\n0 Python: Using Tensordot for tensor x matrix multiplication\n\n2 numpy tensordot related issue\n\n2 tensor dot operation in python\n\n5 How does numpy.tensordot function works step-by-step?\n\n0 Tensordot confusion\n\n0 Numpy: How to properly perform dot products over tensors\n\n0 Tensordot for vectors in numpy\n\n2 Vectorized computation of numpys tensor dot\n\n0 understanding numpy np.tensordot\n\n1 Matrix by Vector multiplication using numpy dot product\n\nHot Network Questions\n\nExplain why ""Calf"" is the answer to ""Ice mass broken off a little lower?""\n\n为什么字形不同的两个字「骨」的编码相同（从而是一个字）？\n\nVacuum flucutuations = local entanglement between quantum fields?\n\nHow shall I find the device of a phone\'s storage so that I can mount it in Linux?\n\nCan I improve my code to extract permutation cycles from a (n,2) matrix?\n\nSeeing edges where there are no edges\n\nIs it possible to abstract an ElGamal encryption for EC and Discrete Log by using a Group Law?\n\nCan you Constrain the Noise Modifier?\n\nControl Blending Mode of the Color Mix node through the custom driver\n\nDoes Justice Sotomayor\'s ""Seal Team 6"" example, in and of itself, explicitly give the President the authority to execute opponents? If not, why not?\n\nWhy was this a draw?\n\nWhy are responses to an attack in a cycling race immediate?\n\nconfidence intervals for proportions containing a theoretically impossible value (zero)\n\nWould this telescope be capable to detect Middle Ages Civilization?\n\nInteresting NT Question With AP and GCD.\n\nWhat is the reason for using decibels to measure sound?\n\nHow to manage talkover in meetings?\n\nIs ""able to find related code by searching keyword \'MyClass \'(variable declaration)"" a reason to avoid using auto in c++?\n\n11 trees in 6 rows with 4 trees in each row\n\nfirefox returns odd results for file:/// or file:///tmp\n\nSiunitx: spread table content accross page\n\nEverything has a tiny nuclear reactor in it. How much of a concern are illegal nuclear bombs?\n\nMeasure by mass vs. \'Spooned and Leveled\' more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_0', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nUnderstanding tensordot\n\nAsked 7 years, 5 months ago\n\nAfter I learned how to use einsum, I am now trying to understand how np.tensordot works.\n\nHowever, I am a little bit lost especially regarding the various possibilities for the parameter axes.\n\nTo understand it, as I have never practiced tensor calculus, I use the following example:\n\nA = np.random.randint(2, size=(2, 3, 5)) B = np.random.randint(2, size=(3, 2, 4))\n\nIn this case, what are the different possible np.tensordot and how would you compute it manually?\n\nImprove this question\n\nedited Dec 26, 2017 at 8:47\n\n60k1515 gold badges168168 silver badges156156 bronze badges\n\nasked Jan 26, 2017 at 9:22\n\n2,30122 gold badges2424 silver badges4646 bronze badges 0\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe idea with tensordot is pretty simple - We input the arrays and the respective axes along which the sum-reductions are intended. The axes that take part in sum-reduction are removed in the output and all of the remaining axes from the input arrays are spread-out as different axes in the output keeping the order in which the input arrays are fed.\n\nLet\'s look at few sample cases with one and two axes of sum-reductions and also swap the input places and see how the order is kept in the output.\n\nI. One axis of sum-reduction\n\nIn [7]: A = np.random.randint(2, size=(2, 6, 5)) ...: B = np.random.randint(2, size=(3, 2, 4)) ...:\n\nIn [9]: np.tensordot(A, B, axes=((0),(1))).shape Out[9]: (6, 5, 3, 4) A : (2, 6, 5) -> reduction of axis=0 B : (3, 2, 4) -> reduction of axis=1 Output : `(2, 6, 5)`, `(3, 2, 4)` ===(2 gone)==> `(6,5)` + `(3,4)` => `(6,5,3,4)`\n\nCase #2 (same as case #1 but the inputs are fed swapped):\n\nIn [8]: np.tensordot(B, A, axes=((1),(0))).shape Out[8]: (3, 4, 6, 5) B : (3, 2, 4) -> reduction of axis=1 A : (2, 6, 5) -> reduction of axis=0 Output : `(3, 2, 4)`, `(2, 6, 5)` ===(2 gone)==> `(3,4)` + `(6,5)` => `(3,4,6,5)`.\n\nII. Two axes of sum-reduction\n\nIn [11]: A = np.random.randint(2, size=(2, 3, 5)) ...: B = np.random.randint(2, size=(3, 2, 4)) ...:\n\nIn [12]: np.tensordot(A, B, axes=((0,1),(1,0))).shape Out[12]: (5, 4) A : (2, 3, 5) -> reduction of axis=(0,1) B : (3, 2, 4) -> reduction of axis=(1,0) Output : `(2, 3, 5)`, `(3, 2, 4)` ===(2,3 gone)==> `(5)` + `(4)` => `(5,4)`\n\nIn [14]: np.tensordot(B, A, axes=((1,0),(0,1))).shape Out[14]: (4, 5) B : (3, 2, 4) -> reduction of axis=(1,0) A : (2, 3, 5) -> reduction of axis=(0,1) Output : `(3, 2, 4)`, `(2, 3, 5)` ===(2,3 gone)==> `(4)` + `(5)` => `(4,5)`\n\nWe can extend this to as many axes as possible.\n\nanswered Jan 26, 2017 at 10:26\n\n221k1919 gold badges265265 silver badges364364 bronze badges 21\n\nWhat do you exactly mean by sum-reduction?\n\n– floflo29 Commented Jan 26, 2017 at 11:56\n\n@floflo29 Well you might know that matrix-multiplication involves elementwise multiplication keeping an axis aligned and then summation of elements along that common aligned axis. With that summation, we are losing that common axis, which is termed as reduction, so in short sum-reduction.\n\n– Divakar Commented Jan 26, 2017 at 12:05\n\n@BryanHead The only way to reorder the output axes using np.tensordot is to swap the inputs. If it doesn\'t get you your desired one, transpose would be the way to go.\n\n– Divakar Commented May 19, 2017 at 17:47\n\nWould have been better if @Divakar have added the example starting from 1-D tensor along with how each entry is computed. E.g. t1=K.variable([[1,2],[2,3]] ) t2=K.variable([2,3]) print(K.eval(tf.tensordot(t1,t2,axes=0))) output: [[[2. 3.] [4. 6.]] [[4. 6.] [6. 9.]]] Not sure how the output shape is 2x2x2.\n\n– CKM Commented Aug 20, 2019 at 16:31\n\n@dereks The sum-reduction term used in this post is an umbrella term for element-wise multiplication and then sum-reduction. In the context of dot/tensordot, I assumed it would be safe to put it that way. Apologies if that was confusing. Now, with matrix-multiplication you have one axis of sum-reduction (second axis of first array against first axis of second array), whereas in tensordot more than one axes of sum-reduction. The examples presented show how axes are aligned in the input arrays and how the output axes are obtained from those.\n\n– Divakar Commented Nov 21, 2019 at 15:25\n\n | Show 16 more comments\n\ntensordot swaps axes and reshapes the inputs so it can apply np.dot to 2 2d arrays. It then swaps and reshapes back to the target. It may be easier to experiment than to explain. There\'s no special tensor math going on, just extending dot to work in higher dimensions. tensor just means arrays with more than 2d. If you are already comfortable with einsum then it will be simplest compare the results to that.\n\nA sample test, summing on 1 pair of axes\n\nIn [823]: np.tensordot(A,B,[0,1]).shape Out[823]: (3, 5, 3, 4) In [824]: np.einsum(\'ijk,lim\',A,B).shape Out[824]: (3, 5, 3, 4) In [825]: np.allclose(np.einsum(\'ijk,lim\',A,B),np.tensordot(A,B,[0,1])) Out[825]: True\n\nanother, summing on two.\n\nIn [826]: np.tensordot(A,B,[(0,1),(1,0)]).shape Out[826]: (5, 4) In [827]: np.einsum(\'ijk,jim\',A,B).shape Out[827]: (5, 4) In [828]: np.allclose(np.einsum(\'ijk,jim\',A,B),np.tensordot(A,B,[(0,1),(1,0)])) Out[828]: True\n\nWe could do same with the (1,0) pair. Given the mix of dimension I don\'t think there\'s another combination.\n\nedited Jan 26, 2017 at 10:11\n\nanswered Jan 26, 2017 at 10:04\n\n228k1414 gold badges249249 silver badges373373 bronze badges 3\n\nI still don\'t fully grasp it :(. In the 1st example from the docs they are multiplying element-wise 2 arrays with shape (4,3) and then doing sum over those 2 axes. How could you get that same result using a dot product?\n\n– Brenlla Commented Apr 22, 2019 at 12:30\n\nThe way I could reproduce the 1st result from the docs is by using np.dot on flattened 2-D arrays: for aa in a.T: for bb in b.T: print(aa.ravel().dot(bb.T.ravel()))\n\n– Brenlla Commented Apr 22, 2019 at 13:12\n\nThe einsum equivalent of tensordot with axes=([1,0],[0,1]), is np.einsum(\'ijk,jil->kl\',a,b). This dot also does it: a.T.reshape(5,12).dot(b.reshape(12,2)). The dot is between a (5,12) and (12,2). The a.T puts the 5 first, and also swaps the (3,4) to match b.\n\n– hpaulj Commented Apr 23, 2019 at 2:43\n\nThe answers above are great and helped me a lot in understanding tensordot. But they don\'t show actual math behind operations. That\'s why I did equivalent operations in TF 2 for myself and decided to share them here:\n\na = tf.constant([1,2.]) b = tf.constant([2,3.]) print(f""{tf.tensordot(a, b, 0)}\\t tf.einsum(\'i,j\', a, b)\\t\\t- ((the last 0 axes of a), (the first 0 axes of b))"") print(f""{tf.tensordot(a, b, ((),()))}\\t tf.einsum(\'i,j\', a, b)\\t\\t- ((() axis of a), (() axis of b))"") print(f""{tf.tensordot(b, a, 0)}\\t tf.einsum(\'i,j->ji\', a, b)\\t- ((the last 0 axes of b), (the first 0 axes of a))"") print(f""{tf.tensordot(a, b, 1)}\\t\\t tf.einsum(\'i,i\', a, b)\\t\\t- ((the last 1 axes of a), (the first 1 axes of b))"") print(f""{tf.tensordot(a, b, ((0,), (0,)))}\\t\\t tf.einsum(\'i,i\', a, b)\\t\\t- ((0th axis of a), (0th axis of b))"") print(f""{tf.tensordot(a, b, (0,0))}\\t\\t tf.einsum(\'i,i\', a, b)\\t\\t- ((0th axis of a), (0th axis of b))"") [[2. 3.] [4. 6.]] tf.einsum(\'i,j\', a, b) - ((the last 0 axes of a), (the first 0 axes of b)) [[2. 3.] [4. 6.]] tf.einsum(\'i,j\', a, b) - ((() axis of a), (() axis of b)) [[2. 4.] [3. 6.]] tf.einsum(\'i,j->ji\', a, b) - ((the last 0 axes of b), (the first 0 axes of a)) 8.0 tf.einsum(\'i,i\', a, b) - ((the last 1 axes of a), (the first 1 axes of b)) 8.0 tf.einsum(\'i,i\', a, b) - ((0th axis of a), (0th axis of b)) 8.0 tf.einsum(\'i,i\', a, b) - ((0th axis of a), (0th axis of b))\n\nAnd for (2,2) shape:\n\na = tf.constant([[1,2], [-2,3.]]) b = tf.constant([[-2,3], [0,4.]]) print(f""{tf.tensordot(a, b, 0)}\\t tf.einsum(\'ij,kl\', a, b)\\t- ((the last 0 axes of a), (the first 0 axes of b))"") print(f""{tf.tensordot(a, b, (0,0))}\\t tf.einsum(\'ij,ik\', a, b)\\t- ((0th axis of a), (0th axis of b))"") print(f""{tf.tensordot(a, b, (0,1))}\\t tf.einsum(\'ij,ki\', a, b)\\t- ((0th axis of a), (1st axis of b))"") print(f""{tf.tensordot(a, b, 1)}\\t tf.matmul(a, b)\\t\\t- ((the last 1 axes of a), (the first 1 axes of b))"") print(f""{tf.tensordot(a, b, ((1,), (0,)))}\\t tf.einsum(\'ij,jk\', a, b)\\t- ((1st axis of a), (0th axis of b))"") print(f""{tf.tensordot(a, b, (1, 0))}\\t tf.matmul(a, b)\\t\\t- ((1st axis of a), (0th axis of b))"") print(f""{tf.tensordot(a, b, 2)}\\t tf.reduce_sum(tf.multiply(a, b))\\t- ((the last 2 axes of a), (the first 2 axes of b))"") print(f""{tf.tensordot(a, b, ((0,1), (0,1)))}\\t tf.einsum(\'ij,ij->\', a, b)\\t\\t- ((0th axis of a, 1st axis of a), (0th axis of b, 1st axis of b))"") [[[[-2. 3.] [ 0. 4.]] [[-4. 6.] [ 0. 8.]]] [[[ 4. -6.] [-0. -8.]] [[-6. 9.] [ 0. 12.]]]] tf.einsum(\'ij,kl\', a, b) - ((the last 0 axes of a), (the first 0 axes of b)) [[-2. -5.] [-4. 18.]] tf.einsum(\'ij,ik\', a, b) - ((0th axis of a), (0th axis of b)) [[-8. -8.] [ 5. 12.]] tf.einsum(\'ij,ki\', a, b) - ((0th axis of a), (1st axis of b)) [[-2. 11.] [ 4. 6.]] tf.matmul(a, b) - ((the last 1 axes of a), (the first 1 axes of b)) [[-2. 11.] [ 4. 6.]] tf.einsum(\'ij,jk\', a, b) - ((1st axis of a), (0th axis of b)) [[-2. 11.] [ 4. 6.]] tf.matmul(a, b) - ((1st axis of a), (0th axis of b)) 16.0 tf.reduce_sum(tf.multiply(a, b)) - ((the last 2 axes of a), (the first 2 axes of b)) 16.0 tf.einsum(\'ij,ij->\', a, b) - ((0th axis of a, 1st axis of a), (0th axis of b, 1st axis of b))\n\nedited Nov 22, 2019 at 19:09\n\nanswered Nov 22, 2019 at 19:02\n\n55411 gold badge1111 silver badges2626 bronze badges\n\nBesides the above answers, it would be easier to understand np.tensordot if breaking it down into nested loops:\n\nimport numpy as np a = np.arange(24).reshape(2,3,4) b = np.arange(30).reshape(3,5,2)\n\narray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]])\n\narray([[[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9]], [[10, 11], [12, 13], [14, 15], [16, 17], [18, 19]], [[20, 21], [22, 23], [24, 25], [26, 27], [28, 29]]])\n\nc = np.tensordot(a, b, axes=([0,1],[2,0]))\n\nwhich is equivalent to\n\nc = np.zeros((4,5)) for i in range(4): for j in range(5): for p in range(2): for q in range(3): c[i,j] += a[p,q,i] * b[q,j,p]\n\nThe axes having the same dimensions (here are 2 and 3) in both tensors can be reduced by the sum over them. And the parameter axes=([0,1],[2,0]) is same as axes=([1,0],[0,2]).\n\narray([[ 808, 928, 1048, 1168, 1288], [ 871, 1003, 1135, 1267, 1399], [ 934, 1078, 1222, 1366, 1510], [ 997, 1153, 1309, 1465, 1621]])\n\nanswered Jun 24, 2023 at 20:03\n\n67166 silver badges66 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ndot-product or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n5 How does numpy.tensordot function works step-by-step?\n\n5 Matrix multiplication on 4D numpy arrays\n\n4 Numpy - multiple 3d array with a 2d array\n\n4 How to avoid Kronecker product in NumPy computations\n\n4 Looping over np.einsum many times... Is there a faster way?\n\n2 Sum of dot products\n\n3 Compute weighted sum of 3D and 2D NumPy arrays\n\n2 numpy - matrix multiple 3x3 and 100x100x3 arrays?\n\n2 Broadcasting np.dot vs tf.matmul for tensor-matrix multiplication (Shape must be rank 2 but is rank 3 error)\n\n2 Dot product broadcasting on a 3D grid with numpy\n\nSee more linked questions\n\n0 Python: Using Tensordot for tensor x matrix multiplication\n\n2 numpy tensordot related issue\n\n2 tensor dot operation in python\n\n5 How does numpy.tensordot function works step-by-step?\n\n0 Tensordot confusion\n\n0 Numpy: How to properly perform dot products over tensors\n\n0 Tensordot for vectors in numpy\n\n2 Vectorized computation of numpys tensor dot\n\n0 understanding numpy np.tensordot\n\n1 Matrix by Vector multiplication using numpy dot product\n\nHot Network Questions\n\nExplain why ""Calf"" is the answer to ""Ice mass broken off a little lower?""\n\n为什么字形不同的两个字「骨」的编码相同（从而是一个字）？\n\nVacuum flucutuations = local entanglement between quantum fields?\n\nHow shall I find the device of a phone\'s storage so that I can mount it in Linux?\n\nCan I improve my code to extract permutation cycles from a (n,2) matrix?\n\nSeeing edges where there are no edges\n\nIs it possible to abstract an ElGamal encryption for EC and Discrete Log by using a Group Law?\n\nCan you Constrain the Noise Modifier?\n\nControl Blending Mode of the Color Mix node through the custom driver\n\nDoes Justice Sotomayor\'s ""Seal Team 6"" example, in and of itself, explicitly give the President the authority to execute opponents? If not, why not?\n\nWhy was this a draw?\n\nWhy are responses to an attack in a cycling race immediate?\n\nconfidence intervals for proportions containing a theoretically impossible value (zero)\n\nWould this telescope be capable to detect Middle Ages Civilization?\n\nInteresting NT Question With AP and GCD.\n\nWhat is the reason for using decibels to measure sound?\n\nHow to manage talkover in meetings?\n\nIs ""able to find related code by searching keyword \'MyClass \'(variable declaration)"" a reason to avoid using auto in c++?\n\n11 trees in 6 rows with 4 trees in each row\n\nfirefox returns odd results for file:/// or file:///tmp\n\nSiunitx: spread table content accross page\n\nEverything has a tiny nuclear reactor in it. How much of a concern are illegal nuclear bombs?\n\nMeasure by mass vs. \'Spooned and Leveled\' more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-05T07:37:52', 'title': 'python - Understanding tensordot - Stack Overflow', 'url': 'https://stackoverflow.com/questions/41870228/understanding-tensordot'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow does numpy.tensordot function works step-by-step?\n\nAsked 5 years, 10 months ago\n\nModified 4 years, 7 months ago\n\nI am new to numpy, So I have some problem visualizing the working of the numpy.tensordot() function. According to the documentation of tensordot, the axes are passed in the arguments where axes=0 or 1 represents a normal matrix multiplication whereas axes=2 represents Contraction.\n\nCan somebody please explain on how the multiplication would proceed with the given examples?\n\nExample-1: a=[1,1] b=[2,2] for axes=0,1 why does it throw an error for axes=2? Example-2: a=[[1,1],[1,1]] b=[[2,2],[2,2]] for axes=0,1,2\n\nedited Aug 23, 2018 at 16:40\n\n10.4k44 gold badges3737 silver badges4242 bronze badges\n\nasked Aug 23, 2018 at 15:42\n\n2,74344 gold badges2424 silver badges3737 bronze badges 2\n\nThis might be relevant - Understanding tensordott.\n\n– Divakar Commented Aug 23, 2018 at 15:44\n\nI saw this post, but it only explains on how to calculate the order of the final matrix instead of the actual multiplication taking place underneath.\n\n– john mich Commented Aug 23, 2018 at 15:45\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nEdit: The initial focus of this answer was on the case where axes is a tuple, specifying one or more axes for each argument. This use allows us to perform variations on the conventional dot, especially for arrays larger than 2d (my answer in the linked question also, https://stackoverflow.com/a/41870980/901925). Axes as scalar is a special case, that gets translated into the tuples version. So at its core it is still a dot product.\n\nIn [235]: a=[1,1]; b=[2,2]\n\na and b are lists; tensordot turns them into arrays.\n\nIn [236]: np.tensordot(a,b,(0,0)) Out[236]: array(4)\n\nSince they are both 1d arrays, we specify the axis values as 0.\n\nIf we try to specify 1:\n\nIn [237]: np.tensordot(a,b,(0,1)) --------------------------------------------------------------------------- 1282 else: 1283 for k in range(na): -> 1284 if as_[axes_a[k]] != bs[axes_b[k]]: 1285 equal = False 1286 break IndexError: tuple index out of range\n\nIt is checking whether size of axis 0 of a matches the size of axis 1 of b. But since b is 1d, it can\'t check that.\n\nIn [239]: np.array(a).shape[0] Out[239]: 2 In [240]: np.array(b).shape[1] IndexError: tuple index out of range\n\nYour second example is 2d arrays:\n\nIn [242]: a=np.array([[1,1],[1,1]]); b=np.array([[2,2],[2,2]])\n\nSpecifying the last axis of a and first of b (second to the last), produces the conventional matrix (dot) product:\n\nIn [243]: np.tensordot(a,b,(1,0)) Out[243]: array([[4, 4], [4, 4]]) In [244]: a.dot(b) Out[244]: array([[4, 4], [4, 4]])\n\nBetter diagnostic values:\n\nIn [250]: a=np.array([[1,2],[3,4]]); b=np.array([[2,3],[2,1]]) In [251]: np.tensordot(a,b,(1,0)) Out[251]: array([[ 6, 5], [14, 13]]) In [252]: np.dot(a,b) Out[252]: array([[ 6, 5], [14, 13]]) In [253]: np.tensordot(a,b,(0,1)) Out[253]: array([[11, 5], [16, 8]]) In [254]: np.dot(b,a) # same numbers, different layout Out[254]: array([[11, 16], [ 5, 8]]) In [255]: np.dot(b,a).T Out[255]: array([[11, 5], [16, 8]])\n\nIn [256]: np.tensordot(a,b,(0,0)) In [257]: np.dot(a.T,b)\n\n(0,1,2) for axis is plain wrong. The axis parameter should be 2 numbers, or 2 tuples, corresponding to the 2 arguments.\n\nThe basic processing in tensordot is to transpose and reshape the inputs so it can then pass the results to np.dot for a conventional (last of a, second to the last of b) matrix product.\n\nIf my reading of tensordot code is right, the axes parameter is converted into two lists with:\n\ndef foo(axes): try: iter(axes) except Exception: axes_a = list(range(-axes, 0)) axes_b = list(range(0, axes)) else: axes_a, axes_b = axes try: na = len(axes_a) axes_a = list(axes_a) except TypeError: axes_a = [axes_a] na = 1 try: nb = len(axes_b) axes_b = list(axes_b) except TypeError: axes_b = [axes_b] nb = 1 return axes_a, axes_b\n\nFor scalar values, 0,1,2 the results are:\n\nIn [281]: foo(0) Out[281]: ([], []) In [282]: foo(1) Out[282]: ([-1], [0]) In [283]: foo(2) Out[283]: ([-2, -1], [0, 1])\n\naxes=1 is the same as specifying in a tuple:\n\nIn [284]: foo((-1,0)) Out[284]: ([-1], [0])\n\nIn [285]: foo(((-2,-1),(0,1))) Out[285]: ([-2, -1], [0, 1])\n\nWith my latest example, axes=2 is the same as specifying a dot over all axes of the 2 arrays:\n\nIn [287]: np.tensordot(a,b,axes=2) Out[287]: array(18) In [288]: np.tensordot(a,b,axes=((0,1),(0,1))) Out[288]: array(18)\n\nThis is the same as doing dot on the flattened, 1d, views of the arrays:\n\nIn [289]: np.dot(a.ravel(), b.ravel()) Out[289]: 18\n\nI already demonstrated the conventional dot product for these arrays, the axes=1 case.\n\naxes=0 is the same as axes=((),()), no summation axes for the 2 arrays:\n\nIn [292]: foo(((),())) Out[292]: ([], [])\n\nnp.tensordot(a,b,((),())) is the same as np.tensordot(a,b,axes=0)\n\nIt\'s the -2 in the foo(2) translation that\'s giving you problems when the input arrays are 1d. axes=1 is the \'contraction\' for 1d array. In other words, don\'t take the word descriptions in the documentation too literally. They just attempt to describe the action of the code; they aren\'t a formal specification.\n\nI think the axes specifications for einsum are clearer and more powerful. Here are the equivalents for 0,1,2\n\nIn [295]: np.einsum(\'ij,kl\',a,b) Out[295]: array([[[[ 2, 3], [ 2, 1]], [[ 4, 6], [ 4, 2]]], [[[ 6, 9], [ 6, 3]], [[ 8, 12], [ 8, 4]]]]) In [296]: np.einsum(\'ij,jk\',a,b) Out[296]: array([[ 6, 5], [14, 13]]) In [297]: np.einsum(\'ij,ij\',a,b) Out[297]: 18\n\nThe axes=0 case, is equivalent to:\n\nnp.dot(a[:,:,None],b[:,None,:])\n\nIt adds a new last axis and new 2nd to last axis, and does a conventional dot product summing over those. But we usually do this sort of \'outer\' multiplication with broadcasting:\n\na[:,:,None,None]*b[None,None,:,:]\n\nWhile the use of 0,1,2 for axes is interesting, it really doesn\'t add new calculation power. The tuple form of axes is more powerful and useful.\n\ncode summary (big steps)\n\n1 - translate axes into axes_a and axes_b as excerpted in the above foo function\n\n2 - make a and b into arrays, and get the shape and ndim\n\n3 - check for matching size on axes that will be summed (contracted)\n\n4 - construct a newshape_a and newaxes_a; same for b (complex step)\n\n5 - at = a.transpose(newaxes_a).reshape(newshape_a); same for b\n\n6 - res = dot(at, bt)\n\n7 - reshape the res to desired return shape\n\n5 and 6 are the calculation core. 4 is conceptually the most complex step. For all axes values the calculation is the same, a dot product, but the setup varies.\n\nWhile the documentation only mentions 0,1,2 for scalar axes, the code isn\'t restricted to those values\n\nIn [331]: foo(3) Out[331]: ([-3, -2, -1], [0, 1, 2])\n\nIf the inputs are 3, axes=3 should work:\n\nIn [330]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=3) Out[330]: array(8.)\n\nIn [325]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=0).shape Out[325]: (2, 2, 2, 2, 2, 2) In [326]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=1).shape Out[326]: (2, 2, 2, 2) In [327]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=2).shape Out[327]: (2, 2) In [328]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=3).shape Out[328]: ()\n\nand if the inputs are 0d, axes=0 works (axes = 1 does not):\n\nIn [335]: np.tensordot(2,3, axes=0) Out[335]: array(6)\n\nCan you explain this?\n\nIn [363]: np.tensordot(np.ones((4,2,3)),np.ones((2,3,4)),axes=2).shape Out[363]: (4, 4)\n\nI\'ve played around with other scalar axes values for 3d arrays. While it is possible to come up with pairs of shapes that work, the more explicit tuple axes values is easier to work with. The 0,1,2 options are short cuts that only work for special cases. The tuple approach is much easier to use - though I still prefer the einsum notation.\n\nedited Aug 24, 2018 at 2:25\n\nanswered Aug 23, 2018 at 16:10\n\n228k1414 gold badges249249 silver badges373373 bronze badges\n\nExample 1-0: np.tensordot([1, 1], [2, 2], axes=0)\n\nIn this case, a and b both have a single axis and have shape (2,).\n\nThe axes=0 argument can be translated to ((the last 0 axes of a), (the first 0 axes of b)), or in this case ((), ()). These are the axes that will be contracted.\n\nAll the other axes will not be contracted. Since each of a and b have a 0-th axis and no others, these are the axes ((0,), (0,)).\n\nThe tensordot operation is then as follows (roughly):\n\n[ [x*y for y in b] # all the non-contraction axes in b for x in a # all the non-contraction axes in a ]\n\nNote that since there are 2 total axes available between a and b and since we\'re contracting 0 of them, the result has 2 axes. The shape is (2,2) since those are the shapes of the respective non-contracted axes in a and b (in order).\n\nExample 1-1: np.tensordot([1, 1], [2, 2], axes=1)\n\nThe axes=1 argument can be translated to ((the last 1 axes of a), (the first 1 axes of b)), or in this case ((0,), (0,)). These are the axes that will be contracted\n\nAll other axes will not be contracted. Since we are already contracting every axis, the remaining axes are ((), ()).\n\nThe tensordot operation is then as follows:\n\nsum( # summing over contraction axis [x*y for x,y in zip(a, b)] # contracted axes must line up )\n\nNote that since we\'re contracting all axes, the result is a scalar (or a 0-shaped tensor). In numpy, you just get a tensor with shape () representing 0 axes rather than an actual scalar.\n\nExample 1-2: np.tensordot([1, 1], [2, 2], axes=2)\n\nThe reason this doesn\'t work is because neither a nor b have two separate axes to contract over.\n\nExample 2-1: np.tensordot([[1,1],[1,1]], [[2,2],[2,2]], axes=1)\n\nI\'m skipping a couple of your examples since they aren\'t quite complicated enough to add more clarity than the first few I don\'t think.\n\nIn this case, a and b both have two axes available (allowing this problem to be a bit more interesting), and they both have shape (2,2).\n\nThe axes=1 argument still represents the last 1 axes of a and the first 1 axes of b, leaving us with ((1,), (0,)). These are the axes that will be contracted over.\n\nThe remaining axes are not contracted and contribute to the shape of the final solution. These are ((0,), (1,)).\n\nWe can then construct the tensordot operation. For the sake of argument, pretend a and b are numpy arrays so that we can use array properties and make the problem cleaner (e.g. b=np.array([[2,2],[2,2]])).\n\n[ [ sum( # summing the contracted indices [x*y for x,y in zip(v,w)] # axis 1 of a and axis 0 of b must line up for the summation ) for w in b.T # iterating over axis 1 of b (i.e. the columns) ] for v in a # iterating over axis 0 of a (i.e. the rows) ]\n\nThe result has shape (a.shape[0], b.shape[1]) since these are the non-contracted axes.\n\nedited Nov 22, 2019 at 17:42\n\nanswered Aug 23, 2018 at 16:42\n\nHans MusgraveHans Musgrave\n\n6,92311 gold badge1818 silver badges3939 bronze badges 2\n\naxes=1 translates to last axis of a and 2n to last of b - the usual dot\n\n– hpaulj Commented Aug 23, 2018 at 18:04\n\n@hpaulj Only for tensors with 2 axes where the 2nd to last is the same as the first. Check the axes documentation under numpy.tensordot\n\n– Hans Musgrave Commented Aug 23, 2018 at 18:36\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nnumpy or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nThe return of Staging Ground to Stack Overflow\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nShould we burninate the [lib] tag?\n\nWhat makes a homepage useful for logged-in users\n\n62 Understanding tensordot\n\n4 Algorithm for tensordot implemented in numba is much slower than numpy\'s\n\n0 Tensordot for vectors in numpy\n\n1 Why does tensordot/reshape not agree with kron?\n\n0 One line einsum functions with ""interleaved"" output indexing impossible to recreate using tensordot?\n\n-1 NumPy Tensordot axes=2\n\n0 Python: Using Tensordot for tensor x matrix multiplication\n\n2 numpy tensordot related issue\n\n2 tensor dot operation in python\n\n1 Numpy tensor: Tensordot over frontal slices of tensor\n\n62 Understanding tensordot\n\n0 Tensordot confusion\n\n0 Tensordot for vectors in numpy\n\n1 How numpy.tensordot command works?and what is the meaning of summing over axis in this command?\n\n0 understanding numpy np.tensordot\n\n2 NumPy tensordot grouped calculation\n\nHot Network Questions\n\nWhat\'s the best way to line up equals on a line of text?\n\nAre the justifications of an irrational society false?\n\nHow do Blok and the other astronaut do a spacewalk to repair the ship? Didn’t they already land?\n\nNew faculty position – expectation to change research direction\n\nAre there substantive differences between the different approaches to ""size issues"" in category theory?\n\nIs it legal to discriminate on marital status for car insurance/pensions etc.?\n\nA puzzle from YOU to ME ;)\n\nWhat is the translation of lawfare in French?\n\nException handling: \'catch\' without explicit \'try\'\n\nHow does the router know to send packets to a VM on bridge mode?\n\nWhat does ""acceptable"" refer to in Romans 12:2?\n\nWhen was the last time a chess rule was modified?\n\nHow to find your contract and employee handbook in the UK?\n\nIs Good and Evil relative or absolute?\n\nWhat is the significance of the word choice of Numbers 18:1?\n\nCut and replace every Nth character on every row\n\nCan a unique position be deduced if pieces are replaced by checkers (can see piece color but not type)\n\nif people are bred like dogs, what can be achieved?\n\nWould a ""plug and play"" prosthetic be possible?\n\nIs this professor being unnecessarily harsh or did I actually make a mistake?\n\nWhat is this weapon used in The Peacemaker?\n\nIsn\'t it problematic to look at the data to decide to use a parametric vs. non-parametric test?\n\nShould mail addresses for logins be stored hashed to minimize impact of data loss?\n\nWhy can Ethernet NICs bridge to VirtualBox and most Wi-Fi NICs don\'t? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_3', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow does numpy.tensordot function works step-by-step?\n\nAsked 5 years, 10 months ago\n\nModified 4 years, 7 months ago\n\nI am new to numpy, So I have some problem visualizing the working of the numpy.tensordot() function. According to the documentation of tensordot, the axes are passed in the arguments where axes=0 or 1 represents a normal matrix multiplication whereas axes=2 represents Contraction.\n\nCan somebody please explain on how the multiplication would proceed with the given examples?\n\nExample-1: a=[1,1] b=[2,2] for axes=0,1 why does it throw an error for axes=2? Example-2: a=[[1,1],[1,1]] b=[[2,2],[2,2]] for axes=0,1,2\n\nedited Aug 23, 2018 at 16:40\n\n10.4k44 gold badges3737 silver badges4242 bronze badges\n\nasked Aug 23, 2018 at 15:42\n\n2,74344 gold badges2424 silver badges3737 bronze badges 2\n\nThis might be relevant - Understanding tensordott.\n\n– Divakar Commented Aug 23, 2018 at 15:44\n\nI saw this post, but it only explains on how to calculate the order of the final matrix instead of the actual multiplication taking place underneath.\n\n– john mich Commented Aug 23, 2018 at 15:45\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nEdit: The initial focus of this answer was on the case where axes is a tuple, specifying one or more axes for each argument. This use allows us to perform variations on the conventional dot, especially for arrays larger than 2d (my answer in the linked question also, https://stackoverflow.com/a/41870980/901925). Axes as scalar is a special case, that gets translated into the tuples version. So at its core it is still a dot product.\n\nIn [235]: a=[1,1]; b=[2,2]\n\na and b are lists; tensordot turns them into arrays.\n\nIn [236]: np.tensordot(a,b,(0,0)) Out[236]: array(4)\n\nSince they are both 1d arrays, we specify the axis values as 0.\n\nIf we try to specify 1:\n\nIn [237]: np.tensordot(a,b,(0,1)) --------------------------------------------------------------------------- 1282 else: 1283 for k in range(na): -> 1284 if as_[axes_a[k]] != bs[axes_b[k]]: 1285 equal = False 1286 break IndexError: tuple index out of range\n\nIt is checking whether size of axis 0 of a matches the size of axis 1 of b. But since b is 1d, it can\'t check that.\n\nIn [239]: np.array(a).shape[0] Out[239]: 2 In [240]: np.array(b).shape[1] IndexError: tuple index out of range\n\nYour second example is 2d arrays:\n\nIn [242]: a=np.array([[1,1],[1,1]]); b=np.array([[2,2],[2,2]])\n\nSpecifying the last axis of a and first of b (second to the last), produces the conventional matrix (dot) product:\n\nIn [243]: np.tensordot(a,b,(1,0)) Out[243]: array([[4, 4], [4, 4]]) In [244]: a.dot(b) Out[244]: array([[4, 4], [4, 4]])\n\nBetter diagnostic values:\n\nIn [250]: a=np.array([[1,2],[3,4]]); b=np.array([[2,3],[2,1]]) In [251]: np.tensordot(a,b,(1,0)) Out[251]: array([[ 6, 5], [14, 13]]) In [252]: np.dot(a,b) Out[252]: array([[ 6, 5], [14, 13]]) In [253]: np.tensordot(a,b,(0,1)) Out[253]: array([[11, 5], [16, 8]]) In [254]: np.dot(b,a) # same numbers, different layout Out[254]: array([[11, 16], [ 5, 8]]) In [255]: np.dot(b,a).T Out[255]: array([[11, 5], [16, 8]])\n\nIn [256]: np.tensordot(a,b,(0,0)) In [257]: np.dot(a.T,b)\n\n(0,1,2) for axis is plain wrong. The axis parameter should be 2 numbers, or 2 tuples, corresponding to the 2 arguments.\n\nThe basic processing in tensordot is to transpose and reshape the inputs so it can then pass the results to np.dot for a conventional (last of a, second to the last of b) matrix product.\n\nIf my reading of tensordot code is right, the axes parameter is converted into two lists with:\n\ndef foo(axes): try: iter(axes) except Exception: axes_a = list(range(-axes, 0)) axes_b = list(range(0, axes)) else: axes_a, axes_b = axes try: na = len(axes_a) axes_a = list(axes_a) except TypeError: axes_a = [axes_a] na = 1 try: nb = len(axes_b) axes_b = list(axes_b) except TypeError: axes_b = [axes_b] nb = 1 return axes_a, axes_b\n\nFor scalar values, 0,1,2 the results are:\n\nIn [281]: foo(0) Out[281]: ([], []) In [282]: foo(1) Out[282]: ([-1], [0]) In [283]: foo(2) Out[283]: ([-2, -1], [0, 1])\n\naxes=1 is the same as specifying in a tuple:\n\nIn [284]: foo((-1,0)) Out[284]: ([-1], [0])\n\nIn [285]: foo(((-2,-1),(0,1))) Out[285]: ([-2, -1], [0, 1])\n\nWith my latest example, axes=2 is the same as specifying a dot over all axes of the 2 arrays:\n\nIn [287]: np.tensordot(a,b,axes=2) Out[287]: array(18) In [288]: np.tensordot(a,b,axes=((0,1),(0,1))) Out[288]: array(18)\n\nThis is the same as doing dot on the flattened, 1d, views of the arrays:\n\nIn [289]: np.dot(a.ravel(), b.ravel()) Out[289]: 18\n\nI already demonstrated the conventional dot product for these arrays, the axes=1 case.\n\naxes=0 is the same as axes=((),()), no summation axes for the 2 arrays:\n\nIn [292]: foo(((),())) Out[292]: ([], [])\n\nnp.tensordot(a,b,((),())) is the same as np.tensordot(a,b,axes=0)\n\nIt\'s the -2 in the foo(2) translation that\'s giving you problems when the input arrays are 1d. axes=1 is the \'contraction\' for 1d array. In other words, don\'t take the word descriptions in the documentation too literally. They just attempt to describe the action of the code; they aren\'t a formal specification.\n\nI think the axes specifications for einsum are clearer and more powerful. Here are the equivalents for 0,1,2\n\nIn [295]: np.einsum(\'ij,kl\',a,b) Out[295]: array([[[[ 2, 3], [ 2, 1]], [[ 4, 6], [ 4, 2]]], [[[ 6, 9], [ 6, 3]], [[ 8, 12], [ 8, 4]]]]) In [296]: np.einsum(\'ij,jk\',a,b) Out[296]: array([[ 6, 5], [14, 13]]) In [297]: np.einsum(\'ij,ij\',a,b) Out[297]: 18\n\nThe axes=0 case, is equivalent to:\n\nnp.dot(a[:,:,None],b[:,None,:])\n\nIt adds a new last axis and new 2nd to last axis, and does a conventional dot product summing over those. But we usually do this sort of \'outer\' multiplication with broadcasting:\n\na[:,:,None,None]*b[None,None,:,:]\n\nWhile the use of 0,1,2 for axes is interesting, it really doesn\'t add new calculation power. The tuple form of axes is more powerful and useful.\n\ncode summary (big steps)\n\n1 - translate axes into axes_a and axes_b as excerpted in the above foo function\n\n2 - make a and b into arrays, and get the shape and ndim\n\n3 - check for matching size on axes that will be summed (contracted)\n\n4 - construct a newshape_a and newaxes_a; same for b (complex step)\n\n5 - at = a.transpose(newaxes_a).reshape(newshape_a); same for b\n\n6 - res = dot(at, bt)\n\n7 - reshape the res to desired return shape\n\n5 and 6 are the calculation core. 4 is conceptually the most complex step. For all axes values the calculation is the same, a dot product, but the setup varies.\n\nWhile the documentation only mentions 0,1,2 for scalar axes, the code isn\'t restricted to those values\n\nIn [331]: foo(3) Out[331]: ([-3, -2, -1], [0, 1, 2])\n\nIf the inputs are 3, axes=3 should work:\n\nIn [330]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=3) Out[330]: array(8.)\n\nIn [325]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=0).shape Out[325]: (2, 2, 2, 2, 2, 2) In [326]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=1).shape Out[326]: (2, 2, 2, 2) In [327]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=2).shape Out[327]: (2, 2) In [328]: np.tensordot(np.ones((2,2,2)), np.ones((2,2,2)), axes=3).shape Out[328]: ()\n\nand if the inputs are 0d, axes=0 works (axes = 1 does not):\n\nIn [335]: np.tensordot(2,3, axes=0) Out[335]: array(6)\n\nCan you explain this?\n\nIn [363]: np.tensordot(np.ones((4,2,3)),np.ones((2,3,4)),axes=2).shape Out[363]: (4, 4)\n\nI\'ve played around with other scalar axes values for 3d arrays. While it is possible to come up with pairs of shapes that work, the more explicit tuple axes values is easier to work with. The 0,1,2 options are short cuts that only work for special cases. The tuple approach is much easier to use - though I still prefer the einsum notation.\n\nedited Aug 24, 2018 at 2:25\n\nanswered Aug 23, 2018 at 16:10\n\n228k1414 gold badges249249 silver badges373373 bronze badges\n\nExample 1-0: np.tensordot([1, 1], [2, 2], axes=0)\n\nIn this case, a and b both have a single axis and have shape (2,).\n\nThe axes=0 argument can be translated to ((the last 0 axes of a), (the first 0 axes of b)), or in this case ((), ()). These are the axes that will be contracted.\n\nAll the other axes will not be contracted. Since each of a and b have a 0-th axis and no others, these are the axes ((0,), (0,)).\n\nThe tensordot operation is then as follows (roughly):\n\n[ [x*y for y in b] # all the non-contraction axes in b for x in a # all the non-contraction axes in a ]\n\nNote that since there are 2 total axes available between a and b and since we\'re contracting 0 of them, the result has 2 axes. The shape is (2,2) since those are the shapes of the respective non-contracted axes in a and b (in order).\n\nExample 1-1: np.tensordot([1, 1], [2, 2], axes=1)\n\nThe axes=1 argument can be translated to ((the last 1 axes of a), (the first 1 axes of b)), or in this case ((0,), (0,)). These are the axes that will be contracted\n\nAll other axes will not be contracted. Since we are already contracting every axis, the remaining axes are ((), ()).\n\nThe tensordot operation is then as follows:\n\nsum( # summing over contraction axis [x*y for x,y in zip(a, b)] # contracted axes must line up )\n\nNote that since we\'re contracting all axes, the result is a scalar (or a 0-shaped tensor). In numpy, you just get a tensor with shape () representing 0 axes rather than an actual scalar.\n\nExample 1-2: np.tensordot([1, 1], [2, 2], axes=2)\n\nThe reason this doesn\'t work is because neither a nor b have two separate axes to contract over.\n\nExample 2-1: np.tensordot([[1,1],[1,1]], [[2,2],[2,2]], axes=1)\n\nI\'m skipping a couple of your examples since they aren\'t quite complicated enough to add more clarity than the first few I don\'t think.\n\nIn this case, a and b both have two axes available (allowing this problem to be a bit more interesting), and they both have shape (2,2).\n\nThe axes=1 argument still represents the last 1 axes of a and the first 1 axes of b, leaving us with ((1,), (0,)). These are the axes that will be contracted over.\n\nThe remaining axes are not contracted and contribute to the shape of the final solution. These are ((0,), (1,)).\n\nWe can then construct the tensordot operation. For the sake of argument, pretend a and b are numpy arrays so that we can use array properties and make the problem cleaner (e.g. b=np.array([[2,2],[2,2]])).\n\n[ [ sum( # summing the contracted indices [x*y for x,y in zip(v,w)] # axis 1 of a and axis 0 of b must line up for the summation ) for w in b.T # iterating over axis 1 of b (i.e. the columns) ] for v in a # iterating over axis 0 of a (i.e. the rows) ]\n\nThe result has shape (a.shape[0], b.shape[1]) since these are the non-contracted axes.\n\nedited Nov 22, 2019 at 17:42\n\nanswered Aug 23, 2018 at 16:42\n\nHans MusgraveHans Musgrave\n\n6,92311 gold badge1818 silver badges3939 bronze badges 2\n\naxes=1 translates to last axis of a and 2n to last of b - the usual dot\n\n– hpaulj Commented Aug 23, 2018 at 18:04\n\n@hpaulj Only for tensors with 2 axes where the 2nd to last is the same as the first. Check the axes documentation under numpy.tensordot\n\n– Hans Musgrave Commented Aug 23, 2018 at 18:36\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nnumpy or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nThe return of Staging Ground to Stack Overflow\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nShould we burninate the [lib] tag?\n\nWhat makes a homepage useful for logged-in users\n\n62 Understanding tensordot\n\n4 Algorithm for tensordot implemented in numba is much slower than numpy\'s\n\n0 Tensordot for vectors in numpy\n\n1 Why does tensordot/reshape not agree with kron?\n\n0 One line einsum functions with ""interleaved"" output indexing impossible to recreate using tensordot?\n\n-1 NumPy Tensordot axes=2\n\n0 Python: Using Tensordot for tensor x matrix multiplication\n\n2 numpy tensordot related issue\n\n2 tensor dot operation in python\n\n1 Numpy tensor: Tensordot over frontal slices of tensor\n\n62 Understanding tensordot\n\n0 Tensordot confusion\n\n0 Tensordot for vectors in numpy\n\n1 How numpy.tensordot command works?and what is the meaning of summing over axis in this command?\n\n0 understanding numpy np.tensordot\n\n2 NumPy tensordot grouped calculation\n\nHot Network Questions\n\nWhat\'s the best way to line up equals on a line of text?\n\nAre the justifications of an irrational society false?\n\nHow do Blok and the other astronaut do a spacewalk to repair the ship? Didn’t they already land?\n\nNew faculty position – expectation to change research direction\n\nAre there substantive differences between the different approaches to ""size issues"" in category theory?\n\nIs it legal to discriminate on marital status for car insurance/pensions etc.?\n\nA puzzle from YOU to ME ;)\n\nWhat is the translation of lawfare in French?\n\nException handling: \'catch\' without explicit \'try\'\n\nHow does the router know to send packets to a VM on bridge mode?\n\nWhat does ""acceptable"" refer to in Romans 12:2?\n\nWhen was the last time a chess rule was modified?\n\nHow to find your contract and employee handbook in the UK?\n\nIs Good and Evil relative or absolute?\n\nWhat is the significance of the word choice of Numbers 18:1?\n\nCut and replace every Nth character on every row\n\nCan a unique position be deduced if pieces are replaced by checkers (can see piece color but not type)\n\nif people are bred like dogs, what can be achieved?\n\nWould a ""plug and play"" prosthetic be possible?\n\nIs this professor being unnecessarily harsh or did I actually make a mistake?\n\nWhat is this weapon used in The Peacemaker?\n\nIsn\'t it problematic to look at the data to decide to use a parametric vs. non-parametric test?\n\nShould mail addresses for logins be stored hashed to minimize impact of data loss?\n\nWhy can Ethernet NICs bridge to VirtualBox and most Wi-Fi NICs don\'t? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-06-27T17:12:44', 'title': 'python - How does numpy.tensordot function works step-by-step? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/51989572/how-does-numpy-tensordot-function-works-step-by-step'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nTensorflow: \'axis\' argument in dot product\n\nAsked 3 years, 4 months ago\n\nModified 3 years, 4 months ago\n\nCan someone show me the way I should use the axis argument in tf.tensordot?\n\nI read the documentation but it was complicated and I\'m still confused. I saw another question that asks about axis in tf.one_hot and in the answers were some good insights about the matter, but that didn\'t help me with tf.tensordot. I thought you can give me some insights on this too.\n\nFor example, I know I can dot product a vector and a tensor like this:\n\nmy_vector = tf.random.uniform(shape=[n]) my_tensor = tf.random.uniform(shape=[m, n]) dp = tf.tensordot(my_tensor, my_vector, 1)\n\nBut when I batch them and add one dimension to them to be of the shape (b, n) and (b, m, n) to obtain a (b, m, 1), now I don\'t know how to dot product every batch.\n\nImprove this question\n\nedited Nov 13, 2020 at 20:25\n\nasked Nov 13, 2020 at 18:46\n\n3,54477 gold badges4141 silver badges6969 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe operation that you want to do cannot be done (in an effective way) with tf.tensordot. There is, however, a dedicated function for that operation, tf.linalg.matvec, which will work with batches out of the box. And you can also do the same thing with tf.einsum, like tf.einsum(\'bmn,bn->bm\', my_tensors, my_vectors).\n\nWith respect to tf.tensordot, in general it computes an ""all vs all"" product of the two given tensors, but matching and reducing some axes. When no axes are given (you have to explicitly pass axes=[[], []] to do this), it creates a tensor with the dimensions of both inputs concatenated. So, if you have my_tensors with shape (b, m, n) and my_vectors with shape (b, n) and you do:\n\nres = tf.tensordot(my_tensors, my_vectors, axes=[[], []])\n\nYou get res with shape (b, m, n, b, n), such that res[p, q, r, s, t] == my_tensors[p, q, r] * my_vectors[s, t].\n\nThe axes argument is used to specify dimensions in the input tensors that are ""matched"". Values along matched axes are multiplied and summed (like a dot product), so those matched dimensions are reduced from the output. axes can take two different forms:\n\nIf it is a single integer, N then the last N dimensions of the first parameter are matched against the first N dimensions of b. In your example, that corresponds to the dimensions with n elements in my_tensor and my_vector.\n\nIf it is a list, it must contain two sublists, axes_a and axes_b, each with the same number N of integers. In this form, you are explicitly indicating which dimensions of the given values are matched. So, in your example, you could have passed axes=[[1], [0]], which means ""match the dimension 1 of the first parameter (my_tensor) to the dimension 0 of the second parameter (my_vector)"".\n\nIf you have now my_tensors with shape (b, m, n) and my_vectors with shape (b, n), then you would want to match the dimension 2 of the first one to the dimension 1 of the second one, so you could pass axes=[[2], [1]]. However, that will give you a result res with shape (b, m, b) such that res[i, :, j] is the product of matrix my_tensors[i] and vector my_vectors[j]. You could take then only the results that you want (those where i == j), with something more or less convoluted like tf.transpose(tf.linalg.diag_part(tf.transpose(res, [1, 0, 2]))), but you would be doing far more computation than you need to get the same result.\n\nedited Nov 16, 2020 at 10:35\n\nanswered Nov 13, 2020 at 19:05\n\n59.1k77 gold badges8080 silver badges125125 bronze badges 4\n\nYour answer is great, thank you. But the result of your dot product is (b, m, b) shape. I want (b, m, 1) shape. How can I do that?\n\n– Peyman Nov 13, 2020 at 20:06\n\n@Peymam You\'re right, I\'m sorry, I didn\'t think that right... I\'ll fix the answer when I\'m back in my computer. At least matvec and einsum should work...\n\n– jdehesa Nov 14, 2020 at 13:01\n\n@Peyman I updated the answer, sorry for the confusion.\n\n– jdehesa Nov 16, 2020 at 10:36\n\nThank you so much. I\'ve found out tf.keras.backend.batch_dot() does the same thing but assuming the first dimension as batch.\n\n– Peyman Nov 16, 2020 at 12:45\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ndot-product or ask your own question.\n\nWhy the creator of Node.js® created a new JavaScript runtime\n\nIs AI making your code worse?\n\nChanging how community leadership works on Stack Exchange: a proposal and...\n\nShifting the data dump schedule: A proposal\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n2024 Community Moderator Election Results\n\n27 In TensorFlow, what is the argument \'axis\' in the function \'tf.one_hot\'\n\n17 Dot product along third axis\n\n8 Multiplying along an arbitrary axis?\n\n30 Dot product of two vectors in tensorflow\n\n1 Why TensorFlow\'s division hasn\'t have axis parameter?\n\n2 Tensor multiply along axis in tensorflow\n\n1 Move axis in tensorflow\n\n0 tensorflow axis definition starts from 0 or 1?\n\n4 Numpy dot product along specific axes\n\n1 How to select numpy tensordot axes\n\n1 TensorFlow multiplication along axis\n\nHot Network Questions\n\nWhy aren\'t helicopter blades tapered?\n\nHow to give feedback on a badly reviewed PR\n\nWhen asked when you can start a job, is it implied they will give you a few days notice?\n\nIdiomatic phrase for ""review passed once you did the following""\n\nA group generated by three rational points has actually two generators?\n\nWhy are nitric acid and hydrogen combinations not used as rocket fuel?\n\nSeeking a Polynomial Time Algorithm for Balanced Weight Assignment to Nodes in a Tree\n\nHow to measure height of inline equations\n\nAre any countries claiming that their humanitarian aid for Gaza is being declined by Israel?\n\nIs there a literary term for a word which looks completely \'flat\' when spelled out in lowercase?\n\nCan a UK photographer legally photograph a US wedding if the wedding day shoot itself is unpaid?\n\nIs there a simple geometric proof of why two simple harmonic oscillators draw a circle?\n\nCooling fan direction\n\nThe effectiveness of ""honors"" classes\n\nCalculating pressure inside a bag using the ideal gas laws\n\nNumbing Tonic and knocked out\n\nClausen–Scholze\'s Theorem 9.1 of Analytic.pdf, in view of light condensed sets, AKA is the Liquid Tensor Experiment easier now?\n\nDoing a (Math) PhD abroad vs the same university\n\nFor stabilizer codes, is a certain logical operation unique？\n\nAt 45°N, how come the Sun rises in a northeasterly direction?\n\nHow to prevent super intelligent humans from existing in a world defined by genetic engineering?\n\nCoin flip game: HH vs HT in a sequence of flips\n\nHow to record an old Foscam system’s live feed locally?\n\n9-hour 40-mn layover in Haneda Airport more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_1', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nTensorflow: \'axis\' argument in dot product\n\nAsked 3 years, 4 months ago\n\nModified 3 years, 4 months ago\n\nCan someone show me the way I should use the axis argument in tf.tensordot?\n\nI read the documentation but it was complicated and I\'m still confused. I saw another question that asks about axis in tf.one_hot and in the answers were some good insights about the matter, but that didn\'t help me with tf.tensordot. I thought you can give me some insights on this too.\n\nFor example, I know I can dot product a vector and a tensor like this:\n\nmy_vector = tf.random.uniform(shape=[n]) my_tensor = tf.random.uniform(shape=[m, n]) dp = tf.tensordot(my_tensor, my_vector, 1)\n\nBut when I batch them and add one dimension to them to be of the shape (b, n) and (b, m, n) to obtain a (b, m, 1), now I don\'t know how to dot product every batch.\n\nImprove this question\n\nedited Nov 13, 2020 at 20:25\n\nasked Nov 13, 2020 at 18:46\n\n3,54477 gold badges4141 silver badges6969 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe operation that you want to do cannot be done (in an effective way) with tf.tensordot. There is, however, a dedicated function for that operation, tf.linalg.matvec, which will work with batches out of the box. And you can also do the same thing with tf.einsum, like tf.einsum(\'bmn,bn->bm\', my_tensors, my_vectors).\n\nWith respect to tf.tensordot, in general it computes an ""all vs all"" product of the two given tensors, but matching and reducing some axes. When no axes are given (you have to explicitly pass axes=[[], []] to do this), it creates a tensor with the dimensions of both inputs concatenated. So, if you have my_tensors with shape (b, m, n) and my_vectors with shape (b, n) and you do:\n\nres = tf.tensordot(my_tensors, my_vectors, axes=[[], []])\n\nYou get res with shape (b, m, n, b, n), such that res[p, q, r, s, t] == my_tensors[p, q, r] * my_vectors[s, t].\n\nThe axes argument is used to specify dimensions in the input tensors that are ""matched"". Values along matched axes are multiplied and summed (like a dot product), so those matched dimensions are reduced from the output. axes can take two different forms:\n\nIf it is a single integer, N then the last N dimensions of the first parameter are matched against the first N dimensions of b. In your example, that corresponds to the dimensions with n elements in my_tensor and my_vector.\n\nIf it is a list, it must contain two sublists, axes_a and axes_b, each with the same number N of integers. In this form, you are explicitly indicating which dimensions of the given values are matched. So, in your example, you could have passed axes=[[1], [0]], which means ""match the dimension 1 of the first parameter (my_tensor) to the dimension 0 of the second parameter (my_vector)"".\n\nIf you have now my_tensors with shape (b, m, n) and my_vectors with shape (b, n), then you would want to match the dimension 2 of the first one to the dimension 1 of the second one, so you could pass axes=[[2], [1]]. However, that will give you a result res with shape (b, m, b) such that res[i, :, j] is the product of matrix my_tensors[i] and vector my_vectors[j]. You could take then only the results that you want (those where i == j), with something more or less convoluted like tf.transpose(tf.linalg.diag_part(tf.transpose(res, [1, 0, 2]))), but you would be doing far more computation than you need to get the same result.\n\nedited Nov 16, 2020 at 10:35\n\nanswered Nov 13, 2020 at 19:05\n\n59.1k77 gold badges8080 silver badges125125 bronze badges 4\n\nYour answer is great, thank you. But the result of your dot product is (b, m, b) shape. I want (b, m, 1) shape. How can I do that?\n\n– Peyman Nov 13, 2020 at 20:06\n\n@Peymam You\'re right, I\'m sorry, I didn\'t think that right... I\'ll fix the answer when I\'m back in my computer. At least matvec and einsum should work...\n\n– jdehesa Nov 14, 2020 at 13:01\n\n@Peyman I updated the answer, sorry for the confusion.\n\n– jdehesa Nov 16, 2020 at 10:36\n\nThank you so much. I\'ve found out tf.keras.backend.batch_dot() does the same thing but assuming the first dimension as batch.\n\n– Peyman Nov 16, 2020 at 12:45\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ndot-product or ask your own question.\n\nWhy the creator of Node.js® created a new JavaScript runtime\n\nIs AI making your code worse?\n\nChanging how community leadership works on Stack Exchange: a proposal and...\n\nShifting the data dump schedule: A proposal\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n2024 Community Moderator Election Results\n\n27 In TensorFlow, what is the argument \'axis\' in the function \'tf.one_hot\'\n\n17 Dot product along third axis\n\n8 Multiplying along an arbitrary axis?\n\n30 Dot product of two vectors in tensorflow\n\n1 Why TensorFlow\'s division hasn\'t have axis parameter?\n\n2 Tensor multiply along axis in tensorflow\n\n1 Move axis in tensorflow\n\n0 tensorflow axis definition starts from 0 or 1?\n\n4 Numpy dot product along specific axes\n\n1 How to select numpy tensordot axes\n\n1 TensorFlow multiplication along axis\n\nHot Network Questions\n\nWhy aren\'t helicopter blades tapered?\n\nHow to give feedback on a badly reviewed PR\n\nWhen asked when you can start a job, is it implied they will give you a few days notice?\n\nIdiomatic phrase for ""review passed once you did the following""\n\nA group generated by three rational points has actually two generators?\n\nWhy are nitric acid and hydrogen combinations not used as rocket fuel?\n\nSeeking a Polynomial Time Algorithm for Balanced Weight Assignment to Nodes in a Tree\n\nHow to measure height of inline equations\n\nAre any countries claiming that their humanitarian aid for Gaza is being declined by Israel?\n\nIs there a literary term for a word which looks completely \'flat\' when spelled out in lowercase?\n\nCan a UK photographer legally photograph a US wedding if the wedding day shoot itself is unpaid?\n\nIs there a simple geometric proof of why two simple harmonic oscillators draw a circle?\n\nCooling fan direction\n\nThe effectiveness of ""honors"" classes\n\nCalculating pressure inside a bag using the ideal gas laws\n\nNumbing Tonic and knocked out\n\nClausen–Scholze\'s Theorem 9.1 of Analytic.pdf, in view of light condensed sets, AKA is the Liquid Tensor Experiment easier now?\n\nDoing a (Math) PhD abroad vs the same university\n\nFor stabilizer codes, is a certain logical operation unique？\n\nAt 45°N, how come the Sun rises in a northeasterly direction?\n\nHow to prevent super intelligent humans from existing in a world defined by genetic engineering?\n\nCoin flip game: HH vs HT in a sequence of flips\n\nHow to record an old Foscam system’s live feed locally?\n\n9-hour 40-mn layover in Haneda Airport more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-03-22T15:40:42', 'title': ""python - Tensorflow: 'axis' argument in dot product - Stack Overflow"", 'url': 'https://stackoverflow.com/questions/64826405/tensorflow-axis-argument-in-dot-product'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nProduct of PyTorch tensors along arbitrary axes à la NumPy\'s `tensordot`\n\nAsked 5 years, 8 months ago\n\nModified 3 years, 1 month ago\n\nNumPy provides the very useful tensordot function. It allows you to compute the product of two ndarrays along any axes (whose sizes match). I\'m having a hard time finding anything similar in PyTorch. mm works only with 2D arrays, and matmul has some undesirable broadcasting properties.\n\nAm I missing something? Am I really meant to reshape the arrays to mimic the products I want using mm?\n\nmatrix-multiplication\n\nImprove this question\n\nedited Jul 12, 2018 at 10:56\n\n14.9k55 gold badges6060 silver badges7070 bronze badges\n\nasked Jul 10, 2018 at 13:25\n\n11.3k44 gold badges4343 silver badges7676 bronze badges 3\n\n@M.Deckers: How could it? It doesn\'t even take arguments to specify which axes to take the product along.\n\n– gspr Jul 10, 2018 at 13:52\n\nIt is not available at the moment but currently discussed here.\n\n– McLawrence Jul 10, 2018 at 13:52\n\n@McLawrence: Thanks, that\'s very clarifying!\n\n– gspr Jul 10, 2018 at 14:00\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe original answer is totally correct, but as an update, Pytorch now supports tensordot natively. Same call signature as numpy but change axes to dims.\n\nimport torch import numpy as np a = np.arange(36.).reshape(3,4,3) b = np.arange(24.).reshape(4,3,2) c = np.tensordot(a, b, axes=([1,0],[0,1])) print(c) # [[ 2640. 2838.] [ 2772. 2982.] [ 2904. 3126.]] a = torch.from_numpy(a) b = torch.from_numpy(b) c = torch.tensordot(a, b, dims=([1,0],[0,1])) print(c) # tensor([[ 2640., 2838.], [ 2772., 2982.], [ 2904., 3126.]], dtype=torch.float64)\n\nedited Feb 23, 2021 at 22:14\n\nanswered Jan 22, 2021 at 20:56\n\nJacob SternJacob Stern\n\n4,16744 gold badges3434 silver badges5959 bronze badges\n\nAs mentioned by @McLawrence, this feature is being currently discussed (issue thread).\n\nIn the meantime, you could consider torch.einsum(), e.g.:\n\nimport torch import numpy as np a = np.arange(36.).reshape(3,4,3) b = np.arange(24.).reshape(4,3,2) c = np.tensordot(a, b, axes=([1,0],[0,1])) print(c) # [[ 2640. 2838.] [ 2772. 2982.] [ 2904. 3126.]] a = torch.from_numpy(a) b = torch.from_numpy(b) c = torch.einsum(""ijk,jil->kl"", (a, b)) print(c) # tensor([[ 2640., 2838.], [ 2772., 2982.], [ 2904., 3126.]], dtype=torch.float64)\n\nanswered Jul 11, 2018 at 14:12\n\nbenjaminplanchebenjaminplanche\n\n14.9k55 gold badges6060 silver badges7070 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nmatrix-multiplication\n\ndot-product or ask your own question.\n\nWill antitrust suits benefit developers?\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n0 Dot Product of Tensors with Different Shapes in PyTorch\n\n38 PyTorch element-wise product of vectors / matrices / tensors\n\n0 Matrix-vector multiplication for only one dimension in a tensor\n\n3 Multi-dimensional tensor dot product in pytorch\n\n8 PyTorch: Row-wise Dot Product\n\n0 Multidimensional tensor product in PyTorch\n\n2 PyTorch - Tensors multiplication along new dimension\n\n0 Element-wise matrix vector multiplication\n\n2 Pytorch dot product across rows from different arrays\n\n0 Pytorch matrix multiplication\n\nHot Network Questions\n\nEarliest English-language fiction about humanoid ""piloted"" by tiny people\n\nIs Ubuntu affected by the xz backdoor compromise?\n\nMaking sense of binary logistic regression results/Interpreting odd ratio in r\n\nDoes a direct flight emit more CO2 than a flight with a layover?\n\nAre we only justified in holding beliefs that are supported by evidence susceptible to peer review, leading to substantial intersubjective consensus?\n\nWhat were the most common software distribution formats for MacOS Classic?\n\nDoes the ""Sniper Firing Squad"" analogy undermine the Anthropic Principle objection to the fine-tuning argument for God\'s existence?\n\nIs there any way to change Sitecore sxa default structure in html without writing custom code. I am getting extra wrapper div row on page\n\nCan a declension form of adjective be used predicatively？\n\nIs this a correct implementation of Comparator?\n\nC++ - Secretary Problem using dynamic programming\n\nShould I cut or detach a ground wire for a light fixture to remove it?\n\nDoes a carbon fiber fuselage suffer wear from pressurization/depressurization?\n\nSkiplagging consequences\n\nWhen bikepacking solo, how to handle luggage when shopping?\n\nCircles crossing every cell of an 8x8 grid\n\nknow the conjugation of a verb\n\nHow to make Homebrew directories writeable by multiple users?\n\nWhat shape should the PCB edge have for this USB connector?\n\n20 inch wheels and spoke tension measurement tool\n\nFlip counters in a grid so that they alternate in color\n\nMedieval fantasy movie where a sorceress disguises herself as the queen to have a child by the king\n\nHow does energy become sound?\n\nCalculating the Volume with three given expression more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_2', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nProduct of PyTorch tensors along arbitrary axes à la NumPy\'s `tensordot`\n\nAsked 5 years, 8 months ago\n\nModified 3 years, 1 month ago\n\nNumPy provides the very useful tensordot function. It allows you to compute the product of two ndarrays along any axes (whose sizes match). I\'m having a hard time finding anything similar in PyTorch. mm works only with 2D arrays, and matmul has some undesirable broadcasting properties.\n\nAm I missing something? Am I really meant to reshape the arrays to mimic the products I want using mm?\n\nmatrix-multiplication\n\nImprove this question\n\nedited Jul 12, 2018 at 10:56\n\n14.9k55 gold badges6060 silver badges7070 bronze badges\n\nasked Jul 10, 2018 at 13:25\n\n11.3k44 gold badges4343 silver badges7676 bronze badges 3\n\n@M.Deckers: How could it? It doesn\'t even take arguments to specify which axes to take the product along.\n\n– gspr Jul 10, 2018 at 13:52\n\nIt is not available at the moment but currently discussed here.\n\n– McLawrence Jul 10, 2018 at 13:52\n\n@McLawrence: Thanks, that\'s very clarifying!\n\n– gspr Jul 10, 2018 at 14:00\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe original answer is totally correct, but as an update, Pytorch now supports tensordot natively. Same call signature as numpy but change axes to dims.\n\nimport torch import numpy as np a = np.arange(36.).reshape(3,4,3) b = np.arange(24.).reshape(4,3,2) c = np.tensordot(a, b, axes=([1,0],[0,1])) print(c) # [[ 2640. 2838.] [ 2772. 2982.] [ 2904. 3126.]] a = torch.from_numpy(a) b = torch.from_numpy(b) c = torch.tensordot(a, b, dims=([1,0],[0,1])) print(c) # tensor([[ 2640., 2838.], [ 2772., 2982.], [ 2904., 3126.]], dtype=torch.float64)\n\nedited Feb 23, 2021 at 22:14\n\nanswered Jan 22, 2021 at 20:56\n\nJacob SternJacob Stern\n\n4,16744 gold badges3434 silver badges5959 bronze badges\n\nAs mentioned by @McLawrence, this feature is being currently discussed (issue thread).\n\nIn the meantime, you could consider torch.einsum(), e.g.:\n\nimport torch import numpy as np a = np.arange(36.).reshape(3,4,3) b = np.arange(24.).reshape(4,3,2) c = np.tensordot(a, b, axes=([1,0],[0,1])) print(c) # [[ 2640. 2838.] [ 2772. 2982.] [ 2904. 3126.]] a = torch.from_numpy(a) b = torch.from_numpy(b) c = torch.einsum(""ijk,jil->kl"", (a, b)) print(c) # tensor([[ 2640., 2838.], [ 2772., 2982.], [ 2904., 3126.]], dtype=torch.float64)\n\nanswered Jul 11, 2018 at 14:12\n\nbenjaminplanchebenjaminplanche\n\n14.9k55 gold badges6060 silver badges7070 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nmatrix-multiplication\n\ndot-product or ask your own question.\n\nWill antitrust suits benefit developers?\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n0 Dot Product of Tensors with Different Shapes in PyTorch\n\n38 PyTorch element-wise product of vectors / matrices / tensors\n\n0 Matrix-vector multiplication for only one dimension in a tensor\n\n3 Multi-dimensional tensor dot product in pytorch\n\n8 PyTorch: Row-wise Dot Product\n\n0 Multidimensional tensor product in PyTorch\n\n2 PyTorch - Tensors multiplication along new dimension\n\n0 Element-wise matrix vector multiplication\n\n2 Pytorch dot product across rows from different arrays\n\n0 Pytorch matrix multiplication\n\nHot Network Questions\n\nEarliest English-language fiction about humanoid ""piloted"" by tiny people\n\nIs Ubuntu affected by the xz backdoor compromise?\n\nMaking sense of binary logistic regression results/Interpreting odd ratio in r\n\nDoes a direct flight emit more CO2 than a flight with a layover?\n\nAre we only justified in holding beliefs that are supported by evidence susceptible to peer review, leading to substantial intersubjective consensus?\n\nWhat were the most common software distribution formats for MacOS Classic?\n\nDoes the ""Sniper Firing Squad"" analogy undermine the Anthropic Principle objection to the fine-tuning argument for God\'s existence?\n\nIs there any way to change Sitecore sxa default structure in html without writing custom code. I am getting extra wrapper div row on page\n\nCan a declension form of adjective be used predicatively？\n\nIs this a correct implementation of Comparator?\n\nC++ - Secretary Problem using dynamic programming\n\nShould I cut or detach a ground wire for a light fixture to remove it?\n\nDoes a carbon fiber fuselage suffer wear from pressurization/depressurization?\n\nSkiplagging consequences\n\nWhen bikepacking solo, how to handle luggage when shopping?\n\nCircles crossing every cell of an 8x8 grid\n\nknow the conjugation of a verb\n\nHow to make Homebrew directories writeable by multiple users?\n\nWhat shape should the PCB edge have for this USB connector?\n\n20 inch wheels and spoke tension measurement tool\n\nFlip counters in a grid so that they alternate in color\n\nMedieval fantasy movie where a sorceress disguises herself as the queen to have a child by the king\n\nHow does energy become sound?\n\nCalculating the Volume with three given expression more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-04-01T23:13:57', 'title': ""matrix multiplication - Product of PyTorch tensors along arbitrary axes à la NumPy's `tensordot` - Stack Overflow"", 'url': 'https://stackoverflow.com/questions/51266507/product-of-pytorch-tensors-along-arbitrary-axes-%C3%A0-la-numpys-tensordot'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nnumpy einsum/tensordot with shared non-contracted axis\n\nAsked 2 years, 5 months ago\n\nModified 2 years, 4 months ago\n\nSuppose I have two arrays:\n\nimport numpy as np a = np.random.randn(32, 6, 6, 20, 64, 3, 3) b = np.random.randn(20, 128, 64, 3, 3)\n\nand want to sum over the last 3 axes, and keep the shared axis. The output dimension should be (32,6,6,20,128). Notice here the axis with 20 is shared in both a and b. Let\'s call this axis the ""group"" axis.\n\nI have two methods for this task: The first one is just a simple einsum:\n\ndef method1(a, b): return np.einsum(\'NHWgihw, goihw -> NHWgo\', a, b, optimize=True) # output shape:(32,6,6,20,128)\n\nIn the second method I loop through group dimension and use einsum/tensordot to compute the result for each group dimension, then stack the results:\n\ndef method2(a, b): result = [] for g in range(b.shape[0]): # loop through each group dimension # result.append(np.tensordot(a[..., g, :, :, :], b[g, ...], axes=((-3,-2,-1),(-3,-2,-1)))) result.append(np.einsum(\'NHWihw, oihw -> NHWo\', a[..., g, :, :, :], b[g, ...], optimize=True)) # output shape:(32,6,6,128) return np.stack(result, axis=-2) # output shape:(32,6,6,20,128)\n\nhere\'s the timing for both methods in my jupyter notebook: we can see the second method with a loop is faster than the first method.\n\nHow come method1 is that much slower? It doesn\'t compute more things.\n\nIs there a more efficient way without using loops? (I\'m a bit reluctant to use loops because they are slow in python)\n\nThanks for any help!\n\nImprove this question\n\nedited Feb 20, 2022 at 12:39\n\n46.7k66 gold badges3737 silver badges6969 bronze badges\n\nasked Jan 29, 2022 at 15:46\n\n79955 silver badges1515 bronze badges 8\n\nA few iterations with a complicated expression can be faster. Here it\'s only 20. But I suspect this can be done with matmul with some reshaping and transpose. tensordot is just dot with reshaping and transposes.\n\n– hpaulj Commented Jan 29, 2022 at 16:09\n\nReshape and transpose so that \'NHWgihw, goihw -> NHWgo\' becomes \'NHWgX,gXo->NHWgo\' and use matmul.\n\n– hpaulj Commented Jan 29, 2022 at 16:50\n\nIn method2, einsum dispatches it to BLAS when you set optimize=True(which is highly optimised than native einsum) . I would say your method2 is already well optimised both in speed and memory.\n\n– user10289025 Commented Jan 29, 2022 at 17:01\n\nI got a matmul working, but it times about the same as the single einsum. Looks like this is a well known case where a modest number of iterations on one dimension is faster, due to memory management issues. I may post some timings later.\n\n– hpaulj Commented Jan 29, 2022 at 17:25\n\nHi @hpaulj I don\'t think reshape is a good idea because a and b in my actual case are views of smaller arrays, using reshape will increase memory usage\n\n– Sam-gege Commented Jan 29, 2022 at 17:35\n\n | Show 3 more comments\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nAs pointed out by @Murali in the comments, method1 is not very efficient because it does not succeed to use a BLAS calls as opposed to method2 which does. In fact, np.einsum is quite good in method1 since it compute the result sequentially while method2 mostly runs in parallel thanks to OpenBLAS (used by Numpy on most machines). That being said, method2 is sub-optimal since it does not fully use the available cores (parts of the computation are done sequentially) and appear not to use the cache efficiently. On my 6-core machine, it barely use 50% of all the cores.\n\nFaster implementation\n\nOne solution to speed up this computation is to write an highly-optimized Numba parallel code for this.\n\nFirst of all, a semi-naive implementation is to use many for loops to compute the Einstein summation and reshape the input/output arrays so Numba can better optimize the code (eg. unrolling, use of SIMD instructions). Here is the result:\n\n@nb.njit(\'float64[:,:,:,:,::1](float64[:,:,:,:,:,:,::1], float64[:,:,:,:,::1])\') def compute(a, b): sN, sH, sW, sg, si, sh, sw = a.shape so = b.shape[1] assert b.shape == (sg, so, si, sh, sw) ra = a.reshape(sN*sH*sW, sg, si*sh*sw) rb = b.reshape(sg, so, si*sh*sw) out = np.empty((sN*sH*sW, sg, so), dtype=np.float64) for NHW in range(sN*sH*sW): for g in range(sg): for o in range(so): s = 0.0 # Reduction for ihw in range(si*sh*sw): s += ra[NHW, g, ihw] * rb[g, o, ihw] out[NHW, g, o] = s return out.reshape((sN, sH, sW, sg, so))\n\nNote that the input array are assumed to be contiguous. If this is not the case, please consider performing a copy (which is cheap compared to the computation).\n\nWhile the above code works, it is far from being efficient. Here are some improvements that can be performed:\n\nrun the outermost NHW loop in parallel;\n\nuse the Numba flag fastmath=True. This flag is unsafe if the input data contains special values like NaN or +inf/-inf. However, this flag help compiler to generate a much faster code using SIMD instructions (this is not possible otherwise since IEEE-754 floating-point operations are not associative);\n\nswap the NHW-based loop and g-based loop results in better performance since it improves cache-locality (rb is more likely to fit in the last-level cache of mainstream CPUs whereas it would likely in fetched from the RAM otherwise);\n\nmake use of register blocking so to saturate better SIMD computing units of the processor and reduce the pressure on the memory hierarchy;\n\nmake use of tiling by splitting the o-based loop so rb can almost fully be read from lower-level caches (eg. L1 or L2).\n\nAll these improvements except the last one are implemented in the following code:\n\n@nb.njit(\'float64[:,:,:,:,::1](float64[:,:,:,:,:,:,::1], float64[:,:,:,:,::1])\', parallel=True, fastmath=True) def method3(a, b): sN, sH, sW, sg, si, sh, sw = a.shape so = b.shape[1] assert b.shape == (sg, so, si, sh, sw) ra = a.reshape(sN*sH*sW, sg, si*sh*sw) rb = b.reshape(sg, so, si*sh*sw) out = np.zeros((sN*sH*sW, sg, so), dtype=np.float64) for g in range(sg): for k in nb.prange((sN*sH*sW)//2): NHW = k*2 so_vect_max = (so // 4) * 4 for o in range(0, so_vect_max, 4): s00 = s01 = s02 = s03 = s10 = s11 = s12 = s13 = 0.0 # Useful since Numba does not optimize well the following loop otherwise ra_row0 = ra[NHW+0, g, :] ra_row1 = ra[NHW+1, g, :] rb_row0 = rb[g, o+0, :] rb_row1 = rb[g, o+1, :] rb_row2 = rb[g, o+2, :] rb_row3 = rb[g, o+3, :] # Highly-optimized reduction using register blocking for ihw in range(si*sh*sw): ra_0 = ra_row0[ihw] ra_1 = ra_row1[ihw] rb_0 = rb_row0[ihw] rb_1 = rb_row1[ihw] rb_2 = rb_row2[ihw] rb_3 = rb_row3[ihw] s00 += ra_0 * rb_0; s01 += ra_0 * rb_1 s02 += ra_0 * rb_2; s03 += ra_0 * rb_3 s10 += ra_1 * rb_0; s11 += ra_1 * rb_1 s12 += ra_1 * rb_2; s13 += ra_1 * rb_3 out[NHW+0, g, o+0] = s00; out[NHW+0, g, o+1] = s01 out[NHW+0, g, o+2] = s02; out[NHW+0, g, o+3] = s03 out[NHW+1, g, o+0] = s10; out[NHW+1, g, o+1] = s11 out[NHW+1, g, o+2] = s12; out[NHW+1, g, o+3] = s13 # Remaining part for `o` for o in range(so_vect_max, so): for ihw in range(si*sh*sw): out[NHW, g, o] += ra[NHW, g, ihw] * rb[g, o, ihw] out[NHW+1, g, o] += ra[NHW+1, g, ihw] * rb[g, o, ihw] # Remaining part for `k` if (sN*sH*sW) % 2 == 1: k = sN*sH*sW - 1 for o in range(so): for ihw in range(si*sh*sw): out[k, g, o] += ra[k, g, ihw] * rb[g, o, ihw] return out.reshape((sN, sH, sW, sg, so))\n\nThis code is much more complex and uglier but also far more efficient. I did not implemented the tiling optimization since it would make the code even less readable. However, it should results in a significantly faster code on many-core processors (especially the ones with a small L2/L3 cache).\n\nHere are performance results on my i5-9600KF 6-core processor:\n\nmethod1: 816 ms method2: 104 ms method3: 40 ms Theoretical optimal: 9 ms (optimistic lower bound)\n\nThe code is about 2.7 faster than method2. There is a room for improvements since the optimal time is about 4 time better than method3.\n\nThe main reason why Numba does not generate a fast code comes from the underlying JIT which fail to efficiently vectorize the loop. Implementing the tiling strategy should slightly improves the execution time very close to the optimal one. The tiling strategy is critical for much bigger arrays. This is especially true if so is much bigger.\n\nIf you want a faster implementation you certainly need to write a C/C++ native code using directly SIMD instrinsics (which are unfortunately not portable) or a SIMD library (eg. XSIMD).\n\nIf you want an even faster implementation, then you need to use a faster hardware (with more cores) or a more dedicated one. Server-based GPUs (ie. not the one of personal computers) not should be able to speed up a lot such a computation since your input is small, clearly compute-bound and massively makes use of FMA floating-point operations. A first start is to try cupy.einsum.\n\nUnder the hood: low-level analysis\n\nIn order to understand why method1 is not faster, I checked the executed code. Here is the main loop:\n\n1a0:┌─→; Part of the reduction (see below) │ movapd xmm0,XMMWORD PTR [rdi-0x1000] │ │ ; Decrement the number of loop cycle │ sub r9,0x8 │ │ ; Prefetch items so to reduce the impact │ ; of the latency of reading from the RAM. │ prefetcht0 BYTE PTR [r8] │ prefetcht0 BYTE PTR [rdi] │ │ ; Part of the reduction (see below) │ mulpd xmm0,XMMWORD PTR [r8-0x1000] │ │ ; Increment iterator for the two arrays │ add rdi,0x40 │ add r8,0x40 │ │ ; Main computational part: │ ; reduction using add+mul SSE2 instructions │ addpd xmm1,xmm0 <--- Slow │ movapd xmm0,XMMWORD PTR [rdi-0x1030] │ mulpd xmm0,XMMWORD PTR [r8-0x1030] │ addpd xmm1,xmm0 <--- Slow │ movapd xmm0,XMMWORD PTR [rdi-0x1020] │ mulpd xmm0,XMMWORD PTR [r8-0x1020] │ addpd xmm0,xmm1 <--- Slow │ movapd xmm1,XMMWORD PTR [rdi-0x1010] │ mulpd xmm1,XMMWORD PTR [r8-0x1010] │ addpd xmm1,xmm0 <--- Slow │ │ ; Is the loop over? │ ; If not, jump to the beginning of the loop. ├──cmp r9,0x7 └──jg 1a0\n\nIt turns out that Numpy use the SSE2 instruction set (which is available on all x86-64 processors). However, my machine, like almost all relatively recent processor support the AVX instruction set which can compute twice more items at once per instruction. My machine also support fuse-multiply add instructions (FMA) that are twice faster in this case. Moreover, the loop is clearly bounded by the addpd which accumulate the result in mostly the same register. The processor cannot execute them efficiently since an addpd takes few cycle of latency and up to two can be executed at the same time on modern x86-64 processors (which is not possible here since only 1 intruction can perform the accumulation in xmm1 at a time).\n\nHere is the executed code of the main computational part of method2 (dgemm call of OpenBLAS):\n\n6a40:┌─→vbroadcastsd ymm0,QWORD PTR [rsi-0x60] │ vbroadcastsd ymm1,QWORD PTR [rsi-0x58] │ vbroadcastsd ymm2,QWORD PTR [rsi-0x50] │ vbroadcastsd ymm3,QWORD PTR [rsi-0x48] │ vfmadd231pd ymm4,ymm0,YMMWORD PTR [rdi-0x80] │ vfmadd231pd ymm5,ymm1,YMMWORD PTR [rdi-0x60] │ vbroadcastsd ymm0,QWORD PTR [rsi-0x40] │ vbroadcastsd ymm1,QWORD PTR [rsi-0x38] │ vfmadd231pd ymm6,ymm2,YMMWORD PTR [rdi-0x40] │ vfmadd231pd ymm7,ymm3,YMMWORD PTR [rdi-0x20] │ vbroadcastsd ymm2,QWORD PTR [rsi-0x30] │ vbroadcastsd ymm3,QWORD PTR [rsi-0x28] │ vfmadd231pd ymm4,ymm0,YMMWORD PTR [rdi] │ vfmadd231pd ymm5,ymm1,YMMWORD PTR [rdi+0x20] │ vfmadd231pd ymm6,ymm2,YMMWORD PTR [rdi+0x40] │ vfmadd231pd ymm7,ymm3,YMMWORD PTR [rdi+0x60] │ add rsi,0x40 │ add rdi,0x100 ├──dec rax └──jne 6a40\n\nThis loop is far more optimized: it makes use of the AVX instruction set as well as the FMA one (ie. vfmadd231pd instructions). Furthermore, the loop is better unrolled and there is not latency/dependency issue like in the Numpy code. However, while this loop is highly-efficient, the cores are not efficiently used due to some sequential checks done in Numpy and a sequential copy performed in OpenBLAS. Moreover, I am not sure the loop makes an efficient use of the cache in this case since a lot of read/writes are performed in RAM on my machine. Indeed, the RAM throughput about 15 GiB/s (over 35~40 GiB/s) due to many cache misses while the thoughput of method3 is 6 GiB/s (so more work is done in the cache) with a significantly faster execution.\n\nHere is the executed code of the main computational part of method3:\n\n.LBB0_5: vorpd 2880(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm2 vmovupd %ymm2, 3040(%rsp) vorpd 2848(%rsp), %ymm8, %ymm1 vpcmpeqd %ymm2, %ymm2, %ymm2 vgatherqpd %ymm2, (%rsi,%ymm1,8), %ymm3 vmovupd %ymm3, 3104(%rsp) vorpd 2912(%rsp), %ymm8, %ymm2 vpcmpeqd %ymm3, %ymm3, %ymm3 vgatherqpd %ymm3, (%rsi,%ymm2,8), %ymm4 vmovupd %ymm4, 3136(%rsp) vorpd 2816(%rsp), %ymm8, %ymm3 vpcmpeqd %ymm4, %ymm4, %ymm4 vgatherqpd %ymm4, (%rsi,%ymm3,8), %ymm5 vmovupd %ymm5, 3808(%rsp) vorpd 2784(%rsp), %ymm8, %ymm9 vpcmpeqd %ymm4, %ymm4, %ymm4 vgatherqpd %ymm4, (%rsi,%ymm9,8), %ymm5 vmovupd %ymm5, 3840(%rsp) vorpd 2752(%rsp), %ymm8, %ymm10 vpcmpeqd %ymm4, %ymm4, %ymm4 vgatherqpd %ymm4, (%rsi,%ymm10,8), %ymm5 vmovupd %ymm5, 3872(%rsp) vpaddq 2944(%rsp), %ymm8, %ymm4 vorpd 2720(%rsp), %ymm8, %ymm11 vpcmpeqd %ymm13, %ymm13, %ymm13 vgatherqpd %ymm13, (%rsi,%ymm11,8), %ymm5 vmovupd %ymm5, 3904(%rsp) vpcmpeqd %ymm13, %ymm13, %ymm13 vgatherqpd %ymm13, (%rdx,%ymm0,8), %ymm5 vmovupd %ymm5, 3552(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm1,8), %ymm5 vmovupd %ymm5, 3616(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm2,8), %ymm1 vmovupd %ymm1, 3648(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm3,8), %ymm1 vmovupd %ymm1, 3680(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm9,8), %ymm1 vmovupd %ymm1, 3712(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm10,8), %ymm1 vmovupd %ymm1, 3744(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm11,8), %ymm1 vmovupd %ymm1, 3776(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rsi,%ymm4,8), %ymm6 vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm4,8), %ymm3 vpaddq 2688(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm7 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3360(%rsp) vpaddq 2656(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm13 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3392(%rsp) vpaddq 2624(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm15 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3424(%rsp) vpaddq 2592(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm9 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3456(%rsp) vpaddq 2560(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm14 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3488(%rsp) vpaddq 2528(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm11 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3520(%rsp) vpaddq 2496(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm10 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3584(%rsp) vpaddq 2464(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm2 vpaddq 2432(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm12 vpaddq 2400(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3168(%rsp) vpaddq 2368(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3200(%rsp) vpaddq 2336(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3232(%rsp) vpaddq 2304(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3264(%rsp) vpaddq 2272(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3296(%rsp) vpaddq 2240(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3328(%rsp) vpaddq 2208(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vpaddq 2176(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm5 vmovupd %ymm5, 2976(%rsp) vpaddq 2144(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm5 vmovupd %ymm5, 3008(%rsp) vpaddq 2112(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm5 vmovupd %ymm5, 3072(%rsp) vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm8,8), %ymm0 vpcmpeqd %ymm5, %ymm5, %ymm5 vgatherqpd %ymm5, (%rdx,%ymm8,8), %ymm1 vmovupd 768(%rsp), %ymm5 vfmadd231pd %ymm0, %ymm1, %ymm5 vmovupd %ymm5, 768(%rsp) vmovupd 32(%rsp), %ymm5 vfmadd231pd %ymm0, %ymm3, %ymm5 vmovupd %ymm5, 32(%rsp) vmovupd 1024(%rsp), %ymm5 vfmadd231pd %ymm0, %ymm2, %ymm5 vmovupd %ymm5, 1024(%rsp) vmovupd 1280(%rsp), %ymm5 vfmadd231pd %ymm0, %ymm4, %ymm5 vmovupd %ymm5, 1280(%rsp) vmovupd 1344(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm6, %ymm0 vmovupd %ymm0, 1344(%rsp) vmovupd 480(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm6, %ymm0 vmovupd %ymm0, 480(%rsp) vmovupd 1600(%rsp), %ymm0 vfmadd231pd %ymm2, %ymm6, %ymm0 vmovupd %ymm0, 1600(%rsp) vmovupd 1856(%rsp), %ymm0 vfmadd231pd %ymm4, %ymm6, %ymm0 vmovupd %ymm0, 1856(%rsp) vpaddq 2080(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm2 vpaddq 2048(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd 800(%rsp), %ymm0 vmovupd 3552(%rsp), %ymm1 vmovupd 3040(%rsp), %ymm3 vfmadd231pd %ymm3, %ymm1, %ymm0 vmovupd %ymm0, 800(%rsp) vmovupd 64(%rsp), %ymm0 vmovupd 3360(%rsp), %ymm5 vfmadd231pd %ymm3, %ymm5, %ymm0 vmovupd %ymm0, 64(%rsp) vmovupd 1056(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm12, %ymm0 vmovupd %ymm0, 1056(%rsp) vmovupd 288(%rsp), %ymm0 vmovupd 2976(%rsp), %ymm6 vfmadd231pd %ymm3, %ymm6, %ymm0 vmovupd %ymm0, 288(%rsp) vmovupd 1376(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm7, %ymm0 vmovupd %ymm0, 1376(%rsp) vmovupd 512(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm7, %ymm0 vmovupd %ymm0, 512(%rsp) vmovupd 1632(%rsp), %ymm0 vfmadd231pd %ymm12, %ymm7, %ymm0 vmovupd %ymm0, 1632(%rsp) vmovupd 1888(%rsp), %ymm0 vfmadd231pd %ymm6, %ymm7, %ymm0 vmovupd %ymm0, 1888(%rsp) vmovupd 832(%rsp), %ymm0 vmovupd 3616(%rsp), %ymm1 vmovupd 3104(%rsp), %ymm6 vfmadd231pd %ymm6, %ymm1, %ymm0 vmovupd %ymm0, 832(%rsp) vmovupd 96(%rsp), %ymm0 vmovupd 3392(%rsp), %ymm3 vfmadd231pd %ymm6, %ymm3, %ymm0 vmovupd %ymm0, 96(%rsp) vmovupd 1088(%rsp), %ymm0 vmovupd 3168(%rsp), %ymm5 vfmadd231pd %ymm6, %ymm5, %ymm0 vmovupd %ymm0, 1088(%rsp) vmovupd 320(%rsp), %ymm0 vmovupd 3008(%rsp), %ymm7 vfmadd231pd %ymm6, %ymm7, %ymm0 vmovupd %ymm0, 320(%rsp) vmovupd 1408(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm13, %ymm0 vmovupd %ymm0, 1408(%rsp) vmovupd 544(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm13, %ymm0 vmovupd %ymm0, 544(%rsp) vmovupd 1664(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm13, %ymm0 vmovupd %ymm0, 1664(%rsp) vmovupd 1920(%rsp), %ymm0 vfmadd231pd %ymm7, %ymm13, %ymm0 vmovupd %ymm0, 1920(%rsp) vpaddq 2016(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm3 vmovupd 864(%rsp), %ymm0 vmovupd 3648(%rsp), %ymm1 vmovupd 3136(%rsp), %ymm6 vfmadd231pd %ymm6, %ymm1, %ymm0 vmovupd %ymm0, 864(%rsp) vmovupd 128(%rsp), %ymm0 vmovupd 3424(%rsp), %ymm5 vfmadd231pd %ymm6, %ymm5, %ymm0 vmovupd %ymm0, 128(%rsp) vmovupd 1120(%rsp), %ymm0 vmovupd 3200(%rsp), %ymm7 vfmadd231pd %ymm6, %ymm7, %ymm0 vmovupd %ymm0, 1120(%rsp) vmovupd 352(%rsp), %ymm0 vmovupd 3072(%rsp), %ymm12 vfmadd231pd %ymm6, %ymm12, %ymm0 vmovupd %ymm0, 352(%rsp) vmovupd 1440(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm15, %ymm0 vmovupd %ymm0, 1440(%rsp) vmovupd 576(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm15, %ymm0 vmovupd %ymm0, 576(%rsp) vmovupd 1696(%rsp), %ymm0 vfmadd231pd %ymm7, %ymm15, %ymm0 vmovupd %ymm0, 1696(%rsp) vmovupd 736(%rsp), %ymm0 vfmadd231pd %ymm12, %ymm15, %ymm0 vmovupd %ymm0, 736(%rsp) vmovupd 896(%rsp), %ymm0 vmovupd 3808(%rsp), %ymm1 vmovupd 3680(%rsp), %ymm5 vfmadd231pd %ymm1, %ymm5, %ymm0 vmovupd %ymm0, 896(%rsp) vmovupd 160(%rsp), %ymm0 vmovupd 3456(%rsp), %ymm6 vfmadd231pd %ymm1, %ymm6, %ymm0 vmovupd %ymm0, 160(%rsp) vmovupd 1152(%rsp), %ymm0 vmovupd 3232(%rsp), %ymm7 vfmadd231pd %ymm1, %ymm7, %ymm0 vmovupd %ymm0, 1152(%rsp) vmovupd 384(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm2, %ymm0 vmovupd %ymm0, 384(%rsp) vmovupd 1472(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm9, %ymm0 vmovupd %ymm0, 1472(%rsp) vmovupd 608(%rsp), %ymm0 vfmadd231pd %ymm6, %ymm9, %ymm0 vmovupd %ymm0, 608(%rsp) vmovupd 1728(%rsp), %ymm0 vfmadd231pd %ymm7, %ymm9, %ymm0 vmovupd %ymm0, 1728(%rsp) vmovupd -128(%rsp), %ymm0 vfmadd231pd %ymm2, %ymm9, %ymm0 vmovupd %ymm0, -128(%rsp) vmovupd 928(%rsp), %ymm0 vmovupd 3840(%rsp), %ymm1 vmovupd 3712(%rsp), %ymm2 vfmadd231pd %ymm1, %ymm2, %ymm0 vmovupd %ymm0, 928(%rsp) vmovupd 192(%rsp), %ymm0 vmovupd 3488(%rsp), %ymm5 vfmadd231pd %ymm1, %ymm5, %ymm0 vmovupd %ymm0, 192(%rsp) vmovupd 1184(%rsp), %ymm0 vmovupd 3264(%rsp), %ymm6 vfmadd231pd %ymm1, %ymm6, %ymm0 vmovupd %ymm0, 1184(%rsp) vmovupd 416(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm4, %ymm0 vmovupd %ymm0, 416(%rsp) vmovupd 1504(%rsp), %ymm0 vfmadd231pd %ymm2, %ymm14, %ymm0 vmovupd %ymm0, 1504(%rsp) vmovupd 640(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm14, %ymm0 vmovupd %ymm0, 640(%rsp) vmovupd 1760(%rsp), %ymm0 vfmadd231pd %ymm6, %ymm14, %ymm0 vmovupd %ymm0, 1760(%rsp) vmovupd -96(%rsp), %ymm0 vfmadd231pd %ymm4, %ymm14, %ymm0 vmovupd %ymm0, -96(%rsp) vpaddq 1984(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm2 vmovupd 960(%rsp), %ymm0 vmovupd 3872(%rsp), %ymm1 vmovupd 3744(%rsp), %ymm4 vfmadd231pd %ymm1, %ymm4, %ymm0 vmovupd %ymm0, 960(%rsp) vmovupd 224(%rsp), %ymm0 vmovupd 3520(%rsp), %ymm5 vfmadd231pd %ymm1, %ymm5, %ymm0 vmovupd %ymm0, 224(%rsp) vmovupd 1216(%rsp), %ymm0 vmovupd 3296(%rsp), %ymm6 vfmadd231pd %ymm1, %ymm6, %ymm0 vmovupd %ymm0, 1216(%rsp) vmovupd 448(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm3, %ymm0 vmovupd %ymm0, 448(%rsp) vmovupd 1536(%rsp), %ymm0 vfmadd231pd %ymm4, %ymm11, %ymm0 vmovupd %ymm0, 1536(%rsp) vmovupd 672(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm11, %ymm0 vmovupd %ymm0, 672(%rsp) vmovupd 1792(%rsp), %ymm0 vfmadd231pd %ymm6, %ymm11, %ymm0 vmovupd %ymm0, 1792(%rsp) vmovupd -64(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm11, %ymm0 vmovupd %ymm0, -64(%rsp) vmovupd 992(%rsp), %ymm0 vmovupd 3904(%rsp), %ymm1 vmovupd 3776(%rsp), %ymm3 vfmadd231pd %ymm1, %ymm3, %ymm0 vmovupd %ymm0, 992(%rsp) vmovupd 256(%rsp), %ymm0 vmovupd 3584(%rsp), %ymm4 vfmadd231pd %ymm1, %ymm4, %ymm0 vmovupd %ymm0, 256(%rsp) vmovupd 1248(%rsp), %ymm0 vmovupd 3328(%rsp), %ymm5 vfmadd231pd %ymm1, %ymm5, %ymm0 vmovupd %ymm0, 1248(%rsp) vmovupd 1312(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm2, %ymm0 vmovupd %ymm0, 1312(%rsp) vmovupd 1568(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm10, %ymm0 vmovupd %ymm0, 1568(%rsp) vmovupd 704(%rsp), %ymm0 vfmadd231pd %ymm4, %ymm10, %ymm0 vmovupd %ymm0, 704(%rsp) vmovupd 1824(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm10, %ymm0 vmovupd %ymm0, 1824(%rsp) vmovupd -32(%rsp), %ymm0 vfmadd231pd %ymm2, %ymm10, %ymm0 vmovupd %ymm0, -32(%rsp) vpaddq 1952(%rsp), %ymm8, %ymm8 addq $-4, %rcx jne .LBB0_5\n\nThe loop is huge and is clearly not vectorized properly: there is a lot of completely useless instructions and loads from memory appear not to be contiguous (see vgatherqpd). Numba does not generate a good code since the underlying JIT (LLVM-Lite) fail to vectorize efficiently the code. In fact, I found out that a similar C++ code is badly vectorized by Clang 13.0 on a simplified example (GCC and ICC also fail on a more complex code) while an hand-written SIMD implementation works much better. It look like a bug of the optimizer or at least a missed optimization. This is why the Numba code is much slower than the optimal code. That being said, this implementation makes a quite efficient use of the cache and is properly multithreaded.\n\nI also found out that the BLAS code is faster on Linux than Windows on my machine (with default packages coming from PIP and the same Numpy at version 1.20.3). Thus, the gap is closer between method2 and method3 but the later is still a significantly faster.\n\nedited Jan 30, 2022 at 20:33\n\nanswered Jan 30, 2022 at 1:41\n\nJérôme RichardJérôme Richard\n\n46.7k66 gold badges3737 silver badges6969 bronze badges 7\n\nThanks Jerome! This is definitely a lot to take in for my level, I\'ve tested out your code and it\'s indeed quite faster, and the result is correct. However, it throws out a warning: NumbaParallelSafetyWarning: Variable NHW used in parallel loop may be written to simultaneously by multiple workers and may result in non-deterministic or unintended results. Is there anything to worry about? Thanks\n\n– Sam-gege Commented Jan 30, 2022 at 4:03\n\n@Sam-gege There was an error in the part that compute the remainder tiles. I forgot to update the SO code when I checked the code on my machine. This is fixed. Note that this part should not have been run anyway in the example case (I checked the results on my machine and it was fine). Thank you for reporting this.\n\n– Jérôme Richard Commented Jan 30, 2022 at 14:16\n\n@Murali For larger arrays, the tiling optimization is very important as the cache should start to be an issue. Besides this, I expect the code to work very well on significantly bigger arrays. As for BLAS and np.einsum, this is an interesting point and I added an advanced analysis section regarding both implementation. It turns out that np.einsum does not generate an efficient assembly loop and can certainly be quite easily improved. The Numba code makes use of the best SIMD instructions like OpenBLAS but both of these two methods add unwanted overheads.\n\n– Jérôme Richard Commented Jan 30, 2022 at 15:30\n\n@Murali It turns out Numba did not use SIMD instructions in the previous implementation (vfmadd231sd only compute 1 item while vfmadd231pd compute 4 items at once and I missed the s instead of a d). Thank you again. I think I will fill a bug about the inefficient vectorization as other similar codes can benefit from a better vectorization.\n\n– Jérôme Richard Commented Jan 30, 2022 at 20:42\n\nThis looks to work: godbolt.org/z/Yc5bnovYo But it doesn\'t really help for the Numba implementation.\n\n– max9111 Commented Feb 11, 2022 at 19:13\n\n | Show 2 more comments\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensordot or ask your own question.\n\nHow to build open source apps in a highly regulated industry\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n2 NumPy tensordot grouped calculation\n\n3 What is the difference between tensordot and einsum in numpy?\n\n2 Same operation with einsum and tensordot with Numpy\n\n3 Efficient tensor contraction in python\n\n1 Optimizing tensor multiplications\n\n1 Einsum slower than explicit Numpy implementation for n-mode tensor-matrix product\n\n0 Making numpy einsum faster for multidimensional tensors\n\n2 Performance difference between einsum and matmul\n\n0 One line einsum functions with ""interleaved"" output indexing impossible to recreate using tensordot?\n\n3 Einsum is slow for tensor multiplication\n\n2 Translating np.einsum to something more performant\n\nHot Network Questions\n\nWho first promoted the idea that the primary purpose of government is to protect its citizens?\n\nBasic stems that end in ""w""?\n\nOrange marks on ceiling underneath bathroom\n\nKiCAD symbols for JST connectors\n\nopenssh-client/openssh-server show different version than ssh -V\n\nA problem that could use substitution or logs, not sure which works better\n\nBook in 90\'s (?) about rewriting your own genetic code\n\nWhere can I place a fire near my bed to gain the benefits but not suffer from the smoke?\n\nIf a lambda is declared as a default argument, is it different for each call site?\n\nDid the German justice system convict a woman for insulting a person who gang raped a minor?\n\nfind with du -sch and very many files\n\nIs there a drawback to using Heart\'s blood rote repeatedly?\n\nmirrorlist.centos.org no longer resolve?\n\nWhy does Paul\'s fight with Feyd-Rautha take so long?\n\nAs an advisor, how can I help students with time management and procrastination?\n\nHow do I lower the similarity index percentage in iThenticate?\n\nWill 2.1"" schwalbe MTB tire in 25mm rim become wider that 2.25"" in 19mm rim?\n\nWhy is a game\'s minor update on Steam (e.g., New World) ~15 GB to download?\n\nWhich part(s) of this proof of Goodstein\'s Theorem are not expressible in Peano arithmetic?\n\nWhy does independent research from people without formal academic qualifications generally turn out to be a complete waste of time?\n\nconfidence interval and rejection\n\nHelp with ""Roll XD12, and keep each middle dice as an individual result""\n\nPCB layout for 16 MHz crystal oscillator more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_5', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nnumpy einsum/tensordot with shared non-contracted axis\n\nAsked 2 years, 5 months ago\n\nModified 2 years, 4 months ago\n\nSuppose I have two arrays:\n\nimport numpy as np a = np.random.randn(32, 6, 6, 20, 64, 3, 3) b = np.random.randn(20, 128, 64, 3, 3)\n\nand want to sum over the last 3 axes, and keep the shared axis. The output dimension should be (32,6,6,20,128). Notice here the axis with 20 is shared in both a and b. Let\'s call this axis the ""group"" axis.\n\nI have two methods for this task: The first one is just a simple einsum:\n\ndef method1(a, b): return np.einsum(\'NHWgihw, goihw -> NHWgo\', a, b, optimize=True) # output shape:(32,6,6,20,128)\n\nIn the second method I loop through group dimension and use einsum/tensordot to compute the result for each group dimension, then stack the results:\n\ndef method2(a, b): result = [] for g in range(b.shape[0]): # loop through each group dimension # result.append(np.tensordot(a[..., g, :, :, :], b[g, ...], axes=((-3,-2,-1),(-3,-2,-1)))) result.append(np.einsum(\'NHWihw, oihw -> NHWo\', a[..., g, :, :, :], b[g, ...], optimize=True)) # output shape:(32,6,6,128) return np.stack(result, axis=-2) # output shape:(32,6,6,20,128)\n\nhere\'s the timing for both methods in my jupyter notebook: we can see the second method with a loop is faster than the first method.\n\nHow come method1 is that much slower? It doesn\'t compute more things.\n\nIs there a more efficient way without using loops? (I\'m a bit reluctant to use loops because they are slow in python)\n\nThanks for any help!\n\nImprove this question\n\nedited Feb 20, 2022 at 12:39\n\n46.7k66 gold badges3737 silver badges6969 bronze badges\n\nasked Jan 29, 2022 at 15:46\n\n79955 silver badges1515 bronze badges 8\n\nA few iterations with a complicated expression can be faster. Here it\'s only 20. But I suspect this can be done with matmul with some reshaping and transpose. tensordot is just dot with reshaping and transposes.\n\n– hpaulj Commented Jan 29, 2022 at 16:09\n\nReshape and transpose so that \'NHWgihw, goihw -> NHWgo\' becomes \'NHWgX,gXo->NHWgo\' and use matmul.\n\n– hpaulj Commented Jan 29, 2022 at 16:50\n\nIn method2, einsum dispatches it to BLAS when you set optimize=True(which is highly optimised than native einsum) . I would say your method2 is already well optimised both in speed and memory.\n\n– user10289025 Commented Jan 29, 2022 at 17:01\n\nI got a matmul working, but it times about the same as the single einsum. Looks like this is a well known case where a modest number of iterations on one dimension is faster, due to memory management issues. I may post some timings later.\n\n– hpaulj Commented Jan 29, 2022 at 17:25\n\nHi @hpaulj I don\'t think reshape is a good idea because a and b in my actual case are views of smaller arrays, using reshape will increase memory usage\n\n– Sam-gege Commented Jan 29, 2022 at 17:35\n\n | Show 3 more comments\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nAs pointed out by @Murali in the comments, method1 is not very efficient because it does not succeed to use a BLAS calls as opposed to method2 which does. In fact, np.einsum is quite good in method1 since it compute the result sequentially while method2 mostly runs in parallel thanks to OpenBLAS (used by Numpy on most machines). That being said, method2 is sub-optimal since it does not fully use the available cores (parts of the computation are done sequentially) and appear not to use the cache efficiently. On my 6-core machine, it barely use 50% of all the cores.\n\nFaster implementation\n\nOne solution to speed up this computation is to write an highly-optimized Numba parallel code for this.\n\nFirst of all, a semi-naive implementation is to use many for loops to compute the Einstein summation and reshape the input/output arrays so Numba can better optimize the code (eg. unrolling, use of SIMD instructions). Here is the result:\n\n@nb.njit(\'float64[:,:,:,:,::1](float64[:,:,:,:,:,:,::1], float64[:,:,:,:,::1])\') def compute(a, b): sN, sH, sW, sg, si, sh, sw = a.shape so = b.shape[1] assert b.shape == (sg, so, si, sh, sw) ra = a.reshape(sN*sH*sW, sg, si*sh*sw) rb = b.reshape(sg, so, si*sh*sw) out = np.empty((sN*sH*sW, sg, so), dtype=np.float64) for NHW in range(sN*sH*sW): for g in range(sg): for o in range(so): s = 0.0 # Reduction for ihw in range(si*sh*sw): s += ra[NHW, g, ihw] * rb[g, o, ihw] out[NHW, g, o] = s return out.reshape((sN, sH, sW, sg, so))\n\nNote that the input array are assumed to be contiguous. If this is not the case, please consider performing a copy (which is cheap compared to the computation).\n\nWhile the above code works, it is far from being efficient. Here are some improvements that can be performed:\n\nrun the outermost NHW loop in parallel;\n\nuse the Numba flag fastmath=True. This flag is unsafe if the input data contains special values like NaN or +inf/-inf. However, this flag help compiler to generate a much faster code using SIMD instructions (this is not possible otherwise since IEEE-754 floating-point operations are not associative);\n\nswap the NHW-based loop and g-based loop results in better performance since it improves cache-locality (rb is more likely to fit in the last-level cache of mainstream CPUs whereas it would likely in fetched from the RAM otherwise);\n\nmake use of register blocking so to saturate better SIMD computing units of the processor and reduce the pressure on the memory hierarchy;\n\nmake use of tiling by splitting the o-based loop so rb can almost fully be read from lower-level caches (eg. L1 or L2).\n\nAll these improvements except the last one are implemented in the following code:\n\n@nb.njit(\'float64[:,:,:,:,::1](float64[:,:,:,:,:,:,::1], float64[:,:,:,:,::1])\', parallel=True, fastmath=True) def method3(a, b): sN, sH, sW, sg, si, sh, sw = a.shape so = b.shape[1] assert b.shape == (sg, so, si, sh, sw) ra = a.reshape(sN*sH*sW, sg, si*sh*sw) rb = b.reshape(sg, so, si*sh*sw) out = np.zeros((sN*sH*sW, sg, so), dtype=np.float64) for g in range(sg): for k in nb.prange((sN*sH*sW)//2): NHW = k*2 so_vect_max = (so // 4) * 4 for o in range(0, so_vect_max, 4): s00 = s01 = s02 = s03 = s10 = s11 = s12 = s13 = 0.0 # Useful since Numba does not optimize well the following loop otherwise ra_row0 = ra[NHW+0, g, :] ra_row1 = ra[NHW+1, g, :] rb_row0 = rb[g, o+0, :] rb_row1 = rb[g, o+1, :] rb_row2 = rb[g, o+2, :] rb_row3 = rb[g, o+3, :] # Highly-optimized reduction using register blocking for ihw in range(si*sh*sw): ra_0 = ra_row0[ihw] ra_1 = ra_row1[ihw] rb_0 = rb_row0[ihw] rb_1 = rb_row1[ihw] rb_2 = rb_row2[ihw] rb_3 = rb_row3[ihw] s00 += ra_0 * rb_0; s01 += ra_0 * rb_1 s02 += ra_0 * rb_2; s03 += ra_0 * rb_3 s10 += ra_1 * rb_0; s11 += ra_1 * rb_1 s12 += ra_1 * rb_2; s13 += ra_1 * rb_3 out[NHW+0, g, o+0] = s00; out[NHW+0, g, o+1] = s01 out[NHW+0, g, o+2] = s02; out[NHW+0, g, o+3] = s03 out[NHW+1, g, o+0] = s10; out[NHW+1, g, o+1] = s11 out[NHW+1, g, o+2] = s12; out[NHW+1, g, o+3] = s13 # Remaining part for `o` for o in range(so_vect_max, so): for ihw in range(si*sh*sw): out[NHW, g, o] += ra[NHW, g, ihw] * rb[g, o, ihw] out[NHW+1, g, o] += ra[NHW+1, g, ihw] * rb[g, o, ihw] # Remaining part for `k` if (sN*sH*sW) % 2 == 1: k = sN*sH*sW - 1 for o in range(so): for ihw in range(si*sh*sw): out[k, g, o] += ra[k, g, ihw] * rb[g, o, ihw] return out.reshape((sN, sH, sW, sg, so))\n\nThis code is much more complex and uglier but also far more efficient. I did not implemented the tiling optimization since it would make the code even less readable. However, it should results in a significantly faster code on many-core processors (especially the ones with a small L2/L3 cache).\n\nHere are performance results on my i5-9600KF 6-core processor:\n\nmethod1: 816 ms method2: 104 ms method3: 40 ms Theoretical optimal: 9 ms (optimistic lower bound)\n\nThe code is about 2.7 faster than method2. There is a room for improvements since the optimal time is about 4 time better than method3.\n\nThe main reason why Numba does not generate a fast code comes from the underlying JIT which fail to efficiently vectorize the loop. Implementing the tiling strategy should slightly improves the execution time very close to the optimal one. The tiling strategy is critical for much bigger arrays. This is especially true if so is much bigger.\n\nIf you want a faster implementation you certainly need to write a C/C++ native code using directly SIMD instrinsics (which are unfortunately not portable) or a SIMD library (eg. XSIMD).\n\nIf you want an even faster implementation, then you need to use a faster hardware (with more cores) or a more dedicated one. Server-based GPUs (ie. not the one of personal computers) not should be able to speed up a lot such a computation since your input is small, clearly compute-bound and massively makes use of FMA floating-point operations. A first start is to try cupy.einsum.\n\nUnder the hood: low-level analysis\n\nIn order to understand why method1 is not faster, I checked the executed code. Here is the main loop:\n\n1a0:┌─→; Part of the reduction (see below) │ movapd xmm0,XMMWORD PTR [rdi-0x1000] │ │ ; Decrement the number of loop cycle │ sub r9,0x8 │ │ ; Prefetch items so to reduce the impact │ ; of the latency of reading from the RAM. │ prefetcht0 BYTE PTR [r8] │ prefetcht0 BYTE PTR [rdi] │ │ ; Part of the reduction (see below) │ mulpd xmm0,XMMWORD PTR [r8-0x1000] │ │ ; Increment iterator for the two arrays │ add rdi,0x40 │ add r8,0x40 │ │ ; Main computational part: │ ; reduction using add+mul SSE2 instructions │ addpd xmm1,xmm0 <--- Slow │ movapd xmm0,XMMWORD PTR [rdi-0x1030] │ mulpd xmm0,XMMWORD PTR [r8-0x1030] │ addpd xmm1,xmm0 <--- Slow │ movapd xmm0,XMMWORD PTR [rdi-0x1020] │ mulpd xmm0,XMMWORD PTR [r8-0x1020] │ addpd xmm0,xmm1 <--- Slow │ movapd xmm1,XMMWORD PTR [rdi-0x1010] │ mulpd xmm1,XMMWORD PTR [r8-0x1010] │ addpd xmm1,xmm0 <--- Slow │ │ ; Is the loop over? │ ; If not, jump to the beginning of the loop. ├──cmp r9,0x7 └──jg 1a0\n\nIt turns out that Numpy use the SSE2 instruction set (which is available on all x86-64 processors). However, my machine, like almost all relatively recent processor support the AVX instruction set which can compute twice more items at once per instruction. My machine also support fuse-multiply add instructions (FMA) that are twice faster in this case. Moreover, the loop is clearly bounded by the addpd which accumulate the result in mostly the same register. The processor cannot execute them efficiently since an addpd takes few cycle of latency and up to two can be executed at the same time on modern x86-64 processors (which is not possible here since only 1 intruction can perform the accumulation in xmm1 at a time).\n\nHere is the executed code of the main computational part of method2 (dgemm call of OpenBLAS):\n\n6a40:┌─→vbroadcastsd ymm0,QWORD PTR [rsi-0x60] │ vbroadcastsd ymm1,QWORD PTR [rsi-0x58] │ vbroadcastsd ymm2,QWORD PTR [rsi-0x50] │ vbroadcastsd ymm3,QWORD PTR [rsi-0x48] │ vfmadd231pd ymm4,ymm0,YMMWORD PTR [rdi-0x80] │ vfmadd231pd ymm5,ymm1,YMMWORD PTR [rdi-0x60] │ vbroadcastsd ymm0,QWORD PTR [rsi-0x40] │ vbroadcastsd ymm1,QWORD PTR [rsi-0x38] │ vfmadd231pd ymm6,ymm2,YMMWORD PTR [rdi-0x40] │ vfmadd231pd ymm7,ymm3,YMMWORD PTR [rdi-0x20] │ vbroadcastsd ymm2,QWORD PTR [rsi-0x30] │ vbroadcastsd ymm3,QWORD PTR [rsi-0x28] │ vfmadd231pd ymm4,ymm0,YMMWORD PTR [rdi] │ vfmadd231pd ymm5,ymm1,YMMWORD PTR [rdi+0x20] │ vfmadd231pd ymm6,ymm2,YMMWORD PTR [rdi+0x40] │ vfmadd231pd ymm7,ymm3,YMMWORD PTR [rdi+0x60] │ add rsi,0x40 │ add rdi,0x100 ├──dec rax └──jne 6a40\n\nThis loop is far more optimized: it makes use of the AVX instruction set as well as the FMA one (ie. vfmadd231pd instructions). Furthermore, the loop is better unrolled and there is not latency/dependency issue like in the Numpy code. However, while this loop is highly-efficient, the cores are not efficiently used due to some sequential checks done in Numpy and a sequential copy performed in OpenBLAS. Moreover, I am not sure the loop makes an efficient use of the cache in this case since a lot of read/writes are performed in RAM on my machine. Indeed, the RAM throughput about 15 GiB/s (over 35~40 GiB/s) due to many cache misses while the thoughput of method3 is 6 GiB/s (so more work is done in the cache) with a significantly faster execution.\n\nHere is the executed code of the main computational part of method3:\n\n.LBB0_5: vorpd 2880(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm2 vmovupd %ymm2, 3040(%rsp) vorpd 2848(%rsp), %ymm8, %ymm1 vpcmpeqd %ymm2, %ymm2, %ymm2 vgatherqpd %ymm2, (%rsi,%ymm1,8), %ymm3 vmovupd %ymm3, 3104(%rsp) vorpd 2912(%rsp), %ymm8, %ymm2 vpcmpeqd %ymm3, %ymm3, %ymm3 vgatherqpd %ymm3, (%rsi,%ymm2,8), %ymm4 vmovupd %ymm4, 3136(%rsp) vorpd 2816(%rsp), %ymm8, %ymm3 vpcmpeqd %ymm4, %ymm4, %ymm4 vgatherqpd %ymm4, (%rsi,%ymm3,8), %ymm5 vmovupd %ymm5, 3808(%rsp) vorpd 2784(%rsp), %ymm8, %ymm9 vpcmpeqd %ymm4, %ymm4, %ymm4 vgatherqpd %ymm4, (%rsi,%ymm9,8), %ymm5 vmovupd %ymm5, 3840(%rsp) vorpd 2752(%rsp), %ymm8, %ymm10 vpcmpeqd %ymm4, %ymm4, %ymm4 vgatherqpd %ymm4, (%rsi,%ymm10,8), %ymm5 vmovupd %ymm5, 3872(%rsp) vpaddq 2944(%rsp), %ymm8, %ymm4 vorpd 2720(%rsp), %ymm8, %ymm11 vpcmpeqd %ymm13, %ymm13, %ymm13 vgatherqpd %ymm13, (%rsi,%ymm11,8), %ymm5 vmovupd %ymm5, 3904(%rsp) vpcmpeqd %ymm13, %ymm13, %ymm13 vgatherqpd %ymm13, (%rdx,%ymm0,8), %ymm5 vmovupd %ymm5, 3552(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm1,8), %ymm5 vmovupd %ymm5, 3616(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm2,8), %ymm1 vmovupd %ymm1, 3648(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm3,8), %ymm1 vmovupd %ymm1, 3680(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm9,8), %ymm1 vmovupd %ymm1, 3712(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm10,8), %ymm1 vmovupd %ymm1, 3744(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm11,8), %ymm1 vmovupd %ymm1, 3776(%rsp) vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rsi,%ymm4,8), %ymm6 vpcmpeqd %ymm0, %ymm0, %ymm0 vgatherqpd %ymm0, (%rdx,%ymm4,8), %ymm3 vpaddq 2688(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm7 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3360(%rsp) vpaddq 2656(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm13 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3392(%rsp) vpaddq 2624(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm15 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3424(%rsp) vpaddq 2592(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm9 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3456(%rsp) vpaddq 2560(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm14 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3488(%rsp) vpaddq 2528(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm11 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3520(%rsp) vpaddq 2496(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm0,8), %ymm10 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3584(%rsp) vpaddq 2464(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm2 vpaddq 2432(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm12 vpaddq 2400(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3168(%rsp) vpaddq 2368(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3200(%rsp) vpaddq 2336(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3232(%rsp) vpaddq 2304(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3264(%rsp) vpaddq 2272(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3296(%rsp) vpaddq 2240(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd %ymm4, 3328(%rsp) vpaddq 2208(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vpaddq 2176(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm5 vmovupd %ymm5, 2976(%rsp) vpaddq 2144(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm5 vmovupd %ymm5, 3008(%rsp) vpaddq 2112(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm5 vmovupd %ymm5, 3072(%rsp) vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rsi,%ymm8,8), %ymm0 vpcmpeqd %ymm5, %ymm5, %ymm5 vgatherqpd %ymm5, (%rdx,%ymm8,8), %ymm1 vmovupd 768(%rsp), %ymm5 vfmadd231pd %ymm0, %ymm1, %ymm5 vmovupd %ymm5, 768(%rsp) vmovupd 32(%rsp), %ymm5 vfmadd231pd %ymm0, %ymm3, %ymm5 vmovupd %ymm5, 32(%rsp) vmovupd 1024(%rsp), %ymm5 vfmadd231pd %ymm0, %ymm2, %ymm5 vmovupd %ymm5, 1024(%rsp) vmovupd 1280(%rsp), %ymm5 vfmadd231pd %ymm0, %ymm4, %ymm5 vmovupd %ymm5, 1280(%rsp) vmovupd 1344(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm6, %ymm0 vmovupd %ymm0, 1344(%rsp) vmovupd 480(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm6, %ymm0 vmovupd %ymm0, 480(%rsp) vmovupd 1600(%rsp), %ymm0 vfmadd231pd %ymm2, %ymm6, %ymm0 vmovupd %ymm0, 1600(%rsp) vmovupd 1856(%rsp), %ymm0 vfmadd231pd %ymm4, %ymm6, %ymm0 vmovupd %ymm0, 1856(%rsp) vpaddq 2080(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm2 vpaddq 2048(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm4 vmovupd 800(%rsp), %ymm0 vmovupd 3552(%rsp), %ymm1 vmovupd 3040(%rsp), %ymm3 vfmadd231pd %ymm3, %ymm1, %ymm0 vmovupd %ymm0, 800(%rsp) vmovupd 64(%rsp), %ymm0 vmovupd 3360(%rsp), %ymm5 vfmadd231pd %ymm3, %ymm5, %ymm0 vmovupd %ymm0, 64(%rsp) vmovupd 1056(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm12, %ymm0 vmovupd %ymm0, 1056(%rsp) vmovupd 288(%rsp), %ymm0 vmovupd 2976(%rsp), %ymm6 vfmadd231pd %ymm3, %ymm6, %ymm0 vmovupd %ymm0, 288(%rsp) vmovupd 1376(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm7, %ymm0 vmovupd %ymm0, 1376(%rsp) vmovupd 512(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm7, %ymm0 vmovupd %ymm0, 512(%rsp) vmovupd 1632(%rsp), %ymm0 vfmadd231pd %ymm12, %ymm7, %ymm0 vmovupd %ymm0, 1632(%rsp) vmovupd 1888(%rsp), %ymm0 vfmadd231pd %ymm6, %ymm7, %ymm0 vmovupd %ymm0, 1888(%rsp) vmovupd 832(%rsp), %ymm0 vmovupd 3616(%rsp), %ymm1 vmovupd 3104(%rsp), %ymm6 vfmadd231pd %ymm6, %ymm1, %ymm0 vmovupd %ymm0, 832(%rsp) vmovupd 96(%rsp), %ymm0 vmovupd 3392(%rsp), %ymm3 vfmadd231pd %ymm6, %ymm3, %ymm0 vmovupd %ymm0, 96(%rsp) vmovupd 1088(%rsp), %ymm0 vmovupd 3168(%rsp), %ymm5 vfmadd231pd %ymm6, %ymm5, %ymm0 vmovupd %ymm0, 1088(%rsp) vmovupd 320(%rsp), %ymm0 vmovupd 3008(%rsp), %ymm7 vfmadd231pd %ymm6, %ymm7, %ymm0 vmovupd %ymm0, 320(%rsp) vmovupd 1408(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm13, %ymm0 vmovupd %ymm0, 1408(%rsp) vmovupd 544(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm13, %ymm0 vmovupd %ymm0, 544(%rsp) vmovupd 1664(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm13, %ymm0 vmovupd %ymm0, 1664(%rsp) vmovupd 1920(%rsp), %ymm0 vfmadd231pd %ymm7, %ymm13, %ymm0 vmovupd %ymm0, 1920(%rsp) vpaddq 2016(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm3 vmovupd 864(%rsp), %ymm0 vmovupd 3648(%rsp), %ymm1 vmovupd 3136(%rsp), %ymm6 vfmadd231pd %ymm6, %ymm1, %ymm0 vmovupd %ymm0, 864(%rsp) vmovupd 128(%rsp), %ymm0 vmovupd 3424(%rsp), %ymm5 vfmadd231pd %ymm6, %ymm5, %ymm0 vmovupd %ymm0, 128(%rsp) vmovupd 1120(%rsp), %ymm0 vmovupd 3200(%rsp), %ymm7 vfmadd231pd %ymm6, %ymm7, %ymm0 vmovupd %ymm0, 1120(%rsp) vmovupd 352(%rsp), %ymm0 vmovupd 3072(%rsp), %ymm12 vfmadd231pd %ymm6, %ymm12, %ymm0 vmovupd %ymm0, 352(%rsp) vmovupd 1440(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm15, %ymm0 vmovupd %ymm0, 1440(%rsp) vmovupd 576(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm15, %ymm0 vmovupd %ymm0, 576(%rsp) vmovupd 1696(%rsp), %ymm0 vfmadd231pd %ymm7, %ymm15, %ymm0 vmovupd %ymm0, 1696(%rsp) vmovupd 736(%rsp), %ymm0 vfmadd231pd %ymm12, %ymm15, %ymm0 vmovupd %ymm0, 736(%rsp) vmovupd 896(%rsp), %ymm0 vmovupd 3808(%rsp), %ymm1 vmovupd 3680(%rsp), %ymm5 vfmadd231pd %ymm1, %ymm5, %ymm0 vmovupd %ymm0, 896(%rsp) vmovupd 160(%rsp), %ymm0 vmovupd 3456(%rsp), %ymm6 vfmadd231pd %ymm1, %ymm6, %ymm0 vmovupd %ymm0, 160(%rsp) vmovupd 1152(%rsp), %ymm0 vmovupd 3232(%rsp), %ymm7 vfmadd231pd %ymm1, %ymm7, %ymm0 vmovupd %ymm0, 1152(%rsp) vmovupd 384(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm2, %ymm0 vmovupd %ymm0, 384(%rsp) vmovupd 1472(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm9, %ymm0 vmovupd %ymm0, 1472(%rsp) vmovupd 608(%rsp), %ymm0 vfmadd231pd %ymm6, %ymm9, %ymm0 vmovupd %ymm0, 608(%rsp) vmovupd 1728(%rsp), %ymm0 vfmadd231pd %ymm7, %ymm9, %ymm0 vmovupd %ymm0, 1728(%rsp) vmovupd -128(%rsp), %ymm0 vfmadd231pd %ymm2, %ymm9, %ymm0 vmovupd %ymm0, -128(%rsp) vmovupd 928(%rsp), %ymm0 vmovupd 3840(%rsp), %ymm1 vmovupd 3712(%rsp), %ymm2 vfmadd231pd %ymm1, %ymm2, %ymm0 vmovupd %ymm0, 928(%rsp) vmovupd 192(%rsp), %ymm0 vmovupd 3488(%rsp), %ymm5 vfmadd231pd %ymm1, %ymm5, %ymm0 vmovupd %ymm0, 192(%rsp) vmovupd 1184(%rsp), %ymm0 vmovupd 3264(%rsp), %ymm6 vfmadd231pd %ymm1, %ymm6, %ymm0 vmovupd %ymm0, 1184(%rsp) vmovupd 416(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm4, %ymm0 vmovupd %ymm0, 416(%rsp) vmovupd 1504(%rsp), %ymm0 vfmadd231pd %ymm2, %ymm14, %ymm0 vmovupd %ymm0, 1504(%rsp) vmovupd 640(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm14, %ymm0 vmovupd %ymm0, 640(%rsp) vmovupd 1760(%rsp), %ymm0 vfmadd231pd %ymm6, %ymm14, %ymm0 vmovupd %ymm0, 1760(%rsp) vmovupd -96(%rsp), %ymm0 vfmadd231pd %ymm4, %ymm14, %ymm0 vmovupd %ymm0, -96(%rsp) vpaddq 1984(%rsp), %ymm8, %ymm0 vpcmpeqd %ymm1, %ymm1, %ymm1 vgatherqpd %ymm1, (%rdx,%ymm0,8), %ymm2 vmovupd 960(%rsp), %ymm0 vmovupd 3872(%rsp), %ymm1 vmovupd 3744(%rsp), %ymm4 vfmadd231pd %ymm1, %ymm4, %ymm0 vmovupd %ymm0, 960(%rsp) vmovupd 224(%rsp), %ymm0 vmovupd 3520(%rsp), %ymm5 vfmadd231pd %ymm1, %ymm5, %ymm0 vmovupd %ymm0, 224(%rsp) vmovupd 1216(%rsp), %ymm0 vmovupd 3296(%rsp), %ymm6 vfmadd231pd %ymm1, %ymm6, %ymm0 vmovupd %ymm0, 1216(%rsp) vmovupd 448(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm3, %ymm0 vmovupd %ymm0, 448(%rsp) vmovupd 1536(%rsp), %ymm0 vfmadd231pd %ymm4, %ymm11, %ymm0 vmovupd %ymm0, 1536(%rsp) vmovupd 672(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm11, %ymm0 vmovupd %ymm0, 672(%rsp) vmovupd 1792(%rsp), %ymm0 vfmadd231pd %ymm6, %ymm11, %ymm0 vmovupd %ymm0, 1792(%rsp) vmovupd -64(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm11, %ymm0 vmovupd %ymm0, -64(%rsp) vmovupd 992(%rsp), %ymm0 vmovupd 3904(%rsp), %ymm1 vmovupd 3776(%rsp), %ymm3 vfmadd231pd %ymm1, %ymm3, %ymm0 vmovupd %ymm0, 992(%rsp) vmovupd 256(%rsp), %ymm0 vmovupd 3584(%rsp), %ymm4 vfmadd231pd %ymm1, %ymm4, %ymm0 vmovupd %ymm0, 256(%rsp) vmovupd 1248(%rsp), %ymm0 vmovupd 3328(%rsp), %ymm5 vfmadd231pd %ymm1, %ymm5, %ymm0 vmovupd %ymm0, 1248(%rsp) vmovupd 1312(%rsp), %ymm0 vfmadd231pd %ymm1, %ymm2, %ymm0 vmovupd %ymm0, 1312(%rsp) vmovupd 1568(%rsp), %ymm0 vfmadd231pd %ymm3, %ymm10, %ymm0 vmovupd %ymm0, 1568(%rsp) vmovupd 704(%rsp), %ymm0 vfmadd231pd %ymm4, %ymm10, %ymm0 vmovupd %ymm0, 704(%rsp) vmovupd 1824(%rsp), %ymm0 vfmadd231pd %ymm5, %ymm10, %ymm0 vmovupd %ymm0, 1824(%rsp) vmovupd -32(%rsp), %ymm0 vfmadd231pd %ymm2, %ymm10, %ymm0 vmovupd %ymm0, -32(%rsp) vpaddq 1952(%rsp), %ymm8, %ymm8 addq $-4, %rcx jne .LBB0_5\n\nThe loop is huge and is clearly not vectorized properly: there is a lot of completely useless instructions and loads from memory appear not to be contiguous (see vgatherqpd). Numba does not generate a good code since the underlying JIT (LLVM-Lite) fail to vectorize efficiently the code. In fact, I found out that a similar C++ code is badly vectorized by Clang 13.0 on a simplified example (GCC and ICC also fail on a more complex code) while an hand-written SIMD implementation works much better. It look like a bug of the optimizer or at least a missed optimization. This is why the Numba code is much slower than the optimal code. That being said, this implementation makes a quite efficient use of the cache and is properly multithreaded.\n\nI also found out that the BLAS code is faster on Linux than Windows on my machine (with default packages coming from PIP and the same Numpy at version 1.20.3). Thus, the gap is closer between method2 and method3 but the later is still a significantly faster.\n\nedited Jan 30, 2022 at 20:33\n\nanswered Jan 30, 2022 at 1:41\n\nJérôme RichardJérôme Richard\n\n46.7k66 gold badges3737 silver badges6969 bronze badges 7\n\nThanks Jerome! This is definitely a lot to take in for my level, I\'ve tested out your code and it\'s indeed quite faster, and the result is correct. However, it throws out a warning: NumbaParallelSafetyWarning: Variable NHW used in parallel loop may be written to simultaneously by multiple workers and may result in non-deterministic or unintended results. Is there anything to worry about? Thanks\n\n– Sam-gege Commented Jan 30, 2022 at 4:03\n\n@Sam-gege There was an error in the part that compute the remainder tiles. I forgot to update the SO code when I checked the code on my machine. This is fixed. Note that this part should not have been run anyway in the example case (I checked the results on my machine and it was fine). Thank you for reporting this.\n\n– Jérôme Richard Commented Jan 30, 2022 at 14:16\n\n@Murali For larger arrays, the tiling optimization is very important as the cache should start to be an issue. Besides this, I expect the code to work very well on significantly bigger arrays. As for BLAS and np.einsum, this is an interesting point and I added an advanced analysis section regarding both implementation. It turns out that np.einsum does not generate an efficient assembly loop and can certainly be quite easily improved. The Numba code makes use of the best SIMD instructions like OpenBLAS but both of these two methods add unwanted overheads.\n\n– Jérôme Richard Commented Jan 30, 2022 at 15:30\n\n@Murali It turns out Numba did not use SIMD instructions in the previous implementation (vfmadd231sd only compute 1 item while vfmadd231pd compute 4 items at once and I missed the s instead of a d). Thank you again. I think I will fill a bug about the inefficient vectorization as other similar codes can benefit from a better vectorization.\n\n– Jérôme Richard Commented Jan 30, 2022 at 20:42\n\nThis looks to work: godbolt.org/z/Yc5bnovYo But it doesn\'t really help for the Numba implementation.\n\n– max9111 Commented Feb 11, 2022 at 19:13\n\n | Show 2 more comments\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensordot or ask your own question.\n\nHow to build open source apps in a highly regulated industry\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n2 NumPy tensordot grouped calculation\n\n3 What is the difference between tensordot and einsum in numpy?\n\n2 Same operation with einsum and tensordot with Numpy\n\n3 Efficient tensor contraction in python\n\n1 Optimizing tensor multiplications\n\n1 Einsum slower than explicit Numpy implementation for n-mode tensor-matrix product\n\n0 Making numpy einsum faster for multidimensional tensors\n\n2 Performance difference between einsum and matmul\n\n0 One line einsum functions with ""interleaved"" output indexing impossible to recreate using tensordot?\n\n3 Einsum is slow for tensor multiplication\n\n2 Translating np.einsum to something more performant\n\nHot Network Questions\n\nWho first promoted the idea that the primary purpose of government is to protect its citizens?\n\nBasic stems that end in ""w""?\n\nOrange marks on ceiling underneath bathroom\n\nKiCAD symbols for JST connectors\n\nopenssh-client/openssh-server show different version than ssh -V\n\nA problem that could use substitution or logs, not sure which works better\n\nBook in 90\'s (?) about rewriting your own genetic code\n\nWhere can I place a fire near my bed to gain the benefits but not suffer from the smoke?\n\nIf a lambda is declared as a default argument, is it different for each call site?\n\nDid the German justice system convict a woman for insulting a person who gang raped a minor?\n\nfind with du -sch and very many files\n\nIs there a drawback to using Heart\'s blood rote repeatedly?\n\nmirrorlist.centos.org no longer resolve?\n\nWhy does Paul\'s fight with Feyd-Rautha take so long?\n\nAs an advisor, how can I help students with time management and procrastination?\n\nHow do I lower the similarity index percentage in iThenticate?\n\nWill 2.1"" schwalbe MTB tire in 25mm rim become wider that 2.25"" in 19mm rim?\n\nWhy is a game\'s minor update on Steam (e.g., New World) ~15 GB to download?\n\nWhich part(s) of this proof of Goodstein\'s Theorem are not expressible in Peano arithmetic?\n\nWhy does independent research from people without formal academic qualifications generally turn out to be a complete waste of time?\n\nconfidence interval and rejection\n\nHelp with ""Roll XD12, and keep each middle dice as an individual result""\n\nPCB layout for 16 MHz crystal oscillator more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-03T04:01:40', 'title': 'performance - numpy einsum/tensordot with shared non-contracted axis - Stack Overflow', 'url': 'https://stackoverflow.com/questions/70907083/numpy-einsum-tensordot-with-shared-non-contracted-axis'})], [Document(page_content=""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nnumpy / numpy Public\n\nDimension transposed after tensordot #12111\n\nydup opened this issue\n\nOct 8, 2018 · 9 comments\n\nDimension transposed after tensordot #12111\n\nydup opened this issue\n\nOct 8, 2018 · 9 comments\n\nHello, I try to product a matrix(2D) with another one (3D), but find that the dimension of the result is transposed.\n\ndiagMat=np.diag([1,1,1]) mat3D = np.array([[[ 0, 1, 2, 10], [ 3, 4, 5, 0], [ 6, 7, 8, 1]], [[ 9, 10, 11, 2], [12, 13, 14,3], [15, 16, 17,0]]]) newMat = np.tensordot(mat3D, diagMat, axes=([1],[1])) print mat3D print newMat print newMat.shape # --> (2, 4, 3) print mat3D.shape # --> (2, 3, 4)\n\nWhen the specific dimension changes, I don't want to see that the other dim transposes. By the way, because I am going to calculate high dimension matrix with tensordot, it can be tedious for me to re-transpose them. Such as manully transpose the dim-1 and dim-2 of the newMat.\n\nimport numpy as np diagMat=np.diag([1,1,1]) mat3D = np.array([[[ 0, 1, 2, 10], [ 3, 4, 5, 0], [ 6, 7, 8, 1]], [[ 9, 10, 11, 2], [12, 13, 14,3], [15, 16, 17,0]]]) newMat = np.tensordot(mat3D, diagMat, axes=([1],[1])) print mat3D print newMat.swapaxes(1,2) print newMat.swapaxes(1,2).shape print mat3D.shape\n\nHope that this description is clear for you : )\n\nThe text was updated successfully, but these errors were encountered:\n\nFor the product of tensor and 2D matrix, I pull a request on #12114. Please feel free to test it : )\n\nYour best option for full control over the order of axes from tensordot is to use np.einsum instead. We are unlikely to add tensordotmat since it's a rather specialized use-case and there are easy work-arounds.\n\nThanks! @shoyer With your advise, I finish this with np.einsum as follow:\n\nnp.einsum('aib,ki->akb', mat3D, diagMat)\n\nBut I still have a question. Is that the order of dimension changes using tensordot a bug? At the source code of tensordot, we can see that it returns to a matrix reshaped into (olda+oldb). It can be tedious for us to finish the dot production if it exists shift of dim. So I fix it with the rules of shift to original dim order based on tensordot, but I am just sure that it can work in the tensor dot 2D matrix, then I pull the request in 12114 with the tensordotmat. I wonder if this problem in the tensordot can be solved.\n\nOddly, np.tensordot does not document the shape of its return value in its docstring. This should be fixed.\n\nI do think it is following a reasonable rule for the axes order: all axes from the first argument appear, followed by all axes from the second argument. Even if it wasn't the most useful rule, we would need to be quite cautious with changing it because it would assuredly break user code.\n\nYes, according to the algorithm of tensordot, the new axes order of the output is based on the calculation order. Worrying about that the adding rules to tensordot will affect the previous usage of tensordot, I think that writing a new one with fixing the axes shift and pointing out the difference between the new one and tensordot can be a soft way for users.\n\neric-wieser mentioned this issue\n\nENH: Add new tensordotmat function to numeric.py. #12114\n\neric-wieser commented\n\nnp.einsum('aib,ki->akb', mat3D, diagMat) np.einsum('...ib,...ki->...kb', mat3D, diagMat) # ... is any number of broadcasted, preserved dimensions np.einsum('...ki,...ib->...kb', diagMat, mat3D) # this should be familiar as stacked matrix multiplication np.matmul(diagMat, mat3d) diagMat @ mat3D\n\nYour proposed function already exists, and it's matmul / @\n\n@eric-jones Yes, definitely, you get a simplified way to solve that the 3D matrix dot product with a 2D one. But this issue aims at the problem existing in the tensordot. In another word, the problem of axes change in the tensordot mismatches the usual mathematical analysis of tensor dot product. And this could cause the tedious work for multiple tensor dot product.\n\neric-wieser commented\n\nSo using einsum-like notation, you're looking for xxxjxxx,jk->xxxkxxx, where xxx is some number of non-contracted dimensions?\n\nYes. And my purpose of writing the tensordotmat is to fix the problem of the non-contracted dimensions change. In another word, I mean tensordotmat could be a soft way to fix the issue of tensordot.\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time."", metadata={'id': 'web-search_0', 'snippet': ""Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nnumpy / numpy Public\n\nDimension transposed after tensordot #12111\n\nydup opened this issue\n\nOct 8, 2018 · 9 comments\n\nDimension transposed after tensordot #12111\n\nydup opened this issue\n\nOct 8, 2018 · 9 comments\n\nHello, I try to product a matrix(2D) with another one (3D), but find that the dimension of the result is transposed.\n\ndiagMat=np.diag([1,1,1]) mat3D = np.array([[[ 0, 1, 2, 10], [ 3, 4, 5, 0], [ 6, 7, 8, 1]], [[ 9, 10, 11, 2], [12, 13, 14,3], [15, 16, 17,0]]]) newMat = np.tensordot(mat3D, diagMat, axes=([1],[1])) print mat3D print newMat print newMat.shape # --> (2, 4, 3) print mat3D.shape # --> (2, 3, 4)\n\nWhen the specific dimension changes, I don't want to see that the other dim transposes. By the way, because I am going to calculate high dimension matrix with tensordot, it can be tedious for me to re-transpose them. Such as manully transpose the dim-1 and dim-2 of the newMat.\n\nimport numpy as np diagMat=np.diag([1,1,1]) mat3D = np.array([[[ 0, 1, 2, 10], [ 3, 4, 5, 0], [ 6, 7, 8, 1]], [[ 9, 10, 11, 2], [12, 13, 14,3], [15, 16, 17,0]]]) newMat = np.tensordot(mat3D, diagMat, axes=([1],[1])) print mat3D print newMat.swapaxes(1,2) print newMat.swapaxes(1,2).shape print mat3D.shape\n\nHope that this description is clear for you : )\n\nThe text was updated successfully, but these errors were encountered:\n\nFor the product of tensor and 2D matrix, I pull a request on #12114. Please feel free to test it : )\n\nYour best option for full control over the order of axes from tensordot is to use np.einsum instead. We are unlikely to add tensordotmat since it's a rather specialized use-case and there are easy work-arounds.\n\nThanks! @shoyer With your advise, I finish this with np.einsum as follow:\n\nnp.einsum('aib,ki->akb', mat3D, diagMat)\n\nBut I still have a question. Is that the order of dimension changes using tensordot a bug? At the source code of tensordot, we can see that it returns to a matrix reshaped into (olda+oldb). It can be tedious for us to finish the dot production if it exists shift of dim. So I fix it with the rules of shift to original dim order based on tensordot, but I am just sure that it can work in the tensor dot 2D matrix, then I pull the request in 12114 with the tensordotmat. I wonder if this problem in the tensordot can be solved.\n\nOddly, np.tensordot does not document the shape of its return value in its docstring. This should be fixed.\n\nI do think it is following a reasonable rule for the axes order: all axes from the first argument appear, followed by all axes from the second argument. Even if it wasn't the most useful rule, we would need to be quite cautious with changing it because it would assuredly break user code.\n\nYes, according to the algorithm of tensordot, the new axes order of the output is based on the calculation order. Worrying about that the adding rules to tensordot will affect the previous usage of tensordot, I think that writing a new one with fixing the axes shift and pointing out the difference between the new one and tensordot can be a soft way for users.\n\neric-wieser mentioned this issue\n\nENH: Add new tensordotmat function to numeric.py. #12114\n\neric-wieser commented\n\nnp.einsum('aib,ki->akb', mat3D, diagMat) np.einsum('...ib,...ki->...kb', mat3D, diagMat) # ... is any number of broadcasted, preserved dimensions np.einsum('...ki,...ib->...kb', diagMat, mat3D) # this should be familiar as stacked matrix multiplication np.matmul(diagMat, mat3d) diagMat @ mat3D\n\nYour proposed function already exists, and it's matmul / @\n\n@eric-jones Yes, definitely, you get a simplified way to solve that the 3D matrix dot product with a 2D one. But this issue aims at the problem existing in the tensordot. In another word, the problem of axes change in the tensordot mismatches the usual mathematical analysis of tensor dot product. And this could cause the tedious work for multiple tensor dot product.\n\neric-wieser commented\n\nSo using einsum-like notation, you're looking for xxxjxxx,jk->xxxkxxx, where xxx is some number of non-contracted dimensions?\n\nYes. And my purpose of writing the tensordotmat is to fix the problem of the non-contracted dimensions change. In another word, I mean tensordotmat could be a soft way to fix the issue of tensordot.\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time."", 'timestamp': '2024-05-22T12:13:41', 'title': 'Dimension transposed after tensordot · Issue #12111 · numpy/numpy', 'url': 'https://github.com/numpy/numpy/issues/12111'}), Document(page_content=""Navigation Menu Skip to content\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ntf.tensordot documentation describes non-existent a_axes and b_axes parameters #31004\n\nbashi-bazouk opened this issue\n\nJul 24, 2019 · 4 comments\n\ntf.tensordot documentation describes non-existent a_axes and b_axes parameters #31004\n\nbashi-bazouk opened this issue\n\nJul 24, 2019 · 4 comments\n\nOPs related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:docs-bug\n\nbashi-bazouk commented\n\nURL(s) with the issue:\n\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tensordot\n\nDescription of issue (what needs changing):\n\nThere are no a_axes and b_axes parameters, but the documentation describes the function as if there are.\n\nThe text was updated successfully, but these errors were encountered:\n\nbashi-bazouk added the type:docs-bug\n\nDocument issues label\n\nbashi-bazouk changed the title\n\ntf.tensordot describe non-existent a_axes and b_axes parameters\n\ntf.tensordot documentation describes non-existent a_axes and b_axes parameters\n\nravikyram self-assigned this\n\nravikyram added TF 2.0\n\nIssues relating to TensorFlow 2.0 comp:ops\n\nOPs related issues labels\n\n@bashi-bazouk In the function tf.tensordot( a, b, axes, name=None) axes parameter means a-axis , b-axis parameters. For eg: tf.tensordot(a, b, [[0], [2]]) means a-axis param =[0] and b-axis param=[2].\n\nFor more details refer this link.\n\nravikyram added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nbashi-bazouk commented\n\nMaybe you should make the documentation say that instead of the github issue.\n\nThe above information is available in the documentation..Please, let me know if you have any questions.Thanks!\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nbashi-bazouk commented\n\n@ravikyram There is no sentence in the documentation that says that axes parameter decomposes into a_axes and b_axes. The documentation simply starts referring to these arguments, as if they are self-evident. However, they're not, and the lack of that documentation is the reason that I created this issue.\n\nThe link you provided points to the exact page that I said is lacking this documentation.\n\nbashi-bazouk closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nOPs related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:docs-bug\n\nYou can’t perform that action at this time."", metadata={'id': 'web-search_1', 'snippet': ""Navigation Menu Skip to content\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\ntf.tensordot documentation describes non-existent a_axes and b_axes parameters #31004\n\nbashi-bazouk opened this issue\n\nJul 24, 2019 · 4 comments\n\ntf.tensordot documentation describes non-existent a_axes and b_axes parameters #31004\n\nbashi-bazouk opened this issue\n\nJul 24, 2019 · 4 comments\n\nOPs related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:docs-bug\n\nbashi-bazouk commented\n\nURL(s) with the issue:\n\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tensordot\n\nDescription of issue (what needs changing):\n\nThere are no a_axes and b_axes parameters, but the documentation describes the function as if there are.\n\nThe text was updated successfully, but these errors were encountered:\n\nbashi-bazouk added the type:docs-bug\n\nDocument issues label\n\nbashi-bazouk changed the title\n\ntf.tensordot describe non-existent a_axes and b_axes parameters\n\ntf.tensordot documentation describes non-existent a_axes and b_axes parameters\n\nravikyram self-assigned this\n\nravikyram added TF 2.0\n\nIssues relating to TensorFlow 2.0 comp:ops\n\nOPs related issues labels\n\n@bashi-bazouk In the function tf.tensordot( a, b, axes, name=None) axes parameter means a-axis , b-axis parameters. For eg: tf.tensordot(a, b, [[0], [2]]) means a-axis param =[0] and b-axis param=[2].\n\nFor more details refer this link.\n\nravikyram added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nbashi-bazouk commented\n\nMaybe you should make the documentation say that instead of the github issue.\n\nThe above information is available in the documentation..Please, let me know if you have any questions.Thanks!\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nbashi-bazouk commented\n\n@ravikyram There is no sentence in the documentation that says that axes parameter decomposes into a_axes and b_axes. The documentation simply starts referring to these arguments, as if they are self-evident. However, they're not, and the lack of that documentation is the reason that I created this issue.\n\nThe link you provided points to the exact page that I said is lacking this documentation.\n\nbashi-bazouk closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nOPs related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:docs-bug\n\nYou can’t perform that action at this time."", 'timestamp': '2024-04-23T01:20:57', 'title': 'tf.tensordot documentation describes non-existent `a_axes` and `b_axes` parameters · Issue #31004 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/31004'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nnumpy / numpy Public\n\nYou must be signed in to change notification settings\n\nExtend tensordot to more than two arguments #11334\n\nmrocklin opened this issue\n\nJun 14, 2018 · 7 comments\n\nExtend tensordot to more than two arguments #11334\n\nmrocklin opened this issue\n\nJun 14, 2018 · 7 comments\n\n01 - Enhancement 54 - Needs decision\n\nDoes it make sense to extend tensordot to support more than two input arrays?\n\nThe definition and API seem to be amenable to this extension, though I can\'t say anything about the implementation.\n\nThe text was updated successfully, but these errors were encountered:\n\nmattip added 01 - Enhancement 54 - Needs decision labels\n\n@dgasmith May have some thoughts ...\n\nHow would you distinguish between tensordot and einsum here? It seems einsum is a superset of tensordot\'s capabilities that already supports multi-tensor contraction, optimization, and BLAS support.\n\nYou could also consider extending the multi_dot interface. Chained GEMM is a lot easier to optimize than general einsum.\n\nIt seems einsum is a superset of tensordot\'s capabilities that already supports multi-tensor contraction, optimization, and BLAS support.\n\nPerhaps it makes sense to swap out the implementation then?\n\nYou could also consider extending the multi_dot interface.\n\nRight, so I guess my thought translates then to ""why aren\'t multi_dot and tensordot just renamed to dot""? Is it that internally we make the promise that np.dot is always the common and simple case and is always backed by a GEMM call while the others might be a bit more complex?\n\nFrom an API perspective it seems like they\'re all facets of the same operation, kind of like how hstack, vstack, and dstack are special cases of stack.\n\nPerhaps it makes sense to swap out the implementation then?\n\nTo make things even more complex np.einsum calls np.tensordot under the hood.\n\nAs for the current interfaces to GEMM each has different broadcasting rules. np.dot will perform looped GEMMs with N-dimensional tensors while tensordot and multi_dot explicitly handle either N-Dimensional GEMMs (reordering without loops like ijkl,klmn->ijmn) or 2-D matrices.\n\nThe einsum and multi_dot interfaces use different order optimizers as multi_dot has a nice (N^2) solution while einsum is formally factorial scaling with ~N^3 greedy solutions.\n\nAt the moment, I am just laying out some of the reasons for the current state of affairs. I agree that some condensing could be a very good thing.\n\nAt the moment, I am just laying out some of the reasons for the current state of affairs.\n\nUnderstood. FWIW I appreciate the engagement here.\n\nhameerabbasi commented\n\nCurrently, N-argument BLAS is a niche that shouldn\'t be in NumPy. If someone wants to do this, they can use einsum already...\n\nWould be curious to know what the original motivation was 🙂\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\n01 - Enhancement 54 - Needs decision\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_2', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nnumpy / numpy Public\n\nYou must be signed in to change notification settings\n\nExtend tensordot to more than two arguments #11334\n\nmrocklin opened this issue\n\nJun 14, 2018 · 7 comments\n\nExtend tensordot to more than two arguments #11334\n\nmrocklin opened this issue\n\nJun 14, 2018 · 7 comments\n\n01 - Enhancement 54 - Needs decision\n\nDoes it make sense to extend tensordot to support more than two input arrays?\n\nThe definition and API seem to be amenable to this extension, though I can\'t say anything about the implementation.\n\nThe text was updated successfully, but these errors were encountered:\n\nmattip added 01 - Enhancement 54 - Needs decision labels\n\n@dgasmith May have some thoughts ...\n\nHow would you distinguish between tensordot and einsum here? It seems einsum is a superset of tensordot\'s capabilities that already supports multi-tensor contraction, optimization, and BLAS support.\n\nYou could also consider extending the multi_dot interface. Chained GEMM is a lot easier to optimize than general einsum.\n\nIt seems einsum is a superset of tensordot\'s capabilities that already supports multi-tensor contraction, optimization, and BLAS support.\n\nPerhaps it makes sense to swap out the implementation then?\n\nYou could also consider extending the multi_dot interface.\n\nRight, so I guess my thought translates then to ""why aren\'t multi_dot and tensordot just renamed to dot""? Is it that internally we make the promise that np.dot is always the common and simple case and is always backed by a GEMM call while the others might be a bit more complex?\n\nFrom an API perspective it seems like they\'re all facets of the same operation, kind of like how hstack, vstack, and dstack are special cases of stack.\n\nPerhaps it makes sense to swap out the implementation then?\n\nTo make things even more complex np.einsum calls np.tensordot under the hood.\n\nAs for the current interfaces to GEMM each has different broadcasting rules. np.dot will perform looped GEMMs with N-dimensional tensors while tensordot and multi_dot explicitly handle either N-Dimensional GEMMs (reordering without loops like ijkl,klmn->ijmn) or 2-D matrices.\n\nThe einsum and multi_dot interfaces use different order optimizers as multi_dot has a nice (N^2) solution while einsum is formally factorial scaling with ~N^3 greedy solutions.\n\nAt the moment, I am just laying out some of the reasons for the current state of affairs. I agree that some condensing could be a very good thing.\n\nAt the moment, I am just laying out some of the reasons for the current state of affairs.\n\nUnderstood. FWIW I appreciate the engagement here.\n\nhameerabbasi commented\n\nCurrently, N-argument BLAS is a niche that shouldn\'t be in NumPy. If someone wants to do this, they can use einsum already...\n\nWould be curious to know what the original motivation was 🙂\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\n01 - Enhancement 54 - Needs decision\n\nYou can’t perform that action at this time.', 'timestamp': '2024-07-03T21:35:41', 'title': 'Extend tensordot to more than two arguments · Issue #11334 · numpy/numpy', 'url': 'https://github.com/numpy/numpy/issues/11334'}), Document(page_content='""""""A `Type` and `Op` classes to work with numpy.ndarrays symbolically.""""""\nfrom __future__ import absolute_import, print_function, division\n\nfrom six.moves import builtins\nimport sys\nimport warnings\n\nimport numpy as np\nfrom six import integer_types\nfrom six.moves import xrange\nimport numbers\n\nimport theano\nfrom theano.compat import izip\nfrom theano import config\nfrom theano import gof\nfrom theano.gof import Apply, Constant, Op, Variable, ParamsType\nfrom theano.gof.type import Generic\n\nfrom theano.scalar import int32 as int32_t\nfrom theano.tensor import elemwise\nfrom theano.tensor.var import (AsTensorError, TensorVariable,\n                               TensorConstant, TensorConstantSignature,\n                               _tensor_py_operators)\nfrom theano.tensor.type import TensorType, values_eq_approx_always_true\nfrom theano.tensor.type_other import NoneConst\nfrom theano import scalar as scal\nfrom functools import partial\nfrom theano import compile, printing\nfrom theano.printing import pprint, min_informative_str\n# For history\nfrom theano.compile import Rebroadcast, Shape, shape\nfrom theano.scalar import int32\n\n\n# We use these exceptions as well.\nimport theano.scalar.sharedvar\nfrom theano.gradient import grad_undefined\nfrom theano.gradient import grad_not_implemented\nfrom theano.gradient import DisconnectedType\n\n# set up the external interface\nfrom theano.tensor.elemwise import Elemwise, DimShuffle, CAReduce, Sum\n\nimport logging\n_logger = logging.getLogger(""theano.tensor.basic"")\n\n__docformat__ = ""restructuredtext en""\n\n# This is needed as we will hide it later\npython_complex = complex\npython_any = any\npython_all = all\n\n# Define common subsets of dtypes (as strings).\ncomplex_dtypes = list(map(str, scal.complex_types))\ncontinuous_dtypes = list(map(str, scal.continuous_types))\nfloat_dtypes = list(map(str, scal.float_types))\ninteger_dtypes = list(map(str, scal.integer_types))\ndiscrete_dtypes = list(map(str, scal.discrete_types))\nall_dtypes = list(map(str, scal.all_types))\nint_dtypes = list(map(str, scal.int_types))\nuint_dtypes = list(map(str, scal.uint_types))\n\n\nclass ShapeError(Exception):\n    """"""Raised when the shape cannot be computed.""""""\n    pass\n\n\ndef check_equal_numpy(x, y):\n    """"""\n    Return True iff x and y are equal.\n\n    Checks the dtype and shape if x and y are numpy.ndarray instances.\n\n    """"""\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        return (x.dtype == y.dtype and x.shape == y.shape and\n                np.all(abs(x - y) < 1e-10))\n    elif (isinstance(x, np.random.RandomState) and\n          isinstance(y, np.random.RandomState)):\n        return python_all(np.all(a == b) for a, b in\n                          izip(x.__getstate__(), y.__getstate__()))\n    else:\n        return x == y\n\ncompile.register_checker(check_equal_numpy)\n\n\n__oplist_constructor_list = []\n""""""List of functions to be listed as op constructors in the oplist\n(`gen_oplist`, doc/oplist.txt).""""""\n\n\ndef constructor(f):\n    """"""Add `f` to :doc:`oplist`.\n\n    Make `f` appear as a constructor in the oplist (`gen_oplist`,\n    doc/oplist.txt).\n\n    """"""\n    __oplist_constructor_list.append(f)\n    return f\n\n\ndef __oplist_tag(thing, tag):\n    tags = getattr(thing, \'__oplist_tags\', [])\n    tags.append(tag)\n    thing.__oplist_tags = tags\n\n\ndef as_tensor_variable(x, name=None, ndim=None):\n    """"""Return `x`, transformed into a `TensorType`.\n\n    This function is often used by `make_node` methods of `Op` subclasses\n    to turn ndarrays, numbers, `Scalar` instances, `Apply` instances and\n    `TensorType` instances into valid input list elements.\n\n    Parameters\n    ----------\n    x : Apply instance, Variable instance, numpy.ndarray, or number\n        This thing will be transformed into a `Variable` in a sensible way. An\n        ndarray argument will not be copied, but a list of numbers will be\n        copied to make an ndarray.\n    name : str or None\n        If a new `Variable` instance is created, it will be named with this\n        string.\n    ndim : None or integer\n        Return a Variable with this many dimensions.\n\n    Raises\n    ------\n    ValueError\n        If an `Apply` with more than one output is fetched or\n        if `x` cannot be made into a Variable with `ndim` dimensions.\n    AsTensorError\n        If `x` cannot be converted to a TensorType Variable.\n\n    """"""\n    if hasattr(x, \'_as_TensorVariable\'):\n        return x._as_TensorVariable()  # TODO: pass name and ndim arguments\n\n    if isinstance(x, gof.Apply):\n        # use Apply\'s default output mechanism\n        if (x.op.default_output is None) and (len(x.outputs) != 1):\n            raise ValueError(\n                ""It is ambiguous which output of a multi-output Op has""\n                "" to be fetched."", x)\n\n        x = x.default_output()\n    if isinstance(x, Variable):\n        if isinstance(x.type, scal.Scalar):\n            x = tensor_from_scalar(x)\n\n        if not isinstance(x.type, TensorType):\n            raise AsTensorError(\n                ""Variable type field must be a TensorType."", x, x.type)\n\n        if ndim is None:\n            return x\n        else:\n            if (x.type.ndim > ndim):\n                # strip off leading broadcastable dimensions\n                first_non_broadcastable = [idx for idx in xrange(x.ndim)\n                                           if not x.broadcastable[idx]][0]\n                x = x.dimshuffle(list(range(x.ndim))[first_non_broadcastable:])\n                if x.ndim > ndim:\n                    raise ValueError(\n                        \'TensorType could not be cast to have %i dimensions\'\n                        % ndim, x.type\n                    )\n                return x\n            elif (x.type.ndim < ndim):\n                return shape_padleft(x, n_ones=(ndim - x.type.ndim))\n            else:\n                return x\n    if isinstance(x, (tuple, list)) and python_any(isinstance(xi, Variable)\n                                                   for xi in x):\n        try:\n            return stack(x)\n        except (TypeError, ValueError):\n            pass\n\n    if isinstance(x, bool):\n        raise AsTensorError(\n            ""Cannot cast True or False as a tensor variable. Please use ""\n            ""np.array(True) or np.array(False) if you need these constants. ""\n            ""This error might be caused by using the == operator on ""\n            ""Variables. v == w does not do what you think it does, ""\n            ""use theano.tensor.eq(v, w) instead."")\n\n    try:\n        return constant(x, name=name, ndim=ndim)\n    except TypeError:\n        try:\n            str_x = str(x)\n        except Exception:\n            str_x = repr(x)\n        raise AsTensorError(""Cannot convert %s to TensorType"" % str_x, type(x))\n\n# this has a different name, because _as_tensor_variable is the\n# function which ops use to upcast their arguments... this\n# internal-use function is a good place to put debugging stuff, better\n# than the global astensor.\n_as_tensor_variable = as_tensor_variable\n\nas_tensor = as_tensor_variable\n\n\ndef constant(x, name=None, ndim=None, dtype=None):\n    """"""Return a symbolic `Constant` with value `x`.\n\n    Raises\n    ------\n    TypeError\n        `x` could not be converted to a numpy.ndarray.\n    ValueError\n        `x` could not be expanded to have ndim dimensions.\n\n    Note\n    ----\n    We create a small cache of frequently used constant.\n    This speed up the Merge optimization for big graph.\n    We want to cache all scalar to don\'t merge as frequently constants.\n    But we don\'t want to cache too much stuff.\n    So we cache integer with dtype [u]int and float where the value is\n    between -10 and 10.\n    We cache all broadcast pattern for scalar.\n\n    """"""\n    x_ = scal.convert(x, dtype=dtype)\n\n    bcastable = [d == 1 for d in x_.shape]\n    if ndim is not None:\n        if len(bcastable) < ndim:\n            bcastable = [True] * (ndim - len(bcastable)) + bcastable\n        elif len(bcastable) > ndim:\n            # TODO: strip off dimensions of size 1\n            raise ValueError(\n                \'ndarray could not be cast to constant with %i dimensions\' %\n                ndim)\n        assert len(bcastable) == ndim\n\n    try:\n        ttype = TensorType(dtype=x_.dtype, broadcastable=bcastable)\n        if not constant.enable:\n            return TensorConstant(ttype, x_, name=name)\n\n        sig = TensorConstantSignature((ttype, x_))\n        if sig in constant_cache:\n            return constant_cache[sig]\n\n        ret = TensorConstant(ttype, x_, name=name)\n        if (x_.size == 1 and\n            (-10) <= x_ <= 10 and\n            (x_.dtype in int_dtypes or x_.dtype in uint_dtypes or\n             (x_.dtype in float_dtypes and\n              # Limit the size of the cache.\n              len(constant_cache) < 10000))):\n            constant_cache[sig] = ret\n            # This is needed to raise a good error to the user.\n            ret.cached = True\n        return ret\n    except Exception:\n        raise TypeError(""Could not convert %s to TensorType"" % x, type(x))\n\n\nconstant.enable = True\nconstant_cache = {}\n\n\ndef _obj_is_wrappable_as_tensor(x):\n    try:\n        constant(x)\n        return True\n    except TypeError:\n        return False\n\n\nif int(config.tensor.cmp_sloppy) > 1:\n    # This config variable is a quick-and-dirty way to get low-precision\n    # comparisons.  For a more precise setting of these tolerances set\n    # them explicitly in your user code by assigning, for example,\n    # ""theano.tensor.basic.float32_atol = ...""\n\n    # When config.tensor.cmp_sloppy>1 we are even more sloppy. This is\n    # useful to test the GPU as they don\'t use extended precision and\n    # this cause some difference bigger then the normal sloppy.\n    float16_atol = 1e-2\n    float16_rtol = 5e-2\n\n    float32_atol = 5e-4\n    float32_rtol = 1e-3\n\n    float64_rtol = 1e-4\n    float64_atol = 1e-3\nelif int(config.tensor.cmp_sloppy):\n    float16_atol = 5e-3\n    float16_rtol = 1e-2\n\n    float32_atol = 1e-4\n    float32_rtol = 1e-3\n\n    float64_rtol = 1e-4\n    float64_atol = 1e-3\nelse:\n    # If you change those value in test don\'t forget to put them back\n    # when the test end.  Don\'t forget the case when the test fail.\n    float16_atol = 1e-3\n    float16_rtol = 1e-3\n\n    float32_atol = 1e-5\n    float32_rtol = 1e-5\n\n    # defaults in numpy.allclose\n    # Don\'t be more strict then numpy rtol\n    # It cause useless error.\n    float64_rtol = 1.0000000000000001e-05\n    float64_atol = 1e-8\n\n\ndef _get_atol_rtol(a, b):\n    tiny = (\'float16\',)\n    narrow = (\'float32\', \'complex64\')\n    if (str(a.dtype) in tiny) or (str(b.dtype) in tiny):\n        atol = float16_atol\n        rtol = float16_rtol\n    elif (str(a.dtype) in narrow) or (str(b.dtype) in narrow):\n        atol = float32_atol\n        rtol = float32_rtol\n    else:\n        atol = float64_atol\n        rtol = float64_rtol\n    return atol, rtol\n\n\ndef _allclose(a, b, rtol=None, atol=None):\n    a = np.asarray(a)\n    b = np.asarray(b)\n    atol_, rtol_ = _get_atol_rtol(a, b)\n    if rtol is not None:\n        rtol_ = rtol\n    if atol is not None:\n        atol_ = atol\n\n    return np.allclose(a, b, atol=atol_, rtol=rtol_)\n\n\nclass NotScalarConstantError(Exception):\n    """"""\n    Raised by get_scalar_constant_value if called on something that is\n    not a scalar constant.\n    """"""\n\n\nclass EmptyConstantError(NotScalarConstantError):\n    """"""\n    Raised by get_scalar_const_value if called on something that is a\n    zero dimensional constant.\n    """"""\n\n\ndef numpy_scalar(data):\n    """""" Return a scalar stored in a numpy ndarray.\n\n    Raises\n    ------\n     NotScalarConstantError\n        If the numpy ndarray is not a scalar.\n\n    # handle case where data is numpy.array([])\n    if (data.ndim > 0 and\n        (len(data.shape) == 0 or\n         builtins.max(data.shape) == 0)):\n        assert np.all(np.array([]) == data)\n        raise EmptyConstantError()\n    try:\n        np.complex(data)  # works for all numeric scalars\n        return data\n    except Exception:\n        raise NotScalarConstantError(\n            \'v.data is non-numeric, non-scalar, or has more than one\'\n            \' unique value\', data)\n\n\nget_scalar_constant_value_elemwises = (\n    scal.Cast, scal.Switch,\n    scal.NEQ, scal.EQ,\n    scal.LT, scal.GT, scal.LE, scal.GE,\n    scal.Sub, scal.Add, scal.Mod, scal.Mul,\n    scal.IntDiv, scal.TrueDiv, scal.Minimum, scal.Maximum)\n\n\ndef get_scalar_constant_value(orig_v, elemwise=True,\n                              only_process_constants=False,\n                              max_recur=10):\n    """"""Return the constant scalar(0-D) value underlying variable `v`.\n\n    If `v` is the output of dimshuffles, fills, allocs, rebroadcasts,\n    cast, OutputGuard, DeepCopyOp, ScalarFromTensor, ScalarOp, Elemwise\n    and some pattern with Subtensor, this function digs through them.\n\n    If `v` is not some view of constant scalar data, then raise a\n    NotScalarConstantError.\n\n    Parameters\n    ----------\n    elemwise : bool\n        If False, we won\'t try to go into elemwise. So this call is faster.\n        But we still investigate in Second Elemwise (as this is a substitute\n        for Alloc)\n    only_process_constants : bool\n        If True, we only attempt to obtain the value of `orig_v` if it\'s\n        directly constant and don\'t try to dig through dimshuffles, fills,\n        allocs, and other to figure out its value.\n    max_recur : int\n        The maximum number of recursion.\n\n    Notes\n    -----\n        There may be another function similar to this one in the code,\n        but I\'m not sure where it is.\n\n    """"""\n    v = orig_v\n    while True:\n        if v is None:\n            # None is not a scalar (and many uses of this function seem\n            # to depend on passing it None)\n            raise NotScalarConstantError()\n\n        if isinstance(v, (np.integer, integer_types, float)):\n            return np.asarray(v)\n\n        if isinstance(v, np.ndarray):\n            return numpy_scalar(v).copy()\n\n        if isinstance(v, Constant):\n            if getattr(v.tag, \'unique_value\', None) is not None:\n                data = v.tag.unique_value\n            else:\n                data = v.data\n            return numpy_scalar(data).copy()\n\n        if (not only_process_constants and\n                getattr(v, \'owner\', None) and\n                max_recur > 0):\n            max_recur -= 1\n            if isinstance(v.owner.op, (Alloc, DimShuffle, Rebroadcast,\n                                       # outputguard is only used in debugmode but we\n                                       # keep it here to avoid problems with old pickels.\n                                       compile.ops.OutputGuard,\n                                       compile.DeepCopyOp)):\n                v = v.owner.inputs[0]\n                continue\n            elif isinstance(v.owner.op, theano.compile.ops.Shape_i):\n                i = v.owner.op.i\n                inp = v.owner.inputs[0]\n                if isinstance(inp, Constant):\n                    return np.asarray(inp.data.shape[i])\n                # The shape of a broadcastable dimension is 1\n                if (hasattr(inp.type, \'broadcastable\') and\n                        inp.type.broadcastable[i]):\n                    return np.asarray(1)\n\n            # Don\'t act as the constant_folding optimization here as this\n            # fct is used too early in the optimization phase.  This would\n            # mess with the stabilization optimization and be too slow.\n            # We put all the scalar Ops used by get_canonical_form_slice()\n            # to allow it to determine the broadcast pattern correctly.\n            elif isinstance(v.owner.op, (ScalarFromTensor, TensorFromScalar)):\n                v = v.owner.inputs[0]\n                continue\n            elif isinstance(v.owner.op, theano.tensor.opt.Assert):\n                # check if all conditions are constant and true\n                cond = [get_scalar_constant_value(c, max_recur=max_recur)\n                        for c in v.owner.inputs[1:]]\n                if builtins.all([0 == c.ndim and c != 0 for c in cond]):\n                    v = v.owner.inputs[0]\n                    continue\n            elif isinstance(v.owner.op, scal.ScalarOp):\n                if isinstance(v.owner.op, scal.Second):\n                    # We don\'t need both input to be constant for second\n                    shp, val = v.owner.inputs\n                    v = val\n                    continue\n                if isinstance(v.owner.op, get_scalar_constant_value_elemwises):\n                    const = [get_scalar_constant_value(i, max_recur=max_recur)\n                             for i in v.owner.inputs]\n                    ret = [[None]]\n                    v.owner.op.perform(v.owner, const, ret)\n                    return ret[0][0].copy()\n            # In fast_compile, we don\'t enable local_fill_to_alloc, so\n            # we need to investigate Second as Alloc. So elemwise\n            # don\'t disable the check for Second.\n            elif isinstance(v.owner.op, Elemwise):\n                if isinstance(v.owner.op.scalar_op, scal.Second):\n                    # We don\'t need both input to be constant for second\n                    shp, val = v.owner.inputs\n                    v = val\n                    continue\n                elif elemwise and isinstance(\n                        v.owner.op.scalar_op,\n                        get_scalar_constant_value_elemwises):\n                    const = [get_scalar_constant_value(i, max_recur=max_recur)\n                             for i in v.owner.inputs]\n                    ret = [[None]]\n                    v.owner.op.perform(v.owner, const, ret)\n                    return ret[0][0].copy()\n            elif (isinstance(v.owner.op, theano.tensor.subtensor.Subtensor) and\n                  v.ndim == 0):\n                if isinstance(v.owner.inputs[0], TensorConstant):\n                    cdata = tuple(v.owner.op.get_constant_idx(v.owner.inputs))\n                    try:\n                        return v.owner.inputs[0].data.__getitem__(cdata).copy()\n                    except IndexError:\n                        raise IndexError(\n                            str(tuple(v.owner.op.idx_list)) +\n                            "" is not a valid index into "" +\n                            str(v.owner.inputs[0].data))\n\n                # The index list \'idx_list\' should have length the same\n                # shape as the input.\n                # TODO: implement the case where we take a scalar in a matrix\n                assert len(v.owner.op.idx_list) == v.owner.inputs[0].ndim\n\n                # Needed to make better graph in this test in\n                # theano/tensor/tests/test_sharedvar.py:\n                # test_shared_options.test_specify_shape_partial\n                if ((v.owner.inputs[0].owner and\n                     isinstance(v.owner.inputs[0].owner.op, Join) and\n                     len(v.owner.op.idx_list) == 1)):\n                    # Ensure the Join is joining only scalar variables (so that\n                    # the constant value can be found at the same index as the\n                    # one used in the sub-tensor).\n                    if python_all(var.ndim == 0 for var in\n                                  v.owner.inputs[0].owner.inputs[1:]):\n                        idx = v.owner.op.idx_list[0]\n                        if isinstance(idx, gof.Type):\n                            idx = get_scalar_constant_value(v.owner.inputs[1],\n                                                            max_recur=max_recur)\n                        # Note the \'+ 1\' is because the first argument to Join\n                        # is the axis.\n                        ret = v.owner.inputs[0].owner.inputs[idx + 1]\n                        ret = get_scalar_constant_value(ret, max_recur=max_recur)\n                        # join can cast implicitly its input in some case.\n                        return theano._asarray(ret, dtype=v.type.dtype)\n                    if python_all(var.ndim == 1 for var in\n                                  v.owner.inputs[0].owner.inputs[1:]):\n                        idx = v.owner.op.idx_list[0]\n                        if isinstance(idx, gof.Type):\n                            idx = get_scalar_constant_value(v.owner.inputs[1],\n                                                            max_recur=max_recur)\n                        try:\n                            # TODO: assert joined axis is 0.\n                            length = 0\n                            loop = False\n                            for joined in v.owner.inputs[0].owner.inputs[1:]:\n                                ll = get_vector_length(joined)\n                                if idx < length + ll:\n                                    v = joined[idx - length]\n                                    loop = True\n                                    break\n                                length += ll\n                            if loop:\n                                continue\n                        except TypeError:\n                            pass\n                        except ValueError:\n                            pass\n\n                elif (v.owner.inputs[0].owner and\n                      isinstance(v.owner.inputs[0].owner.op,\n                                 theano.tensor.opt.MakeVector) and\n                      # MakeVector normally accept only scalar as input.\n                      # We put this check in case there is change in the future\n                      python_all(var.ndim == 0 for var in\n                                 v.owner.inputs[0].owner.inputs) and\n                      len(v.owner.op.idx_list) == 1):\n\n                    idx = v.owner.op.idx_list[0]\n                    if isinstance(idx, gof.Type):\n                        idx = get_scalar_constant_value(v.owner.inputs[1],\n                                                        max_recur=max_recur)\n                    # Python 2.4 does not support indexing with numpy.integer\n                    # So we cast it.\n                    idx = int(idx)\n                    ret = v.owner.inputs[0].owner.inputs[idx]\n                    ret = get_scalar_constant_value(ret, max_recur=max_recur)\n                    # MakeVector can cast implicitly its input in some case.\n                    return theano._asarray(ret, dtype=v.type.dtype)\n\n                # This is needed when we take the grad as the Shape op\n                # are not already changed into MakeVector\n                owner = v.owner\n                leftmost_parent = owner.inputs[0]\n                if (leftmost_parent.owner and\n                    isinstance(leftmost_parent.owner.op,\n                               theano.tensor.Shape)):\n                    op = owner.op\n                    idx_list = op.idx_list\n                    idx = idx_list[0]\n                    if isinstance(idx, gof.Type):\n                        idx = get_scalar_constant_value(owner.inputs[1],\n                                                        max_recur=max_recur)\n                    grandparent = leftmost_parent.owner.inputs[0]\n                    gp_broadcastable = grandparent.type.broadcastable\n                    ndim = grandparent.type.ndim\n                    if grandparent.owner and isinstance(grandparent.owner.op,\n                                                        Rebroadcast):\n                        ggp_broadcastable = grandparent.owner.inputs[0].broadcastable\n                        l = [b1 or b2 for b1, b2 in zip(ggp_broadcastable,\n                                                        gp_broadcastable)]\n                        gp_broadcastable = tuple(l)\n\n                    assert ndim == len(gp_broadcastable)\n\n                    if not (idx < len(gp_broadcastable)):\n                        msg = (""get_scalar_constant_value detected "" +\n                               ""deterministic IndexError: x.shape[%d] "" +\n                               ""when x.ndim=%d."") % (idx, ndim)\n                        if config.exception_verbosity == \'high\':\n                            msg += \' x=%s\' % min_informative_str(v)\n                        else:\n                            msg += \' x=%s\' % str(v)\n                        raise ValueError(msg)\n\n                    if gp_broadcastable[idx]:\n                        return np.asarray(1)\n\n        raise NotScalarConstantError(v)\n\n\n# Easy constructors\n\ndef tensor(*args, **kwargs):\n    name = kwargs.pop(\'name\', None)\n    return TensorType(*args, **kwargs)(name=name)\n\n\ndef _multi(*fns):\n    def f2(f, *names):\n        if names and isinstance(names[0], integer_types):\n            if names == 1:\n                return f()\n            else:\n                return [f() for i in xrange(names[0])]\n        if isinstance(names, tuple):\n            if len(names) == 1:\n                names = names[0]\n        if len(names) == 1:\n            return f(names)\n        else:\n            return [f(name) for name in names]\n    if len(fns) == 1:\n        return partial(f2, fns)\n    else:\n        return [partial(f2, f) for f in fns]\n\ncscalar = TensorType(\'complex64\', ())\nzscalar = TensorType(\'complex128\', ())\nfscalar = TensorType(\'float32\', ())\ndscalar = TensorType(\'float64\', ())\nbscalar = TensorType(\'int8\', ())\nwscalar = TensorType(\'int16\', ())\niscalar = TensorType(\'int32\', ())\nlscalar = TensorType(\'int64\', ())\n\n\ndef scalar(name=None, dtype=None):\n    """"""Return a symbolic scalar variable.\n\n    Parameters\n    ----------\n    dtype: numeric\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, ())\n    return type(name)\n\nscalars, fscalars, dscalars, iscalars, lscalars = _multi(\n    scalar, fscalar, dscalar, iscalar, lscalar)\n\nint_types = bscalar, wscalar, iscalar, lscalar\nfloat_types = fscalar, dscalar\ncomplex_types = cscalar, zscalar\nint_scalar_types = int_types\nfloat_scalar_types = float_types\ncomplex_scalar_types = complex_types\n\ncvector = TensorType(\'complex64\', (False, ))\nzvector = TensorType(\'complex128\', (False, ))\nfvector = TensorType(\'float32\', (False, ))\ndvector = TensorType(\'float64\', (False, ))\nbvector = TensorType(\'int8\', (False,))\nwvector = TensorType(\'int16\', (False,))\nivector = TensorType(\'int32\', (False, ))\nlvector = TensorType(\'int64\', (False, ))\n\n\ndef vector(name=None, dtype=None):\n    """"""Return a symbolic vector variable.\n\n    Parameters\n    ----------\n    dtype: numeric\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, ))\n    return type(name)\n\nvectors, fvectors, dvectors, ivectors, lvectors = _multi(\n    vector, fvector, dvector, ivector, lvector)\n\nint_vector_types = bvector, wvector, ivector, lvector\nfloat_vector_types = fvector, dvector\ncomplex_vector_types = cvector, zvector\n\ncmatrix = TensorType(\'complex64\', (False, False))\nzmatrix = TensorType(\'complex128\', (False, False))\nfmatrix = TensorType(\'float32\', (False, False))\ndmatrix = TensorType(\'float64\', (False, False))\nbmatrix = TensorType(\'int8\', (False, False))\nwmatrix = TensorType(\'int16\', (False, False))\nimatrix = TensorType(\'int32\', (False, False))\nlmatrix = TensorType(\'int64\', (False, False))\n\n\ndef matrix(name=None, dtype=None):\n    """"""Return a symbolic matrix variable.\n\n    Parameters\n    ----------\n    dtype: numeric\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, False))\n    return type(name)\n\nmatrices, fmatrices, dmatrices, imatrices, lmatrices = _multi(\n    matrix, fmatrix, dmatrix, imatrix, lmatrix)\n\nint_matrix_types = bmatrix, wmatrix, imatrix, lmatrix\nfloat_matrix_types = fmatrix, dmatrix\ncomplex_matrix_types = cmatrix, zmatrix\n\ncrow = TensorType(\'complex64\', (True, False))\nzrow = TensorType(\'complex128\', (True, False))\nfrow = TensorType(\'float32\', (True, False))\ndrow = TensorType(\'float64\', (True, False))\nbrow = TensorType(\'int8\', (True, False))\nwrow = TensorType(\'int16\', (True, False))\nirow = TensorType(\'int32\', (True, False))\nlrow = TensorType(\'int64\', (True, False))\n\n\ndef row(name=None, dtype=None):\n    """"""Return a symbolic row variable (ndim=2, broadcastable=[True,False]).\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (True, False))\n    return type(name)\nrows, frows, drows, irows, lrows = _multi(row, frow, drow, irow, lrow)\n\nccol = TensorType(\'complex64\', (False, True))\nzcol = TensorType(\'complex128\', (False, True))\nfcol = TensorType(\'float32\', (False, True))\ndcol = TensorType(\'float64\', (False, True))\nbcol = TensorType(\'int8\', (False, True))\nwcol = TensorType(\'int16\', (False, True))\nicol = TensorType(\'int32\', (False, True))\nlcol = TensorType(\'int64\', (False, True))\n\n\ndef col(name=None, dtype=None):\n    """"""Return a symbolic column variable (ndim=2, broadcastable=[False,True]).\n\n    Parameters\n    ----------\n    dtype : numeric\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, True))\n    return type(name)\ncols, fcols, dcols, icols, lcols = _multi(col, fcol, dcol, icol, lcol)\n\nctensor3 = TensorType(\'complex64\', ((False,) * 3))\nztensor3 = TensorType(\'complex128\', ((False,) * 3))\nftensor3 = TensorType(\'float32\', ((False,) * 3))\ndtensor3 = TensorType(\'float64\', ((False,) * 3))\nbtensor3 = TensorType(\'int8\', ((False,) * 3))\nwtensor3 = TensorType(\'int16\', ((False,) * 3))\nitensor3 = TensorType(\'int32\', ((False,) * 3))\nltensor3 = TensorType(\'int64\', ((False,) * 3))\n\n\ndef tensor3(name=None, dtype=None):\n    """"""Return a symbolic 3-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, False, False))\n    return type(name)\n\ntensor3s, ftensor3s, dtensor3s, itensor3s, ltensor3s = _multi(\n    tensor3, ftensor3, dtensor3, itensor3, ltensor3)\n\nctensor4 = TensorType(\'complex64\', ((False,) * 4))\nztensor4 = TensorType(\'complex128\', ((False,) * 4))\nftensor4 = TensorType(\'float32\', ((False,) * 4))\ndtensor4 = TensorType(\'float64\', ((False,) * 4))\nbtensor4 = TensorType(\'int8\', ((False,) * 4))\nwtensor4 = TensorType(\'int16\', ((False,) * 4))\nitensor4 = TensorType(\'int32\', ((False,) * 4))\nltensor4 = TensorType(\'int64\', ((False,) * 4))\n\n\ndef tensor4(name=None, dtype=None):\n    """"""Return a symbolic 4-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, False, False, False))\n    return type(name)\ntensor4s, ftensor4s, dtensor4s, itensor4s, ltensor4s = _multi(\n    tensor4, ftensor4, dtensor4, itensor4, ltensor4)\n\nctensor5 = TensorType(\'complex64\', ((False,) * 5))\nztensor5 = TensorType(\'complex128\', ((False,) * 5))\nftensor5 = TensorType(\'float32\', ((False,) * 5))\ndtensor5 = TensorType(\'float64\', ((False,) * 5))\nbtensor5 = TensorType(\'int8\', ((False,) * 5))\nwtensor5 = TensorType(\'int16\', ((False,) * 5))\nitensor5 = TensorType(\'int32\', ((False,) * 5))\nltensor5 = TensorType(\'int64\', ((False,) * 5))\n\n\ndef tensor5(name=None, dtype=None):\n    """"""Return a symbolic 5-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, False, False, False, False))\n    return type(name)\ntensor5s, ftensor5s, dtensor5s, itensor5s, ltensor5s = _multi(\n    tensor5, ftensor5, dtensor5, itensor5, ltensor5)\n\nctensor6 = TensorType(\'complex64\', ((False,) * 6))\nztensor6 = TensorType(\'complex128\', ((False,) * 6))\nftensor6 = TensorType(\'float32\', ((False,) * 6))\ndtensor6 = TensorType(\'float64\', ((False,) * 6))\nbtensor6 = TensorType(\'int8\', ((False,) * 6))\nwtensor6 = TensorType(\'int16\', ((False,) * 6))\nitensor6 = TensorType(\'int32\', ((False,) * 6))\nltensor6 = TensorType(\'int64\', ((False,) * 6))\n\n\ndef tensor6(name=None, dtype=None):\n    """"""Return a symbolic 6-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False,) * 6)\n    return type(name)\ntensor6s, ftensor6s, dtensor6s, itensor6s, ltensor6s = _multi(\n    tensor6, ftensor6, dtensor6, itensor6, ltensor6)\n\nctensor7 = TensorType(\'complex64\', ((False,) * 7))\nztensor7 = TensorType(\'complex128\', ((False,) * 7))\nftensor7 = TensorType(\'float32\', ((False,) * 7))\ndtensor7 = TensorType(\'float64\', ((False,) * 7))\nbtensor7 = TensorType(\'int8\', ((False,) * 7))\nwtensor7 = TensorType(\'int16\', ((False,) * 7))\nitensor7 = TensorType(\'int32\', ((False,) * 7))\nltensor7 = TensorType(\'int64\', ((False,) * 7))\n\n\ndef tensor7(name=None, dtype=None):\n    """"""Return a symbolic 7-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False,) * 7)\n    return type(name)\ntensor7s, ftensor7s, dtensor7s, itensor7s, ltensor7s = _multi(\n    tensor7, ftensor7, dtensor7, itensor7, ltensor7)\n\n\nTensor = TensorType\n\n\n# This bizarre push-import avoids a circular dependency.\nelemwise.as_tensor_variable = as_tensor_variable\nelemwise.TensorType = TensorType\nelemwise.TensorVariable = TensorVariable\nelemwise.TensorConstant = TensorConstant\n\n#########################\n# Utilities\n#########################\n\n\ndef _scal_elemwise_with_nfunc(nfunc, nin, nout):\n    """"""\n    Replace a symbol definition with an elementwise version of the\n    corresponding scalar Op.  If it is not None, the nfunc argument\n    should be a string such that getattr(numpy, nfunc) implements\n    a vectorized version of the elemwise operation. nin is the number\n    of inputs expected by that function, and nout is the number of\n    **destination** inputs it takes. That is, the function should\n    take nin+nout inputs. nout == 0 means that the numpy function\n    does not take a numpy array argument to put its result in.\n\n    """"""\n    def construct(symbol):\n        symbolname = symbol.__name__\n        inplace = symbolname.endswith(\'_inplace\')\n        if inplace:\n            msg = ""inplace""\n        else:\n            msg = ""no_inplace""\n\n        n = ""Elemwise{%s,%s}"" % (symbolname, msg)\n\n        if inplace:\n            scalar_op = getattr(scal, symbolname[:-len(\'_inplace\')])\n            inplace_scalar_op = scalar_op.__class__(scal.transfer_type(0))\n            rval = elemwise.Elemwise(inplace_scalar_op, {0: 0}, name=n,\n                                     nfunc_spec=(nfunc and (nfunc, nin, nout)))\n        else:\n            scalar_op = getattr(scal, symbolname)\n            rval = elemwise.Elemwise(scalar_op, name=n,\n                                     nfunc_spec=(nfunc and (nfunc, nin, nout)))\n\n        if getattr(symbol, \'__doc__\', False):\n            rval.__doc__ = symbol.__doc__ + \'\\n\' + rval.__doc__\n\n        # for the meaning of this see the ./epydoc script\n        # it makes epydoc display rval as if it were a function, not an object\n        rval.__epydoc_asRoutine = symbol\n        rval.__module__ = \'tensor\'\n\n        pprint.assign(rval, printing.FunctionPrinter(symbolname))\n\n        return rval\n    return construct\n\n_scal_elemwise = _scal_elemwise_with_nfunc(None, None, None)\n\n\ndef _pack(x):\n    """"""\n    Convert x to a list if it is an iterable, otherwise wrap it in a list.\n    """"""\n    try:\n        return list(x)\n    except TypeError:\n        return [x]\n\n\ndef check_and_normalize_axes(x, axis):\n    """"""\n    Check axes, normalize and convert them to a Python list of integers.\n    Return an empty list if argument is None.\n\n    Parameters\n    ----------\n    x: Tensor variable\n    axis = Integer, tuple or list of integers\n\n    Returns\n    -------\n    axis: list of integers\n    """"""\n    x = as_tensor_variable(x)\n    if axis is None:\n        axis = []\n    elif (isinstance(axis, (integer_types, np.integer)) or\n            (isinstance(axis, np.ndarray) and axis.ndim == 0)):\n                axis = [int(axis)]\n    elif isinstance(axis, (tuple, list, np.ndarray)):\n        axis = [int(i) for i in axis]\n    elif isinstance(axis, Variable):\n        if NoneConst.equals(axis):\n            axis = []\n        elif not isinstance(axis, TensorConstant):\n            raise TypeError(""Computation needs a constant axis. Got %s"" % axis)\n        else:\n            assert axis.dtype in integer_dtypes\n            if (isinstance(axis.data, (integer_types, np.integer)) or\n                    (isinstance(axis.data, np.ndarray) and axis.data.ndim == 0)):\n                        axis = [int(axis.data)]\n            elif isinstance(axis.data, (list, np.ndarray)):\n                axis = [int(i) for i in axis.data]\n    else:\n        raise TypeError(""Axis must be an integer, tuple, list of integers or a TensorVariable. Got %s"" % axis)\n    if len(axis) > 0:\n        for i in range(len(axis)):\n            if axis[i] < 0:\n                axis[i] += x.type.ndim\n            if axis[i] < 0 or axis[i] >= x.type.ndim:\n                raise ValueError(""Computation needs a valid axis number for %d-D tensor. Got %d"" % (x.type.ndim, axis[i]))\n        axis = list(set(axis))\n        axis.sort()\n    return axis\n\n\n#########################\n# Casting Operations\n#########################\n\nclass TensorFromScalar(Op):\n\n    def make_node(self, s):\n        assert isinstance(s.type, scal.Scalar)\n        return Apply(self,\n                     [s],\n                     [tensor(dtype=s.type.dtype,\n                             broadcastable=())])\n\n    def perform(self, node, inp, out_):\n        s, = inp\n        out, = out_\n        out[0] = np.asarray(s)\n\n    def infer_shape(self, node, in_shapes):\n        return [()]\n\n    def grad(self, inp, grads):\n        s, = inp\n        dt, = grads\n        if s.type.dtype in float_dtypes:\n            assert dt.type.dtype in float_dtypes\n            return [scalar_from_tensor(dt)]\n\n        # If the input dtype is an integer, then so is the output dtype,\n        # and the ""zero"" gradient can be represented in that int dtype.\n        # Currently, theano.grad insists that the dtype of the returned\n        # gradient has a float dtype, so we use floatX.\n        if s.type.dtype in discrete_dtypes:\n            return [s.zeros_like().astype(theano.config.floatX)]\n\n        raise NotImplementedError(""grad not implemented for complex dtypes"")\n\ntensor_from_scalar = TensorFromScalar()\n\n\nclass ScalarFromTensor(Op):\n\n    def make_node(self, t):\n        assert isinstance(t.type, TensorType)\n        assert t.type.broadcastable == ()\n        return Apply(self,\n                     [t],\n                     [scal.get_scalar_type(dtype=t.type.dtype).make_variable()]\n                     )\n\n    def perform(self, node, inp, out_):\n        s, = inp\n        out, = out_\n        out[0] = s.flatten()[0]\n\n    def infer_shape(self, node, in_shapes):\n        return [()]\n\n    def grad(self, inp, grads):\n        s, = inp\n        dt, = grads\n        return [tensor_from_scalar(dt)]\n\n    def R_op(self, inputs, eval_points):\n        if None in eval_points:\n            return [None]\n        return self.make_node(*eval_points).outputs\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        x, = inputs\n        z, = outputs\n        fail = sub[\'fail\']\n        return """"""\n        %(z)s = ((dtype_%(x)s*)(PyArray_DATA(%(x)s)))[0];\n        """""" % locals()\n\n    def c_code_cache_version(self):\n        return (1,)\n\nscalar_from_tensor = ScalarFromTensor()\n\n\n# to be removed as we get the epydoc routine-documenting thing going\n# -JB 20080924\ndef _conversion(real_value, name):\n    __oplist_tag(real_value, \'casting\')\n    real_value.__module__ = \'tensor.basic\'\n    pprint.assign(real_value, printing.FunctionPrinter(name))\n    return real_value\n\n\n# These _conver_to_<type> functions have leading underscores to indicate that\n# they should not be called directly.  They do not perform sanity checks about\n# what types you are casting to what.  That logic is implemented by the\n# `cast()` function below.\n\n_convert_to_bool = _conversion(\n    elemwise.Elemwise(scal.convert_to_bool), \'bool\')\n""""""Cast to boolean""""""\n\n_convert_to_int8 = _conversion(\n    elemwise.Elemwise(scal.convert_to_int8), \'int8\')\n""""""Cast to 8-bit integer""""""\n\n_convert_to_int16 = _conversion(\n    elemwise.Elemwise(scal.convert_to_int16), \'int16\')\n""""""Cast to 16-bit integer""""""\n\n_convert_to_int32 = _conversion(\n    elemwise.Elemwise(scal.convert_to_int32), \'int32\')\n""""""Cast to 32-bit integer""""""\n\n_convert_to_int64 = _conversion(\n    elemwise.Elemwise(scal.convert_to_int64), \'int64\')\n""""""Cast to 64-bit integer""""""\n\n_convert_to_uint8 = _conversion(\n    elemwise.Elemwise(scal.convert_to_uint8), \'uint8\')\n""""""Cast to unsigned 8-bit integer""""""\n\n_convert_to_uint16 = _conversion(\n    elemwise.Elemwise(scal.convert_to_uint16), \'uint16\')\n""""""Cast to unsigned 16-bit integer""""""\n\n_convert_to_uint32 = _conversion(\n    elemwise.Elemwise(scal.convert_to_uint32), \'uint32\')\n""""""Cast to unsigned 32-bit integer""""""\n\n_convert_to_uint64 = _conversion(\n    elemwise.Elemwise(scal.convert_to_uint64), \'uint64\')\n""""""Cast to unsigned 64-bit integer""""""\n\n_convert_to_float16 = _conversion(\n    elemwise.Elemwise(scal.convert_to_float16), \'float16\')\n""""""Cast to half-precision floating point""""""\n\n_convert_to_float32 = _conversion(\n    elemwise.Elemwise(scal.convert_to_float32), \'float32\')\n""""""Cast to single-precision floating point""""""\n\n_convert_to_float64 = _conversion(\n    elemwise.Elemwise(scal.convert_to_float64), \'float64\')\n""""""Cast to double-precision floating point""""""\n\n_convert_to_complex64 = _conversion(\n    elemwise.Elemwise(scal.convert_to_complex64), \'complex64\')\n""""""Cast to single-precision complex""""""\n\n_convert_to_complex128 = _conversion(\n    elemwise.Elemwise(scal.convert_to_complex128), \'complex128\')\n""""""Cast to double-precision complex""""""\n\n_cast_mapping = {\n    \'bool\': _convert_to_bool,\n    \'int8\': _convert_to_int8,\n    \'int16\': _convert_to_int16,\n    \'int32\': _convert_to_int32,\n    \'int64\': _convert_to_int64,\n    \'uint8\': _convert_to_uint8,\n    \'uint16\': _convert_to_uint16,\n    \'uint32\': _convert_to_uint32,\n    \'uint64\': _convert_to_uint64,\n    \'float16\': _convert_to_float16,\n    \'float32\': _convert_to_float32,\n    \'float64\': _convert_to_float64,\n    \'complex64\': _convert_to_complex64,\n    \'complex128\': _convert_to_complex128}\n\n\n@constructor\ndef cast(x, dtype):\n    """"""Symbolically cast `x` to a Tensor of type `dtype`.""""""\n    if dtype == \'floatX\':\n        dtype = config.floatX\n\n    _x = as_tensor_variable(x)\n    if _x.type.dtype == dtype:\n        return _x\n    if _x.type.dtype.startswith(\'complex\') and not dtype.startswith(\'complex\'):\n        raise TypeError((\n            \'Casting from complex to real is ambiguous: consider real(), \'\n            \'imag(), angle() or abs()\'))\n    return _cast_mapping[dtype](x)\n\n##########################\n# Unary Operations\n##########################\n\n\nclass MaxAndArgmax(Op):\n    """"""\n    Calculate the max and argmax over a given axis or over all axes.\n\n    """"""\n    nin = 2  # tensor, axis\n    nout = 2  # max val, max idx\n    E_axis = \'invalid axis\'\n    params_type = Generic()\n    __props__ = (\'axis\',)\n    _f16_ok = True\n\n    def __init__(self, axis):\n        assert isinstance(axis, list)\n        self.axis = tuple(axis)\n\n    def get_params(self, node):\n        return self.axis\n\n    def make_node(self, x):\n        x = _as_tensor_variable(x)\n\n        # We keep the original broadcastable flags for dimensions on which\n        # we do not perform the max / argmax.\n        all_axes = set(self.axis)\n        broadcastable = [b for i, b in enumerate(x.type.broadcastable)\n                         if i not in all_axes]\n        inputs = [x]\n        outputs = [tensor(x.type.dtype, broadcastable, name=\'max\'),\n                   tensor(\'int64\', broadcastable, name=\'argmax\')]\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inp, outs, params):\n        x = inp[0]\n        axes = params\n        max, max_idx = outs\n        if axes is None:\n            axes = tuple(range(x.ndim))\n        else:\n            axes = tuple(int(ax) for ax in axes)\n        max[0] = theano._asarray(np.max(x, axes),\n                                 dtype=node.outputs[0].dtype)\n        # Numpy does not support multiple axes for argmax\n        # Work around\n        keep_axes = np.array([i for i in range(x.ndim) if i not in axes],\n                             dtype=\'int64\')\n        # Not-reduced axes in front\n        transposed_x = np.transpose(x, np.concatenate((keep_axes, axes)))\n        kept_shape = transposed_x.shape[:len(keep_axes)]\n        reduced_shape = transposed_x.shape[len(keep_axes):]\n\n        # Numpy.prod returns 1.0 when arg is empty, so we cast it to int64\n        # Otherwise reshape would complain citing float arg\n        new_shape = kept_shape + (np.prod(reduced_shape, dtype=\'int64\'),)\n        reshaped_x = transposed_x.reshape(new_shape)\n\n        max_idx[0] = theano._asarray(np.argmax(reshaped_x, axis=-1),\n                                     dtype=\'int64\')\n\n    def c_code(self, node, name, inp, out, sub):\n        if len(self.axis) != 1 and len(self.axis) != node.inputs[0].ndim:\n            raise NotImplementedError(""NumPy C-API can compute max and argmax only for 1 axis or for all axes."")\n        x = inp[0]\n        axis = sub[\'params\']\n        max, argmax = out\n        fail = sub[""fail""]\n        ret = """"""\n        #if PY_MAJOR_VERSION >= 3\n            #ifndef PyInt_AS_LONG\n                #define PyInt_AS_LONG PyLong_AS_LONG\n            #endif\n        #endif\n\n        if (PyTuple_GET_SIZE(%(axis)s) == PyArray_NDIM(%(x)s)) {\n            axis = NPY_MAXDIMS;\n        } else if(PyTuple_GET_SIZE(%(axis)s) == 1) {\n            PyObject* axis_object = PyTuple_GET_ITEM(%(axis)s, 0);\n            axis = (int)PyInt_AS_LONG(axis_object);\n            if (axis > PyArray_NDIM(%(x)s)-1 || axis < -PyArray_NDIM(%(x)s)) {\n                PyErr_SetString(PyExc_ValueError,\n                ""MaxAndArgmax: bad axis argument"");\n                %(fail)s\n            }\n        } else {\n            PyErr_SetString(PyExc_NotImplementedError,\n            ""MaxAndArgmax: NumPy C-API can compute max and argmax only for 1 axis or for all axes."");\n            %(fail)s\n        }\n\n        Py_CLEAR(%(max)s);\n        Py_CLEAR(%(argmax)s);//todo pass them as out parameter.\n\n        %(max)s = (PyArrayObject*)PyArray_Max(%(x)s, axis, NULL);\n        if (%(max)s == NULL) {\n            %(fail)s;\n        }\n        if (!PyArray_CheckExact(%(max)s)) {\n            %(max)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(max)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);\n            if(%(max)s == NULL){\n                %(fail)s;\n            }\n        }\n\n        %(argmax)s = (PyArrayObject*)PyArray_ArgMax(%(x)s, axis, NULL);\n        if (%(argmax)s == NULL) {\n            Py_CLEAR(%(max)s);\n            %(fail)s;\n        }\n        if (!PyArray_CheckExact(%(argmax)s)) {\n            %(argmax)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(argmax)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);\n            if(%(argmax)s == NULL){\n                %(fail)s;\n            }\n        }\n        if (PyArray_TYPE(%(argmax)s) != NPY_INT64) {\n            PyObject * tmp = PyArray_Cast(%(argmax)s, NPY_INT64);\n            if (NULL == tmp){\n                %(fail)s;\n            }\n            Py_DECREF(%(argmax)s);\n            %(argmax)s = (PyArrayObject*)tmp;\n        }\n        """"""\n        return ret % locals()\n\n    def c_code_cache_version(self):\n        return (5,)\n\n    def infer_shape(self, node, shapes):\n        ishape = shapes[0]\n        rval = tuple(ishape[i] for (i, b) in enumerate(\n            node.inputs[0].type.broadcastable) if i not in self.axis)\n        return [rval, rval]\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None, None]\n        if len(self.axis) != 1:\n            raise ValueError((\'R_op supported for arg_max only for \'\n                              \'one axis!\'))\n        if self.axis[0] > 1:\n            raise ValueError((\'R_op supported for arg_max only when \'\n                              \' axis is 0 or 1\'))\n        if inputs[0].ndim != 2:\n            raise ValueError((\'R_op supported for arg_max only when \'\n                              \' input is a matrix\'))\n        max_vals, max_pos = self.make_node(*inputs).outputs\n        if self.axis[0] == 0:\n            return [eval_points[0][max_pos,\n                                   arange(eval_points[0].shape[1])], None]\n        else:\n            return [eval_points[0][arange(eval_points[0].shape[0]),\n                                   max_pos], None]\n\n    def grad(self, inp, grads):\n        # The strict sense mathematical gradient of the maximum function is\n        # not calculated here for it is not defined at every point where some\n        # coordinates are identical. However, since the latter set has null\n        # Lebesgue measure, the result may be interpreted as weak gradient.\n\n        # @note: This function should work correctly for L{vector}s.\n        # (x, y), (gz, gw)\n        # gz*dz/dx + gw*dw/dx, gz*dz/dy + gw*dw/dy\n        # gMax * dMax/dx + gArgMax * dArgMax/dx,\n        # gMax * dMax/daxis + gArgMax * dArgMax/daxis\n        # g_max has one less dimension than x, so you need to complete\n        # g_max to x\'s shape when axis=0 the broadcasting mechanism\n        # does it automatically\n        x = inp[0]\n        axis = _as_tensor_variable(self.axis)\n        g_max, g_max_idx = grads\n\n        g_max_disconnected = isinstance(g_max.type, DisconnectedType)\n        g_max_idx_disconnected = isinstance(g_max_idx.type, DisconnectedType)\n\n        # if the op is totally disconnected, so are its inputs\n        if g_max_disconnected and g_max_idx_disconnected:\n            return [DisconnectedType()(), DisconnectedType()()]\n\n        # if the max is disconnected but the argmax is not,\n        # the gradient on its inputs is zero\n        if g_max_disconnected:\n            return [x.zeros_like()]\n        if NoneConst.equals(axis):\n            axis_ = list(range(x.ndim))\n        else:\n            axis_ = axis\n        xmax = max(x, axis_)\n\n        # Raise the g_max and xmax to the same number of dim as the input.\n        pattern = []\n        out_dim = 0\n        if NoneConst.equals(axis):\n            # We are taking the max/argmax over all dimensions.\n            axis = None\n        for i in xrange(x.ndim):\n            if axis is None or i in axis.data:\n                pattern.append(\'x\')\n            else:\n                pattern.append(out_dim)\n                out_dim += 1\n        g_max_pad = DimShuffle(g_max.broadcastable, pattern)(g_max)\n        xmax_pad = DimShuffle(xmax.broadcastable, pattern)(xmax)\n\n        # Set the grad to the correct position.\n        g_x = eq(xmax_pad, x) * g_max_pad\n        return g_x,\n\n\nclass Argmax(Op):\n    """"""\n    Calculate the argmax over a given axis or over all axes.\n    """"""\n    nin = 2  # tensor, axis\n    nout = 1\n    E_axis = \'invalid axis\'\n    __props__ = (\'axis\',)\n    _f16_ok = True\n\n    params_type = ParamsType(c_axis=scal.int64)\n\n    def __init__(self, axis):\n        if axis is not None:\n            axis = tuple(axis)\n        self.axis = tuple(axis)\n\n    def get_params(self, node):\n        if self.axis is not None and len(self.axis) == 1:\n            c_axis = np.int64(self.axis[0])\n        else:\n            # The value here doesn\'t matter, it won\'t be used\n            c_axis = np.int64(-1)\n        return self.params_type.get_params(c_axis=c_axis)\n\n    def make_node(self, x, axis=None):\n        x = _as_tensor_variable(x)\n        if self.axis is None:\n            all_axes = list(range(x.ndim))\n        else:\n            all_axes = self.axis\n        inputs = [x]\n\n        # We keep the original broadcastable flags for dimensions on which\n        # we do not perform the argmax.\n        broadcastable = [b for i, b in enumerate(x.type.broadcastable)\n                         if i not in all_axes]\n        outputs = [tensor(\'int64\', broadcastable, name=\'argmax\')]\n        return Apply(self, inputs, outputs)\n\n    def prepare_node(self, node, storage_map, compute_map, impl):\n        if len(node.inputs) == 2:\n            raise ValueError(\'You are trying to compile a graph with an old Argmax node.  Either reoptimize your graph or rebuild it to get the new node format.\')\n\n    def perform(self, node, inp, outs, params):\n        x, = inp\n        axes = self.axis\n        max_idx, = outs\n        if axes is None:\n            axes = tuple(range(x.ndim))\n\n        # Numpy does not support multiple axes for argmax\n        # Work around\n        keep_axes = np.array([i for i in range(x.ndim) if i not in axes],\n                             dtype=\'int64\')\n        # Not-reduced axes in front\n        transposed_x = np.transpose(x, np.concatenate((keep_axes,\n                                                       axes)))\n        kept_shape = transposed_x.shape[:len(keep_axes)]\n        reduced_shape = transposed_x.shape[len(keep_axes):]\n        new_shape = kept_shape + (np.prod(reduced_shape),)\n        reshaped_x = transposed_x.reshape(new_shape)\n\n        max_idx[0] = theano._asarray(np.argmax(reshaped_x, axis=-1),\n                                     dtype=\'int64\')\n\n    def c_code(self, node, name, inp, out, sub):\n        x, = inp\n        argmax, = out\n        fail = sub[""fail""]\n        params = sub[""params""]\n        if self.axis is None:\n            axis_code = ""axis = NPY_MAXDIMS;""\n        else:\n            if len(self.axis) > 1:\n                raise NotImplementedError()\n            # params is only used here for now\n            axis_code = """"""\n            axis = %(params)s->c_axis;\n            if(axis > PyArray_NDIM(%(x)s)-1 || axis < -PyArray_NDIM(%(x)s)){\n                PyErr_SetString(PyExc_ValueError,\n                ""Argmax, bad axis argument"");\n                %(fail)s\n            }\n            """""" % locals()\n        ret = """"""\n        int axis;\n\n        Py_CLEAR(%(argmax)s);//todo pass them as out parameter.\n        %(axis_code)s\n\n        %(argmax)s = (PyArrayObject*)PyArray_ArgMax(%(x)s, axis, NULL);\n        if(%(argmax)s == NULL){\n            %(fail)s;\n        }\n        if(!PyArray_CheckExact(%(argmax)s)){\n            %(argmax)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(argmax)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);\n            if(%(argmax)s == NULL){\n                %(fail)s;\n            }\n        }\n        if(PyArray_TYPE(%(argmax)s) != NPY_INT64){\n            PyObject * tmp = PyArray_Cast(%(argmax)s, NPY_INT64);\n            if (NULL == tmp){\n                %(fail)s;\n            }\n            Py_DECREF(%(argmax)s);\n            %(argmax)s = (PyArrayObject*)tmp;\n        }\n        """"""\n        return ret % locals()\n\n    def c_code_cache_version(self):\n        return (1,)\n\n    def infer_shape(self, node, shapes):\n        ishape, = shapes\n        if self.axis is None:\n            return [()]\n        rval = tuple([ishape[i] for (i, b) in enumerate(\n            node.inputs[0].type.broadcastable) if i not in self.axis])\n        return [rval]\n\n    def grad(self, inp, grads):\n        x, = inp\n\n        return [x.zeros_like()]\n\n\ndef makeKeepDims(x, y, axis):\n    """"""\n    Reintroduces in y with length one the axes of x which have been left out\n    in a prior reduction of x. With this option, the resulting tensor will\n    broadcast correctly against the original tensor x.\n\n    """"""\n    x = as_tensor_variable(x)\n    y = as_tensor_variable(y)\n\n    if axis is None:\n        axis = list(range(x.type.ndim))\n    elif isinstance(axis, (integer_types, np.integer)):\n        axis = [axis]\n    elif isinstance(axis, np.ndarray) and axis.ndim == 0:\n        axis = [int(axis)]\n    else:\n        axis = [int(a) for a in axis]\n    newaxis = []\n    for a in axis:\n        if not isinstance(a, integer_types):\n            raise ValueError(\n                ""keepdims option can be used only with constant axis"")\n        if a < 0:\n            a += x.type.ndim\n        newaxis.append(a)\n    i = 0\n    new_dims = []\n    for j, _ in enumerate(x.type.broadcastable):\n        if j in newaxis:\n            new_dims.append(\'x\')\n        else:\n            new_dims.append(i)\n            i += 1\n    return DimShuffle(y.type.broadcastable, new_dims)(y)\n\n\n@constructor\ndef max_and_argmax(a, axis=None, keepdims=False):\n    """"""\n    Returns maximum elements and their indices obtained by iterating over\n    given axis.\n\n    When axis is None (the default value), the max is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims : bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    """"""\n    # Check axis and convert it to a Python list of integers.\n    # Axis will be used as an op param of MaxAndArgmax.\n    a = as_tensor_variable(a)\n    axis = check_and_normalize_axes(a, axis)\n    if len(axis) == 0:\n        axis = list(range(a.type.ndim))\n    out, argout = MaxAndArgmax(axis)(a)\n\n    if keepdims:\n        out = makeKeepDims(a, out, axis)\n        argout = makeKeepDims(a, argout, axis)\n    return [out, argout]\n\n\n@constructor\ndef max(x, axis=None, keepdims=False):\n    """"""\n    Returns maximum elements obtained by iterating over given axis.\n\n    When axis is None (the default value), the max is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    Notes\n    -----\n    We return an error as numpy when we reduce a dim with a shape of 0.\n\n    # We have a choice of implementing this call with the\n    # CAReduce op or the MaxAndArgmax op.\n\n    # MaxAndArgmax supports grad and Rop, so we prefer to use that.\n    # CAReduce is faster, but optimizations will replace MaxAndArgmax[0]\n    # with CAReduce at compile time, so at this stage the important\n    # thing is supporting all user interface features, not speed.\n    # Some cases can be implemented only with CAReduce.\n\n    # We thus prefer to use MaxAndArgmax, if possible. It does not\n    # support all axis arguments, so we may need to fall back to CAReduce.\n\n    try:\n        out = max_and_argmax(x, axis)[0]\n    except Exception:\n        out = CAReduce(scal.maximum, axis)(x)\n\n    if keepdims:\n        out = makeKeepDims(x, out, axis)\n    return out\n\n\n@constructor\ndef argmax(x, axis=None, keepdims=False):\n    """"""\n    Returns indices of maximum elements obtained by iterating over given axis.\n\n    When axis is None (the default value), the argmax is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims : bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    """"""\n    argout = max_and_argmax(x, axis)[1]\n\n    if keepdims:\n        argout = makeKeepDims(x, argout, axis)\n    return argout\n\n\n@constructor\ndef min(x, axis=None, keepdims=False):\n    """"""\n    Returns minimum elements obtained by iterating over given axis.\n\n    When axis is None (the default value), the min is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    """"""\n    x = as_tensor_variable(x)\n    str_x_type = str(x.dtype)\n    if str_x_type.startswith(\'float\') or str_x_type in int_dtypes:\n        return -max(-x, axis=axis, keepdims=keepdims)\n    elif str_x_type in uint_dtypes:\n        itype = np.iinfo(x.dtype)\n        max_val = np.array(itype.max, dtype=itype.dtype)\n        return max_val - max(max_val - x, axis=axis, keepdims=keepdims)\n    elif str_x_type == \'bool\':\n        return ~max(~x, axis=axis, keepdims=keepdims)\n    else:\n        # Be careful about unsigned integers, complex\n        raise NotImplementedError()\n\n\n@constructor\ndef argmin(x, axis=None, keepdims=False):\n    """"""\n    Returns indices of minimum elements obtained by iterating over given axis.\n\n    When axis is None (the default value), the argmin is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    """"""\n    x = as_tensor_variable(x)\n    str_x_type = str(x.dtype)\n    if str_x_type.startswith(\'float\') or str_x_type in int_dtypes:\n        return argmax(-x, axis=axis, keepdims=keepdims)\n    elif str_x_type in uint_dtypes:\n        itype = np.iinfo(x.dtype)\n        return argmax(itype.max - x, axis=axis, keepdims=keepdims)\n    elif str_x_type == \'bool\':\n        return argmax(~x, axis=axis, keepdims=keepdims)\n    else:\n        # Be careful about unsigned integers, complex\n        raise NotImplementedError()\n\n\n@constructor\ndef smallest(*args):\n    """"""\n    Return the [elementwise] smallest of a variable number of arguments.\n\n    Like python\'s min.\n\n    """"""\n    if len(args) == 2:\n        a, b = args\n        return switch(a < b, a, b)\n    else:\n        return min(stack(args), axis=0)\n\n\n@constructor\ndef largest(*args):\n    """"""\n    Return the [elementwise] largest of a variable number of arguments.\n\n    Like python\'s max.\n\n    """"""\n    if len(args) == 2:\n        a, b = args\n        return switch(a > b, a, b)\n    else:\n        return max(stack(args), axis=0)\n\n\n##########################\n# Comparison\n##########################\n\n@_scal_elemwise\ndef lt(a, b):\n    """"""a < b""""""\n\n\n@_scal_elemwise\ndef gt(a, b):\n    """"""a > b""""""\n\n\n@_scal_elemwise\ndef le(a, b):\n    """"""a <= b""""""\n\n\n@_scal_elemwise\ndef ge(a, b):\n    """"""a >= b""""""\n\n\n@_scal_elemwise\ndef eq(a, b):\n    """"""a == b""""""\n\n\n@_scal_elemwise\ndef neq(a, b):\n    """"""a != b""""""\n\n\n@_scal_elemwise\ndef isnan(a):\n    """"""isnan(a)""""""\n\n# Rename isnan to isnan_ to allow to bypass it when not needed.\n# glibc 2.23 don\'t allow isnan on int, so we remove it from the graph.\nisnan_ = isnan\n\n\ndef isnan(a):\n    """"""isnan(a)""""""\n    a = as_tensor_variable(a)\n    if a.dtype in discrete_dtypes:\n        return alloc(np.asarray(False, dtype=""bool""),\n                     *[a.shape[i] for i in range(a.ndim)])\n    return isnan_(a)\n\n\n@_scal_elemwise\ndef isinf(a):\n    """"""isinf(a)""""""\n\n# Rename isnan to isnan_ to allow to bypass it when not needed.\n# glibc 2.23 don\'t allow isnan on int, so we remove it from the graph.\nisinf_ = isinf\n\n\ndef isinf(a):\n    """"""isinf(a)""""""\n    a = as_tensor_variable(a)\n    if a.dtype in discrete_dtypes:\n        return alloc(np.asarray(False, dtype=""bool""),\n                     *[a.shape[i] for i in range(a.ndim)])\n    return isinf_(a)\n\n\ndef allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    """"""\n    Implement Numpy\'s ``allclose`` on tensors.\n\n    ``absolute(a - b) <= (atol + rtol * absolute(b))``\n\n    Parameters\n    ----------\n    a : tensor\n        Input to compare.\n    b : tensor\n        Input to compare.\n    rtol : float\n        The relative tolerance parameter.\n    atol : float\n        The absolute tolerance parameter.\n    equal_nan: bool\n        Whether to consider nan\'s in the same place to be close.\n\n    Returns\n    -------\n    bool\n        A boolean value (of type int8 returned by the tensor elementwise `all`\n        function) whether all elements in a and b are in the tolerance range\n        defined above.\n\n    Notes\n    -----\n    Not a symmetric equation. See Numpy\'s documentation.\n\n    """"""\n    return all(isclose(a, b, rtol, atol, equal_nan))\n\n\ndef isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    """"""\n    Implements Numpy\'s ``isclose`` on tensors.\n\n    The tolerance values are positive, typically very small numbers. The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    ``absolute(a - b) <= (atol + rtol * absolute(b))``\n\n    Parameters\n    ----------\n    a : tensor\n        Input to compare.\n    b : tensor\n        Input to compare.\n    rtol : float\n        The relative tolerance parameter.\n    atol : float\n        The absolute tolerance parameter.\n    equal_nan : bool\n        Whether to consider nan\'s in the same place to be close\n\n    Returns\n    -------\n    int8\n        A boolean (int8) array where two arrays are element-wise equal\n        within a tolerance.\n\n    Notes\n    -----\n    Not a symmetric equation. See Numpy\'s documentation.\n\n    Examples\n    --------\n    >>> import theano\n    >>> import numpy as np\n    >>> a = theano._asarray([1e10, 1e-7], dtype=""float64"")\n    >>> b = theano._asarray([1.00001e10, 1e-8], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 0], dtype=int8)\n    >>> a = theano._asarray([1e10, 1e-8], dtype=""float64"")\n    >>> b = theano._asarray([1.00001e10, 1e-9], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 1], dtype=int8)\n    >>> a = theano._asarray([1e10, 1e-8], dtype=""float64"")\n    >>> b = theano._asarray([1.0001e10, 1e-9], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([0, 1], dtype=int8)\n    >>> a = theano._asarray([1.0, np.nan], dtype=""float64"")\n    >>> b = theano._asarray([1.0, np.nan], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 0], dtype==int8)\n    >>> a = theano._asarray([1.0, np.nan], dtype=""float64"")\n    >>> b = theano._asarray([1.0, np.nan], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b, equal_nan=True).eval()\n    array([1, 1], dtype==int8)\n    >>> a = theano._asarray([1.0, np.inf], dtype=""float64"")\n    >>> b = theano._asarray([1.0, -np.inf], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 0], dtype==int8)\n    >>> a = theano._asarray([1.0, np.inf], dtype=""float64"")\n    >>> b = theano._asarray([1.0, np.inf], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 1], dtype==int8)\n\n    """"""\n    # close will be an int8 array of 1 where within tolerance\n    # and 0 where not within tolerance or there was a nan or inf value.\n    diff = abs(a - b)\n    tolerance = atol + rtol * abs(b)\n    close_prelim = le(diff, tolerance)\n\n    a_nan = isnan(a)\n    b_nan = isnan(b)\n    nans = bitwise_or(a_nan, b_nan)\n\n    a_inf = isinf(a)\n    b_inf = isinf(b)\n    infs = bitwise_or(a_inf, b_inf)\n\n    nans_or_infs = bitwise_or(nans, infs)\n\n    # close is now an array of 0\'s except where elements are not nan or inf\n    # and are within the tolerance.\n    close = bitwise_and(close_prelim, bitwise_not(nans_or_infs))\n\n    # deal with signed inf values. this will make an array inf_eq of 0\'s\n    # except where inf values have the same sign.\n    both_infs = bitwise_and(a_inf, b_inf)\n    inf_signs_eq = eq(a_inf * sgn(a), b_inf * sgn(b))\n    inf_eq = bitwise_and(both_infs, inf_signs_eq)\n\n    # now create the potential result combining close and inf_eq\n    close_with_infs = bitwise_or(close, inf_eq)\n\n    # deal with comparing nan\'s.\n    if equal_nan:\n        both_nans = bitwise_and(a_nan, b_nan)\n        return bitwise_or(close_with_infs, both_nans)\n    # otherwise nan\'s aren\'t considered close.\n    else:\n        return close_with_infs\n\n\n##########################\n# Condition\n##########################\n\n@_scal_elemwise\ndef switch(cond, ift, iff):\n    """"""if cond then ift else iff""""""\n\nwhere = switch\n##########################\n# Bit-wise\n##########################\n\n\n@_scal_elemwise\ndef and_(a, b):\n    """"""bitwise a & b""""""\nbitwise_and = and_  # numpy name for it\n\n\n@_scal_elemwise\ndef or_(a, b):\n    """"""bitwise a | b""""""\nbitwise_or = or_  # numpy name for it\n\n\n@_scal_elemwise\ndef xor(a, b):\n    """"""bitwise a ^ b""""""\nbitwise_xor = xor  # numpy name for it\n\n\n@_scal_elemwise\ndef invert(a):\n    """"""bitwise ~a""""""\nbitwise_not = invert  # numpy alias for it\n\n\n##########################\n# Math\n##########################\n\n@_scal_elemwise\ndef abs_(a):\n    """"""|`a`|\n\n    TensorVariable overloads the `TensorVariable.__abs__` operator so that\n    this function is called when you type abs(a).\n\npprint.assign(abs_, printing.PatternPrinter((\'|%(0)s|\', -1000)))\n\n\n@_scal_elemwise\ndef exp(a):\n    """"""e^`a`""""""\n\n\n@_scal_elemwise\ndef exp2(a):\n    """"""2^`a`""""""\n\n\n@_scal_elemwise\ndef expm1(a):\n    """"""e^`a` - 1""""""\n\n\n@_scal_elemwise\ndef neg(a):\n    """"""-a""""""\n\n\n# numpy.reciprocal does integer division on integer inputs\n# (which is not very interesting)\n@_scal_elemwise\ndef inv(a):\n    """"""1.0/a""""""\n\n\n@_scal_elemwise\ndef log(a):\n    """"""base e logarithm of a""""""\n\n\n@_scal_elemwise\ndef log2(a):\n    """"""base 2 logarithm of a""""""\n\n\n@_scal_elemwise\ndef log10(a):\n    """"""base 10 logarithm of a""""""\n\n\n@_scal_elemwise\ndef log1p(a):\n    """"""log(1+a)""""""\n\n\n@_scal_elemwise\ndef sgn(a):\n    """"""sign of a""""""\n\n\n@_scal_elemwise\ndef ceil(a):\n    """"""ceiling of a""""""\n\n\n@_scal_elemwise\ndef floor(a):\n    """"""floor of a""""""\n\n\n@_scal_elemwise\ndef trunc(a):\n    """"""trunc of a""""""\n\n\n@constructor\ndef iround(a, mode=None):\n    """"""cast(round(a,mode),\'int64\')""""""\n    return cast(round(a, mode), \'int64\')\n\n\n@constructor\ndef round(a, mode=None):\n    """"""round_mode(a) with mode in [half_away_from_zero, half_to_even].\n    Default to half_to_even.""""""\n    if mode is None:\n        mode = ""half_to_even""\n        if config.warn.round:\n            warnings.warn(\n                ""theano.tensor.round() changed its default from""\n                "" `half_away_from_zero` to `half_to_even` to have""\n                "" the same default as NumPy. Use the Theano flag""\n                "" `warn.round=False` to disable this warning."")\n    if mode == ""half_away_from_zero"":\n        return round_half_away_from_zero(a)\n    elif mode == ""half_to_even"":\n        return round_half_to_even(a)\n    else:\n        raise Exception(""round mode %s is not implemented."" % mode)\n\n\n@_scal_elemwise\ndef round_half_to_even(a):\n    """"""round_half_to_even(a)""""""\n\n\n@_scal_elemwise\ndef round_half_away_from_zero(a):\n    """"""round_half_away_from_zero(a)""""""\n\n\n@_scal_elemwise\ndef sqr(a):\n    """"""square of a""""""\n\n\n# alias to sqr, included to maintain similarity with numpy interface\nsquare = sqr\n\n\ndef cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None):\n    """"""Calculate the covariance matrix.\n    Covariance indicates the level to which two variables vary together.\n    If we examine N-dimensional samples, :math:`m = [x_1, x_2, ... x_N]^T`,\n    then the covariance matrix element :math:`C_{ij}` is the covariance of\n    :math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance\n    of :math:`x_i`. Code and docstring ported from numpy.\n    ----------\n    m : array_like\n        A 2-D array containing multiple variables and observations.\n        Each row of `m` represents a variable, and each column is\n        observations of all those variables.\n    y : array_like, optional\n        An additional set of variables and observations. `y` has the same form\n        as that of `m`.\n    rowvar : bool, optional\n        If `rowvar` is True (default), then each row represents a\n        variable, with observations in the columns. Otherwise, the relationship\n        is transposed: each column represents a variable, while the rows\n        contain observations.\n    bias : bool, optional\n        Default normalization (False) is by ``(N - 1)``, where ``N`` is the\n        number of observations given (unbiased estimate). If `bias` is True, then\n        normalization is by ``N``. These values can be overridden by using the\n        keyword ``ddof``.\n    ddof : int, optional\n        If not ``None`` the default value implied by `bias` is overridden.\n        The default value is ``None``.\n    Returns\n    -------\n    out : The covariance matrix of the variables.\n    """"""\n\n    if fweights is not None:\n        raise NotImplementedError(\'fweights are not implemented\')\n    if aweights is not None:\n        raise NotImplementedError(\'aweights are not implemented\')\n\n    if not rowvar and m.shape[0] != 1:\n        m = m.T\n\n    if y is not None:\n        if not rowvar and y.shape[0] != 1:\n            y = y.T\n        m = theano.tensor.concatenate((m, y), axis=0)\n\n    if ddof is None:\n        if not bias:\n            ddof = 1\n        else:\n            ddof = 0\n\n    # Determine the normalization\n    fact = m.shape[1] - ddof\n\n    m -= m.mean(axis=1, keepdims=1)\n    c = m.dot(m.T)\n    c *= theano.tensor.constant(1) / fact\n    return c.squeeze()\n\n\n@_scal_elemwise\ndef sqrt(a):\n    """"""square root of a""""""\n\n\n@_scal_elemwise\ndef deg2rad(a):\n    """"""convert degree a to radian""""""\n\n\n@_scal_elemwise\ndef rad2deg(a):\n    """"""convert radian a to degree""""""\n\n\n@_scal_elemwise\ndef cos(a):\n    """"""cosine of a""""""\n\n\n@_scal_elemwise\ndef arccos(a):\n    """"""arccosine of a""""""\n\n\n@_scal_elemwise\ndef sin(a):\n    """"""sine of a""""""\n\n\n@_scal_elemwise\ndef arcsin(a):\n    """"""arcsine of a""""""\n\n\n@_scal_elemwise\ndef tan(a):\n    """"""tangent of a""""""\n\n\n@_scal_elemwise\ndef arctan(a):\n    """"""arctangent of a""""""\n\n\n@_scal_elemwise\ndef arctan2(a, b):\n    """"""arctangent of a / b""""""\n\n\n@_scal_elemwise\ndef cosh(a):\n    """"""hyperbolic cosine of a""""""\n\n\n@_scal_elemwise\ndef arccosh(a):\n    """"""hyperbolic arc cosine of a""""""\n\n\n@_scal_elemwise\ndef sinh(a):\n    """"""hyperbolic sine of a""""""\n\n\n@_scal_elemwise\ndef arcsinh(a):\n    """"""hyperbolic arc sine of a""""""\n\n\n@_scal_elemwise\ndef tanh(a):\n    """"""hyperbolic tangent of a""""""\n\n\n@_scal_elemwise\ndef arctanh(a):\n    """"""hyperbolic arc tangent of a""""""\n\n\n@_scal_elemwise\ndef erf(a):\n    """"""error function""""""\n\n\n@_scal_elemwise\ndef erfc(a):\n    """"""complementary error function""""""\n\n\n@_scal_elemwise\ndef erfcx(a):\n    """"""scaled complementary error function""""""\n\n\n@_scal_elemwise\ndef erfinv(a):\n    """"""inverse error function""""""\n\n\n@_scal_elemwise\ndef erfcinv(a):\n    """"""inverse complementary error function""""""\n\n\n@_scal_elemwise\ndef gamma(a):\n    """"""gamma function""""""\n\n\n@_scal_elemwise\ndef gammaln(a):\n    """"""log gamma function""""""\n\n\n@_scal_elemwise\ndef psi(a):\n    """"""derivative of log gamma function""""""\n\n\n@_scal_elemwise\ndef tri_gamma(a):\n    """"""second derivative of the log gamma function""""""\n\n\n@_scal_elemwise\ndef chi2sf(x, k):\n    """"""chi squared survival function""""""\n\n\n@_scal_elemwise\ndef gammainc(k, x):\n    """"""Regularized lower gamma function""""""\n\n\n@_scal_elemwise\ndef gammaincc(k, x):\n    """"""Regularized upper gamma function""""""\n\n\n@_scal_elemwise\ndef gammau(k, x):\n    """"""Upper incomplete gamma function.""""""\n\n\n@_scal_elemwise\ndef gammal(k, x):\n    """"""Lower incomplete gamma function.""""""\n\n\n@_scal_elemwise\ndef j0(x):\n    """"""Bessel function of the first kind of order 0.""""""\n\n\n@_scal_elemwise\ndef j1(x):\n    """"""Bessel function of the first kind of order 1.""""""\n\n\n@_scal_elemwise\ndef jv(v, x):\n    """"""Bessel function of the first kind of order v (real).""""""\n\n\n@_scal_elemwise\ndef i0(x):\n    """"""Modified Bessel function of the first kind of order 0.""""""\n\n\n@_scal_elemwise\ndef i1(x):\n    """"""Modified Bessel function of the first kind of order 1.""""""\n\n\n@_scal_elemwise\ndef iv(v, x):\n    """"""Modified Bessel function of the first kind of order v (real).""""""\n\n\n@_scal_elemwise\ndef real(z):\n    """"""Return real component of complex-valued tensor `z`""""""\n_tensor_py_operators.real = property(real)\n\n\n@_scal_elemwise\ndef imag(z):\n    """"""Return imaginary component of complex-valued tensor `z`""""""\n_tensor_py_operators.imag = property(imag)\n\n\n@_scal_elemwise\ndef angle(z):\n    """"""Return polar-coordinate angle of complex-valued tensor `z`""""""\n\n\n@_scal_elemwise  # numpy.complex cannot build tensors\ndef complex(real, imag):\n    """"""Return complex-valued tensor with `real` and `imag` components""""""\n\n\n@_scal_elemwise\ndef conj(z):\n    """"""Return the complex conjugate of `z`.""""""\n\n\n@_scal_elemwise\ndef complex_from_polar(abs, angle):\n    """"""Return complex-valued tensor from polar coordinate specification.""""""\n\n##########################\n# Misc\n##########################\n\n\n# fill, _fill_inplace = _elemwise(scal.second, \'fill\',\n# """"""fill WRITEME (elemwise)"""""")\n@_scal_elemwise\ndef second(a, b):\n    """"""Create a matrix by filling the shape of a with b""""""\n\nfill = second\npprint.assign(fill, printing.FunctionPrinter(\'fill\'))\n\n\n@constructor\ndef ones_like(model, dtype=None, opt=False):\n    """"""equivalent of numpy.ones_like\n    Parameters\n    ----------\n    model : tensor\n    dtype : data-type, optional\n    opt : If True, we will return a constant instead of a graph when possible.\n          Useful for Theano optimization, not for user building a graph as this\n          have the consequence that model isn\'t always in the graph.\n\n    Returns\n    -------\n    tensor\n        tensor the shape of model containing ones of the type of dtype.\n    """"""\n    if dtype is None:\n        dtype = model.type.dtype\n    ret = constant(1.0, dtype=dtype)\n    if opt and ret.type == model.type:\n        return ret\n    return fill(model, ret)\n\n\n@constructor\ndef zeros_like(model, dtype=None, opt=False):\n    """"""equivalent of numpy.zeros_like\n    Parameters\n    ----------\n    model : tensor\n    dtype : data-type, optional\n    opt : If True, we will return a constant instead of a graph when possible.\n          Useful for Theano optimization, not for user building a graph as this\n          have the consequence that model isn\'t always in the graph.\n\n    Returns\n    -------\n    tensor\n        tensor the shape of model containing zeros of the type of dtype.\n    """"""\n\n    if dtype is None:\n        dtype = model.type.dtype\n    ret = constant(0.0, dtype=dtype)\n    if opt and ret.type == model.type:\n        return ret\n    return fill(model, ret)\n\n\ndef zeros(shape, dtype=None):\n    """"""\n    Create a Tensor filled with zeros, closer to Numpy\'s syntax than ``alloc``.\n    """"""\n    if not isinstance(shape, (list, tuple, TensorVariable)):\n        shape = [shape]\n    if dtype is None:\n        dtype = config.floatX\n    return alloc(np.array(0, dtype=dtype), *shape)\n\n\ndef ones(shape, dtype=None):\n    """"""\n    Create a Tensor filled with ones, closer to Numpy\'s syntax than ``alloc``.\n    """"""\n    if not isinstance(shape, (list, tuple, TensorVariable)):\n        shape = [shape]\n    if dtype is None:\n        dtype = config.floatX\n    return alloc(np.array(1, dtype=dtype), *shape)\n\n\nclass Nonzero(gof.Op):\n    """"""\n    Return the indices of the elements that are non-zero.\n\n    Returns a matrix of shape (ndim, number of nonzero elements) such that\n    element (i,j) is the index in the ith dimension of the jth non-zero\n    element.\n\n    Note this is different than NumPy, which returns a tuple of arrays, one for\n    each dimension of the input array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n\n    Returns\n    -------\n    matrix\n        Matrix containing the indices of the non-zero elements of a.\n\n    See Also\n    --------\n    nonzero_values : Return the non-zero elements of the input array\n    flatnonzero : Return the indices of the non-zero elements of the\n        flattened input array.\n\n    """"""\n    __props__ = ()\n\n    def make_node(self, a):\n        a = as_tensor_variable(a)\n        if a.ndim == 0:\n            raise ValueError(\'Nonzero only supports non-scalar arrays.\')\n        output = [TensorType(dtype=\'int64\', broadcastable=(False, False))()]\n        return gof.Apply(self, [a], output)\n\n    def perform(self, node, inp, out_):\n        a = inp[0]\n        out, = out_\n\n        result_tuple = np.nonzero(a)\n        if len(result_tuple[0]) > 0:\n            result = np.vstack(result_tuple)\n        else:\n            result = np.zeros((len(result_tuple), 0))\n\n        out[0] = result.astype(\'int64\')\n\n    def grad(self, inp, grads):\n        return [grad_undefined(self, 0, inp[0])]\n\n\n_nonzero = Nonzero()\n\n\ndef nonzero(a, return_matrix=False):\n    """"""\n    Returns one of the following:\n\n        If return_matrix is False (default, same as NumPy):\n            A tuple of vector arrays such that the ith element of the jth array\n            is the index of the ith non-zero element of the input array in the\n            jth dimension.\n\n        If return_matrix is True (same as Theano Op):\n            Returns a matrix of shape (ndim, number of nonzero elements) such\n            that element (i,j) is the index in the ith dimension of the jth\n            non-zero element.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    return_matrix : bool\n        If True, returns a symbolic matrix. If False, returns a tuple of\n        arrays. Defaults to False.\n\n    Returns\n    -------\n    tuple of vectors or matrix\n\n    See Also\n    --------\n    nonzero_values : Return the non-zero elements of the input array\n    flatnonzero : Return the indices of the non-zero elements of the\n        flattened input array.\n\n    """"""\n    matrix_result = _nonzero(a)\n    if return_matrix:\n        return matrix_result\n    else:\n        if a.ndim > 0:\n            tuple_result = tuple([matrix_result[i] for i in xrange(a.ndim)])\n        else:\n            tuple_result = tuple([matrix_result[0]])\n        return tuple_result\n\n\ndef flatnonzero(a):\n    """"""\n    Return a vector of indices that are non-zero in the flattened version of a.\n\n    This is equivalent to nonzero(a.flatten(), return_matrix=True)[0]\n\n    Parameters\n    ----------\n    a : tensor\n        Input tensor\n\n    Returns\n    -------\n    vector\n        Output vector, containing the indices of the elements of `a.flatten()`\n        that are non-zero.\n\n    See Also\n    --------\n    nonzero : Return the indices of the non-zero elements of the input array.\n    nonzero_values : Return the non-zero elements of the input array\n\n    """"""\n    if a.ndim == 0:\n        raise ValueError(\'Nonzero only supports non-scalar arrays.\')\n    return nonzero(a.flatten(), return_matrix=True)[0]\n\n\ndef nonzero_values(a):\n    """"""\n    Return a vector of non-zero elements contained in the input array.\n\n    The following behavior works to extract non-zero elements from an array\n    in NumPy but is *NOT* supported by Theano:\n\n        a[numpy.nonzero(a)]\n\n    Instead, the nonzero_values function or method should be used:\n\n        tensor.nonzero_values(a)\n        a.nonzero_values()\n\n    This is equivalent to the following:\n\n        a.flatten()[tensor.flatnonzero(a)]\n\n    Parameters\n    ----------\n    a : tensor\n        Input tensor\n\n    Returns\n    -------\n    vector\n        Output vector, containing the non-zero elements of a.\n\n    See Also\n    --------\n    nonzero : Return the indices of the non-zero elements of the input array.\n    flatnonzero : Return the indices of the non-zero elements of the\n        flattened input array.\n\n    """"""\n    return a.flatten()[flatnonzero(a)]\n\n    __props__ = (""dtype"",)\n\n    def __init__(self, dtype=None):\n        if dtype is None:\n            dtype = config.floatX\n        self.dtype = dtype\n\n    def make_node(self, N, M, k):\n        N = as_tensor_variable(N)\n        M = as_tensor_variable(M)\n        k = as_tensor_variable(k)\n        return gof.Apply(\n            self,\n            [N, M, k],\n            [TensorType(dtype=self.dtype, broadcastable=(False, False))()])\n\n    def perform(self, node, inp, out_):\n        N, M, k = inp\n        out, = out_\n        out[0] = np.tri(N, M, k, dtype=self.dtype)\n\n    def infer_shape(self, node, in_shapes):\n        out_shape = [node.inputs[0], node.inputs[1]]\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        return [grad_undefined(self, i, inp[i]) for i in xrange(3)]\n\n\ndef tri(N, M=None, k=0, dtype=None):\n    """"""\n    An array with ones at and below the given diagonal and zeros elsewhere.\n\n    Parameters\n    ----------\n    N : int\n        Number of rows in the array.\n    M : int, optional\n        Number of columns in the array.\n        By default, `M` is taken equal to `N`.\n    k : int, optional\n        The sub-diagonal at and below which the array is filled.\n        `k` = 0 is the main diagonal, while `k` < 0 is below it,\n        and `k` > 0 is above.  The default is 0.\n    dtype : dtype, optional\n        Data type of the returned array.  The default is float.\n\n    Returns\n    -------\n    Array of shape (N, M)\n        Array with its lower triangle filled with ones and zero elsewhere;\n        in other words ``T[i,j] == 1`` for ``i <= j + k``, 0 otherwise.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    if M is None:\n        M = N\n    op = Tri(dtype)\n    return op(N, M, k)\n\n\ndef tril(m, k=0):\n    """"""\n    Lower triangle of an array.\n\n    Return a copy of an array with elements above the `k`-th diagonal zeroed.\n\n    Parameters\n    ----------\n    m : array_like, shape (M, N)\n        Input array.\n    k : int, optional\n        Diagonal above which to zero elements.  `k = 0` (the default) is the\n        main diagonal, `k < 0` is below it and `k > 0` is above.\n\n    Returns\n    -------\n    array, shape (M, N)\n        Lower triangle of `m`, of same shape and data-type as `m`.\n\n    See Also\n    --------\n    triu : Same thing, only for the upper triangle.\n\n    """"""\n    return m * tri(m.shape[0], m.shape[1], k=k, dtype=m.dtype)\n\n\ndef triu(m, k=0):\n    """"""\n    Upper triangle of an array.\n\n    Return a copy of a matrix with the elements below the `k`-th diagonal\n    zeroed.\n\n    Please refer to the documentation for `tril` for further details.\n\n    See Also\n    --------\n    tril : Lower triangle of an array.\n\n    """"""\n    return m * (1 - tri(m.shape[0], m.shape[1], k=k - 1, dtype=m.dtype))\n\n    __props__ = (""dtype"", )\n\n    def __init__(self, dtype=None):\n        if dtype is None:\n            dtype = config.floatX\n        self.dtype = dtype\n\n    def make_node(self, n, m, k):\n        n = as_tensor_variable(n)\n        m = as_tensor_variable(m)\n        k = as_tensor_variable(k)\n        assert n.ndim == 0\n        assert m.ndim == 0\n        assert k.ndim == 0\n        return gof.Apply(\n            self,\n            [n, m, k],\n            [TensorType(dtype=self.dtype, broadcastable=(False, False))()])\n\n    def perform(self, node, inp, out_):\n        n, m, k = inp\n        out, = out_\n        out[0] = np.eye(n, m, k, dtype=self.dtype)\n\n    def infer_shape(self, node, in_shapes):\n        out_shape = [node.inputs[0], node.inputs[1]]\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        return [grad_undefined(self, i, inp[i]) for i in xrange(3)]\n\n\ndef eye(n, m=None, k=0, dtype=None):\n    """"""Return a 2-D array with ones on the diagonal and zeros elsewhere.\n\n    Parameters\n    ----------\n    n : int\n        Number of rows in the output.\n    m : int, optional\n        Number of columns in the output. If None, defaults to `N`.\n    k : int, optional\n        Index of the diagonal: 0 (the default) refers to the main diagonal,\n        a positive value refers to an upper diagonal, and a negative value\n        to a lower diagonal.\n    dtype : data-type, optional\n        Data-type of the returned array.\n\n    Returns\n    -------\n    ndarray of shape (N,M)\n        An array where all elements are equal to zero, except for the `k`-th\n        diagonal, whose values are equal to one.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    if m is None:\n        m = n\n    localop = Eye(dtype)\n    return localop(n, m, k)\n\n\ndef identity_like(x):\n    return eye(x.shape[0], x.shape[1], k=0, dtype=x.dtype)\n\n\ndef alloc_validate_shape(shape):\n    sh = [as_tensor_variable(s) for s in shape]\n    bcast = []\n    for i, s in enumerate(sh):\n        def err_str():\n            if config.exception_verbosity == \'high\':\n                return \'\\n\' + min_informative_str(s)\n            else:\n                return str(s)\n        if s.type.dtype not in integer_dtypes:\n            s_as_str = err_str()\n            raise TypeError(\'Shape arguments to Alloc must be integers, \'\n                            \'but argument %s is not for apply node: %s\' %\n                            (i, s_as_str))\n        if s.ndim != 0:\n            s_as_str = err_str()\n            raise TypeError(\n                ""Each shape dimension to Alloc must be a scalar, "",\n                \'but dimension %s have %d dimensions for apply node: %s\' %\n                (i, s.ndim, s_as_str))\n\n        # if s is constant 1, then we\'re broadcastable in that dim\n        try:\n            const_shp = get_scalar_constant_value(s)\n        except NotScalarConstantError:\n            const_shp = None\n        bcast.append(1 == const_shp)\n    return sh, bcast\n\n\nclass Alloc(gof.Op):\n    """"""Create a Tensor from an initial value and a desired shape.\n\n    alloc(value, shape0, shape1, ..., shapeN)\n\n    Returns an N-dimensional tensor initialized by `value` using something\n    equivalent to\n\n        z = numpy.zeros(shape, value.dtype)\n        z += value\n\n    The result has N dimensions, has the dtype of `value` and is obtained by\n    broadcasting value over the output ndarray.\n\n    This Op is used to replace fill() during optimizations because after shapes\n    are lifted, the first argument to fill can often be pruned from the graph.\n\n    """"""\n    _f16_ok = True\n    __props__ = ()\n\n    def validate_shape(self, shape):\n        return alloc_validate_shape(shape)\n\n    def make_node(self, value, *shape):\n        v = as_tensor_variable(value)\n        sh, bcast = alloc_validate_shape(shape)\n        if v.ndim > len(sh):\n            raise TypeError(""The Alloc value to use has more dimensions""\n                            "" than the specified dimensions"",\n                            v.ndim, len(sh))\n        otype = TensorType(dtype=v.dtype, broadcastable=bcast)\n        return gof.Apply(self, [v] + sh, [otype()])\n\n    def perform(self, node, inputs, out_):\n        out, = out_\n        v = inputs[0]\n        sh = tuple([int(i) for i in inputs[1:]])\n        if out[0] is None or out[0].shape != sh:\n            if v.size == 1 and v.item() == 0:\n                out[0] = np.zeros(sh, dtype=v.dtype)\n            else:\n                out[0] = np.empty(sh, dtype=v.dtype)\n                out[0][...] = v  # broadcast v to fill us up\n        else:\n            # reuse the allocated memory.\n            out[0][...] = v  # broadcast v to fill us up\n\n    def c_code(self, node, name, inp, out, sub):\n        vv = inp[0]\n        ndim = len(inp[1:])\n        zz, = out\n        fail = sub[\'fail\']\n\n        code = """"""\n            npy_intp shape[%(ndim)s];\n            """""" % dict(ndim=ndim)\n\n        # Initialize shape\n        for i, shp_i in enumerate(inp[1:]):\n            code += """"""\n                shape[%(i)s] = ((dtype_%(shp_i)s*) PyArray_DATA(%(shp_i)s))[0];\n                """""" % dict(i=i, shp_i=shp_i)\n\n        code += """"""\n            int need_new_out = (NULL == %(zz)s);\n            for (int i = 0; i < %(ndim)s; i++)\n                need_new_out = (need_new_out\n                                || (PyArray_DIMS(%(zz)s)[i] != shape[i]));\n\n            if (need_new_out)\n            {\n                Py_XDECREF(%(zz)s);\n                %(zz)s = (PyArrayObject*) PyArray_SimpleNew(%(ndim)s,\n                    shape, PyArray_TYPE((PyArrayObject*) py_%(vv)s));\n                if (!%(zz)s)\n                {\n                    PyErr_SetString(PyExc_MemoryError, ""alloc failed"");\n                    %(fail)s\n                }\n            }\n\n            // This function takes care of broadcasting\n            if (PyArray_CopyInto(%(zz)s, %(vv)s) == -1)\n              %(fail)s\n            """""" % dict(vv=vv, ndim=ndim, zz=zz, fail=fail)\n\n    def c_code_cache_version(self):\n        return (2,)\n\n    def infer_shape(self, node, input_shapes):\n        return [node.inputs[1:]]\n\n    def connection_pattern(self, node):\n\n        rval = [[True]]\n\n        for ipt in node.inputs[1:]:\n            rval.append([False])\n\n    def grad(self, inputs, grads):\n        x = inputs[0]\n        gz = grads[0]\n        n_axes_to_sum = gz.ndim - x.ndim\n        # The number of dimensions added\n        axis = list(range(n_axes_to_sum))\n        # The broadcasted dimensions\n        axis_broadcasted = []\n        axis_kept = []\n        for i, (ib, gb) in enumerate(\n            zip(inputs[0].broadcastable,\n                # We need the dimensions corresponding to x\n                grads[0].broadcastable[-inputs[0].ndim:])):\n            if ib and not gb:\n                axis_broadcasted.append(i + n_axes_to_sum)\n            else:\n                axis_kept.append(i)\n        gx = gz.sum(axis=axis + axis_broadcasted)\n        if axis_broadcasted:\n            new_order = [\'x\'] * x.ndim\n            for idx, axis in enumerate(axis_kept):\n                new_order[axis] = idx\n            gx = gx.dimshuffle(new_order)\n            # Dimshuffle to add back the broadcasted dims\n        # The *elements* of the output are not connected to\n        # the inputs that specify the shape. If you grow the\n        # shape by epsilon, the existing elements do not\n        # change.\n        return [gx] + [DisconnectedType()() for i in inputs[1:]]\n\n    def __call__(self, val, *shapes, **kwargs):\n        """"""\n        If the alloc would be useless, this function returns val.\n\n        If this function is called outside of a graph optimization context\n        (for instance, it is manually called by a user building a graph),\n        then we always return an Alloc node, to allow for DebugMode to check\n        for size mismatches.\n\n        If you always want an Alloc node, call make_node.\n\n        """"""\n        ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n        try:\n            # It makes optimization difficult when useless allocs are thrown\n            # into the graph at every stage of optimization.  This little logic\n            # tries to help at least in some cases.\n            if hasattr(val, \'fgraph\') and (val.type == ret.type):\n                return val\n        except AttributeError:\n            pass\n        return ret\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None]\n        return self(eval_points[0], *inputs[1:], **dict(return_list=True))\n\n    def do_constant_folding(self, node):\n        if not getattr(node.outputs[0], \'clients\', []):\n            # If there are no clients then there is no point doing constant\n            # folding.\n            return False\n        for client in node.outputs[0].clients:\n            if client[0] == \'output\':\n                # If the output is a constant, it will have to be deepcopied\n                # each time the function is called.  So we do not fold.\n                return False\n            elif (\n                # The following ops work inplace of their input id 0.\n                client[1] == 0 and\n                isinstance(client[0].op, (\n                    # Ops that will work inplace on the Alloc. So if they\n                    # get constant_folded, they would copy the\n                    # constant and this is less efficients.\n\n                    # Not doing the constant folding could also lower\n                    # the peak memory usage, as we the ""constant"" won\'t\n                    # always exists.\n                    theano.tensor.subtensor.IncSubtensor,\n                    theano.tensor.subtensor.AdvancedIncSubtensor1,\n                    theano.tensor.subtensor.AdvancedIncSubtensor,\n                    theano.tensor.blas.Gemv,\n                    theano.tensor.blas_c.CGemv,\n                    theano.tensor.blas.Ger,\n                    theano.tensor.blas_c.CGer,\n                    theano.tensor.blas_scipy.ScipyGer))):\n                return False\n            # If the clients is a transfer to the GPU, we don\'t want to\n            # fold. We let the Alloc being moved to the GPU, then we\n            # let the GPU algo decide if it need to fold it or not.\n            elif client[0].op.__class__.__name__.lower().startswith(""gpu""):\n                return False\n        return True\n\nalloc = Alloc()\npprint.assign(alloc, printing.FunctionPrinter(\'alloc\'))\n\n\ndef transfer(var, target):\n    """"""\n    Return a version of `var` transferred to `target`.\n\n    `cpu` mean a TensorType (on the CPU).  Other types may define\n    additional targets.\n\n    Parameters\n    ----------\n    var : variable\n        A theano variable\n    target : str\n        The target of the transfer\n    """"""\n    if target == \'cpu\':\n        return as_tensor_variable(var)\n    else:\n        for trans in transfer._others:\n            res = trans(var, target)\n            if res is not None:\n                return res\n    raise ValueError(""Can\'t transfer to target %s"" % (target,))\n\ntransfer._others = []\n\n\ndef register_transfer(fn):\n    """"""\n    Register a transfer function for alternative targets.\n\n    Parameters\n    ----------\n    fn : callable\n    """"""\n    transfer._others.append(fn)\n\n""""""Create a duplicate of `a` (with duplicated storage)""""""\ntensor_copy = elemwise.Elemwise(scal.identity)\npprint.assign(tensor_copy, printing.IgnorePrinter())\n\n\n@constructor\ndef sum(input, axis=None, dtype=None, keepdims=False, acc_dtype=None):\n    """"""\n    Computes the sum along the given axis(es) of a tensor `input`.\n\n    When axis is None (the default value), the sum is performed\n    over the flattened tensor.\n\n    For full documentation see ``tensor.elemwise.Sum``.\n    In particular please pay attention to the important warning when using\n    a custom acc_dtype.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    out = elemwise.Sum(axis=axis, dtype=dtype, acc_dtype=acc_dtype)(input)\n\n    if keepdims:\n        out = makeKeepDims(input, out, axis)\n    return out\n\npprint.assign(Sum(), printing.FunctionPrinter(\'sum\'))\n\n\n@constructor\ndef prod(input, axis=None, dtype=None, keepdims=False, acc_dtype=None,\n         no_zeros_in_input=False):\n    """"""\n    Computes the product along the given axis(es) of a tensor `input`.\n\n    When axis is None (the default value), the product is performed\n    over the flattened tensor.\n\n    For full documentation see ``tensor.elemwise.Prod``.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    out = elemwise.Prod(axis, dtype=dtype, acc_dtype=acc_dtype,\n                        no_zeros_in_input=no_zeros_in_input)(input)\n\n    if keepdims:\n        out = makeKeepDims(input, out, axis)\n    return out\n\n\nclass Mean(elemwise.CAReduce):\n    def __init__(self, axis=None):\n        elemwise.CAReduce.__init__(self, scal.add, axis)\n        assert self.axis is None or len(self.axis) == 1\n\n    def __str__(self):\n        if self.axis is not None:\n            return ""Mean{%s}"" % ("", "".join(str(x) for x in self.axis))\n        else:\n            return ""Mean""\n\n    def _output_dtype(self, idtype):\n        # we want to protect against overflow\n        return \'float64\'\n\n    def perform(self, node, inp, out):\n        input, = inp\n        output, = out\n        if self.axis is None:\n            axis = None\n        else:\n            axis = self.axis[0]\n        # numpy.asarray is needed as otherwise we can end up with a\n        # numpy scalar.\n        output[0] = np.asarray(np.mean(input, dtype=\'float64\',\n                                       axis=axis))\n\n    def c_code(self, node, name, inames, onames, sub):\n        if self.axis is not None:\n            return super(Op, self).c_code(node, name, inames, onames, sub)\n        ret = elemwise.CAReduce.c_code(self, node, name, inames, onames, sub)\n        # TODO: c_code perform support only axis is None\n        return ret + """"""\n  *((double *)PyArray_DATA(%s)) /= PyArray_SIZE(%s);\n  """""" % (onames[0], inames[0])\n\n# TODO: implement the grad. When done and tested, you can make this the default\n# version.\n#    def grad(self, (x,), (gout,)):\n#      import pdb;pdb.set_trace()\n#      return grad(mean(x, self.axis, op=False),[x])\n\n\n@constructor\ndef mean(input, axis=None, dtype=None, op=False, keepdims=False,\n         acc_dtype=None):\n    """"""\n    Computes the mean value along the given axis(es) of a tensor `input`.\n\n    Parameters\n    ----------\n    axis : None or int or (list of int) (see `Sum`)\n        Compute the mean along this axis of the tensor.\n        None means all axes (like numpy).\n    dtype: None or string\n        Dtype to cast the result of the inner summation into.\n        For instance, by default, a sum of a float32 tensor will be\n        done in float64 (acc_dtype would be float64 by default),\n        but that result will be casted back in float32.\n    keepdims: bool\n        If this is set to True, the axes which are reduced are\n        left in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original tensor.\n    acc_dtype: None or string\n        Dtype to use for the inner summation. This will not\n        necessarily be the dtype of the output (in particular\n        if it is a discrete (int/uint) dtype, the output will\n        be in a float type). If None, then we use the same rules as `sum()`.\n\n    Notes\n    -----\n    For gpu, if you specify dtype=float32, everything will be done on the gpu.\n\n    """"""\n    input = as_tensor_variable(input)\n    if op:\n        if dtype not in (None, \'float64\'):\n            raise NotImplementedError(\n                \'The Mean op does not support the dtype argument, \'\n                \'and will always use float64. If you want to specify \'\n                \'the dtype, call tensor.mean(..., op=False).\',\n                dtype)\n        if acc_dtype not in (None, \'float64\'):\n            raise NotImplementedError(\n                \'The Mean op does not support the acc_dtype argument, \'\n                \'and will always use float64. If you want to specify \'\n                \'acc_dtype, call tensor.mean(..., op=False).\',\n                dtype)\n        out = Mean(axis)(input)\n        if keepdims:\n            out = makeKeepDims(input, out, axis)\n        return out\n\n    if dtype is not None:\n        # The summation will be done with the specified dtype.\n        # sum() will complain if it is not suitable.\n        sum_dtype = dtype\n    else:\n        sum_dtype = None\n        # float16 overflows on the cast way too often\n        if input.dtype == \'float16\':\n            sum_dtype = \'float32\'\n\n    s = sum(input, axis=axis, dtype=sum_dtype, keepdims=keepdims,\n            acc_dtype=acc_dtype)\n    shp = shape(input)\n\n    # Cast shp into a float type\n    # TODO Once we have a consistent casting policy, we could simply\n    # use true_div.\n    if s.dtype in (\'float16\', \'float32\', \'complex64\'):\n        shp = cast(shp, \'float32\')\n    else:\n        shp = cast(shp, \'float64\')\n\n    if axis is None:\n        axis = list(range(input.ndim))\n    elif isinstance(axis, (integer_types, np.integer)):\n        axis = [axis]\n    elif isinstance(axis, np.ndarray) and axis.ndim == 0:\n        axis = [int(axis)]\n    else:\n        axis = [int(a) for a in axis]\n\n    # This sequential division will possibly be optimized by Theano:\n    for i in axis:\n        s = true_div(s, shp[i])\n\n    # This can happen when axis is an empty list/tuple\n    if s.dtype != shp.dtype and s.dtype in discrete_dtypes:\n        s = cast(s, shp.dtype)\n\n    if dtype == \'float16\' or (dtype is None and input.dtype == \'float16\'):\n        s = cast(s, \'float16\')\n    s.name = \'mean\'\n    return s\n\n\n@constructor\ndef var(input, axis=None, ddof=0, keepdims=False, corrected=False):\n    """"""\n    Computes the variance along the given axis(es) of a tensor `input`.\n\n    Parameters\n    ----------\n    axis: None or int or (list of int) (see `Sum`)\n        Compute the variance along this axis of the tensor.\n        None means all axes (like numpy).\n    ddof: Degrees of freedom; 0 would compute the ML estimate, 1 would compute\n        the unbiased estimate.\n    keepdims : bool\n        If this is set to True, the axes which are reduced are\n        left in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original tensor.\n    corrected : bool\n        If this is set to True, the \'corrected_two_pass\' algorithm is\n        used to compute the variance.\n        Refer : http://www.cs.yale.edu/publications/techreports/tr222.pdf\n\n    Notes\n    -----\n    Default uses the two-pass algorithm (reference below).\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Two-pass_algorithm\n    Also supports \'corrected_two_pass\' algorithm (using the \'corrected\' flag)\n    which is numerically more stable. There exist other implementations that\n    offer better stability, but probably slower.\n\n    if isinstance(ddof, (bool)):\n        raise ValueError(\'Parameter keepdims is now at index 3: (input, \\\n                          axis=None, ddof=0, keepdims=False, corrected=False)\')\n\n    input_ndim = input.type.ndim\n    if axis is None:\n        axis = list(range(input_ndim))\n    elif isinstance(axis, (integer_types, np.integer)):\n        axis = [axis]\n    elif isinstance(axis, np.ndarray) and axis.ndim == 0:\n        axis = [int(axis)]\n    else:\n        axis = [int(a) for a in axis]\n\n    # compute the axis-wise mean\n    mean_input = mean(input, axis, keepdims=True)\n\n    # center the input\n    centered_input = input - mean_input\n\n    # return the mean sqr\n    two = constant(2, dtype=centered_input.dtype)\n    if ddof == 0:\n        v = mean((centered_input ** two), axis, keepdims=keepdims)\n    else:\n        shp = shape(input) - ddof\n        v = sum((centered_input ** two), axis=axis, keepdims=keepdims)\n        for i in axis:\n            v = true_div(v, shp[i])\n\n    # use \'corrected_two_pass\' algorithm\n    if corrected:\n        if ddof == 0:\n            error = mean(centered_input, axis, keepdims=keepdims) ** 2\n        else:\n            shp = shape(input) - ddof\n            shp_inp = shape(input)\n            error = sum(centered_input, axis=axis, keepdims=keepdims) ** 2\n            for i in axis:\n                error = true_div(error, shp[i] * shp_inp[i])\n        v = v - error\n\n    v.name = \'var\'\n    return v\n\n\n@constructor\ndef std(input, axis=None, ddof=0, keepdims=False, corrected=False):\n    """"""\n    Computes the standard deviation along the given axis(es) of a tensor `input`.\n\n    Parameters\n    ----------\n    axis: None or int or (list of int) (see `Sum`)\n        Compute the variance along this axis of the tensor.\n        None means all axes (like numpy).\n    ddof: Degrees of freedom; 0 would compute the ML estimate, 1 would compute\n        the unbiased estimate.\n    keepdims : bool\n        If this is set to True, the axes which are reduced are\n        left in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original tensor.\n    corrected : bool\n        If this is set to True, the \'corrected_two_pass\' algorithm is\n        used to compute the variance.\n        Refer : http://www.cs.yale.edu/publications/techreports/tr222.pdf\n\n    Notes\n    -----\n    It calls \'var()\' and \'var()\' uses the two-pass algorithm (reference below).\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Two-pass_algorithm\n    Function \'var()\' also supports \'corrected_two_pass\' algorithm (using the\n    \'corrected\' flag) which is numerically more stable. There exist other\n    implementations that offer better stability, but probably slower.\n\n    if isinstance(ddof, (bool)):\n        raise ValueError(\'Parameter keepdims is now at index 3: (input, \\\n                          axis=None, ddof=0, keepdims=False, corrected=False)\')\n\n    ret = sqrt(var(input=input, axis=axis, ddof=ddof,\n                   keepdims=keepdims, corrected=corrected))\n    ret.name = \'std\'\n    return ret\n\n\nclass Default(gof.Op):\n    """"""\n    Takes an input x and a default value.\n\n    If the input is not None, a reference to it is returned.\n    If the input is None, a copy of the default value is returned instead.\n    The input and the default must have exactly the same type.\n\n    """"""\n    view_map = {0: [0]}\n    __props__ = ()\n\n    def make_node(self, x, default):\n        x, default = as_tensor_variable(x), as_tensor_variable(default)\n        if x.type != default.type:\n            raise TypeError(\'Both default() arguments must have same type\',\n                            x, default)\n        return gof.Apply(self, [x, default], [default.type()])\n\n    def perform(self, node, inp, out_):\n        x, default = inp\n        out, = out_\n        if x is None:\n            # why copy?  Theano can\'t yet understand out[0] being a view of\n            # either x or y, so we can be a view of x, but only a copy of y.\n            out[0] = default.copy()\n        else:\n            out[0] = x\n\ndefault = Default()\nsetdefault = default  # legacy\n\n\n##########################\n# Arithmetics\n##########################\n@_scal_elemwise\ndef maximum(x, y):\n    """"""elemwise maximum. See max for the maximum in one tensor""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef minimum(x, y):\n    """"""elemwise minimum. See min for the minimum in one tensor""""""\n    # see decorator for function body\n\n\ndef div_proxy(x, y):\n    """"""Proxy for either true_div or int_div, depending on types of x, y.""""""\n    f = scal.int_or_true_div(\n        as_tensor_variable(x).dtype in discrete_dtypes,\n        as_tensor_variable(y).dtype in discrete_dtypes)\n    if f is scal.int_div:\n        return int_div(x, y)\n    else:\n        return true_div(x, y)\n\n\ndef divmod(x, y):\n    """"""elementvise divmod, using floor_div and mod_check""""""\n    return floor_div(x, y), mod_check(x, y)\n\n\n@_scal_elemwise\ndef add(a, *other_terms):\n    """"""elementwise addition""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef sub(a, b):\n    """"""elementwise subtraction""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef mul(a, *other_terms):\n    """"""elementwise multiplication""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef true_div(a, b):\n    """"""elementwise [true] division (inverse of multiplication)""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef int_div(a, b):\n    """"""elementwise [floor] division (inverse of multiplication)""""""\n    # see decorator for function body\n\n\n# floor_div and int_div are the same thing\nfloor_div = int_div\n\n\ndef ceil_intdiv(a, b):\n    """"""\n    Safely compute ceil(float_division(a, b)).\n\n    Works for all dtypes, but mostly useful when a and b are int.\n\n    """"""\n    # If a and b are int with not many significant bits, we could\n    # cast them to float to avoid doing the modulo. We do not know if this\n    # is faster or not. But this is not safe for int64 as the cast will\n    # lose precision.\n    # e.g.: cast(cast(a, scalar.upcast(a, \'float32\')) / b, scal.upcast(a, b))\n\n    # We cast for the case when a and b are uint*. Otherwise neq will\n    # force their upcast to int.\n    div = int_div(a, b)\n    ret = cast(neq(a % b, 0), div.dtype) + div\n    assert ret.dtype == scal.upcast(div.owner.inputs[0], div.owner.inputs[1])\n    return ret\n\n\ndef mod_check(x, y):\n    """"""Make sure we do not try to use complex numbers.""""""\n    if ((as_tensor_variable(x).dtype in complex_dtypes or\n         as_tensor_variable(y).dtype in complex_dtypes)):\n        # Currently forbidden.\n        raise scal.Mod.complex_error\n    else:\n        return mod(x, y)\n\n\n@_scal_elemwise\ndef mod(a, b):\n    """"""elementwise modulo""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef pow(a, b):\n    """"""elementwise power""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef clip(x, min, max):\n    """"""\n    Clip x to be between min and max.\n\n    Notes\n    -----\n    When `x` is equal to the boundaries, the output is considered\n    to be `x`, so at these points, the gradient of the cost wrt the output\n    will be propagated to `x`, not to `min` nor `max`. In other words,\n    on these points, the gradient wrt `x` will be equal to the gradient wrt\n    the output, and the gradient wrt `min` and `max` will be zero.\n\n    """"""\n    # see decorator for function body\n    # for grep: clamp, bound\n\npprint.assign(add, printing.OperatorPrinter(\'+\', -2, \'either\'))\npprint.assign(mul, printing.OperatorPrinter(\'*\', -1, \'either\'))\npprint.assign(sub, printing.OperatorPrinter(\'-\', -2, \'left\'))\npprint.assign(neg, printing.OperatorPrinter(\'-\', 0, \'either\'))\npprint.assign(true_div, printing.OperatorPrinter(\'/\', -1, \'left\'))\npprint.assign(int_div, printing.OperatorPrinter(\'//\', -1, \'left\'))\npprint.assign(pow, printing.OperatorPrinter(\'**\', 1, \'right\'))\n\n\n##########################\n# View Operations\n##########################\n\n\ndef extract_constant(x, elemwise=True, only_process_constants=False):\n    """"""\n    This function is basically a call to tensor.get_scalar_constant_value.\n\n    The main difference is the behaviour in case of failure. While\n    get_scalar_constant_value raises an TypeError, this function returns x,\n    as a tensor if possible. If x is a ScalarVariable from a\n    scalar_from_tensor, we remove the conversion. If x is just a\n    ScalarVariable, we convert it to a tensor with tensor_from_scalar.\n\n    """"""\n    try:\n        x = get_scalar_constant_value(x,\n                                      elemwise,\n                                      only_process_constants)\n    except NotScalarConstantError:\n        pass\n    if ((isinstance(x, scal.ScalarVariable) or\n         isinstance(x, scal.sharedvar.ScalarSharedVariable))):\n        if x.owner and isinstance(x.owner.op, ScalarFromTensor):\n            x = x.owner.inputs[0]\n        else:\n            x = tensor_from_scalar(x)\n    return x\n\n\ndef transpose(x, axes=None):\n    """"""\n    Reorder the dimensions of x. (Default: reverse them)\n\n    This is a macro around dimshuffle that matches the numpy.transpose function.\n\n    """"""\n    if axes is None:\n        axes = list(range((x.ndim - 1), -1, -1))\n    ret = DimShuffle(x.broadcastable, axes)(x)\n    if x.name and axes == list(range((x.ndim - 1), -1, -1)):\n        ret.name = x.name + \'.T\'\n    return ret\n\n\ndef batched_dot(a, b):\n    """"""\n    Compute the batched dot product of two variables:\n\n        batched_dot(a, b)[i] = dot(a[i], b[i])\n\n    Note that this batched_dot function does one of three things, in the\n    following sequence:\n\n        1.  If either a or b is a vector, it returns the batched elementwise\n            product without calling the Theano BatchedDot op.\n\n        2.  If both a and b have either 2 or 3 dimensions, it calls Theano\'s\n            BatchedDot op on a and b.\n\n        3.  If either a or b has more than 3 dimensions, it calls Theano\'s\n            batched_tensordot function with appropriate axes. The\n            batched_tensordot function expresses high-dimensional batched\n            dot products in terms of batched matrix-matrix dot products, so\n            it may be possible to futherize optimize for performance.\n    """"""\n    a, b = as_tensor_variable(a), as_tensor_variable(b)\n\n    if a.ndim == 0:\n        raise TypeError(""a must have at least one (batch) axis"")\n    elif b.ndim == 0:\n        raise TypeError(""b must have at least one (batch) axis"")\n    elif a.ndim == 1:\n        return a.dimshuffle(*([0] + [""x""] * (b.ndim - 1))) * b\n    elif b.ndim == 1:\n        return a * b.dimshuffle(*([0] + [""x""] * (a.ndim - 1)))\n    elif a.ndim > 3 or b.ndim > 3:\n        return batched_tensordot(\n            a, b, [[a.ndim - 1], [np.maximum(1, b.ndim - 2)]])\n    else:\n        # avoid circular import\n        return theano.tensor.blas.BatchedDot()(a, b)\n\n\ndef batched_tensordot(x, y, axes=2):\n    """"""\n    Compute a batched tensordot product.\n\n    A hybrid of batched_dot and tensordot, this function computes the\n    tensordot product between the two tensors, by iterating over the\n    first dimension to perform a sequence of tensordots.\n\n    Parameters\n    ----------\n    x : tensor\n        A Tensor with sizes e.g.: for 3D (dim1, dim3, dim2)\n    y : tensor\n        A Tensor with sizes e.g.: for 3D (dim1, dim2, dim4)\n    axes: int or array-like of length 2\n        If an integer, the number of axes to sum over.\n        If an array, it must have two array elements containing the axes to sum\n        over in each tensor.\n\n        If an integer i, it is converted to an array containing\n        the last i dimensions of the first tensor and the first\n        i dimensions of the second tensor (excluding the first\n        (batch) dimension):\n            axes = [list(range(a.ndim - i, b.ndim)), list(range(1,i+1))]\n\n        If an array, its two elements must contain compatible axes\n        of the two tensors. For example, [[1, 2], [2, 4]] means sum\n        over the 2nd and 3rd axes of a and the 3rd and 5th axes of b.\n        (Remember axes are zero-indexed!) The 2nd axis of a and the\n        3rd axis of b must have the same shape; the same is true for\n        the 3rd axis of a and the 5th axis of b.\n\n    Like tensordot, this function uses a series of dimshuffles and\n    reshapes to reduce the tensor dot product to a matrix or vector\n    dot product.  Finally, it calls batched_dot to compute the result.\n    """"""\n    return _tensordot_as_dot(x, y, axes, dot=batched_dot, batched=True)\n\n\ndef split(x, splits_size, n_splits, axis=0):\n    the_split = Split(n_splits)\n    return the_split(x, axis, splits_size)\n\n\nclass Split(Op):\n    """"""Partition a `TensorVariable` along some axis.\n\n    Examples\n    --------\n    >>> x = vector()\n    >>> splits = lvector()\n    You have to declare right away how many split_points there will be.\n    >>> ra, rb, rc = split(x, splits, n_splits = 3, axis = 0)\n    >>> f = function([x, splits], [ra, rb, rc])\n    >>> a, b, c = f([0,1,2,3,4,5], [3, 2, 1])\n    a == [0,1,2]\n    b == [3, 4]\n    c == [5]\n\n    len_splits = None\n    """"""A Split instance will have this many outputs, and require that\n    the splits argument to `perform` have exactly this many elements.\n    """"""\n    __props__ = (""len_splits"",)\n\n    def __init__(self, len_splits):\n        self.len_splits = int(len_splits)\n\n    def __str__(self):\n        return self.__class__.__name__ + ""{%s}"" % self.len_splits\n\n    def make_node(self, x, axis, splits):\n        """"""WRITEME""""""\n        x = as_tensor_variable(x)\n        axis = as_tensor_variable(axis)\n        splits = as_tensor_variable(splits)\n\n        if splits.type not in int_vector_types:\n            raise TypeError(\'splits must have type tensor.lvector\',\n                            splits.type)\n        if axis.type not in int_types:\n            raise TypeError(\'axis must have type lscalar\', axis.type)\n\n#         # The following lines are necessary if we allow splits of zero\n#         if isinstance(axis, gof.Constant):\n#             x = unbroadcast(x, int(axis.data))\n#         else:\n#             x = unbroadcast(x, *range(x.type.ndim))\n\n        inputs = [x, axis, splits]\n        outputs = [x.type() for i in xrange(self.len_splits)]\n\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inputs, outputs):\n        """"""WRITEME""""""\n        x, axis, splits = inputs\n        # in python 2.4, x.shape[numpy.asarray(1)] don\'t work.\n        if sys.version_info[0:2] == (2, 4) and axis.size == 1:\n            axis = int(axis)\n\n        try:\n            len_along_axis = x.shape[axis]\n        except Exception:\n            raise ValueError(\'Split.perform() with axis=(%s) is invalid\'\n                             \' for x.shape==(%s)\'\n                             % (axis, x.shape))\n        if len(splits) != self.len_splits:\n            raise ValueError(\'In Split.perform(), len(splits) != len_splits.\',\n                             (len(splits), self.len_splits))\n\n        if np.sum(splits) != len_along_axis:\n            raise ValueError(\'The splits sum to %s, expected %s\' %\n                             (np.sum(splits), len_along_axis))\n        if python_any([nb < 0 for nb in splits]):\n            raise ValueError(\'Split: you tried to make an ndarray with a \'\n                             \'negative number of elements.\')\n\n        # Checking is done, let\'s roll the splitting algorithm!\n        # Basically we step along the given axis of x, extracting\n        # subtensors of size splits[i] as we go along.\n\n        general_key = [slice(None, None, None) for s in x.shape]\n        lower_idx = 0\n        for i in xrange(self.len_splits):\n            upper_idx = lower_idx + splits[i]\n            general_key[axis] = slice(lower_idx, upper_idx, None)\n            outputs[i][0] = x.__getitem__(tuple(general_key)).copy()\n            lower_idx = upper_idx\n\n    def infer_shape(self, node, in_shapes):\n        axis = node.inputs[1]\n        splits = node.inputs[2]\n        shp_x, shp_axis, shp_splits = in_shapes\n        out_shapes = []\n        for i in xrange(self.len_splits):\n            temp = as_tensor_variable(shp_x)\n            temp = theano.tensor.subtensor.set_subtensor(temp[axis], splits[i])\n            temp = [temp[i] for i in xrange(len(shp_x))]\n            out_shapes.append(temp)\n        return out_shapes\n\n    def grad(self, inputs, g_outputs):\n        """"""Join the gradients along the axis that was used to split x.""""""\n        x, axis, n = inputs\n        outputs = self(*inputs, **dict(return_list=True))\n        # If all the output gradients are disconnected, then so are the inputs\n        if python_all([isinstance(g.type, DisconnectedType)\n                       for g in g_outputs]):\n            return [DisconnectedType()(),\n                    grad_undefined(self, 1, axis),\n                    grad_undefined(self, 2, n)]\n        # Else, we have to make them zeros before joining them\n        new_g_outputs = []\n        for o, g in zip(outputs, g_outputs):\n            if isinstance(g.type, DisconnectedType):\n                new_g_outputs.append(o.zeros_like())\n            else:\n                new_g_outputs.append(g)\n\n        return [join(axis, *new_g_outputs),\n                grad_undefined(self, 1, axis),\n                grad_undefined(self, 2, n)]\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None for i in self.len_splits]\n        return self.make_node(eval_points[0], *inputs[1:]).outputs\n\n    def c_code_cache_version(self):\n        return (2,)\n\n    def c_support_code(self):\n        return """"""\n        /* Return 1 if output has the correct shape. */\n        int split_output_shape_is_correct (\n            PyArrayObject* output, PyArrayObject* array_to_split, int axis_to_split, npy_intp split_size\n        ) {\n            return\n                PyArray_NDIM(output) == PyArray_NDIM(array_to_split)\n                && memcmp(\n                    PyArray_DIMS(output),\n                    PyArray_DIMS(array_to_split),\n                    axis_to_split * sizeof(npy_intp)\n                ) == 0\n                && memcmp(\n                    PyArray_DIMS(output) + axis_to_split + 1,\n                    PyArray_DIMS(array_to_split) + axis_to_split + 1,\n                    (PyArray_NDIM(array_to_split) - axis_to_split - 1) * sizeof(npy_intp)\n                ) == 0\n                && split_size == PyArray_DIM(output, axis_to_split);\n        }\n        """"""\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        if self.len_splits == 0:\n            # There are no outputs, then nothing to do.\n            return \'\'\n\n        # outputs_pointers lists the addresses of the pointers to the outputs.\n        outputs_pointers = \'&\' + (\', &\'.join(outputs))\n        x, axis, splits = inputs\n        fail = sub[\'fail\']\n        x_typenum = np.dtype(node.inputs[0].dtype).num\n        x_itemsize = np.dtype(node.inputs[0].dtype).itemsize\n        axis_dtype = node.inputs[1].type.dtype_specs()[1]\n        splits_dtype = node.inputs[2].type.dtype_specs()[1]\n        expected_splits_count = self.len_splits\n\n        return """"""\n        int ndim = PyArray_NDIM(%(x)s);\n        int axis = (int)(*(%(axis_dtype)s*)PyArray_GETPTR1(%(axis)s, 0));\n        int splits_count = PyArray_DIM(%(splits)s, 0);\n        npy_intp len_along_axis, sum_of_splits = 0, current_split_length = 0, current_split_start = 0;\n        npy_intp* split_dims = NULL;\n        PyObject* split_view = NULL;\n        npy_intp data_offset;\n        int i;\n        PyArrayObject** outputs[] = {%(outputs_pointers)s};\n\n        /* Check inputs. */\n\n        if (splits_count != %(expected_splits_count)s) {\n            PyErr_Format(PyExc_ValueError,\n                ""Split: splits count (%%d) != expected count (%%d)."", splits_count, %(expected_splits_count)s);\n            %(fail)s\n        }\n\n        if (axis < 0) {\n            axis += ndim;\n        }\n        if (axis < 0 || axis >= ndim) {\n            PyErr_Format(PyExc_IndexError, ""Split: invalid axis %%d for a %%d-D array."", axis, ndim);\n            %(fail)s\n        }\n        len_along_axis = PyArray_DIM(%(x)s, axis);\n\n        for (i = 0; i < splits_count; ++i) {\n            current_split_length = (npy_intp)(*(%(splits_dtype)s*)PyArray_GETPTR1(%(splits)s, i));\n            if (current_split_length < 0) {\n                PyErr_Format(PyExc_ValueError,\n                    ""Split: you try to take a negative number (%%ld) of elements."", current_split_length);\n                %(fail)s\n            }\n            sum_of_splits += current_split_length;\n        }\n        if (sum_of_splits != len_along_axis) {\n            PyErr_Format(PyExc_ValueError, ""Split: the splits sums to %%ld, expected %%ld."", sum_of_splits, len_along_axis);\n            %(fail)s\n        }\n\n        /* Check outputs. */\n\n        split_dims = (npy_intp*) malloc(ndim * sizeof(npy_intp));\n        if (split_dims == NULL) {\n            PyErr_NoMemory();\n            %(fail)s\n        }\n\n        memcpy(split_dims, PyArray_DIMS(%(x)s), ndim * sizeof(npy_intp));\n\n        for (i = 0; i < splits_count; ++i) {\n            PyArrayObject** output = outputs[i];\n            current_split_length = (npy_intp) (* (%(splits_dtype)s*) PyArray_GETPTR1(%(splits)s, i));\n            if (*output == NULL || !split_output_shape_is_correct(*output, %(x)s, axis, current_split_length)) {\n                Py_XDECREF(*output);\n                split_dims[axis] = current_split_length;\n                *output = (PyArrayObject*)PyArray_EMPTY(ndim, split_dims, %(x_typenum)s, PyArray_IS_F_CONTIGUOUS(%(x)s));\n                if (outputs == NULL) {\n                    PyErr_SetString(PyExc_RuntimeError, ""Split: unable to allocate an output."");\n                    free(split_dims);\n                    %(fail)s\n                }\n            }\n        }\n\n        /* Compute split. */\n\n        for (i = 0; i < splits_count; ++i) {\n            current_split_length = (npy_intp) (* (%(splits_dtype)s*) PyArray_GETPTR1(%(splits)s, i));\n            data_offset = PyArray_STRIDE(%(x)s, axis) * current_split_start;\n            split_dims[axis] = current_split_length;\n            split_view = PyArray_New(&PyArray_Type,\n                                    ndim, split_dims,\n                                    %(x_typenum)s,\n                                    PyArray_STRIDES(%(x)s),\n                                    PyArray_BYTES(%(x)s) + data_offset,\n                                    %(x_itemsize)s,\n                                    PyArray_FLAGS(%(x)s),\n                                    NULL);\n            if (split_view == NULL) {\n                PyErr_SetString(PyExc_RuntimeError, ""Split: unable to create a view for a split."");\n                free(split_dims);\n                %(fail)s\n            }\n            if (PyArray_CopyInto(*outputs[i], (PyArrayObject*)split_view) != 0) {\n                PyErr_SetString(PyExc_RuntimeError, ""Split: unable to copy a split view into the output."");\n                Py_XDECREF(split_view);\n                free(split_dims);\n                %(fail)s\n            }\n            Py_XDECREF(split_view);\n            current_split_start += current_split_length;\n        }\n\n        free(split_dims);\n        """""" % locals()\n\n\ndef addbroadcast(x, *axes):\n    """"""\n    Make the input broadcastable in the specified axes.\n\n    For example, addbroadcast(x, 0) will make the first dimension of\n    x broadcastable. When performing the function, if the length of\n    x along that dimension is not 1, a ValueError will be raised.\n\n    We apply the opt here not to pollute the graph especially during\n    the gpu optimization\n\n    Parameters\n    ----------\n    x : tensor_like\n        Input theano tensor.\n    axis : an int or an iterable object such as list or tuple of int values\n        The dimension along which the tensor x should be broadcastable.\n        If the length of x along these dimensions is not 1, a ValueError will\n        be raised.\n\n    Returns\n    -------\n    tensor\n        A theano tensor, which is broadcastable along the specified dimensions.\n\n    """"""\n    rval = Rebroadcast(*[(axis, True) for axis in axes])(x)\n    return theano.tensor.opt.apply_rebroadcast_opt(rval)\n\n\ndef unbroadcast(x, *axes):\n    """"""\n    Make the input impossible to broadcast in the specified axes.\n\n    For example, addbroadcast(x, 0) will make the first dimension\n    of x broadcastable. When performing the function, if the length\n    of x along that dimension is not 1, a ValueError will be raised.\n\n    We apply the opt here not to pollute the graph especially during\n    the gpu optimization\n\n    Parameters\n    ----------\n    x : tensor_like\n        Input theano tensor.\n    axis : an int or an iterable object such as list or tuple of int values\n        The dimension along which the tensor x should be unbroadcastable.\n        If the length of x along these dimensions is not 1, a ValueError will\n        be raised.\n\n    Returns\n    -------\n    tensor\n        A theano tensor, which is unbroadcastable along the specified dimensions.\n\n    """"""\n    rval = Rebroadcast(*[(axis, False) for axis in axes])(x)\n    return theano.tensor.opt.apply_rebroadcast_opt(rval)\n\n\ndef patternbroadcast(x, broadcastable):\n    """"""\n    Make the input adopt a specific broadcasting pattern.\n\n    Broadcastable must be iterable. For example,\n    patternbroadcast(x, (True, False)) will make the first\n    dimension of x broadcastable and the second dimension\n    not broadcastable, so x will now be a row.\n\n    We apply the opt here not to pollute the graph especially during the gpu\n    optimization.\n\n    Parameters\n    ----------\n    x : tensor_like\n        Input theano tensor.\n    broadcastable : an iterable object such as list or tuple of bool values\n        A set of boolean values indicating whether a dimension should be\n        broadcastable or not. If the length of x along these dimensions is\n        not 1, a ValueError will be raised.\n\n    Returns\n    -------\n    tensor\n        A theano tensor, which is unbroadcastable along the specified dimensions.\n\n    """"""\n    rval = Rebroadcast(*[(i, broadcastable[i])\n                         for i in xrange(len(broadcastable))])(x)\n    return theano.tensor.opt.apply_rebroadcast_opt(rval)\n\n\nclass Join(Op):\n    """"""\n    Concatenate multiple `TensorVariable`s along some axis.\n\n    The axis must be given as first argument. All tensors must have the same\n    shape along all dimensions other than this axis.\n    Of course, TensorVariable instances do not have a shape, so this error\n    cannot be caught until runtime.  See `perform()`.\n\n    See Also\n    --------\n    stack : For joins involving scalar values\n\n    Examples\n    --------\n    >>> x, y, z = tensor.matrix(), tensor.matrix(), tensor.matrix()\n    >>> u = tensor.vector()\n\n    >>> r = join(0, x, y, z)\n    >>> c = join(1, x, y, z)\n    >>> join(2, x, y, z)    # WRONG: the axis has to be an index into the shape\n    >>> join(0, x, u)       # WRONG: joined tensors must have the same rank\n\n    """"""\n    check_input = False\n    __props__ = (""view"",)\n\n    def __init__(self, view=-1):\n        self.view = view\n        if view != -1:\n            # since the first input is always the axis, the tensors\n            # start from index 1.\n            self.view_map = {0: [1 + view]}\n\n    def __str__(self):\n        if self.view == -1:\n            return self.__class__.__name__\n        else:\n            return ""%s{%s}"" % (\n                self.__class__.__name__,\n                "", "".join(""%s=%r"" % (p, getattr(self, p))\n                          for p in self.__props__))\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        if not hasattr(self, ""view""):\n            self.view = -1\n\n    def make_node(self, *axis_and_tensors):\n        """"""\n        Parameters\n        ----------\n        axis: an Int or integer-valued Variable\n        tensors\n            A variable number (but not zero) of tensors to\n            concatenate along the specified axis.  These tensors must have\n            the same shape along all dimensions other than this axis.\n\n        Returns\n        -------\n        A symbolic Variable\n            It has the same ndim as the input tensors, and the most inclusive\n            dtype.\n\n        """"""\n        axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]\n        if not tensors:\n            raise ValueError(\'Cannot join an empty list of tensors\')\n        as_tensor_variable_args = [as_tensor_variable(x) for x in tensors]\n\n        dtypes = [x.type.dtype for x in as_tensor_variable_args]\n        out_dtype = scal.upcast(*dtypes)\n\n        def output_maker(bcastable):\n            return tensor(dtype=out_dtype, broadcastable=bcastable)\n\n        return self._make_node_internal(\n            axis, tensors, as_tensor_variable_args, output_maker)\n\n    def _make_node_internal(self, axis, tensors,\n                            as_tensor_variable_args, output_maker):\n        if not python_all(targs.type.ndim for targs\n                          in as_tensor_variable_args):\n            raise TypeError(\'Join cannot handle arguments of dimension 0.\'\n                            \' For joining scalar values, see @stack\')\n        # Handle single-tensor joins immediately.\n        if len(as_tensor_variable_args) == 1:\n            bcastable = list(as_tensor_variable_args[0].type.broadcastable)\n        else:\n            # When the axis is fixed, a dimension should be\n            # broadcastable if at least one of the inputs is\n            # broadcastable on that dimension (see justification below),\n            # except for the axis dimension.\n            # Initialize bcastable all false, and then fill in some trues with\n            # the loops.\n            bcastable = [False] * len(\n                as_tensor_variable_args[0].type.broadcastable)\n            ndim = len(bcastable)\n            # Axis can also be a constant\n            if not isinstance(axis, integer_types):\n                try:\n                    # Note : `get_scalar_constant_value` returns a ndarray not\n                    # an int\n                    axis = int(get_scalar_constant_value(axis))\n\n                except NotScalarConstantError:\n                    pass\n            if isinstance(axis, integer_types):\n                # Basically, broadcastable -> length 1, but the\n                # converse does not hold. So we permit e.g. T/F/T\n                # joins, and if they fail at runtime they fail, but if\n                # they don\'t then it means that the argument where\n                # that broadcastable flag was False had length 1 along\n                # this dimension, and therefore this dimension should\n                # be broadcastable for the output.\n\n                if axis < -ndim:\n                    raise IndexError(""Join axis %d out of bounds [0, %d)"" %\n                                     (axis, ndim))\n                if axis < 0:\n                    axis += ndim\n\n                for x in as_tensor_variable_args:\n                    for current_axis, bflag in enumerate(x.type.broadcastable):\n                        # Constant negative axis can no longer be negative at\n                        # this point. It safe to compare this way.\n                        if current_axis == axis:\n                            continue\n                        if bflag:\n                            bcastable[current_axis] = True\n                try:\n                    bcastable[axis] = False\n                except IndexError:\n                    raise ValueError(\'Join argument ""axis"" is out of range\'\n                                     \' (given input dimensions)\')\n            else:\n                # When the axis may vary, no dimension can be guaranteed to be\n                # broadcastable.\n                bcastable = [False] * len(\n                    as_tensor_variable_args[0].type.broadcastable)\n\n        if not python_all([x.ndim == len(bcastable)\n                           for x in as_tensor_variable_args[1:]]):\n            raise TypeError(""Join() can only join tensors with the same ""\n                            ""number of dimensions."")\n\n        inputs = [as_tensor_variable(axis)] + list(as_tensor_variable_args)\n        if inputs[0].type not in int_types:\n            raise TypeError(\'Axis could not be cast to an integer type\',\n                            axis, inputs[0].type, int_types)\n\n        outputs = [output_maker(bcastable)]\n\n        node = Apply(self, inputs, outputs)\n        return node\n\n    def perform(self, node, axis_and_tensors, out_):\n        out, = out_\n        view = self.view\n        axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]\n        # we check these tensors for being empty.\n        if (view != -1) and np.all(\n                [tensor.shape[axis] == 0 for tensor in\n                 tensors[0:view] + tensors[view + 1:]]):\n            out[0] = tensors[view]\n\n        else:\n            ndim = tensors[0].ndim\n            if axis < -ndim:\n                raise IndexError(""Join axis %d out of bounds [0, %d)"" %\n                                 (axis, ndim))\n\n            out[0] = theano._asarray(np.concatenate(tensors, axis=axis),\n                                     dtype=node.outputs[0].type.dtype)\n\n    def c_code_cache_version(self):\n        return (5,)\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        axis, tensors = inputs[0], inputs[1:]\n        view = self.view\n        non_empty_tensor = tensors[view]\n        input_1 = tensors[0]\n        l = len(tensors)\n        out, = outputs\n        fail = sub[\'fail\']\n        adtype = node.inputs[0].type.dtype_specs()[1]\n        copy_to_list = []\n\n        for i, inp in enumerate(tensors):\n            copy_to_list.append(\n                """"""Py_INCREF(%s);\n                   PyList_SetItem(list, %s, (PyObject*)%s);""""""\n                % (inp, i, inp))\n\n        copy_inputs_to_list = \'\\n\'.join(copy_to_list)\n        n = len(tensors)\n\n        code = """"""\n        int axis = ((%(adtype)s *)PyArray_DATA(%(axis)s))[0];\n        PyObject* list = PyList_New(%(l)s);\n        %(copy_inputs_to_list)s\n        int tensors_lens_sum;\n        if(%(view)s != -1) {\n            tensors_lens_sum = 0;\n\n            for(int i=0; i < %(n)s; i++){\n                tensors_lens_sum += PyArray_DIM((PyArrayObject *)(PyList_GetItem(list, i)), axis);\n            }\n            tensors_lens_sum -= PyArray_DIM(%(non_empty_tensor)s, axis);\n        }\n        if(%(view)s != -1 && tensors_lens_sum == 0) {\n            Py_XDECREF(%(out)s);\n            Py_INCREF(%(non_empty_tensor)s);\n            %(out)s = %(non_empty_tensor)s;\n        }else{\n            //PyObject* PyArray_Concatenate(PyObject* obj, int axis)\n            int ndim = PyArray_NDIM(%(input_1)s);\n            if( axis < -ndim ){\n                PyErr_Format(PyExc_IndexError,\n                             ""Join axis %%d out of bounds [0, %%d)"", axis, ndim);\n                %(fail)s\n            }\n            Py_XDECREF(%(out)s);\n            %(out)s = (PyArrayObject *)PyArray_Concatenate(list, axis);\n            Py_DECREF(list);\n            if(!%(out)s){\n                %(fail)s\n            }\n        }\n        """""" % locals()\n        return code\n\n    def R_op(self, inputs, eval_points):\n        if None in eval_points[1:]:\n            return [None]\n        return self.make_node(inputs[0], *eval_points[1:]).outputs\n\n    def grad(self, axis_and_tensors, grads):\n        """""" The gradient wrt a join op is a `Split`, used to partition\n        the gradient along the `axis` which was used for joining.\n        """"""\n        gz, = grads\n        axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]\n\n        rval = [grad_undefined(self, 0, axis)]\n\n        dtypes = [as_tensor_variable(x).type.dtype for x in tensors]\n        out_dtype = scal.upcast(*dtypes)\n\n        if \'float\' in out_dtype or \'complex\' in out_dtype:\n            # assume that this is differentiable\n            split = Split(len(tensors))\n            split_gz = split(gz, axis, stack([shape(x)[axis]\n                                              for x in tensors]))\n            # If there is only one split, it might not be in a list.\n            if not isinstance(split_gz, list):\n                split_gz = [split_gz]\n            # Split.make_node isn\'t always able to infer the right\n            # broadcast. As the grad need to keep the information,\n            # read it if needed.\n            split_gz = [patternbroadcast(g, t.broadcastable)\n                        for t, g in zip(tensors, split_gz)]\n            rval = rval + split_gz\n        else:\n            # the output has integer type, so the gradient through it\n            # is 0\n            rval = rval + [tensor.zeros_like(dtype=config.floatX)\n                           for tensor in tensors]\n\n    def infer_shape(self, node, ishapes):\n        # ishapes[0] contains the size of the axis on which we join\n        # Join op should get at least one input to join\n        assert len(ishapes) > 1\n        n_dim = len(ishapes[1])\n        for shp in ishapes[1:]:\n            assert shp is not None\n            assert len(shp) == n_dim\n\n        # The joining dimension could be negative, but we need it to be\n        # in [0, n_dim) in the loop below.\n        # An axis < -n_dim or >= ndim would be invalid, but this is\n        # not checked here. An Assert op would be a way of addressing that,\n        # but it may disrupt optimizations.\n        join_dim = switch(ge(node.inputs[0], 0),\n                          node.inputs[0],\n                          node.inputs[0] + n_dim)\n        out_shapes = []\n        for dim in xrange(n_dim):\n            # we have to deal with 2 possible cases in here :\n            #   a) we are dealing with the dimension for which we join\n            #     (called t_side from true side of the if, where the if\n            #     compares current dimension with the joining dimension)\n            #   b) a non joining dimension ( in which maybe a symbolic\n            #      assertion can be used to make sure all tensors have\n            #      the same number of elements on this non-joined dimension\n            #      this is f_side\n            # initialize\n            t_side = ishapes[1][dim]\n            f_side = ishapes[1][dim]\n            # loop over tensors and sum for the joining dimension\n            for shp in ishapes[2:]:\n                t_side = t_side + shp[dim]\n            # return the dimensions found\n            out_shapes.append(switch(eq(dim, join_dim),\n                              t_side, f_side))\n\n        return [tuple(out_shapes)]\n\n\njoin_ = Join()\npprint.assign(Join, printing.FunctionPrinter(\'join\'))\n\n\ndef join(axis, *tensors_list):\n    """"""\n    Convenience function to concatenate `TensorType`s along the given axis.\n\n    This function will not add the op in the graph when it is not useful.\n    For example, in the case that the list of tensors to be concatenated\n    is one, it will just return the tensor.\n\n    Parameters\n    ----------\n    tensors : list of tensors (or list-like)\n        A list of tensors to be concatenated along the given axis.\n        The shapes of the tensors to be concatenated must be all\n        identical, except in the dimension (`axis`) on which they are to\n        be joined.\n    axis : int (symbolic or literal)\n        On which dimension should the tensors be joined?  The `axis`\n        must be a valid index into the shape of the tensors to be\n        concatenated.\n        The `axis` parameter may either be an integer or an object that\n        can be converted to a scalar using `as_scalar`(`axis`). In the\n        former case, the axis is fixed at construction, while in the\n        latter it may vary over time depending on the value of the\n        `axis` variable.\n    """"""\n    if len(tensors_list) == 1:\n        return tensors_list[0]\n    else:\n        return join_(axis, *tensors_list)\n\n\ndef roll(x, shift, axis=None):\n    """"""\n    Convenience function to roll TensorTypes along the given axis.\n\n    Syntax copies numpy.roll function.\n\n    Parameters\n    ----------\n    x : tensor_like\n        Input tensor.\n    shift : int (symbolic or literal)\n        The number of places by which elements are shifted.\n    axis : int (symbolic or literal), optional\n        The axis along which elements are shifted. By default, the array\n        is flattened before shifting, after which the original\n        shape is restored.\n\n    Returns\n    -------\n    tensor\n        Output tensor, with the same shape as ``x``.\n\n    """"""\n    if axis is None:\n        if x.ndim > 1:\n            y = x.flatten()\n            return roll(y, shift, axis=0).reshape(x.shape)\n        else:\n            axis = 0\n\n    if axis < 0:\n        axis += x.ndim\n\n    # Shift may be larger than the size of the axis. If so, since the\n    # roll operation is cyclic, we can take the shift modulo the size\n    # of the axis\n    shift = shift % x.shape[axis]\n\n    # A slice of all elements in a dimension \':\'\n    allslice = slice(None)\n    # List of slices describing the front half [:, :, shift:, :]\n    front_slice = slice(-shift, None)\n    front_list = ([allslice] * axis + [front_slice] +\n                  [allslice] * (x.ndim - axis - 1))\n    # List of slices describing the back half [:, :, :shift, :]\n    end_slice = slice(0, -shift)\n    end_list = ([allslice] * axis + [end_slice] +\n                [allslice] * (x.ndim - axis - 1))\n    return join(axis,\n                x.__getitem__(tuple(front_list)),\n                x.__getitem__(tuple(end_list)))\n\n\n@constructor\ndef shape_padleft(t, n_ones=1):\n    """"""Reshape `t` by left-padding the shape with `n_ones` 1s.\n\n    See Also\n    --------\n    shape_padaxis\n    shape_padright\n    Dimshuffle\n\n    """"""\n    _t = as_tensor_variable(t)\n\n    pattern = [\'x\'] * n_ones + [i for i in xrange(_t.type.ndim)]\n    return DimShuffle(_t.broadcastable, pattern)(_t)\n\n\n@constructor\ndef shape_padright(t, n_ones=1):\n    """"""Reshape `t` by right-padding the shape with `n_ones` 1s.\n\n    See Also\n    --------\n    shape_padaxis\n    shape_padleft\n    Dimshuffle\n\n    """"""\n    _t = as_tensor_variable(t)\n\n    pattern = [i for i in xrange(_t.type.ndim)] + [\'x\'] * n_ones\n    return DimShuffle(_t.broadcastable, pattern)(_t)\n\n\n@constructor\ndef shape_padaxis(t, axis):\n    """"""Reshape `t` by inserting 1 at the dimension `axis`.\n\n    Example\n    -------\n    >>> tensor = theano.tensor.tensor3()\n    >>> theano.tensor.shape_padaxis(tensor, axis=0)\n    DimShuffle{x,0,1,2}.0\n    >>> theano.tensor.shape_padaxis(tensor, axis=1)\n    DimShuffle{0,x,1,2}.0\n    >>> theano.tensor.shape_padaxis(tensor, axis=3)\n    DimShuffle{0,1,2,x}.0\n    >>> theano.tensor.shape_padaxis(tensor, axis=-1)\n    DimShuffle{0,1,2,x}.0\n\n    See Also\n    --------\n    shape_padleft\n    shape_padright\n    Dimshuffle\n\n    """"""\n    _t = as_tensor_variable(t)\n\n    ndim = _t.ndim + 1\n    if not -ndim <= axis < ndim:\n        msg = \'axis {0} is out of bounds [-{1}, {1})\'.format(axis, ndim)\n        raise IndexError(msg)\n    if axis < 0:\n        axis += ndim\n\n    pattern = [i for i in xrange(_t.type.ndim)]\n    pattern.insert(axis, \'x\')\n    return DimShuffle(_t.broadcastable, pattern)(_t)\n\n\n@constructor\ndef stack(*tensors, **kwargs):\n    """"""Stack tensors in sequence on given axis (default is 0).\n\n    Take a sequence of tensors and stack them on given axis to make a single\n    tensor. The size in dimension `axis` of the result will be equal to the number\n    of tensors passed.\n\n    Note: The interface stack(*tensors) is deprecated, you should use\n    stack(tensors, axis=0) insted.\n\n    Parameters\n    ----------\n    tensors : list or tuple of tensors\n        A list of tensors to be stacked.\n    axis : int\n        The index of the new axis. Default value is 0.\n\n    Examples\n    --------\n    >>> a = theano.tensor.scalar()\n    >>> b = theano.tensor.scalar()\n    >>> c = theano.tensor.scalar()\n    >>> x = theano.tensor.stack([a, b, c])\n    >>> x.ndim # x is a vector of length 3.\n    1\n    >>> a = theano.tensor.tensor4()\n    >>> b = theano.tensor.tensor4()\n    >>> c = theano.tensor.tensor4()\n    >>> x = theano.tensor.stack([a, b, c])\n    >>> x.ndim # x is a 5d tensor.\n    5\n    >>> rval = x.eval(dict((t, np.zeros((2, 2, 2, 2))) for t in [a, b, c]))\n    >>> rval.shape # 3 tensors are stacked on axis 0\n    (3, 2, 2, 2, 2)\n    >>> x = theano.tensor.stack([a, b, c], axis=3)\n    >>> x.ndim\n    5\n    >>> rval = x.eval(dict((t, np.zeros((2, 2, 2, 2))) for t in [a, b, c]))\n    >>> rval.shape # 3 tensors are stacked on axis 3\n    (2, 2, 2, 3, 2)\n    >>> x = theano.tensor.stack([a, b, c], axis=-2)\n    >>> x.ndim\n    5\n    >>> rval = x.eval(dict((t, np.zeros((2, 2, 2, 2))) for t in [a, b, c]))\n    >>> rval.shape # 3 tensors are stacked on axis -2\n    (2, 2, 2, 3, 2)\n    """"""\n    # ---> Remove this when moving to the new interface:\n    if not tensors and not kwargs:\n        raise Exception(\'theano.tensor.stack(tensors, axis) must have at least\'\n                        \' one parameter\')\n\n    if not kwargs and not isinstance(tensors[0], (list, tuple)):\n        warnings.warn(\'stack(*tensors) interface is deprecated, use\'\n                      \' stack(tensors, axis=0) instead.\', DeprecationWarning,\n                      stacklevel=3)\n        axis = 0\n    elif \'tensors\' in kwargs:\n        tensors = kwargs[\'tensors\']\n        if \'axis\' in kwargs:\n            axis = kwargs[\'axis\']\n        else:\n            axis = 0\n    else:\n        if len(tensors) == 2:\n            axis = tensors[1]\n        elif \'axis\' in kwargs:\n            axis = kwargs[\'axis\']\n        else:\n            axis = 0\n        tensors = tensors[0]\n    # <--- Until here.\n\n    if len(tensors) == 0:\n        raise Exception(\'tensors is empty. You should at least provide one\'\n                        \' tensor to theano.tensor.stack(tensors, axis).\')\n\n    # If all tensors are scalars of the same type, call make_vector.\n    # It makes the graph simpler, by not adding DimShuffles and Rebroadcasts\n\n    # This should be an optimization!\n    # Doing it here make the graph less canonicalized\n    # (more type need to be understood by all optimization)\n    # And DebugMode can\'t detect error in this code as it is not in an\n    # optimization.\n    # See ticket #660\n    if np.all(\n        [  # in case there is direct int in tensors.\n            isinstance(t, (np.number, float, integer_types,\n                           python_complex)) or\n            (isinstance(t, Variable) and\n             isinstance(t.type, TensorType) and\n             t.ndim == 0)\n            for t in tensors]):\n        # in case there is direct int\n        tensors = list(map(as_tensor_variable, tensors))\n        dtype = scal.upcast(*[i.dtype for i in tensors])\n        return theano.tensor.opt.MakeVector(dtype)(*tensors)\n    return join(axis, *[shape_padaxis(t, axis) for t in tensors])\n\n\n@constructor\ndef concatenate(tensor_list, axis=0):\n    """"""Alias for `join`(axis, *tensor_list).\n\n    This function is similar to `join`, but uses the signature of\n    numpy\'s concatenate function.\n\n    Raises\n    ------\n    TypeError\n        The tensor_list must be a tuple or list.\n\n    """"""\n    # Check someone did not make the common mistake to do something like:\n    #   c = concatenate(x, y)\n    # instead of\n    #   c = concatenate((x, y))\n    if not isinstance(tensor_list, (tuple, list)):\n        raise TypeError(\n            ""The \'tensors\' argument must be either a tuple ""\n            ""or a list, make sure you did not forget () or [] around ""\n            ""arguments of concatenate."", tensor_list)\n    return join(axis, *tensor_list)\n\n\ndef get_vector_length(v):\n    """"""Return the run-time length of a symbolic vector.\n\n    Parameters\n    ----------\n    v\n        A rank-1 TensorType variable.\n\n    Raises\n    ------\n    TypeError\n        `v` hasn\'t the proper type.\n    ValueError\n        No special case applies, the length is not known.\n        In general this is not possible, but for a number of special cases\n        the length can be determined at compile / graph-construction time.\n        This function implements these special cases.\n\n    """"""\n    v = as_tensor_variable(v)\n    if v.ndim != 1:\n        raise TypeError(""argument must be symbolic vector, got \'%s\'"" %\n                        v)\n    if v.type.broadcastable[0]:\n        return 1\n    if isinstance(v, gof.Constant) and v.type.ndim == 1:\n        return len(v.data)\n    if v.owner and isinstance(v.owner.op, theano.tensor.opt.MakeVector):\n        return len(v.owner.inputs)\n    if v.owner and isinstance(v.owner.op, Shape):\n        return v.owner.inputs[0].type.ndim\n    # If we take a slice, we know how many elements it will result in\n    if ((v.owner and\n         isinstance(v.owner.op, theano.tensor.subtensor.Subtensor) and\n         isinstance(v.owner.op.idx_list[0], slice) and\n         v.owner.inputs[0].owner and\n         isinstance(v.owner.inputs[0].owner.op, theano.compile.ops.Shape))):\n        start = extract_constant(theano.tensor.subtensor.get_idx_list(\n            v.owner.inputs, v.owner.op.idx_list)[0].start)\n        stop = extract_constant(theano.tensor.subtensor.get_idx_list(\n            v.owner.inputs, v.owner.op.idx_list)[0].stop)\n        step = extract_constant(theano.tensor.subtensor.get_idx_list(\n            v.owner.inputs, v.owner.op.idx_list)[0].step)\n\n        ndim = v.owner.inputs[0].owner.inputs[0].ndim\n        types = (numbers.Integral, np.integer)\n        if start is None:\n            start = 0\n        elif isinstance(start, types) and start < 0:\n            start += ndim\n            if start < 0:\n                start = 0\n        if stop is None:\n            stop = ndim\n        elif isinstance(stop, types):\n            if stop > ndim:\n                stop = ndim\n            elif stop < 0:\n                stop += ndim\n        if step is None:\n            step = 1\n\n        if (isinstance(stop, types) and\n                isinstance(start, types) and\n                isinstance(step, types) and\n                start >= 0 and stop >= 0 and\n                step > 0 and stop >= start):\n            return (stop - start - 1) // step + 1\n    if isinstance(v, Variable):\n        msg = theano.printing.debugprint(v, file=\'str\')\n    else:\n        msg = str(v)\n    raise ValueError(""length not known: %s"" % msg)\n\n\n@constructor\ndef horizontal_stack(*args):\n    """"""\n    Horizontally stack two L{TensorType}s.\n\n    Stack two L{TensorType}s along the second axis (column wise). These\n    L{TensorType}s must have the same shape along all dimensions but the\n    second.\n\n    """"""\n    # Note: \'horizontal_stack\' and \'vertical_stack\' do not behave exactly like\n    # Numpy\'s hstack and vstack functions. This is intended, because Numpy\'s\n    # functions have potentially confusing/incoherent behavior (try them on 1D\n    # arrays). If this is fixed in a future version of Numpy, it may be worth\n    # trying to get closer to Numpy\'s way of doing things. In the meantime,\n    # better keep different names to emphasize the implementation divergences.\n    assert len(args) >= 2\n    for arg in args:\n        assert arg.type.ndim == 2\n    return concatenate(args, axis=1)\n\n\n@constructor\ndef vertical_stack(*args):\n    assert len(args) >= 2\n    for arg in args:\n        assert arg.type.ndim == 2\n    return concatenate(args, axis=0)\n\n\nclass Reshape(Op):\n    """"""Perform a reshape operation of the input x to the new shape shp.\n    The number of dimensions to which to reshape to (ndim) must be\n    known at graph build time.\n    """"""\n    view_map = {0: [0]}  # output 0 is potentially aliased to inputs [0]\n    _f16_ok = True\n\n    check_input = False\n    __props__ = (""ndim"",)\n    params_type = ParamsType(ndim=int32)\n    # name does not participate because it doesn\'t affect computations\n\n    def __init__(self, ndim, name=None):\n        self.ndim = int(ndim)\n        if ndim < 0:\n            raise ValueError(""The output dimensions after reshape must be 0 or greater"")\n        assert name is None, \'name attribute for Reshape has been deprecated\'\n\n    def __str__(self):\n        return \'%s{%s}\' % (self.__class__.__name__, self.ndim)\n\n    def make_node(self, x, shp):\n        x = as_tensor_variable(x)\n        shp_orig = shp\n        shp = as_tensor_variable(shp, ndim=1)\n        if not (shp.dtype in int_dtypes or\n                (isinstance(shp, TensorConstant) and shp.data.size == 0)):\n            # It raises an error if shp is not of integer type,\n            # except when shp is constant and empty\n            # (in this case, shp.dtype does not matter anymore).\n            raise TypeError(""Shape must be integers"", shp, shp.dtype)\n        assert shp.ndim == 1\n        if isinstance(shp, TensorConstant):\n            bcast = [s == 1 for s in shp.data]\n            return gof.Apply(self, [x, shp], [tensor(x.type.dtype, bcast)])\n        else:\n            bcasts = [False] * self.ndim\n            shp_list = shp_orig\n            if hasattr(shp_orig, ""ndim"") and shp_orig.ndim == 0:\n                shp_list = [shp_orig]\n            for index in xrange(self.ndim):\n                y = shp_list[index]\n                y = as_tensor_variable(y)\n                # Try to see if we can infer that y has a constant value of 1.\n                # If so, that dimension should be broadcastable.\n                try:\n                    bcasts[index] = (\n                        hasattr(y, \'get_scalar_constant_value\') and\n                        y.get_scalar_constant_value() == 1)\n                except NotScalarConstantError:\n                    pass\n            return gof.Apply(self, [x, shp], [tensor(x.type.dtype, bcasts)])\n\n    def perform(self, node, inp, out_, params):\n        x, shp = inp\n        out, = out_\n        if (len(shp) != self.ndim):\n            raise ValueError(\'shape argument to Reshape.perform has incorrect\'\n                             \' length %i\'\n                             \', should be %i\' % (len(shp), self.ndim), shp)\n        try:\n            out[0] = np.reshape(x, shp)\n        except Exception:\n            raise ValueError(\'Cannot reshape input of shape %s to shape %s\' %\n                             (x.shape, shp))\n\n    def connection_pattern(self, node):\n        return [[True], [False]]\n\n    def grad(self, inp, grads):\n        x, shp = inp\n        g_out, = grads\n        return [reshape(g_out, shape(x), ndim=x.ndim),\n                DisconnectedType()()]\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None]\n        return self(eval_points[0], *inputs[1:], **dict(return_list=True))\n\n    def infer_shape(self, node, ishapes):\n        # inputs[1] can contain at most one value of \'-1\', meaning the actual\n        # shape of the output will be automatically computed by reshape, so\n        # that the total number of elements stays the same.\n        # TODO: Maybe put that formula here?\n        # It\'s not trivial, because we would have to check if the product of\n        # all the non-minus-one shapes is a divisor of the product of the\n        # original shapes.\n\n        # The following expression leads to cycles in feature_shape,\n        # because it tries to replace the Shape_i node by the switch\n        # statement, which depends on Shape_i.\n        # return [tuple([switch(eq(node.inputs[1][i], -1),\n        #                      theano.tensor.opt.Shape_i(i)(node.outputs[0]),\n        #                      node.inputs[1][i])\n        #                    for i in xrange(self.ndim)]\n        #    )]\n\n        # Here, we only simplify if the shape (node.inputs[1]) is a constant,\n        # ideally it would suffice to check that it is always non-negative.\n\n        # If current variable is a scalar and its dimensionality should\n        # change to self.ndim, then use size 1 for all new dimensions.\n        if len(ishapes[0]) == 0:\n            return [(1,) * self.ndim]\n\n        requ = node.inputs[1]\n        input_size = mul(*ishapes[0])\n        if isinstance(requ, theano.tensor.TensorConstant):\n            requ = list(requ.data)\n            requ_part = [ele for ele in requ if ele != -1]\n            crit = len(requ) - len(requ_part)\n            if crit == 1 and len(requ_part) > 0:\n                # If there are both 0 and -1 in requ_size, it is impossible\n                # to determine a right output, but we can at least prevent\n                # a division by 0. We do not want to keep a negative\n                # size here as it could lead to further weird errors\n                # after other optimizations.\n                requ_size = mul(*requ_part)\n                missing = input_size // (1 if requ_size == 0 else requ_size)\n                for i, ele in enumerate(requ):\n                    if ele == -1:\n                        requ[i] = missing\n            elif crit == 1:  # we reshape to -1\n                requ = [input_size] if ishapes[0] else [1]\n            elif crit > 1:\n                raise ValueError(\'shape argument to Reshape.perform\'\n                                 \' must have at most one entry equal to -1\')\n            return [requ]\n        else:\n            requ = [requ[i] for i in xrange(self.ndim)]\n            # since new_dims can have negative value (-1), the\n            # multiplication of all values should be negated\n            # to give a positive value.\n            # To avoid optimization complexity, we avoid checking\n            # for the case when there are two or more \'-1\' values.\n            if self.ndim:\n                requ_size = -mul(*requ)\n                # If there are both 0 and -1 in requ_size, it is impossible\n                # to determine a right output, but we can at least prevent\n                # a division by 0. We do not want to keep a negative\n                # size here as it could lead to further weird errors\n                # after other optimizations.\n                rest_size = input_size // maximum(requ_size, 1)\n            return [tuple([switch(eq(requ[i], -1),\n                                  rest_size,\n                                  requ[i])\n                           for i in xrange(self.ndim)])]\n\n    def c_code_cache_version(self):\n        return (8,)\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        if isinstance(node.inputs[0], TensorVariable):\n            x, shp = inputs\n            z, = outputs\n            sdtype = node.inputs[1].type.dtype_specs()[1]\n            fail = sub[\'fail\']\n            params = sub[\'params\']\n            return """"""\n            assert (PyArray_NDIM(%(shp)s) == 1);\n            npy_intp new_dims[%(params)s->ndim];\n            PyArray_Dims newshape;\n            newshape.ptr = new_dims;\n            newshape.len = %(params)s->ndim;\n            for (int ii = 0; ii < %(params)s->ndim; ++ii)\n            {\n                // -- We do not want an explicit cast here. the shp can be any\n                // -- int* dtype. The compiler will explicitly upcast it, but\n                // -- will err if this will downcast. This could happen if the\n                // -- user pass an int64 dtype, but npy_intp endup being int32.\n                new_dims[ii] = ((%(sdtype)s*)(\n                        PyArray_BYTES(%(shp)s) +\n                        ii * PyArray_STRIDES(%(shp)s)[0]))[0];\n            }\n            Py_XDECREF(%(z)s);\n            %(z)s = (PyArrayObject *) PyArray_Newshape(%(x)s, &newshape, NPY_CORDER);\n            if (!%(z)s)\n            {\n                //The error message should have been set by PyArray_Newshape\n                %(fail)s;\n            }\n            """""" % locals()\n        else:\n            return Op.c_code(self, node, name, inputs, outputs, sub)\n\n\ndef reshape(x, newshape, ndim=None):\n    if ndim is None:\n        newshape = as_tensor_variable(newshape)\n        if newshape.ndim != 1:\n            raise TypeError(\n                ""New shape in reshape must be a vector or a list/tuple of""\n                "" scalar. Got %s after conversion to a vector."" % newshape)\n        try:\n            ndim = get_vector_length(newshape)\n        except ValueError:\n            raise ValueError(\n                ""The length of the provided shape (%s) cannot ""\n                ""be automatically determined, so Theano is not able ""\n                ""to know what the number of dimensions of the reshaped ""\n                ""variable will be. You can provide the \'ndim\' keyword ""\n                ""argument to \'reshape\' to avoid this problem."" % newshape)\n    op = Reshape(ndim)\n    rval = op(x, newshape)\n    return rval\n\n\nclass Flatten(Op):\n    """"""\n    Flatten a tensor.\n\n    Flattens a tensor to `outdim` dimensions by preserving the leading\n    outdim - 1 shape components.\n\n    .. note:: The interface Flatten(Op) is deprecated, you should use flatten.\n    """"""\n    view_map = {0: [0]}\n\n    check_input = False\n    __props__ = (""outdim"",)\n\n    def __init__(self, outdim=1):\n        warnings.warn(\n            ""Flatten class is deprecated, ""\n            ""please use flatten method instead."",\n            DeprecationWarning,\n            stacklevel=4)\n        self.outdim = int(outdim)\n\n    def __str__(self):\n        return \'%s{%s}\' % (self.__class__.__name__, self.outdim)\n\n    def make_node(self, x):\n        t_x = as_tensor_variable(x)\n        if self.outdim < 1 or (x.ndim and self.outdim > x.ndim):\n            raise ValueError(\'invalid output ndimensions (%i) for tensor of \'\n                             \'rank %i\' % (self.outdim, t_x.ndim))\n\n        # Infer the broadcastable pattern of the output. For every dimension\n        # unaffected by the flatten, the broadcast flag should be unchanged.\n        # For the dimension resulting from the collapse of other dimensions,\n        # it should be broadcastable iff all the collapsed dimensions were\n        # broadcastable.\n        bcast_kept_dims = x.broadcastable[:self.outdim - 1]\n        bcast_new_dim = python_all(x.broadcastable[self.outdim - 1:])\n        broadcastable = bcast_kept_dims + (bcast_new_dim,)\n\n        return gof.Apply(self, [t_x], [tensor(x.type.dtype,\n                                              broadcastable)])\n\n    def perform(self, node, inp, out_):\n        x, = inp\n        out, = out_\n        outdim = self.outdim\n        if outdim == 1:\n            try:\n                out[0] = x.reshape(x.size)\n            except AttributeError:\n                out[0] = x.reshape((np.prod(x.shape),))\n        elif outdim == len(x.shape):\n            out[0] = x\n        else:\n            newshape = (x.shape[:outdim - 1] +\n                        (np.prod(x.shape[outdim - 1:]),))\n            out[0] = x.reshape(newshape)\n\n    def infer_shape(self, node, in_shapes):\n        in_shp, = in_shapes\n        part1 = in_shp[:self.outdim - 1]\n        part2 = in_shp[self.outdim - 1:]\n\n        if len(part2) > 1:\n            part2 = (prod(part2, dtype=\'int64\'),)\n        elif len(part2) == 1:\n            # We do not want to force an upcast of part2 if its length is 1\n            pass\n        else:\n            if len(in_shp) == 0 and self.outdim == 1:\n                part2 = (1,)\n            else:\n                raise ValueError(\'invalid output ndimensions (%i) for tensor \'\n                                 \'of rank %i\' % (self.outdim, len(in_shp)))\n\n        out_shape = (part1 + part2)\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        x, = inp\n        g_out, = grads\n        return [reshape(g_out, shape(x), x.ndim)]\n\n    def R_op(self, inputs, eval_points):\n        if None in eval_points:\n            return [None]\n        return self.make_node(*eval_points).outputs\n\n    def c_code_cache_version(self):\n        return (1, 1)\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        x, = inputs\n        out, = outputs\n        outdim = self.outdim\n        fail = sub[\'fail\']\n        return """"""\n        if (%(outdim)s == PyArray_NDIM(%(x)s))\n        {\n            Py_XDECREF(%(out)s);\n            Py_XINCREF(%(x)s);\n            %(out)s = %(x)s;\n        }\n        else\n        {\n            Py_XDECREF(%(out)s);\n\n            if (%(outdim)s == 1)\n            {\n                npy_intp size = PyArray_SIZE(%(x)s);\n                PyArray_Dims newshape;\n                newshape.ptr = &size;\n                newshape.len = 1;\n                %(out)s = (PyArrayObject*)PyArray_Newshape(%(x)s,\n                                                           &newshape,\n                                                           NPY_CORDER);\n            }\n            else\n            {\n                npy_intp *oldshape = PyArray_DIMS(%(x)s);\n                npy_intp newshape_dims[%(outdim)s];\n\n                int i;\n                for (i = 0; i < %(outdim)s - 1; ++i)\n                    newshape_dims[i] = oldshape[i];\n\n                newshape_dims[i] = 1;\n\n                for (int j = %(outdim)s - 1; j < PyArray_NDIM(%(x)s); ++j)\n                    newshape_dims[i] *= oldshape[j];\n\n                PyArray_Dims newshape;\n                newshape.ptr = newshape_dims;\n                newshape.len = %(outdim)s;\n                %(out)s = (PyArrayObject*)PyArray_Newshape(%(x)s,\n                                                           &newshape,\n                                                           NPY_CORDER);\n            }\n        }\n        if (!%(out)s)\n        {\n            //The error message should have been set by\n            // PyArray_Newshape\n            %(fail)s;\n        }\n        """""" % locals()\n\n\ndef is_flat(var, ndim=None, outdim=None):\n    """"""\n    Verifies the dimensionality of the var is equal to\n    outdim. This method is usually called after flatten method on a\n    variable, where the first outdim-1 dimension size(s) of the variable\n    is kept intact, and the last dimension size of the variable is made\n    equal to the multiplication of its remaining dimension size(s), such that\n    the variable would end up with as many dimension as outdim.\n\n    Parameters\n    ----------\n        var : theano.tensor.var.TensorVariable\n            the theano var on which the dimensionality is checked.\n\n        outdim : int\n            the expected dimensionality of var.\n\n    Returns\n    -------\n    bool\n        the comparison result of var\'s dim\n        and the expected outdim.\n    """"""\n    if outdim is None and ndim is None:\n        ndim = 1\n    elif outdim is not None and ndim is not None:\n        raise ValueError(""You should only specify ndim"")\n    elif outdim is not None:\n        warnings.warn(\n            ""flatten outdim parameter is deprecated, use ndim instead."")\n        ndim = outdim\n    return var.ndim == ndim\n\n\ndef flatten(x, ndim=None, outdim=None):\n    """"""\n    Reshapes the variable x by keeping\n    the first outdim-1 dimension size(s) of x the same,\n    and making the last dimension size of x equal to\n    the multiplication of its remaining dimension size(s).\n\n    Parameters\n    ----------\n        x : theano.tensor.var.TensorVariable\n            the variable that should be reshaped.\n\n        ndim : int\n            the number of dimensions of the returned variable\n            Default 1.\n        outdim : int\n            DEPRECATED synonym for ndim\n    Returns\n    -------\n    theano.tensor.var.TensorVariable\n        the flattend variable with dimensionality of outdim\n    """"""\n    if outdim is None and ndim is None:\n        ndim = 1\n    elif outdim is not None and ndim is not None:\n        raise ValueError(""You should only specify ndim"")\n    elif outdim is not None:\n        warnings.warn(\n            ""flatten outdim parameter is deprecated, use ndim instead."")\n\n        ndim = outdim\n    # Any input variable can be flattened to have ndim of 1,\n    # even if it\'s a scalar. Otherwise, ndim must be positive\n    # and smaller than x.ndim.\n    if ndim < 1 or (ndim > 1 and ndim > x.ndim):\n        raise ValueError(\'ndim %s out of bound [1, %d)\'\n                         % (ndim, x.ndim + 1))\n\n    if ndim > 1:\n        dims = tuple(x.shape[:ndim - 1]) + (-1,)\n    else:\n        dims = (-1,)\n    x_reshaped = x.reshape(dims)\n    bcast_kept_dims = x.broadcastable[:ndim - 1]\n    bcast_new_dim = python_all(x.broadcastable[ndim - 1:])\n    broadcastable = bcast_kept_dims + (bcast_new_dim,)\n    x_reshaped = theano.tensor.addbroadcast(\n        x_reshaped, *filter(lambda i: broadcastable[i], range(ndim)))\n    return x_reshaped\n\n\n# class TileGrad(Op):\n#     """"""\n#     Calculates the gradient of the Tile Op.\n#     """"""\n#     # this is so weird, I can\'t think of how to make this a general thing.\n#     def make_node(self, x, reps, g_out):\n#         return gof.Apply(self, [x, reps, g_out], [x.type()])\n#\n#     def perform(self, node, inp, out):\n#         x, reps, g_out = inp\n#         gx, = out\n#         xsh = x.shape\n#         if len(reps) == 2 and reps[1] == 1 and len(x.shape) == 1:\n#             gx[0] = numpy.sum(g_out, axis=0)\n#         else:\n#             raise NotImplementedError(\'x.shape, reps combination not \'\n#                                       \'supported\', (x.shape, reps))\n#\n# tilegrad = TileGrad()\n\n\nclass Tile(Op):\n    """"""\n    Construct an array by repeating the input x according to reps pattern.\n\n    .. note:: Deprecated\n              Use tile() instead.\n\n    Tiles its input according to reps. The length of reps is the number of\n    dimension of x and contains the number of times to tile x in each\n    dimension.\n\n    See Also\n    --------\n    numpy.tile : http://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html\n\n    """"""\n    __props__ = (""ndim"",)\n\n    def __init__(self, ndim):\n        self.ndim = ndim\n\n    def __str__(self):\n        return self.__class__.__name__ + ""{ndim=%d}"" % self.ndim\n\n    def make_node(self, x, reps):\n        warnings.warn((\n            ""Tile op is deprecated, use tile function instead.""), stacklevel=3)\n        x = as_tensor_variable(x)\n        reps = as_tensor_variable(reps)\n        return gof.Apply(self, [x, reps], [tensor(x.type.dtype, [False] *\n                                                  self.ndim)])\n\n    def perform(self, node, inp, out_):\n        x, reps = inp\n        out, = out_\n        res = np.tile(x, reps)\n        if res.ndim != self.ndim:\n            raise ValueError(\n                \'Tile.perform produced incorrect number of dimensions\')\n\n        if (np.asarray(reps) == 1).all():\n            # In that case, some NumPy version return a view!  As this\n            # op isn\'t declared as inplace, we need to check that and\n            # copy the data.\n            if np.may_share_memory(res, x):\n                res = res.copy()\n        out[0] = res\n\n    def infer_shape(self, node, in_shapes):\n        # Note: in contrast with numpy, it is assumed that x.shape and reps\n        # have equal length;  see also tile function below\n\n        # Note: if reps were to be allowed not to be a constant and x.shape\n        # and reps to be unequal, the following block of code could be used:\n        # prepend 1 to x.shape if needed\n        # if self.ndim > x.ndim:\n        # shp = concatenate(ones(self.ndim - x.ndim), shp)\n        # prepend 1 to reps if needed\n        # reps = concatenate(ones(self.ndim - reps.shape[0]), reps)\n\n        x, reps = node.inputs\n        shp = in_shapes[0]\n        tiled_shp = shp * reps\n        out_shape = []\n        for i in xrange(self.ndim):\n            out_shape.append(tiled_shp[i])\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        x, reps = inp\n        g_out, = grads\n        # return [tilegrad(x, reps, g_out), None]\n        raise NotImplementedError()\n\n\ndef tile(x, reps, ndim=None):\n    """"""\n    Tile input array `x` according to `reps`.\n\n    See the docstring of `numpy.tile` for details.\n\n    \'reps\' can be constant integer (e.g. 3), constant vector(e.g. [2 3]),\n    symbolic scalar (e.g. tensor.iscalar()), symbolic vector (e.g. tensor.ivector())\n    or a list of symbolic scalar (e.g. [tensor.iscalar(), tensor.iscalar()]).\n\n    ndim is the number of the dimensions of the output, if it is provided, ndim\n    should be equal or larger than x.ndim and len(reps), otherwise, we will use\n    max(x.ndim, len(reps)) as ndim. If reps is symbolic vector, the ndim has to\n    be provided.\n\n    if ndim is not None and ndim < x.ndim:\n        raise ValueError(""ndim should be equal or larger than x.ndim"")\n\n    # if reps is tensor.scalar, integer or tensor.vector, we convert it to a list.\n    if not isinstance(reps, (list, tuple)):\n        reps_astensor = as_tensor_variable(reps)\n        ndim_check = reps_astensor.ndim\n        if reps_astensor.dtype not in theano.tensor.discrete_dtypes:\n            raise ValueError(""elements of reps must be integer dtype"")\n\n        # tensor.scalar/integer case\n        if ndim_check == 0:\n            reps = [reps]\n\n        # tensor.vector case\n        elif ndim_check == 1:\n            if ndim is None:\n                raise ValueError(""if reps is tensor.vector, you should specify ""\n                                 ""the ndim"")\n            else:\n                offset = ndim - reps.shape[0]\n\n                # assert that reps.shape[0] does not exceed ndim\n                offset = theano.tensor.opt.assert_(offset, ge(offset, 0))\n\n                # if reps.ndim is less than x.ndim, we pad the reps with\n                # ""1"" so that reps will have the same ndim as x.\n                reps_ = [switch(i < offset, 1, reps[i - offset]) for i in range(ndim)]\n                reps = reps_\n\n        # other raise error\n        else:\n            raise ValueError(""the dimension of reps should not exceed 1"")\n    else:\n        if ndim is not None and len(reps) > ndim:\n            raise ValueError(""len(reps) should be equal or less than ndim"")\n        if not np.all([isinstance(r, integer_types) or\n                       (isinstance(r, TensorVariable) and\n                        r.dtype in theano.tensor.discrete_dtypes) for r in reps]):\n            raise ValueError(""elements of reps must be scalars of integer dtype"")\n\n    # if reps.ndim is less than x.ndim, we pad the reps with\n    # ""1"" so that reps will have the same ndim as x.\n    reps = list(reps)\n    if ndim is None:\n        ndim = builtins.max(len(reps), x.ndim)\n    if len(reps) < ndim:\n        reps = [1] * (ndim - len(reps)) + reps\n\n    shape = [1] * (ndim - x.ndim) + [x.shape[i] for i in xrange(x.ndim)]\n    alloc_shape = reps + shape\n    y = alloc(x, *alloc_shape)\n    shuffle_ind = np.arange(ndim * 2).reshape(2, ndim)\n    shuffle_ind = shuffle_ind.transpose().flatten()\n    y = y.dimshuffle(*shuffle_ind)\n    new_shapes = [sh * reps[i] for i, sh in enumerate(shape)]\n    y = y.reshape(new_shapes)\n\n\nclass ARange(Op):\n    """"""Create an array containing evenly spaced values within a given interval.\n\n    Parameters and behaviour are the same as numpy.arange().\n\n    """"""\n    __props__ = (""dtype"",)\n\n    def __init__(self, dtype):\n        self.dtype = dtype\n\n    def make_node(self, start, stop, step):\n        start, stop, step = map(as_tensor_variable, (start, stop, step))\n        assert start.ndim == 0\n        assert stop.ndim == 0\n        assert step.ndim == 0\n\n        inputs = [start, stop, step]\n        outputs = [tensor(self.dtype, (False,))]\n\n        return Apply(self, inputs, outputs)\n\n    @theano.configparser.change_flags(warn_float64=\'ignore\')\n    def infer_shape(self, node, i_shapes):\n        # Note start, stop and step can be float numbers.\n        start, stop, step = node.inputs\n\n        def is_constant_value(var, value):\n            try:\n                v = get_scalar_constant_value(var)\n                return np.all(v == value)\n            except NotScalarConstantError:\n                pass\n            return False\n\n        def upcast(var):\n            if (var.dtype in integer_dtypes and\n                    # We do not want to cast uint64 to int64 as this can\n                    # loose information. If we upcast uint64 with int64,\n                    # this give float64. This is safer then checking for\n                    # uint64 in case we support [u]int128 or other in the\n                    # future.\n                    scal.upcast(var.dtype, \'int64\') == \'int64\'):\n                return cast(var, \'int64\')\n            return var\n\n        if is_constant_value(step, 1):\n            if is_constant_value(start, 0):\n                return [(cast(stop, \'int64\'),)]\n            else:\n                stop = upcast(stop)\n                start = upcast(start)\n                return [(maximum(cast(stop - start, \'int64\'), 0),)]\n        else:\n            stop = upcast(stop)\n            start = upcast(start)\n            return [(maximum(cast(ceil(cast((stop - start), \'float64\') / step),\n                    \'int64\'), 0),)]\n\n    def perform(self, node, inp, out_):\n        start, stop, step = inp\n        out, = out_\n        start = start.item()\n        stop = stop.item()\n        step = step.item()\n        out[0] = np.arange(start, stop, step, dtype=self.dtype)\n\n    def connection_pattern(self, node):\n\n        return [[True], [False], [True]]\n\n    def L_op(self, inputs, outputs, grads):\n        start, stop, step = inputs\n        gz, = grads\n        # `start` and `step` affect the output values\n        # but the outputs are integers so there\'s\n        # no gradient through them.\n        # When they are not integers, the gradients are\n        # as expressed below.\n        # `stop` does not affect the output values,\n        # just the output shape, so it is disconnected.\n\n        if self.dtype in discrete_dtypes:\n            return [start.zeros_like(dtype=config.floatX),\n                    DisconnectedType()(),\n                    step.zeros_like(dtype=config.floatX)]\n        else:\n            num_steps_taken = outputs[0].shape[0]\n            return [gz.sum(),\n                    DisconnectedType()(),\n                    (gz * arange(num_steps_taken, dtype=self.dtype)).sum()]\n\n    def R_op(self, inputs, eval_points):\n        return [None]\n_arange = {}\n\n\ndef arange(start, stop=None, step=1, dtype=None):\n    # If only one argument is provided, it is in fact the ""stop"" argument,\n    # and start is 0.\n    if stop is None:\n        start, stop = 0, start\n\n    start, stop, step = map(as_tensor_variable, (start, stop, step))\n    # If dtype is not provided, infer it from the other arguments\n    if dtype is None:\n        dtype = scal.upcast(start.type.dtype, stop.type.dtype, step.type.dtype)\n        # don\'t try to be stingy and byte-optimize, this leads to\n        # overflow problems.\n        if dtype in int_dtypes:\n            dtype = \'int64\'\n        if dtype in uint_dtypes:\n            dtype = \'uint64\'\n        if config.cast_policy in (\'numpy\', \'numpy+floatX\'):\n            # We enforce numpy semantics, except in the special case where\n            # `config.cast_policy` is \'numpy+floatX\' and we want to use float32\n            # rather than float64.\n            # As an example, if `start`, `stop` and `step` are all int32,\n            # `numpy.arange` returns an int64 array (on 64-bit platforms),\n            # while the upcast above returns int32.\n            numpy_dtype = np.arange(\n                start=np.array(0, dtype=start.dtype),\n                stop=np.array(1, dtype=stop.dtype),\n                step=np.array(1, dtype=step.dtype)).dtype\n            if numpy_dtype != dtype:\n                if (config.cast_policy == \'numpy+floatX\' and\n                    config.floatX == \'float32\' and\n                    numpy_dtype == \'float64\' and\n                    # No explicit float64 in the three arguments?\n                    python_all(\n                        dt != \'float64\'\n                        for dt in [s.dtype for s in (start, stop, step)])):\n                    # We use float32 instead.\n                    assert dtype != \'float64\'\n                    dtype = \'float32\'\n                else:\n                    # We use the same dtype as numpy instead of the result of\n                    # the upcast.\n                    dtype = str(numpy_dtype)\n\n    if dtype not in _arange:\n        _arange[dtype] = ARange(dtype)\n    return _arange[dtype](start, stop, step)\n\n\nclass _nd_grid(object):\n    """"""Create a dense n-dimensional \'meshgrid\' with equally spaced points.\n\n    Used to create the instance ``mgrid`` and ``ogrid`` which act similarly\n    to their numpy equivalents.\n\n    Parameters\n    ----------\n    sparse : boolean, optional, default=True\n        Specifying False leads to the equivalent of numpy\'s mgrid functionality.\n        Specifying True leads to the equivalent of ogrid.\n\n    Examples\n    --------\n    >>> a = T.mgrid[0:5, 0:3]\n    >>> a[0].eval()\n    array([[0, 0, 0],\n           [1, 1, 1],\n           [2, 2, 2],\n           [3, 3, 3],\n           [4, 4, 4]], dtype=int8)\n    >>> a[1].eval()\n    array([[0, 1, 2],\n           [0, 1, 2],\n           [0, 1, 2],\n           [0, 1, 2],\n           [0, 1, 2]], dtype=int8)\n    >>> b = T.ogrid[0:5, 0:3]\n    >>> b[0].eval()\n    array([[0],\n           [1],\n           [2],\n           [3],\n           [4]], dtype=int8)\n    >>> b[1].eval()\n    array([[0, 1, 2, 3]], dtype=int8)\n\n    def __init__(self, sparse=False):\n        self.sparse = sparse\n\n    def __getitem__(self, *args):\n\n        ndim = len(args[0])\n        for sl in args[0]:\n            if isinstance(sl.step, python_complex):\n                raise NotImplementedError(""Not implemented for slices ""\n                                          ""whose step is complex"")\n        ranges = [arange(sl.start or 0,\n                         sl.stop,\n                         sl.step or 1) for sl in args[0]]\n        shapes = [tuple([1] * j + [r.shape[0]] + [1] * (ndim - 1 - j))\n                  for j, r in enumerate(ranges)]\n        ranges = [r.reshape(shape) for r, shape in zip(ranges, shapes)]\n        if self.sparse:\n            grids = ranges\n        else:\n            grids = []\n            ones = [ones_like(r) for r in ranges]\n            for i in range(ndim):\n                grid = 1\n                for j in range(ndim):\n                    if j == i:\n                        grid = grid * ranges[j]\n                    else:\n                        grid = grid * ones[j]\n                grids.append(grid)\n        return grids\n\n\nmgrid = _nd_grid()\nogrid = _nd_grid(sparse=True)\n\n\nclass PermuteRowElements(Op):\n    """"""Permute the elements of each row (inner-most dim) of a tensor.\n\n    A permutation will be applied to every row (vector) of the input tensor x.\n    Depending on the dimensionality of x and the permutation tensor y,\n    different cases are possible.\n    If y.ndim = 1, y is a single permutation, that will be applied to every\n    vector of x. For instance, if x is a matrix, the same permutation will be\n    applied to each row of x.\n    If x.ndim = y.ndim, each row of x corresponds to a row of y, containing\n    a permutation that will be applied to that row. For instance, if x and y\n    are two matrices, a different permutation will be applied to each row of x.\n    If x.ndim > y.ndim, y will be broadcasted to fit x, then each row (vector)\n    of x will be reordered according to the corresponding row of y. (This is\n    a generalization of the first case).\n    If x.ndim = 1, every permutation in y will be applied to x, and the output\n    will contain all the results.\n    If x.ndim < y.ndim, x will be broadcasted to fit y, and different\n    permutations contained in y will be applied to each vector in x. (This is\n    a generalization of the previous case).\n\n    If the ""inverse"" argument is True, the Op will perform the inverse\n    permutation instead.\n    """"""\n    __props__ = ()\n\n    def make_node(self, x, y, inverse):\n        x = as_tensor_variable(x)\n        y = as_tensor_variable(y)\n        if inverse:  # as_tensor_variable does not accept booleans\n            inverse = as_tensor_variable(1)\n        else:\n            inverse = as_tensor_variable(0)\n\n        # y should contain integers\n        assert y.type.dtype in integer_dtypes\n        # Inverse should be an integer scalar\n        assert (inverse.type.ndim == 0 and inverse.type.dtype in integer_dtypes)\n\n        # Match shapes of x and y\n        x_dim = x.type.ndim\n        y_dim = y.type.ndim\n\n        if x_dim > y_dim:\n            y = shape_padleft(y, n_ones=(x_dim - y_dim))\n        elif x_dim < y_dim:\n            x = shape_padleft(x, n_ones=(y_dim - x_dim))\n\n        # Compute the broadcastable pattern of the output\n        out_broadcastable = [xb and yb for xb, yb in\n                             izip(x.type.broadcastable, y.type.broadcastable)]\n        out_type = tensor(dtype=x.type.dtype, broadcastable=out_broadcastable)\n\n        inputlist = [x, y, inverse]\n        outputlist = [out_type]\n        return Apply(self, inputlist, outputlist)\n\n    def _rec_perform(self, node, x, y, inverse, out, curdim):\n        """"""Perform the permutation by doing a recursion over the input\n        dimensions.\n\n        For every dimension, starting with the leftmost, the right set of\n        indices is determined (depending if broadcasting or not), then\n        the function is recursively called on the appropriate subtensors.\n\n        The terminal case is reached when the current tensors are vector,\n        then the permutation contained in y is applied to x.\n\n        Parameters\n        ----------\n        x : tensor\n            The input tensor, on which the permutation is applied.\n        y : tensor\n            Tensor containing the permutations to apply.\n        out : tensor\n            Tensor storing the output result.\n        curdim : int\n            Counter of the current depth of recursion.\n        inverse\n            Wether to apply permutations or their inverse.\n\n        """"""\n        if len(x.shape) == 1:\n            # Numpy advanced indexing works in this case\n            if inverse:\n                out[y] = x[:]\n            else:\n                out[:] = x[y]\n        else:\n            xs0 = x.shape[0]\n            ys0 = y.shape[0]\n            if xs0 == ys0:\n                for i in xrange(xs0):\n                    self._rec_perform(node, x[i], y[i], inverse, out[i],\n                                      curdim + 1)\n            elif ys0 == 1 and node.inputs[1].type.broadcastable[curdim]:\n                # Broadcast y\n                for i in xrange(xs0):\n                    self._rec_perform(node, x[i], y[0], inverse, out[i],\n                                      curdim + 1)\n            elif xs0 == 1 and node.inputs[0].type.broadcastable[curdim]:\n                # Broadcast x\n                for i in xrange(ys0):\n                    self._rec_perform(node, x[0], y[i], inverse, out[i],\n                                      curdim + 1)\n            else:\n                raise ValueError(\'Dimension mismatch: %s, %s\' % (xs0, ys0))\n\n    def perform(self, node, inp, out):\n        x, y, inverse = inp\n        outs, = out\n        x_s = x.shape\n        y_s = y.shape\n        assert len(x_s) == len(y_s)\n\n        # Make sure the output is big enough\n        out_s = []\n        for xdim, ydim in izip(x_s, y_s):\n            if xdim == ydim:\n                outdim = xdim\n            elif xdim == 1:\n                outdim = ydim\n            elif ydim == 1:\n                outdim = xdim\n            else:\n                raise ValueError(\'Dimension mismatch: %s, %s\' % (xdim, ydim))\n            out_s.append(outdim)\n\n        if outs[0] is None or outs[0].shape != out_s:\n            outs[0] = np.empty(out_s, dtype=x.dtype)\n\n        self._rec_perform(node, x, y, inverse, outs[0], curdim=0)\n\n    def infer_shape(self, node, in_shapes):\n        shp_x = in_shapes[0]\n        shp_y = in_shapes[1]\n        assert len(shp_x) == len(shp_y)\n        out_shape = []\n        for i in xrange(len(shp_x)):\n            out_shape.append(maximum(shp_x[i], shp_y[i]))\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        x, y, inverse = inp\n        gz, = grads\n        # First, compute the gradient wrt the broadcasted x.\n        # If \'inverse\' is False (0), apply the inverse of y on gz.\n        # Else, apply y on gz.\n        gx = permute_row_elements(gz, y, eq(inverse, 0))\n\n        # If x has been broadcasted along some axes, we need to sum\n        # the gradient over these axes, but keep the dimension (as\n        # broadcastable)\n        broadcasted_dims = [dim for dim in xrange(gz.type.ndim)\n                            if x.type.broadcastable[dim] and\n                            not gz.type.broadcastable[dim]]\n        gx = Sum(axis=broadcasted_dims)(gx)\n\n        # Sum(...) removed the dimensions in broadcasted_dims,\n        # so we need to put them back.\n        newdims = []\n        i = 0\n        for dim in xrange(gz.type.ndim):\n            if dim in broadcasted_dims:\n                newdims.append(\'x\')\n            else:\n                newdims.append(i)\n                i += 1\n\n        gx = DimShuffle(gx.type.broadcastable, newdims)(gx)\n        assert gx.type.broadcastable == x.type.broadcastable\n\n        # if x is an integer type, then so is the output.\n        # this means f(x+eps) = f(x) so the gradient with respect\n        # to x is zero\n        if x.type.dtype in discrete_dtypes:\n            gx = x.zeros_like()\n\n        # The elements of y and of inverse both affect the output,\n        # so they are connected to the output,\n        # and the transformation isn\'t defined if their values\n        # are non-integer, so the gradient with respect to them is\n        # undefined\n\n        return [gx, grad_undefined(self, 1, y),\n                grad_undefined(self, 1, inverse)]\n\n_permute_row_elements = PermuteRowElements()\n\n\ndef permute_row_elements(x, y, inverse=0):\n    return _permute_row_elements(x, y, inverse)\n\n\ndef inverse_permutation(perm):\n    """"""Computes the inverse of permutations.\n\n    Each row of input should contain a permutation of the first integers.\n\n    """"""\n    return permute_row_elements(\n        arange(perm.shape[-1], dtype=perm.dtype),\n        perm,\n        inverse=True)\n\n\n#########################\n# Linalg : Dot\n#########################\n#\n# For BLAS-related ops see blas.py\n#\n# TODO: Dotinv should go here, Eigs, Svd, etc.\n\n\nclass Dot(Op):\n    """"""\n    Computes the dot product of two variables. For two matrices, this is\n    equivalent to matrix multiplication. For two vectors, this is the inner\n    product.\n\n    Notes\n    -----\n    Matrix-matrix products are sometimes optimized to Dot22 or Gemm ops\n    (see tensor.blas).\n    Vector-vector products are sometimes optimized to Ger or CGer (see\n    tensor.blas).\n    Matrix-vector products are sometimes optimized to Gemv, CGemv (see\n    tensor.blas).\n\n    """"""\n    __props__ = ()\n\n    # the rationale for Dot22 is related to getting GEMM Ops into the\n    # graph.  See Dot22 in tensor.blas for details.\n\n    def make_node(self, *inputs):\n        inputs = list(map(as_tensor_variable, inputs))\n\n        if len(inputs) != 2:\n            raise TypeError(\n                \'theano.tensor.Dot: 2 arguments required, %d given \' %\n                len(inputs))\n        if inputs[0].ndim not in (1, 2):\n            raise TypeError(\n                \'theano.tensor.Dot: input 0 (0-indexed) must have ndim of \'\n                \'1 or 2, %d given. Consider calling theano.tensor.dot \'\n                \'instead.\' % inputs[0].ndim)\n        if inputs[1].ndim not in (1, 2):\n            raise TypeError(\n                \'theano.tensor.Dot: input 1 (0-indexed) must have ndim of \'\n                \'1 or 2, %d given. Consider calling theano.tensor.dot \'\n                \'instead.\' % inputs[1].ndim)\n\n        i_broadcastables = [input.type.broadcastable for input in inputs]\n        bx, by = i_broadcastables\n        if len(by) == 2:  # y is a matrix\n            bz = bx[:-1] + by[-1:]\n        elif len(by) == 1:  # y is vector\n            bz = bx[:-1]\n\n        i_dtypes = [input.type.dtype for input in inputs]\n        outputs = [tensor(scal.upcast(*i_dtypes), bz)]\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inp, out):\n        x, y = inp\n        z, = out\n\n        # the asarray is here because dot between two vectors\n        # gives a numpy float object but we need to return a 0d\n        # ndarray\n        z[0] = np.asarray(np.dot(x, y))\n\n    def grad(self, inp, grads):\n\n        x, y = inp\n        gz, = grads\n        xdim, ydim, gdim = x.type.ndim, y.type.ndim, gz.type.ndim\n\n        # grad is scalar, so x is vector and y is vector\n        if gdim == 0:\n            xgrad = gz * y\n            ygrad = gz * x\n\n        # x is vector, y is matrix, grad is vector\n        elif xdim == 1 and ydim == 2:\n            xgrad = dot(gz, y.T)\n            ygrad = outer(x.T, gz)\n\n        # x is matrix, y is vector, grad is vector\n        elif xdim == 2 and ydim == 1:\n            xgrad = outer(gz, y.T)\n            ygrad = dot(x.T, gz)\n\n        # x is matrix, y is matrix, grad is matrix\n        elif xdim == ydim == 2:\n            xgrad = dot(gz, y.T)\n            ygrad = dot(x.T, gz)\n\n        # If x or y contain broadcastable dimensions but only one of\n        # them know that a matching dimensions is broadcastable, the\n        # above code don\'t always return the right broadcast pattern.\n        # This cause problem down the road. See gh-1461.\n        if xgrad.broadcastable != x.broadcastable:\n            xgrad = patternbroadcast(xgrad, x.broadcastable)\n        if ygrad.broadcastable != y.broadcastable:\n            ygrad = patternbroadcast(ygrad, y.broadcastable)\n\n        rval = xgrad, ygrad\n\n        for elem in rval:\n            assert elem.dtype.find(\'float\') != -1\n\n    def R_op(self, inputs, eval_points):\n        # R_op for a \\dot b evaluted at c for a and d for b is\n        # simply c \\dot b + a \\dot d\n\n        assert len(inputs) == 2\n        assert len(eval_points) == 2\n        if eval_points[0] is None and eval_points[1] is None:\n            return [None]\n\n        if eval_points[0]:\n            t1 = self(eval_points[0], inputs[1])\n        if eval_points[1]:\n            t2 = self(inputs[0], eval_points[1])\n\n        if eval_points[0] and eval_points[1]:\n            return [t1 + t2]\n        elif eval_points[0]:\n            return [t1]\n        else:\n            return [t2]\n\n    def infer_shape(self, node, shapes):\n        xshp, yshp = shapes\n        x, y = node.inputs\n\n        # vector / vector\n        if x.ndim == 1 and y.ndim == 1:\n            return [()]\n        # matrix / vector\n        if x.ndim == 2 and y.ndim == 1:\n            return [xshp[:-1]]\n        # vector / matrix\n        if x.ndim == 1 and y.ndim == 2:\n            return [yshp[-1:]]\n        # matrix / matrix\n        if x.ndim == 2 and y.ndim == 2:\n            return [xshp[:-1] + yshp[-1:]]\n        raise NotImplementedError()\n\n    def __str__(self):\n        return ""dot""\n\n_dot = Dot()\npprint.assign(_dot, printing.OperatorPrinter(printing.special[\'middle_dot\'],\n                                             -1, \'left\'))\n\n\ndef dot(a, b):\n    """"""\n    Computes the dot product of two variables.\n\n    For two matrices, this is equivalent to matrix multiplication.\n    For two vectors, this is the inner product.\n    When one variable is a scalar, this is like elementwise multiplication.\n    For N dimensions, this is a sum product over the last axis\n    of the first array and the second-to-last axis of the second array:\n\n        dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n\n    Note that this dot function does one of three things, in the following\n    sequence:\n\n        1.  If either a or b is scalar, it returns the elementwise product\n            without calling the Theano Dot op.\n\n        2.  If either a or b has more than 2 dimensions, it calls Theano\'s\n            tensordot function with appropriate axes. The tensordot function\n            expresses high-dimensional dot products in terms of 2D matrix\n            multiplications, so it may be possible to futherize optimize for\n            performance.\n\n        3.  If both a and b have either 1 or 2 dimensions, it calls Theano\'s\n            Dot op on a and b.\n\n    Notes\n    -----\n    Matrix-matrix products are sometimes optimized to Dot22 or Gemm ops\n    (see tensor.blas).\n    Vector-vector products are sometimes optimized to Ger or CGer (see\n    tensor.blas).\n    Matrix-vector products are sometimes optimized to Gemv, CGemv (see\n    tensor.blas).\n\n    """"""\n    a, b = as_tensor_variable(a), as_tensor_variable(b)\n\n    if a.ndim == 0 or b.ndim == 0:\n        return a * b\n    elif a.ndim > 2 or b.ndim > 2:\n        return tensordot(a, b, [[a.ndim - 1], [np.maximum(0, b.ndim - 2)]])\n    else:\n        return _dot(a, b)\n\n\n#########################\n# Linalg : TensorDot\n#########################\n\ndef _tensordot_as_dot(a, b, axes, dot, batched):\n    """"""\n    Reduces a tensor dot product to a matrix or vector dot product. Based\n    on code from Tijmen Tieleman\'s gnumpy\n    (http://www.cs.toronto.edu/~tijmen/gnumpy.html).\n\n    Please see the documentation of tensordot for the meaning of the a, b\n    and axes arguments.\n\n    :param dot: a function that accepts two symbolic variables and computes\n                the appropriate dot product (e.g. dot, batched_dot)\n    :type dot: function\n\n    :param batched: whether to treat the first axis of a and b as a batch\n                    axis.  If so, this axis will be preserved in the output,\n                    allowing this function to be used also for batched\n                    tensor dot products.\n    :type batched: boolean\n\n    :returns: a tensor with shape equal to the concatenation of a\'s shape\n              (less any dimensions that were summed over) and b\'s shape\n              (less the first dimension and any dimensions that were summed\n              over).\n    :rtype: symbolic tensor\n    """"""\n    a, b = as_tensor_variable(a), as_tensor_variable(b)\n\n    if not np.isscalar(axes) and len(axes) != 2:\n        raise ValueError(\'Axes should be an integer or a \'\n                         \'list/tuple of len 2 (%s was provided)\'\n                         % str(axes))\n\n    # if \'axes\' is a number of axes to multiply and sum over (trailing axes\n    # of a, leading axes of b), we can just reshape and use dot.\n    elif np.isscalar(axes):\n        axes = int(axes)\n\n        for operand_name, operand in ((""a"", a), (""b"", b)):\n            if axes > operand.ndim:\n                raise ValueError(\n                    \'axes can not be larger than the dimension of %s \'\n                    \'(%s.ndim=%i, axes=%i)\'\n                    % (operand_name, operand_name, operand.ndim, axes))\n            if batched and axes == operand.ndim:\n                raise ValueError(\n                    \'axes to sum over must not include the batch axis \'\n                    \'of %s (%s.ndim=%i, axes=%i)\'\n                    % (operand_name, operand_name, operand.ndim, axes))\n\n        batch_axes = 1 if batched else 0\n        a_outaxes = slice(0, a.ndim - axes)\n        b_outaxes = slice(batch_axes + axes, b.ndim)\n        outshape = concatenate([a.shape[a_outaxes], b.shape[b_outaxes]])\n        outbcast = a.broadcastable[a_outaxes] + b.broadcastable[b_outaxes]\n        outndim = len(outbcast)\n\n        a_shape = [1] * 2\n        b_shape = [1] * 2\n\n        # compute total size of summed axes\n        for i in xrange(0, axes):\n            a_shape[1] *= a.shape[-(i + 1)]\n            b_shape[0] *= b.shape[batch_axes + i]\n        # compute total size of other axes\n        for i in xrange(0, a.ndim - axes - batch_axes):\n            a_shape[0] *= a.shape[batch_axes + i]\n        for i in xrange(0, b.ndim - axes - batch_axes):\n            b_shape[1] *= b.shape[-(i + 1)]\n\n        if batched:\n            a_shape.insert(0, a.shape[0])\n            b_shape.insert(0, b.shape[0])\n\n        a_reshaped = a.reshape(a_shape)\n        b_reshaped = b.reshape(b_shape)\n\n        out_reshaped = dot(a_reshaped, b_reshaped)\n        out = out_reshaped.reshape(outshape, outndim)\n        # Make sure the broadcastable pattern of the result is correct,\n        # since some shape information can be lost in the reshapes.\n        return patternbroadcast(out, outbcast)\n\n    # if \'axes\' is a list, transpose a and b such that the summed axes of a\n    # are last and the summed axes of b are first.\n    else:\n        axes = [_pack(axes_) for axes_ in axes]\n\n        if len(axes[0]) != len(axes[1]):\n            raise ValueError(\'Axes elements must have the same length.\')\n\n        for i, (operand_name, operand) in enumerate(((""a"", a),\n                                                     (""b"", b))):\n            if len(axes[i]) > operand.ndim:\n                raise ValueError(\n                    \'axes[%i] should be array_like with length less than \'\n                    \'the dimensions of %s (%s.ndim=%i, len(axes[0])=%i).\' %\n                    (i, operand_name, operand_name, operand.ndim,\n                     len(axes[i])))\n            if len(axes[i]) > 0 and np.max(axes[i]) >= operand.ndim:\n                raise ValueError(\n                    \'axes[%i] contains dimensions greater than or equal \'\n                    \'to %s.ndim (%s.ndim=%i, max(axes[0])=%i).\' %\n                    (i, operand_name, operand_name, operand.ndim,\n                     np.max(np.array(axes[i]))))\n            if batched and 0 in axes[i]:\n                raise ValueError(\n                    \'axes to sum over must not contain the batch axis \'\n                    \'(axes[%i]=%s)\' %\n                    (i, axes[i]))\n\n        batch_axes = [0] if batched else []\n        other_axes = [[x for x in xrange(operand.ndim)\n                       if x not in axes[i] and x not in batch_axes]\n                      for i, operand in enumerate((a, b))]\n\n        a_shuffled = a.dimshuffle(batch_axes + other_axes[0] + axes[0])\n        b_shuffled = b.dimshuffle(batch_axes + axes[1] + other_axes[1])\n\n        # now that a and b are in the right order, recur with integer axes\n        return _tensordot_as_dot(a_shuffled, b_shuffled, len(axes[0]),\n                                 dot=dot, batched=batched)\n\n\ndef tensordot(a, b, axes=2):\n    """"""\n    Compute a generalized dot product over provided axes.\n\n    Given two tensors a and b, tensordot computes a generalized dot product over\n    the provided axes. Theano\'s implementation reduces all expressions to\n    matrix or vector dot products and is based on code from Tijmen Tieleman\'s\n    gnumpy (http://www.cs.toronto.edu/~tijmen/gnumpy.html).\n\n    Parameters\n    ----------\n    a: symbolic tensor\n        The first tensor variable.\n    b: symbolic tensor\n        The second tensor variable\n    axes: int or array-like of length 2\n        If an integer, the number of axes to sum over.\n        If an array, it must have two array elements containing the axes\n        to sum over in each tensor.\n\n        Note that the default value of 2 is not guaranteed to work\n        for all values of a and b, and an error will be raised if\n        that is the case. The reason for keeping the default is to\n        maintain the same signature as numpy\'s tensordot function\n        (and np.tensordot raises analogous errors for non-compatible\n        inputs).\n\n        If an integer i, it is converted to an array containing\n        the last i dimensions of the first tensor and the first\n        i dimensions of the second tensor:\n            axes = [list(range(a.ndim - i, b.ndim)), list(range(i))]\n\n        If an array, its two elements must contain compatible axes\n        of the two tensors. For example, [[1, 2], [2, 0]] means sum\n        over the 2nd and 3rd axes of a and the 3rd and 1st axes of b.\n        (Remember axes are zero-indexed!) The 2nd axis of a and the\n        3rd axis of b must have the same shape; the same is true for\n        the 3rd axis of a and the 1st axis of b.\n\n    Returns\n    -------\n    symbolic tensor\n        A tensor with shape equal to the concatenation of a\'s shape\n        (less any dimensions that were summed over) and b\'s shape\n        (less any dimensions that were summed over).\n\n    Examples\n    --------\n    It may be helpful to consider an example to see what tensordot does.\n    Theano\'s implementation is identical to NumPy\'s. Here a has shape (2, 3, 4)\n    and b has shape (5, 6, 4, 3). The axes to sum over are [[1, 2], [3, 2]] --\n    note that a.shape[1] == b.shape[3] and a.shape[2] == b.shape[2]; these axes\n    are compatible. The resulting tensor will have shape (2, 5, 6) -- the\n    dimensions that are not being summed:\n\n    >>> a = np.random.random((2,3,4))\n    >>> b = np.random.random((5,6,4,3))\n\n    #tensordot\n    >>> c = np.tensordot(a, b, [[1,2],[3,2]])\n\n    #loop replicating tensordot\n    >>> a0, a1, a2 = a.shape\n    >>> b0, b1, _, _ = b.shape\n    >>> cloop = np.zeros((a0,b0,b1))\n\n    #loop over non-summed indices -- these exist\n    #in the tensor product.\n    >>> for i in range(a0):\n    ...     for j in range(b0):\n    ...         for k in range(b1):\n    ...             #loop over summed indices -- these don\'t exist\n    ...             #in the tensor product.\n    ...             for l in range(a1):\n    ...                 for m in range(a2):\n    ...                     cloop[i,j,k] += a[i,l,m] * b[j,k,m,l]\n\n    >>> np.allclose(c, cloop)\n    true\n\n    This specific implementation avoids a loop by transposing a and b such that\n    the summed axes of a are last and the summed axes of b are first. The\n    resulting arrays are reshaped to 2 dimensions (or left as vectors, if\n    appropriate) and a matrix or vector dot product is taken. The result is\n    reshaped back to the required output dimensions.\n\n    In an extreme case, no axes may be specified. The resulting tensor\n    will have shape equal to the concatenation of the shapes of a and b:\n\n    >>> c = np.tensordot(a, b, 0)\n    >>> print(a.shape)\n    (2,3,4)\n    >>> print(b.shape)\n    (5,6,4,3)\n    >>> print(c.shape)\n    (2,3,4,5,6,4,3)\n\n    See the documentation of numpy.tensordot for more examples.\n\n    """"""\n    return _tensordot_as_dot(a, b, axes, dot=dot, batched=False)\n\n\ndef outer(x, y):\n    """"""Return vector-vector outer product.\n\n    If an input isn\'t a vector, we flatten it first.\n\n    """"""\n    if x.ndim != 1:\n        x = x.flatten()\n    if y.ndim != 1:\n        y = y.flatten()\n    return dot(\n        x.dimshuffle(0, \'x\'),\n        y.dimshuffle(\'x\', 0))\n\n\ndef any(x, axis=None, keepdims=False):\n    out = elemwise.Any(axis)(x)\n\n    if keepdims:\n        out = makeKeepDims(x, out, axis)\n    return out\n\n\ndef all(x, axis=None, keepdims=False):\n    out = elemwise.All(axis)(x)\n\n    if keepdims:\n        out = makeKeepDims(x, out, axis)\n    return out\n\n\n# Some NumPy version like 1.9.2 return a view for numpy.diagonal\nx = np.zeros((4, 4))\nnumpy_diagonal_return_view = np.may_share_memory(np.diagonal(x), x)\ndel x\n\n\nclass ExtractDiag(Op):\n    """"""\n    Return specified diagonals.\n\n    If x is 2-D, returns the diagonal of x with the given offset,\n    i.e., the collection of elements of the form x[i, i+offset].\n    If x has more than two dimensions, then the axes specified by\n    axis1 and axis2 are used to determine the 2-D sub-array whose\n    diagonal is returned. The shape of the resulting array can be\n    determined by removing axis1 and axis2 and appending an index\n    to the right equal to the size of the resulting diagonals.\n\n    Parameters\n    ----------\n    x: A tensor variable with x.ndim >= 2.\n\n    offset: Offset of the diagonal from the main diagonal.\n        Can be positive or negative.\n        Defaults to main diagonal (0).\n\n    axis1: Axis to be used as the first axis of the 2-D\n        sub-arrays from which the diagonals should be taken.\n        Defaults to first axis (0).\n\n    axis2: Axis to be used as the second axis of the 2-D\n        sub-arrays from which the diagonals should be taken.\n        Defaults to second axis (1).\n\n    Returns\n    -------\n    array_of_diagonals:\n        If x is 2-D, a 1-D array of the same type as a\n        containing the diagonal is returned.\n        If the dimension of x is greater than two, then an\n        array of diagonals is returned, ""packed"" from left-most\n        dimension to right-most (e.g., if x is 3-D, then the\n        diagonals are ""packed"" along rows).\n\n    Raises\n    ------\n    ValueError\n        If the dimension of x is less than 2.\n\n\n    See Also\n    --------\n    numpy.diagonal:\n        https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.diagonal.html\n    """"""\n    __props__ = (""offset"", ""axis1"", ""axis2"", ""view"")\n\n    def __init__(self, offset=0, axis1=0, axis2=1, view=False):\n        self.view = view\n        if self.view and not numpy_diagonal_return_view:\n            warnings.warn(""View will forced to False. ExtractDiag property view is ""\n                          ""set to True but numpy version %s and prior versions of ""\n                          ""numpy.diagonal() do not return a view. Update ""\n                          ""numpy to use ExtractDiag(view=True)"" %\n                          np.version.version)\n            self.view = False\n        if self.view:\n            self.view_map = {0: [0]}\n        self.offset = offset\n        self.axis1 = axis1\n        self.axis2 = axis2\n\n    def make_node(self, x):\n        x = as_tensor_variable(x)\n\n        if x.ndim < 2:\n            raise ValueError(\'ExtractDiag needs an input with 2 or more \'\n                             \'dimensions\', x)\n        return Apply(self, [x], [x.type.__class__(\n            dtype=x.dtype,\n            broadcastable=[False] * (x.ndim - 1))()])\n\n    def perform(self, node, inputs, outputs):\n        (x,) = inputs\n        (z,) = outputs\n        z[0] = x.diagonal(self.offset, self.axis1, self.axis2)\n        if not self.view:\n            z[0] = z[0].copy()\n\n    def grad(self, inputs, gout):\n        (x,) = inputs\n        (gz,) = gout\n\n        if x.ndim == 2:\n            x = theano.tensor.zeros_like(x)\n            xdiag = theano.tensor.AllocDiag(offset=self.offset)(gz)\n            return [theano.tensor.set_subtensor(\n                x[:xdiag.shape[0], :xdiag.shape[1]], xdiag)]\n        else:\n            warnings.warn(""gradient of theano.tensor.basic.ExtractDiag only""\n                          ""works for matrices."")\n            return [grad_not_implemented(self, 0, x)]\n\n    def infer_shape(self, node, shapes):\n        in_shape, = shapes\n        dim1 = in_shape[self.axis1]\n        dim2 = in_shape[self.axis2]\n        out_shape = [d for i, d in enumerate(in_shape)\n                     if i not in (self.axis1, self.axis2)]\n        # The following logic is inspired by C code of PyArray_Diagonal().\n        offset = self.offset\n        if offset > 0:\n            diag_size = clip(dim2 - offset, 0, dim1)\n        elif offset < 0:\n            diag_size = clip(dim1 + offset, 0, dim2)\n        else:\n            diag_size = minimum(dim1, dim2)\n        out_shape.append(diag_size)\n        return [tuple(out_shape)]\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        if self.view and not numpy_diagonal_return_view:\n            warnings.warn(""View will forced to False. ExtractDiag property view is ""\n                          ""set to True but numpy version %s and prior versions of ""\n                          ""numpy.diagonal() do not return a view. Update ""\n                          ""numpy to use ExtractDiag(view=True)"" %\n                          np.version.version)\n            self.view = False\n\n        if self.view:\n            self.view_map = {0: [0]}\n\n        if ""offset"" not in state:\n            self.offset = 0\n        if ""axis1"" not in state:\n            self.axis1 = 0\n        if ""axis2"" not in state:\n            self.axis2 = 1\n\n\ndef diagonal(a, offset=0, axis1=0, axis2=1):\n    """"""\n    A helper function for `theano.tensor.ExtractDiag`. It accepts tensor with\n    `ndim >= 2` as input. The name `diagonal` is just meant to keep it\n    consistent with numpy.\n\n    Parameters\n    ----------\n    a : symbolic tensor\n    offset : int\n        offset\n    axis1 : int\n    axis2 : int\n\n    Returns\n    -------\n    tensor : symbolic tensor\n\n    """"""\n    return ExtractDiag(offset, axis1, axis2)(a)\n\n\nclass AllocDiag(Op):\n    """"""\n    An op that copies a vector to the diagonal of an empty matrix. It does the\n    inverse of ExtractDiag.\n\n    Usage: T.AllocDiag()(x)\n\n    `x` should be a tensor vector. The parenthesis in the front should indicate\n    which main diagonal the vector value goes into. By default it is set to\n    `0`, which corresponds to setting the values of x to the main diagonal in\n    the returned matrix.\n\n    Parameters\n    ----------\n    axis1: Axis to be used as the first axis of the 2-D\n        sub-arrays to which the diagonals will be allocated.\n        Defaults to first axis (0).\n\n    axis2: Axis to be used as the second axis of the 2-D\n        sub-arrays to which the diagonals will be allocated.\n        Defaults to second axis (1).\n\n    offset: Offset of the diagonal from the main diagonal defined by `axis1`\n        and `axis2`.\n        Can be positive or negative.\n        Defaults to main diagonal (0).\n\n    x: symbolic vector\n        A tensor vector consists of diagonal values.\n\n    Returns\n    -------\n    tensor : symbolic tenstor\n        A tensor with passed tensor values at their corresponding diagonals.\n\n    __props__ = (""offset"", ""axis1"", ""axis2"")\n\n    def __init__(self, offset=0, axis1=0, axis2=1):\n        self.offset = offset\n        self.axis1 = axis1\n        self.axis2 = axis2\n\n    def make_node(self, diag):\n        diag = as_tensor_variable(diag)\n        if diag.type.ndim < 1:\n            raise ValueError(\'AllocDiag needs an input with 1 or more \'\n                             \'dimensions\', diag.type)\n        return Apply(\n            self, [diag],\n            [diag.type.__class__(\n                dtype=diag.dtype,\n                broadcastable=[False] * (diag.ndim + 1))()]\n        )\n\n    def perform(self, node, inputs, outputs):\n        (x,) = inputs\n        (z,) = outputs\n\n        axis1 = np.minimum(self.axis1, self.axis2)\n        axis2 = np.maximum(self.axis1, self.axis2)\n        offset = self.offset\n\n        # Create array with one extra dimension for resulting matrix\n        result_shape = x.shape[:-1] + (x.shape[-1] + abs(offset),) * 2\n        result = np.zeros(result_shape, dtype=x.dtype)\n\n        # Create slice for diagonal in final 2 axes\n        idxs = np.arange(x.shape[-1])\n        diagonal_slice = ((len(result_shape) - 2) * [slice(None)] +\n                          [idxs + np.maximum(0, -offset),\n                           idxs + np.maximum(0, offset)])\n\n        # Fill in final 2 axes with x\n        result[tuple(diagonal_slice)] = x\n\n        if len(x.shape) > 1:\n            # Re-order axes so they correspond to diagonals at axis1, axis2\n            axes = list(range(len(x.shape[:-1])))\n            last_idx = axes[-1]\n            axes = axes[:axis1] + [last_idx + 1] + axes[axis1:]\n            axes = axes[:axis2] + [last_idx + 2] + axes[axis2:]\n            result = result.transpose(axes)\n\n        z[0] = result\n\n    def grad(self, inputs, gout):\n        (gz,) = gout\n        return [diagonal(\n            gz,\n            offset=self.offset,\n            axis1=self.axis1,\n            axis2=self.axis2\n        )]\n\n    def infer_shape(self, nodes, shapes):\n        (x_shape,) = shapes\n        axis1 = np.minimum(self.axis1, self.axis2)\n        axis2 = np.maximum(self.axis1, self.axis2)\n\n        result_shape = list(x_shape[:-1])\n        diag_shape = x_shape[-1] + abs(self.offset)\n        result_shape = result_shape[:axis1] + [diag_shape] + result_shape[axis1:]\n        result_shape = result_shape[:axis2] + [diag_shape] + result_shape[axis2:]\n        return [tuple(result_shape)]\n\n    def __setstate__(self, state):\n        if ""view_map"" in state:\n            del state[""view_map""]\n\n        self.__dict__.update(state)\n\n        if ""offset"" not in state:\n            self.offset = 0\n        if ""axis1"" not in state:\n            self.axis1 = 0\n        if ""axis2"" not in state:\n            self.axis2 = 1\n\n\ndef diag(v, k=0):\n    """"""\n    A helper function for two ops: `theano.tensor.ExtractDiag` and\n    `theano.tensor.AllocDiag`. The name `diag` is meant to keep it consistent\n    with numpy. It both accepts tensor vector and tensor matrix.\n    While the passed tensor variable `v` has `v.ndim>=2`, it builds a\n    `ExtractDiag` instance, and returns a vector with its entries equal to\n    `v`\'s main diagonal; otherwise if `v.ndim` is `1`, it builds an `AllocDiag`\n    instance, and returns a matrix with `v` at its k-th diaogonal.\n\n    Parameters\n    ----------\n    v : symbolic tensor\n    k : int\n        offset\n\n    Returns\n    -------\n    tensor : symbolic tensor\n\n    if v.ndim == 1:\n        return AllocDiag(k)(v)\n    elif v.ndim >= 2:\n        return diagonal(v, offset=k)\n    else:\n        raise ValueError(""Input must has v.ndim >= 1."")\n\n\ndef stacklists(arg):\n    """"""\n    Recursively stack lists of tensors to maintain similar structure.\n\n    This function can create a tensor from a shaped list of scalars:\n\n    Examples\n    --------\n    >>> from theano.tensor import stacklists, scalars, matrices\n    >>> from theano import function\n    >>> a, b, c, d = scalars(\'abcd\')\n    >>> X = stacklists([[a, b], [c, d]])\n    >>> f = function([a, b, c, d], X)\n    >>> f(1, 2, 3, 4)\n    array([[ 1.,  2.],\n           [ 3.,  4.]], dtype=float32)\n\n    We can also stack arbitrarily shaped tensors. Here we stack matrices into\n    a 2 by 2 grid:\n\n    >>> from numpy import ones\n    >>> a, b, c, d = matrices(\'abcd\')\n    >>> X = stacklists([[a, b], [c, d]])\n    >>> f = function([a, b, c, d], X)\n    >>> x = ones((4, 4), \'float32\')\n    >>> f(x, x, x, x).shape\n    (2, 2, 4, 4)\n\n    """"""\n    if isinstance(arg, (tuple, list)):\n        return stack(list(map(stacklists, arg)))\n    else:\n        return arg\n\n\ndef ptp(a, axis=None):\n    """"""\n    Range of values (maximum - minimum) along an axis.\n\n    The name of the function comes from the acronym for peak to peak.\n\n    Parameters\n    ----------\n    a\n        Input tensor.\n    axis\n        Axis along which to find the peaks. By default, flatten the array.\n\n    Returns\n    -------\n    array\n        A new array holding the result.\n\n    a = as_tensor_variable(a)\n\n    out = max(a, axis) - min(a, axis)\n\n\ndef power(x, y):\n    return x ** y\n\n\ndef swapaxes(y, axis1, axis2):\n    ""swap axes of inputted tensor""\n    y = as_tensor_variable(y)\n    ndim = y.ndim\n    li = list(range(0, ndim))\n    li[axis1], li[axis2] = li[axis2], li[axis1]\n    return y.dimshuffle(li)\n\n\ndef choose(a, choices, out=None, mode=\'raise\'):\n    """"""\n    Construct an array from an index array and a set of arrays to choose from.\n\n    First of all, if confused or uncertain, definitely look at the Examples -\n    in its full generality, this function is less simple than it might seem\n    from the following code description (below ndi = numpy.lib.index_tricks):\n\n    np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)]).\n\n    But this omits some subtleties. Here is a fully general summary:\n\n    Given an ``index`` array (a) of integers and a sequence of n arrays\n    (choices), a and each choice array are first broadcast, as necessary,\n    to arrays of a common shape; calling these Ba and\n    Bchoices[i], i = 0,...,n-1 we have that, necessarily,\n    Ba.shape == Bchoices[i].shape for each i.\n    Then, a new array with shape Ba.shape is created as follows:\n\n    - if mode=raise (the default), then, first of all, each element of a\n      (and thus Ba) must be in the range [0, n-1]; now, suppose that\n      i (in that range) is the value at the (j0, j1, ..., jm) position in Ba -\n      then the value at the same position in the new array is the value in\n      Bchoices[i] at that same position;\n\n    - if mode=wrap, values in a (and thus Ba) may be any (signed) integer;\n      modular arithmetic is used to map integers outside the range [0, n-1]\n      back into that range; and then the new array is constructed as above;\n\n    - if mode=clip, values in a (and thus Ba) may be any (signed) integer;\n      negative integers are mapped to 0; values greater than n-1 are mapped\n      to n-1; and then the new array is constructed as above.\n\n    Parameters\n    ----------\n    a : int array\n        This array must contain integers in [0, n-1], where n is the number of\n        choices, unless mode=wrap or mode=clip, in which cases any integers\n        are permissible.\n    choices : sequence of arrays\n        Choice arrays. a and all of the choices must be broadcastable to\n        the same shape. If choices is itself an array (not recommended),\n        then its outermost dimension (i.e., the one corresponding to\n        choices.shape[0]) is taken as defining the ``sequence``.\n    out : array, optional\n        If provided, the result will be inserted into this array.\n        It should be of the appropriate shape and dtype.\n    mode : {``raise`` (default), ``wrap``, ``clip``}, optional\n        Specifies how indices outside [0, n-1] will be treated:\n        ``raise`` : an exception is raised\n        ``wrap`` : value becomes value mod n\n        ``clip`` : values < 0 are mapped to 0, values > n-1 are mapped to n-1\n\n    Returns\n    -------\n    merged_array - array\n        The merged result.\n\n    Raises\n    ------\n    ValueError - shape mismatch\n        If a and each choice array are not all broadcastable to the same shape.\n\n    """"""\n    # This is done to keep the same function signature then NumPy.\n    assert out is None\n    return Choose(mode)(a, choices)\n\n\nclass Choose(Op):\n    __props__ = (\'mode\',)\n\n    def __init__(self, mode):\n        assert mode in (""raise"", ""wrap"", ""clip"")\n        self.mode = mode\n\n    def infer_shape(self, node, shapes):\n\n        if isinstance(node.inputs[1], TensorVariable):\n            # We have padded node.inputs[0] to the right number of\n            # dimensions for the output\n            l = []\n            for sh1, sh2, b1 in zip(shapes[0],\n                                    shapes[1][1:],\n                                    node.inputs[0].broadcastable):\n                if b1:\n                    l.append(sh2)\n                else:\n                    l.append(sh1)\n            return [tuple(l)]\n        else:\n            import theano.typed_list\n            assert isinstance(node.inputs[1],\n                              theano.typed_list.TypedListVariable)\n            raise ShapeError(""Case not implemented"")\n            shape = shapes[0]\n            for i in xrange(len(shapes[0]) - 1):\n                shape[i] = shapes[1][i]\n            return [(shape)]\n\n    def make_node(self, a, choices):\n        # Import here as it isn\'t imported by default and we can\'t\n        # import at the top as it would cause circular import.\n        import theano.typed_list\n        a = as_tensor_variable(a)\n        if a.dtype not in theano.tensor.discrete_dtypes:\n            raise TypeError(\n                \'choose first argument must have an [u]int* dtype. Got %s.\'\n                % a.dtype)\n\n        if isinstance(choices, (tuple, list,\n                                theano.typed_list.TypedListVariable)):\n            choice = theano.typed_list.make_list(choices)\n            choice_ndim = choice.ttype.ndim\n            choice_bcast = choice.ttype.broadcastable\n        else:\n            choice = as_tensor_variable(choices)\n            choice_ndim = choice.ndim - 1\n            choice_bcast = choice.broadcastable[1:]\n        out_ndim = np.max([a.ndim, choice_ndim])\n\n        # Make explicit all added broadcastable dimensions.\n        a = shape_padleft(a, out_ndim - a.ndim)\n        if len(choice_bcast) != out_ndim:\n            if isinstance(choice.type, TensorType):\n                choice = choice.dimshuffle(0,\n                                           *((\'x\',) * (out_ndim - choice_ndim) +\n                                             tuple(range(1, choice.ndim))))\n                choice_ndim = choice.ndim - 1\n                choice_bcast = choice.broadcastable[1:]\n            else:\n                raise NotImplementedError(\n                    ""We currently didn\'t implemented that case. ""\n                    ""To make it work, explicitly add dimensions ""\n                    ""of size one for dimensions that will be broadcasted"")\n\n        bcast = [False] * out_ndim\n        for idx, (b1, b2) in enumerate(\n            zip(a.broadcastable,\n                (True,) * (out_ndim - choice_ndim) + choice_bcast)):\n            if b1 and b2:\n                bcast[idx] = True\n        o = TensorType(choice.dtype, bcast)\n        return Apply(self, [a, choice], [o()])\n\n    def perform(self, node, inputs, outputs):\n        (z,) = outputs\n        a = inputs[0]\n        choice = inputs[1]\n        # TODO reuse out?\n        z[0] = np.choose(a, choice, mode=self.mode)\n\n\nclass AllocEmpty(gof.Op):\n    """"""Implement Alloc on the cpu, but without initializing memory.""""""\n\n    __props__ = (""dtype"", )\n    params_type = ParamsType(typecode=int32_t)\n\n    # specify the type of the data\n    def __init__(self, dtype):\n        assert isinstance(dtype, str), dtype\n        self.dtype = dtype.lower()\n\n    @property\n    def typecode(self):\n        return np.dtype(self.dtype).num\n\n    def make_node(self, *shape):\n        shape, bcast = alloc_validate_shape(shape)\n        otype = TensorType(dtype=self.dtype, broadcastable=bcast)\n        output = otype()\n\n        output.tag.values_eq_approx = values_eq_approx_always_true\n        # The outut can contain nan/inf.  output.type is a new\n        # instance, so we can do this only for that variable.\n        output.type.filter_checks_isfinite = False\n\n        # We can\'t reuse filter_checks_isfinite as by default it is\n        # False and it is set to true only in DebugMode.\n        # We can\'t set it in the type as other make_node can reuse the type.\n        # We can\'t set it in the variable as it isn\'t copied when we copy\n        # the variale. So we set it in the tag.\n        output.tag.nan_guard_mode_check = False\n        return Apply(self, shape, [output])\n\n    def debug_perform(self, node, inputs, out_, params):\n        self.perform(node, inputs, out_, params)\n        out_[0][0].fill(-123456789)\n\n    def perform(self, node, inputs, out_, params):\n        out, = out_\n        sh = tuple([int(i) for i in inputs])\n        if out[0] is None or out[0].shape != sh:\n            out[0] = np.empty(sh, dtype=self.dtype)\n\n    def c_code(self, node, name, inputs, out_, sub):\n        out, = out_\n        fail = sub[\'fail\']\n        shps = inputs\n        nd = len(shps)\n        params = sub[\'params\']\n        str = ""npy_intp dims[%(nd)s];\\n"" % locals()\n        for idx, sh in enumerate(shps):\n            str += ""dims[%(idx)s] ="" \\\n                   ""((npy_intp)((dtype_%(sh)s*)"" \\\n                   "" PyArray_DATA(%(sh)s))[0]);\\n"" % locals()\n\n        # Validate that the output storage exists\n        str += ""if(%(out)s==NULL\\n"" % locals()\n        for idx, sh in enumerate(shps):\n            str += ""||PyArray_DIMS(%(out)s)[%(idx)s]!=dims[%(idx)s]"" % locals()\n\n        str += """"""){\n            /* Reference received to invalid output variable.\n            Decrease received reference\'s ref count and allocate new\n            output variable */\n            Py_XDECREF(%(out)s);\n            %(out)s = (PyArrayObject*)PyArray_EMPTY(%(nd)s,\n                                                    dims,\n                                                    %(params)s->typecode,\n                                                    0);\n            if (!%(out)s)\n            {\n                PyErr_SetString(PyExc_MemoryError, ""alloc failed"");\n                %(fail)s;\n            }\n        }\n        """""" % locals()\n        return str\n\n    def infer_shape(self, node, input_shapes):\n        return [node.inputs]\n\n    def c_code_cache_version(self):\n        return (4,)\n\n    def do_constant_folding(self, node):\n        return False\n\n    def connection_pattern(self, node):\n        return [[False] for i in node.inputs]\n\n    def grad(self, inputs, grads):\n        return [DisconnectedType()() for i in inputs]\n\n    def R_op(self, inputs, eval_points):\n        return [zeros(inputs, self.dtype)]', metadata={'id': 'web-search_3', 'snippet': '""""""A `Type` and `Op` classes to work with numpy.ndarrays symbolically.""""""\nfrom __future__ import absolute_import, print_function, division\n\nfrom six.moves import builtins\nimport sys\nimport warnings\n\nimport numpy as np\nfrom six import integer_types\nfrom six.moves import xrange\nimport numbers\n\nimport theano\nfrom theano.compat import izip\nfrom theano import config\nfrom theano import gof\nfrom theano.gof import Apply, Constant, Op, Variable, ParamsType\nfrom theano.gof.type import Generic\n\nfrom theano.scalar import int32 as int32_t\nfrom theano.tensor import elemwise\nfrom theano.tensor.var import (AsTensorError, TensorVariable,\n                               TensorConstant, TensorConstantSignature,\n                               _tensor_py_operators)\nfrom theano.tensor.type import TensorType, values_eq_approx_always_true\nfrom theano.tensor.type_other import NoneConst\nfrom theano import scalar as scal\nfrom functools import partial\nfrom theano import compile, printing\nfrom theano.printing import pprint, min_informative_str\n# For history\nfrom theano.compile import Rebroadcast, Shape, shape\nfrom theano.scalar import int32\n\n\n# We use these exceptions as well.\nimport theano.scalar.sharedvar\nfrom theano.gradient import grad_undefined\nfrom theano.gradient import grad_not_implemented\nfrom theano.gradient import DisconnectedType\n\n# set up the external interface\nfrom theano.tensor.elemwise import Elemwise, DimShuffle, CAReduce, Sum\n\nimport logging\n_logger = logging.getLogger(""theano.tensor.basic"")\n\n__docformat__ = ""restructuredtext en""\n\n# This is needed as we will hide it later\npython_complex = complex\npython_any = any\npython_all = all\n\n# Define common subsets of dtypes (as strings).\ncomplex_dtypes = list(map(str, scal.complex_types))\ncontinuous_dtypes = list(map(str, scal.continuous_types))\nfloat_dtypes = list(map(str, scal.float_types))\ninteger_dtypes = list(map(str, scal.integer_types))\ndiscrete_dtypes = list(map(str, scal.discrete_types))\nall_dtypes = list(map(str, scal.all_types))\nint_dtypes = list(map(str, scal.int_types))\nuint_dtypes = list(map(str, scal.uint_types))\n\n\nclass ShapeError(Exception):\n    """"""Raised when the shape cannot be computed.""""""\n    pass\n\n\ndef check_equal_numpy(x, y):\n    """"""\n    Return True iff x and y are equal.\n\n    Checks the dtype and shape if x and y are numpy.ndarray instances.\n\n    """"""\n    if isinstance(x, np.ndarray) and isinstance(y, np.ndarray):\n        return (x.dtype == y.dtype and x.shape == y.shape and\n                np.all(abs(x - y) < 1e-10))\n    elif (isinstance(x, np.random.RandomState) and\n          isinstance(y, np.random.RandomState)):\n        return python_all(np.all(a == b) for a, b in\n                          izip(x.__getstate__(), y.__getstate__()))\n    else:\n        return x == y\n\ncompile.register_checker(check_equal_numpy)\n\n\n__oplist_constructor_list = []\n""""""List of functions to be listed as op constructors in the oplist\n(`gen_oplist`, doc/oplist.txt).""""""\n\n\ndef constructor(f):\n    """"""Add `f` to :doc:`oplist`.\n\n    Make `f` appear as a constructor in the oplist (`gen_oplist`,\n    doc/oplist.txt).\n\n    """"""\n    __oplist_constructor_list.append(f)\n    return f\n\n\ndef __oplist_tag(thing, tag):\n    tags = getattr(thing, \'__oplist_tags\', [])\n    tags.append(tag)\n    thing.__oplist_tags = tags\n\n\ndef as_tensor_variable(x, name=None, ndim=None):\n    """"""Return `x`, transformed into a `TensorType`.\n\n    This function is often used by `make_node` methods of `Op` subclasses\n    to turn ndarrays, numbers, `Scalar` instances, `Apply` instances and\n    `TensorType` instances into valid input list elements.\n\n    Parameters\n    ----------\n    x : Apply instance, Variable instance, numpy.ndarray, or number\n        This thing will be transformed into a `Variable` in a sensible way. An\n        ndarray argument will not be copied, but a list of numbers will be\n        copied to make an ndarray.\n    name : str or None\n        If a new `Variable` instance is created, it will be named with this\n        string.\n    ndim : None or integer\n        Return a Variable with this many dimensions.\n\n    Raises\n    ------\n    ValueError\n        If an `Apply` with more than one output is fetched or\n        if `x` cannot be made into a Variable with `ndim` dimensions.\n    AsTensorError\n        If `x` cannot be converted to a TensorType Variable.\n\n    """"""\n    if hasattr(x, \'_as_TensorVariable\'):\n        return x._as_TensorVariable()  # TODO: pass name and ndim arguments\n\n    if isinstance(x, gof.Apply):\n        # use Apply\'s default output mechanism\n        if (x.op.default_output is None) and (len(x.outputs) != 1):\n            raise ValueError(\n                ""It is ambiguous which output of a multi-output Op has""\n                "" to be fetched."", x)\n\n        x = x.default_output()\n    if isinstance(x, Variable):\n        if isinstance(x.type, scal.Scalar):\n            x = tensor_from_scalar(x)\n\n        if not isinstance(x.type, TensorType):\n            raise AsTensorError(\n                ""Variable type field must be a TensorType."", x, x.type)\n\n        if ndim is None:\n            return x\n        else:\n            if (x.type.ndim > ndim):\n                # strip off leading broadcastable dimensions\n                first_non_broadcastable = [idx for idx in xrange(x.ndim)\n                                           if not x.broadcastable[idx]][0]\n                x = x.dimshuffle(list(range(x.ndim))[first_non_broadcastable:])\n                if x.ndim > ndim:\n                    raise ValueError(\n                        \'TensorType could not be cast to have %i dimensions\'\n                        % ndim, x.type\n                    )\n                return x\n            elif (x.type.ndim < ndim):\n                return shape_padleft(x, n_ones=(ndim - x.type.ndim))\n            else:\n                return x\n    if isinstance(x, (tuple, list)) and python_any(isinstance(xi, Variable)\n                                                   for xi in x):\n        try:\n            return stack(x)\n        except (TypeError, ValueError):\n            pass\n\n    if isinstance(x, bool):\n        raise AsTensorError(\n            ""Cannot cast True or False as a tensor variable. Please use ""\n            ""np.array(True) or np.array(False) if you need these constants. ""\n            ""This error might be caused by using the == operator on ""\n            ""Variables. v == w does not do what you think it does, ""\n            ""use theano.tensor.eq(v, w) instead."")\n\n    try:\n        return constant(x, name=name, ndim=ndim)\n    except TypeError:\n        try:\n            str_x = str(x)\n        except Exception:\n            str_x = repr(x)\n        raise AsTensorError(""Cannot convert %s to TensorType"" % str_x, type(x))\n\n# this has a different name, because _as_tensor_variable is the\n# function which ops use to upcast their arguments... this\n# internal-use function is a good place to put debugging stuff, better\n# than the global astensor.\n_as_tensor_variable = as_tensor_variable\n\nas_tensor = as_tensor_variable\n\n\ndef constant(x, name=None, ndim=None, dtype=None):\n    """"""Return a symbolic `Constant` with value `x`.\n\n    Raises\n    ------\n    TypeError\n        `x` could not be converted to a numpy.ndarray.\n    ValueError\n        `x` could not be expanded to have ndim dimensions.\n\n    Note\n    ----\n    We create a small cache of frequently used constant.\n    This speed up the Merge optimization for big graph.\n    We want to cache all scalar to don\'t merge as frequently constants.\n    But we don\'t want to cache too much stuff.\n    So we cache integer with dtype [u]int and float where the value is\n    between -10 and 10.\n    We cache all broadcast pattern for scalar.\n\n    """"""\n    x_ = scal.convert(x, dtype=dtype)\n\n    bcastable = [d == 1 for d in x_.shape]\n    if ndim is not None:\n        if len(bcastable) < ndim:\n            bcastable = [True] * (ndim - len(bcastable)) + bcastable\n        elif len(bcastable) > ndim:\n            # TODO: strip off dimensions of size 1\n            raise ValueError(\n                \'ndarray could not be cast to constant with %i dimensions\' %\n                ndim)\n        assert len(bcastable) == ndim\n\n    try:\n        ttype = TensorType(dtype=x_.dtype, broadcastable=bcastable)\n        if not constant.enable:\n            return TensorConstant(ttype, x_, name=name)\n\n        sig = TensorConstantSignature((ttype, x_))\n        if sig in constant_cache:\n            return constant_cache[sig]\n\n        ret = TensorConstant(ttype, x_, name=name)\n        if (x_.size == 1 and\n            (-10) <= x_ <= 10 and\n            (x_.dtype in int_dtypes or x_.dtype in uint_dtypes or\n             (x_.dtype in float_dtypes and\n              # Limit the size of the cache.\n              len(constant_cache) < 10000))):\n            constant_cache[sig] = ret\n            # This is needed to raise a good error to the user.\n            ret.cached = True\n        return ret\n    except Exception:\n        raise TypeError(""Could not convert %s to TensorType"" % x, type(x))\n\n\nconstant.enable = True\nconstant_cache = {}\n\n\ndef _obj_is_wrappable_as_tensor(x):\n    try:\n        constant(x)\n        return True\n    except TypeError:\n        return False\n\n\nif int(config.tensor.cmp_sloppy) > 1:\n    # This config variable is a quick-and-dirty way to get low-precision\n    # comparisons.  For a more precise setting of these tolerances set\n    # them explicitly in your user code by assigning, for example,\n    # ""theano.tensor.basic.float32_atol = ...""\n\n    # When config.tensor.cmp_sloppy>1 we are even more sloppy. This is\n    # useful to test the GPU as they don\'t use extended precision and\n    # this cause some difference bigger then the normal sloppy.\n    float16_atol = 1e-2\n    float16_rtol = 5e-2\n\n    float32_atol = 5e-4\n    float32_rtol = 1e-3\n\n    float64_rtol = 1e-4\n    float64_atol = 1e-3\nelif int(config.tensor.cmp_sloppy):\n    float16_atol = 5e-3\n    float16_rtol = 1e-2\n\n    float32_atol = 1e-4\n    float32_rtol = 1e-3\n\n    float64_rtol = 1e-4\n    float64_atol = 1e-3\nelse:\n    # If you change those value in test don\'t forget to put them back\n    # when the test end.  Don\'t forget the case when the test fail.\n    float16_atol = 1e-3\n    float16_rtol = 1e-3\n\n    float32_atol = 1e-5\n    float32_rtol = 1e-5\n\n    # defaults in numpy.allclose\n    # Don\'t be more strict then numpy rtol\n    # It cause useless error.\n    float64_rtol = 1.0000000000000001e-05\n    float64_atol = 1e-8\n\n\ndef _get_atol_rtol(a, b):\n    tiny = (\'float16\',)\n    narrow = (\'float32\', \'complex64\')\n    if (str(a.dtype) in tiny) or (str(b.dtype) in tiny):\n        atol = float16_atol\n        rtol = float16_rtol\n    elif (str(a.dtype) in narrow) or (str(b.dtype) in narrow):\n        atol = float32_atol\n        rtol = float32_rtol\n    else:\n        atol = float64_atol\n        rtol = float64_rtol\n    return atol, rtol\n\n\ndef _allclose(a, b, rtol=None, atol=None):\n    a = np.asarray(a)\n    b = np.asarray(b)\n    atol_, rtol_ = _get_atol_rtol(a, b)\n    if rtol is not None:\n        rtol_ = rtol\n    if atol is not None:\n        atol_ = atol\n\n    return np.allclose(a, b, atol=atol_, rtol=rtol_)\n\n\nclass NotScalarConstantError(Exception):\n    """"""\n    Raised by get_scalar_constant_value if called on something that is\n    not a scalar constant.\n    """"""\n\n\nclass EmptyConstantError(NotScalarConstantError):\n    """"""\n    Raised by get_scalar_const_value if called on something that is a\n    zero dimensional constant.\n    """"""\n\n\ndef numpy_scalar(data):\n    """""" Return a scalar stored in a numpy ndarray.\n\n    Raises\n    ------\n     NotScalarConstantError\n        If the numpy ndarray is not a scalar.\n\n    # handle case where data is numpy.array([])\n    if (data.ndim > 0 and\n        (len(data.shape) == 0 or\n         builtins.max(data.shape) == 0)):\n        assert np.all(np.array([]) == data)\n        raise EmptyConstantError()\n    try:\n        np.complex(data)  # works for all numeric scalars\n        return data\n    except Exception:\n        raise NotScalarConstantError(\n            \'v.data is non-numeric, non-scalar, or has more than one\'\n            \' unique value\', data)\n\n\nget_scalar_constant_value_elemwises = (\n    scal.Cast, scal.Switch,\n    scal.NEQ, scal.EQ,\n    scal.LT, scal.GT, scal.LE, scal.GE,\n    scal.Sub, scal.Add, scal.Mod, scal.Mul,\n    scal.IntDiv, scal.TrueDiv, scal.Minimum, scal.Maximum)\n\n\ndef get_scalar_constant_value(orig_v, elemwise=True,\n                              only_process_constants=False,\n                              max_recur=10):\n    """"""Return the constant scalar(0-D) value underlying variable `v`.\n\n    If `v` is the output of dimshuffles, fills, allocs, rebroadcasts,\n    cast, OutputGuard, DeepCopyOp, ScalarFromTensor, ScalarOp, Elemwise\n    and some pattern with Subtensor, this function digs through them.\n\n    If `v` is not some view of constant scalar data, then raise a\n    NotScalarConstantError.\n\n    Parameters\n    ----------\n    elemwise : bool\n        If False, we won\'t try to go into elemwise. So this call is faster.\n        But we still investigate in Second Elemwise (as this is a substitute\n        for Alloc)\n    only_process_constants : bool\n        If True, we only attempt to obtain the value of `orig_v` if it\'s\n        directly constant and don\'t try to dig through dimshuffles, fills,\n        allocs, and other to figure out its value.\n    max_recur : int\n        The maximum number of recursion.\n\n    Notes\n    -----\n        There may be another function similar to this one in the code,\n        but I\'m not sure where it is.\n\n    """"""\n    v = orig_v\n    while True:\n        if v is None:\n            # None is not a scalar (and many uses of this function seem\n            # to depend on passing it None)\n            raise NotScalarConstantError()\n\n        if isinstance(v, (np.integer, integer_types, float)):\n            return np.asarray(v)\n\n        if isinstance(v, np.ndarray):\n            return numpy_scalar(v).copy()\n\n        if isinstance(v, Constant):\n            if getattr(v.tag, \'unique_value\', None) is not None:\n                data = v.tag.unique_value\n            else:\n                data = v.data\n            return numpy_scalar(data).copy()\n\n        if (not only_process_constants and\n                getattr(v, \'owner\', None) and\n                max_recur > 0):\n            max_recur -= 1\n            if isinstance(v.owner.op, (Alloc, DimShuffle, Rebroadcast,\n                                       # outputguard is only used in debugmode but we\n                                       # keep it here to avoid problems with old pickels.\n                                       compile.ops.OutputGuard,\n                                       compile.DeepCopyOp)):\n                v = v.owner.inputs[0]\n                continue\n            elif isinstance(v.owner.op, theano.compile.ops.Shape_i):\n                i = v.owner.op.i\n                inp = v.owner.inputs[0]\n                if isinstance(inp, Constant):\n                    return np.asarray(inp.data.shape[i])\n                # The shape of a broadcastable dimension is 1\n                if (hasattr(inp.type, \'broadcastable\') and\n                        inp.type.broadcastable[i]):\n                    return np.asarray(1)\n\n            # Don\'t act as the constant_folding optimization here as this\n            # fct is used too early in the optimization phase.  This would\n            # mess with the stabilization optimization and be too slow.\n            # We put all the scalar Ops used by get_canonical_form_slice()\n            # to allow it to determine the broadcast pattern correctly.\n            elif isinstance(v.owner.op, (ScalarFromTensor, TensorFromScalar)):\n                v = v.owner.inputs[0]\n                continue\n            elif isinstance(v.owner.op, theano.tensor.opt.Assert):\n                # check if all conditions are constant and true\n                cond = [get_scalar_constant_value(c, max_recur=max_recur)\n                        for c in v.owner.inputs[1:]]\n                if builtins.all([0 == c.ndim and c != 0 for c in cond]):\n                    v = v.owner.inputs[0]\n                    continue\n            elif isinstance(v.owner.op, scal.ScalarOp):\n                if isinstance(v.owner.op, scal.Second):\n                    # We don\'t need both input to be constant for second\n                    shp, val = v.owner.inputs\n                    v = val\n                    continue\n                if isinstance(v.owner.op, get_scalar_constant_value_elemwises):\n                    const = [get_scalar_constant_value(i, max_recur=max_recur)\n                             for i in v.owner.inputs]\n                    ret = [[None]]\n                    v.owner.op.perform(v.owner, const, ret)\n                    return ret[0][0].copy()\n            # In fast_compile, we don\'t enable local_fill_to_alloc, so\n            # we need to investigate Second as Alloc. So elemwise\n            # don\'t disable the check for Second.\n            elif isinstance(v.owner.op, Elemwise):\n                if isinstance(v.owner.op.scalar_op, scal.Second):\n                    # We don\'t need both input to be constant for second\n                    shp, val = v.owner.inputs\n                    v = val\n                    continue\n                elif elemwise and isinstance(\n                        v.owner.op.scalar_op,\n                        get_scalar_constant_value_elemwises):\n                    const = [get_scalar_constant_value(i, max_recur=max_recur)\n                             for i in v.owner.inputs]\n                    ret = [[None]]\n                    v.owner.op.perform(v.owner, const, ret)\n                    return ret[0][0].copy()\n            elif (isinstance(v.owner.op, theano.tensor.subtensor.Subtensor) and\n                  v.ndim == 0):\n                if isinstance(v.owner.inputs[0], TensorConstant):\n                    cdata = tuple(v.owner.op.get_constant_idx(v.owner.inputs))\n                    try:\n                        return v.owner.inputs[0].data.__getitem__(cdata).copy()\n                    except IndexError:\n                        raise IndexError(\n                            str(tuple(v.owner.op.idx_list)) +\n                            "" is not a valid index into "" +\n                            str(v.owner.inputs[0].data))\n\n                # The index list \'idx_list\' should have length the same\n                # shape as the input.\n                # TODO: implement the case where we take a scalar in a matrix\n                assert len(v.owner.op.idx_list) == v.owner.inputs[0].ndim\n\n                # Needed to make better graph in this test in\n                # theano/tensor/tests/test_sharedvar.py:\n                # test_shared_options.test_specify_shape_partial\n                if ((v.owner.inputs[0].owner and\n                     isinstance(v.owner.inputs[0].owner.op, Join) and\n                     len(v.owner.op.idx_list) == 1)):\n                    # Ensure the Join is joining only scalar variables (so that\n                    # the constant value can be found at the same index as the\n                    # one used in the sub-tensor).\n                    if python_all(var.ndim == 0 for var in\n                                  v.owner.inputs[0].owner.inputs[1:]):\n                        idx = v.owner.op.idx_list[0]\n                        if isinstance(idx, gof.Type):\n                            idx = get_scalar_constant_value(v.owner.inputs[1],\n                                                            max_recur=max_recur)\n                        # Note the \'+ 1\' is because the first argument to Join\n                        # is the axis.\n                        ret = v.owner.inputs[0].owner.inputs[idx + 1]\n                        ret = get_scalar_constant_value(ret, max_recur=max_recur)\n                        # join can cast implicitly its input in some case.\n                        return theano._asarray(ret, dtype=v.type.dtype)\n                    if python_all(var.ndim == 1 for var in\n                                  v.owner.inputs[0].owner.inputs[1:]):\n                        idx = v.owner.op.idx_list[0]\n                        if isinstance(idx, gof.Type):\n                            idx = get_scalar_constant_value(v.owner.inputs[1],\n                                                            max_recur=max_recur)\n                        try:\n                            # TODO: assert joined axis is 0.\n                            length = 0\n                            loop = False\n                            for joined in v.owner.inputs[0].owner.inputs[1:]:\n                                ll = get_vector_length(joined)\n                                if idx < length + ll:\n                                    v = joined[idx - length]\n                                    loop = True\n                                    break\n                                length += ll\n                            if loop:\n                                continue\n                        except TypeError:\n                            pass\n                        except ValueError:\n                            pass\n\n                elif (v.owner.inputs[0].owner and\n                      isinstance(v.owner.inputs[0].owner.op,\n                                 theano.tensor.opt.MakeVector) and\n                      # MakeVector normally accept only scalar as input.\n                      # We put this check in case there is change in the future\n                      python_all(var.ndim == 0 for var in\n                                 v.owner.inputs[0].owner.inputs) and\n                      len(v.owner.op.idx_list) == 1):\n\n                    idx = v.owner.op.idx_list[0]\n                    if isinstance(idx, gof.Type):\n                        idx = get_scalar_constant_value(v.owner.inputs[1],\n                                                        max_recur=max_recur)\n                    # Python 2.4 does not support indexing with numpy.integer\n                    # So we cast it.\n                    idx = int(idx)\n                    ret = v.owner.inputs[0].owner.inputs[idx]\n                    ret = get_scalar_constant_value(ret, max_recur=max_recur)\n                    # MakeVector can cast implicitly its input in some case.\n                    return theano._asarray(ret, dtype=v.type.dtype)\n\n                # This is needed when we take the grad as the Shape op\n                # are not already changed into MakeVector\n                owner = v.owner\n                leftmost_parent = owner.inputs[0]\n                if (leftmost_parent.owner and\n                    isinstance(leftmost_parent.owner.op,\n                               theano.tensor.Shape)):\n                    op = owner.op\n                    idx_list = op.idx_list\n                    idx = idx_list[0]\n                    if isinstance(idx, gof.Type):\n                        idx = get_scalar_constant_value(owner.inputs[1],\n                                                        max_recur=max_recur)\n                    grandparent = leftmost_parent.owner.inputs[0]\n                    gp_broadcastable = grandparent.type.broadcastable\n                    ndim = grandparent.type.ndim\n                    if grandparent.owner and isinstance(grandparent.owner.op,\n                                                        Rebroadcast):\n                        ggp_broadcastable = grandparent.owner.inputs[0].broadcastable\n                        l = [b1 or b2 for b1, b2 in zip(ggp_broadcastable,\n                                                        gp_broadcastable)]\n                        gp_broadcastable = tuple(l)\n\n                    assert ndim == len(gp_broadcastable)\n\n                    if not (idx < len(gp_broadcastable)):\n                        msg = (""get_scalar_constant_value detected "" +\n                               ""deterministic IndexError: x.shape[%d] "" +\n                               ""when x.ndim=%d."") % (idx, ndim)\n                        if config.exception_verbosity == \'high\':\n                            msg += \' x=%s\' % min_informative_str(v)\n                        else:\n                            msg += \' x=%s\' % str(v)\n                        raise ValueError(msg)\n\n                    if gp_broadcastable[idx]:\n                        return np.asarray(1)\n\n        raise NotScalarConstantError(v)\n\n\n# Easy constructors\n\ndef tensor(*args, **kwargs):\n    name = kwargs.pop(\'name\', None)\n    return TensorType(*args, **kwargs)(name=name)\n\n\ndef _multi(*fns):\n    def f2(f, *names):\n        if names and isinstance(names[0], integer_types):\n            if names == 1:\n                return f()\n            else:\n                return [f() for i in xrange(names[0])]\n        if isinstance(names, tuple):\n            if len(names) == 1:\n                names = names[0]\n        if len(names) == 1:\n            return f(names)\n        else:\n            return [f(name) for name in names]\n    if len(fns) == 1:\n        return partial(f2, fns)\n    else:\n        return [partial(f2, f) for f in fns]\n\ncscalar = TensorType(\'complex64\', ())\nzscalar = TensorType(\'complex128\', ())\nfscalar = TensorType(\'float32\', ())\ndscalar = TensorType(\'float64\', ())\nbscalar = TensorType(\'int8\', ())\nwscalar = TensorType(\'int16\', ())\niscalar = TensorType(\'int32\', ())\nlscalar = TensorType(\'int64\', ())\n\n\ndef scalar(name=None, dtype=None):\n    """"""Return a symbolic scalar variable.\n\n    Parameters\n    ----------\n    dtype: numeric\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, ())\n    return type(name)\n\nscalars, fscalars, dscalars, iscalars, lscalars = _multi(\n    scalar, fscalar, dscalar, iscalar, lscalar)\n\nint_types = bscalar, wscalar, iscalar, lscalar\nfloat_types = fscalar, dscalar\ncomplex_types = cscalar, zscalar\nint_scalar_types = int_types\nfloat_scalar_types = float_types\ncomplex_scalar_types = complex_types\n\ncvector = TensorType(\'complex64\', (False, ))\nzvector = TensorType(\'complex128\', (False, ))\nfvector = TensorType(\'float32\', (False, ))\ndvector = TensorType(\'float64\', (False, ))\nbvector = TensorType(\'int8\', (False,))\nwvector = TensorType(\'int16\', (False,))\nivector = TensorType(\'int32\', (False, ))\nlvector = TensorType(\'int64\', (False, ))\n\n\ndef vector(name=None, dtype=None):\n    """"""Return a symbolic vector variable.\n\n    Parameters\n    ----------\n    dtype: numeric\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, ))\n    return type(name)\n\nvectors, fvectors, dvectors, ivectors, lvectors = _multi(\n    vector, fvector, dvector, ivector, lvector)\n\nint_vector_types = bvector, wvector, ivector, lvector\nfloat_vector_types = fvector, dvector\ncomplex_vector_types = cvector, zvector\n\ncmatrix = TensorType(\'complex64\', (False, False))\nzmatrix = TensorType(\'complex128\', (False, False))\nfmatrix = TensorType(\'float32\', (False, False))\ndmatrix = TensorType(\'float64\', (False, False))\nbmatrix = TensorType(\'int8\', (False, False))\nwmatrix = TensorType(\'int16\', (False, False))\nimatrix = TensorType(\'int32\', (False, False))\nlmatrix = TensorType(\'int64\', (False, False))\n\n\ndef matrix(name=None, dtype=None):\n    """"""Return a symbolic matrix variable.\n\n    Parameters\n    ----------\n    dtype: numeric\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, False))\n    return type(name)\n\nmatrices, fmatrices, dmatrices, imatrices, lmatrices = _multi(\n    matrix, fmatrix, dmatrix, imatrix, lmatrix)\n\nint_matrix_types = bmatrix, wmatrix, imatrix, lmatrix\nfloat_matrix_types = fmatrix, dmatrix\ncomplex_matrix_types = cmatrix, zmatrix\n\ncrow = TensorType(\'complex64\', (True, False))\nzrow = TensorType(\'complex128\', (True, False))\nfrow = TensorType(\'float32\', (True, False))\ndrow = TensorType(\'float64\', (True, False))\nbrow = TensorType(\'int8\', (True, False))\nwrow = TensorType(\'int16\', (True, False))\nirow = TensorType(\'int32\', (True, False))\nlrow = TensorType(\'int64\', (True, False))\n\n\ndef row(name=None, dtype=None):\n    """"""Return a symbolic row variable (ndim=2, broadcastable=[True,False]).\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (True, False))\n    return type(name)\nrows, frows, drows, irows, lrows = _multi(row, frow, drow, irow, lrow)\n\nccol = TensorType(\'complex64\', (False, True))\nzcol = TensorType(\'complex128\', (False, True))\nfcol = TensorType(\'float32\', (False, True))\ndcol = TensorType(\'float64\', (False, True))\nbcol = TensorType(\'int8\', (False, True))\nwcol = TensorType(\'int16\', (False, True))\nicol = TensorType(\'int32\', (False, True))\nlcol = TensorType(\'int64\', (False, True))\n\n\ndef col(name=None, dtype=None):\n    """"""Return a symbolic column variable (ndim=2, broadcastable=[False,True]).\n\n    Parameters\n    ----------\n    dtype : numeric\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, True))\n    return type(name)\ncols, fcols, dcols, icols, lcols = _multi(col, fcol, dcol, icol, lcol)\n\nctensor3 = TensorType(\'complex64\', ((False,) * 3))\nztensor3 = TensorType(\'complex128\', ((False,) * 3))\nftensor3 = TensorType(\'float32\', ((False,) * 3))\ndtensor3 = TensorType(\'float64\', ((False,) * 3))\nbtensor3 = TensorType(\'int8\', ((False,) * 3))\nwtensor3 = TensorType(\'int16\', ((False,) * 3))\nitensor3 = TensorType(\'int32\', ((False,) * 3))\nltensor3 = TensorType(\'int64\', ((False,) * 3))\n\n\ndef tensor3(name=None, dtype=None):\n    """"""Return a symbolic 3-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, False, False))\n    return type(name)\n\ntensor3s, ftensor3s, dtensor3s, itensor3s, ltensor3s = _multi(\n    tensor3, ftensor3, dtensor3, itensor3, ltensor3)\n\nctensor4 = TensorType(\'complex64\', ((False,) * 4))\nztensor4 = TensorType(\'complex128\', ((False,) * 4))\nftensor4 = TensorType(\'float32\', ((False,) * 4))\ndtensor4 = TensorType(\'float64\', ((False,) * 4))\nbtensor4 = TensorType(\'int8\', ((False,) * 4))\nwtensor4 = TensorType(\'int16\', ((False,) * 4))\nitensor4 = TensorType(\'int32\', ((False,) * 4))\nltensor4 = TensorType(\'int64\', ((False,) * 4))\n\n\ndef tensor4(name=None, dtype=None):\n    """"""Return a symbolic 4-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, False, False, False))\n    return type(name)\ntensor4s, ftensor4s, dtensor4s, itensor4s, ltensor4s = _multi(\n    tensor4, ftensor4, dtensor4, itensor4, ltensor4)\n\nctensor5 = TensorType(\'complex64\', ((False,) * 5))\nztensor5 = TensorType(\'complex128\', ((False,) * 5))\nftensor5 = TensorType(\'float32\', ((False,) * 5))\ndtensor5 = TensorType(\'float64\', ((False,) * 5))\nbtensor5 = TensorType(\'int8\', ((False,) * 5))\nwtensor5 = TensorType(\'int16\', ((False,) * 5))\nitensor5 = TensorType(\'int32\', ((False,) * 5))\nltensor5 = TensorType(\'int64\', ((False,) * 5))\n\n\ndef tensor5(name=None, dtype=None):\n    """"""Return a symbolic 5-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False, False, False, False, False))\n    return type(name)\ntensor5s, ftensor5s, dtensor5s, itensor5s, ltensor5s = _multi(\n    tensor5, ftensor5, dtensor5, itensor5, ltensor5)\n\nctensor6 = TensorType(\'complex64\', ((False,) * 6))\nztensor6 = TensorType(\'complex128\', ((False,) * 6))\nftensor6 = TensorType(\'float32\', ((False,) * 6))\ndtensor6 = TensorType(\'float64\', ((False,) * 6))\nbtensor6 = TensorType(\'int8\', ((False,) * 6))\nwtensor6 = TensorType(\'int16\', ((False,) * 6))\nitensor6 = TensorType(\'int32\', ((False,) * 6))\nltensor6 = TensorType(\'int64\', ((False,) * 6))\n\n\ndef tensor6(name=None, dtype=None):\n    """"""Return a symbolic 6-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False,) * 6)\n    return type(name)\ntensor6s, ftensor6s, dtensor6s, itensor6s, ltensor6s = _multi(\n    tensor6, ftensor6, dtensor6, itensor6, ltensor6)\n\nctensor7 = TensorType(\'complex64\', ((False,) * 7))\nztensor7 = TensorType(\'complex128\', ((False,) * 7))\nftensor7 = TensorType(\'float32\', ((False,) * 7))\ndtensor7 = TensorType(\'float64\', ((False,) * 7))\nbtensor7 = TensorType(\'int8\', ((False,) * 7))\nwtensor7 = TensorType(\'int16\', ((False,) * 7))\nitensor7 = TensorType(\'int32\', ((False,) * 7))\nltensor7 = TensorType(\'int64\', ((False,) * 7))\n\n\ndef tensor7(name=None, dtype=None):\n    """"""Return a symbolic 7-D variable.\n\n    Parameters\n    ----------\n    dtype: numeric type\n        None means to use theano.config.floatX.\n    name\n        A name to attach to this variable.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    type = TensorType(dtype, (False,) * 7)\n    return type(name)\ntensor7s, ftensor7s, dtensor7s, itensor7s, ltensor7s = _multi(\n    tensor7, ftensor7, dtensor7, itensor7, ltensor7)\n\n\nTensor = TensorType\n\n\n# This bizarre push-import avoids a circular dependency.\nelemwise.as_tensor_variable = as_tensor_variable\nelemwise.TensorType = TensorType\nelemwise.TensorVariable = TensorVariable\nelemwise.TensorConstant = TensorConstant\n\n#########################\n# Utilities\n#########################\n\n\ndef _scal_elemwise_with_nfunc(nfunc, nin, nout):\n    """"""\n    Replace a symbol definition with an elementwise version of the\n    corresponding scalar Op.  If it is not None, the nfunc argument\n    should be a string such that getattr(numpy, nfunc) implements\n    a vectorized version of the elemwise operation. nin is the number\n    of inputs expected by that function, and nout is the number of\n    **destination** inputs it takes. That is, the function should\n    take nin+nout inputs. nout == 0 means that the numpy function\n    does not take a numpy array argument to put its result in.\n\n    """"""\n    def construct(symbol):\n        symbolname = symbol.__name__\n        inplace = symbolname.endswith(\'_inplace\')\n        if inplace:\n            msg = ""inplace""\n        else:\n            msg = ""no_inplace""\n\n        n = ""Elemwise{%s,%s}"" % (symbolname, msg)\n\n        if inplace:\n            scalar_op = getattr(scal, symbolname[:-len(\'_inplace\')])\n            inplace_scalar_op = scalar_op.__class__(scal.transfer_type(0))\n            rval = elemwise.Elemwise(inplace_scalar_op, {0: 0}, name=n,\n                                     nfunc_spec=(nfunc and (nfunc, nin, nout)))\n        else:\n            scalar_op = getattr(scal, symbolname)\n            rval = elemwise.Elemwise(scalar_op, name=n,\n                                     nfunc_spec=(nfunc and (nfunc, nin, nout)))\n\n        if getattr(symbol, \'__doc__\', False):\n            rval.__doc__ = symbol.__doc__ + \'\\n\' + rval.__doc__\n\n        # for the meaning of this see the ./epydoc script\n        # it makes epydoc display rval as if it were a function, not an object\n        rval.__epydoc_asRoutine = symbol\n        rval.__module__ = \'tensor\'\n\n        pprint.assign(rval, printing.FunctionPrinter(symbolname))\n\n        return rval\n    return construct\n\n_scal_elemwise = _scal_elemwise_with_nfunc(None, None, None)\n\n\ndef _pack(x):\n    """"""\n    Convert x to a list if it is an iterable, otherwise wrap it in a list.\n    """"""\n    try:\n        return list(x)\n    except TypeError:\n        return [x]\n\n\ndef check_and_normalize_axes(x, axis):\n    """"""\n    Check axes, normalize and convert them to a Python list of integers.\n    Return an empty list if argument is None.\n\n    Parameters\n    ----------\n    x: Tensor variable\n    axis = Integer, tuple or list of integers\n\n    Returns\n    -------\n    axis: list of integers\n    """"""\n    x = as_tensor_variable(x)\n    if axis is None:\n        axis = []\n    elif (isinstance(axis, (integer_types, np.integer)) or\n            (isinstance(axis, np.ndarray) and axis.ndim == 0)):\n                axis = [int(axis)]\n    elif isinstance(axis, (tuple, list, np.ndarray)):\n        axis = [int(i) for i in axis]\n    elif isinstance(axis, Variable):\n        if NoneConst.equals(axis):\n            axis = []\n        elif not isinstance(axis, TensorConstant):\n            raise TypeError(""Computation needs a constant axis. Got %s"" % axis)\n        else:\n            assert axis.dtype in integer_dtypes\n            if (isinstance(axis.data, (integer_types, np.integer)) or\n                    (isinstance(axis.data, np.ndarray) and axis.data.ndim == 0)):\n                        axis = [int(axis.data)]\n            elif isinstance(axis.data, (list, np.ndarray)):\n                axis = [int(i) for i in axis.data]\n    else:\n        raise TypeError(""Axis must be an integer, tuple, list of integers or a TensorVariable. Got %s"" % axis)\n    if len(axis) > 0:\n        for i in range(len(axis)):\n            if axis[i] < 0:\n                axis[i] += x.type.ndim\n            if axis[i] < 0 or axis[i] >= x.type.ndim:\n                raise ValueError(""Computation needs a valid axis number for %d-D tensor. Got %d"" % (x.type.ndim, axis[i]))\n        axis = list(set(axis))\n        axis.sort()\n    return axis\n\n\n#########################\n# Casting Operations\n#########################\n\nclass TensorFromScalar(Op):\n\n    def make_node(self, s):\n        assert isinstance(s.type, scal.Scalar)\n        return Apply(self,\n                     [s],\n                     [tensor(dtype=s.type.dtype,\n                             broadcastable=())])\n\n    def perform(self, node, inp, out_):\n        s, = inp\n        out, = out_\n        out[0] = np.asarray(s)\n\n    def infer_shape(self, node, in_shapes):\n        return [()]\n\n    def grad(self, inp, grads):\n        s, = inp\n        dt, = grads\n        if s.type.dtype in float_dtypes:\n            assert dt.type.dtype in float_dtypes\n            return [scalar_from_tensor(dt)]\n\n        # If the input dtype is an integer, then so is the output dtype,\n        # and the ""zero"" gradient can be represented in that int dtype.\n        # Currently, theano.grad insists that the dtype of the returned\n        # gradient has a float dtype, so we use floatX.\n        if s.type.dtype in discrete_dtypes:\n            return [s.zeros_like().astype(theano.config.floatX)]\n\n        raise NotImplementedError(""grad not implemented for complex dtypes"")\n\ntensor_from_scalar = TensorFromScalar()\n\n\nclass ScalarFromTensor(Op):\n\n    def make_node(self, t):\n        assert isinstance(t.type, TensorType)\n        assert t.type.broadcastable == ()\n        return Apply(self,\n                     [t],\n                     [scal.get_scalar_type(dtype=t.type.dtype).make_variable()]\n                     )\n\n    def perform(self, node, inp, out_):\n        s, = inp\n        out, = out_\n        out[0] = s.flatten()[0]\n\n    def infer_shape(self, node, in_shapes):\n        return [()]\n\n    def grad(self, inp, grads):\n        s, = inp\n        dt, = grads\n        return [tensor_from_scalar(dt)]\n\n    def R_op(self, inputs, eval_points):\n        if None in eval_points:\n            return [None]\n        return self.make_node(*eval_points).outputs\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        x, = inputs\n        z, = outputs\n        fail = sub[\'fail\']\n        return """"""\n        %(z)s = ((dtype_%(x)s*)(PyArray_DATA(%(x)s)))[0];\n        """""" % locals()\n\n    def c_code_cache_version(self):\n        return (1,)\n\nscalar_from_tensor = ScalarFromTensor()\n\n\n# to be removed as we get the epydoc routine-documenting thing going\n# -JB 20080924\ndef _conversion(real_value, name):\n    __oplist_tag(real_value, \'casting\')\n    real_value.__module__ = \'tensor.basic\'\n    pprint.assign(real_value, printing.FunctionPrinter(name))\n    return real_value\n\n\n# These _conver_to_<type> functions have leading underscores to indicate that\n# they should not be called directly.  They do not perform sanity checks about\n# what types you are casting to what.  That logic is implemented by the\n# `cast()` function below.\n\n_convert_to_bool = _conversion(\n    elemwise.Elemwise(scal.convert_to_bool), \'bool\')\n""""""Cast to boolean""""""\n\n_convert_to_int8 = _conversion(\n    elemwise.Elemwise(scal.convert_to_int8), \'int8\')\n""""""Cast to 8-bit integer""""""\n\n_convert_to_int16 = _conversion(\n    elemwise.Elemwise(scal.convert_to_int16), \'int16\')\n""""""Cast to 16-bit integer""""""\n\n_convert_to_int32 = _conversion(\n    elemwise.Elemwise(scal.convert_to_int32), \'int32\')\n""""""Cast to 32-bit integer""""""\n\n_convert_to_int64 = _conversion(\n    elemwise.Elemwise(scal.convert_to_int64), \'int64\')\n""""""Cast to 64-bit integer""""""\n\n_convert_to_uint8 = _conversion(\n    elemwise.Elemwise(scal.convert_to_uint8), \'uint8\')\n""""""Cast to unsigned 8-bit integer""""""\n\n_convert_to_uint16 = _conversion(\n    elemwise.Elemwise(scal.convert_to_uint16), \'uint16\')\n""""""Cast to unsigned 16-bit integer""""""\n\n_convert_to_uint32 = _conversion(\n    elemwise.Elemwise(scal.convert_to_uint32), \'uint32\')\n""""""Cast to unsigned 32-bit integer""""""\n\n_convert_to_uint64 = _conversion(\n    elemwise.Elemwise(scal.convert_to_uint64), \'uint64\')\n""""""Cast to unsigned 64-bit integer""""""\n\n_convert_to_float16 = _conversion(\n    elemwise.Elemwise(scal.convert_to_float16), \'float16\')\n""""""Cast to half-precision floating point""""""\n\n_convert_to_float32 = _conversion(\n    elemwise.Elemwise(scal.convert_to_float32), \'float32\')\n""""""Cast to single-precision floating point""""""\n\n_convert_to_float64 = _conversion(\n    elemwise.Elemwise(scal.convert_to_float64), \'float64\')\n""""""Cast to double-precision floating point""""""\n\n_convert_to_complex64 = _conversion(\n    elemwise.Elemwise(scal.convert_to_complex64), \'complex64\')\n""""""Cast to single-precision complex""""""\n\n_convert_to_complex128 = _conversion(\n    elemwise.Elemwise(scal.convert_to_complex128), \'complex128\')\n""""""Cast to double-precision complex""""""\n\n_cast_mapping = {\n    \'bool\': _convert_to_bool,\n    \'int8\': _convert_to_int8,\n    \'int16\': _convert_to_int16,\n    \'int32\': _convert_to_int32,\n    \'int64\': _convert_to_int64,\n    \'uint8\': _convert_to_uint8,\n    \'uint16\': _convert_to_uint16,\n    \'uint32\': _convert_to_uint32,\n    \'uint64\': _convert_to_uint64,\n    \'float16\': _convert_to_float16,\n    \'float32\': _convert_to_float32,\n    \'float64\': _convert_to_float64,\n    \'complex64\': _convert_to_complex64,\n    \'complex128\': _convert_to_complex128}\n\n\n@constructor\ndef cast(x, dtype):\n    """"""Symbolically cast `x` to a Tensor of type `dtype`.""""""\n    if dtype == \'floatX\':\n        dtype = config.floatX\n\n    _x = as_tensor_variable(x)\n    if _x.type.dtype == dtype:\n        return _x\n    if _x.type.dtype.startswith(\'complex\') and not dtype.startswith(\'complex\'):\n        raise TypeError((\n            \'Casting from complex to real is ambiguous: consider real(), \'\n            \'imag(), angle() or abs()\'))\n    return _cast_mapping[dtype](x)\n\n##########################\n# Unary Operations\n##########################\n\n\nclass MaxAndArgmax(Op):\n    """"""\n    Calculate the max and argmax over a given axis or over all axes.\n\n    """"""\n    nin = 2  # tensor, axis\n    nout = 2  # max val, max idx\n    E_axis = \'invalid axis\'\n    params_type = Generic()\n    __props__ = (\'axis\',)\n    _f16_ok = True\n\n    def __init__(self, axis):\n        assert isinstance(axis, list)\n        self.axis = tuple(axis)\n\n    def get_params(self, node):\n        return self.axis\n\n    def make_node(self, x):\n        x = _as_tensor_variable(x)\n\n        # We keep the original broadcastable flags for dimensions on which\n        # we do not perform the max / argmax.\n        all_axes = set(self.axis)\n        broadcastable = [b for i, b in enumerate(x.type.broadcastable)\n                         if i not in all_axes]\n        inputs = [x]\n        outputs = [tensor(x.type.dtype, broadcastable, name=\'max\'),\n                   tensor(\'int64\', broadcastable, name=\'argmax\')]\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inp, outs, params):\n        x = inp[0]\n        axes = params\n        max, max_idx = outs\n        if axes is None:\n            axes = tuple(range(x.ndim))\n        else:\n            axes = tuple(int(ax) for ax in axes)\n        max[0] = theano._asarray(np.max(x, axes),\n                                 dtype=node.outputs[0].dtype)\n        # Numpy does not support multiple axes for argmax\n        # Work around\n        keep_axes = np.array([i for i in range(x.ndim) if i not in axes],\n                             dtype=\'int64\')\n        # Not-reduced axes in front\n        transposed_x = np.transpose(x, np.concatenate((keep_axes, axes)))\n        kept_shape = transposed_x.shape[:len(keep_axes)]\n        reduced_shape = transposed_x.shape[len(keep_axes):]\n\n        # Numpy.prod returns 1.0 when arg is empty, so we cast it to int64\n        # Otherwise reshape would complain citing float arg\n        new_shape = kept_shape + (np.prod(reduced_shape, dtype=\'int64\'),)\n        reshaped_x = transposed_x.reshape(new_shape)\n\n        max_idx[0] = theano._asarray(np.argmax(reshaped_x, axis=-1),\n                                     dtype=\'int64\')\n\n    def c_code(self, node, name, inp, out, sub):\n        if len(self.axis) != 1 and len(self.axis) != node.inputs[0].ndim:\n            raise NotImplementedError(""NumPy C-API can compute max and argmax only for 1 axis or for all axes."")\n        x = inp[0]\n        axis = sub[\'params\']\n        max, argmax = out\n        fail = sub[""fail""]\n        ret = """"""\n        #if PY_MAJOR_VERSION >= 3\n            #ifndef PyInt_AS_LONG\n                #define PyInt_AS_LONG PyLong_AS_LONG\n            #endif\n        #endif\n\n        if (PyTuple_GET_SIZE(%(axis)s) == PyArray_NDIM(%(x)s)) {\n            axis = NPY_MAXDIMS;\n        } else if(PyTuple_GET_SIZE(%(axis)s) == 1) {\n            PyObject* axis_object = PyTuple_GET_ITEM(%(axis)s, 0);\n            axis = (int)PyInt_AS_LONG(axis_object);\n            if (axis > PyArray_NDIM(%(x)s)-1 || axis < -PyArray_NDIM(%(x)s)) {\n                PyErr_SetString(PyExc_ValueError,\n                ""MaxAndArgmax: bad axis argument"");\n                %(fail)s\n            }\n        } else {\n            PyErr_SetString(PyExc_NotImplementedError,\n            ""MaxAndArgmax: NumPy C-API can compute max and argmax only for 1 axis or for all axes."");\n            %(fail)s\n        }\n\n        Py_CLEAR(%(max)s);\n        Py_CLEAR(%(argmax)s);//todo pass them as out parameter.\n\n        %(max)s = (PyArrayObject*)PyArray_Max(%(x)s, axis, NULL);\n        if (%(max)s == NULL) {\n            %(fail)s;\n        }\n        if (!PyArray_CheckExact(%(max)s)) {\n            %(max)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(max)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);\n            if(%(max)s == NULL){\n                %(fail)s;\n            }\n        }\n\n        %(argmax)s = (PyArrayObject*)PyArray_ArgMax(%(x)s, axis, NULL);\n        if (%(argmax)s == NULL) {\n            Py_CLEAR(%(max)s);\n            %(fail)s;\n        }\n        if (!PyArray_CheckExact(%(argmax)s)) {\n            %(argmax)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(argmax)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);\n            if(%(argmax)s == NULL){\n                %(fail)s;\n            }\n        }\n        if (PyArray_TYPE(%(argmax)s) != NPY_INT64) {\n            PyObject * tmp = PyArray_Cast(%(argmax)s, NPY_INT64);\n            if (NULL == tmp){\n                %(fail)s;\n            }\n            Py_DECREF(%(argmax)s);\n            %(argmax)s = (PyArrayObject*)tmp;\n        }\n        """"""\n        return ret % locals()\n\n    def c_code_cache_version(self):\n        return (5,)\n\n    def infer_shape(self, node, shapes):\n        ishape = shapes[0]\n        rval = tuple(ishape[i] for (i, b) in enumerate(\n            node.inputs[0].type.broadcastable) if i not in self.axis)\n        return [rval, rval]\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None, None]\n        if len(self.axis) != 1:\n            raise ValueError((\'R_op supported for arg_max only for \'\n                              \'one axis!\'))\n        if self.axis[0] > 1:\n            raise ValueError((\'R_op supported for arg_max only when \'\n                              \' axis is 0 or 1\'))\n        if inputs[0].ndim != 2:\n            raise ValueError((\'R_op supported for arg_max only when \'\n                              \' input is a matrix\'))\n        max_vals, max_pos = self.make_node(*inputs).outputs\n        if self.axis[0] == 0:\n            return [eval_points[0][max_pos,\n                                   arange(eval_points[0].shape[1])], None]\n        else:\n            return [eval_points[0][arange(eval_points[0].shape[0]),\n                                   max_pos], None]\n\n    def grad(self, inp, grads):\n        # The strict sense mathematical gradient of the maximum function is\n        # not calculated here for it is not defined at every point where some\n        # coordinates are identical. However, since the latter set has null\n        # Lebesgue measure, the result may be interpreted as weak gradient.\n\n        # @note: This function should work correctly for L{vector}s.\n        # (x, y), (gz, gw)\n        # gz*dz/dx + gw*dw/dx, gz*dz/dy + gw*dw/dy\n        # gMax * dMax/dx + gArgMax * dArgMax/dx,\n        # gMax * dMax/daxis + gArgMax * dArgMax/daxis\n        # g_max has one less dimension than x, so you need to complete\n        # g_max to x\'s shape when axis=0 the broadcasting mechanism\n        # does it automatically\n        x = inp[0]\n        axis = _as_tensor_variable(self.axis)\n        g_max, g_max_idx = grads\n\n        g_max_disconnected = isinstance(g_max.type, DisconnectedType)\n        g_max_idx_disconnected = isinstance(g_max_idx.type, DisconnectedType)\n\n        # if the op is totally disconnected, so are its inputs\n        if g_max_disconnected and g_max_idx_disconnected:\n            return [DisconnectedType()(), DisconnectedType()()]\n\n        # if the max is disconnected but the argmax is not,\n        # the gradient on its inputs is zero\n        if g_max_disconnected:\n            return [x.zeros_like()]\n        if NoneConst.equals(axis):\n            axis_ = list(range(x.ndim))\n        else:\n            axis_ = axis\n        xmax = max(x, axis_)\n\n        # Raise the g_max and xmax to the same number of dim as the input.\n        pattern = []\n        out_dim = 0\n        if NoneConst.equals(axis):\n            # We are taking the max/argmax over all dimensions.\n            axis = None\n        for i in xrange(x.ndim):\n            if axis is None or i in axis.data:\n                pattern.append(\'x\')\n            else:\n                pattern.append(out_dim)\n                out_dim += 1\n        g_max_pad = DimShuffle(g_max.broadcastable, pattern)(g_max)\n        xmax_pad = DimShuffle(xmax.broadcastable, pattern)(xmax)\n\n        # Set the grad to the correct position.\n        g_x = eq(xmax_pad, x) * g_max_pad\n        return g_x,\n\n\nclass Argmax(Op):\n    """"""\n    Calculate the argmax over a given axis or over all axes.\n    """"""\n    nin = 2  # tensor, axis\n    nout = 1\n    E_axis = \'invalid axis\'\n    __props__ = (\'axis\',)\n    _f16_ok = True\n\n    params_type = ParamsType(c_axis=scal.int64)\n\n    def __init__(self, axis):\n        if axis is not None:\n            axis = tuple(axis)\n        self.axis = tuple(axis)\n\n    def get_params(self, node):\n        if self.axis is not None and len(self.axis) == 1:\n            c_axis = np.int64(self.axis[0])\n        else:\n            # The value here doesn\'t matter, it won\'t be used\n            c_axis = np.int64(-1)\n        return self.params_type.get_params(c_axis=c_axis)\n\n    def make_node(self, x, axis=None):\n        x = _as_tensor_variable(x)\n        if self.axis is None:\n            all_axes = list(range(x.ndim))\n        else:\n            all_axes = self.axis\n        inputs = [x]\n\n        # We keep the original broadcastable flags for dimensions on which\n        # we do not perform the argmax.\n        broadcastable = [b for i, b in enumerate(x.type.broadcastable)\n                         if i not in all_axes]\n        outputs = [tensor(\'int64\', broadcastable, name=\'argmax\')]\n        return Apply(self, inputs, outputs)\n\n    def prepare_node(self, node, storage_map, compute_map, impl):\n        if len(node.inputs) == 2:\n            raise ValueError(\'You are trying to compile a graph with an old Argmax node.  Either reoptimize your graph or rebuild it to get the new node format.\')\n\n    def perform(self, node, inp, outs, params):\n        x, = inp\n        axes = self.axis\n        max_idx, = outs\n        if axes is None:\n            axes = tuple(range(x.ndim))\n\n        # Numpy does not support multiple axes for argmax\n        # Work around\n        keep_axes = np.array([i for i in range(x.ndim) if i not in axes],\n                             dtype=\'int64\')\n        # Not-reduced axes in front\n        transposed_x = np.transpose(x, np.concatenate((keep_axes,\n                                                       axes)))\n        kept_shape = transposed_x.shape[:len(keep_axes)]\n        reduced_shape = transposed_x.shape[len(keep_axes):]\n        new_shape = kept_shape + (np.prod(reduced_shape),)\n        reshaped_x = transposed_x.reshape(new_shape)\n\n        max_idx[0] = theano._asarray(np.argmax(reshaped_x, axis=-1),\n                                     dtype=\'int64\')\n\n    def c_code(self, node, name, inp, out, sub):\n        x, = inp\n        argmax, = out\n        fail = sub[""fail""]\n        params = sub[""params""]\n        if self.axis is None:\n            axis_code = ""axis = NPY_MAXDIMS;""\n        else:\n            if len(self.axis) > 1:\n                raise NotImplementedError()\n            # params is only used here for now\n            axis_code = """"""\n            axis = %(params)s->c_axis;\n            if(axis > PyArray_NDIM(%(x)s)-1 || axis < -PyArray_NDIM(%(x)s)){\n                PyErr_SetString(PyExc_ValueError,\n                ""Argmax, bad axis argument"");\n                %(fail)s\n            }\n            """""" % locals()\n        ret = """"""\n        int axis;\n\n        Py_CLEAR(%(argmax)s);//todo pass them as out parameter.\n        %(axis_code)s\n\n        %(argmax)s = (PyArrayObject*)PyArray_ArgMax(%(x)s, axis, NULL);\n        if(%(argmax)s == NULL){\n            %(fail)s;\n        }\n        if(!PyArray_CheckExact(%(argmax)s)){\n            %(argmax)s = (PyArrayObject*)PyArray_FromAny((PyObject*)%(argmax)s, NULL, 0, 0, NPY_ARRAY_ENSUREARRAY, NULL);\n            if(%(argmax)s == NULL){\n                %(fail)s;\n            }\n        }\n        if(PyArray_TYPE(%(argmax)s) != NPY_INT64){\n            PyObject * tmp = PyArray_Cast(%(argmax)s, NPY_INT64);\n            if (NULL == tmp){\n                %(fail)s;\n            }\n            Py_DECREF(%(argmax)s);\n            %(argmax)s = (PyArrayObject*)tmp;\n        }\n        """"""\n        return ret % locals()\n\n    def c_code_cache_version(self):\n        return (1,)\n\n    def infer_shape(self, node, shapes):\n        ishape, = shapes\n        if self.axis is None:\n            return [()]\n        rval = tuple([ishape[i] for (i, b) in enumerate(\n            node.inputs[0].type.broadcastable) if i not in self.axis])\n        return [rval]\n\n    def grad(self, inp, grads):\n        x, = inp\n\n        return [x.zeros_like()]\n\n\ndef makeKeepDims(x, y, axis):\n    """"""\n    Reintroduces in y with length one the axes of x which have been left out\n    in a prior reduction of x. With this option, the resulting tensor will\n    broadcast correctly against the original tensor x.\n\n    """"""\n    x = as_tensor_variable(x)\n    y = as_tensor_variable(y)\n\n    if axis is None:\n        axis = list(range(x.type.ndim))\n    elif isinstance(axis, (integer_types, np.integer)):\n        axis = [axis]\n    elif isinstance(axis, np.ndarray) and axis.ndim == 0:\n        axis = [int(axis)]\n    else:\n        axis = [int(a) for a in axis]\n    newaxis = []\n    for a in axis:\n        if not isinstance(a, integer_types):\n            raise ValueError(\n                ""keepdims option can be used only with constant axis"")\n        if a < 0:\n            a += x.type.ndim\n        newaxis.append(a)\n    i = 0\n    new_dims = []\n    for j, _ in enumerate(x.type.broadcastable):\n        if j in newaxis:\n            new_dims.append(\'x\')\n        else:\n            new_dims.append(i)\n            i += 1\n    return DimShuffle(y.type.broadcastable, new_dims)(y)\n\n\n@constructor\ndef max_and_argmax(a, axis=None, keepdims=False):\n    """"""\n    Returns maximum elements and their indices obtained by iterating over\n    given axis.\n\n    When axis is None (the default value), the max is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims : bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    """"""\n    # Check axis and convert it to a Python list of integers.\n    # Axis will be used as an op param of MaxAndArgmax.\n    a = as_tensor_variable(a)\n    axis = check_and_normalize_axes(a, axis)\n    if len(axis) == 0:\n        axis = list(range(a.type.ndim))\n    out, argout = MaxAndArgmax(axis)(a)\n\n    if keepdims:\n        out = makeKeepDims(a, out, axis)\n        argout = makeKeepDims(a, argout, axis)\n    return [out, argout]\n\n\n@constructor\ndef max(x, axis=None, keepdims=False):\n    """"""\n    Returns maximum elements obtained by iterating over given axis.\n\n    When axis is None (the default value), the max is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    Notes\n    -----\n    We return an error as numpy when we reduce a dim with a shape of 0.\n\n    # We have a choice of implementing this call with the\n    # CAReduce op or the MaxAndArgmax op.\n\n    # MaxAndArgmax supports grad and Rop, so we prefer to use that.\n    # CAReduce is faster, but optimizations will replace MaxAndArgmax[0]\n    # with CAReduce at compile time, so at this stage the important\n    # thing is supporting all user interface features, not speed.\n    # Some cases can be implemented only with CAReduce.\n\n    # We thus prefer to use MaxAndArgmax, if possible. It does not\n    # support all axis arguments, so we may need to fall back to CAReduce.\n\n    try:\n        out = max_and_argmax(x, axis)[0]\n    except Exception:\n        out = CAReduce(scal.maximum, axis)(x)\n\n    if keepdims:\n        out = makeKeepDims(x, out, axis)\n    return out\n\n\n@constructor\ndef argmax(x, axis=None, keepdims=False):\n    """"""\n    Returns indices of maximum elements obtained by iterating over given axis.\n\n    When axis is None (the default value), the argmax is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims : bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    """"""\n    argout = max_and_argmax(x, axis)[1]\n\n    if keepdims:\n        argout = makeKeepDims(x, argout, axis)\n    return argout\n\n\n@constructor\ndef min(x, axis=None, keepdims=False):\n    """"""\n    Returns minimum elements obtained by iterating over given axis.\n\n    When axis is None (the default value), the min is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    """"""\n    x = as_tensor_variable(x)\n    str_x_type = str(x.dtype)\n    if str_x_type.startswith(\'float\') or str_x_type in int_dtypes:\n        return -max(-x, axis=axis, keepdims=keepdims)\n    elif str_x_type in uint_dtypes:\n        itype = np.iinfo(x.dtype)\n        max_val = np.array(itype.max, dtype=itype.dtype)\n        return max_val - max(max_val - x, axis=axis, keepdims=keepdims)\n    elif str_x_type == \'bool\':\n        return ~max(~x, axis=axis, keepdims=keepdims)\n    else:\n        # Be careful about unsigned integers, complex\n        raise NotImplementedError()\n\n\n@constructor\ndef argmin(x, axis=None, keepdims=False):\n    """"""\n    Returns indices of minimum elements obtained by iterating over given axis.\n\n    When axis is None (the default value), the argmin is performed\n    over the flattened tensor.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    """"""\n    x = as_tensor_variable(x)\n    str_x_type = str(x.dtype)\n    if str_x_type.startswith(\'float\') or str_x_type in int_dtypes:\n        return argmax(-x, axis=axis, keepdims=keepdims)\n    elif str_x_type in uint_dtypes:\n        itype = np.iinfo(x.dtype)\n        return argmax(itype.max - x, axis=axis, keepdims=keepdims)\n    elif str_x_type == \'bool\':\n        return argmax(~x, axis=axis, keepdims=keepdims)\n    else:\n        # Be careful about unsigned integers, complex\n        raise NotImplementedError()\n\n\n@constructor\ndef smallest(*args):\n    """"""\n    Return the [elementwise] smallest of a variable number of arguments.\n\n    Like python\'s min.\n\n    """"""\n    if len(args) == 2:\n        a, b = args\n        return switch(a < b, a, b)\n    else:\n        return min(stack(args), axis=0)\n\n\n@constructor\ndef largest(*args):\n    """"""\n    Return the [elementwise] largest of a variable number of arguments.\n\n    Like python\'s max.\n\n    """"""\n    if len(args) == 2:\n        a, b = args\n        return switch(a > b, a, b)\n    else:\n        return max(stack(args), axis=0)\n\n\n##########################\n# Comparison\n##########################\n\n@_scal_elemwise\ndef lt(a, b):\n    """"""a < b""""""\n\n\n@_scal_elemwise\ndef gt(a, b):\n    """"""a > b""""""\n\n\n@_scal_elemwise\ndef le(a, b):\n    """"""a <= b""""""\n\n\n@_scal_elemwise\ndef ge(a, b):\n    """"""a >= b""""""\n\n\n@_scal_elemwise\ndef eq(a, b):\n    """"""a == b""""""\n\n\n@_scal_elemwise\ndef neq(a, b):\n    """"""a != b""""""\n\n\n@_scal_elemwise\ndef isnan(a):\n    """"""isnan(a)""""""\n\n# Rename isnan to isnan_ to allow to bypass it when not needed.\n# glibc 2.23 don\'t allow isnan on int, so we remove it from the graph.\nisnan_ = isnan\n\n\ndef isnan(a):\n    """"""isnan(a)""""""\n    a = as_tensor_variable(a)\n    if a.dtype in discrete_dtypes:\n        return alloc(np.asarray(False, dtype=""bool""),\n                     *[a.shape[i] for i in range(a.ndim)])\n    return isnan_(a)\n\n\n@_scal_elemwise\ndef isinf(a):\n    """"""isinf(a)""""""\n\n# Rename isnan to isnan_ to allow to bypass it when not needed.\n# glibc 2.23 don\'t allow isnan on int, so we remove it from the graph.\nisinf_ = isinf\n\n\ndef isinf(a):\n    """"""isinf(a)""""""\n    a = as_tensor_variable(a)\n    if a.dtype in discrete_dtypes:\n        return alloc(np.asarray(False, dtype=""bool""),\n                     *[a.shape[i] for i in range(a.ndim)])\n    return isinf_(a)\n\n\ndef allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    """"""\n    Implement Numpy\'s ``allclose`` on tensors.\n\n    ``absolute(a - b) <= (atol + rtol * absolute(b))``\n\n    Parameters\n    ----------\n    a : tensor\n        Input to compare.\n    b : tensor\n        Input to compare.\n    rtol : float\n        The relative tolerance parameter.\n    atol : float\n        The absolute tolerance parameter.\n    equal_nan: bool\n        Whether to consider nan\'s in the same place to be close.\n\n    Returns\n    -------\n    bool\n        A boolean value (of type int8 returned by the tensor elementwise `all`\n        function) whether all elements in a and b are in the tolerance range\n        defined above.\n\n    Notes\n    -----\n    Not a symmetric equation. See Numpy\'s documentation.\n\n    """"""\n    return all(isclose(a, b, rtol, atol, equal_nan))\n\n\ndef isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    """"""\n    Implements Numpy\'s ``isclose`` on tensors.\n\n    The tolerance values are positive, typically very small numbers. The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    ``absolute(a - b) <= (atol + rtol * absolute(b))``\n\n    Parameters\n    ----------\n    a : tensor\n        Input to compare.\n    b : tensor\n        Input to compare.\n    rtol : float\n        The relative tolerance parameter.\n    atol : float\n        The absolute tolerance parameter.\n    equal_nan : bool\n        Whether to consider nan\'s in the same place to be close\n\n    Returns\n    -------\n    int8\n        A boolean (int8) array where two arrays are element-wise equal\n        within a tolerance.\n\n    Notes\n    -----\n    Not a symmetric equation. See Numpy\'s documentation.\n\n    Examples\n    --------\n    >>> import theano\n    >>> import numpy as np\n    >>> a = theano._asarray([1e10, 1e-7], dtype=""float64"")\n    >>> b = theano._asarray([1.00001e10, 1e-8], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 0], dtype=int8)\n    >>> a = theano._asarray([1e10, 1e-8], dtype=""float64"")\n    >>> b = theano._asarray([1.00001e10, 1e-9], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 1], dtype=int8)\n    >>> a = theano._asarray([1e10, 1e-8], dtype=""float64"")\n    >>> b = theano._asarray([1.0001e10, 1e-9], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([0, 1], dtype=int8)\n    >>> a = theano._asarray([1.0, np.nan], dtype=""float64"")\n    >>> b = theano._asarray([1.0, np.nan], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 0], dtype==int8)\n    >>> a = theano._asarray([1.0, np.nan], dtype=""float64"")\n    >>> b = theano._asarray([1.0, np.nan], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b, equal_nan=True).eval()\n    array([1, 1], dtype==int8)\n    >>> a = theano._asarray([1.0, np.inf], dtype=""float64"")\n    >>> b = theano._asarray([1.0, -np.inf], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 0], dtype==int8)\n    >>> a = theano._asarray([1.0, np.inf], dtype=""float64"")\n    >>> b = theano._asarray([1.0, np.inf], dtype=""float64"")\n    >>> theano.tensor.isclose(a, b).eval()\n    array([1, 1], dtype==int8)\n\n    """"""\n    # close will be an int8 array of 1 where within tolerance\n    # and 0 where not within tolerance or there was a nan or inf value.\n    diff = abs(a - b)\n    tolerance = atol + rtol * abs(b)\n    close_prelim = le(diff, tolerance)\n\n    a_nan = isnan(a)\n    b_nan = isnan(b)\n    nans = bitwise_or(a_nan, b_nan)\n\n    a_inf = isinf(a)\n    b_inf = isinf(b)\n    infs = bitwise_or(a_inf, b_inf)\n\n    nans_or_infs = bitwise_or(nans, infs)\n\n    # close is now an array of 0\'s except where elements are not nan or inf\n    # and are within the tolerance.\n    close = bitwise_and(close_prelim, bitwise_not(nans_or_infs))\n\n    # deal with signed inf values. this will make an array inf_eq of 0\'s\n    # except where inf values have the same sign.\n    both_infs = bitwise_and(a_inf, b_inf)\n    inf_signs_eq = eq(a_inf * sgn(a), b_inf * sgn(b))\n    inf_eq = bitwise_and(both_infs, inf_signs_eq)\n\n    # now create the potential result combining close and inf_eq\n    close_with_infs = bitwise_or(close, inf_eq)\n\n    # deal with comparing nan\'s.\n    if equal_nan:\n        both_nans = bitwise_and(a_nan, b_nan)\n        return bitwise_or(close_with_infs, both_nans)\n    # otherwise nan\'s aren\'t considered close.\n    else:\n        return close_with_infs\n\n\n##########################\n# Condition\n##########################\n\n@_scal_elemwise\ndef switch(cond, ift, iff):\n    """"""if cond then ift else iff""""""\n\nwhere = switch\n##########################\n# Bit-wise\n##########################\n\n\n@_scal_elemwise\ndef and_(a, b):\n    """"""bitwise a & b""""""\nbitwise_and = and_  # numpy name for it\n\n\n@_scal_elemwise\ndef or_(a, b):\n    """"""bitwise a | b""""""\nbitwise_or = or_  # numpy name for it\n\n\n@_scal_elemwise\ndef xor(a, b):\n    """"""bitwise a ^ b""""""\nbitwise_xor = xor  # numpy name for it\n\n\n@_scal_elemwise\ndef invert(a):\n    """"""bitwise ~a""""""\nbitwise_not = invert  # numpy alias for it\n\n\n##########################\n# Math\n##########################\n\n@_scal_elemwise\ndef abs_(a):\n    """"""|`a`|\n\n    TensorVariable overloads the `TensorVariable.__abs__` operator so that\n    this function is called when you type abs(a).\n\npprint.assign(abs_, printing.PatternPrinter((\'|%(0)s|\', -1000)))\n\n\n@_scal_elemwise\ndef exp(a):\n    """"""e^`a`""""""\n\n\n@_scal_elemwise\ndef exp2(a):\n    """"""2^`a`""""""\n\n\n@_scal_elemwise\ndef expm1(a):\n    """"""e^`a` - 1""""""\n\n\n@_scal_elemwise\ndef neg(a):\n    """"""-a""""""\n\n\n# numpy.reciprocal does integer division on integer inputs\n# (which is not very interesting)\n@_scal_elemwise\ndef inv(a):\n    """"""1.0/a""""""\n\n\n@_scal_elemwise\ndef log(a):\n    """"""base e logarithm of a""""""\n\n\n@_scal_elemwise\ndef log2(a):\n    """"""base 2 logarithm of a""""""\n\n\n@_scal_elemwise\ndef log10(a):\n    """"""base 10 logarithm of a""""""\n\n\n@_scal_elemwise\ndef log1p(a):\n    """"""log(1+a)""""""\n\n\n@_scal_elemwise\ndef sgn(a):\n    """"""sign of a""""""\n\n\n@_scal_elemwise\ndef ceil(a):\n    """"""ceiling of a""""""\n\n\n@_scal_elemwise\ndef floor(a):\n    """"""floor of a""""""\n\n\n@_scal_elemwise\ndef trunc(a):\n    """"""trunc of a""""""\n\n\n@constructor\ndef iround(a, mode=None):\n    """"""cast(round(a,mode),\'int64\')""""""\n    return cast(round(a, mode), \'int64\')\n\n\n@constructor\ndef round(a, mode=None):\n    """"""round_mode(a) with mode in [half_away_from_zero, half_to_even].\n    Default to half_to_even.""""""\n    if mode is None:\n        mode = ""half_to_even""\n        if config.warn.round:\n            warnings.warn(\n                ""theano.tensor.round() changed its default from""\n                "" `half_away_from_zero` to `half_to_even` to have""\n                "" the same default as NumPy. Use the Theano flag""\n                "" `warn.round=False` to disable this warning."")\n    if mode == ""half_away_from_zero"":\n        return round_half_away_from_zero(a)\n    elif mode == ""half_to_even"":\n        return round_half_to_even(a)\n    else:\n        raise Exception(""round mode %s is not implemented."" % mode)\n\n\n@_scal_elemwise\ndef round_half_to_even(a):\n    """"""round_half_to_even(a)""""""\n\n\n@_scal_elemwise\ndef round_half_away_from_zero(a):\n    """"""round_half_away_from_zero(a)""""""\n\n\n@_scal_elemwise\ndef sqr(a):\n    """"""square of a""""""\n\n\n# alias to sqr, included to maintain similarity with numpy interface\nsquare = sqr\n\n\ndef cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None):\n    """"""Calculate the covariance matrix.\n    Covariance indicates the level to which two variables vary together.\n    If we examine N-dimensional samples, :math:`m = [x_1, x_2, ... x_N]^T`,\n    then the covariance matrix element :math:`C_{ij}` is the covariance of\n    :math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance\n    of :math:`x_i`. Code and docstring ported from numpy.\n    ----------\n    m : array_like\n        A 2-D array containing multiple variables and observations.\n        Each row of `m` represents a variable, and each column is\n        observations of all those variables.\n    y : array_like, optional\n        An additional set of variables and observations. `y` has the same form\n        as that of `m`.\n    rowvar : bool, optional\n        If `rowvar` is True (default), then each row represents a\n        variable, with observations in the columns. Otherwise, the relationship\n        is transposed: each column represents a variable, while the rows\n        contain observations.\n    bias : bool, optional\n        Default normalization (False) is by ``(N - 1)``, where ``N`` is the\n        number of observations given (unbiased estimate). If `bias` is True, then\n        normalization is by ``N``. These values can be overridden by using the\n        keyword ``ddof``.\n    ddof : int, optional\n        If not ``None`` the default value implied by `bias` is overridden.\n        The default value is ``None``.\n    Returns\n    -------\n    out : The covariance matrix of the variables.\n    """"""\n\n    if fweights is not None:\n        raise NotImplementedError(\'fweights are not implemented\')\n    if aweights is not None:\n        raise NotImplementedError(\'aweights are not implemented\')\n\n    if not rowvar and m.shape[0] != 1:\n        m = m.T\n\n    if y is not None:\n        if not rowvar and y.shape[0] != 1:\n            y = y.T\n        m = theano.tensor.concatenate((m, y), axis=0)\n\n    if ddof is None:\n        if not bias:\n            ddof = 1\n        else:\n            ddof = 0\n\n    # Determine the normalization\n    fact = m.shape[1] - ddof\n\n    m -= m.mean(axis=1, keepdims=1)\n    c = m.dot(m.T)\n    c *= theano.tensor.constant(1) / fact\n    return c.squeeze()\n\n\n@_scal_elemwise\ndef sqrt(a):\n    """"""square root of a""""""\n\n\n@_scal_elemwise\ndef deg2rad(a):\n    """"""convert degree a to radian""""""\n\n\n@_scal_elemwise\ndef rad2deg(a):\n    """"""convert radian a to degree""""""\n\n\n@_scal_elemwise\ndef cos(a):\n    """"""cosine of a""""""\n\n\n@_scal_elemwise\ndef arccos(a):\n    """"""arccosine of a""""""\n\n\n@_scal_elemwise\ndef sin(a):\n    """"""sine of a""""""\n\n\n@_scal_elemwise\ndef arcsin(a):\n    """"""arcsine of a""""""\n\n\n@_scal_elemwise\ndef tan(a):\n    """"""tangent of a""""""\n\n\n@_scal_elemwise\ndef arctan(a):\n    """"""arctangent of a""""""\n\n\n@_scal_elemwise\ndef arctan2(a, b):\n    """"""arctangent of a / b""""""\n\n\n@_scal_elemwise\ndef cosh(a):\n    """"""hyperbolic cosine of a""""""\n\n\n@_scal_elemwise\ndef arccosh(a):\n    """"""hyperbolic arc cosine of a""""""\n\n\n@_scal_elemwise\ndef sinh(a):\n    """"""hyperbolic sine of a""""""\n\n\n@_scal_elemwise\ndef arcsinh(a):\n    """"""hyperbolic arc sine of a""""""\n\n\n@_scal_elemwise\ndef tanh(a):\n    """"""hyperbolic tangent of a""""""\n\n\n@_scal_elemwise\ndef arctanh(a):\n    """"""hyperbolic arc tangent of a""""""\n\n\n@_scal_elemwise\ndef erf(a):\n    """"""error function""""""\n\n\n@_scal_elemwise\ndef erfc(a):\n    """"""complementary error function""""""\n\n\n@_scal_elemwise\ndef erfcx(a):\n    """"""scaled complementary error function""""""\n\n\n@_scal_elemwise\ndef erfinv(a):\n    """"""inverse error function""""""\n\n\n@_scal_elemwise\ndef erfcinv(a):\n    """"""inverse complementary error function""""""\n\n\n@_scal_elemwise\ndef gamma(a):\n    """"""gamma function""""""\n\n\n@_scal_elemwise\ndef gammaln(a):\n    """"""log gamma function""""""\n\n\n@_scal_elemwise\ndef psi(a):\n    """"""derivative of log gamma function""""""\n\n\n@_scal_elemwise\ndef tri_gamma(a):\n    """"""second derivative of the log gamma function""""""\n\n\n@_scal_elemwise\ndef chi2sf(x, k):\n    """"""chi squared survival function""""""\n\n\n@_scal_elemwise\ndef gammainc(k, x):\n    """"""Regularized lower gamma function""""""\n\n\n@_scal_elemwise\ndef gammaincc(k, x):\n    """"""Regularized upper gamma function""""""\n\n\n@_scal_elemwise\ndef gammau(k, x):\n    """"""Upper incomplete gamma function.""""""\n\n\n@_scal_elemwise\ndef gammal(k, x):\n    """"""Lower incomplete gamma function.""""""\n\n\n@_scal_elemwise\ndef j0(x):\n    """"""Bessel function of the first kind of order 0.""""""\n\n\n@_scal_elemwise\ndef j1(x):\n    """"""Bessel function of the first kind of order 1.""""""\n\n\n@_scal_elemwise\ndef jv(v, x):\n    """"""Bessel function of the first kind of order v (real).""""""\n\n\n@_scal_elemwise\ndef i0(x):\n    """"""Modified Bessel function of the first kind of order 0.""""""\n\n\n@_scal_elemwise\ndef i1(x):\n    """"""Modified Bessel function of the first kind of order 1.""""""\n\n\n@_scal_elemwise\ndef iv(v, x):\n    """"""Modified Bessel function of the first kind of order v (real).""""""\n\n\n@_scal_elemwise\ndef real(z):\n    """"""Return real component of complex-valued tensor `z`""""""\n_tensor_py_operators.real = property(real)\n\n\n@_scal_elemwise\ndef imag(z):\n    """"""Return imaginary component of complex-valued tensor `z`""""""\n_tensor_py_operators.imag = property(imag)\n\n\n@_scal_elemwise\ndef angle(z):\n    """"""Return polar-coordinate angle of complex-valued tensor `z`""""""\n\n\n@_scal_elemwise  # numpy.complex cannot build tensors\ndef complex(real, imag):\n    """"""Return complex-valued tensor with `real` and `imag` components""""""\n\n\n@_scal_elemwise\ndef conj(z):\n    """"""Return the complex conjugate of `z`.""""""\n\n\n@_scal_elemwise\ndef complex_from_polar(abs, angle):\n    """"""Return complex-valued tensor from polar coordinate specification.""""""\n\n##########################\n# Misc\n##########################\n\n\n# fill, _fill_inplace = _elemwise(scal.second, \'fill\',\n# """"""fill WRITEME (elemwise)"""""")\n@_scal_elemwise\ndef second(a, b):\n    """"""Create a matrix by filling the shape of a with b""""""\n\nfill = second\npprint.assign(fill, printing.FunctionPrinter(\'fill\'))\n\n\n@constructor\ndef ones_like(model, dtype=None, opt=False):\n    """"""equivalent of numpy.ones_like\n    Parameters\n    ----------\n    model : tensor\n    dtype : data-type, optional\n    opt : If True, we will return a constant instead of a graph when possible.\n          Useful for Theano optimization, not for user building a graph as this\n          have the consequence that model isn\'t always in the graph.\n\n    Returns\n    -------\n    tensor\n        tensor the shape of model containing ones of the type of dtype.\n    """"""\n    if dtype is None:\n        dtype = model.type.dtype\n    ret = constant(1.0, dtype=dtype)\n    if opt and ret.type == model.type:\n        return ret\n    return fill(model, ret)\n\n\n@constructor\ndef zeros_like(model, dtype=None, opt=False):\n    """"""equivalent of numpy.zeros_like\n    Parameters\n    ----------\n    model : tensor\n    dtype : data-type, optional\n    opt : If True, we will return a constant instead of a graph when possible.\n          Useful for Theano optimization, not for user building a graph as this\n          have the consequence that model isn\'t always in the graph.\n\n    Returns\n    -------\n    tensor\n        tensor the shape of model containing zeros of the type of dtype.\n    """"""\n\n    if dtype is None:\n        dtype = model.type.dtype\n    ret = constant(0.0, dtype=dtype)\n    if opt and ret.type == model.type:\n        return ret\n    return fill(model, ret)\n\n\ndef zeros(shape, dtype=None):\n    """"""\n    Create a Tensor filled with zeros, closer to Numpy\'s syntax than ``alloc``.\n    """"""\n    if not isinstance(shape, (list, tuple, TensorVariable)):\n        shape = [shape]\n    if dtype is None:\n        dtype = config.floatX\n    return alloc(np.array(0, dtype=dtype), *shape)\n\n\ndef ones(shape, dtype=None):\n    """"""\n    Create a Tensor filled with ones, closer to Numpy\'s syntax than ``alloc``.\n    """"""\n    if not isinstance(shape, (list, tuple, TensorVariable)):\n        shape = [shape]\n    if dtype is None:\n        dtype = config.floatX\n    return alloc(np.array(1, dtype=dtype), *shape)\n\n\nclass Nonzero(gof.Op):\n    """"""\n    Return the indices of the elements that are non-zero.\n\n    Returns a matrix of shape (ndim, number of nonzero elements) such that\n    element (i,j) is the index in the ith dimension of the jth non-zero\n    element.\n\n    Note this is different than NumPy, which returns a tuple of arrays, one for\n    each dimension of the input array.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n\n    Returns\n    -------\n    matrix\n        Matrix containing the indices of the non-zero elements of a.\n\n    See Also\n    --------\n    nonzero_values : Return the non-zero elements of the input array\n    flatnonzero : Return the indices of the non-zero elements of the\n        flattened input array.\n\n    """"""\n    __props__ = ()\n\n    def make_node(self, a):\n        a = as_tensor_variable(a)\n        if a.ndim == 0:\n            raise ValueError(\'Nonzero only supports non-scalar arrays.\')\n        output = [TensorType(dtype=\'int64\', broadcastable=(False, False))()]\n        return gof.Apply(self, [a], output)\n\n    def perform(self, node, inp, out_):\n        a = inp[0]\n        out, = out_\n\n        result_tuple = np.nonzero(a)\n        if len(result_tuple[0]) > 0:\n            result = np.vstack(result_tuple)\n        else:\n            result = np.zeros((len(result_tuple), 0))\n\n        out[0] = result.astype(\'int64\')\n\n    def grad(self, inp, grads):\n        return [grad_undefined(self, 0, inp[0])]\n\n\n_nonzero = Nonzero()\n\n\ndef nonzero(a, return_matrix=False):\n    """"""\n    Returns one of the following:\n\n        If return_matrix is False (default, same as NumPy):\n            A tuple of vector arrays such that the ith element of the jth array\n            is the index of the ith non-zero element of the input array in the\n            jth dimension.\n\n        If return_matrix is True (same as Theano Op):\n            Returns a matrix of shape (ndim, number of nonzero elements) such\n            that element (i,j) is the index in the ith dimension of the jth\n            non-zero element.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    return_matrix : bool\n        If True, returns a symbolic matrix. If False, returns a tuple of\n        arrays. Defaults to False.\n\n    Returns\n    -------\n    tuple of vectors or matrix\n\n    See Also\n    --------\n    nonzero_values : Return the non-zero elements of the input array\n    flatnonzero : Return the indices of the non-zero elements of the\n        flattened input array.\n\n    """"""\n    matrix_result = _nonzero(a)\n    if return_matrix:\n        return matrix_result\n    else:\n        if a.ndim > 0:\n            tuple_result = tuple([matrix_result[i] for i in xrange(a.ndim)])\n        else:\n            tuple_result = tuple([matrix_result[0]])\n        return tuple_result\n\n\ndef flatnonzero(a):\n    """"""\n    Return a vector of indices that are non-zero in the flattened version of a.\n\n    This is equivalent to nonzero(a.flatten(), return_matrix=True)[0]\n\n    Parameters\n    ----------\n    a : tensor\n        Input tensor\n\n    Returns\n    -------\n    vector\n        Output vector, containing the indices of the elements of `a.flatten()`\n        that are non-zero.\n\n    See Also\n    --------\n    nonzero : Return the indices of the non-zero elements of the input array.\n    nonzero_values : Return the non-zero elements of the input array\n\n    """"""\n    if a.ndim == 0:\n        raise ValueError(\'Nonzero only supports non-scalar arrays.\')\n    return nonzero(a.flatten(), return_matrix=True)[0]\n\n\ndef nonzero_values(a):\n    """"""\n    Return a vector of non-zero elements contained in the input array.\n\n    The following behavior works to extract non-zero elements from an array\n    in NumPy but is *NOT* supported by Theano:\n\n        a[numpy.nonzero(a)]\n\n    Instead, the nonzero_values function or method should be used:\n\n        tensor.nonzero_values(a)\n        a.nonzero_values()\n\n    This is equivalent to the following:\n\n        a.flatten()[tensor.flatnonzero(a)]\n\n    Parameters\n    ----------\n    a : tensor\n        Input tensor\n\n    Returns\n    -------\n    vector\n        Output vector, containing the non-zero elements of a.\n\n    See Also\n    --------\n    nonzero : Return the indices of the non-zero elements of the input array.\n    flatnonzero : Return the indices of the non-zero elements of the\n        flattened input array.\n\n    """"""\n    return a.flatten()[flatnonzero(a)]\n\n    __props__ = (""dtype"",)\n\n    def __init__(self, dtype=None):\n        if dtype is None:\n            dtype = config.floatX\n        self.dtype = dtype\n\n    def make_node(self, N, M, k):\n        N = as_tensor_variable(N)\n        M = as_tensor_variable(M)\n        k = as_tensor_variable(k)\n        return gof.Apply(\n            self,\n            [N, M, k],\n            [TensorType(dtype=self.dtype, broadcastable=(False, False))()])\n\n    def perform(self, node, inp, out_):\n        N, M, k = inp\n        out, = out_\n        out[0] = np.tri(N, M, k, dtype=self.dtype)\n\n    def infer_shape(self, node, in_shapes):\n        out_shape = [node.inputs[0], node.inputs[1]]\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        return [grad_undefined(self, i, inp[i]) for i in xrange(3)]\n\n\ndef tri(N, M=None, k=0, dtype=None):\n    """"""\n    An array with ones at and below the given diagonal and zeros elsewhere.\n\n    Parameters\n    ----------\n    N : int\n        Number of rows in the array.\n    M : int, optional\n        Number of columns in the array.\n        By default, `M` is taken equal to `N`.\n    k : int, optional\n        The sub-diagonal at and below which the array is filled.\n        `k` = 0 is the main diagonal, while `k` < 0 is below it,\n        and `k` > 0 is above.  The default is 0.\n    dtype : dtype, optional\n        Data type of the returned array.  The default is float.\n\n    Returns\n    -------\n    Array of shape (N, M)\n        Array with its lower triangle filled with ones and zero elsewhere;\n        in other words ``T[i,j] == 1`` for ``i <= j + k``, 0 otherwise.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    if M is None:\n        M = N\n    op = Tri(dtype)\n    return op(N, M, k)\n\n\ndef tril(m, k=0):\n    """"""\n    Lower triangle of an array.\n\n    Return a copy of an array with elements above the `k`-th diagonal zeroed.\n\n    Parameters\n    ----------\n    m : array_like, shape (M, N)\n        Input array.\n    k : int, optional\n        Diagonal above which to zero elements.  `k = 0` (the default) is the\n        main diagonal, `k < 0` is below it and `k > 0` is above.\n\n    Returns\n    -------\n    array, shape (M, N)\n        Lower triangle of `m`, of same shape and data-type as `m`.\n\n    See Also\n    --------\n    triu : Same thing, only for the upper triangle.\n\n    """"""\n    return m * tri(m.shape[0], m.shape[1], k=k, dtype=m.dtype)\n\n\ndef triu(m, k=0):\n    """"""\n    Upper triangle of an array.\n\n    Return a copy of a matrix with the elements below the `k`-th diagonal\n    zeroed.\n\n    Please refer to the documentation for `tril` for further details.\n\n    See Also\n    --------\n    tril : Lower triangle of an array.\n\n    """"""\n    return m * (1 - tri(m.shape[0], m.shape[1], k=k - 1, dtype=m.dtype))\n\n    __props__ = (""dtype"", )\n\n    def __init__(self, dtype=None):\n        if dtype is None:\n            dtype = config.floatX\n        self.dtype = dtype\n\n    def make_node(self, n, m, k):\n        n = as_tensor_variable(n)\n        m = as_tensor_variable(m)\n        k = as_tensor_variable(k)\n        assert n.ndim == 0\n        assert m.ndim == 0\n        assert k.ndim == 0\n        return gof.Apply(\n            self,\n            [n, m, k],\n            [TensorType(dtype=self.dtype, broadcastable=(False, False))()])\n\n    def perform(self, node, inp, out_):\n        n, m, k = inp\n        out, = out_\n        out[0] = np.eye(n, m, k, dtype=self.dtype)\n\n    def infer_shape(self, node, in_shapes):\n        out_shape = [node.inputs[0], node.inputs[1]]\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        return [grad_undefined(self, i, inp[i]) for i in xrange(3)]\n\n\ndef eye(n, m=None, k=0, dtype=None):\n    """"""Return a 2-D array with ones on the diagonal and zeros elsewhere.\n\n    Parameters\n    ----------\n    n : int\n        Number of rows in the output.\n    m : int, optional\n        Number of columns in the output. If None, defaults to `N`.\n    k : int, optional\n        Index of the diagonal: 0 (the default) refers to the main diagonal,\n        a positive value refers to an upper diagonal, and a negative value\n        to a lower diagonal.\n    dtype : data-type, optional\n        Data-type of the returned array.\n\n    Returns\n    -------\n    ndarray of shape (N,M)\n        An array where all elements are equal to zero, except for the `k`-th\n        diagonal, whose values are equal to one.\n\n    """"""\n    if dtype is None:\n        dtype = config.floatX\n    if m is None:\n        m = n\n    localop = Eye(dtype)\n    return localop(n, m, k)\n\n\ndef identity_like(x):\n    return eye(x.shape[0], x.shape[1], k=0, dtype=x.dtype)\n\n\ndef alloc_validate_shape(shape):\n    sh = [as_tensor_variable(s) for s in shape]\n    bcast = []\n    for i, s in enumerate(sh):\n        def err_str():\n            if config.exception_verbosity == \'high\':\n                return \'\\n\' + min_informative_str(s)\n            else:\n                return str(s)\n        if s.type.dtype not in integer_dtypes:\n            s_as_str = err_str()\n            raise TypeError(\'Shape arguments to Alloc must be integers, \'\n                            \'but argument %s is not for apply node: %s\' %\n                            (i, s_as_str))\n        if s.ndim != 0:\n            s_as_str = err_str()\n            raise TypeError(\n                ""Each shape dimension to Alloc must be a scalar, "",\n                \'but dimension %s have %d dimensions for apply node: %s\' %\n                (i, s.ndim, s_as_str))\n\n        # if s is constant 1, then we\'re broadcastable in that dim\n        try:\n            const_shp = get_scalar_constant_value(s)\n        except NotScalarConstantError:\n            const_shp = None\n        bcast.append(1 == const_shp)\n    return sh, bcast\n\n\nclass Alloc(gof.Op):\n    """"""Create a Tensor from an initial value and a desired shape.\n\n    alloc(value, shape0, shape1, ..., shapeN)\n\n    Returns an N-dimensional tensor initialized by `value` using something\n    equivalent to\n\n        z = numpy.zeros(shape, value.dtype)\n        z += value\n\n    The result has N dimensions, has the dtype of `value` and is obtained by\n    broadcasting value over the output ndarray.\n\n    This Op is used to replace fill() during optimizations because after shapes\n    are lifted, the first argument to fill can often be pruned from the graph.\n\n    """"""\n    _f16_ok = True\n    __props__ = ()\n\n    def validate_shape(self, shape):\n        return alloc_validate_shape(shape)\n\n    def make_node(self, value, *shape):\n        v = as_tensor_variable(value)\n        sh, bcast = alloc_validate_shape(shape)\n        if v.ndim > len(sh):\n            raise TypeError(""The Alloc value to use has more dimensions""\n                            "" than the specified dimensions"",\n                            v.ndim, len(sh))\n        otype = TensorType(dtype=v.dtype, broadcastable=bcast)\n        return gof.Apply(self, [v] + sh, [otype()])\n\n    def perform(self, node, inputs, out_):\n        out, = out_\n        v = inputs[0]\n        sh = tuple([int(i) for i in inputs[1:]])\n        if out[0] is None or out[0].shape != sh:\n            if v.size == 1 and v.item() == 0:\n                out[0] = np.zeros(sh, dtype=v.dtype)\n            else:\n                out[0] = np.empty(sh, dtype=v.dtype)\n                out[0][...] = v  # broadcast v to fill us up\n        else:\n            # reuse the allocated memory.\n            out[0][...] = v  # broadcast v to fill us up\n\n    def c_code(self, node, name, inp, out, sub):\n        vv = inp[0]\n        ndim = len(inp[1:])\n        zz, = out\n        fail = sub[\'fail\']\n\n        code = """"""\n            npy_intp shape[%(ndim)s];\n            """""" % dict(ndim=ndim)\n\n        # Initialize shape\n        for i, shp_i in enumerate(inp[1:]):\n            code += """"""\n                shape[%(i)s] = ((dtype_%(shp_i)s*) PyArray_DATA(%(shp_i)s))[0];\n                """""" % dict(i=i, shp_i=shp_i)\n\n        code += """"""\n            int need_new_out = (NULL == %(zz)s);\n            for (int i = 0; i < %(ndim)s; i++)\n                need_new_out = (need_new_out\n                                || (PyArray_DIMS(%(zz)s)[i] != shape[i]));\n\n            if (need_new_out)\n            {\n                Py_XDECREF(%(zz)s);\n                %(zz)s = (PyArrayObject*) PyArray_SimpleNew(%(ndim)s,\n                    shape, PyArray_TYPE((PyArrayObject*) py_%(vv)s));\n                if (!%(zz)s)\n                {\n                    PyErr_SetString(PyExc_MemoryError, ""alloc failed"");\n                    %(fail)s\n                }\n            }\n\n            // This function takes care of broadcasting\n            if (PyArray_CopyInto(%(zz)s, %(vv)s) == -1)\n              %(fail)s\n            """""" % dict(vv=vv, ndim=ndim, zz=zz, fail=fail)\n\n    def c_code_cache_version(self):\n        return (2,)\n\n    def infer_shape(self, node, input_shapes):\n        return [node.inputs[1:]]\n\n    def connection_pattern(self, node):\n\n        rval = [[True]]\n\n        for ipt in node.inputs[1:]:\n            rval.append([False])\n\n    def grad(self, inputs, grads):\n        x = inputs[0]\n        gz = grads[0]\n        n_axes_to_sum = gz.ndim - x.ndim\n        # The number of dimensions added\n        axis = list(range(n_axes_to_sum))\n        # The broadcasted dimensions\n        axis_broadcasted = []\n        axis_kept = []\n        for i, (ib, gb) in enumerate(\n            zip(inputs[0].broadcastable,\n                # We need the dimensions corresponding to x\n                grads[0].broadcastable[-inputs[0].ndim:])):\n            if ib and not gb:\n                axis_broadcasted.append(i + n_axes_to_sum)\n            else:\n                axis_kept.append(i)\n        gx = gz.sum(axis=axis + axis_broadcasted)\n        if axis_broadcasted:\n            new_order = [\'x\'] * x.ndim\n            for idx, axis in enumerate(axis_kept):\n                new_order[axis] = idx\n            gx = gx.dimshuffle(new_order)\n            # Dimshuffle to add back the broadcasted dims\n        # The *elements* of the output are not connected to\n        # the inputs that specify the shape. If you grow the\n        # shape by epsilon, the existing elements do not\n        # change.\n        return [gx] + [DisconnectedType()() for i in inputs[1:]]\n\n    def __call__(self, val, *shapes, **kwargs):\n        """"""\n        If the alloc would be useless, this function returns val.\n\n        If this function is called outside of a graph optimization context\n        (for instance, it is manually called by a user building a graph),\n        then we always return an Alloc node, to allow for DebugMode to check\n        for size mismatches.\n\n        If you always want an Alloc node, call make_node.\n\n        """"""\n        ret = super(Alloc, self).__call__(val, *shapes, **kwargs)\n        try:\n            # It makes optimization difficult when useless allocs are thrown\n            # into the graph at every stage of optimization.  This little logic\n            # tries to help at least in some cases.\n            if hasattr(val, \'fgraph\') and (val.type == ret.type):\n                return val\n        except AttributeError:\n            pass\n        return ret\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None]\n        return self(eval_points[0], *inputs[1:], **dict(return_list=True))\n\n    def do_constant_folding(self, node):\n        if not getattr(node.outputs[0], \'clients\', []):\n            # If there are no clients then there is no point doing constant\n            # folding.\n            return False\n        for client in node.outputs[0].clients:\n            if client[0] == \'output\':\n                # If the output is a constant, it will have to be deepcopied\n                # each time the function is called.  So we do not fold.\n                return False\n            elif (\n                # The following ops work inplace of their input id 0.\n                client[1] == 0 and\n                isinstance(client[0].op, (\n                    # Ops that will work inplace on the Alloc. So if they\n                    # get constant_folded, they would copy the\n                    # constant and this is less efficients.\n\n                    # Not doing the constant folding could also lower\n                    # the peak memory usage, as we the ""constant"" won\'t\n                    # always exists.\n                    theano.tensor.subtensor.IncSubtensor,\n                    theano.tensor.subtensor.AdvancedIncSubtensor1,\n                    theano.tensor.subtensor.AdvancedIncSubtensor,\n                    theano.tensor.blas.Gemv,\n                    theano.tensor.blas_c.CGemv,\n                    theano.tensor.blas.Ger,\n                    theano.tensor.blas_c.CGer,\n                    theano.tensor.blas_scipy.ScipyGer))):\n                return False\n            # If the clients is a transfer to the GPU, we don\'t want to\n            # fold. We let the Alloc being moved to the GPU, then we\n            # let the GPU algo decide if it need to fold it or not.\n            elif client[0].op.__class__.__name__.lower().startswith(""gpu""):\n                return False\n        return True\n\nalloc = Alloc()\npprint.assign(alloc, printing.FunctionPrinter(\'alloc\'))\n\n\ndef transfer(var, target):\n    """"""\n    Return a version of `var` transferred to `target`.\n\n    `cpu` mean a TensorType (on the CPU).  Other types may define\n    additional targets.\n\n    Parameters\n    ----------\n    var : variable\n        A theano variable\n    target : str\n        The target of the transfer\n    """"""\n    if target == \'cpu\':\n        return as_tensor_variable(var)\n    else:\n        for trans in transfer._others:\n            res = trans(var, target)\n            if res is not None:\n                return res\n    raise ValueError(""Can\'t transfer to target %s"" % (target,))\n\ntransfer._others = []\n\n\ndef register_transfer(fn):\n    """"""\n    Register a transfer function for alternative targets.\n\n    Parameters\n    ----------\n    fn : callable\n    """"""\n    transfer._others.append(fn)\n\n""""""Create a duplicate of `a` (with duplicated storage)""""""\ntensor_copy = elemwise.Elemwise(scal.identity)\npprint.assign(tensor_copy, printing.IgnorePrinter())\n\n\n@constructor\ndef sum(input, axis=None, dtype=None, keepdims=False, acc_dtype=None):\n    """"""\n    Computes the sum along the given axis(es) of a tensor `input`.\n\n    When axis is None (the default value), the sum is performed\n    over the flattened tensor.\n\n    For full documentation see ``tensor.elemwise.Sum``.\n    In particular please pay attention to the important warning when using\n    a custom acc_dtype.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    out = elemwise.Sum(axis=axis, dtype=dtype, acc_dtype=acc_dtype)(input)\n\n    if keepdims:\n        out = makeKeepDims(input, out, axis)\n    return out\n\npprint.assign(Sum(), printing.FunctionPrinter(\'sum\'))\n\n\n@constructor\ndef prod(input, axis=None, dtype=None, keepdims=False, acc_dtype=None,\n         no_zeros_in_input=False):\n    """"""\n    Computes the product along the given axis(es) of a tensor `input`.\n\n    When axis is None (the default value), the product is performed\n    over the flattened tensor.\n\n    For full documentation see ``tensor.elemwise.Prod``.\n\n    Parameters\n    ----------\n    keepdims: bool\n        If this is set to True, the axes which are reduced are left in\n        the result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original tensor.\n\n    out = elemwise.Prod(axis, dtype=dtype, acc_dtype=acc_dtype,\n                        no_zeros_in_input=no_zeros_in_input)(input)\n\n    if keepdims:\n        out = makeKeepDims(input, out, axis)\n    return out\n\n\nclass Mean(elemwise.CAReduce):\n    def __init__(self, axis=None):\n        elemwise.CAReduce.__init__(self, scal.add, axis)\n        assert self.axis is None or len(self.axis) == 1\n\n    def __str__(self):\n        if self.axis is not None:\n            return ""Mean{%s}"" % ("", "".join(str(x) for x in self.axis))\n        else:\n            return ""Mean""\n\n    def _output_dtype(self, idtype):\n        # we want to protect against overflow\n        return \'float64\'\n\n    def perform(self, node, inp, out):\n        input, = inp\n        output, = out\n        if self.axis is None:\n            axis = None\n        else:\n            axis = self.axis[0]\n        # numpy.asarray is needed as otherwise we can end up with a\n        # numpy scalar.\n        output[0] = np.asarray(np.mean(input, dtype=\'float64\',\n                                       axis=axis))\n\n    def c_code(self, node, name, inames, onames, sub):\n        if self.axis is not None:\n            return super(Op, self).c_code(node, name, inames, onames, sub)\n        ret = elemwise.CAReduce.c_code(self, node, name, inames, onames, sub)\n        # TODO: c_code perform support only axis is None\n        return ret + """"""\n  *((double *)PyArray_DATA(%s)) /= PyArray_SIZE(%s);\n  """""" % (onames[0], inames[0])\n\n# TODO: implement the grad. When done and tested, you can make this the default\n# version.\n#    def grad(self, (x,), (gout,)):\n#      import pdb;pdb.set_trace()\n#      return grad(mean(x, self.axis, op=False),[x])\n\n\n@constructor\ndef mean(input, axis=None, dtype=None, op=False, keepdims=False,\n         acc_dtype=None):\n    """"""\n    Computes the mean value along the given axis(es) of a tensor `input`.\n\n    Parameters\n    ----------\n    axis : None or int or (list of int) (see `Sum`)\n        Compute the mean along this axis of the tensor.\n        None means all axes (like numpy).\n    dtype: None or string\n        Dtype to cast the result of the inner summation into.\n        For instance, by default, a sum of a float32 tensor will be\n        done in float64 (acc_dtype would be float64 by default),\n        but that result will be casted back in float32.\n    keepdims: bool\n        If this is set to True, the axes which are reduced are\n        left in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original tensor.\n    acc_dtype: None or string\n        Dtype to use for the inner summation. This will not\n        necessarily be the dtype of the output (in particular\n        if it is a discrete (int/uint) dtype, the output will\n        be in a float type). If None, then we use the same rules as `sum()`.\n\n    Notes\n    -----\n    For gpu, if you specify dtype=float32, everything will be done on the gpu.\n\n    """"""\n    input = as_tensor_variable(input)\n    if op:\n        if dtype not in (None, \'float64\'):\n            raise NotImplementedError(\n                \'The Mean op does not support the dtype argument, \'\n                \'and will always use float64. If you want to specify \'\n                \'the dtype, call tensor.mean(..., op=False).\',\n                dtype)\n        if acc_dtype not in (None, \'float64\'):\n            raise NotImplementedError(\n                \'The Mean op does not support the acc_dtype argument, \'\n                \'and will always use float64. If you want to specify \'\n                \'acc_dtype, call tensor.mean(..., op=False).\',\n                dtype)\n        out = Mean(axis)(input)\n        if keepdims:\n            out = makeKeepDims(input, out, axis)\n        return out\n\n    if dtype is not None:\n        # The summation will be done with the specified dtype.\n        # sum() will complain if it is not suitable.\n        sum_dtype = dtype\n    else:\n        sum_dtype = None\n        # float16 overflows on the cast way too often\n        if input.dtype == \'float16\':\n            sum_dtype = \'float32\'\n\n    s = sum(input, axis=axis, dtype=sum_dtype, keepdims=keepdims,\n            acc_dtype=acc_dtype)\n    shp = shape(input)\n\n    # Cast shp into a float type\n    # TODO Once we have a consistent casting policy, we could simply\n    # use true_div.\n    if s.dtype in (\'float16\', \'float32\', \'complex64\'):\n        shp = cast(shp, \'float32\')\n    else:\n        shp = cast(shp, \'float64\')\n\n    if axis is None:\n        axis = list(range(input.ndim))\n    elif isinstance(axis, (integer_types, np.integer)):\n        axis = [axis]\n    elif isinstance(axis, np.ndarray) and axis.ndim == 0:\n        axis = [int(axis)]\n    else:\n        axis = [int(a) for a in axis]\n\n    # This sequential division will possibly be optimized by Theano:\n    for i in axis:\n        s = true_div(s, shp[i])\n\n    # This can happen when axis is an empty list/tuple\n    if s.dtype != shp.dtype and s.dtype in discrete_dtypes:\n        s = cast(s, shp.dtype)\n\n    if dtype == \'float16\' or (dtype is None and input.dtype == \'float16\'):\n        s = cast(s, \'float16\')\n    s.name = \'mean\'\n    return s\n\n\n@constructor\ndef var(input, axis=None, ddof=0, keepdims=False, corrected=False):\n    """"""\n    Computes the variance along the given axis(es) of a tensor `input`.\n\n    Parameters\n    ----------\n    axis: None or int or (list of int) (see `Sum`)\n        Compute the variance along this axis of the tensor.\n        None means all axes (like numpy).\n    ddof: Degrees of freedom; 0 would compute the ML estimate, 1 would compute\n        the unbiased estimate.\n    keepdims : bool\n        If this is set to True, the axes which are reduced are\n        left in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original tensor.\n    corrected : bool\n        If this is set to True, the \'corrected_two_pass\' algorithm is\n        used to compute the variance.\n        Refer : http://www.cs.yale.edu/publications/techreports/tr222.pdf\n\n    Notes\n    -----\n    Default uses the two-pass algorithm (reference below).\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Two-pass_algorithm\n    Also supports \'corrected_two_pass\' algorithm (using the \'corrected\' flag)\n    which is numerically more stable. There exist other implementations that\n    offer better stability, but probably slower.\n\n    if isinstance(ddof, (bool)):\n        raise ValueError(\'Parameter keepdims is now at index 3: (input, \\\n                          axis=None, ddof=0, keepdims=False, corrected=False)\')\n\n    input_ndim = input.type.ndim\n    if axis is None:\n        axis = list(range(input_ndim))\n    elif isinstance(axis, (integer_types, np.integer)):\n        axis = [axis]\n    elif isinstance(axis, np.ndarray) and axis.ndim == 0:\n        axis = [int(axis)]\n    else:\n        axis = [int(a) for a in axis]\n\n    # compute the axis-wise mean\n    mean_input = mean(input, axis, keepdims=True)\n\n    # center the input\n    centered_input = input - mean_input\n\n    # return the mean sqr\n    two = constant(2, dtype=centered_input.dtype)\n    if ddof == 0:\n        v = mean((centered_input ** two), axis, keepdims=keepdims)\n    else:\n        shp = shape(input) - ddof\n        v = sum((centered_input ** two), axis=axis, keepdims=keepdims)\n        for i in axis:\n            v = true_div(v, shp[i])\n\n    # use \'corrected_two_pass\' algorithm\n    if corrected:\n        if ddof == 0:\n            error = mean(centered_input, axis, keepdims=keepdims) ** 2\n        else:\n            shp = shape(input) - ddof\n            shp_inp = shape(input)\n            error = sum(centered_input, axis=axis, keepdims=keepdims) ** 2\n            for i in axis:\n                error = true_div(error, shp[i] * shp_inp[i])\n        v = v - error\n\n    v.name = \'var\'\n    return v\n\n\n@constructor\ndef std(input, axis=None, ddof=0, keepdims=False, corrected=False):\n    """"""\n    Computes the standard deviation along the given axis(es) of a tensor `input`.\n\n    Parameters\n    ----------\n    axis: None or int or (list of int) (see `Sum`)\n        Compute the variance along this axis of the tensor.\n        None means all axes (like numpy).\n    ddof: Degrees of freedom; 0 would compute the ML estimate, 1 would compute\n        the unbiased estimate.\n    keepdims : bool\n        If this is set to True, the axes which are reduced are\n        left in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original tensor.\n    corrected : bool\n        If this is set to True, the \'corrected_two_pass\' algorithm is\n        used to compute the variance.\n        Refer : http://www.cs.yale.edu/publications/techreports/tr222.pdf\n\n    Notes\n    -----\n    It calls \'var()\' and \'var()\' uses the two-pass algorithm (reference below).\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Two-pass_algorithm\n    Function \'var()\' also supports \'corrected_two_pass\' algorithm (using the\n    \'corrected\' flag) which is numerically more stable. There exist other\n    implementations that offer better stability, but probably slower.\n\n    if isinstance(ddof, (bool)):\n        raise ValueError(\'Parameter keepdims is now at index 3: (input, \\\n                          axis=None, ddof=0, keepdims=False, corrected=False)\')\n\n    ret = sqrt(var(input=input, axis=axis, ddof=ddof,\n                   keepdims=keepdims, corrected=corrected))\n    ret.name = \'std\'\n    return ret\n\n\nclass Default(gof.Op):\n    """"""\n    Takes an input x and a default value.\n\n    If the input is not None, a reference to it is returned.\n    If the input is None, a copy of the default value is returned instead.\n    The input and the default must have exactly the same type.\n\n    """"""\n    view_map = {0: [0]}\n    __props__ = ()\n\n    def make_node(self, x, default):\n        x, default = as_tensor_variable(x), as_tensor_variable(default)\n        if x.type != default.type:\n            raise TypeError(\'Both default() arguments must have same type\',\n                            x, default)\n        return gof.Apply(self, [x, default], [default.type()])\n\n    def perform(self, node, inp, out_):\n        x, default = inp\n        out, = out_\n        if x is None:\n            # why copy?  Theano can\'t yet understand out[0] being a view of\n            # either x or y, so we can be a view of x, but only a copy of y.\n            out[0] = default.copy()\n        else:\n            out[0] = x\n\ndefault = Default()\nsetdefault = default  # legacy\n\n\n##########################\n# Arithmetics\n##########################\n@_scal_elemwise\ndef maximum(x, y):\n    """"""elemwise maximum. See max for the maximum in one tensor""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef minimum(x, y):\n    """"""elemwise minimum. See min for the minimum in one tensor""""""\n    # see decorator for function body\n\n\ndef div_proxy(x, y):\n    """"""Proxy for either true_div or int_div, depending on types of x, y.""""""\n    f = scal.int_or_true_div(\n        as_tensor_variable(x).dtype in discrete_dtypes,\n        as_tensor_variable(y).dtype in discrete_dtypes)\n    if f is scal.int_div:\n        return int_div(x, y)\n    else:\n        return true_div(x, y)\n\n\ndef divmod(x, y):\n    """"""elementvise divmod, using floor_div and mod_check""""""\n    return floor_div(x, y), mod_check(x, y)\n\n\n@_scal_elemwise\ndef add(a, *other_terms):\n    """"""elementwise addition""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef sub(a, b):\n    """"""elementwise subtraction""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef mul(a, *other_terms):\n    """"""elementwise multiplication""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef true_div(a, b):\n    """"""elementwise [true] division (inverse of multiplication)""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef int_div(a, b):\n    """"""elementwise [floor] division (inverse of multiplication)""""""\n    # see decorator for function body\n\n\n# floor_div and int_div are the same thing\nfloor_div = int_div\n\n\ndef ceil_intdiv(a, b):\n    """"""\n    Safely compute ceil(float_division(a, b)).\n\n    Works for all dtypes, but mostly useful when a and b are int.\n\n    """"""\n    # If a and b are int with not many significant bits, we could\n    # cast them to float to avoid doing the modulo. We do not know if this\n    # is faster or not. But this is not safe for int64 as the cast will\n    # lose precision.\n    # e.g.: cast(cast(a, scalar.upcast(a, \'float32\')) / b, scal.upcast(a, b))\n\n    # We cast for the case when a and b are uint*. Otherwise neq will\n    # force their upcast to int.\n    div = int_div(a, b)\n    ret = cast(neq(a % b, 0), div.dtype) + div\n    assert ret.dtype == scal.upcast(div.owner.inputs[0], div.owner.inputs[1])\n    return ret\n\n\ndef mod_check(x, y):\n    """"""Make sure we do not try to use complex numbers.""""""\n    if ((as_tensor_variable(x).dtype in complex_dtypes or\n         as_tensor_variable(y).dtype in complex_dtypes)):\n        # Currently forbidden.\n        raise scal.Mod.complex_error\n    else:\n        return mod(x, y)\n\n\n@_scal_elemwise\ndef mod(a, b):\n    """"""elementwise modulo""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef pow(a, b):\n    """"""elementwise power""""""\n    # see decorator for function body\n\n\n@_scal_elemwise\ndef clip(x, min, max):\n    """"""\n    Clip x to be between min and max.\n\n    Notes\n    -----\n    When `x` is equal to the boundaries, the output is considered\n    to be `x`, so at these points, the gradient of the cost wrt the output\n    will be propagated to `x`, not to `min` nor `max`. In other words,\n    on these points, the gradient wrt `x` will be equal to the gradient wrt\n    the output, and the gradient wrt `min` and `max` will be zero.\n\n    """"""\n    # see decorator for function body\n    # for grep: clamp, bound\n\npprint.assign(add, printing.OperatorPrinter(\'+\', -2, \'either\'))\npprint.assign(mul, printing.OperatorPrinter(\'*\', -1, \'either\'))\npprint.assign(sub, printing.OperatorPrinter(\'-\', -2, \'left\'))\npprint.assign(neg, printing.OperatorPrinter(\'-\', 0, \'either\'))\npprint.assign(true_div, printing.OperatorPrinter(\'/\', -1, \'left\'))\npprint.assign(int_div, printing.OperatorPrinter(\'//\', -1, \'left\'))\npprint.assign(pow, printing.OperatorPrinter(\'**\', 1, \'right\'))\n\n\n##########################\n# View Operations\n##########################\n\n\ndef extract_constant(x, elemwise=True, only_process_constants=False):\n    """"""\n    This function is basically a call to tensor.get_scalar_constant_value.\n\n    The main difference is the behaviour in case of failure. While\n    get_scalar_constant_value raises an TypeError, this function returns x,\n    as a tensor if possible. If x is a ScalarVariable from a\n    scalar_from_tensor, we remove the conversion. If x is just a\n    ScalarVariable, we convert it to a tensor with tensor_from_scalar.\n\n    """"""\n    try:\n        x = get_scalar_constant_value(x,\n                                      elemwise,\n                                      only_process_constants)\n    except NotScalarConstantError:\n        pass\n    if ((isinstance(x, scal.ScalarVariable) or\n         isinstance(x, scal.sharedvar.ScalarSharedVariable))):\n        if x.owner and isinstance(x.owner.op, ScalarFromTensor):\n            x = x.owner.inputs[0]\n        else:\n            x = tensor_from_scalar(x)\n    return x\n\n\ndef transpose(x, axes=None):\n    """"""\n    Reorder the dimensions of x. (Default: reverse them)\n\n    This is a macro around dimshuffle that matches the numpy.transpose function.\n\n    """"""\n    if axes is None:\n        axes = list(range((x.ndim - 1), -1, -1))\n    ret = DimShuffle(x.broadcastable, axes)(x)\n    if x.name and axes == list(range((x.ndim - 1), -1, -1)):\n        ret.name = x.name + \'.T\'\n    return ret\n\n\ndef batched_dot(a, b):\n    """"""\n    Compute the batched dot product of two variables:\n\n        batched_dot(a, b)[i] = dot(a[i], b[i])\n\n    Note that this batched_dot function does one of three things, in the\n    following sequence:\n\n        1.  If either a or b is a vector, it returns the batched elementwise\n            product without calling the Theano BatchedDot op.\n\n        2.  If both a and b have either 2 or 3 dimensions, it calls Theano\'s\n            BatchedDot op on a and b.\n\n        3.  If either a or b has more than 3 dimensions, it calls Theano\'s\n            batched_tensordot function with appropriate axes. The\n            batched_tensordot function expresses high-dimensional batched\n            dot products in terms of batched matrix-matrix dot products, so\n            it may be possible to futherize optimize for performance.\n    """"""\n    a, b = as_tensor_variable(a), as_tensor_variable(b)\n\n    if a.ndim == 0:\n        raise TypeError(""a must have at least one (batch) axis"")\n    elif b.ndim == 0:\n        raise TypeError(""b must have at least one (batch) axis"")\n    elif a.ndim == 1:\n        return a.dimshuffle(*([0] + [""x""] * (b.ndim - 1))) * b\n    elif b.ndim == 1:\n        return a * b.dimshuffle(*([0] + [""x""] * (a.ndim - 1)))\n    elif a.ndim > 3 or b.ndim > 3:\n        return batched_tensordot(\n            a, b, [[a.ndim - 1], [np.maximum(1, b.ndim - 2)]])\n    else:\n        # avoid circular import\n        return theano.tensor.blas.BatchedDot()(a, b)\n\n\ndef batched_tensordot(x, y, axes=2):\n    """"""\n    Compute a batched tensordot product.\n\n    A hybrid of batched_dot and tensordot, this function computes the\n    tensordot product between the two tensors, by iterating over the\n    first dimension to perform a sequence of tensordots.\n\n    Parameters\n    ----------\n    x : tensor\n        A Tensor with sizes e.g.: for 3D (dim1, dim3, dim2)\n    y : tensor\n        A Tensor with sizes e.g.: for 3D (dim1, dim2, dim4)\n    axes: int or array-like of length 2\n        If an integer, the number of axes to sum over.\n        If an array, it must have two array elements containing the axes to sum\n        over in each tensor.\n\n        If an integer i, it is converted to an array containing\n        the last i dimensions of the first tensor and the first\n        i dimensions of the second tensor (excluding the first\n        (batch) dimension):\n            axes = [list(range(a.ndim - i, b.ndim)), list(range(1,i+1))]\n\n        If an array, its two elements must contain compatible axes\n        of the two tensors. For example, [[1, 2], [2, 4]] means sum\n        over the 2nd and 3rd axes of a and the 3rd and 5th axes of b.\n        (Remember axes are zero-indexed!) The 2nd axis of a and the\n        3rd axis of b must have the same shape; the same is true for\n        the 3rd axis of a and the 5th axis of b.\n\n    Like tensordot, this function uses a series of dimshuffles and\n    reshapes to reduce the tensor dot product to a matrix or vector\n    dot product.  Finally, it calls batched_dot to compute the result.\n    """"""\n    return _tensordot_as_dot(x, y, axes, dot=batched_dot, batched=True)\n\n\ndef split(x, splits_size, n_splits, axis=0):\n    the_split = Split(n_splits)\n    return the_split(x, axis, splits_size)\n\n\nclass Split(Op):\n    """"""Partition a `TensorVariable` along some axis.\n\n    Examples\n    --------\n    >>> x = vector()\n    >>> splits = lvector()\n    You have to declare right away how many split_points there will be.\n    >>> ra, rb, rc = split(x, splits, n_splits = 3, axis = 0)\n    >>> f = function([x, splits], [ra, rb, rc])\n    >>> a, b, c = f([0,1,2,3,4,5], [3, 2, 1])\n    a == [0,1,2]\n    b == [3, 4]\n    c == [5]\n\n    len_splits = None\n    """"""A Split instance will have this many outputs, and require that\n    the splits argument to `perform` have exactly this many elements.\n    """"""\n    __props__ = (""len_splits"",)\n\n    def __init__(self, len_splits):\n        self.len_splits = int(len_splits)\n\n    def __str__(self):\n        return self.__class__.__name__ + ""{%s}"" % self.len_splits\n\n    def make_node(self, x, axis, splits):\n        """"""WRITEME""""""\n        x = as_tensor_variable(x)\n        axis = as_tensor_variable(axis)\n        splits = as_tensor_variable(splits)\n\n        if splits.type not in int_vector_types:\n            raise TypeError(\'splits must have type tensor.lvector\',\n                            splits.type)\n        if axis.type not in int_types:\n            raise TypeError(\'axis must have type lscalar\', axis.type)\n\n#         # The following lines are necessary if we allow splits of zero\n#         if isinstance(axis, gof.Constant):\n#             x = unbroadcast(x, int(axis.data))\n#         else:\n#             x = unbroadcast(x, *range(x.type.ndim))\n\n        inputs = [x, axis, splits]\n        outputs = [x.type() for i in xrange(self.len_splits)]\n\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inputs, outputs):\n        """"""WRITEME""""""\n        x, axis, splits = inputs\n        # in python 2.4, x.shape[numpy.asarray(1)] don\'t work.\n        if sys.version_info[0:2] == (2, 4) and axis.size == 1:\n            axis = int(axis)\n\n        try:\n            len_along_axis = x.shape[axis]\n        except Exception:\n            raise ValueError(\'Split.perform() with axis=(%s) is invalid\'\n                             \' for x.shape==(%s)\'\n                             % (axis, x.shape))\n        if len(splits) != self.len_splits:\n            raise ValueError(\'In Split.perform(), len(splits) != len_splits.\',\n                             (len(splits), self.len_splits))\n\n        if np.sum(splits) != len_along_axis:\n            raise ValueError(\'The splits sum to %s, expected %s\' %\n                             (np.sum(splits), len_along_axis))\n        if python_any([nb < 0 for nb in splits]):\n            raise ValueError(\'Split: you tried to make an ndarray with a \'\n                             \'negative number of elements.\')\n\n        # Checking is done, let\'s roll the splitting algorithm!\n        # Basically we step along the given axis of x, extracting\n        # subtensors of size splits[i] as we go along.\n\n        general_key = [slice(None, None, None) for s in x.shape]\n        lower_idx = 0\n        for i in xrange(self.len_splits):\n            upper_idx = lower_idx + splits[i]\n            general_key[axis] = slice(lower_idx, upper_idx, None)\n            outputs[i][0] = x.__getitem__(tuple(general_key)).copy()\n            lower_idx = upper_idx\n\n    def infer_shape(self, node, in_shapes):\n        axis = node.inputs[1]\n        splits = node.inputs[2]\n        shp_x, shp_axis, shp_splits = in_shapes\n        out_shapes = []\n        for i in xrange(self.len_splits):\n            temp = as_tensor_variable(shp_x)\n            temp = theano.tensor.subtensor.set_subtensor(temp[axis], splits[i])\n            temp = [temp[i] for i in xrange(len(shp_x))]\n            out_shapes.append(temp)\n        return out_shapes\n\n    def grad(self, inputs, g_outputs):\n        """"""Join the gradients along the axis that was used to split x.""""""\n        x, axis, n = inputs\n        outputs = self(*inputs, **dict(return_list=True))\n        # If all the output gradients are disconnected, then so are the inputs\n        if python_all([isinstance(g.type, DisconnectedType)\n                       for g in g_outputs]):\n            return [DisconnectedType()(),\n                    grad_undefined(self, 1, axis),\n                    grad_undefined(self, 2, n)]\n        # Else, we have to make them zeros before joining them\n        new_g_outputs = []\n        for o, g in zip(outputs, g_outputs):\n            if isinstance(g.type, DisconnectedType):\n                new_g_outputs.append(o.zeros_like())\n            else:\n                new_g_outputs.append(g)\n\n        return [join(axis, *new_g_outputs),\n                grad_undefined(self, 1, axis),\n                grad_undefined(self, 2, n)]\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None for i in self.len_splits]\n        return self.make_node(eval_points[0], *inputs[1:]).outputs\n\n    def c_code_cache_version(self):\n        return (2,)\n\n    def c_support_code(self):\n        return """"""\n        /* Return 1 if output has the correct shape. */\n        int split_output_shape_is_correct (\n            PyArrayObject* output, PyArrayObject* array_to_split, int axis_to_split, npy_intp split_size\n        ) {\n            return\n                PyArray_NDIM(output) == PyArray_NDIM(array_to_split)\n                && memcmp(\n                    PyArray_DIMS(output),\n                    PyArray_DIMS(array_to_split),\n                    axis_to_split * sizeof(npy_intp)\n                ) == 0\n                && memcmp(\n                    PyArray_DIMS(output) + axis_to_split + 1,\n                    PyArray_DIMS(array_to_split) + axis_to_split + 1,\n                    (PyArray_NDIM(array_to_split) - axis_to_split - 1) * sizeof(npy_intp)\n                ) == 0\n                && split_size == PyArray_DIM(output, axis_to_split);\n        }\n        """"""\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        if self.len_splits == 0:\n            # There are no outputs, then nothing to do.\n            return \'\'\n\n        # outputs_pointers lists the addresses of the pointers to the outputs.\n        outputs_pointers = \'&\' + (\', &\'.join(outputs))\n        x, axis, splits = inputs\n        fail = sub[\'fail\']\n        x_typenum = np.dtype(node.inputs[0].dtype).num\n        x_itemsize = np.dtype(node.inputs[0].dtype).itemsize\n        axis_dtype = node.inputs[1].type.dtype_specs()[1]\n        splits_dtype = node.inputs[2].type.dtype_specs()[1]\n        expected_splits_count = self.len_splits\n\n        return """"""\n        int ndim = PyArray_NDIM(%(x)s);\n        int axis = (int)(*(%(axis_dtype)s*)PyArray_GETPTR1(%(axis)s, 0));\n        int splits_count = PyArray_DIM(%(splits)s, 0);\n        npy_intp len_along_axis, sum_of_splits = 0, current_split_length = 0, current_split_start = 0;\n        npy_intp* split_dims = NULL;\n        PyObject* split_view = NULL;\n        npy_intp data_offset;\n        int i;\n        PyArrayObject** outputs[] = {%(outputs_pointers)s};\n\n        /* Check inputs. */\n\n        if (splits_count != %(expected_splits_count)s) {\n            PyErr_Format(PyExc_ValueError,\n                ""Split: splits count (%%d) != expected count (%%d)."", splits_count, %(expected_splits_count)s);\n            %(fail)s\n        }\n\n        if (axis < 0) {\n            axis += ndim;\n        }\n        if (axis < 0 || axis >= ndim) {\n            PyErr_Format(PyExc_IndexError, ""Split: invalid axis %%d for a %%d-D array."", axis, ndim);\n            %(fail)s\n        }\n        len_along_axis = PyArray_DIM(%(x)s, axis);\n\n        for (i = 0; i < splits_count; ++i) {\n            current_split_length = (npy_intp)(*(%(splits_dtype)s*)PyArray_GETPTR1(%(splits)s, i));\n            if (current_split_length < 0) {\n                PyErr_Format(PyExc_ValueError,\n                    ""Split: you try to take a negative number (%%ld) of elements."", current_split_length);\n                %(fail)s\n            }\n            sum_of_splits += current_split_length;\n        }\n        if (sum_of_splits != len_along_axis) {\n            PyErr_Format(PyExc_ValueError, ""Split: the splits sums to %%ld, expected %%ld."", sum_of_splits, len_along_axis);\n            %(fail)s\n        }\n\n        /* Check outputs. */\n\n        split_dims = (npy_intp*) malloc(ndim * sizeof(npy_intp));\n        if (split_dims == NULL) {\n            PyErr_NoMemory();\n            %(fail)s\n        }\n\n        memcpy(split_dims, PyArray_DIMS(%(x)s), ndim * sizeof(npy_intp));\n\n        for (i = 0; i < splits_count; ++i) {\n            PyArrayObject** output = outputs[i];\n            current_split_length = (npy_intp) (* (%(splits_dtype)s*) PyArray_GETPTR1(%(splits)s, i));\n            if (*output == NULL || !split_output_shape_is_correct(*output, %(x)s, axis, current_split_length)) {\n                Py_XDECREF(*output);\n                split_dims[axis] = current_split_length;\n                *output = (PyArrayObject*)PyArray_EMPTY(ndim, split_dims, %(x_typenum)s, PyArray_IS_F_CONTIGUOUS(%(x)s));\n                if (outputs == NULL) {\n                    PyErr_SetString(PyExc_RuntimeError, ""Split: unable to allocate an output."");\n                    free(split_dims);\n                    %(fail)s\n                }\n            }\n        }\n\n        /* Compute split. */\n\n        for (i = 0; i < splits_count; ++i) {\n            current_split_length = (npy_intp) (* (%(splits_dtype)s*) PyArray_GETPTR1(%(splits)s, i));\n            data_offset = PyArray_STRIDE(%(x)s, axis) * current_split_start;\n            split_dims[axis] = current_split_length;\n            split_view = PyArray_New(&PyArray_Type,\n                                    ndim, split_dims,\n                                    %(x_typenum)s,\n                                    PyArray_STRIDES(%(x)s),\n                                    PyArray_BYTES(%(x)s) + data_offset,\n                                    %(x_itemsize)s,\n                                    PyArray_FLAGS(%(x)s),\n                                    NULL);\n            if (split_view == NULL) {\n                PyErr_SetString(PyExc_RuntimeError, ""Split: unable to create a view for a split."");\n                free(split_dims);\n                %(fail)s\n            }\n            if (PyArray_CopyInto(*outputs[i], (PyArrayObject*)split_view) != 0) {\n                PyErr_SetString(PyExc_RuntimeError, ""Split: unable to copy a split view into the output."");\n                Py_XDECREF(split_view);\n                free(split_dims);\n                %(fail)s\n            }\n            Py_XDECREF(split_view);\n            current_split_start += current_split_length;\n        }\n\n        free(split_dims);\n        """""" % locals()\n\n\ndef addbroadcast(x, *axes):\n    """"""\n    Make the input broadcastable in the specified axes.\n\n    For example, addbroadcast(x, 0) will make the first dimension of\n    x broadcastable. When performing the function, if the length of\n    x along that dimension is not 1, a ValueError will be raised.\n\n    We apply the opt here not to pollute the graph especially during\n    the gpu optimization\n\n    Parameters\n    ----------\n    x : tensor_like\n        Input theano tensor.\n    axis : an int or an iterable object such as list or tuple of int values\n        The dimension along which the tensor x should be broadcastable.\n        If the length of x along these dimensions is not 1, a ValueError will\n        be raised.\n\n    Returns\n    -------\n    tensor\n        A theano tensor, which is broadcastable along the specified dimensions.\n\n    """"""\n    rval = Rebroadcast(*[(axis, True) for axis in axes])(x)\n    return theano.tensor.opt.apply_rebroadcast_opt(rval)\n\n\ndef unbroadcast(x, *axes):\n    """"""\n    Make the input impossible to broadcast in the specified axes.\n\n    For example, addbroadcast(x, 0) will make the first dimension\n    of x broadcastable. When performing the function, if the length\n    of x along that dimension is not 1, a ValueError will be raised.\n\n    We apply the opt here not to pollute the graph especially during\n    the gpu optimization\n\n    Parameters\n    ----------\n    x : tensor_like\n        Input theano tensor.\n    axis : an int or an iterable object such as list or tuple of int values\n        The dimension along which the tensor x should be unbroadcastable.\n        If the length of x along these dimensions is not 1, a ValueError will\n        be raised.\n\n    Returns\n    -------\n    tensor\n        A theano tensor, which is unbroadcastable along the specified dimensions.\n\n    """"""\n    rval = Rebroadcast(*[(axis, False) for axis in axes])(x)\n    return theano.tensor.opt.apply_rebroadcast_opt(rval)\n\n\ndef patternbroadcast(x, broadcastable):\n    """"""\n    Make the input adopt a specific broadcasting pattern.\n\n    Broadcastable must be iterable. For example,\n    patternbroadcast(x, (True, False)) will make the first\n    dimension of x broadcastable and the second dimension\n    not broadcastable, so x will now be a row.\n\n    We apply the opt here not to pollute the graph especially during the gpu\n    optimization.\n\n    Parameters\n    ----------\n    x : tensor_like\n        Input theano tensor.\n    broadcastable : an iterable object such as list or tuple of bool values\n        A set of boolean values indicating whether a dimension should be\n        broadcastable or not. If the length of x along these dimensions is\n        not 1, a ValueError will be raised.\n\n    Returns\n    -------\n    tensor\n        A theano tensor, which is unbroadcastable along the specified dimensions.\n\n    """"""\n    rval = Rebroadcast(*[(i, broadcastable[i])\n                         for i in xrange(len(broadcastable))])(x)\n    return theano.tensor.opt.apply_rebroadcast_opt(rval)\n\n\nclass Join(Op):\n    """"""\n    Concatenate multiple `TensorVariable`s along some axis.\n\n    The axis must be given as first argument. All tensors must have the same\n    shape along all dimensions other than this axis.\n    Of course, TensorVariable instances do not have a shape, so this error\n    cannot be caught until runtime.  See `perform()`.\n\n    See Also\n    --------\n    stack : For joins involving scalar values\n\n    Examples\n    --------\n    >>> x, y, z = tensor.matrix(), tensor.matrix(), tensor.matrix()\n    >>> u = tensor.vector()\n\n    >>> r = join(0, x, y, z)\n    >>> c = join(1, x, y, z)\n    >>> join(2, x, y, z)    # WRONG: the axis has to be an index into the shape\n    >>> join(0, x, u)       # WRONG: joined tensors must have the same rank\n\n    """"""\n    check_input = False\n    __props__ = (""view"",)\n\n    def __init__(self, view=-1):\n        self.view = view\n        if view != -1:\n            # since the first input is always the axis, the tensors\n            # start from index 1.\n            self.view_map = {0: [1 + view]}\n\n    def __str__(self):\n        if self.view == -1:\n            return self.__class__.__name__\n        else:\n            return ""%s{%s}"" % (\n                self.__class__.__name__,\n                "", "".join(""%s=%r"" % (p, getattr(self, p))\n                          for p in self.__props__))\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        if not hasattr(self, ""view""):\n            self.view = -1\n\n    def make_node(self, *axis_and_tensors):\n        """"""\n        Parameters\n        ----------\n        axis: an Int or integer-valued Variable\n        tensors\n            A variable number (but not zero) of tensors to\n            concatenate along the specified axis.  These tensors must have\n            the same shape along all dimensions other than this axis.\n\n        Returns\n        -------\n        A symbolic Variable\n            It has the same ndim as the input tensors, and the most inclusive\n            dtype.\n\n        """"""\n        axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]\n        if not tensors:\n            raise ValueError(\'Cannot join an empty list of tensors\')\n        as_tensor_variable_args = [as_tensor_variable(x) for x in tensors]\n\n        dtypes = [x.type.dtype for x in as_tensor_variable_args]\n        out_dtype = scal.upcast(*dtypes)\n\n        def output_maker(bcastable):\n            return tensor(dtype=out_dtype, broadcastable=bcastable)\n\n        return self._make_node_internal(\n            axis, tensors, as_tensor_variable_args, output_maker)\n\n    def _make_node_internal(self, axis, tensors,\n                            as_tensor_variable_args, output_maker):\n        if not python_all(targs.type.ndim for targs\n                          in as_tensor_variable_args):\n            raise TypeError(\'Join cannot handle arguments of dimension 0.\'\n                            \' For joining scalar values, see @stack\')\n        # Handle single-tensor joins immediately.\n        if len(as_tensor_variable_args) == 1:\n            bcastable = list(as_tensor_variable_args[0].type.broadcastable)\n        else:\n            # When the axis is fixed, a dimension should be\n            # broadcastable if at least one of the inputs is\n            # broadcastable on that dimension (see justification below),\n            # except for the axis dimension.\n            # Initialize bcastable all false, and then fill in some trues with\n            # the loops.\n            bcastable = [False] * len(\n                as_tensor_variable_args[0].type.broadcastable)\n            ndim = len(bcastable)\n            # Axis can also be a constant\n            if not isinstance(axis, integer_types):\n                try:\n                    # Note : `get_scalar_constant_value` returns a ndarray not\n                    # an int\n                    axis = int(get_scalar_constant_value(axis))\n\n                except NotScalarConstantError:\n                    pass\n            if isinstance(axis, integer_types):\n                # Basically, broadcastable -> length 1, but the\n                # converse does not hold. So we permit e.g. T/F/T\n                # joins, and if they fail at runtime they fail, but if\n                # they don\'t then it means that the argument where\n                # that broadcastable flag was False had length 1 along\n                # this dimension, and therefore this dimension should\n                # be broadcastable for the output.\n\n                if axis < -ndim:\n                    raise IndexError(""Join axis %d out of bounds [0, %d)"" %\n                                     (axis, ndim))\n                if axis < 0:\n                    axis += ndim\n\n                for x in as_tensor_variable_args:\n                    for current_axis, bflag in enumerate(x.type.broadcastable):\n                        # Constant negative axis can no longer be negative at\n                        # this point. It safe to compare this way.\n                        if current_axis == axis:\n                            continue\n                        if bflag:\n                            bcastable[current_axis] = True\n                try:\n                    bcastable[axis] = False\n                except IndexError:\n                    raise ValueError(\'Join argument ""axis"" is out of range\'\n                                     \' (given input dimensions)\')\n            else:\n                # When the axis may vary, no dimension can be guaranteed to be\n                # broadcastable.\n                bcastable = [False] * len(\n                    as_tensor_variable_args[0].type.broadcastable)\n\n        if not python_all([x.ndim == len(bcastable)\n                           for x in as_tensor_variable_args[1:]]):\n            raise TypeError(""Join() can only join tensors with the same ""\n                            ""number of dimensions."")\n\n        inputs = [as_tensor_variable(axis)] + list(as_tensor_variable_args)\n        if inputs[0].type not in int_types:\n            raise TypeError(\'Axis could not be cast to an integer type\',\n                            axis, inputs[0].type, int_types)\n\n        outputs = [output_maker(bcastable)]\n\n        node = Apply(self, inputs, outputs)\n        return node\n\n    def perform(self, node, axis_and_tensors, out_):\n        out, = out_\n        view = self.view\n        axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]\n        # we check these tensors for being empty.\n        if (view != -1) and np.all(\n                [tensor.shape[axis] == 0 for tensor in\n                 tensors[0:view] + tensors[view + 1:]]):\n            out[0] = tensors[view]\n\n        else:\n            ndim = tensors[0].ndim\n            if axis < -ndim:\n                raise IndexError(""Join axis %d out of bounds [0, %d)"" %\n                                 (axis, ndim))\n\n            out[0] = theano._asarray(np.concatenate(tensors, axis=axis),\n                                     dtype=node.outputs[0].type.dtype)\n\n    def c_code_cache_version(self):\n        return (5,)\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        axis, tensors = inputs[0], inputs[1:]\n        view = self.view\n        non_empty_tensor = tensors[view]\n        input_1 = tensors[0]\n        l = len(tensors)\n        out, = outputs\n        fail = sub[\'fail\']\n        adtype = node.inputs[0].type.dtype_specs()[1]\n        copy_to_list = []\n\n        for i, inp in enumerate(tensors):\n            copy_to_list.append(\n                """"""Py_INCREF(%s);\n                   PyList_SetItem(list, %s, (PyObject*)%s);""""""\n                % (inp, i, inp))\n\n        copy_inputs_to_list = \'\\n\'.join(copy_to_list)\n        n = len(tensors)\n\n        code = """"""\n        int axis = ((%(adtype)s *)PyArray_DATA(%(axis)s))[0];\n        PyObject* list = PyList_New(%(l)s);\n        %(copy_inputs_to_list)s\n        int tensors_lens_sum;\n        if(%(view)s != -1) {\n            tensors_lens_sum = 0;\n\n            for(int i=0; i < %(n)s; i++){\n                tensors_lens_sum += PyArray_DIM((PyArrayObject *)(PyList_GetItem(list, i)), axis);\n            }\n            tensors_lens_sum -= PyArray_DIM(%(non_empty_tensor)s, axis);\n        }\n        if(%(view)s != -1 && tensors_lens_sum == 0) {\n            Py_XDECREF(%(out)s);\n            Py_INCREF(%(non_empty_tensor)s);\n            %(out)s = %(non_empty_tensor)s;\n        }else{\n            //PyObject* PyArray_Concatenate(PyObject* obj, int axis)\n            int ndim = PyArray_NDIM(%(input_1)s);\n            if( axis < -ndim ){\n                PyErr_Format(PyExc_IndexError,\n                             ""Join axis %%d out of bounds [0, %%d)"", axis, ndim);\n                %(fail)s\n            }\n            Py_XDECREF(%(out)s);\n            %(out)s = (PyArrayObject *)PyArray_Concatenate(list, axis);\n            Py_DECREF(list);\n            if(!%(out)s){\n                %(fail)s\n            }\n        }\n        """""" % locals()\n        return code\n\n    def R_op(self, inputs, eval_points):\n        if None in eval_points[1:]:\n            return [None]\n        return self.make_node(inputs[0], *eval_points[1:]).outputs\n\n    def grad(self, axis_and_tensors, grads):\n        """""" The gradient wrt a join op is a `Split`, used to partition\n        the gradient along the `axis` which was used for joining.\n        """"""\n        gz, = grads\n        axis, tensors = axis_and_tensors[0], axis_and_tensors[1:]\n\n        rval = [grad_undefined(self, 0, axis)]\n\n        dtypes = [as_tensor_variable(x).type.dtype for x in tensors]\n        out_dtype = scal.upcast(*dtypes)\n\n        if \'float\' in out_dtype or \'complex\' in out_dtype:\n            # assume that this is differentiable\n            split = Split(len(tensors))\n            split_gz = split(gz, axis, stack([shape(x)[axis]\n                                              for x in tensors]))\n            # If there is only one split, it might not be in a list.\n            if not isinstance(split_gz, list):\n                split_gz = [split_gz]\n            # Split.make_node isn\'t always able to infer the right\n            # broadcast. As the grad need to keep the information,\n            # read it if needed.\n            split_gz = [patternbroadcast(g, t.broadcastable)\n                        for t, g in zip(tensors, split_gz)]\n            rval = rval + split_gz\n        else:\n            # the output has integer type, so the gradient through it\n            # is 0\n            rval = rval + [tensor.zeros_like(dtype=config.floatX)\n                           for tensor in tensors]\n\n    def infer_shape(self, node, ishapes):\n        # ishapes[0] contains the size of the axis on which we join\n        # Join op should get at least one input to join\n        assert len(ishapes) > 1\n        n_dim = len(ishapes[1])\n        for shp in ishapes[1:]:\n            assert shp is not None\n            assert len(shp) == n_dim\n\n        # The joining dimension could be negative, but we need it to be\n        # in [0, n_dim) in the loop below.\n        # An axis < -n_dim or >= ndim would be invalid, but this is\n        # not checked here. An Assert op would be a way of addressing that,\n        # but it may disrupt optimizations.\n        join_dim = switch(ge(node.inputs[0], 0),\n                          node.inputs[0],\n                          node.inputs[0] + n_dim)\n        out_shapes = []\n        for dim in xrange(n_dim):\n            # we have to deal with 2 possible cases in here :\n            #   a) we are dealing with the dimension for which we join\n            #     (called t_side from true side of the if, where the if\n            #     compares current dimension with the joining dimension)\n            #   b) a non joining dimension ( in which maybe a symbolic\n            #      assertion can be used to make sure all tensors have\n            #      the same number of elements on this non-joined dimension\n            #      this is f_side\n            # initialize\n            t_side = ishapes[1][dim]\n            f_side = ishapes[1][dim]\n            # loop over tensors and sum for the joining dimension\n            for shp in ishapes[2:]:\n                t_side = t_side + shp[dim]\n            # return the dimensions found\n            out_shapes.append(switch(eq(dim, join_dim),\n                              t_side, f_side))\n\n        return [tuple(out_shapes)]\n\n\njoin_ = Join()\npprint.assign(Join, printing.FunctionPrinter(\'join\'))\n\n\ndef join(axis, *tensors_list):\n    """"""\n    Convenience function to concatenate `TensorType`s along the given axis.\n\n    This function will not add the op in the graph when it is not useful.\n    For example, in the case that the list of tensors to be concatenated\n    is one, it will just return the tensor.\n\n    Parameters\n    ----------\n    tensors : list of tensors (or list-like)\n        A list of tensors to be concatenated along the given axis.\n        The shapes of the tensors to be concatenated must be all\n        identical, except in the dimension (`axis`) on which they are to\n        be joined.\n    axis : int (symbolic or literal)\n        On which dimension should the tensors be joined?  The `axis`\n        must be a valid index into the shape of the tensors to be\n        concatenated.\n        The `axis` parameter may either be an integer or an object that\n        can be converted to a scalar using `as_scalar`(`axis`). In the\n        former case, the axis is fixed at construction, while in the\n        latter it may vary over time depending on the value of the\n        `axis` variable.\n    """"""\n    if len(tensors_list) == 1:\n        return tensors_list[0]\n    else:\n        return join_(axis, *tensors_list)\n\n\ndef roll(x, shift, axis=None):\n    """"""\n    Convenience function to roll TensorTypes along the given axis.\n\n    Syntax copies numpy.roll function.\n\n    Parameters\n    ----------\n    x : tensor_like\n        Input tensor.\n    shift : int (symbolic or literal)\n        The number of places by which elements are shifted.\n    axis : int (symbolic or literal), optional\n        The axis along which elements are shifted. By default, the array\n        is flattened before shifting, after which the original\n        shape is restored.\n\n    Returns\n    -------\n    tensor\n        Output tensor, with the same shape as ``x``.\n\n    """"""\n    if axis is None:\n        if x.ndim > 1:\n            y = x.flatten()\n            return roll(y, shift, axis=0).reshape(x.shape)\n        else:\n            axis = 0\n\n    if axis < 0:\n        axis += x.ndim\n\n    # Shift may be larger than the size of the axis. If so, since the\n    # roll operation is cyclic, we can take the shift modulo the size\n    # of the axis\n    shift = shift % x.shape[axis]\n\n    # A slice of all elements in a dimension \':\'\n    allslice = slice(None)\n    # List of slices describing the front half [:, :, shift:, :]\n    front_slice = slice(-shift, None)\n    front_list = ([allslice] * axis + [front_slice] +\n                  [allslice] * (x.ndim - axis - 1))\n    # List of slices describing the back half [:, :, :shift, :]\n    end_slice = slice(0, -shift)\n    end_list = ([allslice] * axis + [end_slice] +\n                [allslice] * (x.ndim - axis - 1))\n    return join(axis,\n                x.__getitem__(tuple(front_list)),\n                x.__getitem__(tuple(end_list)))\n\n\n@constructor\ndef shape_padleft(t, n_ones=1):\n    """"""Reshape `t` by left-padding the shape with `n_ones` 1s.\n\n    See Also\n    --------\n    shape_padaxis\n    shape_padright\n    Dimshuffle\n\n    """"""\n    _t = as_tensor_variable(t)\n\n    pattern = [\'x\'] * n_ones + [i for i in xrange(_t.type.ndim)]\n    return DimShuffle(_t.broadcastable, pattern)(_t)\n\n\n@constructor\ndef shape_padright(t, n_ones=1):\n    """"""Reshape `t` by right-padding the shape with `n_ones` 1s.\n\n    See Also\n    --------\n    shape_padaxis\n    shape_padleft\n    Dimshuffle\n\n    """"""\n    _t = as_tensor_variable(t)\n\n    pattern = [i for i in xrange(_t.type.ndim)] + [\'x\'] * n_ones\n    return DimShuffle(_t.broadcastable, pattern)(_t)\n\n\n@constructor\ndef shape_padaxis(t, axis):\n    """"""Reshape `t` by inserting 1 at the dimension `axis`.\n\n    Example\n    -------\n    >>> tensor = theano.tensor.tensor3()\n    >>> theano.tensor.shape_padaxis(tensor, axis=0)\n    DimShuffle{x,0,1,2}.0\n    >>> theano.tensor.shape_padaxis(tensor, axis=1)\n    DimShuffle{0,x,1,2}.0\n    >>> theano.tensor.shape_padaxis(tensor, axis=3)\n    DimShuffle{0,1,2,x}.0\n    >>> theano.tensor.shape_padaxis(tensor, axis=-1)\n    DimShuffle{0,1,2,x}.0\n\n    See Also\n    --------\n    shape_padleft\n    shape_padright\n    Dimshuffle\n\n    """"""\n    _t = as_tensor_variable(t)\n\n    ndim = _t.ndim + 1\n    if not -ndim <= axis < ndim:\n        msg = \'axis {0} is out of bounds [-{1}, {1})\'.format(axis, ndim)\n        raise IndexError(msg)\n    if axis < 0:\n        axis += ndim\n\n    pattern = [i for i in xrange(_t.type.ndim)]\n    pattern.insert(axis, \'x\')\n    return DimShuffle(_t.broadcastable, pattern)(_t)\n\n\n@constructor\ndef stack(*tensors, **kwargs):\n    """"""Stack tensors in sequence on given axis (default is 0).\n\n    Take a sequence of tensors and stack them on given axis to make a single\n    tensor. The size in dimension `axis` of the result will be equal to the number\n    of tensors passed.\n\n    Note: The interface stack(*tensors) is deprecated, you should use\n    stack(tensors, axis=0) insted.\n\n    Parameters\n    ----------\n    tensors : list or tuple of tensors\n        A list of tensors to be stacked.\n    axis : int\n        The index of the new axis. Default value is 0.\n\n    Examples\n    --------\n    >>> a = theano.tensor.scalar()\n    >>> b = theano.tensor.scalar()\n    >>> c = theano.tensor.scalar()\n    >>> x = theano.tensor.stack([a, b, c])\n    >>> x.ndim # x is a vector of length 3.\n    1\n    >>> a = theano.tensor.tensor4()\n    >>> b = theano.tensor.tensor4()\n    >>> c = theano.tensor.tensor4()\n    >>> x = theano.tensor.stack([a, b, c])\n    >>> x.ndim # x is a 5d tensor.\n    5\n    >>> rval = x.eval(dict((t, np.zeros((2, 2, 2, 2))) for t in [a, b, c]))\n    >>> rval.shape # 3 tensors are stacked on axis 0\n    (3, 2, 2, 2, 2)\n    >>> x = theano.tensor.stack([a, b, c], axis=3)\n    >>> x.ndim\n    5\n    >>> rval = x.eval(dict((t, np.zeros((2, 2, 2, 2))) for t in [a, b, c]))\n    >>> rval.shape # 3 tensors are stacked on axis 3\n    (2, 2, 2, 3, 2)\n    >>> x = theano.tensor.stack([a, b, c], axis=-2)\n    >>> x.ndim\n    5\n    >>> rval = x.eval(dict((t, np.zeros((2, 2, 2, 2))) for t in [a, b, c]))\n    >>> rval.shape # 3 tensors are stacked on axis -2\n    (2, 2, 2, 3, 2)\n    """"""\n    # ---> Remove this when moving to the new interface:\n    if not tensors and not kwargs:\n        raise Exception(\'theano.tensor.stack(tensors, axis) must have at least\'\n                        \' one parameter\')\n\n    if not kwargs and not isinstance(tensors[0], (list, tuple)):\n        warnings.warn(\'stack(*tensors) interface is deprecated, use\'\n                      \' stack(tensors, axis=0) instead.\', DeprecationWarning,\n                      stacklevel=3)\n        axis = 0\n    elif \'tensors\' in kwargs:\n        tensors = kwargs[\'tensors\']\n        if \'axis\' in kwargs:\n            axis = kwargs[\'axis\']\n        else:\n            axis = 0\n    else:\n        if len(tensors) == 2:\n            axis = tensors[1]\n        elif \'axis\' in kwargs:\n            axis = kwargs[\'axis\']\n        else:\n            axis = 0\n        tensors = tensors[0]\n    # <--- Until here.\n\n    if len(tensors) == 0:\n        raise Exception(\'tensors is empty. You should at least provide one\'\n                        \' tensor to theano.tensor.stack(tensors, axis).\')\n\n    # If all tensors are scalars of the same type, call make_vector.\n    # It makes the graph simpler, by not adding DimShuffles and Rebroadcasts\n\n    # This should be an optimization!\n    # Doing it here make the graph less canonicalized\n    # (more type need to be understood by all optimization)\n    # And DebugMode can\'t detect error in this code as it is not in an\n    # optimization.\n    # See ticket #660\n    if np.all(\n        [  # in case there is direct int in tensors.\n            isinstance(t, (np.number, float, integer_types,\n                           python_complex)) or\n            (isinstance(t, Variable) and\n             isinstance(t.type, TensorType) and\n             t.ndim == 0)\n            for t in tensors]):\n        # in case there is direct int\n        tensors = list(map(as_tensor_variable, tensors))\n        dtype = scal.upcast(*[i.dtype for i in tensors])\n        return theano.tensor.opt.MakeVector(dtype)(*tensors)\n    return join(axis, *[shape_padaxis(t, axis) for t in tensors])\n\n\n@constructor\ndef concatenate(tensor_list, axis=0):\n    """"""Alias for `join`(axis, *tensor_list).\n\n    This function is similar to `join`, but uses the signature of\n    numpy\'s concatenate function.\n\n    Raises\n    ------\n    TypeError\n        The tensor_list must be a tuple or list.\n\n    """"""\n    # Check someone did not make the common mistake to do something like:\n    #   c = concatenate(x, y)\n    # instead of\n    #   c = concatenate((x, y))\n    if not isinstance(tensor_list, (tuple, list)):\n        raise TypeError(\n            ""The \'tensors\' argument must be either a tuple ""\n            ""or a list, make sure you did not forget () or [] around ""\n            ""arguments of concatenate."", tensor_list)\n    return join(axis, *tensor_list)\n\n\ndef get_vector_length(v):\n    """"""Return the run-time length of a symbolic vector.\n\n    Parameters\n    ----------\n    v\n        A rank-1 TensorType variable.\n\n    Raises\n    ------\n    TypeError\n        `v` hasn\'t the proper type.\n    ValueError\n        No special case applies, the length is not known.\n        In general this is not possible, but for a number of special cases\n        the length can be determined at compile / graph-construction time.\n        This function implements these special cases.\n\n    """"""\n    v = as_tensor_variable(v)\n    if v.ndim != 1:\n        raise TypeError(""argument must be symbolic vector, got \'%s\'"" %\n                        v)\n    if v.type.broadcastable[0]:\n        return 1\n    if isinstance(v, gof.Constant) and v.type.ndim == 1:\n        return len(v.data)\n    if v.owner and isinstance(v.owner.op, theano.tensor.opt.MakeVector):\n        return len(v.owner.inputs)\n    if v.owner and isinstance(v.owner.op, Shape):\n        return v.owner.inputs[0].type.ndim\n    # If we take a slice, we know how many elements it will result in\n    if ((v.owner and\n         isinstance(v.owner.op, theano.tensor.subtensor.Subtensor) and\n         isinstance(v.owner.op.idx_list[0], slice) and\n         v.owner.inputs[0].owner and\n         isinstance(v.owner.inputs[0].owner.op, theano.compile.ops.Shape))):\n        start = extract_constant(theano.tensor.subtensor.get_idx_list(\n            v.owner.inputs, v.owner.op.idx_list)[0].start)\n        stop = extract_constant(theano.tensor.subtensor.get_idx_list(\n            v.owner.inputs, v.owner.op.idx_list)[0].stop)\n        step = extract_constant(theano.tensor.subtensor.get_idx_list(\n            v.owner.inputs, v.owner.op.idx_list)[0].step)\n\n        ndim = v.owner.inputs[0].owner.inputs[0].ndim\n        types = (numbers.Integral, np.integer)\n        if start is None:\n            start = 0\n        elif isinstance(start, types) and start < 0:\n            start += ndim\n            if start < 0:\n                start = 0\n        if stop is None:\n            stop = ndim\n        elif isinstance(stop, types):\n            if stop > ndim:\n                stop = ndim\n            elif stop < 0:\n                stop += ndim\n        if step is None:\n            step = 1\n\n        if (isinstance(stop, types) and\n                isinstance(start, types) and\n                isinstance(step, types) and\n                start >= 0 and stop >= 0 and\n                step > 0 and stop >= start):\n            return (stop - start - 1) // step + 1\n    if isinstance(v, Variable):\n        msg = theano.printing.debugprint(v, file=\'str\')\n    else:\n        msg = str(v)\n    raise ValueError(""length not known: %s"" % msg)\n\n\n@constructor\ndef horizontal_stack(*args):\n    """"""\n    Horizontally stack two L{TensorType}s.\n\n    Stack two L{TensorType}s along the second axis (column wise). These\n    L{TensorType}s must have the same shape along all dimensions but the\n    second.\n\n    """"""\n    # Note: \'horizontal_stack\' and \'vertical_stack\' do not behave exactly like\n    # Numpy\'s hstack and vstack functions. This is intended, because Numpy\'s\n    # functions have potentially confusing/incoherent behavior (try them on 1D\n    # arrays). If this is fixed in a future version of Numpy, it may be worth\n    # trying to get closer to Numpy\'s way of doing things. In the meantime,\n    # better keep different names to emphasize the implementation divergences.\n    assert len(args) >= 2\n    for arg in args:\n        assert arg.type.ndim == 2\n    return concatenate(args, axis=1)\n\n\n@constructor\ndef vertical_stack(*args):\n    assert len(args) >= 2\n    for arg in args:\n        assert arg.type.ndim == 2\n    return concatenate(args, axis=0)\n\n\nclass Reshape(Op):\n    """"""Perform a reshape operation of the input x to the new shape shp.\n    The number of dimensions to which to reshape to (ndim) must be\n    known at graph build time.\n    """"""\n    view_map = {0: [0]}  # output 0 is potentially aliased to inputs [0]\n    _f16_ok = True\n\n    check_input = False\n    __props__ = (""ndim"",)\n    params_type = ParamsType(ndim=int32)\n    # name does not participate because it doesn\'t affect computations\n\n    def __init__(self, ndim, name=None):\n        self.ndim = int(ndim)\n        if ndim < 0:\n            raise ValueError(""The output dimensions after reshape must be 0 or greater"")\n        assert name is None, \'name attribute for Reshape has been deprecated\'\n\n    def __str__(self):\n        return \'%s{%s}\' % (self.__class__.__name__, self.ndim)\n\n    def make_node(self, x, shp):\n        x = as_tensor_variable(x)\n        shp_orig = shp\n        shp = as_tensor_variable(shp, ndim=1)\n        if not (shp.dtype in int_dtypes or\n                (isinstance(shp, TensorConstant) and shp.data.size == 0)):\n            # It raises an error if shp is not of integer type,\n            # except when shp is constant and empty\n            # (in this case, shp.dtype does not matter anymore).\n            raise TypeError(""Shape must be integers"", shp, shp.dtype)\n        assert shp.ndim == 1\n        if isinstance(shp, TensorConstant):\n            bcast = [s == 1 for s in shp.data]\n            return gof.Apply(self, [x, shp], [tensor(x.type.dtype, bcast)])\n        else:\n            bcasts = [False] * self.ndim\n            shp_list = shp_orig\n            if hasattr(shp_orig, ""ndim"") and shp_orig.ndim == 0:\n                shp_list = [shp_orig]\n            for index in xrange(self.ndim):\n                y = shp_list[index]\n                y = as_tensor_variable(y)\n                # Try to see if we can infer that y has a constant value of 1.\n                # If so, that dimension should be broadcastable.\n                try:\n                    bcasts[index] = (\n                        hasattr(y, \'get_scalar_constant_value\') and\n                        y.get_scalar_constant_value() == 1)\n                except NotScalarConstantError:\n                    pass\n            return gof.Apply(self, [x, shp], [tensor(x.type.dtype, bcasts)])\n\n    def perform(self, node, inp, out_, params):\n        x, shp = inp\n        out, = out_\n        if (len(shp) != self.ndim):\n            raise ValueError(\'shape argument to Reshape.perform has incorrect\'\n                             \' length %i\'\n                             \', should be %i\' % (len(shp), self.ndim), shp)\n        try:\n            out[0] = np.reshape(x, shp)\n        except Exception:\n            raise ValueError(\'Cannot reshape input of shape %s to shape %s\' %\n                             (x.shape, shp))\n\n    def connection_pattern(self, node):\n        return [[True], [False]]\n\n    def grad(self, inp, grads):\n        x, shp = inp\n        g_out, = grads\n        return [reshape(g_out, shape(x), ndim=x.ndim),\n                DisconnectedType()()]\n\n    def R_op(self, inputs, eval_points):\n        if eval_points[0] is None:\n            return [None]\n        return self(eval_points[0], *inputs[1:], **dict(return_list=True))\n\n    def infer_shape(self, node, ishapes):\n        # inputs[1] can contain at most one value of \'-1\', meaning the actual\n        # shape of the output will be automatically computed by reshape, so\n        # that the total number of elements stays the same.\n        # TODO: Maybe put that formula here?\n        # It\'s not trivial, because we would have to check if the product of\n        # all the non-minus-one shapes is a divisor of the product of the\n        # original shapes.\n\n        # The following expression leads to cycles in feature_shape,\n        # because it tries to replace the Shape_i node by the switch\n        # statement, which depends on Shape_i.\n        # return [tuple([switch(eq(node.inputs[1][i], -1),\n        #                      theano.tensor.opt.Shape_i(i)(node.outputs[0]),\n        #                      node.inputs[1][i])\n        #                    for i in xrange(self.ndim)]\n        #    )]\n\n        # Here, we only simplify if the shape (node.inputs[1]) is a constant,\n        # ideally it would suffice to check that it is always non-negative.\n\n        # If current variable is a scalar and its dimensionality should\n        # change to self.ndim, then use size 1 for all new dimensions.\n        if len(ishapes[0]) == 0:\n            return [(1,) * self.ndim]\n\n        requ = node.inputs[1]\n        input_size = mul(*ishapes[0])\n        if isinstance(requ, theano.tensor.TensorConstant):\n            requ = list(requ.data)\n            requ_part = [ele for ele in requ if ele != -1]\n            crit = len(requ) - len(requ_part)\n            if crit == 1 and len(requ_part) > 0:\n                # If there are both 0 and -1 in requ_size, it is impossible\n                # to determine a right output, but we can at least prevent\n                # a division by 0. We do not want to keep a negative\n                # size here as it could lead to further weird errors\n                # after other optimizations.\n                requ_size = mul(*requ_part)\n                missing = input_size // (1 if requ_size == 0 else requ_size)\n                for i, ele in enumerate(requ):\n                    if ele == -1:\n                        requ[i] = missing\n            elif crit == 1:  # we reshape to -1\n                requ = [input_size] if ishapes[0] else [1]\n            elif crit > 1:\n                raise ValueError(\'shape argument to Reshape.perform\'\n                                 \' must have at most one entry equal to -1\')\n            return [requ]\n        else:\n            requ = [requ[i] for i in xrange(self.ndim)]\n            # since new_dims can have negative value (-1), the\n            # multiplication of all values should be negated\n            # to give a positive value.\n            # To avoid optimization complexity, we avoid checking\n            # for the case when there are two or more \'-1\' values.\n            if self.ndim:\n                requ_size = -mul(*requ)\n                # If there are both 0 and -1 in requ_size, it is impossible\n                # to determine a right output, but we can at least prevent\n                # a division by 0. We do not want to keep a negative\n                # size here as it could lead to further weird errors\n                # after other optimizations.\n                rest_size = input_size // maximum(requ_size, 1)\n            return [tuple([switch(eq(requ[i], -1),\n                                  rest_size,\n                                  requ[i])\n                           for i in xrange(self.ndim)])]\n\n    def c_code_cache_version(self):\n        return (8,)\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        if isinstance(node.inputs[0], TensorVariable):\n            x, shp = inputs\n            z, = outputs\n            sdtype = node.inputs[1].type.dtype_specs()[1]\n            fail = sub[\'fail\']\n            params = sub[\'params\']\n            return """"""\n            assert (PyArray_NDIM(%(shp)s) == 1);\n            npy_intp new_dims[%(params)s->ndim];\n            PyArray_Dims newshape;\n            newshape.ptr = new_dims;\n            newshape.len = %(params)s->ndim;\n            for (int ii = 0; ii < %(params)s->ndim; ++ii)\n            {\n                // -- We do not want an explicit cast here. the shp can be any\n                // -- int* dtype. The compiler will explicitly upcast it, but\n                // -- will err if this will downcast. This could happen if the\n                // -- user pass an int64 dtype, but npy_intp endup being int32.\n                new_dims[ii] = ((%(sdtype)s*)(\n                        PyArray_BYTES(%(shp)s) +\n                        ii * PyArray_STRIDES(%(shp)s)[0]))[0];\n            }\n            Py_XDECREF(%(z)s);\n            %(z)s = (PyArrayObject *) PyArray_Newshape(%(x)s, &newshape, NPY_CORDER);\n            if (!%(z)s)\n            {\n                //The error message should have been set by PyArray_Newshape\n                %(fail)s;\n            }\n            """""" % locals()\n        else:\n            return Op.c_code(self, node, name, inputs, outputs, sub)\n\n\ndef reshape(x, newshape, ndim=None):\n    if ndim is None:\n        newshape = as_tensor_variable(newshape)\n        if newshape.ndim != 1:\n            raise TypeError(\n                ""New shape in reshape must be a vector or a list/tuple of""\n                "" scalar. Got %s after conversion to a vector."" % newshape)\n        try:\n            ndim = get_vector_length(newshape)\n        except ValueError:\n            raise ValueError(\n                ""The length of the provided shape (%s) cannot ""\n                ""be automatically determined, so Theano is not able ""\n                ""to know what the number of dimensions of the reshaped ""\n                ""variable will be. You can provide the \'ndim\' keyword ""\n                ""argument to \'reshape\' to avoid this problem."" % newshape)\n    op = Reshape(ndim)\n    rval = op(x, newshape)\n    return rval\n\n\nclass Flatten(Op):\n    """"""\n    Flatten a tensor.\n\n    Flattens a tensor to `outdim` dimensions by preserving the leading\n    outdim - 1 shape components.\n\n    .. note:: The interface Flatten(Op) is deprecated, you should use flatten.\n    """"""\n    view_map = {0: [0]}\n\n    check_input = False\n    __props__ = (""outdim"",)\n\n    def __init__(self, outdim=1):\n        warnings.warn(\n            ""Flatten class is deprecated, ""\n            ""please use flatten method instead."",\n            DeprecationWarning,\n            stacklevel=4)\n        self.outdim = int(outdim)\n\n    def __str__(self):\n        return \'%s{%s}\' % (self.__class__.__name__, self.outdim)\n\n    def make_node(self, x):\n        t_x = as_tensor_variable(x)\n        if self.outdim < 1 or (x.ndim and self.outdim > x.ndim):\n            raise ValueError(\'invalid output ndimensions (%i) for tensor of \'\n                             \'rank %i\' % (self.outdim, t_x.ndim))\n\n        # Infer the broadcastable pattern of the output. For every dimension\n        # unaffected by the flatten, the broadcast flag should be unchanged.\n        # For the dimension resulting from the collapse of other dimensions,\n        # it should be broadcastable iff all the collapsed dimensions were\n        # broadcastable.\n        bcast_kept_dims = x.broadcastable[:self.outdim - 1]\n        bcast_new_dim = python_all(x.broadcastable[self.outdim - 1:])\n        broadcastable = bcast_kept_dims + (bcast_new_dim,)\n\n        return gof.Apply(self, [t_x], [tensor(x.type.dtype,\n                                              broadcastable)])\n\n    def perform(self, node, inp, out_):\n        x, = inp\n        out, = out_\n        outdim = self.outdim\n        if outdim == 1:\n            try:\n                out[0] = x.reshape(x.size)\n            except AttributeError:\n                out[0] = x.reshape((np.prod(x.shape),))\n        elif outdim == len(x.shape):\n            out[0] = x\n        else:\n            newshape = (x.shape[:outdim - 1] +\n                        (np.prod(x.shape[outdim - 1:]),))\n            out[0] = x.reshape(newshape)\n\n    def infer_shape(self, node, in_shapes):\n        in_shp, = in_shapes\n        part1 = in_shp[:self.outdim - 1]\n        part2 = in_shp[self.outdim - 1:]\n\n        if len(part2) > 1:\n            part2 = (prod(part2, dtype=\'int64\'),)\n        elif len(part2) == 1:\n            # We do not want to force an upcast of part2 if its length is 1\n            pass\n        else:\n            if len(in_shp) == 0 and self.outdim == 1:\n                part2 = (1,)\n            else:\n                raise ValueError(\'invalid output ndimensions (%i) for tensor \'\n                                 \'of rank %i\' % (self.outdim, len(in_shp)))\n\n        out_shape = (part1 + part2)\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        x, = inp\n        g_out, = grads\n        return [reshape(g_out, shape(x), x.ndim)]\n\n    def R_op(self, inputs, eval_points):\n        if None in eval_points:\n            return [None]\n        return self.make_node(*eval_points).outputs\n\n    def c_code_cache_version(self):\n        return (1, 1)\n\n    def c_code(self, node, name, inputs, outputs, sub):\n        x, = inputs\n        out, = outputs\n        outdim = self.outdim\n        fail = sub[\'fail\']\n        return """"""\n        if (%(outdim)s == PyArray_NDIM(%(x)s))\n        {\n            Py_XDECREF(%(out)s);\n            Py_XINCREF(%(x)s);\n            %(out)s = %(x)s;\n        }\n        else\n        {\n            Py_XDECREF(%(out)s);\n\n            if (%(outdim)s == 1)\n            {\n                npy_intp size = PyArray_SIZE(%(x)s);\n                PyArray_Dims newshape;\n                newshape.ptr = &size;\n                newshape.len = 1;\n                %(out)s = (PyArrayObject*)PyArray_Newshape(%(x)s,\n                                                           &newshape,\n                                                           NPY_CORDER);\n            }\n            else\n            {\n                npy_intp *oldshape = PyArray_DIMS(%(x)s);\n                npy_intp newshape_dims[%(outdim)s];\n\n                int i;\n                for (i = 0; i < %(outdim)s - 1; ++i)\n                    newshape_dims[i] = oldshape[i];\n\n                newshape_dims[i] = 1;\n\n                for (int j = %(outdim)s - 1; j < PyArray_NDIM(%(x)s); ++j)\n                    newshape_dims[i] *= oldshape[j];\n\n                PyArray_Dims newshape;\n                newshape.ptr = newshape_dims;\n                newshape.len = %(outdim)s;\n                %(out)s = (PyArrayObject*)PyArray_Newshape(%(x)s,\n                                                           &newshape,\n                                                           NPY_CORDER);\n            }\n        }\n        if (!%(out)s)\n        {\n            //The error message should have been set by\n            // PyArray_Newshape\n            %(fail)s;\n        }\n        """""" % locals()\n\n\ndef is_flat(var, ndim=None, outdim=None):\n    """"""\n    Verifies the dimensionality of the var is equal to\n    outdim. This method is usually called after flatten method on a\n    variable, where the first outdim-1 dimension size(s) of the variable\n    is kept intact, and the last dimension size of the variable is made\n    equal to the multiplication of its remaining dimension size(s), such that\n    the variable would end up with as many dimension as outdim.\n\n    Parameters\n    ----------\n        var : theano.tensor.var.TensorVariable\n            the theano var on which the dimensionality is checked.\n\n        outdim : int\n            the expected dimensionality of var.\n\n    Returns\n    -------\n    bool\n        the comparison result of var\'s dim\n        and the expected outdim.\n    """"""\n    if outdim is None and ndim is None:\n        ndim = 1\n    elif outdim is not None and ndim is not None:\n        raise ValueError(""You should only specify ndim"")\n    elif outdim is not None:\n        warnings.warn(\n            ""flatten outdim parameter is deprecated, use ndim instead."")\n        ndim = outdim\n    return var.ndim == ndim\n\n\ndef flatten(x, ndim=None, outdim=None):\n    """"""\n    Reshapes the variable x by keeping\n    the first outdim-1 dimension size(s) of x the same,\n    and making the last dimension size of x equal to\n    the multiplication of its remaining dimension size(s).\n\n    Parameters\n    ----------\n        x : theano.tensor.var.TensorVariable\n            the variable that should be reshaped.\n\n        ndim : int\n            the number of dimensions of the returned variable\n            Default 1.\n        outdim : int\n            DEPRECATED synonym for ndim\n    Returns\n    -------\n    theano.tensor.var.TensorVariable\n        the flattend variable with dimensionality of outdim\n    """"""\n    if outdim is None and ndim is None:\n        ndim = 1\n    elif outdim is not None and ndim is not None:\n        raise ValueError(""You should only specify ndim"")\n    elif outdim is not None:\n        warnings.warn(\n            ""flatten outdim parameter is deprecated, use ndim instead."")\n\n        ndim = outdim\n    # Any input variable can be flattened to have ndim of 1,\n    # even if it\'s a scalar. Otherwise, ndim must be positive\n    # and smaller than x.ndim.\n    if ndim < 1 or (ndim > 1 and ndim > x.ndim):\n        raise ValueError(\'ndim %s out of bound [1, %d)\'\n                         % (ndim, x.ndim + 1))\n\n    if ndim > 1:\n        dims = tuple(x.shape[:ndim - 1]) + (-1,)\n    else:\n        dims = (-1,)\n    x_reshaped = x.reshape(dims)\n    bcast_kept_dims = x.broadcastable[:ndim - 1]\n    bcast_new_dim = python_all(x.broadcastable[ndim - 1:])\n    broadcastable = bcast_kept_dims + (bcast_new_dim,)\n    x_reshaped = theano.tensor.addbroadcast(\n        x_reshaped, *filter(lambda i: broadcastable[i], range(ndim)))\n    return x_reshaped\n\n\n# class TileGrad(Op):\n#     """"""\n#     Calculates the gradient of the Tile Op.\n#     """"""\n#     # this is so weird, I can\'t think of how to make this a general thing.\n#     def make_node(self, x, reps, g_out):\n#         return gof.Apply(self, [x, reps, g_out], [x.type()])\n#\n#     def perform(self, node, inp, out):\n#         x, reps, g_out = inp\n#         gx, = out\n#         xsh = x.shape\n#         if len(reps) == 2 and reps[1] == 1 and len(x.shape) == 1:\n#             gx[0] = numpy.sum(g_out, axis=0)\n#         else:\n#             raise NotImplementedError(\'x.shape, reps combination not \'\n#                                       \'supported\', (x.shape, reps))\n#\n# tilegrad = TileGrad()\n\n\nclass Tile(Op):\n    """"""\n    Construct an array by repeating the input x according to reps pattern.\n\n    .. note:: Deprecated\n              Use tile() instead.\n\n    Tiles its input according to reps. The length of reps is the number of\n    dimension of x and contains the number of times to tile x in each\n    dimension.\n\n    See Also\n    --------\n    numpy.tile : http://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html\n\n    """"""\n    __props__ = (""ndim"",)\n\n    def __init__(self, ndim):\n        self.ndim = ndim\n\n    def __str__(self):\n        return self.__class__.__name__ + ""{ndim=%d}"" % self.ndim\n\n    def make_node(self, x, reps):\n        warnings.warn((\n            ""Tile op is deprecated, use tile function instead.""), stacklevel=3)\n        x = as_tensor_variable(x)\n        reps = as_tensor_variable(reps)\n        return gof.Apply(self, [x, reps], [tensor(x.type.dtype, [False] *\n                                                  self.ndim)])\n\n    def perform(self, node, inp, out_):\n        x, reps = inp\n        out, = out_\n        res = np.tile(x, reps)\n        if res.ndim != self.ndim:\n            raise ValueError(\n                \'Tile.perform produced incorrect number of dimensions\')\n\n        if (np.asarray(reps) == 1).all():\n            # In that case, some NumPy version return a view!  As this\n            # op isn\'t declared as inplace, we need to check that and\n            # copy the data.\n            if np.may_share_memory(res, x):\n                res = res.copy()\n        out[0] = res\n\n    def infer_shape(self, node, in_shapes):\n        # Note: in contrast with numpy, it is assumed that x.shape and reps\n        # have equal length;  see also tile function below\n\n        # Note: if reps were to be allowed not to be a constant and x.shape\n        # and reps to be unequal, the following block of code could be used:\n        # prepend 1 to x.shape if needed\n        # if self.ndim > x.ndim:\n        # shp = concatenate(ones(self.ndim - x.ndim), shp)\n        # prepend 1 to reps if needed\n        # reps = concatenate(ones(self.ndim - reps.shape[0]), reps)\n\n        x, reps = node.inputs\n        shp = in_shapes[0]\n        tiled_shp = shp * reps\n        out_shape = []\n        for i in xrange(self.ndim):\n            out_shape.append(tiled_shp[i])\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        x, reps = inp\n        g_out, = grads\n        # return [tilegrad(x, reps, g_out), None]\n        raise NotImplementedError()\n\n\ndef tile(x, reps, ndim=None):\n    """"""\n    Tile input array `x` according to `reps`.\n\n    See the docstring of `numpy.tile` for details.\n\n    \'reps\' can be constant integer (e.g. 3), constant vector(e.g. [2 3]),\n    symbolic scalar (e.g. tensor.iscalar()), symbolic vector (e.g. tensor.ivector())\n    or a list of symbolic scalar (e.g. [tensor.iscalar(), tensor.iscalar()]).\n\n    ndim is the number of the dimensions of the output, if it is provided, ndim\n    should be equal or larger than x.ndim and len(reps), otherwise, we will use\n    max(x.ndim, len(reps)) as ndim. If reps is symbolic vector, the ndim has to\n    be provided.\n\n    if ndim is not None and ndim < x.ndim:\n        raise ValueError(""ndim should be equal or larger than x.ndim"")\n\n    # if reps is tensor.scalar, integer or tensor.vector, we convert it to a list.\n    if not isinstance(reps, (list, tuple)):\n        reps_astensor = as_tensor_variable(reps)\n        ndim_check = reps_astensor.ndim\n        if reps_astensor.dtype not in theano.tensor.discrete_dtypes:\n            raise ValueError(""elements of reps must be integer dtype"")\n\n        # tensor.scalar/integer case\n        if ndim_check == 0:\n            reps = [reps]\n\n        # tensor.vector case\n        elif ndim_check == 1:\n            if ndim is None:\n                raise ValueError(""if reps is tensor.vector, you should specify ""\n                                 ""the ndim"")\n            else:\n                offset = ndim - reps.shape[0]\n\n                # assert that reps.shape[0] does not exceed ndim\n                offset = theano.tensor.opt.assert_(offset, ge(offset, 0))\n\n                # if reps.ndim is less than x.ndim, we pad the reps with\n                # ""1"" so that reps will have the same ndim as x.\n                reps_ = [switch(i < offset, 1, reps[i - offset]) for i in range(ndim)]\n                reps = reps_\n\n        # other raise error\n        else:\n            raise ValueError(""the dimension of reps should not exceed 1"")\n    else:\n        if ndim is not None and len(reps) > ndim:\n            raise ValueError(""len(reps) should be equal or less than ndim"")\n        if not np.all([isinstance(r, integer_types) or\n                       (isinstance(r, TensorVariable) and\n                        r.dtype in theano.tensor.discrete_dtypes) for r in reps]):\n            raise ValueError(""elements of reps must be scalars of integer dtype"")\n\n    # if reps.ndim is less than x.ndim, we pad the reps with\n    # ""1"" so that reps will have the same ndim as x.\n    reps = list(reps)\n    if ndim is None:\n        ndim = builtins.max(len(reps), x.ndim)\n    if len(reps) < ndim:\n        reps = [1] * (ndim - len(reps)) + reps\n\n    shape = [1] * (ndim - x.ndim) + [x.shape[i] for i in xrange(x.ndim)]\n    alloc_shape = reps + shape\n    y = alloc(x, *alloc_shape)\n    shuffle_ind = np.arange(ndim * 2).reshape(2, ndim)\n    shuffle_ind = shuffle_ind.transpose().flatten()\n    y = y.dimshuffle(*shuffle_ind)\n    new_shapes = [sh * reps[i] for i, sh in enumerate(shape)]\n    y = y.reshape(new_shapes)\n\n\nclass ARange(Op):\n    """"""Create an array containing evenly spaced values within a given interval.\n\n    Parameters and behaviour are the same as numpy.arange().\n\n    """"""\n    __props__ = (""dtype"",)\n\n    def __init__(self, dtype):\n        self.dtype = dtype\n\n    def make_node(self, start, stop, step):\n        start, stop, step = map(as_tensor_variable, (start, stop, step))\n        assert start.ndim == 0\n        assert stop.ndim == 0\n        assert step.ndim == 0\n\n        inputs = [start, stop, step]\n        outputs = [tensor(self.dtype, (False,))]\n\n        return Apply(self, inputs, outputs)\n\n    @theano.configparser.change_flags(warn_float64=\'ignore\')\n    def infer_shape(self, node, i_shapes):\n        # Note start, stop and step can be float numbers.\n        start, stop, step = node.inputs\n\n        def is_constant_value(var, value):\n            try:\n                v = get_scalar_constant_value(var)\n                return np.all(v == value)\n            except NotScalarConstantError:\n                pass\n            return False\n\n        def upcast(var):\n            if (var.dtype in integer_dtypes and\n                    # We do not want to cast uint64 to int64 as this can\n                    # loose information. If we upcast uint64 with int64,\n                    # this give float64. This is safer then checking for\n                    # uint64 in case we support [u]int128 or other in the\n                    # future.\n                    scal.upcast(var.dtype, \'int64\') == \'int64\'):\n                return cast(var, \'int64\')\n            return var\n\n        if is_constant_value(step, 1):\n            if is_constant_value(start, 0):\n                return [(cast(stop, \'int64\'),)]\n            else:\n                stop = upcast(stop)\n                start = upcast(start)\n                return [(maximum(cast(stop - start, \'int64\'), 0),)]\n        else:\n            stop = upcast(stop)\n            start = upcast(start)\n            return [(maximum(cast(ceil(cast((stop - start), \'float64\') / step),\n                    \'int64\'), 0),)]\n\n    def perform(self, node, inp, out_):\n        start, stop, step = inp\n        out, = out_\n        start = start.item()\n        stop = stop.item()\n        step = step.item()\n        out[0] = np.arange(start, stop, step, dtype=self.dtype)\n\n    def connection_pattern(self, node):\n\n        return [[True], [False], [True]]\n\n    def L_op(self, inputs, outputs, grads):\n        start, stop, step = inputs\n        gz, = grads\n        # `start` and `step` affect the output values\n        # but the outputs are integers so there\'s\n        # no gradient through them.\n        # When they are not integers, the gradients are\n        # as expressed below.\n        # `stop` does not affect the output values,\n        # just the output shape, so it is disconnected.\n\n        if self.dtype in discrete_dtypes:\n            return [start.zeros_like(dtype=config.floatX),\n                    DisconnectedType()(),\n                    step.zeros_like(dtype=config.floatX)]\n        else:\n            num_steps_taken = outputs[0].shape[0]\n            return [gz.sum(),\n                    DisconnectedType()(),\n                    (gz * arange(num_steps_taken, dtype=self.dtype)).sum()]\n\n    def R_op(self, inputs, eval_points):\n        return [None]\n_arange = {}\n\n\ndef arange(start, stop=None, step=1, dtype=None):\n    # If only one argument is provided, it is in fact the ""stop"" argument,\n    # and start is 0.\n    if stop is None:\n        start, stop = 0, start\n\n    start, stop, step = map(as_tensor_variable, (start, stop, step))\n    # If dtype is not provided, infer it from the other arguments\n    if dtype is None:\n        dtype = scal.upcast(start.type.dtype, stop.type.dtype, step.type.dtype)\n        # don\'t try to be stingy and byte-optimize, this leads to\n        # overflow problems.\n        if dtype in int_dtypes:\n            dtype = \'int64\'\n        if dtype in uint_dtypes:\n            dtype = \'uint64\'\n        if config.cast_policy in (\'numpy\', \'numpy+floatX\'):\n            # We enforce numpy semantics, except in the special case where\n            # `config.cast_policy` is \'numpy+floatX\' and we want to use float32\n            # rather than float64.\n            # As an example, if `start`, `stop` and `step` are all int32,\n            # `numpy.arange` returns an int64 array (on 64-bit platforms),\n            # while the upcast above returns int32.\n            numpy_dtype = np.arange(\n                start=np.array(0, dtype=start.dtype),\n                stop=np.array(1, dtype=stop.dtype),\n                step=np.array(1, dtype=step.dtype)).dtype\n            if numpy_dtype != dtype:\n                if (config.cast_policy == \'numpy+floatX\' and\n                    config.floatX == \'float32\' and\n                    numpy_dtype == \'float64\' and\n                    # No explicit float64 in the three arguments?\n                    python_all(\n                        dt != \'float64\'\n                        for dt in [s.dtype for s in (start, stop, step)])):\n                    # We use float32 instead.\n                    assert dtype != \'float64\'\n                    dtype = \'float32\'\n                else:\n                    # We use the same dtype as numpy instead of the result of\n                    # the upcast.\n                    dtype = str(numpy_dtype)\n\n    if dtype not in _arange:\n        _arange[dtype] = ARange(dtype)\n    return _arange[dtype](start, stop, step)\n\n\nclass _nd_grid(object):\n    """"""Create a dense n-dimensional \'meshgrid\' with equally spaced points.\n\n    Used to create the instance ``mgrid`` and ``ogrid`` which act similarly\n    to their numpy equivalents.\n\n    Parameters\n    ----------\n    sparse : boolean, optional, default=True\n        Specifying False leads to the equivalent of numpy\'s mgrid functionality.\n        Specifying True leads to the equivalent of ogrid.\n\n    Examples\n    --------\n    >>> a = T.mgrid[0:5, 0:3]\n    >>> a[0].eval()\n    array([[0, 0, 0],\n           [1, 1, 1],\n           [2, 2, 2],\n           [3, 3, 3],\n           [4, 4, 4]], dtype=int8)\n    >>> a[1].eval()\n    array([[0, 1, 2],\n           [0, 1, 2],\n           [0, 1, 2],\n           [0, 1, 2],\n           [0, 1, 2]], dtype=int8)\n    >>> b = T.ogrid[0:5, 0:3]\n    >>> b[0].eval()\n    array([[0],\n           [1],\n           [2],\n           [3],\n           [4]], dtype=int8)\n    >>> b[1].eval()\n    array([[0, 1, 2, 3]], dtype=int8)\n\n    def __init__(self, sparse=False):\n        self.sparse = sparse\n\n    def __getitem__(self, *args):\n\n        ndim = len(args[0])\n        for sl in args[0]:\n            if isinstance(sl.step, python_complex):\n                raise NotImplementedError(""Not implemented for slices ""\n                                          ""whose step is complex"")\n        ranges = [arange(sl.start or 0,\n                         sl.stop,\n                         sl.step or 1) for sl in args[0]]\n        shapes = [tuple([1] * j + [r.shape[0]] + [1] * (ndim - 1 - j))\n                  for j, r in enumerate(ranges)]\n        ranges = [r.reshape(shape) for r, shape in zip(ranges, shapes)]\n        if self.sparse:\n            grids = ranges\n        else:\n            grids = []\n            ones = [ones_like(r) for r in ranges]\n            for i in range(ndim):\n                grid = 1\n                for j in range(ndim):\n                    if j == i:\n                        grid = grid * ranges[j]\n                    else:\n                        grid = grid * ones[j]\n                grids.append(grid)\n        return grids\n\n\nmgrid = _nd_grid()\nogrid = _nd_grid(sparse=True)\n\n\nclass PermuteRowElements(Op):\n    """"""Permute the elements of each row (inner-most dim) of a tensor.\n\n    A permutation will be applied to every row (vector) of the input tensor x.\n    Depending on the dimensionality of x and the permutation tensor y,\n    different cases are possible.\n    If y.ndim = 1, y is a single permutation, that will be applied to every\n    vector of x. For instance, if x is a matrix, the same permutation will be\n    applied to each row of x.\n    If x.ndim = y.ndim, each row of x corresponds to a row of y, containing\n    a permutation that will be applied to that row. For instance, if x and y\n    are two matrices, a different permutation will be applied to each row of x.\n    If x.ndim > y.ndim, y will be broadcasted to fit x, then each row (vector)\n    of x will be reordered according to the corresponding row of y. (This is\n    a generalization of the first case).\n    If x.ndim = 1, every permutation in y will be applied to x, and the output\n    will contain all the results.\n    If x.ndim < y.ndim, x will be broadcasted to fit y, and different\n    permutations contained in y will be applied to each vector in x. (This is\n    a generalization of the previous case).\n\n    If the ""inverse"" argument is True, the Op will perform the inverse\n    permutation instead.\n    """"""\n    __props__ = ()\n\n    def make_node(self, x, y, inverse):\n        x = as_tensor_variable(x)\n        y = as_tensor_variable(y)\n        if inverse:  # as_tensor_variable does not accept booleans\n            inverse = as_tensor_variable(1)\n        else:\n            inverse = as_tensor_variable(0)\n\n        # y should contain integers\n        assert y.type.dtype in integer_dtypes\n        # Inverse should be an integer scalar\n        assert (inverse.type.ndim == 0 and inverse.type.dtype in integer_dtypes)\n\n        # Match shapes of x and y\n        x_dim = x.type.ndim\n        y_dim = y.type.ndim\n\n        if x_dim > y_dim:\n            y = shape_padleft(y, n_ones=(x_dim - y_dim))\n        elif x_dim < y_dim:\n            x = shape_padleft(x, n_ones=(y_dim - x_dim))\n\n        # Compute the broadcastable pattern of the output\n        out_broadcastable = [xb and yb for xb, yb in\n                             izip(x.type.broadcastable, y.type.broadcastable)]\n        out_type = tensor(dtype=x.type.dtype, broadcastable=out_broadcastable)\n\n        inputlist = [x, y, inverse]\n        outputlist = [out_type]\n        return Apply(self, inputlist, outputlist)\n\n    def _rec_perform(self, node, x, y, inverse, out, curdim):\n        """"""Perform the permutation by doing a recursion over the input\n        dimensions.\n\n        For every dimension, starting with the leftmost, the right set of\n        indices is determined (depending if broadcasting or not), then\n        the function is recursively called on the appropriate subtensors.\n\n        The terminal case is reached when the current tensors are vector,\n        then the permutation contained in y is applied to x.\n\n        Parameters\n        ----------\n        x : tensor\n            The input tensor, on which the permutation is applied.\n        y : tensor\n            Tensor containing the permutations to apply.\n        out : tensor\n            Tensor storing the output result.\n        curdim : int\n            Counter of the current depth of recursion.\n        inverse\n            Wether to apply permutations or their inverse.\n\n        """"""\n        if len(x.shape) == 1:\n            # Numpy advanced indexing works in this case\n            if inverse:\n                out[y] = x[:]\n            else:\n                out[:] = x[y]\n        else:\n            xs0 = x.shape[0]\n            ys0 = y.shape[0]\n            if xs0 == ys0:\n                for i in xrange(xs0):\n                    self._rec_perform(node, x[i], y[i], inverse, out[i],\n                                      curdim + 1)\n            elif ys0 == 1 and node.inputs[1].type.broadcastable[curdim]:\n                # Broadcast y\n                for i in xrange(xs0):\n                    self._rec_perform(node, x[i], y[0], inverse, out[i],\n                                      curdim + 1)\n            elif xs0 == 1 and node.inputs[0].type.broadcastable[curdim]:\n                # Broadcast x\n                for i in xrange(ys0):\n                    self._rec_perform(node, x[0], y[i], inverse, out[i],\n                                      curdim + 1)\n            else:\n                raise ValueError(\'Dimension mismatch: %s, %s\' % (xs0, ys0))\n\n    def perform(self, node, inp, out):\n        x, y, inverse = inp\n        outs, = out\n        x_s = x.shape\n        y_s = y.shape\n        assert len(x_s) == len(y_s)\n\n        # Make sure the output is big enough\n        out_s = []\n        for xdim, ydim in izip(x_s, y_s):\n            if xdim == ydim:\n                outdim = xdim\n            elif xdim == 1:\n                outdim = ydim\n            elif ydim == 1:\n                outdim = xdim\n            else:\n                raise ValueError(\'Dimension mismatch: %s, %s\' % (xdim, ydim))\n            out_s.append(outdim)\n\n        if outs[0] is None or outs[0].shape != out_s:\n            outs[0] = np.empty(out_s, dtype=x.dtype)\n\n        self._rec_perform(node, x, y, inverse, outs[0], curdim=0)\n\n    def infer_shape(self, node, in_shapes):\n        shp_x = in_shapes[0]\n        shp_y = in_shapes[1]\n        assert len(shp_x) == len(shp_y)\n        out_shape = []\n        for i in xrange(len(shp_x)):\n            out_shape.append(maximum(shp_x[i], shp_y[i]))\n        return [out_shape]\n\n    def grad(self, inp, grads):\n        x, y, inverse = inp\n        gz, = grads\n        # First, compute the gradient wrt the broadcasted x.\n        # If \'inverse\' is False (0), apply the inverse of y on gz.\n        # Else, apply y on gz.\n        gx = permute_row_elements(gz, y, eq(inverse, 0))\n\n        # If x has been broadcasted along some axes, we need to sum\n        # the gradient over these axes, but keep the dimension (as\n        # broadcastable)\n        broadcasted_dims = [dim for dim in xrange(gz.type.ndim)\n                            if x.type.broadcastable[dim] and\n                            not gz.type.broadcastable[dim]]\n        gx = Sum(axis=broadcasted_dims)(gx)\n\n        # Sum(...) removed the dimensions in broadcasted_dims,\n        # so we need to put them back.\n        newdims = []\n        i = 0\n        for dim in xrange(gz.type.ndim):\n            if dim in broadcasted_dims:\n                newdims.append(\'x\')\n            else:\n                newdims.append(i)\n                i += 1\n\n        gx = DimShuffle(gx.type.broadcastable, newdims)(gx)\n        assert gx.type.broadcastable == x.type.broadcastable\n\n        # if x is an integer type, then so is the output.\n        # this means f(x+eps) = f(x) so the gradient with respect\n        # to x is zero\n        if x.type.dtype in discrete_dtypes:\n            gx = x.zeros_like()\n\n        # The elements of y and of inverse both affect the output,\n        # so they are connected to the output,\n        # and the transformation isn\'t defined if their values\n        # are non-integer, so the gradient with respect to them is\n        # undefined\n\n        return [gx, grad_undefined(self, 1, y),\n                grad_undefined(self, 1, inverse)]\n\n_permute_row_elements = PermuteRowElements()\n\n\ndef permute_row_elements(x, y, inverse=0):\n    return _permute_row_elements(x, y, inverse)\n\n\ndef inverse_permutation(perm):\n    """"""Computes the inverse of permutations.\n\n    Each row of input should contain a permutation of the first integers.\n\n    """"""\n    return permute_row_elements(\n        arange(perm.shape[-1], dtype=perm.dtype),\n        perm,\n        inverse=True)\n\n\n#########################\n# Linalg : Dot\n#########################\n#\n# For BLAS-related ops see blas.py\n#\n# TODO: Dotinv should go here, Eigs, Svd, etc.\n\n\nclass Dot(Op):\n    """"""\n    Computes the dot product of two variables. For two matrices, this is\n    equivalent to matrix multiplication. For two vectors, this is the inner\n    product.\n\n    Notes\n    -----\n    Matrix-matrix products are sometimes optimized to Dot22 or Gemm ops\n    (see tensor.blas).\n    Vector-vector products are sometimes optimized to Ger or CGer (see\n    tensor.blas).\n    Matrix-vector products are sometimes optimized to Gemv, CGemv (see\n    tensor.blas).\n\n    """"""\n    __props__ = ()\n\n    # the rationale for Dot22 is related to getting GEMM Ops into the\n    # graph.  See Dot22 in tensor.blas for details.\n\n    def make_node(self, *inputs):\n        inputs = list(map(as_tensor_variable, inputs))\n\n        if len(inputs) != 2:\n            raise TypeError(\n                \'theano.tensor.Dot: 2 arguments required, %d given \' %\n                len(inputs))\n        if inputs[0].ndim not in (1, 2):\n            raise TypeError(\n                \'theano.tensor.Dot: input 0 (0-indexed) must have ndim of \'\n                \'1 or 2, %d given. Consider calling theano.tensor.dot \'\n                \'instead.\' % inputs[0].ndim)\n        if inputs[1].ndim not in (1, 2):\n            raise TypeError(\n                \'theano.tensor.Dot: input 1 (0-indexed) must have ndim of \'\n                \'1 or 2, %d given. Consider calling theano.tensor.dot \'\n                \'instead.\' % inputs[1].ndim)\n\n        i_broadcastables = [input.type.broadcastable for input in inputs]\n        bx, by = i_broadcastables\n        if len(by) == 2:  # y is a matrix\n            bz = bx[:-1] + by[-1:]\n        elif len(by) == 1:  # y is vector\n            bz = bx[:-1]\n\n        i_dtypes = [input.type.dtype for input in inputs]\n        outputs = [tensor(scal.upcast(*i_dtypes), bz)]\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inp, out):\n        x, y = inp\n        z, = out\n\n        # the asarray is here because dot between two vectors\n        # gives a numpy float object but we need to return a 0d\n        # ndarray\n        z[0] = np.asarray(np.dot(x, y))\n\n    def grad(self, inp, grads):\n\n        x, y = inp\n        gz, = grads\n        xdim, ydim, gdim = x.type.ndim, y.type.ndim, gz.type.ndim\n\n        # grad is scalar, so x is vector and y is vector\n        if gdim == 0:\n            xgrad = gz * y\n            ygrad = gz * x\n\n        # x is vector, y is matrix, grad is vector\n        elif xdim == 1 and ydim == 2:\n            xgrad = dot(gz, y.T)\n            ygrad = outer(x.T, gz)\n\n        # x is matrix, y is vector, grad is vector\n        elif xdim == 2 and ydim == 1:\n            xgrad = outer(gz, y.T)\n            ygrad = dot(x.T, gz)\n\n        # x is matrix, y is matrix, grad is matrix\n        elif xdim == ydim == 2:\n            xgrad = dot(gz, y.T)\n            ygrad = dot(x.T, gz)\n\n        # If x or y contain broadcastable dimensions but only one of\n        # them know that a matching dimensions is broadcastable, the\n        # above code don\'t always return the right broadcast pattern.\n        # This cause problem down the road. See gh-1461.\n        if xgrad.broadcastable != x.broadcastable:\n            xgrad = patternbroadcast(xgrad, x.broadcastable)\n        if ygrad.broadcastable != y.broadcastable:\n            ygrad = patternbroadcast(ygrad, y.broadcastable)\n\n        rval = xgrad, ygrad\n\n        for elem in rval:\n            assert elem.dtype.find(\'float\') != -1\n\n    def R_op(self, inputs, eval_points):\n        # R_op for a \\dot b evaluted at c for a and d for b is\n        # simply c \\dot b + a \\dot d\n\n        assert len(inputs) == 2\n        assert len(eval_points) == 2\n        if eval_points[0] is None and eval_points[1] is None:\n            return [None]\n\n        if eval_points[0]:\n            t1 = self(eval_points[0], inputs[1])\n        if eval_points[1]:\n            t2 = self(inputs[0], eval_points[1])\n\n        if eval_points[0] and eval_points[1]:\n            return [t1 + t2]\n        elif eval_points[0]:\n            return [t1]\n        else:\n            return [t2]\n\n    def infer_shape(self, node, shapes):\n        xshp, yshp = shapes\n        x, y = node.inputs\n\n        # vector / vector\n        if x.ndim == 1 and y.ndim == 1:\n            return [()]\n        # matrix / vector\n        if x.ndim == 2 and y.ndim == 1:\n            return [xshp[:-1]]\n        # vector / matrix\n        if x.ndim == 1 and y.ndim == 2:\n            return [yshp[-1:]]\n        # matrix / matrix\n        if x.ndim == 2 and y.ndim == 2:\n            return [xshp[:-1] + yshp[-1:]]\n        raise NotImplementedError()\n\n    def __str__(self):\n        return ""dot""\n\n_dot = Dot()\npprint.assign(_dot, printing.OperatorPrinter(printing.special[\'middle_dot\'],\n                                             -1, \'left\'))\n\n\ndef dot(a, b):\n    """"""\n    Computes the dot product of two variables.\n\n    For two matrices, this is equivalent to matrix multiplication.\n    For two vectors, this is the inner product.\n    When one variable is a scalar, this is like elementwise multiplication.\n    For N dimensions, this is a sum product over the last axis\n    of the first array and the second-to-last axis of the second array:\n\n        dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n\n    Note that this dot function does one of three things, in the following\n    sequence:\n\n        1.  If either a or b is scalar, it returns the elementwise product\n            without calling the Theano Dot op.\n\n        2.  If either a or b has more than 2 dimensions, it calls Theano\'s\n            tensordot function with appropriate axes. The tensordot function\n            expresses high-dimensional dot products in terms of 2D matrix\n            multiplications, so it may be possible to futherize optimize for\n            performance.\n\n        3.  If both a and b have either 1 or 2 dimensions, it calls Theano\'s\n            Dot op on a and b.\n\n    Notes\n    -----\n    Matrix-matrix products are sometimes optimized to Dot22 or Gemm ops\n    (see tensor.blas).\n    Vector-vector products are sometimes optimized to Ger or CGer (see\n    tensor.blas).\n    Matrix-vector products are sometimes optimized to Gemv, CGemv (see\n    tensor.blas).\n\n    """"""\n    a, b = as_tensor_variable(a), as_tensor_variable(b)\n\n    if a.ndim == 0 or b.ndim == 0:\n        return a * b\n    elif a.ndim > 2 or b.ndim > 2:\n        return tensordot(a, b, [[a.ndim - 1], [np.maximum(0, b.ndim - 2)]])\n    else:\n        return _dot(a, b)\n\n\n#########################\n# Linalg : TensorDot\n#########################\n\ndef _tensordot_as_dot(a, b, axes, dot, batched):\n    """"""\n    Reduces a tensor dot product to a matrix or vector dot product. Based\n    on code from Tijmen Tieleman\'s gnumpy\n    (http://www.cs.toronto.edu/~tijmen/gnumpy.html).\n\n    Please see the documentation of tensordot for the meaning of the a, b\n    and axes arguments.\n\n    :param dot: a function that accepts two symbolic variables and computes\n                the appropriate dot product (e.g. dot, batched_dot)\n    :type dot: function\n\n    :param batched: whether to treat the first axis of a and b as a batch\n                    axis.  If so, this axis will be preserved in the output,\n                    allowing this function to be used also for batched\n                    tensor dot products.\n    :type batched: boolean\n\n    :returns: a tensor with shape equal to the concatenation of a\'s shape\n              (less any dimensions that were summed over) and b\'s shape\n              (less the first dimension and any dimensions that were summed\n              over).\n    :rtype: symbolic tensor\n    """"""\n    a, b = as_tensor_variable(a), as_tensor_variable(b)\n\n    if not np.isscalar(axes) and len(axes) != 2:\n        raise ValueError(\'Axes should be an integer or a \'\n                         \'list/tuple of len 2 (%s was provided)\'\n                         % str(axes))\n\n    # if \'axes\' is a number of axes to multiply and sum over (trailing axes\n    # of a, leading axes of b), we can just reshape and use dot.\n    elif np.isscalar(axes):\n        axes = int(axes)\n\n        for operand_name, operand in ((""a"", a), (""b"", b)):\n            if axes > operand.ndim:\n                raise ValueError(\n                    \'axes can not be larger than the dimension of %s \'\n                    \'(%s.ndim=%i, axes=%i)\'\n                    % (operand_name, operand_name, operand.ndim, axes))\n            if batched and axes == operand.ndim:\n                raise ValueError(\n                    \'axes to sum over must not include the batch axis \'\n                    \'of %s (%s.ndim=%i, axes=%i)\'\n                    % (operand_name, operand_name, operand.ndim, axes))\n\n        batch_axes = 1 if batched else 0\n        a_outaxes = slice(0, a.ndim - axes)\n        b_outaxes = slice(batch_axes + axes, b.ndim)\n        outshape = concatenate([a.shape[a_outaxes], b.shape[b_outaxes]])\n        outbcast = a.broadcastable[a_outaxes] + b.broadcastable[b_outaxes]\n        outndim = len(outbcast)\n\n        a_shape = [1] * 2\n        b_shape = [1] * 2\n\n        # compute total size of summed axes\n        for i in xrange(0, axes):\n            a_shape[1] *= a.shape[-(i + 1)]\n            b_shape[0] *= b.shape[batch_axes + i]\n        # compute total size of other axes\n        for i in xrange(0, a.ndim - axes - batch_axes):\n            a_shape[0] *= a.shape[batch_axes + i]\n        for i in xrange(0, b.ndim - axes - batch_axes):\n            b_shape[1] *= b.shape[-(i + 1)]\n\n        if batched:\n            a_shape.insert(0, a.shape[0])\n            b_shape.insert(0, b.shape[0])\n\n        a_reshaped = a.reshape(a_shape)\n        b_reshaped = b.reshape(b_shape)\n\n        out_reshaped = dot(a_reshaped, b_reshaped)\n        out = out_reshaped.reshape(outshape, outndim)\n        # Make sure the broadcastable pattern of the result is correct,\n        # since some shape information can be lost in the reshapes.\n        return patternbroadcast(out, outbcast)\n\n    # if \'axes\' is a list, transpose a and b such that the summed axes of a\n    # are last and the summed axes of b are first.\n    else:\n        axes = [_pack(axes_) for axes_ in axes]\n\n        if len(axes[0]) != len(axes[1]):\n            raise ValueError(\'Axes elements must have the same length.\')\n\n        for i, (operand_name, operand) in enumerate(((""a"", a),\n                                                     (""b"", b))):\n            if len(axes[i]) > operand.ndim:\n                raise ValueError(\n                    \'axes[%i] should be array_like with length less than \'\n                    \'the dimensions of %s (%s.ndim=%i, len(axes[0])=%i).\' %\n                    (i, operand_name, operand_name, operand.ndim,\n                     len(axes[i])))\n            if len(axes[i]) > 0 and np.max(axes[i]) >= operand.ndim:\n                raise ValueError(\n                    \'axes[%i] contains dimensions greater than or equal \'\n                    \'to %s.ndim (%s.ndim=%i, max(axes[0])=%i).\' %\n                    (i, operand_name, operand_name, operand.ndim,\n                     np.max(np.array(axes[i]))))\n            if batched and 0 in axes[i]:\n                raise ValueError(\n                    \'axes to sum over must not contain the batch axis \'\n                    \'(axes[%i]=%s)\' %\n                    (i, axes[i]))\n\n        batch_axes = [0] if batched else []\n        other_axes = [[x for x in xrange(operand.ndim)\n                       if x not in axes[i] and x not in batch_axes]\n                      for i, operand in enumerate((a, b))]\n\n        a_shuffled = a.dimshuffle(batch_axes + other_axes[0] + axes[0])\n        b_shuffled = b.dimshuffle(batch_axes + axes[1] + other_axes[1])\n\n        # now that a and b are in the right order, recur with integer axes\n        return _tensordot_as_dot(a_shuffled, b_shuffled, len(axes[0]),\n                                 dot=dot, batched=batched)\n\n\ndef tensordot(a, b, axes=2):\n    """"""\n    Compute a generalized dot product over provided axes.\n\n    Given two tensors a and b, tensordot computes a generalized dot product over\n    the provided axes. Theano\'s implementation reduces all expressions to\n    matrix or vector dot products and is based on code from Tijmen Tieleman\'s\n    gnumpy (http://www.cs.toronto.edu/~tijmen/gnumpy.html).\n\n    Parameters\n    ----------\n    a: symbolic tensor\n        The first tensor variable.\n    b: symbolic tensor\n        The second tensor variable\n    axes: int or array-like of length 2\n        If an integer, the number of axes to sum over.\n        If an array, it must have two array elements containing the axes\n        to sum over in each tensor.\n\n        Note that the default value of 2 is not guaranteed to work\n        for all values of a and b, and an error will be raised if\n        that is the case. The reason for keeping the default is to\n        maintain the same signature as numpy\'s tensordot function\n        (and np.tensordot raises analogous errors for non-compatible\n        inputs).\n\n        If an integer i, it is converted to an array containing\n        the last i dimensions of the first tensor and the first\n        i dimensions of the second tensor:\n            axes = [list(range(a.ndim - i, b.ndim)), list(range(i))]\n\n        If an array, its two elements must contain compatible axes\n        of the two tensors. For example, [[1, 2], [2, 0]] means sum\n        over the 2nd and 3rd axes of a and the 3rd and 1st axes of b.\n        (Remember axes are zero-indexed!) The 2nd axis of a and the\n        3rd axis of b must have the same shape; the same is true for\n        the 3rd axis of a and the 1st axis of b.\n\n    Returns\n    -------\n    symbolic tensor\n        A tensor with shape equal to the concatenation of a\'s shape\n        (less any dimensions that were summed over) and b\'s shape\n        (less any dimensions that were summed over).\n\n    Examples\n    --------\n    It may be helpful to consider an example to see what tensordot does.\n    Theano\'s implementation is identical to NumPy\'s. Here a has shape (2, 3, 4)\n    and b has shape (5, 6, 4, 3). The axes to sum over are [[1, 2], [3, 2]] --\n    note that a.shape[1] == b.shape[3] and a.shape[2] == b.shape[2]; these axes\n    are compatible. The resulting tensor will have shape (2, 5, 6) -- the\n    dimensions that are not being summed:\n\n    >>> a = np.random.random((2,3,4))\n    >>> b = np.random.random((5,6,4,3))\n\n    #tensordot\n    >>> c = np.tensordot(a, b, [[1,2],[3,2]])\n\n    #loop replicating tensordot\n    >>> a0, a1, a2 = a.shape\n    >>> b0, b1, _, _ = b.shape\n    >>> cloop = np.zeros((a0,b0,b1))\n\n    #loop over non-summed indices -- these exist\n    #in the tensor product.\n    >>> for i in range(a0):\n    ...     for j in range(b0):\n    ...         for k in range(b1):\n    ...             #loop over summed indices -- these don\'t exist\n    ...             #in the tensor product.\n    ...             for l in range(a1):\n    ...                 for m in range(a2):\n    ...                     cloop[i,j,k] += a[i,l,m] * b[j,k,m,l]\n\n    >>> np.allclose(c, cloop)\n    true\n\n    This specific implementation avoids a loop by transposing a and b such that\n    the summed axes of a are last and the summed axes of b are first. The\n    resulting arrays are reshaped to 2 dimensions (or left as vectors, if\n    appropriate) and a matrix or vector dot product is taken. The result is\n    reshaped back to the required output dimensions.\n\n    In an extreme case, no axes may be specified. The resulting tensor\n    will have shape equal to the concatenation of the shapes of a and b:\n\n    >>> c = np.tensordot(a, b, 0)\n    >>> print(a.shape)\n    (2,3,4)\n    >>> print(b.shape)\n    (5,6,4,3)\n    >>> print(c.shape)\n    (2,3,4,5,6,4,3)\n\n    See the documentation of numpy.tensordot for more examples.\n\n    """"""\n    return _tensordot_as_dot(a, b, axes, dot=dot, batched=False)\n\n\ndef outer(x, y):\n    """"""Return vector-vector outer product.\n\n    If an input isn\'t a vector, we flatten it first.\n\n    """"""\n    if x.ndim != 1:\n        x = x.flatten()\n    if y.ndim != 1:\n        y = y.flatten()\n    return dot(\n        x.dimshuffle(0, \'x\'),\n        y.dimshuffle(\'x\', 0))\n\n\ndef any(x, axis=None, keepdims=False):\n    out = elemwise.Any(axis)(x)\n\n    if keepdims:\n        out = makeKeepDims(x, out, axis)\n    return out\n\n\ndef all(x, axis=None, keepdims=False):\n    out = elemwise.All(axis)(x)\n\n    if keepdims:\n        out = makeKeepDims(x, out, axis)\n    return out\n\n\n# Some NumPy version like 1.9.2 return a view for numpy.diagonal\nx = np.zeros((4, 4))\nnumpy_diagonal_return_view = np.may_share_memory(np.diagonal(x), x)\ndel x\n\n\nclass ExtractDiag(Op):\n    """"""\n    Return specified diagonals.\n\n    If x is 2-D, returns the diagonal of x with the given offset,\n    i.e., the collection of elements of the form x[i, i+offset].\n    If x has more than two dimensions, then the axes specified by\n    axis1 and axis2 are used to determine the 2-D sub-array whose\n    diagonal is returned. The shape of the resulting array can be\n    determined by removing axis1 and axis2 and appending an index\n    to the right equal to the size of the resulting diagonals.\n\n    Parameters\n    ----------\n    x: A tensor variable with x.ndim >= 2.\n\n    offset: Offset of the diagonal from the main diagonal.\n        Can be positive or negative.\n        Defaults to main diagonal (0).\n\n    axis1: Axis to be used as the first axis of the 2-D\n        sub-arrays from which the diagonals should be taken.\n        Defaults to first axis (0).\n\n    axis2: Axis to be used as the second axis of the 2-D\n        sub-arrays from which the diagonals should be taken.\n        Defaults to second axis (1).\n\n    Returns\n    -------\n    array_of_diagonals:\n        If x is 2-D, a 1-D array of the same type as a\n        containing the diagonal is returned.\n        If the dimension of x is greater than two, then an\n        array of diagonals is returned, ""packed"" from left-most\n        dimension to right-most (e.g., if x is 3-D, then the\n        diagonals are ""packed"" along rows).\n\n    Raises\n    ------\n    ValueError\n        If the dimension of x is less than 2.\n\n\n    See Also\n    --------\n    numpy.diagonal:\n        https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.diagonal.html\n    """"""\n    __props__ = (""offset"", ""axis1"", ""axis2"", ""view"")\n\n    def __init__(self, offset=0, axis1=0, axis2=1, view=False):\n        self.view = view\n        if self.view and not numpy_diagonal_return_view:\n            warnings.warn(""View will forced to False. ExtractDiag property view is ""\n                          ""set to True but numpy version %s and prior versions of ""\n                          ""numpy.diagonal() do not return a view. Update ""\n                          ""numpy to use ExtractDiag(view=True)"" %\n                          np.version.version)\n            self.view = False\n        if self.view:\n            self.view_map = {0: [0]}\n        self.offset = offset\n        self.axis1 = axis1\n        self.axis2 = axis2\n\n    def make_node(self, x):\n        x = as_tensor_variable(x)\n\n        if x.ndim < 2:\n            raise ValueError(\'ExtractDiag needs an input with 2 or more \'\n                             \'dimensions\', x)\n        return Apply(self, [x], [x.type.__class__(\n            dtype=x.dtype,\n            broadcastable=[False] * (x.ndim - 1))()])\n\n    def perform(self, node, inputs, outputs):\n        (x,) = inputs\n        (z,) = outputs\n        z[0] = x.diagonal(self.offset, self.axis1, self.axis2)\n        if not self.view:\n            z[0] = z[0].copy()\n\n    def grad(self, inputs, gout):\n        (x,) = inputs\n        (gz,) = gout\n\n        if x.ndim == 2:\n            x = theano.tensor.zeros_like(x)\n            xdiag = theano.tensor.AllocDiag(offset=self.offset)(gz)\n            return [theano.tensor.set_subtensor(\n                x[:xdiag.shape[0], :xdiag.shape[1]], xdiag)]\n        else:\n            warnings.warn(""gradient of theano.tensor.basic.ExtractDiag only""\n                          ""works for matrices."")\n            return [grad_not_implemented(self, 0, x)]\n\n    def infer_shape(self, node, shapes):\n        in_shape, = shapes\n        dim1 = in_shape[self.axis1]\n        dim2 = in_shape[self.axis2]\n        out_shape = [d for i, d in enumerate(in_shape)\n                     if i not in (self.axis1, self.axis2)]\n        # The following logic is inspired by C code of PyArray_Diagonal().\n        offset = self.offset\n        if offset > 0:\n            diag_size = clip(dim2 - offset, 0, dim1)\n        elif offset < 0:\n            diag_size = clip(dim1 + offset, 0, dim2)\n        else:\n            diag_size = minimum(dim1, dim2)\n        out_shape.append(diag_size)\n        return [tuple(out_shape)]\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        if self.view and not numpy_diagonal_return_view:\n            warnings.warn(""View will forced to False. ExtractDiag property view is ""\n                          ""set to True but numpy version %s and prior versions of ""\n                          ""numpy.diagonal() do not return a view. Update ""\n                          ""numpy to use ExtractDiag(view=True)"" %\n                          np.version.version)\n            self.view = False\n\n        if self.view:\n            self.view_map = {0: [0]}\n\n        if ""offset"" not in state:\n            self.offset = 0\n        if ""axis1"" not in state:\n            self.axis1 = 0\n        if ""axis2"" not in state:\n            self.axis2 = 1\n\n\ndef diagonal(a, offset=0, axis1=0, axis2=1):\n    """"""\n    A helper function for `theano.tensor.ExtractDiag`. It accepts tensor with\n    `ndim >= 2` as input. The name `diagonal` is just meant to keep it\n    consistent with numpy.\n\n    Parameters\n    ----------\n    a : symbolic tensor\n    offset : int\n        offset\n    axis1 : int\n    axis2 : int\n\n    Returns\n    -------\n    tensor : symbolic tensor\n\n    """"""\n    return ExtractDiag(offset, axis1, axis2)(a)\n\n\nclass AllocDiag(Op):\n    """"""\n    An op that copies a vector to the diagonal of an empty matrix. It does the\n    inverse of ExtractDiag.\n\n    Usage: T.AllocDiag()(x)\n\n    `x` should be a tensor vector. The parenthesis in the front should indicate\n    which main diagonal the vector value goes into. By default it is set to\n    `0`, which corresponds to setting the values of x to the main diagonal in\n    the returned matrix.\n\n    Parameters\n    ----------\n    axis1: Axis to be used as the first axis of the 2-D\n        sub-arrays to which the diagonals will be allocated.\n        Defaults to first axis (0).\n\n    axis2: Axis to be used as the second axis of the 2-D\n        sub-arrays to which the diagonals will be allocated.\n        Defaults to second axis (1).\n\n    offset: Offset of the diagonal from the main diagonal defined by `axis1`\n        and `axis2`.\n        Can be positive or negative.\n        Defaults to main diagonal (0).\n\n    x: symbolic vector\n        A tensor vector consists of diagonal values.\n\n    Returns\n    -------\n    tensor : symbolic tenstor\n        A tensor with passed tensor values at their corresponding diagonals.\n\n    __props__ = (""offset"", ""axis1"", ""axis2"")\n\n    def __init__(self, offset=0, axis1=0, axis2=1):\n        self.offset = offset\n        self.axis1 = axis1\n        self.axis2 = axis2\n\n    def make_node(self, diag):\n        diag = as_tensor_variable(diag)\n        if diag.type.ndim < 1:\n            raise ValueError(\'AllocDiag needs an input with 1 or more \'\n                             \'dimensions\', diag.type)\n        return Apply(\n            self, [diag],\n            [diag.type.__class__(\n                dtype=diag.dtype,\n                broadcastable=[False] * (diag.ndim + 1))()]\n        )\n\n    def perform(self, node, inputs, outputs):\n        (x,) = inputs\n        (z,) = outputs\n\n        axis1 = np.minimum(self.axis1, self.axis2)\n        axis2 = np.maximum(self.axis1, self.axis2)\n        offset = self.offset\n\n        # Create array with one extra dimension for resulting matrix\n        result_shape = x.shape[:-1] + (x.shape[-1] + abs(offset),) * 2\n        result = np.zeros(result_shape, dtype=x.dtype)\n\n        # Create slice for diagonal in final 2 axes\n        idxs = np.arange(x.shape[-1])\n        diagonal_slice = ((len(result_shape) - 2) * [slice(None)] +\n                          [idxs + np.maximum(0, -offset),\n                           idxs + np.maximum(0, offset)])\n\n        # Fill in final 2 axes with x\n        result[tuple(diagonal_slice)] = x\n\n        if len(x.shape) > 1:\n            # Re-order axes so they correspond to diagonals at axis1, axis2\n            axes = list(range(len(x.shape[:-1])))\n            last_idx = axes[-1]\n            axes = axes[:axis1] + [last_idx + 1] + axes[axis1:]\n            axes = axes[:axis2] + [last_idx + 2] + axes[axis2:]\n            result = result.transpose(axes)\n\n        z[0] = result\n\n    def grad(self, inputs, gout):\n        (gz,) = gout\n        return [diagonal(\n            gz,\n            offset=self.offset,\n            axis1=self.axis1,\n            axis2=self.axis2\n        )]\n\n    def infer_shape(self, nodes, shapes):\n        (x_shape,) = shapes\n        axis1 = np.minimum(self.axis1, self.axis2)\n        axis2 = np.maximum(self.axis1, self.axis2)\n\n        result_shape = list(x_shape[:-1])\n        diag_shape = x_shape[-1] + abs(self.offset)\n        result_shape = result_shape[:axis1] + [diag_shape] + result_shape[axis1:]\n        result_shape = result_shape[:axis2] + [diag_shape] + result_shape[axis2:]\n        return [tuple(result_shape)]\n\n    def __setstate__(self, state):\n        if ""view_map"" in state:\n            del state[""view_map""]\n\n        self.__dict__.update(state)\n\n        if ""offset"" not in state:\n            self.offset = 0\n        if ""axis1"" not in state:\n            self.axis1 = 0\n        if ""axis2"" not in state:\n            self.axis2 = 1\n\n\ndef diag(v, k=0):\n    """"""\n    A helper function for two ops: `theano.tensor.ExtractDiag` and\n    `theano.tensor.AllocDiag`. The name `diag` is meant to keep it consistent\n    with numpy. It both accepts tensor vector and tensor matrix.\n    While the passed tensor variable `v` has `v.ndim>=2`, it builds a\n    `ExtractDiag` instance, and returns a vector with its entries equal to\n    `v`\'s main diagonal; otherwise if `v.ndim` is `1`, it builds an `AllocDiag`\n    instance, and returns a matrix with `v` at its k-th diaogonal.\n\n    Parameters\n    ----------\n    v : symbolic tensor\n    k : int\n        offset\n\n    Returns\n    -------\n    tensor : symbolic tensor\n\n    if v.ndim == 1:\n        return AllocDiag(k)(v)\n    elif v.ndim >= 2:\n        return diagonal(v, offset=k)\n    else:\n        raise ValueError(""Input must has v.ndim >= 1."")\n\n\ndef stacklists(arg):\n    """"""\n    Recursively stack lists of tensors to maintain similar structure.\n\n    This function can create a tensor from a shaped list of scalars:\n\n    Examples\n    --------\n    >>> from theano.tensor import stacklists, scalars, matrices\n    >>> from theano import function\n    >>> a, b, c, d = scalars(\'abcd\')\n    >>> X = stacklists([[a, b], [c, d]])\n    >>> f = function([a, b, c, d], X)\n    >>> f(1, 2, 3, 4)\n    array([[ 1.,  2.],\n           [ 3.,  4.]], dtype=float32)\n\n    We can also stack arbitrarily shaped tensors. Here we stack matrices into\n    a 2 by 2 grid:\n\n    >>> from numpy import ones\n    >>> a, b, c, d = matrices(\'abcd\')\n    >>> X = stacklists([[a, b], [c, d]])\n    >>> f = function([a, b, c, d], X)\n    >>> x = ones((4, 4), \'float32\')\n    >>> f(x, x, x, x).shape\n    (2, 2, 4, 4)\n\n    """"""\n    if isinstance(arg, (tuple, list)):\n        return stack(list(map(stacklists, arg)))\n    else:\n        return arg\n\n\ndef ptp(a, axis=None):\n    """"""\n    Range of values (maximum - minimum) along an axis.\n\n    The name of the function comes from the acronym for peak to peak.\n\n    Parameters\n    ----------\n    a\n        Input tensor.\n    axis\n        Axis along which to find the peaks. By default, flatten the array.\n\n    Returns\n    -------\n    array\n        A new array holding the result.\n\n    a = as_tensor_variable(a)\n\n    out = max(a, axis) - min(a, axis)\n\n\ndef power(x, y):\n    return x ** y\n\n\ndef swapaxes(y, axis1, axis2):\n    ""swap axes of inputted tensor""\n    y = as_tensor_variable(y)\n    ndim = y.ndim\n    li = list(range(0, ndim))\n    li[axis1], li[axis2] = li[axis2], li[axis1]\n    return y.dimshuffle(li)\n\n\ndef choose(a, choices, out=None, mode=\'raise\'):\n    """"""\n    Construct an array from an index array and a set of arrays to choose from.\n\n    First of all, if confused or uncertain, definitely look at the Examples -\n    in its full generality, this function is less simple than it might seem\n    from the following code description (below ndi = numpy.lib.index_tricks):\n\n    np.choose(a,c) == np.array([c[a[I]][I] for I in ndi.ndindex(a.shape)]).\n\n    But this omits some subtleties. Here is a fully general summary:\n\n    Given an ``index`` array (a) of integers and a sequence of n arrays\n    (choices), a and each choice array are first broadcast, as necessary,\n    to arrays of a common shape; calling these Ba and\n    Bchoices[i], i = 0,...,n-1 we have that, necessarily,\n    Ba.shape == Bchoices[i].shape for each i.\n    Then, a new array with shape Ba.shape is created as follows:\n\n    - if mode=raise (the default), then, first of all, each element of a\n      (and thus Ba) must be in the range [0, n-1]; now, suppose that\n      i (in that range) is the value at the (j0, j1, ..., jm) position in Ba -\n      then the value at the same position in the new array is the value in\n      Bchoices[i] at that same position;\n\n    - if mode=wrap, values in a (and thus Ba) may be any (signed) integer;\n      modular arithmetic is used to map integers outside the range [0, n-1]\n      back into that range; and then the new array is constructed as above;\n\n    - if mode=clip, values in a (and thus Ba) may be any (signed) integer;\n      negative integers are mapped to 0; values greater than n-1 are mapped\n      to n-1; and then the new array is constructed as above.\n\n    Parameters\n    ----------\n    a : int array\n        This array must contain integers in [0, n-1], where n is the number of\n        choices, unless mode=wrap or mode=clip, in which cases any integers\n        are permissible.\n    choices : sequence of arrays\n        Choice arrays. a and all of the choices must be broadcastable to\n        the same shape. If choices is itself an array (not recommended),\n        then its outermost dimension (i.e., the one corresponding to\n        choices.shape[0]) is taken as defining the ``sequence``.\n    out : array, optional\n        If provided, the result will be inserted into this array.\n        It should be of the appropriate shape and dtype.\n    mode : {``raise`` (default), ``wrap``, ``clip``}, optional\n        Specifies how indices outside [0, n-1] will be treated:\n        ``raise`` : an exception is raised\n        ``wrap`` : value becomes value mod n\n        ``clip`` : values < 0 are mapped to 0, values > n-1 are mapped to n-1\n\n    Returns\n    -------\n    merged_array - array\n        The merged result.\n\n    Raises\n    ------\n    ValueError - shape mismatch\n        If a and each choice array are not all broadcastable to the same shape.\n\n    """"""\n    # This is done to keep the same function signature then NumPy.\n    assert out is None\n    return Choose(mode)(a, choices)\n\n\nclass Choose(Op):\n    __props__ = (\'mode\',)\n\n    def __init__(self, mode):\n        assert mode in (""raise"", ""wrap"", ""clip"")\n        self.mode = mode\n\n    def infer_shape(self, node, shapes):\n\n        if isinstance(node.inputs[1], TensorVariable):\n            # We have padded node.inputs[0] to the right number of\n            # dimensions for the output\n            l = []\n            for sh1, sh2, b1 in zip(shapes[0],\n                                    shapes[1][1:],\n                                    node.inputs[0].broadcastable):\n                if b1:\n                    l.append(sh2)\n                else:\n                    l.append(sh1)\n            return [tuple(l)]\n        else:\n            import theano.typed_list\n            assert isinstance(node.inputs[1],\n                              theano.typed_list.TypedListVariable)\n            raise ShapeError(""Case not implemented"")\n            shape = shapes[0]\n            for i in xrange(len(shapes[0]) - 1):\n                shape[i] = shapes[1][i]\n            return [(shape)]\n\n    def make_node(self, a, choices):\n        # Import here as it isn\'t imported by default and we can\'t\n        # import at the top as it would cause circular import.\n        import theano.typed_list\n        a = as_tensor_variable(a)\n        if a.dtype not in theano.tensor.discrete_dtypes:\n            raise TypeError(\n                \'choose first argument must have an [u]int* dtype. Got %s.\'\n                % a.dtype)\n\n        if isinstance(choices, (tuple, list,\n                                theano.typed_list.TypedListVariable)):\n            choice = theano.typed_list.make_list(choices)\n            choice_ndim = choice.ttype.ndim\n            choice_bcast = choice.ttype.broadcastable\n        else:\n            choice = as_tensor_variable(choices)\n            choice_ndim = choice.ndim - 1\n            choice_bcast = choice.broadcastable[1:]\n        out_ndim = np.max([a.ndim, choice_ndim])\n\n        # Make explicit all added broadcastable dimensions.\n        a = shape_padleft(a, out_ndim - a.ndim)\n        if len(choice_bcast) != out_ndim:\n            if isinstance(choice.type, TensorType):\n                choice = choice.dimshuffle(0,\n                                           *((\'x\',) * (out_ndim - choice_ndim) +\n                                             tuple(range(1, choice.ndim))))\n                choice_ndim = choice.ndim - 1\n                choice_bcast = choice.broadcastable[1:]\n            else:\n                raise NotImplementedError(\n                    ""We currently didn\'t implemented that case. ""\n                    ""To make it work, explicitly add dimensions ""\n                    ""of size one for dimensions that will be broadcasted"")\n\n        bcast = [False] * out_ndim\n        for idx, (b1, b2) in enumerate(\n            zip(a.broadcastable,\n                (True,) * (out_ndim - choice_ndim) + choice_bcast)):\n            if b1 and b2:\n                bcast[idx] = True\n        o = TensorType(choice.dtype, bcast)\n        return Apply(self, [a, choice], [o()])\n\n    def perform(self, node, inputs, outputs):\n        (z,) = outputs\n        a = inputs[0]\n        choice = inputs[1]\n        # TODO reuse out?\n        z[0] = np.choose(a, choice, mode=self.mode)\n\n\nclass AllocEmpty(gof.Op):\n    """"""Implement Alloc on the cpu, but without initializing memory.""""""\n\n    __props__ = (""dtype"", )\n    params_type = ParamsType(typecode=int32_t)\n\n    # specify the type of the data\n    def __init__(self, dtype):\n        assert isinstance(dtype, str), dtype\n        self.dtype = dtype.lower()\n\n    @property\n    def typecode(self):\n        return np.dtype(self.dtype).num\n\n    def make_node(self, *shape):\n        shape, bcast = alloc_validate_shape(shape)\n        otype = TensorType(dtype=self.dtype, broadcastable=bcast)\n        output = otype()\n\n        output.tag.values_eq_approx = values_eq_approx_always_true\n        # The outut can contain nan/inf.  output.type is a new\n        # instance, so we can do this only for that variable.\n        output.type.filter_checks_isfinite = False\n\n        # We can\'t reuse filter_checks_isfinite as by default it is\n        # False and it is set to true only in DebugMode.\n        # We can\'t set it in the type as other make_node can reuse the type.\n        # We can\'t set it in the variable as it isn\'t copied when we copy\n        # the variale. So we set it in the tag.\n        output.tag.nan_guard_mode_check = False\n        return Apply(self, shape, [output])\n\n    def debug_perform(self, node, inputs, out_, params):\n        self.perform(node, inputs, out_, params)\n        out_[0][0].fill(-123456789)\n\n    def perform(self, node, inputs, out_, params):\n        out, = out_\n        sh = tuple([int(i) for i in inputs])\n        if out[0] is None or out[0].shape != sh:\n            out[0] = np.empty(sh, dtype=self.dtype)\n\n    def c_code(self, node, name, inputs, out_, sub):\n        out, = out_\n        fail = sub[\'fail\']\n        shps = inputs\n        nd = len(shps)\n        params = sub[\'params\']\n        str = ""npy_intp dims[%(nd)s];\\n"" % locals()\n        for idx, sh in enumerate(shps):\n            str += ""dims[%(idx)s] ="" \\\n                   ""((npy_intp)((dtype_%(sh)s*)"" \\\n                   "" PyArray_DATA(%(sh)s))[0]);\\n"" % locals()\n\n        # Validate that the output storage exists\n        str += ""if(%(out)s==NULL\\n"" % locals()\n        for idx, sh in enumerate(shps):\n            str += ""||PyArray_DIMS(%(out)s)[%(idx)s]!=dims[%(idx)s]"" % locals()\n\n        str += """"""){\n            /* Reference received to invalid output variable.\n            Decrease received reference\'s ref count and allocate new\n            output variable */\n            Py_XDECREF(%(out)s);\n            %(out)s = (PyArrayObject*)PyArray_EMPTY(%(nd)s,\n                                                    dims,\n                                                    %(params)s->typecode,\n                                                    0);\n            if (!%(out)s)\n            {\n                PyErr_SetString(PyExc_MemoryError, ""alloc failed"");\n                %(fail)s;\n            }\n        }\n        """""" % locals()\n        return str\n\n    def infer_shape(self, node, input_shapes):\n        return [node.inputs]\n\n    def c_code_cache_version(self):\n        return (4,)\n\n    def do_constant_folding(self, node):\n        return False\n\n    def connection_pattern(self, node):\n        return [[False] for i in node.inputs]\n\n    def grad(self, inputs, grads):\n        return [DisconnectedType()() for i in inputs]\n\n    def R_op(self, inputs, eval_points):\n        return [zeros(inputs, self.dtype)]', 'timestamp': '2024-04-25T19:42:37', 'title': 'Theano/theano/tensor/basic.py at master · Theano/Theano', 'url': 'https://github.com/Theano/Theano/blob/master/theano/tensor/basic.py'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nOptimize tensordot with rechunk #2225\n\nmrocklin opened this issue\n\nApr 16, 2017 · 28 comments\n\nOptimize tensordot with rechunk #2225\n\nmrocklin opened this issue\n\nApr 16, 2017 · 28 comments\n\nDask.array tensordot operations can be made significantly faster by doing an initial rechunk pass, making all axes over which we intend to contract single-chunked.\n\nFor example, if we have the following block-chunked arrays:\n\nimport dask.array as da x = da.random.random((500, 500, 500), chunks=(50, 50, 50)) y = da.random.random((500, 100), chunks=(50, 50))\n\nAnd we want to contract over the 1st and 0th dimensions respectively\n\nda.tensordot(x, y, axes=[1, 0]).visualize()\n\nThen we may want to preemptively rechunk so that those axes have only one chunk as follows:\n\nx2 = x.rechunk((..., 500, ...)).persist() y2 = y.rechunk((500, ...)).persist()\n\nWe may want to contract other axes while we expand these ones to make sure that we don\'t produce chunks that are too large (and to ensure that the resulting tensordot chunks are not too large). In this case though the outputs are small enough even with a 10x increase in size, so we leave the other dimensions as-is.\n\nx2 = x.rechunk((50, 500, 50)).persist() y2 = y.rechunk((500, 50)).persist()\n\nThis does incur some communication costs up front, but it will generally save us more communication down the line.\n\nSo I think the question here is the following:\n\nGiven the chunks of both arrays and the axis= argument, how should we rechunk these arrays prior to the normal tensordot call. This should both increase the chunksize in the contracted axes to the full extent and possibly reduce the chunksize of the other dimension based on the expected nbytes of out output of the tensordot call.\n\nSnagged this trick from this talk: https://youtu.be/dcT6c-PrloE?t=1584\n\ncc @jcrist and @shoyer who might find this interesting. cc @pitrou who did the rechunk logic and might be able to recommend something.\n\nThe text was updated successfully, but these errors were encountered:\n\n@TomAugspurger if you find yourself with free time you might find this issue interesting. I also think that it is valuable for distributing algorithms that you care about.\n\n@pitrou if you find yourself with some free time you might find this task interesting. The speedups can be substantial and this has relevance for future paid work.\n\nmrocklin mentioned this issue\n\nlz4: first try passing memoryview before fallback dask/distributed#1155\n\n@mrocklin I followed link to this issue from your SciPy talk (which as usual was full of new and exciting things). I just want to point out a related issue: preemptively determining/rechunking the output array. Perhaps with an ""out="" kwarg? The use case is computing a (1000, 1000) column correlation matrix from a 100,000 row / 1000 column csv file (let\'s ignore the format for now ;). After setting chunks to (100000, 1), I couldn\'t find a way to do this without getting a ""number of chunks increasing by factor of 1000"" warning.\n\nThis does incur some communication costs up front, but it will generally save us more communication down the line.\n\nAre there any estimates or measurements of the rechunking cost available at runtime?\n\nAn einsum implementation (#732) with rechunking would be even better, as tensordot could then call that instead.\n\nAre there any estimates or measurements of the rechunking cost available at runtime?\n\nWe know the size of every chunk both before and after. Every task has around 100-300us of overhead.\n\nAn einsum implementation (#732) with rechunking would be even better, as tensordot could then call that instead.\n\nAre you saying that einsum is likely to be faster than tensordot or that it is more general?\n\nIf you have any interest in implementing einsum then that would be quite welcome.\n\nWe know the size of every chunk both before and after. Every task has around 100-300us of overhead.\n\nI was mostly referring to the communication cost of rechunking, is there any data available on that?\n\nI mean that einsum is more general yes, since you can specify both inner and outer products (and it would do singleton expansion -- which tensordot doesn\'t seem to do?). Since that is the case, it makes sense to design rechunking on the einsum level and have those benefits flow down to tensordot automatically.\n\nI think dask has a lot of potential and would love to contribute in the near future. Feature requests like this one together with access to some of the more recent scalable linear algebra algorithms could really help scale NumPy work to new levels. I can\'t promise anything yet though, unfortunately.\n\nI suspect that the performance benefits of using level-3 BLAS operations like GEMM will encourage folks to keep tensordot around. I could be wrong though. This is something that one would have to take up with numpy.\n\nCommunication costs differ based on scheduler. We don\'t generally have this information while constructing the task graph. In my experience the computation costs of tensordot operations tend to overwhelm the communication costs, particularly when contractions are involved. This is generally true of level-3 blas operations. You can be fairly sloppy.\n\nI think it would be great to have someone pushing on dask.array for numerical linear algebra applications. I care about this topic, but tend not to pursue it personally just because of other work priorities.\n\nYou can write/implement any einsum as a batch of independent GEMMs, so it should be able to benefit from the same fast kernels that tensordot does. I also agree that tensordot is still worth having for those cases where you don\'t need the full flexibility of an einsum. Under the covers it could simply call einsum though.\n\nI think einsum is an operation where dask could really shine as a distributed scheduler, because it is an example of a powerful tool where the distribution and scheduling can make a big difference in performance.\n\nI don\'t know if you guys saw this, but I think the plan is for NumPy 1.14.0 to use optimized BLAS kernels when possible. ( numpy/numpy#9425 )\n\ndistributed and/or asynchronous algorithms for numeric methods dask/dask-glm#68\n\ndistributed and/or asynchronous algorithms for numeric methods #3241\n\nmrocklin mentioned this issue\n\nxarray.dot() dask problems pydata/xarray#2074\n\nIt\'s worth noting that despite work to make NumPy\'s einsum use optimized BLAS calls when possible, one can still get bad performance in common cases (e.g. matmul). ( numpy/numpy#7569 )\n\nThe right place to implement the optimization described in the original post is possibly in atop, rather than tensordot.\n\nEdit: I no longer believe this to be true because we also need to incorporate the sum into this process, which happens outside of atop for the tree reductions.\n\nmrocklin mentioned this issue\n\nAutomatic rechunking along particular axes #3506\n\nI spent this morning working on this problem with pen and paper. Here are my thoughts after a few hours:\n\nOur data gets replicated a number of times equal to the product of the number of chunks along each outer dimension. So considering the following example:\n\nx = da.ones((1000, 1000, 1000), chunks=(100, 200, 100)) y = da.ones((1000, 1000), chunks=(100, 200)) z = tensordot(x, y, axes=(0, 0)) # this requires 5 * 10 communications per chunk\n\nThen naively each chunk communicates with 5 * 10 other chunks. In a shared memory situation this isn\'t so bad, but in a distributed memory situation it\'s pretty unusable.\n\nThe solution then, is to decrease the number of chunks in outer (non-contracted) dimensions. The following would perform better\n\nx = da.ones((1000, 1000, 1000), chunks=(10, 1000, 500)) y = da.ones((1000, 1000), chunks=(10, 1000)) z = tensordot(x, y, axes=(0, 0)) # this requires 1 * 2 communications per chunk\n\nHowever there are some costs to be aware of here:\n\nBy increasing the chunksize of the outer dimensions we also increase the chunksize of our result, which is only outer dimensions. This might quickly become too large.\n\nOur input chunks now have very larger outer dimensions (like our output chunks) but also still have full inner dimensions. We\'ll need to rechunk these to be much smaller to compensate.\n\nIf we squeeze our inner dimensions down to 1 then we need to worry about what happens when several of these stack together when performing the sum. Generally we won\'t want our inner dimensions to decrease below the split_every parameter that controls how many intermediate chunks are used in a tree summation (this is available by dask.config.get(\'array.split_every\').\n\nThe process of rechunking also introduces constraints\n\nThere is a single communication cost across the array (but this is additive rather than multiplicative like with tensordot)\n\nRechunking also hurts streamability, or low-memory action. Consider the following example\n\nx = da.ones((100, 100), chunks=(50, 2)).rechunk((2, 50))\n\nThis requires that full (50, 50) subarrays are in memory at once\n\nSo a naive solution to this problem might be to rechunk all of the outer dimensions to the full dimension size, and rechunk all inner dimensions to be as small as possible in order to compensate, and hope that both result and all intermediate rechunkings fit comfortably in memory. This will fail in plenty of cases.\n\nHowever, we do have one benefit to consider:\n\nOnce all-but-one of the outer dimensions are single chunked, it no longer makes sense to single-chunk the last outer dimension\n\nRecall that unpleasant rechunking generally requires a full communication of the data. This made sense when we were competing against the multiplicative cost of multiplying for every chunked dimension, but once every other outer dimension has a single chunk then the multiplicative factor is 1 * 1 * 1 * 1 ... = 1, and so we can choose to accept this communication and not rechunk.\n\nI anticipate that having freedom along one outer dimension of our choice will be very helpful in the common case.\n\nSo a pragmatic common-case solution here might be to find the outer dimension that we least want to single-chunk, and then try hard to single-chunk all other outer dimensions. The measurement of chunk-desirability depends both on the amount of data in the array associated to that outer-dimension (each outer dimension is associated to exactly one array) and the current chunking of that array.\n\nmrocklin mentioned this issue\n\nAdded recursion to array/linalg/tsqr to better manage the single core bottleneck #3586\n\nWhat I\'m seeing from this discussion is two major objectives:\n\nchunking to reduce communication (while keeping to a maximum chunk size)\n\nchunk to meet a specified output chunking outcome (while considering maximum chunk size; so 1 argument on output chunks, another on max memory)\n\nThey seem to be relatively compatible.\n\nMy proposal would be to ask for a max chunk size option (like 10 MiB, possibly more readily given) and a output chunking (possibly less readily given), and work with those two objectives. Some rechunking might be needed in post processing to achieve both.\n\nx = da.ones((1000, 1000, 1000), chunks=(100, 200, 100)) # this requires 5 communications per chunk y = da.ones((1000, 1000), chunks=(100, 200)) # this requires 5 * 10 communications per chunk z = tensordot(x, y, axes=(0, 0)) # output: chunks=(200, 100, 200)\n\nx = da.ones((1000, 1000, 1000), chunks=(100, 1000, 500)) # this requires 2 communications per chunk y = da.ones((1000, 1000), chunks=(100, 500)) # this requires 1 * 2 communications per chunk z = tensordot(x, y, axes=(0, 0)) # output: chunks=(1000, 500, 500)... ouch\n\nThere may need to be some norms with regards to what ""level"" of communication is sensible.\n\nWhile theoretically, increasing chunk sizes does not involve as much communication (""transmissions"") as reducing chunk sizes, practically, when chunk sizes are reduced, workers might just be reorganizing their memory.\n\njcmgray mentioned this issue\n\nAdd support for cupy.einsum #4402\n\nmrocklin mentioned this issue\n\nmultiplication of matrices dask/distributed#2491\n\njsadler2 mentioned this issue\n\nDot product of two zarr stores slow with Dask pangeo-data/pangeo#756\n\nmrocklin mentioned this issue\n\nPairwise distance scalability sgkit-dev/sgkit#375\n\nmrocklin mentioned this issue\n\nIdentify lack of scalability in gwas_linear_regression sgkit-dev/sgkit#390\n\nGenevieveBuckley commented\n\nIf we squeeze our inner dimensions down to 1 then we need to worry about what happens when several of these stack together when performing the sum. Generally we won\'t want our inner dimensions to decrease below the split_every parameter that controls how many intermediate chunks are used in a tree summation (this is available by dask.config.get(\'array.split_every\').\n\nCan you clarify what you mean by ""squeeze our inner dimensions down to 1"" @mrocklin. Are you talking about squeezing the whole dimension, or about changing the chunking of that dimension?\n\nI also don\'t see any kind of key named split_every in the dask config dictionary, so I\'m not sure we can rely on this. I do have a key named split-large-chunks, but it has a value of None.\n\ndask.config.get(\'array.slicing.split-large-chunks\') == None # returns True\n\nGenevieveBuckley mentioned this issue\n\nAutomatically rechunk blockwise when chunk size increases substantially #7124\n\nCan you clarify what you mean by ""squeeze our inner dimensions down to 1"" @mrocklin. Are you talking about squeezing the whole dimension, or about changing the chunking of that dimension?\n\nAll of this will probably make more sense after you\'ve been beaten up a bit by tensordot. I encourage you to run things and see things fail in different ways. I\'m anticipating issues that may or may not arise after we\'ve solved a couple of problems. Probably you should ignore what I\'m about to say until you\'ve crashed your computer a couple dozen times. That being said, here we go.\n\nI\'m saying that as we increase the size of the outer dimensions, a desire to keep modest sized chunks will naturally cause us to choose chunks with smaller sizes along inner dimensions. I suspect that this will have a negative effect when we choose to perform eventual reductions. Currently when doing a reduction we first collapse each chunk, and then we collect many of those chunks together. If each chunk only has size one then we don\'t get much of a size reduction on that first pass, which could become awkward when we collect many of those chunks together. I suspect that we might run into situations where this causes us to run out of RAM.\n\nSo instead after the first pass we might choose to collect these chunks in groups, and then reduce down each group. Then we could collect the groups together and reduce them again. This gradual approach may allow us to perform the reduction in small space, even if we don\'t benefit much from fat inner dimensions.\n\nGenevieveBuckley commented\n\nThe clarification helps, thank you.\n\nGenevieveBuckley commented\n\nCopying over comments from #7124 that really belong more in this issue thread:\n\nTerminology: We didn\'t use these particular phrases in my maths classes, but here\'s what I think they\'re being used to mean in these discussions\n\ninner dimensions <- are summed over by tensordot\n\nouter dimensions <- not summed over by tensordot\n\nFor tensordot, it looks like the process should go something like this:\n\nWork out which are the outer dimension axes for arrays a and b\n\nUse the auto_chunks function to work out what the best chunk size is for the tensordot array output, given the chunk size limit from dask.config.get(\'array.chunk-size\')\n\nWork backwards from this to figure out how the input tensordot arrays should be rechunked for the outer dimension axes.\n\nInner dimension axes - do we (a) make no change to the chunking here, (b) rechunk all inner dimensions to a chunksize of 1, or (c) rechunk inner dimensions to some smaller chunksize but ?? what size they should be.\n\nRechunk the tensordot input arrays\n\nSet kwarg align_arrays=False when tensordot hands over to blockwise (might need to double check if there is any extra logic in the unify_chunks function we now need to take care of in tensordot?)\n\nI\'ve written some code for this, while I was trying to work out the process. I\'ll need a bit more clarity around the questions in point 4 and 6 above. (Hacky code here)\n\nIt is clear that the rechunking logic is going to have to happen in tensordot before it gets to blockwise, blockwise just doesn\'t know enough information to do what we want.\n\nMost of my comments below are relevant for #2225 rather than this issue (I think that that issue is a bigger deal than this one). It\'s worth noting that these are two different concerns and not necessarily fully compatible.\n\nFirst, I highly recommend multiplying two matrices two-dimensional matrices together and experimenting with chunking structure before trying to generalize any of this. I also recommend using the dask.distributed scheduler and having the dashboard up. The chunking structure will make a very large impact on if the computation finishes or not. I think that you probably need to feel this pain viscerally a bit before these issues make much sense.\n\nI would also be totally ok with a first pass that only worked in the 2d x 2d case. This is the common case, probably much easier to get right, and much less likely to stall out with a half-finished PR.\n\nI think that if you build up a small set of interesting examples that you\'ll be able to try out different approaches, like what you propose above, and see how they work. I think that this will quickly give you more intuition than anyone here has. The approach that you lay out above sounds sensible to me, but I would bet large amounts of money that it\'s wrong. I don\'t think that anyone today knows enough to be able to tell if any approach is solid or not. I\'m certainly not able to do so.\n\nGenevieveBuckley commented\n\nOk, I think I have it now.\n\nThe approach I outlined above was wrong - you shouldn\'t focus on the shape of the final output array to work out the optimal chunk sizes. Instead, you should focus on trying to maximize the chunk length/width along the axes that will be summed over by tensordot.\n\nI\'ve updated my gist with this demo\n\nimport dask import dask.array as da import numpy as np def _inner_axes(a_ndim, b_ndim, axes): """"""Given tensordot axes argument, return list of axes to sum over."""""" if isinstance(axes, (int, float)): if axes == 0: inner_axes_a = [] inner_axes_b = [] elif axes > 0: inner_axes_a = list(range(a_ndim))[-axes:] inner_axes_b = list(range(b_ndim))[:axes] else: axes_a, axes_b = axes if isinstance(axes_a, (int, float)): axes_a = [axes_a] if isinstance(axes_b, (int, float)): axes_b = [axes_b] inner_axes_a = [i for i in range(a.ndim) if i in axes_a] inner_axes_b = [i for i in range(b.ndim) if i in axes_b] return inner_axes_a, inner_axes_b def find_optimal_chunks(array, inner_axes, limit=None): if limit is None: limit = dask.utils.parse_bytes(dask.config.get(\'array.chunk-size\')) inner_chunks = [array.shape[ax] for ax in inner_axes] while limit < (np.prod(inner_chunks) * array.dtype.itemsize): inner_chunks[np.argmax(inner_chunks)] = np.max(inner_chunks) // 2 optimal_chunks = [] for ax in range(array.ndim): if ax in inner_axes: idx = inner_axes.index(ax) optimal_chunks.append(inner_chunks[idx]) else: optimal_chunks.append(\'auto\') return optimal_chunks print(""Example"") a = da.ones((100, 10_000_000), chunks=(100, 10_000_000)) b = da.ones((10_000_000, 100), chunks=(10_000_000, 100)) # a = da.ones((50, 20_000_000), chunks=(50, 20_000_000)) # optimal chunks (1, 10_000_000) # b = da.ones((20_000_000, 50), chunks=(20_000_000, 50)) # optimal_chunks (10_000_000, 1) axes = [1, 0] inner_axes_a, inner_axes_b = _inner_axes(a.ndim, b.ndim, axes) optimal_chunks_a = find_optimal_chunks(a, inner_axes_a) optimal_chunks_b = find_optimal_chunks(b, inner_axes_b) print(""optimal_chunks_a:"", optimal_chunks_a) print(""optimal_chunks_b:"", optimal_chunks_b) aa = a.rechunk(optimal_chunks_a) bb = b.rechunk(optimal_chunks_b) z = da.tensordot(aa, bb, axes=axes) z = z.rechunk([\'auto\' for _ in range(z.ndim)]) z.compute() # time consuming, but stays nicely within memeory\n\nGenevieveBuckley commented\n\nOne problem: I put that final rechunk in there because for more commonly used parameters for tensordot (eg: axes=1 with 2D input arrays), we can often end up with a final output array with chunks equal to a single pixel. That\'s something to avoid, and popping a rechunk statement in here seems to work well. Except...\n\nBut there\'s also the other end of the spectrum, where not very many axes have been summed over, so the output array (and also the output chunks) can be very large. For these cases (eg: axes=0), rechunking the final output array z is extremely time consuming. What\'s worse is that it\'s time consuming while building the task graph, rather than only at computation time.\n\nRechunk the output only if the chunk sizes are very small\n\nRechunk the output only if it is 2D or smaller (maybe? this is a very indirect way to guess rechunking will take a long time)\n\nnot rechunk the final output at all\n\nGenevieveBuckley commented\n\nAlso, I haven\'t looked at how this will interact with unify_chunks - possibly poorly under some circumstances. Most likely I shouldn\'t faff around too much more with this, but instead put in a pull request so we can all play around with it a bit.\n\nJul 27, 2021 via email\n\nIt\'s entirely possible that there isn\'t a perfect chunking that makes this computation easy, and that we\'ll have to go with some solution that is the least bad. It\'s worth remembering that what we do today is not great, and that any improvement is an improvement :) …\n\nOn Tue, Jul 27, 2021 at 5:16 AM Genevieve Buckley ***@***.***> wrote: Also, I haven\'t looked at how this will interact with unify_chunks - possibly poorly under some circumstances. Most likely I shouldn\'t faff around too much more with this, but instead put in a pull request so we can all play around with it a bit. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#2225 (comment)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AACKZTAUM56R65EFIORZS5LTZ2BOFANCNFSM4DHZ7XMQ> .\n\nGenevieveBuckley commented\n\nJeremy summarized these two goals earlier:\n\nchunking to reduce communication (while keeping to a maximum chunk size)\n\nchunk to meet a specified output chunking outcome (while considering maximum chunk size; so 1 argument on output chunks, another on max memory)\n\nSo far I\'ve optimized for (1) but not necessarily (2).\n\nAddressing (1) means the computation in tensordot works nicely. Addressing (2) might help for convenience of operations after tensordot. Output chunks that are too big or too small will cause problems for subsequent operations. We might like to add some extra rules as a nod towards (2), but not too much because that is likely to impact performance.\n\nJul 27, 2021 via email\n\nI think that operations should not prematurely optimize for downstream computations. Those computations will know what is best for them. Let\'s let them decide. …\n\nOn Tue, Jul 27, 2021 at 6:29 PM Genevieve Buckley ***@***.***> wrote: Jeremy summarized these two goals earlier: 1. chunking to reduce communication (while keeping to a maximum chunk size) 2. chunk to meet a specified output chunking outcome (while considering maximum chunk size; so 1 argument on output chunks, another on max memory) So far I\'ve optimized for (1) but not necessarily (2). Addressing (1) means the computation in tensordot works nicely. Addressing (2) might help for convenience of operations *after* tensordot. Output chunks that are too big or too small will cause problems for subsequent operations. We might like to add some extra rules as a nod towards (2), but not too much because that is likely to impact performance. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#2225 (comment)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AACKZTBAPB3VBDR2SENJGEDTZ46MJANCNFSM4DHZ7XMQ> .\n\nGenevieveBuckley mentioned this issue\n\nImprove tensordot performance with auto-rechunking #7950\n\nGenevieveBuckley commented\n\nHere is a sketch of the continuum of 2D array shapes we can have for a tensordot operation: https://docs.google.com/drawings/d/1qdq60Exvz1GvVIGd54Bo74n99j1k8-QDVxYEZd5uH4A/edit?usp=sharing\n\nThe three important stages are where:\n\nThe output array is smaller than one or both input arrays (the easiest to deal with)\n\nThe output array size is equal to the largest input array\n\nThe output array size is larger than both input arrays (the hardest to deal with)\n\nFrom what I can tell, it\'s not a case of needing different rules for different situations, it\'s just that there are no perfect solutions for the harder cases. For (3) we can limit the chunksize of the output array, at the cost of data duplication across the cluster. That\'s not good, but there isn\'t another good solution available - the only other alternative will crash the computation as the workers run out of memory.\n\nGenevieveBuckley commented\n\nThere are a couple of general rules that apply everywhere:\n\nChunk boundaries should be aligned along the inner axes of the arrays\n\nWe should prioritize square shaped chunks over long and skinny chunks. It\'s a compromise between low worker memory and reducing data duplication. https://docs.google.com/drawings/d/1TqPAJtWy2PTjUJCFf_TtOVvbkTt37MtiZa52xLG4bzw/edit?usp=sharing\n\nI think this means that for any tensordot operation A.B = C, we find which array shape is largest, A, B, or C. Then we use normalize_chunks to find the biggest, squarest chunks for that array. Then, we fill in what the chunk shapes should be for the other arrays (which should be easy enough because all the chunk boundaries need to line up with one another).\n\nThank you for sharing these diagrams @GenevieveBuckley .\n\n@tomwhite , do these match your operational experience?\n\nGenevieveBuckley added the array label\n\ncharlesgauthier-udm mentioned this issue\n\nAdding the ability to use dask arrays with chunks along spatial axes pangeo-data/xESMF#280\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_5', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nOptimize tensordot with rechunk #2225\n\nmrocklin opened this issue\n\nApr 16, 2017 · 28 comments\n\nOptimize tensordot with rechunk #2225\n\nmrocklin opened this issue\n\nApr 16, 2017 · 28 comments\n\nDask.array tensordot operations can be made significantly faster by doing an initial rechunk pass, making all axes over which we intend to contract single-chunked.\n\nFor example, if we have the following block-chunked arrays:\n\nimport dask.array as da x = da.random.random((500, 500, 500), chunks=(50, 50, 50)) y = da.random.random((500, 100), chunks=(50, 50))\n\nAnd we want to contract over the 1st and 0th dimensions respectively\n\nda.tensordot(x, y, axes=[1, 0]).visualize()\n\nThen we may want to preemptively rechunk so that those axes have only one chunk as follows:\n\nx2 = x.rechunk((..., 500, ...)).persist() y2 = y.rechunk((500, ...)).persist()\n\nWe may want to contract other axes while we expand these ones to make sure that we don\'t produce chunks that are too large (and to ensure that the resulting tensordot chunks are not too large). In this case though the outputs are small enough even with a 10x increase in size, so we leave the other dimensions as-is.\n\nx2 = x.rechunk((50, 500, 50)).persist() y2 = y.rechunk((500, 50)).persist()\n\nThis does incur some communication costs up front, but it will generally save us more communication down the line.\n\nSo I think the question here is the following:\n\nGiven the chunks of both arrays and the axis= argument, how should we rechunk these arrays prior to the normal tensordot call. This should both increase the chunksize in the contracted axes to the full extent and possibly reduce the chunksize of the other dimension based on the expected nbytes of out output of the tensordot call.\n\nSnagged this trick from this talk: https://youtu.be/dcT6c-PrloE?t=1584\n\ncc @jcrist and @shoyer who might find this interesting. cc @pitrou who did the rechunk logic and might be able to recommend something.\n\nThe text was updated successfully, but these errors were encountered:\n\n@TomAugspurger if you find yourself with free time you might find this issue interesting. I also think that it is valuable for distributing algorithms that you care about.\n\n@pitrou if you find yourself with some free time you might find this task interesting. The speedups can be substantial and this has relevance for future paid work.\n\nmrocklin mentioned this issue\n\nlz4: first try passing memoryview before fallback dask/distributed#1155\n\n@mrocklin I followed link to this issue from your SciPy talk (which as usual was full of new and exciting things). I just want to point out a related issue: preemptively determining/rechunking the output array. Perhaps with an ""out="" kwarg? The use case is computing a (1000, 1000) column correlation matrix from a 100,000 row / 1000 column csv file (let\'s ignore the format for now ;). After setting chunks to (100000, 1), I couldn\'t find a way to do this without getting a ""number of chunks increasing by factor of 1000"" warning.\n\nThis does incur some communication costs up front, but it will generally save us more communication down the line.\n\nAre there any estimates or measurements of the rechunking cost available at runtime?\n\nAn einsum implementation (#732) with rechunking would be even better, as tensordot could then call that instead.\n\nAre there any estimates or measurements of the rechunking cost available at runtime?\n\nWe know the size of every chunk both before and after. Every task has around 100-300us of overhead.\n\nAn einsum implementation (#732) with rechunking would be even better, as tensordot could then call that instead.\n\nAre you saying that einsum is likely to be faster than tensordot or that it is more general?\n\nIf you have any interest in implementing einsum then that would be quite welcome.\n\nWe know the size of every chunk both before and after. Every task has around 100-300us of overhead.\n\nI was mostly referring to the communication cost of rechunking, is there any data available on that?\n\nI mean that einsum is more general yes, since you can specify both inner and outer products (and it would do singleton expansion -- which tensordot doesn\'t seem to do?). Since that is the case, it makes sense to design rechunking on the einsum level and have those benefits flow down to tensordot automatically.\n\nI think dask has a lot of potential and would love to contribute in the near future. Feature requests like this one together with access to some of the more recent scalable linear algebra algorithms could really help scale NumPy work to new levels. I can\'t promise anything yet though, unfortunately.\n\nI suspect that the performance benefits of using level-3 BLAS operations like GEMM will encourage folks to keep tensordot around. I could be wrong though. This is something that one would have to take up with numpy.\n\nCommunication costs differ based on scheduler. We don\'t generally have this information while constructing the task graph. In my experience the computation costs of tensordot operations tend to overwhelm the communication costs, particularly when contractions are involved. This is generally true of level-3 blas operations. You can be fairly sloppy.\n\nI think it would be great to have someone pushing on dask.array for numerical linear algebra applications. I care about this topic, but tend not to pursue it personally just because of other work priorities.\n\nYou can write/implement any einsum as a batch of independent GEMMs, so it should be able to benefit from the same fast kernels that tensordot does. I also agree that tensordot is still worth having for those cases where you don\'t need the full flexibility of an einsum. Under the covers it could simply call einsum though.\n\nI think einsum is an operation where dask could really shine as a distributed scheduler, because it is an example of a powerful tool where the distribution and scheduling can make a big difference in performance.\n\nI don\'t know if you guys saw this, but I think the plan is for NumPy 1.14.0 to use optimized BLAS kernels when possible. ( numpy/numpy#9425 )\n\ndistributed and/or asynchronous algorithms for numeric methods dask/dask-glm#68\n\ndistributed and/or asynchronous algorithms for numeric methods #3241\n\nmrocklin mentioned this issue\n\nxarray.dot() dask problems pydata/xarray#2074\n\nIt\'s worth noting that despite work to make NumPy\'s einsum use optimized BLAS calls when possible, one can still get bad performance in common cases (e.g. matmul). ( numpy/numpy#7569 )\n\nThe right place to implement the optimization described in the original post is possibly in atop, rather than tensordot.\n\nEdit: I no longer believe this to be true because we also need to incorporate the sum into this process, which happens outside of atop for the tree reductions.\n\nmrocklin mentioned this issue\n\nAutomatic rechunking along particular axes #3506\n\nI spent this morning working on this problem with pen and paper. Here are my thoughts after a few hours:\n\nOur data gets replicated a number of times equal to the product of the number of chunks along each outer dimension. So considering the following example:\n\nx = da.ones((1000, 1000, 1000), chunks=(100, 200, 100)) y = da.ones((1000, 1000), chunks=(100, 200)) z = tensordot(x, y, axes=(0, 0)) # this requires 5 * 10 communications per chunk\n\nThen naively each chunk communicates with 5 * 10 other chunks. In a shared memory situation this isn\'t so bad, but in a distributed memory situation it\'s pretty unusable.\n\nThe solution then, is to decrease the number of chunks in outer (non-contracted) dimensions. The following would perform better\n\nx = da.ones((1000, 1000, 1000), chunks=(10, 1000, 500)) y = da.ones((1000, 1000), chunks=(10, 1000)) z = tensordot(x, y, axes=(0, 0)) # this requires 1 * 2 communications per chunk\n\nHowever there are some costs to be aware of here:\n\nBy increasing the chunksize of the outer dimensions we also increase the chunksize of our result, which is only outer dimensions. This might quickly become too large.\n\nOur input chunks now have very larger outer dimensions (like our output chunks) but also still have full inner dimensions. We\'ll need to rechunk these to be much smaller to compensate.\n\nIf we squeeze our inner dimensions down to 1 then we need to worry about what happens when several of these stack together when performing the sum. Generally we won\'t want our inner dimensions to decrease below the split_every parameter that controls how many intermediate chunks are used in a tree summation (this is available by dask.config.get(\'array.split_every\').\n\nThe process of rechunking also introduces constraints\n\nThere is a single communication cost across the array (but this is additive rather than multiplicative like with tensordot)\n\nRechunking also hurts streamability, or low-memory action. Consider the following example\n\nx = da.ones((100, 100), chunks=(50, 2)).rechunk((2, 50))\n\nThis requires that full (50, 50) subarrays are in memory at once\n\nSo a naive solution to this problem might be to rechunk all of the outer dimensions to the full dimension size, and rechunk all inner dimensions to be as small as possible in order to compensate, and hope that both result and all intermediate rechunkings fit comfortably in memory. This will fail in plenty of cases.\n\nHowever, we do have one benefit to consider:\n\nOnce all-but-one of the outer dimensions are single chunked, it no longer makes sense to single-chunk the last outer dimension\n\nRecall that unpleasant rechunking generally requires a full communication of the data. This made sense when we were competing against the multiplicative cost of multiplying for every chunked dimension, but once every other outer dimension has a single chunk then the multiplicative factor is 1 * 1 * 1 * 1 ... = 1, and so we can choose to accept this communication and not rechunk.\n\nI anticipate that having freedom along one outer dimension of our choice will be very helpful in the common case.\n\nSo a pragmatic common-case solution here might be to find the outer dimension that we least want to single-chunk, and then try hard to single-chunk all other outer dimensions. The measurement of chunk-desirability depends both on the amount of data in the array associated to that outer-dimension (each outer dimension is associated to exactly one array) and the current chunking of that array.\n\nmrocklin mentioned this issue\n\nAdded recursion to array/linalg/tsqr to better manage the single core bottleneck #3586\n\nWhat I\'m seeing from this discussion is two major objectives:\n\nchunking to reduce communication (while keeping to a maximum chunk size)\n\nchunk to meet a specified output chunking outcome (while considering maximum chunk size; so 1 argument on output chunks, another on max memory)\n\nThey seem to be relatively compatible.\n\nMy proposal would be to ask for a max chunk size option (like 10 MiB, possibly more readily given) and a output chunking (possibly less readily given), and work with those two objectives. Some rechunking might be needed in post processing to achieve both.\n\nx = da.ones((1000, 1000, 1000), chunks=(100, 200, 100)) # this requires 5 communications per chunk y = da.ones((1000, 1000), chunks=(100, 200)) # this requires 5 * 10 communications per chunk z = tensordot(x, y, axes=(0, 0)) # output: chunks=(200, 100, 200)\n\nx = da.ones((1000, 1000, 1000), chunks=(100, 1000, 500)) # this requires 2 communications per chunk y = da.ones((1000, 1000), chunks=(100, 500)) # this requires 1 * 2 communications per chunk z = tensordot(x, y, axes=(0, 0)) # output: chunks=(1000, 500, 500)... ouch\n\nThere may need to be some norms with regards to what ""level"" of communication is sensible.\n\nWhile theoretically, increasing chunk sizes does not involve as much communication (""transmissions"") as reducing chunk sizes, practically, when chunk sizes are reduced, workers might just be reorganizing their memory.\n\njcmgray mentioned this issue\n\nAdd support for cupy.einsum #4402\n\nmrocklin mentioned this issue\n\nmultiplication of matrices dask/distributed#2491\n\njsadler2 mentioned this issue\n\nDot product of two zarr stores slow with Dask pangeo-data/pangeo#756\n\nmrocklin mentioned this issue\n\nPairwise distance scalability sgkit-dev/sgkit#375\n\nmrocklin mentioned this issue\n\nIdentify lack of scalability in gwas_linear_regression sgkit-dev/sgkit#390\n\nGenevieveBuckley commented\n\nIf we squeeze our inner dimensions down to 1 then we need to worry about what happens when several of these stack together when performing the sum. Generally we won\'t want our inner dimensions to decrease below the split_every parameter that controls how many intermediate chunks are used in a tree summation (this is available by dask.config.get(\'array.split_every\').\n\nCan you clarify what you mean by ""squeeze our inner dimensions down to 1"" @mrocklin. Are you talking about squeezing the whole dimension, or about changing the chunking of that dimension?\n\nI also don\'t see any kind of key named split_every in the dask config dictionary, so I\'m not sure we can rely on this. I do have a key named split-large-chunks, but it has a value of None.\n\ndask.config.get(\'array.slicing.split-large-chunks\') == None # returns True\n\nGenevieveBuckley mentioned this issue\n\nAutomatically rechunk blockwise when chunk size increases substantially #7124\n\nCan you clarify what you mean by ""squeeze our inner dimensions down to 1"" @mrocklin. Are you talking about squeezing the whole dimension, or about changing the chunking of that dimension?\n\nAll of this will probably make more sense after you\'ve been beaten up a bit by tensordot. I encourage you to run things and see things fail in different ways. I\'m anticipating issues that may or may not arise after we\'ve solved a couple of problems. Probably you should ignore what I\'m about to say until you\'ve crashed your computer a couple dozen times. That being said, here we go.\n\nI\'m saying that as we increase the size of the outer dimensions, a desire to keep modest sized chunks will naturally cause us to choose chunks with smaller sizes along inner dimensions. I suspect that this will have a negative effect when we choose to perform eventual reductions. Currently when doing a reduction we first collapse each chunk, and then we collect many of those chunks together. If each chunk only has size one then we don\'t get much of a size reduction on that first pass, which could become awkward when we collect many of those chunks together. I suspect that we might run into situations where this causes us to run out of RAM.\n\nSo instead after the first pass we might choose to collect these chunks in groups, and then reduce down each group. Then we could collect the groups together and reduce them again. This gradual approach may allow us to perform the reduction in small space, even if we don\'t benefit much from fat inner dimensions.\n\nGenevieveBuckley commented\n\nThe clarification helps, thank you.\n\nGenevieveBuckley commented\n\nCopying over comments from #7124 that really belong more in this issue thread:\n\nTerminology: We didn\'t use these particular phrases in my maths classes, but here\'s what I think they\'re being used to mean in these discussions\n\ninner dimensions <- are summed over by tensordot\n\nouter dimensions <- not summed over by tensordot\n\nFor tensordot, it looks like the process should go something like this:\n\nWork out which are the outer dimension axes for arrays a and b\n\nUse the auto_chunks function to work out what the best chunk size is for the tensordot array output, given the chunk size limit from dask.config.get(\'array.chunk-size\')\n\nWork backwards from this to figure out how the input tensordot arrays should be rechunked for the outer dimension axes.\n\nInner dimension axes - do we (a) make no change to the chunking here, (b) rechunk all inner dimensions to a chunksize of 1, or (c) rechunk inner dimensions to some smaller chunksize but ?? what size they should be.\n\nRechunk the tensordot input arrays\n\nSet kwarg align_arrays=False when tensordot hands over to blockwise (might need to double check if there is any extra logic in the unify_chunks function we now need to take care of in tensordot?)\n\nI\'ve written some code for this, while I was trying to work out the process. I\'ll need a bit more clarity around the questions in point 4 and 6 above. (Hacky code here)\n\nIt is clear that the rechunking logic is going to have to happen in tensordot before it gets to blockwise, blockwise just doesn\'t know enough information to do what we want.\n\nMost of my comments below are relevant for #2225 rather than this issue (I think that that issue is a bigger deal than this one). It\'s worth noting that these are two different concerns and not necessarily fully compatible.\n\nFirst, I highly recommend multiplying two matrices two-dimensional matrices together and experimenting with chunking structure before trying to generalize any of this. I also recommend using the dask.distributed scheduler and having the dashboard up. The chunking structure will make a very large impact on if the computation finishes or not. I think that you probably need to feel this pain viscerally a bit before these issues make much sense.\n\nI would also be totally ok with a first pass that only worked in the 2d x 2d case. This is the common case, probably much easier to get right, and much less likely to stall out with a half-finished PR.\n\nI think that if you build up a small set of interesting examples that you\'ll be able to try out different approaches, like what you propose above, and see how they work. I think that this will quickly give you more intuition than anyone here has. The approach that you lay out above sounds sensible to me, but I would bet large amounts of money that it\'s wrong. I don\'t think that anyone today knows enough to be able to tell if any approach is solid or not. I\'m certainly not able to do so.\n\nGenevieveBuckley commented\n\nOk, I think I have it now.\n\nThe approach I outlined above was wrong - you shouldn\'t focus on the shape of the final output array to work out the optimal chunk sizes. Instead, you should focus on trying to maximize the chunk length/width along the axes that will be summed over by tensordot.\n\nI\'ve updated my gist with this demo\n\nimport dask import dask.array as da import numpy as np def _inner_axes(a_ndim, b_ndim, axes): """"""Given tensordot axes argument, return list of axes to sum over."""""" if isinstance(axes, (int, float)): if axes == 0: inner_axes_a = [] inner_axes_b = [] elif axes > 0: inner_axes_a = list(range(a_ndim))[-axes:] inner_axes_b = list(range(b_ndim))[:axes] else: axes_a, axes_b = axes if isinstance(axes_a, (int, float)): axes_a = [axes_a] if isinstance(axes_b, (int, float)): axes_b = [axes_b] inner_axes_a = [i for i in range(a.ndim) if i in axes_a] inner_axes_b = [i for i in range(b.ndim) if i in axes_b] return inner_axes_a, inner_axes_b def find_optimal_chunks(array, inner_axes, limit=None): if limit is None: limit = dask.utils.parse_bytes(dask.config.get(\'array.chunk-size\')) inner_chunks = [array.shape[ax] for ax in inner_axes] while limit < (np.prod(inner_chunks) * array.dtype.itemsize): inner_chunks[np.argmax(inner_chunks)] = np.max(inner_chunks) // 2 optimal_chunks = [] for ax in range(array.ndim): if ax in inner_axes: idx = inner_axes.index(ax) optimal_chunks.append(inner_chunks[idx]) else: optimal_chunks.append(\'auto\') return optimal_chunks print(""Example"") a = da.ones((100, 10_000_000), chunks=(100, 10_000_000)) b = da.ones((10_000_000, 100), chunks=(10_000_000, 100)) # a = da.ones((50, 20_000_000), chunks=(50, 20_000_000)) # optimal chunks (1, 10_000_000) # b = da.ones((20_000_000, 50), chunks=(20_000_000, 50)) # optimal_chunks (10_000_000, 1) axes = [1, 0] inner_axes_a, inner_axes_b = _inner_axes(a.ndim, b.ndim, axes) optimal_chunks_a = find_optimal_chunks(a, inner_axes_a) optimal_chunks_b = find_optimal_chunks(b, inner_axes_b) print(""optimal_chunks_a:"", optimal_chunks_a) print(""optimal_chunks_b:"", optimal_chunks_b) aa = a.rechunk(optimal_chunks_a) bb = b.rechunk(optimal_chunks_b) z = da.tensordot(aa, bb, axes=axes) z = z.rechunk([\'auto\' for _ in range(z.ndim)]) z.compute() # time consuming, but stays nicely within memeory\n\nGenevieveBuckley commented\n\nOne problem: I put that final rechunk in there because for more commonly used parameters for tensordot (eg: axes=1 with 2D input arrays), we can often end up with a final output array with chunks equal to a single pixel. That\'s something to avoid, and popping a rechunk statement in here seems to work well. Except...\n\nBut there\'s also the other end of the spectrum, where not very many axes have been summed over, so the output array (and also the output chunks) can be very large. For these cases (eg: axes=0), rechunking the final output array z is extremely time consuming. What\'s worse is that it\'s time consuming while building the task graph, rather than only at computation time.\n\nRechunk the output only if the chunk sizes are very small\n\nRechunk the output only if it is 2D or smaller (maybe? this is a very indirect way to guess rechunking will take a long time)\n\nnot rechunk the final output at all\n\nGenevieveBuckley commented\n\nAlso, I haven\'t looked at how this will interact with unify_chunks - possibly poorly under some circumstances. Most likely I shouldn\'t faff around too much more with this, but instead put in a pull request so we can all play around with it a bit.\n\nJul 27, 2021 via email\n\nIt\'s entirely possible that there isn\'t a perfect chunking that makes this computation easy, and that we\'ll have to go with some solution that is the least bad. It\'s worth remembering that what we do today is not great, and that any improvement is an improvement :) …\n\nOn Tue, Jul 27, 2021 at 5:16 AM Genevieve Buckley ***@***.***> wrote: Also, I haven\'t looked at how this will interact with unify_chunks - possibly poorly under some circumstances. Most likely I shouldn\'t faff around too much more with this, but instead put in a pull request so we can all play around with it a bit. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#2225 (comment)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AACKZTAUM56R65EFIORZS5LTZ2BOFANCNFSM4DHZ7XMQ> .\n\nGenevieveBuckley commented\n\nJeremy summarized these two goals earlier:\n\nchunking to reduce communication (while keeping to a maximum chunk size)\n\nchunk to meet a specified output chunking outcome (while considering maximum chunk size; so 1 argument on output chunks, another on max memory)\n\nSo far I\'ve optimized for (1) but not necessarily (2).\n\nAddressing (1) means the computation in tensordot works nicely. Addressing (2) might help for convenience of operations after tensordot. Output chunks that are too big or too small will cause problems for subsequent operations. We might like to add some extra rules as a nod towards (2), but not too much because that is likely to impact performance.\n\nJul 27, 2021 via email\n\nI think that operations should not prematurely optimize for downstream computations. Those computations will know what is best for them. Let\'s let them decide. …\n\nOn Tue, Jul 27, 2021 at 6:29 PM Genevieve Buckley ***@***.***> wrote: Jeremy summarized these two goals earlier: 1. chunking to reduce communication (while keeping to a maximum chunk size) 2. chunk to meet a specified output chunking outcome (while considering maximum chunk size; so 1 argument on output chunks, another on max memory) So far I\'ve optimized for (1) but not necessarily (2). Addressing (1) means the computation in tensordot works nicely. Addressing (2) might help for convenience of operations *after* tensordot. Output chunks that are too big or too small will cause problems for subsequent operations. We might like to add some extra rules as a nod towards (2), but not too much because that is likely to impact performance. — You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <#2225 (comment)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AACKZTBAPB3VBDR2SENJGEDTZ46MJANCNFSM4DHZ7XMQ> .\n\nGenevieveBuckley mentioned this issue\n\nImprove tensordot performance with auto-rechunking #7950\n\nGenevieveBuckley commented\n\nHere is a sketch of the continuum of 2D array shapes we can have for a tensordot operation: https://docs.google.com/drawings/d/1qdq60Exvz1GvVIGd54Bo74n99j1k8-QDVxYEZd5uH4A/edit?usp=sharing\n\nThe three important stages are where:\n\nThe output array is smaller than one or both input arrays (the easiest to deal with)\n\nThe output array size is equal to the largest input array\n\nThe output array size is larger than both input arrays (the hardest to deal with)\n\nFrom what I can tell, it\'s not a case of needing different rules for different situations, it\'s just that there are no perfect solutions for the harder cases. For (3) we can limit the chunksize of the output array, at the cost of data duplication across the cluster. That\'s not good, but there isn\'t another good solution available - the only other alternative will crash the computation as the workers run out of memory.\n\nGenevieveBuckley commented\n\nThere are a couple of general rules that apply everywhere:\n\nChunk boundaries should be aligned along the inner axes of the arrays\n\nWe should prioritize square shaped chunks over long and skinny chunks. It\'s a compromise between low worker memory and reducing data duplication. https://docs.google.com/drawings/d/1TqPAJtWy2PTjUJCFf_TtOVvbkTt37MtiZa52xLG4bzw/edit?usp=sharing\n\nI think this means that for any tensordot operation A.B = C, we find which array shape is largest, A, B, or C. Then we use normalize_chunks to find the biggest, squarest chunks for that array. Then, we fill in what the chunk shapes should be for the other arrays (which should be easy enough because all the chunk boundaries need to line up with one another).\n\nThank you for sharing these diagrams @GenevieveBuckley .\n\n@tomwhite , do these match your operational experience?\n\nGenevieveBuckley added the array label\n\ncharlesgauthier-udm mentioned this issue\n\nAdding the ability to use dask arrays with chunks along spatial axes pangeo-data/xESMF#280\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-04-25T19:08:58', 'title': 'Optimize tensordot with rechunk · Issue #2225 · dask/dask', 'url': 'https://github.com/dask/dask/issues/2225'}), Document(page_content=""Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\npydata / xarray Public\n\nImplement tensordot for xarray with dask support #723\n\ndeanpospisil opened this issue\n\nJan 26, 2016 · 8 comments\n\nImplement tensordot for xarray with dask support #723\n\ndeanpospisil opened this issue\n\nJan 26, 2016 · 8 comments\n\ndeanpospisil commented\n\nI've started using X-ray to store responses from convolutional neural nets over different transformations of images (translation(x,y), rotation (radians), etc). So far its been very intuitive storing and transforming results, unfortunately much of my analysis requires the use of tensor dot products, where I can choose arbitrary dimensions over which to make a projection, or perform a correlation. While dask implements np.tensordot, xray does not.\n\nOne can implement a dot product manually by multiplying data arrays then summing over dimensions.\n\nfitm = (da_response*da_model).sum('imageID').sum('x_translation').max('models')\n\nbut this ends up being very slow, as I imagine when dot products are implemented by numpy or dask, there is a fair amount of optimization going on.\n\nI am relatively new to GitHub, and this project, would you have any advice on the best way to contribute this functionality? tensordot where in you can put in a list of dimension names in two dataarray over which to compute a sum product, using dasks implementation.\n\nThe text was updated successfully, but these errors were encountered:\n\nYes, this would be a nice addition!\n\nI spent a little bit of a time futzing around with this to see if there is an elegant way to plug this into our existing dispatching system. The short of it is that the answer appears to be no -- we don't have any elegant equivalent to dask.array's generic atop method.\n\nSo, for now I would simply write a function specialized to DataArray objects. Something like the following (barely tested) is a starting point:\n\nfrom xarray import align, DataArray # note: using private imports (e.g., from xarray.core) is definitely discouraged! # this is not guaranteed to work in future versions of xarray from xarray.core.ops import _dask_or_eager_func def tensordot(a, b, dims): if not (isinstance(a, DataArray) and isinstance(b, DataArray)): raise ValueError a, b = align(a, b, join='inner', copy=False) axes = (a.get_axis_num(dims), b.get_axis_num(dims)) f = _dask_or_eager_func('tensordot', n_array_args=2) new_data = f(a.data, b.data, axes=axes) if isinstance(dims, basestring): dims = [dims] new_coords = a.coords.merge(b.coords).drop(dims) new_dims = ([d for d in a.dims if d not in dims] + [d for d in b.dims if d not in dims]) return DataArray(new_data, new_coords, new_dims)\n\nThis would be worth cleaning up so we could add it to the codebase (mostly documentation & tests).\n\n@MaximilianR I do like einsum, but I'm not sure the API would be a good fit for xarray (we already have dimension names), and it also does not exist yet for dask (dask/dask#732).\n\nThat said, I suppose you could make an xarray version of einsum with syntax that looks more like tensordot with *args, e.g., einsum(a, b, c, dims=('x', 'y')).\n\n@shoyer - I thought your answer dominated mine, so I left yours as the only response. But yup, that form of einsum would be pretty nice...\n\nshoyer changed the title\n\nImplementing dask tensordot\n\nImplement tensordot for xarray with dask support\n\ndeanpospisil commented\n\nLooks like it can perform tensor dot for dask and straight xarrays! But apparently dask has not implemented tensordot with multiple axes arguments, and it also does not work performing a tensor dot between a dask xarray and an xarray. Neither of these cases worries me too much, hopefully they don't worry you.\n\nfrom xarray import align, DataArray #note: using private imports (e.g., from xarray.core) is definitely discouraged! #this is not guaranteed to work in future versions of xarray from xarray.core.ops import _dask_or_eager_func def tensordot(a, b, dims): if not (isinstance(a, DataArray) and isinstance(b, DataArray)): raise ValueError a, b = align(a, b, join='inner', copy=False) axes = (a.get_axis_num(dims), b.get_axis_num(dims)) f = _dask_or_eager_func('tensordot', n_array_args=2) new_data = f(a.data, b.data, axes=axes) if isinstance(dims, str): dims = [dims] new_coords = a.coords.merge(b.coords).drop(dims) #drop the dims you are performing the sum product over new_dims = ([d for d in a.dims if d not in dims] + [d for d in b.dims if d not in dims]) return DataArray(new_data, new_coords, new_dims) import xarray as xr import numpy as np x_trans = np.linspace(-3,3,6) y_trans = np.linspace(-3,3,5) imgID = range(4) da = xr.DataArray( np.ones((6,5,4)), coords = [ x_trans, y_trans, imgID ], dims = ['x_trans', 'y_trans', 'imgID'] ) models = range(20) dm = xr.DataArray( np.ones(( 20 , 5, 4 )), coords = [ models, y_trans, imgID], dims = [ 'models', 'y_trans', 'imgID' ] ) #xarray tensordot proj_a = tensordot(da, dm, 'imgID') #dask xarray tensor dot da = da.chunk() dm = dm.chunk() proj_b = tensordot(da, dm, 'imgID') #errors #multiple dims proj_c = tensordot(da, dm, ['imgID', 'y_trans']) #mixed types da = da.chunk() dm = dm.load() proj_d = tensordot(da, dm, 'imgID')\n\ndeanpospisil commented\n\nI wasn't sure where the best place to put the def would be. Currently I have been running it from the xarray class: t = da1.tensordot( da2, 'shapes' ) Let me know if that seems alright, then I'll write some simple tests in test_dataarray for tensor dot. Maybe make my first pull request!\n\ndeanpospisil commented\n\nAlso that einsum does seem pretty ideal. I'll see if I can get it running in dask, so we can port it over here.\n\nI'm split on whether a function or method makes more sense (a.tensordot(b, dim='x') vs xr.tensordot(a, b, dim='x')). I would be OK with either, so yes, please do go ahead!\n\nAdd tensordot to dataarray class also add its test to test_dataarray #731\n\nImplement vnorm for xarray with dask support #735\n\nshoyer closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time."", metadata={'id': 'web-search_4', 'snippet': ""Search or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\npydata / xarray Public\n\nImplement tensordot for xarray with dask support #723\n\ndeanpospisil opened this issue\n\nJan 26, 2016 · 8 comments\n\nImplement tensordot for xarray with dask support #723\n\ndeanpospisil opened this issue\n\nJan 26, 2016 · 8 comments\n\ndeanpospisil commented\n\nI've started using X-ray to store responses from convolutional neural nets over different transformations of images (translation(x,y), rotation (radians), etc). So far its been very intuitive storing and transforming results, unfortunately much of my analysis requires the use of tensor dot products, where I can choose arbitrary dimensions over which to make a projection, or perform a correlation. While dask implements np.tensordot, xray does not.\n\nOne can implement a dot product manually by multiplying data arrays then summing over dimensions.\n\nfitm = (da_response*da_model).sum('imageID').sum('x_translation').max('models')\n\nbut this ends up being very slow, as I imagine when dot products are implemented by numpy or dask, there is a fair amount of optimization going on.\n\nI am relatively new to GitHub, and this project, would you have any advice on the best way to contribute this functionality? tensordot where in you can put in a list of dimension names in two dataarray over which to compute a sum product, using dasks implementation.\n\nThe text was updated successfully, but these errors were encountered:\n\nYes, this would be a nice addition!\n\nI spent a little bit of a time futzing around with this to see if there is an elegant way to plug this into our existing dispatching system. The short of it is that the answer appears to be no -- we don't have any elegant equivalent to dask.array's generic atop method.\n\nSo, for now I would simply write a function specialized to DataArray objects. Something like the following (barely tested) is a starting point:\n\nfrom xarray import align, DataArray # note: using private imports (e.g., from xarray.core) is definitely discouraged! # this is not guaranteed to work in future versions of xarray from xarray.core.ops import _dask_or_eager_func def tensordot(a, b, dims): if not (isinstance(a, DataArray) and isinstance(b, DataArray)): raise ValueError a, b = align(a, b, join='inner', copy=False) axes = (a.get_axis_num(dims), b.get_axis_num(dims)) f = _dask_or_eager_func('tensordot', n_array_args=2) new_data = f(a.data, b.data, axes=axes) if isinstance(dims, basestring): dims = [dims] new_coords = a.coords.merge(b.coords).drop(dims) new_dims = ([d for d in a.dims if d not in dims] + [d for d in b.dims if d not in dims]) return DataArray(new_data, new_coords, new_dims)\n\nThis would be worth cleaning up so we could add it to the codebase (mostly documentation & tests).\n\n@MaximilianR I do like einsum, but I'm not sure the API would be a good fit for xarray (we already have dimension names), and it also does not exist yet for dask (dask/dask#732).\n\nThat said, I suppose you could make an xarray version of einsum with syntax that looks more like tensordot with *args, e.g., einsum(a, b, c, dims=('x', 'y')).\n\n@shoyer - I thought your answer dominated mine, so I left yours as the only response. But yup, that form of einsum would be pretty nice...\n\nshoyer changed the title\n\nImplementing dask tensordot\n\nImplement tensordot for xarray with dask support\n\ndeanpospisil commented\n\nLooks like it can perform tensor dot for dask and straight xarrays! But apparently dask has not implemented tensordot with multiple axes arguments, and it also does not work performing a tensor dot between a dask xarray and an xarray. Neither of these cases worries me too much, hopefully they don't worry you.\n\nfrom xarray import align, DataArray #note: using private imports (e.g., from xarray.core) is definitely discouraged! #this is not guaranteed to work in future versions of xarray from xarray.core.ops import _dask_or_eager_func def tensordot(a, b, dims): if not (isinstance(a, DataArray) and isinstance(b, DataArray)): raise ValueError a, b = align(a, b, join='inner', copy=False) axes = (a.get_axis_num(dims), b.get_axis_num(dims)) f = _dask_or_eager_func('tensordot', n_array_args=2) new_data = f(a.data, b.data, axes=axes) if isinstance(dims, str): dims = [dims] new_coords = a.coords.merge(b.coords).drop(dims) #drop the dims you are performing the sum product over new_dims = ([d for d in a.dims if d not in dims] + [d for d in b.dims if d not in dims]) return DataArray(new_data, new_coords, new_dims) import xarray as xr import numpy as np x_trans = np.linspace(-3,3,6) y_trans = np.linspace(-3,3,5) imgID = range(4) da = xr.DataArray( np.ones((6,5,4)), coords = [ x_trans, y_trans, imgID ], dims = ['x_trans', 'y_trans', 'imgID'] ) models = range(20) dm = xr.DataArray( np.ones(( 20 , 5, 4 )), coords = [ models, y_trans, imgID], dims = [ 'models', 'y_trans', 'imgID' ] ) #xarray tensordot proj_a = tensordot(da, dm, 'imgID') #dask xarray tensor dot da = da.chunk() dm = dm.chunk() proj_b = tensordot(da, dm, 'imgID') #errors #multiple dims proj_c = tensordot(da, dm, ['imgID', 'y_trans']) #mixed types da = da.chunk() dm = dm.load() proj_d = tensordot(da, dm, 'imgID')\n\ndeanpospisil commented\n\nI wasn't sure where the best place to put the def would be. Currently I have been running it from the xarray class: t = da1.tensordot( da2, 'shapes' ) Let me know if that seems alright, then I'll write some simple tests in test_dataarray for tensor dot. Maybe make my first pull request!\n\ndeanpospisil commented\n\nAlso that einsum does seem pretty ideal. I'll see if I can get it running in dask, so we can port it over here.\n\nI'm split on whether a function or method makes more sense (a.tensordot(b, dim='x') vs xr.tensordot(a, b, dim='x')). I would be OK with either, so yes, please do go ahead!\n\nAdd tensordot to dataarray class also add its test to test_dataarray #731\n\nImplement vnorm for xarray with dask support #735\n\nshoyer closed this as completed\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time."", 'timestamp': '2024-01-08T14:58:03', 'title': 'Implement tensordot for xarray with dask support · Issue #723 · pydata/xarray', 'url': 'https://github.com/pydata/xarray/issues/723'})], [Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to Tensors\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport tensorflow as tf import numpy as np\n\n2023-10-28 01:21:58.219231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2023-10-28 01:21:58.219277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2023-10-28 01:21:58.220822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nTensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes at tf.dtypes.\n\nIf you\'re familiar with NumPy, tensors are (kind of) like np.arrays.\n\nAll tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.\n\nFirst, create some basic tensors.\n\nHere is a ""scalar"" or ""rank-0"" tensor . A scalar contains a single value, and no ""axes"".\n\n# This will be an int32 tensor by default; see ""dtypes"" below. rank_0_tensor = tf.constant(4) print(rank_0_tensor)\n\ntf.Tensor(4, shape=(), dtype=int32)\n\nA ""vector"" or ""rank-1"" tensor is like a list of values. A vector has one axis:\n\n# Let\'s make this a float tensor. rank_1_tensor = tf.constant([2.0, 3.0, 4.0]) print(rank_1_tensor)\n\ntf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n\nA ""matrix"" or ""rank-2"" tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time rank_2_tensor = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float16) print(rank_2_tensor)\n\ntf.Tensor( [[1. 2.] [3. 4.] [5. 6.]], shape=(3, 2), dtype=float16)\n\nA vector, shape: [3]\n\nA matrix, shape: [3, 2]\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of # axes (sometimes called ""dimensions"") rank_3_tensor = tf.constant([ [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]], [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]], [[20, 21, 22, 23, 24], [25, 26, 27, 28, 29]],]) print(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nThere are many ways you might visualize a tensor with more than two axes.\n\nA 3-axis tensor, shape: [3, 2, 5]\n\nYou can convert a tensor to a NumPy array either using np.array or the tensor.numpy method:\n\nnp.array(rank_2_tensor)\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nrank_2_tensor.numpy()\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nTensors often contain floats and ints, but have many other types, including:\n\nThe base tf.Tensor class requires tensors to be ""rectangular""---that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\n\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na = tf.constant([[1, 2], [3, 4]]) b = tf.constant([[1, 1], [1, 1]]) # Could have also said `tf.ones([2,2], dtype=tf.int32)` print(tf.add(a, b), ""\\n"") print(tf.multiply(a, b), ""\\n"") print(tf.matmul(a, b), ""\\n"")\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nprint(a + b, ""\\n"") # element-wise addition print(a * b, ""\\n"") # element-wise multiplication print(a @ b, ""\\n"") # matrix multiplication\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nTensors are used in all kinds of operations (or ""Ops"").\n\nc = tf.constant([[4.0, 5.0], [10.0, 1.0]]) # Find the largest value print(tf.reduce_max(c)) # Find the index of the largest value print(tf.math.argmax(c)) # Compute the softmax print(tf.nn.softmax(c))\n\ntf.Tensor(10.0, shape=(), dtype=float32) tf.Tensor([1 0], shape=(2,), dtype=int64) tf.Tensor( [[2.6894143e-01 7.3105854e-01] [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)\n\nNote: Typically, anywhere a TensorFlow function expects a Tensor as input, the function will also accept anything that can be converted to a Tensor using tf.convert_to_tensor. See below for an example.\n\ntf.convert_to_tensor([1,2,3])\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n\ntf.reduce_max([1,2,3])\n\n<tf.Tensor: shape=(), dtype=int32, numpy=3>\n\ntf.reduce_max(np.array([1,2,3]))\n\n<tf.Tensor: shape=(), dtype=int64, numpy=3>\n\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\n\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\n\nAxis or Dimension: A particular dimension of a tensor.\n\nSize: The total number of items in the tensor, the product of the shape vector\'s elements.\n\nNote: Although you may see reference to a ""tensor of two dimensions"", a rank-2 tensor does not usually describe a 2D space.\n\nTensors and tf.TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor = tf.zeros([3, 2, 4, 5])\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\nprint(""Type of every element:"", rank_4_tensor.dtype) print(""Number of axes:"", rank_4_tensor.ndim) print(""Shape of tensor:"", rank_4_tensor.shape) print(""Elements along axis 0 of tensor:"", rank_4_tensor.shape[0]) print(""Elements along the last axis of tensor:"", rank_4_tensor.shape[-1]) print(""Total number of elements (3*2*4*5): "", tf.size(rank_4_tensor).numpy())\n\nType of every element: <dtype: \'float32\'> Number of axes: 4 Shape of tensor: (3, 2, 4, 5) Elements along axis 0 of tensor: 3 Elements along the last axis of tensor: 5 Total number of elements (3*2*4*5): 120\n\nBut note that the Tensor.ndim and Tensor.shape attributes don\'t return Tensor objects. If you need a Tensor use the tf.rank or tf.shape function. This difference is subtle, but it can be important when building graphs (later).\n\ntf.rank(rank_4_tensor)\n\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n\ntf.shape(rank_4_tensor)\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 2, 4, 5], dtype=int32)>\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\nSingle-axis indexing\n\nTensorFlow follows standard Python indexing rules, similar to indexing a list or a string in Python, and the basic rules for NumPy indexing.\n\nnegative indices count backwards from the end\n\ncolons, :, are used for slices: start:stop:step\n\nrank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34]) print(rank_1_tensor.numpy())\n\n[ 0 1 1 2 3 5 8 13 21 34]\n\nIndexing with a scalar removes the axis:\n\nprint(""First:"", rank_1_tensor[0].numpy()) print(""Second:"", rank_1_tensor[1].numpy()) print(""Last:"", rank_1_tensor[-1].numpy())\n\nFirst: 0 Second: 1 Last: 34\n\nIndexing with a : slice keeps the axis:\n\nprint(""Everything:"", rank_1_tensor[:].numpy()) print(""Before 4:"", rank_1_tensor[:4].numpy()) print(""From 4 to the end:"", rank_1_tensor[4:].numpy()) print(""From 2, before 7:"", rank_1_tensor[2:7].numpy()) print(""Every other item:"", rank_1_tensor[::2].numpy()) print(""Reversed:"", rank_1_tensor[::-1].numpy())\n\nEverything: [ 0 1 1 2 3 5 8 13 21 34] Before 4: [0 1 1 2] From 4 to the end: [ 3 5 8 13 21 34] From 2, before 7: [1 2 3 5 8] Every other item: [ 0 1 3 8 21] Reversed: [34 21 13 8 5 3 2 1 1 0]\n\nHigher rank tensors are indexed by passing multiple indices.\n\nThe exact same rules as in the single-axis case apply to each axis independently.\n\nprint(rank_2_tensor.numpy())\n\n[[1. 2.] [3. 4.] [5. 6.]]\n\nPassing an integer for each index, the result is a scalar.\n\n# Pull out a single value from a 2-rank tensor print(rank_2_tensor[1, 1].numpy())\n\nYou can index using any combination of integers and slices:\n\n# Get row and column tensors print(""Second row:"", rank_2_tensor[1, :].numpy()) print(""Second column:"", rank_2_tensor[:, 1].numpy()) print(""Last row:"", rank_2_tensor[-1, :].numpy()) print(""First item in last column:"", rank_2_tensor[0, -1].numpy()) print(""Skip the first row:"") print(rank_2_tensor[1:, :].numpy(), ""\\n"")\n\nSecond row: [3. 4.] Second column: [2. 4. 6.] Last row: [5. 6.] First item in last column: 2.0 Skip the first row: [[3. 4.] [5. 6.]]\n\nHere is an example with a 3-axis tensor:\n\nprint(rank_3_tensor[:, :, 4])\n\ntf.Tensor( [[ 4 9] [14 19] [24 29]], shape=(3, 2), dtype=int32)\n\nSelecting the last feature across all locations in each example in the batch\n\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors.\n\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis x = tf.constant([[1], [2], [3]]) print(x.shape)\n\n# You can convert this object into a Python list, too print(x.shape.as_list())\n\nYou can reshape a tensor into a new shape. The tf.reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape. # Note that you\'re passing in a list reshaped = tf.reshape(x, [1, 3])\n\nprint(x.shape) print(reshaped.shape)\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style ""row-major"" memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nprint(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says ""Whatever fits"". print(tf.reshape(rank_3_tensor, [-1]))\n\ntf.Tensor( [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29], shape=(30,), dtype=int32)\n\nTypically the only reasonable use of tf.reshape is to combine or split adjacent axes (or add/remove 1s).\n\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\nprint(tf.reshape(rank_3_tensor, [3*2, 5]), ""\\n"") print(tf.reshape(rank_3_tensor, [3, -1]))\n\ntf.Tensor( [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]], shape=(6, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n\nReshaping will ""work"" for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\n\nSwapping axes in tf.reshape does not work; you need tf.transpose for that.\n\n# Bad examples: don\'t do this # You can\'t reorder axes with reshape. print(tf.reshape(rank_3_tensor, [2, 3, 5]), ""\\n"") # This is a mess print(tf.reshape(rank_3_tensor, [5, 6]), ""\\n"") # This doesn\'t work at all try: tf.reshape(rank_3_tensor, [7, -1]) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]] [[15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) InvalidArgumentError: { {function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0} } Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n\nYou may run across not-fully-specified shapes. Either the shape contains a None (an axis-length is unknown) or the whole shape is None (the rank of the tensor is unknown).\n\nExcept for tf.RaggedTensor, such shapes will only occur in the context of TensorFlow\'s symbolic, graph-building APIs:\n\nThe keras functional API.\n\nTo inspect a tf.Tensor\'s data type use the Tensor.dtype property.\n\nWhen creating a tf.Tensor from a Python object you may optionally specify the datatype.\n\nIf you don\'t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts Python integers to tf.int32 and Python floating point numbers to tf.float32. Otherwise TensorFlow uses the same rules NumPy uses when converting to arrays.\n\nYou can cast from type to type.\n\nthe_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64) the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16) # Now, cast to an uint8 and lose the decimal precision the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8) print(the_u8_tensor)\n\ntf.Tensor([2 3 4], shape=(3,), dtype=uint8)\n\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are ""stretched"" automatically to fit larger tensors when running combined operations on them.\n\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx = tf.constant([1, 2, 3]) y = tf.constant(2) z = tf.constant([2, 2, 2]) # All of these are the same computation print(tf.multiply(x, 2)) print(x * y) print(x * z)\n\ntf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\n\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations x = tf.reshape(x,[3,1]) y = tf.range(1, 5) print(x, ""\\n"") print(y, ""\\n"") print(tf.multiply(x, y))\n\ntf.Tensor( [[1] [2] [3]], shape=(3, 1), dtype=int32) tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) tf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\nHere is the same operation without broadcasting:\n\nx_stretch = tf.constant([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]) y_stretch = tf.constant([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]) print(x_stretch * y_stretch) # Again, operator overloading\n\ntf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\n\nYou see what broadcasting looks like using tf.broadcast_to.\n\nprint(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))\n\ntf.Tensor( [[1 2 3] [1 2 3] [1 2 3]], shape=(3, 3), dtype=int32)\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\n\nIt can get even more complicated. This section of Jake VanderPlas\'s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy).\n\ntf.convert_to_tensor\n\nMost ops, like tf.matmul and tf.reshape take arguments of class tf.Tensor. However, you\'ll notice in the above case, Python objects shaped like tensors are accepted.\n\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy\'s ndarray, TensorShape, Python lists, and tf.Variable will all convert automatically.\n\nSee tf.register_tensor_conversion_function for more details, and if you have your own type you\'d like to automatically convert to a tensor.\n\nA tensor with variable numbers of elements along some axis is called ""ragged"". Use tf.ragged.RaggedTensor for ragged data.\n\nFor example, This cannot be represented as a regular tensor:\n\nA tf.RaggedTensor, shape: [4, None]\n\nragged_list = [ [0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]\n\ntry: tensor = tf.constant(ragged_list) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\nValueError: Can\'t convert non-rectangular Python sequence to Tensor.\n\nInstead create a tf.RaggedTensor using tf.ragged.constant:\n\nragged_tensor = tf.ragged.constant(ragged_list) print(ragged_tensor)\n\n<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>\n\nThe shape of a tf.RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor.shape)\n\ntf.string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\n\nThe strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the axes of the tensor. See tf.strings for functions to manipulate them.\n\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string. scalar_string_tensor = tf.constant(""Gray wolf"") print(scalar_string_tensor)\n\ntf.Tensor(b\'Gray wolf\', shape=(), dtype=string)\n\nAnd a vector of strings:\n\nA vector of strings, shape: [3,]\n\n# If you have three string tensors of different lengths, this is OK. tensor_of_strings = tf.constant([""Gray wolf"", ""Quick brown fox"", ""Lazy dog""]) # Note that the shape is (3,). The string length is not included. print(tensor_of_strings)\n\ntf.Tensor([b\'Gray wolf\' b\'Quick brown fox\' b\'Lazy dog\'], shape=(3,), dtype=string)\n\nIn the above printout the b prefix indicates that tf.string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\n\nIf you pass unicode characters they are utf-8 encoded.\n\ntf.constant(""🥳👍"")\n\n<tf.Tensor: shape=(), dtype=string, numpy=b\'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d\'>\n\nSome basic functions with strings can be found in tf.strings, including tf.strings.split.\n\n# You can use split to split a string into a set of tensors print(tf.strings.split(scalar_string_tensor, sep="" ""))\n\ntf.Tensor([b\'Gray\' b\'wolf\'], shape=(2,), dtype=string)\n\n# ...but it turns into a `RaggedTensor` if you split up a tensor of strings, # as each string might be split into a different number of parts. print(tf.strings.split(tensor_of_strings))\n\n<tf.RaggedTensor [[b\'Gray\', b\'wolf\'], [b\'Quick\', b\'brown\', b\'fox\'], [b\'Lazy\', b\'dog\']]>\n\nThree strings split, shape: [3, None]\n\nAnd tf.strings.to_number:\n\ntext = tf.constant(""1 10 100"") print(tf.strings.to_number(tf.strings.split(text, "" "")))\n\ntf.Tensor([ 1. 10. 100.], shape=(3,), dtype=float32)\n\nAlthough you can\'t use tf.cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings = tf.strings.bytes_split(tf.constant(""Duck"")) byte_ints = tf.io.decode_raw(tf.constant(""Duck""), tf.uint8) print(""Byte strings:"", byte_strings) print(""Bytes:"", byte_ints)\n\nByte strings: tf.Tensor([b\'D\' b\'u\' b\'c\' b\'k\'], shape=(4,), dtype=string) Bytes: tf.Tensor([ 68 117 99 107], shape=(4,), dtype=uint8)\n\n# Or split it up as unicode and then decode it unicode_bytes = tf.constant(""アヒル 🦆"") unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, ""UTF-8"") unicode_values = tf.strings.unicode_decode(unicode_bytes, ""UTF-8"") print(""\\nUnicode bytes:"", unicode_bytes) print(""\\nUnicode chars:"", unicode_char_bytes) print(""\\nUnicode values:"", unicode_values)\n\nUnicode bytes: tf.Tensor(b\'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86\', shape=(), dtype=string) Unicode chars: tf.Tensor([b\'\\xe3\\x82\\xa2\' b\'\\xe3\\x83\\x92\' b\'\\xe3\\x83\\xab\' b\' \' b\'\\xf0\\x9f\\xa6\\x86\'], shape=(5,), dtype=string) Unicode values: tf.Tensor([ 12450 12498 12523 32 129414], shape=(5,), dtype=int32)\n\nThe tf.string dtype is used for all raw bytes data in TensorFlow. The tf.io module contains functions for converting data to and from bytes, including decoding images and parsing csv.\n\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf.sparse.SparseTensor and related operations to store sparse data efficiently.\n\nA tf.SparseTensor, shape: [3, 4]\n\n# Sparse tensors store values by index in a memory-efficient manner sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]) print(sparse_tensor, ""\\n"") # You can convert sparse tensors to dense print(tf.sparse.to_dense(sparse_tensor))\n\nSparseTensor(indices=tf.Tensor( [[0 0] [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) tf.Tensor( [[1 0 0 0] [0 0 2 0] [0 0 0 0]], shape=(3, 4), dtype=int32)\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-10-28 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_1', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to Tensors\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport tensorflow as tf import numpy as np\n\n2023-10-28 01:21:58.219231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2023-10-28 01:21:58.219277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2023-10-28 01:21:58.220822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nTensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes at tf.dtypes.\n\nIf you\'re familiar with NumPy, tensors are (kind of) like np.arrays.\n\nAll tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.\n\nFirst, create some basic tensors.\n\nHere is a ""scalar"" or ""rank-0"" tensor . A scalar contains a single value, and no ""axes"".\n\n# This will be an int32 tensor by default; see ""dtypes"" below. rank_0_tensor = tf.constant(4) print(rank_0_tensor)\n\ntf.Tensor(4, shape=(), dtype=int32)\n\nA ""vector"" or ""rank-1"" tensor is like a list of values. A vector has one axis:\n\n# Let\'s make this a float tensor. rank_1_tensor = tf.constant([2.0, 3.0, 4.0]) print(rank_1_tensor)\n\ntf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n\nA ""matrix"" or ""rank-2"" tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time rank_2_tensor = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float16) print(rank_2_tensor)\n\ntf.Tensor( [[1. 2.] [3. 4.] [5. 6.]], shape=(3, 2), dtype=float16)\n\nA vector, shape: [3]\n\nA matrix, shape: [3, 2]\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of # axes (sometimes called ""dimensions"") rank_3_tensor = tf.constant([ [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]], [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]], [[20, 21, 22, 23, 24], [25, 26, 27, 28, 29]],]) print(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nThere are many ways you might visualize a tensor with more than two axes.\n\nA 3-axis tensor, shape: [3, 2, 5]\n\nYou can convert a tensor to a NumPy array either using np.array or the tensor.numpy method:\n\nnp.array(rank_2_tensor)\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nrank_2_tensor.numpy()\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nTensors often contain floats and ints, but have many other types, including:\n\nThe base tf.Tensor class requires tensors to be ""rectangular""---that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\n\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na = tf.constant([[1, 2], [3, 4]]) b = tf.constant([[1, 1], [1, 1]]) # Could have also said `tf.ones([2,2], dtype=tf.int32)` print(tf.add(a, b), ""\\n"") print(tf.multiply(a, b), ""\\n"") print(tf.matmul(a, b), ""\\n"")\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nprint(a + b, ""\\n"") # element-wise addition print(a * b, ""\\n"") # element-wise multiplication print(a @ b, ""\\n"") # matrix multiplication\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nTensors are used in all kinds of operations (or ""Ops"").\n\nc = tf.constant([[4.0, 5.0], [10.0, 1.0]]) # Find the largest value print(tf.reduce_max(c)) # Find the index of the largest value print(tf.math.argmax(c)) # Compute the softmax print(tf.nn.softmax(c))\n\ntf.Tensor(10.0, shape=(), dtype=float32) tf.Tensor([1 0], shape=(2,), dtype=int64) tf.Tensor( [[2.6894143e-01 7.3105854e-01] [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)\n\nNote: Typically, anywhere a TensorFlow function expects a Tensor as input, the function will also accept anything that can be converted to a Tensor using tf.convert_to_tensor. See below for an example.\n\ntf.convert_to_tensor([1,2,3])\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n\ntf.reduce_max([1,2,3])\n\n<tf.Tensor: shape=(), dtype=int32, numpy=3>\n\ntf.reduce_max(np.array([1,2,3]))\n\n<tf.Tensor: shape=(), dtype=int64, numpy=3>\n\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\n\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\n\nAxis or Dimension: A particular dimension of a tensor.\n\nSize: The total number of items in the tensor, the product of the shape vector\'s elements.\n\nNote: Although you may see reference to a ""tensor of two dimensions"", a rank-2 tensor does not usually describe a 2D space.\n\nTensors and tf.TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor = tf.zeros([3, 2, 4, 5])\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\nprint(""Type of every element:"", rank_4_tensor.dtype) print(""Number of axes:"", rank_4_tensor.ndim) print(""Shape of tensor:"", rank_4_tensor.shape) print(""Elements along axis 0 of tensor:"", rank_4_tensor.shape[0]) print(""Elements along the last axis of tensor:"", rank_4_tensor.shape[-1]) print(""Total number of elements (3*2*4*5): "", tf.size(rank_4_tensor).numpy())\n\nType of every element: <dtype: \'float32\'> Number of axes: 4 Shape of tensor: (3, 2, 4, 5) Elements along axis 0 of tensor: 3 Elements along the last axis of tensor: 5 Total number of elements (3*2*4*5): 120\n\nBut note that the Tensor.ndim and Tensor.shape attributes don\'t return Tensor objects. If you need a Tensor use the tf.rank or tf.shape function. This difference is subtle, but it can be important when building graphs (later).\n\ntf.rank(rank_4_tensor)\n\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n\ntf.shape(rank_4_tensor)\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 2, 4, 5], dtype=int32)>\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\nSingle-axis indexing\n\nTensorFlow follows standard Python indexing rules, similar to indexing a list or a string in Python, and the basic rules for NumPy indexing.\n\nnegative indices count backwards from the end\n\ncolons, :, are used for slices: start:stop:step\n\nrank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34]) print(rank_1_tensor.numpy())\n\n[ 0 1 1 2 3 5 8 13 21 34]\n\nIndexing with a scalar removes the axis:\n\nprint(""First:"", rank_1_tensor[0].numpy()) print(""Second:"", rank_1_tensor[1].numpy()) print(""Last:"", rank_1_tensor[-1].numpy())\n\nFirst: 0 Second: 1 Last: 34\n\nIndexing with a : slice keeps the axis:\n\nprint(""Everything:"", rank_1_tensor[:].numpy()) print(""Before 4:"", rank_1_tensor[:4].numpy()) print(""From 4 to the end:"", rank_1_tensor[4:].numpy()) print(""From 2, before 7:"", rank_1_tensor[2:7].numpy()) print(""Every other item:"", rank_1_tensor[::2].numpy()) print(""Reversed:"", rank_1_tensor[::-1].numpy())\n\nEverything: [ 0 1 1 2 3 5 8 13 21 34] Before 4: [0 1 1 2] From 4 to the end: [ 3 5 8 13 21 34] From 2, before 7: [1 2 3 5 8] Every other item: [ 0 1 3 8 21] Reversed: [34 21 13 8 5 3 2 1 1 0]\n\nHigher rank tensors are indexed by passing multiple indices.\n\nThe exact same rules as in the single-axis case apply to each axis independently.\n\nprint(rank_2_tensor.numpy())\n\n[[1. 2.] [3. 4.] [5. 6.]]\n\nPassing an integer for each index, the result is a scalar.\n\n# Pull out a single value from a 2-rank tensor print(rank_2_tensor[1, 1].numpy())\n\nYou can index using any combination of integers and slices:\n\n# Get row and column tensors print(""Second row:"", rank_2_tensor[1, :].numpy()) print(""Second column:"", rank_2_tensor[:, 1].numpy()) print(""Last row:"", rank_2_tensor[-1, :].numpy()) print(""First item in last column:"", rank_2_tensor[0, -1].numpy()) print(""Skip the first row:"") print(rank_2_tensor[1:, :].numpy(), ""\\n"")\n\nSecond row: [3. 4.] Second column: [2. 4. 6.] Last row: [5. 6.] First item in last column: 2.0 Skip the first row: [[3. 4.] [5. 6.]]\n\nHere is an example with a 3-axis tensor:\n\nprint(rank_3_tensor[:, :, 4])\n\ntf.Tensor( [[ 4 9] [14 19] [24 29]], shape=(3, 2), dtype=int32)\n\nSelecting the last feature across all locations in each example in the batch\n\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors.\n\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis x = tf.constant([[1], [2], [3]]) print(x.shape)\n\n# You can convert this object into a Python list, too print(x.shape.as_list())\n\nYou can reshape a tensor into a new shape. The tf.reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape. # Note that you\'re passing in a list reshaped = tf.reshape(x, [1, 3])\n\nprint(x.shape) print(reshaped.shape)\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style ""row-major"" memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nprint(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says ""Whatever fits"". print(tf.reshape(rank_3_tensor, [-1]))\n\ntf.Tensor( [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29], shape=(30,), dtype=int32)\n\nTypically the only reasonable use of tf.reshape is to combine or split adjacent axes (or add/remove 1s).\n\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\nprint(tf.reshape(rank_3_tensor, [3*2, 5]), ""\\n"") print(tf.reshape(rank_3_tensor, [3, -1]))\n\ntf.Tensor( [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]], shape=(6, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n\nReshaping will ""work"" for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\n\nSwapping axes in tf.reshape does not work; you need tf.transpose for that.\n\n# Bad examples: don\'t do this # You can\'t reorder axes with reshape. print(tf.reshape(rank_3_tensor, [2, 3, 5]), ""\\n"") # This is a mess print(tf.reshape(rank_3_tensor, [5, 6]), ""\\n"") # This doesn\'t work at all try: tf.reshape(rank_3_tensor, [7, -1]) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]] [[15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) InvalidArgumentError: { {function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0} } Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n\nYou may run across not-fully-specified shapes. Either the shape contains a None (an axis-length is unknown) or the whole shape is None (the rank of the tensor is unknown).\n\nExcept for tf.RaggedTensor, such shapes will only occur in the context of TensorFlow\'s symbolic, graph-building APIs:\n\nThe keras functional API.\n\nTo inspect a tf.Tensor\'s data type use the Tensor.dtype property.\n\nWhen creating a tf.Tensor from a Python object you may optionally specify the datatype.\n\nIf you don\'t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts Python integers to tf.int32 and Python floating point numbers to tf.float32. Otherwise TensorFlow uses the same rules NumPy uses when converting to arrays.\n\nYou can cast from type to type.\n\nthe_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64) the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16) # Now, cast to an uint8 and lose the decimal precision the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8) print(the_u8_tensor)\n\ntf.Tensor([2 3 4], shape=(3,), dtype=uint8)\n\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are ""stretched"" automatically to fit larger tensors when running combined operations on them.\n\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx = tf.constant([1, 2, 3]) y = tf.constant(2) z = tf.constant([2, 2, 2]) # All of these are the same computation print(tf.multiply(x, 2)) print(x * y) print(x * z)\n\ntf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\n\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations x = tf.reshape(x,[3,1]) y = tf.range(1, 5) print(x, ""\\n"") print(y, ""\\n"") print(tf.multiply(x, y))\n\ntf.Tensor( [[1] [2] [3]], shape=(3, 1), dtype=int32) tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) tf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\nHere is the same operation without broadcasting:\n\nx_stretch = tf.constant([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]) y_stretch = tf.constant([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]) print(x_stretch * y_stretch) # Again, operator overloading\n\ntf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\n\nYou see what broadcasting looks like using tf.broadcast_to.\n\nprint(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))\n\ntf.Tensor( [[1 2 3] [1 2 3] [1 2 3]], shape=(3, 3), dtype=int32)\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\n\nIt can get even more complicated. This section of Jake VanderPlas\'s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy).\n\ntf.convert_to_tensor\n\nMost ops, like tf.matmul and tf.reshape take arguments of class tf.Tensor. However, you\'ll notice in the above case, Python objects shaped like tensors are accepted.\n\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy\'s ndarray, TensorShape, Python lists, and tf.Variable will all convert automatically.\n\nSee tf.register_tensor_conversion_function for more details, and if you have your own type you\'d like to automatically convert to a tensor.\n\nA tensor with variable numbers of elements along some axis is called ""ragged"". Use tf.ragged.RaggedTensor for ragged data.\n\nFor example, This cannot be represented as a regular tensor:\n\nA tf.RaggedTensor, shape: [4, None]\n\nragged_list = [ [0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]\n\ntry: tensor = tf.constant(ragged_list) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\nValueError: Can\'t convert non-rectangular Python sequence to Tensor.\n\nInstead create a tf.RaggedTensor using tf.ragged.constant:\n\nragged_tensor = tf.ragged.constant(ragged_list) print(ragged_tensor)\n\n<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>\n\nThe shape of a tf.RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor.shape)\n\ntf.string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\n\nThe strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the axes of the tensor. See tf.strings for functions to manipulate them.\n\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string. scalar_string_tensor = tf.constant(""Gray wolf"") print(scalar_string_tensor)\n\ntf.Tensor(b\'Gray wolf\', shape=(), dtype=string)\n\nAnd a vector of strings:\n\nA vector of strings, shape: [3,]\n\n# If you have three string tensors of different lengths, this is OK. tensor_of_strings = tf.constant([""Gray wolf"", ""Quick brown fox"", ""Lazy dog""]) # Note that the shape is (3,). The string length is not included. print(tensor_of_strings)\n\ntf.Tensor([b\'Gray wolf\' b\'Quick brown fox\' b\'Lazy dog\'], shape=(3,), dtype=string)\n\nIn the above printout the b prefix indicates that tf.string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\n\nIf you pass unicode characters they are utf-8 encoded.\n\ntf.constant(""🥳👍"")\n\n<tf.Tensor: shape=(), dtype=string, numpy=b\'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d\'>\n\nSome basic functions with strings can be found in tf.strings, including tf.strings.split.\n\n# You can use split to split a string into a set of tensors print(tf.strings.split(scalar_string_tensor, sep="" ""))\n\ntf.Tensor([b\'Gray\' b\'wolf\'], shape=(2,), dtype=string)\n\n# ...but it turns into a `RaggedTensor` if you split up a tensor of strings, # as each string might be split into a different number of parts. print(tf.strings.split(tensor_of_strings))\n\n<tf.RaggedTensor [[b\'Gray\', b\'wolf\'], [b\'Quick\', b\'brown\', b\'fox\'], [b\'Lazy\', b\'dog\']]>\n\nThree strings split, shape: [3, None]\n\nAnd tf.strings.to_number:\n\ntext = tf.constant(""1 10 100"") print(tf.strings.to_number(tf.strings.split(text, "" "")))\n\ntf.Tensor([ 1. 10. 100.], shape=(3,), dtype=float32)\n\nAlthough you can\'t use tf.cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings = tf.strings.bytes_split(tf.constant(""Duck"")) byte_ints = tf.io.decode_raw(tf.constant(""Duck""), tf.uint8) print(""Byte strings:"", byte_strings) print(""Bytes:"", byte_ints)\n\nByte strings: tf.Tensor([b\'D\' b\'u\' b\'c\' b\'k\'], shape=(4,), dtype=string) Bytes: tf.Tensor([ 68 117 99 107], shape=(4,), dtype=uint8)\n\n# Or split it up as unicode and then decode it unicode_bytes = tf.constant(""アヒル 🦆"") unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, ""UTF-8"") unicode_values = tf.strings.unicode_decode(unicode_bytes, ""UTF-8"") print(""\\nUnicode bytes:"", unicode_bytes) print(""\\nUnicode chars:"", unicode_char_bytes) print(""\\nUnicode values:"", unicode_values)\n\nUnicode bytes: tf.Tensor(b\'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86\', shape=(), dtype=string) Unicode chars: tf.Tensor([b\'\\xe3\\x82\\xa2\' b\'\\xe3\\x83\\x92\' b\'\\xe3\\x83\\xab\' b\' \' b\'\\xf0\\x9f\\xa6\\x86\'], shape=(5,), dtype=string) Unicode values: tf.Tensor([ 12450 12498 12523 32 129414], shape=(5,), dtype=int32)\n\nThe tf.string dtype is used for all raw bytes data in TensorFlow. The tf.io module contains functions for converting data to and from bytes, including decoding images and parsing csv.\n\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf.sparse.SparseTensor and related operations to store sparse data efficiently.\n\nA tf.SparseTensor, shape: [3, 4]\n\n# Sparse tensors store values by index in a memory-efficient manner sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]) print(sparse_tensor, ""\\n"") # You can convert sparse tensors to dense print(tf.sparse.to_dense(sparse_tensor))\n\nSparseTensor(indices=tf.Tensor( [[0 0] [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) tf.Tensor( [[1 0 0 0] [0 0 2 0] [0 0 0 0]], shape=(3, 4), dtype=int32)\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-10-28 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-06-24T03:57:03', 'title': 'Introduction to Tensors | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/tensor'}), Document(page_content='中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', metadata={'id': 'web-search_0', 'snippet': '中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', 'timestamp': '2024-04-29T10:18:20', 'title': 'tf.tensordot | TensorFlow v2.15.0.post1', 'url': 'https://www.tensorflow.org/api_docs/python/tf/tensordot'})]]??"
71019644,tf.where,"{'https://www.edx.org/learn/tensorflow', 'https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/', 'https://www.edx.org/learn/scikit-learn', 'https://www.edx.org/learn/numpy', 'https://www.coursera.org/learn/fundamental-tools-of-data-wrangling', 'https://www.udemy.com/course/deep-learning-tensorflow-2/', 'https://www.udemy.com/course/data-science-deep-learning-in-theano-tensorflow/'}","{'https://www.youtube.com/watch?v=0qu976IG0hQ', 'https://www.youtube.com/watch?v=PvVRSF_TY-Y', 'https://www.youtube.com/watch?v=tfM5QUNYtEA'}","{'https://stackoverflow.com/questions/38193958/how-to-properly-mask-a-numpy-2d-array', 'https://stackoverflow.com/questions/72345237/is-there-an-alterntive-to-tf-keras-layers-masking-in-pytorch-or-is-there-any-wa', 'https://stackoverflow.com/questions/49977236/tensorflow-broadcasting', 'https://stackoverflow.com/questions/56731361/advanced-broadcasting-in-tensorflow-or-numpy', 'https://stackoverflow.com/questions/66978119/is-broadcasting-in-tensorflow-a-view-or-a-copy', 'https://stackoverflow.com/questions/75410827/how-does-masking-work-in-tensorflow-keras'}","??[[Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\n2024 Developer survey is here and we would like to hear from you! Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nTensorFlow broadcasting\n\nAsked 6 years, 1 month ago\n\nModified 5 years, 8 months ago\n\nBroadcasting is the process of making arrays with different shapes have compatible shapes for arithmetic operations. In numpy, we can broadcast arrays. Does TensorFlow graph support broadcasting similar to the numpy one?\n\nImprove this question\n\nedited Sep 28, 2018 at 10:58\n\n59.2k77 gold badges8181 silver badges125125 bronze badges\n\nasked Apr 23, 2018 at 9:09\n\nnairouz mrabahnairouz mrabah\n\n1,21722 gold badges1414 silver badges2626 bronze badges 1\n\nYes, if you just google ""tensorflow broadcasting"" you will find plenty of examples like this.\n\n– javidcf Commented Apr 23, 2018 at 9:13\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nyes it is supported. Open a terminal and try this:\n\nimport tensorflow as tf #define tensors a=tf.constant([[10,20],[30,40]]) #Dimension 2X2 b=tf.constant([5]) c=tf.constant([2,2]) d=tf.constant([[3],[3]]) sess=tf.Session() #start a session #Run tensors to generate arrays mat,scalar,one_d,two_d = sess.run([a,b,c,d]) #broadcast multiplication with scalar sess.run(tf.multiply(mat,scalar)) #broadcast multiplication with 1_D array (Dimension 1X2) sess.run(tf.multiply(mat,one_d)) #broadcast multiply 2_d array (Dimension 2X1) sess.run(tf.multiply(mat,two_d)) sess.close()\n\nanswered Apr 23, 2018 at 11:44\n\nPratik KumarPratik Kumar\n\n2,23111 gold badge1818 silver badges4141 bronze badges\n\nThe short answer is yes.\n\nc.f. Tensorflow Math doc\n\nNote: Elementwise binary operations in TensorFlow follow numpy-style broadcasting.\n\nc.f. tf.add() doc, or tf.multiply() doc, etc.:\n\nNOTE: [the operation] supports broadcasting. More about broadcasting here\n\nedited Apr 23, 2018 at 10:49\n\nanswered Apr 23, 2018 at 9:27\n\nbenjaminplanchebenjaminplanche\n\n15k55 gold badges6060 silver badges7070 bronze badges 2\n\nit is just in XLA! I don\'t want to use another compiler\n\n– nairouz mrabah Commented Apr 23, 2018 at 9:29\n\nThe wrong link was shared, sorry. The answer has been updated. You can also check @jdehesa\'s link.\n\n– benjaminplanche Commented Apr 23, 2018 at 10:44\n\nNot the answer you\'re looking for? Browse other questions tagged\n\narray-broadcasting or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n0 Tensorflow Dimension Understanding\n\n5 Backpropagating gradients through nested tf.map_fn\n\n3 tensorflow variable assign broadcasting\n\n2 What is supported by broadcasting in tensorflow? How dimensions matches determined?\n\n3 Automatic broadcasting in Tensorflow\n\n0 Tensorflow matrices broadcast\n\n0 Tensorflow Broadcasting across one dimension\n\n0 Advanced broadcasting in TensorFlow (or Numpy)\n\n3 Broadcasting dynamic dimension in Tensorflow\n\n2 Trouble Understanding broadcasting behavior for tensors\n\n1 PyTorch Tensor broadcasting\n\n0 broadcasting and reducing dimension in tensorflow\n\nHot Network Questions\n\nHow do I efficiently update myself on what\'s expected of me as a postdoc?\n\nBringing a game console into Saudi Arabia\n\nWhy aren\'t the plains people conquering the city people?\n\nSchengen visa issued by Germany - Is a layover in Vienna okay before heading to Berlin?\n\nIn Mandarin, can you use 我爱你 platonically?\n\nIs there any position where giving checkmate by En Passant is a brilliant move on Chess.com?\n\nCan we use 3rd person singular for ""Come Find""?\n\nOrigin of ""That tracks"" to mean ""That makes sense.""\n\nIs it possible to avoid ending Time Stop by making attacks from inside an Antimagic Field?\n\nWhat is the appropriate behavior when management decisions lead to tasks no longer being done?\n\nCould a kickback have caused my mitre saw fence to bend?\n\nWhat\'s this plant with saw-toothed leaves, scaly stems and white pom-pom flowers?\n\nBinding to an IP address on an interface that comes and goes\n\nWhy does setting a variable readonly in the outer scope prevents defining a local variable with the same name?\n\nPreparing lumber by splitting verses cutting\n\nWhat\'s funny about ""He leveraged his @#% deep into soy beans and cocoa futures"" in The Jerk (1979)?\n\nHow to write a rule that matches alternatives but return one of them always?\n\nFourth species counterpoint treating fourth\n\nComposition of vectorvalued functions\n\nIs ""Shopping malls are a posh place"" grammatical when ""malls"" is a plural and ""place"" is a singular?\n\nTips on removing solder stuck in very small hole\n\nDid Greer really corrupt Samaritan?\n\nForm all possible well-formed strings of \'n\' nested pairs of 2 types of ASCII brackets\n\nAt what point does working memory and IQ become a problem when trying to follow a lecture? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_0', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\n2024 Developer survey is here and we would like to hear from you! Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nTensorFlow broadcasting\n\nAsked 6 years, 1 month ago\n\nModified 5 years, 8 months ago\n\nBroadcasting is the process of making arrays with different shapes have compatible shapes for arithmetic operations. In numpy, we can broadcast arrays. Does TensorFlow graph support broadcasting similar to the numpy one?\n\nImprove this question\n\nedited Sep 28, 2018 at 10:58\n\n59.2k77 gold badges8181 silver badges125125 bronze badges\n\nasked Apr 23, 2018 at 9:09\n\nnairouz mrabahnairouz mrabah\n\n1,21722 gold badges1414 silver badges2626 bronze badges 1\n\nYes, if you just google ""tensorflow broadcasting"" you will find plenty of examples like this.\n\n– javidcf Commented Apr 23, 2018 at 9:13\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nyes it is supported. Open a terminal and try this:\n\nimport tensorflow as tf #define tensors a=tf.constant([[10,20],[30,40]]) #Dimension 2X2 b=tf.constant([5]) c=tf.constant([2,2]) d=tf.constant([[3],[3]]) sess=tf.Session() #start a session #Run tensors to generate arrays mat,scalar,one_d,two_d = sess.run([a,b,c,d]) #broadcast multiplication with scalar sess.run(tf.multiply(mat,scalar)) #broadcast multiplication with 1_D array (Dimension 1X2) sess.run(tf.multiply(mat,one_d)) #broadcast multiply 2_d array (Dimension 2X1) sess.run(tf.multiply(mat,two_d)) sess.close()\n\nanswered Apr 23, 2018 at 11:44\n\nPratik KumarPratik Kumar\n\n2,23111 gold badge1818 silver badges4141 bronze badges\n\nThe short answer is yes.\n\nc.f. Tensorflow Math doc\n\nNote: Elementwise binary operations in TensorFlow follow numpy-style broadcasting.\n\nc.f. tf.add() doc, or tf.multiply() doc, etc.:\n\nNOTE: [the operation] supports broadcasting. More about broadcasting here\n\nedited Apr 23, 2018 at 10:49\n\nanswered Apr 23, 2018 at 9:27\n\nbenjaminplanchebenjaminplanche\n\n15k55 gold badges6060 silver badges7070 bronze badges 2\n\nit is just in XLA! I don\'t want to use another compiler\n\n– nairouz mrabah Commented Apr 23, 2018 at 9:29\n\nThe wrong link was shared, sorry. The answer has been updated. You can also check @jdehesa\'s link.\n\n– benjaminplanche Commented Apr 23, 2018 at 10:44\n\nNot the answer you\'re looking for? Browse other questions tagged\n\narray-broadcasting or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n0 Tensorflow Dimension Understanding\n\n5 Backpropagating gradients through nested tf.map_fn\n\n3 tensorflow variable assign broadcasting\n\n2 What is supported by broadcasting in tensorflow? How dimensions matches determined?\n\n3 Automatic broadcasting in Tensorflow\n\n0 Tensorflow matrices broadcast\n\n0 Tensorflow Broadcasting across one dimension\n\n0 Advanced broadcasting in TensorFlow (or Numpy)\n\n3 Broadcasting dynamic dimension in Tensorflow\n\n2 Trouble Understanding broadcasting behavior for tensors\n\n1 PyTorch Tensor broadcasting\n\n0 broadcasting and reducing dimension in tensorflow\n\nHot Network Questions\n\nHow do I efficiently update myself on what\'s expected of me as a postdoc?\n\nBringing a game console into Saudi Arabia\n\nWhy aren\'t the plains people conquering the city people?\n\nSchengen visa issued by Germany - Is a layover in Vienna okay before heading to Berlin?\n\nIn Mandarin, can you use 我爱你 platonically?\n\nIs there any position where giving checkmate by En Passant is a brilliant move on Chess.com?\n\nCan we use 3rd person singular for ""Come Find""?\n\nOrigin of ""That tracks"" to mean ""That makes sense.""\n\nIs it possible to avoid ending Time Stop by making attacks from inside an Antimagic Field?\n\nWhat is the appropriate behavior when management decisions lead to tasks no longer being done?\n\nCould a kickback have caused my mitre saw fence to bend?\n\nWhat\'s this plant with saw-toothed leaves, scaly stems and white pom-pom flowers?\n\nBinding to an IP address on an interface that comes and goes\n\nWhy does setting a variable readonly in the outer scope prevents defining a local variable with the same name?\n\nPreparing lumber by splitting verses cutting\n\nWhat\'s funny about ""He leveraged his @#% deep into soy beans and cocoa futures"" in The Jerk (1979)?\n\nHow to write a rule that matches alternatives but return one of them always?\n\nFourth species counterpoint treating fourth\n\nComposition of vectorvalued functions\n\nIs ""Shopping malls are a posh place"" grammatical when ""malls"" is a plural and ""place"" is a singular?\n\nTips on removing solder stuck in very small hole\n\nDid Greer really corrupt Samaritan?\n\nForm all possible well-formed strings of \'n\' nested pairs of 2 types of ASCII brackets\n\nAt what point does working memory and IQ become a problem when trying to follow a lecture? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-06-16T14:37:05', 'title': 'python - TensorFlow broadcasting - Stack Overflow', 'url': 'https://stackoverflow.com/questions/49977236/tensorflow-broadcasting'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nJobs Programming & related technical career opportunities\n\nTalent Recruit tech talent & build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nSign up or log in to customize your list.\n\nmore stack exchange communities company blog\n\nCollectives on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more\n\nAdvanced broadcasting in TensorFlow (or Numpy)\n\nAsked 2 years, 7 months ago\n\nActive 2 years, 7 months ago\n\nIn TensorFlow I have a rank-2 tensor M (a matrix) of shape [D, D] and a rank-3 tensor T of shape [D, D, D].\n\nI need to combine them to form a new matrix R as follows: the element R[a, b+c-a] is given by the sum of all the elements T[a, b, c]*M[b, c] where b+c-a is constant (where b+c-a has to be between 0 and D-1).\n\nAn inefficient way to create R is with nested for loops over the indices and a check that b+c-a does not exceed D-1 (e.g. in numpy):\n\nR = np.zeros([D,D]) for a in range(D): for b in range(D): for c in range(D): if 0 <= b+c-a < D: R[a, b+c-a] += T[a, b, c]*M[b, c]\n\nbut I would like to use broadcasting and/or other more efficient methods.\n\nHow can I achieve this?\n\npython numpy tensorflow array-broadcasting\n\nImprove this question\n\nedited Jun 24 \'19 at 11:03\n\nasked Jun 24 \'19 at 7:04\n\n1,50111 gold badge1414 silver badges2626 bronze badges 3\n\nShow the for loop code, it\'s not possible to understand what you want from this explanation.\n\n– Daniel F Jun 24 \'19 at 7:52\n\nBut b + c - a may take the same value for more than one combination of a, b and c, even if a is fixed (e.g. for (a=2, b=1, c=3) and (a=2, b=3, c=1) the result is 2). Which of the values should R[a, b + c - a] take? Or should it be a combination (addition, product)?\n\n– jdehesa Jun 24 \'19 at 10:24\n\n@jedhesa I corrected that part, thanks for pointing it out\n\n– Ziofil Jun 24 \'19 at 11:04\n\nYou can vectorize that calculation as follows:\n\nimport numpy as np np.random.seed(0) D = 10 M = np.random.rand(D, D) T = np.random.rand(D, D, D) # Original calculation R = np.zeros([D, D]) for a in range(D): for b in range(D): for c in range(D): if 0 <= b + c - a < D: R[a, b + c - a] += T[a, b, c] * M[b, c] # Vectorized calculation tm = T * M a = np.arange(D)[:, np.newaxis, np.newaxis] b, c = np.ogrid[:D, :D] col_idx = b + c - a m = (col_idx >= 0) & (col_idx < D) row_idx = np.tile(a, [1, D, D]) R2 = np.zeros([D, D]) np.add.at(R2, (row_idx[m], col_idx[m]), tm[m]) # Check result print(np.allclose(R, R2)) # True\n\nAlternatively, you could consider using Numba to accelerate the loops:\n\nimport numpy as np import numba as nb @nb.njit def calculation_nb(T, M, D): tm = T * M R = np.zeros((D, D), dtype=tm.dtype) for a in nb.prange(D): for b in range(D): for c in range(max(a - b, 0), min(D + a - b, D)): R[a, b + c - a] += tm[a, b, c] return R print(np.allclose(R, calculation_nb(T, M, D))) # True\n\nIn a couple of quick tests, even without parallelization, this is quite faster than NumPy.\n\nedited Jun 24 \'19 at 15:45\n\nanswered Jun 24 \'19 at 12:37\n\n54.6k66 gold badges7171 silver badges104104 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged python numpy tensorflow array-broadcasting or ask your own question.\n\nHere’s how Stack Overflow users responded to Log4Shell, the Log4j...\n\nWho’s going to pay to fix open source security?\n\nWe’ve made changes to our Terms of Service & Privacy Policy - January 2022\n\nNew post summary designs on greatest hits now, everywhere else eventually\n\nSunsetting Jobs & Developer Story\n\n146 What does tf.nn.conv2d do in tensorflow?\n\n657 TensorFlow not found using pip\n\n1 How to get rows of matrix without using any loops in numpy or tensorflow?\n\n0 Tensorflow access elements in tensor using tenors on indices\n\n77 What is the difference between Dataset.from_tensors and Dataset.from_tensor_slices?\n\n1 Stuck on tensorflow advanced indexing\n\n0 Efficient numpy broadcasting not found\n\n0 Is broadcasting in Tensorflow a view or a copy?\n\nHot Network Questions\n\nIs this word in standard order?\n\nJWST - Why unfurl before reaching L2?\n\nCan a planet with no atmosphere be orbited at extremely low altitudes?\n\nThe ""Amazing Hidden Power"" of Random Search?\n\nDid president Garfield make any contributions to Mathematics?\n\nReplicate ""dotnet ef"" unicorn\n\nField Calculator function caused all my rows to become Null\n\nSpice blends have no flavor?\n\nCan a robot be fooled by a human posing as a robot?\n\nDifference between lotteries and events that involve randomness?\n\nWhat are the height (and weight) size ranges for each size category of creature?\n\nHow to stop soldiers from selling their equipment?\n\nWhat does 食わず嫌いな奴は貪欲さで試合に負けるわ! Mean\n\nIs it legal to describe something exactly as ""cheese"" without asterisks or other qualification if it doesn\'t contain any dairy?\n\nHow come SFP modules are so incompatible?\n\nLegal repercussions for an act of discrimination\n\nWhat kind of wood are these fence panels made out of and can they be restored?\n\nWhy didn\'t Obi-Wan offer an apology or pay for the mess at Chalmun\'s Cantina?\n\nWhat does ""Somebody still hearts waterboarding"" mean?\n\nHow to reduce size of an image texture\n\nWhat is the word for the airflow effect from opening two windows on opposite sides of a room?\n\nHow to disable LED on Acer ED270UP Monitor\n\nWhy are numeric citations used?\n\nWhat Shader Node Is It? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Customize settings', metadata={'id': 'web-search_1', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nJobs Programming & related technical career opportunities\n\nTalent Recruit tech talent & build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nSign up or log in to customize your list.\n\nmore stack exchange communities company blog\n\nCollectives on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more\n\nAdvanced broadcasting in TensorFlow (or Numpy)\n\nAsked 2 years, 7 months ago\n\nActive 2 years, 7 months ago\n\nIn TensorFlow I have a rank-2 tensor M (a matrix) of shape [D, D] and a rank-3 tensor T of shape [D, D, D].\n\nI need to combine them to form a new matrix R as follows: the element R[a, b+c-a] is given by the sum of all the elements T[a, b, c]*M[b, c] where b+c-a is constant (where b+c-a has to be between 0 and D-1).\n\nAn inefficient way to create R is with nested for loops over the indices and a check that b+c-a does not exceed D-1 (e.g. in numpy):\n\nR = np.zeros([D,D]) for a in range(D): for b in range(D): for c in range(D): if 0 <= b+c-a < D: R[a, b+c-a] += T[a, b, c]*M[b, c]\n\nbut I would like to use broadcasting and/or other more efficient methods.\n\nHow can I achieve this?\n\npython numpy tensorflow array-broadcasting\n\nImprove this question\n\nedited Jun 24 \'19 at 11:03\n\nasked Jun 24 \'19 at 7:04\n\n1,50111 gold badge1414 silver badges2626 bronze badges 3\n\nShow the for loop code, it\'s not possible to understand what you want from this explanation.\n\n– Daniel F Jun 24 \'19 at 7:52\n\nBut b + c - a may take the same value for more than one combination of a, b and c, even if a is fixed (e.g. for (a=2, b=1, c=3) and (a=2, b=3, c=1) the result is 2). Which of the values should R[a, b + c - a] take? Or should it be a combination (addition, product)?\n\n– jdehesa Jun 24 \'19 at 10:24\n\n@jedhesa I corrected that part, thanks for pointing it out\n\n– Ziofil Jun 24 \'19 at 11:04\n\nYou can vectorize that calculation as follows:\n\nimport numpy as np np.random.seed(0) D = 10 M = np.random.rand(D, D) T = np.random.rand(D, D, D) # Original calculation R = np.zeros([D, D]) for a in range(D): for b in range(D): for c in range(D): if 0 <= b + c - a < D: R[a, b + c - a] += T[a, b, c] * M[b, c] # Vectorized calculation tm = T * M a = np.arange(D)[:, np.newaxis, np.newaxis] b, c = np.ogrid[:D, :D] col_idx = b + c - a m = (col_idx >= 0) & (col_idx < D) row_idx = np.tile(a, [1, D, D]) R2 = np.zeros([D, D]) np.add.at(R2, (row_idx[m], col_idx[m]), tm[m]) # Check result print(np.allclose(R, R2)) # True\n\nAlternatively, you could consider using Numba to accelerate the loops:\n\nimport numpy as np import numba as nb @nb.njit def calculation_nb(T, M, D): tm = T * M R = np.zeros((D, D), dtype=tm.dtype) for a in nb.prange(D): for b in range(D): for c in range(max(a - b, 0), min(D + a - b, D)): R[a, b + c - a] += tm[a, b, c] return R print(np.allclose(R, calculation_nb(T, M, D))) # True\n\nIn a couple of quick tests, even without parallelization, this is quite faster than NumPy.\n\nedited Jun 24 \'19 at 15:45\n\nanswered Jun 24 \'19 at 12:37\n\n54.6k66 gold badges7171 silver badges104104 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged python numpy tensorflow array-broadcasting or ask your own question.\n\nHere’s how Stack Overflow users responded to Log4Shell, the Log4j...\n\nWho’s going to pay to fix open source security?\n\nWe’ve made changes to our Terms of Service & Privacy Policy - January 2022\n\nNew post summary designs on greatest hits now, everywhere else eventually\n\nSunsetting Jobs & Developer Story\n\n146 What does tf.nn.conv2d do in tensorflow?\n\n657 TensorFlow not found using pip\n\n1 How to get rows of matrix without using any loops in numpy or tensorflow?\n\n0 Tensorflow access elements in tensor using tenors on indices\n\n77 What is the difference between Dataset.from_tensors and Dataset.from_tensor_slices?\n\n1 Stuck on tensorflow advanced indexing\n\n0 Efficient numpy broadcasting not found\n\n0 Is broadcasting in Tensorflow a view or a copy?\n\nHot Network Questions\n\nIs this word in standard order?\n\nJWST - Why unfurl before reaching L2?\n\nCan a planet with no atmosphere be orbited at extremely low altitudes?\n\nThe ""Amazing Hidden Power"" of Random Search?\n\nDid president Garfield make any contributions to Mathematics?\n\nReplicate ""dotnet ef"" unicorn\n\nField Calculator function caused all my rows to become Null\n\nSpice blends have no flavor?\n\nCan a robot be fooled by a human posing as a robot?\n\nDifference between lotteries and events that involve randomness?\n\nWhat are the height (and weight) size ranges for each size category of creature?\n\nHow to stop soldiers from selling their equipment?\n\nWhat does 食わず嫌いな奴は貪欲さで試合に負けるわ! Mean\n\nIs it legal to describe something exactly as ""cheese"" without asterisks or other qualification if it doesn\'t contain any dairy?\n\nHow come SFP modules are so incompatible?\n\nLegal repercussions for an act of discrimination\n\nWhat kind of wood are these fence panels made out of and can they be restored?\n\nWhy didn\'t Obi-Wan offer an apology or pay for the mess at Chalmun\'s Cantina?\n\nWhat does ""Somebody still hearts waterboarding"" mean?\n\nHow to reduce size of an image texture\n\nWhat is the word for the airflow effect from opening two windows on opposite sides of a room?\n\nHow to disable LED on Acer ED270UP Monitor\n\nWhy are numeric citations used?\n\nWhat Shader Node Is It? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Customize settings', 'timestamp': '2022-01-22T08:16:22', 'title': 'python - Advanced broadcasting in TensorFlow (or Numpy) - Stack Overflow', 'url': 'https://stackoverflow.com/questions/56731361/advanced-broadcasting-in-tensorflow-or-numpy'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow does masking work in Tensorflow Keras\n\nAsked 1 year, 3 months ago\n\nModified 8 months ago\n\nI have difficulty understanding how exactly masking works in Tensorflow/Keras. On the Keras website (https://www.tensorflow.org/guide/keras/masking_and_padding) they simply say that the neural network layers skip/ignore the masked values but it doesn\'t explain how? Does it force the weights to zero? (I know a boolean array is being created but I don\'t know how it\'s being used)\n\nFor example check this simple example:\n\ntf.random.set_seed(1) embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=True) masked_output = embedding(np.array([[1,2,0]])) print(masked_output)\n\nI asked the Embedding layer to mask zero inputs. Now look at the output:\n\ntf.Tensor( [[[ 0.00300496 -0.02925059 -0.01254098] [ 0.04872786 0.01087702 -0.03656749] [ 0.00446818 0.00290152 -0.02269397]]], shape=(1, 3, 3), dtype=float32)\n\nIf you change the ""mask_zero"" argument to False you get the exact same results. Does anyone know what\'s happening behind the scene? Any resources explaining the masking mechanism more thoroughly is highly appreciated.\n\nP.S: This is also an example of a full Neural Network which gives an identical outcome with and without masking:\n\ntf.random.set_seed(1) input = np.array([[1,2,0]]) # <--- 0 should be masked and ignored embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=True) masked_output = embedding(input) flatten = tf.keras.layers.Flatten()(masked_output) dense_middle = tf.keras.layers.Dense(4)(flatten) out = tf.keras.layers.Dense(1)(dense_middle) print(out)\n\nImprove this question\n\nedited Feb 10, 2023 at 12:24\n\nasked Feb 10, 2023 at 11:58\n\n58244 silver badges1313 bronze badges 2\n\nDoes this answer your question? How does mask_zero in Keras Embedding layer work?\n\n– Franciska Feb 10, 2023 at 12:59\n\n@Franciska Not really, the answer mostly repeats Tensorflow\'s manual which is remotely clear. For example, what does ""ignore"" mean? In math there is no such term and we are doing math in NNs. Does ""ignore"" mean setting weights to zero? Also I gave an example here which shows that the mask doesn\'t affect the following layers at all (as opposed to the answer given in that link).\n\n– Amin Shn Feb 10, 2023 at 13:16\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIn TensorFlow/Keras, masking enables you to disregard certain parts of a tensor, typically those set to zero, when executing the forward pass of your neural network. This can be helpful when dealing with sequences of varying length, where padding is used to make all sequences the same length. In the forward pass, the covered-up elements are taken as having a value of 0, so that their effect on the output is ignored.\n\nIn the example you provided, the Embedding layer is set to mask zeros via the mask_zero argument, yet the outcome is the same regardless of whether mask_zero is set to True or False. This is because the example just has one input tensor with no zero values, thus there is no contrast in the output.\n\nUnderneath, TensorFlow implements masking by using a special tensor mask that is multiplied element-wise with the input tensor during the forward pass. This mask tensor has the same shape as the input tensor and comprises binary values that indicate if each element should be included or not.\n\ninputs = tf.keras.layers.Input(shape=(3,)) embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=True)(inputs) masking = tf.keras.layers.Masking()(embedding) flatten = tf.keras.layers.Flatten()(masking) dense_middle = tf.keras.layers.Dense(4)(flatten) output = tf.keras.layers.Dense(1)(dense_middle) model = tf.keras.Model(inputs, output)\n\nBy doing this, the network will be able to take advantage of the zeros when the ""mask_zero"" argument is set to False and will disregard them when it is True, resulting in different predictions.\n\nedited Feb 10, 2023 at 12:48\n\nanswered Feb 10, 2023 at 12:07\n\nsilentlyakittensilentlyakitten\n\n1611 bronze badge 10\n\nThe last element of the input is actually zero. Also I added another part you might want to check.\n\n– Amin Shn Feb 10, 2023 at 12:16\n\nIt is true that the final element of the input in this example is zero, which gets disregarded when ""mask_zero"" is enabled. Consequently, the final element of the embedded input won\'t be taken into account when training or making predictions, and its gradients will not be calculated during the training procedure. Also, I checked out the other part that you included.\n\n– silentlyakitten Feb 10, 2023 at 12:27\n\nWhen ""mask_zero"" is set to false, the Embedding layer does not apply a mask to the input and accounts for every element in training and forecasting. The output is then pushed through the Flatten layer, which reduces the shape to (1, 9). This flattened output is then handled by two Dense layers, creating the final output of (1, 1). Regardless of whether the ""mask_zero"" is false, the zero element is still accounted for in the calculation of the output, so the result is the same.\n\n– silentlyakitten Feb 10, 2023 at 12:28\n\nSo are you saying the mask is not broadcasted through the network? How can I make it broadcasted so that the network uses zero if the mask_zero is false and not using it when it is True and result in different predictions?\n\n– Amin Shn Feb 10, 2023 at 12:30\n\nThe mask created by the Embedding layer with the ""mask_zero"" argument set to True is not automatically broadcasted to subsequent layers. To pass the mask information, you need to wrap the output of the Embedding layer with a Keras Masking layer\n\n– silentlyakitten Feb 10, 2023 at 12:37\n\n | Show 5 more comments\n\nAs I see it, the question does not look at the right thing to see the differences. Masking does not work by means of modifying the values of the output of a layer. Masking adds a _keras_mask attribute, that specifies values in the tensor that should not be taken into account when calculating loss or metric functions. I share a modified version of your code that shows that while the values of the masked_output Tensor are the same for both cases, when you calculate the MAE error with the same target you get different results. (Note that I don\'t use seed to force reproducible results for the embedding layer, I use ones as initialiser which forces all embedding to be one, so that the different entries will be counted)\n\nimport tensorflow as tf embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=True, embeddings_initializer=""ones"") masked_output = embedding(np.array([[1,2,0]])) print(""+++++++++++++++++++++"") print(""+++ masking = on +++"") print(""ATTR: "", hasattr(masked_output, ""_keras_mask"")) print(""ATTR-VALS:"", masked_output._keras_mask) print(""MAE loss :"",tf.keras.losses.MeanAbsoluteError(reduction=""sum"")(tf.zeros_like(masked_output), masked_output)) embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=False, embeddings_initializer=""ones"") masked_output = embedding(np.array([[1,2,0]])) print(""+++++++++++++++++++++"") print(""+++ masking = off +++"") print(""ATTR: "", hasattr(masked_output, ""_keras_mask"")) print(""MAE loss :"",tf.keras.losses.MeanAbsoluteError(reduction=""sum"")(tf.zeros_like(masked_output), masked_output))\n\n+++++++++++++++++++++ +++ masking = on +++ ATTR: True ATTR-VALS: tf.Tensor([[ True True False]], shape=(1, 3), dtype=bool) MAE loss : tf.Tensor(2.0, shape=(), dtype=float32) +++++++++++++++++++++ +++ masking = off +++ ATTR: False MAE loss : tf.Tensor(3.0, shape=(), dtype=float32)\n\nYou see in the code the existence of the _keras_mask attribute and its values. It is present when masking is on, but not present when masking is off. _keras_mask is a boolean mask that is used within Loss and Metrics classes to restrict the loss to take into account only valid values.\n\nVery interestingly, you can do this manually as well. Adding the following code at the ned of the code above\n\nprint(""+++ masking = manual +++"") masked_output._keras_mask=tf.constant([[False, False, True]], tf.bool) print(""ATTR: "", hasattr(masked_output, ""_keras_mask"")) print(""ATTR-VALS:"", masked_output._keras_mask) print(""MAE loss :"",tf.keras.losses.MeanAbsoluteError(reduction=""sum"")(tf.zeros_like(masked_output), masked_output))\n\n+++ masking = manual +++ ATTR: True ATTR-VALS: tf.Tensor([[False False True]], shape=(1, 3), dtype=bool) MAE loss : tf.Tensor(1.0, shape=(), dtype=float32)\n\nThis means you can manually mask arbitrary parts of the tensors when calculating the loss.\n\nYou can see how it is performed in the call method of the Masking layer\n\nhttps://github.com/keras-team/keras/blob/v2.14.0/keras/layers/core/masking.py#L26-L91\n\nFor other layers supporting masking, they provide the compute_mask method which will be called in the Layer base class.\n\nedited Sep 28, 2023 at 9:05\n\nanswered Sep 28, 2023 at 8:21\n\n29933 silver badges88 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nAn open-source development paradigm\n\nDevelopers get by with a little help from AI: Stack Overflow Knows code...\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n37 How does mask_zero in Keras Embedding layer work?\n\n0 How to display the output that Microsoft Graph API returns\n\n3 how to get raw message with microsoft graph?\n\n1 Get Information from Microsoft Academic Graph API\n\n1 Accessing all data in API response\n\n1 Query to pulled data using Microsoft graph\n\n1 How to get response header in c# Microsoft graph api request\n\n1 Microsoft Graph API from microsoft streams app\n\n1 Cannot understand or convert Microsoft Graph API HTTP request to python request\n\n0 Python - how to get JSON from a GraphQL api?\n\n0 Microsoft Graph API Read Mail with Python\n\nHot Network Questions\n\nLate IRA contribution\n\nIs there a word that means both ""house"" and ""apartment""?\n\nHow long should it take you to finish a page on average?\n\nDisplaying the Footnoterule and Footnotes Farther Up the Page (One Time)\n\nBack Transforming log-log Model for Prediction\n\nWhat is specifically Dutch about light bulbs?\n\nIs it bad that I gave the wrong answer to a question during a recorded talk?\n\nWhat is causing my oil pan to fill with a mysterious fluid?\n\nlinkedin job post ""product tester\' recieved a email check PDF to deposit so I can purchase the product which I am supposed to test..Scam?\n\nDynamic name with glue in mutate call\n\nA short story about two identical twins raised separately\n\nHow do Theistic Evolutionists interpret Genesis 2:7 in light of Ezekiel 37:1-14?\n\nShould I use the ""time"" keyword, the ""/usr/bin/time"" command or the ""times"" builtin to time the execution of commands in bash?\n\nHarmonic minor \'A\'. How is left hand (bass staff) affected?\n\nSciFi book, about a human colony on the moon. A central computer becomes sentient and forms an alliance or teams up with the main character\n\nMad Max is sentenced to the Gulags but gets cast out?\n\nIs there any etymological connection between the two dominant meanings of ""or""?\n\nCan a rental agreement state that no guests or parties are allowed?\n\nCan I get the location of bash before executing a script?\n\nSplit a number in half, sum it, square it and get the number back\n\nDo ends really exist?\n\nHoneymoon in Sri Lanka - what should my marital status be on the visa application?\n\nIs it bad style to write x^2, 2^\\frac{1}{2} and 2^\\sqrt{2}?\n\nWas the appearance of the sand worms in David Lynch\'s Dune (1984) completely original to that film? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_5', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow does masking work in Tensorflow Keras\n\nAsked 1 year, 3 months ago\n\nModified 8 months ago\n\nI have difficulty understanding how exactly masking works in Tensorflow/Keras. On the Keras website (https://www.tensorflow.org/guide/keras/masking_and_padding) they simply say that the neural network layers skip/ignore the masked values but it doesn\'t explain how? Does it force the weights to zero? (I know a boolean array is being created but I don\'t know how it\'s being used)\n\nFor example check this simple example:\n\ntf.random.set_seed(1) embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=True) masked_output = embedding(np.array([[1,2,0]])) print(masked_output)\n\nI asked the Embedding layer to mask zero inputs. Now look at the output:\n\ntf.Tensor( [[[ 0.00300496 -0.02925059 -0.01254098] [ 0.04872786 0.01087702 -0.03656749] [ 0.00446818 0.00290152 -0.02269397]]], shape=(1, 3, 3), dtype=float32)\n\nIf you change the ""mask_zero"" argument to False you get the exact same results. Does anyone know what\'s happening behind the scene? Any resources explaining the masking mechanism more thoroughly is highly appreciated.\n\nP.S: This is also an example of a full Neural Network which gives an identical outcome with and without masking:\n\ntf.random.set_seed(1) input = np.array([[1,2,0]]) # <--- 0 should be masked and ignored embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=True) masked_output = embedding(input) flatten = tf.keras.layers.Flatten()(masked_output) dense_middle = tf.keras.layers.Dense(4)(flatten) out = tf.keras.layers.Dense(1)(dense_middle) print(out)\n\nImprove this question\n\nedited Feb 10, 2023 at 12:24\n\nasked Feb 10, 2023 at 11:58\n\n58244 silver badges1313 bronze badges 2\n\nDoes this answer your question? How does mask_zero in Keras Embedding layer work?\n\n– Franciska Feb 10, 2023 at 12:59\n\n@Franciska Not really, the answer mostly repeats Tensorflow\'s manual which is remotely clear. For example, what does ""ignore"" mean? In math there is no such term and we are doing math in NNs. Does ""ignore"" mean setting weights to zero? Also I gave an example here which shows that the mask doesn\'t affect the following layers at all (as opposed to the answer given in that link).\n\n– Amin Shn Feb 10, 2023 at 13:16\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIn TensorFlow/Keras, masking enables you to disregard certain parts of a tensor, typically those set to zero, when executing the forward pass of your neural network. This can be helpful when dealing with sequences of varying length, where padding is used to make all sequences the same length. In the forward pass, the covered-up elements are taken as having a value of 0, so that their effect on the output is ignored.\n\nIn the example you provided, the Embedding layer is set to mask zeros via the mask_zero argument, yet the outcome is the same regardless of whether mask_zero is set to True or False. This is because the example just has one input tensor with no zero values, thus there is no contrast in the output.\n\nUnderneath, TensorFlow implements masking by using a special tensor mask that is multiplied element-wise with the input tensor during the forward pass. This mask tensor has the same shape as the input tensor and comprises binary values that indicate if each element should be included or not.\n\ninputs = tf.keras.layers.Input(shape=(3,)) embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=True)(inputs) masking = tf.keras.layers.Masking()(embedding) flatten = tf.keras.layers.Flatten()(masking) dense_middle = tf.keras.layers.Dense(4)(flatten) output = tf.keras.layers.Dense(1)(dense_middle) model = tf.keras.Model(inputs, output)\n\nBy doing this, the network will be able to take advantage of the zeros when the ""mask_zero"" argument is set to False and will disregard them when it is True, resulting in different predictions.\n\nedited Feb 10, 2023 at 12:48\n\nanswered Feb 10, 2023 at 12:07\n\nsilentlyakittensilentlyakitten\n\n1611 bronze badge 10\n\nThe last element of the input is actually zero. Also I added another part you might want to check.\n\n– Amin Shn Feb 10, 2023 at 12:16\n\nIt is true that the final element of the input in this example is zero, which gets disregarded when ""mask_zero"" is enabled. Consequently, the final element of the embedded input won\'t be taken into account when training or making predictions, and its gradients will not be calculated during the training procedure. Also, I checked out the other part that you included.\n\n– silentlyakitten Feb 10, 2023 at 12:27\n\nWhen ""mask_zero"" is set to false, the Embedding layer does not apply a mask to the input and accounts for every element in training and forecasting. The output is then pushed through the Flatten layer, which reduces the shape to (1, 9). This flattened output is then handled by two Dense layers, creating the final output of (1, 1). Regardless of whether the ""mask_zero"" is false, the zero element is still accounted for in the calculation of the output, so the result is the same.\n\n– silentlyakitten Feb 10, 2023 at 12:28\n\nSo are you saying the mask is not broadcasted through the network? How can I make it broadcasted so that the network uses zero if the mask_zero is false and not using it when it is True and result in different predictions?\n\n– Amin Shn Feb 10, 2023 at 12:30\n\nThe mask created by the Embedding layer with the ""mask_zero"" argument set to True is not automatically broadcasted to subsequent layers. To pass the mask information, you need to wrap the output of the Embedding layer with a Keras Masking layer\n\n– silentlyakitten Feb 10, 2023 at 12:37\n\n | Show 5 more comments\n\nAs I see it, the question does not look at the right thing to see the differences. Masking does not work by means of modifying the values of the output of a layer. Masking adds a _keras_mask attribute, that specifies values in the tensor that should not be taken into account when calculating loss or metric functions. I share a modified version of your code that shows that while the values of the masked_output Tensor are the same for both cases, when you calculate the MAE error with the same target you get different results. (Note that I don\'t use seed to force reproducible results for the embedding layer, I use ones as initialiser which forces all embedding to be one, so that the different entries will be counted)\n\nimport tensorflow as tf embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=True, embeddings_initializer=""ones"") masked_output = embedding(np.array([[1,2,0]])) print(""+++++++++++++++++++++"") print(""+++ masking = on +++"") print(""ATTR: "", hasattr(masked_output, ""_keras_mask"")) print(""ATTR-VALS:"", masked_output._keras_mask) print(""MAE loss :"",tf.keras.losses.MeanAbsoluteError(reduction=""sum"")(tf.zeros_like(masked_output), masked_output)) embedding = tf.keras.layers.Embedding(input_dim=10, output_dim=3, mask_zero=False, embeddings_initializer=""ones"") masked_output = embedding(np.array([[1,2,0]])) print(""+++++++++++++++++++++"") print(""+++ masking = off +++"") print(""ATTR: "", hasattr(masked_output, ""_keras_mask"")) print(""MAE loss :"",tf.keras.losses.MeanAbsoluteError(reduction=""sum"")(tf.zeros_like(masked_output), masked_output))\n\n+++++++++++++++++++++ +++ masking = on +++ ATTR: True ATTR-VALS: tf.Tensor([[ True True False]], shape=(1, 3), dtype=bool) MAE loss : tf.Tensor(2.0, shape=(), dtype=float32) +++++++++++++++++++++ +++ masking = off +++ ATTR: False MAE loss : tf.Tensor(3.0, shape=(), dtype=float32)\n\nYou see in the code the existence of the _keras_mask attribute and its values. It is present when masking is on, but not present when masking is off. _keras_mask is a boolean mask that is used within Loss and Metrics classes to restrict the loss to take into account only valid values.\n\nVery interestingly, you can do this manually as well. Adding the following code at the ned of the code above\n\nprint(""+++ masking = manual +++"") masked_output._keras_mask=tf.constant([[False, False, True]], tf.bool) print(""ATTR: "", hasattr(masked_output, ""_keras_mask"")) print(""ATTR-VALS:"", masked_output._keras_mask) print(""MAE loss :"",tf.keras.losses.MeanAbsoluteError(reduction=""sum"")(tf.zeros_like(masked_output), masked_output))\n\n+++ masking = manual +++ ATTR: True ATTR-VALS: tf.Tensor([[False False True]], shape=(1, 3), dtype=bool) MAE loss : tf.Tensor(1.0, shape=(), dtype=float32)\n\nThis means you can manually mask arbitrary parts of the tensors when calculating the loss.\n\nYou can see how it is performed in the call method of the Masking layer\n\nhttps://github.com/keras-team/keras/blob/v2.14.0/keras/layers/core/masking.py#L26-L91\n\nFor other layers supporting masking, they provide the compute_mask method which will be called in the Layer base class.\n\nedited Sep 28, 2023 at 9:05\n\nanswered Sep 28, 2023 at 8:21\n\n29933 silver badges88 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nkeras or ask your own question.\n\nAn open-source development paradigm\n\nDevelopers get by with a little help from AI: Stack Overflow Knows code...\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe return of Staging Ground to Stack Overflow\n\nThe 2024 Developer Survey Is Live\n\n37 How does mask_zero in Keras Embedding layer work?\n\n0 How to display the output that Microsoft Graph API returns\n\n3 how to get raw message with microsoft graph?\n\n1 Get Information from Microsoft Academic Graph API\n\n1 Accessing all data in API response\n\n1 Query to pulled data using Microsoft graph\n\n1 How to get response header in c# Microsoft graph api request\n\n1 Microsoft Graph API from microsoft streams app\n\n1 Cannot understand or convert Microsoft Graph API HTTP request to python request\n\n0 Python - how to get JSON from a GraphQL api?\n\n0 Microsoft Graph API Read Mail with Python\n\nHot Network Questions\n\nLate IRA contribution\n\nIs there a word that means both ""house"" and ""apartment""?\n\nHow long should it take you to finish a page on average?\n\nDisplaying the Footnoterule and Footnotes Farther Up the Page (One Time)\n\nBack Transforming log-log Model for Prediction\n\nWhat is specifically Dutch about light bulbs?\n\nIs it bad that I gave the wrong answer to a question during a recorded talk?\n\nWhat is causing my oil pan to fill with a mysterious fluid?\n\nlinkedin job post ""product tester\' recieved a email check PDF to deposit so I can purchase the product which I am supposed to test..Scam?\n\nDynamic name with glue in mutate call\n\nA short story about two identical twins raised separately\n\nHow do Theistic Evolutionists interpret Genesis 2:7 in light of Ezekiel 37:1-14?\n\nShould I use the ""time"" keyword, the ""/usr/bin/time"" command or the ""times"" builtin to time the execution of commands in bash?\n\nHarmonic minor \'A\'. How is left hand (bass staff) affected?\n\nSciFi book, about a human colony on the moon. A central computer becomes sentient and forms an alliance or teams up with the main character\n\nMad Max is sentenced to the Gulags but gets cast out?\n\nIs there any etymological connection between the two dominant meanings of ""or""?\n\nCan a rental agreement state that no guests or parties are allowed?\n\nCan I get the location of bash before executing a script?\n\nSplit a number in half, sum it, square it and get the number back\n\nDo ends really exist?\n\nHoneymoon in Sri Lanka - what should my marital status be on the visa application?\n\nIs it bad style to write x^2, 2^\\frac{1}{2} and 2^\\sqrt{2}?\n\nWas the appearance of the sand worms in David Lynch\'s Dune (1984) completely original to that film? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-05-30T08:08:10', 'title': 'python - How does masking work in Tensorflow Keras - Stack Overflow', 'url': 'https://stackoverflow.com/questions/75410827/how-does-masking-work-in-tensorflow-keras'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow to properly mask a numpy 2D array?\n\nModified 1 year, 6 months ago\n\nSay I have a two dimensional array of coordinates that looks something like\n\nx = array([[1,2],[2,3],[3,4]])\n\nPreviously in my work so far, I generated a mask that ends up looking something like\n\nmask = [False,False,True]\n\nWhen I try to use this mask on the 2D coordinate vector, I get an error\n\nnewX = np.ma.compressed(np.ma.masked_array(x,mask)) >>>numpy.ma.core.MaskError: Mask and data not compatible: data size is 6, mask size is 3.`\n\nwhich makes sense, I suppose. So I tried to simply use the following mask instead:\n\nmask2 = np.column_stack((mask,mask)) newX = np.ma.compressed(np.ma.masked_array(x,mask2))\n\nAnd what I get is close:\n\nto what I would expect (and want):\n\n>>>array([[1,2],[2,3]])\n\nThere must be an easier way to do this?\n\nImprove this question\n\nasked Jul 5, 2016 at 1:18\n\npretzlstylepretzlstyle\n\n2,89277 gold badges2626 silver badges4545 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIs this what you are looking for?\n\nimport numpy as np x[~np.array(mask)] # array([[1, 2], # [2, 3]])\n\nOr from numpy masked array:\n\nnewX = np.ma.array(x, mask = np.column_stack((mask, mask))) newX # masked_array(data = # [[1 2] # [2 3] # [-- --]], # mask = # [[False False] # [False False] # [ True True]], # fill_value = 999999)\n\nedited Jul 5, 2016 at 1:39\n\nanswered Jul 5, 2016 at 1:30\n\n213k3333 gold badges355355 silver badges372372 bronze badges 0\n\nWith np.where you can do all sorts of things:\n\nx_maskd = np.where(mask, x, 0)\n\nnp.where takes three arguments, a condition, x, and y. All three arguments must be broadcast-able to the same shape. In locations where mask is True, the x value is returned. Otherwise, the y value is returned.\n\nedited Dec 21, 2022 at 15:34\n\n2,15911 gold badge1111 silver badges2727 bronze badges\n\nanswered Jan 20, 2020 at 14:11\n\nMendi BarelMendi Barel\n\n3,58711 gold badge2424 silver badges2424 bronze badges 1\n\nNot many understand that np.where is a line-saver !\n\n– Artashes Commented Nov 17, 2022 at 15:17\n\nIn [379]: x Out[379]: array([[1, 2], [2, 3], [3, 4]])\n\nMake a 3 element boolean mask:\n\nIn [380]: rowmask=np.array([False,False,True])\n\nThat can be used to select the rows where it is True, or where it is False. In both cases the result is 2d:\n\nIn [381]: x[rowmask,:] Out[381]: array([[3, 4]]) In [382]: x[~rowmask,:] Out[382]: array([[1, 2], [2, 3]])\n\nThis is without using the MaskedArray subclass. To make such array, we need a mask that matches x in shape. There isn\'t provision for masking just one dimension.\n\nIn [393]: xmask=np.stack((rowmask,rowmask),-1) # column stack In [394]: xmask Out[394]: array([[False, False], [False, False], [ True, True]], dtype=bool) In [395]: np.ma.MaskedArray(x,xmask) Out[395]: masked_array(data = [[1 2] [2 3] [-- --]], mask = [[False False] [False False] [ True True]], fill_value = 999999)\n\nApplying compressed to that produces a raveled array: array([1, 2, 2, 3])\n\nSince masking is element by element, it could mask one element in row 1, 2 in row 2 etc. So in general compressing, removing the masked elements, will not yield a 2d array. The flattened form is the only general choice.\n\nnp.ma makes most sense when there\'s a scattering of masked values. It isn\'t of much value if you want want to select, or deselect, whole rows or columns.\n\nHere are more typical masked arrays:\n\nIn [403]: np.ma.masked_inside(x,2,3) Out[403]: masked_array(data = [[1 --] [-- --] [-- 4]], mask = [[False True] [ True True] [ True False]], fill_value = 999999) In [404]: np.ma.masked_equal(x,2) Out[404]: masked_array(data = [[1 --] [-- 3] [3 4]], mask = [[False True] [ True False] [False False]], fill_value = 2) In [406]: np.ma.masked_outside(x,2,3) Out[406]: masked_array(data = [[-- 2] [2 3] [3 --]], mask = [[ True False] [False False] [False True]], fill_value = 999999)\n\nedited Jul 5, 2016 at 2:42\n\nanswered Jul 5, 2016 at 2:37\n\n228k1414 gold badges249249 silver badges373373 bronze badges\n\nSince none of these solutions worked for me, I thought to write down what solution did, maybe it will useful for somebody else. I use python 3.x and I worked on two 3D arrays. One, which I call data_3D contains float values of recordings in a brain scan, and the other, template_3D contains integers which represent regions of the brain. I wanted to choose those values from data_3D corresponding to an integer region_code as per template_3D:\n\nmy_mask = np.in1d(template_3D, region_code).reshape(template_3D.shape) data_3D_masked = data_3D[my_mask]\n\nwhich gives me a 1D array of only relevant recordings.\n\nanswered Jun 8, 2017 at 10:31\n\nVahid S. BokharaieVahid S. Bokharaie\n\n98722 gold badges99 silver badges2626 bronze badges\n\nA = [[ 8. 0. 165. 22. 164. 47. 184. 185.] [ 0. 6. -74. -27. 63. 49. -46. -48.] [165. -74. 0. 0. 0. 0. 0. 0.] [ 22. -27. 0. 0. 0. 0. 0. 0.] [164. 63. 0. 0. 0. 0. 0. 0.] [ 47. 49. 0. 0. 0. 0. 0. 0.] [184. -46. 0. 0. 0. 0. 0. 0.] [185. -48. 0. 0. 0. 0. 0. 0.]]\n\nmask = np.array([True, True, True, False, True, False, True, False])\n\nthen your masked A becomes\n\nA[mask, :][:, mask] = [[ 8. 0. 165. 164. 184.] [ 0. 6. -74. 63. -46.] [165. -74. 0. 0. 0.] [164. 63. 0. 0. 0.] [184. -46. 0. 0. 0.]]\n\nedited Sep 22, 2021 at 21:35\n\nFrancois Vanderseypen\n\n1,52122 gold badges1212 silver badges2323 bronze badges\n\nanswered Sep 22, 2021 at 11:46\n\nYeonTaek KimYeonTaek Kim\n\nIn your last example, the problem is not the mask. It is your use of compressed. From the docstring of compressed:\n\nReturn all the non-masked data as a 1-D array.\n\nSo compressed flattens the nonmasked values into a 1-d array. (It has to, because there is no guarantee that the compressed data will have an n-dimensional structure.)\n\nTake a look at the masked array before you compress it:\n\nIn [8]: np.ma.masked_array(x, mask2) Out[8]: masked_array(data = [[1 2] [2 3] [-- --]], mask = [[False False] [False False] [ True True]], fill_value = 999999)\n\nanswered Jul 5, 2016 at 1:46\n\nWarren WeckesserWarren Weckesser\n\n113k1919 gold badges202202 silver badges220220 bronze badges 2\n\nYou\'re right, its correct before I compress it. I will read the documentation for a way to remove masked elements while preserving array dimensionality. Thanks\n\n– pretzlstyle Commented Jul 5, 2016 at 1:48\n\nIf I understand what you are trying to do, @Psidom\'s first suggestion looks reasonable. In particular, you probably don\'t need a masked array. Just index a regular array with a boolean array.\n\n– Warren Weckesser Commented Jul 5, 2016 at 1:50\n\nmasked_X = np.where(mask, X, 0) is the fastest & the simplest way to mask a data :\n\nX = np.array([[2,-1,4], [3,-3,1], [9,-7,2]]) mask = np.identity(3)\n\n%timeit np.where(mask,X,0)\n\n969 ns ± 14.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n\n%timeit np.ma.array(X, mask=mask)\n\n6.47 µs ± 85.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\nI let you conclude !\n\nanswered Nov 17, 2022 at 15:38\n\n12011 silver badge99 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nmasked-array or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n0 Numpy:Getting rows of array where the corresponding array has a specific value results in IndexError: boolean index did not match indexed array along\n\n2 Most efficient way to mask an opencv bgr with a boolean array\n\n1 Is there a verctorized approach in PyTorch to get this result?\n\n1 numpy Mask 2d array rows by ranges from 1d array\n\n0 How to best loop through masked 2d array\n\n1 Mask an array by value then mask the corresponding Matrix\n\n1 strange behaviour of numpy masked array\n\n0 Python mask for 2D array\n\n7 Mask 2D numpy array\n\n1 Numpy Masking with Array\n\n0 creating 2d mask array and applying on a 2d array\n\n0 python 2-d array masking error\n\n1 How to mask rows of a 2D numpy matrix by values in 1D list?\n\n1 How to mask numpy 2D array by index?\n\n1 building mask for 2d array by index\n\nHot Network Questions\n\nBig zeros in block diagonal matrix\n\nAlternatives to iterrow loops in python pandas dataframes\n\nHow does \\if:w work?\n\nDHCP assigned addresses following devices/users and routing\n\nIs it an option for the ls utility specified in POSIX.1-2017?\n\nAre you radical enough to solve this SURDOKU?\n\nIs the variance of the mean of a set of possibly dependent random variables less than the average of their respective variances?\n\nTime integration of first-order ODE with higher-order information\n\nWhen, if ever, is bribery legal?\n\nWhat properties to look for in a bottom bracket to survive winter use on a road bike\n\nWhy does IPC 2221 require so much more spacing at elevation?\n\nCoping with consequences of a dog bite before buying a puppy\n\nDoes the cosmological constant entail a mass for the graviton?\n\nWhy do jet aircraft need chocks when they have parking brakes?\n\nMOSFET Datasheet Confusion\n\nHow can one count how many pixels a GIF image has via command line?\n\nSci fi book that has a tunnel. Depending on where you go through wall, go to different planet\n\nWhere did Wordsworth describe Keats\'s poetry as ""very pretty paganism""?\n\nIs infinity a number?\n\nIs the resurrection of righteous and wicked at the same time?, or two separate events separated by a millenium time period?\n\nLargest possible relative error\n\nHas the Journal of Fluid Mechanics really published more than 800 volumes?\n\nMathematical Induction over two numbers\n\nWhy seperating a sphere gives different shades? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_3', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow to properly mask a numpy 2D array?\n\nModified 1 year, 6 months ago\n\nSay I have a two dimensional array of coordinates that looks something like\n\nx = array([[1,2],[2,3],[3,4]])\n\nPreviously in my work so far, I generated a mask that ends up looking something like\n\nmask = [False,False,True]\n\nWhen I try to use this mask on the 2D coordinate vector, I get an error\n\nnewX = np.ma.compressed(np.ma.masked_array(x,mask)) >>>numpy.ma.core.MaskError: Mask and data not compatible: data size is 6, mask size is 3.`\n\nwhich makes sense, I suppose. So I tried to simply use the following mask instead:\n\nmask2 = np.column_stack((mask,mask)) newX = np.ma.compressed(np.ma.masked_array(x,mask2))\n\nAnd what I get is close:\n\nto what I would expect (and want):\n\n>>>array([[1,2],[2,3]])\n\nThere must be an easier way to do this?\n\nImprove this question\n\nasked Jul 5, 2016 at 1:18\n\npretzlstylepretzlstyle\n\n2,89277 gold badges2626 silver badges4545 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIs this what you are looking for?\n\nimport numpy as np x[~np.array(mask)] # array([[1, 2], # [2, 3]])\n\nOr from numpy masked array:\n\nnewX = np.ma.array(x, mask = np.column_stack((mask, mask))) newX # masked_array(data = # [[1 2] # [2 3] # [-- --]], # mask = # [[False False] # [False False] # [ True True]], # fill_value = 999999)\n\nedited Jul 5, 2016 at 1:39\n\nanswered Jul 5, 2016 at 1:30\n\n213k3333 gold badges355355 silver badges372372 bronze badges 0\n\nWith np.where you can do all sorts of things:\n\nx_maskd = np.where(mask, x, 0)\n\nnp.where takes three arguments, a condition, x, and y. All three arguments must be broadcast-able to the same shape. In locations where mask is True, the x value is returned. Otherwise, the y value is returned.\n\nedited Dec 21, 2022 at 15:34\n\n2,15911 gold badge1111 silver badges2727 bronze badges\n\nanswered Jan 20, 2020 at 14:11\n\nMendi BarelMendi Barel\n\n3,58711 gold badge2424 silver badges2424 bronze badges 1\n\nNot many understand that np.where is a line-saver !\n\n– Artashes Commented Nov 17, 2022 at 15:17\n\nIn [379]: x Out[379]: array([[1, 2], [2, 3], [3, 4]])\n\nMake a 3 element boolean mask:\n\nIn [380]: rowmask=np.array([False,False,True])\n\nThat can be used to select the rows where it is True, or where it is False. In both cases the result is 2d:\n\nIn [381]: x[rowmask,:] Out[381]: array([[3, 4]]) In [382]: x[~rowmask,:] Out[382]: array([[1, 2], [2, 3]])\n\nThis is without using the MaskedArray subclass. To make such array, we need a mask that matches x in shape. There isn\'t provision for masking just one dimension.\n\nIn [393]: xmask=np.stack((rowmask,rowmask),-1) # column stack In [394]: xmask Out[394]: array([[False, False], [False, False], [ True, True]], dtype=bool) In [395]: np.ma.MaskedArray(x,xmask) Out[395]: masked_array(data = [[1 2] [2 3] [-- --]], mask = [[False False] [False False] [ True True]], fill_value = 999999)\n\nApplying compressed to that produces a raveled array: array([1, 2, 2, 3])\n\nSince masking is element by element, it could mask one element in row 1, 2 in row 2 etc. So in general compressing, removing the masked elements, will not yield a 2d array. The flattened form is the only general choice.\n\nnp.ma makes most sense when there\'s a scattering of masked values. It isn\'t of much value if you want want to select, or deselect, whole rows or columns.\n\nHere are more typical masked arrays:\n\nIn [403]: np.ma.masked_inside(x,2,3) Out[403]: masked_array(data = [[1 --] [-- --] [-- 4]], mask = [[False True] [ True True] [ True False]], fill_value = 999999) In [404]: np.ma.masked_equal(x,2) Out[404]: masked_array(data = [[1 --] [-- 3] [3 4]], mask = [[False True] [ True False] [False False]], fill_value = 2) In [406]: np.ma.masked_outside(x,2,3) Out[406]: masked_array(data = [[-- 2] [2 3] [3 --]], mask = [[ True False] [False False] [False True]], fill_value = 999999)\n\nedited Jul 5, 2016 at 2:42\n\nanswered Jul 5, 2016 at 2:37\n\n228k1414 gold badges249249 silver badges373373 bronze badges\n\nSince none of these solutions worked for me, I thought to write down what solution did, maybe it will useful for somebody else. I use python 3.x and I worked on two 3D arrays. One, which I call data_3D contains float values of recordings in a brain scan, and the other, template_3D contains integers which represent regions of the brain. I wanted to choose those values from data_3D corresponding to an integer region_code as per template_3D:\n\nmy_mask = np.in1d(template_3D, region_code).reshape(template_3D.shape) data_3D_masked = data_3D[my_mask]\n\nwhich gives me a 1D array of only relevant recordings.\n\nanswered Jun 8, 2017 at 10:31\n\nVahid S. BokharaieVahid S. Bokharaie\n\n98722 gold badges99 silver badges2626 bronze badges\n\nA = [[ 8. 0. 165. 22. 164. 47. 184. 185.] [ 0. 6. -74. -27. 63. 49. -46. -48.] [165. -74. 0. 0. 0. 0. 0. 0.] [ 22. -27. 0. 0. 0. 0. 0. 0.] [164. 63. 0. 0. 0. 0. 0. 0.] [ 47. 49. 0. 0. 0. 0. 0. 0.] [184. -46. 0. 0. 0. 0. 0. 0.] [185. -48. 0. 0. 0. 0. 0. 0.]]\n\nmask = np.array([True, True, True, False, True, False, True, False])\n\nthen your masked A becomes\n\nA[mask, :][:, mask] = [[ 8. 0. 165. 164. 184.] [ 0. 6. -74. 63. -46.] [165. -74. 0. 0. 0.] [164. 63. 0. 0. 0.] [184. -46. 0. 0. 0.]]\n\nedited Sep 22, 2021 at 21:35\n\nFrancois Vanderseypen\n\n1,52122 gold badges1212 silver badges2323 bronze badges\n\nanswered Sep 22, 2021 at 11:46\n\nYeonTaek KimYeonTaek Kim\n\nIn your last example, the problem is not the mask. It is your use of compressed. From the docstring of compressed:\n\nReturn all the non-masked data as a 1-D array.\n\nSo compressed flattens the nonmasked values into a 1-d array. (It has to, because there is no guarantee that the compressed data will have an n-dimensional structure.)\n\nTake a look at the masked array before you compress it:\n\nIn [8]: np.ma.masked_array(x, mask2) Out[8]: masked_array(data = [[1 2] [2 3] [-- --]], mask = [[False False] [False False] [ True True]], fill_value = 999999)\n\nanswered Jul 5, 2016 at 1:46\n\nWarren WeckesserWarren Weckesser\n\n113k1919 gold badges202202 silver badges220220 bronze badges 2\n\nYou\'re right, its correct before I compress it. I will read the documentation for a way to remove masked elements while preserving array dimensionality. Thanks\n\n– pretzlstyle Commented Jul 5, 2016 at 1:48\n\nIf I understand what you are trying to do, @Psidom\'s first suggestion looks reasonable. In particular, you probably don\'t need a masked array. Just index a regular array with a boolean array.\n\n– Warren Weckesser Commented Jul 5, 2016 at 1:50\n\nmasked_X = np.where(mask, X, 0) is the fastest & the simplest way to mask a data :\n\nX = np.array([[2,-1,4], [3,-3,1], [9,-7,2]]) mask = np.identity(3)\n\n%timeit np.where(mask,X,0)\n\n969 ns ± 14.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n\n%timeit np.ma.array(X, mask=mask)\n\n6.47 µs ± 85.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\nI let you conclude !\n\nanswered Nov 17, 2022 at 15:38\n\n12011 silver badge99 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\nmasked-array or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n0 Numpy:Getting rows of array where the corresponding array has a specific value results in IndexError: boolean index did not match indexed array along\n\n2 Most efficient way to mask an opencv bgr with a boolean array\n\n1 Is there a verctorized approach in PyTorch to get this result?\n\n1 numpy Mask 2d array rows by ranges from 1d array\n\n0 How to best loop through masked 2d array\n\n1 Mask an array by value then mask the corresponding Matrix\n\n1 strange behaviour of numpy masked array\n\n0 Python mask for 2D array\n\n7 Mask 2D numpy array\n\n1 Numpy Masking with Array\n\n0 creating 2d mask array and applying on a 2d array\n\n0 python 2-d array masking error\n\n1 How to mask rows of a 2D numpy matrix by values in 1D list?\n\n1 How to mask numpy 2D array by index?\n\n1 building mask for 2d array by index\n\nHot Network Questions\n\nBig zeros in block diagonal matrix\n\nAlternatives to iterrow loops in python pandas dataframes\n\nHow does \\if:w work?\n\nDHCP assigned addresses following devices/users and routing\n\nIs it an option for the ls utility specified in POSIX.1-2017?\n\nAre you radical enough to solve this SURDOKU?\n\nIs the variance of the mean of a set of possibly dependent random variables less than the average of their respective variances?\n\nTime integration of first-order ODE with higher-order information\n\nWhen, if ever, is bribery legal?\n\nWhat properties to look for in a bottom bracket to survive winter use on a road bike\n\nWhy does IPC 2221 require so much more spacing at elevation?\n\nCoping with consequences of a dog bite before buying a puppy\n\nDoes the cosmological constant entail a mass for the graviton?\n\nWhy do jet aircraft need chocks when they have parking brakes?\n\nMOSFET Datasheet Confusion\n\nHow can one count how many pixels a GIF image has via command line?\n\nSci fi book that has a tunnel. Depending on where you go through wall, go to different planet\n\nWhere did Wordsworth describe Keats\'s poetry as ""very pretty paganism""?\n\nIs infinity a number?\n\nIs the resurrection of righteous and wicked at the same time?, or two separate events separated by a millenium time period?\n\nLargest possible relative error\n\nHas the Journal of Fluid Mechanics really published more than 800 volumes?\n\nMathematical Induction over two numbers\n\nWhy seperating a sphere gives different shades? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-09T10:10:53', 'title': 'python - How to properly mask a numpy 2D array? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/38193958/how-to-properly-mask-a-numpy-2d-array'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nIs broadcasting in Tensorflow a view or a copy?\n\nAsked 3 years, 2 months ago\n\nModified 3 years, 2 months ago\n\nPlease clarify if broadcasting in Tensorflow will allocate a new memory buffer at broadcasting.\n\nIn the Tensorflow document Introduction to Tensors - Broadcasting, one sentence says (emphasis added):\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory\n\nHowever in another sentence it says:\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\n\nprint(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))\n\ntf.broadcast_to says it is a broadcast operation.\n\nBroadcast an array for a compatible shape.\n\nThen according to ""the broadcast operation never materializes the expanded tensors in memory"" statement above, it should not be materializing.\n\nPlease help clarify what the document is actually saying.\n\nImprove this question\n\nedited Apr 12, 2021 at 12:54\n\n59.6k2929 gold badges149149 silver badges169169 bronze badges\n\nasked Apr 7, 2021 at 0:25\n\n21.2k2828 gold badges133133 silver badges232232 bronze badges 0\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIt says normally broadcast operation never materializes the expanded tensor in memory because of both time and space efficiency.\n\nx = tf.constant([1, 2, 3]) y = tf.constant(2) print(x * y) tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n\nBut if we want to look at how it looks after broadcasting then we use tf.broadcast_to which of course needs to materializing the tensor.\n\nx = tf.constant([1, 2, 3, 4]) y = tf.broadcast_to(x, [3, 4]) print(y) tf.Tensor( [[1 2 3 4] [1 2 3 4] [1 2 3 4]], shape=(3, 4), dtype=int32)\n\nAccording to the documentation\n\nWhen doing broadcasted operations such as multiplying a tensor by a scalar, broadcasting (usually) confers some time or space benefit, as the broadcasted tensor is never materialized.\n\nHowever, broadcast_to does not carry with it any such benefits. The newly-created tensor takes the full memory of the broadcasted shape. (In a graph context, broadcast_to might be fused to subsequent operation and then be optimized away, however.)\n\nanswered Apr 7, 2021 at 2:10\n\n17k66 gold badges5757 silver badges111111 bronze badges 1\n\nThanks for the answer. I am still unsure what is occurring under the hood. When (x * y) is executed, x is rank 1 and y is rank 0, hence y is broadcast to rank 1 to match x. This broadcast may not need additional memory but how about the result (x * y)? I believe it is a new memory allocated and does not share memory buffer of x nor y. Then somewhere it needs to materialize (allocate buffer) to store (x*y), then it is materializing. If so how ""never materialize"" can be true... no sure.\n\n– mon Commented Apr 7, 2021 at 9:22\n\nNot the answer you\'re looking for? Browse other questions tagged\n\narray-broadcasting or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n1 Pytorch Error: RuntimeError: output with shape [1, 3, 1] doesn\'t match the broadcast shape [3, 3, 7]\n\n4 How is broadcasting applying in this example of numpy?\n\n3 tensorflow variable assign broadcasting\n\n0 Broadcasting between two same-rank tensors in tensorflow\n\n2 What is supported by broadcasting in tensorflow? How dimensions matches determined?\n\n3 Automatic broadcasting in Tensorflow\n\n4 TensorFlow broadcasting\n\n0 Tensorflow matrices broadcast\n\n4 what happens under the hood of broadcasting a numpy array\n\n0 Advanced broadcasting in TensorFlow (or Numpy)\n\n2 Trouble Understanding broadcasting behavior for tensors\n\nHot Network Questions\n\nWhat is the value of air anisotropy?\n\nWhat type of interaction in a π-complex?\n\nStaying in USA longer than 3 months\n\nIs there a generalization of factoring that can be extended to the Real numbers?\n\nAre there any parts of the US Constitution that state that the laws apply universally to all citizens?\n\nmirrorlist.centos.org no longer resolve?\n\nJava: Benchmark findFirst() and findAny() methods on non-parallel streams\n\nWhy is pressure in the outermost layer of a star lower than at its center?\n\nOld SF story about someone who detonated an atomic bomb, sacrificing self to save society from an evil government\n\nDoes the Grimme D3 correction improve band gaps of vdW heterostructures?\n\nHave the inventors of LLMs/image-generators/w/e fulfilled Kant\'s assertion about the ""art"" of the productive imagination?\n\nCan someone explain the Trump immunity ruling?\n\nCan you arrange 25 whole numbers (not necessarily all different) so that the sum of any three successive terms is even but the sum of all 25 is odd?\n\nBasic stems that end in ""w""?\n\nAre US enlisted personnel (as opposed to officers) required, or allowed, to disobey unlawful orders?\n\nReduce the column padding in tabular environment\n\nShort exact sequence in the ideal class group\n\nWhat is this thin stream coming out from somewhere near the engine?\n\nWhat is this component - 8 legged inductor?\n\nPlausible reasons for the usage of Flying Ships\n\nWhy do I see low voltage in a repaired underground cable?\n\nAre Amalekites considered Edomites Halachically?\n\nClassification of efficient and inefficient algorithms and the scientific reasoning behind them\n\nIs there a drawback to using Heart\'s blood rote repeatedly? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_2', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nIs broadcasting in Tensorflow a view or a copy?\n\nAsked 3 years, 2 months ago\n\nModified 3 years, 2 months ago\n\nPlease clarify if broadcasting in Tensorflow will allocate a new memory buffer at broadcasting.\n\nIn the Tensorflow document Introduction to Tensors - Broadcasting, one sentence says (emphasis added):\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory\n\nHowever in another sentence it says:\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\n\nprint(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))\n\ntf.broadcast_to says it is a broadcast operation.\n\nBroadcast an array for a compatible shape.\n\nThen according to ""the broadcast operation never materializes the expanded tensors in memory"" statement above, it should not be materializing.\n\nPlease help clarify what the document is actually saying.\n\nImprove this question\n\nedited Apr 12, 2021 at 12:54\n\n59.6k2929 gold badges149149 silver badges169169 bronze badges\n\nasked Apr 7, 2021 at 0:25\n\n21.2k2828 gold badges133133 silver badges232232 bronze badges 0\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nIt says normally broadcast operation never materializes the expanded tensor in memory because of both time and space efficiency.\n\nx = tf.constant([1, 2, 3]) y = tf.constant(2) print(x * y) tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n\nBut if we want to look at how it looks after broadcasting then we use tf.broadcast_to which of course needs to materializing the tensor.\n\nx = tf.constant([1, 2, 3, 4]) y = tf.broadcast_to(x, [3, 4]) print(y) tf.Tensor( [[1 2 3 4] [1 2 3 4] [1 2 3 4]], shape=(3, 4), dtype=int32)\n\nAccording to the documentation\n\nWhen doing broadcasted operations such as multiplying a tensor by a scalar, broadcasting (usually) confers some time or space benefit, as the broadcasted tensor is never materialized.\n\nHowever, broadcast_to does not carry with it any such benefits. The newly-created tensor takes the full memory of the broadcasted shape. (In a graph context, broadcast_to might be fused to subsequent operation and then be optimized away, however.)\n\nanswered Apr 7, 2021 at 2:10\n\n17k66 gold badges5757 silver badges111111 bronze badges 1\n\nThanks for the answer. I am still unsure what is occurring under the hood. When (x * y) is executed, x is rank 1 and y is rank 0, hence y is broadcast to rank 1 to match x. This broadcast may not need additional memory but how about the result (x * y)? I believe it is a new memory allocated and does not share memory buffer of x nor y. Then somewhere it needs to materialize (allocate buffer) to store (x*y), then it is materializing. If so how ""never materialize"" can be true... no sure.\n\n– mon Commented Apr 7, 2021 at 9:22\n\nNot the answer you\'re looking for? Browse other questions tagged\n\narray-broadcasting or ask your own question.\n\nCommunity Products Roadmap Update, July 2024\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n1 Pytorch Error: RuntimeError: output with shape [1, 3, 1] doesn\'t match the broadcast shape [3, 3, 7]\n\n4 How is broadcasting applying in this example of numpy?\n\n3 tensorflow variable assign broadcasting\n\n0 Broadcasting between two same-rank tensors in tensorflow\n\n2 What is supported by broadcasting in tensorflow? How dimensions matches determined?\n\n3 Automatic broadcasting in Tensorflow\n\n4 TensorFlow broadcasting\n\n0 Tensorflow matrices broadcast\n\n4 what happens under the hood of broadcasting a numpy array\n\n0 Advanced broadcasting in TensorFlow (or Numpy)\n\n2 Trouble Understanding broadcasting behavior for tensors\n\nHot Network Questions\n\nWhat is the value of air anisotropy?\n\nWhat type of interaction in a π-complex?\n\nStaying in USA longer than 3 months\n\nIs there a generalization of factoring that can be extended to the Real numbers?\n\nAre there any parts of the US Constitution that state that the laws apply universally to all citizens?\n\nmirrorlist.centos.org no longer resolve?\n\nJava: Benchmark findFirst() and findAny() methods on non-parallel streams\n\nWhy is pressure in the outermost layer of a star lower than at its center?\n\nOld SF story about someone who detonated an atomic bomb, sacrificing self to save society from an evil government\n\nDoes the Grimme D3 correction improve band gaps of vdW heterostructures?\n\nHave the inventors of LLMs/image-generators/w/e fulfilled Kant\'s assertion about the ""art"" of the productive imagination?\n\nCan someone explain the Trump immunity ruling?\n\nCan you arrange 25 whole numbers (not necessarily all different) so that the sum of any three successive terms is even but the sum of all 25 is odd?\n\nBasic stems that end in ""w""?\n\nAre US enlisted personnel (as opposed to officers) required, or allowed, to disobey unlawful orders?\n\nReduce the column padding in tabular environment\n\nShort exact sequence in the ideal class group\n\nWhat is this thin stream coming out from somewhere near the engine?\n\nWhat is this component - 8 legged inductor?\n\nPlausible reasons for the usage of Flying Ships\n\nWhy do I see low voltage in a repaired underground cable?\n\nAre Amalekites considered Edomites Halachically?\n\nClassification of efficient and inefficient algorithms and the scientific reasoning behind them\n\nIs there a drawback to using Heart\'s blood rote repeatedly? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-03T14:19:03', 'title': 'Is broadcasting in Tensorflow a view or a copy? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/66978119/is-broadcasting-in-tensorflow-a-view-or-a-copy'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nIs there an alterntive to tf.keras.layers.Masking in PyTorch? Or Is there any way to implement it from scratch?\n\nAsked 2 years, 1 month ago\n\nModified 2 years, 1 month ago\n\nI have a time series dataset with a lot of NAs that I need to use with LSTM network. Previously with TensorFlow, I used to initially replace NAs with -1(Which is not present in the data) and use tf.keras.layers.Masking(Documentation) within the model to stop learning when the model encounters -1 and resume when encountering something else. Since then, I have switched to PyTorch and need to use something similar again. So, my question is this: Is there an alternative to this in PyTorch? Or is there any way to do this while using PyTorch? See the code version of what I do below:\n\n... dataset.fillna(-1, inplace = True) # Replace na cells with -1 for masking in LSTM ... ... model = tf.keras.models.Sequential() model.add(tf.keras.layers.Masking(mask_value=-1,input_shape=(timesteps, features))) model.add(tf.keras.layers.LSTM(32)) ... ...\n\nSuggestion made below is different as in tf.keras.layers.Masking, for each timestep in the input tensor, if all values in the input tensor at that timestep are equal to mask_value(-1), then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).\n\nImprove this question\n\nedited May 23, 2022 at 13:04\n\nasked May 23, 2022 at 8:12\n\nBakerStHoundBakerStHound\n\nDoes this answer your question? What would be the equivalent of keras.layers.Masking in pytorch?\n\n– Szymon Maszke Commented May 23, 2022 at 10:39\n\nHi @SzymonMaszke, thank you for the reply. In the example provided in the link, how would the zeros be interpreted by the RNN? In TensorFlow, tf.keras.layers.Masking masks a sequence by using a mask value to skip time-steps. Whenever mask value is encountered, learning is stopped and resumes with normal data.\n\n– BakerStHound Commented May 23, 2022 at 11:03\n\nYou can use PackedSequence and simply provide data of different length (via Python’s lists) to it. Instead of masking simply delete time-steps which won’t be used for teaching RNN.\n\n– Szymon Maszke Commented May 23, 2022 at 15:14\n\nDoes tensorflow have the function similar to pytorch\'s ""masked_fill_""\n\nHow to support masking in custom tf.keras.layers.Layer\n\nany similar function like df.mask for tensor in pytorch?\n\nDoes tensorflow have the function similar to pytorch\'s ""masked_fill_""\n\nHow to support masking in custom tf.keras.layers.Layer\n\nany similar function like df.mask for tensor in pytorch?\n\nWhat would be the equivalent of keras.layers.Masking in pytorch?\n\nHow do I do masking in PyTorch / Numpy with different dimensions?\n\nPyTorch differentiable mask\n\nSeeking Masking support for Dense Layer in Keras\n\nPyTorch: apply mask with different shape\n\nHow to apply mask to image tensors in PyTorch?\n\nPytorch: Assign values from one mask to another, masked by itself\n\nLoad 7 more related questions Show fewer related questions\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nKnow someone who can answer? Share a link to this question via email, Twitter, or Facebook.\n\nBrowse other questions tagged\n\npytorch or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n6 What would be the equivalent of keras.layers.Masking in pytorch?\n\nHot Network Questions\n\nLess ridiculous way to prove that an Ascii character compares equal with itself in Coq\n\n""All due respect to jazz."" - Does this mean the speaker likes it or dislikes it?\n\nHow do you say ""living being"" in Classical Latin?\n\nHave children\'s car seats not been proven to be more effective than seat belts alone for kids older than 24 months?\n\nHow to fix misaligned objects that look fine in viewport but not in render?\n\nIs there a way to non-destructively test whether an Ethernet cable is pure copper or copper-clad aluminum (CCA)?\n\nWhat to do if you disagree with a juror during master\'s thesis defense?\n\nAre both vocal cord and vocal chord correct?\n\nIs there any other reason to stockpile minerals aside preparing for war?\n\nWhite grids appears when export GraphicsRow to PDF\n\nException handling: is one exception type sufficient?\n\nDoes it matter if a fuse is on a positive or negative voltage?\n\nAre there paintings with Adam and Eve in paradise with the snake with legs?\n\nReconstructing Euro results\n\nAre there any CID episodes based on real-life events?\n\nWhat could explain that small planes near an airport are perceived as harassing homeowners?\n\nShould I accept an offer of being a teacher assistant without pay?\n\nÀ + infinitive at start of sentence\n\nHow would I say the exclamation ""What a [blank]"" in Latin?\n\nWhat is the relationship between gravitation, centripetal and centrifugal force on the Earth?\n\nHow to make D&D easier for kids?\n\nHow to bid a very strong hand with values in only 2 suits?\n\nWhat\'s Wrong With My Math - Odds of 3 Cards of the Same Suit When Drawing 10 Cards\n\nWhat does ‘a grade-hog’ mean? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_4', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nIs there an alterntive to tf.keras.layers.Masking in PyTorch? Or Is there any way to implement it from scratch?\n\nAsked 2 years, 1 month ago\n\nModified 2 years, 1 month ago\n\nI have a time series dataset with a lot of NAs that I need to use with LSTM network. Previously with TensorFlow, I used to initially replace NAs with -1(Which is not present in the data) and use tf.keras.layers.Masking(Documentation) within the model to stop learning when the model encounters -1 and resume when encountering something else. Since then, I have switched to PyTorch and need to use something similar again. So, my question is this: Is there an alternative to this in PyTorch? Or is there any way to do this while using PyTorch? See the code version of what I do below:\n\n... dataset.fillna(-1, inplace = True) # Replace na cells with -1 for masking in LSTM ... ... model = tf.keras.models.Sequential() model.add(tf.keras.layers.Masking(mask_value=-1,input_shape=(timesteps, features))) model.add(tf.keras.layers.LSTM(32)) ... ...\n\nSuggestion made below is different as in tf.keras.layers.Masking, for each timestep in the input tensor, if all values in the input tensor at that timestep are equal to mask_value(-1), then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).\n\nImprove this question\n\nedited May 23, 2022 at 13:04\n\nasked May 23, 2022 at 8:12\n\nBakerStHoundBakerStHound\n\nDoes this answer your question? What would be the equivalent of keras.layers.Masking in pytorch?\n\n– Szymon Maszke Commented May 23, 2022 at 10:39\n\nHi @SzymonMaszke, thank you for the reply. In the example provided in the link, how would the zeros be interpreted by the RNN? In TensorFlow, tf.keras.layers.Masking masks a sequence by using a mask value to skip time-steps. Whenever mask value is encountered, learning is stopped and resumes with normal data.\n\n– BakerStHound Commented May 23, 2022 at 11:03\n\nYou can use PackedSequence and simply provide data of different length (via Python’s lists) to it. Instead of masking simply delete time-steps which won’t be used for teaching RNN.\n\n– Szymon Maszke Commented May 23, 2022 at 15:14\n\nDoes tensorflow have the function similar to pytorch\'s ""masked_fill_""\n\nHow to support masking in custom tf.keras.layers.Layer\n\nany similar function like df.mask for tensor in pytorch?\n\nDoes tensorflow have the function similar to pytorch\'s ""masked_fill_""\n\nHow to support masking in custom tf.keras.layers.Layer\n\nany similar function like df.mask for tensor in pytorch?\n\nWhat would be the equivalent of keras.layers.Masking in pytorch?\n\nHow do I do masking in PyTorch / Numpy with different dimensions?\n\nPyTorch differentiable mask\n\nSeeking Masking support for Dense Layer in Keras\n\nPyTorch: apply mask with different shape\n\nHow to apply mask to image tensors in PyTorch?\n\nPytorch: Assign values from one mask to another, masked by itself\n\nLoad 7 more related questions Show fewer related questions\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nKnow someone who can answer? Share a link to this question via email, Twitter, or Facebook.\n\nBrowse other questions tagged\n\npytorch or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n6 What would be the equivalent of keras.layers.Masking in pytorch?\n\nHot Network Questions\n\nLess ridiculous way to prove that an Ascii character compares equal with itself in Coq\n\n""All due respect to jazz."" - Does this mean the speaker likes it or dislikes it?\n\nHow do you say ""living being"" in Classical Latin?\n\nHave children\'s car seats not been proven to be more effective than seat belts alone for kids older than 24 months?\n\nHow to fix misaligned objects that look fine in viewport but not in render?\n\nIs there a way to non-destructively test whether an Ethernet cable is pure copper or copper-clad aluminum (CCA)?\n\nWhat to do if you disagree with a juror during master\'s thesis defense?\n\nAre both vocal cord and vocal chord correct?\n\nIs there any other reason to stockpile minerals aside preparing for war?\n\nWhite grids appears when export GraphicsRow to PDF\n\nException handling: is one exception type sufficient?\n\nDoes it matter if a fuse is on a positive or negative voltage?\n\nAre there paintings with Adam and Eve in paradise with the snake with legs?\n\nReconstructing Euro results\n\nAre there any CID episodes based on real-life events?\n\nWhat could explain that small planes near an airport are perceived as harassing homeowners?\n\nShould I accept an offer of being a teacher assistant without pay?\n\nÀ + infinitive at start of sentence\n\nHow would I say the exclamation ""What a [blank]"" in Latin?\n\nWhat is the relationship between gravitation, centripetal and centrifugal force on the Earth?\n\nHow to make D&D easier for kids?\n\nHow to bid a very strong hand with values in only 2 suits?\n\nWhat\'s Wrong With My Math - Odds of 3 Cards of the Same Suit When Drawing 10 Cards\n\nWhat does ‘a grade-hog’ mean? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-06-30T03:23:28', 'title': 'python - Is there an alterntive to tf.keras.layers.Masking in PyTorch? Or Is there any way to implement it from scratch? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/72345237/is-there-an-alterntive-to-tf-keras-layers-masking-in-pytorch-or-is-there-any-wa'})], [Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nDoes masking only work for homogeneous batches? #1206\n\nEderSantana opened this issue\n\nDec 8, 2015 · 12 comments\n\nDoes masking only work for homogeneous batches? #1206\n\nEderSantana opened this issue\n\nDec 8, 2015 · 12 comments\n\nEderSantana commented\n\nPlease, let me know if I got this wrong.\n\nI was doing some experiments with masking and it seems that it doesn\'t work for heterogeneous batches, I mean those where in a single batch we have sequences with different lengths. Here is why: https://github.com/fchollet/keras/blob/master/keras/backend/theano_backend.py#L418\n\nIt will only mask out if all the values for all dimensions and all samples are zero. This means that mask will only kick out for homogenous batches (all samples have same length). But, training with homogeneous batches don\'t work well since the gradient steps will be biased to each sequence length, one at time instead of properly averaging across different lengths.\n\nIn the Theano backend, sequences can have different lengths in different batches, so this kind of masking could be replaced by using batches where everybody already have similar lengths. If that is also the case for TF, we may just get rid of masking.\n\nIn case we are looking to support heterogenous batches, one solution could be:\n\n# calculate masking per sample and reshape it to the size of the input switch = input.sum(axis=1, keepdims=True).repeat(input.shape[1], axis=1) output = T.switch(switch, output, 0. * output) return_states = [] for state, new_state in zip(states, new_states): # same for states switch = input.sum(axis=1, keepdims=True).repeat(states.shape[1], axis=1) return_states.append(T.switch(switch, new_state, state))\n\nOne thing about that solutions is that we have to make sure that all states are matrices, which is the case for conventional RNN. For the case of Neural Turing Machines and others where the states are tensors, we can just reshape them back and forth.\n\nI don\'t know if that solution would work for TensorFlow though. But here is one thing. Masking users should be warned to use homogenous batches for now.\n\nAgain, let me know if I got anything wrong.\n\nThe text was updated successfully, but these errors were encountered:\n\nConceptually I don\'t think other samples in the batch should affect masking for a specific sample. So that should definitely be fixed...\n\nBug fix + Allow Layers to be called with mask argument #1212\n\nHow does mask (for RNN) work in new version of keras? #1224\n\nWhat would be some suggestions for a fix?\n\nReminder of the position of the problem:\n\noutput, new_states = step_function(input, states) if masking: # if all-zero input timestep, return # all-zero output and unchanged states switch = T.any(input) output = T.switch(switch, output, 0. * output) return_states = [] for state, new_state in zip(states, new_states): return_states.append(T.switch(switch, new_state, state)) return [output] + return_states\n\noutput, new_states = step_function(input, states) if masking: # for now we raise an exception because tf.reduce_any will not work raise Exception(""Masking is Theano-only for the time being."") # if all-zero input timestep, return # all-zero output and unchanged states switch = tf.reduce_any(input) output = tf.control_flow_ops.cond(switch, lambda: output, lambda: 0. * output) return_states = [] for state, new_state in zip(states, new_states): return_states.append(tf.control_flow_ops.cond(switch, lambda: new_state, lambda: state)) states = return_states\n\n@fchollet I\'m just starting to look at this issue (we\'ve been using a somewhat older version of Keras recently). Can you speak to why you have returned to the ""masking value"" approach from the approach we had converged on before where mask was sent separately through the network? There are a host of issues with masking values that we discussed before: I don\'t see the appeal.\n\nAt any rate: an approach I would take to solving the problem would be to pass the mask into the rnn call, instead of using it to transform the input. Then the Theano backend can pass the mask into scan and use it in there like I had originally implemented before multiple backends.\n\nTo articulate the problem with the masked value approach: with ReLU activations, zero outputs are common, so there is no reason to think that it will be that uncommon to get an all-zero input to an RNN layer, especially with many stacked RNNs.\n\nThis could be further compounded by model pruning techniques which involve setting a very high percentage of weights to 0.\n\nI can redo this backend to match how I designed masking before (with mask passed along with input), but would like to understand if there was a reason to move away from that approach before I put in the effort.\n\nEderSantana commented\n\nFor the interested Keras users: I tested Sequential right before the backend and it still works with the layers defined with the new code. So, while we look for a better solution, if anybody has an urgency of using the past mask and sample_weight behavior just copy this somewhere: https://github.com/fchollet/keras/blob/9c1afbb66782fe67c796fae85cd68d8f619b8aa6/keras/models.py\n\nNOTE: This is a temporary solution, and is not guaranteed to work. It would be better to just have a separate env with Keras on that commit. You can do that with git checkout 9c1afbb66782fe67c796fa. But I hope that helps.\n\nwould like to understand if there was a reason to move away from that approach\n\nThe reason was the switch away from scan to an API that had a single input and a single output (which simplifies lots of things). The thread on the switch to the modular backend contains a discussion about the design choices made when developing the current RNN API. The reasons you mention are among those why masking is optional in the current setup, and will only be applied if a mask object is passed to the RNN layer.\n\nIf you have a better solution that involves explicit masking, then that\'s great.\n\nOK great. I\'ll take a look next week — NIPS is taking up most of my time right now.\n\nFor reference to people looking at this issue, most of the discussion about the new design was in #1014.\n\nMy initial thoughts are that we should slightly extend the new rnn function to take an arbitrary mask parameter.\n\ndef rnn(step_function, inputs, initial_states, go_backwards=False, input_mask=None):\n\nIt would then also need to return the mask as part of its returned tuple.\n\nThen it\'s trivial to reintroduce masking via the previous approach. It does mean that when you write your step_function you need to be mask aware, but that is not a major issue as that was already the case.\n\nThoughts? I\'ll start building that now on my branch and we can see how it looks.\n\n@wxs what are your thoughts on #1258?\n\nI think that #1258 fixes the base bug where masking ""bleeds"" across timesteps, but not the more subtle issue around value masking being dangerous.\n\nShould fix the bug for most people, though the tests don\'t currently work because of a dependency issue. Looks right to me.\n\nIt assumes input is 3d, though the docstring implies that input can be 4d or more (which I suppose could happen with, e.g. time distributed image inputs), though I think that was already an issue with the inputs = inputs.dimshuffle((1, 0, 2) line.\n\nIn other words, pending tests I think it\'s definitely worth merging since RNNs as it stands are broken, and I will in the meantime try to build a more robust system via mask relaying. I\'m intrigued by some of @dbonadiman ideas in #1014 around ""apply"" and ""broadcast"" being operations that layers can do to masks, but need to think about it some more.\n\njeffzhengye commented\n\n@wxs then can you help with the test in #1258 ? Because I am not sure how to fit it into test_recurrent.py. All the test passed locally on my computer.\n\nDo you also have time to have a look at the bidirectional rnn BUG I mentioned in #1258 ?\n\nCurrent masking approach propagates internal biases in recurrent layers #1300\n\nMasking passed to the backend RNN calls. #1310\n\nEderSantana mentioned this issue\n\nUsing Masking Layer for Sequence to Sequence Learning #957\n\nstale bot added the stale label\n\nstale bot closed this as completed\n\nvisionscaper commented\n\n@fchollet back in Dec. 2015 you accepted the pull request by @jeffzhengye that resolves this issue for Theano, back then there was no fix for Tensorflow.\n\nMy question is, has this been resolved for Tensorflow by now? A lot of time has past, so chances are good that this has been resolved, but I just wanted to have this confirmed.\n\nThe issue was that masking would only be applied if all the samples in a batch had the masking value at a certain time step, instead of allowing different masking per sample, per timestep.\n\nThanks for your time and awesome work on Keras!\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_2', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nDoes masking only work for homogeneous batches? #1206\n\nEderSantana opened this issue\n\nDec 8, 2015 · 12 comments\n\nDoes masking only work for homogeneous batches? #1206\n\nEderSantana opened this issue\n\nDec 8, 2015 · 12 comments\n\nEderSantana commented\n\nPlease, let me know if I got this wrong.\n\nI was doing some experiments with masking and it seems that it doesn\'t work for heterogeneous batches, I mean those where in a single batch we have sequences with different lengths. Here is why: https://github.com/fchollet/keras/blob/master/keras/backend/theano_backend.py#L418\n\nIt will only mask out if all the values for all dimensions and all samples are zero. This means that mask will only kick out for homogenous batches (all samples have same length). But, training with homogeneous batches don\'t work well since the gradient steps will be biased to each sequence length, one at time instead of properly averaging across different lengths.\n\nIn the Theano backend, sequences can have different lengths in different batches, so this kind of masking could be replaced by using batches where everybody already have similar lengths. If that is also the case for TF, we may just get rid of masking.\n\nIn case we are looking to support heterogenous batches, one solution could be:\n\n# calculate masking per sample and reshape it to the size of the input switch = input.sum(axis=1, keepdims=True).repeat(input.shape[1], axis=1) output = T.switch(switch, output, 0. * output) return_states = [] for state, new_state in zip(states, new_states): # same for states switch = input.sum(axis=1, keepdims=True).repeat(states.shape[1], axis=1) return_states.append(T.switch(switch, new_state, state))\n\nOne thing about that solutions is that we have to make sure that all states are matrices, which is the case for conventional RNN. For the case of Neural Turing Machines and others where the states are tensors, we can just reshape them back and forth.\n\nI don\'t know if that solution would work for TensorFlow though. But here is one thing. Masking users should be warned to use homogenous batches for now.\n\nAgain, let me know if I got anything wrong.\n\nThe text was updated successfully, but these errors were encountered:\n\nConceptually I don\'t think other samples in the batch should affect masking for a specific sample. So that should definitely be fixed...\n\nBug fix + Allow Layers to be called with mask argument #1212\n\nHow does mask (for RNN) work in new version of keras? #1224\n\nWhat would be some suggestions for a fix?\n\nReminder of the position of the problem:\n\noutput, new_states = step_function(input, states) if masking: # if all-zero input timestep, return # all-zero output and unchanged states switch = T.any(input) output = T.switch(switch, output, 0. * output) return_states = [] for state, new_state in zip(states, new_states): return_states.append(T.switch(switch, new_state, state)) return [output] + return_states\n\noutput, new_states = step_function(input, states) if masking: # for now we raise an exception because tf.reduce_any will not work raise Exception(""Masking is Theano-only for the time being."") # if all-zero input timestep, return # all-zero output and unchanged states switch = tf.reduce_any(input) output = tf.control_flow_ops.cond(switch, lambda: output, lambda: 0. * output) return_states = [] for state, new_state in zip(states, new_states): return_states.append(tf.control_flow_ops.cond(switch, lambda: new_state, lambda: state)) states = return_states\n\n@fchollet I\'m just starting to look at this issue (we\'ve been using a somewhat older version of Keras recently). Can you speak to why you have returned to the ""masking value"" approach from the approach we had converged on before where mask was sent separately through the network? There are a host of issues with masking values that we discussed before: I don\'t see the appeal.\n\nAt any rate: an approach I would take to solving the problem would be to pass the mask into the rnn call, instead of using it to transform the input. Then the Theano backend can pass the mask into scan and use it in there like I had originally implemented before multiple backends.\n\nTo articulate the problem with the masked value approach: with ReLU activations, zero outputs are common, so there is no reason to think that it will be that uncommon to get an all-zero input to an RNN layer, especially with many stacked RNNs.\n\nThis could be further compounded by model pruning techniques which involve setting a very high percentage of weights to 0.\n\nI can redo this backend to match how I designed masking before (with mask passed along with input), but would like to understand if there was a reason to move away from that approach before I put in the effort.\n\nEderSantana commented\n\nFor the interested Keras users: I tested Sequential right before the backend and it still works with the layers defined with the new code. So, while we look for a better solution, if anybody has an urgency of using the past mask and sample_weight behavior just copy this somewhere: https://github.com/fchollet/keras/blob/9c1afbb66782fe67c796fae85cd68d8f619b8aa6/keras/models.py\n\nNOTE: This is a temporary solution, and is not guaranteed to work. It would be better to just have a separate env with Keras on that commit. You can do that with git checkout 9c1afbb66782fe67c796fa. But I hope that helps.\n\nwould like to understand if there was a reason to move away from that approach\n\nThe reason was the switch away from scan to an API that had a single input and a single output (which simplifies lots of things). The thread on the switch to the modular backend contains a discussion about the design choices made when developing the current RNN API. The reasons you mention are among those why masking is optional in the current setup, and will only be applied if a mask object is passed to the RNN layer.\n\nIf you have a better solution that involves explicit masking, then that\'s great.\n\nOK great. I\'ll take a look next week — NIPS is taking up most of my time right now.\n\nFor reference to people looking at this issue, most of the discussion about the new design was in #1014.\n\nMy initial thoughts are that we should slightly extend the new rnn function to take an arbitrary mask parameter.\n\ndef rnn(step_function, inputs, initial_states, go_backwards=False, input_mask=None):\n\nIt would then also need to return the mask as part of its returned tuple.\n\nThen it\'s trivial to reintroduce masking via the previous approach. It does mean that when you write your step_function you need to be mask aware, but that is not a major issue as that was already the case.\n\nThoughts? I\'ll start building that now on my branch and we can see how it looks.\n\n@wxs what are your thoughts on #1258?\n\nI think that #1258 fixes the base bug where masking ""bleeds"" across timesteps, but not the more subtle issue around value masking being dangerous.\n\nShould fix the bug for most people, though the tests don\'t currently work because of a dependency issue. Looks right to me.\n\nIt assumes input is 3d, though the docstring implies that input can be 4d or more (which I suppose could happen with, e.g. time distributed image inputs), though I think that was already an issue with the inputs = inputs.dimshuffle((1, 0, 2) line.\n\nIn other words, pending tests I think it\'s definitely worth merging since RNNs as it stands are broken, and I will in the meantime try to build a more robust system via mask relaying. I\'m intrigued by some of @dbonadiman ideas in #1014 around ""apply"" and ""broadcast"" being operations that layers can do to masks, but need to think about it some more.\n\njeffzhengye commented\n\n@wxs then can you help with the test in #1258 ? Because I am not sure how to fit it into test_recurrent.py. All the test passed locally on my computer.\n\nDo you also have time to have a look at the bidirectional rnn BUG I mentioned in #1258 ?\n\nCurrent masking approach propagates internal biases in recurrent layers #1300\n\nMasking passed to the backend RNN calls. #1310\n\nEderSantana mentioned this issue\n\nUsing Masking Layer for Sequence to Sequence Learning #957\n\nstale bot added the stale label\n\nstale bot closed this as completed\n\nvisionscaper commented\n\n@fchollet back in Dec. 2015 you accepted the pull request by @jeffzhengye that resolves this issue for Theano, back then there was no fix for Tensorflow.\n\nMy question is, has this been resolved for Tensorflow by now? A lot of time has past, so chances are good that this has been resolved, but I just wanted to have this confirmed.\n\nThe issue was that masking would only be applied if all the samples in a batch had the masking value at a certain time step, instead of allowing different masking per sample, per timestep.\n\nThanks for your time and awesome work on Keras!\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-07-02T09:41:45', 'title': 'Does masking only work for homogeneous batches? · Issue #1206 · keras-team/keras', 'url': 'https://github.com/keras-team/keras/issues/1206'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nHow does Masking work? #3086\n\npoyuwu opened this issue\n\nJun 27, 2016 · 32 comments\n\nHow does Masking work? #3086\n\npoyuwu opened this issue\n\nJun 27, 2016 · 32 comments\n\nI\'m wondering how Masking Layer works. I try to write simple model to test Masking on Activation Layer\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input a = np.array([[3.,1.,2.,2.,0.,0.]]) inputs = Input(shape=(6,)) mask = Masking(mask_value=0.0)(inputs) softmax = Activation(\'softmax\')(mask) model = Model(input=inputs,output=softmax) model.predict(a)\n\nand the result of prediction is\n\narray([[ 0.50744212, 0.06867483, 0.18667753, 0.18667753, 0.02526405, 0.02526405]])\n\nIs this the correct behavior? My keras version is 1.0.5\n\nThe text was updated successfully, but these errors were encountered:\n\nI\'m also interested with this question. It seems to me you expected to get something like the following: [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194]] ? But from the other side, - according to the explanations in core.py : class Masking(Layer) - masking doesn\'t work with 1D input data. So, if you try this, for example:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking,Input,TimeDistributed,Dense a = np.array([[[3,1,2,2,0,0],[0,0,0,0,0,0],[2,1,1,2,0,0]]]) input = Input(shape=(3,6)) mask = Masking(mask_value=0)(input) out = TimeDistributed(Dense(1,activation=\'linear\'))(mask) model = Model(input=input,output=out) q = model.predict(a) print (q[0])\n\n...you will get [[-0.20101213],[ 0. ],[-0.51546627]] as expected. But I think that, most likely, there\'s something wrong in my understanding.\n\n@ipoletaev yes, sure. However keras can also return [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194, 0.0, 0.0 ]] by padding it to keep shape. Here is another example about Masknig on bi-LSTM layer but sum two layer\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, LSTM, merge a = np.array([[[.3,.1,.2,.2,.1,.1],[.2,.3,.3,.3,.3,.1],[0,0,0,0,0,0]]]) inputs = Input(shape=(3,6)) mask = Masking(mask_value=0.0)(inputs) fw = LSTM(1,return_sequences=True)(mask) bw = LSTM(1,return_sequences=True,go_backwards=True)(mask) merged = merge([fw,bw],mode=\'sum\') model = Model(input=inputs,output=fw) model2 = Model(input=inputs,output=bw) model3 = Model(input=inputs,output=merged)\n\nthe fw\'s output is array([[[-0.07041532], [-0.12203699], [-0.12203699]]]) the bw\'s output is array([[[ 0. ], [-0.03112165], [ 0.02271803]]]) the merge\'s output is array([[[-0.07041532], [-0.15315863], [-0.09931896]]]) but I think it should be (Here it also can padding 0 to keep shape.) array([[[-0.10153697], [-0.09931896]]]) which -0.10153697 = (-0.07041532) + (-0.03112165) and -0.09931896 = -0.12203699 + 0.02271803 Is is anything wrong on Keras?\n\nHowever keras can also return [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194, 0.0, 0.0 ]] by padding it to keep shape.\n\nHmm... I don\'t know how to make such out only through the Keras.\n\nAbout your example: I think it\'s similar to the aforementioned example, so you should get array([[[-0.07041532], [-0.15315863], [0.02271803]]]). And it\'s really strange that bw works right but fw doesn\'t, because of its third output is not equal to zero, but it must...\n\nlomizandtyd commented\n\nHi guys, I got this question too.. Especially for LSTM (BRNN).\n\nMasking Layer gives a masked vector, only work for the inputs, not for inner states. So in @poyuwu \'s example, the fw\'s output still has value in step 3.\n\nThis might be correct because inputs are surely masked. While, I want to find a way to skip the calculation step when coming masked value, like some special tags.\n\nHowever, I think using Masking layer in bidirectional RNN for sequences with different lengths may be totally wrong.\n\nSo in @poyuwu \'s example, the fw\'s output still has value in step 3.\n\nYes, it\'s logically, but in any case we want to get zero at the third place, isn\'t it?\n\nWhile, I want to find a way to skip the calculation step when coming masked value.\n\nI think it doesn\'t matter because of, as I understood, you should specify output sample_weights in fit() in order to skip necessary timesteps with all zeros in feature vector (I have already asked about this #3023 ). But if this is so, then it is not clear why do we need masking if we can specify it in fit(): what are the steps in the examples is using for training, and what - no. I mean it is not important to process these ""empty"" vectors by network, it is important to train network without backpropagation with errors calculated on such vectors.\n\nMaybe there is some way to use a batch_size=1 and do not bother with padding?\n\nlomizandtyd commented\n\n@ipoletaev Wow, thanks a lot for this!\n\nYes, we want to get zero at the masked position. The problem is we also want to keep the inner states across the masked step.\n\nMaybe we can deliver another sample_weights in the predict() function?. If do so, BRNN is still wrong...\n\n...keep the inner states across the masked step.\n\nI think it\'s not necessary, because the network shouldn\'t remember what responses it need to get at empty vectors...\n\nMaybe we can deliver another sample_weights in the predict() function?.\n\nI don\'t understand for what task you want to use it? After all you always know in advance what data you process, and you respectively know - which output of the network corresponds to the empty vectors, so you can just skip such positions in output, I guess.\n\nIf do so, BRNN is still wrong...\n\nAs far as I understood Keras has been ""fighting"" with RNN masking task about year :)\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking,Input,TimeDistributed,Dense a = np.array([[[3,1,2,2,0,0],[0,0,0,0,0,0],[2,1,1,2,0,0]]]) input = Input(shape=(3,6)) mask = Masking(mask_value=0)(input) out = TimeDistributed(Dense(1,activation=\'linear\'))(mask) model = Model(input=input,output=out) q = model.predict(a) print (q[0])\n\n@ipoletaev I think it\'s just Dense Layer that have zero inputs, so that its output is 0. If you change activation function to softmax, then you will get wrong answer. Besides, batch_size set None on time steps will raise other error in some case (especially on merge layer).\n\nIn lasagne, it seems to use Masking matrix to deal with padding. (I do not test its accuracy)\n\n@poyuwu : yes, I had checked it - and you are right. It means,as I understood, that and simple Dense doesn\'t keep masked values in the way we want...\n\nI write again what does not converge with the expectations:\n\nForward RNN doesn\'t keep mask values, backward does it. It\'s strange.\n\nIs this task solving with batch_size = 1?\n\nHow to specify correctly what timesteps the network should to skip.\n\nAnd it\'s not clear in which moment BiLSTM does reset_state - only in the end of timesteps in current sample, or when the network meets with empty vector?\n\n@ipoletaev I don\'t think\n\nForward RNN doesn\'t keep mask values, backward does it. It\'s strange.\n\nthis statement is true. That\'s because padding argument is \'post\', not \'pre\'. Hence, the reason is the same as Dense layer I said.\n\nHow to specify correctly what timesteps the network should to skip.\n\nAs I said, in lasagne, we provide a mask numpy.array (the same shape as input) to deal with it. If go_backwards=True, it needs to keep padding argument the same.\n\nBesides, Embedding layer mask_zeros seems to be the same.\n\n@poyuwu so you want to say that now, there\'s no way to solve this issue with Keras? I mean is it necessary to use masking if we use sample weights?\n\ncbdbdd mentioned this issue\n\nLSTM CudaNdarrayType(float32, col)\' and \'CudaNdarrayType(float32, matrix) error #3641\n\nzumpchke mentioned this issue\n\nRecurrent Models with sequences of mixed length #40\n\nSame here. It seems masking mechanism in Keras is not fully supported.\n\nI don\'t think Masking masks input values (neither during forward or back-propagation). It just skips a time-step where all features are equal to the mask value (i.e. when you pad a sequence). You can confirm this by:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, TimeDistributed, Dense if __name__ == ""__main__"": a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[2,1,1,2,0.1,0.1]]]) print \'Input array:\' print a print \'\' input = Input(shape=(3,6)) mask = Masking(mask_value=0.1)(input) out = TimeDistributed(Dense(1, activation=\'linear\'))(mask) model = Model(input=input, output=out) model.set_weights([np.array([[ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1.]], dtype=np.float32), np.array([ 0.], dtype=np.float32)]) print \'Weights\' print model.get_weights() q = model.predict(a) print q\n\nInput array: [[[ 3. 1. 2. 2. 0.1 0.1] [ 0. 0. 0. 0. 0. 0. ] [ 2. 1. 1. 2. 0.1 0.1]]] Weights [array([[ 1.], [ 1.], [ 1.], [ 1.], [ 1.], [ 1.]], dtype=float32), array([ 0.], dtype=float32)] [[[ 8.20000076] [ 0. ] [ 6.19999981]]]\n\nIf it masked the inputs of value 0.1, you would expect result to be\n\n[[[ 8. ] [ 0. ] [ 6. ]]]\n\nActually Masking works exactly as expected. The problem is that you are working with the wrong dimension order: in input = (3,6) the 3 is the time dimension and the Masking layer masks only along that dimension, making the net ignore a time sample if that sample is composed of all elements equal to the masked value.\n\nimport keras from keras.utils.visualize_util import plot from keras.layers import * from keras.models import Model net_input = Input(shape = ( 3, 10)) mask = Masking(mask_value = 0.5)(net_input) conv = TimeDistributed(Dense(1, activation = \'linear\', init=\'one\'))(mask) out = LSTM(1, init=\'one\', inner_init=\'one\',activation=\'tanh\', inner_activation=\'tanh\',)(conv) model = Model(net_input, out) print(\'W: \' + str(model.get_weights())) net_in = np.ones((1,3, 10)) val = 0.5 net_in[0, 2, :] = val out = model.predict(net_in) print(\'Input: \' + str(net_in)) print(\'Output: \' + str(out))\n\nIn this case the answers are:\n\nmask = 0.5, val = 0.0 : 0.73566443 mask = 0.0, val = 0.0 : 0.96402758 mask = 0.0, val = 0.5 : 0.99504161 mask = 0.5, val = 0.5 : 0.96402758\n\nso from here you can see that when we mask val we get the same result, while when we mask something else, even if val = 0, we get a different result.\n\nMoreover, I just tested, if you have a Multi-input net (with multiple input branches) and you have a masking layer on each branch, it is enough that just one of the inputs at time step t is equal to the masked value that all the time step is skipped.\n\nI guess that if one wants to skip the time step only if all the inputs are equal to the masked value, the branches need to be merged, right?\n\nirrationalagent commented\n\nHi Fragore, I have a similar question to you about masking with multiple inputs. I have two input branches and all I want to do is mask 0 from both. Am I right in thinking that adding a mask to the end of each branch is equivalent to adding a single mask AFTER the inputs are merged? here\'s my example\n\ninput1 = Sequential() input1.add(TimeDistributed(Dense(50), input_shape=(MAX_SEQUENCE_LENGTH,48))) input2 = Sequential() input2.add(Embedding(nb_words+2,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False,input_length=MAX_SEQUENCE_LENGTH)) model = Sequential() model.add(keras.engine.topology.Merge([input1,input2],mode=\'concat\',concat_axis=-1)) model.add(keras.layers.core.Masking(mask_value=0.0)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(512,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(TimeDistributed(Dense(nb_words + 1))) model.add(Activation(\'softmax\'))\n\nor version with a mask after each branch prior to merging\n\ninput1 = Sequential() input1.add(TimeDistributed(Dense(50), input_shape=(MAX_SEQUENCE_LENGTH,48))) input1.add(keras.layers.core.Masking(mask_value=0.0)) input2 = Sequential() input2.add(Embedding(nb_words+2,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False,input_length=MAX_SEQUENCE_LENGTH,mask_zero=True)) model = Sequential() model.add(keras.engine.topology.Merge([input1,input2],mode=\'concat\',concat_axis=-1)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(512,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(TimeDistributed(Dense(nb_words + 1))) model.add(Activation(\'softmax\'))\n\nWait, you want to mask the output of the branches that are 0? In that case both of your approaches should give you the same result. But usually you mask inputs, this means to put the mask layer as input of the net. Ps it may also be more convenient to use the functional API :) PPS the last dense layer doesn\'t need TimeDistributed anymore cause the LSTM removes the time dimension.\n\nI\'ve been experimenting with and without masking for a little bit now and I have finally figured out what the Masking layer actually does. It doesn\'t actually ""skip"" the timepoint that has all masked values, it just forces all the values for that timepoint to be equal to 0... So effectively Masking(mask_value=0.) does nothing. That is why in the example provided by @GPaolo above the results for mask_value=0 and mask_value=0.5 are the same when val matches them.\n\nHere is some easy code to demonstrate what I mean.\n\n`input1 = Input(batch_shape=(1,1,10) mask1 = Masking(mask_value=2)(input1) dense_layer1 = Dense(1, activation=\'sigmoid\') dense_layer1.setattr(\'supports_masking\', True) output1 = dense_layer1(mask1)\n\nmodel = Model(input1, output1) model.compile(optimizer=\'adam\', loss=\'binary_crossentropy\') ` Data:\n\n`data = np.ones((10, 1, 10), dtype=\'float32\') #set half of the data equal to mask value for index in range(5,10): data[index,0,:] = 2\n\n#set first data point equal to mask value to show that this line is uneffected data[0,0,0] = 2`\n\n`get_mask_output = K.function([model.layers[0].input], [model.layers[1].output]) mask_output = get_mask_output([data])[0]\n\nprint(data) print(mask_output)\n\n[[[ 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]]\n\n[[[ 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]`\n\n`test_data = np.ones((5,1,10)) test_data[1,0,:] = 2 test_data[2,0,:] = 0 predictions = model.predict(test_data, batch_size=1)\n\nprint(test_data) print(predictions) `\n\ntest_data: `[[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n\npredictions: [[[ 0.09200736]]\n\nAs you can imagine, ""masking"" values by setting them to 0 and still calculating the results for those lines in layers causes some mistakes from backpropagation (treating unknown values as a real result) as well as added unneeded computation time. I\'m going to try to rework how masking is done in Keras a bit...\n\nEdit: I did a little bit of digging into the training.py code and I found that the ""masking"" information (even with mask_value = 0.) does get incorporated into the training of the weights. The masked lines effectively get ignored after the calculation is done (which is good!). The problem that I am encountering in my actual network is that although ""masked lines"" are ignored during weight training, they are still evaluated by the network going forward which effects the outputs of future layers based on false information. To be able to build a network that handles variably sized inputs (not all have max timepoints) I want to completely ignore the masked lines entirely... I\'m going to try to work that out\n\nBuilding on @slaterb1 and @GPaolo \'s snippets I tried digging around to see the benefits of masking but haven\'t found it yet. It feels like I\'m missing something.\n\nIt does not seem to propagate numerically sound values through time\n\nIt propagates np.nan, see gist\n\nFeels (TODO:test) quite numerically unstable to propagate possibly absurd values down the network? Like mask output 0 may not always be in place.\n\nIt has to test each input\n\nQuick testing (see gist) seem to show that there\'s no immediate performance gains\n\nDoes anyone have an idea about if/when it gives performance gains? I didn\'t have time to run for long/deep/wide and I\'m not comfortable about how Python/Keras/Tensorflow/Theano compiles\n\nIs mask an intricate way of doing what I think weights should to be doing? I.e multiplying with the loss and dividing by sum of weights in batch? It\'s literally what seems to be done here anyway: https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L453\n\nDoes it actually halt any execution (yet)?\n\n@ragulpr, it\'s my understanding that masking does more than just loss scaling. If a timestep has been masked, the previous output and state will be reused. See here and here.\n\n@ragulpr, I\'m not sure about performance gains but Theano is pretty smart about knowing what it needs to hang on to and what it doesn\'t (based on the API doc: http://deeplearning.net/software/theano/library/scan.html)\n\nMore specifically this line: ""Note that there is an optimization, that at compile time will detect that you are using just the last value of the result and ensure that scan does not store all the intermediate values that are used. So do not worry if A and k are large.""\n\nSo after compiling the model it might pass over the masked values (or at least not hold them in memory as long), but that is pure speculation based on similarities in the underlying code.\n\n@carlthome, I came across the mask snippet in the ""theano_backend.py"" as well and you are right that the masking has a direct effect on how the states are evaluated and passed on (T.switch). Maybe this is too general a question but how does this layer accept the mask? Just to give an example, if I have a model with multiple layers, defined as so:\n\nmodel = Model(input1, output1)\n\nI understand that Theano wraps this up as a mathematical equation to calculate:\n\noutput1 = input1 -> [ layers[0] -> layers[1] -> ... layers[N] ]\n\nbut if I have somewhere in the middle:\n\nprev_layer -> Masking_layer -> RNN_layer\n\nThe output from the Masking_layer gets put into the RNN_layer as input (""x""). Does the ""supports_masking"" attribute tell the RNN_layer to figure out the mask? I could not find anywhere in the code where the mask is evaluated or interpreted by the RNN_layer, except that I can pass in a ""mask"" variable via the call() method of the Recurrent(Layer) object.\n\nI tried calling RNN_layer(prev_layer, mask=Masking_layer) but it didn\'t do anything different. The last comment in the thread, #176 suggests that it has to be called with a mask but I\'m not sure how to do that... Any thoughts?\n\nI could not find anywhere in the code where the mask is evaluated or interpreted by the RNN_layer\n\nEach Keras layer declares if it supports masking. Each layer is also responsible for using the mask in a sensible way (which I believe is the primary source of confusion: that the masking functionality is implemented across a bunch of different classes). For RNN layers in particular, they rely on the fact that the underlying K.rnn operation has mask support so if you\'re looking for where precisely the logic is, you\'ll note that the RNN layers simply pass the mask argument into the backend, where the magic happens.\n\n@carlthome, I saw that in the code but was not able to get the mask to work in my RNN network. For clarity I was trying to rework stuff in RecurrentShop to setup an encoder decoder network that adjusts the next input based on a prediction made on the previous state from both the encoder and the decoder (a custom RNN that uses a .single_step_rnn() instead of the regular .rnn() ).\n\nBut based on your advice, I tried to just build a basic LSTM network to act as a NOT Gate (pointless but simple) and it does interpret the mask correctly, when it is passed a mask mid network! I\'m including the gist. It shows that masking works for both return_sequences=True and return_sequences=False. It also shows that if you train the network with data that does not have \'masked\' input, \'masked\' lines in the test data will still get masked appropriately. Hope that helps people understand the masking stuff better!\n\n@fferroni @GPaolo apparently, the TimeDistributed layer didn\'t support masking, since this feature has been added in Pull #6401?\n\nmehrdadscomputer commented\n\nHey Guys, there is a seq2seq example which it\'s input is a string (sequence) like \'5+9\' and output is another string \'14\'. The author used pre padding to have sequences with same lengths at input but he didn\'t use masking. I add a simple line to add masking to his model and there is about 8 percent improvement in accuracy. Is my case a correct use of masking?\n\nfrom random import seed from random import randint from numpy import array from math import ceil from math import log10 from math import sqrt from numpy import argmax from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.layers import TimeDistributed from keras.layers import RepeatVector def random_sum_pairs(n_examples, n_numbers, largest): X, y = list(), list() for i in range(n_examples): in_pattern = [randint(1,largest) for _ in range(n_numbers)] out_pattern = sum(in_pattern) X.append(in_pattern) y.append(out_pattern) return X, y def to_string(X, y, n_numbers, largest): max_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1 Xstr = list() for pattern in X: strp = \'+\'.join([str(n) for n in pattern]) strp = \'\'.join([\' \' for _ in range(max_length-len(strp))]) + strp Xstr.append(strp) max_length = ceil(log10(n_numbers * (largest+1))) ystr = list() for pattern in y: strp = str(pattern) strp = \'\'.join([\' \' for _ in range(max_length-len(strp))]) + strp ystr.append(strp) return Xstr, ystr def integer_encode(X, y, alphabet): char_to_int = dict((c, i) for i, c in enumerate(alphabet)) Xenc = list() for pattern in X: integer_encoded = [char_to_int[char] for char in pattern] Xenc.append(integer_encoded) yenc = list() for pattern in y: integer_encoded = [char_to_int[char] for char in pattern] yenc.append(integer_encoded) return Xenc, yenc def one_hot_encode(X, y, max_int): Xenc = list() for seq in X: pattern = list() for index in seq: vector = [0 for _ in range(max_int)] vector[index] = 1 pattern.append(vector) Xenc.append(pattern) yenc = list() for seq in y: pattern = list() for index in seq: vector = [0 for _ in range(max_int)] vector[index] = 1 pattern.append(vector) yenc.append(pattern) return Xenc, yenc def generate_data(n_samples, n_numbers, largest, alphabet): X, y = random_sum_pairs(n_samples, n_numbers, largest) X, y = to_string(X, y, n_numbers, largest) X, y = integer_encode(X, y, alphabet) X, y = one_hot_encode(X, y, len(alphabet)) X, y = array(X), array(y) return X, y def invert(seq, alphabet): int_to_char = dict((i, c) for i, c in enumerate(alphabet)) strings = list() for pattern in seq: string = int_to_char[argmax(pattern)] strings.append(string) return \'\'.join(strings) seed(1) n_samples = 1000 n_numbers = 2 largest = 10 alphabet = [\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\', \'+\', \' \'] n_chars = len(alphabet) n_in_seq_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1 n_out_seq_length = ceil(log10(n_numbers * (largest+1))) n_batch = 10 n_epoch = 10 model = Sequential() model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars))) model.add(RepeatVector(n_out_seq_length)) model.add(LSTM(50, return_sequences=True)) model.add(TimeDistributed(Dense(n_chars, activation=\'softmax\'))) model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\']) print(model.summary()) for i in range(n_epoch): X, y = generate_data(n_samples, n_numbers, largest, alphabet) print(i) model.fit(X, y, epochs=1, batch_size=n_batch) X, y = generate_data(n_samples, n_numbers, largest, alphabet) result = model.predict(X, batch_size=n_batch, verbose=0) expected = [invert(x, alphabet) for x in y] predicted = [invert(x, alphabet) for x in result] for i in range(20): print(\'Expected=%s, Predicted=%s\' % (expected[i], predicted[i]))\n\nand I just change this part:\n\nmodel = Sequential() model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars)))\n\nfrom keras.layers import Masking model = Sequential() model.add(Masking(mask_value = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], input_shape=(n_in_seq_length, n_chars))) model.add(LSTM(100))\n\nsources: http://machinelearningmastery.com/learn-add-numbers-seq2seq-recurrent-neural-networks/#comment-400854\n\nstale bot added the stale label\n\nThis issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n\nstale bot closed this as completed\n\nandersjohanandreassen mentioned this issue\n\nTimeDistributed(Dense) with Masking not masking bias #12495\n\nI don\'t think Masking masks input values (neither during forward or back-propagation). It just skips a time-step where all features are equal to the mask value (i.e. when you pad a sequence). You can confirm this by:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, TimeDistributed, Dense if __name__ == ""__main__"": a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[2,1,1,2,0.1,0.1]]]) print \'Input array:\' print a print \'\' input = Input(shape=(3,6)) mask = Masking(mask_value=0.1)(input) out = TimeDistributed(Dense(1, activation=\'linear\'))(mask) model = Model(input=input, output=out) model.set_weights([np.array([[ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1.]], dtype=np.float32), np.array([ 0.], dtype=np.float32)]) print \'Weights\' print model.get_weights() q = model.predict(a) print q\n\nInput array: [[[ 3. 1. 2. 2. 0.1 0.1] [ 0. 0. 0. 0. 0. 0. ] [ 2. 1. 1. 2. 0.1 0.1]]] Weights [array([[ 1.], [ 1.], [ 1.], [ 1.], [ 1.], [ 1.]], dtype=float32), array([ 0.], dtype=float32)] [[[ 8.20000076] [ 0. ] [ 6.19999981]]]\n\nIf it masked the inputs of value 0.1, you would expect result to be\n\n[[[ 8. ] [ 0. ] [ 6. ]]]\n\nMask layer will work only when all feature of a timestep equals to the mask value.In you case,the input a is a 3d matrix with the shape(1,3,6),1 means batch_size,3 means timesteps,and 10 means the feature of that timestep.Mask will work when the feature of a timestep all equal to 0.1.if you change a to: a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[0.1,0.1,0.1,0.1,0.1,0.1]]])\n\nyou will get the output like:\n\n[[[8.200001] [0. ] [0. ]]]\n\nhoangcuong2011 commented\n\nI struggled a lot with this recently, and here is some experience I learnt. I hope it would be useful for people.\n\nMasking is extremely powerful. I found it perhaps the only way to deal with several ""hard"" problems that are with sequence of missing inputs, missing outputs as follows.\n\nMasking is not that complicated if we understand how the loss is computed with masking. For instance let us assume we have a sequence with length 256. From this sequence we have a masking with only 4 elements that are with masking of 1 (others are with masking 0). I thought the loss is computed as the average between these 4 elements. Guess what - it is not! The average loss will be divided by 256 instead. For this reason sometimes the loss will be extremely small (0.0something) if we have only few 1 elements and long sequence. Does it matter? I guess not, as what we need is the gradient of loss, rather than the loss itself.\n\nWhen we use softmax as the last layer, the denominator would be the sum of exponential of all elements, regarding whether their masking is 1 or 0.\n\nI thought the output of masking inputs is zeros all the time in LSTM. But it is not the case. Let us assume we have a masking:\n\nWith this case, the three first elements with masking zero has output of 0. However, the three last zeros have output that is as the same as the output of the last element with masking 1.\n\nMeanwhile, Keras is very convenient in the sense that the loss it computes will be based on only elements with masking of 1. I found this is a big plus of using Keras, something a bit too good too be true as I guess implementing this is not that easy.\n\nHowever, the accuracy in Keras is not computed that way. It is thus not trivial in keras to write a custom metric (for fit). There is something very mysterious to me. I am pretty sure my code for writing custom metric is correct but somehow it does not give me accurate result. Because of this I think it is much much easier if we write such an accuracy function with a custom callback class.\n\nThat is it, I hope it is helpful!\n\nzhanjiezhu commented\n\nI struggled a lot with this recently, and here is some experience I learnt. I hope it would be useful for people.\n\nMasking is extremely powerful. I found it perhaps the only way to deal with several ""hard"" problems that are with sequence of missing inputs, missing outputs as follows.\n\nMasking is not that complicated if we understand how the loss is computed with masking. For instance let us assume we have a sequence with length 256. From this sequence we have a masking with only 4 elements that are with masking of 1 (others are with masking 0). I thought the loss is computed as the average between these 4 elements. Guess what - it is not! The average loss will be divided by 256 instead. For this reason sometimes the loss will be extremely small (0.0something) if we have only few 1 elements and long sequence. Does it matter? I guess not, as what we need is the gradient of loss, rather than the loss itself.\n\nWhen we use softmax as the last layer, the denominator would be the sum of exponential of all elements, regarding whether their masking is 1 or 0.\n\nI thought the output of masking inputs is zeros all the time in LSTM. But it is not the case. Let us assume we have a masking:\n\nWith this case, the three first elements with masking zero has output of 0. However, the three last zeros have output that is as the same as the output of the last element with masking 1.\n\nMeanwhile, Keras is very convenient in the sense that the loss it computes will be based on only elements with masking of 1. I found this is a big plus of using Keras, something a bit too good too be true as I guess implementing this is not that easy.\n\nHowever, the accuracy in Keras is not computed that way. It is thus not trivial in keras to write a custom metric (for fit). There is something very mysterious to me. I am pretty sure my code for writing custom metric is correct but somehow it does not give me accurate result. Because of this I think it is much much easier if we write such an accuracy function with a custom callback class.\n\nThat is it, I hope it is helpful!\n\nHi @hoangcuong2011 , thanks for your explanations. I\'ve validated your second point and indeed it\'s exactly what you said. I\'m currently trying to implement a LSTM-autoencoder model to encode sequence into sequence, in which it involves a LSTM layer with return_sequence = False and then RepeatVector layer to copy that back to the previous timestep dimension. However, the mask get lost right after the LSTM because return_sequence = False (if True it returns the input_mask), then I\'m wondering how I can get back the mask so that the loss will also ignore the padded timesteps? Thanks!\n\nhoangcuong2011 mentioned this issue\n\ntf.keras.layers.Softmax does not support masking? tensorflow/tensorflow#27010\n\nhoangcuong2011 commented\n\n@zhangwj618 I am not really sure what your question is about. I guess you would like to write a custom masking layer. If you explain the question in more detail, I think I can help. Thx!\n\nsushreebarsa mentioned this issue\n\nMasking layer does not work after training #14108\n\nhossain666 commented\n\n](./typescript-kurulumu.md) |\n\nhossain666 commented\n\n](./typescript-kurulumu.md) |\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_0', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nHow does Masking work? #3086\n\npoyuwu opened this issue\n\nJun 27, 2016 · 32 comments\n\nHow does Masking work? #3086\n\npoyuwu opened this issue\n\nJun 27, 2016 · 32 comments\n\nI\'m wondering how Masking Layer works. I try to write simple model to test Masking on Activation Layer\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input a = np.array([[3.,1.,2.,2.,0.,0.]]) inputs = Input(shape=(6,)) mask = Masking(mask_value=0.0)(inputs) softmax = Activation(\'softmax\')(mask) model = Model(input=inputs,output=softmax) model.predict(a)\n\nand the result of prediction is\n\narray([[ 0.50744212, 0.06867483, 0.18667753, 0.18667753, 0.02526405, 0.02526405]])\n\nIs this the correct behavior? My keras version is 1.0.5\n\nThe text was updated successfully, but these errors were encountered:\n\nI\'m also interested with this question. It seems to me you expected to get something like the following: [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194]] ? But from the other side, - according to the explanations in core.py : class Masking(Layer) - masking doesn\'t work with 1D input data. So, if you try this, for example:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking,Input,TimeDistributed,Dense a = np.array([[[3,1,2,2,0,0],[0,0,0,0,0,0],[2,1,1,2,0,0]]]) input = Input(shape=(3,6)) mask = Masking(mask_value=0)(input) out = TimeDistributed(Dense(1,activation=\'linear\'))(mask) model = Model(input=input,output=out) q = model.predict(a) print (q[0])\n\n...you will get [[-0.20101213],[ 0. ],[-0.51546627]] as expected. But I think that, most likely, there\'s something wrong in my understanding.\n\n@ipoletaev yes, sure. However keras can also return [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194, 0.0, 0.0 ]] by padding it to keep shape. Here is another example about Masknig on bi-LSTM layer but sum two layer\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, LSTM, merge a = np.array([[[.3,.1,.2,.2,.1,.1],[.2,.3,.3,.3,.3,.1],[0,0,0,0,0,0]]]) inputs = Input(shape=(3,6)) mask = Masking(mask_value=0.0)(inputs) fw = LSTM(1,return_sequences=True)(mask) bw = LSTM(1,return_sequences=True,go_backwards=True)(mask) merged = merge([fw,bw],mode=\'sum\') model = Model(input=inputs,output=fw) model2 = Model(input=inputs,output=bw) model3 = Model(input=inputs,output=merged)\n\nthe fw\'s output is array([[[-0.07041532], [-0.12203699], [-0.12203699]]]) the bw\'s output is array([[[ 0. ], [-0.03112165], [ 0.02271803]]]) the merge\'s output is array([[[-0.07041532], [-0.15315863], [-0.09931896]]]) but I think it should be (Here it also can padding 0 to keep shape.) array([[[-0.10153697], [-0.09931896]]]) which -0.10153697 = (-0.07041532) + (-0.03112165) and -0.09931896 = -0.12203699 + 0.02271803 Is is anything wrong on Keras?\n\nHowever keras can also return [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194, 0.0, 0.0 ]] by padding it to keep shape.\n\nHmm... I don\'t know how to make such out only through the Keras.\n\nAbout your example: I think it\'s similar to the aforementioned example, so you should get array([[[-0.07041532], [-0.15315863], [0.02271803]]]). And it\'s really strange that bw works right but fw doesn\'t, because of its third output is not equal to zero, but it must...\n\nlomizandtyd commented\n\nHi guys, I got this question too.. Especially for LSTM (BRNN).\n\nMasking Layer gives a masked vector, only work for the inputs, not for inner states. So in @poyuwu \'s example, the fw\'s output still has value in step 3.\n\nThis might be correct because inputs are surely masked. While, I want to find a way to skip the calculation step when coming masked value, like some special tags.\n\nHowever, I think using Masking layer in bidirectional RNN for sequences with different lengths may be totally wrong.\n\nSo in @poyuwu \'s example, the fw\'s output still has value in step 3.\n\nYes, it\'s logically, but in any case we want to get zero at the third place, isn\'t it?\n\nWhile, I want to find a way to skip the calculation step when coming masked value.\n\nI think it doesn\'t matter because of, as I understood, you should specify output sample_weights in fit() in order to skip necessary timesteps with all zeros in feature vector (I have already asked about this #3023 ). But if this is so, then it is not clear why do we need masking if we can specify it in fit(): what are the steps in the examples is using for training, and what - no. I mean it is not important to process these ""empty"" vectors by network, it is important to train network without backpropagation with errors calculated on such vectors.\n\nMaybe there is some way to use a batch_size=1 and do not bother with padding?\n\nlomizandtyd commented\n\n@ipoletaev Wow, thanks a lot for this!\n\nYes, we want to get zero at the masked position. The problem is we also want to keep the inner states across the masked step.\n\nMaybe we can deliver another sample_weights in the predict() function?. If do so, BRNN is still wrong...\n\n...keep the inner states across the masked step.\n\nI think it\'s not necessary, because the network shouldn\'t remember what responses it need to get at empty vectors...\n\nMaybe we can deliver another sample_weights in the predict() function?.\n\nI don\'t understand for what task you want to use it? After all you always know in advance what data you process, and you respectively know - which output of the network corresponds to the empty vectors, so you can just skip such positions in output, I guess.\n\nIf do so, BRNN is still wrong...\n\nAs far as I understood Keras has been ""fighting"" with RNN masking task about year :)\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking,Input,TimeDistributed,Dense a = np.array([[[3,1,2,2,0,0],[0,0,0,0,0,0],[2,1,1,2,0,0]]]) input = Input(shape=(3,6)) mask = Masking(mask_value=0)(input) out = TimeDistributed(Dense(1,activation=\'linear\'))(mask) model = Model(input=input,output=out) q = model.predict(a) print (q[0])\n\n@ipoletaev I think it\'s just Dense Layer that have zero inputs, so that its output is 0. If you change activation function to softmax, then you will get wrong answer. Besides, batch_size set None on time steps will raise other error in some case (especially on merge layer).\n\nIn lasagne, it seems to use Masking matrix to deal with padding. (I do not test its accuracy)\n\n@poyuwu : yes, I had checked it - and you are right. It means,as I understood, that and simple Dense doesn\'t keep masked values in the way we want...\n\nI write again what does not converge with the expectations:\n\nForward RNN doesn\'t keep mask values, backward does it. It\'s strange.\n\nIs this task solving with batch_size = 1?\n\nHow to specify correctly what timesteps the network should to skip.\n\nAnd it\'s not clear in which moment BiLSTM does reset_state - only in the end of timesteps in current sample, or when the network meets with empty vector?\n\n@ipoletaev I don\'t think\n\nForward RNN doesn\'t keep mask values, backward does it. It\'s strange.\n\nthis statement is true. That\'s because padding argument is \'post\', not \'pre\'. Hence, the reason is the same as Dense layer I said.\n\nHow to specify correctly what timesteps the network should to skip.\n\nAs I said, in lasagne, we provide a mask numpy.array (the same shape as input) to deal with it. If go_backwards=True, it needs to keep padding argument the same.\n\nBesides, Embedding layer mask_zeros seems to be the same.\n\n@poyuwu so you want to say that now, there\'s no way to solve this issue with Keras? I mean is it necessary to use masking if we use sample weights?\n\ncbdbdd mentioned this issue\n\nLSTM CudaNdarrayType(float32, col)\' and \'CudaNdarrayType(float32, matrix) error #3641\n\nzumpchke mentioned this issue\n\nRecurrent Models with sequences of mixed length #40\n\nSame here. It seems masking mechanism in Keras is not fully supported.\n\nI don\'t think Masking masks input values (neither during forward or back-propagation). It just skips a time-step where all features are equal to the mask value (i.e. when you pad a sequence). You can confirm this by:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, TimeDistributed, Dense if __name__ == ""__main__"": a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[2,1,1,2,0.1,0.1]]]) print \'Input array:\' print a print \'\' input = Input(shape=(3,6)) mask = Masking(mask_value=0.1)(input) out = TimeDistributed(Dense(1, activation=\'linear\'))(mask) model = Model(input=input, output=out) model.set_weights([np.array([[ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1.]], dtype=np.float32), np.array([ 0.], dtype=np.float32)]) print \'Weights\' print model.get_weights() q = model.predict(a) print q\n\nInput array: [[[ 3. 1. 2. 2. 0.1 0.1] [ 0. 0. 0. 0. 0. 0. ] [ 2. 1. 1. 2. 0.1 0.1]]] Weights [array([[ 1.], [ 1.], [ 1.], [ 1.], [ 1.], [ 1.]], dtype=float32), array([ 0.], dtype=float32)] [[[ 8.20000076] [ 0. ] [ 6.19999981]]]\n\nIf it masked the inputs of value 0.1, you would expect result to be\n\n[[[ 8. ] [ 0. ] [ 6. ]]]\n\nActually Masking works exactly as expected. The problem is that you are working with the wrong dimension order: in input = (3,6) the 3 is the time dimension and the Masking layer masks only along that dimension, making the net ignore a time sample if that sample is composed of all elements equal to the masked value.\n\nimport keras from keras.utils.visualize_util import plot from keras.layers import * from keras.models import Model net_input = Input(shape = ( 3, 10)) mask = Masking(mask_value = 0.5)(net_input) conv = TimeDistributed(Dense(1, activation = \'linear\', init=\'one\'))(mask) out = LSTM(1, init=\'one\', inner_init=\'one\',activation=\'tanh\', inner_activation=\'tanh\',)(conv) model = Model(net_input, out) print(\'W: \' + str(model.get_weights())) net_in = np.ones((1,3, 10)) val = 0.5 net_in[0, 2, :] = val out = model.predict(net_in) print(\'Input: \' + str(net_in)) print(\'Output: \' + str(out))\n\nIn this case the answers are:\n\nmask = 0.5, val = 0.0 : 0.73566443 mask = 0.0, val = 0.0 : 0.96402758 mask = 0.0, val = 0.5 : 0.99504161 mask = 0.5, val = 0.5 : 0.96402758\n\nso from here you can see that when we mask val we get the same result, while when we mask something else, even if val = 0, we get a different result.\n\nMoreover, I just tested, if you have a Multi-input net (with multiple input branches) and you have a masking layer on each branch, it is enough that just one of the inputs at time step t is equal to the masked value that all the time step is skipped.\n\nI guess that if one wants to skip the time step only if all the inputs are equal to the masked value, the branches need to be merged, right?\n\nirrationalagent commented\n\nHi Fragore, I have a similar question to you about masking with multiple inputs. I have two input branches and all I want to do is mask 0 from both. Am I right in thinking that adding a mask to the end of each branch is equivalent to adding a single mask AFTER the inputs are merged? here\'s my example\n\ninput1 = Sequential() input1.add(TimeDistributed(Dense(50), input_shape=(MAX_SEQUENCE_LENGTH,48))) input2 = Sequential() input2.add(Embedding(nb_words+2,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False,input_length=MAX_SEQUENCE_LENGTH)) model = Sequential() model.add(keras.engine.topology.Merge([input1,input2],mode=\'concat\',concat_axis=-1)) model.add(keras.layers.core.Masking(mask_value=0.0)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(512,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(TimeDistributed(Dense(nb_words + 1))) model.add(Activation(\'softmax\'))\n\nor version with a mask after each branch prior to merging\n\ninput1 = Sequential() input1.add(TimeDistributed(Dense(50), input_shape=(MAX_SEQUENCE_LENGTH,48))) input1.add(keras.layers.core.Masking(mask_value=0.0)) input2 = Sequential() input2.add(Embedding(nb_words+2,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False,input_length=MAX_SEQUENCE_LENGTH,mask_zero=True)) model = Sequential() model.add(keras.engine.topology.Merge([input1,input2],mode=\'concat\',concat_axis=-1)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(512,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(TimeDistributed(Dense(nb_words + 1))) model.add(Activation(\'softmax\'))\n\nWait, you want to mask the output of the branches that are 0? In that case both of your approaches should give you the same result. But usually you mask inputs, this means to put the mask layer as input of the net. Ps it may also be more convenient to use the functional API :) PPS the last dense layer doesn\'t need TimeDistributed anymore cause the LSTM removes the time dimension.\n\nI\'ve been experimenting with and without masking for a little bit now and I have finally figured out what the Masking layer actually does. It doesn\'t actually ""skip"" the timepoint that has all masked values, it just forces all the values for that timepoint to be equal to 0... So effectively Masking(mask_value=0.) does nothing. That is why in the example provided by @GPaolo above the results for mask_value=0 and mask_value=0.5 are the same when val matches them.\n\nHere is some easy code to demonstrate what I mean.\n\n`input1 = Input(batch_shape=(1,1,10) mask1 = Masking(mask_value=2)(input1) dense_layer1 = Dense(1, activation=\'sigmoid\') dense_layer1.setattr(\'supports_masking\', True) output1 = dense_layer1(mask1)\n\nmodel = Model(input1, output1) model.compile(optimizer=\'adam\', loss=\'binary_crossentropy\') ` Data:\n\n`data = np.ones((10, 1, 10), dtype=\'float32\') #set half of the data equal to mask value for index in range(5,10): data[index,0,:] = 2\n\n#set first data point equal to mask value to show that this line is uneffected data[0,0,0] = 2`\n\n`get_mask_output = K.function([model.layers[0].input], [model.layers[1].output]) mask_output = get_mask_output([data])[0]\n\nprint(data) print(mask_output)\n\n[[[ 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]]\n\n[[[ 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]`\n\n`test_data = np.ones((5,1,10)) test_data[1,0,:] = 2 test_data[2,0,:] = 0 predictions = model.predict(test_data, batch_size=1)\n\nprint(test_data) print(predictions) `\n\ntest_data: `[[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n\npredictions: [[[ 0.09200736]]\n\nAs you can imagine, ""masking"" values by setting them to 0 and still calculating the results for those lines in layers causes some mistakes from backpropagation (treating unknown values as a real result) as well as added unneeded computation time. I\'m going to try to rework how masking is done in Keras a bit...\n\nEdit: I did a little bit of digging into the training.py code and I found that the ""masking"" information (even with mask_value = 0.) does get incorporated into the training of the weights. The masked lines effectively get ignored after the calculation is done (which is good!). The problem that I am encountering in my actual network is that although ""masked lines"" are ignored during weight training, they are still evaluated by the network going forward which effects the outputs of future layers based on false information. To be able to build a network that handles variably sized inputs (not all have max timepoints) I want to completely ignore the masked lines entirely... I\'m going to try to work that out\n\nBuilding on @slaterb1 and @GPaolo \'s snippets I tried digging around to see the benefits of masking but haven\'t found it yet. It feels like I\'m missing something.\n\nIt does not seem to propagate numerically sound values through time\n\nIt propagates np.nan, see gist\n\nFeels (TODO:test) quite numerically unstable to propagate possibly absurd values down the network? Like mask output 0 may not always be in place.\n\nIt has to test each input\n\nQuick testing (see gist) seem to show that there\'s no immediate performance gains\n\nDoes anyone have an idea about if/when it gives performance gains? I didn\'t have time to run for long/deep/wide and I\'m not comfortable about how Python/Keras/Tensorflow/Theano compiles\n\nIs mask an intricate way of doing what I think weights should to be doing? I.e multiplying with the loss and dividing by sum of weights in batch? It\'s literally what seems to be done here anyway: https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L453\n\nDoes it actually halt any execution (yet)?\n\n@ragulpr, it\'s my understanding that masking does more than just loss scaling. If a timestep has been masked, the previous output and state will be reused. See here and here.\n\n@ragulpr, I\'m not sure about performance gains but Theano is pretty smart about knowing what it needs to hang on to and what it doesn\'t (based on the API doc: http://deeplearning.net/software/theano/library/scan.html)\n\nMore specifically this line: ""Note that there is an optimization, that at compile time will detect that you are using just the last value of the result and ensure that scan does not store all the intermediate values that are used. So do not worry if A and k are large.""\n\nSo after compiling the model it might pass over the masked values (or at least not hold them in memory as long), but that is pure speculation based on similarities in the underlying code.\n\n@carlthome, I came across the mask snippet in the ""theano_backend.py"" as well and you are right that the masking has a direct effect on how the states are evaluated and passed on (T.switch). Maybe this is too general a question but how does this layer accept the mask? Just to give an example, if I have a model with multiple layers, defined as so:\n\nmodel = Model(input1, output1)\n\nI understand that Theano wraps this up as a mathematical equation to calculate:\n\noutput1 = input1 -> [ layers[0] -> layers[1] -> ... layers[N] ]\n\nbut if I have somewhere in the middle:\n\nprev_layer -> Masking_layer -> RNN_layer\n\nThe output from the Masking_layer gets put into the RNN_layer as input (""x""). Does the ""supports_masking"" attribute tell the RNN_layer to figure out the mask? I could not find anywhere in the code where the mask is evaluated or interpreted by the RNN_layer, except that I can pass in a ""mask"" variable via the call() method of the Recurrent(Layer) object.\n\nI tried calling RNN_layer(prev_layer, mask=Masking_layer) but it didn\'t do anything different. The last comment in the thread, #176 suggests that it has to be called with a mask but I\'m not sure how to do that... Any thoughts?\n\nI could not find anywhere in the code where the mask is evaluated or interpreted by the RNN_layer\n\nEach Keras layer declares if it supports masking. Each layer is also responsible for using the mask in a sensible way (which I believe is the primary source of confusion: that the masking functionality is implemented across a bunch of different classes). For RNN layers in particular, they rely on the fact that the underlying K.rnn operation has mask support so if you\'re looking for where precisely the logic is, you\'ll note that the RNN layers simply pass the mask argument into the backend, where the magic happens.\n\n@carlthome, I saw that in the code but was not able to get the mask to work in my RNN network. For clarity I was trying to rework stuff in RecurrentShop to setup an encoder decoder network that adjusts the next input based on a prediction made on the previous state from both the encoder and the decoder (a custom RNN that uses a .single_step_rnn() instead of the regular .rnn() ).\n\nBut based on your advice, I tried to just build a basic LSTM network to act as a NOT Gate (pointless but simple) and it does interpret the mask correctly, when it is passed a mask mid network! I\'m including the gist. It shows that masking works for both return_sequences=True and return_sequences=False. It also shows that if you train the network with data that does not have \'masked\' input, \'masked\' lines in the test data will still get masked appropriately. Hope that helps people understand the masking stuff better!\n\n@fferroni @GPaolo apparently, the TimeDistributed layer didn\'t support masking, since this feature has been added in Pull #6401?\n\nmehrdadscomputer commented\n\nHey Guys, there is a seq2seq example which it\'s input is a string (sequence) like \'5+9\' and output is another string \'14\'. The author used pre padding to have sequences with same lengths at input but he didn\'t use masking. I add a simple line to add masking to his model and there is about 8 percent improvement in accuracy. Is my case a correct use of masking?\n\nfrom random import seed from random import randint from numpy import array from math import ceil from math import log10 from math import sqrt from numpy import argmax from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.layers import TimeDistributed from keras.layers import RepeatVector def random_sum_pairs(n_examples, n_numbers, largest): X, y = list(), list() for i in range(n_examples): in_pattern = [randint(1,largest) for _ in range(n_numbers)] out_pattern = sum(in_pattern) X.append(in_pattern) y.append(out_pattern) return X, y def to_string(X, y, n_numbers, largest): max_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1 Xstr = list() for pattern in X: strp = \'+\'.join([str(n) for n in pattern]) strp = \'\'.join([\' \' for _ in range(max_length-len(strp))]) + strp Xstr.append(strp) max_length = ceil(log10(n_numbers * (largest+1))) ystr = list() for pattern in y: strp = str(pattern) strp = \'\'.join([\' \' for _ in range(max_length-len(strp))]) + strp ystr.append(strp) return Xstr, ystr def integer_encode(X, y, alphabet): char_to_int = dict((c, i) for i, c in enumerate(alphabet)) Xenc = list() for pattern in X: integer_encoded = [char_to_int[char] for char in pattern] Xenc.append(integer_encoded) yenc = list() for pattern in y: integer_encoded = [char_to_int[char] for char in pattern] yenc.append(integer_encoded) return Xenc, yenc def one_hot_encode(X, y, max_int): Xenc = list() for seq in X: pattern = list() for index in seq: vector = [0 for _ in range(max_int)] vector[index] = 1 pattern.append(vector) Xenc.append(pattern) yenc = list() for seq in y: pattern = list() for index in seq: vector = [0 for _ in range(max_int)] vector[index] = 1 pattern.append(vector) yenc.append(pattern) return Xenc, yenc def generate_data(n_samples, n_numbers, largest, alphabet): X, y = random_sum_pairs(n_samples, n_numbers, largest) X, y = to_string(X, y, n_numbers, largest) X, y = integer_encode(X, y, alphabet) X, y = one_hot_encode(X, y, len(alphabet)) X, y = array(X), array(y) return X, y def invert(seq, alphabet): int_to_char = dict((i, c) for i, c in enumerate(alphabet)) strings = list() for pattern in seq: string = int_to_char[argmax(pattern)] strings.append(string) return \'\'.join(strings) seed(1) n_samples = 1000 n_numbers = 2 largest = 10 alphabet = [\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\', \'+\', \' \'] n_chars = len(alphabet) n_in_seq_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1 n_out_seq_length = ceil(log10(n_numbers * (largest+1))) n_batch = 10 n_epoch = 10 model = Sequential() model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars))) model.add(RepeatVector(n_out_seq_length)) model.add(LSTM(50, return_sequences=True)) model.add(TimeDistributed(Dense(n_chars, activation=\'softmax\'))) model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\']) print(model.summary()) for i in range(n_epoch): X, y = generate_data(n_samples, n_numbers, largest, alphabet) print(i) model.fit(X, y, epochs=1, batch_size=n_batch) X, y = generate_data(n_samples, n_numbers, largest, alphabet) result = model.predict(X, batch_size=n_batch, verbose=0) expected = [invert(x, alphabet) for x in y] predicted = [invert(x, alphabet) for x in result] for i in range(20): print(\'Expected=%s, Predicted=%s\' % (expected[i], predicted[i]))\n\nand I just change this part:\n\nmodel = Sequential() model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars)))\n\nfrom keras.layers import Masking model = Sequential() model.add(Masking(mask_value = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], input_shape=(n_in_seq_length, n_chars))) model.add(LSTM(100))\n\nsources: http://machinelearningmastery.com/learn-add-numbers-seq2seq-recurrent-neural-networks/#comment-400854\n\nstale bot added the stale label\n\nThis issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n\nstale bot closed this as completed\n\nandersjohanandreassen mentioned this issue\n\nTimeDistributed(Dense) with Masking not masking bias #12495\n\nI don\'t think Masking masks input values (neither during forward or back-propagation). It just skips a time-step where all features are equal to the mask value (i.e. when you pad a sequence). You can confirm this by:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, TimeDistributed, Dense if __name__ == ""__main__"": a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[2,1,1,2,0.1,0.1]]]) print \'Input array:\' print a print \'\' input = Input(shape=(3,6)) mask = Masking(mask_value=0.1)(input) out = TimeDistributed(Dense(1, activation=\'linear\'))(mask) model = Model(input=input, output=out) model.set_weights([np.array([[ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1.]], dtype=np.float32), np.array([ 0.], dtype=np.float32)]) print \'Weights\' print model.get_weights() q = model.predict(a) print q\n\nInput array: [[[ 3. 1. 2. 2. 0.1 0.1] [ 0. 0. 0. 0. 0. 0. ] [ 2. 1. 1. 2. 0.1 0.1]]] Weights [array([[ 1.], [ 1.], [ 1.], [ 1.], [ 1.], [ 1.]], dtype=float32), array([ 0.], dtype=float32)] [[[ 8.20000076] [ 0. ] [ 6.19999981]]]\n\nIf it masked the inputs of value 0.1, you would expect result to be\n\n[[[ 8. ] [ 0. ] [ 6. ]]]\n\nMask layer will work only when all feature of a timestep equals to the mask value.In you case,the input a is a 3d matrix with the shape(1,3,6),1 means batch_size,3 means timesteps,and 10 means the feature of that timestep.Mask will work when the feature of a timestep all equal to 0.1.if you change a to: a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[0.1,0.1,0.1,0.1,0.1,0.1]]])\n\nyou will get the output like:\n\n[[[8.200001] [0. ] [0. ]]]\n\nhoangcuong2011 commented\n\nI struggled a lot with this recently, and here is some experience I learnt. I hope it would be useful for people.\n\nMasking is extremely powerful. I found it perhaps the only way to deal with several ""hard"" problems that are with sequence of missing inputs, missing outputs as follows.\n\nMasking is not that complicated if we understand how the loss is computed with masking. For instance let us assume we have a sequence with length 256. From this sequence we have a masking with only 4 elements that are with masking of 1 (others are with masking 0). I thought the loss is computed as the average between these 4 elements. Guess what - it is not! The average loss will be divided by 256 instead. For this reason sometimes the loss will be extremely small (0.0something) if we have only few 1 elements and long sequence. Does it matter? I guess not, as what we need is the gradient of loss, rather than the loss itself.\n\nWhen we use softmax as the last layer, the denominator would be the sum of exponential of all elements, regarding whether their masking is 1 or 0.\n\nI thought the output of masking inputs is zeros all the time in LSTM. But it is not the case. Let us assume we have a masking:\n\nWith this case, the three first elements with masking zero has output of 0. However, the three last zeros have output that is as the same as the output of the last element with masking 1.\n\nMeanwhile, Keras is very convenient in the sense that the loss it computes will be based on only elements with masking of 1. I found this is a big plus of using Keras, something a bit too good too be true as I guess implementing this is not that easy.\n\nHowever, the accuracy in Keras is not computed that way. It is thus not trivial in keras to write a custom metric (for fit). There is something very mysterious to me. I am pretty sure my code for writing custom metric is correct but somehow it does not give me accurate result. Because of this I think it is much much easier if we write such an accuracy function with a custom callback class.\n\nThat is it, I hope it is helpful!\n\nzhanjiezhu commented\n\nI struggled a lot with this recently, and here is some experience I learnt. I hope it would be useful for people.\n\nMasking is extremely powerful. I found it perhaps the only way to deal with several ""hard"" problems that are with sequence of missing inputs, missing outputs as follows.\n\nMasking is not that complicated if we understand how the loss is computed with masking. For instance let us assume we have a sequence with length 256. From this sequence we have a masking with only 4 elements that are with masking of 1 (others are with masking 0). I thought the loss is computed as the average between these 4 elements. Guess what - it is not! The average loss will be divided by 256 instead. For this reason sometimes the loss will be extremely small (0.0something) if we have only few 1 elements and long sequence. Does it matter? I guess not, as what we need is the gradient of loss, rather than the loss itself.\n\nWhen we use softmax as the last layer, the denominator would be the sum of exponential of all elements, regarding whether their masking is 1 or 0.\n\nI thought the output of masking inputs is zeros all the time in LSTM. But it is not the case. Let us assume we have a masking:\n\nWith this case, the three first elements with masking zero has output of 0. However, the three last zeros have output that is as the same as the output of the last element with masking 1.\n\nMeanwhile, Keras is very convenient in the sense that the loss it computes will be based on only elements with masking of 1. I found this is a big plus of using Keras, something a bit too good too be true as I guess implementing this is not that easy.\n\nHowever, the accuracy in Keras is not computed that way. It is thus not trivial in keras to write a custom metric (for fit). There is something very mysterious to me. I am pretty sure my code for writing custom metric is correct but somehow it does not give me accurate result. Because of this I think it is much much easier if we write such an accuracy function with a custom callback class.\n\nThat is it, I hope it is helpful!\n\nHi @hoangcuong2011 , thanks for your explanations. I\'ve validated your second point and indeed it\'s exactly what you said. I\'m currently trying to implement a LSTM-autoencoder model to encode sequence into sequence, in which it involves a LSTM layer with return_sequence = False and then RepeatVector layer to copy that back to the previous timestep dimension. However, the mask get lost right after the LSTM because return_sequence = False (if True it returns the input_mask), then I\'m wondering how I can get back the mask so that the loss will also ignore the padded timesteps? Thanks!\n\nhoangcuong2011 mentioned this issue\n\ntf.keras.layers.Softmax does not support masking? tensorflow/tensorflow#27010\n\nhoangcuong2011 commented\n\n@zhangwj618 I am not really sure what your question is about. I guess you would like to write a custom masking layer. If you explain the question in more detail, I think I can help. Thx!\n\nsushreebarsa mentioned this issue\n\nMasking layer does not work after training #14108\n\nhossain666 commented\n\n](./typescript-kurulumu.md) |\n\nhossain666 commented\n\n](./typescript-kurulumu.md) |\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-27T14:08:10', 'title': 'How does Masking work? · Issue #3086 · keras-team/keras', 'url': 'https://github.com/keras-team/keras/issues/3086'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nLoss and metrics differ with masking or sample weights #34158\n\nseandaug opened this issue\n\nNov 11, 2019 · 11 comments\n\nLoss and metrics differ with masking or sample weights #34158\n\nseandaug opened this issue\n\nNov 11, 2019 · 11 comments\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nUpdate on Jan 8, 2021: I updated the title from ""Metrics incorrect for RNN with mask"" as I discovered more information that widens the scope of this issue. See comment on that date.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\n\nTensorFlow installed from (source or binary): binary (conda)\n\nTensorFlow version (use command below): 2.0.0\n\nPython version: 3.6.8\n\nBazel version (if compiling from source): n/a\n\nGCC/Compiler version (if compiling from source): n/a\n\nCUDA/cuDNN version: 10.0.130 / 7.6.0\n\nGPU model and memory: 1080 Ti\n\nDescribe the current behavior I am training an RNN (GRU) where my varying-length sequences are right-padded with 0s and a mask is applied. Many sequences are more than half 0s (padding). I compile the model with a loss of \'mean_squared_error\' and a metric of \'mean_squared_error\', but the output is different when the mask is in effect.\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss=\'mean_squared_error\', metrics=[\'mean_squared_error\'])\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.MeanSquaredError()])\n\nExample output (note the different values for loss vs. mean_squared_error for both training and validation):\n\nEpoch 1/50 210328/210328 [==============================] - 610s 3ms/sample - loss: 4.5338e-06 - mean_squared_error: 1.1923e-05 - val_loss: 2.5456e-06 - val_mean_squared_error: 6.7928e-06 Epoch 2/50 210328/210328 [==============================] - 525s 2ms/sample - loss: 2.1835e-06 - mean_squared_error: 5.7421e-06 - val_loss: 2.2920e-06 - val_mean_squared_error: 6.1160e-06 Epoch 3/50 210328/210328 [==============================] - 513s 2ms/sample - loss: 1.9939e-06 - mean_squared_error: 5.2437e-06 - val_loss: 2.2535e-06 - val_mean_squared_error: 6.0133e-06 ... Epoch 50/50 210328/210328 [==============================] - 527s 3ms/sample - loss: 1.5595e-06 - mean_squared_error: 4.1011e-06 - val_loss: 1.7867e-06 - val_mean_squared_error: 4.7677e-06\n\nWhen I disable the masking, I get the following output:\n\nEpoch 1/3 210328/210328 [==============================] - 516s 2ms/sample - loss: 7.1682e-06 - mean_squared_error: 7.1682e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06 Epoch 2/3 210328/210328 [==============================] - 434s 2ms/sample - loss: 5.9133e-06 - mean_squared_error: 5.9133e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06 Epoch 3/3 210328/210328 [==============================] - 442s 2ms/sample - loss: 5.9085e-06 - mean_squared_error: 5.9085e-06 - val_loss: 6.7073e-06 - val_mean_squared_error: 6.7073e-06\n\nWithout the mask, the values for loss and mean_squared_error agree. For the validation set, the values are not really improving and the value of 6.7e-06 seems to be what you get when you evaluate on the 0s that would otherwise be ignored by the masking. Comparing the values between the runs suggests that the mean_squared_error calculations are not using the mask when it is in effect, but the loss calculations do use the mask. (We\'d expect lower values when we correctly ignore irrelevant time steps.)\n\nDescribe the expected behavior The values for loss and mean_squared_error should agree and both use the masking.\n\nCode to reproduce the issue I don\'t have full code and data to share since my model and data are proprietary.\n\nOther info / logs I can\'t think of any relevant logs.\n\nThe text was updated successfully, but these errors were encountered:\n\ngadagashwini-zz self-assigned this\n\ngadagashwini-zz added TF 2.0\n\nIssues relating to TensorFlow 2.0 comp:keras\n\nKeras related issues labels\n\ngadagashwini-zz commented\n\n@seandaug, Please provide the complete code to reproduce the reported issue. Thanks!\n\ngadagashwini-zz added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nHere\'s a small example that illustrates the issue.\n\nimport tensorflow as tf from tensorflow import keras import numpy as np # %% Make some sine wave data num_time_series = 100 max_length = 200 num_points = 1 training_data = np.zeros((num_time_series, max_length, num_points)) np.random.seed(123) tf.random.set_seed(123) for i in range(num_time_series): # Make a sequence that doesn\'t fill the array so there\'s padding at the end length = np.random.randint(0.25 * max_length, .75 * max_length + 1) period = np.random.random() * 20 + 5 shift = np.random.random() training_data[i, 0:length, 0] = np.sin(2 * np.pi / period * np.linspace(0, length - 1, length) + shift) # %% Define the model def make_model(use_mask): input_seq = keras.layers.Input(shape=(None, num_points)) masked_input_seq = keras.layers.Masking(mask_value=0.0)(input_seq) if use_mask else input_seq gru = keras.layers.GRU(units=3, return_sequences=True)(masked_input_seq) output = keras.layers.Dense(units=1)(gru) model = keras.Model(input_seq, output) model.compile(optimizer=keras.optimizers.RMSprop(), loss=\'mean_squared_error\', metrics=[\'mean_squared_error\']) return model # %% Train model with mask. Note that reported \'loss\' and \'mean_squared_error\' differ make_model(True).fit(training_data[:, :-1, :], training_data[:, 1:, :], batch_size=10, epochs=3, verbose=1) # %% Train model without mask. Note that reported \'loss\' and \'mean_squared_error\' match make_model(False).fit(training_data[:, :-1, :], training_data[:, 1:, :], batch_size=10, epochs=3, verbose=1)\n\nExample output when mask is used (note that loss and mean_squared_error differ):\n\nEpoch 1/3 100/100 [==============================] - 10s 98ms/sample - loss: 0.2741 - mean_squared_error: 0.5393 Epoch 2/3 100/100 [==============================] - 2s 20ms/sample - loss: 0.2690 - mean_squared_error: 0.5292 Epoch 3/3 100/100 [==============================] - 2s 20ms/sample - loss: 0.2650 - mean_squared_error: 0.5214\n\nExample output when no mask is used (note that loss and mean_squared_error match):\n\nEpoch 1/3 100/100 [==============================] - 4s 37ms/sample - loss: 0.2475 - mean_squared_error: 0.2475 Epoch 2/3 100/100 [==============================] - 1s 13ms/sample - loss: 0.2305 - mean_squared_error: 0.2305 Epoch 3/3 100/100 [==============================] - 1s 13ms/sample - loss: 0.2174 - mean_squared_error: 0.2174\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ngadagashwini-zz commented\n\nI could replicate the issue with Tf 2.0. Please take a look at the gist here. Thanks!\n\ngadagashwini-zz added the type:bug\n\ngadagashwini-zz assigned gowthamkpr and unassigned gadagashwini-zz\n\ngowthamkpr assigned qlzh727 and unassigned gowthamkpr\n\ngowthamkpr added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nAdding @pavithrasv who is the owner of metric and loss. I think the loss will take into account of the masks and exclude the masked value, but I don\'t think metric will do that, which is why you see the value difference here. I will let @pavithrasv to confirm.\n\nqlzh727 assigned pavithrasv and unassigned qlzh727\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\njvishnuvardhan commented\n\n@seandaug I think this was resolved in the tf-nightly. I am not able to reproduce the error with tf-nightly. Please take a look at the gist here. Thanks!\n\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!\n\njvishnuvardhan closed this as completed\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\ngeetachavan1 added this to Done in TensorFlow 2.3.0\n\n@jvishnuvardhan The gist you posted using tf-nightly does not appear to work anymore. The issue seems to have reappeared.\n\nIt appears that the difference is in how the ""mean"" is computed over the values for a ""loss"" vs. a ""metric"". Consider this very simple example that just outputs the input after applying a mask:\n\nimport tensorflow as tf import numpy as np y_true = np.array([[[0.], [1.], [1.]]]) x = np.array([[[1.], [0.], [1.]]]) model = tf.keras.Sequential([tf.keras.layers.Masking(mask_value=0., input_shape=(3, 1))]) model.compile(loss=\'mae\', metrics=[\'mae\']) model.fit(x, y_true) # Would intuitively expect mask to give loss of 0.5, but get 0.3333\n\nThis generates the following output:\n\n1/1 [==============================] - 0s 432us/step - loss: 0.3333 - mae: 0.5000\n\nHere we have a sequence of length 3, the second value of which is masked out. Therefore, the MAE should be 0.5 because the total absolute error is 1.0 and it is divided among 2 entries (after applying the mask). However, as a ""loss"", the MAE is reported as 0.3333 instead of 0.5.\n\nWe get similar intuitively unexpected results when using sample_weight as well (no masking involved). Consider this simple example that just outputs the input and allows sample weights to be specified:\n\nimport tensorflow as tf import numpy as np y_true = np.array([[0.], [1.]]) x = np.array([[1.], [0.]]) weights = np.array([1., 0.]) model = tf.keras.Sequential([tf.keras.layers.InputLayer(input_shape=(1,))]) model.compile(loss=\'mae\', metrics=[\'mae\']) model.fit(x, y_true, sample_weight=weights) # Might expect to see 1.0, but get 0.5\n\nThis generates the following output:\n\n1/1 [==============================] - 0s 427us/step - loss: 0.5000 - mae: 1.0000\n\nHere we have two samples, each with a loss of 1.0, but due to the provided sample weights, one of them should be ignored. Thus, we would expect to see an MAE of 1.0, which is correctly reported using the ""metric"". However, the ""loss"" does not match intuition because it is dividing by the batch size, not the total sample weight.\n\nI suspect these observations are because of the use of reduction=losses_utils.ReductionV2.AUTO in tf.keras.losses.Loss that relies on losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE. Then the denominator of the MAE calculation is the batch size (including sequence length) that does not consider the mask nor the sample weights.\n\nIt appears that for the metric calculation, MAE is computed using Mean in tensorflow.python.keras.metrics that specifies reduction=metrics_utils.Reduction.WEIGHTED_MEAN. However, there\'s no analogous WEIGHTED_MEAN reduction option for loss functions.\n\nSo to me, the big question here is: Why do loss functions use SUM_OVER_BATCH_SIZE that ignores masking and sample weights?\n\nseandaug changed the title\n\nMetrics incorrect for RNN with mask\n\nLoss and metrics differ with masking or sample weights\n\npavithrasv removed their assignment\n\nI agree with @seandaug. Loss and metrics differ if I enable masking on an LSTM model. I am trying to find a way to solve this problem as well.\n\nHi. I\'ve encountered this problem as well. Is there any update on this issue?\n\nseandaug mentioned this issue\n\ntf.keras computes incorrect loss values with Masking #34491\n\n@recepcan @Taoup See #34491 (comment) for a workaround.\n\nchethanjjj mentioned this issue\n\nUnderstanding How Masking Affects BCE Loss Function #50710\n\nseandaug mentioned this issue\n\ntf.keras computes incorrect loss values with Masking keras-team/tf-keras#130\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_1', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nLoss and metrics differ with masking or sample weights #34158\n\nseandaug opened this issue\n\nNov 11, 2019 · 11 comments\n\nLoss and metrics differ with masking or sample weights #34158\n\nseandaug opened this issue\n\nNov 11, 2019 · 11 comments\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nUpdate on Jan 8, 2021: I updated the title from ""Metrics incorrect for RNN with mask"" as I discovered more information that widens the scope of this issue. See comment on that date.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\n\nTensorFlow installed from (source or binary): binary (conda)\n\nTensorFlow version (use command below): 2.0.0\n\nPython version: 3.6.8\n\nBazel version (if compiling from source): n/a\n\nGCC/Compiler version (if compiling from source): n/a\n\nCUDA/cuDNN version: 10.0.130 / 7.6.0\n\nGPU model and memory: 1080 Ti\n\nDescribe the current behavior I am training an RNN (GRU) where my varying-length sequences are right-padded with 0s and a mask is applied. Many sequences are more than half 0s (padding). I compile the model with a loss of \'mean_squared_error\' and a metric of \'mean_squared_error\', but the output is different when the mask is in effect.\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss=\'mean_squared_error\', metrics=[\'mean_squared_error\'])\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.MeanSquaredError()])\n\nExample output (note the different values for loss vs. mean_squared_error for both training and validation):\n\nEpoch 1/50 210328/210328 [==============================] - 610s 3ms/sample - loss: 4.5338e-06 - mean_squared_error: 1.1923e-05 - val_loss: 2.5456e-06 - val_mean_squared_error: 6.7928e-06 Epoch 2/50 210328/210328 [==============================] - 525s 2ms/sample - loss: 2.1835e-06 - mean_squared_error: 5.7421e-06 - val_loss: 2.2920e-06 - val_mean_squared_error: 6.1160e-06 Epoch 3/50 210328/210328 [==============================] - 513s 2ms/sample - loss: 1.9939e-06 - mean_squared_error: 5.2437e-06 - val_loss: 2.2535e-06 - val_mean_squared_error: 6.0133e-06 ... Epoch 50/50 210328/210328 [==============================] - 527s 3ms/sample - loss: 1.5595e-06 - mean_squared_error: 4.1011e-06 - val_loss: 1.7867e-06 - val_mean_squared_error: 4.7677e-06\n\nWhen I disable the masking, I get the following output:\n\nEpoch 1/3 210328/210328 [==============================] - 516s 2ms/sample - loss: 7.1682e-06 - mean_squared_error: 7.1682e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06 Epoch 2/3 210328/210328 [==============================] - 434s 2ms/sample - loss: 5.9133e-06 - mean_squared_error: 5.9133e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06 Epoch 3/3 210328/210328 [==============================] - 442s 2ms/sample - loss: 5.9085e-06 - mean_squared_error: 5.9085e-06 - val_loss: 6.7073e-06 - val_mean_squared_error: 6.7073e-06\n\nWithout the mask, the values for loss and mean_squared_error agree. For the validation set, the values are not really improving and the value of 6.7e-06 seems to be what you get when you evaluate on the 0s that would otherwise be ignored by the masking. Comparing the values between the runs suggests that the mean_squared_error calculations are not using the mask when it is in effect, but the loss calculations do use the mask. (We\'d expect lower values when we correctly ignore irrelevant time steps.)\n\nDescribe the expected behavior The values for loss and mean_squared_error should agree and both use the masking.\n\nCode to reproduce the issue I don\'t have full code and data to share since my model and data are proprietary.\n\nOther info / logs I can\'t think of any relevant logs.\n\nThe text was updated successfully, but these errors were encountered:\n\ngadagashwini-zz self-assigned this\n\ngadagashwini-zz added TF 2.0\n\nIssues relating to TensorFlow 2.0 comp:keras\n\nKeras related issues labels\n\ngadagashwini-zz commented\n\n@seandaug, Please provide the complete code to reproduce the reported issue. Thanks!\n\ngadagashwini-zz added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nHere\'s a small example that illustrates the issue.\n\nimport tensorflow as tf from tensorflow import keras import numpy as np # %% Make some sine wave data num_time_series = 100 max_length = 200 num_points = 1 training_data = np.zeros((num_time_series, max_length, num_points)) np.random.seed(123) tf.random.set_seed(123) for i in range(num_time_series): # Make a sequence that doesn\'t fill the array so there\'s padding at the end length = np.random.randint(0.25 * max_length, .75 * max_length + 1) period = np.random.random() * 20 + 5 shift = np.random.random() training_data[i, 0:length, 0] = np.sin(2 * np.pi / period * np.linspace(0, length - 1, length) + shift) # %% Define the model def make_model(use_mask): input_seq = keras.layers.Input(shape=(None, num_points)) masked_input_seq = keras.layers.Masking(mask_value=0.0)(input_seq) if use_mask else input_seq gru = keras.layers.GRU(units=3, return_sequences=True)(masked_input_seq) output = keras.layers.Dense(units=1)(gru) model = keras.Model(input_seq, output) model.compile(optimizer=keras.optimizers.RMSprop(), loss=\'mean_squared_error\', metrics=[\'mean_squared_error\']) return model # %% Train model with mask. Note that reported \'loss\' and \'mean_squared_error\' differ make_model(True).fit(training_data[:, :-1, :], training_data[:, 1:, :], batch_size=10, epochs=3, verbose=1) # %% Train model without mask. Note that reported \'loss\' and \'mean_squared_error\' match make_model(False).fit(training_data[:, :-1, :], training_data[:, 1:, :], batch_size=10, epochs=3, verbose=1)\n\nExample output when mask is used (note that loss and mean_squared_error differ):\n\nEpoch 1/3 100/100 [==============================] - 10s 98ms/sample - loss: 0.2741 - mean_squared_error: 0.5393 Epoch 2/3 100/100 [==============================] - 2s 20ms/sample - loss: 0.2690 - mean_squared_error: 0.5292 Epoch 3/3 100/100 [==============================] - 2s 20ms/sample - loss: 0.2650 - mean_squared_error: 0.5214\n\nExample output when no mask is used (note that loss and mean_squared_error match):\n\nEpoch 1/3 100/100 [==============================] - 4s 37ms/sample - loss: 0.2475 - mean_squared_error: 0.2475 Epoch 2/3 100/100 [==============================] - 1s 13ms/sample - loss: 0.2305 - mean_squared_error: 0.2305 Epoch 3/3 100/100 [==============================] - 1s 13ms/sample - loss: 0.2174 - mean_squared_error: 0.2174\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ngadagashwini-zz commented\n\nI could replicate the issue with Tf 2.0. Please take a look at the gist here. Thanks!\n\ngadagashwini-zz added the type:bug\n\ngadagashwini-zz assigned gowthamkpr and unassigned gadagashwini-zz\n\ngowthamkpr assigned qlzh727 and unassigned gowthamkpr\n\ngowthamkpr added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nAdding @pavithrasv who is the owner of metric and loss. I think the loss will take into account of the masks and exclude the masked value, but I don\'t think metric will do that, which is why you see the value difference here. I will let @pavithrasv to confirm.\n\nqlzh727 assigned pavithrasv and unassigned qlzh727\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\njvishnuvardhan commented\n\n@seandaug I think this was resolved in the tf-nightly. I am not able to reproduce the error with tf-nightly. Please take a look at the gist here. Thanks!\n\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!\n\njvishnuvardhan closed this as completed\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\ngeetachavan1 added this to Done in TensorFlow 2.3.0\n\n@jvishnuvardhan The gist you posted using tf-nightly does not appear to work anymore. The issue seems to have reappeared.\n\nIt appears that the difference is in how the ""mean"" is computed over the values for a ""loss"" vs. a ""metric"". Consider this very simple example that just outputs the input after applying a mask:\n\nimport tensorflow as tf import numpy as np y_true = np.array([[[0.], [1.], [1.]]]) x = np.array([[[1.], [0.], [1.]]]) model = tf.keras.Sequential([tf.keras.layers.Masking(mask_value=0., input_shape=(3, 1))]) model.compile(loss=\'mae\', metrics=[\'mae\']) model.fit(x, y_true) # Would intuitively expect mask to give loss of 0.5, but get 0.3333\n\nThis generates the following output:\n\n1/1 [==============================] - 0s 432us/step - loss: 0.3333 - mae: 0.5000\n\nHere we have a sequence of length 3, the second value of which is masked out. Therefore, the MAE should be 0.5 because the total absolute error is 1.0 and it is divided among 2 entries (after applying the mask). However, as a ""loss"", the MAE is reported as 0.3333 instead of 0.5.\n\nWe get similar intuitively unexpected results when using sample_weight as well (no masking involved). Consider this simple example that just outputs the input and allows sample weights to be specified:\n\nimport tensorflow as tf import numpy as np y_true = np.array([[0.], [1.]]) x = np.array([[1.], [0.]]) weights = np.array([1., 0.]) model = tf.keras.Sequential([tf.keras.layers.InputLayer(input_shape=(1,))]) model.compile(loss=\'mae\', metrics=[\'mae\']) model.fit(x, y_true, sample_weight=weights) # Might expect to see 1.0, but get 0.5\n\nThis generates the following output:\n\n1/1 [==============================] - 0s 427us/step - loss: 0.5000 - mae: 1.0000\n\nHere we have two samples, each with a loss of 1.0, but due to the provided sample weights, one of them should be ignored. Thus, we would expect to see an MAE of 1.0, which is correctly reported using the ""metric"". However, the ""loss"" does not match intuition because it is dividing by the batch size, not the total sample weight.\n\nI suspect these observations are because of the use of reduction=losses_utils.ReductionV2.AUTO in tf.keras.losses.Loss that relies on losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE. Then the denominator of the MAE calculation is the batch size (including sequence length) that does not consider the mask nor the sample weights.\n\nIt appears that for the metric calculation, MAE is computed using Mean in tensorflow.python.keras.metrics that specifies reduction=metrics_utils.Reduction.WEIGHTED_MEAN. However, there\'s no analogous WEIGHTED_MEAN reduction option for loss functions.\n\nSo to me, the big question here is: Why do loss functions use SUM_OVER_BATCH_SIZE that ignores masking and sample weights?\n\nseandaug changed the title\n\nMetrics incorrect for RNN with mask\n\nLoss and metrics differ with masking or sample weights\n\npavithrasv removed their assignment\n\nI agree with @seandaug. Loss and metrics differ if I enable masking on an LSTM model. I am trying to find a way to solve this problem as well.\n\nHi. I\'ve encountered this problem as well. Is there any update on this issue?\n\nseandaug mentioned this issue\n\ntf.keras computes incorrect loss values with Masking #34491\n\n@recepcan @Taoup See #34491 (comment) for a workaround.\n\nchethanjjj mentioned this issue\n\nUnderstanding How Masking Affects BCE Loss Function #50710\n\nseandaug mentioned this issue\n\ntf.keras computes incorrect loss values with Masking keras-team/tf-keras#130\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nYou can’t perform that action at this time.', 'timestamp': '2024-07-09T18:12:28', 'title': 'Loss and metrics differ with masking or sample weights · Issue #34158 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/34158'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nLoss and metrics differ with masking or sample weights #34158\n\nseandaug opened this issue\n\nNov 11, 2019 · 11 comments\n\nLoss and metrics differ with masking or sample weights #34158\n\nseandaug opened this issue\n\nNov 11, 2019 · 11 comments\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nUpdate on Jan 8, 2021: I updated the title from ""Metrics incorrect for RNN with mask"" as I discovered more information that widens the scope of this issue. See comment on that date.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\n\nTensorFlow installed from (source or binary): binary (conda)\n\nTensorFlow version (use command below): 2.0.0\n\nPython version: 3.6.8\n\nBazel version (if compiling from source): n/a\n\nGCC/Compiler version (if compiling from source): n/a\n\nCUDA/cuDNN version: 10.0.130 / 7.6.0\n\nGPU model and memory: 1080 Ti\n\nDescribe the current behavior I am training an RNN (GRU) where my varying-length sequences are right-padded with 0s and a mask is applied. Many sequences are more than half 0s (padding). I compile the model with a loss of \'mean_squared_error\' and a metric of \'mean_squared_error\', but the output is different when the mask is in effect.\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss=\'mean_squared_error\', metrics=[\'mean_squared_error\'])\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.MeanSquaredError()])\n\nExample output (note the different values for loss vs. mean_squared_error for both training and validation):\n\nEpoch 1/50 210328/210328 [==============================] - 610s 3ms/sample - loss: 4.5338e-06 - mean_squared_error: 1.1923e-05 - val_loss: 2.5456e-06 - val_mean_squared_error: 6.7928e-06 Epoch 2/50 210328/210328 [==============================] - 525s 2ms/sample - loss: 2.1835e-06 - mean_squared_error: 5.7421e-06 - val_loss: 2.2920e-06 - val_mean_squared_error: 6.1160e-06 Epoch 3/50 210328/210328 [==============================] - 513s 2ms/sample - loss: 1.9939e-06 - mean_squared_error: 5.2437e-06 - val_loss: 2.2535e-06 - val_mean_squared_error: 6.0133e-06 ... Epoch 50/50 210328/210328 [==============================] - 527s 3ms/sample - loss: 1.5595e-06 - mean_squared_error: 4.1011e-06 - val_loss: 1.7867e-06 - val_mean_squared_error: 4.7677e-06\n\nWhen I disable the masking, I get the following output:\n\nEpoch 1/3 210328/210328 [==============================] - 516s 2ms/sample - loss: 7.1682e-06 - mean_squared_error: 7.1682e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06 Epoch 2/3 210328/210328 [==============================] - 434s 2ms/sample - loss: 5.9133e-06 - mean_squared_error: 5.9133e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06 Epoch 3/3 210328/210328 [==============================] - 442s 2ms/sample - loss: 5.9085e-06 - mean_squared_error: 5.9085e-06 - val_loss: 6.7073e-06 - val_mean_squared_error: 6.7073e-06\n\nWithout the mask, the values for loss and mean_squared_error agree. For the validation set, the values are not really improving and the value of 6.7e-06 seems to be what you get when you evaluate on the 0s that would otherwise be ignored by the masking. Comparing the values between the runs suggests that the mean_squared_error calculations are not using the mask when it is in effect, but the loss calculations do use the mask. (We\'d expect lower values when we correctly ignore irrelevant time steps.)\n\nDescribe the expected behavior The values for loss and mean_squared_error should agree and both use the masking.\n\nCode to reproduce the issue I don\'t have full code and data to share since my model and data are proprietary.\n\nOther info / logs I can\'t think of any relevant logs.\n\nThe text was updated successfully, but these errors were encountered:\n\ngadagashwini-zz self-assigned this\n\ngadagashwini-zz added TF 2.0\n\nIssues relating to TensorFlow 2.0 comp:keras\n\nKeras related issues labels\n\ngadagashwini-zz commented\n\n@seandaug, Please provide the complete code to reproduce the reported issue. Thanks!\n\ngadagashwini-zz added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nHere\'s a small example that illustrates the issue.\n\nimport tensorflow as tf from tensorflow import keras import numpy as np # %% Make some sine wave data num_time_series = 100 max_length = 200 num_points = 1 training_data = np.zeros((num_time_series, max_length, num_points)) np.random.seed(123) tf.random.set_seed(123) for i in range(num_time_series): # Make a sequence that doesn\'t fill the array so there\'s padding at the end length = np.random.randint(0.25 * max_length, .75 * max_length + 1) period = np.random.random() * 20 + 5 shift = np.random.random() training_data[i, 0:length, 0] = np.sin(2 * np.pi / period * np.linspace(0, length - 1, length) + shift) # %% Define the model def make_model(use_mask): input_seq = keras.layers.Input(shape=(None, num_points)) masked_input_seq = keras.layers.Masking(mask_value=0.0)(input_seq) if use_mask else input_seq gru = keras.layers.GRU(units=3, return_sequences=True)(masked_input_seq) output = keras.layers.Dense(units=1)(gru) model = keras.Model(input_seq, output) model.compile(optimizer=keras.optimizers.RMSprop(), loss=\'mean_squared_error\', metrics=[\'mean_squared_error\']) return model # %% Train model with mask. Note that reported \'loss\' and \'mean_squared_error\' differ make_model(True).fit(training_data[:, :-1, :], training_data[:, 1:, :], batch_size=10, epochs=3, verbose=1) # %% Train model without mask. Note that reported \'loss\' and \'mean_squared_error\' match make_model(False).fit(training_data[:, :-1, :], training_data[:, 1:, :], batch_size=10, epochs=3, verbose=1)\n\nExample output when mask is used (note that loss and mean_squared_error differ):\n\nEpoch 1/3 100/100 [==============================] - 10s 98ms/sample - loss: 0.2741 - mean_squared_error: 0.5393 Epoch 2/3 100/100 [==============================] - 2s 20ms/sample - loss: 0.2690 - mean_squared_error: 0.5292 Epoch 3/3 100/100 [==============================] - 2s 20ms/sample - loss: 0.2650 - mean_squared_error: 0.5214\n\nExample output when no mask is used (note that loss and mean_squared_error match):\n\nEpoch 1/3 100/100 [==============================] - 4s 37ms/sample - loss: 0.2475 - mean_squared_error: 0.2475 Epoch 2/3 100/100 [==============================] - 1s 13ms/sample - loss: 0.2305 - mean_squared_error: 0.2305 Epoch 3/3 100/100 [==============================] - 1s 13ms/sample - loss: 0.2174 - mean_squared_error: 0.2174\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ngadagashwini-zz commented\n\nI could replicate the issue with Tf 2.0. Please take a look at the gist here. Thanks!\n\ngadagashwini-zz added the type:bug\n\ngadagashwini-zz assigned gowthamkpr and unassigned gadagashwini-zz\n\ngowthamkpr assigned qlzh727 and unassigned gowthamkpr\n\ngowthamkpr added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nAdding @pavithrasv who is the owner of metric and loss. I think the loss will take into account of the masks and exclude the masked value, but I don\'t think metric will do that, which is why you see the value difference here. I will let @pavithrasv to confirm.\n\nqlzh727 assigned pavithrasv and unassigned qlzh727\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\njvishnuvardhan commented\n\n@seandaug I think this was resolved in the tf-nightly. I am not able to reproduce the error with tf-nightly. Please take a look at the gist here. Thanks!\n\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!\n\njvishnuvardhan closed this as completed\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\ngeetachavan1 added this to Done in TensorFlow 2.3.0\n\n@jvishnuvardhan The gist you posted using tf-nightly does not appear to work anymore. The issue seems to have reappeared.\n\nIt appears that the difference is in how the ""mean"" is computed over the values for a ""loss"" vs. a ""metric"". Consider this very simple example that just outputs the input after applying a mask:\n\nimport tensorflow as tf import numpy as np y_true = np.array([[[0.], [1.], [1.]]]) x = np.array([[[1.], [0.], [1.]]]) model = tf.keras.Sequential([tf.keras.layers.Masking(mask_value=0., input_shape=(3, 1))]) model.compile(loss=\'mae\', metrics=[\'mae\']) model.fit(x, y_true) # Would intuitively expect mask to give loss of 0.5, but get 0.3333\n\nThis generates the following output:\n\n1/1 [==============================] - 0s 432us/step - loss: 0.3333 - mae: 0.5000\n\nHere we have a sequence of length 3, the second value of which is masked out. Therefore, the MAE should be 0.5 because the total absolute error is 1.0 and it is divided among 2 entries (after applying the mask). However, as a ""loss"", the MAE is reported as 0.3333 instead of 0.5.\n\nWe get similar intuitively unexpected results when using sample_weight as well (no masking involved). Consider this simple example that just outputs the input and allows sample weights to be specified:\n\nimport tensorflow as tf import numpy as np y_true = np.array([[0.], [1.]]) x = np.array([[1.], [0.]]) weights = np.array([1., 0.]) model = tf.keras.Sequential([tf.keras.layers.InputLayer(input_shape=(1,))]) model.compile(loss=\'mae\', metrics=[\'mae\']) model.fit(x, y_true, sample_weight=weights) # Might expect to see 1.0, but get 0.5\n\nThis generates the following output:\n\n1/1 [==============================] - 0s 427us/step - loss: 0.5000 - mae: 1.0000\n\nHere we have two samples, each with a loss of 1.0, but due to the provided sample weights, one of them should be ignored. Thus, we would expect to see an MAE of 1.0, which is correctly reported using the ""metric"". However, the ""loss"" does not match intuition because it is dividing by the batch size, not the total sample weight.\n\nI suspect these observations are because of the use of reduction=losses_utils.ReductionV2.AUTO in tf.keras.losses.Loss that relies on losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE. Then the denominator of the MAE calculation is the batch size (including sequence length) that does not consider the mask nor the sample weights.\n\nIt appears that for the metric calculation, MAE is computed using Mean in tensorflow.python.keras.metrics that specifies reduction=metrics_utils.Reduction.WEIGHTED_MEAN. However, there\'s no analogous WEIGHTED_MEAN reduction option for loss functions.\n\nSo to me, the big question here is: Why do loss functions use SUM_OVER_BATCH_SIZE that ignores masking and sample weights?\n\nseandaug changed the title\n\nMetrics incorrect for RNN with mask\n\nLoss and metrics differ with masking or sample weights\n\npavithrasv removed their assignment\n\nI agree with @seandaug. Loss and metrics differ if I enable masking on an LSTM model. I am trying to find a way to solve this problem as well.\n\nHi. I\'ve encountered this problem as well. Is there any update on this issue?\n\nseandaug mentioned this issue\n\ntf.keras computes incorrect loss values with Masking #34491\n\n@recepcan @Taoup See #34491 (comment) for a workaround.\n\nchethanjjj mentioned this issue\n\nUnderstanding How Masking Affects BCE Loss Function #50710\n\nseandaug mentioned this issue\n\ntf.keras computes incorrect loss values with Masking keras-team/tf-keras#130\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_3', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\ntensorflow / tensorflow Public\n\nYou must be signed in to change notification settings\n\nLoss and metrics differ with masking or sample weights #34158\n\nseandaug opened this issue\n\nNov 11, 2019 · 11 comments\n\nLoss and metrics differ with masking or sample weights #34158\n\nseandaug opened this issue\n\nNov 11, 2019 · 11 comments\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nUpdate on Jan 8, 2021: I updated the title from ""Metrics incorrect for RNN with mask"" as I discovered more information that widens the scope of this issue. See comment on that date.\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\n\nTensorFlow installed from (source or binary): binary (conda)\n\nTensorFlow version (use command below): 2.0.0\n\nPython version: 3.6.8\n\nBazel version (if compiling from source): n/a\n\nGCC/Compiler version (if compiling from source): n/a\n\nCUDA/cuDNN version: 10.0.130 / 7.6.0\n\nGPU model and memory: 1080 Ti\n\nDescribe the current behavior I am training an RNN (GRU) where my varying-length sequences are right-padded with 0s and a mask is applied. Many sequences are more than half 0s (padding). I compile the model with a loss of \'mean_squared_error\' and a metric of \'mean_squared_error\', but the output is different when the mask is in effect.\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss=\'mean_squared_error\', metrics=[\'mean_squared_error\'])\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(), loss=keras.losses.MeanSquaredError(), metrics=[keras.metrics.MeanSquaredError()])\n\nExample output (note the different values for loss vs. mean_squared_error for both training and validation):\n\nEpoch 1/50 210328/210328 [==============================] - 610s 3ms/sample - loss: 4.5338e-06 - mean_squared_error: 1.1923e-05 - val_loss: 2.5456e-06 - val_mean_squared_error: 6.7928e-06 Epoch 2/50 210328/210328 [==============================] - 525s 2ms/sample - loss: 2.1835e-06 - mean_squared_error: 5.7421e-06 - val_loss: 2.2920e-06 - val_mean_squared_error: 6.1160e-06 Epoch 3/50 210328/210328 [==============================] - 513s 2ms/sample - loss: 1.9939e-06 - mean_squared_error: 5.2437e-06 - val_loss: 2.2535e-06 - val_mean_squared_error: 6.0133e-06 ... Epoch 50/50 210328/210328 [==============================] - 527s 3ms/sample - loss: 1.5595e-06 - mean_squared_error: 4.1011e-06 - val_loss: 1.7867e-06 - val_mean_squared_error: 4.7677e-06\n\nWhen I disable the masking, I get the following output:\n\nEpoch 1/3 210328/210328 [==============================] - 516s 2ms/sample - loss: 7.1682e-06 - mean_squared_error: 7.1682e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06 Epoch 2/3 210328/210328 [==============================] - 434s 2ms/sample - loss: 5.9133e-06 - mean_squared_error: 5.9133e-06 - val_loss: 6.7091e-06 - val_mean_squared_error: 6.7091e-06 Epoch 3/3 210328/210328 [==============================] - 442s 2ms/sample - loss: 5.9085e-06 - mean_squared_error: 5.9085e-06 - val_loss: 6.7073e-06 - val_mean_squared_error: 6.7073e-06\n\nWithout the mask, the values for loss and mean_squared_error agree. For the validation set, the values are not really improving and the value of 6.7e-06 seems to be what you get when you evaluate on the 0s that would otherwise be ignored by the masking. Comparing the values between the runs suggests that the mean_squared_error calculations are not using the mask when it is in effect, but the loss calculations do use the mask. (We\'d expect lower values when we correctly ignore irrelevant time steps.)\n\nDescribe the expected behavior The values for loss and mean_squared_error should agree and both use the masking.\n\nCode to reproduce the issue I don\'t have full code and data to share since my model and data are proprietary.\n\nOther info / logs I can\'t think of any relevant logs.\n\nThe text was updated successfully, but these errors were encountered:\n\ngadagashwini-zz self-assigned this\n\ngadagashwini-zz added TF 2.0\n\nIssues relating to TensorFlow 2.0 comp:keras\n\nKeras related issues labels\n\ngadagashwini-zz commented\n\n@seandaug, Please provide the complete code to reproduce the reported issue. Thanks!\n\ngadagashwini-zz added the stat:awaiting response\n\nStatus - Awaiting response from author label\n\nHere\'s a small example that illustrates the issue.\n\nimport tensorflow as tf from tensorflow import keras import numpy as np # %% Make some sine wave data num_time_series = 100 max_length = 200 num_points = 1 training_data = np.zeros((num_time_series, max_length, num_points)) np.random.seed(123) tf.random.set_seed(123) for i in range(num_time_series): # Make a sequence that doesn\'t fill the array so there\'s padding at the end length = np.random.randint(0.25 * max_length, .75 * max_length + 1) period = np.random.random() * 20 + 5 shift = np.random.random() training_data[i, 0:length, 0] = np.sin(2 * np.pi / period * np.linspace(0, length - 1, length) + shift) # %% Define the model def make_model(use_mask): input_seq = keras.layers.Input(shape=(None, num_points)) masked_input_seq = keras.layers.Masking(mask_value=0.0)(input_seq) if use_mask else input_seq gru = keras.layers.GRU(units=3, return_sequences=True)(masked_input_seq) output = keras.layers.Dense(units=1)(gru) model = keras.Model(input_seq, output) model.compile(optimizer=keras.optimizers.RMSprop(), loss=\'mean_squared_error\', metrics=[\'mean_squared_error\']) return model # %% Train model with mask. Note that reported \'loss\' and \'mean_squared_error\' differ make_model(True).fit(training_data[:, :-1, :], training_data[:, 1:, :], batch_size=10, epochs=3, verbose=1) # %% Train model without mask. Note that reported \'loss\' and \'mean_squared_error\' match make_model(False).fit(training_data[:, :-1, :], training_data[:, 1:, :], batch_size=10, epochs=3, verbose=1)\n\nExample output when mask is used (note that loss and mean_squared_error differ):\n\nEpoch 1/3 100/100 [==============================] - 10s 98ms/sample - loss: 0.2741 - mean_squared_error: 0.5393 Epoch 2/3 100/100 [==============================] - 2s 20ms/sample - loss: 0.2690 - mean_squared_error: 0.5292 Epoch 3/3 100/100 [==============================] - 2s 20ms/sample - loss: 0.2650 - mean_squared_error: 0.5214\n\nExample output when no mask is used (note that loss and mean_squared_error match):\n\nEpoch 1/3 100/100 [==============================] - 4s 37ms/sample - loss: 0.2475 - mean_squared_error: 0.2475 Epoch 2/3 100/100 [==============================] - 1s 13ms/sample - loss: 0.2305 - mean_squared_error: 0.2305 Epoch 3/3 100/100 [==============================] - 1s 13ms/sample - loss: 0.2174 - mean_squared_error: 0.2174\n\ntensorflowbutler removed the stat:awaiting response\n\nStatus - Awaiting response from author label\n\ngadagashwini-zz commented\n\nI could replicate the issue with Tf 2.0. Please take a look at the gist here. Thanks!\n\ngadagashwini-zz added the type:bug\n\ngadagashwini-zz assigned gowthamkpr and unassigned gadagashwini-zz\n\ngowthamkpr assigned qlzh727 and unassigned gowthamkpr\n\ngowthamkpr added the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\nAdding @pavithrasv who is the owner of metric and loss. I think the loss will take into account of the masks and exclude the masked value, but I don\'t think metric will do that, which is why you see the value difference here. I will let @pavithrasv to confirm.\n\nqlzh727 assigned pavithrasv and unassigned qlzh727\n\ntensorflowbutler removed the stat:awaiting tensorflower\n\nStatus - Awaiting response from tensorflower label\n\njvishnuvardhan commented\n\n@seandaug I think this was resolved in the tf-nightly. I am not able to reproduce the error with tf-nightly. Please take a look at the gist here. Thanks!\n\nI am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!\n\njvishnuvardhan closed this as completed\n\ntensorflow-bot bot commented\n\nAre you satisfied with the resolution of your issue? Yes No\n\ngeetachavan1 added this to Done in TensorFlow 2.3.0\n\n@jvishnuvardhan The gist you posted using tf-nightly does not appear to work anymore. The issue seems to have reappeared.\n\nIt appears that the difference is in how the ""mean"" is computed over the values for a ""loss"" vs. a ""metric"". Consider this very simple example that just outputs the input after applying a mask:\n\nimport tensorflow as tf import numpy as np y_true = np.array([[[0.], [1.], [1.]]]) x = np.array([[[1.], [0.], [1.]]]) model = tf.keras.Sequential([tf.keras.layers.Masking(mask_value=0., input_shape=(3, 1))]) model.compile(loss=\'mae\', metrics=[\'mae\']) model.fit(x, y_true) # Would intuitively expect mask to give loss of 0.5, but get 0.3333\n\nThis generates the following output:\n\n1/1 [==============================] - 0s 432us/step - loss: 0.3333 - mae: 0.5000\n\nHere we have a sequence of length 3, the second value of which is masked out. Therefore, the MAE should be 0.5 because the total absolute error is 1.0 and it is divided among 2 entries (after applying the mask). However, as a ""loss"", the MAE is reported as 0.3333 instead of 0.5.\n\nWe get similar intuitively unexpected results when using sample_weight as well (no masking involved). Consider this simple example that just outputs the input and allows sample weights to be specified:\n\nimport tensorflow as tf import numpy as np y_true = np.array([[0.], [1.]]) x = np.array([[1.], [0.]]) weights = np.array([1., 0.]) model = tf.keras.Sequential([tf.keras.layers.InputLayer(input_shape=(1,))]) model.compile(loss=\'mae\', metrics=[\'mae\']) model.fit(x, y_true, sample_weight=weights) # Might expect to see 1.0, but get 0.5\n\nThis generates the following output:\n\n1/1 [==============================] - 0s 427us/step - loss: 0.5000 - mae: 1.0000\n\nHere we have two samples, each with a loss of 1.0, but due to the provided sample weights, one of them should be ignored. Thus, we would expect to see an MAE of 1.0, which is correctly reported using the ""metric"". However, the ""loss"" does not match intuition because it is dividing by the batch size, not the total sample weight.\n\nI suspect these observations are because of the use of reduction=losses_utils.ReductionV2.AUTO in tf.keras.losses.Loss that relies on losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE. Then the denominator of the MAE calculation is the batch size (including sequence length) that does not consider the mask nor the sample weights.\n\nIt appears that for the metric calculation, MAE is computed using Mean in tensorflow.python.keras.metrics that specifies reduction=metrics_utils.Reduction.WEIGHTED_MEAN. However, there\'s no analogous WEIGHTED_MEAN reduction option for loss functions.\n\nSo to me, the big question here is: Why do loss functions use SUM_OVER_BATCH_SIZE that ignores masking and sample weights?\n\nseandaug changed the title\n\nMetrics incorrect for RNN with mask\n\nLoss and metrics differ with masking or sample weights\n\npavithrasv removed their assignment\n\nI agree with @seandaug. Loss and metrics differ if I enable masking on an LSTM model. I am trying to find a way to solve this problem as well.\n\nHi. I\'ve encountered this problem as well. Is there any update on this issue?\n\nseandaug mentioned this issue\n\ntf.keras computes incorrect loss values with Masking #34491\n\n@recepcan @Taoup See #34491 (comment) for a workaround.\n\nchethanjjj mentioned this issue\n\nUnderstanding How Masking Affects BCE Loss Function #50710\n\nseandaug mentioned this issue\n\ntf.keras computes incorrect loss values with Masking keras-team/tf-keras#130\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nKeras related issues TF 2.0\n\nIssues relating to TensorFlow 2.0 type:bug\n\nYou can’t perform that action at this time.', 'timestamp': '2024-07-09T18:12:28', 'title': 'Loss and metrics differ with masking or sample weights · Issue #34158 · tensorflow/tensorflow', 'url': 'https://github.com/tensorflow/tensorflow/issues/34158'}), Document(page_content='Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nHow does Masking work? #3086\n\npoyuwu opened this issue\n\nJun 27, 2016 · 32 comments\n\nHow does Masking work? #3086\n\npoyuwu opened this issue\n\nJun 27, 2016 · 32 comments\n\nI\'m wondering how Masking Layer works. I try to write simple model to test Masking on Activation Layer\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input a = np.array([[3.,1.,2.,2.,0.,0.]]) inputs = Input(shape=(6,)) mask = Masking(mask_value=0.0)(inputs) softmax = Activation(\'softmax\')(mask) model = Model(input=inputs,output=softmax) model.predict(a)\n\nand the result of prediction is\n\narray([[ 0.50744212, 0.06867483, 0.18667753, 0.18667753, 0.02526405, 0.02526405]])\n\nIs this the correct behavior? My keras version is 1.0.5\n\nThe text was updated successfully, but these errors were encountered:\n\nI\'m also interested with this question. It seems to me you expected to get something like the following: [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194]] ? But from the other side, - according to the explanations in core.py : class Masking(Layer) - masking doesn\'t work with 1D input data. So, if you try this, for example:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking,Input,TimeDistributed,Dense a = np.array([[[3,1,2,2,0,0],[0,0,0,0,0,0],[2,1,1,2,0,0]]]) input = Input(shape=(3,6)) mask = Masking(mask_value=0)(input) out = TimeDistributed(Dense(1,activation=\'linear\'))(mask) model = Model(input=input,output=out) q = model.predict(a) print (q[0])\n\n...you will get [[-0.20101213],[ 0. ],[-0.51546627]] as expected. But I think that, most likely, there\'s something wrong in my understanding.\n\n@ipoletaev yes, sure. However keras can also return [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194, 0.0, 0.0 ]] by padding it to keep shape. Here is another example about Masknig on bi-LSTM layer but sum two layer\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, LSTM, merge a = np.array([[[.3,.1,.2,.2,.1,.1],[.2,.3,.3,.3,.3,.1],[0,0,0,0,0,0]]]) inputs = Input(shape=(3,6)) mask = Masking(mask_value=0.0)(inputs) fw = LSTM(1,return_sequences=True)(mask) bw = LSTM(1,return_sequences=True,go_backwards=True)(mask) merged = merge([fw,bw],mode=\'sum\') model = Model(input=inputs,output=fw) model2 = Model(input=inputs,output=bw) model3 = Model(input=inputs,output=merged)\n\nthe fw\'s output is array([[[-0.07041532], [-0.12203699], [-0.12203699]]]) the bw\'s output is array([[[ 0. ], [-0.03112165], [ 0.02271803]]]) the merge\'s output is array([[[-0.07041532], [-0.15315863], [-0.09931896]]]) but I think it should be (Here it also can padding 0 to keep shape.) array([[[-0.10153697], [-0.09931896]]]) which -0.10153697 = (-0.07041532) + (-0.03112165) and -0.09931896 = -0.12203699 + 0.02271803 Is is anything wrong on Keras?\n\nHowever keras can also return [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194, 0.0, 0.0 ]] by padding it to keep shape.\n\nHmm... I don\'t know how to make such out only through the Keras.\n\nAbout your example: I think it\'s similar to the aforementioned example, so you should get array([[[-0.07041532], [-0.15315863], [0.02271803]]]). And it\'s really strange that bw works right but fw doesn\'t, because of its third output is not equal to zero, but it must...\n\nlomizandtyd commented\n\nHi guys, I got this question too.. Especially for LSTM (BRNN).\n\nMasking Layer gives a masked vector, only work for the inputs, not for inner states. So in @poyuwu \'s example, the fw\'s output still has value in step 3.\n\nThis might be correct because inputs are surely masked. While, I want to find a way to skip the calculation step when coming masked value, like some special tags.\n\nHowever, I think using Masking layer in bidirectional RNN for sequences with different lengths may be totally wrong.\n\nSo in @poyuwu \'s example, the fw\'s output still has value in step 3.\n\nYes, it\'s logically, but in any case we want to get zero at the third place, isn\'t it?\n\nWhile, I want to find a way to skip the calculation step when coming masked value.\n\nI think it doesn\'t matter because of, as I understood, you should specify output sample_weights in fit() in order to skip necessary timesteps with all zeros in feature vector (I have already asked about this #3023 ). But if this is so, then it is not clear why do we need masking if we can specify it in fit(): what are the steps in the examples is using for training, and what - no. I mean it is not important to process these ""empty"" vectors by network, it is important to train network without backpropagation with errors calculated on such vectors.\n\nMaybe there is some way to use a batch_size=1 and do not bother with padding?\n\nlomizandtyd commented\n\n@ipoletaev Wow, thanks a lot for this!\n\nYes, we want to get zero at the masked position. The problem is we also want to keep the inner states across the masked step.\n\nMaybe we can deliver another sample_weights in the predict() function?. If do so, BRNN is still wrong...\n\n...keep the inner states across the masked step.\n\nI think it\'s not necessary, because the network shouldn\'t remember what responses it need to get at empty vectors...\n\nMaybe we can deliver another sample_weights in the predict() function?.\n\nI don\'t understand for what task you want to use it? After all you always know in advance what data you process, and you respectively know - which output of the network corresponds to the empty vectors, so you can just skip such positions in output, I guess.\n\nIf do so, BRNN is still wrong...\n\nAs far as I understood Keras has been ""fighting"" with RNN masking task about year :)\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking,Input,TimeDistributed,Dense a = np.array([[[3,1,2,2,0,0],[0,0,0,0,0,0],[2,1,1,2,0,0]]]) input = Input(shape=(3,6)) mask = Masking(mask_value=0)(input) out = TimeDistributed(Dense(1,activation=\'linear\'))(mask) model = Model(input=input,output=out) q = model.predict(a) print (q[0])\n\n@ipoletaev I think it\'s just Dense Layer that have zero inputs, so that its output is 0. If you change activation function to softmax, then you will get wrong answer. Besides, batch_size set None on time steps will raise other error in some case (especially on merge layer).\n\nIn lasagne, it seems to use Masking matrix to deal with padding. (I do not test its accuracy)\n\n@poyuwu : yes, I had checked it - and you are right. It means,as I understood, that and simple Dense doesn\'t keep masked values in the way we want...\n\nI write again what does not converge with the expectations:\n\nForward RNN doesn\'t keep mask values, backward does it. It\'s strange.\n\nIs this task solving with batch_size = 1?\n\nHow to specify correctly what timesteps the network should to skip.\n\nAnd it\'s not clear in which moment BiLSTM does reset_state - only in the end of timesteps in current sample, or when the network meets with empty vector?\n\n@ipoletaev I don\'t think\n\nForward RNN doesn\'t keep mask values, backward does it. It\'s strange.\n\nthis statement is true. That\'s because padding argument is \'post\', not \'pre\'. Hence, the reason is the same as Dense layer I said.\n\nHow to specify correctly what timesteps the network should to skip.\n\nAs I said, in lasagne, we provide a mask numpy.array (the same shape as input) to deal with it. If go_backwards=True, it needs to keep padding argument the same.\n\nBesides, Embedding layer mask_zeros seems to be the same.\n\n@poyuwu so you want to say that now, there\'s no way to solve this issue with Keras? I mean is it necessary to use masking if we use sample weights?\n\ncbdbdd mentioned this issue\n\nLSTM CudaNdarrayType(float32, col)\' and \'CudaNdarrayType(float32, matrix) error #3641\n\nzumpchke mentioned this issue\n\nRecurrent Models with sequences of mixed length #40\n\nSame here. It seems masking mechanism in Keras is not fully supported.\n\nI don\'t think Masking masks input values (neither during forward or back-propagation). It just skips a time-step where all features are equal to the mask value (i.e. when you pad a sequence). You can confirm this by:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, TimeDistributed, Dense if __name__ == ""__main__"": a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[2,1,1,2,0.1,0.1]]]) print \'Input array:\' print a print \'\' input = Input(shape=(3,6)) mask = Masking(mask_value=0.1)(input) out = TimeDistributed(Dense(1, activation=\'linear\'))(mask) model = Model(input=input, output=out) model.set_weights([np.array([[ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1.]], dtype=np.float32), np.array([ 0.], dtype=np.float32)]) print \'Weights\' print model.get_weights() q = model.predict(a) print q\n\nInput array: [[[ 3. 1. 2. 2. 0.1 0.1] [ 0. 0. 0. 0. 0. 0. ] [ 2. 1. 1. 2. 0.1 0.1]]] Weights [array([[ 1.], [ 1.], [ 1.], [ 1.], [ 1.], [ 1.]], dtype=float32), array([ 0.], dtype=float32)] [[[ 8.20000076] [ 0. ] [ 6.19999981]]]\n\nIf it masked the inputs of value 0.1, you would expect result to be\n\n[[[ 8. ] [ 0. ] [ 6. ]]]\n\nActually Masking works exactly as expected. The problem is that you are working with the wrong dimension order: in input = (3,6) the 3 is the time dimension and the Masking layer masks only along that dimension, making the net ignore a time sample if that sample is composed of all elements equal to the masked value.\n\nimport keras from keras.utils.visualize_util import plot from keras.layers import * from keras.models import Model net_input = Input(shape = ( 3, 10)) mask = Masking(mask_value = 0.5)(net_input) conv = TimeDistributed(Dense(1, activation = \'linear\', init=\'one\'))(mask) out = LSTM(1, init=\'one\', inner_init=\'one\',activation=\'tanh\', inner_activation=\'tanh\',)(conv) model = Model(net_input, out) print(\'W: \' + str(model.get_weights())) net_in = np.ones((1,3, 10)) val = 0.5 net_in[0, 2, :] = val out = model.predict(net_in) print(\'Input: \' + str(net_in)) print(\'Output: \' + str(out))\n\nIn this case the answers are:\n\nmask = 0.5, val = 0.0 : 0.73566443 mask = 0.0, val = 0.0 : 0.96402758 mask = 0.0, val = 0.5 : 0.99504161 mask = 0.5, val = 0.5 : 0.96402758\n\nso from here you can see that when we mask val we get the same result, while when we mask something else, even if val = 0, we get a different result.\n\nMoreover, I just tested, if you have a Multi-input net (with multiple input branches) and you have a masking layer on each branch, it is enough that just one of the inputs at time step t is equal to the masked value that all the time step is skipped.\n\nI guess that if one wants to skip the time step only if all the inputs are equal to the masked value, the branches need to be merged, right?\n\nirrationalagent commented\n\nHi Fragore, I have a similar question to you about masking with multiple inputs. I have two input branches and all I want to do is mask 0 from both. Am I right in thinking that adding a mask to the end of each branch is equivalent to adding a single mask AFTER the inputs are merged? here\'s my example\n\ninput1 = Sequential() input1.add(TimeDistributed(Dense(50), input_shape=(MAX_SEQUENCE_LENGTH,48))) input2 = Sequential() input2.add(Embedding(nb_words+2,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False,input_length=MAX_SEQUENCE_LENGTH)) model = Sequential() model.add(keras.engine.topology.Merge([input1,input2],mode=\'concat\',concat_axis=-1)) model.add(keras.layers.core.Masking(mask_value=0.0)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(512,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(TimeDistributed(Dense(nb_words + 1))) model.add(Activation(\'softmax\'))\n\nor version with a mask after each branch prior to merging\n\ninput1 = Sequential() input1.add(TimeDistributed(Dense(50), input_shape=(MAX_SEQUENCE_LENGTH,48))) input1.add(keras.layers.core.Masking(mask_value=0.0)) input2 = Sequential() input2.add(Embedding(nb_words+2,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False,input_length=MAX_SEQUENCE_LENGTH,mask_zero=True)) model = Sequential() model.add(keras.engine.topology.Merge([input1,input2],mode=\'concat\',concat_axis=-1)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(512,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(TimeDistributed(Dense(nb_words + 1))) model.add(Activation(\'softmax\'))\n\nWait, you want to mask the output of the branches that are 0? In that case both of your approaches should give you the same result. But usually you mask inputs, this means to put the mask layer as input of the net. Ps it may also be more convenient to use the functional API :) PPS the last dense layer doesn\'t need TimeDistributed anymore cause the LSTM removes the time dimension.\n\nI\'ve been experimenting with and without masking for a little bit now and I have finally figured out what the Masking layer actually does. It doesn\'t actually ""skip"" the timepoint that has all masked values, it just forces all the values for that timepoint to be equal to 0... So effectively Masking(mask_value=0.) does nothing. That is why in the example provided by @GPaolo above the results for mask_value=0 and mask_value=0.5 are the same when val matches them.\n\nHere is some easy code to demonstrate what I mean.\n\n`input1 = Input(batch_shape=(1,1,10) mask1 = Masking(mask_value=2)(input1) dense_layer1 = Dense(1, activation=\'sigmoid\') dense_layer1.setattr(\'supports_masking\', True) output1 = dense_layer1(mask1)\n\nmodel = Model(input1, output1) model.compile(optimizer=\'adam\', loss=\'binary_crossentropy\') ` Data:\n\n`data = np.ones((10, 1, 10), dtype=\'float32\') #set half of the data equal to mask value for index in range(5,10): data[index,0,:] = 2\n\n#set first data point equal to mask value to show that this line is uneffected data[0,0,0] = 2`\n\n`get_mask_output = K.function([model.layers[0].input], [model.layers[1].output]) mask_output = get_mask_output([data])[0]\n\nprint(data) print(mask_output)\n\n[[[ 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]]\n\n[[[ 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]`\n\n`test_data = np.ones((5,1,10)) test_data[1,0,:] = 2 test_data[2,0,:] = 0 predictions = model.predict(test_data, batch_size=1)\n\nprint(test_data) print(predictions) `\n\ntest_data: `[[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n\npredictions: [[[ 0.09200736]]\n\nAs you can imagine, ""masking"" values by setting them to 0 and still calculating the results for those lines in layers causes some mistakes from backpropagation (treating unknown values as a real result) as well as added unneeded computation time. I\'m going to try to rework how masking is done in Keras a bit...\n\nEdit: I did a little bit of digging into the training.py code and I found that the ""masking"" information (even with mask_value = 0.) does get incorporated into the training of the weights. The masked lines effectively get ignored after the calculation is done (which is good!). The problem that I am encountering in my actual network is that although ""masked lines"" are ignored during weight training, they are still evaluated by the network going forward which effects the outputs of future layers based on false information. To be able to build a network that handles variably sized inputs (not all have max timepoints) I want to completely ignore the masked lines entirely... I\'m going to try to work that out\n\nBuilding on @slaterb1 and @GPaolo \'s snippets I tried digging around to see the benefits of masking but haven\'t found it yet. It feels like I\'m missing something.\n\nIt does not seem to propagate numerically sound values through time\n\nIt propagates np.nan, see gist\n\nFeels (TODO:test) quite numerically unstable to propagate possibly absurd values down the network? Like mask output 0 may not always be in place.\n\nIt has to test each input\n\nQuick testing (see gist) seem to show that there\'s no immediate performance gains\n\nDoes anyone have an idea about if/when it gives performance gains? I didn\'t have time to run for long/deep/wide and I\'m not comfortable about how Python/Keras/Tensorflow/Theano compiles\n\nIs mask an intricate way of doing what I think weights should to be doing? I.e multiplying with the loss and dividing by sum of weights in batch? It\'s literally what seems to be done here anyway: https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L453\n\nDoes it actually halt any execution (yet)?\n\n@ragulpr, it\'s my understanding that masking does more than just loss scaling. If a timestep has been masked, the previous output and state will be reused. See here and here.\n\n@ragulpr, I\'m not sure about performance gains but Theano is pretty smart about knowing what it needs to hang on to and what it doesn\'t (based on the API doc: http://deeplearning.net/software/theano/library/scan.html)\n\nMore specifically this line: ""Note that there is an optimization, that at compile time will detect that you are using just the last value of the result and ensure that scan does not store all the intermediate values that are used. So do not worry if A and k are large.""\n\nSo after compiling the model it might pass over the masked values (or at least not hold them in memory as long), but that is pure speculation based on similarities in the underlying code.\n\n@carlthome, I came across the mask snippet in the ""theano_backend.py"" as well and you are right that the masking has a direct effect on how the states are evaluated and passed on (T.switch). Maybe this is too general a question but how does this layer accept the mask? Just to give an example, if I have a model with multiple layers, defined as so:\n\nmodel = Model(input1, output1)\n\nI understand that Theano wraps this up as a mathematical equation to calculate:\n\noutput1 = input1 -> [ layers[0] -> layers[1] -> ... layers[N] ]\n\nbut if I have somewhere in the middle:\n\nprev_layer -> Masking_layer -> RNN_layer\n\nThe output from the Masking_layer gets put into the RNN_layer as input (""x""). Does the ""supports_masking"" attribute tell the RNN_layer to figure out the mask? I could not find anywhere in the code where the mask is evaluated or interpreted by the RNN_layer, except that I can pass in a ""mask"" variable via the call() method of the Recurrent(Layer) object.\n\nI tried calling RNN_layer(prev_layer, mask=Masking_layer) but it didn\'t do anything different. The last comment in the thread, #176 suggests that it has to be called with a mask but I\'m not sure how to do that... Any thoughts?\n\nI could not find anywhere in the code where the mask is evaluated or interpreted by the RNN_layer\n\nEach Keras layer declares if it supports masking. Each layer is also responsible for using the mask in a sensible way (which I believe is the primary source of confusion: that the masking functionality is implemented across a bunch of different classes). For RNN layers in particular, they rely on the fact that the underlying K.rnn operation has mask support so if you\'re looking for where precisely the logic is, you\'ll note that the RNN layers simply pass the mask argument into the backend, where the magic happens.\n\n@carlthome, I saw that in the code but was not able to get the mask to work in my RNN network. For clarity I was trying to rework stuff in RecurrentShop to setup an encoder decoder network that adjusts the next input based on a prediction made on the previous state from both the encoder and the decoder (a custom RNN that uses a .single_step_rnn() instead of the regular .rnn() ).\n\nBut based on your advice, I tried to just build a basic LSTM network to act as a NOT Gate (pointless but simple) and it does interpret the mask correctly, when it is passed a mask mid network! I\'m including the gist. It shows that masking works for both return_sequences=True and return_sequences=False. It also shows that if you train the network with data that does not have \'masked\' input, \'masked\' lines in the test data will still get masked appropriately. Hope that helps people understand the masking stuff better!\n\n@fferroni @GPaolo apparently, the TimeDistributed layer didn\'t support masking, since this feature has been added in Pull #6401?\n\nmehrdadscomputer commented\n\nHey Guys, there is a seq2seq example which it\'s input is a string (sequence) like \'5+9\' and output is another string \'14\'. The author used pre padding to have sequences with same lengths at input but he didn\'t use masking. I add a simple line to add masking to his model and there is about 8 percent improvement in accuracy. Is my case a correct use of masking?\n\nfrom random import seed from random import randint from numpy import array from math import ceil from math import log10 from math import sqrt from numpy import argmax from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.layers import TimeDistributed from keras.layers import RepeatVector def random_sum_pairs(n_examples, n_numbers, largest): X, y = list(), list() for i in range(n_examples): in_pattern = [randint(1,largest) for _ in range(n_numbers)] out_pattern = sum(in_pattern) X.append(in_pattern) y.append(out_pattern) return X, y def to_string(X, y, n_numbers, largest): max_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1 Xstr = list() for pattern in X: strp = \'+\'.join([str(n) for n in pattern]) strp = \'\'.join([\' \' for _ in range(max_length-len(strp))]) + strp Xstr.append(strp) max_length = ceil(log10(n_numbers * (largest+1))) ystr = list() for pattern in y: strp = str(pattern) strp = \'\'.join([\' \' for _ in range(max_length-len(strp))]) + strp ystr.append(strp) return Xstr, ystr def integer_encode(X, y, alphabet): char_to_int = dict((c, i) for i, c in enumerate(alphabet)) Xenc = list() for pattern in X: integer_encoded = [char_to_int[char] for char in pattern] Xenc.append(integer_encoded) yenc = list() for pattern in y: integer_encoded = [char_to_int[char] for char in pattern] yenc.append(integer_encoded) return Xenc, yenc def one_hot_encode(X, y, max_int): Xenc = list() for seq in X: pattern = list() for index in seq: vector = [0 for _ in range(max_int)] vector[index] = 1 pattern.append(vector) Xenc.append(pattern) yenc = list() for seq in y: pattern = list() for index in seq: vector = [0 for _ in range(max_int)] vector[index] = 1 pattern.append(vector) yenc.append(pattern) return Xenc, yenc def generate_data(n_samples, n_numbers, largest, alphabet): X, y = random_sum_pairs(n_samples, n_numbers, largest) X, y = to_string(X, y, n_numbers, largest) X, y = integer_encode(X, y, alphabet) X, y = one_hot_encode(X, y, len(alphabet)) X, y = array(X), array(y) return X, y def invert(seq, alphabet): int_to_char = dict((i, c) for i, c in enumerate(alphabet)) strings = list() for pattern in seq: string = int_to_char[argmax(pattern)] strings.append(string) return \'\'.join(strings) seed(1) n_samples = 1000 n_numbers = 2 largest = 10 alphabet = [\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\', \'+\', \' \'] n_chars = len(alphabet) n_in_seq_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1 n_out_seq_length = ceil(log10(n_numbers * (largest+1))) n_batch = 10 n_epoch = 10 model = Sequential() model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars))) model.add(RepeatVector(n_out_seq_length)) model.add(LSTM(50, return_sequences=True)) model.add(TimeDistributed(Dense(n_chars, activation=\'softmax\'))) model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\']) print(model.summary()) for i in range(n_epoch): X, y = generate_data(n_samples, n_numbers, largest, alphabet) print(i) model.fit(X, y, epochs=1, batch_size=n_batch) X, y = generate_data(n_samples, n_numbers, largest, alphabet) result = model.predict(X, batch_size=n_batch, verbose=0) expected = [invert(x, alphabet) for x in y] predicted = [invert(x, alphabet) for x in result] for i in range(20): print(\'Expected=%s, Predicted=%s\' % (expected[i], predicted[i]))\n\nand I just change this part:\n\nmodel = Sequential() model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars)))\n\nfrom keras.layers import Masking model = Sequential() model.add(Masking(mask_value = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], input_shape=(n_in_seq_length, n_chars))) model.add(LSTM(100))\n\nsources: http://machinelearningmastery.com/learn-add-numbers-seq2seq-recurrent-neural-networks/#comment-400854\n\nstale bot added the stale label\n\nThis issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n\nstale bot closed this as completed\n\nandersjohanandreassen mentioned this issue\n\nTimeDistributed(Dense) with Masking not masking bias #12495\n\nI don\'t think Masking masks input values (neither during forward or back-propagation). It just skips a time-step where all features are equal to the mask value (i.e. when you pad a sequence). You can confirm this by:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, TimeDistributed, Dense if __name__ == ""__main__"": a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[2,1,1,2,0.1,0.1]]]) print \'Input array:\' print a print \'\' input = Input(shape=(3,6)) mask = Masking(mask_value=0.1)(input) out = TimeDistributed(Dense(1, activation=\'linear\'))(mask) model = Model(input=input, output=out) model.set_weights([np.array([[ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1.]], dtype=np.float32), np.array([ 0.], dtype=np.float32)]) print \'Weights\' print model.get_weights() q = model.predict(a) print q\n\nInput array: [[[ 3. 1. 2. 2. 0.1 0.1] [ 0. 0. 0. 0. 0. 0. ] [ 2. 1. 1. 2. 0.1 0.1]]] Weights [array([[ 1.], [ 1.], [ 1.], [ 1.], [ 1.], [ 1.]], dtype=float32), array([ 0.], dtype=float32)] [[[ 8.20000076] [ 0. ] [ 6.19999981]]]\n\nIf it masked the inputs of value 0.1, you would expect result to be\n\n[[[ 8. ] [ 0. ] [ 6. ]]]\n\nMask layer will work only when all feature of a timestep equals to the mask value.In you case,the input a is a 3d matrix with the shape(1,3,6),1 means batch_size,3 means timesteps,and 10 means the feature of that timestep.Mask will work when the feature of a timestep all equal to 0.1.if you change a to: a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[0.1,0.1,0.1,0.1,0.1,0.1]]])\n\nyou will get the output like:\n\n[[[8.200001] [0. ] [0. ]]]\n\nhoangcuong2011 commented\n\nI struggled a lot with this recently, and here is some experience I learnt. I hope it would be useful for people.\n\nMasking is extremely powerful. I found it perhaps the only way to deal with several ""hard"" problems that are with sequence of missing inputs, missing outputs as follows.\n\nMasking is not that complicated if we understand how the loss is computed with masking. For instance let us assume we have a sequence with length 256. From this sequence we have a masking with only 4 elements that are with masking of 1 (others are with masking 0). I thought the loss is computed as the average between these 4 elements. Guess what - it is not! The average loss will be divided by 256 instead. For this reason sometimes the loss will be extremely small (0.0something) if we have only few 1 elements and long sequence. Does it matter? I guess not, as what we need is the gradient of loss, rather than the loss itself.\n\nWhen we use softmax as the last layer, the denominator would be the sum of exponential of all elements, regarding whether their masking is 1 or 0.\n\nI thought the output of masking inputs is zeros all the time in LSTM. But it is not the case. Let us assume we have a masking:\n\nWith this case, the three first elements with masking zero has output of 0. However, the three last zeros have output that is as the same as the output of the last element with masking 1.\n\nMeanwhile, Keras is very convenient in the sense that the loss it computes will be based on only elements with masking of 1. I found this is a big plus of using Keras, something a bit too good too be true as I guess implementing this is not that easy.\n\nHowever, the accuracy in Keras is not computed that way. It is thus not trivial in keras to write a custom metric (for fit). There is something very mysterious to me. I am pretty sure my code for writing custom metric is correct but somehow it does not give me accurate result. Because of this I think it is much much easier if we write such an accuracy function with a custom callback class.\n\nThat is it, I hope it is helpful!\n\nzhanjiezhu commented\n\nI struggled a lot with this recently, and here is some experience I learnt. I hope it would be useful for people.\n\nMasking is extremely powerful. I found it perhaps the only way to deal with several ""hard"" problems that are with sequence of missing inputs, missing outputs as follows.\n\nMasking is not that complicated if we understand how the loss is computed with masking. For instance let us assume we have a sequence with length 256. From this sequence we have a masking with only 4 elements that are with masking of 1 (others are with masking 0). I thought the loss is computed as the average between these 4 elements. Guess what - it is not! The average loss will be divided by 256 instead. For this reason sometimes the loss will be extremely small (0.0something) if we have only few 1 elements and long sequence. Does it matter? I guess not, as what we need is the gradient of loss, rather than the loss itself.\n\nWhen we use softmax as the last layer, the denominator would be the sum of exponential of all elements, regarding whether their masking is 1 or 0.\n\nI thought the output of masking inputs is zeros all the time in LSTM. But it is not the case. Let us assume we have a masking:\n\nWith this case, the three first elements with masking zero has output of 0. However, the three last zeros have output that is as the same as the output of the last element with masking 1.\n\nMeanwhile, Keras is very convenient in the sense that the loss it computes will be based on only elements with masking of 1. I found this is a big plus of using Keras, something a bit too good too be true as I guess implementing this is not that easy.\n\nHowever, the accuracy in Keras is not computed that way. It is thus not trivial in keras to write a custom metric (for fit). There is something very mysterious to me. I am pretty sure my code for writing custom metric is correct but somehow it does not give me accurate result. Because of this I think it is much much easier if we write such an accuracy function with a custom callback class.\n\nThat is it, I hope it is helpful!\n\nHi @hoangcuong2011 , thanks for your explanations. I\'ve validated your second point and indeed it\'s exactly what you said. I\'m currently trying to implement a LSTM-autoencoder model to encode sequence into sequence, in which it involves a LSTM layer with return_sequence = False and then RepeatVector layer to copy that back to the previous timestep dimension. However, the mask get lost right after the LSTM because return_sequence = False (if True it returns the input_mask), then I\'m wondering how I can get back the mask so that the loss will also ignore the padded timesteps? Thanks!\n\nhoangcuong2011 mentioned this issue\n\ntf.keras.layers.Softmax does not support masking? tensorflow/tensorflow#27010\n\nhoangcuong2011 commented\n\n@zhangwj618 I am not really sure what your question is about. I guess you would like to write a custom masking layer. If you explain the question in more detail, I think I can help. Thx!\n\nsushreebarsa mentioned this issue\n\nMasking layer does not work after training #14108\n\nhossain666 commented\n\n](./typescript-kurulumu.md) |\n\nhossain666 commented\n\n](./typescript-kurulumu.md) |\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_4', 'snippet': 'Navigation Menu Toggle navigation\n\nSearch or jump to...\n\nSearch code, repositories, users, issues, pull requests...\n\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nkeras-team / keras Public\n\nYou must be signed in to change notification settings\n\nHow does Masking work? #3086\n\npoyuwu opened this issue\n\nJun 27, 2016 · 32 comments\n\nHow does Masking work? #3086\n\npoyuwu opened this issue\n\nJun 27, 2016 · 32 comments\n\nI\'m wondering how Masking Layer works. I try to write simple model to test Masking on Activation Layer\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input a = np.array([[3.,1.,2.,2.,0.,0.]]) inputs = Input(shape=(6,)) mask = Masking(mask_value=0.0)(inputs) softmax = Activation(\'softmax\')(mask) model = Model(input=inputs,output=softmax) model.predict(a)\n\nand the result of prediction is\n\narray([[ 0.50744212, 0.06867483, 0.18667753, 0.18667753, 0.02526405, 0.02526405]])\n\nIs this the correct behavior? My keras version is 1.0.5\n\nThe text was updated successfully, but these errors were encountered:\n\nI\'m also interested with this question. It seems to me you expected to get something like the following: [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194]] ? But from the other side, - according to the explanations in core.py : class Masking(Layer) - masking doesn\'t work with 1D input data. So, if you try this, for example:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking,Input,TimeDistributed,Dense a = np.array([[[3,1,2,2,0,0],[0,0,0,0,0,0],[2,1,1,2,0,0]]]) input = Input(shape=(3,6)) mask = Masking(mask_value=0)(input) out = TimeDistributed(Dense(1,activation=\'linear\'))(mask) model = Model(input=input,output=out) q = model.predict(a) print (q[0])\n\n...you will get [[-0.20101213],[ 0. ],[-0.51546627]] as expected. But I think that, most likely, there\'s something wrong in my understanding.\n\n@ipoletaev yes, sure. However keras can also return [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194, 0.0, 0.0 ]] by padding it to keep shape. Here is another example about Masknig on bi-LSTM layer but sum two layer\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, LSTM, merge a = np.array([[[.3,.1,.2,.2,.1,.1],[.2,.3,.3,.3,.3,.1],[0,0,0,0,0,0]]]) inputs = Input(shape=(3,6)) mask = Masking(mask_value=0.0)(inputs) fw = LSTM(1,return_sequences=True)(mask) bw = LSTM(1,return_sequences=True,go_backwards=True)(mask) merged = merge([fw,bw],mode=\'sum\') model = Model(input=inputs,output=fw) model2 = Model(input=inputs,output=bw) model3 = Model(input=inputs,output=merged)\n\nthe fw\'s output is array([[[-0.07041532], [-0.12203699], [-0.12203699]]]) the bw\'s output is array([[[ 0. ], [-0.03112165], [ 0.02271803]]]) the merge\'s output is array([[[-0.07041532], [-0.15315863], [-0.09931896]]]) but I think it should be (Here it also can padding 0 to keep shape.) array([[[-0.10153697], [-0.09931896]]]) which -0.10153697 = (-0.07041532) + (-0.03112165) and -0.09931896 = -0.12203699 + 0.02271803 Is is anything wrong on Keras?\n\nHowever keras can also return [[ 0.53444666, 0.07232948, 0.19661194, 0.19661194, 0.0, 0.0 ]] by padding it to keep shape.\n\nHmm... I don\'t know how to make such out only through the Keras.\n\nAbout your example: I think it\'s similar to the aforementioned example, so you should get array([[[-0.07041532], [-0.15315863], [0.02271803]]]). And it\'s really strange that bw works right but fw doesn\'t, because of its third output is not equal to zero, but it must...\n\nlomizandtyd commented\n\nHi guys, I got this question too.. Especially for LSTM (BRNN).\n\nMasking Layer gives a masked vector, only work for the inputs, not for inner states. So in @poyuwu \'s example, the fw\'s output still has value in step 3.\n\nThis might be correct because inputs are surely masked. While, I want to find a way to skip the calculation step when coming masked value, like some special tags.\n\nHowever, I think using Masking layer in bidirectional RNN for sequences with different lengths may be totally wrong.\n\nSo in @poyuwu \'s example, the fw\'s output still has value in step 3.\n\nYes, it\'s logically, but in any case we want to get zero at the third place, isn\'t it?\n\nWhile, I want to find a way to skip the calculation step when coming masked value.\n\nI think it doesn\'t matter because of, as I understood, you should specify output sample_weights in fit() in order to skip necessary timesteps with all zeros in feature vector (I have already asked about this #3023 ). But if this is so, then it is not clear why do we need masking if we can specify it in fit(): what are the steps in the examples is using for training, and what - no. I mean it is not important to process these ""empty"" vectors by network, it is important to train network without backpropagation with errors calculated on such vectors.\n\nMaybe there is some way to use a batch_size=1 and do not bother with padding?\n\nlomizandtyd commented\n\n@ipoletaev Wow, thanks a lot for this!\n\nYes, we want to get zero at the masked position. The problem is we also want to keep the inner states across the masked step.\n\nMaybe we can deliver another sample_weights in the predict() function?. If do so, BRNN is still wrong...\n\n...keep the inner states across the masked step.\n\nI think it\'s not necessary, because the network shouldn\'t remember what responses it need to get at empty vectors...\n\nMaybe we can deliver another sample_weights in the predict() function?.\n\nI don\'t understand for what task you want to use it? After all you always know in advance what data you process, and you respectively know - which output of the network corresponds to the empty vectors, so you can just skip such positions in output, I guess.\n\nIf do so, BRNN is still wrong...\n\nAs far as I understood Keras has been ""fighting"" with RNN masking task about year :)\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking,Input,TimeDistributed,Dense a = np.array([[[3,1,2,2,0,0],[0,0,0,0,0,0],[2,1,1,2,0,0]]]) input = Input(shape=(3,6)) mask = Masking(mask_value=0)(input) out = TimeDistributed(Dense(1,activation=\'linear\'))(mask) model = Model(input=input,output=out) q = model.predict(a) print (q[0])\n\n@ipoletaev I think it\'s just Dense Layer that have zero inputs, so that its output is 0. If you change activation function to softmax, then you will get wrong answer. Besides, batch_size set None on time steps will raise other error in some case (especially on merge layer).\n\nIn lasagne, it seems to use Masking matrix to deal with padding. (I do not test its accuracy)\n\n@poyuwu : yes, I had checked it - and you are right. It means,as I understood, that and simple Dense doesn\'t keep masked values in the way we want...\n\nI write again what does not converge with the expectations:\n\nForward RNN doesn\'t keep mask values, backward does it. It\'s strange.\n\nIs this task solving with batch_size = 1?\n\nHow to specify correctly what timesteps the network should to skip.\n\nAnd it\'s not clear in which moment BiLSTM does reset_state - only in the end of timesteps in current sample, or when the network meets with empty vector?\n\n@ipoletaev I don\'t think\n\nForward RNN doesn\'t keep mask values, backward does it. It\'s strange.\n\nthis statement is true. That\'s because padding argument is \'post\', not \'pre\'. Hence, the reason is the same as Dense layer I said.\n\nHow to specify correctly what timesteps the network should to skip.\n\nAs I said, in lasagne, we provide a mask numpy.array (the same shape as input) to deal with it. If go_backwards=True, it needs to keep padding argument the same.\n\nBesides, Embedding layer mask_zeros seems to be the same.\n\n@poyuwu so you want to say that now, there\'s no way to solve this issue with Keras? I mean is it necessary to use masking if we use sample weights?\n\ncbdbdd mentioned this issue\n\nLSTM CudaNdarrayType(float32, col)\' and \'CudaNdarrayType(float32, matrix) error #3641\n\nzumpchke mentioned this issue\n\nRecurrent Models with sequences of mixed length #40\n\nSame here. It seems masking mechanism in Keras is not fully supported.\n\nI don\'t think Masking masks input values (neither during forward or back-propagation). It just skips a time-step where all features are equal to the mask value (i.e. when you pad a sequence). You can confirm this by:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, TimeDistributed, Dense if __name__ == ""__main__"": a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[2,1,1,2,0.1,0.1]]]) print \'Input array:\' print a print \'\' input = Input(shape=(3,6)) mask = Masking(mask_value=0.1)(input) out = TimeDistributed(Dense(1, activation=\'linear\'))(mask) model = Model(input=input, output=out) model.set_weights([np.array([[ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1.]], dtype=np.float32), np.array([ 0.], dtype=np.float32)]) print \'Weights\' print model.get_weights() q = model.predict(a) print q\n\nInput array: [[[ 3. 1. 2. 2. 0.1 0.1] [ 0. 0. 0. 0. 0. 0. ] [ 2. 1. 1. 2. 0.1 0.1]]] Weights [array([[ 1.], [ 1.], [ 1.], [ 1.], [ 1.], [ 1.]], dtype=float32), array([ 0.], dtype=float32)] [[[ 8.20000076] [ 0. ] [ 6.19999981]]]\n\nIf it masked the inputs of value 0.1, you would expect result to be\n\n[[[ 8. ] [ 0. ] [ 6. ]]]\n\nActually Masking works exactly as expected. The problem is that you are working with the wrong dimension order: in input = (3,6) the 3 is the time dimension and the Masking layer masks only along that dimension, making the net ignore a time sample if that sample is composed of all elements equal to the masked value.\n\nimport keras from keras.utils.visualize_util import plot from keras.layers import * from keras.models import Model net_input = Input(shape = ( 3, 10)) mask = Masking(mask_value = 0.5)(net_input) conv = TimeDistributed(Dense(1, activation = \'linear\', init=\'one\'))(mask) out = LSTM(1, init=\'one\', inner_init=\'one\',activation=\'tanh\', inner_activation=\'tanh\',)(conv) model = Model(net_input, out) print(\'W: \' + str(model.get_weights())) net_in = np.ones((1,3, 10)) val = 0.5 net_in[0, 2, :] = val out = model.predict(net_in) print(\'Input: \' + str(net_in)) print(\'Output: \' + str(out))\n\nIn this case the answers are:\n\nmask = 0.5, val = 0.0 : 0.73566443 mask = 0.0, val = 0.0 : 0.96402758 mask = 0.0, val = 0.5 : 0.99504161 mask = 0.5, val = 0.5 : 0.96402758\n\nso from here you can see that when we mask val we get the same result, while when we mask something else, even if val = 0, we get a different result.\n\nMoreover, I just tested, if you have a Multi-input net (with multiple input branches) and you have a masking layer on each branch, it is enough that just one of the inputs at time step t is equal to the masked value that all the time step is skipped.\n\nI guess that if one wants to skip the time step only if all the inputs are equal to the masked value, the branches need to be merged, right?\n\nirrationalagent commented\n\nHi Fragore, I have a similar question to you about masking with multiple inputs. I have two input branches and all I want to do is mask 0 from both. Am I right in thinking that adding a mask to the end of each branch is equivalent to adding a single mask AFTER the inputs are merged? here\'s my example\n\ninput1 = Sequential() input1.add(TimeDistributed(Dense(50), input_shape=(MAX_SEQUENCE_LENGTH,48))) input2 = Sequential() input2.add(Embedding(nb_words+2,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False,input_length=MAX_SEQUENCE_LENGTH)) model = Sequential() model.add(keras.engine.topology.Merge([input1,input2],mode=\'concat\',concat_axis=-1)) model.add(keras.layers.core.Masking(mask_value=0.0)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(512,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(TimeDistributed(Dense(nb_words + 1))) model.add(Activation(\'softmax\'))\n\nor version with a mask after each branch prior to merging\n\ninput1 = Sequential() input1.add(TimeDistributed(Dense(50), input_shape=(MAX_SEQUENCE_LENGTH,48))) input1.add(keras.layers.core.Masking(mask_value=0.0)) input2 = Sequential() input2.add(Embedding(nb_words+2,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False,input_length=MAX_SEQUENCE_LENGTH,mask_zero=True)) model = Sequential() model.add(keras.engine.topology.Merge([input1,input2],mode=\'concat\',concat_axis=-1)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(1024,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(LSTM(512,dropout_W=.2,dropout_U=.2,return_sequences=True)) model.add(TimeDistributed(Dense(nb_words + 1))) model.add(Activation(\'softmax\'))\n\nWait, you want to mask the output of the branches that are 0? In that case both of your approaches should give you the same result. But usually you mask inputs, this means to put the mask layer as input of the net. Ps it may also be more convenient to use the functional API :) PPS the last dense layer doesn\'t need TimeDistributed anymore cause the LSTM removes the time dimension.\n\nI\'ve been experimenting with and without masking for a little bit now and I have finally figured out what the Masking layer actually does. It doesn\'t actually ""skip"" the timepoint that has all masked values, it just forces all the values for that timepoint to be equal to 0... So effectively Masking(mask_value=0.) does nothing. That is why in the example provided by @GPaolo above the results for mask_value=0 and mask_value=0.5 are the same when val matches them.\n\nHere is some easy code to demonstrate what I mean.\n\n`input1 = Input(batch_shape=(1,1,10) mask1 = Masking(mask_value=2)(input1) dense_layer1 = Dense(1, activation=\'sigmoid\') dense_layer1.setattr(\'supports_masking\', True) output1 = dense_layer1(mask1)\n\nmodel = Model(input1, output1) model.compile(optimizer=\'adam\', loss=\'binary_crossentropy\') ` Data:\n\n`data = np.ones((10, 1, 10), dtype=\'float32\') #set half of the data equal to mask value for index in range(5,10): data[index,0,:] = 2\n\n#set first data point equal to mask value to show that this line is uneffected data[0,0,0] = 2`\n\n`get_mask_output = K.function([model.layers[0].input], [model.layers[1].output]) mask_output = get_mask_output([data])[0]\n\nprint(data) print(mask_output)\n\n[[[ 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]]\n\n[[[ 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]`\n\n`test_data = np.ones((5,1,10)) test_data[1,0,:] = 2 test_data[2,0,:] = 0 predictions = model.predict(test_data, batch_size=1)\n\nprint(test_data) print(predictions) `\n\ntest_data: `[[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]\n\n[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n\n[[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]]\n\npredictions: [[[ 0.09200736]]\n\nAs you can imagine, ""masking"" values by setting them to 0 and still calculating the results for those lines in layers causes some mistakes from backpropagation (treating unknown values as a real result) as well as added unneeded computation time. I\'m going to try to rework how masking is done in Keras a bit...\n\nEdit: I did a little bit of digging into the training.py code and I found that the ""masking"" information (even with mask_value = 0.) does get incorporated into the training of the weights. The masked lines effectively get ignored after the calculation is done (which is good!). The problem that I am encountering in my actual network is that although ""masked lines"" are ignored during weight training, they are still evaluated by the network going forward which effects the outputs of future layers based on false information. To be able to build a network that handles variably sized inputs (not all have max timepoints) I want to completely ignore the masked lines entirely... I\'m going to try to work that out\n\nBuilding on @slaterb1 and @GPaolo \'s snippets I tried digging around to see the benefits of masking but haven\'t found it yet. It feels like I\'m missing something.\n\nIt does not seem to propagate numerically sound values through time\n\nIt propagates np.nan, see gist\n\nFeels (TODO:test) quite numerically unstable to propagate possibly absurd values down the network? Like mask output 0 may not always be in place.\n\nIt has to test each input\n\nQuick testing (see gist) seem to show that there\'s no immediate performance gains\n\nDoes anyone have an idea about if/when it gives performance gains? I didn\'t have time to run for long/deep/wide and I\'m not comfortable about how Python/Keras/Tensorflow/Theano compiles\n\nIs mask an intricate way of doing what I think weights should to be doing? I.e multiplying with the loss and dividing by sum of weights in batch? It\'s literally what seems to be done here anyway: https://github.com/fchollet/keras/blob/master/keras/engine/training.py#L453\n\nDoes it actually halt any execution (yet)?\n\n@ragulpr, it\'s my understanding that masking does more than just loss scaling. If a timestep has been masked, the previous output and state will be reused. See here and here.\n\n@ragulpr, I\'m not sure about performance gains but Theano is pretty smart about knowing what it needs to hang on to and what it doesn\'t (based on the API doc: http://deeplearning.net/software/theano/library/scan.html)\n\nMore specifically this line: ""Note that there is an optimization, that at compile time will detect that you are using just the last value of the result and ensure that scan does not store all the intermediate values that are used. So do not worry if A and k are large.""\n\nSo after compiling the model it might pass over the masked values (or at least not hold them in memory as long), but that is pure speculation based on similarities in the underlying code.\n\n@carlthome, I came across the mask snippet in the ""theano_backend.py"" as well and you are right that the masking has a direct effect on how the states are evaluated and passed on (T.switch). Maybe this is too general a question but how does this layer accept the mask? Just to give an example, if I have a model with multiple layers, defined as so:\n\nmodel = Model(input1, output1)\n\nI understand that Theano wraps this up as a mathematical equation to calculate:\n\noutput1 = input1 -> [ layers[0] -> layers[1] -> ... layers[N] ]\n\nbut if I have somewhere in the middle:\n\nprev_layer -> Masking_layer -> RNN_layer\n\nThe output from the Masking_layer gets put into the RNN_layer as input (""x""). Does the ""supports_masking"" attribute tell the RNN_layer to figure out the mask? I could not find anywhere in the code where the mask is evaluated or interpreted by the RNN_layer, except that I can pass in a ""mask"" variable via the call() method of the Recurrent(Layer) object.\n\nI tried calling RNN_layer(prev_layer, mask=Masking_layer) but it didn\'t do anything different. The last comment in the thread, #176 suggests that it has to be called with a mask but I\'m not sure how to do that... Any thoughts?\n\nI could not find anywhere in the code where the mask is evaluated or interpreted by the RNN_layer\n\nEach Keras layer declares if it supports masking. Each layer is also responsible for using the mask in a sensible way (which I believe is the primary source of confusion: that the masking functionality is implemented across a bunch of different classes). For RNN layers in particular, they rely on the fact that the underlying K.rnn operation has mask support so if you\'re looking for where precisely the logic is, you\'ll note that the RNN layers simply pass the mask argument into the backend, where the magic happens.\n\n@carlthome, I saw that in the code but was not able to get the mask to work in my RNN network. For clarity I was trying to rework stuff in RecurrentShop to setup an encoder decoder network that adjusts the next input based on a prediction made on the previous state from both the encoder and the decoder (a custom RNN that uses a .single_step_rnn() instead of the regular .rnn() ).\n\nBut based on your advice, I tried to just build a basic LSTM network to act as a NOT Gate (pointless but simple) and it does interpret the mask correctly, when it is passed a mask mid network! I\'m including the gist. It shows that masking works for both return_sequences=True and return_sequences=False. It also shows that if you train the network with data that does not have \'masked\' input, \'masked\' lines in the test data will still get masked appropriately. Hope that helps people understand the masking stuff better!\n\n@fferroni @GPaolo apparently, the TimeDistributed layer didn\'t support masking, since this feature has been added in Pull #6401?\n\nmehrdadscomputer commented\n\nHey Guys, there is a seq2seq example which it\'s input is a string (sequence) like \'5+9\' and output is another string \'14\'. The author used pre padding to have sequences with same lengths at input but he didn\'t use masking. I add a simple line to add masking to his model and there is about 8 percent improvement in accuracy. Is my case a correct use of masking?\n\nfrom random import seed from random import randint from numpy import array from math import ceil from math import log10 from math import sqrt from numpy import argmax from keras.models import Sequential from keras.layers import Dense from keras.layers import LSTM from keras.layers import TimeDistributed from keras.layers import RepeatVector def random_sum_pairs(n_examples, n_numbers, largest): X, y = list(), list() for i in range(n_examples): in_pattern = [randint(1,largest) for _ in range(n_numbers)] out_pattern = sum(in_pattern) X.append(in_pattern) y.append(out_pattern) return X, y def to_string(X, y, n_numbers, largest): max_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1 Xstr = list() for pattern in X: strp = \'+\'.join([str(n) for n in pattern]) strp = \'\'.join([\' \' for _ in range(max_length-len(strp))]) + strp Xstr.append(strp) max_length = ceil(log10(n_numbers * (largest+1))) ystr = list() for pattern in y: strp = str(pattern) strp = \'\'.join([\' \' for _ in range(max_length-len(strp))]) + strp ystr.append(strp) return Xstr, ystr def integer_encode(X, y, alphabet): char_to_int = dict((c, i) for i, c in enumerate(alphabet)) Xenc = list() for pattern in X: integer_encoded = [char_to_int[char] for char in pattern] Xenc.append(integer_encoded) yenc = list() for pattern in y: integer_encoded = [char_to_int[char] for char in pattern] yenc.append(integer_encoded) return Xenc, yenc def one_hot_encode(X, y, max_int): Xenc = list() for seq in X: pattern = list() for index in seq: vector = [0 for _ in range(max_int)] vector[index] = 1 pattern.append(vector) Xenc.append(pattern) yenc = list() for seq in y: pattern = list() for index in seq: vector = [0 for _ in range(max_int)] vector[index] = 1 pattern.append(vector) yenc.append(pattern) return Xenc, yenc def generate_data(n_samples, n_numbers, largest, alphabet): X, y = random_sum_pairs(n_samples, n_numbers, largest) X, y = to_string(X, y, n_numbers, largest) X, y = integer_encode(X, y, alphabet) X, y = one_hot_encode(X, y, len(alphabet)) X, y = array(X), array(y) return X, y def invert(seq, alphabet): int_to_char = dict((i, c) for i, c in enumerate(alphabet)) strings = list() for pattern in seq: string = int_to_char[argmax(pattern)] strings.append(string) return \'\'.join(strings) seed(1) n_samples = 1000 n_numbers = 2 largest = 10 alphabet = [\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\', \'+\', \' \'] n_chars = len(alphabet) n_in_seq_length = n_numbers * ceil(log10(largest+1)) + n_numbers - 1 n_out_seq_length = ceil(log10(n_numbers * (largest+1))) n_batch = 10 n_epoch = 10 model = Sequential() model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars))) model.add(RepeatVector(n_out_seq_length)) model.add(LSTM(50, return_sequences=True)) model.add(TimeDistributed(Dense(n_chars, activation=\'softmax\'))) model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\']) print(model.summary()) for i in range(n_epoch): X, y = generate_data(n_samples, n_numbers, largest, alphabet) print(i) model.fit(X, y, epochs=1, batch_size=n_batch) X, y = generate_data(n_samples, n_numbers, largest, alphabet) result = model.predict(X, batch_size=n_batch, verbose=0) expected = [invert(x, alphabet) for x in y] predicted = [invert(x, alphabet) for x in result] for i in range(20): print(\'Expected=%s, Predicted=%s\' % (expected[i], predicted[i]))\n\nand I just change this part:\n\nmodel = Sequential() model.add(LSTM(100, input_shape=(n_in_seq_length, n_chars)))\n\nfrom keras.layers import Masking model = Sequential() model.add(Masking(mask_value = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], input_shape=(n_in_seq_length, n_chars))) model.add(LSTM(100))\n\nsources: http://machinelearningmastery.com/learn-add-numbers-seq2seq-recurrent-neural-networks/#comment-400854\n\nstale bot added the stale label\n\nThis issue has been automatically marked as stale because it has not had recent activity. It will be closed after 30 days if no further activity occurs, but feel free to re-open a closed issue if needed.\n\nstale bot closed this as completed\n\nandersjohanandreassen mentioned this issue\n\nTimeDistributed(Dense) with Masking not masking bias #12495\n\nI don\'t think Masking masks input values (neither during forward or back-propagation). It just skips a time-step where all features are equal to the mask value (i.e. when you pad a sequence). You can confirm this by:\n\nfrom keras.models import Model import numpy as np from keras.layers import Masking, Activation, Input, TimeDistributed, Dense if __name__ == ""__main__"": a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[2,1,1,2,0.1,0.1]]]) print \'Input array:\' print a print \'\' input = Input(shape=(3,6)) mask = Masking(mask_value=0.1)(input) out = TimeDistributed(Dense(1, activation=\'linear\'))(mask) model = Model(input=input, output=out) model.set_weights([np.array([[ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1. ], [ 1.]], dtype=np.float32), np.array([ 0.], dtype=np.float32)]) print \'Weights\' print model.get_weights() q = model.predict(a) print q\n\nInput array: [[[ 3. 1. 2. 2. 0.1 0.1] [ 0. 0. 0. 0. 0. 0. ] [ 2. 1. 1. 2. 0.1 0.1]]] Weights [array([[ 1.], [ 1.], [ 1.], [ 1.], [ 1.], [ 1.]], dtype=float32), array([ 0.], dtype=float32)] [[[ 8.20000076] [ 0. ] [ 6.19999981]]]\n\nIf it masked the inputs of value 0.1, you would expect result to be\n\n[[[ 8. ] [ 0. ] [ 6. ]]]\n\nMask layer will work only when all feature of a timestep equals to the mask value.In you case,the input a is a 3d matrix with the shape(1,3,6),1 means batch_size,3 means timesteps,and 10 means the feature of that timestep.Mask will work when the feature of a timestep all equal to 0.1.if you change a to: a = np.array([[[3,1,2,2,0.1,0.1],[0,0,0,0,0,0],[0.1,0.1,0.1,0.1,0.1,0.1]]])\n\nyou will get the output like:\n\n[[[8.200001] [0. ] [0. ]]]\n\nhoangcuong2011 commented\n\nI struggled a lot with this recently, and here is some experience I learnt. I hope it would be useful for people.\n\nMasking is extremely powerful. I found it perhaps the only way to deal with several ""hard"" problems that are with sequence of missing inputs, missing outputs as follows.\n\nMasking is not that complicated if we understand how the loss is computed with masking. For instance let us assume we have a sequence with length 256. From this sequence we have a masking with only 4 elements that are with masking of 1 (others are with masking 0). I thought the loss is computed as the average between these 4 elements. Guess what - it is not! The average loss will be divided by 256 instead. For this reason sometimes the loss will be extremely small (0.0something) if we have only few 1 elements and long sequence. Does it matter? I guess not, as what we need is the gradient of loss, rather than the loss itself.\n\nWhen we use softmax as the last layer, the denominator would be the sum of exponential of all elements, regarding whether their masking is 1 or 0.\n\nI thought the output of masking inputs is zeros all the time in LSTM. But it is not the case. Let us assume we have a masking:\n\nWith this case, the three first elements with masking zero has output of 0. However, the three last zeros have output that is as the same as the output of the last element with masking 1.\n\nMeanwhile, Keras is very convenient in the sense that the loss it computes will be based on only elements with masking of 1. I found this is a big plus of using Keras, something a bit too good too be true as I guess implementing this is not that easy.\n\nHowever, the accuracy in Keras is not computed that way. It is thus not trivial in keras to write a custom metric (for fit). There is something very mysterious to me. I am pretty sure my code for writing custom metric is correct but somehow it does not give me accurate result. Because of this I think it is much much easier if we write such an accuracy function with a custom callback class.\n\nThat is it, I hope it is helpful!\n\nzhanjiezhu commented\n\nI struggled a lot with this recently, and here is some experience I learnt. I hope it would be useful for people.\n\nMasking is extremely powerful. I found it perhaps the only way to deal with several ""hard"" problems that are with sequence of missing inputs, missing outputs as follows.\n\nMasking is not that complicated if we understand how the loss is computed with masking. For instance let us assume we have a sequence with length 256. From this sequence we have a masking with only 4 elements that are with masking of 1 (others are with masking 0). I thought the loss is computed as the average between these 4 elements. Guess what - it is not! The average loss will be divided by 256 instead. For this reason sometimes the loss will be extremely small (0.0something) if we have only few 1 elements and long sequence. Does it matter? I guess not, as what we need is the gradient of loss, rather than the loss itself.\n\nWhen we use softmax as the last layer, the denominator would be the sum of exponential of all elements, regarding whether their masking is 1 or 0.\n\nI thought the output of masking inputs is zeros all the time in LSTM. But it is not the case. Let us assume we have a masking:\n\nWith this case, the three first elements with masking zero has output of 0. However, the three last zeros have output that is as the same as the output of the last element with masking 1.\n\nMeanwhile, Keras is very convenient in the sense that the loss it computes will be based on only elements with masking of 1. I found this is a big plus of using Keras, something a bit too good too be true as I guess implementing this is not that easy.\n\nHowever, the accuracy in Keras is not computed that way. It is thus not trivial in keras to write a custom metric (for fit). There is something very mysterious to me. I am pretty sure my code for writing custom metric is correct but somehow it does not give me accurate result. Because of this I think it is much much easier if we write such an accuracy function with a custom callback class.\n\nThat is it, I hope it is helpful!\n\nHi @hoangcuong2011 , thanks for your explanations. I\'ve validated your second point and indeed it\'s exactly what you said. I\'m currently trying to implement a LSTM-autoencoder model to encode sequence into sequence, in which it involves a LSTM layer with return_sequence = False and then RepeatVector layer to copy that back to the previous timestep dimension. However, the mask get lost right after the LSTM because return_sequence = False (if True it returns the input_mask), then I\'m wondering how I can get back the mask so that the loss will also ignore the padded timesteps? Thanks!\n\nhoangcuong2011 mentioned this issue\n\ntf.keras.layers.Softmax does not support masking? tensorflow/tensorflow#27010\n\nhoangcuong2011 commented\n\n@zhangwj618 I am not really sure what your question is about. I guess you would like to write a custom masking layer. If you explain the question in more detail, I think I can help. Thx!\n\nsushreebarsa mentioned this issue\n\nMasking layer does not work after training #14108\n\nhossain666 commented\n\n](./typescript-kurulumu.md) |\n\nhossain666 commented\n\n](./typescript-kurulumu.md) |\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-27T14:08:10', 'title': 'How does Masking work? · Issue #3086 · keras-team/keras', 'url': 'https://github.com/keras-team/keras/issues/3086'})], [Document(page_content='中文 – 简体 GitHub\n\nUnderstanding masking & padding\n\nStay organized with collections Save and categorize content based on your preferences.\n\nAuthors: Scott Zhu, Francois Chollet\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport numpy as np import tensorflow as tf import keras from keras import layers\n\nMasking is a way to tell sequence-processing layers that certain timesteps in an input are missing, and thus should be skipped when processing the data.\n\nPadding is a special form of masking where the masked steps are at the start or the end of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences.\n\nLet\'s take a close look.\n\nPadding sequence data\n\nWhen processing sequence data, it is very common for individual samples to have different lengths. Consider the following example (text tokenized as words):\n\n[ [""Hello"", ""world"", ""!""], [""How"", ""are"", ""you"", ""doing"", ""today""], [""The"", ""weather"", ""will"", ""be"", ""nice"", ""tomorrow""], ]\n\nAfter vocabulary lookup, the data might be vectorized as integers, e.g.:\n\n[ [71, 1331, 4231] [73, 8, 3215, 55, 927], [83, 91, 1, 645, 1253, 927], ]\n\nThe data is a nested list where individual samples have length 3, 5, and 6, respectively. Since the input data for a deep learning model must be a single tensor (of shape e.g. (batch_size, 6, vocab_size) in this case), samples that are shorter than the longest item need to be padded with some placeholder value (alternatively, one might also truncate long samples before padding short samples).\n\nKeras provides a utility function to truncate and pad Python lists to a common length: tf.keras.utils.pad_sequences.\n\nraw_inputs = [ [711, 632, 71], [73, 8, 3215, 55, 927], [83, 91, 1, 645, 1253, 927], ] # By default, this will pad using 0s; it is configurable via the # ""value"" parameter. # Note that you could use ""pre"" padding (at the beginning) or # ""post"" padding (at the end). # We recommend using ""post"" padding when working with RNN layers # (in order to be able to use the # CuDNN implementation of the layers). padded_inputs = tf.keras.utils.pad_sequences(raw_inputs, padding=""post"") print(padded_inputs)\n\n[[ 711 632 71 0 0 0] [ 73 8 3215 55 927 0] [ 83 91 1 645 1253 927]]\n\nNow that all samples have a uniform length, the model must be informed that some part of the data is actually padding and should be ignored. That mechanism is masking.\n\nThere are three ways to introduce input masks in Keras models:\n\nAdd a keras.layers.Masking layer.\n\nConfigure a keras.layers.Embedding layer with mask_zero=True.\n\nPass a mask argument manually when calling layers that support this argument (e.g. RNN layers).\n\nMask-generating layers: Embedding and Masking\n\nUnder the hood, these layers will create a mask tensor (2D tensor with shape (batch, sequence_length)), and attach it to the tensor output returned by the Masking or Embedding layer.\n\nembedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True) masked_output = embedding(padded_inputs) print(masked_output._keras_mask) masking_layer = layers.Masking() # Simulate the embedding lookup by expanding the 2D input to 3D, # with embedding dimension of 10. unmasked_embedding = tf.cast( tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), tf.float32 ) masked_embedding = masking_layer(unmasked_embedding) print(masked_embedding._keras_mask)\n\ntf.Tensor( [[ True True True False False False] [ True True True True True False] [ True True True True True True]], shape=(3, 6), dtype=bool) tf.Tensor( [[ True True True False False False] [ True True True True True False] [ True True True True True True]], shape=(3, 6), dtype=bool)\n\nAs you can see from the printed result, the mask is a 2D boolean tensor with shape (batch_size, sequence_length), where each individual False entry indicates that the corresponding timestep should be ignored during processing.\n\nMask propagation in the Functional API and Sequential API\n\nWhen using the Functional API or the Sequential API, a mask generated by an Embedding or Masking layer will be propagated through the network for any layer that is capable of using them (for example, RNN layers). Keras will automatically fetch the mask corresponding to an input and pass it to any layer that knows how to use it.\n\nFor instance, in the following Sequential model, the LSTM layer will automatically receive a mask, which means it will ignore padded values:\n\nmodel = keras.Sequential( [ layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True), layers.LSTM(32), ] )\n\nThis is also the case for the following Functional API model:\n\ninputs = keras.Input(shape=(None,), dtype=""int32"") x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs) outputs = layers.LSTM(32)(x) model = keras.Model(inputs, outputs)\n\nPassing mask tensors directly to layers\n\nLayers that can handle masks (such as the LSTM layer) have a mask argument in their __call__ method.\n\nMeanwhile, layers that produce a mask (e.g. Embedding) expose a compute_mask(input, previous_mask) method which you can call.\n\nThus, you can pass the output of the compute_mask() method of a mask-producing layer to the __call__ method of a mask-consuming layer, like this:\n\nclass MyLayer(layers.Layer): def __init__(self, **kwargs): super().__init__(**kwargs) self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True) self.lstm = layers.LSTM(32) def call(self, inputs): x = self.embedding(inputs) # Note that you could also prepare a `mask` tensor manually. # It only needs to be a boolean tensor # with the right shape, i.e. (batch_size, timesteps). mask = self.embedding.compute_mask(inputs) output = self.lstm(x, mask=mask) # The layer will ignore the masked values return output layer = MyLayer() x = np.random.random((32, 10)) * 100 x = x.astype(""int32"") layer(x)\n\n<tf.Tensor: shape=(32, 32), dtype=float32, numpy= array([[ 1.1063378e-04, -5.7033719e-03, 3.0645048e-03, ..., 3.6328615e-04, -2.8766368e-03, -1.3289017e-03], [-9.2790304e-03, -1.5139847e-02, 5.7660388e-03, ..., 3.5337124e-03, 4.0699611e-03, -3.9524431e-04], [-3.4190060e-03, 7.9529232e-04, 3.7830453e-03, ..., -6.8300538e-04, 4.7965860e-03, 4.4357078e-03], ..., [-4.3796434e-04, 3.5149506e-03, 5.0854073e-03, ..., 6.3023632e-03, -4.6664057e-03, -2.1111544e-03], [ 1.2171637e-03, -1.8671650e-03, 8.6708134e-03, ..., -2.6730294e-03, -1.6238958e-03, 5.9354519e-03], [-7.1832030e-03, -6.0863695e-03, 4.3814078e-05, ..., 3.8765911e-03, -1.7828923e-03, -2.3530782e-03]], dtype=float32)>\n\nSupporting masking in your custom layers\n\nSometimes, you may need to write layers that generate a mask (like Embedding), or layers that need to modify the current mask.\n\nFor instance, any layer that produces a tensor with a different time dimension than its input, such as a Concatenate layer that concatenates on the time dimension, will need to modify the current mask so that downstream layers will be able to properly take masked timesteps into account.\n\nTo do this, your layer should implement the layer.compute_mask() method, which produces a new mask given the input and the current mask.\n\nHere is an example of a TemporalSplit layer that needs to modify the current mask.\n\nclass TemporalSplit(keras.layers.Layer): """"""Split the input tensor into 2 tensors along the time dimension."""""" def call(self, inputs): # Expect the input to be 3D and mask to be 2D, split the input tensor into 2 # subtensors along the time axis (axis 1). return tf.split(inputs, 2, axis=1) def compute_mask(self, inputs, mask=None): # Also split the mask into 2 if it presents. if mask is None: return None return tf.split(mask, 2, axis=1) first_half, second_half = TemporalSplit()(masked_embedding) print(first_half._keras_mask) print(second_half._keras_mask)\n\ntf.Tensor( [[ True True True] [ True True True] [ True True True]], shape=(3, 3), dtype=bool) tf.Tensor( [[False False False] [ True True False] [ True True True]], shape=(3, 3), dtype=bool)\n\nHere is another example of a CustomEmbedding layer that is capable of generating a mask from input values:\n\nclass CustomEmbedding(keras.layers.Layer): def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs): super().__init__(**kwargs) self.input_dim = input_dim self.output_dim = output_dim self.mask_zero = mask_zero def build(self, input_shape): self.embeddings = self.add_weight( shape=(self.input_dim, self.output_dim), initializer=""random_normal"", dtype=""float32"", ) def call(self, inputs): return tf.nn.embedding_lookup(self.embeddings, inputs) def compute_mask(self, inputs, mask=None): if not self.mask_zero: return None return tf.not_equal(inputs, 0) layer = CustomEmbedding(10, 32, mask_zero=True) x = np.random.random((3, 10)) * 9 x = x.astype(""int32"") y = layer(x) mask = layer.compute_mask(x) print(mask)\n\ntf.Tensor( [[ True True True True True True True True True True] [ True True True True False True False True True True] [ True False True False True True True True True True]], shape=(3, 10), dtype=bool)\n\nNote: For more details about format limitations related to masking, see the serialization guide.\n\nOpting-in to mask propagation on compatible layers\n\nMost layers don\'t modify the time dimension, so don\'t need to modify the current mask. However, they may still want to be able to propagate the current mask, unchanged, to the next layer. This is an opt-in behavior. By default, a custom layer will destroy the current mask (since the framework has no way to tell whether propagating the mask is safe to do).\n\nIf you have a custom layer that does not modify the time dimension, and if you want it to be able to propagate the current input mask, you should set self.supports_masking = True in the layer constructor. In this case, the default behavior of compute_mask() is to just pass the current mask through.\n\nHere\'s an example of a layer that is whitelisted for mask propagation:\n\n@keras.saving.register_keras_serializable() class MyActivation(keras.layers.Layer): def __init__(self, **kwargs): super().__init__(**kwargs) # Signal that the layer is safe for mask propagation self.supports_masking = True def call(self, inputs): return tf.nn.relu(inputs)\n\nYou can now use this custom layer in-between a mask-generating layer (like Embedding) and a mask-consuming layer (like LSTM), and it will pass the mask along so that it reaches the mask-consuming layer.\n\ninputs = keras.Input(shape=(None,), dtype=""int32"") x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs) x = MyActivation()(x) # Will pass the mask along print(""Mask found:"", x._keras_mask) outputs = layers.LSTM(32)(x) # Will receive the mask model = keras.Model(inputs, outputs)\n\nMask found: KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.bool, name=None), name=\'Placeholder_1:0\')\n\nWriting layers that need mask information\n\nSome layers are mask consumers: they accept a mask argument in call and use it to determine whether to skip certain time steps.\n\nTo write such a layer, you can simply add a mask=None argument in your call signature. The mask associated with the inputs will be passed to your layer whenever it is available.\n\nHere\'s a simple example below: a layer that computes a softmax over the time dimension (axis 1) of an input sequence, while discarding masked timesteps.\n\n@keras.saving.register_keras_serializable() class TemporalSoftmax(keras.layers.Layer): def call(self, inputs, mask=None): broadcast_float_mask = tf.expand_dims(tf.cast(mask, ""float32""), -1) inputs_exp = tf.exp(inputs) * broadcast_float_mask inputs_sum = tf.reduce_sum( inputs_exp * broadcast_float_mask, axis=-1, keepdims=True ) return inputs_exp / inputs_sum inputs = keras.Input(shape=(None,), dtype=""int32"") x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs) x = layers.Dense(1)(x) outputs = TemporalSoftmax()(x) model = keras.Model(inputs, outputs) y = model(np.random.randint(0, 10, size=(32, 100)), np.random.random((32, 100, 1)))\n\nThat is all you need to know about padding & masking in Keras. To recap:\n\n""Masking"" is how layers are able to know when to skip / ignore certain timesteps in sequence inputs.\n\nSome layers are mask-generators: Embedding can generate a mask from input values (if mask_zero=True), and so can the Masking layer.\n\nSome layers are mask-consumers: they expose a mask argument in their __call__ method. This is the case for RNN layers.\n\nIn the Functional API and Sequential API, mask information is propagated automatically.\n\nWhen using layers in a standalone way, you can pass the mask arguments to layers manually.\n\nYou can easily write layers that modify the current mask, that generate a new mask, or that consume the mask associated with the inputs.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-07-24 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_5', 'snippet': '中文 – 简体 GitHub\n\nUnderstanding masking & padding\n\nStay organized with collections Save and categorize content based on your preferences.\n\nAuthors: Scott Zhu, Francois Chollet\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport numpy as np import tensorflow as tf import keras from keras import layers\n\nMasking is a way to tell sequence-processing layers that certain timesteps in an input are missing, and thus should be skipped when processing the data.\n\nPadding is a special form of masking where the masked steps are at the start or the end of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences.\n\nLet\'s take a close look.\n\nPadding sequence data\n\nWhen processing sequence data, it is very common for individual samples to have different lengths. Consider the following example (text tokenized as words):\n\n[ [""Hello"", ""world"", ""!""], [""How"", ""are"", ""you"", ""doing"", ""today""], [""The"", ""weather"", ""will"", ""be"", ""nice"", ""tomorrow""], ]\n\nAfter vocabulary lookup, the data might be vectorized as integers, e.g.:\n\n[ [71, 1331, 4231] [73, 8, 3215, 55, 927], [83, 91, 1, 645, 1253, 927], ]\n\nThe data is a nested list where individual samples have length 3, 5, and 6, respectively. Since the input data for a deep learning model must be a single tensor (of shape e.g. (batch_size, 6, vocab_size) in this case), samples that are shorter than the longest item need to be padded with some placeholder value (alternatively, one might also truncate long samples before padding short samples).\n\nKeras provides a utility function to truncate and pad Python lists to a common length: tf.keras.utils.pad_sequences.\n\nraw_inputs = [ [711, 632, 71], [73, 8, 3215, 55, 927], [83, 91, 1, 645, 1253, 927], ] # By default, this will pad using 0s; it is configurable via the # ""value"" parameter. # Note that you could use ""pre"" padding (at the beginning) or # ""post"" padding (at the end). # We recommend using ""post"" padding when working with RNN layers # (in order to be able to use the # CuDNN implementation of the layers). padded_inputs = tf.keras.utils.pad_sequences(raw_inputs, padding=""post"") print(padded_inputs)\n\n[[ 711 632 71 0 0 0] [ 73 8 3215 55 927 0] [ 83 91 1 645 1253 927]]\n\nNow that all samples have a uniform length, the model must be informed that some part of the data is actually padding and should be ignored. That mechanism is masking.\n\nThere are three ways to introduce input masks in Keras models:\n\nAdd a keras.layers.Masking layer.\n\nConfigure a keras.layers.Embedding layer with mask_zero=True.\n\nPass a mask argument manually when calling layers that support this argument (e.g. RNN layers).\n\nMask-generating layers: Embedding and Masking\n\nUnder the hood, these layers will create a mask tensor (2D tensor with shape (batch, sequence_length)), and attach it to the tensor output returned by the Masking or Embedding layer.\n\nembedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True) masked_output = embedding(padded_inputs) print(masked_output._keras_mask) masking_layer = layers.Masking() # Simulate the embedding lookup by expanding the 2D input to 3D, # with embedding dimension of 10. unmasked_embedding = tf.cast( tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), tf.float32 ) masked_embedding = masking_layer(unmasked_embedding) print(masked_embedding._keras_mask)\n\ntf.Tensor( [[ True True True False False False] [ True True True True True False] [ True True True True True True]], shape=(3, 6), dtype=bool) tf.Tensor( [[ True True True False False False] [ True True True True True False] [ True True True True True True]], shape=(3, 6), dtype=bool)\n\nAs you can see from the printed result, the mask is a 2D boolean tensor with shape (batch_size, sequence_length), where each individual False entry indicates that the corresponding timestep should be ignored during processing.\n\nMask propagation in the Functional API and Sequential API\n\nWhen using the Functional API or the Sequential API, a mask generated by an Embedding or Masking layer will be propagated through the network for any layer that is capable of using them (for example, RNN layers). Keras will automatically fetch the mask corresponding to an input and pass it to any layer that knows how to use it.\n\nFor instance, in the following Sequential model, the LSTM layer will automatically receive a mask, which means it will ignore padded values:\n\nmodel = keras.Sequential( [ layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True), layers.LSTM(32), ] )\n\nThis is also the case for the following Functional API model:\n\ninputs = keras.Input(shape=(None,), dtype=""int32"") x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs) outputs = layers.LSTM(32)(x) model = keras.Model(inputs, outputs)\n\nPassing mask tensors directly to layers\n\nLayers that can handle masks (such as the LSTM layer) have a mask argument in their __call__ method.\n\nMeanwhile, layers that produce a mask (e.g. Embedding) expose a compute_mask(input, previous_mask) method which you can call.\n\nThus, you can pass the output of the compute_mask() method of a mask-producing layer to the __call__ method of a mask-consuming layer, like this:\n\nclass MyLayer(layers.Layer): def __init__(self, **kwargs): super().__init__(**kwargs) self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True) self.lstm = layers.LSTM(32) def call(self, inputs): x = self.embedding(inputs) # Note that you could also prepare a `mask` tensor manually. # It only needs to be a boolean tensor # with the right shape, i.e. (batch_size, timesteps). mask = self.embedding.compute_mask(inputs) output = self.lstm(x, mask=mask) # The layer will ignore the masked values return output layer = MyLayer() x = np.random.random((32, 10)) * 100 x = x.astype(""int32"") layer(x)\n\n<tf.Tensor: shape=(32, 32), dtype=float32, numpy= array([[ 1.1063378e-04, -5.7033719e-03, 3.0645048e-03, ..., 3.6328615e-04, -2.8766368e-03, -1.3289017e-03], [-9.2790304e-03, -1.5139847e-02, 5.7660388e-03, ..., 3.5337124e-03, 4.0699611e-03, -3.9524431e-04], [-3.4190060e-03, 7.9529232e-04, 3.7830453e-03, ..., -6.8300538e-04, 4.7965860e-03, 4.4357078e-03], ..., [-4.3796434e-04, 3.5149506e-03, 5.0854073e-03, ..., 6.3023632e-03, -4.6664057e-03, -2.1111544e-03], [ 1.2171637e-03, -1.8671650e-03, 8.6708134e-03, ..., -2.6730294e-03, -1.6238958e-03, 5.9354519e-03], [-7.1832030e-03, -6.0863695e-03, 4.3814078e-05, ..., 3.8765911e-03, -1.7828923e-03, -2.3530782e-03]], dtype=float32)>\n\nSupporting masking in your custom layers\n\nSometimes, you may need to write layers that generate a mask (like Embedding), or layers that need to modify the current mask.\n\nFor instance, any layer that produces a tensor with a different time dimension than its input, such as a Concatenate layer that concatenates on the time dimension, will need to modify the current mask so that downstream layers will be able to properly take masked timesteps into account.\n\nTo do this, your layer should implement the layer.compute_mask() method, which produces a new mask given the input and the current mask.\n\nHere is an example of a TemporalSplit layer that needs to modify the current mask.\n\nclass TemporalSplit(keras.layers.Layer): """"""Split the input tensor into 2 tensors along the time dimension."""""" def call(self, inputs): # Expect the input to be 3D and mask to be 2D, split the input tensor into 2 # subtensors along the time axis (axis 1). return tf.split(inputs, 2, axis=1) def compute_mask(self, inputs, mask=None): # Also split the mask into 2 if it presents. if mask is None: return None return tf.split(mask, 2, axis=1) first_half, second_half = TemporalSplit()(masked_embedding) print(first_half._keras_mask) print(second_half._keras_mask)\n\ntf.Tensor( [[ True True True] [ True True True] [ True True True]], shape=(3, 3), dtype=bool) tf.Tensor( [[False False False] [ True True False] [ True True True]], shape=(3, 3), dtype=bool)\n\nHere is another example of a CustomEmbedding layer that is capable of generating a mask from input values:\n\nclass CustomEmbedding(keras.layers.Layer): def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs): super().__init__(**kwargs) self.input_dim = input_dim self.output_dim = output_dim self.mask_zero = mask_zero def build(self, input_shape): self.embeddings = self.add_weight( shape=(self.input_dim, self.output_dim), initializer=""random_normal"", dtype=""float32"", ) def call(self, inputs): return tf.nn.embedding_lookup(self.embeddings, inputs) def compute_mask(self, inputs, mask=None): if not self.mask_zero: return None return tf.not_equal(inputs, 0) layer = CustomEmbedding(10, 32, mask_zero=True) x = np.random.random((3, 10)) * 9 x = x.astype(""int32"") y = layer(x) mask = layer.compute_mask(x) print(mask)\n\ntf.Tensor( [[ True True True True True True True True True True] [ True True True True False True False True True True] [ True False True False True True True True True True]], shape=(3, 10), dtype=bool)\n\nNote: For more details about format limitations related to masking, see the serialization guide.\n\nOpting-in to mask propagation on compatible layers\n\nMost layers don\'t modify the time dimension, so don\'t need to modify the current mask. However, they may still want to be able to propagate the current mask, unchanged, to the next layer. This is an opt-in behavior. By default, a custom layer will destroy the current mask (since the framework has no way to tell whether propagating the mask is safe to do).\n\nIf you have a custom layer that does not modify the time dimension, and if you want it to be able to propagate the current input mask, you should set self.supports_masking = True in the layer constructor. In this case, the default behavior of compute_mask() is to just pass the current mask through.\n\nHere\'s an example of a layer that is whitelisted for mask propagation:\n\n@keras.saving.register_keras_serializable() class MyActivation(keras.layers.Layer): def __init__(self, **kwargs): super().__init__(**kwargs) # Signal that the layer is safe for mask propagation self.supports_masking = True def call(self, inputs): return tf.nn.relu(inputs)\n\nYou can now use this custom layer in-between a mask-generating layer (like Embedding) and a mask-consuming layer (like LSTM), and it will pass the mask along so that it reaches the mask-consuming layer.\n\ninputs = keras.Input(shape=(None,), dtype=""int32"") x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs) x = MyActivation()(x) # Will pass the mask along print(""Mask found:"", x._keras_mask) outputs = layers.LSTM(32)(x) # Will receive the mask model = keras.Model(inputs, outputs)\n\nMask found: KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.bool, name=None), name=\'Placeholder_1:0\')\n\nWriting layers that need mask information\n\nSome layers are mask consumers: they accept a mask argument in call and use it to determine whether to skip certain time steps.\n\nTo write such a layer, you can simply add a mask=None argument in your call signature. The mask associated with the inputs will be passed to your layer whenever it is available.\n\nHere\'s a simple example below: a layer that computes a softmax over the time dimension (axis 1) of an input sequence, while discarding masked timesteps.\n\n@keras.saving.register_keras_serializable() class TemporalSoftmax(keras.layers.Layer): def call(self, inputs, mask=None): broadcast_float_mask = tf.expand_dims(tf.cast(mask, ""float32""), -1) inputs_exp = tf.exp(inputs) * broadcast_float_mask inputs_sum = tf.reduce_sum( inputs_exp * broadcast_float_mask, axis=-1, keepdims=True ) return inputs_exp / inputs_sum inputs = keras.Input(shape=(None,), dtype=""int32"") x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs) x = layers.Dense(1)(x) outputs = TemporalSoftmax()(x) model = keras.Model(inputs, outputs) y = model(np.random.randint(0, 10, size=(32, 100)), np.random.random((32, 100, 1)))\n\nThat is all you need to know about padding & masking in Keras. To recap:\n\n""Masking"" is how layers are able to know when to skip / ignore certain timesteps in sequence inputs.\n\nSome layers are mask-generators: Embedding can generate a mask from input values (if mask_zero=True), and so can the Masking layer.\n\nSome layers are mask-consumers: they expose a mask argument in their __call__ method. This is the case for RNN layers.\n\nIn the Functional API and Sequential API, mask information is propagated automatically.\n\nWhen using layers in a standalone way, you can pass the mask arguments to layers manually.\n\nYou can easily write layers that modify the current mask, that generate a new mask, or that consume the mask associated with the inputs.\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-07-24 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-04-17T09:44:19', 'title': 'Understanding masking & padding | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/keras/understanding_masking_and_padding'}), Document(page_content='中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', metadata={'id': 'web-search_2', 'snippet': '中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', 'timestamp': '2024-05-03T16:14:00', 'title': 'tf.boolean_mask | TensorFlow v2.15.0.post1', 'url': 'https://www.tensorflow.org/api_docs/python/tf/boolean_mask'})]]??"
52572275,tf.scatter_nd,"{'https://www.edx.org/learn/tensorflow', 'https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187', 'https://www.udemy.com/course/tensorflow-interview-questions-and-answers/', 'https://www.edx.org/learn/numpy', 'https://www.udemy.com/course/modern-computer-vision/', 'https://www.udemy.com/course/complete-tensorflow-2-and-keras-deep-learning-bootcamp/', 'https://www.coursera.org/learn/introduction-tensorflow', 'https://www.udemy.com/course/deep-learning-tensorflow-2/', 'https://www.udemy.com/course/tensorflow-on-googles-cloud-platform-for-data-engineers/', 'https://www.udemy.com/course/machine-learning-with-tensorflow-for-business-intelligence/'}",{'https://www.youtube.com/watch?v=9buk4Z_JlXk'},"{'https://stackoverflow.com/questions/37670886/how-do-i-select-certain-columns-of-a-2d-tensor-in-tensorflow', 'https://stackoverflow.com/questions/52572275/tensorflow-how-to-interleave-columns-of-two-tensors-e-g-using-tf-scatter-nd', 'https://stackoverflow.com/questions/44952886/tensorflow-merge-two-2-d-tensors-according-to-even-and-odd-indices'}","??[[Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nTensorflow: Merge two 2-D tensors according to even and odd indices\n\nAsked 5 years, 10 months ago\n\nModified 2 years, 5 months ago\n\nI want to perform a check for even and odd elements of the batch and swap them if needed. I managed to result with two tensors I want to interweave:\n\ndef tf_oplu(x, name=None): even = x[:,::2] #slicing into odd and even parts on the batch odd = x[:,1::2] even_flatten = tf.reshape(even, [-1]) # flatten tensors #in row-major order to apply function across them odd_flatten = tf.reshape(odd, [-1]) compare = tf.to_float(even_flatten<odd_flatten) compare_not = tf.to_float(even_flatten>=odd_flatten) #def oplu(x,y): # trivial function # if x<y : # (x<y)==1 # return y, x # else: # return x, y # (x<y)==0 even_flatten_new = odd_flatten * compare + even_flatten * compare_not odd_flatten_new = odd_flatten * compare_not + even_flatten * compare # convolute back even_new = tf.reshape(even_flatten_new,[100,128]) odd_new = tf.reshape(odd_flatten_new,[100,128])\n\nNow I want to get back $[100,256]$ tensor with even and odd places filled. In numpy I would of course do:\n\ny = np.empty((even_new.size + odd_newsize,), dtype=even_new.dtype) y[:,0::2] = even_new y[:,1::2] = odd_new return y\n\nBut such thing is not possible for tensoflow, as tensor is not modifiable. I suppose it is possible with either sparse tensor or tf.gather_nd, but both require generating array of indices, which is again non-trivial task for me. One more note: I don not want to use any python functions via tf.py_func, as I checked that they run on CPU only. Maybe lambda and tf.map_fn may help somehow? Thanks!\n\nImprove this question\n\nedited Jul 10, 2017 at 19:40\n\n22.8k77 gold badges8585 silver badges103103 bronze badges\n\nasked Jul 6, 2017 at 15:15\n\n1,05911 gold badge1313 silver badges3636 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nTo interleave two matrices vertically, you do not big guns such as gather or map_fn. You can simply interleave them as follows:\n\ntf.reshape( tf.stack([even_new, odd_new], axis=1), [-1, tf.shape(even_new)[1]])\n\nTo interleave them horizontally:\n\ntf.reshape( tf.concat([even_new[...,tf.newaxis], odd_new[...,tf.newaxis]], axis=-1), [tf.shape(even_new)[0],-1])\n\nThe idea is to use stack to interleave them in memory. The dimension where the stack occurs gives the granularity of the interleaving. If we stack at axis=0, then the interleaving occurs at each element, mixing columns. If we stack at axis=1, entire input rows remain contiguous, interleaving occurs between rows.\n\nedited Jul 10, 2017 at 19:43\n\nanswered Jul 6, 2017 at 15:47\n\n22.8k77 gold badges8585 silver badges103103 bronze badges 7\n\nThanks very much! But this code produces tensor [200,128] instead of [100,256]. I\'ve changed it to y = tf.reshape(tf.stack([even_new, odd_new], axis=0), [tf.shape(even_new)[0],-1]) so the output is as expected. Can you please provide me a little explanation why it indeed places even and odd elements where needed?\n\n– Slowpoke Jul 6, 2017 at 15:56\n\nAs far, as I understand, it stacks them vertically and then does reshape that should place elements that lay under each other together horizontally.\n\n– Slowpoke Jul 6, 2017 at 16:06\n\nYes, I followed your numpy example, which also stacks tensors vertically (along the first dimension). Your modification to stack them horizontally is right.\n\n– P-Gn Jul 6, 2017 at 16:12\n\nOh, sorry - that was typo, I meant y[:, ::2] and y[:,1::2] !\n\n– Slowpoke Jul 6, 2017 at 16:19\n\n@Slowpoke sorry you are right, I should have checked your formula better. I propose you a new formula instead, that is not based on transpose, which is somewhat of a heavyweight operation. Tell me how it goes.\n\n– P-Gn Jul 10, 2017 at 19:38\n\n | Show 2 more comments\n\nyou can use tf.dynamic_stitch, that takes as first argument a list of tensors of indices for each tensor to interleave and as second argument a list of tensors to interleave. The tensors will be interleaved along the first dimension so we need to transpose them and then transpose back. Here is the code:\n\neven_new = tf.transpose(even_new,perm=[1,0]) odd_new = tf.transpose(odd_new,perm=[1,0]) even_pos = tf.convert_to_tensor(list(range(0,256,2)),dtype=tf.int32) odd_pos = tf.convert_to_tensor(list(range(1,256,2)),dtype=tf.int32) interleaved = tf.dynamic_stitch([even_pos,odd_pos],[even_new,odd_new]) interleaved = tf.transpose(interleaved,perm=[1,0])\n\nanswered Dec 17, 2020 at 11:06\n\n18111 silver badge44 bronze badges\n\nYou can use assign to assign into slices.\n\nodd_new = tf.constant([1,3,5]) even_new = tf.constant([2,4,6]) y=tf.Variable(tf.zeros(6, dtype=tf.int32)) sess = tf.InteractiveSession() sess.run(tf.global_variables_initializer()) y[0::2].assign(odd_new).eval() y[1::2].assign(even_new).eval()\n\nanswered Jul 6, 2017 at 16:23\n\nManolo SantosManolo Santos\n\n1,90511 gold badge1414 silver badges2525 bronze badges 1\n\nThanks very much! I initially tried to construct my network using assign operators, but faced some problems and was discouraged from doing it (however, I think that code was correct), so now I prefer using embedded tensorflow functions, so the first answer suits me. But thanks nevertheless!\n\n– Slowpoke Jul 6, 2017 at 16:27\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nFor those who just don’t Git it (Ep. 573)\n\nHow to use marketing techniques to build a better resume\n\nAI/ML Tool examples part 3 - Title-Drafting Assistant\n\nWe are graduating the updated button styling for vote arrows\n\nTemporary policy: ChatGPT is banned\n\nThe [connect] tag is being burninated\n\nStack Overflow will be testing a title-drafting assistant, and we’d like your...\n\nWe are graduating the ""Related questions using Machine Learning"" experiment\n\n62 Adjust Single Value within Tensor -- TensorFlow\n\n5 Tensorflow: How to tile a tensor that duplicate in certain order?\n\n2 Tensorflow: assign multiple variable values in single run without recomputation of other expressions\n\n0 Tensorflow: Interlieving two ragged tensors\n\n1 Keras Custom Merge Two Tensors\n\n2 Concatenate two tensors in alternate fashion (Tensorflow)\n\n3 tensorflow merge and zip two tensors\n\n2 How to merge two tensor?\n\n4 How to concatenate two tensors having different shape with TensorFlow?\n\n1 Concat two 2D tensors\n\n2 How can I merge two 3D tensors by interleaving them along a certain axis?\n\n2 Merge two tensor in pytorch\n\n3 Combine arbitrary shaped tensors\n\n3 Merge one tensor into other tensor on specific indexes in PyTorch\n\nHot Network Questions\n\nMTB rear wheel selection\n\nCreate line-type symbology perpendicular to a segment in QGIS\n\nMy girlfriend helped me buy a car - Who is responsible if she takes if back?\n\nWhich IP address would be most safe and suitable to use as a placeholder in a live system?\n\nTeaching high school math between PhD and postdoc\n\nWhy are there two inverses to exponentiation?\n\nHow does social inequality differ from wealth inequality? (In a society with capitalism esque ideals)\n\nWhat does the mention of Israel suggest here in Greenmantle?\n\nWhat kind of heating would occur during a suborbital re-entry?\n\nWhat does the ""proportional symbol"" (∝) mean when used as a time unit prefix?\n\nHow does 一つとして work in this sentence?\n\nWhat is the basic difference between curry powder and garam masala?\n\nHow to list only tar files\n\nBook/book series where the main character\'s mind is copied into different alien clones\n\nPlease help to beautify this connection-graph\n\nWhy don\'t my hover tanks fly?\n\nWhat are the implications of the recent finding in Vatican Library in relation to Matt 12:1?\n\nWhat does \'length\' means?\n\nHow serious is it that my new teammates didn\'t show up to a meeting I set up that they agreed to attend?\n\nI will grab a taxi back. vs. I will grab back a taxi\n\nLoading render kernels - what exactly that means?\n\nAre two localized single-photon states always invariant under the particle exchange?\n\nWhat\'s the secret to well-seasoned pulled pork?\n\nYield curve bootstrapping not producing expected cash flow start date more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', metadata={'id': 'web-search_1', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nTensorflow: Merge two 2-D tensors according to even and odd indices\n\nAsked 5 years, 10 months ago\n\nModified 2 years, 5 months ago\n\nI want to perform a check for even and odd elements of the batch and swap them if needed. I managed to result with two tensors I want to interweave:\n\ndef tf_oplu(x, name=None): even = x[:,::2] #slicing into odd and even parts on the batch odd = x[:,1::2] even_flatten = tf.reshape(even, [-1]) # flatten tensors #in row-major order to apply function across them odd_flatten = tf.reshape(odd, [-1]) compare = tf.to_float(even_flatten<odd_flatten) compare_not = tf.to_float(even_flatten>=odd_flatten) #def oplu(x,y): # trivial function # if x<y : # (x<y)==1 # return y, x # else: # return x, y # (x<y)==0 even_flatten_new = odd_flatten * compare + even_flatten * compare_not odd_flatten_new = odd_flatten * compare_not + even_flatten * compare # convolute back even_new = tf.reshape(even_flatten_new,[100,128]) odd_new = tf.reshape(odd_flatten_new,[100,128])\n\nNow I want to get back $[100,256]$ tensor with even and odd places filled. In numpy I would of course do:\n\ny = np.empty((even_new.size + odd_newsize,), dtype=even_new.dtype) y[:,0::2] = even_new y[:,1::2] = odd_new return y\n\nBut such thing is not possible for tensoflow, as tensor is not modifiable. I suppose it is possible with either sparse tensor or tf.gather_nd, but both require generating array of indices, which is again non-trivial task for me. One more note: I don not want to use any python functions via tf.py_func, as I checked that they run on CPU only. Maybe lambda and tf.map_fn may help somehow? Thanks!\n\nImprove this question\n\nedited Jul 10, 2017 at 19:40\n\n22.8k77 gold badges8585 silver badges103103 bronze badges\n\nasked Jul 6, 2017 at 15:15\n\n1,05911 gold badge1313 silver badges3636 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nTo interleave two matrices vertically, you do not big guns such as gather or map_fn. You can simply interleave them as follows:\n\ntf.reshape( tf.stack([even_new, odd_new], axis=1), [-1, tf.shape(even_new)[1]])\n\nTo interleave them horizontally:\n\ntf.reshape( tf.concat([even_new[...,tf.newaxis], odd_new[...,tf.newaxis]], axis=-1), [tf.shape(even_new)[0],-1])\n\nThe idea is to use stack to interleave them in memory. The dimension where the stack occurs gives the granularity of the interleaving. If we stack at axis=0, then the interleaving occurs at each element, mixing columns. If we stack at axis=1, entire input rows remain contiguous, interleaving occurs between rows.\n\nedited Jul 10, 2017 at 19:43\n\nanswered Jul 6, 2017 at 15:47\n\n22.8k77 gold badges8585 silver badges103103 bronze badges 7\n\nThanks very much! But this code produces tensor [200,128] instead of [100,256]. I\'ve changed it to y = tf.reshape(tf.stack([even_new, odd_new], axis=0), [tf.shape(even_new)[0],-1]) so the output is as expected. Can you please provide me a little explanation why it indeed places even and odd elements where needed?\n\n– Slowpoke Jul 6, 2017 at 15:56\n\nAs far, as I understand, it stacks them vertically and then does reshape that should place elements that lay under each other together horizontally.\n\n– Slowpoke Jul 6, 2017 at 16:06\n\nYes, I followed your numpy example, which also stacks tensors vertically (along the first dimension). Your modification to stack them horizontally is right.\n\n– P-Gn Jul 6, 2017 at 16:12\n\nOh, sorry - that was typo, I meant y[:, ::2] and y[:,1::2] !\n\n– Slowpoke Jul 6, 2017 at 16:19\n\n@Slowpoke sorry you are right, I should have checked your formula better. I propose you a new formula instead, that is not based on transpose, which is somewhat of a heavyweight operation. Tell me how it goes.\n\n– P-Gn Jul 10, 2017 at 19:38\n\n | Show 2 more comments\n\nyou can use tf.dynamic_stitch, that takes as first argument a list of tensors of indices for each tensor to interleave and as second argument a list of tensors to interleave. The tensors will be interleaved along the first dimension so we need to transpose them and then transpose back. Here is the code:\n\neven_new = tf.transpose(even_new,perm=[1,0]) odd_new = tf.transpose(odd_new,perm=[1,0]) even_pos = tf.convert_to_tensor(list(range(0,256,2)),dtype=tf.int32) odd_pos = tf.convert_to_tensor(list(range(1,256,2)),dtype=tf.int32) interleaved = tf.dynamic_stitch([even_pos,odd_pos],[even_new,odd_new]) interleaved = tf.transpose(interleaved,perm=[1,0])\n\nanswered Dec 17, 2020 at 11:06\n\n18111 silver badge44 bronze badges\n\nYou can use assign to assign into slices.\n\nodd_new = tf.constant([1,3,5]) even_new = tf.constant([2,4,6]) y=tf.Variable(tf.zeros(6, dtype=tf.int32)) sess = tf.InteractiveSession() sess.run(tf.global_variables_initializer()) y[0::2].assign(odd_new).eval() y[1::2].assign(even_new).eval()\n\nanswered Jul 6, 2017 at 16:23\n\nManolo SantosManolo Santos\n\n1,90511 gold badge1414 silver badges2525 bronze badges 1\n\nThanks very much! I initially tried to construct my network using assign operators, but faced some problems and was discouraged from doing it (however, I think that code was correct), so now I prefer using embedded tensorflow functions, so the first answer suits me. But thanks nevertheless!\n\n– Slowpoke Jul 6, 2017 at 16:27\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nFor those who just don’t Git it (Ep. 573)\n\nHow to use marketing techniques to build a better resume\n\nAI/ML Tool examples part 3 - Title-Drafting Assistant\n\nWe are graduating the updated button styling for vote arrows\n\nTemporary policy: ChatGPT is banned\n\nThe [connect] tag is being burninated\n\nStack Overflow will be testing a title-drafting assistant, and we’d like your...\n\nWe are graduating the ""Related questions using Machine Learning"" experiment\n\n62 Adjust Single Value within Tensor -- TensorFlow\n\n5 Tensorflow: How to tile a tensor that duplicate in certain order?\n\n2 Tensorflow: assign multiple variable values in single run without recomputation of other expressions\n\n0 Tensorflow: Interlieving two ragged tensors\n\n1 Keras Custom Merge Two Tensors\n\n2 Concatenate two tensors in alternate fashion (Tensorflow)\n\n3 tensorflow merge and zip two tensors\n\n2 How to merge two tensor?\n\n4 How to concatenate two tensors having different shape with TensorFlow?\n\n1 Concat two 2D tensors\n\n2 How can I merge two 3D tensors by interleaving them along a certain axis?\n\n2 Merge two tensor in pytorch\n\n3 Combine arbitrary shaped tensors\n\n3 Merge one tensor into other tensor on specific indexes in PyTorch\n\nHot Network Questions\n\nMTB rear wheel selection\n\nCreate line-type symbology perpendicular to a segment in QGIS\n\nMy girlfriend helped me buy a car - Who is responsible if she takes if back?\n\nWhich IP address would be most safe and suitable to use as a placeholder in a live system?\n\nTeaching high school math between PhD and postdoc\n\nWhy are there two inverses to exponentiation?\n\nHow does social inequality differ from wealth inequality? (In a society with capitalism esque ideals)\n\nWhat does the mention of Israel suggest here in Greenmantle?\n\nWhat kind of heating would occur during a suborbital re-entry?\n\nWhat does the ""proportional symbol"" (∝) mean when used as a time unit prefix?\n\nHow does 一つとして work in this sentence?\n\nWhat is the basic difference between curry powder and garam masala?\n\nHow to list only tar files\n\nBook/book series where the main character\'s mind is copied into different alien clones\n\nPlease help to beautify this connection-graph\n\nWhy don\'t my hover tanks fly?\n\nWhat are the implications of the recent finding in Vatican Library in relation to Matt 12:1?\n\nWhat does \'length\' means?\n\nHow serious is it that my new teammates didn\'t show up to a meeting I set up that they agreed to attend?\n\nI will grab a taxi back. vs. I will grab back a taxi\n\nLoading render kernels - what exactly that means?\n\nAre two localized single-photon states always invariant under the particle exchange?\n\nWhat\'s the secret to well-seasoned pulled pork?\n\nYield curve bootstrapping not producing expected cash flow start date more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', 'timestamp': '2024-01-13T10:02:59', 'title': 'python - Tensorflow: Merge two 2-D tensors according to even and odd indices - Stack Overflow', 'url': 'https://stackoverflow.com/questions/44952886/tensorflow-merge-two-2-d-tensors-according-to-even-and-odd-indices'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\ntensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)?\n\nAsked 4 years, 5 months ago\n\nModified 4 years, 5 months ago\n\nI\'ve read the tf.scatter_nd documentation and run the example code for 1D and 3D tensors... and now I\'m trying to do it for a 2D tensor. I want to \'interleave\' the columns of two tensors. For 1D tensors, one can do this via\n\n\'\'\' We want to interleave elements of 1D tensors arr1 and arr2, where arr1 = [10, 11, 12] arr2 = [1, 2, 3, 4, 5, 6] such that desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12] \'\'\' import tensorflow as tf with tf.Session() as sess: updates1 = tf.constant([1,2,3,4,5,6]) indices1 = tf.constant([[0], [1], [3], [4], [6], [7]]) shape = tf.constant([9]) scatter1 = tf.scatter_nd(indices1, updates1, shape) updates2 = tf.constant([10,11,12]) indices2 = tf.constant([[2], [5], [8]]) scatter2 = tf.scatter_nd(indices2, updates2, shape) result = scatter1 + scatter2 print(sess.run(result))\n\n(aside: is there a better way to do this? I\'m all ears.)\n\nThis gives the output\n\n[ 1 2 10 3 4 11 5 6 12]\n\nNow lets\' try to extend this to 2D.\n\n\'\'\' We want to interleave the *columns* (not rows; rows would be easy!) of arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]] arr2 = [[10 11 12], [10 11 12], [10 11 12]] such that desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]] \'\'\' updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]) indices1 = tf.constant([[0], [1], [3], [4], [6], [7]]) shape = tf.constant([3, 9]) scatter1 = tf.scatter_nd(indices1, updates1, shape)\n\nThis gives the error ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1 dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but are 6 and 3. Shapes are [6] and [3]. for \'ScatterNd_2\' (op: \'ScatterNd\') with input shapes: [6,1], [3,6], [2].\n\nSeems like my indices is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean I need to explicitly specify every single pair of indices for every element in updates1? Or is there some kind of \'wildcard\' specification I can use for the rows? (Note indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]]) gives syntax errors, as it probably should.)\n\nWould it be easier to just do a transpose, interleave the rows, then transpose back? Because I tried that...\n\nscatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape)) print(sess.run(tf.transpose(scatter1)))\n\n...and got a much longer error message, that I don\'t feel like posting unless someone requests it.\n\nPS- I searched to make sure this isn\'t a duplicate -- I find it hard to imagine that someone else hasn\'t asked this before -- but turned up nothing.\n\nImprove this question\n\nedited Sep 30, 2018 at 18:41\n\nasked Sep 29, 2018 at 20:24\n\n1,31011 gold badge1515 silver badges3636 bronze badges 2\n\nOk, the following lines actually work, but I have no idea why... shape = tf.constant([9, 3]), scatter1 = tf.transpose(tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))) ...Particularly strange that I have to define shape as [9,3] and take its transpose, whereas just defining it as [3,9] and using it that way gives an error.\n\n– sh37211 Sep 29, 2018 at 20:41\n\nI would have thought I need to use a shape of [9,3] if I\'m using the traspose, or else define [3,9] and then take its transpose. ? ...So, while I now have \'working code\', I\'d rather not \'answer my own question\': If you can either explain why this is necessary, or offer a better way to do what I want to do, then the prize is yours! ;-)\n\n– sh37211 Sep 29, 2018 at 20:49\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThis is pure slicing but I didn\'t know that syntax like arr1[0:,:][:,:2] actually works. It seems it does but not sure if it is better.\n\nThis may be the wildcard slicing mechanism you are looking for.\n\narr1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,7],[1,2,3,4,5,8]]) arr2 = tf.constant([[10, 11, 12], [10, 11, 12], [10, 11, 12]]) with tf.Session() as sess : sess.run( tf.global_variables_initializer() ) print(sess.run(tf.concat([arr1[0:,:][:,:2], arr2[0:,:] [:,:1], arr1[0:,:][:,2:4],arr2[0:, :][:, 1:2], arr1[0:,:][:,4:6],arr2[0:, :][:, 2:3]],axis=1)))\n\n[[ 1 2 10 3 4 11 5 6 12] [ 1 2 10 3 4 11 5 7 12] [ 1 2 10 3 4 11 5 8 12]]\n\n[[1 2 3 4 5 6] [1 2 3 4 5 7] [1 2 3 4 5 8]]\n\nand arr1[0:,:][:,:2] returns the first two columns\n\nedited Sep 30, 2018 at 5:31\n\nanswered Sep 30, 2018 at 5:25\n\nMohan RadhakrishnanMohan Radhakrishnan\n\n2,94255 gold badges2828 silver badges4242 bronze badges 1\n\nThanks! I was using scatter_nd instead of concatenate because I need the solution to scale up to hundreds of columns, which I can\'t count on being able to specify ""by hand"". Still, if there\'s a way to make this ""scale"", i.e. programatically specifying the columns (without a hundred concat operations which would be slow), then this answer wins. I also hit upon a different (non-scatter_nd) answer using permutation matrices, which I\'ll post in a bit...\n\n– sh37211 Sep 30, 2018 at 18:13\n\nSome moderators might have regarded my question as a duplicate of this one, not because the questions are the same, but only because the answers contain parts one can use to answer this question -- i.e. specifying every index combination by hand.\n\nA totally different method would be to multiply by a permutation matrix as shown in the last answer to this question. Since my original question was about scatter_nd, I\'m going to post this solution but wait to see what other answers come in... (Alternatively, I or someone could edit the question to make it about reordering columns, not specific to scatter_nd --EDIT: I have just edited the question title to reflect this).\n\nHere, we concatenate the two different arrays/tensors...\n\nimport numpy as np import tensorflow as tf sess = tf.Session() # the ultimate application is for merging variables which should be in groups, # e.g. in this example, [1,2,10] is a group of 3, and there are 3 groups of 3 n_groups = 3 vars_per_group = 3 # once the single value from arr2 (below) is included arr1 = 10+tf.range(n_groups, dtype=float) arr1 = tf.stack((arr1,arr1,arr1),0) arr2 = 1+tf.range(n_groups * (vars_per_group-1), dtype=float) arr2 = tf.stack((arr2,arr2,arr2),0) catted = tf.concat((arr1,arr2),1) # concatenate the two arrays together print(""arr1 = \\n"",sess.run(arr1)) print(""arr2 = \\n"",sess.run(arr2)) print(""catted = \\n"",sess.run(catted))\n\narr1 = [[10. 11. 12.] [10. 11. 12.] [10. 11. 12.]] arr2 = [[1. 2. 3. 4. 5. 6.] [1. 2. 3. 4. 5. 6.] [1. 2. 3. 4. 5. 6.]] catted = [[10. 11. 12. 1. 2. 3. 4. 5. 6.] [10. 11. 12. 1. 2. 3. 4. 5. 6.] [10. 11. 12. 1. 2. 3. 4. 5. 6.]]\n\nNow we build the permutation matrix and multiply...\n\nstart_index = 2 # location of where the interleaving begins # cml = ""column map list"" is the list of where each column will get mapped to cml = [start_index + x*(vars_per_group) for x in range(n_groups)] # first array for i in range(n_groups): # second array cml += [x + i*(vars_per_group) for x in range(start_index)] # vars before start_index cml += [1 + x + i*(vars_per_group) + start_index \\ for x in range(vars_per_group-start_index-1)] # vars after start_index print(""\\n cml = "",cml,""\\n"") # Create a permutation matrix using p np_perm_mat = np.zeros((len(cml), len(cml))) for idx, i in enumerate(cml): np_perm_mat[idx, i] = 1 perm_mat = tf.constant(np_perm_mat,dtype=float) result = tf.matmul(catted, perm_mat) print(""result = \\n"",sess.run(result))\n\ncml = [2, 5, 8, 0, 1, 3, 4, 6, 7] result = [[ 1. 2. 10. 3. 4. 11. 5. 6. 12.] [ 1. 2. 10. 3. 4. 11. 5. 6. 12.] [ 1. 2. 10. 3. 4. 11. 5. 6. 12.]]\n\nEven though this doesn\'t use scatter_nd as the original question asked, one thing I like about this is, you can allocate the perm_mat once in some __init__() method, and hang on to it, and after that initial overhead it\'s just matrix-matrix multiplication by a sparse, constant matrix, which should be pretty fast. (?)\n\nStill happy to wait and see what other answers might come in.\n\nedited Sep 30, 2018 at 18:42\n\nanswered Sep 30, 2018 at 18:24\n\n1,31011 gold badge1515 silver badges3636 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nHow Intuit democratizes AI development across teams through reusability\n\nThe nature of simulating nature: A Q&A with IBM Quantum researcher Dr. Jamie...\n\nWe\'ve added a ""Necessary cookies only"" option to the cookie consent popup\n\nLaunching the CI/CD and R Collectives and community editing features for...\n\nThe [amazon] tag is being burninated\n\nTemporary policy: ChatGPT is banned\n\nStaging Ground Beta 1 Recap, and Reviewers needed for Beta 2\n\n78 Rearrange columns of numpy 2D array\n\n1 Swapping elements within a matrix rows and columns - TensorFlow scatter_nd\n\n6640 How do I merge two dictionaries in a single expression in Python?\n\n3244 How do I concatenate two lists in Python?\n\n2304 How do I escape curly-brace ({}) characters in a string while using .format (or an f-string)?\n\n1518 How to change the order of DataFrame columns?\n\n909 Combine two columns of text in pandas dataframe\n\n698 TensorFlow not found using pip\n\n101 How to get Tensorflow tensor dimensions (shape) as int values?\n\n8 Tensorflow on MacOS: Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n\n2 tensorflow scatter_nd with empty tensors?\n\n0 Tensorflow: Interlieving two ragged tensors\n\nHot Network Questions\n\nhow to fix object creating a duplicate of itself during animation?\n\nImtiaz Germain Primes\n\nWhat sort of strategies would a medieval military use against a fantasy giant?\n\nAre there any other options to mitigate the Volley trait?\n\nCovering of a knot complement\n\nShould I ask why they are interested in me in an interview for a faculty position?\n\nHow to Fix my DIY Smart Switch Install\n\nWill DL360 Gen 8 servers fit in rails designed for DL360 Gen 7 servers?\n\nExtracting N elements of the table satisfying the given condition\n\nLagrange Points in General Relativity\n\nGoogle maps for Space exploration\n\nWhy is there a voltage on my HDMI and coaxial cables?\n\nstd::to_array for multi dimensional array\n\nWhy does Mister Mxyzptlk need to have a weakness in the comics?\n\nCan carbocations exist in a nonpolar solvent?\n\nBlender python - set location does not use centimeters\n\nLoose bottom bracket on MTB\n\nNew owner of large lawn, struggling to demoss & deweed\n\nWhy do we calculate the second half of frequencies in DFT?\n\nExample of trickiness of finite lattice representation problem?\n\n""Is"" or ""are"" for two uncountable words?\n\nWhat would be the advantage of launching an UN-led inquiry over the Nord Stream sabotage?\n\nLinear Algebra - Linear transformation question\n\nAs (It) Was Explained to You more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', metadata={'id': 'web-search_0', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\ntensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)?\n\nAsked 4 years, 5 months ago\n\nModified 4 years, 5 months ago\n\nI\'ve read the tf.scatter_nd documentation and run the example code for 1D and 3D tensors... and now I\'m trying to do it for a 2D tensor. I want to \'interleave\' the columns of two tensors. For 1D tensors, one can do this via\n\n\'\'\' We want to interleave elements of 1D tensors arr1 and arr2, where arr1 = [10, 11, 12] arr2 = [1, 2, 3, 4, 5, 6] such that desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12] \'\'\' import tensorflow as tf with tf.Session() as sess: updates1 = tf.constant([1,2,3,4,5,6]) indices1 = tf.constant([[0], [1], [3], [4], [6], [7]]) shape = tf.constant([9]) scatter1 = tf.scatter_nd(indices1, updates1, shape) updates2 = tf.constant([10,11,12]) indices2 = tf.constant([[2], [5], [8]]) scatter2 = tf.scatter_nd(indices2, updates2, shape) result = scatter1 + scatter2 print(sess.run(result))\n\n(aside: is there a better way to do this? I\'m all ears.)\n\nThis gives the output\n\n[ 1 2 10 3 4 11 5 6 12]\n\nNow lets\' try to extend this to 2D.\n\n\'\'\' We want to interleave the *columns* (not rows; rows would be easy!) of arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]] arr2 = [[10 11 12], [10 11 12], [10 11 12]] such that desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]] \'\'\' updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]) indices1 = tf.constant([[0], [1], [3], [4], [6], [7]]) shape = tf.constant([3, 9]) scatter1 = tf.scatter_nd(indices1, updates1, shape)\n\nThis gives the error ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1 dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but are 6 and 3. Shapes are [6] and [3]. for \'ScatterNd_2\' (op: \'ScatterNd\') with input shapes: [6,1], [3,6], [2].\n\nSeems like my indices is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean I need to explicitly specify every single pair of indices for every element in updates1? Or is there some kind of \'wildcard\' specification I can use for the rows? (Note indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]]) gives syntax errors, as it probably should.)\n\nWould it be easier to just do a transpose, interleave the rows, then transpose back? Because I tried that...\n\nscatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape)) print(sess.run(tf.transpose(scatter1)))\n\n...and got a much longer error message, that I don\'t feel like posting unless someone requests it.\n\nPS- I searched to make sure this isn\'t a duplicate -- I find it hard to imagine that someone else hasn\'t asked this before -- but turned up nothing.\n\nImprove this question\n\nedited Sep 30, 2018 at 18:41\n\nasked Sep 29, 2018 at 20:24\n\n1,31011 gold badge1515 silver badges3636 bronze badges 2\n\nOk, the following lines actually work, but I have no idea why... shape = tf.constant([9, 3]), scatter1 = tf.transpose(tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))) ...Particularly strange that I have to define shape as [9,3] and take its transpose, whereas just defining it as [3,9] and using it that way gives an error.\n\n– sh37211 Sep 29, 2018 at 20:41\n\nI would have thought I need to use a shape of [9,3] if I\'m using the traspose, or else define [3,9] and then take its transpose. ? ...So, while I now have \'working code\', I\'d rather not \'answer my own question\': If you can either explain why this is necessary, or offer a better way to do what I want to do, then the prize is yours! ;-)\n\n– sh37211 Sep 29, 2018 at 20:49\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThis is pure slicing but I didn\'t know that syntax like arr1[0:,:][:,:2] actually works. It seems it does but not sure if it is better.\n\nThis may be the wildcard slicing mechanism you are looking for.\n\narr1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,7],[1,2,3,4,5,8]]) arr2 = tf.constant([[10, 11, 12], [10, 11, 12], [10, 11, 12]]) with tf.Session() as sess : sess.run( tf.global_variables_initializer() ) print(sess.run(tf.concat([arr1[0:,:][:,:2], arr2[0:,:] [:,:1], arr1[0:,:][:,2:4],arr2[0:, :][:, 1:2], arr1[0:,:][:,4:6],arr2[0:, :][:, 2:3]],axis=1)))\n\n[[ 1 2 10 3 4 11 5 6 12] [ 1 2 10 3 4 11 5 7 12] [ 1 2 10 3 4 11 5 8 12]]\n\n[[1 2 3 4 5 6] [1 2 3 4 5 7] [1 2 3 4 5 8]]\n\nand arr1[0:,:][:,:2] returns the first two columns\n\nedited Sep 30, 2018 at 5:31\n\nanswered Sep 30, 2018 at 5:25\n\nMohan RadhakrishnanMohan Radhakrishnan\n\n2,94255 gold badges2828 silver badges4242 bronze badges 1\n\nThanks! I was using scatter_nd instead of concatenate because I need the solution to scale up to hundreds of columns, which I can\'t count on being able to specify ""by hand"". Still, if there\'s a way to make this ""scale"", i.e. programatically specifying the columns (without a hundred concat operations which would be slow), then this answer wins. I also hit upon a different (non-scatter_nd) answer using permutation matrices, which I\'ll post in a bit...\n\n– sh37211 Sep 30, 2018 at 18:13\n\nSome moderators might have regarded my question as a duplicate of this one, not because the questions are the same, but only because the answers contain parts one can use to answer this question -- i.e. specifying every index combination by hand.\n\nA totally different method would be to multiply by a permutation matrix as shown in the last answer to this question. Since my original question was about scatter_nd, I\'m going to post this solution but wait to see what other answers come in... (Alternatively, I or someone could edit the question to make it about reordering columns, not specific to scatter_nd --EDIT: I have just edited the question title to reflect this).\n\nHere, we concatenate the two different arrays/tensors...\n\nimport numpy as np import tensorflow as tf sess = tf.Session() # the ultimate application is for merging variables which should be in groups, # e.g. in this example, [1,2,10] is a group of 3, and there are 3 groups of 3 n_groups = 3 vars_per_group = 3 # once the single value from arr2 (below) is included arr1 = 10+tf.range(n_groups, dtype=float) arr1 = tf.stack((arr1,arr1,arr1),0) arr2 = 1+tf.range(n_groups * (vars_per_group-1), dtype=float) arr2 = tf.stack((arr2,arr2,arr2),0) catted = tf.concat((arr1,arr2),1) # concatenate the two arrays together print(""arr1 = \\n"",sess.run(arr1)) print(""arr2 = \\n"",sess.run(arr2)) print(""catted = \\n"",sess.run(catted))\n\narr1 = [[10. 11. 12.] [10. 11. 12.] [10. 11. 12.]] arr2 = [[1. 2. 3. 4. 5. 6.] [1. 2. 3. 4. 5. 6.] [1. 2. 3. 4. 5. 6.]] catted = [[10. 11. 12. 1. 2. 3. 4. 5. 6.] [10. 11. 12. 1. 2. 3. 4. 5. 6.] [10. 11. 12. 1. 2. 3. 4. 5. 6.]]\n\nNow we build the permutation matrix and multiply...\n\nstart_index = 2 # location of where the interleaving begins # cml = ""column map list"" is the list of where each column will get mapped to cml = [start_index + x*(vars_per_group) for x in range(n_groups)] # first array for i in range(n_groups): # second array cml += [x + i*(vars_per_group) for x in range(start_index)] # vars before start_index cml += [1 + x + i*(vars_per_group) + start_index \\ for x in range(vars_per_group-start_index-1)] # vars after start_index print(""\\n cml = "",cml,""\\n"") # Create a permutation matrix using p np_perm_mat = np.zeros((len(cml), len(cml))) for idx, i in enumerate(cml): np_perm_mat[idx, i] = 1 perm_mat = tf.constant(np_perm_mat,dtype=float) result = tf.matmul(catted, perm_mat) print(""result = \\n"",sess.run(result))\n\ncml = [2, 5, 8, 0, 1, 3, 4, 6, 7] result = [[ 1. 2. 10. 3. 4. 11. 5. 6. 12.] [ 1. 2. 10. 3. 4. 11. 5. 6. 12.] [ 1. 2. 10. 3. 4. 11. 5. 6. 12.]]\n\nEven though this doesn\'t use scatter_nd as the original question asked, one thing I like about this is, you can allocate the perm_mat once in some __init__() method, and hang on to it, and after that initial overhead it\'s just matrix-matrix multiplication by a sparse, constant matrix, which should be pretty fast. (?)\n\nStill happy to wait and see what other answers might come in.\n\nedited Sep 30, 2018 at 18:42\n\nanswered Sep 30, 2018 at 18:24\n\n1,31011 gold badge1515 silver badges3636 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nHow Intuit democratizes AI development across teams through reusability\n\nThe nature of simulating nature: A Q&A with IBM Quantum researcher Dr. Jamie...\n\nWe\'ve added a ""Necessary cookies only"" option to the cookie consent popup\n\nLaunching the CI/CD and R Collectives and community editing features for...\n\nThe [amazon] tag is being burninated\n\nTemporary policy: ChatGPT is banned\n\nStaging Ground Beta 1 Recap, and Reviewers needed for Beta 2\n\n78 Rearrange columns of numpy 2D array\n\n1 Swapping elements within a matrix rows and columns - TensorFlow scatter_nd\n\n6640 How do I merge two dictionaries in a single expression in Python?\n\n3244 How do I concatenate two lists in Python?\n\n2304 How do I escape curly-brace ({}) characters in a string while using .format (or an f-string)?\n\n1518 How to change the order of DataFrame columns?\n\n909 Combine two columns of text in pandas dataframe\n\n698 TensorFlow not found using pip\n\n101 How to get Tensorflow tensor dimensions (shape) as int values?\n\n8 Tensorflow on MacOS: Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n\n2 tensorflow scatter_nd with empty tensors?\n\n0 Tensorflow: Interlieving two ragged tensors\n\nHot Network Questions\n\nhow to fix object creating a duplicate of itself during animation?\n\nImtiaz Germain Primes\n\nWhat sort of strategies would a medieval military use against a fantasy giant?\n\nAre there any other options to mitigate the Volley trait?\n\nCovering of a knot complement\n\nShould I ask why they are interested in me in an interview for a faculty position?\n\nHow to Fix my DIY Smart Switch Install\n\nWill DL360 Gen 8 servers fit in rails designed for DL360 Gen 7 servers?\n\nExtracting N elements of the table satisfying the given condition\n\nLagrange Points in General Relativity\n\nGoogle maps for Space exploration\n\nWhy is there a voltage on my HDMI and coaxial cables?\n\nstd::to_array for multi dimensional array\n\nWhy does Mister Mxyzptlk need to have a weakness in the comics?\n\nCan carbocations exist in a nonpolar solvent?\n\nBlender python - set location does not use centimeters\n\nLoose bottom bracket on MTB\n\nNew owner of large lawn, struggling to demoss & deweed\n\nWhy do we calculate the second half of frequencies in DFT?\n\nExample of trickiness of finite lattice representation problem?\n\n""Is"" or ""are"" for two uncountable words?\n\nWhat would be the advantage of launching an UN-led inquiry over the Nord Stream sabotage?\n\nLinear Algebra - Linear transformation question\n\nAs (It) Was Explained to You more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', 'timestamp': '2023-03-05T22:52:20', 'title': 'python - tensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/52572275/tensorflow-how-to-interleave-columns-of-two-tensors-e-g-using-tf-scatter-nd'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow do I select certain columns of a 2D tensor in TensorFlow?\n\nAsked 7 years, 11 months ago\n\nModified 4 years, 10 months ago\n\nAs generalized slicing is being worked on in this issue, what would be the best way to achieve an op gathering columns of a 2D tensor (matrix)? For example, for tensor t:\n\nand indices [1,3], I would like to get:\n\nwhich is equivalent to numpy t[:, [1,3]].\n\nImprove this question\n\nedited Jul 6, 2018 at 12:50\n\n15.8k3434 gold badges116116 silver badges208208 bronze badges\n\nasked Jun 7, 2016 at 5:05\n\nAndrzej PronobisAndrzej Pronobis\n\n35.1k1818 gold badges7979 silver badges9292 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nMeanwhile the gather method has an axis parameter.\n\nimport tensorflow as tf params = tf.constant([[1,2,3],[4,5,6]]) indices = [0,2] op = tf.gather(params, indices, axis=1)\n\nanswered Jul 25, 2019 at 8:40\n\nAlexConfusedAlexConfused\n\n82111 gold badge1010 silver badges1616 bronze badges\n\nThere is a function named tf.nn.embedding_lookup(params, ind) which retrieves the rows of the params tensor.\n\nTo achieve what you want, we can first transpose the tensor t from which you want to select certain columns from. Then look up the rows of tf.transpose(t) (columns of t). After the selection, we transpose the result back.\n\nimport tensorflow as tf t = tf.constant([[1, 2, 3], [4, 5, 6]]) ind = tf.constant([0, 2]) result = tf.transpose(tf.nn.embedding_lookup(tf.transpose(t), ind)) with tf.Session() as sess: print(sess.run(result))\n\nedited Jul 6, 2018 at 12:49\n\n15.8k3434 gold badges116116 silver badges208208 bronze badges\n\nanswered Jun 7, 2016 at 11:35\n\n98477 silver badges1010 bronze badges 1\n\nWhy not just using gather if you want to transpose? I though that transposing is expensive in TF.\n\n– Andrzej Pronobis Jun 7, 2016 at 17:30\n\nSo far, I created a workaround by flattening the input and using gather:\n\ndef gather_cols(params, indices, name=None): """"""Gather columns of a 2D tensor. Args: params: A 2D tensor. indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``. name: A name for the operation (optional). Returns: A 2D Tensor. Has the same type as ``params``. """""" with tf.op_scope([params, indices], name, ""gather_cols"") as scope: # Check input params = tf.convert_to_tensor(params, name=""params"") indices = tf.convert_to_tensor(indices, name=""indices"") try: params.get_shape().assert_has_rank(2) except ValueError: raise ValueError(\'\\\'params\\\' must be 2D.\') try: indices.get_shape().assert_has_rank(1) except ValueError: raise ValueError(\'\\\'indices\\\' must be 1D.\') # Define op p_shape = tf.shape(params) p_flat = tf.reshape(params, [-1]) i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1], [-1, 1]) + indices, [-1]) return tf.reshape(tf.gather(p_flat, i_flat), [p_shape[0], -1])\n\nparams = tf.constant([[1, 2, 3], [4, 5, 6]]) indices = [0, 2] op = gather_cols(params, indices)\n\nproduces the expected output:\n\nedited Feb 2, 2019 at 15:55\n\n5,85544 gold badges4444 silver badges7575 bronze badges\n\nanswered Jun 7, 2016 at 5:39\n\nAndrzej PronobisAndrzej Pronobis\n\n35.1k1818 gold badges7979 silver badges9292 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nAn open-source development paradigm\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe 2024 Developer Survey Is Live\n\nThe return of Staging Ground to Stack Overflow\n\n174 What does tf.nn.embedding_lookup function do?\n\n153 What does tf.nn.conv2d do in tensorflow?\n\n715 TensorFlow not found using pip\n\n38 TensorFlow: Max of a tensor along an axis\n\n35 In TensorFlow, how can I get nonzero values and their indices from a tensor with python?\n\n5 Slicing Tensorflow Tensor with Tensor\n\n31 TensorFlow - numpy-like tensor indexing\n\n1 Convert tensor of (row, column) coordinates to boolean mask in TensorFlow\n\n335 Convert a tensor to numpy array in Tensorflow?\n\nHot Network Questions\n\nHow to handle a collaborator who doesn\'t work?\n\nHow to straighten new coiled Ethernet cord?\n\nWhy zero-pad at the end of a signal?\n\nMisleading readers on the motives/true personality of a character\n\nAdult vs. Kids BMX Sizes\n\nA client did an unannounced penetration test on our platform\n\nAre there any advantages of evaluating expressions differently in compile time and runtime?\n\nWhat should I do with my 60 page undergraduate thesis?\n\nI would like to understand Michaelis-Menten law\n\nHow can I learn the intuition behind the proofs of theorems in Graph Theory? They all seem like random algorithms that just happen to work\n\nThe Austrian government took my daughter\'s US driver\'s license\n\nThe Ultimate Battle of two players\n\nSimple size difference measuring tool\n\nA Sea of Mist and Steam? How would boats move?\n\nIs mindfulness of feeling the middle way?\n\nExponentials of Truth Values\n\nMy use case diagram is a mess. What can I do?\n\nWhy can\'t the water companies go bankrupt?\n\nUsing zmv to batch rename files and resolve filename collisions\n\nHow can we understand the Tower of Babel timeline?\n\nTikZ: Why can\'t I use saved path via spath3 with `local bounding box` set\n\nIs DVI used as an intermediate format by pdfLaTeX and LuaLaTeX?\n\nHilbert\'s Satz 90 for real simply-connected groups? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_5', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nShare Your Experience: Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow do I select certain columns of a 2D tensor in TensorFlow?\n\nAsked 7 years, 11 months ago\n\nModified 4 years, 10 months ago\n\nAs generalized slicing is being worked on in this issue, what would be the best way to achieve an op gathering columns of a 2D tensor (matrix)? For example, for tensor t:\n\nand indices [1,3], I would like to get:\n\nwhich is equivalent to numpy t[:, [1,3]].\n\nImprove this question\n\nedited Jul 6, 2018 at 12:50\n\n15.8k3434 gold badges116116 silver badges208208 bronze badges\n\nasked Jun 7, 2016 at 5:05\n\nAndrzej PronobisAndrzej Pronobis\n\n35.1k1818 gold badges7979 silver badges9292 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nMeanwhile the gather method has an axis parameter.\n\nimport tensorflow as tf params = tf.constant([[1,2,3],[4,5,6]]) indices = [0,2] op = tf.gather(params, indices, axis=1)\n\nanswered Jul 25, 2019 at 8:40\n\nAlexConfusedAlexConfused\n\n82111 gold badge1010 silver badges1616 bronze badges\n\nThere is a function named tf.nn.embedding_lookup(params, ind) which retrieves the rows of the params tensor.\n\nTo achieve what you want, we can first transpose the tensor t from which you want to select certain columns from. Then look up the rows of tf.transpose(t) (columns of t). After the selection, we transpose the result back.\n\nimport tensorflow as tf t = tf.constant([[1, 2, 3], [4, 5, 6]]) ind = tf.constant([0, 2]) result = tf.transpose(tf.nn.embedding_lookup(tf.transpose(t), ind)) with tf.Session() as sess: print(sess.run(result))\n\nedited Jul 6, 2018 at 12:49\n\n15.8k3434 gold badges116116 silver badges208208 bronze badges\n\nanswered Jun 7, 2016 at 11:35\n\n98477 silver badges1010 bronze badges 1\n\nWhy not just using gather if you want to transpose? I though that transposing is expensive in TF.\n\n– Andrzej Pronobis Jun 7, 2016 at 17:30\n\nSo far, I created a workaround by flattening the input and using gather:\n\ndef gather_cols(params, indices, name=None): """"""Gather columns of a 2D tensor. Args: params: A 2D tensor. indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``. name: A name for the operation (optional). Returns: A 2D Tensor. Has the same type as ``params``. """""" with tf.op_scope([params, indices], name, ""gather_cols"") as scope: # Check input params = tf.convert_to_tensor(params, name=""params"") indices = tf.convert_to_tensor(indices, name=""indices"") try: params.get_shape().assert_has_rank(2) except ValueError: raise ValueError(\'\\\'params\\\' must be 2D.\') try: indices.get_shape().assert_has_rank(1) except ValueError: raise ValueError(\'\\\'indices\\\' must be 1D.\') # Define op p_shape = tf.shape(params) p_flat = tf.reshape(params, [-1]) i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1], [-1, 1]) + indices, [-1]) return tf.reshape(tf.gather(p_flat, i_flat), [p_shape[0], -1])\n\nparams = tf.constant([[1, 2, 3], [4, 5, 6]]) indices = [0, 2] op = gather_cols(params, indices)\n\nproduces the expected output:\n\nedited Feb 2, 2019 at 15:55\n\n5,85544 gold badges4444 silver badges7575 bronze badges\n\nanswered Jun 7, 2016 at 5:39\n\nAndrzej PronobisAndrzej Pronobis\n\n35.1k1818 gold badges7979 silver badges9292 bronze badges\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow or ask your own question.\n\nAn open-source development paradigm\n\nTesting a new version of Stack Overflow Jobs\n\nWhat deliverables would you like to see out of a working group?\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [price] tag is being burninated\n\nThe 2024 Developer Survey Is Live\n\nThe return of Staging Ground to Stack Overflow\n\n174 What does tf.nn.embedding_lookup function do?\n\n153 What does tf.nn.conv2d do in tensorflow?\n\n715 TensorFlow not found using pip\n\n38 TensorFlow: Max of a tensor along an axis\n\n35 In TensorFlow, how can I get nonzero values and their indices from a tensor with python?\n\n5 Slicing Tensorflow Tensor with Tensor\n\n31 TensorFlow - numpy-like tensor indexing\n\n1 Convert tensor of (row, column) coordinates to boolean mask in TensorFlow\n\n335 Convert a tensor to numpy array in Tensorflow?\n\nHot Network Questions\n\nHow to handle a collaborator who doesn\'t work?\n\nHow to straighten new coiled Ethernet cord?\n\nWhy zero-pad at the end of a signal?\n\nMisleading readers on the motives/true personality of a character\n\nAdult vs. Kids BMX Sizes\n\nA client did an unannounced penetration test on our platform\n\nAre there any advantages of evaluating expressions differently in compile time and runtime?\n\nWhat should I do with my 60 page undergraduate thesis?\n\nI would like to understand Michaelis-Menten law\n\nHow can I learn the intuition behind the proofs of theorems in Graph Theory? They all seem like random algorithms that just happen to work\n\nThe Austrian government took my daughter\'s US driver\'s license\n\nThe Ultimate Battle of two players\n\nSimple size difference measuring tool\n\nA Sea of Mist and Steam? How would boats move?\n\nIs mindfulness of feeling the middle way?\n\nExponentials of Truth Values\n\nMy use case diagram is a mess. What can I do?\n\nWhy can\'t the water companies go bankrupt?\n\nUsing zmv to batch rename files and resolve filename collisions\n\nHow can we understand the Tower of Babel timeline?\n\nTikZ: Why can\'t I use saved path via spath3 with `local bounding box` set\n\nIs DVI used as an intermediate format by pdfLaTeX and LuaLaTeX?\n\nHilbert\'s Satz 90 for real simply-connected groups? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-05-29T12:32:25', 'title': 'How do I select certain columns of a 2D tensor in TensorFlow? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/37670886/how-do-i-select-certain-columns-of-a-2d-tensor-in-tensorflow'})], [Document(page_content='You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nInstantly share code, notes, and snippets.\n\nlhoangan/tf-interleaving.py\n\nYou must be signed in to star a gist\n\nYou must be signed in to fork a gist\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/lhoangan/4d8a7d4a94ee5fb8d2c021b8f7631bcc.js&quot;&gt;&lt;/script&gt;\n\nSave lhoangan/4d8a7d4a94ee5fb8d2c021b8f7631bcc to your computer and use it in GitHub Desktop.\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/lhoangan/4d8a7d4a94ee5fb8d2c021b8f7631bcc.js&quot;&gt;&lt;/script&gt;\n\nSave lhoangan/4d8a7d4a94ee5fb8d2c021b8f7631bcc to your computer and use it in GitHub Desktop.\n\nInterleaving 2 or more arrays in tensorflow\n\nThis file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\nShow hidden characters\n\nimport tensorflow as tf\n\na = np.array([[1, 4, 7, 10], [11, 44, 77, 110]]) # shape (2, 4)\n\nb = np.array([[2, 5, 8, 11], [22, 55, 88, 111]]) # shape (2, 4)\n\nc = np.array([[3, 6, 9, 12], [33, 66, 99, 122]]) # shape (2, 4)\n\nA = tf.convert_to_tensor(a)\n\nB = tf.convert_to_tensor(b)\n\nC = tf.convert_to_tensor(c)\n\ntf.InteractiveSession()\n\nABC_hor = tf.reshape(T(tf.stack([T(A), T(B), T(C)], axis=0)), [-1,tf.shape(A)[1]*3])\n\n#array([[ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n\n# [ 11, 22, 33, 44, 55, 66, 77, 88, 99, 110, 111, 122]])\n\nABC_ver = tf.reshape(tf.stack([A, B, C], axis=-1), [tf.shape(A)[0]*3,-1])\n\n#Out: # This is wrong\n\n#array([[ 1, 2, 3, 4],\n\n# [ 11, 22, 33, 44],\n\n# [ 55, 66, 77, 88],\n\n# [ 99, 110, 111, 122]])\n\n# ABC = stack(a, b, c) # of shape 2, 4, 3\n\n# use transpose to move ""3"" to after the dimension to be interleave\n\n# then reshape to the final dimension\n\n# https://stackoverflow.com/questions/44952886/tensorflow-merge-two-2-d-tensors-according-to-even-and-odd-indices\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_0', 'snippet': 'You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nInstantly share code, notes, and snippets.\n\nlhoangan/tf-interleaving.py\n\nYou must be signed in to star a gist\n\nYou must be signed in to fork a gist\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/lhoangan/4d8a7d4a94ee5fb8d2c021b8f7631bcc.js&quot;&gt;&lt;/script&gt;\n\nSave lhoangan/4d8a7d4a94ee5fb8d2c021b8f7631bcc to your computer and use it in GitHub Desktop.\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/lhoangan/4d8a7d4a94ee5fb8d2c021b8f7631bcc.js&quot;&gt;&lt;/script&gt;\n\nSave lhoangan/4d8a7d4a94ee5fb8d2c021b8f7631bcc to your computer and use it in GitHub Desktop.\n\nInterleaving 2 or more arrays in tensorflow\n\nThis file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\nShow hidden characters\n\nimport tensorflow as tf\n\na = np.array([[1, 4, 7, 10], [11, 44, 77, 110]]) # shape (2, 4)\n\nb = np.array([[2, 5, 8, 11], [22, 55, 88, 111]]) # shape (2, 4)\n\nc = np.array([[3, 6, 9, 12], [33, 66, 99, 122]]) # shape (2, 4)\n\nA = tf.convert_to_tensor(a)\n\nB = tf.convert_to_tensor(b)\n\nC = tf.convert_to_tensor(c)\n\ntf.InteractiveSession()\n\nABC_hor = tf.reshape(T(tf.stack([T(A), T(B), T(C)], axis=0)), [-1,tf.shape(A)[1]*3])\n\n#array([[ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n\n# [ 11, 22, 33, 44, 55, 66, 77, 88, 99, 110, 111, 122]])\n\nABC_ver = tf.reshape(tf.stack([A, B, C], axis=-1), [tf.shape(A)[0]*3,-1])\n\n#Out: # This is wrong\n\n#array([[ 1, 2, 3, 4],\n\n# [ 11, 22, 33, 44],\n\n# [ 55, 66, 77, 88],\n\n# [ 99, 110, 111, 122]])\n\n# ABC = stack(a, b, c) # of shape 2, 4, 3\n\n# use transpose to move ""3"" to after the dimension to be interleave\n\n# then reshape to the final dimension\n\n# https://stackoverflow.com/questions/44952886/tensorflow-merge-two-2-d-tensors-according-to-even-and-odd-indices\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-05-10T04:36:58', 'title': 'Interleaving 2 or more arrays in tensorflow · GitHub', 'url': 'https://gist.github.com/lhoangan/4d8a7d4a94ee5fb8d2c021b8f7631bcc'})], [Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to Tensors\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport tensorflow as tf import numpy as np\n\n2023-10-28 01:21:58.219231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2023-10-28 01:21:58.219277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2023-10-28 01:21:58.220822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nTensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes at tf.dtypes.\n\nIf you\'re familiar with NumPy, tensors are (kind of) like np.arrays.\n\nAll tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.\n\nFirst, create some basic tensors.\n\nHere is a ""scalar"" or ""rank-0"" tensor . A scalar contains a single value, and no ""axes"".\n\n# This will be an int32 tensor by default; see ""dtypes"" below. rank_0_tensor = tf.constant(4) print(rank_0_tensor)\n\ntf.Tensor(4, shape=(), dtype=int32)\n\nA ""vector"" or ""rank-1"" tensor is like a list of values. A vector has one axis:\n\n# Let\'s make this a float tensor. rank_1_tensor = tf.constant([2.0, 3.0, 4.0]) print(rank_1_tensor)\n\ntf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n\nA ""matrix"" or ""rank-2"" tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time rank_2_tensor = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float16) print(rank_2_tensor)\n\ntf.Tensor( [[1. 2.] [3. 4.] [5. 6.]], shape=(3, 2), dtype=float16)\n\nA vector, shape: [3]\n\nA matrix, shape: [3, 2]\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of # axes (sometimes called ""dimensions"") rank_3_tensor = tf.constant([ [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]], [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]], [[20, 21, 22, 23, 24], [25, 26, 27, 28, 29]],]) print(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nThere are many ways you might visualize a tensor with more than two axes.\n\nA 3-axis tensor, shape: [3, 2, 5]\n\nYou can convert a tensor to a NumPy array either using np.array or the tensor.numpy method:\n\nnp.array(rank_2_tensor)\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nrank_2_tensor.numpy()\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nTensors often contain floats and ints, but have many other types, including:\n\nThe base tf.Tensor class requires tensors to be ""rectangular""---that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\n\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na = tf.constant([[1, 2], [3, 4]]) b = tf.constant([[1, 1], [1, 1]]) # Could have also said `tf.ones([2,2], dtype=tf.int32)` print(tf.add(a, b), ""\\n"") print(tf.multiply(a, b), ""\\n"") print(tf.matmul(a, b), ""\\n"")\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nprint(a + b, ""\\n"") # element-wise addition print(a * b, ""\\n"") # element-wise multiplication print(a @ b, ""\\n"") # matrix multiplication\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nTensors are used in all kinds of operations (or ""Ops"").\n\nc = tf.constant([[4.0, 5.0], [10.0, 1.0]]) # Find the largest value print(tf.reduce_max(c)) # Find the index of the largest value print(tf.math.argmax(c)) # Compute the softmax print(tf.nn.softmax(c))\n\ntf.Tensor(10.0, shape=(), dtype=float32) tf.Tensor([1 0], shape=(2,), dtype=int64) tf.Tensor( [[2.6894143e-01 7.3105854e-01] [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)\n\nNote: Typically, anywhere a TensorFlow function expects a Tensor as input, the function will also accept anything that can be converted to a Tensor using tf.convert_to_tensor. See below for an example.\n\ntf.convert_to_tensor([1,2,3])\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n\ntf.reduce_max([1,2,3])\n\n<tf.Tensor: shape=(), dtype=int32, numpy=3>\n\ntf.reduce_max(np.array([1,2,3]))\n\n<tf.Tensor: shape=(), dtype=int64, numpy=3>\n\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\n\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\n\nAxis or Dimension: A particular dimension of a tensor.\n\nSize: The total number of items in the tensor, the product of the shape vector\'s elements.\n\nNote: Although you may see reference to a ""tensor of two dimensions"", a rank-2 tensor does not usually describe a 2D space.\n\nTensors and tf.TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor = tf.zeros([3, 2, 4, 5])\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\nprint(""Type of every element:"", rank_4_tensor.dtype) print(""Number of axes:"", rank_4_tensor.ndim) print(""Shape of tensor:"", rank_4_tensor.shape) print(""Elements along axis 0 of tensor:"", rank_4_tensor.shape[0]) print(""Elements along the last axis of tensor:"", rank_4_tensor.shape[-1]) print(""Total number of elements (3*2*4*5): "", tf.size(rank_4_tensor).numpy())\n\nType of every element: <dtype: \'float32\'> Number of axes: 4 Shape of tensor: (3, 2, 4, 5) Elements along axis 0 of tensor: 3 Elements along the last axis of tensor: 5 Total number of elements (3*2*4*5): 120\n\nBut note that the Tensor.ndim and Tensor.shape attributes don\'t return Tensor objects. If you need a Tensor use the tf.rank or tf.shape function. This difference is subtle, but it can be important when building graphs (later).\n\ntf.rank(rank_4_tensor)\n\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n\ntf.shape(rank_4_tensor)\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 2, 4, 5], dtype=int32)>\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\nSingle-axis indexing\n\nTensorFlow follows standard Python indexing rules, similar to indexing a list or a string in Python, and the basic rules for NumPy indexing.\n\nnegative indices count backwards from the end\n\ncolons, :, are used for slices: start:stop:step\n\nrank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34]) print(rank_1_tensor.numpy())\n\n[ 0 1 1 2 3 5 8 13 21 34]\n\nIndexing with a scalar removes the axis:\n\nprint(""First:"", rank_1_tensor[0].numpy()) print(""Second:"", rank_1_tensor[1].numpy()) print(""Last:"", rank_1_tensor[-1].numpy())\n\nFirst: 0 Second: 1 Last: 34\n\nIndexing with a : slice keeps the axis:\n\nprint(""Everything:"", rank_1_tensor[:].numpy()) print(""Before 4:"", rank_1_tensor[:4].numpy()) print(""From 4 to the end:"", rank_1_tensor[4:].numpy()) print(""From 2, before 7:"", rank_1_tensor[2:7].numpy()) print(""Every other item:"", rank_1_tensor[::2].numpy()) print(""Reversed:"", rank_1_tensor[::-1].numpy())\n\nEverything: [ 0 1 1 2 3 5 8 13 21 34] Before 4: [0 1 1 2] From 4 to the end: [ 3 5 8 13 21 34] From 2, before 7: [1 2 3 5 8] Every other item: [ 0 1 3 8 21] Reversed: [34 21 13 8 5 3 2 1 1 0]\n\nHigher rank tensors are indexed by passing multiple indices.\n\nThe exact same rules as in the single-axis case apply to each axis independently.\n\nprint(rank_2_tensor.numpy())\n\n[[1. 2.] [3. 4.] [5. 6.]]\n\nPassing an integer for each index, the result is a scalar.\n\n# Pull out a single value from a 2-rank tensor print(rank_2_tensor[1, 1].numpy())\n\nYou can index using any combination of integers and slices:\n\n# Get row and column tensors print(""Second row:"", rank_2_tensor[1, :].numpy()) print(""Second column:"", rank_2_tensor[:, 1].numpy()) print(""Last row:"", rank_2_tensor[-1, :].numpy()) print(""First item in last column:"", rank_2_tensor[0, -1].numpy()) print(""Skip the first row:"") print(rank_2_tensor[1:, :].numpy(), ""\\n"")\n\nSecond row: [3. 4.] Second column: [2. 4. 6.] Last row: [5. 6.] First item in last column: 2.0 Skip the first row: [[3. 4.] [5. 6.]]\n\nHere is an example with a 3-axis tensor:\n\nprint(rank_3_tensor[:, :, 4])\n\ntf.Tensor( [[ 4 9] [14 19] [24 29]], shape=(3, 2), dtype=int32)\n\nSelecting the last feature across all locations in each example in the batch\n\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors.\n\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis x = tf.constant([[1], [2], [3]]) print(x.shape)\n\n# You can convert this object into a Python list, too print(x.shape.as_list())\n\nYou can reshape a tensor into a new shape. The tf.reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape. # Note that you\'re passing in a list reshaped = tf.reshape(x, [1, 3])\n\nprint(x.shape) print(reshaped.shape)\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style ""row-major"" memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nprint(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says ""Whatever fits"". print(tf.reshape(rank_3_tensor, [-1]))\n\ntf.Tensor( [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29], shape=(30,), dtype=int32)\n\nTypically the only reasonable use of tf.reshape is to combine or split adjacent axes (or add/remove 1s).\n\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\nprint(tf.reshape(rank_3_tensor, [3*2, 5]), ""\\n"") print(tf.reshape(rank_3_tensor, [3, -1]))\n\ntf.Tensor( [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]], shape=(6, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n\nReshaping will ""work"" for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\n\nSwapping axes in tf.reshape does not work; you need tf.transpose for that.\n\n# Bad examples: don\'t do this # You can\'t reorder axes with reshape. print(tf.reshape(rank_3_tensor, [2, 3, 5]), ""\\n"") # This is a mess print(tf.reshape(rank_3_tensor, [5, 6]), ""\\n"") # This doesn\'t work at all try: tf.reshape(rank_3_tensor, [7, -1]) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]] [[15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) InvalidArgumentError: { {function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0} } Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n\nYou may run across not-fully-specified shapes. Either the shape contains a None (an axis-length is unknown) or the whole shape is None (the rank of the tensor is unknown).\n\nExcept for tf.RaggedTensor, such shapes will only occur in the context of TensorFlow\'s symbolic, graph-building APIs:\n\nThe keras functional API.\n\nTo inspect a tf.Tensor\'s data type use the Tensor.dtype property.\n\nWhen creating a tf.Tensor from a Python object you may optionally specify the datatype.\n\nIf you don\'t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts Python integers to tf.int32 and Python floating point numbers to tf.float32. Otherwise TensorFlow uses the same rules NumPy uses when converting to arrays.\n\nYou can cast from type to type.\n\nthe_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64) the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16) # Now, cast to an uint8 and lose the decimal precision the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8) print(the_u8_tensor)\n\ntf.Tensor([2 3 4], shape=(3,), dtype=uint8)\n\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are ""stretched"" automatically to fit larger tensors when running combined operations on them.\n\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx = tf.constant([1, 2, 3]) y = tf.constant(2) z = tf.constant([2, 2, 2]) # All of these are the same computation print(tf.multiply(x, 2)) print(x * y) print(x * z)\n\ntf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\n\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations x = tf.reshape(x,[3,1]) y = tf.range(1, 5) print(x, ""\\n"") print(y, ""\\n"") print(tf.multiply(x, y))\n\ntf.Tensor( [[1] [2] [3]], shape=(3, 1), dtype=int32) tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) tf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\nHere is the same operation without broadcasting:\n\nx_stretch = tf.constant([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]) y_stretch = tf.constant([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]) print(x_stretch * y_stretch) # Again, operator overloading\n\ntf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\n\nYou see what broadcasting looks like using tf.broadcast_to.\n\nprint(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))\n\ntf.Tensor( [[1 2 3] [1 2 3] [1 2 3]], shape=(3, 3), dtype=int32)\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\n\nIt can get even more complicated. This section of Jake VanderPlas\'s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy).\n\ntf.convert_to_tensor\n\nMost ops, like tf.matmul and tf.reshape take arguments of class tf.Tensor. However, you\'ll notice in the above case, Python objects shaped like tensors are accepted.\n\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy\'s ndarray, TensorShape, Python lists, and tf.Variable will all convert automatically.\n\nSee tf.register_tensor_conversion_function for more details, and if you have your own type you\'d like to automatically convert to a tensor.\n\nA tensor with variable numbers of elements along some axis is called ""ragged"". Use tf.ragged.RaggedTensor for ragged data.\n\nFor example, This cannot be represented as a regular tensor:\n\nA tf.RaggedTensor, shape: [4, None]\n\nragged_list = [ [0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]\n\ntry: tensor = tf.constant(ragged_list) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\nValueError: Can\'t convert non-rectangular Python sequence to Tensor.\n\nInstead create a tf.RaggedTensor using tf.ragged.constant:\n\nragged_tensor = tf.ragged.constant(ragged_list) print(ragged_tensor)\n\n<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>\n\nThe shape of a tf.RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor.shape)\n\ntf.string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\n\nThe strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the axes of the tensor. See tf.strings for functions to manipulate them.\n\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string. scalar_string_tensor = tf.constant(""Gray wolf"") print(scalar_string_tensor)\n\ntf.Tensor(b\'Gray wolf\', shape=(), dtype=string)\n\nAnd a vector of strings:\n\nA vector of strings, shape: [3,]\n\n# If you have three string tensors of different lengths, this is OK. tensor_of_strings = tf.constant([""Gray wolf"", ""Quick brown fox"", ""Lazy dog""]) # Note that the shape is (3,). The string length is not included. print(tensor_of_strings)\n\ntf.Tensor([b\'Gray wolf\' b\'Quick brown fox\' b\'Lazy dog\'], shape=(3,), dtype=string)\n\nIn the above printout the b prefix indicates that tf.string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\n\nIf you pass unicode characters they are utf-8 encoded.\n\ntf.constant(""🥳👍"")\n\n<tf.Tensor: shape=(), dtype=string, numpy=b\'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d\'>\n\nSome basic functions with strings can be found in tf.strings, including tf.strings.split.\n\n# You can use split to split a string into a set of tensors print(tf.strings.split(scalar_string_tensor, sep="" ""))\n\ntf.Tensor([b\'Gray\' b\'wolf\'], shape=(2,), dtype=string)\n\n# ...but it turns into a `RaggedTensor` if you split up a tensor of strings, # as each string might be split into a different number of parts. print(tf.strings.split(tensor_of_strings))\n\n<tf.RaggedTensor [[b\'Gray\', b\'wolf\'], [b\'Quick\', b\'brown\', b\'fox\'], [b\'Lazy\', b\'dog\']]>\n\nThree strings split, shape: [3, None]\n\nAnd tf.strings.to_number:\n\ntext = tf.constant(""1 10 100"") print(tf.strings.to_number(tf.strings.split(text, "" "")))\n\ntf.Tensor([ 1. 10. 100.], shape=(3,), dtype=float32)\n\nAlthough you can\'t use tf.cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings = tf.strings.bytes_split(tf.constant(""Duck"")) byte_ints = tf.io.decode_raw(tf.constant(""Duck""), tf.uint8) print(""Byte strings:"", byte_strings) print(""Bytes:"", byte_ints)\n\nByte strings: tf.Tensor([b\'D\' b\'u\' b\'c\' b\'k\'], shape=(4,), dtype=string) Bytes: tf.Tensor([ 68 117 99 107], shape=(4,), dtype=uint8)\n\n# Or split it up as unicode and then decode it unicode_bytes = tf.constant(""アヒル 🦆"") unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, ""UTF-8"") unicode_values = tf.strings.unicode_decode(unicode_bytes, ""UTF-8"") print(""\\nUnicode bytes:"", unicode_bytes) print(""\\nUnicode chars:"", unicode_char_bytes) print(""\\nUnicode values:"", unicode_values)\n\nUnicode bytes: tf.Tensor(b\'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86\', shape=(), dtype=string) Unicode chars: tf.Tensor([b\'\\xe3\\x82\\xa2\' b\'\\xe3\\x83\\x92\' b\'\\xe3\\x83\\xab\' b\' \' b\'\\xf0\\x9f\\xa6\\x86\'], shape=(5,), dtype=string) Unicode values: tf.Tensor([ 12450 12498 12523 32 129414], shape=(5,), dtype=int32)\n\nThe tf.string dtype is used for all raw bytes data in TensorFlow. The tf.io module contains functions for converting data to and from bytes, including decoding images and parsing csv.\n\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf.sparse.SparseTensor and related operations to store sparse data efficiently.\n\nA tf.SparseTensor, shape: [3, 4]\n\n# Sparse tensors store values by index in a memory-efficient manner sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]) print(sparse_tensor, ""\\n"") # You can convert sparse tensors to dense print(tf.sparse.to_dense(sparse_tensor))\n\nSparseTensor(indices=tf.Tensor( [[0 0] [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) tf.Tensor( [[1 0 0 0] [0 0 2 0] [0 0 0 0]], shape=(3, 4), dtype=int32)\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-10-28 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_2', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nIntroduction to Tensors\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nimport tensorflow as tf import numpy as np\n\n2023-10-28 01:21:58.219231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2023-10-28 01:21:58.219277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2023-10-28 01:21:58.220822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nTensors are multi-dimensional arrays with a uniform type (called a dtype). You can see all supported dtypes at tf.dtypes.\n\nIf you\'re familiar with NumPy, tensors are (kind of) like np.arrays.\n\nAll tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.\n\nFirst, create some basic tensors.\n\nHere is a ""scalar"" or ""rank-0"" tensor . A scalar contains a single value, and no ""axes"".\n\n# This will be an int32 tensor by default; see ""dtypes"" below. rank_0_tensor = tf.constant(4) print(rank_0_tensor)\n\ntf.Tensor(4, shape=(), dtype=int32)\n\nA ""vector"" or ""rank-1"" tensor is like a list of values. A vector has one axis:\n\n# Let\'s make this a float tensor. rank_1_tensor = tf.constant([2.0, 3.0, 4.0]) print(rank_1_tensor)\n\ntf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n\nA ""matrix"" or ""rank-2"" tensor has two axes:\n\n# If you want to be specific, you can set the dtype (see below) at creation time rank_2_tensor = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float16) print(rank_2_tensor)\n\ntf.Tensor( [[1. 2.] [3. 4.] [5. 6.]], shape=(3, 2), dtype=float16)\n\nA vector, shape: [3]\n\nA matrix, shape: [3, 2]\n\nTensors may have more axes; here is a tensor with three axes:\n\n# There can be an arbitrary number of # axes (sometimes called ""dimensions"") rank_3_tensor = tf.constant([ [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]], [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]], [[20, 21, 22, 23, 24], [25, 26, 27, 28, 29]],]) print(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nThere are many ways you might visualize a tensor with more than two axes.\n\nA 3-axis tensor, shape: [3, 2, 5]\n\nYou can convert a tensor to a NumPy array either using np.array or the tensor.numpy method:\n\nnp.array(rank_2_tensor)\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nrank_2_tensor.numpy()\n\narray([[1., 2.], [3., 4.], [5., 6.]], dtype=float16)\n\nTensors often contain floats and ints, but have many other types, including:\n\nThe base tf.Tensor class requires tensors to be ""rectangular""---that is, along each axis, every element is the same size. However, there are specialized types of tensors that can handle different shapes:\n\nRagged tensors (see RaggedTensor below)\n\nSparse tensors (see SparseTensor below)\n\nYou can do basic math on tensors, including addition, element-wise multiplication, and matrix multiplication.\n\na = tf.constant([[1, 2], [3, 4]]) b = tf.constant([[1, 1], [1, 1]]) # Could have also said `tf.ones([2,2], dtype=tf.int32)` print(tf.add(a, b), ""\\n"") print(tf.multiply(a, b), ""\\n"") print(tf.matmul(a, b), ""\\n"")\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nprint(a + b, ""\\n"") # element-wise addition print(a * b, ""\\n"") # element-wise multiplication print(a @ b, ""\\n"") # matrix multiplication\n\ntf.Tensor( [[2 3] [4 5]], shape=(2, 2), dtype=int32) tf.Tensor( [[1 2] [3 4]], shape=(2, 2), dtype=int32) tf.Tensor( [[3 3] [7 7]], shape=(2, 2), dtype=int32)\n\nTensors are used in all kinds of operations (or ""Ops"").\n\nc = tf.constant([[4.0, 5.0], [10.0, 1.0]]) # Find the largest value print(tf.reduce_max(c)) # Find the index of the largest value print(tf.math.argmax(c)) # Compute the softmax print(tf.nn.softmax(c))\n\ntf.Tensor(10.0, shape=(), dtype=float32) tf.Tensor([1 0], shape=(2,), dtype=int64) tf.Tensor( [[2.6894143e-01 7.3105854e-01] [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)\n\nNote: Typically, anywhere a TensorFlow function expects a Tensor as input, the function will also accept anything that can be converted to a Tensor using tf.convert_to_tensor. See below for an example.\n\ntf.convert_to_tensor([1,2,3])\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n\ntf.reduce_max([1,2,3])\n\n<tf.Tensor: shape=(), dtype=int32, numpy=3>\n\ntf.reduce_max(np.array([1,2,3]))\n\n<tf.Tensor: shape=(), dtype=int64, numpy=3>\n\nTensors have shapes. Some vocabulary:\n\nShape: The length (number of elements) of each of the axes of a tensor.\n\nRank: Number of tensor axes. A scalar has rank 0, a vector has rank 1, a matrix is rank 2.\n\nAxis or Dimension: A particular dimension of a tensor.\n\nSize: The total number of items in the tensor, the product of the shape vector\'s elements.\n\nNote: Although you may see reference to a ""tensor of two dimensions"", a rank-2 tensor does not usually describe a 2D space.\n\nTensors and tf.TensorShape objects have convenient properties for accessing these:\n\nrank_4_tensor = tf.zeros([3, 2, 4, 5])\n\nA rank-4 tensor, shape: [3, 2, 4, 5]\n\nprint(""Type of every element:"", rank_4_tensor.dtype) print(""Number of axes:"", rank_4_tensor.ndim) print(""Shape of tensor:"", rank_4_tensor.shape) print(""Elements along axis 0 of tensor:"", rank_4_tensor.shape[0]) print(""Elements along the last axis of tensor:"", rank_4_tensor.shape[-1]) print(""Total number of elements (3*2*4*5): "", tf.size(rank_4_tensor).numpy())\n\nType of every element: <dtype: \'float32\'> Number of axes: 4 Shape of tensor: (3, 2, 4, 5) Elements along axis 0 of tensor: 3 Elements along the last axis of tensor: 5 Total number of elements (3*2*4*5): 120\n\nBut note that the Tensor.ndim and Tensor.shape attributes don\'t return Tensor objects. If you need a Tensor use the tf.rank or tf.shape function. This difference is subtle, but it can be important when building graphs (later).\n\ntf.rank(rank_4_tensor)\n\n<tf.Tensor: shape=(), dtype=int32, numpy=4>\n\ntf.shape(rank_4_tensor)\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([3, 2, 4, 5], dtype=int32)>\n\nWhile axes are often referred to by their indices, you should always keep track of the meaning of each. Often axes are ordered from global to local: The batch axis first, followed by spatial dimensions, and features for each location last. This way feature vectors are contiguous regions of memory.\n\nSingle-axis indexing\n\nTensorFlow follows standard Python indexing rules, similar to indexing a list or a string in Python, and the basic rules for NumPy indexing.\n\nnegative indices count backwards from the end\n\ncolons, :, are used for slices: start:stop:step\n\nrank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34]) print(rank_1_tensor.numpy())\n\n[ 0 1 1 2 3 5 8 13 21 34]\n\nIndexing with a scalar removes the axis:\n\nprint(""First:"", rank_1_tensor[0].numpy()) print(""Second:"", rank_1_tensor[1].numpy()) print(""Last:"", rank_1_tensor[-1].numpy())\n\nFirst: 0 Second: 1 Last: 34\n\nIndexing with a : slice keeps the axis:\n\nprint(""Everything:"", rank_1_tensor[:].numpy()) print(""Before 4:"", rank_1_tensor[:4].numpy()) print(""From 4 to the end:"", rank_1_tensor[4:].numpy()) print(""From 2, before 7:"", rank_1_tensor[2:7].numpy()) print(""Every other item:"", rank_1_tensor[::2].numpy()) print(""Reversed:"", rank_1_tensor[::-1].numpy())\n\nEverything: [ 0 1 1 2 3 5 8 13 21 34] Before 4: [0 1 1 2] From 4 to the end: [ 3 5 8 13 21 34] From 2, before 7: [1 2 3 5 8] Every other item: [ 0 1 3 8 21] Reversed: [34 21 13 8 5 3 2 1 1 0]\n\nHigher rank tensors are indexed by passing multiple indices.\n\nThe exact same rules as in the single-axis case apply to each axis independently.\n\nprint(rank_2_tensor.numpy())\n\n[[1. 2.] [3. 4.] [5. 6.]]\n\nPassing an integer for each index, the result is a scalar.\n\n# Pull out a single value from a 2-rank tensor print(rank_2_tensor[1, 1].numpy())\n\nYou can index using any combination of integers and slices:\n\n# Get row and column tensors print(""Second row:"", rank_2_tensor[1, :].numpy()) print(""Second column:"", rank_2_tensor[:, 1].numpy()) print(""Last row:"", rank_2_tensor[-1, :].numpy()) print(""First item in last column:"", rank_2_tensor[0, -1].numpy()) print(""Skip the first row:"") print(rank_2_tensor[1:, :].numpy(), ""\\n"")\n\nSecond row: [3. 4.] Second column: [2. 4. 6.] Last row: [5. 6.] First item in last column: 2.0 Skip the first row: [[3. 4.] [5. 6.]]\n\nHere is an example with a 3-axis tensor:\n\nprint(rank_3_tensor[:, :, 4])\n\ntf.Tensor( [[ 4 9] [14 19] [24 29]], shape=(3, 2), dtype=int32)\n\nSelecting the last feature across all locations in each example in the batch\n\nRead the tensor slicing guide to learn how you can apply indexing to manipulate individual elements in your tensors.\n\nReshaping a tensor is of great utility.\n\n# Shape returns a `TensorShape` object that shows the size along each axis x = tf.constant([[1], [2], [3]]) print(x.shape)\n\n# You can convert this object into a Python list, too print(x.shape.as_list())\n\nYou can reshape a tensor into a new shape. The tf.reshape operation is fast and cheap as the underlying data does not need to be duplicated.\n\n# You can reshape a tensor to a new shape. # Note that you\'re passing in a list reshaped = tf.reshape(x, [1, 3])\n\nprint(x.shape) print(reshaped.shape)\n\nThe data maintains its layout in memory and a new tensor is created, with the requested shape, pointing to the same data. TensorFlow uses C-style ""row-major"" memory ordering, where incrementing the rightmost index corresponds to a single step in memory.\n\nprint(rank_3_tensor)\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9]] [[10 11 12 13 14] [15 16 17 18 19]] [[20 21 22 23 24] [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n# A `-1` passed in the `shape` argument says ""Whatever fits"". print(tf.reshape(rank_3_tensor, [-1]))\n\ntf.Tensor( [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29], shape=(30,), dtype=int32)\n\nTypically the only reasonable use of tf.reshape is to combine or split adjacent axes (or add/remove 1s).\n\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both reasonable things to do, as the slices do not mix:\n\nprint(tf.reshape(rank_3_tensor, [3*2, 5]), ""\\n"") print(tf.reshape(rank_3_tensor, [3, -1]))\n\ntf.Tensor( [[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14] [15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]], shape=(6, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n\nReshaping will ""work"" for any new shape with the same total number of elements, but it will not do anything useful if you do not respect the order of the axes.\n\nSwapping axes in tf.reshape does not work; you need tf.transpose for that.\n\n# Bad examples: don\'t do this # You can\'t reorder axes with reshape. print(tf.reshape(rank_3_tensor, [2, 3, 5]), ""\\n"") # This is a mess print(tf.reshape(rank_3_tensor, [5, 6]), ""\\n"") # This doesn\'t work at all try: tf.reshape(rank_3_tensor, [7, -1]) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\ntf.Tensor( [[[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]] [[15 16 17 18 19] [20 21 22 23 24] [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) tf.Tensor( [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) InvalidArgumentError: { {function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0} } Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n\nYou may run across not-fully-specified shapes. Either the shape contains a None (an axis-length is unknown) or the whole shape is None (the rank of the tensor is unknown).\n\nExcept for tf.RaggedTensor, such shapes will only occur in the context of TensorFlow\'s symbolic, graph-building APIs:\n\nThe keras functional API.\n\nTo inspect a tf.Tensor\'s data type use the Tensor.dtype property.\n\nWhen creating a tf.Tensor from a Python object you may optionally specify the datatype.\n\nIf you don\'t, TensorFlow chooses a datatype that can represent your data. TensorFlow converts Python integers to tf.int32 and Python floating point numbers to tf.float32. Otherwise TensorFlow uses the same rules NumPy uses when converting to arrays.\n\nYou can cast from type to type.\n\nthe_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64) the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16) # Now, cast to an uint8 and lose the decimal precision the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8) print(the_u8_tensor)\n\ntf.Tensor([2 3 4], shape=(3,), dtype=uint8)\n\nBroadcasting is a concept borrowed from the equivalent feature in NumPy. In short, under certain conditions, smaller tensors are ""stretched"" automatically to fit larger tensors when running combined operations on them.\n\nThe simplest and most common case is when you attempt to multiply or add a tensor to a scalar. In that case, the scalar is broadcast to be the same shape as the other argument.\n\nx = tf.constant([1, 2, 3]) y = tf.constant(2) z = tf.constant([2, 2, 2]) # All of these are the same computation print(tf.multiply(x, 2)) print(x * y) print(x * z)\n\ntf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32) tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n\nLikewise, axes with length 1 can be stretched out to match the other arguments. Both arguments can be stretched in the same computation.\n\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to produce a 3x4 matrix. Note how the leading 1 is optional: The shape of y is [4].\n\n# These are the same computations x = tf.reshape(x,[3,1]) y = tf.range(1, 5) print(x, ""\\n"") print(y, ""\\n"") print(tf.multiply(x, y))\n\ntf.Tensor( [[1] [2] [3]], shape=(3, 1), dtype=int32) tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) tf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nA broadcasted add: a [3, 1] times a [1, 4] gives a [3,4]\n\nHere is the same operation without broadcasting:\n\nx_stretch = tf.constant([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]) y_stretch = tf.constant([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]]) print(x_stretch * y_stretch) # Again, operator overloading\n\ntf.Tensor( [[ 1 2 3 4] [ 2 4 6 8] [ 3 6 9 12]], shape=(3, 4), dtype=int32)\n\nMost of the time, broadcasting is both time and space efficient, as the broadcast operation never materializes the expanded tensors in memory.\n\nYou see what broadcasting looks like using tf.broadcast_to.\n\nprint(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))\n\ntf.Tensor( [[1 2 3] [1 2 3] [1 2 3]], shape=(3, 3), dtype=int32)\n\nUnlike a mathematical op, for example, broadcast_to does nothing special to save memory. Here, you are materializing the tensor.\n\nIt can get even more complicated. This section of Jake VanderPlas\'s book Python Data Science Handbook shows more broadcasting tricks (again in NumPy).\n\ntf.convert_to_tensor\n\nMost ops, like tf.matmul and tf.reshape take arguments of class tf.Tensor. However, you\'ll notice in the above case, Python objects shaped like tensors are accepted.\n\nMost, but not all, ops call convert_to_tensor on non-tensor arguments. There is a registry of conversions, and most object classes like NumPy\'s ndarray, TensorShape, Python lists, and tf.Variable will all convert automatically.\n\nSee tf.register_tensor_conversion_function for more details, and if you have your own type you\'d like to automatically convert to a tensor.\n\nA tensor with variable numbers of elements along some axis is called ""ragged"". Use tf.ragged.RaggedTensor for ragged data.\n\nFor example, This cannot be represented as a regular tensor:\n\nA tf.RaggedTensor, shape: [4, None]\n\nragged_list = [ [0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]\n\ntry: tensor = tf.constant(ragged_list) except Exception as e: print(f""{type(e).__name__}: {e}"")\n\nValueError: Can\'t convert non-rectangular Python sequence to Tensor.\n\nInstead create a tf.RaggedTensor using tf.ragged.constant:\n\nragged_tensor = tf.ragged.constant(ragged_list) print(ragged_tensor)\n\n<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>\n\nThe shape of a tf.RaggedTensor will contain some axes with unknown lengths:\n\nprint(ragged_tensor.shape)\n\ntf.string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors.\n\nThe strings are atomic and cannot be indexed the way Python strings are. The length of the string is not one of the axes of the tensor. See tf.strings for functions to manipulate them.\n\nHere is a scalar string tensor:\n\n# Tensors can be strings, too here is a scalar string. scalar_string_tensor = tf.constant(""Gray wolf"") print(scalar_string_tensor)\n\ntf.Tensor(b\'Gray wolf\', shape=(), dtype=string)\n\nAnd a vector of strings:\n\nA vector of strings, shape: [3,]\n\n# If you have three string tensors of different lengths, this is OK. tensor_of_strings = tf.constant([""Gray wolf"", ""Quick brown fox"", ""Lazy dog""]) # Note that the shape is (3,). The string length is not included. print(tensor_of_strings)\n\ntf.Tensor([b\'Gray wolf\' b\'Quick brown fox\' b\'Lazy dog\'], shape=(3,), dtype=string)\n\nIn the above printout the b prefix indicates that tf.string dtype is not a unicode string, but a byte-string. See the Unicode Tutorial for more about working with unicode text in TensorFlow.\n\nIf you pass unicode characters they are utf-8 encoded.\n\ntf.constant(""🥳👍"")\n\n<tf.Tensor: shape=(), dtype=string, numpy=b\'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d\'>\n\nSome basic functions with strings can be found in tf.strings, including tf.strings.split.\n\n# You can use split to split a string into a set of tensors print(tf.strings.split(scalar_string_tensor, sep="" ""))\n\ntf.Tensor([b\'Gray\' b\'wolf\'], shape=(2,), dtype=string)\n\n# ...but it turns into a `RaggedTensor` if you split up a tensor of strings, # as each string might be split into a different number of parts. print(tf.strings.split(tensor_of_strings))\n\n<tf.RaggedTensor [[b\'Gray\', b\'wolf\'], [b\'Quick\', b\'brown\', b\'fox\'], [b\'Lazy\', b\'dog\']]>\n\nThree strings split, shape: [3, None]\n\nAnd tf.strings.to_number:\n\ntext = tf.constant(""1 10 100"") print(tf.strings.to_number(tf.strings.split(text, "" "")))\n\ntf.Tensor([ 1. 10. 100.], shape=(3,), dtype=float32)\n\nAlthough you can\'t use tf.cast to turn a string tensor into numbers, you can convert it into bytes, and then into numbers.\n\nbyte_strings = tf.strings.bytes_split(tf.constant(""Duck"")) byte_ints = tf.io.decode_raw(tf.constant(""Duck""), tf.uint8) print(""Byte strings:"", byte_strings) print(""Bytes:"", byte_ints)\n\nByte strings: tf.Tensor([b\'D\' b\'u\' b\'c\' b\'k\'], shape=(4,), dtype=string) Bytes: tf.Tensor([ 68 117 99 107], shape=(4,), dtype=uint8)\n\n# Or split it up as unicode and then decode it unicode_bytes = tf.constant(""アヒル 🦆"") unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, ""UTF-8"") unicode_values = tf.strings.unicode_decode(unicode_bytes, ""UTF-8"") print(""\\nUnicode bytes:"", unicode_bytes) print(""\\nUnicode chars:"", unicode_char_bytes) print(""\\nUnicode values:"", unicode_values)\n\nUnicode bytes: tf.Tensor(b\'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86\', shape=(), dtype=string) Unicode chars: tf.Tensor([b\'\\xe3\\x82\\xa2\' b\'\\xe3\\x83\\x92\' b\'\\xe3\\x83\\xab\' b\' \' b\'\\xf0\\x9f\\xa6\\x86\'], shape=(5,), dtype=string) Unicode values: tf.Tensor([ 12450 12498 12523 32 129414], shape=(5,), dtype=int32)\n\nThe tf.string dtype is used for all raw bytes data in TensorFlow. The tf.io module contains functions for converting data to and from bytes, including decoding images and parsing csv.\n\nSometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf.sparse.SparseTensor and related operations to store sparse data efficiently.\n\nA tf.SparseTensor, shape: [3, 4]\n\n# Sparse tensors store values by index in a memory-efficient manner sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]) print(sparse_tensor, ""\\n"") # You can convert sparse tensors to dense print(tf.sparse.to_dense(sparse_tensor))\n\nSparseTensor(indices=tf.Tensor( [[0 0] [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) tf.Tensor( [[1 0 0 0] [0 0 2 0] [0 0 0 0]], shape=(3, 4), dtype=int32)\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-10-28 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-06-24T03:57:03', 'title': 'Introduction to Tensors | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/tensor'}), Document(page_content='中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', metadata={'id': 'web-search_3', 'snippet': '中文 – 简体 GitHub\n\nTensorFlow v2.15.0.post1', 'timestamp': '2024-04-13T06:48:37', 'title': 'tf.transpose | TensorFlow v2.15.0.post1', 'url': 'https://www.tensorflow.org/api_docs/python/tf/transpose'})]]??"
71129505,tf.data.Dataset,"{'https://www.edx.org/learn/javascript/google-google-ai-for-javascript-developers-with-tensorflow-js', 'https://www.coursera.org/learn/data-pipelines-tensorflow', 'https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187', 'https://www.udemy.com/course/complete-machine-learning-and-data-science-zero-to-mastery/', 'https://www.udacity.com/course/intro-to-machine-learning-with-tensorflow-nanodegree--nd230', 'https://www.udemy.com/course/deep-learning-in-practice-i/', 'https://www.udemy.com/course/deep-learning-masterclass-with-tensorflow-2-over-15-projects/', 'https://www.edx.org/learn/deep-learning/ibm-deep-learning-with-tensorflow'}",{'https://www.youtube.com/watch?v=k9cuKNnLF9w'},"{'https://stackoverflow.com/questions/51125266/how-do-i-split-tensorflow-datasets', 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test', 'https://stackoverflow.com/questions/59669413/what-is-the-canonical-way-to-split-tf-dataset-into-test-and-validation-subsets', 'https://stackoverflow.com/questions/57218230/how-to-split-mnist-dataset-into-multiple-subsets-for-distributed-nodes-using-pyt', 'https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets', 'https://stackoverflow.com/questions/71129505/is-it-possible-to-split-a-tensorflow-dataset-into-train-validation-and-test-dat'}","??[[Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow do I split a custom dataset into training and test datasets?\n\nAsked 6 years, 1 month ago\n\nModified 1 year, 7 months ago\n\nimport pandas as pd import numpy as np import cv2 from torch.utils.data.dataset import Dataset class CustomDatasetFromCSV(Dataset): def __init__(self, csv_path, transform=None): self.data = pd.read_csv(csv_path) self.labels = pd.get_dummies(self.data[\'emotion\']).as_matrix() self.height = 48 self.width = 48 self.transform = transform def __getitem__(self, index): pixels = self.data[\'pixels\'].tolist() faces = [] for pixel_sequence in pixels: face = [int(pixel) for pixel in pixel_sequence.split(\' \')] # print(np.asarray(face).shape) face = np.asarray(face).reshape(self.width, self.height) face = cv2.resize(face.astype(\'uint8\'), (self.width, self.height)) faces.append(face.astype(\'float32\')) faces = np.asarray(faces) faces = np.expand_dims(faces, -1) return faces, self.labels def __len__(self): return len(self.data)\n\nThis is what I could manage to do by using references from other repositories. However, I want to split this dataset into train and test.\n\nHow can I do that inside this class? Or do I need to make a separate class to do that?\n\nImprove this question\n\nedited Jan 7, 2019 at 14:42\n\n15.9k3434 gold badges117117 silver badges208208 bronze badges\n\nasked May 26, 2018 at 16:16\n\n4,1501212 gold badges5757 silver badges9393 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nStarting in PyTorch 0.4.1 you can use random_split:\n\ntrain_size = int(0.8 * len(full_dataset)) test_size = len(full_dataset) - train_size train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n\nedited Sep 25, 2018 at 9:54\n\nanswered Aug 9, 2018 at 13:41\n\nFábio PerezFábio Perez\n\n25.3k2222 gold badges7878 silver badges100100 bronze badges 4\n\nI followed your answer and got this problem while iterating throught the split train_loader stackoverflow.com/questions/53916594/…\n\n– joe Commented Dec 25, 2018 at 8:04\n\nAttributeError: \'Subset\' object has no attribute \'targets\' how can I access targets of only one of the subsets? I want to print something like this for train and test data separately {0: 111, 1: 722, 2: 813, 3: 175, 4: 283, 5: 2846, 6: 290, 7: 106}\n\n– Amin Bashiri Commented Jun 11, 2020 at 10:22\n\nFor others: If you\'re getting TypeError \'DataLoader\' object is not subscriptable you probably also want to look at stackoverflow.com/a/60150673/12068941\n\n– trujello Commented Jul 13, 2021 at 4:18\n\nAnyway to include spatial resampling strategy?\n\n– Sheykhmousa Commented Feb 4, 2022 at 12:58\n\nUsing Pytorch\'s SubsetRandomSampler:\n\nimport torch import numpy as np from torchvision import datasets from torchvision import transforms from torch.utils.data.sampler import SubsetRandomSampler class CustomDatasetFromCSV(Dataset): def __init__(self, csv_path, transform=None): self.data = pd.read_csv(csv_path) self.labels = pd.get_dummies(self.data[\'emotion\']).as_matrix() self.height = 48 self.width = 48 self.transform = transform def __getitem__(self, index): # This method should return only 1 sample and label # (according to ""index""), not the whole dataset # So probably something like this for you: pixel_sequence = self.data[\'pixels\'][index] face = [int(pixel) for pixel in pixel_sequence.split(\' \')] face = np.asarray(face).reshape(self.width, self.height) face = cv2.resize(face.astype(\'uint8\'), (self.width, self.height)) label = self.labels[index] return face, label def __len__(self): return len(self.labels) dataset = CustomDatasetFromCSV(my_path) batch_size = 16 validation_split = .2 shuffle_dataset = True random_seed= 42 # Creating data indices for training and validation splits: dataset_size = len(dataset) indices = list(range(dataset_size)) split = int(np.floor(validation_split * dataset_size)) if shuffle_dataset : np.random.seed(random_seed) np.random.shuffle(indices) train_indices, val_indices = indices[split:], indices[:split] # Creating PT data samplers and loaders: train_sampler = SubsetRandomSampler(train_indices) valid_sampler = SubsetRandomSampler(val_indices) train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler) validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler) # Usage Example: num_epochs = 10 for epoch in range(num_epochs): # Train: for batch_index, (faces, labels) in enumerate(train_loader): # ...\n\nedited Dec 15, 2018 at 14:33\n\nanswered May 26, 2018 at 16:33\n\nbenjaminplanchebenjaminplanche\n\n15k55 gold badges6060 silver badges7070 bronze badges 11\n\n– nirvair Commented May 26, 2018 at 16:49\n\nMy bad, it has been renamed appropriately (dataset_size).\n\n– benjaminplanche Commented May 26, 2018 at 16:51\n\nAlso when I put this in model, the function forward takes the input data. And the shape of that data is 5D tensor - (32L, 35887L, 48L, 48L, 1L). 32 is the batch size, next is the length of dataset and then image height, width and channel.\n\n– nirvair Commented May 26, 2018 at 19:08\n\nDataset.__getitem__() should return a single sample and label, not the whole dataset. I edited my post to give you an example how it should look.\n\n– benjaminplanche Commented May 26, 2018 at 19:18\n\n@AnaClaudia: batch_size defines the number of samples stacked together into a mini-batch passed to the neural network each training iteration. See Dataloader documentation or this Cross-Validated thread for more info.\n\n– benjaminplanche Commented Nov 21, 2019 at 11:10\n\n | Show 6 more comments\n\nIf you would like to ensure your splits have balanced classes, you can use train_test_split from sklearn.\n\nAssuming you have wrapped your data in a custom Dataset object:\n\nfrom torch.utils.data import DataLoader, Subset from sklearn.model_selection import train_test_split TEST_SIZE = 0.1 BATCH_SIZE = 64 SEED = 42 # generate indices: instead of the actual data we pass in integers instead train_indices, test_indices, _, _ = train_test_split( range(len(data)), data.targets, stratify=data.targets, test_size=TEST_SIZE, random_state=SEED ) # generate subset based on indices train_split = Subset(data, train_indices) test_split = Subset(data, test_indices) # create batches train_batches = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True) test_batches = DataLoader(test_split, batch_size=BATCH_SIZE)\n\nedited Jun 18, 2022 at 12:44\n\nanswered Jul 11, 2021 at 17:45\n\n42655 silver badges99 bronze badges\n\nCurrent answers do random splits which has disadvantage that number of samples per class is not guaranteed to be balanced. This is especially problematic when you want to have small number of samples per class. For example, MNIST has 60,000 examples, i.e. 6000 per digit. Assume that you want only 30 examples per digit in your training set. In this case, random split may produce imbalance between classes (one digit with more training data then others). So you want to make sure each digit precisely has only 30 labels. This is called stratified sampling.\n\nOne way to do this is using sampler interface in Pytorch and sample code is here.\n\nAnother way to do this is just hack your way through :). For example, below is simple implementation for MNIST where ds is MNIST dataset and k is number of samples needed for each class.\n\ndef sampleFromClass(ds, k): class_counts = {} train_data = [] train_label = [] test_data = [] test_label = [] for data, label in ds: c = label.item() class_counts[c] = class_counts.get(c, 0) + 1 if class_counts[c] <= k: train_data.append(data) train_label.append(torch.unsqueeze(label, 0)) else: test_data.append(data) test_label.append(torch.unsqueeze(label, 0)) train_data = torch.cat(train_data) for ll in train_label: print(ll) train_label = torch.cat(train_label) test_data = torch.cat(test_data) test_label = torch.cat(test_label) return (TensorDataset(train_data, train_label), TensorDataset(test_data, test_label))\n\nYou can use this function like this:\n\ndef main(): train_ds = datasets.MNIST(\'../data\', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor() ])) train_ds, test_ds = sampleFromClass(train_ds, 3)\n\nedited Oct 17, 2018 at 22:49\n\nanswered Sep 11, 2018 at 21:46\n\nShital ShahShital Shah\n\n67k1818 gold badges252252 silver badges193193 bronze badges\n\nThis is the PyTorch Subset class attached holding the random_split method. Note that this method is base for the SubsetRandomSampler.\n\nFor MNIST if we use random_split:\n\nloader = DataLoader( torchvision.datasets.MNIST(\'/data/mnist\', train=True, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.5,), (0.5,)) ])), batch_size=16, shuffle=False) print(loader.dataset.data.shape) test_ds, valid_ds = torch.utils.data.random_split(loader.dataset, (50000, 10000)) print(test_ds, valid_ds) print(test_ds.indices, valid_ds.indices) print(test_ds.indices.shape, valid_ds.indices.shape)\n\ntorch.Size([60000, 28, 28]) <torch.utils.data.dataset.Subset object at 0x0000020FD1880B00> <torch.utils.data.dataset.Subset object at 0x0000020FD1880C50> tensor([ 1520, 4155, 45472, ..., 37969, 45782, 34080]) tensor([ 9133, 51600, 22067, ..., 3950, 37306, 31400]) torch.Size([50000]) torch.Size([10000])\n\nOur test_ds.indices and valid_ds.indices will be random from range (0, 600000). But if I would like to get sequence of indices from (0, 49999) and from (50000, 59999) I cannot do that at the moment unfortunately, except this way.\n\nHandy in case you run the MNIST benchmark where it is predefined what should be the test and what should be the validation dataset.\n\nedited Jun 19, 2019 at 17:37\n\nanswered Jun 19, 2019 at 17:31\n\n45.1k1717 gold badges192192 silver badges160160 bronze badges 2\n\nclearly the easiest way to go\n\n– Valentin Commented Jan 19, 2022 at 9:39\n\nIs there a reason why the code is a screenshot? Please avoid that.\n\n– rbaleksandar Commented Mar 29, 2022 at 19:26\n\nAdding to Fábio Perez answer you can provide fractions to the random split. Note that you first split dataset, not dataloader.\n\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [0.8, 0.1, 0.1])\n\nanswered Dec 1, 2022 at 11:50\n\nIn case you want up to X samples per class in the train dataset you can use this code:\n\ndef stratify_split(dataset: Dataset, train_samples_per_class: int): import collections train_indices = [] val_indices = [] TRAIN_SAMPLES_PER_CLASS = 10 target_counter = collections.Counter() for idx, data in enumerate(dataset): target = data[\'target\'] target_counter[target] += 1 if target_counter[target] <= train_samples_per_class: train_indices.append(idx) else: val_indices.append(idx) train_dataset = Subset(dataset, train_indices) val_dataset = Subset(dataset, val_indices) return train_dataset, val_dataset\n\nanswered Sep 25, 2021 at 7:52\n\nBear in mind that most canonical examples are already spited. For instance on this page you will find MNIST. One common belief is that is has 60.000 images. Bang! Wrong! It has 70.000 images out of that 60.000 training and 10.000 validation (test) images.\n\nSo for the canonical datasets the flavor of PyTorch is to provide you already spited datasets.\n\nimport torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader, Dataset, TensorDataset from torch.optim import * import torchvision import torchvision.transforms as transforms import matplotlib.pyplot as plt import os import numpy as np import random bs=512 t = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=(0), std=(1))] ) dl_train = DataLoader( torchvision.datasets.MNIST(\'/data/mnist\', download=True, train=True, transform=t), batch_size=bs, drop_last=True, shuffle=True) dl_valid = DataLoader( torchvision.datasets.MNIST(\'/data/mnist\', download=True, train=False, transform=t), batch_size=bs, drop_last=True, shuffle=True)\n\nedited Jul 6, 2020 at 12:55\n\nanswered Jun 19, 2019 at 17:03\n\n45.1k1717 gold badges192192 silver badges160160 bronze badges 5\n\nIt seems to me the pipeline should be load data, split then transform - in particular in your case you\'ve hardcoded the inputs to Normalize. In general, these should be determined only from the training dataset, but with pytorch the transforms always seem to be applied to the full dataset.\n\n– David Waterworth Commented Jun 18, 2020 at 23:41\n\nFrom the data you have you ideally create train, validation and test datasets. (TRAVALTES). Train for training, validation to check if you are overfitting/underfitting. You calculate accuracy score, or some other score (f1...) to get some clues and ideally create confusion matrix if you have a problem like classification. So this post of mine sucks. I will improve it later today.\n\n– prosti Commented Jun 19, 2020 at 15:51\n\nYeah, my comment is more about how most of the canonical pytorch examples seem to hard code the mean / std of the features as an input into Transform, usually with pre-split test / validation data. This seems a little circular as in reality you\'d want to split the data and compute the Transformer parameters from the train set, then apply to the validation (and/or test). But the DataSet / Transformer design doesn\'t make this as easy as say sklearn. Sometimes I wonder if the scaling should be performed by a nn layer and hence be learnable parameters - but I guess that could affect convergence.\n\n– David Waterworth Commented Jun 20, 2020 at 7:18\n\nI updated the article. Most of the time set the mean to 0 and std to 1 if you train from scratch. For the pretrained models just follow the nomalization parameters provided with the model. You use the same normalization translation for the train and test sets (transforms.Normalize). @DavidWaterworth. Yeah I know some practitioners using the BN layer at the very start of the model to do normalization.\n\n– prosti Commented Jul 6, 2020 at 13:01\n\nWith the mean=(0), std=(1) as in the upper showcase I am getting 99.3% validation accuracy on simple hand made ResNet. Same would be for the mean=(0.5), std=(0.5) as we often seen that example wise.\n\n– prosti Commented Jul 6, 2020 at 13:05\n\nNot the answer you\'re looking for? Browse other questions tagged\n\npytorch or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n36 Get single random example from PyTorch DataLoader\n\n13 Number of instances per class in pytorch dataset\n\n10 Validation dataset in PyTorch using DataLoaders\n\n7 TypeError: object of type \'numpy.int64\' has no len()\n\n9 How to split data into train and test sets using torchvision.datasets.Imagefolder?\n\n5 Iterating over subsets from torch.utils.data.random_split\n\n1 Loading train/val/test datasets with images in separate folders using Pytorch\n\n1 Spliting the dataset using SubsetRandomSampler not working\n\n1 DataLoader messing up transformed data\n\n4 How to split own data set to train and validation in Tensorflow CNN\n\n2 How do I split the training dataset into training, validation and test datasets?\n\n17 Split image dataset into train-test datasets\n\n1 How to split a dataset into a custom training set and a custom validation set with pytorch?\n\n6 Train-Valid-Test split for custom dataset using PyTorch and TorchVision\n\n7 python.data.ops.dataset_ops.BatchDataset - how to use it to create training and test datasets\n\n0 split dataset into train and test using tensorflow\n\n0 TensorFlow Dataset train/test split\n\n1 PyTorch tutorial using testing dataset in training epoch\n\n1 How do I split an iterable dataset into training and test datasets?\n\nHot Network Questions\n\nHow can I export my Location History now that this data is only stored locally on the phone?\n\nWhat Does Feynman Mean When He Says Amplitude and Probabilities?\n\nWhat\'s the meaning of ""lex fundamentum est libertatis, qua fruimur. legum omnes servi sumus, ut liberi esse pssimus""?\n\nIs ""sinnate"" a word? What does it mean?\n\nIs the variance of the mean of a set of possibly dependent random variables less than the average of their respective variances?\n\nInfinity is not a number\n\nWhy the number of bits or bytes is different for a folder that has been copied between two external drives?\n\nIs a ""single"" cpu safer than multiple cores?\n\nWhy bother with planetary battlefields?\n\nBasic question about deriving MAP estimator\n\nOffline, multi-machine, 2-factor authentication information vault?\n\nRolling median of all K-length ranges\n\nWhy do Electric Aircraft Seem to Eschew Photovoltaics?\n\nFilling the areas enclosed by two curves and calculate them\n\nARK: Survival Ascended keeps crashing with LowLevelFatalError (error DXGI_ERROR_DEVICE_REMOVED with Reason: DXGI_ERROR_DEVICE_HUNG)\n\nHow can one count how many pixels a GIF image has via command line?\n\nWe have differing types of infinity; do we also have differing types of randomness?\n\nWhy does Google Maps only rotate 90º but not 180º when I rotate my iPhone?\n\nAn algorithm for generating a permutation of N numbers ranging from 1 to N with the maximum of smallest neighbouring differences\n\nHow to POSIX-ly ignore ""warning: command substitution: ignored null byte in input""?\n\nHow to arrange three identical habitable planets in one solar system in similar or, if possible, same orbit?\n\nDoes there exist a Dehn filling of an irreducible 3-manifold with toroidal boundaries which is still irreducible?\n\nWhat is the best epoch to evaluate the test images?\n\n向こう as a pronoun (""he/she/they"")? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_4', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow do I split a custom dataset into training and test datasets?\n\nAsked 6 years, 1 month ago\n\nModified 1 year, 7 months ago\n\nimport pandas as pd import numpy as np import cv2 from torch.utils.data.dataset import Dataset class CustomDatasetFromCSV(Dataset): def __init__(self, csv_path, transform=None): self.data = pd.read_csv(csv_path) self.labels = pd.get_dummies(self.data[\'emotion\']).as_matrix() self.height = 48 self.width = 48 self.transform = transform def __getitem__(self, index): pixels = self.data[\'pixels\'].tolist() faces = [] for pixel_sequence in pixels: face = [int(pixel) for pixel in pixel_sequence.split(\' \')] # print(np.asarray(face).shape) face = np.asarray(face).reshape(self.width, self.height) face = cv2.resize(face.astype(\'uint8\'), (self.width, self.height)) faces.append(face.astype(\'float32\')) faces = np.asarray(faces) faces = np.expand_dims(faces, -1) return faces, self.labels def __len__(self): return len(self.data)\n\nThis is what I could manage to do by using references from other repositories. However, I want to split this dataset into train and test.\n\nHow can I do that inside this class? Or do I need to make a separate class to do that?\n\nImprove this question\n\nedited Jan 7, 2019 at 14:42\n\n15.9k3434 gold badges117117 silver badges208208 bronze badges\n\nasked May 26, 2018 at 16:16\n\n4,1501212 gold badges5757 silver badges9393 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nStarting in PyTorch 0.4.1 you can use random_split:\n\ntrain_size = int(0.8 * len(full_dataset)) test_size = len(full_dataset) - train_size train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n\nedited Sep 25, 2018 at 9:54\n\nanswered Aug 9, 2018 at 13:41\n\nFábio PerezFábio Perez\n\n25.3k2222 gold badges7878 silver badges100100 bronze badges 4\n\nI followed your answer and got this problem while iterating throught the split train_loader stackoverflow.com/questions/53916594/…\n\n– joe Commented Dec 25, 2018 at 8:04\n\nAttributeError: \'Subset\' object has no attribute \'targets\' how can I access targets of only one of the subsets? I want to print something like this for train and test data separately {0: 111, 1: 722, 2: 813, 3: 175, 4: 283, 5: 2846, 6: 290, 7: 106}\n\n– Amin Bashiri Commented Jun 11, 2020 at 10:22\n\nFor others: If you\'re getting TypeError \'DataLoader\' object is not subscriptable you probably also want to look at stackoverflow.com/a/60150673/12068941\n\n– trujello Commented Jul 13, 2021 at 4:18\n\nAnyway to include spatial resampling strategy?\n\n– Sheykhmousa Commented Feb 4, 2022 at 12:58\n\nUsing Pytorch\'s SubsetRandomSampler:\n\nimport torch import numpy as np from torchvision import datasets from torchvision import transforms from torch.utils.data.sampler import SubsetRandomSampler class CustomDatasetFromCSV(Dataset): def __init__(self, csv_path, transform=None): self.data = pd.read_csv(csv_path) self.labels = pd.get_dummies(self.data[\'emotion\']).as_matrix() self.height = 48 self.width = 48 self.transform = transform def __getitem__(self, index): # This method should return only 1 sample and label # (according to ""index""), not the whole dataset # So probably something like this for you: pixel_sequence = self.data[\'pixels\'][index] face = [int(pixel) for pixel in pixel_sequence.split(\' \')] face = np.asarray(face).reshape(self.width, self.height) face = cv2.resize(face.astype(\'uint8\'), (self.width, self.height)) label = self.labels[index] return face, label def __len__(self): return len(self.labels) dataset = CustomDatasetFromCSV(my_path) batch_size = 16 validation_split = .2 shuffle_dataset = True random_seed= 42 # Creating data indices for training and validation splits: dataset_size = len(dataset) indices = list(range(dataset_size)) split = int(np.floor(validation_split * dataset_size)) if shuffle_dataset : np.random.seed(random_seed) np.random.shuffle(indices) train_indices, val_indices = indices[split:], indices[:split] # Creating PT data samplers and loaders: train_sampler = SubsetRandomSampler(train_indices) valid_sampler = SubsetRandomSampler(val_indices) train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler) validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler) # Usage Example: num_epochs = 10 for epoch in range(num_epochs): # Train: for batch_index, (faces, labels) in enumerate(train_loader): # ...\n\nedited Dec 15, 2018 at 14:33\n\nanswered May 26, 2018 at 16:33\n\nbenjaminplanchebenjaminplanche\n\n15k55 gold badges6060 silver badges7070 bronze badges 11\n\n– nirvair Commented May 26, 2018 at 16:49\n\nMy bad, it has been renamed appropriately (dataset_size).\n\n– benjaminplanche Commented May 26, 2018 at 16:51\n\nAlso when I put this in model, the function forward takes the input data. And the shape of that data is 5D tensor - (32L, 35887L, 48L, 48L, 1L). 32 is the batch size, next is the length of dataset and then image height, width and channel.\n\n– nirvair Commented May 26, 2018 at 19:08\n\nDataset.__getitem__() should return a single sample and label, not the whole dataset. I edited my post to give you an example how it should look.\n\n– benjaminplanche Commented May 26, 2018 at 19:18\n\n@AnaClaudia: batch_size defines the number of samples stacked together into a mini-batch passed to the neural network each training iteration. See Dataloader documentation or this Cross-Validated thread for more info.\n\n– benjaminplanche Commented Nov 21, 2019 at 11:10\n\n | Show 6 more comments\n\nIf you would like to ensure your splits have balanced classes, you can use train_test_split from sklearn.\n\nAssuming you have wrapped your data in a custom Dataset object:\n\nfrom torch.utils.data import DataLoader, Subset from sklearn.model_selection import train_test_split TEST_SIZE = 0.1 BATCH_SIZE = 64 SEED = 42 # generate indices: instead of the actual data we pass in integers instead train_indices, test_indices, _, _ = train_test_split( range(len(data)), data.targets, stratify=data.targets, test_size=TEST_SIZE, random_state=SEED ) # generate subset based on indices train_split = Subset(data, train_indices) test_split = Subset(data, test_indices) # create batches train_batches = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True) test_batches = DataLoader(test_split, batch_size=BATCH_SIZE)\n\nedited Jun 18, 2022 at 12:44\n\nanswered Jul 11, 2021 at 17:45\n\n42655 silver badges99 bronze badges\n\nCurrent answers do random splits which has disadvantage that number of samples per class is not guaranteed to be balanced. This is especially problematic when you want to have small number of samples per class. For example, MNIST has 60,000 examples, i.e. 6000 per digit. Assume that you want only 30 examples per digit in your training set. In this case, random split may produce imbalance between classes (one digit with more training data then others). So you want to make sure each digit precisely has only 30 labels. This is called stratified sampling.\n\nOne way to do this is using sampler interface in Pytorch and sample code is here.\n\nAnother way to do this is just hack your way through :). For example, below is simple implementation for MNIST where ds is MNIST dataset and k is number of samples needed for each class.\n\ndef sampleFromClass(ds, k): class_counts = {} train_data = [] train_label = [] test_data = [] test_label = [] for data, label in ds: c = label.item() class_counts[c] = class_counts.get(c, 0) + 1 if class_counts[c] <= k: train_data.append(data) train_label.append(torch.unsqueeze(label, 0)) else: test_data.append(data) test_label.append(torch.unsqueeze(label, 0)) train_data = torch.cat(train_data) for ll in train_label: print(ll) train_label = torch.cat(train_label) test_data = torch.cat(test_data) test_label = torch.cat(test_label) return (TensorDataset(train_data, train_label), TensorDataset(test_data, test_label))\n\nYou can use this function like this:\n\ndef main(): train_ds = datasets.MNIST(\'../data\', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor() ])) train_ds, test_ds = sampleFromClass(train_ds, 3)\n\nedited Oct 17, 2018 at 22:49\n\nanswered Sep 11, 2018 at 21:46\n\nShital ShahShital Shah\n\n67k1818 gold badges252252 silver badges193193 bronze badges\n\nThis is the PyTorch Subset class attached holding the random_split method. Note that this method is base for the SubsetRandomSampler.\n\nFor MNIST if we use random_split:\n\nloader = DataLoader( torchvision.datasets.MNIST(\'/data/mnist\', train=True, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.5,), (0.5,)) ])), batch_size=16, shuffle=False) print(loader.dataset.data.shape) test_ds, valid_ds = torch.utils.data.random_split(loader.dataset, (50000, 10000)) print(test_ds, valid_ds) print(test_ds.indices, valid_ds.indices) print(test_ds.indices.shape, valid_ds.indices.shape)\n\ntorch.Size([60000, 28, 28]) <torch.utils.data.dataset.Subset object at 0x0000020FD1880B00> <torch.utils.data.dataset.Subset object at 0x0000020FD1880C50> tensor([ 1520, 4155, 45472, ..., 37969, 45782, 34080]) tensor([ 9133, 51600, 22067, ..., 3950, 37306, 31400]) torch.Size([50000]) torch.Size([10000])\n\nOur test_ds.indices and valid_ds.indices will be random from range (0, 600000). But if I would like to get sequence of indices from (0, 49999) and from (50000, 59999) I cannot do that at the moment unfortunately, except this way.\n\nHandy in case you run the MNIST benchmark where it is predefined what should be the test and what should be the validation dataset.\n\nedited Jun 19, 2019 at 17:37\n\nanswered Jun 19, 2019 at 17:31\n\n45.1k1717 gold badges192192 silver badges160160 bronze badges 2\n\nclearly the easiest way to go\n\n– Valentin Commented Jan 19, 2022 at 9:39\n\nIs there a reason why the code is a screenshot? Please avoid that.\n\n– rbaleksandar Commented Mar 29, 2022 at 19:26\n\nAdding to Fábio Perez answer you can provide fractions to the random split. Note that you first split dataset, not dataloader.\n\ntrain_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [0.8, 0.1, 0.1])\n\nanswered Dec 1, 2022 at 11:50\n\nIn case you want up to X samples per class in the train dataset you can use this code:\n\ndef stratify_split(dataset: Dataset, train_samples_per_class: int): import collections train_indices = [] val_indices = [] TRAIN_SAMPLES_PER_CLASS = 10 target_counter = collections.Counter() for idx, data in enumerate(dataset): target = data[\'target\'] target_counter[target] += 1 if target_counter[target] <= train_samples_per_class: train_indices.append(idx) else: val_indices.append(idx) train_dataset = Subset(dataset, train_indices) val_dataset = Subset(dataset, val_indices) return train_dataset, val_dataset\n\nanswered Sep 25, 2021 at 7:52\n\nBear in mind that most canonical examples are already spited. For instance on this page you will find MNIST. One common belief is that is has 60.000 images. Bang! Wrong! It has 70.000 images out of that 60.000 training and 10.000 validation (test) images.\n\nSo for the canonical datasets the flavor of PyTorch is to provide you already spited datasets.\n\nimport torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader, Dataset, TensorDataset from torch.optim import * import torchvision import torchvision.transforms as transforms import matplotlib.pyplot as plt import os import numpy as np import random bs=512 t = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=(0), std=(1))] ) dl_train = DataLoader( torchvision.datasets.MNIST(\'/data/mnist\', download=True, train=True, transform=t), batch_size=bs, drop_last=True, shuffle=True) dl_valid = DataLoader( torchvision.datasets.MNIST(\'/data/mnist\', download=True, train=False, transform=t), batch_size=bs, drop_last=True, shuffle=True)\n\nedited Jul 6, 2020 at 12:55\n\nanswered Jun 19, 2019 at 17:03\n\n45.1k1717 gold badges192192 silver badges160160 bronze badges 5\n\nIt seems to me the pipeline should be load data, split then transform - in particular in your case you\'ve hardcoded the inputs to Normalize. In general, these should be determined only from the training dataset, but with pytorch the transforms always seem to be applied to the full dataset.\n\n– David Waterworth Commented Jun 18, 2020 at 23:41\n\nFrom the data you have you ideally create train, validation and test datasets. (TRAVALTES). Train for training, validation to check if you are overfitting/underfitting. You calculate accuracy score, or some other score (f1...) to get some clues and ideally create confusion matrix if you have a problem like classification. So this post of mine sucks. I will improve it later today.\n\n– prosti Commented Jun 19, 2020 at 15:51\n\nYeah, my comment is more about how most of the canonical pytorch examples seem to hard code the mean / std of the features as an input into Transform, usually with pre-split test / validation data. This seems a little circular as in reality you\'d want to split the data and compute the Transformer parameters from the train set, then apply to the validation (and/or test). But the DataSet / Transformer design doesn\'t make this as easy as say sklearn. Sometimes I wonder if the scaling should be performed by a nn layer and hence be learnable parameters - but I guess that could affect convergence.\n\n– David Waterworth Commented Jun 20, 2020 at 7:18\n\nI updated the article. Most of the time set the mean to 0 and std to 1 if you train from scratch. For the pretrained models just follow the nomalization parameters provided with the model. You use the same normalization translation for the train and test sets (transforms.Normalize). @DavidWaterworth. Yeah I know some practitioners using the BN layer at the very start of the model to do normalization.\n\n– prosti Commented Jul 6, 2020 at 13:01\n\nWith the mean=(0), std=(1) as in the upper showcase I am getting 99.3% validation accuracy on simple hand made ResNet. Same would be for the mean=(0.5), std=(0.5) as we often seen that example wise.\n\n– prosti Commented Jul 6, 2020 at 13:05\n\nNot the answer you\'re looking for? Browse other questions tagged\n\npytorch or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n36 Get single random example from PyTorch DataLoader\n\n13 Number of instances per class in pytorch dataset\n\n10 Validation dataset in PyTorch using DataLoaders\n\n7 TypeError: object of type \'numpy.int64\' has no len()\n\n9 How to split data into train and test sets using torchvision.datasets.Imagefolder?\n\n5 Iterating over subsets from torch.utils.data.random_split\n\n1 Loading train/val/test datasets with images in separate folders using Pytorch\n\n1 Spliting the dataset using SubsetRandomSampler not working\n\n1 DataLoader messing up transformed data\n\n4 How to split own data set to train and validation in Tensorflow CNN\n\n2 How do I split the training dataset into training, validation and test datasets?\n\n17 Split image dataset into train-test datasets\n\n1 How to split a dataset into a custom training set and a custom validation set with pytorch?\n\n6 Train-Valid-Test split for custom dataset using PyTorch and TorchVision\n\n7 python.data.ops.dataset_ops.BatchDataset - how to use it to create training and test datasets\n\n0 split dataset into train and test using tensorflow\n\n0 TensorFlow Dataset train/test split\n\n1 PyTorch tutorial using testing dataset in training epoch\n\n1 How do I split an iterable dataset into training and test datasets?\n\nHot Network Questions\n\nHow can I export my Location History now that this data is only stored locally on the phone?\n\nWhat Does Feynman Mean When He Says Amplitude and Probabilities?\n\nWhat\'s the meaning of ""lex fundamentum est libertatis, qua fruimur. legum omnes servi sumus, ut liberi esse pssimus""?\n\nIs ""sinnate"" a word? What does it mean?\n\nIs the variance of the mean of a set of possibly dependent random variables less than the average of their respective variances?\n\nInfinity is not a number\n\nWhy the number of bits or bytes is different for a folder that has been copied between two external drives?\n\nIs a ""single"" cpu safer than multiple cores?\n\nWhy bother with planetary battlefields?\n\nBasic question about deriving MAP estimator\n\nOffline, multi-machine, 2-factor authentication information vault?\n\nRolling median of all K-length ranges\n\nWhy do Electric Aircraft Seem to Eschew Photovoltaics?\n\nFilling the areas enclosed by two curves and calculate them\n\nARK: Survival Ascended keeps crashing with LowLevelFatalError (error DXGI_ERROR_DEVICE_REMOVED with Reason: DXGI_ERROR_DEVICE_HUNG)\n\nHow can one count how many pixels a GIF image has via command line?\n\nWe have differing types of infinity; do we also have differing types of randomness?\n\nWhy does Google Maps only rotate 90º but not 180º when I rotate my iPhone?\n\nAn algorithm for generating a permutation of N numbers ranging from 1 to N with the maximum of smallest neighbouring differences\n\nHow to POSIX-ly ignore ""warning: command substitution: ignored null byte in input""?\n\nHow to arrange three identical habitable planets in one solar system in similar or, if possible, same orbit?\n\nDoes there exist a Dehn filling of an irreducible 3-manifold with toroidal boundaries which is still irreducible?\n\nWhat is the best epoch to evaluate the test images?\n\n向こう as a pronoun (""he/she/they"")? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-08T11:57:09', 'title': 'python - How do I split a custom dataset into training and test datasets? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nHow to split MNIST dataset into multiple subsets for distributed nodes using Pytorch?\n\nAsked 3 years, 7 months ago\n\nModified 3 years, 7 months ago\n\nI am implementing DistributedDataParallel training to a simple CNN for torchvision.datasets.MNIST simultaneously running on 3 distributed nodes. I want to partition the datasets into 3 non-overlapping subsets (A,B,C) and that should contain 20000 images each. Individual subsets should be further split into training and testing partitions, i.e. 0.7% training and 0.3% testing. I plan to provide each subset to each distributed node separately so that they can train and test in a DistributedDataParallel fashion.\n\nThe basic code as shown below, downloads MNIST dataset from torchvision.datasets.MNIST and then uses torch.utils.data.distributed.DistributedSampler and torch.utils.data.DataLoader to create data batches for training and testing on a single node.\n\n# TRAINING DATA train_dataset = datasets.MNIST(\'data\', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])) train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=3, rank=dist.get_rank()) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=False, num_workers=3, pin_memory=True, sampler=True) # TESTING DATA test_dataset = datasets.MNIST(\'data\', train=False, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])) test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=3, pin_memory=True)\n\nI expect the answer should create train_dataset_a, train_dataset_b, and train_dataset_c, as well as, test_dataset_a, test_dataset_b, and test_dataset_c.\n\nImprove this question\n\nedited Jul 29, 2019 at 9:46\n\nasked Jul 26, 2019 at 10:34\n\nShakeel AnjumShakeel Anjum\n\n14188 bronze badges 1\n\nWe built BetterLoader (binitai.github.io/BetterLoader) to do stuff just like this! The project\'s still really new, but you may find it helpful if you\'re still working on similar problems :)\n\n– Raghav Sep 15, 2020 at 22:02\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nKnow someone who can answer? Share a link to this question via email, Twitter, or Facebook.\n\nBrowse other questions tagged\n\ndataloader or ask your own question.\n\nHow Intuit democratizes AI development across teams through reusability\n\nThe nature of simulating nature: A Q&A with IBM Quantum researcher Dr. Jamie...\n\nWe\'ve added a ""Necessary cookies only"" option to the cookie consent popup\n\nLaunching the CI/CD and R Collectives and community editing features for...\n\nThe [amazon] tag is being burninated\n\nTemporary policy: ChatGPT is banned\n\nStaging Ground Beta 1 Recap, and Reviewers needed for Beta 2\n\n459 How do I check if PyTorch is using the GPU?\n\n0 What does it mean to assert an object in python?\n\n3 Process stuck when training on multiple nodes using PyTorch DistributedDataParallel\n\n2 Error in transformation of EMNIST data through Pytorch\n\n6 PyTorch: Shuffle DataLoader\n\n12 HTTP Error when trying to download MNIST data\n\n0 How to load custom MNIST dataset using pytorch\n\nHot Network Questions\n\nQuestion about Nyquist sampling - filtering of high image copies\n\nWhy is there a voltage on my HDMI and coaxial cables?\n\nHow or would these mechanical wings work?\n\nTips for golfing in SVG\n\nNBA G League free throw\n\nWhat sort of strategies would a medieval military use against a fantasy giant?\n\nWhy does Mister Mxyzptlk need to have a weakness in the comics?\n\nAre the plants animated by an Assassin Vine considered magical?\n\nHow to measure the power in mW of a radio signal\n\nDrywall repair - trying to match texture\n\nExample of trickiness of finite lattice representation problem?\n\nWhat is the point of Thrower\'s Bandolier?\n\nDifference between ""select-editor"" and ""update-alternatives --config editor""\n\nWhat is the purpose of non-series Shimano components?\n\nRadial axis transformation in polar kernel density estimate\n\nDoes single case chance actually exist?\n\nAmenable link groups\n\nbetter understanding wash sale rule\n\nIsolate page to turn off header\n\nWhy do academics stay as adjuncts for years rather than move around?\n\nHaunted house movie that focuses on a basement door and a ghost who wants to steal a mother\'s child\n\nShould sticker on top of HDD be peeled?\n\nAre there tables of wastage rates for different fruit and veg?\n\nLoose bottom bracket on MTB more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', metadata={'id': 'web-search_5', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nHow to split MNIST dataset into multiple subsets for distributed nodes using Pytorch?\n\nAsked 3 years, 7 months ago\n\nModified 3 years, 7 months ago\n\nI am implementing DistributedDataParallel training to a simple CNN for torchvision.datasets.MNIST simultaneously running on 3 distributed nodes. I want to partition the datasets into 3 non-overlapping subsets (A,B,C) and that should contain 20000 images each. Individual subsets should be further split into training and testing partitions, i.e. 0.7% training and 0.3% testing. I plan to provide each subset to each distributed node separately so that they can train and test in a DistributedDataParallel fashion.\n\nThe basic code as shown below, downloads MNIST dataset from torchvision.datasets.MNIST and then uses torch.utils.data.distributed.DistributedSampler and torch.utils.data.DataLoader to create data batches for training and testing on a single node.\n\n# TRAINING DATA train_dataset = datasets.MNIST(\'data\', train=True, download=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])) train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=3, rank=dist.get_rank()) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=False, num_workers=3, pin_memory=True, sampler=True) # TESTING DATA test_dataset = datasets.MNIST(\'data\', train=False, download=False, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])) test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=3, pin_memory=True)\n\nI expect the answer should create train_dataset_a, train_dataset_b, and train_dataset_c, as well as, test_dataset_a, test_dataset_b, and test_dataset_c.\n\nImprove this question\n\nedited Jul 29, 2019 at 9:46\n\nasked Jul 26, 2019 at 10:34\n\nShakeel AnjumShakeel Anjum\n\n14188 bronze badges 1\n\nWe built BetterLoader (binitai.github.io/BetterLoader) to do stuff just like this! The project\'s still really new, but you may find it helpful if you\'re still working on similar problems :)\n\n– Raghav Sep 15, 2020 at 22:02\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nKnow someone who can answer? Share a link to this question via email, Twitter, or Facebook.\n\nBrowse other questions tagged\n\ndataloader or ask your own question.\n\nHow Intuit democratizes AI development across teams through reusability\n\nThe nature of simulating nature: A Q&A with IBM Quantum researcher Dr. Jamie...\n\nWe\'ve added a ""Necessary cookies only"" option to the cookie consent popup\n\nLaunching the CI/CD and R Collectives and community editing features for...\n\nThe [amazon] tag is being burninated\n\nTemporary policy: ChatGPT is banned\n\nStaging Ground Beta 1 Recap, and Reviewers needed for Beta 2\n\n459 How do I check if PyTorch is using the GPU?\n\n0 What does it mean to assert an object in python?\n\n3 Process stuck when training on multiple nodes using PyTorch DistributedDataParallel\n\n2 Error in transformation of EMNIST data through Pytorch\n\n6 PyTorch: Shuffle DataLoader\n\n12 HTTP Error when trying to download MNIST data\n\n0 How to load custom MNIST dataset using pytorch\n\nHot Network Questions\n\nQuestion about Nyquist sampling - filtering of high image copies\n\nWhy is there a voltage on my HDMI and coaxial cables?\n\nHow or would these mechanical wings work?\n\nTips for golfing in SVG\n\nNBA G League free throw\n\nWhat sort of strategies would a medieval military use against a fantasy giant?\n\nWhy does Mister Mxyzptlk need to have a weakness in the comics?\n\nAre the plants animated by an Assassin Vine considered magical?\n\nHow to measure the power in mW of a radio signal\n\nDrywall repair - trying to match texture\n\nExample of trickiness of finite lattice representation problem?\n\nWhat is the point of Thrower\'s Bandolier?\n\nDifference between ""select-editor"" and ""update-alternatives --config editor""\n\nWhat is the purpose of non-series Shimano components?\n\nRadial axis transformation in polar kernel density estimate\n\nDoes single case chance actually exist?\n\nAmenable link groups\n\nbetter understanding wash sale rule\n\nIsolate page to turn off header\n\nWhy do academics stay as adjuncts for years rather than move around?\n\nHaunted house movie that focuses on a basement door and a ghost who wants to steal a mother\'s child\n\nShould sticker on top of HDD be peeled?\n\nAre there tables of wastage rates for different fruit and veg?\n\nLoose bottom bracket on MTB more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\n\nAccept all cookies Necessary cookies only', 'timestamp': '2023-11-08T00:27:12', 'title': 'deep learning - How to split MNIST dataset into multiple subsets for distributed nodes using Pytorch? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/57218230/how-to-split-mnist-dataset-into-multiple-subsets-for-distributed-nodes-using-pyt'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nWhat is the canonical way to split tf.Dataset into test and validation subsets?\n\nAsked 4 years, 6 months ago\n\nModified 4 years, 5 months ago\n\nI was following a Tensorflow 2 tutorial on how to load images with pure Tensorflow, because it is supposed to be faster than with Keras. The tutorial ends before showing how to split the resulting dataset (~tf.Dataset) into a train and validation dataset.\n\nI checked the reference for tf.Dataset and it does not contain a split() method.\n\nI tried slicing it manually but tf.Dataset neither contains a size() nor a length() method, so I don\'t see how I could slice it myself.\n\nI can\'t use the validation_split argument of Model.fit() because I need to augment the training dataset but not the validation dataset.\n\nWhat is the intended way to split a tf.Dataset or should I use a different workflow where I won\'t have to do this?\n\nBATCH_SIZE = 32 IMG_HEIGHT = 224 IMG_WIDTH = 224 list_ds = tf.data.Dataset.list_files(str(data_dir/\'*/*\')) def get_label(file_path): # convert the path to a list of path components parts = tf.strings.split(file_path, os.path.sep) # The second to last is the class-directory return parts[-2] == CLASS_NAMES def decode_img(img): # convert the compressed string to a 3D uint8 tensor img = tf.image.decode_jpeg(img, channels=3) # Use `convert_image_dtype` to convert to floats in the [0,1] range. img = tf.image.convert_image_dtype(img, tf.float32) # resize the image to the desired size. return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) def process_path(file_path): label = get_label(file_path) # load the raw data from the file as a string img = tf.io.read_file(file_path) img = decode_img(img) return img, label labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE) #... #...\n\nI can either split list_ds (list of files) or labeled_ds (list of images and labels), but how?\n\nImprove this question\n\nedited Jan 9, 2020 at 17:54\n\nproblemofficer - n.f. Monica\n\nasked Jan 9, 2020 at 17:35\n\nproblemofficer - n.f. Monicaproblemofficer - n.f. Monica\n\n2,23033 gold badges2222 silver badges3636 bronze badges 3\n\nI saw several people using scikit-learn. Firstly, I would like to avoid having another dependency just for this one task. Secondly, I assume there must be something that simple in Tensorflow 2.\n\n– problemofficer - n.f. Monica Commented Jan 9, 2020 at 17:45\n\ntensorflow.org/datasets/api_docs/python/tfds/Split\n\n– albert Commented Jan 9, 2020 at 18:06\n\n@albert I saw that page already and the ""guide on splits"" but this does not seem to answer my question.\n\n– problemofficer - n.f. Monica Commented Jan 9, 2020 at 20:04\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nI don\'t think there\'s a canonical way (typically, data is being split e.g. in separate directories). But here\'s a recipe that will let you do it dynamically:\n\n# Caveat: cache list_ds, otherwise it will perform the directory listing twice. ds = list_ds.cache() # Add some indices. ds = ds.enumerate() # Do a rougly 70-30 split. train_list_ds = ds.filter(lambda i, data: i % 10 < 7) test_list_ds = ds.filter(lambda i, data: i % 10 >= 7) # Drop indices. train_list_ds = train_list_ds.map(lambda i, data: data) test_list_ds = test_list_ds.map(lambda i, data: data)\n\nanswered Jan 9, 2020 at 20:15\n\nDan MoldovanDan Moldovan\n\n96566 silver badges88 bronze badges 4\n\n...typically, data is being split e.g. in separate directories... But how would you implement cross validation then?\n\n– problemofficer - n.f. Monica Commented Jan 10, 2020 at 13:17\n\nFor cross-validation you indeed need a dynamic mechanism like the one above.\n\n– Dan Moldovan Commented Jan 10, 2020 at 14:26\n\nWhy will it perform the directory listing twice? Which method specifically?\n\n– problemofficer - n.f. Monica Commented Jan 11, 2020 at 15:16\n\nEach time we call ds.filter in that code, we create an entire pipeline, which is independent of the others. So train_list_ds will list all files and discard every first 7. And test_list_ds will again list all files and discard every last 3. Not a big deal unless you have a very large amount of files.\n\n– Dan Moldovan Commented Jan 13, 2020 at 20:16\n\nBased on Dan Moldovan\'s answer I created a reusable function. Maybe this is useful to other people.\n\ndef split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float): """""" Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are rounded up to two decimal places. @param dataset: the input dataset to split. @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1. @return: a tuple of two tf.data.Datasets as (training, validation) """""" validation_data_percent = round(validation_data_fraction * 100) if not (0 <= validation_data_percent <= 100): raise ValueError(""validation data fraction must be ∈ [0,1]"") dataset = dataset.enumerate() train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent) validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent) # remove enumeration train_dataset = train_dataset.map(lambda f, data: data) validation_dataset = validation_dataset.map(lambda f, data: data) return train_dataset, validation_dataset\n\nedited Jan 14, 2020 at 7:43\n\nanswered Jan 11, 2020 at 16:06\n\nproblemofficer - n.f. Monicaproblemofficer - n.f. Monica\n\n2,23033 gold badges2222 silver badges3636 bronze badges 2\n\nNice! Are the train_dataset and validation_dataset variables then also DataSets that are iterable, or are they fully loaded in memory? Asking because my initial dataset is a large file that I would prefer to not load completely in RAM.\n\n– Tominator Commented Apr 14, 2020 at 12:38\n\n@Tominator They are evaluated lazily just like normal DataSets, so they are not loaded into memory and are ""iterable"" like you said.\n\n– problemofficer - n.f. Monica Commented Apr 15, 2020 at 7:33\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow2.0 or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n0 Split and Recombine Tensorflow Dataset\n\n25 Split tensor into training and test sets\n\n4 How to split own data set to train and validation in Tensorflow CNN\n\n1 Splitting data in training/validation in Tensorflow CIFAR-10 tutorial\n\n3 Keras: How to expand validation_split to generate a third set i.e. test set?\n\n6 How to create train, test and validation splits in tensorflow 2.0\n\n10 Split train data to train and validation by using tensorflow_datasets.load (TF 2.1)\n\n1 How to split a tensorflow dataset into train, test and validation in a Python script?\n\n0 split dataset into train and test using tensorflow\n\n10 Splitting a tensorflow dataset into training, test, and validation sets from keras.preprocessing API\n\n0 How should I split the dataset in the Keras data generator train-valid-test?\n\nHot Network Questions\n\nWhere did Wordsworth describe Keats\'s poetry as ""very pretty paganism""?\n\nCan computer components be damaged if they stay off for a long time?\n\nSubmitting 2 manuscripts explaining the same scientific fact but by two different methods\n\nDHCP assigned addresses following devices/users and routing\n\nHow to reference a picture with two images?\n\nMathematical Induction over two numbers\n\nWhat is wrong with samsara and dukkha from the perspective of advaita philosophy?\n\nCombinatoric Problem in Stardew Valley about Keg Layout\n\nShould I apologise to a professor after a gift authorship attempt, which they refused?\n\nCan I prettify a XML string using Apex?\n\nAre the North Star and the moon ever visible in the night sky at the same time?\n\nGetting fatal error when using manage_media_columns filter of WordPress\n\nIs it possible to have multiple versions of MacOS on the same laptop at the same time?\n\nDid any other European leader praise China for its peace initiatives since the outbreak of the Ukraine war?\n\nMOSFET Datasheet Confusion\n\n8x8 grid with no unmarked L-pentomino\n\nEfficient proof of Bessel\'s correction\n\nIs it alright to display mean CPU usage of algorithm instead of CPU cores available?\n\nWhat is the difference between 居る and 要る?\n\nmore than what people think vs more than people think\n\nHow should I deal with curves in new deck boards during installation?\n\nHow to turn a sum into an integral?\n\nCould a Black Market exist in a cashless society (digital currency)?\n\nRolling median of all K-length ranges more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_1', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nWhat is the canonical way to split tf.Dataset into test and validation subsets?\n\nAsked 4 years, 6 months ago\n\nModified 4 years, 5 months ago\n\nI was following a Tensorflow 2 tutorial on how to load images with pure Tensorflow, because it is supposed to be faster than with Keras. The tutorial ends before showing how to split the resulting dataset (~tf.Dataset) into a train and validation dataset.\n\nI checked the reference for tf.Dataset and it does not contain a split() method.\n\nI tried slicing it manually but tf.Dataset neither contains a size() nor a length() method, so I don\'t see how I could slice it myself.\n\nI can\'t use the validation_split argument of Model.fit() because I need to augment the training dataset but not the validation dataset.\n\nWhat is the intended way to split a tf.Dataset or should I use a different workflow where I won\'t have to do this?\n\nBATCH_SIZE = 32 IMG_HEIGHT = 224 IMG_WIDTH = 224 list_ds = tf.data.Dataset.list_files(str(data_dir/\'*/*\')) def get_label(file_path): # convert the path to a list of path components parts = tf.strings.split(file_path, os.path.sep) # The second to last is the class-directory return parts[-2] == CLASS_NAMES def decode_img(img): # convert the compressed string to a 3D uint8 tensor img = tf.image.decode_jpeg(img, channels=3) # Use `convert_image_dtype` to convert to floats in the [0,1] range. img = tf.image.convert_image_dtype(img, tf.float32) # resize the image to the desired size. return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT]) def process_path(file_path): label = get_label(file_path) # load the raw data from the file as a string img = tf.io.read_file(file_path) img = decode_img(img) return img, label labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE) #... #...\n\nI can either split list_ds (list of files) or labeled_ds (list of images and labels), but how?\n\nImprove this question\n\nedited Jan 9, 2020 at 17:54\n\nproblemofficer - n.f. Monica\n\nasked Jan 9, 2020 at 17:35\n\nproblemofficer - n.f. Monicaproblemofficer - n.f. Monica\n\n2,23033 gold badges2222 silver badges3636 bronze badges 3\n\nI saw several people using scikit-learn. Firstly, I would like to avoid having another dependency just for this one task. Secondly, I assume there must be something that simple in Tensorflow 2.\n\n– problemofficer - n.f. Monica Commented Jan 9, 2020 at 17:45\n\ntensorflow.org/datasets/api_docs/python/tfds/Split\n\n– albert Commented Jan 9, 2020 at 18:06\n\n@albert I saw that page already and the ""guide on splits"" but this does not seem to answer my question.\n\n– problemofficer - n.f. Monica Commented Jan 9, 2020 at 20:04\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nI don\'t think there\'s a canonical way (typically, data is being split e.g. in separate directories). But here\'s a recipe that will let you do it dynamically:\n\n# Caveat: cache list_ds, otherwise it will perform the directory listing twice. ds = list_ds.cache() # Add some indices. ds = ds.enumerate() # Do a rougly 70-30 split. train_list_ds = ds.filter(lambda i, data: i % 10 < 7) test_list_ds = ds.filter(lambda i, data: i % 10 >= 7) # Drop indices. train_list_ds = train_list_ds.map(lambda i, data: data) test_list_ds = test_list_ds.map(lambda i, data: data)\n\nanswered Jan 9, 2020 at 20:15\n\nDan MoldovanDan Moldovan\n\n96566 silver badges88 bronze badges 4\n\n...typically, data is being split e.g. in separate directories... But how would you implement cross validation then?\n\n– problemofficer - n.f. Monica Commented Jan 10, 2020 at 13:17\n\nFor cross-validation you indeed need a dynamic mechanism like the one above.\n\n– Dan Moldovan Commented Jan 10, 2020 at 14:26\n\nWhy will it perform the directory listing twice? Which method specifically?\n\n– problemofficer - n.f. Monica Commented Jan 11, 2020 at 15:16\n\nEach time we call ds.filter in that code, we create an entire pipeline, which is independent of the others. So train_list_ds will list all files and discard every first 7. And test_list_ds will again list all files and discard every last 3. Not a big deal unless you have a very large amount of files.\n\n– Dan Moldovan Commented Jan 13, 2020 at 20:16\n\nBased on Dan Moldovan\'s answer I created a reusable function. Maybe this is useful to other people.\n\ndef split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float): """""" Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are rounded up to two decimal places. @param dataset: the input dataset to split. @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1. @return: a tuple of two tf.data.Datasets as (training, validation) """""" validation_data_percent = round(validation_data_fraction * 100) if not (0 <= validation_data_percent <= 100): raise ValueError(""validation data fraction must be ∈ [0,1]"") dataset = dataset.enumerate() train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent) validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent) # remove enumeration train_dataset = train_dataset.map(lambda f, data: data) validation_dataset = validation_dataset.map(lambda f, data: data) return train_dataset, validation_dataset\n\nedited Jan 14, 2020 at 7:43\n\nanswered Jan 11, 2020 at 16:06\n\nproblemofficer - n.f. Monicaproblemofficer - n.f. Monica\n\n2,23033 gold badges2222 silver badges3636 bronze badges 2\n\nNice! Are the train_dataset and validation_dataset variables then also DataSets that are iterable, or are they fully loaded in memory? Asking because my initial dataset is a large file that I would prefer to not load completely in RAM.\n\n– Tominator Commented Apr 14, 2020 at 12:38\n\n@Tominator They are evaluated lazily just like normal DataSets, so they are not loaded into memory and are ""iterable"" like you said.\n\n– problemofficer - n.f. Monica Commented Apr 15, 2020 at 7:33\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow2.0 or ask your own question.\n\nWe spent a sprint addressing your requests — here’s how it went\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network...\n\nWhat makes a homepage useful for logged-in users\n\n0 Split and Recombine Tensorflow Dataset\n\n25 Split tensor into training and test sets\n\n4 How to split own data set to train and validation in Tensorflow CNN\n\n1 Splitting data in training/validation in Tensorflow CIFAR-10 tutorial\n\n3 Keras: How to expand validation_split to generate a third set i.e. test set?\n\n6 How to create train, test and validation splits in tensorflow 2.0\n\n10 Split train data to train and validation by using tensorflow_datasets.load (TF 2.1)\n\n1 How to split a tensorflow dataset into train, test and validation in a Python script?\n\n0 split dataset into train and test using tensorflow\n\n10 Splitting a tensorflow dataset into training, test, and validation sets from keras.preprocessing API\n\n0 How should I split the dataset in the Keras data generator train-valid-test?\n\nHot Network Questions\n\nWhere did Wordsworth describe Keats\'s poetry as ""very pretty paganism""?\n\nCan computer components be damaged if they stay off for a long time?\n\nSubmitting 2 manuscripts explaining the same scientific fact but by two different methods\n\nDHCP assigned addresses following devices/users and routing\n\nHow to reference a picture with two images?\n\nMathematical Induction over two numbers\n\nWhat is wrong with samsara and dukkha from the perspective of advaita philosophy?\n\nCombinatoric Problem in Stardew Valley about Keg Layout\n\nShould I apologise to a professor after a gift authorship attempt, which they refused?\n\nCan I prettify a XML string using Apex?\n\nAre the North Star and the moon ever visible in the night sky at the same time?\n\nGetting fatal error when using manage_media_columns filter of WordPress\n\nIs it possible to have multiple versions of MacOS on the same laptop at the same time?\n\nDid any other European leader praise China for its peace initiatives since the outbreak of the Ukraine war?\n\nMOSFET Datasheet Confusion\n\n8x8 grid with no unmarked L-pentomino\n\nEfficient proof of Bessel\'s correction\n\nIs it alright to display mean CPU usage of algorithm instead of CPU cores available?\n\nWhat is the difference between 居る and 要る?\n\nmore than what people think vs more than people think\n\nHow should I deal with curves in new deck boards during installation?\n\nHow to turn a sum into an integral?\n\nCould a Black Market exist in a cashless society (digital currency)?\n\nRolling median of all K-length ranges more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-07-09T13:29:50', 'title': 'python - What is the canonical way to split tf.Dataset into test and validation subsets? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/59669413/what-is-the-canonical-way-to-split-tf-dataset-into-test-and-validation-subsets'}), Document(page_content='Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\n2024 Developer survey is here and we would like to hear from you! Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow do I split Tensorflow datasets?\n\nAsked 5 years, 11 months ago\n\nModified 7 months ago\n\nI have a tensorflow dataset based on one .tfrecord file. How do I split the dataset into test and train datasets? E.g. 70% Train and 30% test?\n\nMy Tensorflow Version: 1.8 I\'ve checked, there is no ""split_v"" function as mentioned in the possible duplicate. Also I am working with a tfrecord file.\n\nImprove this question\n\nedited Jul 1, 2018 at 17:13\n\nasked Jul 1, 2018 at 17:00\n\nLukas HestermeyerLukas Hestermeyer\n\n95311 gold badge77 silver badges2020 bronze badges 4\n\nPossible duplicate of Split inputs into training and test sets\n\n– ted Jul 1, 2018 at 17:06\n\nDoes this answer your question? Split a dataset created by Tensorflow dataset API in to Train and Test?\n\n– desertnaut Dec 3, 2020 at 0:17\n\nThe question was already answered years ago, but thanks for the link\n\n– Lukas Hestermeyer Dec 10, 2020 at 12:25\n\nAlso related: stackoverflow.com/questions/54519309/…\n\n– xdhmoore Jan 22, 2021 at 2:11\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou may use Dataset.take() and Dataset.skip():\n\ntrain_size = int(0.7 * DATASET_SIZE) val_size = int(0.15 * DATASET_SIZE) test_size = int(0.15 * DATASET_SIZE) full_dataset = tf.data.TFRecordDataset(FLAGS.input_file) full_dataset = full_dataset.shuffle() train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) val_dataset = test_dataset.skip(test_size) test_dataset = test_dataset.take(test_size)\n\nFor more generality, I gave an example using a 70/15/15 train/val/test split but if you don\'t need a test or a val set, just ignore the last 2 lines.\n\nCreates a Dataset with at most count elements from this dataset.\n\nCreates a Dataset that skips count elements from this dataset.\n\nYou may also want to look into Dataset.shard():\n\nCreates a Dataset that includes only 1/num_shards of this dataset.\n\nedited Oct 8, 2019 at 13:03\n\n5271010 silver badges2323 bronze badges\n\nanswered Jul 1, 2018 at 20:40\n\n14.3k1010 gold badges6666 silver badges110110 bronze badges 6\n\nIsn\'t there a randomness issue here if the dataset is much larger than the shuffle buffer size? Since samples are shuffled only within the (relatively) small buffer, this means approximately the first 70% of samples will be the training set, next 15% will be the test set, etc. If the data is ordered somehow this would introduce bias into the training results. Probably the solution is to shard the data, then shuffle it, then split it.\n\n– buzjwa Jul 15, 2018 at 13:19\n\nI agree. Good comment. I suppose most use cases would then simply need to shuffle the whole dataset at once but to be truly scalable you\'re right\n\n– ted Jul 15, 2018 at 18:29\n\nNote that skip actually iterates over the dataset so it can cause big latency on large dataset\n\n– Tomasz Sętkowski Mar 14, 2019 at 22:04\n\nI don\'t recommend this as train and test sets are not disjoint: it happens that the test set contains elements of the training set\n\n– xdola Apr 15, 2020 at 17:38\n\nYou\'ll want to set shuffle(reshuffle_each_iteration=False). Without that, each time a new iteration of training starts, items from the training set and validation set will get shuffled into each-other.\n\n– Marc Stogaitis Sep 10, 2020 at 7:54\n\n | Show 1 more comment\n\nThis question is similar to this one and this one, and I am afraid we have not had a satisfactory answer yet.\n\nUsing take() and skip() requires knowing the dataset size. What if I don\'t know that, or don\'t want to find out?\n\nUsing shard() only gives 1 / num_shards of dataset. What if I want the rest?\n\nI try to present a better solution below, tested on TensorFlow 2 only. Assuming you already have a shuffled dataset, you can then use filter() to split it into two:\n\nimport tensorflow as tf all = tf.data.Dataset.from_tensor_slices(list(range(1, 21))) \\ .shuffle(10, reshuffle_each_iteration=False) test_dataset = all.enumerate() \\ .filter(lambda x,y: x % 4 == 0) \\ .map(lambda x,y: y) train_dataset = all.enumerate() \\ .filter(lambda x,y: x % 4 != 0) \\ .map(lambda x,y: y) for i in test_dataset: print(i) print() for i in train_dataset: print(i)\n\nThe parameter reshuffle_each_iteration=False is important. It makes sure the original dataset is shuffled once and no more. Otherwise, the two resulting sets may have some overlaps.\n\nUse enumerate() to add an index.\n\nUse filter(lambda x,y: x % 4 == 0) to take 1 sample out of 4. Likewise, x % 4 != 0 takes 3 out of 4.\n\nUse map(lambda x,y: y) to strip the index and recover the original sample.\n\nThis example achieves a 75/25 split.\n\nx % 5 == 0 and x % 5 != 0 gives a 80/20 split.\n\nIf you really want a 70/30 split, x % 10 < 3 and x % 10 >= 3 should do.\n\nAs of TensorFlow 2.0.0, above code may result in some warnings due to AutoGraph\'s limitations. To eliminate those warnings, declare all lambda functions separately:\n\ndef is_test(x, y): return x % 4 == 0 def is_train(x, y): return not is_test(x, y) recover = lambda x,y: y test_dataset = all.enumerate() \\ .filter(is_test) \\ .map(recover) train_dataset = all.enumerate() \\ .filter(is_train) \\ .map(recover)\n\nThis gives no warning on my machine. And making is_train() to be not is_test() is definitely a good practice.\n\nedited Oct 18, 2019 at 15:09\n\nanswered Oct 18, 2019 at 13:51\n\n5,78933 gold badges3030 silver badges3636 bronze badges 10\n\nNice answer. Suggestion: caching (with .cache()) each subsample will prevent for tensorflow performing a full iteration each time (the first full iteration for each subset seems unavoidable).\n\n– Javier JC Jan 26, 2020 at 6:41\n\nI would assume this reads the entire dataset once, but are the test_dataset and train_dataset variables then also DataSets that are iterable, or are they fully loaded in memory? Asking because I have a large file that I would prefer to not load completely in RAM.\n\n– Tominator Apr 14, 2020 at 12:06\n\n@Tominator, I am not sure. The way my example is set up, test_dataset being read in full before train_dataset is read, train_dataset has to be fully stored in RAM for some time, especially because I tell it to shuffle only once. But, what if the reading is controlled so that test_dataset is read once for every 3 time train_dataset is read? That way, data does not have to be fully stored in RAM. Is that the actual implementation? I suspect so. TF dataset (and this kind of data-pulling API in general) is designed precisely to deal with huge dataset ........\n\n– Nick Lee Apr 14, 2020 at 13:03\n\nYou should not use all as a variable name as it overrides Python\'s built-in all(enum) function\n\n– ted Nov 20, 2020 at 23:55\n\nIt would be good to mention that a 70/20/10% split for train/val/test datasets are possible too with modulo 7. test_dataset = dataset.enumerate().filter(lambda x,y: x\x10==7).map(lambda x,y: y) val_dataset = dataset.enumerate().filter(lambda x,y: x\x10>7).map(lambda x,y: y) train_dataset = dataset.enumerate().filter(lambda x,y: x\x10<7).map(lambda x,y: y)\n\n– Tim Jan 11, 2021 at 5:24\n\n | Show 5 more comments\n\nI will first explain why the accepted answer is wrong and secondly will provide a simple working solution, using take(), skip() and seed.\n\nWhen working with pipelines, such as TF/Torch Datasets, beware of lazy evaluation. Avoid:\n\n# DONT full_dataset = full_dataset.shuffle(10) train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size)\n\nbecause take and skip will synchronize to single shuffle, but rather gets executed as shuffle+take and shuffle+skip separately (yes !), overlapping typically in 80%*20%=16% of cases. So, information leak.\n\nPlay with this code in case of doubt\n\nimport tensorflow as tf def gen_data(): return iter(range(10)) full_dataset = tf.data.Dataset.from_generator( gen_data, output_signature=tf.TensorSpec(shape=(),dtype=tf.int32,name=""element"")) train_size = 8 # WRONG WAY full_dataset = full_dataset.shuffle(10) train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) A = set(train_dataset.as_numpy_iterator()) B = set(test_dataset.as_numpy_iterator()) # EXPECT OVERLAP assert A.intersection(B)==set() print(list(A)) print(list(B))\n\nNow, what works is repeating and seeding shuffle in both train and test datasets, which is also good for reproducibility. This should work with any deterministically ordered iterator:\n\nimport tensorflow as tf def gen_data(): return iter(range(10)) ds = tf.data.Dataset.from_generator( gen_data, output_signature=tf.TensorSpec(shape=(),dtype=tf.int32,name=""element"")) SEED = 42 # NOTE: change this ds_train = ds.shuffle(100,seed=SEED).take(8).shuffle(100) ds_test = ds.shuffle(100,seed=SEED).skip(8) A = set(ds_train.as_numpy_iterator()) B = set(ds_test.as_numpy_iterator()) assert A.intersection(B)==set() print(list(A)) print(list(B))\n\nBy playing with SEED you can for instance inspect/estimate generalization (bootstraping in place of cross-validation).\n\nedited Oct 27, 2023 at 16:23\n\nanswered Oct 27, 2023 at 14:44\n\nMaciej SkorskiMaciej Skorski\n\n3,0451212 silver badges1717 bronze badges 3\n\nThen, do we just need to use the same seed? Or do we also need to use iterators?\n\n– skan Jan 18 at 20:19\n\nwhat about: full_dataset = full_dataset.shuffle(10, seed=SEED, reshuffle_each_iteration=False) train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size)\n\n– ATES Jun 1 at 8:28\n\nfull_dataset = full_dataset.shuffle(10, seed=SEED, reshuffle_each_iteration=False) train_dataset = full_dataset.take(train_size).shuffle(10) test_dataset = full_dataset.skip(train_size)\n\n– ATES Jun 1 at 8:42\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow-datasets or ask your own question.\n\nHow to prevent your new chatbot from giving away company secrets\n\nIntroducing Staging Ground: The private space to get feedback on questions...\n\nTesting a new version of Stack Overflow Jobs\n\nThe 2024 Developer Survey Is Live\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nShould we burninate the [tax] tag?\n\nThe return of Staging Ground to Stack Overflow\n\n0 Create Train Test Split on PrefetchDataset\n\n72 Split a dataset created by Tensorflow dataset API in to Train and Test?\n\n25 Split tensor into training and test sets\n\n14 Split .tfrecords file into many .tfrecords files\n\n8 ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found: (keras.preprocessing.sequence.TimeseriesGenerator object)\n\n6 How to create train, test and validation splits in tensorflow 2.0\n\n2 Filter Tensorflow dataset by id\n\n2 Split a dataset issue in Tensorflow dataset API\n\n0 Is there a way to partition a tf.Dataset with TensorFlow’s Dataset API?\n\n1 How to perform sklearn style train-test split on feature and label tensors using built in tensorflow methods?\n\nSee more linked questions\n\n72 Split a dataset created by Tensorflow dataset API in to Train and Test?\n\n2 Split tensorflow dataset in dataset per class\n\n2 Split a dataset issue in Tensorflow dataset API\n\n2 Trouble with splitting data from Tensorflow Datasets\n\n0 Spliting datasets with tfds\n\n0 Split and Recombine Tensorflow Dataset\n\n0 How to split a tensorflow dataset\n\n0 TensorFlow Dataset train/test split\n\n0 Split tf tf.data.Dataset tuple into several datasets\n\n0 loading data using TensorFlow Datasets and splitting\n\nHot Network Questions\n\nWhat happens if two reviewers have incompatible recommendations for a paper?\n\nHow to push back against request to use personal laptop at conference due to risk of machine compromise\n\nWhich was the first story to feature humans achieving immortality through science and technology advancements?\n\nIs rolling resistance that significant?\n\nConverses to Cartan\'s Theorem B\n\nIf Trump pardons himself regarding the federal crime he tried to hide by cooking the books, is the New York conviction void?\n\nFalsifying Business Records Unanimous Jury\n\nWhat math object is the wire star on Will Hunting\'s prof\'s desk?\n\nIs there an expression that can be used to evaluate the threshold that characterizes a substantial difference between sample means for t-test?\n\nHow can God not be a magician if Jesus\'s conception was supernatural?\n\nCentre of mass thought experiment\n\nPunishment for breaking Yom Tov\n\nUpdated database for polling?\n\nHow to get more accurate performances with difficult passages (eg. the end of Chopin Nocturne in C# minor)\n\nSpecify the Method for `NIntegrate` to evaluate a integral of special functions\n\nAre there any *real* reasons not to long hold leveraged ETFS like TQQQ?\n\nCan subpanel be mounted on the backside of the main panel wall?\n\nKorsør - Nyborg train: is a bicycle with trailer possible?\n\nUnderstanding the Motivation Behind Euclidean Domains\n\nConvert string name to uppercase\n\nExamples of finitely presented subgroups of GL(n, Z) with unsolvable decision problems\n\nWhat does ""Belgian"" really mean, in the Fast and Furious series?\n\nDo I still need OnPush if my app is Zoneless? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_0', 'snippet': 'Skip to main content\n\nStack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\n2024 Developer survey is here and we would like to hear from you! Take the 2024 Developer Survey\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nHow do I split Tensorflow datasets?\n\nAsked 5 years, 11 months ago\n\nModified 7 months ago\n\nI have a tensorflow dataset based on one .tfrecord file. How do I split the dataset into test and train datasets? E.g. 70% Train and 30% test?\n\nMy Tensorflow Version: 1.8 I\'ve checked, there is no ""split_v"" function as mentioned in the possible duplicate. Also I am working with a tfrecord file.\n\nImprove this question\n\nedited Jul 1, 2018 at 17:13\n\nasked Jul 1, 2018 at 17:00\n\nLukas HestermeyerLukas Hestermeyer\n\n95311 gold badge77 silver badges2020 bronze badges 4\n\nPossible duplicate of Split inputs into training and test sets\n\n– ted Jul 1, 2018 at 17:06\n\nDoes this answer your question? Split a dataset created by Tensorflow dataset API in to Train and Test?\n\n– desertnaut Dec 3, 2020 at 0:17\n\nThe question was already answered years ago, but thanks for the link\n\n– Lukas Hestermeyer Dec 10, 2020 at 12:25\n\nAlso related: stackoverflow.com/questions/54519309/…\n\n– xdhmoore Jan 22, 2021 at 2:11\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nYou may use Dataset.take() and Dataset.skip():\n\ntrain_size = int(0.7 * DATASET_SIZE) val_size = int(0.15 * DATASET_SIZE) test_size = int(0.15 * DATASET_SIZE) full_dataset = tf.data.TFRecordDataset(FLAGS.input_file) full_dataset = full_dataset.shuffle() train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) val_dataset = test_dataset.skip(test_size) test_dataset = test_dataset.take(test_size)\n\nFor more generality, I gave an example using a 70/15/15 train/val/test split but if you don\'t need a test or a val set, just ignore the last 2 lines.\n\nCreates a Dataset with at most count elements from this dataset.\n\nCreates a Dataset that skips count elements from this dataset.\n\nYou may also want to look into Dataset.shard():\n\nCreates a Dataset that includes only 1/num_shards of this dataset.\n\nedited Oct 8, 2019 at 13:03\n\n5271010 silver badges2323 bronze badges\n\nanswered Jul 1, 2018 at 20:40\n\n14.3k1010 gold badges6666 silver badges110110 bronze badges 6\n\nIsn\'t there a randomness issue here if the dataset is much larger than the shuffle buffer size? Since samples are shuffled only within the (relatively) small buffer, this means approximately the first 70% of samples will be the training set, next 15% will be the test set, etc. If the data is ordered somehow this would introduce bias into the training results. Probably the solution is to shard the data, then shuffle it, then split it.\n\n– buzjwa Jul 15, 2018 at 13:19\n\nI agree. Good comment. I suppose most use cases would then simply need to shuffle the whole dataset at once but to be truly scalable you\'re right\n\n– ted Jul 15, 2018 at 18:29\n\nNote that skip actually iterates over the dataset so it can cause big latency on large dataset\n\n– Tomasz Sętkowski Mar 14, 2019 at 22:04\n\nI don\'t recommend this as train and test sets are not disjoint: it happens that the test set contains elements of the training set\n\n– xdola Apr 15, 2020 at 17:38\n\nYou\'ll want to set shuffle(reshuffle_each_iteration=False). Without that, each time a new iteration of training starts, items from the training set and validation set will get shuffled into each-other.\n\n– Marc Stogaitis Sep 10, 2020 at 7:54\n\n | Show 1 more comment\n\nThis question is similar to this one and this one, and I am afraid we have not had a satisfactory answer yet.\n\nUsing take() and skip() requires knowing the dataset size. What if I don\'t know that, or don\'t want to find out?\n\nUsing shard() only gives 1 / num_shards of dataset. What if I want the rest?\n\nI try to present a better solution below, tested on TensorFlow 2 only. Assuming you already have a shuffled dataset, you can then use filter() to split it into two:\n\nimport tensorflow as tf all = tf.data.Dataset.from_tensor_slices(list(range(1, 21))) \\ .shuffle(10, reshuffle_each_iteration=False) test_dataset = all.enumerate() \\ .filter(lambda x,y: x % 4 == 0) \\ .map(lambda x,y: y) train_dataset = all.enumerate() \\ .filter(lambda x,y: x % 4 != 0) \\ .map(lambda x,y: y) for i in test_dataset: print(i) print() for i in train_dataset: print(i)\n\nThe parameter reshuffle_each_iteration=False is important. It makes sure the original dataset is shuffled once and no more. Otherwise, the two resulting sets may have some overlaps.\n\nUse enumerate() to add an index.\n\nUse filter(lambda x,y: x % 4 == 0) to take 1 sample out of 4. Likewise, x % 4 != 0 takes 3 out of 4.\n\nUse map(lambda x,y: y) to strip the index and recover the original sample.\n\nThis example achieves a 75/25 split.\n\nx % 5 == 0 and x % 5 != 0 gives a 80/20 split.\n\nIf you really want a 70/30 split, x % 10 < 3 and x % 10 >= 3 should do.\n\nAs of TensorFlow 2.0.0, above code may result in some warnings due to AutoGraph\'s limitations. To eliminate those warnings, declare all lambda functions separately:\n\ndef is_test(x, y): return x % 4 == 0 def is_train(x, y): return not is_test(x, y) recover = lambda x,y: y test_dataset = all.enumerate() \\ .filter(is_test) \\ .map(recover) train_dataset = all.enumerate() \\ .filter(is_train) \\ .map(recover)\n\nThis gives no warning on my machine. And making is_train() to be not is_test() is definitely a good practice.\n\nedited Oct 18, 2019 at 15:09\n\nanswered Oct 18, 2019 at 13:51\n\n5,78933 gold badges3030 silver badges3636 bronze badges 10\n\nNice answer. Suggestion: caching (with .cache()) each subsample will prevent for tensorflow performing a full iteration each time (the first full iteration for each subset seems unavoidable).\n\n– Javier JC Jan 26, 2020 at 6:41\n\nI would assume this reads the entire dataset once, but are the test_dataset and train_dataset variables then also DataSets that are iterable, or are they fully loaded in memory? Asking because I have a large file that I would prefer to not load completely in RAM.\n\n– Tominator Apr 14, 2020 at 12:06\n\n@Tominator, I am not sure. The way my example is set up, test_dataset being read in full before train_dataset is read, train_dataset has to be fully stored in RAM for some time, especially because I tell it to shuffle only once. But, what if the reading is controlled so that test_dataset is read once for every 3 time train_dataset is read? That way, data does not have to be fully stored in RAM. Is that the actual implementation? I suspect so. TF dataset (and this kind of data-pulling API in general) is designed precisely to deal with huge dataset ........\n\n– Nick Lee Apr 14, 2020 at 13:03\n\nYou should not use all as a variable name as it overrides Python\'s built-in all(enum) function\n\n– ted Nov 20, 2020 at 23:55\n\nIt would be good to mention that a 70/20/10% split for train/val/test datasets are possible too with modulo 7. test_dataset = dataset.enumerate().filter(lambda x,y: x\x10==7).map(lambda x,y: y) val_dataset = dataset.enumerate().filter(lambda x,y: x\x10>7).map(lambda x,y: y) train_dataset = dataset.enumerate().filter(lambda x,y: x\x10<7).map(lambda x,y: y)\n\n– Tim Jan 11, 2021 at 5:24\n\n | Show 5 more comments\n\nI will first explain why the accepted answer is wrong and secondly will provide a simple working solution, using take(), skip() and seed.\n\nWhen working with pipelines, such as TF/Torch Datasets, beware of lazy evaluation. Avoid:\n\n# DONT full_dataset = full_dataset.shuffle(10) train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size)\n\nbecause take and skip will synchronize to single shuffle, but rather gets executed as shuffle+take and shuffle+skip separately (yes !), overlapping typically in 80%*20%=16% of cases. So, information leak.\n\nPlay with this code in case of doubt\n\nimport tensorflow as tf def gen_data(): return iter(range(10)) full_dataset = tf.data.Dataset.from_generator( gen_data, output_signature=tf.TensorSpec(shape=(),dtype=tf.int32,name=""element"")) train_size = 8 # WRONG WAY full_dataset = full_dataset.shuffle(10) train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) A = set(train_dataset.as_numpy_iterator()) B = set(test_dataset.as_numpy_iterator()) # EXPECT OVERLAP assert A.intersection(B)==set() print(list(A)) print(list(B))\n\nNow, what works is repeating and seeding shuffle in both train and test datasets, which is also good for reproducibility. This should work with any deterministically ordered iterator:\n\nimport tensorflow as tf def gen_data(): return iter(range(10)) ds = tf.data.Dataset.from_generator( gen_data, output_signature=tf.TensorSpec(shape=(),dtype=tf.int32,name=""element"")) SEED = 42 # NOTE: change this ds_train = ds.shuffle(100,seed=SEED).take(8).shuffle(100) ds_test = ds.shuffle(100,seed=SEED).skip(8) A = set(ds_train.as_numpy_iterator()) B = set(ds_test.as_numpy_iterator()) assert A.intersection(B)==set() print(list(A)) print(list(B))\n\nBy playing with SEED you can for instance inspect/estimate generalization (bootstraping in place of cross-validation).\n\nedited Oct 27, 2023 at 16:23\n\nanswered Oct 27, 2023 at 14:44\n\nMaciej SkorskiMaciej Skorski\n\n3,0451212 silver badges1717 bronze badges 3\n\nThen, do we just need to use the same seed? Or do we also need to use iterators?\n\n– skan Jan 18 at 20:19\n\nwhat about: full_dataset = full_dataset.shuffle(10, seed=SEED, reshuffle_each_iteration=False) train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size)\n\n– ATES Jun 1 at 8:28\n\nfull_dataset = full_dataset.shuffle(10, seed=SEED, reshuffle_each_iteration=False) train_dataset = full_dataset.take(train_size).shuffle(10) test_dataset = full_dataset.skip(train_size)\n\n– ATES Jun 1 at 8:42\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow-datasets or ask your own question.\n\nHow to prevent your new chatbot from giving away company secrets\n\nIntroducing Staging Ground: The private space to get feedback on questions...\n\nTesting a new version of Stack Overflow Jobs\n\nThe 2024 Developer Survey Is Live\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nShould we burninate the [tax] tag?\n\nThe return of Staging Ground to Stack Overflow\n\n0 Create Train Test Split on PrefetchDataset\n\n72 Split a dataset created by Tensorflow dataset API in to Train and Test?\n\n25 Split tensor into training and test sets\n\n14 Split .tfrecords file into many .tfrecords files\n\n8 ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found: (keras.preprocessing.sequence.TimeseriesGenerator object)\n\n6 How to create train, test and validation splits in tensorflow 2.0\n\n2 Filter Tensorflow dataset by id\n\n2 Split a dataset issue in Tensorflow dataset API\n\n0 Is there a way to partition a tf.Dataset with TensorFlow’s Dataset API?\n\n1 How to perform sklearn style train-test split on feature and label tensors using built in tensorflow methods?\n\nSee more linked questions\n\n72 Split a dataset created by Tensorflow dataset API in to Train and Test?\n\n2 Split tensorflow dataset in dataset per class\n\n2 Split a dataset issue in Tensorflow dataset API\n\n2 Trouble with splitting data from Tensorflow Datasets\n\n0 Spliting datasets with tfds\n\n0 Split and Recombine Tensorflow Dataset\n\n0 How to split a tensorflow dataset\n\n0 TensorFlow Dataset train/test split\n\n0 Split tf tf.data.Dataset tuple into several datasets\n\n0 loading data using TensorFlow Datasets and splitting\n\nHot Network Questions\n\nWhat happens if two reviewers have incompatible recommendations for a paper?\n\nHow to push back against request to use personal laptop at conference due to risk of machine compromise\n\nWhich was the first story to feature humans achieving immortality through science and technology advancements?\n\nIs rolling resistance that significant?\n\nConverses to Cartan\'s Theorem B\n\nIf Trump pardons himself regarding the federal crime he tried to hide by cooking the books, is the New York conviction void?\n\nFalsifying Business Records Unanimous Jury\n\nWhat math object is the wire star on Will Hunting\'s prof\'s desk?\n\nIs there an expression that can be used to evaluate the threshold that characterizes a substantial difference between sample means for t-test?\n\nHow can God not be a magician if Jesus\'s conception was supernatural?\n\nCentre of mass thought experiment\n\nPunishment for breaking Yom Tov\n\nUpdated database for polling?\n\nHow to get more accurate performances with difficult passages (eg. the end of Chopin Nocturne in C# minor)\n\nSpecify the Method for `NIntegrate` to evaluate a integral of special functions\n\nAre there any *real* reasons not to long hold leveraged ETFS like TQQQ?\n\nCan subpanel be mounted on the backside of the main panel wall?\n\nKorsør - Nyborg train: is a bicycle with trailer possible?\n\nUnderstanding the Motivation Behind Euclidean Domains\n\nConvert string name to uppercase\n\nExamples of finitely presented subgroups of GL(n, Z) with unsolvable decision problems\n\nWhat does ""Belgian"" really mean, in the Fast and Furious series?\n\nDo I still need OnPush if my app is Zoneless? more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-06-05T06:44:29', 'title': 'How do I split Tensorflow datasets? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/51125266/how-do-i-split-tensorflow-datasets'}), Document(page_content='Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nSplit a dataset created by Tensorflow dataset API in to Train and Test?\n\nAsked 6 years, 5 months ago\n\nModified 8 months ago\n\nDoes anyone know how to split a dataset created by the dataset API (tf.data.Dataset) in Tensorflow into Test and Train?\n\nImprove this question\n\nedited Dec 2, 2020 at 22:10\n\n59.5k2929 gold badges149149 silver badges169169 bronze badges\n\nasked Jan 11, 2018 at 18:34\n\n88711 gold badge77 silver badges88 bronze badges 2\n\ntake(), skip(), and shard() all have their own problems. I just posted my answer over here. I hope it better answers your question.\n\n– Nick Lee Commented Oct 18, 2019 at 13:55\n\nuse Keras - model.fit(dataset,.., validation.split=0.7, ...) see its all possible arguments\n\n– JeeyCi Commented Apr 29, 2022 at 18:51\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nAssuming you have all_dataset variable of tf.data.Dataset type:\n\ntest_dataset = all_dataset.take(1000) train_dataset = all_dataset.skip(1000)\n\nTest dataset now has first 1000 elements and the rest goes for training.\n\nanswered May 5, 2018 at 3:10\n\n1,6041212 silver badges1212 bronze badges 3\n\nAs also mentioned in ted\'s answer, adding all_dataset.shuffle() allows for a shuffled split. Possibly add as code comment in answer like so? # all_dataset = all_dataset.shuffle() # in case you want a shuffled split\n\n– Christian Steinmeyer Commented Oct 2, 2020 at 14:15\n\nTensorFlow 2.10.0 will have a utility function for splitting, see my answer: stackoverflow.com/a/73591823/1389680\n\n– Robert Pollak Commented Sep 3, 2022 at 11:20\n\ntake and skip return TfTakeDatasets/SkipDatasets which have less functionality than TfDatasets. Does anyone know how to map those to tfDatasets or split into train test splits and get back TfDataset objects?\n\n– Nikos H. Commented Sep 5, 2022 at 14:33\n\nYou may use Dataset.take() and Dataset.skip():\n\ntrain_size = int(0.7 * DATASET_SIZE) val_size = int(0.15 * DATASET_SIZE) test_size = int(0.15 * DATASET_SIZE) full_dataset = tf.data.TFRecordDataset(FLAGS.input_file) full_dataset = full_dataset.shuffle() train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) val_dataset = test_dataset.skip(val_size) test_dataset = test_dataset.take(test_size)\n\nFor more generality, I gave an example using a 70/15/15 train/val/test split but if you don\'t need a test or a val set, just ignore the last 2 lines.\n\nCreates a Dataset with at most count elements from this dataset.\n\nCreates a Dataset that skips count elements from this dataset.\n\nYou may also want to look into Dataset.shard():\n\nCreates a Dataset that includes only 1/num_shards of this dataset.\n\nDisclaimer I stumbled upon this question after answering this one so I thought I\'d spread the love\n\nanswered Jul 10, 2018 at 6:42\n\n14.3k1010 gold badges6666 silver badges111111 bronze badges 7\n\nThank you very much @ted! Is there a way to divide the dataset in a stratified way? Or, alternatively, how can we have an idea of the class proportions (suppose a binary problem) after the train/val/test split? Thanks a lot in advance!\n\n– Tommaso Di Noto Commented Aug 27, 2019 at 13:21\n\nHave a look at this blogpost I wrote; eventhough it\'s for multilabel datasets, should be easily usable for single label, multiclass datasets -> vict0rs.ch/2018/06/17/multilabel-text-classification-tensorflow\n\n– ted Commented Aug 27, 2019 at 13:50\n\nThis causes my train,validation and test datasets to have overlap between them. Is this supposed to happen and not a big deal? I would assume it\'s not a good idea to have the model train on validation and test data.\n\n– bw0248 Commented Jan 24, 2020 at 19:51\n\n@c_student I had the same problem and I figured out what I was missing: when you shuffle use the option reshuffle_each_iteration=False otherwise elements could be repeated in train, test and val\n\n– xdola Commented Apr 15, 2020 at 17:57\n\nThis is very true @xdola, and in particular when using list_files you should use shuffle=False and then shuffle with the .shuffle with reshuffle_each_iteration=False.\n\n– Zaccharie Ramzi Commented May 27, 2020 at 9:48\n\n | Show 2 more comments\n\nMost of the answers here use take() and skip(), which requires knowing the size of your dataset before hand. This isn\'t always possible, or is difficult/intensive to ascertain.\n\nInstead what you can do is to essentially slice the dataset up so that 1 every N records becomes a validation record.\n\nTo accomplish this, lets start with a simple dataset of 0-9:\n\ndataset = tf.data.Dataset.range(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nNow for our example, we\'re going to slice it so that we have a 3/1 train/validation split. Meaning 3 records will go to training, then 1 record to validation, then repeat.\n\nsplit = 3 dataset_train = dataset.window(split, split + 1).flat_map(lambda ds: ds) # [0, 1, 2, 4, 5, 6, 8, 9] dataset_validation = dataset.skip(split).window(1, split + 1).flat_map(lambda ds: ds) # [3, 7]\n\nSo the first dataset.window(split, split + 1) says to grab split number (3) of elements, then advance split + 1 elements, and repeat. That + 1 effectively skips the 1 element we\'re going to use in our validation dataset. The flat_map(lambda ds: ds) is because window() returns the results in batches, which we don\'t want. So we flatten it back out.\n\nThen for the validation data we first skip(split), which skips over the first split number (3) of elements that were grabbed in the first training window, so we start our iteration on the 4th element. The window(1, split + 1) then grabs 1 element, advances split + 1 (4), and repeats.\n\nNote on nested datasets: The above example works well for simple datasets, but flat_map() will generate an error if the dataset is nested. To address this, you can swap out the flat_map() with a more complicated version that can handle both simple and nested datasets:\n\n.flat_map(lambda *ds: ds[0] if len(ds) == 1 else tf.data.Dataset.zip(ds))\n\nedited Nov 19, 2020 at 1:53\n\nanswered Mar 3, 2020 at 8:32\n\n8,11833 gold badges3434 silver badges3333 bronze badges 7\n\nDoesn\'t window just use skip under the hood? How does is the disadvantage The other disadvantage is that with skip() it has to read, and then discard, all the skipped records, which if your data source is slow means you might have a large spool-up time before results are emitted. adressed?\n\n– Frederik Bode Commented Mar 3, 2020 at 9:06\n\nIf you have a dataset of 1000 records, and you want a 10% for validation, you would have to skip the first 900 records before a single validation record is emitted. With this solution, it only has to skip 9 records. It does end up skipping the same amount overall, but if you use dataset.prefetch(), it can read in the background while doing other things. The difference is just saving the initial spool-up time.\n\n– phemmer Commented Mar 3, 2020 at 9:11\n\nThinking about it a bit more, and I removed the statement. There\'s probably a dozen ways to solve that problem, and it\'s probably minute, if present at all, for most people.\n\n– phemmer Commented Mar 3, 2020 at 9:40\n\nYou should probably set the without knowing the dataset size beforehand to boldface, or like a header or something, it\'s pretty important. This should really be the accepted answer, as it fits into the premise of tf.data.Dataset treating data like infinite streams.\n\n– Frederik Bode Commented Mar 3, 2020 at 9:43\n\nOne thing when I was trying this method was that RAM consumption was much higher than when using the method described by @ted. So much higher that I couldn\'t get it to run on my maschine at all. Maybe I\'m doing something wrong, but what would be a feasible approach wenn I don\'t know the size of the dataset and also have data that doesn\'t fit into memory?\n\n– witsyke Commented Jul 5, 2021 at 11:51\n\n | Show 2 more comments\n\n@ted\'s answer will cause some overlap. Try this.\n\ntrain_ds_size = int(0.64 * full_ds_size) valid_ds_size = int(0.16 * full_ds_size) train_ds = full_ds.take(train_ds_size) remaining = full_ds.skip(train_ds_size) valid_ds = remaining.take(valid_ds_size) test_ds = remaining.skip(valid_ds_size)\n\nuse code below to test.\n\ntf.enable_eager_execution() dataset = tf.data.Dataset.range(100) train_size = 20 valid_size = 30 test_size = 50 train = dataset.take(train_size) remaining = dataset.skip(train_size) valid = remaining.take(valid_size) test = remaining.skip(valid_size) for i in train: print(i) for i in valid: print(i) for i in test: print(i)\n\nanswered Mar 27, 2020 at 21:43\n\n18333 silver badges77 bronze badges 2\n\nI love how everyone assumes you know the full_ds_size but no one explains how to find it\n\n– Bersan Commented Mar 30, 2021 at 15:28\n\n@Bersan len(list(dataset)) is the most straightforward stackoverflow.com/questions/50737192/… ...but... my understanding is that datasets can be extremely large (might not fit in memory) so iterating over them can take a very long time. It is probably best to figure out how large the dataset is based on external knowledge of the dataset.\n\n– BobtheMagicMoose Commented Jun 7, 2021 at 14:00\n\nThe upcoming TensorFlow 2.10.0 will have a tf.keras.utils.split_dataset function, see the rc3 release notes:\n\nAdded tf.keras.utils.split_dataset utility to split a Dataset object or a list/tuple of arrays into two Dataset objects (e.g. train/test).\n\nanswered Sep 3, 2022 at 11:18\n\nRobert PollakRobert Pollak\n\n4,01944 gold badges3232 silver badges5555 bronze badges 1\n\nBy the way, I found that using this separate split_dataset function makes the shuffling of image_dataset_from_directory re-iteration stable, yielding correctly ordered results of Model.predict later on. See discuss.tensorflow.org/t/…\n\n– Robert Pollak Commented Sep 3, 2022 at 13:02\n\nNow Tensorflow doesn\'t contain any tools for that. You could use sklearn.model_selection.train_test_split to generate train/eval/test dataset, then create tf.data.Dataset respectively.\n\nanswered Mar 12, 2018 at 8:35\n\n34733 silver badges44 bronze badges 1\n\nsklearn requires that stuff fits in memory, TF Data does not.\n\n– Denziloe Commented Dec 5, 2020 at 5:08\n\ndataset = dataset.shuffle() # optional trainset = dataset.shard(2, 0) testset = dataset.shard(2, 1)\n\nSee: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard\n\nanswered Nov 21, 2018 at 19:17\n\n6,04855 gold badges4040 silver badges6262 bronze badges 2\n\n– vgoklani Commented May 9, 2019 at 0:07\n\n@vgoklani are you sure? I don\'t see anything saying it is deprecated.\n\n– BobtheMagicMoose Commented Jun 7, 2021 at 14:22\n\nIn case size of the dataset is known:\n\nfrom typing import Tuple import tensorflow as tf def split_dataset(dataset: tf.data.Dataset, dataset_size: int, train_ratio: float, validation_ratio: float) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]: assert (train_ratio + validation_ratio) < 1 train_count = int(dataset_size * train_ratio) validation_count = int(dataset_size * validation_ratio) test_count = dataset_size - (train_count + validation_count) dataset = dataset.shuffle(dataset_size) train_dataset = dataset.take(train_count) validation_dataset = dataset.skip(train_count).take(validation_count) test_dataset = dataset.skip(validation_count + train_count).take(test_count) return train_dataset, validation_dataset, test_dataset\n\nsize_of_ds = 1001 train_ratio = 0.6 val_ratio = 0.2 ds = tf.data.Dataset.from_tensor_slices(list(range(size_of_ds))) train_ds, val_ds, test_ds = split_dataset(ds, size_of_ds, train_ratio, val_ratio)\n\nedited Jan 26, 2020 at 15:34\n\nanswered Jan 26, 2020 at 15:28\n\nDaniel BraunDaniel Braun\n\n2,6322828 silver badges2525 bronze badges\n\nA robust way to split dataset into two parts is to first deterministically map every item in the dataset into a bucket with, for example, tf.strings.to_hash_bucket_fast. Then you can split the dataset into two by filtering by the bucket. If you split your data into five buckets, you get 80-20 split assuming that the split is even.\n\nAs an example, assume that your dataset contains dictionaries with key filename. We split the data into five buckets based on this key. With this add_fold function, we add the key ""fold"" in the dictionaries:\n\ndef add_fold(buckets: int): def add_(sample, label): fold = tf.strings.to_hash_bucket(sample[""filename""], num_buckets=buckets) return {**sample, ""fold"": fold}, label return add_ dataset = dataset.map(add_fold(buckets=5))\n\nNow we can split the dataset into two disjoint datasets with Dataset.filter:\n\ndef pick_fold(fold: int): def filter_fn(sample, _): return tf.math.equal(sample[""fold""], fold) return filter_fn def skip_fold(fold: int): def filter_fn(sample, _): return tf.math.not_equal(sample[""fold""], fold) return filter_fn train_dataset = dataset.filter(skip_fold(0)) val_dataset = dataset.filter(pick_fold(0))\n\nThe key that you use for hashing should be one that captures the correlations in the dataset. For example, if your samples collected by the same person are correlated and you want all samples with the same collector end up in the same bucket (and the same split), you should use the collector name or ID as the hashing column.\n\nOf course, you can skip the part with dataset.map and do the hashing and filtering in one filter function. Here\'s a full example:\n\ndataset = tf.data.Dataset.from_tensor_slices([f""value-{i}"" for i in range(10000)]) def to_bucket(sample): return tf.strings.to_hash_bucket_fast(sample, 5) def filter_train_fn(sample): return tf.math.not_equal(to_bucket(sample), 0) def filter_val_fn(sample): return tf.math.logical_not(filter_train_fn(sample)) train_ds = dataset.filter(filter_train_fn) val_ds = dataset.filter(filter_val_fn) print(f""Length of training set: {len(list(train_ds.as_numpy_iterator()))}"") print(f""Length of validation set: {len(list(val_ds.as_numpy_iterator()))}"")\n\nLength of training set: 7995 Length of validation set: 2005\n\nedited Jan 29, 2022 at 13:40\n\nanswered Jan 29, 2022 at 13:23\n\n19133 silver badges99 bronze badges\n\nBeware of lazy evaluation which produces two pipelines shuffle+take and shuffle+skip that do overlap. Due to this, some of the high-scored answers produce information leaks. Here is the correct way by repeating and seeding shuffle in both train and test datasets.\n\nimport tensorflow as tf def gen_data(): return iter(range(10)) ds = tf.data.Dataset.from_generator( gen_data, output_signature=tf.TensorSpec(shape=(),dtype=tf.int32,name=""element"")) SEED = 42 # NOTE: with no seed, you overlap train and test! ds_train = ds.shuffle(100,seed=SEED).take(8).shuffle(100) ds_test = ds.shuffle(100,seed=SEED).skip(8) A = set(ds_train.as_numpy_iterator()) B = set(ds_test.as_numpy_iterator()) assert A.intersection(B)==set() print(list(A)) print(list(B))\n\nNOTE: This works for any deterministically ordered iterator.\n\nedited Oct 27, 2023 at 16:21\n\nanswered Oct 27, 2023 at 14:54\n\nMaciej SkorskiMaciej Skorski\n\n3,0751212 silver badges1717 bronze badges\n\nCan\'t comment, but above answer has overlap and is incorrect. Set BUFFER_SIZE to DATASET_SIZE for perfect shuffle. Try different sized val/test size to verify. Answer should be:\n\nDATASET_SIZE = tf.data.experimental.cardinality(full_dataset).numpy() train_size = int(0.7 * DATASET_SIZE) val_size = int(0.15 * DATASET_SIZE) test_size = int(0.15 * DATASET_SIZE) full_dataset = full_dataset.shuffle(BUFFER_SIZE) train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) val_dataset = test_dataset.take(val_size) test_dataset = test_dataset.skip(val_size)\n\nanswered Mar 2, 2020 at 20:00\n\nTyler LozanoTyler Lozano\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow-datasets or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n0 Create Train Test Split on PrefetchDataset\n\n71 tf.data.Dataset: how to get the dataset size (number of elements in an epoch)?\n\n42 How do I split Tensorflow datasets?\n\n2 Tensorflow DataSet Shuffle Impact the validation training accuracy and ambiguous behavior\n\n2 How do I extract some items from a tf.data.Dataset without starting from the first one?\n\n0 Splitting a data set for CNN\n\n0 Is there a way to partition a tf.Dataset with TensorFlow’s Dataset API?\n\n0 How to split dataset and feed into input_fn\n\n0 How to separate dataset to validate CNN?\n\n1 RAM issues when trying to create tensorflow dataset pipeline that loads from multiple files and splits data into training/validation\n\nSee more linked questions\n\n25 Split tensor into training and test sets\n\n42 How do I split Tensorflow datasets?\n\n2 Split tensorflow dataset in dataset per class\n\n2 Split a dataset issue in Tensorflow dataset API\n\n7 python.data.ops.dataset_ops.BatchDataset - how to use it to create training and test datasets\n\n0 split dataset into train and test using tensorflow\n\n0 How to split a tensorflow dataset\n\n0 TensorFlow Dataset train/test split\n\n1 spliting custom binary dataset in train/test subsets using tensorflow io\n\n0 loading data using TensorFlow Datasets and splitting\n\nHot Network Questions\n\nAre there any CID episodes based on real-life events?\n\nHow to produce this table: Probability datatable with multirow\n\nIs it consistent with ZFC that the real line is approachable by sets with no accumulation points?\n\nHow is Victor Timely a variant of He Who Remains in the 19th century?\n\nSets of algebraic integers whose differences are units\n\nBoth my adult kids and their father (ex husband) dual citizens\n\nCan you help me to identify the aircraft in a 1920s photograph?\n\nHow can a landlord receive rent in cash using western union\n\nWhat type of black color text for brochure print in CMYK?\n\nCloud masking ECOSTRESS LST data\n\nWhat does ‘a grade-hog’ mean?\n\nException handling: is one exception type sufficient?\n\nIs there any legal justification for content on the web without an explicit licence being freeware?\n\nWhere can I access records of the 1947 Superman copyright trial?\n\nAre ordinary or qualified dividends taxed first?\n\nHow to Pick Out Strings of a Specified Length\n\nWhy can\'t LaTeX (seem to?) Support Arbitrary Text Sizes?\n\nTrying to determine what this small glass-enclosed item is\n\nCan I tell a MILP solver to prefer solutions with fewer fractions?\n\nSMTP Header confusion - Delivered-To: and To: are different\n\nWhat is the original source of this Sigurimi logo?\n\nadd an apostrophe to equation number having a distant scope\n\nViewport Shader Render different from 1 computer to another more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_2', 'snippet': 'Skip to main content\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nAdvertising & Talent Reach devs & technologists worldwide about your product, service or employer brand\n\nOverflowAI GenAI features for Teams\n\nOverflowAPI Train & fine-tune LLMs\n\nLabs The future of collective knowledge sharing\n\nAbout the company Visit the blog\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nSplit a dataset created by Tensorflow dataset API in to Train and Test?\n\nAsked 6 years, 5 months ago\n\nModified 8 months ago\n\nDoes anyone know how to split a dataset created by the dataset API (tf.data.Dataset) in Tensorflow into Test and Train?\n\nImprove this question\n\nedited Dec 2, 2020 at 22:10\n\n59.5k2929 gold badges149149 silver badges169169 bronze badges\n\nasked Jan 11, 2018 at 18:34\n\n88711 gold badge77 silver badges88 bronze badges 2\n\ntake(), skip(), and shard() all have their own problems. I just posted my answer over here. I hope it better answers your question.\n\n– Nick Lee Commented Oct 18, 2019 at 13:55\n\nuse Keras - model.fit(dataset,.., validation.split=0.7, ...) see its all possible arguments\n\n– JeeyCi Commented Apr 29, 2022 at 18:51\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nAssuming you have all_dataset variable of tf.data.Dataset type:\n\ntest_dataset = all_dataset.take(1000) train_dataset = all_dataset.skip(1000)\n\nTest dataset now has first 1000 elements and the rest goes for training.\n\nanswered May 5, 2018 at 3:10\n\n1,6041212 silver badges1212 bronze badges 3\n\nAs also mentioned in ted\'s answer, adding all_dataset.shuffle() allows for a shuffled split. Possibly add as code comment in answer like so? # all_dataset = all_dataset.shuffle() # in case you want a shuffled split\n\n– Christian Steinmeyer Commented Oct 2, 2020 at 14:15\n\nTensorFlow 2.10.0 will have a utility function for splitting, see my answer: stackoverflow.com/a/73591823/1389680\n\n– Robert Pollak Commented Sep 3, 2022 at 11:20\n\ntake and skip return TfTakeDatasets/SkipDatasets which have less functionality than TfDatasets. Does anyone know how to map those to tfDatasets or split into train test splits and get back TfDataset objects?\n\n– Nikos H. Commented Sep 5, 2022 at 14:33\n\nYou may use Dataset.take() and Dataset.skip():\n\ntrain_size = int(0.7 * DATASET_SIZE) val_size = int(0.15 * DATASET_SIZE) test_size = int(0.15 * DATASET_SIZE) full_dataset = tf.data.TFRecordDataset(FLAGS.input_file) full_dataset = full_dataset.shuffle() train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) val_dataset = test_dataset.skip(val_size) test_dataset = test_dataset.take(test_size)\n\nFor more generality, I gave an example using a 70/15/15 train/val/test split but if you don\'t need a test or a val set, just ignore the last 2 lines.\n\nCreates a Dataset with at most count elements from this dataset.\n\nCreates a Dataset that skips count elements from this dataset.\n\nYou may also want to look into Dataset.shard():\n\nCreates a Dataset that includes only 1/num_shards of this dataset.\n\nDisclaimer I stumbled upon this question after answering this one so I thought I\'d spread the love\n\nanswered Jul 10, 2018 at 6:42\n\n14.3k1010 gold badges6666 silver badges111111 bronze badges 7\n\nThank you very much @ted! Is there a way to divide the dataset in a stratified way? Or, alternatively, how can we have an idea of the class proportions (suppose a binary problem) after the train/val/test split? Thanks a lot in advance!\n\n– Tommaso Di Noto Commented Aug 27, 2019 at 13:21\n\nHave a look at this blogpost I wrote; eventhough it\'s for multilabel datasets, should be easily usable for single label, multiclass datasets -> vict0rs.ch/2018/06/17/multilabel-text-classification-tensorflow\n\n– ted Commented Aug 27, 2019 at 13:50\n\nThis causes my train,validation and test datasets to have overlap between them. Is this supposed to happen and not a big deal? I would assume it\'s not a good idea to have the model train on validation and test data.\n\n– bw0248 Commented Jan 24, 2020 at 19:51\n\n@c_student I had the same problem and I figured out what I was missing: when you shuffle use the option reshuffle_each_iteration=False otherwise elements could be repeated in train, test and val\n\n– xdola Commented Apr 15, 2020 at 17:57\n\nThis is very true @xdola, and in particular when using list_files you should use shuffle=False and then shuffle with the .shuffle with reshuffle_each_iteration=False.\n\n– Zaccharie Ramzi Commented May 27, 2020 at 9:48\n\n | Show 2 more comments\n\nMost of the answers here use take() and skip(), which requires knowing the size of your dataset before hand. This isn\'t always possible, or is difficult/intensive to ascertain.\n\nInstead what you can do is to essentially slice the dataset up so that 1 every N records becomes a validation record.\n\nTo accomplish this, lets start with a simple dataset of 0-9:\n\ndataset = tf.data.Dataset.range(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nNow for our example, we\'re going to slice it so that we have a 3/1 train/validation split. Meaning 3 records will go to training, then 1 record to validation, then repeat.\n\nsplit = 3 dataset_train = dataset.window(split, split + 1).flat_map(lambda ds: ds) # [0, 1, 2, 4, 5, 6, 8, 9] dataset_validation = dataset.skip(split).window(1, split + 1).flat_map(lambda ds: ds) # [3, 7]\n\nSo the first dataset.window(split, split + 1) says to grab split number (3) of elements, then advance split + 1 elements, and repeat. That + 1 effectively skips the 1 element we\'re going to use in our validation dataset. The flat_map(lambda ds: ds) is because window() returns the results in batches, which we don\'t want. So we flatten it back out.\n\nThen for the validation data we first skip(split), which skips over the first split number (3) of elements that were grabbed in the first training window, so we start our iteration on the 4th element. The window(1, split + 1) then grabs 1 element, advances split + 1 (4), and repeats.\n\nNote on nested datasets: The above example works well for simple datasets, but flat_map() will generate an error if the dataset is nested. To address this, you can swap out the flat_map() with a more complicated version that can handle both simple and nested datasets:\n\n.flat_map(lambda *ds: ds[0] if len(ds) == 1 else tf.data.Dataset.zip(ds))\n\nedited Nov 19, 2020 at 1:53\n\nanswered Mar 3, 2020 at 8:32\n\n8,11833 gold badges3434 silver badges3333 bronze badges 7\n\nDoesn\'t window just use skip under the hood? How does is the disadvantage The other disadvantage is that with skip() it has to read, and then discard, all the skipped records, which if your data source is slow means you might have a large spool-up time before results are emitted. adressed?\n\n– Frederik Bode Commented Mar 3, 2020 at 9:06\n\nIf you have a dataset of 1000 records, and you want a 10% for validation, you would have to skip the first 900 records before a single validation record is emitted. With this solution, it only has to skip 9 records. It does end up skipping the same amount overall, but if you use dataset.prefetch(), it can read in the background while doing other things. The difference is just saving the initial spool-up time.\n\n– phemmer Commented Mar 3, 2020 at 9:11\n\nThinking about it a bit more, and I removed the statement. There\'s probably a dozen ways to solve that problem, and it\'s probably minute, if present at all, for most people.\n\n– phemmer Commented Mar 3, 2020 at 9:40\n\nYou should probably set the without knowing the dataset size beforehand to boldface, or like a header or something, it\'s pretty important. This should really be the accepted answer, as it fits into the premise of tf.data.Dataset treating data like infinite streams.\n\n– Frederik Bode Commented Mar 3, 2020 at 9:43\n\nOne thing when I was trying this method was that RAM consumption was much higher than when using the method described by @ted. So much higher that I couldn\'t get it to run on my maschine at all. Maybe I\'m doing something wrong, but what would be a feasible approach wenn I don\'t know the size of the dataset and also have data that doesn\'t fit into memory?\n\n– witsyke Commented Jul 5, 2021 at 11:51\n\n | Show 2 more comments\n\n@ted\'s answer will cause some overlap. Try this.\n\ntrain_ds_size = int(0.64 * full_ds_size) valid_ds_size = int(0.16 * full_ds_size) train_ds = full_ds.take(train_ds_size) remaining = full_ds.skip(train_ds_size) valid_ds = remaining.take(valid_ds_size) test_ds = remaining.skip(valid_ds_size)\n\nuse code below to test.\n\ntf.enable_eager_execution() dataset = tf.data.Dataset.range(100) train_size = 20 valid_size = 30 test_size = 50 train = dataset.take(train_size) remaining = dataset.skip(train_size) valid = remaining.take(valid_size) test = remaining.skip(valid_size) for i in train: print(i) for i in valid: print(i) for i in test: print(i)\n\nanswered Mar 27, 2020 at 21:43\n\n18333 silver badges77 bronze badges 2\n\nI love how everyone assumes you know the full_ds_size but no one explains how to find it\n\n– Bersan Commented Mar 30, 2021 at 15:28\n\n@Bersan len(list(dataset)) is the most straightforward stackoverflow.com/questions/50737192/… ...but... my understanding is that datasets can be extremely large (might not fit in memory) so iterating over them can take a very long time. It is probably best to figure out how large the dataset is based on external knowledge of the dataset.\n\n– BobtheMagicMoose Commented Jun 7, 2021 at 14:00\n\nThe upcoming TensorFlow 2.10.0 will have a tf.keras.utils.split_dataset function, see the rc3 release notes:\n\nAdded tf.keras.utils.split_dataset utility to split a Dataset object or a list/tuple of arrays into two Dataset objects (e.g. train/test).\n\nanswered Sep 3, 2022 at 11:18\n\nRobert PollakRobert Pollak\n\n4,01944 gold badges3232 silver badges5555 bronze badges 1\n\nBy the way, I found that using this separate split_dataset function makes the shuffling of image_dataset_from_directory re-iteration stable, yielding correctly ordered results of Model.predict later on. See discuss.tensorflow.org/t/…\n\n– Robert Pollak Commented Sep 3, 2022 at 13:02\n\nNow Tensorflow doesn\'t contain any tools for that. You could use sklearn.model_selection.train_test_split to generate train/eval/test dataset, then create tf.data.Dataset respectively.\n\nanswered Mar 12, 2018 at 8:35\n\n34733 silver badges44 bronze badges 1\n\nsklearn requires that stuff fits in memory, TF Data does not.\n\n– Denziloe Commented Dec 5, 2020 at 5:08\n\ndataset = dataset.shuffle() # optional trainset = dataset.shard(2, 0) testset = dataset.shard(2, 1)\n\nSee: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard\n\nanswered Nov 21, 2018 at 19:17\n\n6,04855 gold badges4040 silver badges6262 bronze badges 2\n\n– vgoklani Commented May 9, 2019 at 0:07\n\n@vgoklani are you sure? I don\'t see anything saying it is deprecated.\n\n– BobtheMagicMoose Commented Jun 7, 2021 at 14:22\n\nIn case size of the dataset is known:\n\nfrom typing import Tuple import tensorflow as tf def split_dataset(dataset: tf.data.Dataset, dataset_size: int, train_ratio: float, validation_ratio: float) -> Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]: assert (train_ratio + validation_ratio) < 1 train_count = int(dataset_size * train_ratio) validation_count = int(dataset_size * validation_ratio) test_count = dataset_size - (train_count + validation_count) dataset = dataset.shuffle(dataset_size) train_dataset = dataset.take(train_count) validation_dataset = dataset.skip(train_count).take(validation_count) test_dataset = dataset.skip(validation_count + train_count).take(test_count) return train_dataset, validation_dataset, test_dataset\n\nsize_of_ds = 1001 train_ratio = 0.6 val_ratio = 0.2 ds = tf.data.Dataset.from_tensor_slices(list(range(size_of_ds))) train_ds, val_ds, test_ds = split_dataset(ds, size_of_ds, train_ratio, val_ratio)\n\nedited Jan 26, 2020 at 15:34\n\nanswered Jan 26, 2020 at 15:28\n\nDaniel BraunDaniel Braun\n\n2,6322828 silver badges2525 bronze badges\n\nA robust way to split dataset into two parts is to first deterministically map every item in the dataset into a bucket with, for example, tf.strings.to_hash_bucket_fast. Then you can split the dataset into two by filtering by the bucket. If you split your data into five buckets, you get 80-20 split assuming that the split is even.\n\nAs an example, assume that your dataset contains dictionaries with key filename. We split the data into five buckets based on this key. With this add_fold function, we add the key ""fold"" in the dictionaries:\n\ndef add_fold(buckets: int): def add_(sample, label): fold = tf.strings.to_hash_bucket(sample[""filename""], num_buckets=buckets) return {**sample, ""fold"": fold}, label return add_ dataset = dataset.map(add_fold(buckets=5))\n\nNow we can split the dataset into two disjoint datasets with Dataset.filter:\n\ndef pick_fold(fold: int): def filter_fn(sample, _): return tf.math.equal(sample[""fold""], fold) return filter_fn def skip_fold(fold: int): def filter_fn(sample, _): return tf.math.not_equal(sample[""fold""], fold) return filter_fn train_dataset = dataset.filter(skip_fold(0)) val_dataset = dataset.filter(pick_fold(0))\n\nThe key that you use for hashing should be one that captures the correlations in the dataset. For example, if your samples collected by the same person are correlated and you want all samples with the same collector end up in the same bucket (and the same split), you should use the collector name or ID as the hashing column.\n\nOf course, you can skip the part with dataset.map and do the hashing and filtering in one filter function. Here\'s a full example:\n\ndataset = tf.data.Dataset.from_tensor_slices([f""value-{i}"" for i in range(10000)]) def to_bucket(sample): return tf.strings.to_hash_bucket_fast(sample, 5) def filter_train_fn(sample): return tf.math.not_equal(to_bucket(sample), 0) def filter_val_fn(sample): return tf.math.logical_not(filter_train_fn(sample)) train_ds = dataset.filter(filter_train_fn) val_ds = dataset.filter(filter_val_fn) print(f""Length of training set: {len(list(train_ds.as_numpy_iterator()))}"") print(f""Length of validation set: {len(list(val_ds.as_numpy_iterator()))}"")\n\nLength of training set: 7995 Length of validation set: 2005\n\nedited Jan 29, 2022 at 13:40\n\nanswered Jan 29, 2022 at 13:23\n\n19133 silver badges99 bronze badges\n\nBeware of lazy evaluation which produces two pipelines shuffle+take and shuffle+skip that do overlap. Due to this, some of the high-scored answers produce information leaks. Here is the correct way by repeating and seeding shuffle in both train and test datasets.\n\nimport tensorflow as tf def gen_data(): return iter(range(10)) ds = tf.data.Dataset.from_generator( gen_data, output_signature=tf.TensorSpec(shape=(),dtype=tf.int32,name=""element"")) SEED = 42 # NOTE: with no seed, you overlap train and test! ds_train = ds.shuffle(100,seed=SEED).take(8).shuffle(100) ds_test = ds.shuffle(100,seed=SEED).skip(8) A = set(ds_train.as_numpy_iterator()) B = set(ds_test.as_numpy_iterator()) assert A.intersection(B)==set() print(list(A)) print(list(B))\n\nNOTE: This works for any deterministically ordered iterator.\n\nedited Oct 27, 2023 at 16:21\n\nanswered Oct 27, 2023 at 14:54\n\nMaciej SkorskiMaciej Skorski\n\n3,0751212 silver badges1717 bronze badges\n\nCan\'t comment, but above answer has overlap and is incorrect. Set BUFFER_SIZE to DATASET_SIZE for perfect shuffle. Try different sized val/test size to verify. Answer should be:\n\nDATASET_SIZE = tf.data.experimental.cardinality(full_dataset).numpy() train_size = int(0.7 * DATASET_SIZE) val_size = int(0.15 * DATASET_SIZE) test_size = int(0.15 * DATASET_SIZE) full_dataset = full_dataset.shuffle(BUFFER_SIZE) train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) val_dataset = test_dataset.take(val_size) test_dataset = test_dataset.skip(val_size)\n\nanswered Mar 2, 2020 at 20:00\n\nTyler LozanoTyler Lozano\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntensorflow-datasets or ask your own question.\n\nUpcoming sign-up experiments related to tags\n\nPolicy: Generative AI (e.g., ChatGPT) is banned\n\nThe [lib] tag is being burninated\n\nWhat makes a homepage useful for logged-in users\n\n0 Create Train Test Split on PrefetchDataset\n\n71 tf.data.Dataset: how to get the dataset size (number of elements in an epoch)?\n\n42 How do I split Tensorflow datasets?\n\n2 Tensorflow DataSet Shuffle Impact the validation training accuracy and ambiguous behavior\n\n2 How do I extract some items from a tf.data.Dataset without starting from the first one?\n\n0 Splitting a data set for CNN\n\n0 Is there a way to partition a tf.Dataset with TensorFlow’s Dataset API?\n\n0 How to split dataset and feed into input_fn\n\n0 How to separate dataset to validate CNN?\n\n1 RAM issues when trying to create tensorflow dataset pipeline that loads from multiple files and splits data into training/validation\n\nSee more linked questions\n\n25 Split tensor into training and test sets\n\n42 How do I split Tensorflow datasets?\n\n2 Split tensorflow dataset in dataset per class\n\n2 Split a dataset issue in Tensorflow dataset API\n\n7 python.data.ops.dataset_ops.BatchDataset - how to use it to create training and test datasets\n\n0 split dataset into train and test using tensorflow\n\n0 How to split a tensorflow dataset\n\n0 TensorFlow Dataset train/test split\n\n1 spliting custom binary dataset in train/test subsets using tensorflow io\n\n0 loading data using TensorFlow Datasets and splitting\n\nHot Network Questions\n\nAre there any CID episodes based on real-life events?\n\nHow to produce this table: Probability datatable with multirow\n\nIs it consistent with ZFC that the real line is approachable by sets with no accumulation points?\n\nHow is Victor Timely a variant of He Who Remains in the 19th century?\n\nSets of algebraic integers whose differences are units\n\nBoth my adult kids and their father (ex husband) dual citizens\n\nCan you help me to identify the aircraft in a 1920s photograph?\n\nHow can a landlord receive rent in cash using western union\n\nWhat type of black color text for brochure print in CMYK?\n\nCloud masking ECOSTRESS LST data\n\nWhat does ‘a grade-hog’ mean?\n\nException handling: is one exception type sufficient?\n\nIs there any legal justification for content on the web without an explicit licence being freeware?\n\nWhere can I access records of the 1947 Superman copyright trial?\n\nAre ordinary or qualified dividends taxed first?\n\nHow to Pick Out Strings of a Specified Length\n\nWhy can\'t LaTeX (seem to?) Support Arbitrary Text Sizes?\n\nTrying to determine what this small glass-enclosed item is\n\nCan I tell a MILP solver to prefer solutions with fewer fractions?\n\nSMTP Header confusion - Delivered-To: and To: are different\n\nWhat is the original source of this Sigurimi logo?\n\nadd an apostrophe to equation number having a distant scope\n\nViewport Shader Render different from 1 computer to another more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-06-30T14:30:52', 'title': 'Split a dataset created by Tensorflow dataset API in to Train and Test? - Stack Overflow', 'url': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test'}), Document(page_content='Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nIs it possible to split a tensorflow dataset into train, validation AND test datasets when using image_dataset_from_directory?\n\nAsked 2 years, 1 month ago\n\nModified 1 month ago\n\nI am using tf.keras.utils.image_dataset_from_directory to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the validation_split parameter), I want to split it into training, testing, and validation subsets.\n\nI have tried using dataset.skip() and dataset.take() to further split one of the resulting subsets, but these functions return a SkipDataset and a TakeDataset respectively (by the way, contrary to the documentation, where it is claimed that these functions return a Dataset). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history.\n\nSo, my question is: is there a way to split a Dataset into three subsets for training, validation and testing, so that all three subsets are also Dataset objects?\n\nCode used to load the data\n\ndef load_data_tf(data_path: str, img_shape=(256,256), batch_size: int=8): train_ds = tf.keras.utils.image_dataset_from_directory( data_path, validation_split=0.2, subset=""training"", label_mode=\'categorical\', seed=123, image_size=img_shape, batch_size=batch_size) val_ds = tf.keras.utils.image_dataset_from_directory( data_path, validation_split=0.3, subset=""validation"", label_mode=\'categorical\', seed=123, image_size=img_shape, batch_size=batch_size) return train_ds, val_ds train_dataset, test_val_ds = load_data_tf(\'data_folder\', img_shape = (256,256), batch_size=8) test_dataset = test_val_ds.take(686) val_dataset = test_val_ds.skip(686)\n\nModel compilation and fitting\n\nmodel.compile(optimizer=\'sgd\', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=[\'accuracy\']) history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, verbose=1)\n\nWhen using a normal Dataset, val_accuracy and val_loss are present in the history of the model:\n\nBut when using a SkipDataset, they are not:\n\nImprove this question\n\nedited Feb 16, 2022 at 6:55\n\n26.4k55 gold badges2020 silver badges4040 bronze badges\n\nasked Feb 15, 2022 at 15:56\n\nandrii kliachkinandrii kliachkin\n\n4511 silver badge77 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe issue is that you are not taking and skipping samples when you do test_val_ds.take(686) and test_val_ds.skip(686), but actually batches. Try running print(val_dataset.cardinality()) and you will see how many batches you really have reserved for validation. I am guessing val_dataset is empty, because you do not have 686 batches for validation. Here is a working example:\n\nimport tensorflow as tf import pathlib dataset_url = ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"" data_dir = tf.keras.utils.get_file(\'flower_photos\', origin=dataset_url, untar=True) data_dir = pathlib.Path(data_dir) batch_size = 32 train_ds = tf.keras.utils.image_dataset_from_directory( data_dir, validation_split=0.2, subset=""training"", seed=123, image_size=(180, 180), batch_size=batch_size) val_ds = tf.keras.utils.image_dataset_from_directory( data_dir, validation_split=0.2, subset=""validation"", seed=123, image_size=(180, 180), batch_size=batch_size) test_dataset = val_ds.take(5) val_ds = val_ds.skip(5) print(\'Batches for testing -->\', test_dataset.cardinality()) print(\'Batches for validating -->\', val_ds.cardinality()) model = tf.keras.Sequential([ tf.keras.layers.Rescaling(1./255, input_shape=(180, 180, 3)), tf.keras.layers.Conv2D(16, 3, padding=\'same\', activation=\'relu\'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Conv2D(32, 3, padding=\'same\', activation=\'relu\'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Conv2D(64, 3, padding=\'same\', activation=\'relu\'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=\'relu\'), tf.keras.layers.Dense(5) ]) model.compile(optimizer=\'adam\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\'accuracy\']) epochs=1 history = model.fit( train_ds, validation_data=val_ds, epochs=1 )\n\nFound 3670 files belonging to 5 classes. Using 2936 files for training. Found 3670 files belonging to 5 classes. Using 734 files for validation. Batches for testing --> tf.Tensor(5, shape=(), dtype=int64) Batches for validating --> tf.Tensor(18, shape=(), dtype=int64) 92/92 [==============================] - 96s 1s/step - loss: 1.3516 - accuracy: 0.4489 - val_loss: 1.1332 - val_accuracy: 0.5645\n\nIn this example, with a batch_size of 32, you can clearly see that the validation set reserved 23 batches. Afterwards, 5 batches were given to the test set and 18 batches remained for the validation set.\n\nedited Feb 16, 2022 at 7:01\n\nanswered Feb 16, 2022 at 6:54\n\nAloneTogetherAloneTogether\n\n26.4k55 gold badges2020 silver badges4040 bronze badges 2\n\nyes, this does work, thank you! I never thought that count in take() represents the number of batches, not individual elements.\n\n– andrii kliachkin Feb 16, 2022 at 11:39\n\nHow do you know take and skip are taking and skipping the same files/images?\n\n– JeffreyShran Oct 2, 2022 at 13:16\n\nI can\'t comment, so have to answer to JeffreyShran about how we can be sure about take and skip taking the same pictures in that block. Here is the check code:\n\ndataset = tf.data.Dataset.range(10) take = int(len(dataset)/2) test = dataset.take(take) print(\'test:\', list(test.as_numpy_iterator())) dataset = dataset.skip(take) print(\'valid:\', list(dataset.as_numpy_iterator()))\n\ntest: [0, 1, 2, 3, 4] valid: [5, 6, 7, 8, 9]\n\nI\'m a newcomer, so my apologies if I\'m writing not in the appropriate place. But I think that consideration above must have been proved.\n\nedited Mar 4, 2023 at 22:10\n\nanswered Mar 4, 2023 at 22:09\n\nRed ApplicataRed Applicata\n\n1122 bronze badges 2\n\nDoes this solve the question? Or are you trying to ask JeffreyShran questions about their comment on AloneTogether\'s answer?\n\n– James Risner Mar 4, 2023 at 23:13\n\nJust move the code to the comments under JeffreyShran question if you are able and delete my \'answer\'. I can\'t understand this restriction to comment if you do not have \'reputation\', that was the reason I had to write in the not appropriate place - there is just no other choice to leave the comments for the newcomers like me.\n\n– Red Applicata Mar 6, 2023 at 9:07\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntf.keras or ask your own question.\n\nWill antitrust suits benefit developers?\n\nAre long context windows the end of RAG?\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n0 Tensorflow image_dataset_from_directory split to train, validation, testing\n\n72 Split a dataset created by Tensorflow dataset API in to Train and Test?\n\n42 How do I split Tensorflow datasets?\n\n6 How to create train, test and validation splits in tensorflow 2.0\n\n10 Split train data to train and validation by using tensorflow_datasets.load (TF 2.1)\n\n2 How to split an image dataset in X_train, y_train, X_test, y_test by tensorflow?\n\n5 how to Split data in 3 folds (train,validation,test) using ImageDataGenerator when data is in different directories of each class\n\n0 TensorFlow Dataset train/test split\n\n2 To split the main data directory into Train/validation/test Set\n\n2 How can I split the dataset obtained from image_dataset_from_directory into data and labels?\n\n1 what is the correct way of splitting dataset into train, validation and test?\n\nHot Network Questions\n\nWhat does 人の字 mean here?\n\nLife on a planet with never-ending rain\n\nBehaviour of Functions in Sourced Scripts\n\nWhy do we associate negative sign to electric charges when they are a scalar quantity?\n\nWhat is the proper etiquette when my advisor asks me to add non-contributing authors to author list?\n\nC++ - Lexer for the Monkey Programming Language from the book ""Writing An Interpreter In Go""\n\nCases in which we can use reason but not logic\n\nWrong solution for Green function of one-dimensional Poisson equation\n\nImplementation of Monte-Carlo Integration\n\nMy variegated holly bush/tree\n\nWhich episodes are the scenes depicting the ""worst things Rick did to make sure someone else survived"" from?\n\nRStudio + LaTeX template: compilation error undefined control sequence -- how to debug this myself?\n\nDoes a carbon fiber fuselage suffer wear from pressurization/depressurization?\n\nHOA fees before my house is ready\n\nCan the laws of physics and the constants of nature exist in a fundamental sense without mathematical realism?\n\nDoes lemon or lime juice need a Hechsher\n\nShould there be consequences or penalty when such mistakes published?\n\nWhat is the meaning of 8va combined with the bracket?\n\nWhat are ways to better show depth along a 3D curve?\n\nCircles crossing every cell of an 8x8 grid\n\nWhat\'s role of software verification in modern software engineering\n\nHow many cacti can I plant here?\n\nMissing re-entry stamp after stay in Bulgaria\n\nMedieval fantasy movie where a sorceress disguises herself as the queen to have a child by the king more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_3', 'snippet': 'Stack Overflow Public questions & answers\n\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n\nTalent Build your employer brand\n\nAdvertising Reach developers & technologists worldwide\n\nLabs The future of collective knowledge sharing\n\nCollectives™ on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\n\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\n\nGet early access and see previews of new features. Learn more about Labs\n\nIs it possible to split a tensorflow dataset into train, validation AND test datasets when using image_dataset_from_directory?\n\nAsked 2 years, 1 month ago\n\nModified 1 month ago\n\nI am using tf.keras.utils.image_dataset_from_directory to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the validation_split parameter), I want to split it into training, testing, and validation subsets.\n\nI have tried using dataset.skip() and dataset.take() to further split one of the resulting subsets, but these functions return a SkipDataset and a TakeDataset respectively (by the way, contrary to the documentation, where it is claimed that these functions return a Dataset). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history.\n\nSo, my question is: is there a way to split a Dataset into three subsets for training, validation and testing, so that all three subsets are also Dataset objects?\n\nCode used to load the data\n\ndef load_data_tf(data_path: str, img_shape=(256,256), batch_size: int=8): train_ds = tf.keras.utils.image_dataset_from_directory( data_path, validation_split=0.2, subset=""training"", label_mode=\'categorical\', seed=123, image_size=img_shape, batch_size=batch_size) val_ds = tf.keras.utils.image_dataset_from_directory( data_path, validation_split=0.3, subset=""validation"", label_mode=\'categorical\', seed=123, image_size=img_shape, batch_size=batch_size) return train_ds, val_ds train_dataset, test_val_ds = load_data_tf(\'data_folder\', img_shape = (256,256), batch_size=8) test_dataset = test_val_ds.take(686) val_dataset = test_val_ds.skip(686)\n\nModel compilation and fitting\n\nmodel.compile(optimizer=\'sgd\', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=[\'accuracy\']) history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, verbose=1)\n\nWhen using a normal Dataset, val_accuracy and val_loss are present in the history of the model:\n\nBut when using a SkipDataset, they are not:\n\nImprove this question\n\nedited Feb 16, 2022 at 6:55\n\n26.4k55 gold badges2020 silver badges4040 bronze badges\n\nasked Feb 15, 2022 at 15:56\n\nandrii kliachkinandrii kliachkin\n\n4511 silver badge77 bronze badges\n\nSorted by: Reset to default\n\nHighest score (default)\n\nTrending (recent votes count more)\n\nDate modified (newest first)\n\nDate created (oldest first)\n\nThe issue is that you are not taking and skipping samples when you do test_val_ds.take(686) and test_val_ds.skip(686), but actually batches. Try running print(val_dataset.cardinality()) and you will see how many batches you really have reserved for validation. I am guessing val_dataset is empty, because you do not have 686 batches for validation. Here is a working example:\n\nimport tensorflow as tf import pathlib dataset_url = ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"" data_dir = tf.keras.utils.get_file(\'flower_photos\', origin=dataset_url, untar=True) data_dir = pathlib.Path(data_dir) batch_size = 32 train_ds = tf.keras.utils.image_dataset_from_directory( data_dir, validation_split=0.2, subset=""training"", seed=123, image_size=(180, 180), batch_size=batch_size) val_ds = tf.keras.utils.image_dataset_from_directory( data_dir, validation_split=0.2, subset=""validation"", seed=123, image_size=(180, 180), batch_size=batch_size) test_dataset = val_ds.take(5) val_ds = val_ds.skip(5) print(\'Batches for testing -->\', test_dataset.cardinality()) print(\'Batches for validating -->\', val_ds.cardinality()) model = tf.keras.Sequential([ tf.keras.layers.Rescaling(1./255, input_shape=(180, 180, 3)), tf.keras.layers.Conv2D(16, 3, padding=\'same\', activation=\'relu\'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Conv2D(32, 3, padding=\'same\', activation=\'relu\'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Conv2D(64, 3, padding=\'same\', activation=\'relu\'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=\'relu\'), tf.keras.layers.Dense(5) ]) model.compile(optimizer=\'adam\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\'accuracy\']) epochs=1 history = model.fit( train_ds, validation_data=val_ds, epochs=1 )\n\nFound 3670 files belonging to 5 classes. Using 2936 files for training. Found 3670 files belonging to 5 classes. Using 734 files for validation. Batches for testing --> tf.Tensor(5, shape=(), dtype=int64) Batches for validating --> tf.Tensor(18, shape=(), dtype=int64) 92/92 [==============================] - 96s 1s/step - loss: 1.3516 - accuracy: 0.4489 - val_loss: 1.1332 - val_accuracy: 0.5645\n\nIn this example, with a batch_size of 32, you can clearly see that the validation set reserved 23 batches. Afterwards, 5 batches were given to the test set and 18 batches remained for the validation set.\n\nedited Feb 16, 2022 at 7:01\n\nanswered Feb 16, 2022 at 6:54\n\nAloneTogetherAloneTogether\n\n26.4k55 gold badges2020 silver badges4040 bronze badges 2\n\nyes, this does work, thank you! I never thought that count in take() represents the number of batches, not individual elements.\n\n– andrii kliachkin Feb 16, 2022 at 11:39\n\nHow do you know take and skip are taking and skipping the same files/images?\n\n– JeffreyShran Oct 2, 2022 at 13:16\n\nI can\'t comment, so have to answer to JeffreyShran about how we can be sure about take and skip taking the same pictures in that block. Here is the check code:\n\ndataset = tf.data.Dataset.range(10) take = int(len(dataset)/2) test = dataset.take(take) print(\'test:\', list(test.as_numpy_iterator())) dataset = dataset.skip(take) print(\'valid:\', list(dataset.as_numpy_iterator()))\n\ntest: [0, 1, 2, 3, 4] valid: [5, 6, 7, 8, 9]\n\nI\'m a newcomer, so my apologies if I\'m writing not in the appropriate place. But I think that consideration above must have been proved.\n\nedited Mar 4, 2023 at 22:10\n\nanswered Mar 4, 2023 at 22:09\n\nRed ApplicataRed Applicata\n\n1122 bronze badges 2\n\nDoes this solve the question? Or are you trying to ask JeffreyShran questions about their comment on AloneTogether\'s answer?\n\n– James Risner Mar 4, 2023 at 23:13\n\nJust move the code to the comments under JeffreyShran question if you are able and delete my \'answer\'. I can\'t understand this restriction to comment if you do not have \'reputation\', that was the reason I had to write in the not appropriate place - there is just no other choice to leave the comments for the newcomers like me.\n\n– Red Applicata Mar 6, 2023 at 9:07\n\nNot the answer you\'re looking for? Browse other questions tagged\n\ntf.keras or ask your own question.\n\nWill antitrust suits benefit developers?\n\nAre long context windows the end of RAG?\n\nNew Focus Styles & Updated Styling for Button Groups\n\nUpcoming initiatives on Stack Overflow and across the Stack Exchange network\n\nStaging Ground is coming back and moving out of beta\n\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\n\n0 Tensorflow image_dataset_from_directory split to train, validation, testing\n\n72 Split a dataset created by Tensorflow dataset API in to Train and Test?\n\n42 How do I split Tensorflow datasets?\n\n6 How to create train, test and validation splits in tensorflow 2.0\n\n10 Split train data to train and validation by using tensorflow_datasets.load (TF 2.1)\n\n2 How to split an image dataset in X_train, y_train, X_test, y_test by tensorflow?\n\n5 how to Split data in 3 folds (train,validation,test) using ImageDataGenerator when data is in different directories of each class\n\n0 TensorFlow Dataset train/test split\n\n2 To split the main data directory into Train/validation/test Set\n\n2 How can I split the dataset obtained from image_dataset_from_directory into data and labels?\n\n1 what is the correct way of splitting dataset into train, validation and test?\n\nHot Network Questions\n\nWhat does 人の字 mean here?\n\nLife on a planet with never-ending rain\n\nBehaviour of Functions in Sourced Scripts\n\nWhy do we associate negative sign to electric charges when they are a scalar quantity?\n\nWhat is the proper etiquette when my advisor asks me to add non-contributing authors to author list?\n\nC++ - Lexer for the Monkey Programming Language from the book ""Writing An Interpreter In Go""\n\nCases in which we can use reason but not logic\n\nWrong solution for Green function of one-dimensional Poisson equation\n\nImplementation of Monte-Carlo Integration\n\nMy variegated holly bush/tree\n\nWhich episodes are the scenes depicting the ""worst things Rick did to make sure someone else survived"" from?\n\nRStudio + LaTeX template: compilation error undefined control sequence -- how to debug this myself?\n\nDoes a carbon fiber fuselage suffer wear from pressurization/depressurization?\n\nHOA fees before my house is ready\n\nCan the laws of physics and the constants of nature exist in a fundamental sense without mathematical realism?\n\nDoes lemon or lime juice need a Hechsher\n\nShould there be consequences or penalty when such mistakes published?\n\nWhat is the meaning of 8va combined with the bracket?\n\nWhat are ways to better show depth along a 3D curve?\n\nCircles crossing every cell of an 8x8 grid\n\nWhat\'s role of software verification in modern software engineering\n\nHow many cacti can I plant here?\n\nMissing re-entry stamp after stay in Bulgaria\n\nMedieval fantasy movie where a sorceress disguises herself as the queen to have a child by the king more hot questions\n\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-04-02T20:33:08', 'title': 'python - Is it possible to split a tensorflow dataset into train, validation AND test datasets when using image_dataset_from_directory?', 'url': 'https://stackoverflow.com/questions/71129505/is-it-possible-to-split-a-tensorflow-dataset-into-train-validation-and-test-dat'})], [Document(page_content=""You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nInstantly share code, notes, and snippets.\n\nangeligareta/get_dataset_partitions_tf.py\n\nYou must be signed in to star a gist\n\nYou must be signed in to fork a gist\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/angeligareta/e3332c7a955dba8eaca71bf388d028c2.js&quot;&gt;&lt;/script&gt;\n\nSave angeligareta/e3332c7a955dba8eaca71bf388d028c2 to your computer and use it in GitHub Desktop.\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/angeligareta/e3332c7a955dba8eaca71bf388d028c2.js&quot;&gt;&lt;/script&gt;\n\nSave angeligareta/e3332c7a955dba8eaca71bf388d028c2 to your computer and use it in GitHub Desktop.\n\nMethod to split a tensorflow dataset (tf.data.Dataset) into train, validation and test splits\n\nget_dataset_partitions_tf.py\n\nThis file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\nShow hidden characters\n\ndef get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n\nassert (train_split + test_split + val_split) == 1\n\n# Specify seed to always have the same split distribution between runs\n\nds = ds.shuffle(shuffle_size, seed=12)\n\ntrain_size = int(train_split * ds_size)\n\nval_size = int(val_split * ds_size)\n\ntrain_ds = ds.take(train_size)\n\nval_ds = ds.skip(train_size).take(val_size)\n\ntest_ds = ds.skip(train_size).skip(val_size)\n\nreturn train_ds, val_ds, test_ds\n\nhsparkastro commented\n\nds.shuffle, without the additional parameter shuffle_each_iteration=False, will shuffle the dataset in each iteration before splitting into three separate datasets. This will cause the the three sets to be different every iteration, and a datapoint that was in val_ds could be in train_ds in the next iteration.\n\nDennis-Malonza commented\n\nhello guys, am currently working on my tensorflow model to fit into a CNN model but then the problem am experiencing is that my kernel is not allow me to visualize my dataset. numpys are working prety good but when I run imshow my kernel says it's dead and will restart again,this problem is happening again and again even after restarting the kernel and running all the cells. Kindly, help me guys i'll appreciate so much when i get someone help solve this problem\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time."", metadata={'id': 'web-search_1', 'snippet': ""You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nInstantly share code, notes, and snippets.\n\nangeligareta/get_dataset_partitions_tf.py\n\nYou must be signed in to star a gist\n\nYou must be signed in to fork a gist\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/angeligareta/e3332c7a955dba8eaca71bf388d028c2.js&quot;&gt;&lt;/script&gt;\n\nSave angeligareta/e3332c7a955dba8eaca71bf388d028c2 to your computer and use it in GitHub Desktop.\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/angeligareta/e3332c7a955dba8eaca71bf388d028c2.js&quot;&gt;&lt;/script&gt;\n\nSave angeligareta/e3332c7a955dba8eaca71bf388d028c2 to your computer and use it in GitHub Desktop.\n\nMethod to split a tensorflow dataset (tf.data.Dataset) into train, validation and test splits\n\nget_dataset_partitions_tf.py\n\nThis file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\nShow hidden characters\n\ndef get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n\nassert (train_split + test_split + val_split) == 1\n\n# Specify seed to always have the same split distribution between runs\n\nds = ds.shuffle(shuffle_size, seed=12)\n\ntrain_size = int(train_split * ds_size)\n\nval_size = int(val_split * ds_size)\n\ntrain_ds = ds.take(train_size)\n\nval_ds = ds.skip(train_size).take(val_size)\n\ntest_ds = ds.skip(train_size).skip(val_size)\n\nreturn train_ds, val_ds, test_ds\n\nhsparkastro commented\n\nds.shuffle, without the additional parameter shuffle_each_iteration=False, will shuffle the dataset in each iteration before splitting into three separate datasets. This will cause the the three sets to be different every iteration, and a datapoint that was in val_ds could be in train_ds in the next iteration.\n\nDennis-Malonza commented\n\nhello guys, am currently working on my tensorflow model to fit into a CNN model but then the problem am experiencing is that my kernel is not allow me to visualize my dataset. numpys are working prety good but when I run imshow my kernel says it's dead and will restart again,this problem is happening again and again even after restarting the kernel and running all the cells. Kindly, help me guys i'll appreciate so much when i get someone help solve this problem\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time."", 'timestamp': '2024-06-30T14:30:28', 'title': 'Method to split a tensorflow dataset (tf.data.Dataset) into train, validation and test splits · GitHub', 'url': 'https://gist.github.com/angeligareta/e3332c7a955dba8eaca71bf388d028c2'}), Document(page_content='You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nInstantly share code, notes, and snippets.\n\nfernandonieuwveldt/tensorflow_data_split.py\n\nJanuary 25, 2022 17:36\n\nYou must be signed in to star a gist\n\nYou must be signed in to fork a gist\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/fernandonieuwveldt/29d57830c7a7f5874a656abf562a58fa.js&quot;&gt;&lt;/script&gt;\n\nSave fernandonieuwveldt/29d57830c7a7f5874a656abf562a58fa to your computer and use it in GitHub Desktop.\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/fernandonieuwveldt/29d57830c7a7f5874a656abf562a58fa.js&quot;&gt;&lt;/script&gt;\n\nSave fernandonieuwveldt/29d57830c7a7f5874a656abf562a58fa to your computer and use it in GitHub Desktop.\n\nSplit tensorflow datasets for training and validation without knowing the size of the dataset\n\ntensorflow_data_split.py\n\nThis file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\nShow hidden characters\n\ndef split_data_set(dataset, val_split_fraction=0.25):\n\n""""""Split data for training and validation\n\ndataset (tf.data.Dataset): Dataset to split into training and validation\n\n(tf.data.Dataset, tf.data.Dataset): train and validation datasets\n\nrows = dataset.cardinality().numpy()\n\ndataset = dataset.shuffle(rows)\n\ntraining_size = int((1 - val_split_fraction) * rows)\n\ntrain_data_set = dataset.take(training_size)\n\nval_data_set = dataset.skip(training_size)\n\nreturn train_data_set, val_data_set\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', metadata={'id': 'web-search_5', 'snippet': 'You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session.\n\nInstantly share code, notes, and snippets.\n\nfernandonieuwveldt/tensorflow_data_split.py\n\nJanuary 25, 2022 17:36\n\nYou must be signed in to star a gist\n\nYou must be signed in to fork a gist\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/fernandonieuwveldt/29d57830c7a7f5874a656abf562a58fa.js&quot;&gt;&lt;/script&gt;\n\nSave fernandonieuwveldt/29d57830c7a7f5874a656abf562a58fa to your computer and use it in GitHub Desktop.\n\nEmbed Embed this gist in your website.\n\nShare Copy sharable link for this gist.\n\nClone via HTTPS Clone using the web URL.\n\nLearn more about clone URLs\n\nClone this repository at &lt;script src=&quot;https://gist.github.com/fernandonieuwveldt/29d57830c7a7f5874a656abf562a58fa.js&quot;&gt;&lt;/script&gt;\n\nSave fernandonieuwveldt/29d57830c7a7f5874a656abf562a58fa to your computer and use it in GitHub Desktop.\n\nSplit tensorflow datasets for training and validation without knowing the size of the dataset\n\ntensorflow_data_split.py\n\nThis file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\nShow hidden characters\n\ndef split_data_set(dataset, val_split_fraction=0.25):\n\n""""""Split data for training and validation\n\ndataset (tf.data.Dataset): Dataset to split into training and validation\n\n(tf.data.Dataset, tf.data.Dataset): train and validation datasets\n\nrows = dataset.cardinality().numpy()\n\ndataset = dataset.shuffle(rows)\n\ntraining_size = int((1 - val_split_fraction) * rows)\n\ntrain_data_set = dataset.take(training_size)\n\nval_data_set = dataset.skip(training_size)\n\nreturn train_data_set, val_data_set\n\nSign up for free to join this conversation on GitHub. Already have an account? Sign in to comment\n\nYou can’t perform that action at this time.', 'timestamp': '2024-06-01T10:02:19', 'title': 'Split tensorflow datasets for training and validation without knowing the size of the dataset · GitHub', 'url': 'https://gist.github.com/fernandonieuwveldt/29d57830c7a7f5874a656abf562a58fa'})], [Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nTFDS now supports the Croissant 🥐 format! Read the documentation to know more.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nAll TFDS datasets expose various data splits (e.g. \'train\', \'test\') which can be explored in the catalog. Any alphabetical string can be used as split name, apart from all (which is a reserved term which corresponds to the union of all splits, see below).\n\nIn addition of the ""official"" dataset splits, TFDS allow to select slice(s) of split(s) and various combinations.\n\nSlicing instructions are specified in tfds.load or tfds.DatasetBuilder.as_dataset through the split= kwarg.\n\nds = tfds.load(\'my_dataset\', split=\'train[:75%]\')\n\nbuilder = tfds.builder(\'my_dataset\') ds = builder.as_dataset(split=\'test+train[:75%]\')\n\nPlain split names (a string such as \'train\', \'test\', ...): All examples within the split selected.\n\nSlices: Slices have the same semantic as python slice notation. Slices can be:\n\nAbsolute (\'train[123:450]\', train[:4000]): (see note below for caveat about read order)\n\nPercent (\'train[:75%]\', \'train[25%:75%]\'): Divide the full data into even slices. If the data is not evenly divisible, some percent might contain additional examples. Fractional percent are supported.\n\nShard (train[:4shard], train[4shard]): Select all examples in the requested shard. (see info.splits[\'train\'].num_shards to get the number of shards of the split)\n\nUnion of splits (\'train+test\', \'train[:25%]+test\'): Splits will be interleaved together.\n\nFull dataset (\'all\'): \'all\' is a special split name corresponding to the union of all splits (equivalent to \'train+test+...\').\n\nList of splits ([\'train\', \'test\']): Multiple tf.data.Dataset are returned separately:\n\n# Returns both train and test split separately train_ds, test_ds = tfds.load(\'mnist\', split=[\'train\', \'test[:50%]\'])\n\nNote: Due to the shards being interleaved, order isn\'t guaranteed to be consistent between sub-splits. In other words reading test[0:100] followed by test[100:200] may yield examples in a different order than reading test[:200]. See determinism guide to understand the order in which TFDS read examples.\n\ntfds.even_splits & multi-host training\n\ntfds.even_splits generates a list of non-overlapping sub-splits of the same size.\n\n# Divide the dataset into 3 even parts, each containing 1/3 of the data split0, split1, split2 = tfds.even_splits(\'train\', n=3) ds = tfds.load(\'my_dataset\', split=split2)\n\nThis can be particularly useful when training in a distributed setting, where each host should receive a slice of the original data.\n\nWith Jax, this can be simplified even further using tfds.split_for_jax_process:\n\nsplit = tfds.split_for_jax_process(\'train\', drop_remainder=True) ds = tfds.load(\'my_dataset\', split=split)\n\ntfds.split_for_jax_process is a simple alias for:\n\n# The current `process_index` loads only `1 / process_count` of the data. splits = tfds.even_splits(\'train\', n=jax.process_count(), drop_remainder=True) split = splits[jax.process_index()]\n\ntfds.even_splits, tfds.split_for_jax_process accepts on any split value as input (e.g. \'train[75%:]+test\')\n\nSlicing and metadata\n\nIt is possible to get additional info on the splits/subsplits (num_examples, file_instructions,...) using the dataset info:\n\nbuilder = tfds.builder(\'my_dataset\') builder.info.splits[\'train\'].num_examples # 10_000 builder.info.splits[\'train[:75%]\'].num_examples # 7_500 (also works with slices) builder.info.splits.keys() # [\'train\', \'test\']\n\nExamples of 10-fold cross-validation using the string API:\n\nvals_ds = tfds.load(\'mnist\', split=[ f\'train[{k}%:{k+10}%]\' for k in range(0, 100, 10) ]) trains_ds = tfds.load(\'mnist\', split=[ f\'train[:{k}%]+train[{k+10}%:]\' for k in range(0, 100, 10) ])\n\nThe validation datasets are each going to be 10%: [0%:10%], [10%:20%], ..., [90%:100%]. And the training datasets are each going to be the complementary 90%: [10%:100%] (for a corresponding validation set of [0%:10%]), `[0%:10%]\n\n[20%:100%](for a validation set of[10%:20%]`),...\n\ntfds.core.ReadInstruction and rounding\n\nRather than str, it is possible to pass splits as tfds.core.ReadInstruction:\n\nFor example, split = \'train[50%:75%] + test\' is equivalent to:\n\nsplit = ( tfds.core.ReadInstruction( \'train\', from_=50, to=75, unit=\'%\', ) + tfds.core.ReadInstruction(\'test\') ) ds = tfds.load(\'my_dataset\', split=split)\n\nabs: Absolute slicing\n\nshard: Shard slicing\n\ntfds.ReadInstruction also has a rounding argument. If the number of example in the dataset is not divide evenly:\n\nrounding=\'closest\' (default): The remaining examples are distributed among the percent, so some percent might contain additional examples.\n\nrounding=\'pct1_dropremainder\': The remaining examples are dropped, but this guarantee all percent contain the exact same number of example (eg: len(5%) == 5 * len(1%)).\n\nReproducibility & determinism\n\nDuring generation, for a given dataset version, TFDS guarantee that examples are deterministically shuffled on disk. So generating the dataset twice (in 2 different computers) won\'t change the example order.\n\nSimilarly, the subsplit API will always select the same set of examples, regardless of platform, architecture, etc. This mean set(\'train[:20%]\') == set(\'train[:10%]\') + set(\'train[10%:20%]\').\n\nHowever, the order in which examples are read might not be deterministic. This depends on other parameters (e.g. whether shuffle_files=True).\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-09-05 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_0', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\nTFDS now supports the Croissant 🥐 format! Read the documentation to know more.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nAll TFDS datasets expose various data splits (e.g. \'train\', \'test\') which can be explored in the catalog. Any alphabetical string can be used as split name, apart from all (which is a reserved term which corresponds to the union of all splits, see below).\n\nIn addition of the ""official"" dataset splits, TFDS allow to select slice(s) of split(s) and various combinations.\n\nSlicing instructions are specified in tfds.load or tfds.DatasetBuilder.as_dataset through the split= kwarg.\n\nds = tfds.load(\'my_dataset\', split=\'train[:75%]\')\n\nbuilder = tfds.builder(\'my_dataset\') ds = builder.as_dataset(split=\'test+train[:75%]\')\n\nPlain split names (a string such as \'train\', \'test\', ...): All examples within the split selected.\n\nSlices: Slices have the same semantic as python slice notation. Slices can be:\n\nAbsolute (\'train[123:450]\', train[:4000]): (see note below for caveat about read order)\n\nPercent (\'train[:75%]\', \'train[25%:75%]\'): Divide the full data into even slices. If the data is not evenly divisible, some percent might contain additional examples. Fractional percent are supported.\n\nShard (train[:4shard], train[4shard]): Select all examples in the requested shard. (see info.splits[\'train\'].num_shards to get the number of shards of the split)\n\nUnion of splits (\'train+test\', \'train[:25%]+test\'): Splits will be interleaved together.\n\nFull dataset (\'all\'): \'all\' is a special split name corresponding to the union of all splits (equivalent to \'train+test+...\').\n\nList of splits ([\'train\', \'test\']): Multiple tf.data.Dataset are returned separately:\n\n# Returns both train and test split separately train_ds, test_ds = tfds.load(\'mnist\', split=[\'train\', \'test[:50%]\'])\n\nNote: Due to the shards being interleaved, order isn\'t guaranteed to be consistent between sub-splits. In other words reading test[0:100] followed by test[100:200] may yield examples in a different order than reading test[:200]. See determinism guide to understand the order in which TFDS read examples.\n\ntfds.even_splits & multi-host training\n\ntfds.even_splits generates a list of non-overlapping sub-splits of the same size.\n\n# Divide the dataset into 3 even parts, each containing 1/3 of the data split0, split1, split2 = tfds.even_splits(\'train\', n=3) ds = tfds.load(\'my_dataset\', split=split2)\n\nThis can be particularly useful when training in a distributed setting, where each host should receive a slice of the original data.\n\nWith Jax, this can be simplified even further using tfds.split_for_jax_process:\n\nsplit = tfds.split_for_jax_process(\'train\', drop_remainder=True) ds = tfds.load(\'my_dataset\', split=split)\n\ntfds.split_for_jax_process is a simple alias for:\n\n# The current `process_index` loads only `1 / process_count` of the data. splits = tfds.even_splits(\'train\', n=jax.process_count(), drop_remainder=True) split = splits[jax.process_index()]\n\ntfds.even_splits, tfds.split_for_jax_process accepts on any split value as input (e.g. \'train[75%:]+test\')\n\nSlicing and metadata\n\nIt is possible to get additional info on the splits/subsplits (num_examples, file_instructions,...) using the dataset info:\n\nbuilder = tfds.builder(\'my_dataset\') builder.info.splits[\'train\'].num_examples # 10_000 builder.info.splits[\'train[:75%]\'].num_examples # 7_500 (also works with slices) builder.info.splits.keys() # [\'train\', \'test\']\n\nExamples of 10-fold cross-validation using the string API:\n\nvals_ds = tfds.load(\'mnist\', split=[ f\'train[{k}%:{k+10}%]\' for k in range(0, 100, 10) ]) trains_ds = tfds.load(\'mnist\', split=[ f\'train[:{k}%]+train[{k+10}%:]\' for k in range(0, 100, 10) ])\n\nThe validation datasets are each going to be 10%: [0%:10%], [10%:20%], ..., [90%:100%]. And the training datasets are each going to be the complementary 90%: [10%:100%] (for a corresponding validation set of [0%:10%]), `[0%:10%]\n\n[20%:100%](for a validation set of[10%:20%]`),...\n\ntfds.core.ReadInstruction and rounding\n\nRather than str, it is possible to pass splits as tfds.core.ReadInstruction:\n\nFor example, split = \'train[50%:75%] + test\' is equivalent to:\n\nsplit = ( tfds.core.ReadInstruction( \'train\', from_=50, to=75, unit=\'%\', ) + tfds.core.ReadInstruction(\'test\') ) ds = tfds.load(\'my_dataset\', split=split)\n\nabs: Absolute slicing\n\nshard: Shard slicing\n\ntfds.ReadInstruction also has a rounding argument. If the number of example in the dataset is not divide evenly:\n\nrounding=\'closest\' (default): The remaining examples are distributed among the percent, so some percent might contain additional examples.\n\nrounding=\'pct1_dropremainder\': The remaining examples are dropped, but this guarantee all percent contain the exact same number of example (eg: len(5%) == 5 * len(1%)).\n\nReproducibility & determinism\n\nDuring generation, for a given dataset version, TFDS guarantee that examples are deterministically shuffled on disk. So generating the dataset twice (in 2 different computers) won\'t change the example order.\n\nSimilarly, the subsplit API will always select the same set of examples, regardless of platform, architecture, etc. This mean set(\'train[:20%]\') == set(\'train[:10%]\') + set(\'train[10%:20%]\').\n\nHowever, the order in which examples are read might not be deterministic. This depends on other parameters (e.g. whether shuffle_files=True).\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2023-09-05 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-04-16T13:59:41', 'title': 'Splits and slicing | TensorFlow Datasets', 'url': 'https://www.tensorflow.org/datasets/splits'}), Document(page_content='Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\ntf.data: Build TensorFlow input pipelines\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nThe tf.data API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations.\n\nThe tf.data API introduces a tf.data.Dataset abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\n\nThere are two distinct ways to create a dataset:\n\nA data source constructs a Dataset from data stored in memory or in one or more files.\n\nA data transformation constructs a dataset from one or more tf.data.Dataset objects.\n\nimport tensorflow as tf\n\n2024-01-17 02:24:38.925403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2024-01-17 02:24:38.925446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2024-01-17 02:24:38.927059: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nimport pathlib import os import matplotlib.pyplot as plt import pandas as pd import numpy as np np.set_printoptions(precision=4)\n\nTo create an input pipeline, you must start with a data source. For example, to construct a Dataset from data in memory, you can use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices(). Alternatively, if your input data is stored in a file in the recommended TFRecord format, you can use tf.data.TFRecordDataset().\n\nOnce you have a Dataset object, you can transform it into a new Dataset by chaining method calls on the tf.data.Dataset object. For example, you can apply per-element transformations such as Dataset.map, and multi-element transformations such as Dataset.batch. Refer to the documentation for tf.data.Dataset for a complete list of transformations.\n\nThe Dataset object is a Python iterable. This makes it possible to consume its elements using a for loop:\n\ndataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1]) dataset\n\n<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n\nfor elem in dataset: print(elem.numpy())\n\nOr by explicitly creating a Python iterator using iter and consuming its elements using next:\n\nit = iter(dataset) print(next(it).numpy())\n\nAlternatively, dataset elements can be consumed using the reduce transformation, which reduces all elements to produce a single result. The following example illustrates how to use the reduce transformation to compute the sum of a dataset of integers.\n\nprint(dataset.reduce(0, lambda state, value: state + value).numpy())\n\nA dataset produces a sequence of elements, where each element is the same (nested) structure of components. Individual components of the structure can be of any type representable by tf.TypeSpec, including tf.Tensor, tf.sparse.SparseTensor, tf.RaggedTensor, tf.TensorArray, or tf.data.Dataset.\n\nThe Python constructs that can be used to express the (nested) structure of elements include tuple, dict, NamedTuple, and OrderedDict. In particular, list is not a valid construct for expressing the structure of dataset elements. This is because early tf.data users felt strongly about list inputs (for example, when passed to tf.data.Dataset.from_tensors) being automatically packed as tensors and list outputs (for example, return values of user-defined functions) being coerced into a tuple. As a consequence, if you would like a list input to be treated as a structure, you need to convert it into tuple and if you would like a list output to be a single component, then you need to explicitly pack it using tf.stack.\n\nThe Dataset.element_spec property allows you to inspect the type of each element component. The property returns a nested structure of tf.TypeSpec objects, matching the structure of the element, which may be a single component, a tuple of components, or a nested tuple of components. For example:\n\ndataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10])) dataset1.element_spec\n\nTensorSpec(shape=(10,), dtype=tf.float32, name=None)\n\ndataset2 = tf.data.Dataset.from_tensor_slices( (tf.random.uniform([4]), tf.random.uniform([4, 100], maxval=100, dtype=tf.int32))) dataset2.element_spec\n\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))\n\ndataset3 = tf.data.Dataset.zip((dataset1, dataset2)) dataset3.element_spec\n\n(TensorSpec(shape=(10,), dtype=tf.float32, name=None), (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None)))\n\n# Dataset containing a sparse tensor. dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])) dataset4.element_spec\n\nSparseTensorSpec(TensorShape([3, 4]), tf.int32)\n\n# Use value_type to see the type of value represented by the element spec dataset4.element_spec.value_type\n\ntensorflow.python.framework.sparse_tensor.SparseTensor\n\nThe Dataset transformations support datasets of any structure. When using the Dataset.map, and Dataset.filter transformations, which apply a function to each element, the element structure determines the arguments of the function:\n\ndataset1 = tf.data.Dataset.from_tensor_slices( tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32)) dataset1\n\n<_TensorSliceDataset element_spec=TensorSpec(shape=(10,), dtype=tf.int32, name=None)>\n\nfor z in dataset1: print(z.numpy())\n\n[2 1 3 2 1 3 9 6 4 5] [8 9 2 9 1 4 7 2 4 5] [3 9 6 8 4 6 8 4 9 5] [4 3 7 2 8 6 4 9 7 6]\n\ndataset2 = tf.data.Dataset.from_tensor_slices( (tf.random.uniform([4]), tf.random.uniform([4, 100], maxval=100, dtype=tf.int32))) dataset2\n\n<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))>\n\ndataset3 = tf.data.Dataset.zip((dataset1, dataset2)) dataset3\n\n<_ZipDataset element_spec=(TensorSpec(shape=(10,), dtype=tf.int32, name=None), (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None)))>\n\nfor a, (b,c) in dataset3: print(\'shapes: {a.shape}, {b.shape}, {c.shape}\'.format(a=a, b=b, c=c))\n\nshapes: (10,), (), (100,) shapes: (10,), (), (100,) shapes: (10,), (), (100,) shapes: (10,), (), (100,)\n\nConsuming NumPy arrays\n\nRefer to the Loading NumPy arrays tutorial for more examples.\n\nIf all of your input data fits in memory, the simplest way to create a Dataset from them is to convert them to tf.Tensor objects and use Dataset.from_tensor_slices.\n\ntrain, test = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 29515/29515 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26421880/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 5148/5148 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4422102/4422102 [==============================] - 0s 0us/step\n\nimages, labels = train images = images/255 dataset = tf.data.Dataset.from_tensor_slices((images, labels)) dataset\n\n<_TensorSliceDataset element_spec=(TensorSpec(shape=(28, 28), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>\n\nNote: The above code snippet will embed the features and labels arrays in your TensorFlow graph as tf.constant() operations. This works well for a small dataset, but wastes memory---because the contents of the array will be copied multiple times---and can run into the 2GB limit for the tf.GraphDef protocol buffer.\n\nConsuming Python generators\n\nAnother common data source that can easily be ingested as a tf.data.Dataset is the python generator.\n\nCaution: While this is a convenient approach it has limited portability and scalability. It must run in the same python process that created the generator, and is still subject to the Python GIL.\n\ndef count(stop): i = 0 while i<stop: yield i i += 1\n\nfor n in count(5): print(n)\n\nThe Dataset.from_generator constructor converts the python generator to a fully functional tf.data.Dataset.\n\nThe constructor takes a callable as input, not an iterator. This allows it to restart the generator when it reaches the end. It takes an optional args argument, which is passed as the callable\'s arguments.\n\nThe output_types argument is required because tf.data builds a tf.Graph internally, and graph edges require a tf.dtype.\n\nds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes = (), )\n\nfor count_batch in ds_counter.repeat().batch(10).take(10): print(count_batch.numpy())\n\n[0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 0 1 2 3 4] [ 5 6 7 8 9 10 11 12 13 14] [15 16 17 18 19 20 21 22 23 24] [0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 0 1 2 3 4] [ 5 6 7 8 9 10 11 12 13 14] [15 16 17 18 19 20 21 22 23 24]\n\nThe output_shapes argument is not required but is highly recommended as many TensorFlow operations do not support tensors with an unknown rank. If the length of a particular axis is unknown or variable, set it as None in the output_shapes.\n\nIt\'s also important to note that the output_shapes and output_types follow the same nesting rules as other dataset methods.\n\nHere is an example generator that demonstrates both aspects: it returns tuples of arrays, where the second array is a vector with unknown length.\n\ndef gen_series(): i = 0 while True: size = np.random.randint(0, 10) yield i, np.random.normal(size=(size,)) i += 1\n\nfor i, series in gen_series(): print(i, "":"", str(series)) if i > 5: break\n\n0 : [-1.1977 -1.3099] 1 : [-0.3763 1.3639 0.0703 0.7888 0.3659 -0.0422 -0.5699 -1.4458] 2 : [ 0.4438 0.2206 -0.8348 3.0743 -0.2304 0.6876] 3 : [ 0.1138 0.3484 -0.3989 0.1871 -0.9462 0.7905 0.0224 0.204 -1.2715] 4 : [ 1.0292 -1.7965 -1.1569 0.437 1.9364 0.4718 -0.5036 -0.1318] 5 : [0.6893] 6 : [-0.2385 -0.3129 0.4913 0.2546 1.4849 -1.3109 -0.3785]\n\nThe first output is an int32 the second is a float32.\n\nThe first item is a scalar, shape (), and the second is a vector of unknown length, shape (None,)\n\nds_series = tf.data.Dataset.from_generator( gen_series, output_types=(tf.int32, tf.float32), output_shapes=((), (None,))) ds_series\n\n<_FlatMapDataset element_spec=(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n\nNow it can be used like a regular tf.data.Dataset. Note that when batching a dataset with a variable shape, you need to use Dataset.padded_batch.\n\nds_series_batch = ds_series.shuffle(20).padded_batch(10) ids, sequence_batch = next(iter(ds_series_batch)) print(ids.numpy()) print() print(sequence_batch.numpy())\n\n[ 4 18 19 2 8 22 13 0 25 27] [[-0.5268 0.8465 1.8949 -0.6337 -0.9212 0.2917 0.1995 -0.2283 1.5621] [-0.7196 0.3447 -0.5744 -1.6807 1.9387 -0.7832 1.1232 0.5444 0.3566] [-1.0073 0. 0. 0. 0. 0. 0. 0. 0. ] [ 1.3614 -0.0866 0.4309 -1.1438 0.066 0.3847 -0.8009 0. 0. ] [-0.7528 0. 0. 0. 0. 0. 0. 0. 0. ] [-0.006 0.9022 1.2462 0.0703 0. 0. 0. 0. 0. ] [ 0.5811 0. 0. 0. 0. 0. 0. 0. 0. ] [-0.1996 1.6923 -0.274 -0.7509 -0.6734 -1.687 -0.8438 -1.0904 0. ] [ 0.3178 0.0775 1.3367 1.0921 0.1651 0.9298 0.0764 -0.4039 0. ] [ 1.5668 -1.3154 0.8587 -0.7022 0. 0. 0. 0. 0. ]]\n\nFor a more realistic example, try wrapping preprocessing.image.ImageDataGenerator as a tf.data.Dataset.\n\nFirst download the data:\n\nflowers = tf.keras.utils.get_file( \'flower_photos\', \'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\', untar=True)\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz 228813984/228813984 [==============================] - 1s 0us/step\n\nCreate the image.ImageDataGenerator\n\nimg_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)\n\nimages, labels = next(img_gen.flow_from_directory(flowers))\n\nFound 3670 images belonging to 5 classes.\n\nprint(images.dtype, images.shape) print(labels.dtype, labels.shape)\n\nfloat32 (32, 256, 256, 3) float32 (32, 5)\n\nds = tf.data.Dataset.from_generator( lambda: img_gen.flow_from_directory(flowers), output_types=(tf.float32, tf.float32), output_shapes=([32,256,256,3], [32,5]) ) ds.element_spec\n\n(TensorSpec(shape=(32, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 5), dtype=tf.float32, name=None))\n\nfor images, labels in ds.take(1): print(\'images.shape: \', images.shape) print(\'labels.shape: \', labels.shape)\n\nFound 3670 images belonging to 5 classes. images.shape: (32, 256, 256, 3) labels.shape: (32, 5)\n\nConsuming TFRecord data\n\nRefer to the Loading TFRecords tutorial for an end-to-end example.\n\nThe tf.data API supports a variety of file formats so that you can process large datasets that do not fit in memory. For example, the TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. The tf.data.TFRecordDataset class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline.\n\nHere is an example using the test file from the French Street Name Signs (FSNS).\n\n# Creates a dataset that reads all of the examples from two files. fsns_test_file = tf.keras.utils.get_file(""fsns.tfrec"", ""https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001"")\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001 7904079/7904079 [==============================] - 0s 0us/step\n\nThe filenames argument to the TFRecordDataset initializer can either be a string, a list of strings, or a tf.Tensor of strings. Therefore if you have two sets of files for training and validation purposes, you can create a factory method that produces the dataset, taking filenames as an input argument:\n\ndataset = tf.data.TFRecordDataset(filenames = [fsns_test_file]) dataset\n\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\nMany TensorFlow projects use serialized tf.train.Example records in their TFRecord files. These need to be decoded before they can be inspected:\n\nraw_example = next(iter(dataset)) parsed = tf.train.Example.FromString(raw_example.numpy()) parsed.features.feature[\'image/text\']\n\nbytes_list { value: ""Rue Perreyon"" }\n\nRefer to the Load text tutorial for an end-to-end example.\n\nMany datasets are distributed as one or more text files. The tf.data.TextLineDataset provides an easy way to extract lines from one or more text files. Given one or more filenames, a TextLineDataset will produce one string-valued element per line of those files.\n\ndirectory_url = \'https://storage.googleapis.com/download.tensorflow.org/data/illiad/\' file_names = [\'cowper.txt\', \'derby.txt\', \'butler.txt\'] file_paths = [ tf.keras.utils.get_file(file_name, directory_url + file_name) for file_name in file_names ]\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt 815980/815980 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt 809730/809730 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt 807992/807992 [==============================] - 0s 0us/step\n\ndataset = tf.data.TextLineDataset(file_paths)\n\nHere are the first few lines of the first file:\n\nfor line in dataset.take(5): print(line.numpy())\n\nb""\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus\' son;"" b\'His wrath pernicious, who ten thousand woes\' b""Caused to Achaia\'s host, sent many a soul"" b\'Illustrious into Ades premature,\' b\'And Heroes gave (so stood the will of Jove)\'\n\nTo alternate lines between files use Dataset.interleave. This makes it easier to shuffle files together. Here are the first, second and third lines from each translation:\n\nfiles_ds = tf.data.Dataset.from_tensor_slices(file_paths) lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3) for i, line in enumerate(lines_ds.take(9)): if i % 3 == 0: print() print(line.numpy())\n\nb""\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus\' son;"" b""\\xef\\xbb\\xbfOf Peleus\' son, Achilles, sing, O Muse,"" b\'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought\' b\'His wrath pernicious, who ten thousand woes\' b\'The vengeance, deep and deadly; whence to Greece\' b\'countless ills upon the Achaeans. Many a brave soul did it send\' b""Caused to Achaia\'s host, sent many a soul"" b\'Unnumbered ills arose; which many a soul\' b\'hurrying down to Hades, and many a hero did it yield a prey to dogs and\'\n\nBy default, a TextLineDataset yields every line of each file, which may not be desirable, for example, if the file starts with a header line, or contains comments. These lines can be removed using the Dataset.skip() or Dataset.filter transformations. Here, you skip the first line, then filter to find only survivors.\n\ntitanic_file = tf.keras.utils.get_file(""train.csv"", ""https://storage.googleapis.com/tf-datasets/titanic/train.csv"") titanic_lines = tf.data.TextLineDataset(titanic_file)\n\nDownloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv 30874/30874 [==============================] - 0s 0us/step\n\nfor line in titanic_lines.take(10): print(line.numpy())\n\nb\'survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\' b\'0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\' b\'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\' b\'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\' b\'1,female,35.0,1,0,53.1,First,C,Southampton,n\' b\'0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\' b\'0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\' b\'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\' b\'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\' b\'1,female,4.0,1,1,16.7,Third,G,Southampton,n\'\n\ndef survived(line): return tf.not_equal(tf.strings.substr(line, 0, 1), ""0"") survivors = titanic_lines.skip(1).filter(survived)\n\nfor line in survivors.take(10): print(line.numpy())\n\nb\'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\' b\'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\' b\'1,female,35.0,1,0,53.1,First,C,Southampton,n\' b\'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\' b\'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\' b\'1,female,4.0,1,1,16.7,Third,G,Southampton,n\' b\'1,male,28.0,0,0,13.0,Second,unknown,Southampton,y\' b\'1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y\' b\'1,male,28.0,0,0,35.5,First,A,Southampton,y\' b\'1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n\'\n\nRefer to the Loading CSV Files and Loading Pandas DataFrames tutorials for more examples.\n\nThe CSV file format is a popular format for storing tabular data in plain text.\n\ntitanic_file = tf.keras.utils.get_file(""train.csv"", ""https://storage.googleapis.com/tf-datasets/titanic/train.csv"")\n\ndf = pd.read_csv(titanic_file) df.head()\n\nIf your data fits in memory the same Dataset.from_tensor_slices method works on dictionaries, allowing this data to be easily imported:\n\ntitanic_slices = tf.data.Dataset.from_tensor_slices(dict(df)) for feature_batch in titanic_slices.take(1): for key, value in feature_batch.items(): print("" {!r:20s}: {}"".format(key, value))\n\n\'survived\' : 0 \'sex\' : b\'male\' \'age\' : 22.0 \'n_siblings_spouses\': 1 \'parch\' : 0 \'fare\' : 7.25 \'class\' : b\'Third\' \'deck\' : b\'unknown\' \'embark_town\' : b\'Southampton\' \'alone\' : b\'n\'\n\nA more scalable approach is to load from disk as necessary.\n\nThe tf.data module provides methods to extract records from one or more CSV files that comply with RFC 4180.\n\nThe tf.data.experimental.make_csv_dataset function is the high-level interface for reading sets of CSV files. It supports column type inference and many other features, like batching and shuffling, to make usage simple.\n\ntitanic_batches = tf.data.experimental.make_csv_dataset( titanic_file, batch_size=4, label_name=""survived"")\n\nfor feature_batch, label_batch in titanic_batches.take(1): print(""\'survived\': {}"".format(label_batch)) print(""features:"") for key, value in feature_batch.items(): print("" {!r:20s}: {}"".format(key, value))\n\n\'survived\': [0 1 0 0] features: \'sex\' : [b\'male\' b\'male\' b\'male\' b\'female\'] \'age\' : [28. 25. 30. 28.] \'n_siblings_spouses\': [1 1 0 3] \'parch\' : [0 0 0 1] \'fare\' : [15.5 7.775 27.75 25.4667] \'class\' : [b\'Third\' b\'Third\' b\'First\' b\'Third\'] \'deck\' : [b\'unknown\' b\'unknown\' b\'C\' b\'unknown\'] \'embark_town\' : [b\'Queenstown\' b\'Southampton\' b\'Cherbourg\' b\'Southampton\'] \'alone\' : [b\'n\' b\'n\' b\'y\' b\'n\']\n\nYou can use the select_columns argument if you only need a subset of columns.\n\ntitanic_batches = tf.data.experimental.make_csv_dataset( titanic_file, batch_size=4, label_name=""survived"", select_columns=[\'class\', \'fare\', \'survived\'])\n\nfor feature_batch, label_batch in titanic_batches.take(1): print(""\'survived\': {}"".format(label_batch)) for key, value in feature_batch.items(): print("" {!r:20s}: {}"".format(key, value))\n\n\'survived\': [1 0 1 0] \'fare\' : [11.1333 24.15 7.925 52. ] \'class\' : [b\'Third\' b\'Third\' b\'Third\' b\'First\']\n\nThere is also a lower-level experimental.CsvDataset class which provides finer grained control. It does not support column type inference. Instead you must specify the type of each column.\n\ntitanic_types = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string] dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True) for line in dataset.take(10): print([item.numpy() for item in line])\n\n[0, b\'male\', 22.0, 1, 0, 7.25, b\'Third\', b\'unknown\', b\'Southampton\', b\'n\'] [1, b\'female\', 38.0, 1, 0, 71.2833, b\'First\', b\'C\', b\'Cherbourg\', b\'n\'] [1, b\'female\', 26.0, 0, 0, 7.925, b\'Third\', b\'unknown\', b\'Southampton\', b\'y\'] [1, b\'female\', 35.0, 1, 0, 53.1, b\'First\', b\'C\', b\'Southampton\', b\'n\'] [0, b\'male\', 28.0, 0, 0, 8.4583, b\'Third\', b\'unknown\', b\'Queenstown\', b\'y\'] [0, b\'male\', 2.0, 3, 1, 21.075, b\'Third\', b\'unknown\', b\'Southampton\', b\'n\'] [1, b\'female\', 27.0, 0, 2, 11.1333, b\'Third\', b\'unknown\', b\'Southampton\', b\'n\'] [1, b\'female\', 14.0, 1, 0, 30.0708, b\'Second\', b\'unknown\', b\'Cherbourg\', b\'n\'] [1, b\'female\', 4.0, 1, 1, 16.7, b\'Third\', b\'G\', b\'Southampton\', b\'n\'] [0, b\'male\', 20.0, 0, 0, 8.05, b\'Third\', b\'unknown\', b\'Southampton\', b\'y\']\n\nIf some columns are empty, this low-level interface allows you to provide default values instead of column types.\n\n%%writefile missing.csv 1,2,3,4 ,2,3,4 1,,3,4 1,2,,4 1,2,3, ,,,\n\n# Creates a dataset that reads all of the records from two CSV files, each with # four float columns which may have missing values. record_defaults = [999,999,999,999] dataset = tf.data.experimental.CsvDataset(""missing.csv"", record_defaults) dataset = dataset.map(lambda *items: tf.stack(items)) dataset\n\n<_MapDataset element_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None)>\n\nfor line in dataset: print(line.numpy())\n\n[1 2 3 4] [999 2 3 4] [ 1 999 3 4] [ 1 2 999 4] [ 1 2 3 999] [999 999 999 999]\n\nBy default, a CsvDataset yields every column of every line of the file, which may not be desirable, for example if the file starts with a header line that should be ignored, or if some columns are not required in the input. These lines and fields can be removed with the header and select_cols arguments respectively.\n\n# Creates a dataset that reads all of the records from two CSV files with # headers, extracting float data from columns 2 and 4. record_defaults = [999, 999] # Only provide defaults for the selected columns dataset = tf.data.experimental.CsvDataset(""missing.csv"", record_defaults, select_cols=[1, 3]) dataset = dataset.map(lambda *items: tf.stack(items)) dataset\n\n<_MapDataset element_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None)>\n\nfor line in dataset: print(line.numpy())\n\n[2 4] [2 4] [999 4] [2 4] [ 2 999] [999 999]\n\nConsuming sets of files\n\nThere are many datasets distributed as a set of files, where each file is an example.\n\nflowers_root = tf.keras.utils.get_file( \'flower_photos\', \'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\', untar=True) flowers_root = pathlib.Path(flowers_root)\n\nNote: these images are licensed CC-BY, see LICENSE.txt for details.\n\nThe root directory contains a directory for each class:\n\nfor item in flowers_root.glob(""*""): print(item.name)\n\nroses sunflowers LICENSE.txt dandelion tulips daisy\n\nThe files in each class directory are examples:\n\nlist_ds = tf.data.Dataset.list_files(str(flowers_root/\'*/*\')) for f in list_ds.take(5): print(f.numpy())\n\nb\'/home/kbuilder/.keras/datasets/flower_photos/tulips/4520577328_a94c11e806_n.jpg\' b\'/home/kbuilder/.keras/datasets/flower_photos/dandelion/22679060358_561ec823ae_m.jpg\' b\'/home/kbuilder/.keras/datasets/flower_photos/daisy/6978826370_7b9aa7c7d5.jpg\' b\'/home/kbuilder/.keras/datasets/flower_photos/tulips/3447650747_8299786b80_n.jpg\' b\'/home/kbuilder/.keras/datasets/flower_photos/tulips/112951022_4892b1348b_n.jpg\'\n\nRead the data using the tf.io.read_file function and extract the label from the path, returning (image, label) pairs:\n\ndef process_path(file_path): label = tf.strings.split(file_path, os.sep)[-2] return tf.io.read_file(file_path), label labeled_ds = list_ds.map(process_path)\n\nfor image_raw, label_text in labeled_ds.take(1): print(repr(image_raw.numpy()[:100])) print() print(label_text.numpy())\n\nb""\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe1\\t\\xcbXMP\\x00://ns.adobe.com/xap/1.0/\\x00<?xpacket begin=\'\\xef\\xbb\\xbf\' id=\'W5M0MpCehiHzreSzNTczk"" b\'roses\'\n\nBatching dataset elements\n\nThe simplest form of batching stacks n consecutive elements of a dataset into a single element. The Dataset.batch() transformation does exactly this, with the same constraints as the tf.stack() operator, applied to each component of the elements: i.e., for each component i, all elements must have a tensor of the exact same shape.\n\ninc_dataset = tf.data.Dataset.range(100) dec_dataset = tf.data.Dataset.range(0, -100, -1) dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset)) batched_dataset = dataset.batch(4) for batch in batched_dataset.take(4): print([arr.numpy() for arr in batch])\n\n[array([0, 1, 2, 3]), array([ 0, -1, -2, -3])] [array([4, 5, 6, 7]), array([-4, -5, -6, -7])] [array([ 8, 9, 10, 11]), array([ -8, -9, -10, -11])] [array([12, 13, 14, 15]), array([-12, -13, -14, -15])]\n\nWhile tf.data tries to propagate shape information, the default settings of Dataset.batch result in an unknown batch size because the last batch may not be full. Note the Nones in the shape:\n\n<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n\nUse the drop_remainder argument to ignore that last batch, and get full shape propagation:\n\nbatched_dataset = dataset.batch(7, drop_remainder=True) batched_dataset\n\n<_BatchDataset element_spec=(TensorSpec(shape=(7,), dtype=tf.int64, name=None), TensorSpec(shape=(7,), dtype=tf.int64, name=None))>\n\nBatching tensors with padding\n\nThe above recipe works for tensors that all have the same size. However, many models (including sequence models) work with input data that can have varying size (for example, sequences of different lengths). To handle this case, the Dataset.padded_batch transformation enables you to batch tensors of different shapes by specifying one or more dimensions in which they may be padded.\n\ndataset = tf.data.Dataset.range(100) dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x)) dataset = dataset.padded_batch(4, padded_shapes=(None,)) for batch in dataset.take(2): print(batch.numpy()) print()\n\n[[0 0 0] [1 0 0] [2 2 0] [3 3 3]] [[4 4 4 4 0 0 0] [5 5 5 5 5 0 0] [6 6 6 6 6 6 0] [7 7 7 7 7 7 7]]\n\nThe Dataset.padded_batch transformation allows you to set different padding for each dimension of each component, and it may be variable-length (signified by None in the example above) or constant-length. It is also possible to override the padding value, which defaults to 0.\n\nProcessing multiple epochs\n\nThe tf.data API offers two main ways to process multiple epochs of the same data.\n\nThe simplest way to iterate over a dataset in multiple epochs is to use the Dataset.repeat() transformation. First, create a dataset of titanic data:\n\ntitanic_file = tf.keras.utils.get_file(""train.csv"", ""https://storage.googleapis.com/tf-datasets/titanic/train.csv"") titanic_lines = tf.data.TextLineDataset(titanic_file)\n\ndef plot_batch_sizes(ds): batch_sizes = [batch.shape[0] for batch in ds] plt.bar(range(len(batch_sizes)), batch_sizes) plt.xlabel(\'Batch number\') plt.ylabel(\'Batch size\')\n\nApplying the Dataset.repeat() transformation with no arguments will repeat the input indefinitely.\n\nThe Dataset.repeat transformation concatenates its arguments without signaling the end of one epoch and the beginning of the next epoch. Because of this a Dataset.batch applied after Dataset.repeat will yield batches that straddle epoch boundaries:\n\ntitanic_batches = titanic_lines.repeat(3).batch(128) plot_batch_sizes(titanic_batches)\n\nIf you need clear epoch separation, put Dataset.batch before the repeat:\n\ntitanic_batches = titanic_lines.batch(128).repeat(3) plot_batch_sizes(titanic_batches)\n\nIf you would like to perform a custom computation (for example, to collect statistics) at the end of each epoch then it\'s simplest to restart the dataset iteration on each epoch:\n\nepochs = 3 dataset = titanic_lines.batch(128) for epoch in range(epochs): for batch in dataset: print(batch.shape) print(""End of epoch: "", epoch)\n\n(128,) (128,) (128,) (128,) (116,) End of epoch: 0 (128,) (128,) (128,) (128,) (116,) End of epoch: 1 (128,) (128,) (128,) (128,) (116,) End of epoch: 2\n\nRandomly shuffling input data\n\nThe Dataset.shuffle() transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer.\n\nNote: While large buffer_sizes shuffle more thoroughly, they can take a lot of memory, and significant time to fill. Consider using Dataset.interleave across files if this becomes a problem.\n\nAdd an index to the dataset so you can see the effect:\n\nlines = tf.data.TextLineDataset(titanic_file) counter = tf.data.experimental.Counter() dataset = tf.data.Dataset.zip((counter, lines)) dataset = dataset.shuffle(buffer_size=100) dataset = dataset.batch(20) dataset\n\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_14491/4092668703.py:2: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.counter(...)` instead. <_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n\nSince the buffer_size is 100, and the batch size is 20, the first batch contains no elements with an index over 120.\n\nn,line_batch = next(iter(dataset)) print(n.numpy())\n\n[ 43 13 17 72 33 89 83 105 96 81 0 67 97 84 73 32 30 71 64 103]\n\nAs with Dataset.batch the order relative to Dataset.repeat matters.\n\nDataset.shuffle doesn\'t signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next:\n\ndataset = tf.data.Dataset.zip((counter, lines)) shuffled = dataset.shuffle(buffer_size=100).batch(10).repeat(2) print(""Here are the item ID\'s near the epoch boundary:\\n"") for n, line_batch in shuffled.skip(60).take(5): print(n.numpy())\n\nHere are the item ID\'s near the epoch boundary: [625 583 586 606 504 597 453 615 575 429] [424 456 452 605 483 566 395 556 492 365] [570 573 611 540 545 559 388 579] [ 0 18 92 79 81 86 62 103 29 82] [47 69 17 95 9 11 77 84 31 53]\n\nshuffle_repeat = [n.numpy().mean() for n, line_batch in shuffled] plt.plot(shuffle_repeat, label=""shuffle().repeat()"") plt.ylabel(""Mean item ID"") plt.legend()\n\n<matplotlib.legend.Legend at 0x7f32b0355dc0>\n\nBut a repeat before a shuffle mixes the epoch boundaries together:\n\ndataset = tf.data.Dataset.zip((counter, lines)) shuffled = dataset.repeat(2).shuffle(buffer_size=100).batch(10) print(""Here are the item ID\'s near the epoch boundary:\\n"") for n, line_batch in shuffled.skip(55).take(15): print(n.numpy())\n\nHere are the item ID\'s near the epoch boundary: [464 377 595 328 614 504 7 20 433 623] [419 550 615 509 499 540 557 622 570 544] [588 618 616 4 460 38 16 617 31 591] [440 610 585 600 36 17 35 52 592 19] [523 59 545 624 607 51 53 26 33 318] [510 37 6 448 612 469 32 10 39 594] [ 41 63 13 627 67 76 386 579 412 55] [ 1 54 626 71 64 22 47 553 525 65] [ 69 3 15 102 14 455 23 98 74 78] [596 12 50 5 18 112 114 97 61 42] [103 84 583 90 350 575 606 85 107 108] [115 127 60 602 118 43 34 58 46 587] [119 56 620 75 564 625 88 140 539 45] [589 100 149 452 110 11 66 132 142 111] [101 334 94 497 520 158 120 86 135 95]\n\nrepeat_shuffle = [n.numpy().mean() for n, line_batch in shuffled] plt.plot(shuffle_repeat, label=""shuffle().repeat()"") plt.plot(repeat_shuffle, label=""repeat().shuffle()"") plt.ylabel(""Mean item ID"") plt.legend()\n\n<matplotlib.legend.Legend at 0x7f32b033bdf0>\n\nThe Dataset.map(f) transformation produces a new dataset by applying a given function f to each element of the input dataset. It is based on the map() function that is commonly applied to lists (and other structures) in functional programming languages. The function f takes the tf.Tensor objects that represent a single element in the input, and returns the tf.Tensor objects that will represent a single element in the new dataset. Its implementation uses standard TensorFlow operations to transform one element into another.\n\nThis section covers common examples of how to use Dataset.map().\n\nDecoding image data and resizing it\n\nWhen training a neural network on real-world image data, it is often necessary to convert images of different sizes to a common size, so that they may be batched into a fixed size.\n\nRebuild the flower filenames dataset:\n\nlist_ds = tf.data.Dataset.list_files(str(flowers_root/\'*/*\'))\n\nWrite a function that manipulates the dataset elements.\n\n# Reads an image from a file, decodes it into a dense tensor, and resizes it # to a fixed shape. def parse_image(filename): parts = tf.strings.split(filename, os.sep) label = parts[-2] image = tf.io.read_file(filename) image = tf.io.decode_jpeg(image) image = tf.image.convert_image_dtype(image, tf.float32) image = tf.image.resize(image, [128, 128]) return image, label\n\nfile_path = next(iter(list_ds)) image, label = parse_image(file_path) def show(image, label): plt.figure() plt.imshow(image) plt.title(label.numpy().decode(\'utf-8\')) plt.axis(\'off\') show(image, label)\n\nMap it over the dataset.\n\nimages_ds = list_ds.map(parse_image) for image, label in images_ds.take(2): show(image, label)\n\nApplying arbitrary Python logic\n\nFor performance reasons, use TensorFlow operations for preprocessing your data whenever possible. However, it is sometimes useful to call external Python libraries when parsing your input data. You can use the tf.py_function operation in a Dataset.map transformation.\n\nFor example, if you want to apply a random rotation, the tf.image module only has tf.image.rot90, which is not very useful for image augmentation.\n\nNote: tensorflow_addons has a TensorFlow compatible rotate in tensorflow_addons.image.rotate.\n\nTo demonstrate tf.py_function, try using the scipy.ndimage.rotate function instead:\n\nimport scipy.ndimage as ndimage @tf.py_function(Tout=tf.float32) def random_rotate_image(image): image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False) return image\n\nimage, label = next(iter(images_ds)) image = random_rotate_image(image) show(image, label)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\nTo use this function with Dataset.map the same caveats apply as with Dataset.from_generator, you need to describe the return shapes and types when you apply the function:\n\ndef tf_random_rotate_image(image, label): im_shape = image.shape image = random_rotate_image(image) image.set_shape(im_shape) return image, label\n\nrot_ds = images_ds.map(tf_random_rotate_image) for image, label in rot_ds.take(2): show(image, label)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\nParsing tf.Example protocol buffer messages\n\nMany input pipelines extract tf.train.Example protocol buffer messages from a TFRecord format. Each tf.train.Example record contains one or more ""features"", and the input pipeline typically converts these features into tensors.\n\nfsns_test_file = tf.keras.utils.get_file(""fsns.tfrec"", ""https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001"") dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file]) dataset\n\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\nYou can work with tf.train.Example protos outside of a tf.data.Dataset to understand the data:\n\nraw_example = next(iter(dataset)) parsed = tf.train.Example.FromString(raw_example.numpy()) feature = parsed.features.feature raw_img = feature[\'image/encoded\'].bytes_list.value[0] img = tf.image.decode_png(raw_img) plt.imshow(img) plt.axis(\'off\') _ = plt.title(feature[""image/text""].bytes_list.value[0])\n\nraw_example = next(iter(dataset))\n\ndef tf_parse(eg): example = tf.io.parse_example( eg[tf.newaxis], { \'image/encoded\': tf.io.FixedLenFeature(shape=(), dtype=tf.string), \'image/text\': tf.io.FixedLenFeature(shape=(), dtype=tf.string) }) return example[\'image/encoded\'][0], example[\'image/text\'][0]\n\nimg, txt = tf_parse(raw_example) print(txt.numpy()) print(repr(img.numpy()[:20]), ""..."")\n\nb\'Rue Perreyon\' b\'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02X\' ...\n\ndecoded = dataset.map(tf_parse) decoded\n\n<_MapDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>\n\nimage_batch, text_batch = next(iter(decoded.batch(10))) image_batch.shape\n\nTime series windowing\n\nFor an end-to-end time series example see: Time series forecasting.\n\nTime series data is often organized with the time axis intact.\n\nUse a simple Dataset.range to demonstrate:\n\nrange_ds = tf.data.Dataset.range(100000)\n\nTypically, models based on this sort of data will want a contiguous time slice.\n\nThe simplest approach would be to batch the data:\n\nbatches = range_ds.batch(10, drop_remainder=True) for batch in batches.take(5): print(batch.numpy())\n\n[0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28 29] [30 31 32 33 34 35 36 37 38 39] [40 41 42 43 44 45 46 47 48 49]\n\nOr to make dense predictions one step into the future, you might shift the features and labels by one step relative to each other:\n\ndef dense_1_step(batch): # Shift features and labels one step relative to each other. return batch[:-1], batch[1:] predict_dense_1_step = batches.map(dense_1_step) for features, label in predict_dense_1_step.take(3): print(features.numpy(), "" => "", label.numpy())\n\n[0 1 2 3 4 5 6 7 8] => [1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18] => [11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28] => [21 22 23 24 25 26 27 28 29]\n\nTo predict a whole window instead of a fixed offset you can split the batches into two parts:\n\nbatches = range_ds.batch(15, drop_remainder=True) def label_next_5_steps(batch): return (batch[:-5], # Inputs: All except the last 5 steps batch[-5:]) # Labels: The last 5 steps predict_5_steps = batches.map(label_next_5_steps) for features, label in predict_5_steps.take(3): print(features.numpy(), "" => "", label.numpy())\n\n[0 1 2 3 4 5 6 7 8 9] => [10 11 12 13 14] [15 16 17 18 19 20 21 22 23 24] => [25 26 27 28 29] [30 31 32 33 34 35 36 37 38 39] => [40 41 42 43 44]\n\nTo allow some overlap between the features of one batch and the labels of another, use Dataset.zip:\n\nfeature_length = 10 label_length = 3 features = range_ds.batch(feature_length, drop_remainder=True) labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:label_length]) predicted_steps = tf.data.Dataset.zip((features, labels)) for features, label in predicted_steps.take(5): print(features.numpy(), "" => "", label.numpy())\n\n[0 1 2 3 4 5 6 7 8 9] => [10 11 12] [10 11 12 13 14 15 16 17 18 19] => [20 21 22] [20 21 22 23 24 25 26 27 28 29] => [30 31 32] [30 31 32 33 34 35 36 37 38 39] => [40 41 42] [40 41 42 43 44 45 46 47 48 49] => [50 51 52]\n\nWhile using Dataset.batch works, there are situations where you may need finer control. The Dataset.window method gives you complete control, but requires some care: it returns a Dataset of Datasets. Go to the Dataset structure section for details.\n\nwindow_size = 5 windows = range_ds.window(window_size, shift=1) for sub_ds in windows.take(5): print(sub_ds)\n\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\nThe Dataset.flat_map method can take a dataset of datasets and flatten it into a single dataset:\n\nfor x in windows.flat_map(lambda x: x).take(30): print(x.numpy(), end=\' \')\n\n0 1 2 3 4 1 2 3 4 5 2 3 4 5 6 3 4 5 6 7 4 5 6 7 8 5 6 7 8 9\n\nIn nearly all cases, you will want to Dataset.batch the dataset first:\n\ndef sub_to_batch(sub): return sub.batch(window_size, drop_remainder=True) for example in windows.flat_map(sub_to_batch).take(5): print(example.numpy())\n\n[0 1 2 3 4] [1 2 3 4 5] [2 3 4 5 6] [3 4 5 6 7] [4 5 6 7 8]\n\nNow, you can see that the shift argument controls how much each window moves over.\n\nPutting this together you might write this function:\n\ndef make_window_dataset(ds, window_size=5, shift=1, stride=1): windows = ds.window(window_size, shift=shift, stride=stride) def sub_to_batch(sub): return sub.batch(window_size, drop_remainder=True) windows = windows.flat_map(sub_to_batch) return windows\n\nds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3) for example in ds.take(10): print(example.numpy())\n\n[ 0 3 6 9 12 15 18 21 24 27] [ 5 8 11 14 17 20 23 26 29 32] [10 13 16 19 22 25 28 31 34 37] [15 18 21 24 27 30 33 36 39 42] [20 23 26 29 32 35 38 41 44 47] [25 28 31 34 37 40 43 46 49 52] [30 33 36 39 42 45 48 51 54 57] [35 38 41 44 47 50 53 56 59 62] [40 43 46 49 52 55 58 61 64 67] [45 48 51 54 57 60 63 66 69 72]\n\nThen it\'s easy to extract labels, as before:\n\ndense_labels_ds = ds.map(dense_1_step) for inputs,labels in dense_labels_ds.take(3): print(inputs.numpy(), ""=>"", labels.numpy())\n\n[ 0 3 6 9 12 15 18 21 24] => [ 3 6 9 12 15 18 21 24 27] [ 5 8 11 14 17 20 23 26 29] => [ 8 11 14 17 20 23 26 29 32] [10 13 16 19 22 25 28 31 34] => [13 16 19 22 25 28 31 34 37]\n\nWhen working with a dataset that is very class-imbalanced, you may want to resample the dataset. tf.data provides two methods to do this. The credit card fraud dataset is a good example of this sort of problem.\n\nNote: Go to Classification on imbalanced data for a full tutorial.\n\nzip_path = tf.keras.utils.get_file( origin=\'https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip\', fname=\'creditcard.zip\', extract=True) csv_path = zip_path.replace(\'.zip\', \'.csv\')\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip 69155632/69155632 [==============================] - 0s 0us/step\n\ncreditcard_ds = tf.data.experimental.make_csv_dataset( csv_path, batch_size=1024, label_name=""Class"", # Set the column types: 30 floats and an int. column_defaults=[float()]*30+[int()])\n\nNow, check the distribution of classes, it is highly skewed:\n\ndef count(counts, batch): features, labels = batch class_1 = labels == 1 class_1 = tf.cast(class_1, tf.int32) class_0 = labels == 0 class_0 = tf.cast(class_0, tf.int32) counts[\'class_0\'] += tf.reduce_sum(class_0) counts[\'class_1\'] += tf.reduce_sum(class_1) return counts\n\ncounts = creditcard_ds.take(10).reduce( initial_state={\'class_0\': 0, \'class_1\': 0}, reduce_func = count) counts = np.array([counts[\'class_0\'].numpy(), counts[\'class_1\'].numpy()]).astype(np.float32) fractions = counts/counts.sum() print(fractions)\n\nA common approach to training with an imbalanced dataset is to balance it. tf.data includes a few methods which enable this workflow:\n\nOne approach to resampling a dataset is to use sample_from_datasets. This is more applicable when you have a separate tf.data.Dataset for each class.\n\nHere, just use filter to generate them from the credit card fraud data:\n\nnegative_ds = ( creditcard_ds .unbatch() .filter(lambda features, label: label==0) .repeat()) positive_ds = ( creditcard_ds .unbatch() .filter(lambda features, label: label==1) .repeat())\n\nfor features, label in positive_ds.batch(10).take(1): print(label.numpy())\n\n[1 1 1 1 1 1 1 1 1 1]\n\nTo use tf.data.Dataset.sample_from_datasets pass the datasets, and the weight for each:\n\nbalanced_ds = tf.data.Dataset.sample_from_datasets( [negative_ds, positive_ds], [0.5, 0.5]).batch(10)\n\nNow the dataset produces examples of each class with a 50/50 probability:\n\nfor features, labels in balanced_ds.take(10): print(labels.numpy())\n\n[0 0 0 1 0 0 1 0 0 0] [1 1 0 0 1 1 1 0 0 1] [1 0 0 0 1 1 0 0 0 0] [1 0 0 0 0 1 0 1 0 0] [1 1 1 1 0 1 0 0 1 0] [0 1 1 0 0 0 1 1 0 1] [0 1 1 0 1 1 0 1 1 1] [0 0 0 1 0 0 1 1 0 0] [0 1 0 0 1 1 1 0 1 0] [1 1 1 1 1 1 0 0 0 0]\n\nRejection resampling\n\nOne problem with the above Dataset.sample_from_datasets approach is that it needs a separate tf.data.Dataset per class. You could use Dataset.filter to create those two datasets, but that results in all the data being loaded twice.\n\nThe tf.data.Dataset.rejection_resample method can be applied to a dataset to rebalance it, while only loading it once. Elements will be dropped or repeated to achieve balance.\n\nThe rejection_resample method takes a class_func argument. This class_func is applied to each dataset element, and is used to determine which class an example belongs to for the purposes of balancing.\n\nThe goal here is to balance the label distribution, and the elements of creditcard_ds are already (features, label) pairs. So the class_func just needs to return those labels:\n\ndef class_func(features, label): return label\n\nThe resampling method deals with individual examples, so in this case you must unbatch the dataset before applying that method.\n\nThe method needs a target distribution, and optionally an initial distribution estimate as inputs.\n\nresample_ds = ( creditcard_ds .unbatch() .rejection_resample(class_func, target_dist=[0.5,0.5], initial_dist=fractions) .batch(10))\n\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4963: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20. Instructions for updating: Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n\nThe rejection_resample method returns (class, example) pairs where the class is the output of the class_func. In this case, the example was already a (feature, label) pair, so use map to drop the extra copy of the labels:\n\nbalanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)\n\nNow the dataset produces examples of each class with a 50/50 probability:\n\nfor features, labels in balanced_ds.take(10): print(labels.numpy())\n\nProportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] [0 1 0 1 0 0 1 1 0 1] [1 1 0 1 0 0 1 1 0 0] [0 0 1 0 1 0 0 0 0 1] [1 0 0 1 0 0 1 0 1 0] [0 1 1 0 0 1 0 1 1 0] [1 1 0 1 1 0 1 1 0 1] [1 1 1 0 0 0 0 0 0 1] [0 1 0 0 0 0 0 1 1 1] [0 1 0 1 1 0 0 1 0 1] [0 0 1 0 1 1 1 0 0 1]\n\nIterator Checkpointing\n\nTensorflow supports taking checkpoints so that when your training process restarts it can restore the latest checkpoint to recover most of its progress. In addition to checkpointing the model variables, you can also checkpoint the progress of the dataset iterator. This could be useful if you have a large dataset and don\'t want to start the dataset from the beginning on each restart. Note however that iterator checkpoints may be large, since transformations such as Dataset.shuffle and Dataset.prefetch require buffering elements within the iterator.\n\nTo include your iterator in a checkpoint, pass the iterator to the tf.train.Checkpoint constructor.\n\nrange_ds = tf.data.Dataset.range(20) iterator = iter(range_ds) ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator) manager = tf.train.CheckpointManager(ckpt, \'/tmp/my_ckpt\', max_to_keep=3) print([next(iterator).numpy() for _ in range(5)]) save_path = manager.save() print([next(iterator).numpy() for _ in range(5)]) ckpt.restore(manager.latest_checkpoint) print([next(iterator).numpy() for _ in range(5)])\n\n[0, 1, 2, 3, 4] [5, 6, 7, 8, 9] [5, 6, 7, 8, 9]\n\nNote: It is not possible to checkpoint an iterator which relies on an external state, such as a tf.py_function. Attempting to do so will raise an exception complaining about the external state.\n\nUsing tf.data with tf.keras\n\nThe tf.keras API simplifies many aspects of creating and executing machine learning models. Its Model.fit and Model.evaluate and Model.predict APIs support datasets as inputs. Here is a quick dataset and model setup:\n\ntrain, test = tf.keras.datasets.fashion_mnist.load_data() images, labels = train images = images/255.0 labels = labels.astype(np.int32)\n\nfmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels)) fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32) model = tf.keras.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(10) ]) model.compile(optimizer=\'adam\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\'accuracy\'])\n\nPassing a dataset of (feature, label) pairs is all that\'s needed for Model.fit and Model.evaluate:\n\nmodel.fit(fmnist_train_ds, epochs=2)\n\nEpoch 1/2 26/1875 [..............................] - ETA: 3s - loss: 1.7645 - accuracy: 0.3930 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1705458306.961075 14743 device_compiler.h:186] Compiled cluster using XLA! This line is logged at most once for the lifetime of the process. 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5948 - accuracy: 0.7994 Epoch 2/2 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4601 - accuracy: 0.8427 <keras.src.callbacks.History at 0x7f32b0722250>\n\nIf you pass an infinite dataset, for example by calling Dataset.repeat, you just need to also pass the steps_per_epoch argument:\n\nmodel.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)\n\nEpoch 1/2 20/20 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.8344 Epoch 2/2 20/20 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.8438 <keras.src.callbacks.History at 0x7f32b0457100>\n\nFor evaluation you can pass the number of evaluation steps:\n\nloss, accuracy = model.evaluate(fmnist_train_ds) print(""Loss :"", loss) print(""Accuracy :"", accuracy)\n\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.4396 - accuracy: 0.8490 Loss : 0.4396059811115265 Accuracy : 0.8489833474159241\n\nFor long datasets, set the number of steps to evaluate:\n\nloss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10) print(""Loss :"", loss) print(""Accuracy :"", accuracy)\n\n10/10 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8844 Loss : 0.4079427719116211 Accuracy : 0.8843749761581421\n\nThe labels are not required when calling Model.predict.\n\npredict_ds = tf.data.Dataset.from_tensor_slices(images).batch(32) result = model.predict(predict_ds, steps = 10) print(result.shape)\n\n10/10 [==============================] - 0s 1ms/step (320, 10)\n\nBut the labels are ignored if you do pass a dataset containing them:\n\nresult = model.predict(fmnist_train_ds, steps = 10) print(result.shape)\n\n10/10 [==============================] - 0s 1ms/step (320, 10)\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-01-17 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', metadata={'id': 'web-search_4', 'snippet': 'Español – América Latina\n\nPortuguês – Brasil\n\nภาษาไทย\n\ntf.data: Build TensorFlow input pipelines\n\nStay organized with collections Save and categorize content based on your preferences.\n\nView on TensorFlow.org\n\nView source on GitHub\n\nThe tf.data API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. The tf.data API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations.\n\nThe tf.data API introduces a tf.data.Dataset abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\n\nThere are two distinct ways to create a dataset:\n\nA data source constructs a Dataset from data stored in memory or in one or more files.\n\nA data transformation constructs a dataset from one or more tf.data.Dataset objects.\n\nimport tensorflow as tf\n\n2024-01-17 02:24:38.925403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2024-01-17 02:24:38.925446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2024-01-17 02:24:38.927059: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nimport pathlib import os import matplotlib.pyplot as plt import pandas as pd import numpy as np np.set_printoptions(precision=4)\n\nTo create an input pipeline, you must start with a data source. For example, to construct a Dataset from data in memory, you can use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices(). Alternatively, if your input data is stored in a file in the recommended TFRecord format, you can use tf.data.TFRecordDataset().\n\nOnce you have a Dataset object, you can transform it into a new Dataset by chaining method calls on the tf.data.Dataset object. For example, you can apply per-element transformations such as Dataset.map, and multi-element transformations such as Dataset.batch. Refer to the documentation for tf.data.Dataset for a complete list of transformations.\n\nThe Dataset object is a Python iterable. This makes it possible to consume its elements using a for loop:\n\ndataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1]) dataset\n\n<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n\nfor elem in dataset: print(elem.numpy())\n\nOr by explicitly creating a Python iterator using iter and consuming its elements using next:\n\nit = iter(dataset) print(next(it).numpy())\n\nAlternatively, dataset elements can be consumed using the reduce transformation, which reduces all elements to produce a single result. The following example illustrates how to use the reduce transformation to compute the sum of a dataset of integers.\n\nprint(dataset.reduce(0, lambda state, value: state + value).numpy())\n\nA dataset produces a sequence of elements, where each element is the same (nested) structure of components. Individual components of the structure can be of any type representable by tf.TypeSpec, including tf.Tensor, tf.sparse.SparseTensor, tf.RaggedTensor, tf.TensorArray, or tf.data.Dataset.\n\nThe Python constructs that can be used to express the (nested) structure of elements include tuple, dict, NamedTuple, and OrderedDict. In particular, list is not a valid construct for expressing the structure of dataset elements. This is because early tf.data users felt strongly about list inputs (for example, when passed to tf.data.Dataset.from_tensors) being automatically packed as tensors and list outputs (for example, return values of user-defined functions) being coerced into a tuple. As a consequence, if you would like a list input to be treated as a structure, you need to convert it into tuple and if you would like a list output to be a single component, then you need to explicitly pack it using tf.stack.\n\nThe Dataset.element_spec property allows you to inspect the type of each element component. The property returns a nested structure of tf.TypeSpec objects, matching the structure of the element, which may be a single component, a tuple of components, or a nested tuple of components. For example:\n\ndataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10])) dataset1.element_spec\n\nTensorSpec(shape=(10,), dtype=tf.float32, name=None)\n\ndataset2 = tf.data.Dataset.from_tensor_slices( (tf.random.uniform([4]), tf.random.uniform([4, 100], maxval=100, dtype=tf.int32))) dataset2.element_spec\n\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))\n\ndataset3 = tf.data.Dataset.zip((dataset1, dataset2)) dataset3.element_spec\n\n(TensorSpec(shape=(10,), dtype=tf.float32, name=None), (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None)))\n\n# Dataset containing a sparse tensor. dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])) dataset4.element_spec\n\nSparseTensorSpec(TensorShape([3, 4]), tf.int32)\n\n# Use value_type to see the type of value represented by the element spec dataset4.element_spec.value_type\n\ntensorflow.python.framework.sparse_tensor.SparseTensor\n\nThe Dataset transformations support datasets of any structure. When using the Dataset.map, and Dataset.filter transformations, which apply a function to each element, the element structure determines the arguments of the function:\n\ndataset1 = tf.data.Dataset.from_tensor_slices( tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32)) dataset1\n\n<_TensorSliceDataset element_spec=TensorSpec(shape=(10,), dtype=tf.int32, name=None)>\n\nfor z in dataset1: print(z.numpy())\n\n[2 1 3 2 1 3 9 6 4 5] [8 9 2 9 1 4 7 2 4 5] [3 9 6 8 4 6 8 4 9 5] [4 3 7 2 8 6 4 9 7 6]\n\ndataset2 = tf.data.Dataset.from_tensor_slices( (tf.random.uniform([4]), tf.random.uniform([4, 100], maxval=100, dtype=tf.int32))) dataset2\n\n<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))>\n\ndataset3 = tf.data.Dataset.zip((dataset1, dataset2)) dataset3\n\n<_ZipDataset element_spec=(TensorSpec(shape=(10,), dtype=tf.int32, name=None), (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None)))>\n\nfor a, (b,c) in dataset3: print(\'shapes: {a.shape}, {b.shape}, {c.shape}\'.format(a=a, b=b, c=c))\n\nshapes: (10,), (), (100,) shapes: (10,), (), (100,) shapes: (10,), (), (100,) shapes: (10,), (), (100,)\n\nConsuming NumPy arrays\n\nRefer to the Loading NumPy arrays tutorial for more examples.\n\nIf all of your input data fits in memory, the simplest way to create a Dataset from them is to convert them to tf.Tensor objects and use Dataset.from_tensor_slices.\n\ntrain, test = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 29515/29515 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26421880/26421880 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 5148/5148 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4422102/4422102 [==============================] - 0s 0us/step\n\nimages, labels = train images = images/255 dataset = tf.data.Dataset.from_tensor_slices((images, labels)) dataset\n\n<_TensorSliceDataset element_spec=(TensorSpec(shape=(28, 28), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>\n\nNote: The above code snippet will embed the features and labels arrays in your TensorFlow graph as tf.constant() operations. This works well for a small dataset, but wastes memory---because the contents of the array will be copied multiple times---and can run into the 2GB limit for the tf.GraphDef protocol buffer.\n\nConsuming Python generators\n\nAnother common data source that can easily be ingested as a tf.data.Dataset is the python generator.\n\nCaution: While this is a convenient approach it has limited portability and scalability. It must run in the same python process that created the generator, and is still subject to the Python GIL.\n\ndef count(stop): i = 0 while i<stop: yield i i += 1\n\nfor n in count(5): print(n)\n\nThe Dataset.from_generator constructor converts the python generator to a fully functional tf.data.Dataset.\n\nThe constructor takes a callable as input, not an iterator. This allows it to restart the generator when it reaches the end. It takes an optional args argument, which is passed as the callable\'s arguments.\n\nThe output_types argument is required because tf.data builds a tf.Graph internally, and graph edges require a tf.dtype.\n\nds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes = (), )\n\nfor count_batch in ds_counter.repeat().batch(10).take(10): print(count_batch.numpy())\n\n[0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 0 1 2 3 4] [ 5 6 7 8 9 10 11 12 13 14] [15 16 17 18 19 20 21 22 23 24] [0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 0 1 2 3 4] [ 5 6 7 8 9 10 11 12 13 14] [15 16 17 18 19 20 21 22 23 24]\n\nThe output_shapes argument is not required but is highly recommended as many TensorFlow operations do not support tensors with an unknown rank. If the length of a particular axis is unknown or variable, set it as None in the output_shapes.\n\nIt\'s also important to note that the output_shapes and output_types follow the same nesting rules as other dataset methods.\n\nHere is an example generator that demonstrates both aspects: it returns tuples of arrays, where the second array is a vector with unknown length.\n\ndef gen_series(): i = 0 while True: size = np.random.randint(0, 10) yield i, np.random.normal(size=(size,)) i += 1\n\nfor i, series in gen_series(): print(i, "":"", str(series)) if i > 5: break\n\n0 : [-1.1977 -1.3099] 1 : [-0.3763 1.3639 0.0703 0.7888 0.3659 -0.0422 -0.5699 -1.4458] 2 : [ 0.4438 0.2206 -0.8348 3.0743 -0.2304 0.6876] 3 : [ 0.1138 0.3484 -0.3989 0.1871 -0.9462 0.7905 0.0224 0.204 -1.2715] 4 : [ 1.0292 -1.7965 -1.1569 0.437 1.9364 0.4718 -0.5036 -0.1318] 5 : [0.6893] 6 : [-0.2385 -0.3129 0.4913 0.2546 1.4849 -1.3109 -0.3785]\n\nThe first output is an int32 the second is a float32.\n\nThe first item is a scalar, shape (), and the second is a vector of unknown length, shape (None,)\n\nds_series = tf.data.Dataset.from_generator( gen_series, output_types=(tf.int32, tf.float32), output_shapes=((), (None,))) ds_series\n\n<_FlatMapDataset element_spec=(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n\nNow it can be used like a regular tf.data.Dataset. Note that when batching a dataset with a variable shape, you need to use Dataset.padded_batch.\n\nds_series_batch = ds_series.shuffle(20).padded_batch(10) ids, sequence_batch = next(iter(ds_series_batch)) print(ids.numpy()) print() print(sequence_batch.numpy())\n\n[ 4 18 19 2 8 22 13 0 25 27] [[-0.5268 0.8465 1.8949 -0.6337 -0.9212 0.2917 0.1995 -0.2283 1.5621] [-0.7196 0.3447 -0.5744 -1.6807 1.9387 -0.7832 1.1232 0.5444 0.3566] [-1.0073 0. 0. 0. 0. 0. 0. 0. 0. ] [ 1.3614 -0.0866 0.4309 -1.1438 0.066 0.3847 -0.8009 0. 0. ] [-0.7528 0. 0. 0. 0. 0. 0. 0. 0. ] [-0.006 0.9022 1.2462 0.0703 0. 0. 0. 0. 0. ] [ 0.5811 0. 0. 0. 0. 0. 0. 0. 0. ] [-0.1996 1.6923 -0.274 -0.7509 -0.6734 -1.687 -0.8438 -1.0904 0. ] [ 0.3178 0.0775 1.3367 1.0921 0.1651 0.9298 0.0764 -0.4039 0. ] [ 1.5668 -1.3154 0.8587 -0.7022 0. 0. 0. 0. 0. ]]\n\nFor a more realistic example, try wrapping preprocessing.image.ImageDataGenerator as a tf.data.Dataset.\n\nFirst download the data:\n\nflowers = tf.keras.utils.get_file( \'flower_photos\', \'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\', untar=True)\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz 228813984/228813984 [==============================] - 1s 0us/step\n\nCreate the image.ImageDataGenerator\n\nimg_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)\n\nimages, labels = next(img_gen.flow_from_directory(flowers))\n\nFound 3670 images belonging to 5 classes.\n\nprint(images.dtype, images.shape) print(labels.dtype, labels.shape)\n\nfloat32 (32, 256, 256, 3) float32 (32, 5)\n\nds = tf.data.Dataset.from_generator( lambda: img_gen.flow_from_directory(flowers), output_types=(tf.float32, tf.float32), output_shapes=([32,256,256,3], [32,5]) ) ds.element_spec\n\n(TensorSpec(shape=(32, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 5), dtype=tf.float32, name=None))\n\nfor images, labels in ds.take(1): print(\'images.shape: \', images.shape) print(\'labels.shape: \', labels.shape)\n\nFound 3670 images belonging to 5 classes. images.shape: (32, 256, 256, 3) labels.shape: (32, 5)\n\nConsuming TFRecord data\n\nRefer to the Loading TFRecords tutorial for an end-to-end example.\n\nThe tf.data API supports a variety of file formats so that you can process large datasets that do not fit in memory. For example, the TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. The tf.data.TFRecordDataset class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline.\n\nHere is an example using the test file from the French Street Name Signs (FSNS).\n\n# Creates a dataset that reads all of the examples from two files. fsns_test_file = tf.keras.utils.get_file(""fsns.tfrec"", ""https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001"")\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001 7904079/7904079 [==============================] - 0s 0us/step\n\nThe filenames argument to the TFRecordDataset initializer can either be a string, a list of strings, or a tf.Tensor of strings. Therefore if you have two sets of files for training and validation purposes, you can create a factory method that produces the dataset, taking filenames as an input argument:\n\ndataset = tf.data.TFRecordDataset(filenames = [fsns_test_file]) dataset\n\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\nMany TensorFlow projects use serialized tf.train.Example records in their TFRecord files. These need to be decoded before they can be inspected:\n\nraw_example = next(iter(dataset)) parsed = tf.train.Example.FromString(raw_example.numpy()) parsed.features.feature[\'image/text\']\n\nbytes_list { value: ""Rue Perreyon"" }\n\nRefer to the Load text tutorial for an end-to-end example.\n\nMany datasets are distributed as one or more text files. The tf.data.TextLineDataset provides an easy way to extract lines from one or more text files. Given one or more filenames, a TextLineDataset will produce one string-valued element per line of those files.\n\ndirectory_url = \'https://storage.googleapis.com/download.tensorflow.org/data/illiad/\' file_names = [\'cowper.txt\', \'derby.txt\', \'butler.txt\'] file_paths = [ tf.keras.utils.get_file(file_name, directory_url + file_name) for file_name in file_names ]\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt 815980/815980 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt 809730/809730 [==============================] - 0s 0us/step Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt 807992/807992 [==============================] - 0s 0us/step\n\ndataset = tf.data.TextLineDataset(file_paths)\n\nHere are the first few lines of the first file:\n\nfor line in dataset.take(5): print(line.numpy())\n\nb""\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus\' son;"" b\'His wrath pernicious, who ten thousand woes\' b""Caused to Achaia\'s host, sent many a soul"" b\'Illustrious into Ades premature,\' b\'And Heroes gave (so stood the will of Jove)\'\n\nTo alternate lines between files use Dataset.interleave. This makes it easier to shuffle files together. Here are the first, second and third lines from each translation:\n\nfiles_ds = tf.data.Dataset.from_tensor_slices(file_paths) lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3) for i, line in enumerate(lines_ds.take(9)): if i % 3 == 0: print() print(line.numpy())\n\nb""\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus\' son;"" b""\\xef\\xbb\\xbfOf Peleus\' son, Achilles, sing, O Muse,"" b\'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought\' b\'His wrath pernicious, who ten thousand woes\' b\'The vengeance, deep and deadly; whence to Greece\' b\'countless ills upon the Achaeans. Many a brave soul did it send\' b""Caused to Achaia\'s host, sent many a soul"" b\'Unnumbered ills arose; which many a soul\' b\'hurrying down to Hades, and many a hero did it yield a prey to dogs and\'\n\nBy default, a TextLineDataset yields every line of each file, which may not be desirable, for example, if the file starts with a header line, or contains comments. These lines can be removed using the Dataset.skip() or Dataset.filter transformations. Here, you skip the first line, then filter to find only survivors.\n\ntitanic_file = tf.keras.utils.get_file(""train.csv"", ""https://storage.googleapis.com/tf-datasets/titanic/train.csv"") titanic_lines = tf.data.TextLineDataset(titanic_file)\n\nDownloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv 30874/30874 [==============================] - 0s 0us/step\n\nfor line in titanic_lines.take(10): print(line.numpy())\n\nb\'survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\' b\'0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\' b\'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\' b\'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\' b\'1,female,35.0,1,0,53.1,First,C,Southampton,n\' b\'0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\' b\'0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\' b\'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\' b\'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\' b\'1,female,4.0,1,1,16.7,Third,G,Southampton,n\'\n\ndef survived(line): return tf.not_equal(tf.strings.substr(line, 0, 1), ""0"") survivors = titanic_lines.skip(1).filter(survived)\n\nfor line in survivors.take(10): print(line.numpy())\n\nb\'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\' b\'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\' b\'1,female,35.0,1,0,53.1,First,C,Southampton,n\' b\'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\' b\'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\' b\'1,female,4.0,1,1,16.7,Third,G,Southampton,n\' b\'1,male,28.0,0,0,13.0,Second,unknown,Southampton,y\' b\'1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y\' b\'1,male,28.0,0,0,35.5,First,A,Southampton,y\' b\'1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n\'\n\nRefer to the Loading CSV Files and Loading Pandas DataFrames tutorials for more examples.\n\nThe CSV file format is a popular format for storing tabular data in plain text.\n\ntitanic_file = tf.keras.utils.get_file(""train.csv"", ""https://storage.googleapis.com/tf-datasets/titanic/train.csv"")\n\ndf = pd.read_csv(titanic_file) df.head()\n\nIf your data fits in memory the same Dataset.from_tensor_slices method works on dictionaries, allowing this data to be easily imported:\n\ntitanic_slices = tf.data.Dataset.from_tensor_slices(dict(df)) for feature_batch in titanic_slices.take(1): for key, value in feature_batch.items(): print("" {!r:20s}: {}"".format(key, value))\n\n\'survived\' : 0 \'sex\' : b\'male\' \'age\' : 22.0 \'n_siblings_spouses\': 1 \'parch\' : 0 \'fare\' : 7.25 \'class\' : b\'Third\' \'deck\' : b\'unknown\' \'embark_town\' : b\'Southampton\' \'alone\' : b\'n\'\n\nA more scalable approach is to load from disk as necessary.\n\nThe tf.data module provides methods to extract records from one or more CSV files that comply with RFC 4180.\n\nThe tf.data.experimental.make_csv_dataset function is the high-level interface for reading sets of CSV files. It supports column type inference and many other features, like batching and shuffling, to make usage simple.\n\ntitanic_batches = tf.data.experimental.make_csv_dataset( titanic_file, batch_size=4, label_name=""survived"")\n\nfor feature_batch, label_batch in titanic_batches.take(1): print(""\'survived\': {}"".format(label_batch)) print(""features:"") for key, value in feature_batch.items(): print("" {!r:20s}: {}"".format(key, value))\n\n\'survived\': [0 1 0 0] features: \'sex\' : [b\'male\' b\'male\' b\'male\' b\'female\'] \'age\' : [28. 25. 30. 28.] \'n_siblings_spouses\': [1 1 0 3] \'parch\' : [0 0 0 1] \'fare\' : [15.5 7.775 27.75 25.4667] \'class\' : [b\'Third\' b\'Third\' b\'First\' b\'Third\'] \'deck\' : [b\'unknown\' b\'unknown\' b\'C\' b\'unknown\'] \'embark_town\' : [b\'Queenstown\' b\'Southampton\' b\'Cherbourg\' b\'Southampton\'] \'alone\' : [b\'n\' b\'n\' b\'y\' b\'n\']\n\nYou can use the select_columns argument if you only need a subset of columns.\n\ntitanic_batches = tf.data.experimental.make_csv_dataset( titanic_file, batch_size=4, label_name=""survived"", select_columns=[\'class\', \'fare\', \'survived\'])\n\nfor feature_batch, label_batch in titanic_batches.take(1): print(""\'survived\': {}"".format(label_batch)) for key, value in feature_batch.items(): print("" {!r:20s}: {}"".format(key, value))\n\n\'survived\': [1 0 1 0] \'fare\' : [11.1333 24.15 7.925 52. ] \'class\' : [b\'Third\' b\'Third\' b\'Third\' b\'First\']\n\nThere is also a lower-level experimental.CsvDataset class which provides finer grained control. It does not support column type inference. Instead you must specify the type of each column.\n\ntitanic_types = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string] dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True) for line in dataset.take(10): print([item.numpy() for item in line])\n\n[0, b\'male\', 22.0, 1, 0, 7.25, b\'Third\', b\'unknown\', b\'Southampton\', b\'n\'] [1, b\'female\', 38.0, 1, 0, 71.2833, b\'First\', b\'C\', b\'Cherbourg\', b\'n\'] [1, b\'female\', 26.0, 0, 0, 7.925, b\'Third\', b\'unknown\', b\'Southampton\', b\'y\'] [1, b\'female\', 35.0, 1, 0, 53.1, b\'First\', b\'C\', b\'Southampton\', b\'n\'] [0, b\'male\', 28.0, 0, 0, 8.4583, b\'Third\', b\'unknown\', b\'Queenstown\', b\'y\'] [0, b\'male\', 2.0, 3, 1, 21.075, b\'Third\', b\'unknown\', b\'Southampton\', b\'n\'] [1, b\'female\', 27.0, 0, 2, 11.1333, b\'Third\', b\'unknown\', b\'Southampton\', b\'n\'] [1, b\'female\', 14.0, 1, 0, 30.0708, b\'Second\', b\'unknown\', b\'Cherbourg\', b\'n\'] [1, b\'female\', 4.0, 1, 1, 16.7, b\'Third\', b\'G\', b\'Southampton\', b\'n\'] [0, b\'male\', 20.0, 0, 0, 8.05, b\'Third\', b\'unknown\', b\'Southampton\', b\'y\']\n\nIf some columns are empty, this low-level interface allows you to provide default values instead of column types.\n\n%%writefile missing.csv 1,2,3,4 ,2,3,4 1,,3,4 1,2,,4 1,2,3, ,,,\n\n# Creates a dataset that reads all of the records from two CSV files, each with # four float columns which may have missing values. record_defaults = [999,999,999,999] dataset = tf.data.experimental.CsvDataset(""missing.csv"", record_defaults) dataset = dataset.map(lambda *items: tf.stack(items)) dataset\n\n<_MapDataset element_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None)>\n\nfor line in dataset: print(line.numpy())\n\n[1 2 3 4] [999 2 3 4] [ 1 999 3 4] [ 1 2 999 4] [ 1 2 3 999] [999 999 999 999]\n\nBy default, a CsvDataset yields every column of every line of the file, which may not be desirable, for example if the file starts with a header line that should be ignored, or if some columns are not required in the input. These lines and fields can be removed with the header and select_cols arguments respectively.\n\n# Creates a dataset that reads all of the records from two CSV files with # headers, extracting float data from columns 2 and 4. record_defaults = [999, 999] # Only provide defaults for the selected columns dataset = tf.data.experimental.CsvDataset(""missing.csv"", record_defaults, select_cols=[1, 3]) dataset = dataset.map(lambda *items: tf.stack(items)) dataset\n\n<_MapDataset element_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None)>\n\nfor line in dataset: print(line.numpy())\n\n[2 4] [2 4] [999 4] [2 4] [ 2 999] [999 999]\n\nConsuming sets of files\n\nThere are many datasets distributed as a set of files, where each file is an example.\n\nflowers_root = tf.keras.utils.get_file( \'flower_photos\', \'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\', untar=True) flowers_root = pathlib.Path(flowers_root)\n\nNote: these images are licensed CC-BY, see LICENSE.txt for details.\n\nThe root directory contains a directory for each class:\n\nfor item in flowers_root.glob(""*""): print(item.name)\n\nroses sunflowers LICENSE.txt dandelion tulips daisy\n\nThe files in each class directory are examples:\n\nlist_ds = tf.data.Dataset.list_files(str(flowers_root/\'*/*\')) for f in list_ds.take(5): print(f.numpy())\n\nb\'/home/kbuilder/.keras/datasets/flower_photos/tulips/4520577328_a94c11e806_n.jpg\' b\'/home/kbuilder/.keras/datasets/flower_photos/dandelion/22679060358_561ec823ae_m.jpg\' b\'/home/kbuilder/.keras/datasets/flower_photos/daisy/6978826370_7b9aa7c7d5.jpg\' b\'/home/kbuilder/.keras/datasets/flower_photos/tulips/3447650747_8299786b80_n.jpg\' b\'/home/kbuilder/.keras/datasets/flower_photos/tulips/112951022_4892b1348b_n.jpg\'\n\nRead the data using the tf.io.read_file function and extract the label from the path, returning (image, label) pairs:\n\ndef process_path(file_path): label = tf.strings.split(file_path, os.sep)[-2] return tf.io.read_file(file_path), label labeled_ds = list_ds.map(process_path)\n\nfor image_raw, label_text in labeled_ds.take(1): print(repr(image_raw.numpy()[:100])) print() print(label_text.numpy())\n\nb""\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe1\\t\\xcbXMP\\x00://ns.adobe.com/xap/1.0/\\x00<?xpacket begin=\'\\xef\\xbb\\xbf\' id=\'W5M0MpCehiHzreSzNTczk"" b\'roses\'\n\nBatching dataset elements\n\nThe simplest form of batching stacks n consecutive elements of a dataset into a single element. The Dataset.batch() transformation does exactly this, with the same constraints as the tf.stack() operator, applied to each component of the elements: i.e., for each component i, all elements must have a tensor of the exact same shape.\n\ninc_dataset = tf.data.Dataset.range(100) dec_dataset = tf.data.Dataset.range(0, -100, -1) dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset)) batched_dataset = dataset.batch(4) for batch in batched_dataset.take(4): print([arr.numpy() for arr in batch])\n\n[array([0, 1, 2, 3]), array([ 0, -1, -2, -3])] [array([4, 5, 6, 7]), array([-4, -5, -6, -7])] [array([ 8, 9, 10, 11]), array([ -8, -9, -10, -11])] [array([12, 13, 14, 15]), array([-12, -13, -14, -15])]\n\nWhile tf.data tries to propagate shape information, the default settings of Dataset.batch result in an unknown batch size because the last batch may not be full. Note the Nones in the shape:\n\n<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n\nUse the drop_remainder argument to ignore that last batch, and get full shape propagation:\n\nbatched_dataset = dataset.batch(7, drop_remainder=True) batched_dataset\n\n<_BatchDataset element_spec=(TensorSpec(shape=(7,), dtype=tf.int64, name=None), TensorSpec(shape=(7,), dtype=tf.int64, name=None))>\n\nBatching tensors with padding\n\nThe above recipe works for tensors that all have the same size. However, many models (including sequence models) work with input data that can have varying size (for example, sequences of different lengths). To handle this case, the Dataset.padded_batch transformation enables you to batch tensors of different shapes by specifying one or more dimensions in which they may be padded.\n\ndataset = tf.data.Dataset.range(100) dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x)) dataset = dataset.padded_batch(4, padded_shapes=(None,)) for batch in dataset.take(2): print(batch.numpy()) print()\n\n[[0 0 0] [1 0 0] [2 2 0] [3 3 3]] [[4 4 4 4 0 0 0] [5 5 5 5 5 0 0] [6 6 6 6 6 6 0] [7 7 7 7 7 7 7]]\n\nThe Dataset.padded_batch transformation allows you to set different padding for each dimension of each component, and it may be variable-length (signified by None in the example above) or constant-length. It is also possible to override the padding value, which defaults to 0.\n\nProcessing multiple epochs\n\nThe tf.data API offers two main ways to process multiple epochs of the same data.\n\nThe simplest way to iterate over a dataset in multiple epochs is to use the Dataset.repeat() transformation. First, create a dataset of titanic data:\n\ntitanic_file = tf.keras.utils.get_file(""train.csv"", ""https://storage.googleapis.com/tf-datasets/titanic/train.csv"") titanic_lines = tf.data.TextLineDataset(titanic_file)\n\ndef plot_batch_sizes(ds): batch_sizes = [batch.shape[0] for batch in ds] plt.bar(range(len(batch_sizes)), batch_sizes) plt.xlabel(\'Batch number\') plt.ylabel(\'Batch size\')\n\nApplying the Dataset.repeat() transformation with no arguments will repeat the input indefinitely.\n\nThe Dataset.repeat transformation concatenates its arguments without signaling the end of one epoch and the beginning of the next epoch. Because of this a Dataset.batch applied after Dataset.repeat will yield batches that straddle epoch boundaries:\n\ntitanic_batches = titanic_lines.repeat(3).batch(128) plot_batch_sizes(titanic_batches)\n\nIf you need clear epoch separation, put Dataset.batch before the repeat:\n\ntitanic_batches = titanic_lines.batch(128).repeat(3) plot_batch_sizes(titanic_batches)\n\nIf you would like to perform a custom computation (for example, to collect statistics) at the end of each epoch then it\'s simplest to restart the dataset iteration on each epoch:\n\nepochs = 3 dataset = titanic_lines.batch(128) for epoch in range(epochs): for batch in dataset: print(batch.shape) print(""End of epoch: "", epoch)\n\n(128,) (128,) (128,) (128,) (116,) End of epoch: 0 (128,) (128,) (128,) (128,) (116,) End of epoch: 1 (128,) (128,) (128,) (128,) (116,) End of epoch: 2\n\nRandomly shuffling input data\n\nThe Dataset.shuffle() transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer.\n\nNote: While large buffer_sizes shuffle more thoroughly, they can take a lot of memory, and significant time to fill. Consider using Dataset.interleave across files if this becomes a problem.\n\nAdd an index to the dataset so you can see the effect:\n\nlines = tf.data.TextLineDataset(titanic_file) counter = tf.data.experimental.Counter() dataset = tf.data.Dataset.zip((counter, lines)) dataset = dataset.shuffle(buffer_size=100) dataset = dataset.batch(20) dataset\n\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_14491/4092668703.py:2: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version. Instructions for updating: Use `tf.data.Dataset.counter(...)` instead. <_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n\nSince the buffer_size is 100, and the batch size is 20, the first batch contains no elements with an index over 120.\n\nn,line_batch = next(iter(dataset)) print(n.numpy())\n\n[ 43 13 17 72 33 89 83 105 96 81 0 67 97 84 73 32 30 71 64 103]\n\nAs with Dataset.batch the order relative to Dataset.repeat matters.\n\nDataset.shuffle doesn\'t signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next:\n\ndataset = tf.data.Dataset.zip((counter, lines)) shuffled = dataset.shuffle(buffer_size=100).batch(10).repeat(2) print(""Here are the item ID\'s near the epoch boundary:\\n"") for n, line_batch in shuffled.skip(60).take(5): print(n.numpy())\n\nHere are the item ID\'s near the epoch boundary: [625 583 586 606 504 597 453 615 575 429] [424 456 452 605 483 566 395 556 492 365] [570 573 611 540 545 559 388 579] [ 0 18 92 79 81 86 62 103 29 82] [47 69 17 95 9 11 77 84 31 53]\n\nshuffle_repeat = [n.numpy().mean() for n, line_batch in shuffled] plt.plot(shuffle_repeat, label=""shuffle().repeat()"") plt.ylabel(""Mean item ID"") plt.legend()\n\n<matplotlib.legend.Legend at 0x7f32b0355dc0>\n\nBut a repeat before a shuffle mixes the epoch boundaries together:\n\ndataset = tf.data.Dataset.zip((counter, lines)) shuffled = dataset.repeat(2).shuffle(buffer_size=100).batch(10) print(""Here are the item ID\'s near the epoch boundary:\\n"") for n, line_batch in shuffled.skip(55).take(15): print(n.numpy())\n\nHere are the item ID\'s near the epoch boundary: [464 377 595 328 614 504 7 20 433 623] [419 550 615 509 499 540 557 622 570 544] [588 618 616 4 460 38 16 617 31 591] [440 610 585 600 36 17 35 52 592 19] [523 59 545 624 607 51 53 26 33 318] [510 37 6 448 612 469 32 10 39 594] [ 41 63 13 627 67 76 386 579 412 55] [ 1 54 626 71 64 22 47 553 525 65] [ 69 3 15 102 14 455 23 98 74 78] [596 12 50 5 18 112 114 97 61 42] [103 84 583 90 350 575 606 85 107 108] [115 127 60 602 118 43 34 58 46 587] [119 56 620 75 564 625 88 140 539 45] [589 100 149 452 110 11 66 132 142 111] [101 334 94 497 520 158 120 86 135 95]\n\nrepeat_shuffle = [n.numpy().mean() for n, line_batch in shuffled] plt.plot(shuffle_repeat, label=""shuffle().repeat()"") plt.plot(repeat_shuffle, label=""repeat().shuffle()"") plt.ylabel(""Mean item ID"") plt.legend()\n\n<matplotlib.legend.Legend at 0x7f32b033bdf0>\n\nThe Dataset.map(f) transformation produces a new dataset by applying a given function f to each element of the input dataset. It is based on the map() function that is commonly applied to lists (and other structures) in functional programming languages. The function f takes the tf.Tensor objects that represent a single element in the input, and returns the tf.Tensor objects that will represent a single element in the new dataset. Its implementation uses standard TensorFlow operations to transform one element into another.\n\nThis section covers common examples of how to use Dataset.map().\n\nDecoding image data and resizing it\n\nWhen training a neural network on real-world image data, it is often necessary to convert images of different sizes to a common size, so that they may be batched into a fixed size.\n\nRebuild the flower filenames dataset:\n\nlist_ds = tf.data.Dataset.list_files(str(flowers_root/\'*/*\'))\n\nWrite a function that manipulates the dataset elements.\n\n# Reads an image from a file, decodes it into a dense tensor, and resizes it # to a fixed shape. def parse_image(filename): parts = tf.strings.split(filename, os.sep) label = parts[-2] image = tf.io.read_file(filename) image = tf.io.decode_jpeg(image) image = tf.image.convert_image_dtype(image, tf.float32) image = tf.image.resize(image, [128, 128]) return image, label\n\nfile_path = next(iter(list_ds)) image, label = parse_image(file_path) def show(image, label): plt.figure() plt.imshow(image) plt.title(label.numpy().decode(\'utf-8\')) plt.axis(\'off\') show(image, label)\n\nMap it over the dataset.\n\nimages_ds = list_ds.map(parse_image) for image, label in images_ds.take(2): show(image, label)\n\nApplying arbitrary Python logic\n\nFor performance reasons, use TensorFlow operations for preprocessing your data whenever possible. However, it is sometimes useful to call external Python libraries when parsing your input data. You can use the tf.py_function operation in a Dataset.map transformation.\n\nFor example, if you want to apply a random rotation, the tf.image module only has tf.image.rot90, which is not very useful for image augmentation.\n\nNote: tensorflow_addons has a TensorFlow compatible rotate in tensorflow_addons.image.rotate.\n\nTo demonstrate tf.py_function, try using the scipy.ndimage.rotate function instead:\n\nimport scipy.ndimage as ndimage @tf.py_function(Tout=tf.float32) def random_rotate_image(image): image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False) return image\n\nimage, label = next(iter(images_ds)) image = random_rotate_image(image) show(image, label)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\nTo use this function with Dataset.map the same caveats apply as with Dataset.from_generator, you need to describe the return shapes and types when you apply the function:\n\ndef tf_random_rotate_image(image, label): im_shape = image.shape image = random_rotate_image(image) image.set_shape(im_shape) return image, label\n\nrot_ds = images_ds.map(tf_random_rotate_image) for image, label in rot_ds.take(2): show(image, label)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\nParsing tf.Example protocol buffer messages\n\nMany input pipelines extract tf.train.Example protocol buffer messages from a TFRecord format. Each tf.train.Example record contains one or more ""features"", and the input pipeline typically converts these features into tensors.\n\nfsns_test_file = tf.keras.utils.get_file(""fsns.tfrec"", ""https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001"") dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file]) dataset\n\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\nYou can work with tf.train.Example protos outside of a tf.data.Dataset to understand the data:\n\nraw_example = next(iter(dataset)) parsed = tf.train.Example.FromString(raw_example.numpy()) feature = parsed.features.feature raw_img = feature[\'image/encoded\'].bytes_list.value[0] img = tf.image.decode_png(raw_img) plt.imshow(img) plt.axis(\'off\') _ = plt.title(feature[""image/text""].bytes_list.value[0])\n\nraw_example = next(iter(dataset))\n\ndef tf_parse(eg): example = tf.io.parse_example( eg[tf.newaxis], { \'image/encoded\': tf.io.FixedLenFeature(shape=(), dtype=tf.string), \'image/text\': tf.io.FixedLenFeature(shape=(), dtype=tf.string) }) return example[\'image/encoded\'][0], example[\'image/text\'][0]\n\nimg, txt = tf_parse(raw_example) print(txt.numpy()) print(repr(img.numpy()[:20]), ""..."")\n\nb\'Rue Perreyon\' b\'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02X\' ...\n\ndecoded = dataset.map(tf_parse) decoded\n\n<_MapDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>\n\nimage_batch, text_batch = next(iter(decoded.batch(10))) image_batch.shape\n\nTime series windowing\n\nFor an end-to-end time series example see: Time series forecasting.\n\nTime series data is often organized with the time axis intact.\n\nUse a simple Dataset.range to demonstrate:\n\nrange_ds = tf.data.Dataset.range(100000)\n\nTypically, models based on this sort of data will want a contiguous time slice.\n\nThe simplest approach would be to batch the data:\n\nbatches = range_ds.batch(10, drop_remainder=True) for batch in batches.take(5): print(batch.numpy())\n\n[0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28 29] [30 31 32 33 34 35 36 37 38 39] [40 41 42 43 44 45 46 47 48 49]\n\nOr to make dense predictions one step into the future, you might shift the features and labels by one step relative to each other:\n\ndef dense_1_step(batch): # Shift features and labels one step relative to each other. return batch[:-1], batch[1:] predict_dense_1_step = batches.map(dense_1_step) for features, label in predict_dense_1_step.take(3): print(features.numpy(), "" => "", label.numpy())\n\n[0 1 2 3 4 5 6 7 8] => [1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18] => [11 12 13 14 15 16 17 18 19] [20 21 22 23 24 25 26 27 28] => [21 22 23 24 25 26 27 28 29]\n\nTo predict a whole window instead of a fixed offset you can split the batches into two parts:\n\nbatches = range_ds.batch(15, drop_remainder=True) def label_next_5_steps(batch): return (batch[:-5], # Inputs: All except the last 5 steps batch[-5:]) # Labels: The last 5 steps predict_5_steps = batches.map(label_next_5_steps) for features, label in predict_5_steps.take(3): print(features.numpy(), "" => "", label.numpy())\n\n[0 1 2 3 4 5 6 7 8 9] => [10 11 12 13 14] [15 16 17 18 19 20 21 22 23 24] => [25 26 27 28 29] [30 31 32 33 34 35 36 37 38 39] => [40 41 42 43 44]\n\nTo allow some overlap between the features of one batch and the labels of another, use Dataset.zip:\n\nfeature_length = 10 label_length = 3 features = range_ds.batch(feature_length, drop_remainder=True) labels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:label_length]) predicted_steps = tf.data.Dataset.zip((features, labels)) for features, label in predicted_steps.take(5): print(features.numpy(), "" => "", label.numpy())\n\n[0 1 2 3 4 5 6 7 8 9] => [10 11 12] [10 11 12 13 14 15 16 17 18 19] => [20 21 22] [20 21 22 23 24 25 26 27 28 29] => [30 31 32] [30 31 32 33 34 35 36 37 38 39] => [40 41 42] [40 41 42 43 44 45 46 47 48 49] => [50 51 52]\n\nWhile using Dataset.batch works, there are situations where you may need finer control. The Dataset.window method gives you complete control, but requires some care: it returns a Dataset of Datasets. Go to the Dataset structure section for details.\n\nwindow_size = 5 windows = range_ds.window(window_size, shift=1) for sub_ds in windows.take(5): print(sub_ds)\n\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)> <_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\nThe Dataset.flat_map method can take a dataset of datasets and flatten it into a single dataset:\n\nfor x in windows.flat_map(lambda x: x).take(30): print(x.numpy(), end=\' \')\n\n0 1 2 3 4 1 2 3 4 5 2 3 4 5 6 3 4 5 6 7 4 5 6 7 8 5 6 7 8 9\n\nIn nearly all cases, you will want to Dataset.batch the dataset first:\n\ndef sub_to_batch(sub): return sub.batch(window_size, drop_remainder=True) for example in windows.flat_map(sub_to_batch).take(5): print(example.numpy())\n\n[0 1 2 3 4] [1 2 3 4 5] [2 3 4 5 6] [3 4 5 6 7] [4 5 6 7 8]\n\nNow, you can see that the shift argument controls how much each window moves over.\n\nPutting this together you might write this function:\n\ndef make_window_dataset(ds, window_size=5, shift=1, stride=1): windows = ds.window(window_size, shift=shift, stride=stride) def sub_to_batch(sub): return sub.batch(window_size, drop_remainder=True) windows = windows.flat_map(sub_to_batch) return windows\n\nds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3) for example in ds.take(10): print(example.numpy())\n\n[ 0 3 6 9 12 15 18 21 24 27] [ 5 8 11 14 17 20 23 26 29 32] [10 13 16 19 22 25 28 31 34 37] [15 18 21 24 27 30 33 36 39 42] [20 23 26 29 32 35 38 41 44 47] [25 28 31 34 37 40 43 46 49 52] [30 33 36 39 42 45 48 51 54 57] [35 38 41 44 47 50 53 56 59 62] [40 43 46 49 52 55 58 61 64 67] [45 48 51 54 57 60 63 66 69 72]\n\nThen it\'s easy to extract labels, as before:\n\ndense_labels_ds = ds.map(dense_1_step) for inputs,labels in dense_labels_ds.take(3): print(inputs.numpy(), ""=>"", labels.numpy())\n\n[ 0 3 6 9 12 15 18 21 24] => [ 3 6 9 12 15 18 21 24 27] [ 5 8 11 14 17 20 23 26 29] => [ 8 11 14 17 20 23 26 29 32] [10 13 16 19 22 25 28 31 34] => [13 16 19 22 25 28 31 34 37]\n\nWhen working with a dataset that is very class-imbalanced, you may want to resample the dataset. tf.data provides two methods to do this. The credit card fraud dataset is a good example of this sort of problem.\n\nNote: Go to Classification on imbalanced data for a full tutorial.\n\nzip_path = tf.keras.utils.get_file( origin=\'https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip\', fname=\'creditcard.zip\', extract=True) csv_path = zip_path.replace(\'.zip\', \'.csv\')\n\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip 69155632/69155632 [==============================] - 0s 0us/step\n\ncreditcard_ds = tf.data.experimental.make_csv_dataset( csv_path, batch_size=1024, label_name=""Class"", # Set the column types: 30 floats and an int. column_defaults=[float()]*30+[int()])\n\nNow, check the distribution of classes, it is highly skewed:\n\ndef count(counts, batch): features, labels = batch class_1 = labels == 1 class_1 = tf.cast(class_1, tf.int32) class_0 = labels == 0 class_0 = tf.cast(class_0, tf.int32) counts[\'class_0\'] += tf.reduce_sum(class_0) counts[\'class_1\'] += tf.reduce_sum(class_1) return counts\n\ncounts = creditcard_ds.take(10).reduce( initial_state={\'class_0\': 0, \'class_1\': 0}, reduce_func = count) counts = np.array([counts[\'class_0\'].numpy(), counts[\'class_1\'].numpy()]).astype(np.float32) fractions = counts/counts.sum() print(fractions)\n\nA common approach to training with an imbalanced dataset is to balance it. tf.data includes a few methods which enable this workflow:\n\nOne approach to resampling a dataset is to use sample_from_datasets. This is more applicable when you have a separate tf.data.Dataset for each class.\n\nHere, just use filter to generate them from the credit card fraud data:\n\nnegative_ds = ( creditcard_ds .unbatch() .filter(lambda features, label: label==0) .repeat()) positive_ds = ( creditcard_ds .unbatch() .filter(lambda features, label: label==1) .repeat())\n\nfor features, label in positive_ds.batch(10).take(1): print(label.numpy())\n\n[1 1 1 1 1 1 1 1 1 1]\n\nTo use tf.data.Dataset.sample_from_datasets pass the datasets, and the weight for each:\n\nbalanced_ds = tf.data.Dataset.sample_from_datasets( [negative_ds, positive_ds], [0.5, 0.5]).batch(10)\n\nNow the dataset produces examples of each class with a 50/50 probability:\n\nfor features, labels in balanced_ds.take(10): print(labels.numpy())\n\n[0 0 0 1 0 0 1 0 0 0] [1 1 0 0 1 1 1 0 0 1] [1 0 0 0 1 1 0 0 0 0] [1 0 0 0 0 1 0 1 0 0] [1 1 1 1 0 1 0 0 1 0] [0 1 1 0 0 0 1 1 0 1] [0 1 1 0 1 1 0 1 1 1] [0 0 0 1 0 0 1 1 0 0] [0 1 0 0 1 1 1 0 1 0] [1 1 1 1 1 1 0 0 0 0]\n\nRejection resampling\n\nOne problem with the above Dataset.sample_from_datasets approach is that it needs a separate tf.data.Dataset per class. You could use Dataset.filter to create those two datasets, but that results in all the data being loaded twice.\n\nThe tf.data.Dataset.rejection_resample method can be applied to a dataset to rebalance it, while only loading it once. Elements will be dropped or repeated to achieve balance.\n\nThe rejection_resample method takes a class_func argument. This class_func is applied to each dataset element, and is used to determine which class an example belongs to for the purposes of balancing.\n\nThe goal here is to balance the label distribution, and the elements of creditcard_ds are already (features, label) pairs. So the class_func just needs to return those labels:\n\ndef class_func(features, label): return label\n\nThe resampling method deals with individual examples, so in this case you must unbatch the dataset before applying that method.\n\nThe method needs a target distribution, and optionally an initial distribution estimate as inputs.\n\nresample_ds = ( creditcard_ds .unbatch() .rejection_resample(class_func, target_dist=[0.5,0.5], initial_dist=fractions) .batch(10))\n\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4963: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20. Instructions for updating: Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n\nThe rejection_resample method returns (class, example) pairs where the class is the output of the class_func. In this case, the example was already a (feature, label) pair, so use map to drop the extra copy of the labels:\n\nbalanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)\n\nNow the dataset produces examples of each class with a 50/50 probability:\n\nfor features, labels in balanced_ds.take(10): print(labels.numpy())\n\nProportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] Proportion of examples rejected by sampler is high: [0.995312512][0.995312512 0.0046875][0 1] [0 1 0 1 0 0 1 1 0 1] [1 1 0 1 0 0 1 1 0 0] [0 0 1 0 1 0 0 0 0 1] [1 0 0 1 0 0 1 0 1 0] [0 1 1 0 0 1 0 1 1 0] [1 1 0 1 1 0 1 1 0 1] [1 1 1 0 0 0 0 0 0 1] [0 1 0 0 0 0 0 1 1 1] [0 1 0 1 1 0 0 1 0 1] [0 0 1 0 1 1 1 0 0 1]\n\nIterator Checkpointing\n\nTensorflow supports taking checkpoints so that when your training process restarts it can restore the latest checkpoint to recover most of its progress. In addition to checkpointing the model variables, you can also checkpoint the progress of the dataset iterator. This could be useful if you have a large dataset and don\'t want to start the dataset from the beginning on each restart. Note however that iterator checkpoints may be large, since transformations such as Dataset.shuffle and Dataset.prefetch require buffering elements within the iterator.\n\nTo include your iterator in a checkpoint, pass the iterator to the tf.train.Checkpoint constructor.\n\nrange_ds = tf.data.Dataset.range(20) iterator = iter(range_ds) ckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator) manager = tf.train.CheckpointManager(ckpt, \'/tmp/my_ckpt\', max_to_keep=3) print([next(iterator).numpy() for _ in range(5)]) save_path = manager.save() print([next(iterator).numpy() for _ in range(5)]) ckpt.restore(manager.latest_checkpoint) print([next(iterator).numpy() for _ in range(5)])\n\n[0, 1, 2, 3, 4] [5, 6, 7, 8, 9] [5, 6, 7, 8, 9]\n\nNote: It is not possible to checkpoint an iterator which relies on an external state, such as a tf.py_function. Attempting to do so will raise an exception complaining about the external state.\n\nUsing tf.data with tf.keras\n\nThe tf.keras API simplifies many aspects of creating and executing machine learning models. Its Model.fit and Model.evaluate and Model.predict APIs support datasets as inputs. Here is a quick dataset and model setup:\n\ntrain, test = tf.keras.datasets.fashion_mnist.load_data() images, labels = train images = images/255.0 labels = labels.astype(np.int32)\n\nfmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels)) fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32) model = tf.keras.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(10) ]) model.compile(optimizer=\'adam\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\'accuracy\'])\n\nPassing a dataset of (feature, label) pairs is all that\'s needed for Model.fit and Model.evaluate:\n\nmodel.fit(fmnist_train_ds, epochs=2)\n\nEpoch 1/2 26/1875 [..............................] - ETA: 3s - loss: 1.7645 - accuracy: 0.3930 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1705458306.961075 14743 device_compiler.h:186] Compiled cluster using XLA! This line is logged at most once for the lifetime of the process. 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5948 - accuracy: 0.7994 Epoch 2/2 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4601 - accuracy: 0.8427 <keras.src.callbacks.History at 0x7f32b0722250>\n\nIf you pass an infinite dataset, for example by calling Dataset.repeat, you just need to also pass the steps_per_epoch argument:\n\nmodel.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)\n\nEpoch 1/2 20/20 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.8344 Epoch 2/2 20/20 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.8438 <keras.src.callbacks.History at 0x7f32b0457100>\n\nFor evaluation you can pass the number of evaluation steps:\n\nloss, accuracy = model.evaluate(fmnist_train_ds) print(""Loss :"", loss) print(""Accuracy :"", accuracy)\n\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.4396 - accuracy: 0.8490 Loss : 0.4396059811115265 Accuracy : 0.8489833474159241\n\nFor long datasets, set the number of steps to evaluate:\n\nloss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10) print(""Loss :"", loss) print(""Accuracy :"", accuracy)\n\n10/10 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8844 Loss : 0.4079427719116211 Accuracy : 0.8843749761581421\n\nThe labels are not required when calling Model.predict.\n\npredict_ds = tf.data.Dataset.from_tensor_slices(images).batch(32) result = model.predict(predict_ds, steps = 10) print(result.shape)\n\n10/10 [==============================] - 0s 1ms/step (320, 10)\n\nBut the labels are ignored if you do pass a dataset containing them:\n\nresult = model.predict(fmnist_train_ds, steps = 10) print(result.shape)\n\n10/10 [==============================] - 0s 1ms/step (320, 10)\n\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2024-01-17 UTC.\n\n[{ ""type"": ""thumb-down"", ""id"": ""missingTheInformationINeed"", ""label"":""Missing the information I need"" },{ ""type"": ""thumb-down"", ""id"": ""tooComplicatedTooManySteps"", ""label"":""Too complicated / too many steps"" },{ ""type"": ""thumb-down"", ""id"": ""outOfDate"", ""label"":""Out of date"" },{ ""type"": ""thumb-down"", ""id"": ""samplesCodeIssue"", ""label"":""Samples / code issue"" },{ ""type"": ""thumb-down"", ""id"": ""otherDown"", ""label"":""Other"" }]\n\n[{ ""type"": ""thumb-up"", ""id"": ""easyToUnderstand"", ""label"":""Easy to understand"" },{ ""type"": ""thumb-up"", ""id"": ""solvedMyProblem"", ""label"":""Solved my problem"" },{ ""type"": ""thumb-up"", ""id"": ""otherUp"", ""label"":""Other"" }]', 'timestamp': '2024-07-09T13:53:33', 'title': 'tf.data: Build TensorFlow input pipelines | TensorFlow Core', 'url': 'https://www.tensorflow.org/guide/data'})]]??"
