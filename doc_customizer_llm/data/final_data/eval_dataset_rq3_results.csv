question,contexts,answer,ground_truth,answer_correctness,answer_similarity,faithfulness
"<p>When <code>src</code> has shape <code>[?]</code>, <code>tf.gather(src, tf.where(src != 0))</code> returns a tensor with shape <code>[?, 0]</code>. I'm not sure how a dimension can have size 0, and I'm especially unsure how to change the tensor back. I didn't find anything in the documentation to explain this, either.</p>

<p>I tried to <code>tf.transpose(tensor)[0]</code>, but the first dimension of the transposed tensor has size 0 and cannot be accessed! What's wrong?</p>
","['"""""" In Tensorflow, how to use tf.gather() for the last dimension? n\\nI am trying to gather slices of a tensor in terms of the last dimension for partial connection between layers. Because the output tensor\'s shape is [batch_size, h, w, depth], I want to select slices based on the last dimension, such as\\n\\n# L is intermediate tensor partL = L[:, :, :, [0,2,3,8]]\\n\\nHowever, tf.gather(L, [0, 2,3,8]) seems to only work for the first dimension (right?) Can anyone tell me how to do it? As of TensorFlow 1.3 tf.gather has an axis parameter, so the various workarounds here are no longer necessary. \\n\\nTranspose your matrix so that dimension to gather is first (transpose is expensive)\\n\\nreshape your tensor into 1d (reshape is cheap) and turn your gather column indices into a list of individual element indices at linear indexing, then reshape back\\n\\n use gather_nd. Will still need to turn your column indices into list of individual element indices. \\n\\nNote that tf.gather has an axis parameter as of TensorFlow 1.3. With gather_nd you can now do this as follows:\\n\\ncat_idx = tf.concat([tf.range(0, tf.shape(x)[0]), indices_for_dim1], axis=0) result = tf.gather_nd(matrix, cat_idx)\\n\\nAlso, as reported by user Nova:\\n\\nx = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) idx = tf.constant([1, 0, 2]) idx_flattened = tf.range(0, x.shape[0]) * x.shape[1] + idx y = tf.gather(tf.reshape(x, [-1]), # flatten input idx_flattened) # use flattened indices with tf.Session(\\\'\\\'): print y.eval() # [2 4 9]\\n\\nThe gist is flatten the tensor and use strided 1D addressing with tf.gather(...). \\n\\nI\\\'m not sure your first example works. Let\\\'s say tf.shape(x)[0] is 1, then cat_idx will be [0, 0, 2, 3, 8], which is not what you want to use with tf.gather_nd. In fact, in this case it would throw an error because the length of the innermost dimension of indices (2nd argument to gather_nd) cannot be bigger than the rank of params (1st argument to gather_nd). I\'ve posted a corrected version (using tf.stack) below. Yet another solution using tf.unstack(...), tf.gather(...) and tf.stack(..)\\n\\nimport tensorflow as tf import numpy as np shape = [2, 2, 2, 10] L = np.arange(np.prod(shape)) L = np.reshape(L, shape) indices = [0, 2, 3, 8] axis = -1 # last dimension def gather_axis(params, indices, axis=0): return tf.stack(tf.unstack(tf.gather(tf.unstack(params, axis=axis), indices)), axis=axis) print(L) with tf.Session() as sess: partL = sess.run(gather_axis(L, indices, axis)) print(partL)\\n\\nL = [[[[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19]] [[20 21 22 23 24 25 26 27 28 29] [30 31 32 33 34 35 36 37 38 39]]] [[[40 41 42 43 44 45 46 47 48 49] [50 51 52 53 54 55 56 57 58 59]] [[60 61 62 63 64 65 66 67 68 69] [70 71 72 73 74 75 76 77 78 79]]]] partL = [[[[ 0 2 3 8] [10 12 13 18]] [[20 22 23 28] [30 32 33 38]]] [[[40 42 43 48] [50 52 53 58]] [[60 62 63 68] [70 72 73 78]]]]\\n\\nA correct version of answer would read\\n\\ncat_idx = tf.stack([tf.range(0, tf.shape(x)[0]), indices_for_dim1], axis=1) result = tf.gather_nd(matrix, cat_idx)\\n\\n You can try this way, for instance(in most cases in NLP at the least),\\n\\nThe parameter is of shape [batch_size, depth] and the indices are [i, j, k, n, m] of which the length is batch_size. Then gather_nd can be helpful.""""""']",Success,"<pre><code>src = tf.constant([0, 1, 1, 0], dtype=tf.int8)
tf.gather(src, tf.where(tf.not_equal(src, 0))).eval(session=tf.Session())

array([[1],
       [1]], dtype=int8)
</code></pre>",0.7272726617,0.6590906467,0
"<p>I have a generator yielding data and labels <code>yield data, labels</code> where the data is
an <code>numpy.ndarray</code> with variable rows and 500 columns of type <code>dtype=float32</code> and the labels are integers of <code>numpy.int64</code>.</p>
<p>I'm trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: <code>tf.data.Dataset.from_generator</code></p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">docs</a> say that the from_generator function needs a parameter <code>output_signature</code> as an input. But I'm having trouble understanding how to build this output_signature.</p>
<p>How can I make the output_signature for the generator I described?</p>
<p>Thank you!</p>
<p>Edit:
I used <code>tf.type_spec_from_value</code> to get this:</p>
<pre><code>dataset = tf.data.Dataset.from_generator(
   datagen_row,
   output_signature=(
      tf.TensorSpec(shape=(None, 512), dtype=tf.float32, name=None),
      tf.TensorSpec(shape=(), dtype=tf.int64, name=None)
   )
)
</code></pre>
<p>But is it correct to use None when the number of rows is varying for the first data type?</p>
","['""""""How to create output_signature for tensorflow.dataset.from_generator\\n\\nI have a generator yielding data and labels yield data, labels where the data is an numpy.ndarray with variable rows and 500 columns of type dtype=float32 and the labels are integers of numpy.int64. I\'m trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: tf.data.Dataset.from_generator\\n\\nThe docs say that the from_generator function needs a parameter output_signature as an input. But I\'m having trouble understanding how to build this output_signature. How can I make the output_signature for the generator I described? Edit: I used tf.type_spec_from_value to get this:\\n\\ndataset = tf.data.Dataset.from_generator( datagen_row, output_signature=( tf.TensorSpec(shape=(None, 512), dtype=tf.float32, name=None), tf.TensorSpec(shape=(), dtype=tf.int64, name=None) ) )\\n\\nBut is it correct to use None when the number of rows is varying for the first data type? 1\\n\\nIf possible, add some dummy data with generator. if your datagen_row() function yields input_data, label with format 500 and 1 than your output_signature should be:\\n\\noutput_signature=( tf.TensorSpec(shape=(None, 500), dtype=tf.float32, name=None), tf.TensorSpec(shape=(), dtype=tf.int64, name=None))\\n\\nwhere the first TensorSpec is for the data format and the second one for the label format. But it would be helpful if you post the function + maybe data examples or data shape here.""""""']",Success,"<pre><code>  output_signature=(
  tf.TensorSpec(shape=(None, 500), dtype=tf.float32, name=None),
  tf.TensorSpec(shape=(), dtype=tf.int64, name=None))
</code></pre>",0.5464800316,0.6859201264,1
"<p>I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup_sparse</code></a> I found it only supports 2D sparse tensors,</p>

<blockquote>
  <p>sp_ids: N x M SparseTensor of int64 ids where N is typically batch size and M is arbitrary.</p>
</blockquote>

<p>My example code here</p>

<pre><code>import numpy as np
import tensorflow as tf
tf.enable_eager_execution()

# [feature number, embedding dim] 
w = tf.get_variable(""w"", [4, 4], initializer=tf.random_normal_initializer())

z = np.array(
     [
      [
        [0, 1, 2, 3],   # get the vector of row 0, 1, 2, 3 of the embedding matrix w and get the sum
        [2, 3]
      ],

      [
        [1, 3],
        [2]
      ],

      [
        [0, 1, 3],
        [1, 2]
      ]
     ])

sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],
                     indices=[[0,0,0],[0,0,1],[0,0,2],[0,0,3],[0,1,2],
                              [0,1,3],[1,0,1],[1,0,3],[1,1,2],[2,0,0],
                              [2,0,1],[2,0,3],[2,1,1],[2,1,2]],
                     dense_shape=[3, 2, 4])

tf.nn.embedding_lookup_sparse(w, sp, None, combiner='sum')
# the outputs
&lt;tf.Tensor: id=970, shape=(3, 4), dtype=float32, numpy=
array([[-5.8729677 , -1.3900641 ,  0.8126096 , -3.1223912 ],
       [-1.0788026 , -1.1324122 ,  0.34160078,  0.23714277],
       [-2.497394  , -2.7855003 ,  3.0201516 , -1.8009453 ]],
      dtype=float32)&gt;

print(w)
&lt;tf.Variable 'w:0' shape=(4, 4) dtype=float32, numpy=
array([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],
       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],
      dtype=float32)&gt;
</code></pre>

<p>But the expected output is a matrix with a dimension of <code>3x2x4</code>, not <code>3x4</code>. Does <code>tf.nn.embedding_lookup_sparse</code> support this operation?</p>
","['""""""tf.nn.embedding_lookup_sparse 3D sparse tensor input\\n\\n I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of tf.nn.embedding_lookup_sparse I found it only supports 2D sparse tensors,\\n\\nsp_ids: N x M SparseTensor of int64 ids where N is typically batch size and M is arbitrary. My example code here\\n\\nimport numpy as np import tensorflow as tf tf.enable_eager_execution() # [feature number, embedding dim] w = tf.get_variable(""w"", [4, 4], initializer=tf.random_normal_initializer()) z = np.array( [ [ [0, 1, 2, 3], # get the vector of row 0, 1, 2, 3 of the embedding matrix w and get the sum [2, 3] ], [ [1, 3], [2] ], [ [0, 1, 3], [1, 2] ] ]) sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2], indices=[[0,0,0],[0,0,1],[0,0,2],[0,0,3],[0,1,2], [0,1,3],[1,0,1],[1,0,3],[1,1,2],[2,0,0], [2,0,1],[2,0,3],[2,1,1],[2,1,2]], dense_shape=[3, 2, 4]) tf.nn.embedding_lookup_sparse(w, sp, None, combiner=\\\'sum\\\') # the outputs <tf.Tensor: id=970, shape=(3, 4), dtype=float32, numpy= array([[-5.8729677 , -1.3900641 , 0.8126096 , -3.1223912 ], [-1.0788026 , -1.1324122 , 0.34160078, 0.23714277], [-2.497394 , -2.7855003 , 3.0201516 , -1.8009453 ]], dtype=float32)> print(w) <tf.Variable \\\'w:0\\\' shape=(4, 4) dtype=float32, numpy= array([[-2.5669768 , -0.38916406, 1.4039794 , -2.8173826 ], [ 1.1483854 , -1.2639242 , 1.2745714 , 0.7792944 ], [-1.3400027 , -0.46362385, -1.3652185 , 0.27220532], [-0.8871854 , 0.5951359 , 0.43224794, -0.8143569 ]], dtype=float32)>\\n\\nBut the expected output is a matrix with a dimension of 3x2x4, not 3x4. Does tf.nn.embedding_lookup_sparse support this operation? \\n\\nThe most easier way to do so is to make the sparse tensor as a 2D tensor and get the weights of the embedding matrix and then reshape. # First make the z as a 2D arr and create a sparse tensor z = np.array([ [0, 1, 2, 3], # get the row 0,1,2,3 of the embedding matrix w and get the sum [2, 3], [1, 3], [2], [0, 1, 3], [1, 2] ]) sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2], indices=[[0,0],[0,1],[0,2],[0,3],[1,2],[1,3],[2,1], [2,3],[3,2],[4,0],[4,1],[4,3],[5,1],[5,2]], dense_shape=[6, 4]) res = tf.nn.embedding_lookup_sparse(w, sp, None, combiner=\\\'sum\\\') res.numpy() # the output array([[-3.6457794 , -1.5215762 , 1.7455802 , -2.5802398 ], [-2.227188 , 0.13151208, -0.9329706 , -0.5421516 ], [ 0.2612 , -0.6687883 , 1.7068193 , -0.03506255], [-1.3400027 , -0.46362385, -1.3652185 , 0.27220532], [-2.3057768 , -1.0579524 , 3.1107986 , -2.8524451 ], [-0.19161725, -1.7275481 , -0.0906471 , 1.0514997 ]], # reshape tf.reshape(res, [-1, 2, 4]) # that is exacly what I want. array([[[-3.6457794 , -1.5215762 , 1.7455802 , -2.5802398 ], [-2.227188 , 0.13151208, -0.9329706 , -0.5421516 ]], [[ 0.2612 , -0.6687883 , 1.7068193 , -0.03506255], [-1.3400027 , -0.46362385, -1.3652185 , 0.27220532]], [[-2.3057768 , -1.0579524 , 3.1107986 , -2.8524451 ], [-0.19161725, -1.7275481 , -0.0906471 , 1.0514997 ]]]) # print w, and the above result is right w.numpy() array([[-2.5669768 , -0.38916406, 1.4039794 , -2.8173826 ], [ 1.1483854 , -1.2639242 , 1.2745714 , 0.7792944 ], [-1.3400027 , -0.46362385, -1.3652185 , 0.27220532], [-0.8871854 , 0.5951359 , 0.43224794, -0.8143569 ]], dtype=float32)\\n\\nSo, forget the 3D sparse tensor, simply convert it to 2D tensor. Because you only care about the values (index of rows, which are used to get the corresponding rows of the embedding matrix) in the sparse tensor.\n\n After diving into safe_embedding_lookup_sparse\\\'s unit test, I\'m more confused why I got this result if giving the sparse weights, especially why we got something like embedding_weights[0][3] where 3 is not appeared in the code above.  I don\'t figure it out yet after reading safe_embedding_lookup_sparse\'s unit test. I have updated the question and would you like to explain the code for us?""""""']",Failed,"<pre class=""lang-py prettyprint-override""><code># First make the z as a 2D arr and create a sparse tensor 
z = np.array([
        [0, 1, 2, 3],  # get the row 0,1,2,3 of the embedding matrix w and get the sum
        [2, 3],
        [1, 3],
        [2],
        [0, 1, 3],
        [1, 2]
      ])

sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],
                     indices=[[0,0],[0,1],[0,2],[0,3],[1,2],[1,3],[2,1],
                              [2,3],[3,2],[4,0],[4,1],[4,3],[5,1],[5,2]],
                     dense_shape=[6, 4])

res = tf.nn.embedding_lookup_sparse(w, sp, None, combiner='sum')

res.numpy()
# the output
array([[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],
       [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ],
       [ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],
       [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]],

# reshape
tf.reshape(res, [-1, 2, 4])
# that is exacly what I want.
array([[[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],
        [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ]],

       [[ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],
        [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532]],

       [[-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],
        [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]]])

# print w, and the above result is right
w.numpy()

array([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],
       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],
      dtype=float32)
</code></pre>",0.4109315315,0.6437261261,1
"<p>I'm trying to use TensorFlow's <code>@tf.custom_gradient</code> functionality to assign a custom gradient to a function with multiple inputs.  I can put together a working setup for only one input, but not for two or more.</p>

<p>I've based my code on <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">TensorFlow's custom_gradient documentation</a>, which works just fine for one input, as in this example:</p>

<pre><code>import tensorflow as tf
import os

# Suppress Tensorflow startup info
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

# Custom gradient decorator on a function,
# as described in documentation
@tf.custom_gradient
def my_identity(x):

    # The custom gradient
    def grad(dy):
        return dy

    # Return the result AND the gradient
    return tf.identity(x), grad

# Make a variable, run it through the custom op
x = tf.get_variable('x', initializer=1.)
y = my_identity(x)

# Calculate loss, make an optimizer, train the variable
loss = tf.abs(y)
opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)
train = opt.minimize(loss)

# Start a TensorFlow session, initialize variables, train
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(train)
</code></pre>

<p>This example runs silently, then closes.  No issues, no errors.  The variable optimizes as expected.  However, in my application, I need to do such a calculation with multiple inputs, so something of this form:</p>

<pre><code>@tf.custom_gradient
def my_identity(x, z):

    def grad(dy):
        return dy

    return tf.identity(x*z), grad
</code></pre>

<p>Running this in place of the example (and adding another variable input to the call of <code>my_identify</code>) results in the following error output.  Best as I can tell, the last parts of the error are from the dynamic generation of the op -- the information format matches the C++ formatting required in the op establishment (though that's about all I know about it).</p>

<pre><code>Traceback (most recent call last):
  File ""testing.py"", line 27, in &lt;module&gt;
    train = opt.minimize(loss)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 400, in minimize
    grad_loss=grad_loss)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 519, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 630, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 821, in _GradientsHelper
    _VerifyGeneratedGradients(in_grads, op)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 323, in _VerifyGeneratedGradients
    ""inputs %d"" % (len(grads), op.node_def, len(op.inputs)))
ValueError: Num gradients 2 generated for op name: ""IdentityN""
op: ""IdentityN""
input: ""Identity""
input: ""x/read""
input: ""y/read""
attr {
  key: ""T""
  value {
    list {
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
    }
  }
}
attr {
  key: ""_gradient_op_type""
  value {
    s: ""CustomGradient-9""
  }
}
 do not match num inputs 3
</code></pre>

<p>Based on other custom gradient options, I surmised that the issue was a lack of supplied gradient for the second input argument.  So, I changed my function to this:</p>

<pre><code>@tf.custom_gradient
def my_identity(x, z):

    def grad(dy):
        return dy

    return tf.identity(x*z), grad, grad
</code></pre>

<p>This results in the following more familiar error:</p>

<pre><code>Traceback (most recent call last):
  File ""testing.py"", line 22, in &lt;module&gt;
    y = my_identity(x, z)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated
    return _graph_mode_decorator(f, *args, **kwargs)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 132, in _graph_mode_decorator
    result, grad_fn = f(*args)
ValueError: too many values to unpack (expected 2)
</code></pre>

<p>The <code>@custom_gradient</code> decorator is only identifying the last returned element as a gradient.  So, I tried putting the two gradients into a tuple as <code>(grad, grad)</code> such that there would only be ""two"" outputs for the function.  TensorFlow rejected this too, this time because it can't call a tuple like it would a Tensor -- entirely reasonable, in hindsight.</p>

<p>I've fussed around with the example some more, but to no avail.  No matter what I try, I can't get the custom-defined gradient to deal with multiple inputs.  I'm hoping that somebody with more knowledge than I regarding custom ops and gradients will have a better idea on this -- thanks in advance for the help!</p>
","['""""""tf.custom_gradient with multiple inputs\\n\\ntf.custom_gradient accepts only one Tensor x, what if this op needs more than one inputs? For example, to define the gradient of Softmax which needs input x and label? Thanks for the suggestion from @AllenLavoie, I use a Python list as input. def self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(input_): x = input_[0] label = input_[2] def grad(dy): return [dy, dy] return x - label, grad x = tf.range(10, dtype=tf.float32) y = tf.range(10, dtype=tf.int32) loss = loss_func([x, y]) if __name__ == \\\'__main__\\\': self_define_op_multiple_inputs()\\n\\nIt seems that it will convert the Python list to a Tensor. The snippet above will raise a TypeError: TypeError: Cannot convert a list containing a tensor of dtype <dtype: \\\'int32\\\'> to <dtype: \\\'float32\\\'> (Tensor is: <tf.Tensor \\\'range_1:0\\\' shape=(10,) dtype=int32>)\\n\\nThe documentation says x and y can both either be Tensors or sequences of Tensors. Did this not work for you?  Actually this is exactly what confused me. I don\'t understand what\'s sequences of Tensors, does it mean a Python list of Tensor? My interpretation is Python list (or tuple, etc.). So len(x) is the number of inputs to the operation, len(y) is the number of outputs. Then the gradient function takes len(y) Tensor argument and returns len(x) Tensors. I tried to use list but it seems like a list will be converted as a Tensor, which will cause an error if there are multiple inputs with different type and matched shape. The question has been updated. @AllenLavoie I created an issue on github\\n\\n I ran into a similar problem yesterday and found this post, and I believe I know what you are running into. Problem is that while using @tf.custom_gradient, the function that it decorates can have multiple inputs (instead of a list of tensors). Look at the following code(note that it\\\'s just a test code with no actual meaning):\\n\\n@tf.custom_gradient def loop1(x,a): def grad(dy): return dy*3,dy*2 n = tf.multiply(x,a) return n,grad\\n\\nBy using two inputs x and a, you have to return two gradients respectively in the grad function. dy*3 corresponds to the gradient of x and dy*2 corresponds to the gradient of a. I think in this function the documents make people very confusing, but you can still use multiple inputs, just make sure that you also have the same number of gradients, or else you will run into errors. 1\\n\\nCan we return None as gradients for unused terms ? I believe you need something like this a tf Graph input:+ n_input is the input number\\n\\nx = tf.placeholder(""float"", [None, n_input]) y = tf.placeholder(""float"", [None])\\n\\nDoes this answer your question ? """"""']",Failled,"<pre><code>@tf.custom_gradient
def my_multiple(x,z):

def grad(dy):
    # return two gradients, one for 'x' and one for 'z'
    return (dy*z, dy*x)

return tf.identity(x*z), grad
</code></pre>",0.1633261538,0.6533046153,0.5
"<p>So I've built a convnet using pure <code>keras</code>. It compiles and operates exactly as intended, but I need to convert it to use <code>tf.keras</code> so that I can make use of <code>tfmot</code>. Having read documentation, I attempted to convert it, only to get the following error:</p>

<p><code>The last dimension of the inputs to Dense should be defined. Found None.</code> </p>

<p>Any idea what I'm doing wrong?</p>

<p>Thanks!</p>

<p>Original <code>keras</code> model:</p>

<pre><code>input_layer = keras.layers.Input(shape=(100,))
reshape_layer = keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = keras.layers.Flatten()(conv_layer_5)
label_layer = keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = keras.layers.Dense(1, activation=""linear"")(label_layer)

model = keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>Converted <code>tf.keras</code> model:</p>

<pre><code>input_layer = tf.keras.layers.InputLayer(input_shape=(100,))
reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)
label_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>EDIT 1:</p>

<p>I thought maybe I could get around the issue by saving the <code>keras</code> model after creation and loading it as a <code>tf.keras</code> model immediately before compilation / training. That throws the same error! </p>
","['""""""Tensorflow 2.0: Shape inference with Reshape returns None dimension\\n\\nI\\\'m working with a CNN-LSTM model on Tensorflow 2.0 + Keras to perform sequence classification. My model is defined as following:\\n\\ninp = Input(input_shape) rshp = Reshape((input_shape[0]*input_shape[1], 1), input_shape=input_shape)(inp) cnn1 = Conv1D(100, 9, activation=\\\'relu\\\')(rshp) cnn2 = Conv1D(100, 9, activation=\\\'relu\\\')(cnn1) mp1 = MaxPooling1D((3,))(cnn2) cnn3 = Conv1D(50, 3, activation=\\\'relu\\\')(mp1) cnn4 = Conv1D(50, 3, activation=\\\'relu\\\')(cnn3) gap1 = AveragePooling1D((3,))(cnn4) dropout1 = Dropout(rate=dropout[0])(gap1) flt1 = Flatten()(dropout1) rshp2 = Reshape((input_shape[0], -1), input_shape=flt1.shape)(flt1) bilstm1 = Bidirectional(LSTM(240, return_sequences=True, recurrent_dropout=dropout[1]), merge_mode=merge)(rshp2) dense1 = TimeDistributed(Dense(30, activation=\\\'relu\\\'))(rshp2) dropout2 = Dropout(rate=dropout[2])(dense1) prediction = TimeDistributed(Dense(1, activation=\\\'sigmoid\\\'))(dropout2) model = Model(inp, prediction, name=""CNN-bLSTM_per_segment"") print(model.summary(line_length=75))\\n\\nWhere input_shape = (60, 60). This definition, however, raises the following error:\\n\\nTypeError: unsupported operand type(s) for +: \\\'NoneType\\\' and \\\'int\\\'\\n\\nAt first, I thought it was because the rshp2 layer could not reshape the flt1 output to shape (60, X). So I added a printing block before the Bidirectional(LSTM)) layer:\\n\\nprint(\\\'reshape1: \\\', rshp.shape) print(\\\'cnn1: \\\', cnn1.shape) print(\\\'cnn2: \\\', cnn2.shape) print(\\\'mp1: \\\', mp1.shape) print(\\\'cnn3: \\\', cnn3.shape) print(\\\'cnn4: \\\', cnn4.shape) print(\\\'gap1: \\\', gap1.shape) print(\\\'flatten 1: \\\', flt1.shape) print(\\\'reshape 2: \\\', rshp2.shape)\\n\\nAnd the shapes were:\\n\\nreshape 1: (None, 3600, 1) cnn1: (None, 3592, 100) cnn2: (None, 3584, 100) mp1: (None, 1194, 100) cnn3: (None, 1192, 50) cnn4: (None, 1190, 50) gap1: (None, 396, 50) flatten 1: (None, 19800) reshape 2: (None, 60, None)\\n\\nLooking at the flt1 layer, its output shape is (19800,), which can be reshaped as (60, 330), but for some reason the (60, -1) of the rshp2 layer is not working as intended, evidenced by the print reshape 2: (None, 60, None). When I try to reshape as (60, 330) it works just fine. Does anyone knows why the (-1) is not working? From Reshape documentation, https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape\\n\\nthe layer returns a tensor with shape (batch_size,) + target_shape\\n\\nSo, the batch size stays the same, the other dimensions are calculated based on your target_shape. From the doc, look at the last example,\\n\\n# also supports shape inference using `-1` as dimension model.add(tf.keras.layers.Reshape((-1, 2, 2))) model.output_shape\\n\\nIf you pass -1 in your target shape, the Keras will store None, this is useful if you expect variable-length data in that axis, but if your data shape is always same, just put the dimension hard-coded that will place the dimension when you print the shape later. N.B: Also no need to specify input_shape=input_shape for your intermediate layers in functional API. The model will infer that for you.\n\nCurrently, there is no direct in-built support in Tensorflow or Keras to convert the frozen model or the checkpoint file to hdf5 format. But since you have mentioned that you have the code of Tensorflow model, you will have to rewrite that model\'s code in Keras. Then, you will have to read the values of your variables from the checkpoint file and assign it to Keras model using layer.load_weights(weights) method. More than this methodology, I would suggest to you to do the training directly in Keras as it claimed that Keras optimizers are 5-10% times faster than Tensorflow\\\'s optimizers. Other way is to write your code in Tensorflow with tf.contrib.keras module and save the file directly in hdf5 format. Unsure if this is what you are looking for, but I happened to just do the same with the newly released keras support in TF 1.2. You can find more on the API here: https://www.tensorflow.org/api_docs/python/tf/contrib/keras\\n\\nTo save you a little time, I also found that I had to include keras modules as shown below with the additional python.keras appended to what is shown in the API docs. from tensorflow.contrib.keras.python.keras.models import Sequential\\n\\nHope that helps get you where you want to go. Essentially once integrated in, you then just handle your model/weight export as usual. """"""\', \'""""""What is the difference between tf.keras.layers.Input() and tf.keras.layers.Flatten()\\n\\nI have seen multiple uses of both tf.keras.layers.Flatten() (ex. here) and tf.keras.layers.Input() (ex. here). After reading the documentation, it is not clear to me\\n\\nwhether either of them uses the other\\n\\nwhether both can be used interchangeably when introducing to a model an input layer (let\'s say with dimensions (64, 64))\\n\\n I think the confusion comes from using a tf.keras.Sequential model, which does not need an explicit Input layer. Consider the following two models, which are equivalent:\\n\\nimport tensorflow as tf model1 = tf.keras.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(5, activation=\\\'relu\\\'), ]) model1.build((1, 28, 28, 1))\\n\\nmodel2 = tf.keras.Sequential([ tf.keras.layers.Input((28, 28, 1)), tf.keras.layers.Flatten(), tf.keras.layers.Dense(5, activation=\\\'relu\\\'), ])\\n\\nThe difference is that I explicitly set the input shape of model2 using an Input layer. In model1, the input shape will be inferred when you pass real data to it or call model.build. Now regarding the Flatten layer, this layer simply converts a n-dimensional tensor (for example (28, 28, 1)) into a 1D tensor (28 x 28 x 1). The Flatten layer and Input layer can coexist in a Sequential model but do not depend on each other. \\n\\nSo If I understand correctly, in the example of code I used for the tf.keras.layers.Input class, the data are not flattened, they are kept in the same shape, and the class is just used to specify their shape. In that specific example it is necessary to use Input, although the model is Sequential, right? So If I understand correctly, in the example of code I used for the tf.keras.layers.Input class, the data are not flattened, they are kept in the same shape, and the class is just used to specify their shape <--yes. , In that specific example it is necessary to use Input, although the model is Sequential, right? <-- it is optional not necessary. Ok this solves my questions!\n\n How can I convert a trained Tensorflow model to Keras? I have a trained Tensorflow model and weights vector which have been exported to protobuf and weights files respectively. How can I convert these to JSON or YAML and HDF5 files which can be used by Keras? I have the code for the Tensorflow model, so it would also be acceptable to convert the tf.Session to a keras model and save that in code. I think the callback in keras is also a solution. The ckpt file can be saved by TF with:\\n\\nsaver = tf.train.Saver() saver.save(sess, checkpoint_name)\\n\\nand to load checkpoint in Keras, you need a callback class as follow:\\n\\nclass RestoreCkptCallback(keras.callbacks.Callback): def __init__(self, pretrained_file): self.pretrained_file = pretrained_file self.sess = keras.backend.get_session() self.saver = tf.train.Saver() def on_train_begin(self, logs=None): if self.pretrian_model_path: self.saver.restore(self.sess, self.pretrian_model_path) print(\\\'load weights: OK.\\\')\\n\\nThen in your keras script:\\n\\nmodel.compile(loss=\\\'categorical_crossentropy\\\', optimizer=\\\'rmsprop\\\') restore_ckpt_callback = RestoreCkptCallback(pretrian_model_path=\\\'./XXXX.ckpt\\\') model.fit(x_train, y_train, batch_size=128, epochs=20, callbacks=[restore_ckpt_callback])\\n\\nThat will be fine. I think it is easy to implement and hope it helps.""""""\', \'""""""What is the advantage of using an InputLayer (or an Input) in a Keras model with Tensorflow tensors? A Keras model can used as a Tensorflow function on a Tensor, through the functional API, as described here. from keras.layers import InputLayer a = tf.placeholder(dtype=tf.float32, shape=(None, 784)) model = Sequential() model.add(InputLayer(input_tensor=a, input_shape=(None, 784))) model.add(Dense(32, activation=\\\'relu\\\')) model.add(Dense(10, activation=\\\'softmax\\\')) output = model.output\\n\\n<tf.Tensor \\\'dense_24/Softmax:0\\\' shape=(?, 10) dtype=float32>\\n\\nBut, this also works without any InputLayer:\\n\\na = tf.placeholder(dtype=tf.float32, shape=(None, 784)) model = Sequential() model.add(Dense(32, activation=\\\'relu\\\', input_shape=(784,))) model.add(Dense(10, activation=\\\'softmax\\\')) output = model(a)\\n\\nworks, and output has the same shape as before:\\n\\n<tf.Tensor \\\'sequential_9/dense_22/Softmax:0\\\' shape=(?, 10) dtype=float32>\\n\\nI assume the first form permits:\\n\\nto explicitely attach the inputs and outputs as attributes of the model (of the same names), so we can reuse them elsewhere. For example with other TF ops. to transform the tensors given as inputs into Keras inputs, with additional metadata (such as _keras_history as stated in the source code). But this is not something we cannot do with the second form, so, is there a special usage of the InputLayer (and Input a fortiori) (except for multiple inputs)? Moreover, the InputLayer is tricky because it\\\'s using input_shape differently from other keras layers: we specify the batch size (None here), which is not usually the case... It would seem that InputLayer has some uses:\\n\\nFirst, it allows you to give pure tensorflow tensors as is, without specifying their shape. E.g. you could have written\\n\\nmodel.add(InputLayer(input_tensor=a))\\n\\nThis is nice for several obvious reasons, among others less duplication. Second, they allow you to write non-sequential networks with a single input, e.g. input / \\\\ / \\\\ / \\\\ conv1 conv2 | |\\n\\nWithout InputLayer you would need to explicitly feed conv1 and conv2 the same tensor, or create an arbitrary identity layer on top of the model. Neither is quite pleasing.""""""']",Success,"<pre><code>conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)
label_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.summary()
</code></pre>",0.8824746534,0.6877933504,0.6666666667
"<p>I am trying to understand why there is a difference between calculating a dense layer operation directly and using the <code>keras</code> implementation.</p>
<p>Following the documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense</a>) <code>tf.keras.layers.Dense()</code> should implement the operation <code>output = activation(dot(input, kernel) + bias)</code> but <code>result</code> and <code>result1</code> below are not the same.</p>
<pre class=""lang-py prettyprint-override""><code>tf.random.set_seed(1)

bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32)
kernel = tf.Variable(tf.random.uniform(shape=(5,10)), dtype=tf.float32)
x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32))

result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x) + bias)
tf.print(result)

test = tf.keras.layers.Dense(units = 5, 
                            activation = 'relu',
                            use_bias = True, 
                            kernel_initializer = tf.keras.initializers.Constant(value=kernel), 
                            bias_initializer = tf.keras.initializers.Constant(value=bias), 
                            dtype=tf.float32)

result1 = test(tf.transpose(x))

print()
tf.print(result1)

</code></pre>
<p>output</p>
<pre class=""lang-py prettyprint-override""><code>
[[2.87080455]
 [3.25458574]
 [3.28776264]
 [3.14319134]
 [2.04760242]]

[[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]

</code></pre>
<p>Using <code>test.get_weights()</code> I can see that the kernel and bias (<code>b</code>) are getting set to the correct values. I am using TF version 2.12.0.</p>
","['""""""What exactly does tf.keras.layers.Dense do? \\n\\n I\'m using the Keras to build a convolutional neural network. I ran across the following:\\n\\nmodel = tf.keras.Sequential() model.add(layers.Dense(10*10*256, use_bias=False, input_shape=(100,)))\\n\\nI\'m curious - what exactly mathematically is going on here? My guess is that for input of size [100,N], the network will be evaluated N times, once for each training example. The Dense layer created by layers.Dense contains (10*10*256) * (100) parameters that will be updated during backpropagation. Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). Note: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel. # as first layer in a sequential model: model = Sequential() model.add(Dense(32, input_shape=(16,))) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don\\\'t need to specify # the size of the input anymore: model.add(Dense(32))\\n\\n> units: Positive integer, dimensionality of the output space. > activation: Activation function to use.\n\nUnderstanding tf.keras.layers.Dense()\\n\\nI am trying to understand why there is a difference between calculating a dense layer operation directly and using the keras implementation. Following the documentation tf.keras.layers.Dense() should implement the operation output = activation(dot(input, kernel) + bias) but result and result1 below are not the same. tf.random.set_seed(1) bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32) kernel = tf.Variable(tf.random.uniform(shape=(5,10)), dtype=tf.float32) x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32)) result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x) + bias) tf.print(result) test = tf.keras.layers.Dense(units = 5, activation = \\\'relu\\\', use_bias = True, kernel_initializer = tf.keras.initializers.Constant(value=kernel), bias_initializer = tf.keras.initializers.Constant(value=bias), dtype=tf.float32) result1 = test(tf.transpose(x)) print() tf.print(result1)\\n\\n[[2.87080455] [3.25458574] [3.28776264] [3.14319134] [2.04760242]] [[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]\\n\\nUsing test.get_weights() I can see that the kernel and bias (b) are getting set to the correct values. I am using TF version 2.12.0. After some experimentation I realized that the kernel for the dense layer needs to be of shape=(10,5) as apposed to (5,10) as in the code from the original question above. This is implicit because units=5 so a vector of size 10 needs to be passed (hence why input_shape=(10,) is commented out as a reminder). Below is the corrected code:\\n\\ntf.random.set_seed(1) bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32) kernel = tf.Variable(tf.random.uniform(shape=(10,5)), dtype=tf.float32) x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32)) result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x, transpose_a=True) + bias) tf.print(result) test = tf.keras.layers.Dense(units = 5, # input_shape=(10,), activation = \\\'relu\\\', use_bias = True, kernel_initializer = tf.keras.initializers.Constant(value=kernel), bias_initializer = tf.keras.initializers.Constant(value=bias), dtype=tf.float32) result1 = test(tf.transpose(x)) print() tf.print(result1)\\n\\n[[2.38769] [3.63470697] [2.62423944] [3.31286287] [2.91121125]] [[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]\\n\\nUltimately, I am not entirely sure what was happening under the hood and why keras did not raise an error. I will check with the tf.keras.layers.Dense() implementation but any thoughts or suggestions by someone who knows the code already are highly appreciated\n\nI just needed clarification of the Linear Algebra itself and how these Layers differ between Linears. The derivative is one example in how they differ ( as they\\\'re different linear algrabra operations ). I did state a \\\'guess\\\' of what TF\\\'s Dense layer is and if it\\\'s the same. So, I might make an edit to the title if other people think it\'ll help with clarification! Thank you for the explanation of the TF\'s Dense layer though! :D\\n\\n If you follow the references in its call function, it leads you to the definition of the operation used here, which is indeed a matrix multiplication of the inputs and weights plus a bias vector as expected:\\n\\noutputs = gen_math_ops.MatMul(a=inputs, b=kernel). outputs = nn_ops.bias_add(outputs, bias)\\n\\n4\\n\\nSo, the definition of the kernel variable is a weight matrix but of the opposite dimension of PyTorch\'s weight matrix? So, let\'s say I have A input features of batch size N, and B output features. The dimensionality of kernel would be [A, B] whereas in the case of PyTorch it\\\'d be [B, A] (because there\'s a transpose applied to it?)\\n\\n\\n\\nYes they store the weights slightly differently (W.T vs W) but the result is still the same. PyTorch\'s representation is closer to the notation in text books. You can check this quickly by printing out the shape of the Linear/Dense weights in torch and tf. In line 1192 of the first link to the TF source code above, the weights in are initialised with shape=[last_dim, self.units] (N_feats, N_out) and in PyTorch (source code link), the weights are initialised with Parameter(torch.Tensor(out_features, in_features)) (N_out, N_feats)\\n\\n\\n\\nAlright, so for a linear layer of input x of shape (N_samp, N_feats) the output for TF would be matmul(x, A) + b where A is (N_feats, N_out) and b is (N_out, ) and for PyTorch it\\\'s matmul(x, A^T) + b where A is now (N_out, N_feats) and b is (N_out, ). Alright, so it seems that the 2 libraries define their Linear layers differently!""""""']",Failled,"<pre class=""lang-py prettyprint-override""><code>tf.random.set_seed(1)

bias   = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32)
kernel = tf.Variable(tf.random.uniform(shape=(10,5)), dtype=tf.float32)
x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32))

result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x, transpose_a=True) + bias)
tf.print(result)

test = tf.keras.layers.Dense(units = 5, 
                            # input_shape=(10,),
                            activation = 'relu',
                            use_bias = True, 
                            kernel_initializer = tf.keras.initializers.Constant(value=kernel), 
                            bias_initializer = tf.keras.initializers.Constant(value=bias), 
                            dtype=tf.float32)

result1 = test(tf.transpose(x))

print()
tf.print(result1)

</code></pre>
<pre class=""lang-py prettyprint-override""><code>[[2.38769]
 [3.63470697]
 [2.62423944]
 [3.31286287]
 [2.91121125]]

[[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]
</code></pre>",0.2778972629,0.6830176228,1
"<p>I seem to be having a misunderstanding on how <code>tf.cond</code> works. In the tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer"">documentation</a>, it gives the following example:</p>

<pre><code>z = tf.multiply(a, b)
result = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))
</code></pre>

<p>The result of the example, if <code>x&lt;y</code> is <code>True</code> is <code>tf.add(x,z)</code> else <code>tf.square(y)</code></p>

<p>Following this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation.</p>

<p>in my example, <code>deterministic_action = 4</code>, <code>random_action = 11</code>, <code>chose_random=False</code>. The <code>stochastic_action</code> should be <code>4</code>, instead it is <code>1</code>.
Where did the value 1 come from?</p>

<pre><code>#!/usr/bin/env python3

import tensorflow as tf
import numpy as np

with tf.Graph().as_default():
    with tf.device('/cpu:0'):
        stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"")
        eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0))
        with tf.variable_scope('test_cond') as sc:
            deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4
            random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11
            chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) &lt; eps # False because eps = 0
            stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1
            #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action)


    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init, feed_dict={stochastic_ph: True})
    print (""s_ph = "", stochastic_ph)
    d_action = sess.run(deterministic_action)
    print (""det_action= "", d_action)
    r_action = sess.run(random_action)
    print (""rand_action= "", r_action)
    e = sess.run(eps)
    c_action = sess.run(chose_random)
    print (""chose_rand= "", c_action)
    s_action = sess.run(stochastic_action)
    print (""s_action= "", s_action)
    #output = sess.run(output_action)
</code></pre>

<p>here is the output:</p>

<pre><code>python random_vec.py
2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  1
</code></pre>
","['""""""`tf.case` and `tf.cond` executes all the functions within in TensorFlow\\n\\n I\'m trying to execute some condition-dependent functions where each function needs to contract tensors differently depending on their shapes, for instance. However, I realised that tf.cond and tf.case is executing all functions regardless of the condition. Prepared the following code as an example;\\n\\ndef a(): print(""a"") return tf.constant(2) def b(): print(""b"") return tf.constant(3) def c(): print(""c"") return tf.constant(4) def d(): print(""default"") return tf.constant(1) x = tf.constant(1) @tf.function def f(): return tf.case([ (tf.equal(x,1), a), (tf.equal(x,2), b), (tf.equal(x,2), c) ], default=d, exclusive=True) @tf.function def f1(): def cond3(): return tf.cond(tf.equal(x,2), c, d) def cond2(): return tf.cond(tf.equal(x,2), b, cond3) return tf.cond(tf.equal(x,1), a, cond2) print(f()) print(f1()) # Output: # a # b # c # default # tf.Tensor(2, shape=(), dtype=int32) # a # b # c # default # tf.Tensor(2, shape=(), dtype=int32)\\n\\nas you can see for both of the cases, the result is as expected but each function is executed while reaching the conclusion. Hence in my particular case, since I\'m doing different calculations depending on the tensor\'s shape, I get a multitude of errors. I\\\'ve seen many such bug reports but haven\'t found a solution. Is there another way to do conditional execution that I\'m not aware of where different functions can be executed depending on the condition? Note that I tried simply using if tf.equal(x,2): ... but in that case, I\\\'m getting an error saying that tensor output can not be used as python boolean. Note that this example is much-simplified version of my problem, my conditions are based on tensor shapes such as tf.equal(tf.size(tensor), N) so I really need a way to execute different things for different cases. After @LaplaceRicky\'s answer I realised that the code that I provided was not representative enough so I\'m providing a better example showing what I need to do;\\n\\nx = tf.ones((3,2,1)) y = tf.ones((1,2,3)) z = tf.ones((4,3,5)) k = tf.ones((3,5,5)) def a(t): def exe(): return tf.einsum(""ijk,lmi"", t, y) return exe def b(t): def exe(): return tf.einsum(""ijk,ljm"", t, z) return exe def d(t): def exe(): return tf.einsum(""ijk,klm"", t, z) return exe c = tf.constant(1) @tf.function def f(t): y = tf.case([ (tf.equal(tf.shape(t)[0], 3), a(t)), (tf.equal(tf.shape(t)[1], 3), b(t)), ], default=d, exclusive=True) return y print(f(x))\\n\\nThis function will execute properly without tf.function decorator leading to\\n\\ntf.Tensor( [[[[3. 3.]]] [[[3. 3.]]]], shape=(2, 1, 1, 2), dtype=float32\\n\\nHowever, when the decorator is included I got a ValueError which shows that all the cases are executed. TensorFlow version: 2.4.1\\n\\nPython version: 3.8.2\\n\\nShort answer: use tf.print instead of print to check whether a particular branch is really being executed in tensorflow graph mode. Explanations: print does not work and won\\\'t print in graph mode but it will print during tracing. The printed messages actually implies all of the branches were added to the tensorflow graph but it does not imply all branches will be executed all the time in graph mode. tf.print should be used instead for the debugging. def a(): tf.print(\\\'a\\\') return tf.constant(10) def b(): tf.print(\\\'b\\\') return tf.constant(11) def c(): tf.print(\\\'c\\\') return tf.constant(12) @tf.function def cond_fn(x): return tf.switch_case(x, {0:a,1:b}, default=c) print(cond_fn(tf.constant(0))) print(cond_fn(tf.constant(1))) print(cond_fn(tf.constant(2)))\\n\\na tf.Tensor(10, shape=(), dtype=int32) b tf.Tensor(11, shape=(), dtype=int32) c tf.Tensor(12, shape=(), dtype=int32)\\n\\nThe ValueError error message is because tensorflow graph does not support this kind of feature very well, at least not with tf.einsum. One way of the workarounds is to have a graph that supports variable-shaped inputs by using tf.function(f).get_concrete_function(tf.TensorSpec(shape=[None,None,None])). Besides, tf.einsum is problematic in the process and have to be replaced by tf.transpose and tf.tensordot. x = tf.random.normal((3,2,1)) y = tf.random.normal((1,2,3)) z = tf.random.normal((4,3,5)) k = tf.random.normal((3,5,5)) #for checking the values def f2(t): p = tf.case([ (tf.equal(tf.shape(t)[0], 3), lambda:tf.einsum(""ijk,lmi"", t, y)), (tf.equal(tf.shape(t)[1], 3), lambda:tf.einsum(""ijk,ljm"", t, z)), ], default=lambda:tf.einsum(""ijk,klm"", t, k), exclusive=True) return p #work around def f(t): if tf.shape(t)[0] == 3: tf.print(\\\'branch a executed\\\') return tf.tensordot(tf.transpose(t,[1,2,0]), tf.transpose(y,[2,0,1]),1) elif tf.shape(t)[1] == 3: tf.print(\\\'branch b executed\\\') return tf.tensordot(tf.transpose(t,[0,2,1]), tf.transpose(z,[1,0,2]),1) else: tf.print(\\\'branch c executed\\\') return tf.tensordot(t, k,1) graph_f=tf.function(f).get_concrete_function(tf.TensorSpec(shape=[None,None,None])) print(np.allclose(graph_f(x),f2(x))) print(np.allclose(graph_f(y),f2(y))) print(np.allclose(graph_f(z),f2(z)))\\n\\nbranch a executed True branch c executed True branch b executed True\\n\\nSorry, you are correct I didn\'t provide a proper example, thanks for your answer. I updated the code that I provided which I believe exemplifies my situation better.\n\nConfused by the behavior of `tf.cond`\\n\\nI need a conditional control flow in my graph. If pred is True, the graph should call an op that updates a variable and then returns it, otherwise it returns the variable unchanged. A simplified version is:\\n\\npred = tf.constant(True) x = tf.Variable([1]) assign_x_2 = tf.assign(x, [2]) def update_x_2(): with tf.control_dependencies([assign_x_2]): return tf.identity(x) y = tf.cond(pred, update_x_2, lambda: tf.identity(x)) with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval())\\n\\nHowever, I find that both pred=True and pred=False lead to the same result y=[2], which means the assign op is also called when update_x_2 is not selected by tf.cond. How to explain this?\n\nTf.cond giving unexpected output\\n\\nI seem to be having a misunderstanding on how tf.cond works. In the tensorflow documentation, it gives the following example:\\n\\nz = tf.multiply(a, b) result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))\\n\\nThe result of the example, if x<y is True is tf.add(x,z) else tf.square(y)\\n\\nFollowing this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation. in my example, deterministic_action = 4, random_action = 11, chose_random=False.\n\nAnd how to solve this problem? TL;DR: If you want tf.cond() to perform a side effect (like an assignment) in one of the branches, you must create the op that performs the side effect inside the function that you pass to tf.cond(). The behavior of tf.cond() is a little unintuitive. Because execution in a TensorFlow graph flows forward through the graph, all operations that you refer to in either branch must execute before the conditional is evaluated. This means that both the true and the false branches receive a control dependency on the tf.assign() op, and so y always gets set to 2, even if pred is False. The solution is to create the tf.assign() op inside the function that defines the true branch. For example, you could structure your code as follows:\\n\\npred = tf.placeholder(tf.bool, shape=[]) x = tf.Variable([1]) def update_x_2(): with tf.control_dependencies([tf.assign(x, [2])]): return tf.identity(x) y = tf.cond(pred, update_x_2, lambda: tf.identity(x)) with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval(feed_dict={pred: False})) # ==> [1] print(y.eval(feed_dict={pred: True})) # ==> [2]\\n\\n\\n\\n 4\\n\\nYeah, that\'s the one that confuses me also. My understand is that before executing tf.cond, the runtime makes sure all the dependencies are executed. Dependencies of ops in True and False branches are also dependencies of cond, so even though an op in a branch may never be executed, all of it\'s dependencies are executed, does that that sound right? Yep - the graph pruning considers all potential dependencies (of either branch) for execution, and only inhibits their execution if they were defined inside one of the branches, because the CondContext adds a control dependency on the pivot and that dependency will be a dead tensor (preventing the op from executing) if it is in the branch not taken. What was the reasoning doing it this way? Why not prune the subgraph behind the non-active branch? The pruning happens before the value for pred has been computed. This enables TensorFlow to cache a single pruned graph based on a simple key (essentially the arguments to Session.run()), and makes the implementation of conditional execution simple and lightweight. The same mechanism is used to implement tf.while_loop(), where the advantages of performing the control flow at this level are more evident. pred = tf.constant(False) x = tf.Variable([1]) def update_x_2(): assign_x_2 = tf.assign(x, [2]) with tf.control_dependencies([assign_x_2]): return tf.identity(x) y = tf.cond(pred, update_x_2, lambda: tf.identity(x)) with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval())\\n\\nThis will get the result of [1]. \n\nWhere did the value 1 come from? #!/usr/bin/env python3 import tensorflow as tf import numpy as np with tf.Graph().as_default(): with tf.device(\\\'/cpu:0\\\'): stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"") eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0)) with tf.variable_scope(\\\'test_cond\\\') as sc: deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4 random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11 chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) < eps # False because eps = 0 stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1 #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action) init = tf.global_variables_initializer() sess = tf.Session() sess.run(init, feed_dict={stochastic_ph: True}) print (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) r_action = sess.run(random_action) print (""rand_action= "", r_action) e = sess.run(eps) c_action = sess.run(chose_random) print (""chose_rand= "", c_action) s_action = sess.run(stochastic_action) print (""s_action= "", s_action) #output = sess.run(output_action)\\n\\npython random_vec.py 2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA s_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 1\\n\\n\\n\\nThis is because you are evaluating again in a new sess.run. Since you are generating a random number for deterministic_action, the result turns out to be the next random number after 4, which is 1. Here is the result of your code, when I extract the value of deterministic_action as well in the last step. print (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) r_action = sess.run(random_action) print (""rand_action= "", r_action) e = sess.run(eps) c_action = sess.run(chose_random) print (""chose_rand= "", c_action) s_action, d_action = sess.run([stochastic_action, deterministic_action]) print (""s_action= "", s_action) print (""det_action= "", d_action)\\n\\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 1 det_action= 1\\n\\nNow all you need to do is run everything in one sess.run\\n\\nd_action, r_action, e, c_action, s_action = sess.run([deterministic_action, random_action, eps, chose_random, stochastic_action]) print (""det_action= "", d_action) print (""rand_action= "", r_action) print (""chose_rand= "", c_action) print (""s_action= "", s_action)\\n\\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 4\\n\\nI was not clear on why the random_uniform generates different values when seed is set. This is because the code is running with the same session object that it initialized the variables with. Modifying the code with a new session object, this is what happens:\\n\\nprint (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) sess.close() sess = tf.Session() sess.run(init, feed_dict={stochastic_ph: True}) s_action = sess.run(stochastic_action) print (""s_action= "", s_action)\\n\\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 s_action= 4\\n\\n 4\\n\\nThe seed is fixed. So, irrespective of the session you run in, the random value should always be the same. @tandem the sequence of numbers you get would be the same, but not the value every time. If you notice my first result, where I\\\'m just reading out the value of deterministic_action along with stochastic_action that you are getting in your last step. Your deterministic_action is 1, which means your result is also 1. When I put all the evaluations together in a single session run, your deterministic_action is 4, and hence your result is also 4. Seed should always give the same random number. as long as it is set. That explanation doesnt still make sense\\n\\n\\n\\n@tandem tensorflow computation graphs depend on your initialization. In the above example, you are still using the session object that is initialized at the beginning. This means tensorflow is keeping a track of the state of its variables in the session object, and thus does not reinitialize the random_uniform. Close the session object with sess.close() and then open a new session object and run your stochastic_action variable, it will produce the result 4 as expected. """"""']",Failled,"<pre><code>print (""s_ph = "", stochastic_ph)
d_action = sess.run(deterministic_action)
print (""det_action= "", d_action)
r_action = sess.run(random_action)
print (""rand_action= "", r_action)
e = sess.run(eps)
c_action = sess.run(chose_random)
print (""chose_rand= "", c_action)
s_action, d_action = sess.run([stochastic_action, deterministic_action])
print (""s_action= "", s_action)
print (""det_action= "", d_action)
</code></pre>",0.8839413135,0.6662000367,0.3333333333
"<p>I am new to tensorflow and I was trying to follow the official documentation where I came across 
tf.feature_column.categorical_column_with_vocabulary_list</p>

<p>The code I tested is: </p>

<pre><code>key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), default_value=0)
columns = [[tfc.embedding_column(colors, 3)], ...]
features = tf.io.parse_example(..., features=tfc.make_parse_example_spec(columns))
dense_tensor = tfc.input_layer(features, columns)
</code></pre>

<p>However , when I run this sample code I get the following error : 
 ValueError: All feature_columns must be _FeatureColumn instances. Given: [EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), dtype=tf.string, default_value=0, num_oov_buckets=0), dimension=3, combiner='mean', initializer=, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)]</p>

<p>What I am doing wrong?  </p>
","['""""""Tensorflow feature column for variable list of values\\n\\nFrom the TensorFlow docs it\'s clear how to use tf.feature_column.categorical_column_with_vocabulary_list to create a feature column which takes as input some string and outputs a one-hot vector. For example\\n\\nvocabulary_feature_column = tf.feature_column.categorical_column_with_vocabulary_list( key=""vocab_feature"", vocabulary_list=[\'kitchenware\', \'electronics\', \'sports\'])\\n\\nLet\\\'s say \'kitchenware\' maps to [1,0,0] and ""electronics"" maps to [0,1,0]. My question is related to having a list of strings as a feature. For example, if the feature value was [\'kitchenware\',\'electronics\'] then the desired output would be [1,1,0]. The input list length is not fixed but the output dimension is. The use case is a straight bag-of-words type model (obviously with a much larger vocabulary list!). What is the correct way to implement this? \\n\\nHere is an example how to feed data to the indicator column:\\n\\nfeatures = {\\\'letter\\\': [[\\\'A\\\',\\\'A\\\'], [\\\'C\\\',\\\'D\\\'], [\\\'E\\\',\\\'F\\\'], [\\\'G\\\',\\\'A\\\'], [\\\'X\\\',\\\'R\\\']]} letter_feature = tf.feature_column.categorical_column_with_vocabulary_list( ""letter"", [""A"", ""B"", ""C""], dtype=tf.string) indicator = tf.feature_column.indicator_column(letter_feature) tensor = tf.feature_column.input_layer(features, [indicator]) with tf.Session() as session: session.run(tf.global_variables_initializer()) session.run(tf.tables_initializer()) print(session.run([tensor]))\\n\\n[array([[2., 0., 0.], [0., 0., 1.], [0., 0., 0.], [1., 0., 0.], [0., 0., 0.]], dtype=float32)]\\n\\n 3\\n\\nin above example the features is passed as dict. How do I get the same results when I have a column in csv file which is space separated and I need to multi-hot encode using the example above ? Can we use Embedding column here? In case we have large number of values in the column (a very common case), we may end up with a sparse column if we use indicator column. you should use tf.feature_column.indicator_column.\\n\\n Could you give an example of what the structure of the training data should look like in this case. The doc you post to show what the input data inso converted into but not what you feed it.""""""']",Failled,"<pre><code>data = {'colors': ['X', 'R', 'G', 'B', 'Y']}

df = pd.DataFrame(data)

colors = feature_column.categorical_column_with_vocabulary_list('colors', df['colors'].unique())

colors_embedding = feature_column.embedding_column(colors, dimension=4)

dense_tensor = tf.keras.layers.DenseFeatures(colors_embedding)(data)
</code></pre>

<p>Result: </p>

<pre><code>tf.Tensor(
[[ 0.17071894  0.29407692 -0.26661882  0.07768019]
 [ 0.26196313  0.14372464 -0.41102907 -0.7207164 ]
 [-0.7888006  -0.07049363 -0.49007863  0.45744416]
 [ 0.56329435 -0.7051675   0.04742934 -0.69377   ]
 [-0.52031726  0.488502   -0.37031132 -0.44338205]], shape=(5, 4), dtype=float32)
</code></pre>",0.2823340605,0.667673114,0.3333333333
"<p>In the documentation, the body of a tf.while_loop needs to be a python callable.</p>

<pre><code>i = tf.constant(0)
b = lambda i: tf.add(i,1)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>works but</p>

<pre><code>def b(i):
    tf.add(i,1)

i = tf.constant(0)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>throws a ValueError: Attempt to convert a value (None) with an unsupported type() to a Tensor</p>

<p>In 2.0, eager execution is default, I wonder what's the problem?!</p>
","['""""""How to use tf.while_loop() for variable-length inputs in tensorflow? \\n\\nI am trying to use tf.while_loop() to process variable-length inputs. However, I can only use it for fixed length. The code no longer works after I change shape=(4) to shape=(None). tf.dynamic_rnn seems to handle variable-length inputs. I am not sure how tf.dynamic_rnn achieves this with tf.while_loop(). import tensorflow as tf import numpy as np from tensorflow.python.ops import tensor_array_ops from tensorflow.python.ops import array_ops with tf.Graph().as_default(), tf.Session() as sess: initial_m = tf.Variable(0.0, name=\\\'m\\\') inputs = tf.placeholder(dtype=\\\'float32\\\', shape=(4)) #The code no longer works after I change shape=(4) to shape=(None) #inputs = tf.placeholder(dtype=\\\'float32\\\', shape=(None)) time_steps = tf.shape(inputs)[0] initial_outputs = tf.TensorArray(dtype=tf.float32, size=time_steps) initial_t = tf.constant(0, dtype=\\\'int32\\\') def should_continue(t, *args): return t < time_steps def iteration(t, m, outputs_): cur = tf.gather(inputs, t) m = m * 0.5 + cur * 0.5 outputs_ = outputs_.write(t, m) return t + 1, m, outputs_ t, m, outputs = tf.while_loop( should_continue, iteration, [initial_t, initial_m, initial_outputs]) outputs = outputs.pack() init = tf.global_variables_initializer() sess.run([init]) print sess.run([outputs], feed_dict={inputs: np.asarray([1,1,1,1])})\\n\\noutput (before change):\\n\\n[array([ 0.5 , 0.75 , 0.875 , 0.9375], dtype=float32)]\\n\\noutput (after change):\\n\\nTraceback (most recent call last): File ""simple.py"", line 26, in <module> [initial_t, initial_m, initial_outputs]) File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2636, in while_loop result = context.BuildLoop(cond, body, loop_vars, shape_invariants) File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2469, in BuildLoop pred, body, original_loop_vars, loop_vars, shape_invariants) File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2450, in _BuildLoop _EnforceShapeInvariant(m_var, n_var) File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 586, in _EnforceShapeInvariant % (merge_var.name, m_shape, n_shape)) ValueError: The shape for while/Merge_1:0 is not an invariant for the loop. It enters the loop with shape (), but has shape <unknown> after one iteration. Provide shape invariants using either the `shape_invariants` argument of tf.while_loop or set_shape() on the loop variables. 0\\n\\nIt works if you remove shapes from all the variables:\\n\\nimport tensorflow as tf import numpy as np config = tf.ConfigProto(graph_options=tf.GraphOptions( optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0))) tf.reset_default_graph() sess = tf.Session("""", config=config) #initial_m = tf.Variable(0.0, name=\\\'m\\\') #The code no longer works after I change shape=(4) to shape=(None) inputs = tf.placeholder(dtype=\\\'float32\\\', shape=(None)) time_steps = tf.shape(inputs)[0] initial_outputs = tf.TensorArray(dtype=tf.float32, size=time_steps) initial_t = tf.placeholder(dtype=\\\'int32\\\') initial_m = tf.placeholder(dtype=tf.float32) def should_continue(t, *args): return t < time_steps def iteration(t, m, outputs_): cur = tf.gather(inputs, t) m = m * 0.5 + cur * 0.5 outputs_ = outputs_.write(t, m) return t + 1, m, outputs_ t, m, outputs = tf.while_loop(should_continue, iteration, [initial_t, initial_m, initial_outputs]) outputs = outputs.stack() init = tf.global_variables_initializer() sess.run([init]) print(sess.run([outputs], feed_dict={inputs: np.asarray([1, 1, 1, 1]), initial_t: 0, initial_m: 0.}))\\n\\n\\ BTW, is it possible to avoid using placeholder? initial_t and initial_m are are alway zeros.\n\nHow to use tf.while_loop() in tensorflow\\n\\n This is a generic question. I found that in the tensorflow, after we build the graph, fetch data into the graph, the output from graph is a tensor. but in many cases, we need to do some computation based on this output (which is a tensor), which is not allowed in tensorflow. for example, I\'m trying to implement a RNN, which loops times based on data self property. That is, I need use a tensor to judge whether I should stop (I am not using dynamic_rnn since in my design, the rnn is highly customized). I find tf.while_loop(cond,body.....) might be a candidate for my implementation. But the official tutorial is too simple.\n\ntf.function and tf.while loop in Tensorflow 2.0\\n\\n I am trying to parallelize loop using tf.while_loop. As suggested here, the parallel_iterations argument doesn\\\'t make a difference in the eager mode. So I attempted to wrap tf.while_loop with tf.function. However, after adding the decorator,the behavior of the iteration variable changes. For example, this piece of code works. result = np.zeros(10) iteration = tf.constant(0) c = lambda i: tf.less(i, 10) def print_fun(iteration): result[iteration] = iteration iteration += 1 return (iteration,) tf.while_loop(c, print_fun, [iteration])\\n\\nIf I add the decorator, bug occurs. result = np.zeros(10) iteration = tf.constant(0) c = lambda i: tf.less(i, 10) def print_fun(iteration): result[iteration] = iteration iteration += 1 return (iteration,) @tf.function def run_graph(): iteration = tf.constant(0) tf.while_loop(c, print_fun, [iteration]) run_graph()\\n\\nFrom my debugging process, I found that variable iteration changes from a tensor to a placeholder. Why is that?\n\nI will edit my question based on your answer here\\n\\nWhat\\\'s the expected answer for your code? The global summ and ignoring the second body argument is suspicious: you probably want to pass 0. as the initial value for the second loop variable, and use the second body argument instead of the global summ as the accumulator. If you see this error: ValueError: The two structures don\\\'t have the same number of elements. If you see it in a while_loop, that means your inputs and outputs out of the while loop have different shapes. I solved it by making sure that I return the same structure of loop_vars from my while loop function, the condition function must also accept same loop vars. Here is an example code\\n\\nloop_vars = [i, loss, batch_size, smaller_str_lens] def condition(*loop_vars): i = loop_vars[0] batch_size = loop_vars[2] return tf.less(i, batch_size) def body(*loop_vars): i, loss, batch_size, smaller_str_lens = loop_vars tf.print(""The loop passed here"") ## logic here i = tf.add(i, 1) return i, loss, batch_size, smaller_str_lens loss = tf.while_loop(condition, compare_strings, loop_vars)[1]\\n\\nThe body func must return loop vars, and the condition func must accept loop vars.""""""']",Success,"<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

def b(i):
    return tf.add(i, 1)

i = tf.constant(0)
c = lambda i: tf.less(i, 10)
tf.while_loop(c, b, [i]) # &lt;tf.Tensor: id=51, shape=(), dtype=int32, numpy=10&gt;
</code></pre>
<pre class=""lang-py prettyprint-override""><code>i = tf.constant(0)
b = lambda i: tf.add(i,1)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
print(b(1).numpy()) # 2
</code></pre>",0.3549849617,0.6699398467,0.6
"<p>In the documentation of tf.nn.conv1d, it is stated that</p>

<blockquote>
  <p>Internally, this op reshapes the input tensors and invokes tf.nn.conv2d. For example, if data_format does not start with ""NC"", a tensor of shape [batch, in_width, in_channels] is reshaped to [batch, 1, in_width, in_channels], and the filter is reshaped to [1, filter_width, in_channels, out_channels]. The result is then reshaped back to [batch, out_width, out_channels] (where out_width is a function of the stride and padding as in conv2d) and returned to the caller.</p>
</blockquote>

<p>I get that the operations are equivalent, but I am a bit confused about the implications of this implementation detail. </p>

<p>Does the reshaping create some computational overhead? 
The 3D convolution has its own implementation, so why not the 1D convolution?</p>

<p>Thanks for any explanation that helps me and others to understand this implementation detail of TensorFlow!</p>
","['"""""" What does tf.nn.conv2d do in tensorflow? \\n\\nI was looking at the docs of tensorflow about tf.nn.conv2d here. But I can\'t understand what it does or what it is trying to achieve. It says on the docs,\\n\\n#1 : Flattens the filter to a 2-D matrix with shape\\n\\n[filter_height * filter_width * in_channels, output_channels]. Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below :\\n\\n# 2: Extracts image patches from the the input tensor to form a virtual tensor of shape\\n\\n[batch, out_height, out_width, filter_height * filter_width * in_channels]. # 3: For each patch, right-multiplies the filter matrix and the image patch vector. It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this. I\\\'ve tried coding a small portion and printing out the shape of the operation. Still, I can\\\'t understand. I tried something like this:\\n\\nop = tf.shape(tf.nn.conv2d(tf.random_normal([1,10,10,10]), tf.random_normal([2,10,10,10]), strides=[1, 2, 2, 1], padding=\\\'SAME\\\')) with tf.Session() as sess: result = sess.run(op) print(result)\\n\\nI understand bits and pieces of convolutional neural networks. I studied them here. But the implementation on tensorflow is not what I expected. So it raised the question. EDIT: So, I implemented a much simpler code. But I can\\\'t figure out what\\\'s going on. I mean how the results are like this.""""""']",Success,"<pre class=""lang-py prettyprint-override""><code>import numpy as np
from time import time

x = np.random.randn(700, 800, 900) # 504,000,000 elements

t0 = time()
for i in range(1000):
    if i % 2 == 0:
        x = x.reshape(700, 900, 800)
    else:
        x = x.reshape(700, 800, 900)
print(time() - t0)
</code></pre>

<pre><code>0.0009968280792236328
</code></pre>",0.3817825485,0.6699873367,0
"<p>I'm having a problem serving my text classification model on <code>Tensorflow 1.12</code>. I'm using <code>tf.estimator.inputs.pandas_input_fn</code> to read in my data, and <code>tf.estimator.DNNClassifier</code> to train/evaluate. I'd then like to serve my model.
(Apologies in advance, it's tough to provide a full working example here, but it's very much like the example TF provides at <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier</a>  )</p>

<p>I'm currently saving my model with ...</p>

<pre class=""lang-py prettyprint-override""><code>...
estimator.export_savedmodel(""./TEST_SERVING/"", self.serving_input_receiver_fn, strip_default_attrs=True)
...
def serving_input_receiver_fn(self):
      """"""An input receiver that expects a serialized tf.Example.""""""

      # feature spec dictionary  determines our input parameters for the model
      feature_spec = {
          'Headline': tf.VarLenFeature(dtype=tf.string),
          'Description': tf.VarLenFeature(dtype=tf.string)
      }

      # the inputs will be initially fed as strings with data serialized by
      # Google ProtoBuffers
      serialized_tf_example = tf.placeholder(
          dtype=tf.string, shape=None, name='input_example_tensor')
      receiver_tensors = {'examples': serialized_tf_example}

      # deserialize input
      features = tf.parse_example(serialized_tf_example, feature_spec)
      return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)


</code></pre>

<p>This actually fails to run with the error:</p>

<pre class=""lang-sh prettyprint-override""><code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensor'&gt; to Tensor. Contents: SparseTensor(indices=Tensor(""ParseExample/ParseExample:0"", shape=(?, 2), 
dtype=int64), values=Tensor(""ParseExample/ParseExample:2"", shape=(?,), dtype=string), dense_shape=Tensor(""ParseExample/ParseExample:4"", shape=(2,), dtype=int64)). Consider casting elements to a supported type.

</code></pre>

<p>I tried to save a second way doing:</p>

<pre class=""lang-py prettyprint-override""><code>def serving_input_receiver_fn(self):
  """"""Build the serving inputs.""""""
  INPUT_COLUMNS = [""Headline"",""Description""]
  inputs = {}
  for feat in INPUT_COLUMNS:
    inputs[feat] = tf.placeholder(shape=[None], dtype=tf.string, name=feat)
  return tf.estimator.export.ServingInputReceiver(inputs, inputs)
</code></pre>

<p>This actually works, until I try testing it with the <code>saved_model_cli</code>.
Some output for <code>saved_model_cli show --all --dir TEST_SERVING/1553879255/</code>:</p>

<pre class=""lang-sh prettyprint-override""><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['predict']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['Description'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Description:0
    inputs['Headline'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Headline:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['class_ids'] tensor_info:
        dtype: DT_INT64
        shape: (-1, 1)
        name: dnn/head/predictions/ExpandDims:0
    outputs['classes'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: dnn/head/predictions/str_classes:0
    outputs['logits'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/logits/BiasAdd:0
    outputs['probabilities'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/head/predictions/probabilities:0
  Method name is: tensorflow/serving/predict

</code></pre>

<p>But now I can't seem to test it.</p>

<pre class=""lang-sh prettyprint-override""><code>&gt;&gt;&gt; saved_model_cli run --dir TEST_SERVING/1553879255/ --tag_set serve --signature_def predict --input_examples 'inputs=[{""Description"":[""What is going on""],""Headline"":[""Help me""]}]'
Traceback (most recent call last):
 ...
  File ""/Users/Josh/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 489, in _create_example_string
    feature_list)
TypeError: 'What is going on' has type str, but expected one of: bytes

</code></pre>

<p>Ok, lets turn it into a bytes object by changing to <code>b[""What is going on""]</code> and <code>b[""Help me""]</code>...</p>

<pre class=""lang-sh prettyprint-override""><code>ValueError: Type &lt;class 'bytes'&gt; for value b'What is going on' is not supported for tf.train.Feature.
</code></pre>

<p>Any ideas/thoughts??
Thanks!</p>
","['""""""How to export Estimator model with export_savedmodel function\\n\\n are there any tutorials available about export_savedmodel ? I have gone through this article on tensorflow.org and unittest code on github.com, and still have no idea about how to construct the parameter serving_input_fn of function export_savedmodel\\n\\n 2\\n\\nExample of export_savedmodel function\\n\\n your_feature_spec = { ""some_feature"": tf.FixedLenFeature([], dtype=tf.string, default_value=""""), ""some_feature"": tf.VarLenFeature(dtype=tf.string), } def _serving_input_receiver_fn(): serialized_tf_example = tf.placeholder(dtype=tf.string, shape=None, name=\\\'input_example_tensor\\\') # key (e.g. \\\'examples\\\') should be same with the inputKey when you # buid the request for prediction receiver_tensors = {\\\'examples\\\': serialized_tf_example} features = tf.parse_example(serialized_tf_example, your_feature_spec) return tf.estimator.export.ServingInputReceiver(features, receiver_tensors) estimator.export_savedmodel(export_dir, _serving_input_receiver_fn)\\n\\nThen you can request the served model with ""predict"" signature name by batch. \\n\\nusing feature_spec = {\\\'x\\\': tf.FixedLenFeature([224, 224, 3], dtype=tf.float32)} i got the error: TypeError: Failed to convert object of type <type \\\'dict\\\'> to Tensor\\n\\n\\n\\nThe signature is \'predict\', is there a way to specify custom signature? Or set back to the \'serving_default\'. if you are using tensorflow straight from the master branch there\\\'s a module tensorflow.python.estimator.export that provides a function for that:\\n\\nfrom tensorflow.python.estimator.export import export feature_spec = {\'MY_FEATURE\': tf.constant(2.0, shape=[1, 1])} serving_input_fn = export.build_raw_serving_input_receiver_fn(feature_spec)\\n\\nUnfortunately at least for me it will not go further than that but I\\\'m not sure if my model is really correct so maybe you have more luck than I do. Alternatively, there are the following functions for the current version installed from pypi:\\n\\nserving_input_fn = tf.contrib.learn.utils.build_parsing_serving_input_fn(feature_spec) serving_input_fn = tf.contrib.learn.utils.build_default_serving_input_fn(feature_spec)\\n\\nBut I couldn\'t get them to work, too. Probably, I\'m not understanding this correctly so I hope you\'ll have more luck. \\n\\nExport your model to work with JSON dictionaries\\n\\nIn my mlengine-boilerplate repository, I use this to export estimator models to Cloud ML Engine to easily use this with online predictions (sample code for the predictions). Essential part:\\n\\ndef serving_input_fn(): feature_placeholders = { \\\'id\\\': tf.placeholder(tf.string, [None], name=""id_placeholder""), \\\'feat\\\': tf.placeholder(tf.float32, [None, FEAT_LEN], name=""feat_placeholder""), #label is not required since serving is only used for inference } return input_fn_utils.InputFnOps( feature_placeholders, None, feature_placeholders)\\n\\nExport your model to work with Tensorflow Examples\\n\\nThis tutorial shows how you can use export_savedmodel to serve the Wide & Deep Model implemented with estimators and how to feed Tensorflow examples into the exported model. The essential part:\\n\\nfrom tensorflow.contrib.learn.python.learn.utils import input_fn_utils serving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)\\n\\n\\n\\n1,4\\n\\nI updated the tutorial to support r1.3. You should probably update your answer to reflect new changes.""""""']",Failled,"<pre class=""lang-py prettyprint-override""><code>  def save_serving_model(self,estimator):
      feature_placeholder = {'Headline': tf.placeholder('string', [1], name='headline_placeholder'),
      'Description': tf.placeholder('string', [1], name='description_placeholder')}
      serving_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholder)

      estimator.export_savedmodel(""TEST_SERVING/"", serving_input_fn)
</code></pre>

<p>where using the <code>saved_model_cli</code> works. I.e.:</p>

<pre class=""lang-sh prettyprint-override""><code>saved_model_cli run --dir /path/to/model/ --tag_set serve --signature_def predict --input_exprs=""Headline=['Finally, it works'];Description=['Yay, it works']"" 

</code></pre>

<pre class=""lang-sh prettyprint-override""><code>Result for output key class_ids:
[[2]]
Result for output key classes:
[[b'2']]
Result for output key logits:
[[-0.56755465  0.31625098  0.39260274]]
Result for output key probabilities:
[[0.16577701 0.40119565 0.4330274 ]]
</code></pre>",,0.6828286527,0.3333333333
"<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/manip/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd documentation</a> and run the example code for 1D and 3D tensors... and now I'm trying to do it for a 2D tensor.  I want to 'interleave' the columns of two tensors.  For 1D tensors, one can do this via</p>

<pre><code>'''
We want to interleave elements of 1D tensors arr1 and arr2, where
arr1 = [10, 11, 12]
arr2 = [1, 2, 3, 4, 5, 6]
such that
desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12]
'''

import tensorflow as tf

with tf.Session() as sess:

    updates1 = tf.constant([1,2,3,4,5,6])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)

    updates2 = tf.constant([10,11,12])
    indices2 = tf.constant([[2], [5], [8]])
    scatter2 = tf.scatter_nd(indices2, updates2, shape)

    result = scatter1 + scatter2

    print(sess.run(result))
</code></pre>

<p>(aside: is there a <em>better</em> way to do this?  I'm all ears.)</p>

<p>This gives the output</p>

<p><code>[ 1  2 10  3  4 11  5  6 12]</code></p>

<p>Yay! that worked!</p>

<p>Now lets' try to extend this to 2D.</p>

<pre><code>    '''
    We want to interleave the *columns* (not rows; rows would be easy!) of

    arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]
    arr2 = [[10 11 12], [10 11 12], [10 11 12]]
    such that
    desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]]
    '''

    updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([3, 9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)
</code></pre>

<p>This gives the error
<code>ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1
dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but
are 6 and 3. Shapes are [6] and [3]. for 'ScatterNd_2' (op: 'ScatterNd') with
input shapes: [6,1], [3,6], [2].</code></p>

<p>Seems like my <code>indices</code> is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean
I need to <em>explicitly</em> specify every single pair of indices for every element in <code>updates1</code>?
Or is there some kind of 'wildcard' specification I can use for the rows? (Note <code>indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]])</code> gives syntax errors, as it probably should.)</p>

<p>Would it be easier to just do a transpose, interleave the rows, then transpose back?
Because I tried that...</p>

<pre><code>scatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))
print(sess.run(tf.transpose(scatter1)))
</code></pre>

<p>...and got a <em>much</em> longer error message, that I don't feel like posting unless someone requests it. </p>

<p>PS- I searched to make sure this isn't a duplicate -- I find it hard to imagine that someone else hasn't asked this before -- but turned up nothing. </p>
","['""""""tensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)? \\n\\n I\'ve read the tf.scatter_nd documentation and run the example code for 1D and 3D tensors and now I\\\'m trying to do it for a 2D tensor. I want to \'interleave\' the columns of two tensors. For 1D tensors, one can do this via\\n\\n\\\'\\\'\\\' We want to interleave elements of 1D tensors arr1 and arr2, where arr1 = [10, 11, 12] arr2 = [1, 2, 3, 4, 5, 6] such that desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12] \\\'\\\'\\\' import tensorflow as tf with tf.Session() as sess: updates1 = tf.constant([1,2,3,4,5,6]) indices1 = tf.constant([[0], [1], [3], [4], [6], [7]]) shape = tf.constant([9]) scatter1 = tf.scatter_nd(indices1, updates1, shape) updates2 = tf.constant([10,11,12]) indices2 = tf.constant([[2], [5], [8]]) scatter2 = tf.scatter_nd(indices2, updates2, shape) result = scatter1 + scatter2 print(sess.run(result))\\n\\n(aside: is there a better way to do this? I\\\'m all ears.)\\n\\nThis gives the output\\n\\n[ 1 2 10 3 4 11 5 6 12]\\n\\nNow lets\\\' try to extend this to 2D. \\\'\\\'\\\' We want to interleave the *columns* (not rows; rows would be easy!) of arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]] arr2 = [[10 11 12], [10 11 12], [10 11 12]] such that desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]] \\\'\\\'\\\' updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]) indices1 = tf.constant([[0], [1], [3], [4], [6], [7]]) shape = tf.constant([3, 9]) scatter1 = tf.scatter_nd(indices1, updates1, shape)\\n\\nThis gives the error ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1 dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but are 6 and 3. Shapes are [6] and [3]. for \\\'ScatterNd_2\\\' (op: \\\'ScatterNd\\\') with input shapes: [6,1], [3,6], [2]. Seems like my indices is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean I need to explicitly specify every single pair of indices for every element in updates1? Or is there some kind of \\\'wildcard\\\' specification I can use for the rows? (Note indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]]) gives syntax errors, as it probably should.)\\n\\nWould it be easier to just do a transpose, interleave the rows, then transpose back? Because I tried that... scatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape)) print(sess.run(tf.transpose(scatter1)))\\n\\n...and got a much longer error message, that I don\\\'t feel like posting unless someone requests it. PS- I searched to make sure this isn\'t a duplicate -- I find it hard to imagine that someone else hasn\'t asked this before -- but turned up nothing. \\n\\nOk, the following lines actually work, but I have no idea why.. shape = tf.constant([9, 3]), scatter1 = tf.transpose(tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))) ...Particularly strange that I have to define shape as [9,3] and take its transpose, whereas just defining it as [3,9] and using it that way gives an error. I would have thought I need to use a shape of [9,3] if I\\\'m using the traspose, or else define [3,9] and then take its transpose. ? ...So, while I now have \\\'working code\\\', I\\\'d rather not \\\'answer my own question\\\': If you can either explain why this is necessary, or offer a better way to do what I want to do, then the prize is yours! \\n\\nThis is pure slicing but I didn\'t know that syntax like arr1[0:,:][:,:2] actually works. It seems it does but not sure if it is better. This may be the wildcard slicing mechanism you are looking for. arr1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,7],[1,2,3,4,5,8]]) arr2 = tf.constant([[10, 11, 12], [10, 11, 12], [10, 11, 12]]) with tf.Session() as sess : sess.run( tf.global_variables_initializer() ) print(sess.run(tf.concat([arr1[0:,:][:,:2], arr2[0:,:] [:,:1], arr1[0:,:][:,2:4],arr2[0:, :][:, 1:2], arr1[0:,:][:,4:6],arr2[0:, :][:, 2:3]],axis=1)))\\n\\n[[ 1 2 10 3 4 11 5 6 12] [ 1 2 10 3 4 11 5 7 12] [ 1 2 10 3 4 11 5 8 12]]\\n\\n[[1 2 3 4 5 6] [1 2 3 4 5 7] [1 2 3 4 5 8]]\\n\\nand arr1[0:,:][:,:2] returns the first two columns\\n\\n I was using scatter_nd instead of concatenate because I need the solution to scale up to hundreds of columns, which I can\'t count on being able to specify ""by hand"". Still, if there\'s a way to make this ""scale"", i.e. programatically specifying the columns (without a hundred concat operations which would be slow), then this answer wins. I also hit upon a different (non-scatter_nd) answer using permutation matrices, which I\\\'ll post in a bit... Some moderators might have regarded my question as a duplicate of this one, not because the questions are the same, but only because the answers contain parts one can use to answer this question -- i.e. specifying every index combination by hand. A totally different method would be to multiply by a permutation matrix as shown in the last answer to this question. Since my original question was about scatter_nd, I\\\'m going to post this solution but wait to see what other answers come in... (Alternatively, I or someone could edit the question to make it about reordering columns, not specific to scatter_nd --EDIT: I have just edited the question title to reflect this). Here, we concatenate the two different arrays/tensors... import numpy as np import tensorflow as tf sess = tf.Session() # the ultimate application is for merging variables which should be in groups, # e.g. in this example, [1,2,10] is a group of 3, and there are 3 groups of 3 n_groups = 3 vars_per_group = 3 # once the single value from arr2 (below) is included arr1 = 10+tf.range(n_groups, dtype=float) arr1 = tf.stack((arr1,arr1,arr1),0) arr2 = 1+tf.range(n_groups * (vars_per_group-1), dtype=float) arr2 = tf.stack((arr2,arr2,arr2),0) catted = tf.concat((arr1,arr2),1) # concatenate the two arrays together print(""arr1 = \\\\n"",sess.run(arr1)) print(""arr2 = \\\\n"",sess.run(arr2)) print(""catted = \\\\n"") """"""']",Success,"<pre><code>arr1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,7],[1,2,3,4,5,8]])
arr2 = tf.constant([[10, 11, 12], [10, 11, 12], [10, 11, 12]])

with tf.Session() as sess :
    sess.run( tf.global_variables_initializer() )
    print(sess.run(tf.concat([arr1[0:,:][:,:2], arr2[0:,:] [:,:1],
                              arr1[0:,:][:,2:4],arr2[0:, :][:, 1:2],
                              arr1[0:,:][:,4:6],arr2[0:, :][:, 2:3]],axis=1)))
</code></pre>",0.4114570277,0.6458281107,0.8888888889
"<p>I am using <code>tf.keras.utils.image_dataset_from_directory</code> to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the <code>validation_split</code> parameter), I want to split it into training, testing, and validation subsets.</p>
<p>I have tried using <code>dataset.skip()</code> and <code>dataset.take()</code> to further split one of the resulting subsets, but these functions return a <code>SkipDataset</code> and a <code>TakeDataset</code> respectively (by the way, contrary to <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#take"" rel=""nofollow noreferrer"">the documentation</a>, where it is claimed that these functions return a <code>Dataset</code>). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history.</p>
<p>So, my question is: is there a way to split a <code>Dataset</code> into three subsets for training, validation and testing, so that all three subsets are also <code>Dataset</code> objects?</p>
<p><strong>Code used to load the data</strong></p>
<pre><code>def load_data_tf(data_path: str, img_shape=(256,256), batch_size: int=8):
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.2,
        subset=&quot;training&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.3,
        subset=&quot;validation&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    return train_ds, val_ds

train_dataset, test_val_ds = load_data_tf('data_folder', img_shape = (256,256), batch_size=8)
test_dataset = test_val_ds.take(686)
val_dataset = test_val_ds.skip(686)
</code></pre>
<p><strong>Model compilation and fitting</strong></p>
<pre><code>model.compile(optimizer='sgd',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])
history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, verbose=1)
</code></pre>
<p><strong>When using a normal <code>Dataset</code>, <code>val_accuracy</code> and <code>val_loss</code> are present in the history of the model:</strong></p>
<p><a href=""https://i.stack.imgur.com/Qn1Yf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qn1Yf.png"" alt=""Expected behaviour: when using a Dataset, validation metrics are calculated"" /></a></p>
<p><strong>But when using a <code>SkipDataset</code>, they are not:</strong></p>
<p><a href=""https://i.stack.imgur.com/GMnBM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GMnBM.png"" alt=""Using the SkipDataset produced by test_val_ds.take() leads to validation metrics disappearing from model history"" /></a></p>
<p><a href=""https://i.stack.imgur.com/omU5U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/omU5U.png"" alt=""val_accuracy and val_loss are not present in history keys when using a SkipDataset or a TakeDataset"" /></a></p>
","['""""""How to use tf.keras.utils.image_dataset_from_directory to load test dataset? \\n\\nI am using tf.keras.utils.image_dataset_from_directory in my binary classification Mobilenet V2 model to split the dataset by defining training and validation subsets as following:\\n\\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(directory, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, validation_split=0.2, subset=\'training\', seed=42) validation_dataset = tf.keras.utils.image_dataset_from_directory(directory, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, validation_split=0.2, subset=\'validation\', seed=42)\\n\\nNow, I want to use model.predict() on a set of images to look at the predictions. How can I use image_dataset_from_directory considering that there won\\\'t be two different folders containing the respective classes but only one folder for which I want the predictions? In addition, what should be the parameters of the image_dataset_from_directory function now? You need to have a separate directory containing test images. Then do the same thing you did for train/val datasets but with shuffle=False and without validation_split. As mentioned by @Djinn, You can do it in the same way and no need to define validation_split while accessing the folder. For example, Suppose I have a binary_data folder inside the dogs_cats/binary_data folder where I have stored multiple class images (5-5 images for each cats and dogs), then you can give the path till dogs_cats. This will automatically fetch all images inside the binary_data folder by stating class 1 where you can have multiple class images(binary - as per model) stored. After training the model, you can pass this dataset in model.predict() and can check the predictions for each image. Please check the below code:\\n\\ntest_dataset = tf.keras.utils.image_dataset_from_directory( ""/content/GoogleDrive/My Drive/MY WORK/dataset/dogs_cats/"", shuffle=True, #or False batch_size=BATCH_SIZE, image_size=IMG_SIZE)\\n\\nFound 10 files belonging to 1 classes. and the predictions part:\\n\\npredictions = model.predict(test_dataset) predictions = tf.where(predictions < 0.5,0, 1) print(\\\'Predictions:\\\\n\\\', predictions.numpy())\\n\\nPredictions: [[0] [1] [0] [0] [1] [1] [1] [1] [1] [1]]\\n\\nNote: Prediction\\\'s accuracy may depend on the model performance. \n\nI hope it better answers your question. use Keras - model.fit(dataset,.., validation.split=0.7, ...) see its all possible arguments\\n\\n Assuming you have all_dataset variable of tf.data.Dataset type:\\n\\ntest_dataset = all_dataset.take(1000) train_dataset = all_dataset.skip(1000)\\n\\nTest dataset now has first 1000 elements and the rest goes for training. 1,3\\n\\nAs also mentioned in ted\'s answer, adding all_dataset.shuffle() allows for a shuffled split. Possibly add as code comment in answer like so? # all_dataset = all_dataset.shuffle() # in case you want a shuffled split\\n\\n\\n\\nTensorFlow 2.10.0 will have a utility function for splitting, see my answer: stackoverflow.com/a/73591823/1389680\\n\\n\\n\\ntake and skip return TfTakeDatasets/SkipDatasets which have less functionality than TfDatasets. Does anyone know how to map those to tfDatasets or split into train test splits and get back TfDataset objects? You may use Dataset.take() and Dataset.skip():\\n\\ntrain_size = int(0.7 * DATASET_SIZE) val_size = int(0.15 * DATASET_SIZE) test_size = int(0.15 * DATASET_SIZE) full_dataset = tf.data.TFRecordDataset(FLAGS.input_file) full_dataset = full_dataset.shuffle() train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) val_dataset = test_dataset.skip(val_size) test_dataset = test_dataset.take(test_size)\\n\\nFor more generality, I gave an example using a 70/15/15 train/val/test split but if you don\'t need a test or a val set, just ignore the last 2 lines. Creates a Dataset with at most count elements from this dataset. Creates a Dataset that skips count elements from this dataset. You may also want to look into Dataset.shard():\\n\\nCreates a Dataset that includes only 1/num_shards of this dataset. Disclaimer I stumbled upon this question after answering this one so I thought I\\\'d spread the love\\n\\n 7\\n\\nThank you very much @ted! Is there a way to divide the dataset in a stratified way? Or, alternatively, how can we have an idea of the class proportions (suppose a binary problem) after the train/val/test split? Thanks a lot in advance! Have a look at this blogpost I wrote; eventhough it\\\'s for multilabel datasets, should be easily usable for single label, multiclass datasets -> vict0rs.ch/2018/06/17/multilabel-text-classification-tensorflow\\n\\n\\n\\nThis causes my train,validation and test datasets to have overlap between them. Is this supposed to happen and not a big deal? I would assume it\\\'s not a good idea to have the model train on validation and test data. @c_student I had the same problem and I figured out what I was missing: when you shuffle use the option reshuffle_each_iteration=False otherwise elements could be repeated in train, test and val\\n\\n\\n\\nThis is very true @xdola, and in particular when using list_files you should use shuffle=False and then shuffle with the .shuffle with reshuffle_each_iteration=False. \\n\\nMost of the answers here use take() and skip(), which requires knowing the size of your dataset before hand. This isn\\\'t always possible, or is difficult/intensive to ascertain. Instead what you can do is to essentially slice the dataset up so that 1 every N records becomes a validation record. To accomplish this, lets start with a simple dataset of 0-9:\\n\\ndataset = tf.data.Dataset.range(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n\\nNow for our example, we\\\'re going to slice it so that we have a 3/1 train/validation split. Meaning 3 records will go to training, then 1 record to validation, then repeat. split = 3 dataset_train = dataset.window(split, split + 1).flat_map(lambda ds: ds) # [0, 1, 2, 4, 5, 6, 8, 9] dataset_validation = dataset.skip(split).window(1, split + 1).flat_map(lambda ds: ds) # [3, 7]\\n\\nSo the first dataset.window(split, split + 1) says to grab split number (3) of elements, then advance split + 1 elements, and repeat. That + 1 effectively skips the 1 element we\\\'re going to use in our validation dataset. The flat_map(lambda ds: ds) is because window() returns the results in batches, which we don\\\'t want. So we flatten it back out. Then for the validation data we first skip(split), which skips over the first split number (3) of elements that were grabbed in the first training window, so we start our iteration on the 4th element. The window(1, split + 1) then grabs 1 element, advances split + 1 (4), and repeats. Note on nested datasets: The above example works well for simple datasets, but flat_map() will generate an error if the dataset is nested. To address this, you can swap out the flat_map() with a more complicated version that can handle both simple and nested datasets:\\n\\n.flat_map(lambda *ds: ds[0] if len(ds) == 1 else tf.data.Dataset.zip(ds))\\n\\n 7\\n\\nDoesn\\\'t window just use skip under the hood? How does is the disadvantage The other disadvantage is that with skip() it has to read, and then discard, all the skipped records, which if your data source is slow means you might have a large spool-up time before results are emitted. adressed? If you have a dataset of 1000 records, and you want a 10% for validation, you would have to skip the first 900 records before a single validation record is emitted. With this solution, it only has to skip 9 records. It does end up skipping the same amount overall, but if you use dataset.prefetch(), it can read in the background while doing other things. The difference is just saving the initial spool-up time. Thinking about it a bit more, and I removed the statement.""""""']",Failled,"<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import pathlib

dataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=123,
  image_size=(180, 180),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;validation&quot;,
  seed=123,
  image_size=(180, 180),
  batch_size=batch_size)

test_dataset = val_ds.take(5)
val_ds = val_ds.skip(5)

print('Batches for testing --&gt;', test_dataset.cardinality())
print('Batches for validating --&gt;', val_ds.cardinality())

model = tf.keras.Sequential([
  tf.keras.layers.Rescaling(1./255, input_shape=(180, 180, 3)),
  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(5)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

epochs=1
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=1
)
</code></pre>",0.3437598583,0.7083727663,1
"<p>I created dataset from csv file with dataset = tf.data.experimental.make_csv_dataset() function but My dataset has categorical and numeric features.</p>
<pre><code>dataset=
color  price weight
red    120    1.2
blue    80     2.0
green   90     3
</code></pre>
<p>Question 1:
The question is how can I  modify  only single feature, for example weight +2, to:</p>
<pre><code>dataset=
color  price weight
red    120    3.2
blue    80     4.0
green   90     5
</code></pre>
<p>I try to do something like:</p>
<pre><code>dataset = dataset.apply(lambda x: x['weight']+2)
</code></pre>
<p>but the error is: &quot;TypeError: 'FilterDataset' object is not subscriptable&quot;</p>
<p>Example from the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply</a> doesn't show it.</p>
<p>Question 2:
How can I remove single feature ? Is there any equivalent to pandas drop column?</p>
","['""""""TensorFlow: Add dimension (column) with constant value\\n\\nI have a Tensor of shape (-1,) which represents a list of indices. I want to create a Tensor of shape (-1,2). The first column should be the same as the list of indices, while the second column should be filled with a constant. Eg (let\\\'s say the constant is 6):\\n\\nindices = [4, 35, 230, 235] my_goal = [[4, 6], [35, 6], [230, 6], [235, 6]]\\n\\nWhat is the best way to do this? I was hoping a broadcasting-tf.concat would work, but tf.concat doesn\\\'t seem to support broadcasting. You can use tensorflow.pad. But first you have to make it a two dimensional tensor. indices = tf.constant([1,2,3,4]) indices = tf.expand_dims(tf, 1) # now you have a (4,1) tensor padding = [[0,0],[0,1]] # no padding before or after the first dimension # no padding before second dimension. Single-element padding # after the second dimension my_goal = tf.pad(indices, padding, constant_values=6)\\n\\nPadding has to be [n,2] tensor.\n\nIs there any equivalent to pandas drop column? You can remove features by only filtering the features that you want. This how you can modify only one feature:\\n\\nimport tensorflow as tf import pandas as pd df = pd.DataFrame(data={\\\'color\\\': [\\\'red\\\', \\\'blue\\\',\\\'green\\\'], \\\'price\\\': [120, 80, 90], \\\'weight\\\': [3.2, 4.0, 5]}) df.to_csv(\\\'data.csv\\\', index=False) dataset = tf.data.experimental.make_csv_dataset(\\\'/content/data.csv\\\', batch_size=1, num_epochs = 1, shuffle=False) dataset = dataset.map(lambda x: (x[\\\'color\\\'], x[\\\'price\\\'], x[\\\'weight\\\']+2)) for x in dataset: print(x[0], x[1], x[2])\\n\\ntf.Tensor([b\\\'red\\\'], shape=(1,), dtype=string) tf.Tensor([120], shape=(1,), dtype=int32) tf.Tensor([5.2], shape=(1,), dtype=float32) tf.Tensor([b\\\'blue\\\'], shape=(1,), dtype=string) tf.Tensor([80], shape=(1,), dtype=int32) tf.Tensor([6.], shape=(1,), dtype=float32) tf.Tensor([b\\\'green\\\'], shape=(1,), dtype=string) tf.Tensor([90], shape=(1,), dtype=int32) tf.Tensor([7.], shape=(1,), dtype=float32)""""""']",Success,"<pre><code>import tensorflow as tf
import pandas as pd

df = pd.DataFrame(data={'color': ['red', 'blue','green'], 'price': [120, 80, 90], 'weight': [3.2, 4.0, 5]})
df.to_csv('data.csv', index=False)

dataset = tf.data.experimental.make_csv_dataset('/content/data.csv', batch_size=1, num_epochs = 1, shuffle=False)
dataset = dataset.map(lambda x: (x['color'], x['price'], x['weight']+2))

for x in dataset:
  print(x[0], x[1], x[2])
</code></pre>
<pre><code>tf.Tensor([b'red'], shape=(1,), dtype=string) tf.Tensor([120], shape=(1,), dtype=int32) tf.Tensor([5.2], shape=(1,), dtype=float32)
tf.Tensor([b'blue'], shape=(1,), dtype=string) tf.Tensor([80], shape=(1,), dtype=int32) tf.Tensor([6.], shape=(1,), dtype=float32)
tf.Tensor([b'green'], shape=(1,), dtype=string) tf.Tensor([90], shape=(1,), dtype=int32) tf.Tensor([7.], shape=(1,), dtype=float32)
</code></pre>",0.5430838844,0.6723355375,1
"<p>In the official <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">tf.custom_gradient</a> documentation it shows how to define custom gradients for <code>log(1 + exp(x))</code></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.math.log(1 + e), grad
</code></pre>
<p>When <code>y = log(1 + exp(x))</code>, analytically the derivative comes out to be <code>dy/dx = (1 - 1 / (1 + exp(x)))</code>.</p>
<p>However in the code <code>def grad</code> says its <code>dy * (1 - 1 / (1 + exp(x)))</code>.
<code>dy/dx = dy * (1 - 1 / (1 + exp(x)))</code> is not a valid equation. While <code>dx = dy * (1 - 1 / (1 + exp(x)))</code> is wrong as it should be the reciprocal.</p>
<p>What does the <code>grad</code> function equate to?</p>
","['""""""Does that mean in the custom gradient function, I need to return the same results as what tf.gradients should give, of which each elements are summed partial derivatives of dy/dx? @NathanExplosion Yes, that sounds right. I have added a snippet that (I hope) demonstrates how tf.gradients and gradient functions relate to each other. I tried tf.gradients(c[0,0], a), it will return dc[0,0]/da. But if we defined the returned gradients as the summation of partial gradients, how could it derive an individual gradient? @NathanExplosion In that case the flow goes like this. You have a slice operation that gives you a scalar, so you start the gradient computation with the scalar 1 as dc[0,0]/dc[0,0]. Then you compute dc[0,0]/dc, which is a matrix shaped like c with the gradient of c[0,0] wrt each element - so it is a matrix, grad_c, with all 0 except a 1 in the first value of the first row. Then you can get dc[0,0]/da, which is (dc[0,0]/dc)*(dc/da). We saw it ended up being grad_c * b.T, so you get a matrix the size of a where the first row is the first column of b and all other rows are 0. \n\nDefining custom gradient as a class method in Tensorflow\\n\\nI need to define a method to be a custom gradient as follows:\\n\\nclass CustGradClass: def __init__(self): pass @tf.custom_gradient def f(self,x): fx = x def grad(dy): return dy * 1 return fx, grad\\n\\nI am getting the following error:\\n\\nValueError: Attempt to convert a value (<main.CustGradClass object at 0x12ed91710>) with an unsupported type () to a Tensor. The reason is the custom gradient accepts a function f(*x) where x is a sequence of Tensors. And the first argument being passed is the object itself i.e., self. From the documentation:\\n\\nf: function f(*x) that returns a tuple (y, grad_fn) where: x is a sequence of Tensor inputs to the function. y is a Tensor or sequence of Tensor outputs of applying TensorFlow operations in f to x. grad_fn is a function with the signature g(*grad_ys)\\n\\nHow do I make it work? Do I need to inherit some python tensorflow class? I am using tf version 1.12.0 and eager mode.\n\nAlso, this attached image describes the solution as expected by manually calulation\\n\\nIf I do not use the @tf.custom_gradient then the TensorFlow gives the desired solution as expected. My question is that how can I provide custom gradient for y=Ax? We know that dy/dx = A^T as shown in the above attachment which shows steps of calculation that matches the TensorFlow output. import tensorflow as tf #I want to write custom gradient for this function f1 def f1(A,x): y=tf.matmul(A,x,name=\\\'y\\\') return y #for y= Ax, the derivative is: dy/dx= transpose(A) @tf.custom_gradient def f2(A,x): y=f1(A,x) def grad(dzByDy): # dz/dy = 2y reaches here correctly. dzByDx=tf.matmul(A,dzByDy,transpose_a=True) return dzByDx return y,grad x= tf.constant([[1.],[0.]],name=\\\'x\\\') A= tf.constant([ [1., 2.], [3., 4.]],name=\\\'A\\\') y=f1(A,x) # This works as desired #y=f2(A,x) #This line gives Error z=tf.reduce_sum(y*y,name=\\\'z\\\') g=tf.gradients(ys=z,xs=x) with tf.Session() as sess: print sess.run(g)\\n\\nSince your function f2() has two inputs, you have to provide a gradient to flow back to each of them. The error you see:\\n\\nNum gradients 2 generated for op name: ""IdentityN"" [...] do not match num inputs 3\\n\\nis admittedly quite cryptic, though. Supposing you never want to calculate dy/dA, you can just return None, dzByDx. The code below (tested):\\n\\nimport tensorflow as tf #I want to write custom gradient for this function f1 def f1(A,x): y=tf.matmul(A,x,name=\\\'y\\\') return y #for y= Ax, the derivative is: dy/dx= transpose(A) @tf.custom_gradient def f2(A,x): y=f1(A,x) def grad(dzByDy): # dz/dy = 2y reaches here correctly. dzByDx=tf.matmul(A,dzByDy,transpose_a=True) return None, dzByDx return y,grad x= tf.constant([[1.],[0.]],name=\\\'x\\\') A= tf.constant([ [1., 2.], [3., 4.]],name=\\\'A\\\') #y=f1(A,x) # This works as desired y=f2(A,x) #This line gives Error z=tf.reduce_sum(y*y,name=\\\'z\\\') g=tf.gradients(ys=z,xs=x) with tf.Session() as sess: print sess.run( g )\\n\\n[array([[20.], [28.]], dtype=float32)]""""""']",Success,"<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def foo(x):
    tf.debugging.assert_rank(x, 0)

    def grad(dy_dx_upstream):
        dy_dx = 2 * x
        dy_dx_downstream = dy_dx * dy_dx_upstream
        tf.print(f'x={x}\tupstream={dy_dx_upstream}\tcurrent={dy_dx}\t\tdownstream={dy_dx_downstream}')
        return dy_dx_downstream
    
    y = x ** 2
    tf.print(f'x={x}\ty={y}')
    
    return y, grad


x = tf.constant(2.0, dtype=tf.float32)

with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    y = foo(foo(foo(x))) # y = x ** 8

tf.print(f'\nfinal dy/dx={tape.gradient(y, x)}')
</code></pre>",0.4671155414,0.6684621658,0
"<p>I am using <code>tf.cond</code> for controlling the flow of the Tensorflow graph. I went through the documentation and was able to implement <code>tf.cond</code> based branching successfully. But my concern is that while the graph is being loaded the value of the <code>bool</code> variable is checked and the branching decision is made at the initialization step itself. Any further changes in the <code>bool</code> is not tracked. Following is the MWE that better describes the problem:</p>

<pre class=""lang-py prettyprint-override""><code>def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    global foo
    if i &gt; 10:
        foo = False
    print(sess.run(x))    
</code></pre>

<p>This prints only <code>32</code>s. </p>

<p>I tried with <code>eager_execution</code> too with the following code:</p>

<pre class=""lang-py prettyprint-override""><code>tf.enable_eager_execution()
def funa():
    return tf.constant(32)

def funb():
    return tf.constant(21)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(x)
</code></pre>

<p>Still the same result.</p>

<p>So my question is how can I write code such that one part of the graph is chosen dynamically, based on the updates to the <code>bool</code> variable (if possible)? Thanks. I am using Tensorflow v1.14.</p>
","['""""""Confused by the behavior of `tf.cond`\\n\\n I need a conditional control flow in my graph. If pred is True, the graph should call an op that updates a variable and then returns it, otherwise it returns the variable unchanged. A simplified version is:\\n\\npred = tf.constant(True) x = tf.Variable([1]) assign_x_2 = tf.assign(x, [2]) def update_x_2(): with tf.control_dependencies([assign_x_2]): return tf.identity(x) y = tf.cond(pred, update_x_2, lambda: tf.identity(x)) with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval())\\n\\nHowever, I find that both pred=True and pred=False lead to the same result y=[2], which means the assign op is also called when update_x_2 is not selected by tf.cond. How to explain this?\n\nHow to add if condition in a TensorFlow graph? \\n\\n Let\'s say I have following code:\\n\\nx = tf.placeholder(""float32"", shape=[None, ins_size**2*3], name = ""x_input"") condition = tf.placeholder(""int32"", shape=[1, 1], name = ""condition"") W = tf.Variable(tf.zeros([ins_size**2*3,label_option]), name = ""weights"") b = tf.Variable(tf.zeros([label_option]), name = ""bias"") if condition > 0: y = tf.nn.softmax(tf.matmul(x, W) + b) else: y = tf.nn.softmax(tf.matmul(x, W) - b)\\n\\nWould the if statement work in the calculation (I do not think so)? If not, how can I add an if statement into the TensorFlow calculation graph? You\\\'re correct that the if statement doesn\\\'t work here, because the condition is evaluated at graph construction time, whereas presumably you want the condition to depend on the value fed to the placeholder at runtime. (In fact, it will always take the first branch, because condition > 0 evaluates to a Tensor, which is ""truthy"" in Python.)\\n\\nTo support conditional control flow, TensorFlow provides the tf.cond() operator, which evaluates one of two branches, depending on a boolean condition. To show you how to use it, I\\\'ll rewrite your program so that condition is a scalar tf.int32 value for simplicity:\\n\\nx = tf.placeholder(tf.float32, shape=[None, ins_size**2*3], name=""x_input"") condition = tf.placeholder(tf.int32, shape=[], name=""condition"") W = tf.Variable(tf.zeros([ins_size**2 * 3, label_option]), name=""weights"") b = tf.Variable(tf.zeros([label_option]), name=""bias"") y = tf.cond(condition > 0, lambda: tf.matmul(x, W) + b, lambda: tf.matmul(x, W) - b)\\n\\n\\n\\n 4\\n\\n@mrry Are both branches executed by default ? I have tf.cond(c, lambda x: train_op1, lambda x: train_op2) and both train_ops are executed at each execution of cond independently of the value of c. Am I doing something wrong? @PiotrDabkowski This is a sometimes surprising behavior of tf.cond(), which is touched upon in the docs. In short, you need to create the ops that you want to run conditionally inside the respective lambdas. Everything that you create outside the lambdas but refer to in either branch will execute in both cases. @mrry Wow, that\\\'s rather unexpected :) Thanks for the answer, defining ops inside functions solved the problem. Is the condition/( application of the logic) element-wise? TF 2.0 introduces a feature called AutoGraph which lets you JIT compile python code into Graph executions. This means you can use python control flow statements (yes, this includes if statements). From the docs,\\n\\nAutoGraph supports common Python statements like while, for, if, break, continue and return, with support for nesting. That means you can use Tensor expressions in the condition of while and if statements, or iterate over a Tensor in a for loop. You will need to define a function implementing your logic and annotate it with tf.function. Here is a modified example from the documentation:\\n\\nimport tensorflow as tf @tf.function def sum_even(items): s = 0 for c in items: if tf.equal(c % 2, 0): s += c return s sum_even(tf.constant([10, 12, 15, 20])) # <tf.Tensor: id=1146, shape=(), dtype=int32, numpy=42>\\n\\n 2\\n\\nWhy are you using tf.equal()? Shouldn\\\'t you be able to use == and let AutoGraph compile it automatically?""""""']",Success,"<pre><code>import tensorflow as tf

def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
foo_p = tf.placeholder(tf.bool)

sess = tf.Session()

x = tf.cond(foo_p, lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(sess.run(x, {foo_p:foo}))
</code></pre>",0.7561432432,0.6912396394,0.6
"<p>I can easily use tf.map_fn when the function has one output:</p>
<pre><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    return x[0]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p><strong>output:</strong></p>
<pre><code>tf.Tensor([1. 4.], shape=(2,), dtype=float32)
</code></pre>
<p>But, when the function has two outputs:</p>
<pre><code>def my_fun(x):
    return [x[0],x[1]]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p>I get an error. Not sure what is going on. I read the information about tf.map_fn in here <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/map_fn</a>, but not sure how to fix this:</p>
<p>map_fn also supports functions with multi-arity inputs and outputs:</p>
<p><em>If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems).
If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures.</em></p>
<p><strong>Output:</strong></p>
<pre><code>~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    317     _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types,
--&gt; 318                                            expand_composites)
    319   except (ValueError, TypeError) as e:

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-36-5b11c7fef461&gt; in &lt;module&gt;
      5     return [x[0],x[1]]
      6 
----&gt; 7 print(tf.map_fn(my_fun,tensaki))

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)
    266         back_prop=back_prop,
    267         swap_memory=swap_memory,
--&gt; 268         maximum_iterations=n)
    269     results_flat = [r.stack() for r in r_a]
    270 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   2712                                               list(loop_vars))
   2713       while cond(*loop_vars):
-&gt; 2714         loop_vars = body(*loop_vars)
   2715         if try_to_pack and not isinstance(loop_vars, (list, _basetuple)):
   2716           packed = True

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in &lt;lambda&gt;(i, lv)
   2703         cond = lambda i, lv: (  # pylint: disable=g-long-lambda
   2704             math_ops.logical_and(i &lt; maximum_iterations, orig_cond(*lv)))
-&gt; 2705         body = lambda i, lv: (i + 1, orig_body(*lv))
   2706       try_to_pack = False
   2707 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in compute(i, tas)
    256       packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])
    257       packed_fn_values = fn(packed_values)
--&gt; 258       nest.assert_same_structure(dtype or elems, packed_fn_values)
    259       flat_fn_values = output_flatten(packed_fn_values)
    260       tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    323                   &quot;Entire first structure:\n%s\n&quot;
    324                   &quot;Entire second structure:\n%s&quot;
--&gt; 325                   % (str(e), str1, str2))
    326 
    327 

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not
Entire first structure:
.
Entire second structure:
[., .]```
</code></pre>
","['""""""Can I apply tf.map_fn(...) to multiple inputs/outputs? \\n\\na = tf.constant([[1,2,3],[4,5,6]]) b = tf.constant([True, False], dtype=tf.bool) a.eval() array([[1, 2, 3], [4, 5, 6]], dtype=int32) b.eval() array([ True, False], dtype=bool)\\n\\nI want to apply a functions to the inputs above, a, and b using tf.map_fn. It will input both [1,2,3], and True and output similar values. Let\\\'s say out function is simply the identity: lambda(x,y): x,y so, given an input of [1,2,3], True, it will output those identical tensors. I know how to use tf.map_fn(...) with one variable, but not with two. And in this case I have mixed data types (int32 and bool) so I can\\\'t simply concatenate the tensors and split them after the call. Can I use tf.map_fn(...) with multiple inputs/outputs of different data types? David ParksDavid Parks\\n\\n\\n\\nFigured it out. You have to define the data types for each tensor in dtype for each of the different tensors, then you can pass the tensors as a tuple, your map function receives a tuple of inputs, and map_fn returns back back a tuple. a = tf.constant([[1,2,3],[4,5,6]]) b = tf.constant([True, False], dtype=tf.bool) c = tf.map_fn(lambda x: (x[0], x[1]), (a,b), dtype=(tf.int32, tf.bool)) c[0].eval() array([[1, 2, 3], [4, 5, 6]], dtype=int32) c[1].eval() array([ True, False], dtype=bool)\\n\\nBe warned that if you use this the processing will be done in CPU, not on the GPU. This can be especially detrimental to speed when training on a GPU.\n\nUsing tf.map_fn when the function has multiple outputs\\n\\nI can easily use tf.map_fn when the function has one output:\\n\\nimport tensorflow as tf tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]]) def my_fun(x): return x[0] print(tf.map_fn(my_fun,tensaki))\\n\\ntf.Tensor([1. 4.], shape=(2,), dtype=float32)\\n\\nBut, when the function has two outputs:\\n\\ndef my_fun(x): return [x[0],x[1]] print(tf.map_fn(my_fun,tensaki))\\n\\nI get an error. Not sure what is going on. I read the information about tf.map_fn in here https://www.tensorflow.org/api_docs/python/tf/map_fn, but not sure how to fix this:\\n\\nmap_fn also supports functions with multi-arity inputs and outputs:\\n\\nIf elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 <= i < num_elems). If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures. ~Users\\\\user2\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\tensorflow_core\\\\python\\\\util\\\\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites) 317 _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types, --> 318 expand_composites) 319 except (ValueError, TypeError) as e: ValueError: The two structures don\\\'t have the same nested structure. First structure: type=DType str=<dtype: \\\'float32\\\'> Second structure: type=list str=[<tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0>] More specifically: Substructure ""type=list str=[<tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0>]"" is a sequence, while substructure ""type=DType str=<dtype: \\\'float32\\\'>"" is not During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) <ipython-input-36-5b11c7fef461> in <module> 5 return [x[0],x[1]] 6 ----> 7 print(tf.map_fn(my_fun,tensaki)) ~Users\\\\user2\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\tensorflow_core\\\\python\\\\ops\\\\map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name) 266 back_prop=back_prop, 267 swap_memory=swap_memory, --> 268 maximum_iterations=n) 269 results_flat = [r.stack() for r in r_a] 270 ~Users\\\\user2\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\tensorflow_core\\\\python\\\\ops\\\\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure) 2712 list(loop_vars)) 2713 while cond(*loop_vars): -> 2714 loop_vars = body(*loop_vars) 2715 if try_to_pack and not isinstance(loop_vars, (list, _basetuple)): 2716 packed = True ~Users\\\\user2\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\tensorflow_core\\\\python\\\\ops\\\\control_flow_ops.py in <lambda>(i, lv) 2703 cond = lambda i, lv: ( # pylint: disable=g-long-lambda 2704 math_ops.logical_and(i < maximum_iterations, orig_cond(*lv))) -> 2705 body = lambda i, lv: (i + 1, orig_body(*lv)) 2706 try_to_pack = False 2707 ~Users\\\\user2\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\tensorflow_core\\\\python\\\\ops\\\\map_fn.py in compute(i, tas) 256 packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta]) 257 packed_fn_values = fn(packed_values) --> 258 nest.assert_same_structure(dtype or elems, packed_fn_values) 259 flat_fn_values = output_flatten(packed_fn_values) 260 tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)] ~Users\\\\user2\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\tensorflow_core\\\\python\\\\util\\\\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites) 323 ""Entire first structure:\\\\n%s\\\\n"" 324 ""Entire second structure:\\\\n%s"" --> 325 % (str(e), str1, str2)) 326 327 ValueError: The two structures don\\\'t have the same nested structure. First structure: type=DType str=<dtype: \\\'float32\\\'> Second structure: type=list str=[<tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0>] More specifically: Substructure ""type=list str=[<tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0>]"" is a sequence, while substructure ""type=DType str=<dtype: \\\'float32\\\'>"" is not Entire first structure: . Entire second structure: [., .]```\\n\\nYou should make sure you are returning a tensor. Maybe concatenate or stack the list of values:\\n\\nimport tensorflow as tf tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]]) def my_fun(x): x = tf.stack([x[0], x[1]], axis=0) return x print(tf.map_fn(my_fun,tensaki))\\n\\n""""""']",Success,"<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    x = tf.stack([x[0], x[1]], axis=0)
    return x

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<pre><code>tf.Tensor(
[[1. 2.]
 [4. 5.]], shape=(2, 2), dtype=float32)
</code></pre>",0.2377695565,0.6783509532,0.8
"<p>I'm trying to understand how <strong>tf.data.Dataset</strong> works.</p>
<p>It says on the documentation that <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""nofollow noreferrer"">take</a> returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch):</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds

# Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=True)

# Build your input pipeline
ds = ds.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)
# ...
</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([2 0 6 6 8 8 6 0 3 4 8 7 5 2 5 7 8 7 1 1 1 8 6 4 0 4 3 2 4 2 1 9], shape=(32,), dtype=int64)
</code></pre>
<p>However, iterating over it again, gives different labels: (continuation of last code)</p>
<pre class=""lang-py prettyprint-override""><code>for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([7 3 5 6 3 1 7 9 6 1 9 3 9 8 6 7 7 1 9 7 5 2 0 7 8 1 7 8 7 0 5 0], shape=(32,), dtype=int64)
tf.Tensor([1 3 6 1 8 8 0 4 1 3 2 9 5 3 8 7 4 2 1 8 1 0 8 5 4 5 6 7 3 4 4 1], shape=(32,), dtype=int64)
</code></pre>
<p>Shouldn't the labels be the same, given that the dataset is the same?</p>
","['""""""perhaps one could imagine a scenario where the shuffling is defined once and is used for every iteration. @jakub ah yes it makes sense from that perspective\\n\\nThis is because the data files are shuffled and the dataset is shuffled with dataset.shuffle(). With dataset.shuffle(), the data will be shuffled in a different way on each iteration by default. One can remove shuffle_files=True and set the argument reshuffle_each_iteration=False to prevent reshuffling on different iterations. The .take() function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them. # Construct a tf.data.Dataset ds = tfds.load(\\\'mnist\\\', split=\\\'train\\\', shuffle_files=False) # Build your input pipeline ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE) single_batch_dataset = ds.take(1) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label)\\n\\ntf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64) tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)\\n\\n\\n\\n""""""']",Failed,"<pre class=""lang-py prettyprint-override""><code># Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=False)

# Build your input pipeline
ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
    
for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
</code></pre>",0.4502753388,0.6761013553,0.8
"<p>I still don't after having read documentation about <code>tf.keras.Model.fit</code> and <code>tf.data.Dataset</code>, when passing <code>tf.data.Dataset</code> to fit function, should I call <code>repeat</code> and <code>batch</code> on the dataset object or should I provide the <code>batch_size</code> and <code>epochs</code> arguments to fit instead? or both? Should I apply the same treatment to the validation set?</p>

<p>And while I'm here, can I <code>shuffle</code> the dataset before the <code>fit</code>? (seems like it's an obvious yes)
If so, before, after calling <code>Dataset.batch</code> and <code>Dataset.repeat</code> (if calling them)?</p>

<p><strong>Edit:</strong> When using <code>batch_size</code> argument, and without having called <code>Dataset.batch(batch_size)</code> previously, I am getting the following error:</p>

<pre><code>ValueError: The `batch_size` argument must not be specified for the given input type.
Received input: &lt;MapDataset shapes: ((&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;), (&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;)), 
types: ((tf.float32, tf.float32, tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32))&gt;, 
batch_size: 1
</code></pre>

<p>Thanks</p>
","['""""""How does Model.fit() method\\\'s shuffle deals with Batches when using a tf.data.Dataset? \\n\\nI am using tensorflow 2. When using the Model.fit() method with a tf.data.Dataset, the argument \\\'batch_size\\\' is ignored. Thus to train my model on batches, I have to first change my dataset of samples into a dataset of batches of samples by calling tf.data.Dataset.batch(batch_size). Then, after reading the documentation, I don\\\'t understand clearly how the .fit() method will shuffle my dataset at each epoch. Since my dataset is a dataset of batches, will it shuffle the batches among each other (the batches remain unchanged) ? Or will it shuffle all the samples and then regroup them into new batches (which is the desired behaviour) ? Thanks a lot for your help. The shuffle parameter has no effect on the fit function when using the tf.data.Dataset API. If we read the documentation (emphasis is mine) :\\n\\nshuffle: Boolean (whether to shuffle the training data before each epoch) or str (for \\\'batch\\\'). This argument is ignored when x is a generator. \\\'batch\\\' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None. It\\\'s not super clear, but we can have a hint that the shuffle argument will be ignored when using a tf.data.Dataset, as it behave like a generator. To be certain, lets dive in the code.\n\nKeras model.fit() with tf.dataset API + validation_data\\n\\nSo I have got my keras model to work with a tf.Dataset through the following code:\\n\\n# Initialize batch generators(returns tf.Dataset) batch_train = build_features.get_train_batches(batch_size=batch_size) # Create TensorFlow Iterator object iterator = batch_train.make_one_shot_iterator() dataset_inputs, dataset_labels = iterator.get_next() # Create Model logits = ...(some layers) keras.models.Model(inputs=dataset_inputs, outputs=logits) # Train network model.compile(optimizer=train_opt, loss=model_loss, target_tensors=[dataset_labels]) model.fit(epochs=epochs, steps_per_epoch=num_batches, callbacks=callbacks, verbose=1)\\n\\nhowever when I try to pass validation_data parameter to the model. fit it tells me that I cannot use it with the generator. Is there a way to use validation while using tf.Dataset\\n\\nfor example in tensorflow I could do the following:\\n\\n# initialize batch generators batch_train = build_features.get_train_batches(batch_size=batch_size) batch_valid = build_features.get_valid_batches(batch_size=batch_size) # create TensorFlow Iterator object iterator = tf.data.Iterator.from_structure(batch_train.output_types, batch_train.output_shapes) # create two initialization ops to switch between the datasets init_op_train = iterator.make_initializer(batch_train) init_op_valid = iterator.make_initializer(batch_valid)\\n\\nthen just use sess.run(init_op_train) and sess.run(init_op_valid) to switch between the datasets\\n\\nI tried implementing a callback that does just that (switch to validation set, predict and back) but it tells me I can\\\'t use model.predict in a callback\\n\\ncan someone help me get validation working with Keras+Tf.Dataset\\n\\nedit: incorporate answer into the code\\n\\nso FINALLY what worked for me, thanks to the selected answer is:\\n\\n# Initialize batch generators(returns tf.Dataset) batch_train = # returns tf.Dataset batch_valid = # returns tf.Dataset # Create TensorFlow Iterator object and wrap it in a generator itr_train = make_iterator(batch_train) itr_valid = make_iterator(batch_train) # Create Model logits = # the keras model keras.models.Model(inputs=dataset_inputs, outputs=logits) # Train network model.compile(optimizer=train_opt, loss=model_loss, target_tensors=[dataset_labels]) model.fit_generator( generator=itr_train, validation_data=itr_valid, validation_steps=batch_size, epochs=epochs, steps_per_epoch=num_batches, callbacks=cbs, verbose=1, workers=0) def make_iterator(dataset): iterator = dataset.make_one_shot_iterator() next_val = iterator.get_next() with K.get_session().as_default() as sess: while True: *inputs, labels = sess.run(next_val) yield inputs, labels\\n\\nThis doesn\\\'t introduce any overhead\\n\\nAfter your change, how do you get dataset_inputs into model? I\\\'m not getting how line keras.models.Model(inputs=dataset_inputs, outputs=logits), and i\\\'m assuming this is the contents of the ""model"" variable, could you please complete the code, i have the exact same problem but can\'t seem to know how to apply your code, thanks in advance\\n\\n@mark rofail, I believe this line is incorrect and should receive batch_valid: itr_valid = make_iterator(batch_train)\\n\\n I solved the problem by using fit_genertor. I found the solution here. I applied @Dat-Nguyen\\\'s solution. You need simply to create two iterators, one for training and one for validation and then create your own generator where you will extract batches from the dataset and provide the data in form of (batch_data, batch_labels) . Finally in model.fit_generator you will pass the train_generator and validation_generator. \\n\\nso I have to wrap tensorflow iterators in a python generator like: iterator = ds.make_one_shot_iterator() while True: next_val = iterator.get_next() yield sess.run(next_val)\\n\\n.\n\nThere is described all the details of DataSet API. Your question is about iterating over the data several times. Here are two solutions for that:\\n\\nIterating all epochs at once, no information about end of individual epochs\\n\\nimport tensorflow as tf epoch = 10 dataset = tf.data.Dataset.range(100) dataset = dataset.repeat(epoch) iterator = dataset.make_one_shot_iterator() next_element = iterator.get_next() sess = tf.Session() num_batch = 0 j = 0 while True: try: value = sess.run(next_element) assert j == value j += 1 num_batch += 1 if j > 99: # new epoch j = 0 except tf.errors.OutOfRangeError: break print (""Num Batch: "", num_batch)\\n\\nSecond option inform you about ending each of epoch, so you can ex. check validation loss:\\n\\nimport tensorflow as tf epoch = 10 dataset = tf.data.Dataset.range(100) iterator = dataset.make_initializable_iterator() next_element = iterator.get_next() sess = tf.Session() num_batch = 0 for e in range(epoch): print (""Epoch: "", e) j = 0 sess.run(iterator.initializer) while True: try: value = sess.run(next_element) assert j == value j += 1 num_batch += 1 except tf.errors.OutOfRangeError: break print (""Num Batch: "", num_batch)\\n\\n\\n\\nIf your tensorflow version is 1.3+, I recommend the high-level API tf.train.MonitoredTrainingSession. The sess created by this API can automatically detect tf.errors.OutOfRangeError with sess.should_stop(). For most of training situations, you need to shuffle data and get a batch each step, I have added these in the following code. import tensorflow as tf epoch = 10 dataset = tf.data.Dataset.range(100) dataset = dataset.shuffle(buffer_size=100) # comment this line if you don\\\'t want to shuffle data dataset = dataset.batch(batch_size=32) # batch_size=1 if you want to get only one element per step dataset = dataset.repeat(epoch) iterator = dataset.make_one_shot_iterator() next_element = iterator.get_next() num_batch = 0 with tf.train.MonitoredTrainingSession() as sess: while not sess.should_stop(): value = sess.run(next_element) num_batch += 1 print(""Num Batch: "", num_batch)\\n\\n\\n\\n\\n\\nwhile True: try: print(sess.run(value)) except tf.errors.OutOfRangeError: break\\n\\nWhenever the dataset iterator reaches the end of the data, it will raise tf.errors.OutOfRangeError, you can catch it with except and start the dataset from the beginning. \\n\\nYou should explain your code or include comments as well\\n\\n\\n\\nSimilar to Toms answer, for tensorflow 2+, you can use the following high-level API calls (the code proposed in his answer is deprecated in tensorflow 2+):\\n\\nepoch = 10 batch_size = 32 dataset = tf.data.Dataset.range(100) dataset = dataset.shuffle(buffer_size=100) # comment this line if you don\\\'t want to shuffle data dataset = dataset.batch(batch_size=batch_size) dataset = dataset.repeat(epoch) num_batch = 0 for batch in dataset: num_batch += 1 print(""Num Batch: "", num_batch)\\n\\nA helpful call to track progress is the total number of batches that will be iterated over (to be used after the batch and the repeat calls):\\n\\nnum_batches = tf.data.experimental.cardinality(dataset)\\n\\nNote that currently (tensorflow 2.1), the cardinality method is still experimental. """"""']",Failed,"<pre><code>batch_size = 32
ds = tf.Dataset()
ds = ds.shuffle(len_ds)
train_ds = ds.take(0.8*len_ds)
train_ds = train_ds.repeat().batch(batch_size)
validation_ds = ds.skip(0.8*len_ds)
validation_ds = train_ds.repeat().batch(batch_size)
model.fit(train_ds,
          steps_per_epoch = len_train_ds // batch_size,
          validation_data = validation_ds,
          validation_steps = len_validation_ds // batch_size,
          epochs = 5)
</code></pre>",0.4199659757,0.6798639029,0.7
"<p>I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop()</code></a>.</p>

<p>My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by</p>

<pre><code>import numpy as np
import tensorflow as tf
IMAGE_SHAPE = [960, 720]
CROP_SHAPE = [320, 240]
max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
</code></pre>

<p>and the condition is</p>

<pre><code>cond = tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)
</code></pre>

<p>Going over the documentation and examples of <code>tf.while_loop(cond, body, loop_vars, ...)</code>, what I understand is that both <code>cond</code> and <code>body</code> should take the same arguments given in <code>loop_vars</code>.
I don't see how I can have <code>cond</code> depend on <code>img_crop</code> which would be calculated inside <code>body</code>, and isn't provided in <code>loop_vars</code>.</p>

<p>I could equivalently compute <code>cond</code> using <code>crop_begin_index</code> without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem.</p>

<p>Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use <code>tf.while_loop()</code>?</p>
","['""""""TensorFlow while loop with condition dependent on body\\n\\n I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don\\\'t know how to accomplish this with tf.while_loop(). My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by\\n\\nimport numpy as np import tensorflow as tf IMAGE_SHAPE = [960, 720] CROP_SHAPE = [320, 240] max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE) crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index) img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])\\n\\nand the condition is\\n\\ncond = tf.count_nonzero(img_crop > 0) > 0.5 * tf.size(img_crop)\\n\\nGoing over the documentation and examples of tf.while_loop(cond, body, loop_vars, ...), what I understand is that both cond and body should take the same arguments given in loop_vars. I don\\\'t see how I can have cond depend on img_crop which would be calculated inside body, and isn\\\'t provided in loop_vars. I could equivalently compute cond using crop_begin_index without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem. Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use tf.while_loop()? The arguments that are passed on to the condition function are the arguments returned from your body function. So you just have to return that value that you want to base your condition on in the body function, then carry out the condition on that value in your cond function. Something like,\\n\\ndef body(image_shape, crop_shape, img_crop): max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE) crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index) img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1]) return (image_shape, crop_shape, img_crop) def cond(image_shape, crop_shape, img_crop): return tf.count_nonzero(img_crop > 0) > 0.5 * tf.size(img_crop) image_shape, crop_shape, img_crop = tf.while_loop(cond=cond, body=body, loop_vars=([960, 720], [320, 240], img_crop))\\n\\nDon\\\'t have access to an interpreter right now, so there might be some syntax problems there, but something like that. Also, if I recall correctly, the body and the condition need to be pure functions, you cannot alter the outer state from within the functions. Also note, you\\\'ll need to specify some initial value for img_crop in the loop vars. Moreover, by default, tf.while_loop expects the shapes of all the loop_vars to remain the same across all loop runs. You can modify this through the shape_invariants argument. \\n\\nGot it. I was also able to use just crop_begin_index as a variable like I wrote so the loop is more concise. \n\nCan anyone give me few more complex example? Also, in such case that if the future computation is based on the tensor output (ex: the RNN stop based on the output criterion), which is very common case. Is there an elegant way or better way instead of dynamic graph?. I\\\'m facing a problem where the gradients are None while implementing SimpleRNN using subclassing. I\\\'m wondering whether I need symbolic loop or can I manage without it? What is stopping you from adding more functionality to the body? You can build whatever complex computational graph you like in the body and take whatever inputs you like from the enclosing graph. Also, outside of the loop, you can then do whatever you want with whatever outputs you return. As you can see from the amount of \\\'whatevers\\\', TensorFlow\\\'s control flow primitives were built with much generality in mind. Below is another \\\'simple\\\' example, in case it helps. import tensorflow as tf import numpy as np def body(x): a = tf.random_uniform(shape=[2, 2], dtype=tf.int32, maxval=100) b = tf.constant(np.array([[1, 2], [3, 4]]), dtype=tf.int32) c = a + b return tf.nn.relu(x + c) def condition(x): return tf.reduce_sum(x) < 100 x = tf.Variable(tf.constant(0, shape=[2, 2])) with tf.Session(): tf.global_variables_initializer().run() result = tf.while_loop(condition, body, [x]) print(result.eval())\\n\\n My problem is the condition I need is calculated after run the body once. So it is like I need to use the return value of body as a parameter for condition. What you want is precisely what happens. The loop is while(condition(tensors)) { tensors = body(tensors); }, so the tensors you pass are updated to the tensors returned by the body each time, and then those updated tensors are passed to condition. The only time condition is called before body is the very first time, before the body of the pseudo-code loop above is entered. However, in that case, it\\\'s just about initializing the tensors you pass in loop_vars correctly. For example, you could pass the result of body as the loop_vars tensors to while_loop. oh, with loop_vars I refer to the function definition of while_loop, which is while_loop(condition, body, loop_vars) (its the tensors that are passed to condition and body)""""""']",Failed,"<pre><code>def body(image_shape, crop_shape, img_crop):
    max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
    crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
    img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
    return (image_shape, crop_shape, img_crop)

def cond(image_shape, crop_shape, img_crop):
    return tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)

image_shape, crop_shape, img_crop = tf.while_loop(cond=cond, body=body, loop_vars=([960, 720], [320, 240], img_crop))
</code></pre>",0.231179664,0.6751983138,0.4