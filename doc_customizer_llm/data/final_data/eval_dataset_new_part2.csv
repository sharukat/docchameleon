QuestionId,QuestionAPI,IssueType,Title,Question,GroundTruth,GT_Code,yt_queries,so_queries,yt_urls,yt_transcripts,so_urls,contexts,generated_explanation
56047272,tf.constant,example required,Explicit vs implicit type definition in TensorFlow,"<p>I'm just beginning to learn TensorFlow. Quoting from the <a href=""https://www.tensorflow.org/guide/low_level_intro#graph"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Let's build a simple computational graph. The most basic operation is a constant. The Python function that builds the operation takes a tensor value as input. The resulting operation takes no inputs. When run, it outputs the value that was passed to the constructor. We can create two floating point constants a and b as follows:</p>
</blockquote>

<pre><code>a = tf.constant(3.0, dtype=tf.float32)
b = tf.constant(4.0) # also tf.float32 implicitly
total = a + b
print(a)
print(b)
print(total)
</code></pre>

<p>The second constant is implicitly typed as a float32. Is that based on the explicit typing of the first constant? And does that imply that the first <code>dtype</code> is required? <a href=""https://www.tensorflow.org/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">tf.constant documentation</a> would imply that it does not:</p>

<blockquote>
  <p>If the argument dtype is not specified, then the type is inferred from the type of <code>value</code>.</p>
</blockquote>

<p>But then it would be unnecessary to explicitly type the 3.0 constant above.</p>

<p>I'm just looking for some clarification on this, since, like I said, I'm just starting out.</p>
","<blockquote>
  <p>But then it would be unnecessary to explicitly type the 3.0 constant
  above.</p>
</blockquote>

<p>Absolutely correct. </p>

<pre><code>a = tf.constant(3.0, dtype=tf.float32)
</code></pre>

<p>is equivalent to:</p>

<pre><code>a = tf.constant(3.0)
</code></pre>

<p>The documentation is just demonstrating the different overloads. We might choose to explicitly provide the type if we want a different numerical precision (or even just to aid human readability) but if you want the default data type TF infers, then it's entirely unnecessary.</p>
",,"['TensorFlow tf.constant dtype inference', 'TensorFlow tf.constant documentation', 'TensorFlow constant type inference example', 'TensorFlow dtype specification necessity', 'TensorFlow beginner tutorial on constants and dtype']","['Is it necessary to explicitly specify the dtype for tf.constant in TensorFlow?', 'Does the dtype of one tf.constant affect the dtype of another tf.constant in the same computational graph?', 'What is the default dtype for tf.constant if dtype is not specified?', 'Why would one explicitly specify the dtype for tf.constant if it can be inferred from the value?']","{'https://www.youtube.com/watch?v=Jn5sQjYi1FU', 'https://www.youtube.com/watch?v=tPYj3fFJGjk'}",,"{'https://stackoverflow.com/questions/61059725/why-does-tf-constant-give-a-dtype-error-if-we-pass-in-a-tensor', 'https://stackoverflow.com/questions/44880564/tf-variable-vs-tf-constant-in-tensorflow'}","['""""""Why does tf.constant give a dtype error if we pass in a tensor? \n\na = tf.range(10) b = tf.constant(a, dtype=tf.float32)\n\ngives the following error:\n\nTypeError: Expected tensor with type tf.float32 not tf.int32\n\nAlthough from the documentation, setting dtype means that tf.constant is supposed to cast a to the specified data type. So I don\'t see why this should give a type error. a = np.arange(10) b = tf.constant(a, dtype=tf.float32)\n\ndoes not give an error. So actually, I\'m mainly wondering about what\'s happening under the hood here.""""""', '""""""If you look at the source here, you will see that EagerTensor gets a special treatment. Basically, if the dtype of an EagerTensor doesn\'t match the new dtype, an error is raised. Here, tf.range() produces an EagerTensor. I\'m not sure why the special treatment for EagerTensors though. Could be a performance related restriction. """"""']","### Explanation of Explicit vs Implicit Type Definition in TensorFlow

In TensorFlow, when creating constants using `tf.constant`, the data type (`dtype`) of the constant can either be explicitly specified or implicitly inferred. The `dtype` argument is optional. If it is not provided, TensorFlow will infer the type from the value provided.

In the example given, `a = tf.constant(3.0, dtype=tf.float32)` explicitly sets the data type of the constant `a` to `tf.float32`. On the other hand, `b = tf.constant(4.0)` does not explicitly specify the `dtype`. In this case, TensorFlow infers the data type from the value `4.0`, which is a floating-point number, and thus `b` is also of type `tf.float32`.

The explicit specification of `dtype` is not required for the second constant `b` because TensorFlow can infer the type from the value. The documentation states that if the `dtype` is not specified, it will be inferred from the type of `value`. This means that the explicit typing of the first constant does not influence the typing of the second constant. Each constant's type is determined independently based on the provided value unless explicitly specified otherwise.

The confusion might arise from the assumption that the explicit typing of one constant affects the typing of another, which is not the case. The explicit `dtype` is only necessary if you want to ensure a specific type for a constant, especially when the value could be interpreted in multiple ways (e.g., integers vs. floating-point numbers).

In summary, the explicit `dtype` is not required if the type can be correctly inferred from the value. The explicit typing of one constant does not influence the implicit typing of another; each constant's type is determined based on its own value unless explicitly specified."
71149271,tf.data.Dataset,example required,"How to remove single feature from tensorflow dataset, how to use apply on single feture?","<p>I created dataset from csv file with dataset = tf.data.experimental.make_csv_dataset() function but My dataset has categorical and numeric features.</p>
<pre><code>dataset=
color  price weight
red    120    1.2
blue    80     2.0
green   90     3
</code></pre>
<p>Question 1:
The question is how can I  modify  only single feature, for example weight +2, to:</p>
<pre><code>dataset=
color  price weight
red    120    3.2
blue    80     4.0
green   90     5
</code></pre>
<p>I try to do something like:</p>
<pre><code>dataset = dataset.apply(lambda x: x['weight']+2)
</code></pre>
<p>but the error is: &quot;TypeError: 'FilterDataset' object is not subscriptable&quot;</p>
<p>Example from the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply</a> doesn't show it.</p>
<p>Question 2:
How can I remove single feature ? Is there any equivalent to pandas drop column?</p>
","<p>You can remove features by only filtering the features that you want. This how you can modify only one feature:</p>
<pre><code>import tensorflow as tf
import pandas as pd

df = pd.DataFrame(data={'color': ['red', 'blue','green'], 'price': [120, 80, 90], 'weight': [3.2, 4.0, 5]})
df.to_csv('data.csv', index=False)

dataset = tf.data.experimental.make_csv_dataset('/content/data.csv', batch_size=1, num_epochs = 1, shuffle=False)
dataset = dataset.map(lambda x: (x['color'], x['price'], x['weight']+2))

for x in dataset:
  print(x[0], x[1], x[2])
</code></pre>
<pre><code>tf.Tensor([b'red'], shape=(1,), dtype=string) tf.Tensor([120], shape=(1,), dtype=int32) tf.Tensor([5.2], shape=(1,), dtype=float32)
tf.Tensor([b'blue'], shape=(1,), dtype=string) tf.Tensor([80], shape=(1,), dtype=int32) tf.Tensor([6.], shape=(1,), dtype=float32)
tf.Tensor([b'green'], shape=(1,), dtype=string) tf.Tensor([90], shape=(1,), dtype=int32) tf.Tensor([7.], shape=(1,), dtype=float32)
</code></pre>
","<pre><code>import tensorflow as tf
import pandas as pd

df = pd.DataFrame(data={'color': ['red', 'blue','green'], 'price': [120, 80, 90], 'weight': [3.2, 4.0, 5]})
df.to_csv('data.csv', index=False)

dataset = tf.data.experimental.make_csv_dataset('/content/data.csv', batch_size=1, num_epochs = 1, shuffle=False)
dataset = dataset.map(lambda x: (x['color'], x['price'], x['weight']+2))

for x in dataset:
  print(x[0], x[1], x[2])
</code></pre>
<pre><code>tf.Tensor([b'red'], shape=(1,), dtype=string) tf.Tensor([120], shape=(1,), dtype=int32) tf.Tensor([5.2], shape=(1,), dtype=float32)
tf.Tensor([b'blue'], shape=(1,), dtype=string) tf.Tensor([80], shape=(1,), dtype=int32) tf.Tensor([6.], shape=(1,), dtype=float32)
tf.Tensor([b'green'], shape=(1,), dtype=string) tf.Tensor([90], shape=(1,), dtype=int32) tf.Tensor([7.], shape=(1,), dtype=float32)
</code></pre>","['How to modify a single feature in a TensorFlow dataset created from a CSV file?', 'How to apply transformations to specific columns in a TensorFlow dataset?', 'How to add a constant value to a numeric feature in a TensorFlow dataset?', 'How to remove a single feature from a TensorFlow dataset?', 'Is there an equivalent to pandas drop column in TensorFlow dataset API?', 'How to use tf.data.Dataset.map to modify specific columns?']","['How to modify a single feature in a TensorFlow dataset created from a CSV file?', 'How to add a constant value to a specific column in a TensorFlow dataset?', 'How to remove a single feature from a TensorFlow dataset?', 'Is there an equivalent to pandas drop column in TensorFlow dataset?', 'How to apply transformations to specific columns in a TensorFlow dataset?']",set(),[],"{'https://stackoverflow.com/questions/52787372/tensorflow-add-dimension-column-with-constant-value', 'https://stackoverflow.com/questions/71149271/how-to-remove-single-feature-from-tensorflow-dataset-how-to-use-apply-on-single'}","['""""""TensorFlow: Add dimension (column) with constant value\n\nI have a Tensor of shape (-1,) which represents a list of indices. I want to create a Tensor of shape (-1,2). The first column should be the same as the list of indices, while the second column should be filled with a constant. Eg (let\'s say the constant is 6):\n\nindices = [4, 35, 230, 235] my_goal = [[4, 6], [35, 6], [230, 6], [235, 6]]\n\nWhat is the best way to do this? I was hoping a broadcasting-tf.concat would work, but tf.concat doesn\'t seem to support broadcasting. You can use tensorflow.pad. But first you have to make it a two dimensional tensor. indices = tf.constant([1,2,3,4]) indices = tf.expand_dims(tf, 1) # now you have a (4,1) tensor padding = [[0,0],[0,1]] # no padding before or after the first dimension # no padding before second dimension. Single-element padding # after the second dimension my_goal = tf.pad(indices, padding, constant_values=6)\n\nPadding has to be [n,2] tensor..""""""', '""""""Is there any equivalent to pandas drop column? You can remove features by only filtering the features that you want. This how you can modify only one feature:\n\nimport tensorflow as tf import pandas as pd df = pd.DataFrame(data={\'color\': [\'red\', \'blue\',\'green\'], \'price\': [120, 80, 90], \'weight\': [3.2, 4.0, 5]}) df.to_csv(\'data.csv\', index=False) dataset = tf.data.experimental.make_csv_dataset(\'/content/data.csv\', batch_size=1, num_epochs = 1, shuffle=False) dataset = dataset.map(lambda x: (x[\'color\'], x[\'price\'], x[\'weight\']+2)) for x in dataset: print(x[0], x[1], x[2])\n\ntf.Tensor([b\'red\'], shape=(1,), dtype=string) tf.Tensor([120], shape=(1,), dtype=int32) tf.Tensor([5.2], shape=(1,), dtype=float32) tf.Tensor([b\'blue\'], shape=(1,), dtype=string) tf.Tensor([80], shape=(1,), dtype=int32) tf.Tensor([6.], shape=(1,), dtype=float32) tf.Tensor([b\'green\'], shape=(1,), dtype=string) tf.Tensor([90], shape=(1,), dtype=int32) tf.Tensor([7.], shape=(1,), dtype=float32)""""""']","To address the questions on modifying and removing features from a TensorFlow dataset created using `tf.data.experimental.make_csv_dataset`, we will explore how to modify a single feature and how to remove a feature.

### Modifying a Single Feature

To modify a single feature in a TensorFlow dataset, such as adding 2 to the 'weight' feature, you can use the `map` transformation. The `map` function applies a transformation to each element of the dataset. In this case, you can define a lambda function that modifies the 'weight' feature by adding 2 to its value. The `map` function will then apply this transformation to each element in the dataset, resulting in the desired modification.

### Removing a Single Feature

To remove a single feature from a TensorFlow dataset, you can use the `map` transformation to filter out the unwanted feature. The `map` function allows you to return a modified version of each element, excluding the feature you want to remove. By defining a lambda function that constructs a new dictionary without the specified feature, you can effectively drop the column from the dataset.

### Summary

In summary, to modify a single feature in a TensorFlow dataset, use the `map` transformation with a lambda function that applies the desired modification. To remove a single feature, use the `map` transformation with a lambda function that constructs a new dictionary excluding the feature to be removed. These transformations allow you to efficiently preprocess your dataset while maintaining the structure and integrity of the data."
63146831,tf.custom_gradient,example required,What is the analytic interpretation for Tensorflow custom gradient?,"<p>In the official <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">tf.custom_gradient</a> documentation it shows how to define custom gradients for <code>log(1 + exp(x))</code></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.math.log(1 + e), grad
</code></pre>
<p>When <code>y = log(1 + exp(x))</code>, analytically the derivative comes out to be <code>dy/dx = (1 - 1 / (1 + exp(x)))</code>.</p>
<p>However in the code <code>def grad</code> says its <code>dy * (1 - 1 / (1 + exp(x)))</code>.
<code>dy/dx = dy * (1 - 1 / (1 + exp(x)))</code> is not a valid equation. While <code>dx = dy * (1 - 1 / (1 + exp(x)))</code> is wrong as it should be the reciprocal.</p>
<p>What does the <code>grad</code> function equate to?</p>
","<p>I finally figured it out. The <code>dy</code> should be called <code>upstream_gradient</code> or <code>upstream_dy_dx</code>.</p>
<p>By chain rule we know that</p>
<p><a href=""https://i.stack.imgur.com/7g3aZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7g3aZ.png"" alt=""chain rule"" /></a></p>
<p>where <code>dx[i]/dx[i+1]</code> is the gradient of the current function.</p>
<p>So <code>dy</code> is the product of all the gradients upstream before this function.</p>
<p><a href=""https://i.stack.imgur.com/nu4Z8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nu4Z8.png"" alt=""enter image description here"" /></a></p>
<p>So, if you forget to multiply the <code>dy</code> it is effectively the same as <a href=""https://www.tensorflow.org/api_docs/python/tf/stop_gradient"" rel=""nofollow noreferrer"">tf.stop_gradient</a></p>
<p>Here is a code which demos this. Full notebook <a href=""https://github.com/Ghost---Shadow/differentiable-programming-handbook/blob/master/notebooks/custom-gradient.ipynb"" rel=""nofollow noreferrer"">here</a></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def foo(x):
    tf.debugging.assert_rank(x, 0)

    def grad(dy_dx_upstream):
        dy_dx = 2 * x
        dy_dx_downstream = dy_dx * dy_dx_upstream
        tf.print(f'x={x}\tupstream={dy_dx_upstream}\tcurrent={dy_dx}\t\tdownstream={dy_dx_downstream}')
        return dy_dx_downstream
    
    y = x ** 2
    tf.print(f'x={x}\ty={y}')
    
    return y, grad


x = tf.constant(2.0, dtype=tf.float32)

with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    y = foo(foo(foo(x))) # y = x ** 8

tf.print(f'\nfinal dy/dx={tape.gradient(y, x)}')
</code></pre>
<p>Output</p>
<pre><code>x=2.0   y=4.0
x=4.0   y=16.0
x=16.0  y=256.0
x=16.0  upstream=1.0    current=32.0        downstream=32.0
x=4.0   upstream=32.0   current=8.0     downstream=256.0
x=2.0   upstream=256.0  current=4.0     downstream=1024.0

final dy/dx=1024.0
</code></pre>
","<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def foo(x):
    tf.debugging.assert_rank(x, 0)

    def grad(dy_dx_upstream):
        dy_dx = 2 * x
        dy_dx_downstream = dy_dx * dy_dx_upstream
        tf.print(f'x={x}\tupstream={dy_dx_upstream}\tcurrent={dy_dx}\t\tdownstream={dy_dx_downstream}')
        return dy_dx_downstream
    
    y = x ** 2
    tf.print(f'x={x}\ty={y}')
    
    return y, grad


x = tf.constant(2.0, dtype=tf.float32)

with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    y = foo(foo(foo(x))) # y = x ** 8

tf.print(f'\nfinal dy/dx={tape.gradient(y, x)}')
</code></pre>","['Understanding tf.custom_gradient in TensorFlow', 'How to define custom gradients in TensorFlow', 'Explanation of custom gradient function in TensorFlow', 'TensorFlow custom gradient example log1pexp', 'How does the grad function work in tf.custom_gradient', 'TensorFlow custom gradient tutorial', 'Defining and using custom gradients in TensorFlow', 'TensorFlow custom gradient API documentation explained', 'Step-by-step guide to tf.custom_gradient in TensorFlow', 'Understanding the grad function in TensorFlow custom gradients']","['What does the grad function in tf.custom_gradient represent?', 'How does the grad function in tf.custom_gradient relate to the derivative of the function?', 'Why is dy multiplied by the derivative in the grad function of tf.custom_gradient?', ""How to correctly interpret the grad function in TensorFlow's tf.custom_gradient?"", 'What is the purpose of the dy parameter in the grad function of tf.custom_gradient?']","{'https://www.youtube.com/watch?v=-0bF9tEv9YU', 'https://www.youtube.com/watch?v=VmaBfi-CWv4'}",,"{'https://stackoverflow.com/questions/50030026/how-to-provide-custom-gradient-in-tensorflow', 'https://stackoverflow.com/questions/52622343/tensorflow-what-gradients-needed-to-be-defined-for-custom-operation', 'https://stackoverflow.com/questions/54819947/defining-custom-gradient-as-a-class-method-in-tensorflow', 'https://stackoverflow.com/questions/52604879/how-excute-custom-gradient-with-tf-multiply', 'https://stackoverflow.com/questions/43839431/tensorflow-how-to-replace-or-modify-gradient', 'https://stackoverflow.com/questions/50203668/using-tf-custom-gradient-in-tensorflow-r1-8'}","['""""""Does that mean in the custom gradient function, I need to return the same results as what tf.gradients should give, of which each elements are summed partial derivatives of dy/dx? @NathanExplosion Yes, that sounds right. I have added a snippet that (I hope) demonstrates how tf.gradients and gradient functions relate to each other. I tried tf.gradients(c[0,0], a), it will return dc[0,0]/da. But if we defined the returned gradients as the summation of partial gradients, how could it derive an individual gradient? @NathanExplosion In that case the flow goes like this. You have a slice operation that gives you a scalar, so you start the gradient computation with the scalar 1 as dc[0,0]/dc[0,0]. Then you compute dc[0,0]/dc, which is a matrix shaped like c with the gradient of c[0,0] wrt each element - so it is a matrix, grad_c, with all 0 except a 1 in the first value of the first row. Then you can get dc[0,0]/da, which is (dc[0,0]/dc)*(dc/da). We saw it ended up being grad_c * b.T, so you get a matrix the size of a where the first row is the first column of b and all other rows are 0. """"""', '""""""Defining custom gradient as a class method in Tensorflow\n\nI need to define a method to be a custom gradient as follows:\n\nclass CustGradClass: def __init__(self): pass @tf.custom_gradient def f(self,x): fx = x def grad(dy): return dy * 1 return fx, grad\n\nI am getting the following error:\n\nValueError: Attempt to convert a value (<main.CustGradClass object at 0x12ed91710>) with an unsupported type () to a Tensor. The reason is the custom gradient accepts a function f(*x) where x is a sequence of Tensors. And the first argument being passed is the object itself i.e., self. From the documentation:\n\nf: function f(*x) that returns a tuple (y, grad_fn) where: x is a sequence of Tensor inputs to the function. y is a Tensor or sequence of Tensor outputs of applying TensorFlow operations in f to x. grad_fn is a function with the signature g(*grad_ys)\n\nHow do I make it work? Do I need to inherit some python tensorflow class? I am using tf version 1.12.0 and eager mode.""""""', '""""""Also, this attached image describes the solution as expected by manually calulation\n\nIf I do not use the @tf.custom_gradient then the TensorFlow gives the desired solution as expected. My question is that how can I provide custom gradient for y=Ax? We know that dy/dx = A^T as shown in the above attachment which shows steps of calculation that matches the TensorFlow output. import tensorflow as tf #I want to write custom gradient for this function f1 def f1(A,x): y=tf.matmul(A,x,name=\'y\') return y #for y= Ax, the derivative is: dy/dx= transpose(A) @tf.custom_gradient def f2(A,x): y=f1(A,x) def grad(dzByDy): # dz/dy = 2y reaches here correctly. dzByDx=tf.matmul(A,dzByDy,transpose_a=True) return dzByDx return y,grad x= tf.constant([[1.],[0.]],name=\'x\') A= tf.constant([ [1., 2.], [3., 4.]],name=\'A\') y=f1(A,x) # This works as desired #y=f2(A,x) #This line gives Error z=tf.reduce_sum(y*y,name=\'z\') g=tf.gradients(ys=z,xs=x) with tf.Session() as sess: print sess.run(g)\n\nSince your function f2() has two inputs, you have to provide a gradient to flow back to each of them. The error you see:\n\nNum gradients 2 generated for op name: ""IdentityN"" [...] do not match num inputs 3\n\nis admittedly quite cryptic, though. Supposing you never want to calculate dy/dA, you can just return None, dzByDx. The code below (tested):\n\nimport tensorflow as tf #I want to write custom gradient for this function f1 def f1(A,x): y=tf.matmul(A,x,name=\'y\') return y #for y= Ax, the derivative is: dy/dx= transpose(A) @tf.custom_gradient def f2(A,x): y=f1(A,x) def grad(dzByDy): # dz/dy = 2y reaches here correctly. dzByDx=tf.matmul(A,dzByDy,transpose_a=True) return None, dzByDx return y,grad x= tf.constant([[1.],[0.]],name=\'x\') A= tf.constant([ [1., 2.], [3., 4.]],name=\'A\') #y=f1(A,x) # This works as desired y=f2(A,x) #This line gives Error z=tf.reduce_sum(y*y,name=\'z\') g=tf.gradients(ys=z,xs=x) with tf.Session() as sess: print sess.run( g )\n\n[array([[20.], [28.]], dtype=float32)]""""""']","### Analytic Interpretation of TensorFlow Custom Gradient

In TensorFlow, the `tf.custom_gradient` decorator allows users to define custom gradients for functions, providing fine-grained control over the gradient computation. This can be particularly useful for improving numerical stability or efficiency in gradient calculations.

The question pertains to the analytic interpretation of the custom gradient function provided in the TensorFlow documentation for the function `log(1 + exp(x))`. When defining a custom gradient, the function `grad` is responsible for computing the gradient of the output with respect to the input.

For the function `y = log(1 + exp(x))`, the analytic derivative is `dy/dx = 1 - 1 / (1 + exp(x))`. In the custom gradient definition, the `grad` function returns `dy * (1 - 1 / (1 + exp(x)))`. Here, `dy` represents the upstream gradient, which is the gradient flowing from the subsequent layers or operations in the computational graph.

The expression `dy * (1 - 1 / (1 + exp(x)))` is the product of the upstream gradient `dy` and the local gradient `1 - 1 / (1 + exp(x))`. This follows the chain rule of differentiation, where the total gradient is the product of the upstream gradient and the local gradient. Therefore, the `grad` function correctly computes the gradient of `y` with respect to `x` by multiplying the upstream gradient `dy` with the local gradient `1 - 1 / (1 + exp(x))`.

In summary, the `grad` function in the custom gradient definition represents the chain rule application, where `dy` is the upstream gradient and `1 - 1 / (1 + exp(x))` is the local gradient. This ensures that the overall gradient computation is accurate and aligns with the mathematical principles of differentiation."
59998335,tf.cond,example required,Constantly update tf.cond based on bool value,"<p>I am using <code>tf.cond</code> for controlling the flow of the Tensorflow graph. I went through the documentation and was able to implement <code>tf.cond</code> based branching successfully. But my concern is that while the graph is being loaded the value of the <code>bool</code> variable is checked and the branching decision is made at the initialization step itself. Any further changes in the <code>bool</code> is not tracked. Following is the MWE that better describes the problem:</p>

<pre class=""lang-py prettyprint-override""><code>def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    global foo
    if i &gt; 10:
        foo = False
    print(sess.run(x))    
</code></pre>

<p>This prints only <code>32</code>s. </p>

<p>I tried with <code>eager_execution</code> too with the following code:</p>

<pre class=""lang-py prettyprint-override""><code>tf.enable_eager_execution()
def funa():
    return tf.constant(32)

def funb():
    return tf.constant(21)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(x)
</code></pre>

<p>Still the same result.</p>

<p>So my question is how can I write code such that one part of the graph is chosen dynamically, based on the updates to the <code>bool</code> variable (if possible)? Thanks. I am using Tensorflow v1.14.</p>
","<p>You can make a placeholder for <code>foo</code> and feed it's value while running the session. Modified code:</p>

<pre><code>import tensorflow as tf

def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
foo_p = tf.placeholder(tf.bool)

sess = tf.Session()

x = tf.cond(foo_p, lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(sess.run(x, {foo_p:foo}))
</code></pre>
","<pre><code>import tensorflow as tf

def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
foo_p = tf.placeholder(tf.bool)

sess = tf.Session()

x = tf.cond(foo_p, lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(sess.run(x, {foo_p:foo}))
</code></pre>","['How to use tf.cond with dynamic boolean values in TensorFlow?', 'TensorFlow v1.14: How to dynamically control graph flow with tf.cond?', 'How to update TensorFlow graph based on changing boolean variable?', 'Using tf.cond for dynamic branching in TensorFlow', 'How to make TensorFlow graph respond to variable changes during execution?', 'TensorFlow v1.14: Handling dynamic conditions in graph execution', 'How to implement dynamic control flow in TensorFlow with tf.cond?', 'Why does tf.cond not update with changing boolean values in TensorFlow?', 'How to use tf.cond with variables that change during TensorFlow session?', 'Best practices for dynamic branching in TensorFlow graphs']","['How to dynamically update the condition in tf.cond in TensorFlow?', 'How to use tf.cond with a variable that changes during execution in TensorFlow?', 'How to make tf.cond re-evaluate the condition on each run in TensorFlow?', 'How to control TensorFlow graph flow based on a variable that changes during execution?', 'How to use TensorFlow v1.14 to dynamically choose graph parts based on a changing boolean variable?']","{'https://www.youtube.com/watch?v=IzKXEbpT9Lg', 'https://www.youtube.com/watch?v=FmIcZoLmvL0', 'https://www.youtube.com/watch?v=jh4ITuOytE4'}",,"{'https://stackoverflow.com/questions/37063952/confused-by-the-behavior-of-tf-cond', 'https://stackoverflow.com/questions/35833011/how-to-add-if-condition-in-a-tensorflow-graph'}","['""""""Confused by the behavior of `tf.cond`\n\nModified 2 years, 9 months ago\n\nI need a conditional control flow in my graph. If pred is True, the graph should call an op that updates a variable and then returns it, otherwise it returns the variable unchanged. A simplified version is:\n\npred = tf.constant(True) x = tf.Variable([1]) assign_x_2 = tf.assign(x, [2]) def update_x_2(): with tf.control_dependencies([assign_x_2]): return tf.identity(x) y = tf.cond(pred, update_x_2, lambda: tf.identity(x)) with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval())\n\nHowever, I find that both pred=True and pred=False lead to the same result y=[2], which means the assign op is also called when update_x_2 is not selected by tf.cond. How to explain this?""""""', '""""""How to add if condition in a TensorFlow graph? Asked 8 years, 5 months ago\n\nModified 5 years, 1 month ago\n\nLet\'s say I have following code:\n\nx = tf.placeholder(""float32"", shape=[None, ins_size**2*3], name = ""x_input"") condition = tf.placeholder(""int32"", shape=[1, 1], name = ""condition"") W = tf.Variable(tf.zeros([ins_size**2*3,label_option]), name = ""weights"") b = tf.Variable(tf.zeros([label_option]), name = ""bias"") if condition > 0: y = tf.nn.softmax(tf.matmul(x, W) + b) else: y = tf.nn.softmax(tf.matmul(x, W) - b)\n\nWould the if statement work in the calculation (I do not think so)? If not, how can I add an if statement into the TensorFlow calculation graph? You\'re correct that the if statement doesn\'t work here, because the condition is evaluated at graph construction time, whereas presumably you want the condition to depend on the value fed to the placeholder at runtime. (In fact, it will always take the first branch, because condition > 0 evaluates to a Tensor, which is ""truthy"" in Python.)\n\nTo support conditional control flow, TensorFlow provides the tf.cond() operator, which evaluates one of two branches, depending on a boolean condition. To show you how to use it, I\'ll rewrite your program so that condition is a scalar tf.int32 value for simplicity:\n\nx = tf.placeholder(tf.float32, shape=[None, ins_size**2*3], name=""x_input"") condition = tf.placeholder(tf.int32, shape=[], name=""condition"") W = tf.Variable(tf.zeros([ins_size**2 * 3, label_option]), name=""weights"") b = tf.Variable(tf.zeros([label_option]), name=""bias"") y = tf.cond(condition > 0, lambda: tf.matmul(x, W) + b, lambda: tf.matmul(x, W) - b)\n\n\n\n 4\n\n@mrry Are both branches executed by default ? I have tf.cond(c, lambda x: train_op1, lambda x: train_op2) and both train_ops are executed at each execution of cond independently of the value of c. Am I doing something wrong? @PiotrDabkowski This is a sometimes surprising behavior of tf.cond(), which is touched upon in the docs. In short, you need to create the ops that you want to run conditionally inside the respective lambdas. Everything that you create outside the lambdas but refer to in either branch will execute in both cases. @mrry Wow, that\'s rather unexpected :) Thanks for the answer, defining ops inside functions solved the problem. Is the condition/( application of the logic) element-wise? TF 2.0 introduces a feature called AutoGraph which lets you JIT compile python code into Graph executions. This means you can use python control flow statements (yes, this includes if statements). From the docs,\n\nAutoGraph supports common Python statements like while, for, if, break, continue and return, with support for nesting. That means you can use Tensor expressions in the condition of while and if statements, or iterate over a Tensor in a for loop. You will need to define a function implementing your logic and annotate it with tf.function. Here is a modified example from the documentation:\n\nimport tensorflow as tf @tf.function def sum_even(items): s = 0 for c in items: if tf.equal(c % 2, 0): s += c return s sum_even(tf.constant([10, 12, 15, 20])) # <tf.Tensor: id=1146, shape=(), dtype=int32, numpy=42>\n\n 2\n\nWhy are you using tf.equal()? Shouldn\'t you be able to use == and let AutoGraph compile it automatically?""""""']","### Explanation Description

The question addresses the issue of dynamically updating the branching decision in `tf.cond` based on changes to a boolean variable during runtime. The user observes that the branching decision is made at the initialization step and does not reflect subsequent changes to the boolean variable.

In TensorFlow, `tf.cond` is used to create conditional branches in the computation graph. The predicate `pred` is evaluated, and based on its value, either `true_fn` or `false_fn` is executed. However, it is important to understand that `tf.cond` evaluates the predicate and constructs the graph branches at graph construction time, not at runtime. This means that any changes to the boolean variable after the graph has been constructed will not affect the branching decision.

In the provided example, the boolean variable `foo` is converted to a tensor and passed to `tf.cond`. Since `foo` is evaluated at graph construction time, the initial value of `foo` determines the branch that will be executed, and subsequent changes to `foo` are not tracked.

To achieve dynamic branching based on updates to a boolean variable, you need to ensure that the predicate is a tensor that can be fed with different values at runtime. This can be done by using a placeholder or a variable that can be updated within the session. By feeding different values to the placeholder or updating the variable, you can control the branching decision dynamically.

In summary, the key point is that `tf.cond` makes the branching decision at graph construction time based on the initial value of the predicate. To dynamically update the branching decision, you need to use a tensor that can be fed with different values at runtime, such as a placeholder or an updatable variable. This allows the branching decision to reflect changes to the boolean variable during the execution of the graph."
70747499,tf.map_fn,example required,Using tf.map_fn when the function has multiple outputs,"<p>I can easily use tf.map_fn when the function has one output:</p>
<pre><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    return x[0]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p><strong>output:</strong></p>
<pre><code>tf.Tensor([1. 4.], shape=(2,), dtype=float32)
</code></pre>
<p>But, when the function has two outputs:</p>
<pre><code>def my_fun(x):
    return [x[0],x[1]]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p>I get an error. Not sure what is going on. I read the information about tf.map_fn in here <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/map_fn</a>, but not sure how to fix this:</p>
<p>map_fn also supports functions with multi-arity inputs and outputs:</p>
<p><em>If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems).
If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures.</em></p>
<p><strong>Output:</strong></p>
<pre><code>~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    317     _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types,
--&gt; 318                                            expand_composites)
    319   except (ValueError, TypeError) as e:

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-36-5b11c7fef461&gt; in &lt;module&gt;
      5     return [x[0],x[1]]
      6 
----&gt; 7 print(tf.map_fn(my_fun,tensaki))

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)
    266         back_prop=back_prop,
    267         swap_memory=swap_memory,
--&gt; 268         maximum_iterations=n)
    269     results_flat = [r.stack() for r in r_a]
    270 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   2712                                               list(loop_vars))
   2713       while cond(*loop_vars):
-&gt; 2714         loop_vars = body(*loop_vars)
   2715         if try_to_pack and not isinstance(loop_vars, (list, _basetuple)):
   2716           packed = True

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in &lt;lambda&gt;(i, lv)
   2703         cond = lambda i, lv: (  # pylint: disable=g-long-lambda
   2704             math_ops.logical_and(i &lt; maximum_iterations, orig_cond(*lv)))
-&gt; 2705         body = lambda i, lv: (i + 1, orig_body(*lv))
   2706       try_to_pack = False
   2707 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in compute(i, tas)
    256       packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])
    257       packed_fn_values = fn(packed_values)
--&gt; 258       nest.assert_same_structure(dtype or elems, packed_fn_values)
    259       flat_fn_values = output_flatten(packed_fn_values)
    260       tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    323                   &quot;Entire first structure:\n%s\n&quot;
    324                   &quot;Entire second structure:\n%s&quot;
--&gt; 325                   % (str(e), str1, str2))
    326 
    327 

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not
Entire first structure:
.
Entire second structure:
[., .]```
</code></pre>
","<p>You should make sure you are returning a tensor. Maybe concatenate or stack the list of values:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    x = tf.stack([x[0], x[1]], axis=0)
    return x

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<pre><code>tf.Tensor(
[[1. 2.]
 [4. 5.]], shape=(2, 2), dtype=float32)
</code></pre>
<p>Of course, it all depends on the output you are expecting.</p>
","<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    x = tf.stack([x[0], x[1]], axis=0)
    return x

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<pre><code>tf.Tensor(
[[1. 2.]
 [4. 5.]], shape=(2, 2), dtype=float32)
</code></pre>","['How to use tf.map_fn with functions that return multiple outputs?', 'Handling multiple outputs in tf.map_fn in TensorFlow', 'Examples of tf.map_fn with multi-arity outputs', 'Common errors and solutions when using tf.map_fn with multiple return values', 'TensorFlow tf.map_fn function with nested structures']","['How to use tf.map_fn with a function that returns multiple outputs?', 'Handling multiple outputs in tf.map_fn in TensorFlow', 'Example of tf.map_fn with a function returning a list of tensors', 'How to structure the output of a function used in tf.map_fn when it returns multiple values?', 'Common errors and solutions when using tf.map_fn with multiple outputs']",{'https://www.youtube.com/watch?v=jipSAXT2E_I'},"['""""""[Document(page_content=""[Music] all right guys we\'re back with another technical question I\'ll be going through the question the answer and I\'m hoping just like me you stay a little bit crazy to get that answer all the way through guys let\'s get [Music] started [Music] [Music] [Music] [Music] [Music] n [Music] and that\'s it guys I hope that video helped find the resolution you\'re looking for if it did please I\'d appreciate it if you could hit subscribe until the next time you need more technical help I hope you have a good one cheers"", metadata={\'source\': \'jipSAXT2E_I\'})]""""""']","{'https://stackoverflow.com/questions/56663912/how-can-i-apply-tf-map-fn-using-multiple-inputs-in-tensorflow', 'https://stackoverflow.com/questions/70747499/using-tf-map-fn-when-the-function-has-multiple-outputs', 'https://stackoverflow.com/questions/42892347/can-i-apply-tf-map-fn-to-multiple-inputs-outputs', 'https://stackoverflow.com/questions/64508203/i-dont-understand-map-fn-with-two-inputs'}","['""""""Can I apply tf.map_fn(...) to multiple inputs/outputs? \n\na = tf.constant([[1,2,3],[4,5,6]]) b = tf.constant([True, False], dtype=tf.bool) a.eval() array([[1, 2, 3], [4, 5, 6]], dtype=int32) b.eval() array([ True, False], dtype=bool)\n\nI want to apply a functions to the inputs above, a, and b using tf.map_fn. It will input both [1,2,3], and True and output similar values. Let\'s say out function is simply the identity: lambda(x,y): x,y so, given an input of [1,2,3], True, it will output those identical tensors. I know how to use tf.map_fn(...) with one variable, but not with two. And in this case I have mixed data types (int32 and bool) so I can\'t simply concatenate the tensors and split them after the call. Can I use tf.map_fn(...) with multiple inputs/outputs of different data types? David ParksDavid Parks\n\n\n\nFigured it out. You have to define the data types for each tensor in dtype for each of the different tensors, then you can pass the tensors as a tuple, your map function receives a tuple of inputs, and map_fn returns back back a tuple. a = tf.constant([[1,2,3],[4,5,6]]) b = tf.constant([True, False], dtype=tf.bool) c = tf.map_fn(lambda x: (x[0], x[1]), (a,b), dtype=(tf.int32, tf.bool)) c[0].eval() array([[1, 2, 3], [4, 5, 6]], dtype=int32) c[1].eval() array([ True, False], dtype=bool)\n\nBe warned that if you use this the processing will be done in CPU, not on the GPU. This can be especially detrimental to speed when training on a GPU.""""""', '""""""Using tf.map_fn when the function has multiple outputs\n\nI can easily use tf.map_fn when the function has one output:\n\nimport tensorflow as tf tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]]) def my_fun(x): return x[0] print(tf.map_fn(my_fun,tensaki))\n\ntf.Tensor([1. 4.], shape=(2,), dtype=float32)\n\nBut, when the function has two outputs:\n\ndef my_fun(x): return [x[0],x[1]] print(tf.map_fn(my_fun,tensaki))\n\nI get an error. Not sure what is going on. I read the information about tf.map_fn in here https://www.tensorflow.org/api_docs/python/tf/map_fn, but not sure how to fix this:\n\nmap_fn also supports functions with multi-arity inputs and outputs:\n\nIf elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 <= i < num_elems). If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures. ~Users\\user2\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites) 317 _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types, --> 318 expand_composites) 319 except (ValueError, TypeError) as e: ValueError: The two structures don\'t have the same nested structure. First structure: type=DType str=<dtype: \'float32\'> Second structure: type=list str=[<tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0>] More specifically: Substructure ""type=list str=[<tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0>]"" is a sequence, while substructure ""type=DType str=<dtype: \'float32\'>"" is not During handling of the above exception, another exception occurred: ValueError Traceback (most recent call last) <ipython-input-36-5b11c7fef461> in <module> 5 return [x[0],x[1]] 6 ----> 7 print(tf.map_fn(my_fun,tensaki)) ~Users\\user2\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name) 266 back_prop=back_prop, 267 swap_memory=swap_memory, --> 268 maximum_iterations=n) 269 results_flat = [r.stack() for r in r_a] 270 ~Users\\user2\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure) 2712 list(loop_vars)) 2713 while cond(*loop_vars): -> 2714 loop_vars = body(*loop_vars) 2715 if try_to_pack and not isinstance(loop_vars, (list, _basetuple)): 2716 packed = True ~Users\\user2\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py in <lambda>(i, lv) 2703 cond = lambda i, lv: ( # pylint: disable=g-long-lambda 2704 math_ops.logical_and(i < maximum_iterations, orig_cond(*lv))) -> 2705 body = lambda i, lv: (i + 1, orig_body(*lv)) 2706 try_to_pack = False 2707 ~Users\\user2\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py in compute(i, tas) 256 packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta]) 257 packed_fn_values = fn(packed_values) --> 258 nest.assert_same_structure(dtype or elems, packed_fn_values) 259 flat_fn_values = output_flatten(packed_fn_values) 260 tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)] ~Users\\user2\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites) 323 ""Entire first structure:\\n%s\\n"" 324 ""Entire second structure:\\n%s"" --> 325 % (str(e), str1, str2)) 326 327 ValueError: The two structures don\'t have the same nested structure. First structure: type=DType str=<dtype: \'float32\'> Second structure: type=list str=[<tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0>] More specifically: Substructure ""type=list str=[<tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0>]"" is a sequence, while substructure ""type=DType str=<dtype: \'float32\'>"" is not Entire first structure: . Entire second structure: [., .]```\n\nYou should make sure you are returning a tensor. Maybe concatenate or stack the list of values:\n\nimport tensorflow as tf tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]]) def my_fun(x): x = tf.stack([x[0], x[1]], axis=0) return x print(tf.map_fn(my_fun,tensaki))\n\ntf.Tensor( [[1. 2.] [4.""""""']","To address the question of using `tf.map_fn` when the function has multiple outputs, it is important to understand how `tf.map_fn` handles functions with multi-arity inputs and outputs. The error encountered arises because the structure of the output from the function does not match the expected structure.

When using `tf.map_fn` with a function that returns multiple outputs, the function must return a structure that matches the `fn_output_signature` if provided, or the structure of the input `elems` otherwise. If the function returns a list or tuple of tensors, `tf.map_fn` will stack the corresponding elements from those structures.

To resolve the issue, ensure that the function's output structure is consistent and matches the expected output signature. If the function returns a list of tensors, you should specify the `fn_output_signature` to describe the structure and data types of the outputs. This can be done using `tf.DType`, `tf.TensorSpec`, or other appropriate specifications.

In summary, the key to using `tf.map_fn` with functions that have multiple outputs is to ensure that the output structure is correctly specified and matches the expected signature. This will prevent the `ValueError` related to mismatched structures and allow `tf.map_fn` to correctly stack the results."
57449484,tf.compat.v1.layers.batch_normalization,example required,What is trainable parameter in tensorflow?,"<p>tf.compat.v1.layers.batch_normalization takes <code>trainable</code> as an input. The documentation says:</p>

<blockquote>
  <p>Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).</p>
</blockquote>

<p>I think only scaling factor (gamma) and offset (beta) should be added to trainable variables and I am skeptical if even moving averages will get added to GraphKeys.TRAINABLE_VARIABLES. Can somebody tell me how trainable input is influencing the behavior of batch_normalization</p>
","<p>First of all, this function is <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">deprecated</a> and should not be used.</p>

<p><code>trainable</code> arguments means that scaling factor (gamma) and offset (beta) will be trainable and it's true by default.</p>

<p>When it comes to moving averages, those <strong>are not trainable</strong>, they are only updated after each batch pass, those are not parameters (<code>tf.Variable</code> objects).</p>

<p>Please notice, you can set <code>trainable</code> to false, in such case, if <code>beta</code> and <code>gamma</code> are set to defaults (zero and one respectively), they won't affect the moving averages. You can turn them off by issuing <code>center</code> (for <code>beta</code>) or <code>scale</code> (for <code>gamma</code>).</p>
",,"[""How does the 'trainable' parameter in tf.compat.v1.layers.batch_normalization affect the variables added to GraphKeys.TRAINABLE_VARIABLES?"", ""Does the 'trainable' parameter in tf.compat.v1.layers.batch_normalization add moving averages to GraphKeys.TRAINABLE_VARIABLES?"", ""What variables are added to GraphKeys.TRAINABLE_VARIABLES when 'trainable' is set to True in tf.compat.v1.layers.batch_normalization?"", 'Are the scaling factor (gamma) and offset (beta) the only variables added to GraphKeys.TRAINABLE_VARIABLES in tf.compat.v1.layers.batch_normalization?']","[""How does the 'trainable' parameter in tf.compat.v1.layers.batch_normalization affect the variables added to GraphKeys.TRAINABLE_VARIABLES?"", ""Does setting 'trainable=True' in tf.compat.v1.layers.batch_normalization add moving averages to GraphKeys.TRAINABLE_VARIABLES?"", ""Which variables are added to GraphKeys.TRAINABLE_VARIABLES when using tf.compat.v1.layers.batch_normalization with 'trainable=True'?"", 'Are the scaling factor (gamma) and offset (beta) the only trainable variables in tf.compat.v1.layers.batch_normalization?']",set(),[],"{'https://stackoverflow.com/questions/58097348/whats-the-differences-between-tf-graphkeys-global-variables-and-tf-graphkeys-tr', 'https://stackoverflow.com/questions/55619070/graphkeys-trainable-variables-vs-tf-trainable-variables'}","['""""""What\'s the differences between tf.GraphKeys.GLOBAL_VARIABLES and tf.GraphKeys.TRAINABLE_VARIABLES in tensorflow?\n\nFrom https://www.tensorflow.org/api_docs/python/tf/GraphKeys\n\nGLOBAL_VARIABLES: the default collection of Variable objects, shared across distributed environment (model variables are subset of these).  Commonly, all TRAINABLE_VARIABLES variables will be in MODEL_VARIABLES, and all MODEL_VARIABLES variables will be in GLOBAL_VARIABLES\n\nTRAINABLE_VARIABLES: the subset of Variable objects that will be trained by an optimizer. See tf.compat.v1.trainable_variables for more details. So as I understand TRAINABLE_VARIABLES is subset of GLOBAL_VARIABLES, so what else GLOBAL_VARIABLES contain? Also for this simple example statement Commonly, all TRAINABLE_VARIABLES variables will be in MODEL_VARIABLES, and all MODEL_VARIABLES variables will be in GLOBAL_VARIABLES don\'t hold:\n\nIMAGE_HEIGHT = 5 IMAGE_WIDTH = 5 with tf.Graph().as_default(): with tf.variable_scope(\'my_scope\', reuse=tf.AUTO_REUSE): x_ph = tf.placeholder( dtype=tf.float32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 3], name=\'input\' ) x_tf = tf.layers.conv2d(x_ph, 32, 1, 1, padding=\'valid\') with tf.Session() as sess: sess.run(tf.global_variables_initializer()) x_np = np.random.rand(1, IMAGE_HEIGHT, IMAGE_WIDTH, 3) out_np = sess.run(x_tf, {x_ph:x_np}) print(\'out_np.shape\', out_np.shape) print(\'-\'*60) global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) print(\'len(global_vars)\', len(global_vars)) print(\'global_vars params:\', sum([np.prod(var.shape) for var in global_vars])) print(global_vars) print(\'-\'*60) model_vars = tf.get_collection(tf.GraphKeys.MODEL_VARIABLES) print(\'len(model_vars)\', len(model_vars)) print(\'model_vars params:\', sum([np.prod(var.shape) for var in model_vars])) print(model_vars) print(\'-\'*60) trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) print(\'len(trainable_vars)\', len(trainable_vars)) print(\'trainable_vars params:\', sum([np.prod(var.shape) for var in trainable_vars])) print(trainable_vars)\n\nout_np.shape (1, 5, 5, 32) ------------------------------------------------------------ len(global_vars) 2 global_vars params: 128 [<tf.Variable \'my_scope/conv2d/kernel:0\' shape=(1, 1, 3, 32) dtype=float32_ref>, <tf.Variable \'my_scope/conv2d/bias:0\' shape=(32,) dtype=float32_ref>] ------------------------------------------------------------ len(model_vars) 0 model_vars params: 0 [] ------------------------------------------------------------ len(trainable_vars) 2 trainable_vars params: 128 [<tf.Variable \'my_scope/conv2d/kernel:0\' shape=(1, 1, 3, 32) dtype=float32_ref>, <tf.Variable \'my_scope/conv2d/bias:0\' shape=(32,) dtype=float32_ref>]\n\nWhy Commonly, all TRAINABLE_VARIABLES variables will be in MODEL_VARIABLES, and all MODEL_VARIABLES variables will be in GLOBAL_VARIABLES don\'t hold for this example. What additional variables GLOBAL_VARIABLES contain besides TRAINABLE_VARIABLES? Is it true that TRAINABLE_VARIABLES will be always subset of GLOBAL_VARIABLES or they can just intersect partially? Note: All of this applies to TF version 1 only, as all variable collections have been deprecated and (IIRC) won\'t be in TF v2. Starting from question 2:\n\nWhat additional variables GLOBAL_VARIABLES contain besides TRAINABLE_VARIABLES? global_step, for example, is a global variable which is not trainable. It\'s a variable, because you update it at every step, it\'s not trainable because it\'s not part of the optimization process (as in, it\'s not a weight/bias that is altered in order to minimize the loss). Is it true that TRAINABLE_VARIABLES will be always subset of GLOBAL_VARIABLES or they can just intersect partially? In principle, the two groups can just intersect partially, although this would be very odd. An example I can think of would be something like a custom distributed training environment where each machine has its own optimizer and where some of the trainable variables are defined as local variables (i.e., each machine has its own copy and these copies are not kept in sync). Why would you do that?""""""', '""""""GraphKeys.TRAINABLE_VARIABLES vs tf.trainable_variables()\n\n Is GraphKeys.TRAINABLE_VARIABLES is the same as tf.trainable_variables() ? Is GraphKeys.TRAINABLE_VARIABLES actually tf.GraphKeys.TRAINABLE_VARIABLES? Looks like networks successfully trains with:\n\noptimizer = tf.train.AdamOptimizer(config.LEARNING_RATE) with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): self.train_op = optimizer.minimize(self.loss, var_list=tf.trainable_variables())\n\noptimizer = tf.train.AdamOptimizer(config.LEARNING_RATE) with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): self.train_op = optimizer.minimize(self.loss)\n\nAccording to documentation:\n\nvar_list: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.""""""']","### Description

In TensorFlow, a trainable parameter refers to a variable that is adjusted during the training process to minimize the loss function. The `tf.compat.v1.layers.batch_normalization` function includes a `trainable` argument, which is a boolean. When set to `True`, this argument ensures that the variables associated with the batch normalization layer are added to the `GraphKeys.TRAINABLE_VARIABLES` collection. This collection is a subset of `GraphKeys.GLOBAL_VARIABLES` and includes variables that will be trained by an optimizer.

The primary trainable parameters in batch normalization are the scaling factor (gamma) and the offset (beta). These parameters are adjusted during training to normalize the input data. The moving averages, such as moving mean and moving variance, are not considered trainable parameters because they are updated based on the statistics of the current batch and are not directly optimized to minimize the loss function.

The `trainable` argument influences the behavior of the batch normalization layer by determining whether the gamma and beta variables are included in the `GraphKeys.TRAINABLE_VARIABLES` collection. If `trainable` is set to `True`, these variables will be optimized during training. If set to `False`, they will not be included in the trainable variables collection, and their values will remain fixed during training.

In summary, the `trainable` argument in `tf.compat.v1.layers.batch_normalization` controls whether the gamma and beta parameters are added to the `GraphKeys.TRAINABLE_VARIABLES` collection, thereby influencing their optimization during the training process. The moving averages are not affected by this argument as they are not trainable parameters."
63004540,tf.pad,example required,How to pad 1 dimensinal vector in tensorflow? Getting InvalidArgumentError: paddings must be a matrix with 2 columns with tf.pad,"<p>I am trying to use tf.pad. Here is my attempt to pad the tensor to length 20, with values 10.</p>
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), paddings=20, constant_values=10)
</code></pre>
<p>I get this error message</p>
<pre><code>InvalidArgumentError: paddings must be a matrix with 2 columns: [2,1] [Op:PadV2]
</code></pre>
<p>I am looking at the documentation</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/pad</a></p>
<blockquote>
<p>paddings is an integer tensor with shape [n, 2], where n is the rank of tensor. For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension</p>
</blockquote>
<p>But I am unable to figure out how to shape the pad value</p>
","<p>You have to specify the padding at the beginning and the padding at the end of your vector by matrix of shape (1,2) :</p>
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), [[ 0 , 20]], constant_values=10)
</code></pre>
<p>if you have three-dimensional tensor (rank = 3 e.g : (225,225,3) ) the padding matrix has to be of shape (3, 2 ) where &quot;3&quot; is the rank, and &quot;2&quot; to specify the padding at the beginning and end of each dimension.</p>
<p>For example, a padding matrix  = [ [0,2], [5,5], [2,0] ], means that we want to pad the first dimension by 0 at the beginning (=no padding) and 2 at the end .padding the second dimension by 5 at beginning and 5 at the end.</p>
",,"['How to use tf.pad in TensorFlow?', 'What is the correct way to specify the paddings parameter in tf.pad?', 'How to pad a tensor to a specific length using tf.pad in TensorFlow?', 'Examples of using tf.pad to pad a tensor with constant values in TensorFlow']","['How to use tf.pad to pad a tensor to a specific length in TensorFlow?', 'What is the correct way to specify the paddings parameter in tf.pad?', 'Examples of using tf.pad with constant_values in TensorFlow', 'How to pad a 1D tensor to a specific length with a constant value in TensorFlow?']",set(),[],"{'https://stackoverflow.com/questions/42334646/tensorflow-pad-unknown-size-tensor-to-a-specific-size', 'https://stackoverflow.com/questions/43928642/how-does-tensorflow-pad-work'}","['""""""For processing, a static length makes them easier to work with. Yes. There is. Provided you do not need to change the rank of the tensor, it\'s very simple. tf.pad() accepts regular python lists with tensors. The format of the padding is a list of pairs of how much to pad on each side of that dimension. t = tf.constant([[1, 2], [3, 4]]) paddings = [[0, 0], [0, 4-tf.shape(t)[0]]] out = tf.pad(t, paddings, \'CONSTANT\', constant_values=-1) sess.run(out) # gives: # array([[ 1, 2, -1, -1], # [ 3, 4, -1, -1]], dtype=int32)\n\nIf you want to generalise this to a useful function, you could do something like:\n\ndef pad_up_to(t, max_in_dims, constant_values): diff = max_in_dims - tf.shape(t) paddings = tf.pad(diff[:, None], [[0, 0], [1, 0]]) return tf.pad(t, paddings, \'CONSTANT\', constant_values=constant_values) # (note: see edits for the solution referred to by other answers on this question)\n\nwhere max_in_dims is essentially the desired shape of the output. Note: this function will fail if you provide a shape that is strictly smaller than t in any dimension. You can use it like:\n\nt = tf.constant([[1, 2], [3, 4]]) # shape = [2, 2] t_padded = pad_up_to(t, [2, 4], -1) # shape = [2, 4], padded with -1s\n\nt = tf.placeholder(tf.float32, [None, None]) # shape = [?, ?] t_padded = pad_up_to(t, [5,5], -1) # shape = [5, 5], padded with -1s t_np = np.random.uniform(0, 1, [3,4]) # shape = [3,4], no padding t_padded_out = sess.run(t_padded, {t: t_np}) t_np2 = np.random.uniform(0, 1, [2,1]) # shape = [2,1], no padding t_padded_out2 = sess.run(t_padded, {t: t_np2})\n\nAlthough the dimension sizes are calculated dynamically, the number of dimensions is not, so make sure that max_in_dims has the same number of elements as t.shape. MultihunterMultihunter\n\n 12\n\nWhat if t has a dynamic size (e.g., its size is determined only after some placeholder is fed)? In my provided function, s is a tensor that is the shape of t, so the amount to pad is calculated dynamically. The number of dimensions is not calculated dynamically, so just make sure your max_in_dims is a vector with has the same number of elements as your t has dimensions. If you do this it will just work (I wrote the function with this use-case in mind). I didn\'t expect it to work with a dynamic size but to my surprise, it does! Thanks! Good reference to not waste time finding a more off the shelf solution. This didn\'t really work for me in TF 2.3 with dynamic sizes since m is evaluated to None which throws an error for the subtraction. However, the fix is to simply change the line to [[0, m - s[i]] if m != None else [0,0] for (i, m) in enumerate(max_in_dims)]. An extension of Multihunter\'s solution so that padding is only performed when necessary and does not yield an error for longer inputs:\n\nSuppose we have a sequential input called inp_seq, which is a tensor of rank 4 and should be padded in order to have a minimum length of filter_size in dimension 1. def dynamic_padding(inp, min_size): pad_size = min_size - tf.shape(inp)[1] paddings = [[0, 0], [0, pad_size], [0, 0], [0, 0]] # assign here, during graph execution return tf.pad(inp, paddings) # Pad only if necessary padded = tf.cond(tf.less(tf.shape(inp_seq)[1], filter_size), true_fn=lambda: dynamic_padding(inp_seq, filter_size), false_fn=lambda: inp_seq)\n\n1,2\n\nThe line creating a tf.Variable is redundant, since the subsequent line overwrites it with a python list. You can remove that line and it will function the same. (Also, a sequence is a class defined by the python base libraries, while a tensor is defined by tensorflow: I think you should clarify which of these your inp_seq actually is; I presume that what you\'re dealing with is actually a sequence (or list) of Tensors like inp_seq=[Tensor, Tensor, Tensor])\n\n\n\nI removed the redundant line, thank you for the suggestion. The input is simply a tensor; I used the term sequence with its broader meaning (to refer to data of high dimensionality which are sequential along one dimension, namely the one to pad), I was not referring to the python base libraries. I clarified this in the edit.""""""']","To pad a 1-dimensional vector in TensorFlow, you need to use the `tf.pad` function correctly by specifying the `paddings` argument as a matrix with 2 columns. The `paddings` argument should be an integer tensor with shape `[n, 2]`, where `n` is the rank of the tensor you are padding. For each dimension `D` of the input tensor, `paddings[D, 0]` indicates how many values to add before the contents of the tensor in that dimension, and `paddings[D, 1]` indicates how many values to add after the contents of the tensor in that dimension.

In your case, you are trying to pad a 1-dimensional tensor to a length of 20 with values of 10. The rank of your tensor is 1, so `paddings` should be a matrix with one row and two columns. The first column specifies the padding before the tensor, and the second column specifies the padding after the tensor. 

To achieve the desired padding, you need to calculate the appropriate values for the `paddings` matrix. For example, if your original tensor has a length of 4 and you want to pad it to a length of 20, you need to add 16 values in total. You can distribute these values before and after the tensor as needed.

Ensure that the `constant_values` argument is set to the value you want to pad with, which in this case is 10. The `mode` argument should be set to ""CONSTANT"" to use constant padding.

By following these guidelines, you can correctly pad your 1-dimensional tensor in TensorFlow without encountering the `InvalidArgumentError`."
62752605,tf.nn.sampled_softmax_loss,example required,Loss function in tf.nn.sampled_softmax_loss,"<p>I have a question regarding Tensorflow:</p>
<p>Which loss function is used in <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer""><code>tf.nn.sampled_softmax_loss</code></a>?</p>
<p>I believe it's <em><strong>cross-entropy</strong></em>, but it is not written on the official website. Can anyone confirm my guess?</p>
","<p>Based on <a href=""https://stackoverflow.com/questions/35241251/in-tensorflow-what-is-the-difference-between-sampled-softmax-loss-and-softmax-c"">this other question</a>, it looks like it is cross entropy.</p>
<p>Besides, the main difference between <code>sampled_softmax_loss</code> and <code>softmax_cross_entropy_with_logits</code> (the standard cross_entropy loss in TF) is that the first only takes into account a subset V of your vocabulary to calculate your loss, while the second takes into account your entire vocabulary.</p>
",,"['What is the loss function used in tf.nn.sampled_softmax_loss in TensorFlow?', 'Does tf.nn.sampled_softmax_loss use cross-entropy as its loss function?', 'Explanation of tf.nn.sampled_softmax_loss in TensorFlow', 'How does tf.nn.sampled_softmax_loss work in TensorFlow?', 'TensorFlow tf.nn.sampled_softmax_loss detailed tutorial']","['What loss function does tf.nn.sampled_softmax_loss use in TensorFlow?', 'Is cross-entropy the loss function used in tf.nn.sampled_softmax_loss?', 'Can someone confirm if tf.nn.sampled_softmax_loss in TensorFlow uses cross-entropy as its loss function?', 'What type of loss function is implemented in tf.nn.sampled_softmax_loss according to TensorFlow documentation?']",{'https://www.youtube.com/watch?v=FZ-iwMSkJoQ'},"['""""""[Document(page_content=""in this video we are going to look at how to use sampled soft Max loss in tensor flow how do we calculate it and for that I\'ve created these examples where we have the tensor for weights then we have tensor for bias labels inputs and then we have uh generated sample values so all of these we are going to input it to the ef. NN do sampled uncore soft maxcore loss as we see here in the docs so in the docs we have all of these values and the rest of them they are refilled except for the sampled values we are going to leave the others as is and so once we put all the values so weights we have bias and then labels we are uh uh using the tf. expands labels and then for inputs we have the inputs number sampled number classes that we have specified here two and three and then the sampled value so when we run this the you get here in the output which is the sampled softmax loss if you have any questions or comments please do let me know in the comments comment section below I hope to see you all in the next video thank you"", metadata={\'source\': \'FZ-iwMSkJoQ\'})]""""""']","{'https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow', 'https://stackoverflow.com/questions/35241251/in-tensorflow-what-is-the-difference-between-sampled-softmax-loss-and-softmax-c'}","['""""""The arguments weights and biases specify a separate fully-connected layer that is used to compute the logits for a chosen sample. Like above, labels are not one-hot encoded, but have the shape [batch_size, num_true]. Sampled functions are only suitable for training. In test time, it\'s recommended to use a standard softmax loss (either sparse or one-hot) to get an actual distribution. Another alternative loss is tf.nn.nce_loss, which performs noise-contrastive estimation (if you\'re interested, see this very detailed discussion). I\'ve included this function to the softmax family, because NCE guarantees approximation to softmax in the limit. 1\n\nMay I ask for a point of clarification regarding sigmoid cross entropy (sigCE)? If it solves for N binary classification tasks at once, is N = prod(output.shape), e.g. shape = [batch, examples, channels]; N = (batch * examples * channels)? If tf.losses expect ""logits"" (output of network), should I also return the probabilities for ease of use? \n\nHowever, for version 1.5, softmax_cross_entropy_with_logits_v2 must be used instead, while using its argument with the argument key=..., for example\n\nsoftmax_cross_entropy_with_logits_v2(_sentinel=None, labels=y, logits=my_prediction, dim=-1, name=None)\n\n\n\n 0\n\nWhile it is great that the accepted answer contains lot more info than what is asked, I felt that sharing a few generic thumb rules will make the answer more compact and intuitive:\n\nThere is just one real loss function. This is cross-entropy (CE). For a special case of a binary classification, this loss is called binary CE (note that the formula does not change) and for non-binary or multi-class situations the same is called categorical CE (CCE). Sparse functions are a special case of categorical CE where the expected values are not one-hot encoded but is an integer\n\nWe have the softmax formula which is an activation for multi-class scenario. For binary scenario, same formula is given a special name - sigmoid activation\n\nBecause there are sometimes numerical instabilities (for extreme values) when dealing with logarithmic functions, TF recommends combining the activation layer and the loss layer into one single function. This combined function is numerically more stable. TF provides these combined functions and they are suffixed with _with_logits\n\nWith this, let us now approach some situations. Say there is a simple binary classification problem - Is a cat present or not in the image? What is the choice of activation and loss function? It will be a sigmoid activation and a (binary)CE. So one could use sigmoid_cross_entropy or more preferably sigmoid_cross_entropy_with_logits. The latter combines the activation and the loss function and is supposed to be numerically stable. How about a multi-class classification. Say we want to know if a cat or a dog or a donkey is present in the image. What is the choice of activation and loss function? It will be a softmax activation and a (categorical)CE. So one could use softmax_cross_entropy or more preferably softmax_cross_entropy_with_logits. We assume that the expected value is one-hot encoded (100 or 010 or 001). If (for some weird reason), this is not the case and the expected value is an integer (either 1 or 2 or 3) you could use the \'sparse\' counterparts of the above functions. There could be a third case.""""""', '""""""In Tensorflow, what is the difference between sampled_softmax_loss and softmax_cross_entropy_with_logits\n\nIn tensorflow, there are methods called softmax_cross_entropy_with_logits and sampled_softmax_loss. I read the tensorflow document and searched google for more information but I couldn\'t find the difference. It looks like to me both calculates the loss using softmax function. Using sampled_softmax_loss to calculate the loss\n\nloss = tf.reduce_mean(tf.nn.sampled_softmax_loss(...))\n\nUsing softmax_cross_entropy_with_logits to calculate the loss\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(P, Q))\n\nTo me, calculating softmax loss is same as calculating softmaxed cross entropy (e.g. cross_entropy(softmax(train_x)))\n\nCould somebody tell me the why there is two different methods and which method should I use in which case? \n\nIf your target vocabulary(or in other words amount of classes you want to predict) is really big, it is very hard to use regular softmax, because you have to calculate probability for every word in dictionary. By Using sampled_softmax_loss you only take in account subset V of your vocabulary to calculate your loss. Sampled softmax only makes sense if we sample(our V) less than vocabulary size. If your vocabulary(amount of labels) is small, there is no point using sampled_softmax_loss. Sequence to sequence translation in this example\n\n 2\n\nHi Farseer, I kind of struggle with the same issue as above. My output vocabulary is only around 100 labels. Now, tensorflow only provides the sampled_softmax_loss function. Instead of this one, could I use the tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases) function? Or can I just use the seq2seq_model.py as it is?""""""']","The `tf.nn.sampled_softmax_loss` function in TensorFlow computes and returns the sampled softmax training loss. This function is designed to provide a more efficient way to train a softmax classifier when dealing with a large number of classes. The loss function used in `tf.nn.sampled_softmax_loss` is indeed based on cross-entropy. 

In the context of sampled softmax, the cross-entropy loss is computed over a subset of the possible classes, rather than the full set. This subset is randomly sampled, which makes the computation more efficient, especially when the number of classes is very large. The sampled softmax loss is generally an underestimate of the full softmax loss, but it is much faster to compute during training. For evaluation or inference, it is recommended to use the full softmax loss to get an accurate measure of performance.

To summarize, the loss function used in `tf.nn.sampled_softmax_loss` is a form of cross-entropy loss, specifically adapted for efficient training with a large number of classes by sampling a subset of the classes."
66874943,tf.data.Dataset,example required,Why iterations over the same tf.data.Dataset give different data each iteration?,"<p>I'm trying to understand how <strong>tf.data.Dataset</strong> works.</p>
<p>It says on the documentation that <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""nofollow noreferrer"">take</a> returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch):</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds

# Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=True)

# Build your input pipeline
ds = ds.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)
# ...
</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([2 0 6 6 8 8 6 0 3 4 8 7 5 2 5 7 8 7 1 1 1 8 6 4 0 4 3 2 4 2 1 9], shape=(32,), dtype=int64)
</code></pre>
<p>However, iterating over it again, gives different labels: (continuation of last code)</p>
<pre class=""lang-py prettyprint-override""><code>for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([7 3 5 6 3 1 7 9 6 1 9 3 9 8 6 7 7 1 9 7 5 2 0 7 8 1 7 8 7 0 5 0], shape=(32,), dtype=int64)
tf.Tensor([1 3 6 1 8 8 0 4 1 3 2 9 5 3 8 7 4 2 1 8 1 0 8 5 4 5 6 7 3 4 4 1], shape=(32,), dtype=int64)
</code></pre>
<p>Shouldn't the labels be the same, given that the dataset is the same?</p>
","<p>This is because the data files are shuffled and the dataset is shuffled with <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=ru#shuffle"" rel=""nofollow noreferrer""><code>dataset.shuffle()</code></a>.</p>
<p>With <code>dataset.shuffle()</code>, the data will be shuffled in a different way on each iteration by default.</p>
<p>One can remove <code>shuffle_files=True</code> and set the argument <code>reshuffle_each_iteration=False</code> to prevent reshuffling on different iterations.</p>
<p>The <code>.take()</code> function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them.</p>
<pre class=""lang-py prettyprint-override""><code># Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=False)

# Build your input pipeline
ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
    
for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)
tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)
</code></pre>
","<pre class=""lang-py prettyprint-override""><code># Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=False)

# Build your input pipeline
ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
    
for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
</code></pre>","['Understanding tf.data.Dataset take method in TensorFlow', 'How does tf.data.Dataset.shuffle affect dataset iteration in TensorFlow', 'Why does iterating over a tf.data.Dataset with take(1) give different results each time', 'TensorFlow tf.data.Dataset take method behavior explained', 'How to ensure consistent batches with tf.data.Dataset in TensorFlow']","['Why does iterating over a tf.data.Dataset multiple times yield different results?', 'How does the shuffle operation in tf.data.Dataset affect the output of the dataset?', 'Does the take method in tf.data.Dataset create a static snapshot of the dataset?', 'How to ensure consistent output when iterating over a tf.data.Dataset multiple times?', 'What is the effect of prefetching on the output of a tf.data.Dataset?']",{'https://www.youtube.com/watch?v=n7byMbl2VUQ'},"['""""""[Document(page_content=""ROHAN JAIN: Hi, all. I\'m Rohan, and I\'m\\nhere to talk to you about how you can scale up\\nor input data processing with tf.data. So let\'s start with a high-level\\nview of your ML training job. Typically, your ML training\\nstep will have two phases to it. The first is data\\npreprocessing, where you\'re going to look\\nat the input files and do all kinds of\\ntransformations on them to make them ready for\\nthe next phase, which is model computation. While you\'re doing data\\npreprocessing, which happens in the CPU,\\nyou might be doing some kind of things such as-- for images, you\'re\\ncropping them. For videos, you may be\\nsampling them and whatnot. So if your training\\nspeed is slow, you could have a bottleneck in\\neither one of these two places. And I hope that the\\ntalk on profiling would give you an\\nindication on how to figure out which\\none of the two phases you\'re getting slow at. And I\'m here to talk to\\nyou about the first kind of preprocessing bottleneck--\\nthe bottleneck which is data preprocessing. So let\'s try to look into what\\nthis bottleneck really is. So in the last few years\\nwe\'ve done a fantastic job making accelerators which do\\nthe ML operations really fast. And so the amount\\nof time it takes us to do a matrix\\noperation and all the linear algebra our\\noperations is a lot smaller. But the hosts and the\\nCPUs that feed the data to these accelerators have not\\nbeen able to keep up with them, and so there ends up\\nbeing a bottleneck. We thought that we\\ncould mitigate this by making the\\nmodels more complex, but what happens is that the\\naccelerators have constraints on how much RAM they have,\\nand, more importantly, where you deploy\\nthese models tends to be something like a mobile\\ndevice or something like that, which tends to restrict\\nthe amount of complexity you can introduce\\ninto your model. So that hasn\'t\\nreally panned out. The second approach\\npeople take is that they try to turn larger batch sizes. But larger batch sizes\\nrequire a larger amount of preprocessing to\\nassemble the batch, so then that puts\\nfurther pressure on them. So that\'s why this is becoming\\nan increasingly larger problem within Alphabet and\\neven externally. And I\'m going to\\ntalk to you about how you can solve it using tf.data. tf.data is TensorFlow\'s data\\npreprocessing framework. It\'s fast, it\'s flexible,\\nand it\'s easy to use. And you can learn more\\nabout it at our guide. For background for\\nthe rest of the talk, I think I\'m going to go through\\na typical tf.data pipeline, and that\'ll help us\\nin the later stages. So suppose you have some data in\\nsome tf.data record files which are your training data. So you can now start\\noff with the TF record data set with that data. And then after that, you start\\ndoing your preprocessing. This is typically the\\nbulk of the logic. So if it\'s images, you\'re\\ndoing cropping, maybe flipping, all sorts of things there. After that, you\\nshuffle the data so that you don\'t train to\\nthe order in which you see the examples and the input. And that helps you with\\ntheir training accuracy. And after that, we will batch it\\nso that the accelerator can now make use of vectorized\\ncomputations. Finally, you want to do some\\nsoftware pipelining so that you ensure that while\\nthe model is off working on one batch of data,\\nthe preprocessing side can produce the next batch\\nso that everything works very efficiently. Finally, you can then\\nfeed this tf.data dataset to a Keras model,\\nso that you can now start doing your training. So given that sort\\nof basic pipeline, and suppose you\\nhave a bottleneck, the first thing I\'d\\nrecommend you to do is to go through our single\\nhost performance guide, and try to utilize every\\ntrick and transformation that is available in tf-data\\nto be able to extract the maximum possible\\nperformance, so that you\'re using all the\\n[INAUDIBLE] and whatever. There\'s excellent information\\nat the guide that we have here. And [INAUDIBLE] did a\\ngreat talk at the ML Tokyo Summit, which you can take\\na look at to learn more about this. So that\'s the first thing\\nI\'d recommend you do. But suppose you have\\ndone that and you\'ve tried all the different\\nrecommendations that we have here, but you\'re still\\nbottlenecked on that data preprocessing part. And don\'t worry,\\nyou\'re not alone. This is very common. We\'ve increasingly\\nseen this with a lot of internal customers. And so now I\'m very pleased to\\npresent a couple of solutions that we\'ve been\\nworking on on the team to help you solve that problem. So the first idea\\nis that why don\'t we just reuse the computation? So suppose you\'re playing\\naround with different model architectures. Your input pre-processing\\nsort of part kind of remains the same. And if it\'s expensive and\\ntime-consuming, why don\'t we just do it once,\\nsave it, and then every subsequent time,\\nwe just read from it, and do that quickly? So we noticed a bunch of\\ninternal customers, teams within Alphabet, who\\nwere trying to do this on their own outside\\nof tf.data, and we decided to bring\\nit in to tf.data and make it incredibly fast,\\nflexible, and easy to use. And so this is what\\nwe call Snapshot. The idea is what I\\nexplained to you. You materialize the output of\\nyour data pre-processing once, and then you can use\\nit many, many times. This is incredibly\\nuseful for playing around with different\\nmodel architectures and if you settle down\\non an architecture doing hyperparameter tuning. And so you can get that\\nspeed up using Snapshot. Next, I\'m going to go\\nthrough the pipeline that we talked about before and\\nsee how you can add Snapshot to it to make it faster. So that\'s the original\\npipeline that we had. And so notice that there\'s\\nthis pre-processing step, which is expensive. So now with Snapshot, you just\\nadd a snapshot transformation right after that with a\\ndirectory [INAUDIBLE].. And with this, everything that\\nis before the snapshot will now be written to disk the\\nfirst time it\'s run. And then every subsequent time,\\nwe will just read from it. And we would go through the\\nrest of the steps as usual. One thing I\'d like\\nto point out is that we place the snapshot\\nat a particular location before the shuffle, because\\nif it\'s after the shuffle, everything gets frozen. So all the randomization\\nthat you get out of shuffle you lose, because\\nevery subsequent time, you\'re just going to be reading\\nthe same exact order again and again. So that\'s why we introduce it\\nat that stage in the pipeline. So Snapshot, we\\ndeveloped it internally. There are internal\\nusers and teams that are using it and\\nderiving benefit out of it. And now we\'re bringing it\\nto the open source world. We published an RFC, which\\nhas more information about it and some other\\ntechnical details. And this will be available\\nin TensorFlow 2.3, but I believe it will be\\navailable in the [INAUDIBLE] shortly. So remember, I talked\\nabout two ideas. So the second idea is that,\\nnow, not all computation is reusable, so because suppose\\nyou had someone randomized crops in there. And if you wrote that to\\ndisk and read them back, you\'d, again, lose\\nthat randomization. And so a snapshot\\nis probably not applicable in that scenario. So the second idea is to be able\\nto distribute the computation. So the initial setup is that\\nyou have one host CPU, which is driving a bunch of\\nthese accelerators, but now you can offload\\nthis computation from this host to\\nmaybe a cluster. And now you can\\nutilize the ability and the computational\\npower that you have for all these\\ndifferent workers to be able to feed the\\nhost, so that you\'re not bottlenecked on the input\\npre-processing anymore and things move fast. This is tf.data service. It\'s a tf.data feature\\nthat allows you to scale your workload horizontally. So if you\'re seeing a slowness\\nin your input pre-processing, you can start adding workers,\\nand it\'ll just scale up. It\'s got a master-worker\\narchitecture, where the master drives the work\\nfor the different workers and it gives you\\nfault tolerance. So if one of the workers\\nfails, you\'re still good and you still can make progress. So let\'s see how you can use the\\ntf.data service for the example that we have. So here, instead of having sort\\nof an expensive pre-processing, let\'s say you have some\\nrandomized pre-processing. So now this is not snapshotable,\\nbecause if you snapshot, then you lose the randomization. So we\'ll provide\\nyou a binary which allows you to run the data\\nservice on the cluster setup manager that you like, whether\\nit\'s Kubernetes or Cloud or something like that. And then once you have\\nthat up and running, you can just add a\\ndistribute transformation to your tf.data pipeline and\\nprovide the master address. Anything before the\\ndistribute transformation would now get run on the\\ncluster that you have set up and everything after\\nwill run on the host. And so this allows you\\nto sort of scale up. Again, note that\\nbecause we are not doing any kind of\\nfreezing of the data, we can output this\\ntransformation as late as possible in there. So notice that I\'ve put it after\\nthe shuffle transformation. The service, like\\nSnapshot, has been developed with internal users. They\'ve been using it. And it\'s been, like, a\\ngame-changer in terms of [INAUDIBLE] utilization. And now, again, we\'re\\nbringing it to you. And so we published an RFC,\\nwhich was well-received, and this should be\\navailable in 2.3 for you to play around with. So to summarize, what\\ndid I talk about today? So as with various trends\\nin hardware and software, we\'ve ended up in a scenario\\nwhere a lot of input machine learning jobs are\\ngetting bottlenecked on input pre-processing. And I\'ve told\\nabout two solutions that tf.data team\\nhas been working on to help you solve\\nthis bottleneck. First is Snapshot,\\nwhich allows you to reuse your pre-processing,\\nso that you don\'t have to do it multiple times. And the second is\\nthe tf.data service, which allows you to distribute\\nthis computation to a cluster, so that you get the\\nscale-up that you need. I hope you play around with\\nthese and give us feedback. And thank you for your time. [MUSIC PLAYING]"", metadata={\'source\': \'n7byMbl2VUQ\'})]""""""']","{'https://stackoverflow.com/questions/58663198/does-tf-data-dataset-take-return-random-sample', 'https://stackoverflow.com/questions/47067401/how-to-iterate-a-dataset-several-times-using-tensorflows-dataset-api', 'https://stackoverflow.com/questions/66874943/why-iterations-over-the-same-tf-data-dataset-give-different-data-each-iteration', 'https://stackoverflow.com/questions/51995267/what-is-the-mechanism-for-tf-data-dataset-shuffle'}","['""""""perhaps one could imagine a scenario where the shuffling is defined once and is used for every iteration. @jakub ah yes it makes sense from that perspective\n\n\n\nThis is because the data files are shuffled and the dataset is shuffled with dataset.shuffle(). With dataset.shuffle(), the data will be shuffled in a different way on each iteration by default. One can remove shuffle_files=True and set the argument reshuffle_each_iteration=False to prevent reshuffling on different iterations. The .take() function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them. # Construct a tf.data.Dataset ds = tfds.load(\'mnist\', split=\'train\', shuffle_files=False) # Build your input pipeline ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE) single_batch_dataset = ds.take(1) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label)\n\ntf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64) tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)\n\n\n\n""""""']","### Explanation of Why Iterations Over the Same `tf.data.Dataset` Give Different Data Each Iteration

The question addresses why iterating over the same `tf.data.Dataset` multiple times yields different data each iteration, even though the dataset itself remains unchanged.

The behavior observed is due to the shuffling mechanism applied to the dataset. When a `tf.data.Dataset` is shuffled using the `shuffle` method, it fills a buffer with a specified number of elements (`buffer_size`) and randomly samples elements from this buffer, replacing the selected elements with new ones. This process ensures that the elements are shuffled in a random order.

By default, the `shuffle` method reshuffles the dataset for each iteration. This is controlled by the `reshuffle_each_iteration` parameter, which is set to `True` by default. As a result, each time you iterate over the dataset, the elements are shuffled differently, leading to different outputs for each iteration.

In the provided context, the dataset is shuffled with `ds.shuffle(1024)`, and since `reshuffle_each_iteration` is not explicitly set to `False`, it defaults to `True`. Consequently, each iteration over the dataset results in a different order of elements, which explains why the labels printed in each iteration are different.

To achieve deterministic behavior where the dataset yields the same order of elements in each iteration, you can set `reshuffle_each_iteration=False` in the `shuffle` method. This will ensure that the dataset is shuffled only once, and the same order of elements is maintained across multiple iterations.

In summary, the different outputs in each iteration over the same `tf.data.Dataset` are due to the default behavior of the `shuffle` method, which reshuffles the dataset for each iteration. This can be controlled by setting the `reshuffle_each_iteration` parameter to `False` if deterministic behavior is desired."
60398554,tf.keras.Model,example required,"Should we apply repeat, batch shuffle to tf.data.Dataset when passing it to fit function?","<p>I still don't after having read documentation about <code>tf.keras.Model.fit</code> and <code>tf.data.Dataset</code>, when passing <code>tf.data.Dataset</code> to fit function, should I call <code>repeat</code> and <code>batch</code> on the dataset object or should I provide the <code>batch_size</code> and <code>epochs</code> arguments to fit instead? or both? Should I apply the same treatment to the validation set?</p>

<p>And while I'm here, can I <code>shuffle</code> the dataset before the <code>fit</code>? (seems like it's an obvious yes)
If so, before, after calling <code>Dataset.batch</code> and <code>Dataset.repeat</code> (if calling them)?</p>

<p><strong>Edit:</strong> When using <code>batch_size</code> argument, and without having called <code>Dataset.batch(batch_size)</code> previously, I am getting the following error:</p>

<pre><code>ValueError: The `batch_size` argument must not be specified for the given input type.
Received input: &lt;MapDataset shapes: ((&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;), (&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;)), 
types: ((tf.float32, tf.float32, tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32))&gt;, 
batch_size: 1
</code></pre>

<p>Thanks</p>
","<p>There's different ways to do what you want here, but the one I always use is: </p>

<pre><code>batch_size = 32
ds = tf.Dataset()
ds = ds.shuffle(len_ds)
train_ds = ds.take(0.8*len_ds)
train_ds = train_ds.repeat().batch(batch_size)
validation_ds = ds.skip(0.8*len_ds)
validation_ds = train_ds.repeat().batch(batch_size)
model.fit(train_ds,
          steps_per_epoch = len_train_ds // batch_size,
          validation_data = validation_ds,
          validation_steps = len_validation_ds // batch_size,
          epochs = 5)
</code></pre>

<p>This way you have access to all the variables after model fitting as well, for example if you want to visualize the validation set, you can. This is not really possible with <code>validation_split</code>. If you remove <code>.batch(batch_size)</code>, you should remove the <code>// batch_size</code>s, but I would leave them, as it clearer what is happening now.</p>

<p>You always have to provide epochs.</p>

<p>Calculating the length of your train/validation sets requires you to loop over them: </p>

<pre><code>len_train_ds = 0
for i in train_ds:
  len_train_ds += 1
</code></pre>

<p>if in <code>tf.Dataset</code> form.</p>
","<pre><code>batch_size = 32
ds = tf.Dataset()
ds = ds.shuffle(len_ds)
train_ds = ds.take(0.8*len_ds)
train_ds = train_ds.repeat().batch(batch_size)
validation_ds = ds.skip(0.8*len_ds)
validation_ds = train_ds.repeat().batch(batch_size)
model.fit(train_ds,
          steps_per_epoch = len_train_ds // batch_size,
          validation_data = validation_ds,
          validation_steps = len_validation_ds // batch_size,
          epochs = 5)
</code></pre>","['How to use tf.data.Dataset with tf.keras.Model.fit?', 'Should I call repeat and batch on tf.data.Dataset before passing to tf.keras.Model.fit?', 'How to handle batch_size and epochs arguments when using tf.data.Dataset with tf.keras.Model.fit?', 'How to prepare validation set when using tf.data.Dataset with tf.keras.Model.fit?', 'Can I shuffle tf.data.Dataset before using it with tf.keras.Model.fit?', 'When to call shuffle, batch, and repeat on tf.data.Dataset for training with tf.keras.Model.fit?', 'Error handling for batch_size argument in tf.keras.Model.fit with tf.data.Dataset']","['When using tf.data.Dataset with tf.keras.Model.fit, should I call repeat and batch on the dataset object or provide batch_size and epochs arguments to fit?', 'Should I apply the same treatment to the validation set when using tf.data.Dataset with tf.keras.Model.fit?', 'Can I shuffle the dataset before calling fit in TensorFlow? If so, should I shuffle before or after calling Dataset.batch and Dataset.repeat?', ""What does the error 'ValueError: The `batch_size` argument must not be specified for the given input type' mean when using tf.data.Dataset with tf.keras.Model.fit?""]","{'https://www.youtube.com/watch?v=dzoh8cfnvnI', 'https://www.youtube.com/watch?v=U8Ixc2OLSkQ'}","['""""""[Document(page_content=""hey I\'m maybe from deep lizard in this episode we\'ll demonstrate how we can use tensorflow scarus API to create a validation set on the fly during training before we demonstrate how to build a validation set using Kerris let\'s first talk about what exactly a validation set is so whenever we train a model our hope is that when we train it that we see good results from the training output that we have low loss and high accuracy but we don\'t ever train a model just for the sake of training it we want to take that model and hopefully be able to use it in some way on data that it wasn\'t necessarily exposed to during the training process and although this new data is data that the models never seen before the hope is that the model will be good enough to be able to generalize well on this new data and give accurate predictions for it we can actually get an understanding of how well our model is generalizing by introducing a validation set during the training process to create a validation set before training begins we can choose to take a subset of the training set and then separate it into a separate set labeled as validation data and then during the training process the model will only train on the training data and then we\'ll validate on the separated validation data so what do we mean by validating well essentially if we have the addition of a validation set then during training the model will be learning the features of the training set just as we\'ve already seen but in addition in each epoch after the model has gone through the actual training process it\'ll take what it\'s learned from the training data and then validate by predicting on the data in the validation set using only what it\'s learned from the training data though so then during the training process when we look at the output of the accuracy and loss not only will we be seeing that accuracy and loss computed for the training set we\'ll also see that computed on the validation set it\'s important to understand though that the model is only learning on or training on the training data it\'s not taking the validation set into account during training the validation set is just for us to be able to see how well the model is able to predict on data that it was not exposed to during the training process in other words it allows us to see how general our model is how well it\'s able to generalize on data that is not included in the training data so knowing this information will allow us to see if our model is running into the famous overfitting problem so overfitting occurs when the model has learned the specific features of the training set really well but it\'s unable to generalize on data it hasn\'t seen before so if while training we see that the model is giving really good results for the training set but less than good results for the validation set then we can conclude that we have an overfitting problem and then take the steps necessary to combat that specific issue if you\'d like to see the overfitting problem covered in more detail then there is an episode for that in the deep learning fundamentals course all right so now let\'s discuss how we can create and use a validation set with a Karass sequential model there\'s actually two ways that we can create and work with validation sets with a sequential model and the first way is to have a completely separate validation set from the training set and then to pass that validation set to the model in the fit function there is a validation data parameter and so we can just set that equal to the structure that is holding our validation data and there\'s a write-up in the corresponding blog for this episode that contains more details about the format that that data needs to be in but we\'re going to actually only focus on the second way of creating and using a validation set this step actually saves us a step because we don\'t have to explicitly go through the creation process of the validation set instead we can get carers to create it for us all right so we\'re back in our Jupiter notebook right where we left off last time and we\'re here on the model dot Fit function and recall this is what we use last time to train our model now I\'ve already edited this cell to include this new parameter which is validation split and what validation split does is it does what it sounds like it splits out a portion of the training set into a validation set so we just set this to a number between 0 and 1 so just a fractional number to tell Karis how much of the training set we need to split out into the validation set so here I\'m splitting out 10% of the training set so it\'s important to note that whenever we do this the validation set is completely held out of the training set so the training samples that we remove from the training set into validation set are no longer contained within the training data any longer so using this approach the validation set will be created on the fly whenever we call the fit function now there\'s one other thing worth mentioning here and remember last time I discussed this shuffle equals true parameter and I said that by default the training set is shuffled whenever we call fit so this shuffle equals true is already set by default but I was just bringing it up to let you know that that the training set is being shuffled so that is a good thing we want the training set to be shuffled but whenever we call validation split in this way this split occurs before the training set is shuffled meaning that if we created our training set and say we put all of the sick patients first and then the non sick patients second and then we say that we want to split off the last 10% of the training data to be our validation data it\'s going to take the last 10% of the training data and therefore it could just take all of the the second group that we put in the training set and not get any of the first group so I wanted to mention that because although the training data is being shuffled with the fit function if you haven\'t already shuffled your training data before you pass it to fit then you also use the split parameter it\'s important to know that your validation set is going to be the last X percent of your training set and therefore may not be shuffled and may yield some strange results because you think that everything has been shuffled when really it\'s only the training set has been shuffled after the validation set has been taken out so just keep that in mind the way that we created our training set before this episode we actually shuffled the training data before it\'s ever passed to the fit function so in the future whenever you\'re working with data it\'s a good idea to make sure that your data is also shuffled beforehand especially if you\'re going to be making use of the validation split parameter to create a validation set all right so now we\'ll run this cell one more time calling the fit function but this time not only will we see loss and accuracy metrics for the training set we\'ll also see these metrics for the validation set all right so the model has just finished running it\'s 30 epochs and now we see both the loss and accuracy on the left-hand side as well as the validation loss and validation accuracy on the right-hand side so we can see let\'s just look at the accuracy between the two they\'re both starting at around the same 50 percent mark and going up gradually around the same rate so we just scroll all the way to our last epoch we can see that the accuracy and validation accuracy are pretty similar with only one percent difference between the two and yeah the loss values are similar as well so we can see in this example that our model is not overfitting it is actually performing pretty well or just as well rather on the validation set as it is on the training set so our model is generalizing well if however we saw that the opposite case was true and our validation accuracy was seriously lagging behind our training accuracy then we know that we have a overfitting problem and we would need to take steps to address that issue alright so we\'ve now seen how to train the model how to validate the model and how to make use of both training and validation that\'s in the next episode we\'re going to see how to make use of a third data set the test data set to use the model for inference by the way we are currently in Vietnam filming this episode if you didn\'t know we also have a vlog channel where we document our travels and share a little bit more about ourselves so check that out at people\'s our vlog on YouTube also be sure to check out the corresponding blog for this episode along with other resources available on deep loser calm and check out the people\'s archive mine where you can gain exclusive access to perks and rewards thanks for contributing to collective intelligence I\'ll see you next time [Music] [Music]"", metadata={\'source\': \'U8Ixc2OLSkQ\'})]""""""', '""""""[Document(page_content=""[Music] in this video we\'re going to be demonstrating how to use care us to create a validation set in our previous video we showed how to train the model that we\'ve been working with in the past several videos of this playlist and we\'re going to continue working with the same model so as a prerequisite to this video I would recommend you watching all of the videos in my getting started with Karis playlist first so that you can fully understand where we are and what we\'re dealing with in this video the videos that will get you up to speed to where we are now we\'ll cover the prerequisites need to start working with Karos how to pre-process data for training building a Karass model and training a Karos model so assuming that these prerequisites have been met we\'re now going to show how to create a validation set in this video so before we show how to do that we need to discuss exactly what a validation set is and before directly answering that let\'s start with a bit of background so we\'ve built our training set that contains all of our samples and during each epoch our model is going to be getting trained over and over and over again on that data and continuing to learn the features of that data so the hope is then later that you can take this model and apply it to new data and the model will be able to accurately predict on data that it\'s not seen before based solely on the data that it was trained on so with the validation set you\'re essentially taking some percentage of your existing samples in your training set and saying no you\'re not going to train on these samples instead you\'re going to validate on them so you want to train on most of the data that you have and your training set and then you want to take data that you\'ve stored elsewhere and a validation set and have the model predict on that data during training and look and see how well it did so it\'s learning over and over again the features of the training set and then during each epoch when it\'s being trained it\'s going to be predicting on the validation set so moving forward you\'re not going to just be seeing the loss in accuracy on your training set you\'re also going to see loss and accuracy on your validation set as well the thing that\'s cool about this is that your models not seeing this validation data but or since it wasn\'t included in the training set so essentially it\'s brand-new data so the models taking what it\'s learned from that training and then trying to generalize and make a prediction on data that it\'s not seen before in the validation set and this allows you to see how well your model generalizes it also helps you make sure that you\'re not overfitting and over fitting is when your model is only learning the specificities to the training data and it\'s not able to generalize well on data outside of the training set all right so now that we\'ve got down what a validation set is how do we actually create one so there\'s two ways one is that you can just have a structure that holds your validation set you would tell the fit function which is the function that we call when we say model dot fit you would tell that about your validation set just like you would the training set in labels so let\'s just see how that would look so here\'s what we\'re calling fit like we did in previous videos so now if I created a set called valid set I would set it equal to a structure that looks like this it would be an array and each element in this array would be a tuple that would have a sample in its corresponding label and the next element would have a sample in a corresponding label and these samples and labels are not the ones that are included in your existing training set and labels this is going to be separate data so if we did that then whenever we called model dot fit we would pass in validation data equals and then we would pass in our variable here that we called a valid set and then whenever we ran this function the model would be training on everything in this scaled trained samples here along with its corresponding train labels and it would be validating on our valid set there\'s another way within Kerris that you can create a validation set and it\'s a bit simpler to me so let\'s get this out of here we\'re not going to use that anymore and rather than specifying validation data equals valid set we are going to specify validation split equals 0.1 so what is this doing caris is going to look at the scaled trained samples along with its course labels and it\'s going to split out 10% in this case because I\'m supposed to find point 1 but you can specify any fraction between 0 & 1 it\'s going to split out whatever you specify here into a validation set that\'s going to basically look like that validation set that we just created as an example so it\'s going to take this validation data the 10% that it\'s splitting off from the training samples it\'s going to hold it out and whatever it trains it\'s not going to be training on this 10% here it\'s going to be training on our scale train samples with their corresponding labels and then it\'s going to be validating on the 10% that you\'ve split out so what we\'re going to do now is just run this cell again so before this is what we saw we saw we have loss and accuracy being displayed for each epoch and we ran 20 epochs here we had lost going steadily down and accuracy going steadily up now let\'s see what it looks like whenever we run the same exact set function the only thing that we\'ve changed is that we added this validation split equals zero point 1 parameter all right so now we see we have similar output but we have two new attributes that are being shown so along with the loss and accuracy as we were seeing last time we now have Val loss and Val accuracy which is the loss in accuracy that we\'re getting only on this 10% of our data which is in our validation data set and if we look here we have this output that says training on 1890 samples validating on 210 samples so I have 20 100 samples total and I specified the validation split to be 10% so that\'s where this 210 is being calculated from so if we look now solely at the Lawson accuracy what we had last time we see it we\'re starting at 0.7 three on the loss and we are steadily going down until we reach about 0.3 0 on the accuracy we\'re starting at about 50% and we are steadily going up to again reaching about 93% all right so that is similar to what we saw last time when we didn\'t have a validation set now if we look at our validation loss so our loss calculated only on these 210 samples that are being validated on on each run through the data on each epoch we\'re going from point seven one loss and we are steadily declining until we reach about point one nine and then with the accuracy we\'re starting at about 50% steadily climbing until we\'re actually reaching about 100% accuracy on our validation set so like I said this is really good for you to be able to have to see how well your model is generalizing on data that it\'s not been trained on and which is ultimately able to tell you if your model is overfitting to your training set or not in this case we see that our models not overfitting and the reason why is because we have similar results both on our training loss and accuracy as well as our validation loss and accuracy so an indication of overfitting would be if our model was continuously going up in accuracy and down in loss for our training loss in accuracy but our validation loss and accuracy we\'re not doing so well so maybe they\'re stalling out or maybe the accuracy can\'t get past 50% that would be a good indication that our model is learning only the specificities of our training data and it\'s not generalizing well on data that it\'s not seen before now one last thing that I\'d like to point out before we wrap up here is whenever we specify this validation split equals 10% recall how I mentioned last time this shuffle equals true parameter means that all of the data that your model is training on it\'s going to be shuffled over each epoch that\'s not going to be true for your validation data so the validation split is actually taking whatever you specify here in my case 10% is going to take literally the last 10% of the data that are in your training samples so it\'s not going to be shuffled it\'s going to be the same data every time for over every run and it\'s going to be the last 10% that is here so because of that I did need to make a quick change in how I was generating the sample data that we used here so if you were following along from the beginning and actually writing the same code and using the same data as I was and used the data that I generated in the pre-processing data video then I\'m just going to scroll back up to the top of this notebook and show you the change that I made so that you can pause the video rearrange your code in a similar fashion as well and like I said the only reason I did it is so that the data at the end of my training samples was at all uniform because then our validation split wouldn\'t be a very accurate depiction it would all be uniform data and that our model wouldn\'t be able to validate really well on that so if we scroll back up here this is where I generated the sample data so I\'ll just leave this on the screen here so that you can maybe pause the video and compare what I have in this cell to what you wrote previously so that you can rearrange the only thing that I did actually is change the order in which these four loops run so I\'ll let you take a look at that let me know if you have any questions about that step in the comments below in future videos we\'re going to continue learning new techniques that we can use with Kara\'s we\'re going to start building more complex models and convolutional neural networks so I hope you stick around for those and I hope you found this video helpful if you did please like the video subscribe suggest and comment and thanks for watching [Music]"", metadata={\'source\': \'dzoh8cfnvnI\'})]""""""']","{'https://stackoverflow.com/questions/50184144/shuffle-in-the-model-fit-of-keras', 'https://stackoverflow.com/questions/64356209/how-does-model-fit-methods-shuffle-deals-with-batches-when-using-a-tf-data-da', 'https://stackoverflow.com/questions/50955798/keras-model-fit-with-tf-dataset-api-validation-data'}","['""""""How does Model.fit() method\'s shuffle deals with Batches when using a tf.data.Dataset? \n\nI am using tensorflow 2. When using the Model.fit() method with a tf.data.Dataset, the argument \'batch_size\' is ignored. Thus to train my model on batches, I have to first change my dataset of samples into a dataset of batches of samples by calling tf.data.Dataset.batch(batch_size). Then, after reading the documentation, I don\'t understand clearly how the .fit() method will shuffle my dataset at each epoch. Since my dataset is a dataset of batches, will it shuffle the batches among each other (the batches remain unchanged) ? Or will it shuffle all the samples and then regroup them into new batches (which is the desired behaviour) ? Thanks a lot for your help. The shuffle parameter has no effect on the fit function when using the tf.data.Dataset API. If we read the documentation (emphasis is mine) :\n\nshuffle: Boolean (whether to shuffle the training data before each epoch) or str (for \'batch\'). This argument is ignored when x is a generator. \'batch\' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None. It\'s not super clear, but we can have a hint that the shuffle argument will be ignored when using a tf.data.Dataset, as it behave like a generator. To be certain, lets dive in the code.""""""', '"""""" Keras model.fit() with tf.dataset API + validation_data\n\nSo I have got my keras model to work with a tf.Dataset through the following code:\n\n# Initialize batch generators(returns tf.Dataset) batch_train = build_features.get_train_batches(batch_size=batch_size) # Create TensorFlow Iterator object iterator = batch_train.make_one_shot_iterator() dataset_inputs, dataset_labels = iterator.get_next() # Create Model logits = .....(some layers) keras.models.Model(inputs=dataset_inputs, outputs=logits) # Train network model.compile(optimizer=train_opt, loss=model_loss, target_tensors=[dataset_labels]) model.fit(epochs=epochs, steps_per_epoch=num_batches, callbacks=callbacks, verbose=1)\n\nhowever when I try to pass validation_data parameter to the model. fit it tells me that I cannot use it with the generator. Is there a way to use validation while using tf.Dataset\n\nfor example in tensorflow I could do the following:\n\n# initialize batch generators batch_train = build_features.get_train_batches(batch_size=batch_size) batch_valid = build_features.get_valid_batches(batch_size=batch_size) # create TensorFlow Iterator object iterator = tf.data.Iterator.from_structure(batch_train.output_types, batch_train.output_shapes) # create two initialization ops to switch between the datasets init_op_train = iterator.make_initializer(batch_train) init_op_valid = iterator.make_initializer(batch_valid)\n\nthen just use sess.run(init_op_train) and sess.run(init_op_valid) to switch between the datasets\n\nI tried implementing a callback that does just that (switch to validation set, predict and back) but it tells me I can\'t use model.predict in a callback\n\ncan someone help me get validation working with Keras+Tf.Dataset\n\nedit: incorporate answer into the code\n\nso FINALLY what worked for me, thanks to the selected answer is:\n\n# Initialize batch generators(returns tf.Dataset) batch_train = # returns tf.Dataset batch_valid = # returns tf.Dataset # Create TensorFlow Iterator object and wrap it in a generator itr_train = make_iterator(batch_train) itr_valid = make_iterator(batch_train) # Create Model logits = # the keras model keras.models.Model(inputs=dataset_inputs, outputs=logits) # Train network model.compile(optimizer=train_opt, loss=model_loss, target_tensors=[dataset_labels]) model.fit_generator( generator=itr_train, validation_data=itr_valid, validation_steps=batch_size, epochs=epochs, steps_per_epoch=num_batches, callbacks=cbs, verbose=1, workers=0) def make_iterator(dataset): iterator = dataset.make_one_shot_iterator() next_val = iterator.get_next() with K.get_session().as_default() as sess: while True: *inputs, labels = sess.run(next_val) yield inputs, labels\n\nThis doesn\'t introduce any overhead\n\nAfter your change, how do you get dataset_inputs into model? I\'m not getting how line keras.models.Model(inputs=dataset_inputs, outputs=logits), and i\'m assuming this is the contents of the ""model"" variable, could you please complete the code, i have the exact same problem but can\'t seem to know how to apply your code, thanks in advance\n\n@mark rofail, I believe this line is incorrect and should receive batch_valid: itr_valid = make_iterator(batch_train)\n\n I solved the problem by using fit_genertor. I found the solution here. I applied @Dat-Nguyen\'s solution. You need simply to create two iterators, one for training and one for validation and then create your own generator where you will extract batches from the dataset and provide the data in form of (batch_data, batch_labels) . Finally in model.fit_generator you will pass the train_generator and validation_generator. \n\nso I have to wrap tensorflow iterators in a python generator like: iterator = ds.make_one_shot_iterator() while True: next_val = iterator.get_next() yield sess.run(next_val)\n\n.""""""', '""""""There is described all the details of DataSet API. Your question is about iterating over the data several times. Here are two solutions for that:\n\nIterating all epochs at once, no information about end of individual epochs\n\nimport tensorflow as tf epoch = 10 dataset = tf.data.Dataset.range(100) dataset = dataset.repeat(epoch) iterator = dataset.make_one_shot_iterator() next_element = iterator.get_next() sess = tf.Session() num_batch = 0 j = 0 while True: try: value = sess.run(next_element) assert j == value j += 1 num_batch += 1 if j > 99: # new epoch j = 0 except tf.errors.OutOfRangeError: break print (""Num Batch: "", num_batch)\n\nSecond option inform you about ending each of epoch, so you can ex. check validation loss:\n\nimport tensorflow as tf epoch = 10 dataset = tf.data.Dataset.range(100) iterator = dataset.make_initializable_iterator() next_element = iterator.get_next() sess = tf.Session() num_batch = 0 for e in range(epoch): print (""Epoch: "", e) j = 0 sess.run(iterator.initializer) while True: try: value = sess.run(next_element) assert j == value j += 1 num_batch += 1 except tf.errors.OutOfRangeError: break print (""Num Batch: "", num_batch)\n\n\n\nIf your tensorflow version is 1.3+, I recommend the high-level API tf.train.MonitoredTrainingSession. The sess created by this API can automatically detect tf.errors.OutOfRangeError with sess.should_stop(). For most of training situations, you need to shuffle data and get a batch each step, I have added these in the following code. import tensorflow as tf epoch = 10 dataset = tf.data.Dataset.range(100) dataset = dataset.shuffle(buffer_size=100) # comment this line if you don\'t want to shuffle data dataset = dataset.batch(batch_size=32) # batch_size=1 if you want to get only one element per step dataset = dataset.repeat(epoch) iterator = dataset.make_one_shot_iterator() next_element = iterator.get_next() num_batch = 0 with tf.train.MonitoredTrainingSession() as sess: while not sess.should_stop(): value = sess.run(next_element) num_batch += 1 print(""Num Batch: "", num_batch)\n\n\n\n\n\nwhile True: try: print(sess.run(value)) except tf.errors.OutOfRangeError: break\n\nWhenever the dataset iterator reaches the end of the data, it will raise tf.errors.OutOfRangeError, you can catch it with except and start the dataset from the beginning. \n\nYou should explain your code or include comments as well\n\n\n\nSimilar to Toms answer, for tensorflow 2+, you can use the following high-level API calls (the code proposed in his answer is deprecated in tensorflow 2+):\n\nepoch = 10 batch_size = 32 dataset = tf.data.Dataset.range(100) dataset = dataset.shuffle(buffer_size=100) # comment this line if you don\'t want to shuffle data dataset = dataset.batch(batch_size=batch_size) dataset = dataset.repeat(epoch) num_batch = 0 for batch in dataset: num_batch += 1 print(""Num Batch: "", num_batch)\n\nA helpful call to track progress is the total number of batches that will be iterated over (to be used after the batch and the repeat calls):\n\nnum_batches = tf.data.experimental.cardinality(dataset)\n\nNote that currently (tensorflow 2.1), the cardinality method is still experimental. """"""']""""","When using the `tf.keras.Model.fit` function with a `tf.data.Dataset`, it is important to understand how to properly prepare your dataset for training. The `batch_size` argument in `fit` is ignored when the input is a `tf.data.Dataset`. Therefore, you must call `Dataset.batch(batch_size)` on your dataset before passing it to `fit`. This ensures that your data is processed in batches during training.

You should also call `Dataset.repeat(epochs)` on your dataset if you want to iterate over the dataset for multiple epochs. This is necessary because `fit` will not automatically repeat the dataset for multiple epochs if it is a `tf.data.Dataset`.

Regarding shuffling, you can and should shuffle your dataset to ensure that the training data is randomized. It is recommended to call `Dataset.shuffle(buffer_size)` before batching and repeating the dataset. This ensures that the data is shuffled at the sample level before being grouped into batches, providing a more randomized training process.

For the validation set, you should apply the same treatment: batch, repeat, and shuffle (if desired) the validation dataset before passing it to `fit` via the `validation_data` argument. This ensures consistency in how both the training and validation datasets are processed.

In summary, when using `tf.data.Dataset` with `fit`, you should call `Dataset.batch(batch_size)` and `Dataset.repeat(epochs)` on your dataset. Additionally, you should shuffle your dataset before batching and repeating it. This approach ensures that your data is properly prepared for training and validation, avoiding errors related to the `batch_size` argument and ensuring a randomized training process."
55909188,tf.nn.conv2d,example required,How can I apply a TensorFlow 2D Convolution (tf.nn.conv2d) to a single (non-batch) 2D image?,"<p>I would like to use the function <code>tf.nn.conv2d()</code> on a <strong>single</strong> image example, but the TensorFlow documentation seems to only mention applying this transformation to a <strong>batch</strong> of images. </p>

<p>The docs mention that the input image must be of shape <code>[batch, in_height, in_width, in_channels]</code> and the kernel must be of shape <code>[filter_height, filter_width, in_channels, out_channels]</code>. However, what is the most straightforward way to achieve 2D convolution with input shape <code>[in_height, in_width, in_channels]</code>?</p>

<p>Here is an example of the current approach, where <code>img</code> has shape (height, width, channels):</p>

<pre><code>img = tf.random_uniform((10,10,3))  # a single image
img = tf.nn.conv2d([img], kernel)[0] # creating a batch of 1, then indexing the single example
</code></pre>

<p>I am reshaping the input as follows:</p>

<p><code>[in_height, in_width, in_channels]-&gt;[1, in_height, in_width, in_channels]-&gt;[in_height, in_width, in_channels]</code> </p>

<p>This feels like an unnecessary and costly operation when I am only interested in transforming one example.</p>

<p>Is there a simple/standard way to do this that doesn't involve reshaping?</p>
","<p>AFAIK there is no way around it. It seems (<a href=""https://stackoverflow.com/questions/50779869/does-tensorflow-tf-slice-incur-allocation-and-or-memory-copy"">here</a> and <a href=""https://stackoverflow.com/questions/53398721/tensorflow-can-reshape-create-a-copy"">here</a>) that the first operation creates a copy (someone correct me if I'm wrong). You may use <a href=""https://www.tensorflow.org/api_docs/python/tf/expand_dims"" rel=""nofollow noreferrer""><code>tf.expand_dims</code></a> instead though, it's IMO more readable because of it's verbosity.</p>

<p>On the other hand, taking <code>0</code> element from the tensor should not perform a copy in this case and is almost free.</p>

<p><strong>Most importantly</strong>, except for a little inconvenience with syntax (e.g. <code>[0]</code>) those operations definitely <strong>are not costly</strong>, especially in the context of performing convolution.</p>

<p>BTW. Other ready alternative layers like the ones in <code>tf.keras</code>, require batch as first dimension as well.</p>
",,"['How to use tf.nn.conv2d on a single image in TensorFlow?', 'TensorFlow tf.nn.conv2d single image example', 'Efficient way to apply 2D convolution on a single image using TensorFlow', 'Avoid reshaping when using tf.nn.conv2d on a single image', 'TensorFlow 2D convolution without batch dimension', 'Best practices for using tf.nn.conv2d with single image input']","['How to use tf.nn.conv2d on a single image in TensorFlow?', 'Is there a way to apply tf.nn.conv2d without reshaping a single image to a batch of one?', 'Efficient methods to perform 2D convolution on a single image using TensorFlow', 'Can tf.nn.conv2d be used directly on an image with shape [in_height, in_width, in_channels]?', 'Best practices for applying convolution to a single image in TensorFlow']",{'https://www.youtube.com/watch?v=7gGxBGvSAa0'},"['""""""[Document(page_content=""hi welcome back to you new listen this is let\'s me can\'t you re and we are gonna use a tensor flow 2.0 with the inbuilt Kara\'s to make a two dimensional CNN in which we will be using C for 10 dataset and it will be a kind of image classifier and the dataset is available at Carol here and the C for 10 object recognition in images it has total 60,000 the training images and ten thousands for for the testing and and and the size of each image is 32 cross 32 pixels and it is RGB image color image does mean the the size of each image is 32 cross 32 cross 3 and the last three is three dimension is four red blue and green green pixels to represent its its image into a at the color image perfect so and the another thing here new in this lesson is that instead of using Jupiter notebook we are gonna use here at Google collab why we need to use Google : because we need to implement deep learning algorithm and the Google the collab provides a free GPU and the tip you and the size of RAM and the disk you can get from here and it says that I think yeah it\'s a 12 GB of RAM and 48 and 49 it\'s you can see around 50 GB of a hard disk so the Google Cola provides this in a free of cost and you can go ahead at this link cool I have not researched or google.com and once you open this it will open this notebook welcome dot IP and why we are the default page and then you need to create here a new Python 3 notebook right so it will create a new file entry notebook and although this is the online color but moreover it\'s like the Jupiter notebook which we have been using and then you can change here the name name of your file and other things will be almost the same like inserting the text let\'s say okay and if you want to bring this text upper you can go ahead and move these syllables and then this is kind of the mark of where you can give your heading okay so and then here you can code so this is how you will be working with the google cool laughs notebook now let\'s go ahead and come back to deletion so I have already created this notebook online so I\'m going to work with this notebook but you can work in this new notebook otherwise you have some other the way to you know to download my this notebook from the github repository the link for this notebook is given into a video description and then you can go out here at add the file and then you can upload a notebook so you can upload it from your computer itself otherwise from Google Drive or github okay so all these options are available perfect alright so let\'s come back here and in this lesson we are gonna cover what is CNN and the important terms of a scene in like what is the convolutional layer activation function a filter or a kernel size with the number of filters stride size padding max pooling flattening and dense layer and then finally we will start working with download of the data and model building so these convolutional neural network which is in sort known as CNN is a class of deep neural networks and most commonly it is applied in to our images and you know the visual imaginary techniques and these sealants are regular eyes to version of multi-layer perceptrons which we have seen in our previous video and if you don\'t know you can go ahead at my channel kgp talky there you can see all those previous videos or the deep learning and the Fightin\'s and along with the classical machine learning with the feature selections and so so the multi-layer perceptrons usually means a fully connected networks that is each neuron in one layer is connected to all neurons in the next layer right but in but in the in the convolutional neural network it doesn\'t happen like that because in that case like a multi-layer perceptron model or a fully connected layers and in that case the models are very prone to overfit so that is why the ceilings are more over the regularized two version of multi-layer perceptrons right and these convolutional layers are inspired by biological process in there the connectivity patterns between the neurons resembles and the organization of the you know the visual cortex so this is how it it is working and these are you can say the convolutional layer and these are the pooling layers and then finally the convolutional neural networks have a flattening layers and fully connected layers in the last are the network and then finally with the softmax softmax activation function it cry it detects the classes it detects the classes perfect now let\'s go ahead and see what are the important terms like what is the convolutional layer what is the activation functions what is the pooling layers and what are the flattening fully connected layers so the convolutional layers as I said that so this is kind of regularized fortune of multi-layer perceptron and moreover it uses the techniques kind of biological techniques visual cortex technique so there it try to the scan image are the two-dimensional at the data are three dimensional multi-dimensional data and nowadays the one dimensional CNN is also coming so he tries to the scan the data and and the size of this scanner is the filter size and otherwise it is also known as a kernel and then it it scans and then it tries to find out the features of these images okay so then it stress the features and you see here this number of you know these number of the layers so these are tea you can say the depth of the convolutional neural network otherwise you can see these are the total number of failures and these things you know go on if you make a multi layer the convolutional neural network and but you know the data gets larger and the larger as you add the number of filters more then what we need to do we need to minimize the data with the minimal loss of the information then we come with the new term moon as a max pooling so we apply some kind of here a filter in which we we see all the parameters within those filter and then we get the and we extract the maximum numbers from that filters and then finally the information get you know the feed it to the fully connected layers before the Platinum the flattening and then after fully connected layers and then finally with the softmax or the sigmoid if there is two two classes 2d tapes then the see what can be used but if there are multiple classes then we need to use here the softmax are some other kind of under the activation function so the question is what is the activation function if you remember in my previous video I have explained what is these neurons so this is kind of you know the biological in our brain so brains are made of the billions of the billion neurons and the similarly the machine learning artificial neural networks we are multiple neurons and there\'s been the multiple neurons with multiplied by the weights the input weights and this these comes to a single points okay these comes to a single point and there with the activation functions they get multiplied and they get multiplied and you know a dash together so these the activation function desire decides where what will be the final value okay so there are multiple kind of the activation functions like here a sigmoid which have just zero to one value to maximum and here we have a tiny we have a rail ooh but the real you have a problem if input is you know the less than the zero then then there is no gradient so so the another version of the rail you comes like is Leakey value so the leaky Valu comes with some kind of the gradient if input is negative and then we have here a maxout and the ALU this kind of you know activation functions as I told you the filter size the filter size means you know how many filters see each filters extracts different kind of the features from the image so we need to apply there the multiple filter multiple filters so that it can scan the images be correctly and it can extract the maximum amount of the information but the number of filters should not you know the exceed some you know some particular level otherwise your model might get or fit okay and these number of filters are also known as a depth of your you know the convolutional neural network for that particular Network okay I mean the D depth of B filters actually and then here we have a stride sighs okay let\'s go ahead and see what is this tri size and the padding let\'s suppose that you have these just these without the zero so this is your two dimensional network and these the blue you know the blue rectangle this blue square is the filter hour and these the filter size total number of pixels 3 cross 3 is the filter size and how it is moving I mean how many pixels it is skipping that is known as a stride size and if you see in this it is skipping just one that is keeping just one one pixel that\'s mean default I stride size here each one and the padding means after this you know the actual data there is zero padded along with all those height although the dimensions all these four dimensions so that is known as your padding so the padding get sided with the zero and then these the filter goes along and it scans two dimensional data and then finally it gets multiplied by with this cardinal value and then finally it extracts these features out of this out of these images okay and these are the more relevant features perfect and the max pooling is kind of it gets the maximum value see here we have max pooling and the average pooling so let\'s say these are the square in square it detects the maximum number so in this that we have a maximum 20 and then with this tight size of the two so this move here along with this side with the two pixels then it selects the maximum here is the 30 and in this square we have a maximum 112 and here 37 and similarly it is taking every of all these force here okay now let\'s go ahead and to see the final layer which is flattening in the dense layer in the CNN we definitely need a flattening and the dense layer in between these congressional layers and the fully connected layer there has to be a flattening layer we transform this two-dimensional data into a matrix feed into it to a into a vector then finally that can be fed into a fully connected neural into a classifier and then after with the softmax our sigmoid regular resistor Sigma and softmax are a sigmoid activation function then we finally can ticket the final output perfect so this is all about in the theory now let\'s go ahead and learn how we can download the data and then how we can finally implement our two-dimensional the convolutional neural network into tensorflow 2.0 all right so let\'s the start download the data and the model building we need to first since we are working with the in the collab so every time we need to install a tensorflow GPU and we will be also working with the email extent library to plot our confusion matrix after the model training so we need to install these two library it\'s not like a typical computer where we need to just install a single time the as many number of times you reset your runtime you need to install this live but before installing these libraries come back to the runtime once you come here at the runtime then you need to change your runtime type here okay so if you have a nun then you need to either select a GPU or a TPU I have found that the TPU is a little faster than the GPU so I\'m going to select here a TPU and select a TPU and the save it so once you save it now the backend of this Google Kolev file will be using it tip you and after that let\'s go ahead and install these two libraries you need to install every time okay how many times you open this file after closing this file you need to install otherwise here it will not work so let\'s go ahead and run this it will install tensorflow GPU 2.0 Vita version so currently we have a tensorflow 2.0 Vita version only and it takes sometimes a little time so let\'s go ahead run this one and this cell as well and here we have very recent words any zero point one seven zero if you don\'t put this line then it installs ml extends 0.14 version that is lower than you know three versions lower than the current versions and that doesn\'t supports you know the advanced techniques in ml extend perfect so just let\'s wait for some time to install these two library I think it has an install so now let\'s go ahead and import tensorflow as a TF in meantime we can keep writing the code while it is installing so we are gonna here import TF sorry we are gonna import here a tensorflow as DF and in meantime we are gonna import Kara\'s from a tensorflow I don\'t think we need Akira so we are not gonna import Kira\'s but we are gonna import the sequences model and and the layers so here we have a tensor view dot sorry dot Kira\'s import here sequins ear okay and after need after that we need to import these layers like a flattening layer so dense convolutional smack spooling and dropouts and and vice normalizes I I think I\'ll take another lessons and West normalization so in this lesson we are not gonna work with match normalization okay so here we have installed over the tensorflow GPU version of 2.0 tensorflow and we have installed ml x10 0.17 we are good to go now we are gonna import I\'m sorry I this should be actually from instead of import so here from the tensorflow sorry now so here we have a tensor flow dot Kira\'s dot layers and then we need to import flatten dense if you are wondering how I am getting these suggestions then I am using kind of advanced version of Google cool air editor you can also switch to advanced version of Google cool air editor you need to come here these are the new features if it is really well then there you would get an options there okay if you want to go to check a new editor then you can just tell you to change this helical type and I am finding this editor is quite good so I\'m going to stay opt in for this editor okay so while on the fly it suggests you so here we have now dents and then here we have a convolutional 2d that\'s the kana 2d and then we have here a max pool but the max pool today so if you type here the max fool you get so many you know the options like max pool 2d and the max pooling 2d and max pool today and the max pooling TD is Max pulling 2d is the same thing so you don\'t to worry you know anything now let\'s go ahead and you know use the drop out as well so these are the some layers which we will be using like flattened ends come to d-max pull to D in the drop out it says that it doesn\'t have sequins here so let\'s go ahead and see ya here it has sequins here ok so we have got here our layers and the Kira\'s model now let\'s go ahead and check the version the tensorflow version which we can change which we can check by D F dot and then version so let\'s go ahead and run this it should say here the 2.0 release candidate zero which is a Vita version now let\'s go ahead and the import supporting libraries as well like numpy pandas in my flat lip although we don\'t need panels but we will we might need an umpire so I\'m just going to import the numpy SNP and I\'m gonna import the matplotlib dot Phi plot as PFD and I\'m gonna also import Matt Ratliff we will be needing this Matt float live the during the plot of our the confusion matrix perfect now let\'s go ahead so we have imported no necessary necessary packages now we need to download the data so we are going to work with the c-4 dataset which we have here in at the Kira\'s but we are not going to download from the Kira\'s the tensorflow 2.0 automatically have it here you know the c4 10 dataset so we are going to download that from Kira\'s data sets itself so that we can download from Kira\'s sorry for tensorflow dot Kira\'s and then dot data sets and then here we have import and then finally here we have C 14 perfect so we have imported C 14 reference so let\'s go ahead and download this data set which we can download data sent by calling here by calling the c14 then dot load data so it will return at Apple for training in the testing so we will get to Tuffle here so the first of all is for training and the data file is for testing so let\'s go ahead and read this data into X underscore train and the y underscore train and then finally X underscore test and then Y underscore test right so let\'s go ahead and load this data so it takes a little time to download a complete data in meantime you see here this data set have a ten classes and c14 these large teams represents that it has a ten classes if you come here at the data it might under so you otherwise you yeah I think it has here so these are the ten classes which are the airplane Auto Mobile\'s bird cat etc so we are going to create here a list of the classes so they\'re the classes name and we are gonna create this list like we have here airplane images of airplane and then here we have images of the automobile and then we have images of bird as well as we have images of cat and then we have images of deer as well and the images of dog and then finally we have images a frog then finally we have horse and sheep and the truck so here we have sheep and then finally we have a truck perfect so these are the classes which we have in our c4 ten dataset we have already got the training and the testing data set since these images which we are gonna huge however are the maximum value which is 255 and how you can get that you can write year X underscore train and then dot match it will return you the maximum value present in this training data set which is 255 but the artificial neural network in general the prefer if value is closer to 1 since we have seen in two hour you know the activation function so the mostly the value we get the maximum gradients when we lose our somewhere nearby Cheeto okay r1 so that\'s why we prefer mostly if values you know almost nearby a zero so moreover we would prefer if it is in between somewhere zero to one so what we are going to do here we are going to divide our X train data set with the 255 so that the maximum value should be 1 and similarly we are gonna divide our X test dataset with 255 as well so now maximum value has come under 1 now let\'s go ahead and check the safe of these data sets so here we have a safe which should be 32 by 32 and then 3 so this is the safe of each images and this says that we have total 50,000 image actually I am sorry initially I told you that we have 60,000 images but we have actually you know the 50,000 images and 10,000 images for training purposes right so which we can check here with X test dot zip and there it says that we have a 10,000 image and each images have 32 cross 32 width pixels and each pixels have a 3 depth which is RGB that\'s mean each pixels are color of XS now let\'s go ahead and see see some images how we can see those images with the PLT dot I am I am so so with these I am so which is image so with this we can pass here the first value of x test are extreme whatever with the 0 that\'s mean it will show a first image so here you get this is a bird actually how would you make sure that season bar you can print here at whitest and if you print whitelist you see your first level is 3 and the 3 is which one is 3 ah it is actually the kite ok so this is the 3 since it starts from the 0 that\'s why this is actually a kite okay so this array starts from 0 to 9 maximum ok so it says that it has a 3 there perfect so now let\'s go ahead and build our CNN model and before building scene and model let\'s go ahead and insert a few cells for coding so that we can get in a free space to code and before that I\'m gonna insert a text here where I\'m gonna write build CNN model alright so here I\'m gonna build a scene in model so scene and building a scene in model is fairly simple we need to first get a sequential object here so once we get this sequential model and then we are going to I adhere the convolutional layers are flattened layers and the dense layers so the model dot ahead and then here we have Cana 2d okay so here we have a convolutional 2d inside the convolutional 2d now you see it ask suppose how many filters do you want what should be a kernel size what is a stride size what would be the padding and what will be the data formats etc and that Commission size way so these things we can pass here so let us go ahead and pass these information so the first we are gonna pass here of filters how many filters we are going to use we are going to use initially 32 filters so it is up to you how many filters you want to use moreover it depends on testing and the trial error so you need to apply a multiple type of the filters with a multiple size then you can select which one is here now giving maximum results for your task so let\'s go ahead and fit a kernel size which is a 3 here if you give just a single name of 3 and then it will make a 3 by 3 otherwise you can give here a couple like a three three by two okay as well so let\'s go ahead and make it simple here so we have here a 3 cross 3 carnal sides and now here the pairing which we are gonna use instead of valid we are gonna use the same pairing here and then the activation function we are gonna here huge array Lu actuation and the input shape so input CF we have initially how many input hold the shape of our input that is a 32 cross 32 cross 3 so we have 32 across 32 cross 3 right so these are the our inputs if so we have build our the first model now let us go ahead and run this and see and see okay what happens it says that there is error perhaps I think yes we need to close this one as well so now you see we have here we will add the new convolutional layer perfect so now let\'s go ahead and actually the best practice to keep all those the model building at the same in the same cell otherwise sometimes what happens you you know if you run a cell multiple times so you could get disturbs and now we have added the first convolutional layer so let\'s go ahead and copy this and paste it here and since we are going to add the another kernel snow layer that will be the inside so in that case we don\'t need actually the input safe okay so in that case we can delete this input safe so let\'s go ahead and delete why if you see these can volitional layers so these are the first Canales the layer where we need to define what is the input size but after that we don\'t need to define the input size all right and now we are going to apply here max pooling okay so have we imported the max pool yes so we have a max pool and the drop okay so let\'s go ahead and add a max pooling as well so that we can reduce the overhead so here we have a max pool 2d and the max pool 2d it asked about what will be the full size okay that\'s fine what will be decides on which we are going to look for the maximum value so the pole size we are gonna use here a 2 cross 2 right and then what will be destroyed which we are going to use so the stride size I am gonna use here too and then it asks about what will be depending okay so I\'m gonna use so pairing is you know the valid can see right so in case of the invalid pairing it try to you know reduce the dimensions but with the same padding it try to keep the same dimensions on which we are applying the filter so we have max pull the data now let\'s go ahead and apply a dropout so after a max pooling I am also going to apply a dropout so the dropout what it will do it will try to you know it will try to make sure that the some cell in convolutional layer gets died out okay so that it should not over fit actually so the dropout actually prevent overfitting alright so we have here a dropout in which I\'m going to drop 50 percents of input so that is 0.5 so the 50% input will not be the considered while calculating the next stage now let us go ahead so here we are going to just use the 2 layer of the convolutional layer and i\'m now going to add a flattening layer so here we have a model dot and and then here we have here a flattening layer with a flattened and after this I\'m going to add a finally a dense layer okay so we have a model dot ad we have got our flattening layer now I\'m going to add here a dense layer so the dense layer asks the vols mult asks about her multiple input parameters like how many units that\'s mean how many are how many neurons we want we want let\'s say the 128 neurons and then it says that which activation function we want to apply on these freely connected networks which is also known as internet work I\'m going to say that we want here array Lu so on this activation on this fully connected layer I have applied here riilu activation function now let\'s go ahead and add a final output layer so in output layer we need to keep the the units which will be the equal which will be equal to number of outputs so here we should have units is equal to 10 since we have a ten classes to predict and then the activation function we should keep here softmax we cannot use riilu we cannot use sigmoid and we should use here softmax why we cannot use a Sigma because Sigma can\'t predict only the binary prediction 0 r1 so here we have a sort max finally we are done with the building our model so the model has been builded let us go ahead and print the summary of this model which we can print with model dot summary and in this you see this is summary of our model so we have here two convolutional layer and then finally we have a max pooling after second convolutional layer then we have a dropout after that I have added a flattening layer then dense layer and then finally output layer which is also a dense layer a kind of okay so this is one of the very simplest convolutional neural network if you want to add the more then what you can do you can just copy these three lines and you can paste it here and you can keep pasting you can increase the size of your model okay right so all the way but as of now we are going to just to go ahead with this simple model so that it should not take much you know the training time otherwise we need to wait for much larger much much larger time to complete the training if we make a complex network now let\'s go ahead compile our model so we can compile our model by calling model dot compiled and there we need to pass what is the loss what should be the optimizes and what should be the metrics on which your model should be optimized so so I\'m gonna first pass here the optimal here and I\'m gonna use here ada mokka measure and then it is asking about what should we do love us okay so I\'m gonna use loss which will be a sparse categorical cross entropy so here we have sparse categorical cross entropy then finally it asks about what should be the metrics on which we are going to calculate the output off okay we are going to optimize our model so so the metrics on which we are going to optimize our model is sparse categorical accuracy which we can get by here with sparse categorical accuracy perfect so let us go ahead and compile this model and after this now the most exciting part of this convolutional neural network is model fitting so once we type here model dot fit there you see it asks about multiple parameter like X train and Y train the best size and a pox and the verbose so what we are going to do here we are going to the pass here the first extreme dataset on which the training will be done and then why trained by which it will be compared and then I\'m gonna pass here the bite size I\'m gonna pass here a 10 bite size and then it asks about how many a pox we want so here I\'m gonna pass a total teeny pox and then it finally asks about the huevos okay so here I\'m gonna pass over woes is equal to 1 that\'s mean it will keep printing the progress of our training model and then I\'m gonna pass the validation data as well so what should be a validation data that\'s been during the training it will also keep validating our model okay so for a validation data what I am going to pass here I\'m going to pass here at Apple that\'s mean the X test and the Y test will be used for a validation okay so this is the all information and other informations we can neglect right so one more thing if you see here our model dot faith returns returns our history of our training a tree in the model okay so to record that history what we can do we can just record that history if we you know take that parameter in history so after the training this model then we will be getting all the history for each a box its training accuracies and the loss for validation in the training set then finally we can plot the learning curve of this model as well so let\'s go ahead and hit the enter it will take some time to complete the training of this model so we need to wait to who get it done so it might take about an hour to complete this training although we are using the GPU so this was the difficult to train on your computer so if you are using a computer please don\'t run it in your computer if it doesn\'t have a GPU if you are working in the CPU then it can\'t do you need to come back here at the Google cool F where you can play at the GPU on this training on this training of seen in model so let us go ahead and wait for some time to get it complete alright so our model has been trained here so we had selected a batch size tin box tin and verbose one and this is a complete verbose to our model and it says that here the maximum training I could see it has achieved at the point six five but if you see here a validation accuracy well it is an accuracy is just seventy point to it this is the difference okay in between accuracy the training and validation accuracy says that our model is you were 15 if well it is an accuracy is less than training accuracy then the mobile model over field and if validation accuracy is you know the more than the training accuracy that\'s when our model is under fitting so in this case our model is overfitting so let\'s plot our this learning curve against the loss I\'m the accuracy of a training and validation set then let\'s understand with this loss how these models are overfitting and at which training epoch model started overfitting perhaps after fourth ��poque here you see the fourth epoch which we have the training accuracy is around 70 percents and here it\'s around the 68% and perhaps after this after the third epoch and between the fourth high pop the model started overfitting so let\'s go ahead and plot this and see how it is overfitting so we need to plot it in in the matplotlib to do that what we we need to do here we need to the clock the call matplotlib PLT function so I have already written the code since this is a very traditional code and if you don\'t know how to plot do plotting in the mat flatly then what you can do you can to go ahead my my flat lip tutorial here and I have shown there a detailed you know the matplotlib tutorial and you can learn more about the matplotlib so let\'s come back to this cell here let me explain you the info currents which we are taking here since this is a 10-8 box so we should take here 11 the range will be 1 to 10 and here we are plotting a PO currents against the one to ten that\'s mean here the epoch which we had selected for her those number of epochs for which we had three in our model and here you see the history dot history so the history returns the it\'s kind of the call back which will cause all these information that\'s been the losses and I couldn\'t see over the training set as well as for winding sunset and then the history taught history so this is kind of our the dictionary the sparse categorical accuracy so I mean these values is there and validation sparse degree accuracy these values will be you know recorded is recorded actually in digital in these variables and India just the title and the Y and X level along with diligence and then he had the same we are doing for a loss as well history taught history with in in log in the dictionary and the lost keys and the validation lost key is representing the loss of the training and the weld is sunset respectively so let\'s go ahead and run this cell and see a plot of learning curve so here we have a plot of the learning curve and in this you see the accuracy of a training set it\'s kept increasing okay for each of these epochs but the accuracy for a validation say it has came to you know the V Lowden the training set after the third Apoc that\'s mean it is saying that that\'s mean it is saying that the accuracy the accuracy validation accuracy is less than the training set that\'s been the model is your footing and here with the loss you can also verify that a validation loss has you know saturated after the three epoch that\'s mean the model is not learning but we you know we kept running our model and I I mean we kept training our model and the training error has you know the training loss kept decreasing but the validation loss was not decreasing this says that our model is overfitting that\'s mean we had increased the complexity of our model but but it doesn\'t love actually you know generalize it it could not generalize actually and the one more thing you can\'t make the changes here in the model you can copy in the paste it and you can similarly change the drop out layers and the filter size stride size the kernel sizes are all or the filter size and then finally you can you can really in this model I had taken the simple CNN model because it takes a lot of the time it has taken almost one hour in my you know this cool ever the notebook it will lie I think I will take almost the same time in your cool air as well so that\'s why I had taken just as small and simple scene in model for this so let\'s go ahead and plot a confusion matrix for this to plot a confusion matrix as I have already told initially in this video that we need to install a mail exchanger library so let\'s go ahead and import broad conclusion matrix from ml extant library so from that we need to write here email email extent dot the plotting and then we need to write here import and then brought confusion matrix and then here from a scalar so hidden from a scale and dot matrix import the confusion matrix so here we have a confusion matrix now we are going to import the mat floor I think I have already imported the matplotlib so I don\'t need to import that one now let\'s go ahead and the import now now let\'s go ahead and do a pull and the class predict so what we can do here we can call here Y prayer is equal to the model and then dot are the predict classes and for predict classes we need to pass here X underscore test so in now wipe read it will predict the classes and then in Y test the y underscore test is the original class predict original the result you see here and let\'s go ahead and the print wipe read here and if you see the wipe right there and here the original class so it says that the 3/8 the correctly predicted but the 0 here the prediction is wrong for this one so this is how we can see the y test and the wipe read now let\'s go ahead and plot a confusion matrix in just a three line of code we can do it very easily the math is equal to the confusion matrix here which we had imported from ml extent library it takes the white roof which is y test and then it takes wipe read and then finally it takes the yeah so this is all it takes now let\'s go ahead and get the confusion matrix so with this we have got the confusion matrix which is in to the mat now you can see this is a confusion matrix and with this here you see the label as well there we can also pass here label which is actually classes names with this level I think on the level is not true here so let\'s just delete it we will pass this level in while plotting the curve a while while plotting this the confusion matrix so we have here the matrix let\'s go ahead and the plot it this is really very simple in one line of the code we can plot this so here we have a plot and then underscore confusion matrix and here you see a input it takes input confusion matrix in that confusion matrix we need to fast math and then hire this fine we don\'t need to give any parameters there let\'s go ahead and set figure sighs and I\'m gonna figure set a figure site or vital and then after this it takes the input like like the classes and here we take classes names is equal to the classes names and after that it also takes few other inputs but we are not going to pass that let\'s go ahead and see the confusion matrix all right so here you see the confusion matrix I think the plot I have plotted it quite large plot let\'s go ahead and see it into the six by six plot so with this confusion matrix we can also see the classes here and this is a predicted class and this is a true level and we can also get here are some the probability I mean the accuracy for each classes that we can get with the so normed is equal to the true so if I write here so normed is equal to true then we will also get the probability for each classes so we can also increase it this by nine by nine let\'s see nine by nine then there would be a larger figure there and now we can see the accuracy for each classes so with this you can see the maximum accuracy which we are getting in this sheep classes the truck and frogs cetera but the lower accuracy has been reported for our cat and for a dog and for a bird so these are the lower accuracy but for others the accuracy is quite good okay and now you see here the cat this is the predicted and this is the true level so let\'s say if the true level this is the plated one and this is the true level so with this it says that here the true level for the airplane okay and the predicted level is for airplane is you know it\'s around the 687 that is the truly predicted but for the airplane the 23-year plane has been predicted as the automobiles and the 59 has been predicted al-jabbar and the 21 airplane has been predicted as a cat so these are the error actually for the airplane but if you see here the automobiles okay so these are the automobiles here it has been you know the predicted the wrongly you know 136 automobiles this is the true label so 136 automobiles has been predicted as a truck and you see here the 199 cat has been predicted as a dog and now you see here 182 that is 18% 18% dogs has been predicted as a cat so this this can be you know the in co2 as a dog can be predicted in the cat and the cat has been predicted into the dog alright so but you see here the accuracy in the Miss classification in the dog and the automobiles is really very low right so with this you can understand the meaning of the confusion matrix so this is all about in this lesson so in this lesson you have learned so many things the first you have learned what is the scene in and how it works and then you have learned how you can install the tensorflow GPU and email extent library and then you have learned how you can the build your scene in model and then finally how you can train it and then finally how you can draw the learning curve and the confusion matrix in this lesson so thank you so much for watching this video please do not forget to Like and subscribe this channel bye bye have a nice day"", metadata={\'source\': \'7gGxBGvSAa0\'})]""""""']","{'https://stackoverflow.com/questions/52923062/tensorflow-compute-tf-nn-conv2d', 'https://stackoverflow.com/questions/34619177/what-does-tf-nn-conv2d-do-in-tensorflow', 'https://stackoverflow.com/questions/55909188/how-can-i-apply-a-tensorflow-2d-convolution-tf-nn-conv2d-to-a-single-non-batc', 'https://stackoverflow.com/questions/65542469/how-do-i-use-tensorflow-tf-nn-conv2-to-make-a-convolutional-layer'}","['""""""It seems (here and here) that the first operation creates a copy (someone correct me if I\'m wrong). You may use tf.expand_dims instead though, it\'s IMO more readable because of it\'s verbosity. On the other hand, taking 0 element from the tensor should not perform a copy in this case and is almost free. Most importantly, except for a little inconvenience with syntax (e.g. [0]) those operations definitely are not costly, especially in the context of performing convolution. BTW. Other ready alternative layers like the ones in tf.keras, require batch as first dimension as well. """"""', '""""""It would be extremely helpful if anyone could tell me what process yields this output. input = tf.Variable(tf.random_normal([1,2,2,1])) filter = tf.Variable(tf.random_normal([1,1,1,1])) op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding=\'SAME\') init = tf.initialize_all_variables() with tf.Session() as sess: sess.run(init) print(""input"") print(input.eval()) print(""filter"") print(filter.eval()) print(""result"") result = sess.run(op) print(result)\n\ninput [[[[ 1.60314465] [-0.55022103]] [[ 0.00595062] [-0.69889867]]]] filter [[[[-0.59594476]]]] result [[[[-0.95538563] [ 0.32790133]] [[-0.00354624] [ 0.41650501]]]]\n\n Actually cudnn is enabled by default on GPU in tf.nn.conv2d(), so the method in question is not used at all when we use TF with GPU support, unless use_cudnn_on_gpu=False is specified explicitly. n\nOk I think this is about the simplest way to explain it all.""""""', '""""""How can I apply a TensorFlow 2D Convolution (tf.nn.conv2d) to a single (non-batch) 2D image? \n\nI would like to use the function tf.nn.conv2d() on a single image example, but the TensorFlow documentation seems to only mention applying this transformation to a batch of images. The docs mention that the input image must be of shape [batch, in_height, in_width, in_channels] and the kernel must be of shape [filter_height, filter_width, in_channels, out_channels]. However, what is the most straightforward way to achieve 2D convolution with input shape [in_height, in_width, in_channels]? Here is an example of the current approach, where img has shape (height, width, channels):\n\nimg = tf.random_uniform((10,10,3)) # a single image img = tf.nn.conv2d([img], kernel)[0] # creating a batch of 1, then indexing the single example\n\nI am reshaping the input as follows:\n\n[in_height, in_width, in_channels]->[1, in_height, in_width, in_channels]->[in_height, in_width, in_channels]\n\nThis feels like an unnecessary and costly operation when I am only interested in transforming one example. Is there a simple/standard way to do this that doesn\'t involve reshaping?""""""', '""""""..... Now with ""SAME"" padding:\n\ninput = tf.Variable(tf.random_normal([1,5,5,5])) filter = tf.Variable(tf.random_normal([3,3,5,1])) op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding=\'SAME\')\n\nThis gives a 5x5 output image (size 1x5x5x1). This is done by centering the filter at each position on the image. Any of the 5-element dot products where the filter sticks out past the edge of the image get a value of zero. So the corners are only sums of 4, 5-element dot products. Now with multiple filters. input = tf.Variable(tf.random_normal([1,5,5,5])) filter = tf.Variable(tf.random_normal([3,3,5,7])) op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding=\'SAME\')\n\nThis still gives a 5x5 output image, but with 7 channels (size 1x5x5x7). Where each channel is produced by one of the filters in the set. Now with strides 2,2:\n\ninput = tf.Variable(tf.random_normal([1,5,5,5])) filter = tf.Variable(tf.random_normal([3,3,5,7])) op = tf.nn.conv2d(input, filter, strides=[1, 2, 2, 1], padding=\'SAME\')\n\nNow the result still has 7 channels, but is only 3x3 (size 1x3x3x7). This is because instead of centering the filters at every point on the image, the filters are centered at every other point on the image, taking steps (strides) of width 2. The x\'s below represent the filter center for each output pixel, on the input image. x.x.x ..... x.x.x ..... x.x.x\n\nAnd of course the first dimension of the input is the number of images so you can apply it over a batch of 10 images, for example:\n\ninput = tf.Variable(tf.random_normal([10,5,5,5])) filter = tf.Variable(tf.random_normal([3,3,5,7])) op = tf.nn.conv2d(input, filter, strides=[1, 2, 2, 1], padding=\'SAME\')\n\nThis performs the same operation, for each image independently, giving a stack of 10 images as the result (size 10x3x3x7)\n\n 7\n\n@ZijunLost No, the docs state that the first and last element must be 1. Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1]. Is this Toeplitz matrix-based implementation of convolution? Regarding this: ""This still gives a 5x5 output image, but with 7 channels (size 1x5x5x7). Where each channel is produced by one of the filters in the set."", I still have difficulty understanding where the 7 channels are from? what do you mean ""filters in the set""? Thanks. @mdaoust Hi, regarding your second example where the 3x3 image and the 1x1 filter each have 5 channels, I find the result is different from the manually calculated dot product. @derek I have the same question, does the ""output_channel"" same as ""number of filters""??? if so why are they named ""output_channel"" in the tensorflow docs? | Show 2 more comments\n\n2D convolution is computed in a similar way one would calculate 1D convolution: you slide your kernel over the input, calculate the element-wise multiplications and sum them up. But instead of your kernel/input being an array, here they are matrices. In the most basic example there is no padding and stride=1. Let\'s assume your input and kernel are:\n\nWhen you use your kernel you will receive the following output: , which is calculated in the following way:\n\n14 = 4 * 1 + 3 * 0 + 1 * 1 + 2 * 2 + 1 * 1 + 0 * 0 + 1 * 0 + 2 * 0 + 4 * 1\n\n6 = 3 * 1 + 1 * 0 + 0 * 1 + 1 * 2 + 0 * 1 + 1 * 0 + 2 * 0 + 4 * 0 + 1 * 1\n\n6 = 2 * 1 + 1 * 0 + 0 * 1 + 1 * 2 + 2 * 1 + 4 * 0 + 3 * 0 + 1 * 0 + 0 * 1\n\n12 = 1 * 1 + 0 * 0 + 1 * 1 + 2 * 2 + 4 * 1 + 1 * 0 + 1 * 0 + 0 * 0 + 2 * 1\n\nTF\'s conv2d function calculates convolutions in batches and uses a slightly different format. For an input it is [batch, in_height, in_width, in_channels] for the kernel it is [filter_height, filter_width, in_channels, out_channels]. So we need to provide the data in the correct format:\n\nimport tensorflow as tf k = tf.constant([ [1, 0, 1], [2, 1, 0], [0, 0, 1] ], dtype=tf.float32, name=\'k\') i = tf.constant([ [4, 3, 1, 0], [2, 1, 0, 1], [1, 2, 4, 1], [3, 1, 0, 2] ], dtype=tf.float32, name=\'i\') kernel = tf.reshape(k, [3, 3, 1, 1], name=\'kernel\') image = tf.reshape(i, [1, 4, 4, 1], name=\'image\')\n\nAfterwards the convolution is computed with:\n\nres = tf.squeeze(tf.nn.conv2d(image, kernel, [1, 1, 1, 1], ""VALID"")) # VALID means no padding with tf.Session() as sess: print sess.run(res)\n\nAnd will be equivalent to the one we calculated by hand. For examples with padding/strides, take a look here. Salvador DaliSalvador Dali\n\n 2\n\nNice example, however some links are broken.""""""', '"""""" One big motivation is to use dilate and stride at the same time, which seems to work with tf.nn.conv2d, but not with keras Conv2D. For some reasons, keras has not implemented that. """"""']","To apply a TensorFlow 2D Convolution (`tf.nn.conv2d`) to a single (non-batch) 2D image, you need to ensure that the input tensor conforms to the expected shape `[batch, in_height, in_width, in_channels]`. The TensorFlow documentation specifies that the input image must be in this 4-D format, where the first dimension represents the batch size.

For a single image, you can achieve this by reshaping the input tensor to include a batch dimension of size 1. This means converting the shape from `[in_height, in_width, in_channels]` to `[1, in_height, in_width, in_channels]`. After performing the convolution operation, you can remove the batch dimension to revert to the original shape.

Although this reshaping might seem unnecessary and costly, it is required to use the `tf.nn.conv2d` function, as it is designed to operate on batches of images. The reshaping operation itself is not computationally expensive, especially in the context of performing convolutions.

In summary, to apply `tf.nn.conv2d` to a single 2D image, you must reshape the image to include a batch dimension, perform the convolution, and then remove the batch dimension. This process ensures compatibility with the function's requirements without significant performance overhead."
63919438,tf.keras.Model,example required,TensorFlow keras model fit() parameters steps_per_epoch and epochs behavior on train set,"<p>I'm using a tf.data dataset containing my training data consisting of (lets say) 100k images.
I'm also using a tf.data dataset containing my validation set.
Since an epoch of all 100k images takes quite long (in my case approximately one hour) before I get any feedback on performance on the validation set, I set the <code>steps_per_epoch</code> parameter in tf.keras.Model <code>fit()</code> to <code>10000</code>.
Using a batch size of 1 this results into having 10 validation scores when reaching 100k of images.
In order to complete one epoch of 100k images of my entire training dataset, I set the <code>epochs</code> parameter to <code>10</code></p>
<p>However, I'm not sure if using <code>steps_per_epoch</code> and <code>epochs</code> this way has any other consequences. Is it correct to use these parameters in order to get more frequent feedback on performance?
And also a more specific question, does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'?
I already dug into the <a href=""https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">TensorFlow docs</a> and read several different stack overflow questions, but I couldn't find anything conclusive to answer my own question. Hope you can help!</p>
<p>Tensorflow version I'm using is 2.2.0.</p>
","<blockquote>
<p>Is it correct to use these parameters in order to get more frequent
feedback on performance?</p>
</blockquote>
<p>Yes, it is correct to use these parameters. Here is the code that i used to fit the model.</p>
<pre><code>model.fit(
train_data,
steps_per_epoch = train_samples//batch_size,
epochs = epochs,
validation_data = test_data,
verbose = 1,
validation_steps = test_samples//batch_size)
</code></pre>
<blockquote>
<p>does it use all 100k images or does it use the same first 10k images of my
training set at every 'epoch'?</p>
</blockquote>
<p>It use all images in your training data.</p>
<p>For better understanding <code>Epoch</code> is the number times the learning algorithm will work through the entire training data set.</p>
<p>Where as <code>steps_per_epoch</code> is the total number of samples in your training data set divided by the batch size.</p>
<p>For example, if you have 100000 training samples and use a batch size of 100, one epoch will be equivalent to 1000 steps_per_epoch.</p>
<p><em>Note: We generally observe batch size to be the power of 2, this is because of the effective work of optimized matrix operation libraries.</em></p>
",,"['How to use steps_per_epoch and epochs parameters in tf.keras.Model fit() in TensorFlow 2.2.0?', 'Does setting steps_per_epoch in tf.keras.Model fit() use the same subset of training data in each epoch?', 'How to get more frequent feedback on validation performance using tf.keras.Model fit()?', 'What are the consequences of setting steps_per_epoch and epochs in TensorFlow 2.2.0?', 'How does tf.data dataset interact with steps_per_epoch and epochs in TensorFlow 2.2.0?']","['What does the steps_per_epoch parameter do in tf.keras.Model.fit()?', 'How does the epochs parameter interact with steps_per_epoch in tf.keras.Model.fit()?', 'Does setting steps_per_epoch to a value less than the total number of batches in the dataset cause the same subset of data to be used in each epoch?', 'How to ensure that all data in the tf.data dataset is used when steps_per_epoch is set in tf.keras.Model.fit()?', 'What are the best practices for setting steps_per_epoch and epochs to get frequent feedback on validation performance in TensorFlow?', 'Does tf.keras.Model.fit() shuffle the dataset when using steps_per_epoch and epochs parameters?']","{'https://www.youtube.com/watch?v=SftOqbMrGfE', 'https://www.youtube.com/watch?v=S6tLSI8bjGs'}","['""""""[Document(page_content=""at the start of every deep learning problem we have a data set and a largely untrained model now there are multiple open questions about how and how often we present this data to the model and i will address some of them in this video let\'s first look at the general training cycle from our database we get features for example images and labels or targets for example describing what is on the image we send the features into the network and let it make a prediction on that data we then compare the prediction with the true labels from the database by calculating the loss behind the scenes in pi torch or tensorflow the calculations up to this point are being tracked and we can automatically get the gradient which points us into the direction in which the network will be better at the just shown data that is roughly what happens during back propagation normally we then adjust the weights of the network with this new information after each prediction so that\'s the basics that are important to this video iterations are the first keyword we\'re talking about here because every for loop or iterative update can be called an iteration this is not really a clear-cut definition as far as i could tell but you can either say one iteration is finished every time the network weights are updated or every time we make a prediction which would be an iteration of the for loop that goes through the data loader in most cases these two are the same so don\'t worry if you don\'t really understand the distinction you could however do gradient descend and accumulate gradients until you have seen the whole data set before making an update in which case these two definitions would be different we have multiple for loops over the data loader but then at the end only one update of the network weights if you\'re a beginner just think of an iteration as one loop through this training cycle on the right also most research literature i\'ve read often talks about training steps to specifically mean the times when we update the neural network weights so i would encourage you to use that term as well next is the epoch an epoch is finished once the neural network has seen all the data once typically we are not finished after that because gradient descent variants only take small update steps and we usually need more updates than are possible within one epoch to reach a good model so we train for multiple epochs and it\'s hard to give a number because it\'s very dependent on the network size and training data however it\'s often more than 40 epochs i personally have trained some small image classification models on my local hardware and i used between 5 and 40 epochs just to throw this out as a super rough estimate it\'s also very different how long an epoch takes in terms of time it could be just seconds or even hours per epoch again that\'s very dependent on the type of network being trained and the data set overall epochs are a nice metric to describe training length so they are often used in papers and such because they are independent from the batch size and the concrete layout of iterations we have talked about iterations and epochs but what about batch size there\'s a bit more to talk about here we need to show the training data to the network and one of the ways we can do this is with full batch gradient descent for that we use the full data set at once the pros are that it is the most accurate at minimizing the training loss function after all we want the network to become better at all the training examples and not only some of them so it needs to take all of them into account when optimizing we can also calculate many of the example predictions on parallel with gpus which makes batch descend possible in the first place the cons however are firstly logistical the memory we have available is often too small modern neural networks are already quite big in themselves and the data sets can be many gigabytes big so they will not fit into memory completely as i mentioned before we could also split them into batches and then accumulate the gradients and only do an update at the end however that would take extremely long because gradient descent needs to take many training steps to get good and if every training step takes super long it\'s just ineffective the main takeaway from this is that you need to choose the batch size at least small enough so you don\'t run out of memory now let\'s look at what happens if we don\'t wait with our network update until we have seen the whole data set in stochastic gradient descent we only select one data point randomly for each training update most of what i say now also applies to the often used mini batch stochastic gradient descent in this algorithm we choose a random selection of points from the data set how many depends on the batch size that we choose for our problem the main advantage originally for this algorithm was that we can train faster by making more frequent updates even without having complete information about the whole data set this speeds up training but what happens to our accuracy first it\'s not as good as gradient descent because like i said we make updates before having all the information in this plot you can see three different functions f1 f2 and f3 and they represent the loss of just one data point and you can see the average of those three in red which would be the loss over all data points that we want to minimize so ultimately we want to minimize the red line to reach the point at almost x equals negative 0.5 however if we look at what would happen if we follow one of the other lines to their minimum then we can see that they would lead us in different directions sometimes even in the opposite direction of where we want to go but since we only take small steps and switch the leading line after every step they still roughly average out over time just not exactly to the average so that is something to keep in mind similarly to this view stochastic gradient descent introduces noise and in this image here i try to showcase the path which we would go with gradient descent as the solid line and the path we would take with storage gradient descent as the dotted line this is quite a mild view because here we never go into the opposite direction of gradient descent but it is still a good mental image to have of the process you can also see that we do not end up in exactly the same spot as grade intercent does now there is some discussion and theories going on that this noise is actually helping us train the network to some extent but that is a topic for its own video and it\'s also not fully understood in research yet but i still wanted to mention this because this randomness and training is not completely negative which is why it is used in most applications successfully to train newer networks some last remarks as a conclusion like i said mini badge is most often used so not the whole data set at once but also not just single points the exact batch size depends on your project and you should try out different ones to see which one works best in every case a good guideline however is to choose exponentials of two so 16 or 32 or 64 for your batch size as that uses the memory most efficiently so that\'s all for this quick lesson in the basics of deep learning thank you for watching and see you in the next one"", metadata={\'source\': \'SftOqbMrGfE\'})]""""""', '""""""[Document(page_content=""alrighty what is going on guys welcome back for another video in this video we\'re gonna explore uh how to build uh more flexible training loops uh so far we\'ve been using model.fit and uh if you can use modeler fit that\'s great but sometimes you need more flexibility so in this video we will look at customizing model.fit and then in the next video how we will look at how to build custom training loops from scratch all right so first of all here are just some basic imports those are you\'ve all seen those uh before and then we\'re just gonna load the uh the mnist data set so we\'re not going to do anything complicated i\'m just going to show you the general structure and then that can be applied to many different problems all right so we\'re going to x train y train x test y test and we\'re just going to mnist load data then we\'re going to do x train is x stream dot reshape then we\'re just going to have i guess a minus one for all the examples and then 28.81 and we\'re doing reshape here just to add this channel right here and then as type converted to flow 32 and then normalize with dividing by 255. so let\'s see let\'s copy this and let\'s go new line and do x test test and then let\'s create our model first of all so let\'s do model is equal to keras dot sequential then we\'re going to do layers dot input and then the shape of the input is 28 28 1 layers come to the 64 3 kernel size and i\'m padding same so i\'m just going through this quickly this is not really the most important part of the video so now that we have a model we\'re going to create a class and we\'re going to call it custom fit and we\'re going to inherit from keras.model and then the first thing we\'re going to do is we\'re going to create an init function and all we\'re going to send in here is the model so we\'re going to first call super to inherit from carousel model so we\'re going to do self and then init then we\'re going to do self.models equal to model then what we\'re going to do is we\'re going to define one training step and that\'s going to be used in a model.fit right so our goal is basically we want to do something like uh training is a custom uh custom wait custom what the hell custom fit of that model we\'re going to send in that model then we\'re going to do training dot fit and we\'re going to send in x train y train and then batch size and the number of epochs sort of as normal although this dot fit is going to be done in a custom way we\'re gonna sort of define how we want that to be done so um i mean there are many use cases of this where you want where you need to do custom training loops um and sort of you you use model of it when you can and when you can\'t you try to customize your model that fit which is what we\'re doing in this video and then for that most flexibility you do the training loops from scratch but an example of when you actually need to do this is uh generative adversarial networks i\'m not assuming you\'re familiar with that i\'m just sort of saying there are many examples where uh this is useful all right so let\'s then do uh train step uh we\'re gonna send in data and then we\'re gonna that\'s gonna be a tuple of x and y so we\'re just gonna do x and y is equal to data then what we\'re gonna do is uh we\'re gonna do with tf gradient tape as tape and why we\'re doing this is uh because now we\'re going to do the forward propagation and then the loss function and when we\'re doing it under that tape it\'s going to record all of the operations that was done and then that will then be useful for calculating the gradients for back propagation so basically we\'re going to do y prediction is a self.model we\'re going to send in x we\'re going to specify training is true and then for this loss function we\'re going to do loss equals self.compiled loss and then we\'re going to set in y and then y prediction and this is going to be done in the compile so right here we\'re going to do training dot compile we\'re going to send in and here we\'re going to send in optimizer is kara\'s optimizers adam then we\'re going to send in loss is keras losses sparse categorical cross entropy and from logits equals true then also we\'re going to do metrics is uh accuracy and so this is for the first one where we\'re doing the compile i\'m also going to show you how to do a custom compile um but but let\'s let\'s take that as we go so we we\'re going to first now continue doing the train step when we have this compile and so this self.compiledloss is using this sparse categorical cross-entropy from the training.compile after that we basically want to get the gradients right we\'ve now done the forward propagation this part is a for propagation which we\'re doing with this uh gradient under this tape to record all of the operations then we\'re going to do training variables is our self.trainable variables and these are all stored from from this uh parent class this uh keras.model so we don\'t have to bother with that then we want to get the gradients we\'re going to do tape dot gradient and we\'re going to do uh loss and then training variables right so we\'re getting the gradient uh of the loss with respect to the the training variables and which is ultimately what we want to change then uh we\'re gonna do a step an optimizer step a gradient descent step and we\'re gonna do self.optimizer.apply gradients and then here we\'re gonna do zip gradients and then training variables and then we\'re going to do self.compiledmetrics.updatestate y and then my prediction and this is this is going to be for the accuracy and then in the end we\'re going to return m.name and you\'ll see what it means so m.name m.result for m in self.metrics all right so we\'re getting the m.name which is going to be the loss for example and then we\'re getting the result which is the current loss and then we\'re doing that for all of the metrics and that\'s going to be the loss and the accuracy in this case and uh yeah so i think that\'s it for just this first step and i think we should now be able to run this and as you can see here it uh does seem to work and uh yeah so basically basically the next step now is that we want to do our own uh compile so what we\'re going to do right here is we\'re going to define compile we\'re going to send in the optimizer and we\'re going to send in the loss and we\'re going to do super custom fit self dot compile so yeah and then we\'re going to do self.optimizer is equal to optimizer self.loss is equal to loss and all we have to do then is uh we have basically the same thing right uh training.compile except we\'re not going to send in a metric right here so we\'re just going to use the optimizer and the loss and uh and that should also basically be it now we just have to change this right here to this compiled loss we\'re just going to do self.loss it\'s uh which we\'ve stored right here so self.loss and then let\'s see yeah and then we can still use self.optimizer and let\'s just rerun it and now as you can see we\'re not getting an accuracy so we\'re going to have to keep track of a that metric by ourself so what we can do is um for example we could uh create it right here we could do accuracy metric is uh keras.metrics dot sparse categorical accuracy and let\'s just call it name name equals accuracy and then in the and then right here instead of the compiled metric what we\'re going to do is uh is accuracymetric.updatestate we\'re going to send in y and then y prediction and then we can remove this compiled metric um and yeah so that should hopefully be it let\'s see if we can run this all right so now since we\'re keeping track of the accuracy by ourself what we\'re going to do here is uh we\'re going to write it explicitly so we\'re going to do loss is in this case just loss and then we\'re going to do accuracy is accuracy metric dot result and hopefully now we should get the loss and uh the accuracy yeah so this looks pretty familiar to what we did previously except now we\'re doing the compile completely by ourself and then all right so now we got the compile we got a train step what we normally do as well is in the end after training we\'re doing training dot evaluate and then x test y test and then we\'re specifying batch size let\'s say 32 one thing here is that this dot fit works on the train step and then evaluate works on a test step so to make this work we actually need to define another function i\'m going to do test step although this one is going to be a little bit easier since um well first of all we\'re going to unpack the data and then we\'re going to compute predictions we\'re going to do y prediction is self.model x and then we\'re specifying the training is false and what we\'re doing is this is if we\'re using batch norm or dropout that has different behaviors during testing and training we\'re just telling the model this is now in testing so make sure that those modules that have different behaviors are set to test mode or evaluation mode then we\'re going to compute the loss which is self.loss of y y prediction and then we\'re going to do accuracymetric accuracymetric.update state y y prediction and in the end we\'re going to return a dictionary of loss which is just going to be loss and then accuracy we\'re doing accuracymetric.result all right so this is a very like it\'s very similar to the training step although it\'s much more simplified and it\'s simplified because we\'re not doing a gradient descent update so we don\'t need to keep track of of this tape of making sure that we have all the gradients and and all of that stuff and yeah so let\'s run this for yeah two epochs and then let\'s do the evaluation all right so after this we see that we get 93 at the first epoch 97 and then almost 98 on the test set but yeah so i mean what we want to establish here that this does seem to train and it\'s working so so yeah that\'s how you create your own um you know specifying the training step and a test that which overwrites how the training that fit and then training that evaluate is done so in this way you can build more complicated and complex models uh in the in the training steps uh but still have the flexibility of doing training.fit um and that means that you can still use the training.compile although in this last one we we overwrote the the compile but but you get the point and you can still use the their compile and the metrics and all of that stuff but yeah if you have any questions leave them in the comment section below thank you so much for watching the video and i hope to see you in the next one [Music]"", metadata={\'source\': \'S6tLSI8bjGs\'})]""""""']","{'https://stackoverflow.com/questions/38340311/what-is-the-difference-between-steps-and-epochs-in-tensorflow', 'https://stackoverflow.com/questions/54575714/combining-keras-model-fits-steps-per-epoch-with-tensorflows-dataset-apis-b'}","['""""""If we look at the code of the fit method, you will see that the data is handled by a special class, DataHandler. Looking at the code of this class, we see that this is an Adapter class to handle different kind of data. We are interrested in the class that handle tf.data.Dataset, DatasetAdapter, and we can see that this class does not take into account the shuffle parameter :\n\ndef __init__(self, x, y=None, sample_weights=None, steps=None, **kwargs): super(DatasetAdapter, self).__init__(x, y, **kwargs) # Note that the dataset instance is immutable, its fine to reuse the user # provided dataset. self._dataset = x # The user-provided steps. self._user_steps = steps self._validate_args(y, sample_weights, steps)\n\nIf you want to shuffle your dataset, use the shuffle function from the tf.data.Dataset API. 2\n\nThank you for your explanation. To go further, If I want to shuffle my dataset each epoch during training. Would doing dataset.shuffle(len_dataset).batch(batch_size) and model.fit(num_epochs, ....) be enough ? Or do I have have to call, dataset.shuffle(len_dataset).batch(batch_size).repeat() and model.fit(steps_per_epoch=len_dataset//batch_size, ...). Or is it equivalent ? @Matt I am pretty certain that calling shuffle once should reshuffle the dataset between each epoch. It is something you could verify with a toy example.""""""', '""""""there I start doubting this method. As I am currently experimenting with the tf.estimator API I would like to add my dewy findings here, too. I don\'t know yet if the usage of steps and epochs parameters is consistent throughout TensorFlow and therefore I am just relating to tf.estimator (specifically tf.estimator.LinearRegressor) for now. Training steps defined by num_epochs: steps not explicitly defined\n\nestimator = tf.estimator.LinearRegressor(feature_columns=ft_cols) train_input = tf.estimator.inputs.numpy_input_fn({\'x\':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True) estimator.train(input_fn=train_input)\n\nComment: I have set num_epochs=1 for the training input and the doc entry for numpy_input_fn tells me ""num_epochs: Integer, number of epochs to iterate over data. If None will run forever."". With num_epochs=1 in the above example the training runs exactly x_train.size/batch_size times/steps (in my case this was 175000 steps as x_train had a size of 700000 and batch_size was 4). Training steps defined by num_epochs: steps explicitly defined higher than number of steps implicitly defined by num_epochs=1\n\nestimator = tf.estimator.LinearRegressor(feature_columns=ft_cols) train_input = tf.estimator.inputs.numpy_input_fn({\'x\':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True) estimator.train(input_fn=train_input, steps=200000)\n\nComment: num_epochs=1 in my case would mean 175000 steps (x_train.size/batch_size with x_train.size=700,000 and batch_size=4) and this is exactly the number of steps estimator.train albeit the steps parameter was set to 200,000 estimator.train(input_fn=train_input, steps=200000). Training steps defined by steps\n\nestimator = tf.estimator.LinearRegressor(feature_columns=ft_cols) train_input = tf.estimator.inputs.numpy_input_fn({\'x\':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True) estimator.train(input_fn=train_input, steps=1000)\n\nComment: Although I have set num_epochs=1 when calling numpy_input_fnthe training stops after 1000 steps. This is because steps=1000 in estimator.train(input_fn=train_input, steps=1000) overwrites the num_epochs=1 in tf.estimator.inputs.numpy_input_fn({\'x\':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True). Conclusion: Whatever the parameters num_epochs for tf.estimator.inputs.numpy_input_fn and steps for estimator.train define, the lower bound determines the number of steps which will be run through. 1,0\n\nEpoch: A training epoch represents a complete use of all training data for gradients calculation and optimizations(train the model). Step: A training step means using one batch size of training data to train the model. Number of training steps per epoch: total_number_of_training_examples / batch_size. Total number of training steps: number_of_epochs x Number of training steps per epoch. 1\n\nJust to add on to this, if there is a validation set of size V, then the number of training steps per epoch is (total_number_of_training_examples - V)/batch_size\n\n\n\nAccording to Google\'s Machine Learning Glossary, an epoch is defined as\n\n""A full training pass over the entire dataset such that each example has been seen once. Thus, an epoch represents N/batch_size training iterations, where N is the total number of examples.""\n\nIf you are training model for 10 epochs with batch size 6, given total 12 samples that means:\n\nthe model will be able to see whole dataset in 2 iterations ( 12 / 6 = 2) i.e. single epoch. overall, the model will have 2 X 10 = 20 iterations (iterations-per-epoch X no-of-epochs)\n\nre-evaluation of loss and model parameters will be performed after each iteration!  By default an epoch run over all your training data. In this case you have n steps, with n = Training_lenght / batch_size. If your training data is too big you can decide to limit the number of steps during an epoch.[https://www.tensorflow.org/tutorials/structured_data/time_series?_sm_byp=iVVF1rD6n2Q68VSN]\n\nWhen the number of steps reaches the limit that you've set the process will start over, beginning the next epoch. When working in TF, your data is usually transformed first into a list of batches that will be fed to the model for training. At each step you process one batch. As to whether it's better to set 1000 steps for 1 epoch or 100 steps with 10 epochs I don't know if there's a straight answer. But here are results on training a CNN with both approaches using TensorFlow timeseries data tutorials :\n\nIn this case, both approaches lead to very similar prediction, only the training profiles differ. steps = 20 / epochs = 100\n\nsteps = 200 / epochs = 10\n\nYoan B. M.ScYoan B. M.Sc\n\n1,Divide the length of x_train by the batch size with\n\nsteps_per_epoch = x_train.shape[0] // batch_size\n\n\n\nWe split the training set into many batches. When we run the algorithm, it requires one epoch to analyze the full training set. An epoch is composed of many iterations (or batches). Iterations: the number of batches needed to complete one Epoch. Batch Size: The number of training samples used in one iteration. Epoch: one full cycle through the training dataset. A cycle is composed of many iterations. Number of Steps per Epoch = (Total Number of Training Samples) / (Batch Size)\n\nExample Training Set = 2,000 images Batch Size = 10\n\nNumber of Steps per Epoch = 2,000 / 10 = 200 steps\n\nHope this helps for better understanding. Tejas BadheTejas Badhe\n\nIn a nutshell the expression can be denoted as follows\n\n the above expression would be used calculate the steps\n\nand would be used conversantly to calculate the epochs""""""', '""""""Combining Keras model.fit\'s `steps_per_epoch` with TensorFlow\'s Dataset API\'s `batch()`\n\n I\'m looking at the performance and GPU usage during training of a CNN model with Keras+TensorFlow. Similar to this question, I\'m having a hard time to understand the combined use of Keras model.fit\'s steps_per_epoch and TensorFlow\'s Dataset API\'s .batch(): I set a certain batch size on the input pipeline dataset = dataset.batch(batch_size) and later I use\n\nfit = model.fit(dataset, epochs=num_epochs, steps_per_epoch=training_set_size//batch_size)\n\nbut I see that one can actually set any number of steps per epoch, even more than training_set_size//batch_size. From the documentation I understand that on Keras an epoch is not necessarily a pass over the entire training set as usually, but anyway I\'m a bit confused and now I\'m not entirely sure if I\'m using it right. Is dataset.batch(batch_size) + steps_per_epoch=training_set_size//batch_size defining a minibatch SGD that runs over the entire training set by minibatches of batch_size samples? Are epochs larger than one pass over the training set if steps_per_epoch is set to more than training_set_size//batch_size? steps_per_epoch is the number of batches of your set batch size is ran through the network in one epoch. You have set your steps_per_epoch to be training_set_size//batch_size for a good reason. This ensures all data are trained upon in one epoch, providing the number divides exactly (if not it rounds by the // operator). That is to say if you had a batch size of 10 and a training set size of 30, then steps_per_epoch = 3 ensures all data are used. And to quote your question:\n\n""Are epochs larger than one pass over the training set if steps_per_epoch is set to more than training_set_size//batch_size?""\n\nYes. Some data will be passed through again in the same epoch.""""""', '""""""edit: twice\n\n\n\n@NicolasGervais - perhaps one could imagine a scenario where the shuffling is defined once and is used for every iteration. @jakub ah yes it makes sense from that perspective\n\n\n\nThis is because the data files are shuffled and the dataset is shuffled with dataset.shuffle(). With dataset.shuffle(), the data will be shuffled in a different way on each iteration by default. One can remove shuffle_files=True and set the argument reshuffle_each_iteration=False to prevent reshuffling on different iterations. The .take() function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them. # Construct a tf.data.Dataset ds = tfds.load(\'mnist\', split=\'train\', shuffle_files=False) # Build your input pipeline ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE) single_batch_dataset = ds.take(1) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label)\n\ntf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64) tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)\n\n\n\n""""""']","### Explanation of `steps_per_epoch` and `epochs` Parameters in `tf.keras.Model.fit()`

The question addresses the behavior of the `steps_per_epoch` and `epochs` parameters in the `tf.keras.Model.fit()` method, specifically in the context of training a model on a large dataset and obtaining more frequent feedback on validation performance.

#### Understanding `steps_per_epoch` and `epochs`

1. **`steps_per_epoch`**: This parameter defines the number of batches of samples to be used in one epoch. If you set `steps_per_epoch=10000` with a batch size of 1, it means that each epoch will consist of 10,000 steps, where each step processes one image. This allows you to get intermediate feedback on the model's performance without having to wait for the entire dataset to be processed.

2. **`epochs`**: This parameter defines the number of times the entire dataset is passed through the model. If you set `epochs=10`, it means the model will go through the dataset 10 times. However, since you have set `steps_per_epoch=10000`, the model will process 10,000 images per epoch, and over 10 epochs, it will process a total of 100,000 images.

#### Behavior and Consequences

- **Frequent Feedback**: By setting `steps_per_epoch=10000`, you effectively get validation feedback after every 10,000 images, rather than waiting for the entire 100,000 images to be processed. This can be useful for monitoring the model's performance more frequently and making adjustments if necessary.

- **Data Usage**: The `steps_per_epoch` parameter does not limit the model to using only the first 10,000 images repeatedly. Instead, it processes the dataset in batches, and if the dataset is shuffled (which is recommended), different subsets of the data will be used in each epoch. This ensures that the model sees the entire dataset over the course of training.

- **Shuffling**: If the dataset is shuffled using the `tf.data.Dataset.shuffle()` method, the model will see different batches of data in each epoch, even if `steps_per_epoch` is set to a smaller number than the total dataset size. This helps in better generalization and prevents the model from overfitting to a specific subset of the data.

#### Conclusion

Using `steps_per_epoch` and `epochs` in this manner is a valid approach to obtain more frequent feedback on the model's performance. It allows you to monitor validation metrics more frequently without having to wait for the entire dataset to be processed. Additionally, if the dataset is shuffled, the model will see different subsets of the data in each epoch, ensuring that it eventually processes the entire dataset over the course of training. This approach does not have any adverse consequences and can be beneficial for large datasets where training time per epoch is significant."
53032922,tf.while_loop,example required,TensorFlow while loop with condition dependent on body,"<p>I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop()</code></a>.</p>

<p>My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by</p>

<pre><code>import numpy as np
import tensorflow as tf
IMAGE_SHAPE = [960, 720]
CROP_SHAPE = [320, 240]
max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
</code></pre>

<p>and the condition is</p>

<pre><code>cond = tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)
</code></pre>

<p>Going over the documentation and examples of <code>tf.while_loop(cond, body, loop_vars, ...)</code>, what I understand is that both <code>cond</code> and <code>body</code> should take the same arguments given in <code>loop_vars</code>.
I don't see how I can have <code>cond</code> depend on <code>img_crop</code> which would be calculated inside <code>body</code>, and isn't provided in <code>loop_vars</code>.</p>

<p>I could equivalently compute <code>cond</code> using <code>crop_begin_index</code> without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem.</p>

<p>Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use <code>tf.while_loop()</code>?</p>
","<p>The arguments that are passed on to the <code>condition</code> function are the arguments returned from your <code>body</code> function. So you just have to return that value that you want to base your condition on in the <code>body</code> function, then carry out the condition on that value in your <code>cond</code> function. Something like, </p>

<pre><code>def body(image_shape, crop_shape, img_crop):
    max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
    crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
    img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
    return (image_shape, crop_shape, img_crop)

def cond(image_shape, crop_shape, img_crop):
    return tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)

image_shape, crop_shape, img_crop = tf.while_loop(cond=cond, body=body, loop_vars=([960, 720], [320, 240], img_crop))
</code></pre>

<p>Don't have access to an interpreter right now, so there might be some syntax problems there, but something like that. </p>

<p>Also, if I recall correctly, the body and the condition need to be pure functions, you cannot alter the outer state from within the functions.</p>

<p>Also note, you'll need to specify some initial value for <code>img_crop</code> in the loop vars.</p>

<p>Moreover, by default, <code>tf.while_loop</code> expects the shapes of all the <code>loop_vars</code> to remain the same across all loop runs. You can modify this through the <code>shape_invariants</code> argument. </p>
",,"['How to use tf.while_loop with condition dependent on tensor computed in the loop body', 'TensorFlow tf.while_loop tutorial with dynamic condition', 'Using tf.while_loop for random cropping with quality check', 'TensorFlow while loop with condition based on intermediate tensor', 'Implementing conditional random cropping in TensorFlow using tf.while_loop', 'How to pass loop variables in tf.while_loop for dynamic conditions', 'TensorFlow tf.while_loop example with tensor condition', 'Handling random operations inside tf.while_loop in TensorFlow']","['How to use tf.while_loop with a condition dependent on a tensor computed inside the loop body?', 'How to handle random cropping with quality checks in TensorFlow using tf.while_loop?', 'Examples of using tf.while_loop where the condition depends on values computed within the loop body', 'How to pass and update loop variables in tf.while_loop?', 'How to implement a loop in TensorFlow that discards low-quality examples based on a condition evaluated inside the loop?']","{'https://www.youtube.com/watch?v=306tG48lhhQ', 'https://www.youtube.com/watch?v=H_9dthbtV68', 'https://www.youtube.com/watch?v=A6qJMB3stE4', 'https://www.youtube.com/watch?v=iFZRUwGcrxQ', 'https://www.youtube.com/watch?v=_u7AVsxANes'}",,"{'https://stackoverflow.com/questions/53032922/tensorflow-while-loop-with-condition-dependent-on-body', 'https://stackoverflow.com/questions/37441140/how-to-use-tf-while-loop-in-tensorflow'}","['""""""TensorFlow while loop with condition dependent on body\n\n I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don\'t know how to accomplish this with tf.while_loop(). My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by\n\nimport numpy as np import tensorflow as tf IMAGE_SHAPE = [960, 720] CROP_SHAPE = [320, 240] max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE) crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index) img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])\n\nand the condition is\n\ncond = tf.count_nonzero(img_crop > 0) > 0.5 * tf.size(img_crop)\n\nGoing over the documentation and examples of tf.while_loop(cond, body, loop_vars, ...), what I understand is that both cond and body should take the same arguments given in loop_vars. I don\'t see how I can have cond depend on img_crop which would be calculated inside body, and isn\'t provided in loop_vars. I could equivalently compute cond using crop_begin_index without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem. Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use tf.while_loop()? The arguments that are passed on to the condition function are the arguments returned from your body function. So you just have to return that value that you want to base your condition on in the body function, then carry out the condition on that value in your cond function. Something like,\n\ndef body(image_shape, crop_shape, img_crop): max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE) crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index) img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1]) return (image_shape, crop_shape, img_crop) def cond(image_shape, crop_shape, img_crop): return tf.count_nonzero(img_crop > 0) > 0.5 * tf.size(img_crop) image_shape, crop_shape, img_crop = tf.while_loop(cond=cond, body=body, loop_vars=([960, 720], [320, 240], img_crop))\n\nDon\'t have access to an interpreter right now, so there might be some syntax problems there, but something like that. Also, if I recall correctly, the body and the condition need to be pure functions, you cannot alter the outer state from within the functions. Also note, you\'ll need to specify some initial value for img_crop in the loop vars. Moreover, by default, tf.while_loop expects the shapes of all the loop_vars to remain the same across all loop runs. You can modify this through the shape_invariants argument. \n\nGot it. I was also able to use just crop_begin_index as a variable like I wrote so the loop is more concise. Thanks!""""""', '""""""Can anyone give me few more complex example? Also, in such case that if the future computation is based on the tensor output (ex: the RNN stop based on the output criterion), which is very common case. Is there an elegant way or better way instead of dynamic graph?. I\'m facing a problem where the gradients are None while implementing SimpleRNN using subclassing. I\'m wondering whether I need symbolic loop or can I manage without it? What is stopping you from adding more functionality to the body? You can build whatever complex computational graph you like in the body and take whatever inputs you like from the enclosing graph. Also, outside of the loop, you can then do whatever you want with whatever outputs you return. As you can see from the amount of \'whatevers\', TensorFlow\'s control flow primitives were built with much generality in mind. Below is another \'simple\' example, in case it helps. import tensorflow as tf import numpy as np def body(x): a = tf.random_uniform(shape=[2, 2], dtype=tf.int32, maxval=100) b = tf.constant(np.array([[1, 2], [3, 4]]), dtype=tf.int32) c = a + b return tf.nn.relu(x + c) def condition(x): return tf.reduce_sum(x) < 100 x = tf.Variable(tf.constant(0, shape=[2, 2])) with tf.Session(): tf.global_variables_initializer().run() result = tf.while_loop(condition, body, [x]) print(result.eval())\n\n My problem is the condition I need is calculated after run the body once. So it is like I need to use the return value of body as a parameter for condition. What you want is precisely what happens. The loop is while(condition(tensors)) { tensors = body(tensors); }, so the tensors you pass are updated to the tensors returned by the body each time, and then those updated tensors are passed to condition. The only time condition is called before body is the very first time, before the body of the pseudo-code loop above is entered. However, in that case, it\'s just about initializing the tensors you pass in loop_vars correctly. For example, you could pass the result of body as the loop_vars tensors to while_loop. oh, with loop_vars I refer to the function definition of while_loop, which is while_loop(condition, body, loop_vars) (its the tensors that are passed to condition and body)""""""']","To address the question of implementing a TensorFlow while loop with a condition dependent on a tensor computed within the loop body, it is essential to understand how `tf.while_loop` operates. The `tf.while_loop` function allows for the creation of a loop in which the body of the loop and the condition for continuation are defined by separate callable functions. Both the condition (`cond`) and the body (`body`) functions must take the same arguments, which are provided in the `loop_vars`.

In the given scenario, the user wants to perform random cropping on an image and continue this process until a crop of sufficient quality is obtained. The quality of the crop is determined by a condition that depends on the cropped image itself, which is computed within the loop body.

To achieve this, the key is to ensure that the tensor representing the cropped image (`img_crop`) is included in the `loop_vars` so that it can be passed between the `body` and `cond` functions. The `body` function will perform the random cropping and return the updated `img_crop` along with any other loop variables. The `cond` function will then evaluate the quality of the `img_crop` and determine whether the loop should continue.

The `tf.while_loop` function will call the `cond` function with the initial values of `loop_vars` before entering the loop. After each iteration, the updated values of `loop_vars` returned by the `body` function are passed back to the `cond` function to check if the loop should continue. This process ensures that the condition can depend on the tensor computed within the loop body.

Additionally, it is important to initialize `img_crop` with an appropriate initial value and ensure that the shapes of all loop variables remain consistent across iterations. If the shapes of the loop variables may change, the `shape_invariants` argument can be used to specify less specific shape invariants.

In summary, the solution involves defining the `body` function to perform the random cropping and return the updated `img_crop`, and the `cond` function to evaluate the quality of `img_crop`. By including `img_crop` in the `loop_vars`, the `tf.while_loop` can effectively manage the dependency of the condition on the tensor computed within the loop body."
