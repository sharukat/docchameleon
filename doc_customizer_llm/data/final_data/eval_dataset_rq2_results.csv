question,contexts,answer,ground_truth,answer_correctness,answer_similarity,faithfulness
"<p>When <code>src</code> has shape <code>[?]</code>, <code>tf.gather(src, tf.where(src != 0))</code> returns a tensor with shape <code>[?, 0]</code>. I'm not sure how a dimension can have size 0, and I'm especially unsure how to change the tensor back. I didn't find anything in the documentation to explain this, either.</p>

<p>I tried to <code>tf.transpose(tensor)[0]</code>, but the first dimension of the transposed tensor has size 0 and cannot be accessed! What's wrong?</p>
","['"""""" In Tensorflow, how to use tf.gather() for the last dimension? n\\nI am trying to gather slices of a tensor in terms of the last dimension for partial connection between layers. Because the output tensor\'s shape is [batch_size, h, w, depth], I want to select slices based on the last dimension, such as\\n\\n# L is intermediate tensor partL = L[:, :, :, [0,2,3,8]]\\n\\nHowever, tf.gather(L, [0, 2,3,8]) seems to only work for the first dimension (right?) Can anyone tell me how to do it? As of TensorFlow 1.3 tf.gather has an axis parameter, so the various workarounds here are no longer necessary. \\n\\nTranspose your matrix so that dimension to gather is first (transpose is expensive)\\n\\nreshape your tensor into 1d (reshape is cheap) and turn your gather column indices into a list of individual element indices at linear indexing, then reshape back\\n\\n use gather_nd. Will still need to turn your column indices into list of individual element indices. \\n\\nNote that tf.gather has an axis parameter as of TensorFlow 1.3. With gather_nd you can now do this as follows:\\n\\ncat_idx = tf.concat([tf.range(0, tf.shape(x)[0]), indices_for_dim1], axis=0) result = tf.gather_nd(matrix, cat_idx)\\n\\nAlso, as reported by user Nova:\\n\\nx = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) idx = tf.constant([1, 0, 2]) idx_flattened = tf.range(0, x.shape[0]) * x.shape[1] + idx y = tf.gather(tf.reshape(x, [-1]), # flatten input idx_flattened) # use flattened indices with tf.Session(\\\'\\\'): print y.eval() # [2 4 9]\\n\\nThe gist is flatten the tensor and use strided 1D addressing with tf.gather(...). \\n\\nI\\\'m not sure your first example works. Let\\\'s say tf.shape(x)[0] is 1, then cat_idx will be [0, 0, 2, 3, 8], which is not what you want to use with tf.gather_nd. In fact, in this case it would throw an error because the length of the innermost dimension of indices (2nd argument to gather_nd) cannot be bigger than the rank of params (1st argument to gather_nd). I\'ve posted a corrected version (using tf.stack) below. Yet another solution using tf.unstack(...), tf.gather(...) and tf.stack(..)\\n\\nimport tensorflow as tf import numpy as np shape = [2, 2, 2, 10] L = np.arange(np.prod(shape)) L = np.reshape(L, shape) indices = [0, 2, 3, 8] axis = -1 # last dimension def gather_axis(params, indices, axis=0): return tf.stack(tf.unstack(tf.gather(tf.unstack(params, axis=axis), indices)), axis=axis) print(L) with tf.Session() as sess: partL = sess.run(gather_axis(L, indices, axis)) print(partL)\\n\\nL = [[[[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19]] [[20 21 22 23 24 25 26 27 28 29] [30 31 32 33 34 35 36 37 38 39]]] [[[40 41 42 43 44 45 46 47 48 49] [50 51 52 53 54 55 56 57 58 59]] [[60 61 62 63 64 65 66 67 68 69] [70 71 72 73 74 75 76 77 78 79]]]] partL = [[[[ 0 2 3 8] [10 12 13 18]] [[20 22 23 28] [30 32 33 38]]] [[[40 42 43 48] [50 52 53 58]] [[60 62 63 68] [70 72 73 78]]]]\\n\\nA correct version of answer would read\\n\\ncat_idx = tf.stack([tf.range(0, tf.shape(x)[0]), indices_for_dim1], axis=1) result = tf.gather_nd(matrix, cat_idx)\\n\\n You can try this way, for instance(in most cases in NLP at the least),\\n\\nThe parameter is of shape [batch_size, depth] and the indices are [i, j, k, n, m] of which the length is batch_size. Then gather_nd can be helpful.""""""']","When working with TensorFlow, you may encounter a situation where a tensor has a shape of `[?, 0]` after using operations like `tf.gather` with `tf.where`. This shape indicates that the tensor has an unknown number of rows (`?`) and zero columns, which can be confusing and problematic for further processing.

The issue arises because `tf.where` returns the indices of non-zero elements, and when used with `tf.gather`, it can result in a tensor with a shape that includes a dimension of size 0. This happens because the condition in `tf.where` might not find any non-zero elements, leading to an empty tensor along the specified dimension.

To reshape the tensor from `[?, 0]` to `[?,]`, you need to understand that a dimension of size 0 means there are no elements along that dimension. Therefore, the tensor effectively has no data to reshape. The goal is to remove the dimension with size 0, resulting in a tensor with a shape of `[?,]`, which is a one-dimensional tensor with an unknown number of elements.

In TensorFlow, you can achieve this by using operations that remove dimensions of size 0. One approach is to use `tf.squeeze`, which removes dimensions of size 1, but in this case, you need to handle the dimension of size 0 specifically. Another approach is to use `tf.reshape` to explicitly reshape the tensor to the desired shape.

By understanding the nature of the tensor's shape and using appropriate TensorFlow operations, you can effectively reshape a tensor from `[?, 0]` to `[?,]`, making it suitable for further processing in your machine learning pipeline.",I think you should use <code>tf.not_equal</code> to perform elementwise comparison on the tensor. You can also shorten this a bit and use  <code>tf.boolean_mask</code> instead of <code>tf.where</code> and <code>tf.gather</code>.,0.7849768183,0.7920811864,0.5
"<p>I'm trying to get my head around <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/metrics/SensitivityAtSpecificity"" rel=""nofollow noreferrer"">tf.keras.metrics.SensitivityAtSpecificity</a>. I'm fine with the concept of sensity and specificity in isolation, but I'm unsure how the two are related in this single metric.</p>

<p>More specifically, I'm unsure how to interpret the <code>num_thresholds</code> argument. The example in documentation has <code>num_thresholds=1</code>. Setting <code>num_thresholds</code> greater than 1 with the same input data seems to always return a metric value of 1.0.</p>

<pre class=""lang-py prettyprint-override""><code>def print_metric_value(num_thresholds):
    # other values based on docs example
    m = tf.keras.metrics.SensitivityAtSpecificity(
        0.4, num_thresholds=num_thresholds)
    m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
    print('Result with num_thresholds = %d: %.1f' %
          (num_thresholds, m.result().numpy()))

print_metric_value(1)    # 0.5 - same as docs
print_metric_value(2)    # 1.0
print_metric_value(200)  # 1.0
</code></pre>
","['""""""tf.keras.metrics.SpecificityAtSensitivity num_thresholds interpretation\\n\\nI\'m trying to get my head around tf.keras.metrics.SensitivityAtSpecificity. I\'m fine with the concept of sensity and specificity in isolation, but I\'m unsure how the two are related in this single metric. More specifically, I\'m unsure how to interpret the num_thresholds argument. The example in documentation has num_thresholds=1. Setting num_thresholds greater than 1 with the same input data seems to always return a metric value of 1.0. def print_metric_value(num_thresholds): # other values based on docs example m = tf.keras.metrics.SensitivityAtSpecificity( 0.4, num_thresholds=num_thresholds) m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9]) print(\\\'Result with num_thresholds = %d: %.1f\\\' % (num_thresholds, m.result().numpy())) print_metric_value(1) # 0.5 - same as docs print_metric_value(2) # 1.0 print_metric_value(200) # 1.0\\n\\n. The num_thresholds refers to the number of thresholds. But you might ask: what is a threshold (in this context)? And the answer is that the threshold, which is in the range [0,1], is actually the value which all the predictions greater than that will be considered as positive (i.e. 1) and all the prediction lower than that will be considered as negative (i.e. 0). For example, consider the prediction vector as [0, 0.5, 0.3, 0.9] which are actually confidences scores (e.g. probabilities). Now if we apply the threshold value of 0.1, we get [0, 1, 1, 1]; or if we apply threshold value of 0.6 we get [0, 0, 0, 1] (i.e. only the confidence of last prediction is higher than 0.6). Now suppose you want to monitor the changes to specificity at a fixed sensitivity. What SensitivityAtSpecificity metric does is that, to compute the value of sensitivity, it would first compute the specificity at different thresholds and then chooses the threshold which has the closest specificity to the specificity value you have provided (for example, in your question you have given 0.4 as the specificity value). Then the sensitivity is computed at that threshold and will be returned as the value of this metric. The same thing applies to SpecificityAtSensitivity metric, just swap ""specificity"" and ""sensitivity"" in this paragraph. You might also ask: what are the threshold values? The answer is if num_thresholds=1 then the only threshold is 0.5. If num_thresholds > 1 then, besides 0 and 1 as thresholds, the interval (0,1) will be split into num_thresholds - 1 equal sub-intervals and the split points are chosen as additional threshold values. For example:\\n\\nnum_threshold | thresholds = 1 | [0.5] 2 | [0, 1] 3 | [0, 0.5, 1] 4 | [0, 0.33, 0.66, 1] 5 | [0, 0.25, 0.5, 0.75, 1] . \\n\\nExactly the answer I was looking for. Turns out my data leads to ambiguous thresholds - anything in (0, 0.5) gives a specificity of 0.5 (closest possible value to 0.4), but the sensitivity value changes dramatically at 0.3. """"""']","The question addresses the interpretation of the `num_thresholds` argument in the `tf.keras.metrics.SensitivityAtSpecificity` metric. This metric is designed to compute the best sensitivity where the specificity is greater than or equal to a specified value. 

The `num_thresholds` parameter determines the number of thresholds to be used for matching the given specificity. A threshold in this context is a value in the range [0,1] that determines whether a prediction is considered positive or negative. For example, with a threshold of 0.5, predictions greater than 0.5 are considered positive, and those less than or equal to 0.5 are considered negative.

When `num_thresholds` is set to 1, the only threshold considered is 0.5. This means that the metric will evaluate the specificity and sensitivity at this single threshold. If `num_thresholds` is greater than 1, the interval (0,1) is split into `num_thresholds - 1` equal sub-intervals, and these split points, along with 0 and 1, are used as additional thresholds. For instance, if `num_thresholds` is 2, the thresholds are [0, 1]; if `num_thresholds` is 3, the thresholds are [0, 0.5, 1]; and so on.

The metric computes the specificity at each threshold and selects the threshold that has the closest specificity to the specified value. It then computes the sensitivity at this selected threshold and returns it as the metric value. 

In the provided example, setting `num_thresholds` to 1 results in a metric value of 0.5, which matches the documentation. However, increasing `num_thresholds` to values greater than 1 (e.g., 2 or 200) results in a metric value of 1.0. This occurs because the data leads to ambiguous thresholds where any threshold in the range (0, 0.5) gives a specificity of 0.5, but the sensitivity value changes dramatically at 0.3.

In conclusion, the `num_thresholds` parameter controls the granularity of the thresholds used to compute the specificity and sensitivity. When `num_thresholds` is set to 1, it is equivalent to evaluating the specificity at a threshold of 0.5. For values greater than 1, the metric evaluates multiple thresholds to find the best sensitivity at the specified specificity.","The <code>num_thresholds</code> refers to the number of thresholds. But you might ask: what is a threshold (in this context)? And the answer is that the threshold, which is in the range [0,1], is actually the value which all the predictions greater than that will be considered as positive (i.e. 1) and all the prediction lower than that will be considered as negative (i.e. 0).
For example, consider the prediction vector as <code>[0, 0.5, 0.3, 0.9]</code> which are actually confidences scores (e.g. probabilities). Now if we apply the threshold value of <code>0.1</code>, we get <code>[0, 1, 1, 1]</code>; or if we apply threshold value of <code>0.6</code> we get <code>[0, 0, 0, 1]</code> (i.e. only the confidence of last prediction is higher than <code>0.6</code>).
Now suppose you want to monitor the changes to specificity at a fixed sensitivity. What <code>SensitivityAtSpecificity</code> metric does is that, to compute the value of sensitivity, it would first compute the specificity at different thresholds and then chooses the threshold which has the closest specificity to the specificity value you have provided (for example, in your question you have given <code>0.4</code> as the specificity value). Then the sensitivity is computed at that threshold and will be returned as the value of this metric. The same thing applies to <code>SpecificityAtSensitivity</code> metric, just swap ""specificity"" and ""sensitivity"" in this paragraph.
You might also ask: what are the threshold values? The answer is if <code>num_thresholds=1</code> then the only threshold is 0.5. If <code>num_thresholds &gt; 1</code> then, besides 0 and 1 as thresholds, the interval (0,1) will be split into <code>num_thresholds - 1</code> equal sub-intervals and the split points are chosen as additional threshold values. For example:",0.7590044001,0.9360176002,0.7
"<p>I have a generator yielding data and labels <code>yield data, labels</code> where the data is
an <code>numpy.ndarray</code> with variable rows and 500 columns of type <code>dtype=float32</code> and the labels are integers of <code>numpy.int64</code>.</p>
<p>I'm trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: <code>tf.data.Dataset.from_generator</code></p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">docs</a> say that the from_generator function needs a parameter <code>output_signature</code> as an input. But I'm having trouble understanding how to build this output_signature.</p>
<p>How can I make the output_signature for the generator I described?</p>
<p>Thank you!</p>
<p>Edit:
I used <code>tf.type_spec_from_value</code> to get this:</p>
<pre><code>dataset = tf.data.Dataset.from_generator(
   datagen_row,
   output_signature=(
      tf.TensorSpec(shape=(None, 512), dtype=tf.float32, name=None),
      tf.TensorSpec(shape=(), dtype=tf.int64, name=None)
   )
)
</code></pre>
<p>But is it correct to use None when the number of rows is varying for the first data type?</p>
","['""""""How to create output_signature for tensorflow.dataset.from_generator\\n\\nI have a generator yielding data and labels yield data, labels where the data is an numpy.ndarray with variable rows and 500 columns of type dtype=float32 and the labels are integers of numpy.int64. I\'m trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: tf.data.Dataset.from_generator\\n\\nThe docs say that the from_generator function needs a parameter output_signature as an input. But I\'m having trouble understanding how to build this output_signature. How can I make the output_signature for the generator I described? Edit: I used tf.type_spec_from_value to get this:\\n\\ndataset = tf.data.Dataset.from_generator( datagen_row, output_signature=( tf.TensorSpec(shape=(None, 512), dtype=tf.float32, name=None), tf.TensorSpec(shape=(), dtype=tf.int64, name=None) ) )\\n\\nBut is it correct to use None when the number of rows is varying for the first data type? 1\\n\\nIf possible, add some dummy data with generator. if your datagen_row() function yields input_data, label with format 500 and 1 than your output_signature should be:\\n\\noutput_signature=( tf.TensorSpec(shape=(None, 500), dtype=tf.float32, name=None), tf.TensorSpec(shape=(), dtype=tf.int64, name=None))\\n\\nwhere the first TensorSpec is for the data format and the second one for the label format. But it would be helpful if you post the function + maybe data examples or data shape here.""""""']","To create the `output_signature` for `tf.data.Dataset.from_generator` when your generator yields data and labels, where the data is a `numpy.ndarray` with variable rows and 500 columns of type `dtype=float32` and the labels are integers of `numpy.int64`, you need to define the `output_signature` using `tf.TensorSpec`. 

The `output_signature` parameter is a nested structure of `tf.TypeSpec` objects that specifies the type and shape of each component of an element yielded by the generator. In your case, the data has a variable number of rows and a fixed number of 500 columns, and the labels are scalar integers.

For the data, you should use `tf.TensorSpec` with `shape=(None, 500)` to indicate that the number of rows is variable, and `dtype=tf.float32` to match the data type. For the labels, you should use `tf.TensorSpec` with `shape=()` to indicate a scalar value, and `dtype=tf.int64` to match the label type.

Using `None` for the number of rows in the `shape` is correct when the number of rows is variable. This allows TensorFlow to handle batches of data with different numbers of rows while maintaining the fixed number of columns.

In summary, the `output_signature` should be defined as a tuple of `tf.TensorSpec` objects, where the first `TensorSpec` corresponds to the data with a shape of `(None, 500)` and `dtype=tf.float32`, and the second `TensorSpec` corresponds to the labels with a shape of `()` and `dtype=tf.int64`. This setup ensures that TensorFlow correctly interprets the structure and types of the elements yielded by your generator.","If your datagen_row() function yields input_data, label with format 500 and 1 than your output_signature should be:
<pre><code>  output_signature=(
  tf.TensorSpec(shape=(None, 500), dtype=tf.float32, name=None),
  tf.TensorSpec(shape=(), dtype=tf.int64, name=None))
</code></pre>
where the first TensorSpec is for the data format and the second one for the label format. But it would be helpful if you post the function + maybe data examples or data shape here. Otherwise it is hard to help.",0.7871086051,0.8984344204,1
"<p>I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup_sparse</code></a> I found it only supports 2D sparse tensors,</p>

<blockquote>
  <p>sp_ids: N x M SparseTensor of int64 ids where N is typically batch size and M is arbitrary.</p>
</blockquote>

<p>My example code here</p>

<pre><code>import numpy as np
import tensorflow as tf
tf.enable_eager_execution()

# [feature number, embedding dim] 
w = tf.get_variable(""w"", [4, 4], initializer=tf.random_normal_initializer())

z = np.array(
     [
      [
        [0, 1, 2, 3],   # get the vector of row 0, 1, 2, 3 of the embedding matrix w and get the sum
        [2, 3]
      ],

      [
        [1, 3],
        [2]
      ],

      [
        [0, 1, 3],
        [1, 2]
      ]
     ])

sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],
                     indices=[[0,0,0],[0,0,1],[0,0,2],[0,0,3],[0,1,2],
                              [0,1,3],[1,0,1],[1,0,3],[1,1,2],[2,0,0],
                              [2,0,1],[2,0,3],[2,1,1],[2,1,2]],
                     dense_shape=[3, 2, 4])

tf.nn.embedding_lookup_sparse(w, sp, None, combiner='sum')
# the outputs
&lt;tf.Tensor: id=970, shape=(3, 4), dtype=float32, numpy=
array([[-5.8729677 , -1.3900641 ,  0.8126096 , -3.1223912 ],
       [-1.0788026 , -1.1324122 ,  0.34160078,  0.23714277],
       [-2.497394  , -2.7855003 ,  3.0201516 , -1.8009453 ]],
      dtype=float32)&gt;

print(w)
&lt;tf.Variable 'w:0' shape=(4, 4) dtype=float32, numpy=
array([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],
       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],
      dtype=float32)&gt;
</code></pre>

<p>But the expected output is a matrix with a dimension of <code>3x2x4</code>, not <code>3x4</code>. Does <code>tf.nn.embedding_lookup_sparse</code> support this operation?</p>
","['""""""tf.nn.embedding_lookup_sparse 3D sparse tensor input\\n\\n I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of tf.nn.embedding_lookup_sparse I found it only supports 2D sparse tensors,\\n\\nsp_ids: N x M SparseTensor of int64 ids where N is typically batch size and M is arbitrary. My example code here\\n\\nimport numpy as np import tensorflow as tf tf.enable_eager_execution() # [feature number, embedding dim] w = tf.get_variable(""w"", [4, 4], initializer=tf.random_normal_initializer()) z = np.array( [ [ [0, 1, 2, 3], # get the vector of row 0, 1, 2, 3 of the embedding matrix w and get the sum [2, 3] ], [ [1, 3], [2] ], [ [0, 1, 3], [1, 2] ] ]) sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2], indices=[[0,0,0],[0,0,1],[0,0,2],[0,0,3],[0,1,2], [0,1,3],[1,0,1],[1,0,3],[1,1,2],[2,0,0], [2,0,1],[2,0,3],[2,1,1],[2,1,2]], dense_shape=[3, 2, 4]) tf.nn.embedding_lookup_sparse(w, sp, None, combiner=\\\'sum\\\') # the outputs <tf.Tensor: id=970, shape=(3, 4), dtype=float32, numpy= array([[-5.8729677 , -1.3900641 , 0.8126096 , -3.1223912 ], [-1.0788026 , -1.1324122 , 0.34160078, 0.23714277], [-2.497394 , -2.7855003 , 3.0201516 , -1.8009453 ]], dtype=float32)> print(w) <tf.Variable \\\'w:0\\\' shape=(4, 4) dtype=float32, numpy= array([[-2.5669768 , -0.38916406, 1.4039794 , -2.8173826 ], [ 1.1483854 , -1.2639242 , 1.2745714 , 0.7792944 ], [-1.3400027 , -0.46362385, -1.3652185 , 0.27220532], [-0.8871854 , 0.5951359 , 0.43224794, -0.8143569 ]], dtype=float32)>\\n\\nBut the expected output is a matrix with a dimension of 3x2x4, not 3x4. Does tf.nn.embedding_lookup_sparse support this operation? \\n\\nThe most easier way to do so is to make the sparse tensor as a 2D tensor and get the weights of the embedding matrix and then reshape. # First make the z as a 2D arr and create a sparse tensor z = np.array([ [0, 1, 2, 3], # get the row 0,1,2,3 of the embedding matrix w and get the sum [2, 3], [1, 3], [2], [0, 1, 3], [1, 2] ]) sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2], indices=[[0,0],[0,1],[0,2],[0,3],[1,2],[1,3],[2,1], [2,3],[3,2],[4,0],[4,1],[4,3],[5,1],[5,2]], dense_shape=[6, 4]) res = tf.nn.embedding_lookup_sparse(w, sp, None, combiner=\\\'sum\\\') res.numpy() # the output array([[-3.6457794 , -1.5215762 , 1.7455802 , -2.5802398 ], [-2.227188 , 0.13151208, -0.9329706 , -0.5421516 ], [ 0.2612 , -0.6687883 , 1.7068193 , -0.03506255], [-1.3400027 , -0.46362385, -1.3652185 , 0.27220532], [-2.3057768 , -1.0579524 , 3.1107986 , -2.8524451 ], [-0.19161725, -1.7275481 , -0.0906471 , 1.0514997 ]], # reshape tf.reshape(res, [-1, 2, 4]) # that is exacly what I want. array([[[-3.6457794 , -1.5215762 , 1.7455802 , -2.5802398 ], [-2.227188 , 0.13151208, -0.9329706 , -0.5421516 ]], [[ 0.2612 , -0.6687883 , 1.7068193 , -0.03506255], [-1.3400027 , -0.46362385, -1.3652185 , 0.27220532]], [[-2.3057768 , -1.0579524 , 3.1107986 , -2.8524451 ], [-0.19161725, -1.7275481 , -0.0906471 , 1.0514997 ]]]) # print w, and the above result is right w.numpy() array([[-2.5669768 , -0.38916406, 1.4039794 , -2.8173826 ], [ 1.1483854 , -1.2639242 , 1.2745714 , 0.7792944 ], [-1.3400027 , -0.46362385, -1.3652185 , 0.27220532], [-0.8871854 , 0.5951359 , 0.43224794, -0.8143569 ]], dtype=float32)\\n\\nSo, forget the 3D sparse tensor, simply convert it to 2D tensor. Because you only care about the values (index of rows, which are used to get the corresponding rows of the embedding matrix) in the sparse tensor.\n\n After diving into safe_embedding_lookup_sparse\\\'s unit test, I\'m more confused why I got this result if giving the sparse weights, especially why we got something like embedding_weights[0][3] where 3 is not appeared in the code above.  I don\'t figure it out yet after reading safe_embedding_lookup_sparse\'s unit test. I have updated the question and would you like to explain the code for us?""""""']","The question addresses whether `tf.nn.embedding_lookup_sparse` supports 3D sparse tensors for embedding lookups, given that the expected output is a matrix with dimensions `3x2x4`, not `3x4`.

`tf.nn.embedding_lookup_sparse` is designed to look up embeddings for given ids and weights from a list of tensors. According to the TensorFlow API documentation, `sp_ids` must be a 2D `SparseTensor` or `RaggedTensor`. This means that `tf.nn.embedding_lookup_sparse` does not natively support 3D sparse tensors.

In the provided example, the user has an embedding matrix and a 3D sparse tensor. When attempting to use `tf.nn.embedding_lookup_sparse`, the output is a matrix with dimensions `3x4`, which does not match the expected `3x2x4` dimensions. This discrepancy arises because the function only supports 2D sparse tensors, and thus, it cannot directly handle the 3D sparse tensor input.

To achieve the desired output dimensions, the recommended approach is to convert the 3D sparse tensor into a 2D sparse tensor. This involves flattening the 3D structure into a 2D array, performing the embedding lookup, and then reshaping the result back into the desired 3D shape. By doing so, the embedding lookup can be performed correctly, and the final output can be reshaped to match the expected dimensions.

In summary, `tf.nn.embedding_lookup_sparse` does not support 3D sparse tensors directly. The solution is to convert the 3D sparse tensor into a 2D sparse tensor, perform the embedding lookup, and then reshape the result to the desired dimensions. This approach ensures that the embedding lookup operation is compatible with the function's requirements and produces the expected output shape.","The most easier way to do so is to make the sparse tensor as a 2D tensor and get the weights of the embedding matrix and then reshape.
So, forget the 3D sparse tensor, simply convert it to 2D tensor. Because you only care about the values (index of rows, which are used to get the corresponding rows of the embedding matrix) in the sparse tensor.

",0.3339490954,0.8357963815,0.9090909091
"<p>How to use new tf.keras API with recurrent neural network? I have checked the documentation but there is no example of such a situation.
There is this great book Hands on machine learning from 2017. Since that year the API of tensorflow has evolved and I am trying to rewrite recurrent neural network for time series prediction with using version <code>1.14</code> code.
The code from the book is using older <code>tf.nn.dynamic_rnn</code> and <code>tf.nn.rnn_cell.BasicRNNCell</code>:</p>

<pre><code>n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1
learning_rate = 0.001

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])
cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)
rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
loss = tf.reduce_mean(tf.square(outputs - y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()
n_iterations = 500
batch_size = 50

with tf.Session() as sess:
    init.run()
        for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})
</code></pre>

<p>And this code works just fine (except that it throws warnings about deprecation left and right). I wanted to use <code>tf.keras</code> API as suggested in warning. My code is the same except:</p>

<pre><code>cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)  
rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
</code></pre>

<p>But this yields following exception:</p>

<pre><code>InvalidArgumentError: Input to reshape is a tensor with 50 values, but the requested shape requires a multiple of 20
 [[node Reshape_1 (defined at &lt;ipython-input-9-879361be49dd&gt;:3) ]]
</code></pre>

<p>so I understand that the problematic line is</p>

<pre><code>outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
</code></pre>

<p>After checking and comparing documentation for both cells <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a> and 
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN</a> I can't find the culprit.</p>

<p><strong>What is the difference with these two cells? How to use tf.keras API with time series?</strong></p>

<p>Full old code: <a href=""https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb"" rel=""nofollow noreferrer"">https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb</a></p>

<p>Full ""my"" code:</p>

<pre><code>import numpy as np
import tensorflow as tf
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd
from utils import shuffle_batch, variable_summaries
import os


dir_path = os.getcwd()

now = datetime.utcnow().strftime(""%Y%m%d%H%M%S"")
root_logdir = ""tf_logs""
logdir = ""{}/run-{}/"".format(root_logdir, now)
print(dir_path)


t_min, t_max = -5, 5
section_start = (t_max + t_min) / 2
resolution = 0.1
n_steps = 20

def time_series(t):
    return np.sin(t)

def next_batch(batch_size, n_steps):
    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)
    Ts = t0 + np.arange(0., n_steps + 1) * resolution
    ys = time_series(Ts)
    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)


t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))

t_instance = np.linspace(start = section_start, stop = section_start + resolution * (n_steps + 1),num = n_steps + 1)

plt.figure(figsize=(11,4))
plt.subplot(121)
plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"")
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)
#plt.axis([-10, 10, -17, 13])
plt.xlabel(""Time"")
plt.ylabel(""Value"")

plt.subplot(122)
plt.title(""A training instance"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""c*"", markersize=10, label=""target"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")


# In[6]:


n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])


# In[7]:


cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)                        


rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
print(rnn_outputs.get_shape())


stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons], name='reshape1')
stacked_outputs = tf.keras.layers.Dense(n_outputs,name=""hidden2"")(stacked_rnn_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs], name='reshape2')


learning_rate = 0.001

loss = tf.reduce_mean(tf.square(outputs - y)) # MSE
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()

n_iterations = 1500
batch_size = 50
save_path =os.path.join(dir_path,""model"",""recurrent_sinus_model"")

with tf.Session() as sess:
    init.run()
    for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    saver.save(sess, save_path)


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})


plt.title(""Testing the model"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""w*"", markersize=10, label=""target"")
plt.plot(t_instance[1:], y_pred[0,:,0], ""r."", markersize=10, label=""prediction"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")

plt.show()


# In[ ]:


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t.reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})



plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"",linewidth=5,c='r')
plt.plot(t[:-1], time_series(t[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)

plt.xlabel(""Time"")
plt.ylabel(""Value"")
</code></pre>
","['""""""RNN in Tensorflow vs Keras, depreciation of tf.nn.dynamic_rnn()\\n\\nMy question is: Are the tf.nn.dynamic_rnn and keras.layers.RNN(cell) truly identical as stated in docs? I am planning on building an RNN, however, it seems that tf.nn.dynamic_rnn is depricated in favour of Keras. In particular, it states that:\\n\\nWarning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use keras.layers.RNN(cell), which is equivalent to this API\\n\\nBut I don\\\'t see how the APIs are equivalent, in the case of variable sequence lengths! In raw TF, we can specify a tensor of shape (batch_size, seq_lengths). This way, if our sequence is [0, 1, 2, 3, 4] and the longest sequence in the batch is of size 10, we can pad it with 0s and [0, 1, 2, 3, 4, 0, 0, 0, 0, 0], we can say seq_length=5 to process [0, 1, 2, 3, 4]. However, in Keras, this is not how it works! What we can do, is specify the mask_zero=True in previous Layers, e.g. the Embedding Layer. This will also mask the 1st zero! I can go around it by adding ones to the whole vector, but then thats extra preprocessing that I need to do after processing using tft.compute_vocabulary(), which maps vocabulary words to 0 indexed vector. 6\\n\\nare you talking about keras or tf.keras?\n\nHow to use new tf.keras API with recurrent neural network? I have checked the documentation but there is no example of such a situation. The code from the book is using older tf.nn.dynamic_rnn and tf.nn.rnn_cell.BasicRNNCell:\\n\\nn_steps = 20 n_inputs = 1 n_neurons = 100 n_outputs = 1 learning_rate = 0.001 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) y = tf.placeholder(tf.float32, [None, n_steps, n_outputs]) cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu) rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32) stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs) outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs]) loss = tf.reduce_mean(tf.square(outputs - y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) training_op = optimizer.minimize(loss) init = tf.global_variables_initializer() saver = tf.train.Saver() n_iterations = 500 batch_size = 50 with tf.Session() as sess: init.run() for iteration in range(n_iterations): X_batch, y_batch = next_batch(batch_size, n_steps) sess.run(training_op, feed_dict={X: X_batch, y: y_batch}) if iteration % 100 == 0: mse = loss.eval(feed_dict={X: X_batch, y: y_batch}) print(iteration, ""\\\\tMSE:"", mse) X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs))) y_pred = sess.run(outputs, feed_dict={X: X_new})\\n\\nAnd this code works just fine (except that it throws warnings about deprecation left and right). I wanted to use tf.keras API as suggested in warning. My code is the same except:\\n\\ncell = tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu) rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)\\n\\nBut this yields following exception:\\n\\nInvalidArgumentError: Input to reshape is a tensor with 50 values, but the requested shape requires a multiple of 20 [[node Reshape_1 (defined at <ipython-input-9-879361be49dd>:3) ]]\\n\\nso I understand that the problematic line is\\n\\noutputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\\n\\n import numpy as np import tensorflow as tf from datetime import datetime import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split import pandas as pd from utils import shuffle_batch, variable_summaries import os dir_path = os.getcwd() now = datetime.utcnow().strftime(""%Y%m%d%H%M%S"") root_logdir = ""tf_logs"" logdir = ""{}/run-{}/"".format(root_logdir, now) print(dir_path) t_min, t_max = -5, 5 section_start = (t_max + t_min) / 2 resolution = 0.1 n_steps = 20 def time_series(t): return np.sin(t) def next_batch(batch_size, n_steps): t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution) Ts = t0 + np.arange(0., n_steps + 1) * resolution ys = time_series(Ts) return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1) t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution)) t_instance = np.linspace(start = section_start, stop = section_start + resolution * (n_steps + 1),num = n_steps + 1) plt.figure(figsize=(11,4)) plt.subplot(121) plt.title(""A time series (generated)"", fontsize=14) plt.plot(t, time_series(t), label=r""original"") plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""b-"", linewidth=3, label=""A training instance"") plt.legend(loc=""lower left"", fontsize=14) #plt.axis([-10, 10, -17, 13]) plt.xlabel(""Time"") plt.ylabel(""Value"") plt.subplot(122) plt.title(""A training instance"", fontsize=14) plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"") plt.plot(t_instance[1:], time_series(t_instance[1:]), ""c*"", markersize=10, label=""target"") plt.legend(loc=""upper left"") plt.xlabel(""Time"") # In[6]: n_steps = 20 n_inputs = 1 n_neurons = 100 n_outputs = 1 X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) y = tf.placeholder(tf.float32, [None, n_steps, n_outputs]) # In[7]: cell = tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu) rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X) print(rnn_outputs.get_shape()) stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons], name=\\\'reshape1\\\') stacked_outputs = tf.keras.layers.Dense(n_outputs,name=""hidden2"")(stacked_rnn_outputs) outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs], name=\\\'reshape2\\\') learning_rate = 0.001 loss = tf.reduce_mean(tf.square(outputs - y)) # MSE optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) training_op = optimizer.minimize(loss) init = tf.global_variables_initializer() saver = tf.train.Saver() n_iterations = 1500 batch_size = 50 save_path =os.path.join(dir_path,""model"",""recurrent_sinus_model"") with tf.Session() as sess: init.run() for iteration in range(n_iterations): X_batch, y_batch = next_batch(batch_size, n_steps) sess.run(training_op, feed_dict={X: X_batch, y: y_batch}) if iteration % 100 == 0: mse = loss.eval(feed_dict={X: X_batch, y: y_batch}) print(iteration, ""\\\\tMSE:"", mse) saver.save(sess, save_path) with tf.Session() as sess: saver.restore(sess, save_path) X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs))) y_pred = sess.run(outputs, feed_dict={X: X_new}) plt.title(""Testing the model"", fontsize=14) plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"") plt.plot(t_instance[1:], time_series(t_instance[1:]), ""w*"", markersize=10, label=""target"") plt.plot(t_instance[1:], y_pred[0,:,0], ""r."", markersize=10, label=""prediction"") plt.legend(loc=""upper left"") plt.xlabel(""Time"") plt.show() # In[ ]: with tf.Session() as sess: saver.restore(sess, save_path) X_new = time_series(np.array(t.reshape(-1, n_steps, n_inputs))) y_pred = sess.run(outputs, feed_dict={X: X_new}) plt.title(""A time series (generated)"", fontsize=14) plt.plot(t, time_series(t), label=r""original"",linewidth=5,c=\\\'r\\\') plt.plot(t[:-1], time_series(t[:-1]), ""b-"", linewidth=3, label=""A training instance"") plt.legend(loc=""lower left"", fontsize=14) plt.xlabel(""Time"") plt.ylabel(""Value"")\\n\\nrecurrent-neural-network\\n\\n rnn_outputs, rnn_states = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"", return_state=True, return_sequences=True)(X)\\n\\nrnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)\\n\\nso the parameter return_sequences=True make the RNN return the time series as well, and well, this is the point. """"""']","### Description

The question addresses the transition from using the deprecated `tf.nn.dynamic_rnn` and `tf.nn.rnn_cell.BasicRNNCell` to the newer `tf.keras` API for building a recurrent neural network (RNN) for time series prediction. The user is encountering an issue with reshaping tensors when using `tf.keras.layers.RNN` and `tf.keras.layers.SimpleRNNCell`.

### Explanation

The `tf.nn.dynamic_rnn` function and `tf.keras.layers.RNN` are designed to serve similar purposes, but they have differences in their implementation and usage, especially when handling variable sequence lengths and tensor shapes.

1. **Deprecation of `tf.nn.dynamic_rnn`**:
   - The `tf.nn.dynamic_rnn` function is deprecated in favor of the `tf.keras.layers.RNN` API. The deprecation notice suggests using `tf.keras.layers.RNN` as it provides a more integrated and flexible approach within the Keras framework.

2. **Handling Variable Sequence Lengths**:
   - In `tf.nn.dynamic_rnn`, you can specify a tensor of shape `(batch_size, seq_lengths)` to handle variable sequence lengths. This allows you to pad sequences and specify the actual lengths to process.
   - In `tf.keras`, handling variable sequence lengths is typically done using the `mask_zero=True` parameter in layers like `tf.keras.layers.Embedding`. This masks the zero values in the input sequences. However, this approach might require additional preprocessing if your vocabulary maps words to a zero-indexed vector.

3. **Tensor Reshaping Issue**:
   - The error `InvalidArgumentError: Input to reshape is a tensor with 50 values, but the requested shape requires a multiple of 20` indicates a mismatch in the expected tensor shape during reshaping.
   - In the original code using `tf.nn.dynamic_rnn`, the reshaping of `rnn_outputs` to `stacked_rnn_outputs` and then to `stacked_outputs` is straightforward because the output shape is well-defined.
   - When using `tf.keras.layers.RNN`, the output tensor shape might differ, leading to issues during reshaping. The `tf.keras.layers.RNN` layer returns the entire sequence by default, which might not match the expected shape for further processing.

4. **Solution**:
   - Ensure that the `return_sequences` parameter is set correctly in the `tf.keras.layers.RNN` layer. If you need the entire sequence, set `return_sequences=True`. If you only need the final output, set `return_sequences=False`.
   - Verify the shapes of the tensors at each step to ensure they match the expected dimensions for reshaping and further processing.

By understanding these differences and ensuring the correct configuration of the `tf.keras.layers.RNN` layer, you can successfully transition from the deprecated `tf.nn.dynamic_rnn` to the newer `tf.keras` API for building RNNs in TensorFlow.","So the answer is:
<pre><code>rnn_outputs, rnn_states  = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"", return_state=True, return_sequences=True)(X)
</code></pre>

instead of
<pre><code>rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
</code></pre>

so the parameter <code>return_sequences=True</code> make the RNN return the time series as well, and well, this is the point.",0.8030915332,0.8123661327,1
"<p>The TensorFlow documentation for <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> explicitly declares that I should not apply softmax to the inputs of this op:</p>

<blockquote>
  <p>This op expects unscaled logits, since it performs a softmax on logits
  internally for efficiency. Do not call this op with the output of
  softmax, as it will produce incorrect results.</p>
</blockquote>

<p>However if I use cross entropy without softmax it gives me unexpected results. According to <a href=""https://cs231n.github.io/neural-networks-3/#sanitycheck"" rel=""nofollow noreferrer"">CS231n course</a> the expected loss value is around 2.3 for CIFAR-10:</p>

<blockquote>
  <p>For example, for CIFAR-10 with a Softmax classifier we would expect
  the initial loss to be 2.302, because we expect a diffuse probability
  of 0.1 for each class (since there are 10 classes), and Softmax loss
  is the negative log probability of the correct class so: -ln(0.1) =
  2.302.</p>
</blockquote>

<p>However without softmax I get much bigger values, for example 108.91984.</p>

<p>What exactly am I doing wrong with <code>sparse_softmax_cross_entropy_with_logits</code>? The TF code is shown below.</p>

<pre><code>import tensorflow as tf
import numpy as np
from tensorflow.python import keras


(_, _), (x_test, y_test) = keras.datasets.cifar10.load_data()
x_test = np.reshape(x_test, [-1, 32, 32, 3])

y_test = np.reshape(y_test, (10000,))
y_test = y_test.astype(np.int32)

x = tf.placeholder(dtype=tf.float32, shape=(None, 32, 32, 3))
y = tf.placeholder(dtype=tf.int32, shape=(None,))

layer = tf.layers.Conv2D(filters=16, kernel_size=3)(x)
layer = tf.nn.relu(layer)
layer = tf.layers.Flatten()(layer)
layer = tf.layers.Dense(units=1000)(layer)
layer = tf.nn.relu(layer)
logits = tf.layers.Dense(units=10)(layer)

# If this line is uncommented I get expected value around 2.3
# logits = tf.nn.softmax(logits)

loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,
                                                      logits=logits)
loss = tf.reduce_mean(loss, name='cross_entropy')

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    res = sess.run(loss, feed_dict={x: x_test[0:256], y: y_test[0:256]})
    print(""loss: "", res)
    # Expected output is value close to 2.3
    # Real outputs are 108.91984, 72.82324, etc.

</code></pre>
","['""""""We can compute the cross-entropy loss on a row-wise basis and see the results. Below we can see that training instance 1 has a loss of 0.479, while training instance 2 has a higher loss of 1.200. This result makes sense because in our example above, y_hat_softmax showed that training instance 1\\\'s highest probability was for ""Class 2"", which matches training instance 1 in y_true; however, the prediction for training instance 2 showed a highest probability for ""Class 1"", which does not match the true class ""Class 3"". loss_per_instance_1 = -tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1]) sess.run(loss_per_instance_1) # array([ 0.4790107 , 1.19967598])\\n\\nWhat we really want is the total loss over all the training instances. So we can compute:\\n\\ntotal_loss_1 = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1])) sess.run(total_loss_1) # 0.83934333897877944\\n\\nUsing softmax_cross_entropy_with_logits()\\n\\nWe can instead compute the total cross entropy loss using the tf.nn.softmax_cross_entropy_with_logits() function, as shown below. loss_per_instance_2 = tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true) sess.run(loss_per_instance_2) # array([ 0.4790107 , 1.19967598]) total_loss_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true)) sess.run(total_loss_2) # 0.83934333897877922\\n\\nNote that total_loss_1 and total_loss_2 produce essentially equivalent results with some small differences in the very final digits. However, you might as well use the second approach: it takes one less line of code and accumulates less numerical error because the softmax is done for you inside of softmax_cross_entropy_with_logits(). \\n\\nI confirm all of the above. The simple code: M = tf.random.uniform([100, 10], minval=-1.0, maxval=1.0); labels = tf.one_hot(tf.random.uniform([100], minval=0, maxval=10 , dtype=\\\'int32\\\'), 10); tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=M) - tf.reduce_sum(-tf.nn.log_softmax(M)*tf.one_hot(labels, 10), -1) returns close-to-zero everywhere\\n\\n\\n\\nSorry for simple/dummy question. I didn\\\'t understand getting loss \\\'0.479\\\' from training instance-1. True label for instance-1 is \\\'2\\\'. If I apply -1xlog2(0.619) I get 0.691. Edit: Loss is calculated using log \\\'e\\\' base, okay. tf.nn.softmax computes the forward propagation through a softmax layer. You use it during evaluation of the model when you compute the probabilities that the model outputs. tf.nn.softmax_cross_entropy_with_logits computes the cost for a softmax layer. It is only used during training. The logits are the unnormalized log probabilities output the model (the values output before the softmax normalization is applied to them). \\n\\nI get it. Why not call the function, tf.nn.softmax_cross_entropy_sans_normalization? @auro because it normalizes the values (internally) during the cross-entropy computation. The point of tf.nn.softmax_cross_entropy_with_logits is to evaluate how much the model deviates from the gold labels, not to provide a normalized output. In the case of using tf.nn.sparse_softmax_cross_entropy_with_logits() computes the cost of a sparse softmax layer, and thus should only be used during training what would be the alternative when running the model against new data, is it possible to obtain probabilities from this one. @SerialDev, it\'s not possible to get probabilities from tf.nn.sparse_softmax_cross_entropy_with_logits. To get probabilities use tf.nn.softmax. They\'re not log probabilities but log odds. Mathematical motivation for term\\n\\nWhen we wish to constrain an output between 0 and 1, but our model architecture outputs unconstrained values, we can add a normalisation layer to enforce this. A common choice is a sigmoid function.1 In binary classification this is typically the logistic function, and in multi-class tasks the multinomial logistic function (a.k.a softmax).2\\n\\nIf we want to interpret the outputs of our new final layer as \'probabilities\', then (by implication) the unconstrained inputs to our sigmoid must be inverse-sigmoid(probabilities). In the logistic case this is equivalent to the log-odds of our probability (i.e. the log of the odds) a.k.a. logit:\\n\\nThat is why the arguments to softmax is called logits in Tensorflow - because under the assumption that softmax is the final layer in the model, and the output p is interpreted as a probability, the input x to this layer is interpretable as a logit:\\n\\nIn Machine Learning there is a propensity to generalise terminology borrowed from maths/stats/computer science, hence in Tensorflow logit (by analogy) is used as a synonym for the input to many normalisation functions. While it has nice properties such as being easily diferentiable, and the aforementioned probabilistic interpretation, it is somewhat arbitrary. softmax might be more accurately called softargmax, as it is a smooth approximation of the argmax function. \\n\\n Adding to that, Tensorflow has optimised the operation of applying the activation function then calculating cost using its own activation followed by cost functions. Hence it is a good practice to use: tf.nn.softmax_cross_entropy() over tf.nn.softmax(); tf.nn.cross_entropy()\\n\\nYou can find prominent difference between them in a resource intensive model. \\n\\nthe answer above clearly haven\'t read the question.. They all say the same things, which are known, but don\'t answer the question itself\\n\\n\\n\\n@abhish Did you mean, tf.nn.softmax followed by tf.losses.softmax_cross_entropy? Tensorflow 2.0 Compatible Answer: The explanations are very detailed about Logits and the related Functions. All those functions, when used in Tensorflow 1.x will work fine, but if you migrate your code from 1.x (1.14, 1.15, etc) to 2.x (2.0, 2.1, etc..), using those functions result in error. Hence, specifying the 2.0 Compatible Calls for all the functions, we discussed above, if we migrate from 1.x to 2.x, for the benefit of the community. tf.nn.softmax_cross_entropy_with_logits\\n\\ntf.nn.sparse_softmax_cross_entropy_with_logits\\n\\nRespective Functions when Migrated from 1.x to 2.x:\\n\\ntf.compat.v2.nn.softmax\\n\\ntf.compat.v2.nn.softmax_cross_entropy_with_logits\\n\\ntf.compat.v2.nn.sparse_softmax_cross_entropy_with_logits\\n\\n Logits are the unnormalized outputs of a neural network. Softmax is a normalization function that squashes the outputs of a neural network so that they are all between 0 and 1 and sum to 1. Softmax_cross_entropy_with_logits is a loss function that takes in the outputs of a neural network (after they have been squashed by softmax) and the true labels for those outputs, and returns a loss value. One more thing that I would definitely like to highlight as logit is just a raw output, generally the output of last layer. This can be a negative value as well. If we use it as it\\\'s for ""cross entropy"" evaluation as mentioned below:\\n\\n-tf.reduce_sum(y_true * tf.log(logits))\\n\\nthen it wont work. As log of -ve is not defined.\n\nWhat\'s the difference between sparse_softmax_cross_entropy_with_logits and softmax_cross_entropy_with_logits? \\n\\n I recently came across tf.nn.sparse_softmax_cross_entropy_with_logits and I can not figure out what the difference is compared to tf.nn.softmax_cross_entropy_with_logits. Is the only difference that training vectors y have to be one-hot encoded when using sparse_softmax_cross_entropy_with_logits? Reading the API, I was unable to find any other difference compared to softmax_cross_entropy_with_logits. But why do we need the extra function then? Shouldn\'t softmax_cross_entropy_with_logits produce the same results as sparse_softmax_cross_entropy_with_logits, if it is supplied with one-hot encoded training data/vectors? \\n\\nI\'m interested in seeing a comparison of their performance if both can be used (e.g. with exclusive image labels); I\'d expect the sparse version to be more efficient, at least memory-wise. See also this question, which discusses all cross-entropy functions in tensorflow (turns out there are lots of them). Having two different functions is a convenience, as they produce the same result. The difference is simple:\\n\\nFor sparse_softmax_cross_entropy_with_logits, labels must have the shape [batch_size] and the dtype int32 or int64. Each label is an int in range [0, num_classes-1]. For softmax_cross_entropy_with_logits, labels must have the shape [batch_size, num_classes] and dtype float32 or float64. Labels used in softmax_cross_entropy_with_logits are the one hot version of labels used in sparse_softmax_cross_entropy_with_logits. Another tiny difference is that with sparse_softmax_cross_entropy_with_logits, you can give -1 as a label to have loss 0 on this label.  \\n\\nIs the -1 correct? As the documentation reads: ""Each entry in labels must be an index in [0, num_classes). Other values will raise an exception when this op is run on CPU, and return NaN for corresponding loss and gradient rows on GPU.""\\n\\n\\n\\n[0, num_classes) = [0, num_classes-1]\\n\\nIs this statement correct?  Labels used in softmax_cross_entropy_with_logits are the one hot version of labels used in sparse_softmax_cross_entropy_with_logits. Is it backwards? Isn\'t the sparse loss function the one with int of 0, so isn\'t the sparse one the one-hot version? I would just like to add 2 things to accepted answer that you can also find in TF documentation. tf.nn.softmax_cross_entropy_with_logits\\n\\nNOTE: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect. tf.nn.sparse_softmax_cross_entropy_with_logits\\n\\nNOTE: For this operation, the probability of a given label is considered exclusive. That is, soft classes are not allowed, and the labels vector must provide a single specific index for the true class for each row of logits (each minibatch entry). \\n\\nWhat should we use if the classes are not mutually exclusive.\n\nUnexpected output for tf.nn.sparse_softmax_cross_entropy_with_logits\\n\\nThe TensorFlow documentation for tf.nn.sparse_softmax_cross_entropy_with_logits explicitly declares that I should not apply softmax to the inputs of this op:\\n\\nThis op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results. However if I use cross entropy without softmax it gives me unexpected results. According to CS231n course the expected loss value is around 2.3 for CIFAR-10:\\n\\nFor example, for CIFAR-10 with a Softmax classifier we would expect the initial loss to be 2.302, because we expect a diffuse probability of 0.1 for each class (since there are 10 classes), and Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302. However without softmax I get much bigger values, for example 108.91984. What exactly am I doing wrong with sparse_softmax_cross_entropy_with_logits? The TF code is shown below. import tensorflow as tf import numpy as np from tensorflow.python import keras (_, _), (x_test, y_test) = keras.datasets.cifar10.load_data() x_test = np.reshape(x_test, [-1, 32, 32, 3]) y_test = np.reshape(y_test, (10000,)) y_test = y_test.astype(np.int32) x = tf.placeholder(dtype=tf.float32, shape=(None, 32, 32, 3)) y = tf.placeholder(dtype=tf.int32, shape=(None,)) layer = tf.layers.Conv2D(filters=16, kernel_size=3)(x) layer = tf.nn.relu(layer) layer = tf.layers.Flatten()(layer) layer = tf.layers.Dense(units=1000)(layer) layer = tf.nn.relu(layer) logits = tf.layers.Dense(units=10)(layer) # If this line is uncommented I get expected value around 2.3 # logits = tf.nn.softmax(logits) loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(loss, name=\\\'cross_entropy\\\') with tf.Session() as sess: sess.run(tf.global_variables_initializer()) res = sess.run(loss, feed_dict={x: x_test[0:256], y: y_test[0:256]}) print(""loss: "", res) # Expected output is value close to 2.3 # Real outputs are 108.91984, 72.82324, etc. The issue is not in the lines\\n\\n# If this line is uncommented I get expected value around 2.3 # logits = tf.nn.softmax(logits)\\n\\nImages in cifar10 dataset are in RGB, thus pixel values are in range [0, 256). If you divide your x_test by 255\\n\\nx_test = np.reshape(x_test, [-1, 32, 32, 3]).astype(np.float32) / 255\\n\\nthe values will be rescaled to [0,1] and tf.nn.sparse_softmax_cross_entropy_with_logits will return expected values""""""']","The question addresses an unexpected output when using `tf.nn.sparse_softmax_cross_entropy_with_logits` in TensorFlow. The user expects an initial loss value around 2.3 for the CIFAR-10 dataset, but instead receives much larger values, such as 108.91984.

The key point to understand is that `tf.nn.sparse_softmax_cross_entropy_with_logits` expects unscaled logits as input because it performs the softmax operation internally. This means you should not apply the softmax function to the logits before passing them to this function, as it will produce incorrect results.

In the provided code, the user correctly avoids applying the softmax function to the logits before passing them to `tf.nn.sparse_softmax_cross_entropy_with_logits`. However, the unexpected high loss values are likely due to the input data not being normalized. The CIFAR-10 dataset images have pixel values in the range [0, 255], and these values should be normalized to the range [0, 1] to ensure the model's logits are appropriately scaled.

To address the issue, the user should normalize the `x_test` data by dividing it by 255. This normalization step ensures that the input data is on a similar scale to what the model expects, leading to more reasonable initial loss values.

In summary, the unexpected high loss values are due to the lack of normalization of the input data. By normalizing the input data to the range [0, 1], the user can achieve the expected initial loss value around 2.3 for the CIFAR-10 dataset when using `tf.nn.sparse_softmax_cross_entropy_with_logits`.","The issue is not in the lines 
<pre><code># If this line is uncommented I get expected value around 2.3
# logits = tf.nn.softmax(logits)
</code></pre>

Images in cifar10 dataset are in RGB, thus pixel values are in range [0, 256). If you divide your <code>x_test</code> by 255 
<pre><code>x_test = np.reshape(x_test, [-1, 32, 32, 3]).astype(np.float32) / 255
</code></pre>

the values will be rescaled to [0,1] and <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> will return expected values.",0.4601873256,0.8933808812,0.7
"<p>I'm trying to use TensorFlow's <code>@tf.custom_gradient</code> functionality to assign a custom gradient to a function with multiple inputs.  I can put together a working setup for only one input, but not for two or more.</p>

<p>I've based my code on <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">TensorFlow's custom_gradient documentation</a>, which works just fine for one input, as in this example:</p>

<pre><code>import tensorflow as tf
import os

# Suppress Tensorflow startup info
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

# Custom gradient decorator on a function,
# as described in documentation
@tf.custom_gradient
def my_identity(x):

    # The custom gradient
    def grad(dy):
        return dy

    # Return the result AND the gradient
    return tf.identity(x), grad

# Make a variable, run it through the custom op
x = tf.get_variable('x', initializer=1.)
y = my_identity(x)

# Calculate loss, make an optimizer, train the variable
loss = tf.abs(y)
opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)
train = opt.minimize(loss)

# Start a TensorFlow session, initialize variables, train
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(train)
</code></pre>

<p>This example runs silently, then closes.  No issues, no errors.  The variable optimizes as expected.  However, in my application, I need to do such a calculation with multiple inputs, so something of this form:</p>

<pre><code>@tf.custom_gradient
def my_identity(x, z):

    def grad(dy):
        return dy

    return tf.identity(x*z), grad
</code></pre>

<p>Running this in place of the example (and adding another variable input to the call of <code>my_identify</code>) results in the following error output.  Best as I can tell, the last parts of the error are from the dynamic generation of the op -- the information format matches the C++ formatting required in the op establishment (though that's about all I know about it).</p>

<pre><code>Traceback (most recent call last):
  File ""testing.py"", line 27, in &lt;module&gt;
    train = opt.minimize(loss)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 400, in minimize
    grad_loss=grad_loss)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 519, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 630, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 821, in _GradientsHelper
    _VerifyGeneratedGradients(in_grads, op)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 323, in _VerifyGeneratedGradients
    ""inputs %d"" % (len(grads), op.node_def, len(op.inputs)))
ValueError: Num gradients 2 generated for op name: ""IdentityN""
op: ""IdentityN""
input: ""Identity""
input: ""x/read""
input: ""y/read""
attr {
  key: ""T""
  value {
    list {
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
    }
  }
}
attr {
  key: ""_gradient_op_type""
  value {
    s: ""CustomGradient-9""
  }
}
 do not match num inputs 3
</code></pre>

<p>Based on other custom gradient options, I surmised that the issue was a lack of supplied gradient for the second input argument.  So, I changed my function to this:</p>

<pre><code>@tf.custom_gradient
def my_identity(x, z):

    def grad(dy):
        return dy

    return tf.identity(x*z), grad, grad
</code></pre>

<p>This results in the following more familiar error:</p>

<pre><code>Traceback (most recent call last):
  File ""testing.py"", line 22, in &lt;module&gt;
    y = my_identity(x, z)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated
    return _graph_mode_decorator(f, *args, **kwargs)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 132, in _graph_mode_decorator
    result, grad_fn = f(*args)
ValueError: too many values to unpack (expected 2)
</code></pre>

<p>The <code>@custom_gradient</code> decorator is only identifying the last returned element as a gradient.  So, I tried putting the two gradients into a tuple as <code>(grad, grad)</code> such that there would only be ""two"" outputs for the function.  TensorFlow rejected this too, this time because it can't call a tuple like it would a Tensor -- entirely reasonable, in hindsight.</p>

<p>I've fussed around with the example some more, but to no avail.  No matter what I try, I can't get the custom-defined gradient to deal with multiple inputs.  I'm hoping that somebody with more knowledge than I regarding custom ops and gradients will have a better idea on this -- thanks in advance for the help!</p>
","['""""""tf.custom_gradient with multiple inputs\\n\\ntf.custom_gradient accepts only one Tensor x, what if this op needs more than one inputs? For example, to define the gradient of Softmax which needs input x and label? Thanks for the suggestion from @AllenLavoie, I use a Python list as input. def self_define_op_multiple_inputs(): @tf.custom_gradient def loss_func(input_): x = input_[0] label = input_[2] def grad(dy): return [dy, dy] return x - label, grad x = tf.range(10, dtype=tf.float32) y = tf.range(10, dtype=tf.int32) loss = loss_func([x, y]) if __name__ == \\\'__main__\\\': self_define_op_multiple_inputs()\\n\\nIt seems that it will convert the Python list to a Tensor. The snippet above will raise a TypeError: TypeError: Cannot convert a list containing a tensor of dtype <dtype: \\\'int32\\\'> to <dtype: \\\'float32\\\'> (Tensor is: <tf.Tensor \\\'range_1:0\\\' shape=(10,) dtype=int32>)\\n\\nThe documentation says x and y can both either be Tensors or sequences of Tensors. Did this not work for you?  Actually this is exactly what confused me. I don\'t understand what\'s sequences of Tensors, does it mean a Python list of Tensor? My interpretation is Python list (or tuple, etc.). So len(x) is the number of inputs to the operation, len(y) is the number of outputs. Then the gradient function takes len(y) Tensor argument and returns len(x) Tensors. I tried to use list but it seems like a list will be converted as a Tensor, which will cause an error if there are multiple inputs with different type and matched shape. The question has been updated. @AllenLavoie I created an issue on github\\n\\n I ran into a similar problem yesterday and found this post, and I believe I know what you are running into. Problem is that while using @tf.custom_gradient, the function that it decorates can have multiple inputs (instead of a list of tensors). Look at the following code(note that it\\\'s just a test code with no actual meaning):\\n\\n@tf.custom_gradient def loop1(x,a): def grad(dy): return dy*3,dy*2 n = tf.multiply(x,a) return n,grad\\n\\nBy using two inputs x and a, you have to return two gradients respectively in the grad function. dy*3 corresponds to the gradient of x and dy*2 corresponds to the gradient of a. I think in this function the documents make people very confusing, but you can still use multiple inputs, just make sure that you also have the same number of gradients, or else you will run into errors. 1\\n\\nCan we return None as gradients for unused terms ? I believe you need something like this a tf Graph input:+ n_input is the input number\\n\\nx = tf.placeholder(""float"", [None, n_input]) y = tf.placeholder(""float"", [None])\\n\\nDoes this answer your question ? """"""']","To address the question of how to assign a custom gradient to a TensorFlow operation with multiple inputs using `@tf.custom_gradient`, it is important to understand the correct way to define and return gradients for each input. The `@tf.custom_gradient` decorator allows you to define a function with custom gradients, which can be particularly useful for ensuring numerical stability or efficiency in gradient computation.

When using `@tf.custom_gradient` with multiple inputs, the decorated function should return a tuple consisting of the output and a gradient function. The gradient function must return a gradient for each input. If the function has two inputs, the gradient function should return two gradients, one for each input. The number of gradients returned by the gradient function must match the number of inputs to the decorated function.

In the context of the provided example, the issue arises because the gradient function does not return the correct number of gradients for the inputs. The correct approach is to ensure that the gradient function returns a gradient for each input. If the function has two inputs, the gradient function should return two gradients, one for each input. This ensures that TensorFlow can correctly compute the gradients for each input during backpropagation.

Additionally, it is important to note that the `@tf.custom_gradient` decorator expects the decorated function to return exactly two values: the output of the function and the gradient function. The gradient function itself should return a tuple of gradients corresponding to each input.

In summary, to assign a custom gradient to a TensorFlow operation with multiple inputs using `@tf.custom_gradient`, ensure that the gradient function returns a gradient for each input, and that the decorated function returns a tuple consisting of the output and the gradient function. This approach aligns with the TensorFlow documentation and ensures that the custom gradient is correctly applied to each input.","If we use multiple variables as input, the number of gradients return from ""grad"" function should be equals to number of input variables, though we maybe don't care about some of them.
Note that the second output of ""my_multiple"" is a function, not a gradient tensor.",0.8076078195,0.8304312779,1
"<p>So I've built a convnet using pure <code>keras</code>. It compiles and operates exactly as intended, but I need to convert it to use <code>tf.keras</code> so that I can make use of <code>tfmot</code>. Having read documentation, I attempted to convert it, only to get the following error:</p>

<p><code>The last dimension of the inputs to Dense should be defined. Found None.</code> </p>

<p>Any idea what I'm doing wrong?</p>

<p>Thanks!</p>

<p>Original <code>keras</code> model:</p>

<pre><code>input_layer = keras.layers.Input(shape=(100,))
reshape_layer = keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = keras.layers.Flatten()(conv_layer_5)
label_layer = keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = keras.layers.Dense(1, activation=""linear"")(label_layer)

model = keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>Converted <code>tf.keras</code> model:</p>

<pre><code>input_layer = tf.keras.layers.InputLayer(input_shape=(100,))
reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)
label_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>EDIT 1:</p>

<p>I thought maybe I could get around the issue by saving the <code>keras</code> model after creation and loading it as a <code>tf.keras</code> model immediately before compilation / training. That throws the same error! </p>
","['""""""Tensorflow 2.0: Shape inference with Reshape returns None dimension\\n\\nI\\\'m working with a CNN-LSTM model on Tensorflow 2.0 + Keras to perform sequence classification. My model is defined as following:\\n\\ninp = Input(input_shape) rshp = Reshape((input_shape[0]*input_shape[1], 1), input_shape=input_shape)(inp) cnn1 = Conv1D(100, 9, activation=\\\'relu\\\')(rshp) cnn2 = Conv1D(100, 9, activation=\\\'relu\\\')(cnn1) mp1 = MaxPooling1D((3,))(cnn2) cnn3 = Conv1D(50, 3, activation=\\\'relu\\\')(mp1) cnn4 = Conv1D(50, 3, activation=\\\'relu\\\')(cnn3) gap1 = AveragePooling1D((3,))(cnn4) dropout1 = Dropout(rate=dropout[0])(gap1) flt1 = Flatten()(dropout1) rshp2 = Reshape((input_shape[0], -1), input_shape=flt1.shape)(flt1) bilstm1 = Bidirectional(LSTM(240, return_sequences=True, recurrent_dropout=dropout[1]), merge_mode=merge)(rshp2) dense1 = TimeDistributed(Dense(30, activation=\\\'relu\\\'))(rshp2) dropout2 = Dropout(rate=dropout[2])(dense1) prediction = TimeDistributed(Dense(1, activation=\\\'sigmoid\\\'))(dropout2) model = Model(inp, prediction, name=""CNN-bLSTM_per_segment"") print(model.summary(line_length=75))\\n\\nWhere input_shape = (60, 60). This definition, however, raises the following error:\\n\\nTypeError: unsupported operand type(s) for +: \\\'NoneType\\\' and \\\'int\\\'\\n\\nAt first, I thought it was because the rshp2 layer could not reshape the flt1 output to shape (60, X). So I added a printing block before the Bidirectional(LSTM)) layer:\\n\\nprint(\\\'reshape1: \\\', rshp.shape) print(\\\'cnn1: \\\', cnn1.shape) print(\\\'cnn2: \\\', cnn2.shape) print(\\\'mp1: \\\', mp1.shape) print(\\\'cnn3: \\\', cnn3.shape) print(\\\'cnn4: \\\', cnn4.shape) print(\\\'gap1: \\\', gap1.shape) print(\\\'flatten 1: \\\', flt1.shape) print(\\\'reshape 2: \\\', rshp2.shape)\\n\\nAnd the shapes were:\\n\\nreshape 1: (None, 3600, 1) cnn1: (None, 3592, 100) cnn2: (None, 3584, 100) mp1: (None, 1194, 100) cnn3: (None, 1192, 50) cnn4: (None, 1190, 50) gap1: (None, 396, 50) flatten 1: (None, 19800) reshape 2: (None, 60, None)\\n\\nLooking at the flt1 layer, its output shape is (19800,), which can be reshaped as (60, 330), but for some reason the (60, -1) of the rshp2 layer is not working as intended, evidenced by the print reshape 2: (None, 60, None). When I try to reshape as (60, 330) it works just fine. Does anyone knows why the (-1) is not working? From Reshape documentation, https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape\\n\\nthe layer returns a tensor with shape (batch_size,) + target_shape\\n\\nSo, the batch size stays the same, the other dimensions are calculated based on your target_shape. From the doc, look at the last example,\\n\\n# also supports shape inference using `-1` as dimension model.add(tf.keras.layers.Reshape((-1, 2, 2))) model.output_shape\\n\\nIf you pass -1 in your target shape, the Keras will store None, this is useful if you expect variable-length data in that axis, but if your data shape is always same, just put the dimension hard-coded that will place the dimension when you print the shape later. N.B: Also no need to specify input_shape=input_shape for your intermediate layers in functional API. The model will infer that for you.\n\nCurrently, there is no direct in-built support in Tensorflow or Keras to convert the frozen model or the checkpoint file to hdf5 format. But since you have mentioned that you have the code of Tensorflow model, you will have to rewrite that model\'s code in Keras. Then, you will have to read the values of your variables from the checkpoint file and assign it to Keras model using layer.load_weights(weights) method. More than this methodology, I would suggest to you to do the training directly in Keras as it claimed that Keras optimizers are 5-10% times faster than Tensorflow\\\'s optimizers. Other way is to write your code in Tensorflow with tf.contrib.keras module and save the file directly in hdf5 format. Unsure if this is what you are looking for, but I happened to just do the same with the newly released keras support in TF 1.2. You can find more on the API here: https://www.tensorflow.org/api_docs/python/tf/contrib/keras\\n\\nTo save you a little time, I also found that I had to include keras modules as shown below with the additional python.keras appended to what is shown in the API docs. from tensorflow.contrib.keras.python.keras.models import Sequential\\n\\nHope that helps get you where you want to go. Essentially once integrated in, you then just handle your model/weight export as usual. """"""\', \'""""""What is the difference between tf.keras.layers.Input() and tf.keras.layers.Flatten()\\n\\nI have seen multiple uses of both tf.keras.layers.Flatten() (ex. here) and tf.keras.layers.Input() (ex. here). After reading the documentation, it is not clear to me\\n\\nwhether either of them uses the other\\n\\nwhether both can be used interchangeably when introducing to a model an input layer (let\'s say with dimensions (64, 64))\\n\\n I think the confusion comes from using a tf.keras.Sequential model, which does not need an explicit Input layer. Consider the following two models, which are equivalent:\\n\\nimport tensorflow as tf model1 = tf.keras.Sequential([ tf.keras.layers.Flatten(), tf.keras.layers.Dense(5, activation=\\\'relu\\\'), ]) model1.build((1, 28, 28, 1))\\n\\nmodel2 = tf.keras.Sequential([ tf.keras.layers.Input((28, 28, 1)), tf.keras.layers.Flatten(), tf.keras.layers.Dense(5, activation=\\\'relu\\\'), ])\\n\\nThe difference is that I explicitly set the input shape of model2 using an Input layer. In model1, the input shape will be inferred when you pass real data to it or call model.build. Now regarding the Flatten layer, this layer simply converts a n-dimensional tensor (for example (28, 28, 1)) into a 1D tensor (28 x 28 x 1). The Flatten layer and Input layer can coexist in a Sequential model but do not depend on each other. \\n\\nSo If I understand correctly, in the example of code I used for the tf.keras.layers.Input class, the data are not flattened, they are kept in the same shape, and the class is just used to specify their shape. In that specific example it is necessary to use Input, although the model is Sequential, right? So If I understand correctly, in the example of code I used for the tf.keras.layers.Input class, the data are not flattened, they are kept in the same shape, and the class is just used to specify their shape <--yes. , In that specific example it is necessary to use Input, although the model is Sequential, right? <-- it is optional not necessary. Ok this solves my questions!\n\n How can I convert a trained Tensorflow model to Keras? I have a trained Tensorflow model and weights vector which have been exported to protobuf and weights files respectively. How can I convert these to JSON or YAML and HDF5 files which can be used by Keras? I have the code for the Tensorflow model, so it would also be acceptable to convert the tf.Session to a keras model and save that in code. I think the callback in keras is also a solution. The ckpt file can be saved by TF with:\\n\\nsaver = tf.train.Saver() saver.save(sess, checkpoint_name)\\n\\nand to load checkpoint in Keras, you need a callback class as follow:\\n\\nclass RestoreCkptCallback(keras.callbacks.Callback): def __init__(self, pretrained_file): self.pretrained_file = pretrained_file self.sess = keras.backend.get_session() self.saver = tf.train.Saver() def on_train_begin(self, logs=None): if self.pretrian_model_path: self.saver.restore(self.sess, self.pretrian_model_path) print(\\\'load weights: OK.\\\')\\n\\nThen in your keras script:\\n\\nmodel.compile(loss=\\\'categorical_crossentropy\\\', optimizer=\\\'rmsprop\\\') restore_ckpt_callback = RestoreCkptCallback(pretrian_model_path=\\\'./XXXX.ckpt\\\') model.fit(x_train, y_train, batch_size=128, epochs=20, callbacks=[restore_ckpt_callback])\\n\\nThat will be fine. I think it is easy to implement and hope it helps.""""""\', \'""""""What is the advantage of using an InputLayer (or an Input) in a Keras model with Tensorflow tensors? A Keras model can used as a Tensorflow function on a Tensor, through the functional API, as described here. from keras.layers import InputLayer a = tf.placeholder(dtype=tf.float32, shape=(None, 784)) model = Sequential() model.add(InputLayer(input_tensor=a, input_shape=(None, 784))) model.add(Dense(32, activation=\\\'relu\\\')) model.add(Dense(10, activation=\\\'softmax\\\')) output = model.output\\n\\n<tf.Tensor \\\'dense_24/Softmax:0\\\' shape=(?, 10) dtype=float32>\\n\\nBut, this also works without any InputLayer:\\n\\na = tf.placeholder(dtype=tf.float32, shape=(None, 784)) model = Sequential() model.add(Dense(32, activation=\\\'relu\\\', input_shape=(784,))) model.add(Dense(10, activation=\\\'softmax\\\')) output = model(a)\\n\\nworks, and output has the same shape as before:\\n\\n<tf.Tensor \\\'sequential_9/dense_22/Softmax:0\\\' shape=(?, 10) dtype=float32>\\n\\nI assume the first form permits:\\n\\nto explicitely attach the inputs and outputs as attributes of the model (of the same names), so we can reuse them elsewhere. For example with other TF ops. to transform the tensors given as inputs into Keras inputs, with additional metadata (such as _keras_history as stated in the source code). But this is not something we cannot do with the second form, so, is there a special usage of the InputLayer (and Input a fortiori) (except for multiple inputs)? Moreover, the InputLayer is tricky because it\\\'s using input_shape differently from other keras layers: we specify the batch size (None here), which is not usually the case... It would seem that InputLayer has some uses:\\n\\nFirst, it allows you to give pure tensorflow tensors as is, without specifying their shape. E.g. you could have written\\n\\nmodel.add(InputLayer(input_tensor=a))\\n\\nThis is nice for several obvious reasons, among others less duplication. Second, they allow you to write non-sequential networks with a single input, e.g. input / \\\\ / \\\\ / \\\\ conv1 conv2 | |\\n\\nWithout InputLayer you would need to explicitly feed conv1 and conv2 the same tensor, or create an arbitrary identity layer on top of the model. Neither is quite pleasing.""""""']","### Description

This response addresses the issue of converting a Keras model to a `tf.keras` model, specifically focusing on the error: ""The last dimension of the inputs to Dense should be defined. Found None.""

### Explanation

The error arises because the `tf.keras.layers.InputLayer` is used incorrectly in the converted model. In the original Keras model, `keras.layers.Input` is used to define the input layer, which correctly specifies the input shape. However, in the converted `tf.keras` model, `tf.keras.layers.InputLayer` is used instead of `tf.keras.layers.Input`. This is a crucial difference.

The `tf.keras.layers.Input` function is used to instantiate a Keras tensor, which is necessary for defining the input shape explicitly. On the other hand, `tf.keras.layers.InputLayer` is typically used within a Sequential model to add an input layer, but it does not instantiate a Keras tensor in the same way.

In the original Keras model, the input layer is defined with `keras.layers.Input(shape=(100,))`, which correctly sets the input shape. This input tensor is then passed through the subsequent layers, ensuring that the dimensions are properly defined.

In the converted `tf.keras` model, replacing `tf.keras.layers.InputLayer(input_shape=(100,))` with `tf.keras.layers.Input(shape=(100,))` will resolve the issue. This change ensures that the input tensor is correctly instantiated, and the dimensions are properly defined for the subsequent layers, including the Dense layers.

Additionally, the use of `tf.keras.layers.Reshape((-1, 100, 1))` should be carefully reviewed. The `-1` in the reshape layer is used for shape inference, but it requires that the other dimensions are correctly specified. If the input shape is not properly defined, the reshape operation may not work as intended, leading to the error encountered.

In summary, the error ""The last dimension of the inputs to Dense should be defined. Found None."" is due to the incorrect use of `tf.keras.layers.InputLayer` instead of `tf.keras.layers.Input`. By using `tf.keras.layers.Input` to define the input layer, the input shape is correctly specified, and the dimensions are properly defined for the subsequent layers, resolving the issue.","There's few issues with your code. Fix them and you should be good to go,
Issue 1: Use <code>Input</code> instead of <code>InputLayer</code></h2>

The standard is to use <code>Input</code> layer instead of <code>InputLayer</code> (which infact uses <code>InputLayer</code> internally). You also need to change <code>input_shape</code> to <code>shape</code> if you are using <code>Input</code> layer.

<pre><code>input_layer = tf.keras.layers.Input(shape=(100,))
</code></pre>

Issue 2: 2 <code>None</code> dimensions in the output</h2>

When you execute the following line, you get two <code>None</code> dimensions in your output.
<pre><code>reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)
</code></pre>

Which is why you are getting the above error. When defining the <code>Reshape</code> layer you don't define the <code>batch</code> dimension, which will be None. And that's the only dimension you can have as <code>None</code> if you want to use a <code>Dense</code> layer. Otherwise, the <code>Dense</code> layer cannot infer the shape of its weights (which is why you get the error). So change that to,

<pre><code>reshape_layer = tf.keras.layers.Reshape((1, 100, 1))(input_layer)
</code></pre>",0.6613133163,0.8805473829,0.7142857143
"<p>In the Tensorflow tf.Estimator and tf.feature_column docs it is well documented, how to use feature columns together with an Estimator e.g. in order to one-hot encode the categorical features in the dataset being used.</p>

<p>However, I want to ""apply"" my feature columns directly to a tf.dataset which I create from a .csv file (with two columns: UserID, MovieID), without even defining a model or an Estimator. (Reason: I want to check what's happening exactly in my datapipeline, i.e. I'd like to be able to run a batch of samples through my the pipeline, and then see in the output how the features got encoded.)</p>

<p>This is what I have tried so far:</p>

<pre><code>column_names = ['UserID', 'MovieID']

user_col = tf.feature_column.categorical_column_with_hash_bucket(key='UserID', hash_bucket_size=1000)
movie_col = tf.feature_column.categorical_column_with_hash_bucket(key='MovieID', hash_bucket_size=1000)
feature_columns = [tf.feature_column.indicator_column(user_col), tf.feature_column.indicator_column(movie_col)]

feature_layer = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)

def process_csv(line):
  fields = tf.io.decode_csv(line, record_defaults=[tf.constant([], dtype=tf.int32)]*2, field_delim="";"")
  features = dict(zip(column_names, fields))

  return features 

ds = tf.data.TextLineDataset(csv_filepath)
ds = ds.map(process_csv, num_parallel_calls=4)
ds = ds.batch(10)
ds.map(lambda x: feature_layer(x))
</code></pre>

<p>However the last line with the map call raises the following error:</p>

<blockquote>
  <p>ValueError: Column dtype and SparseTensors dtype must be compatible.
  key: MovieID, column dtype: , tensor dtype: </p>
</blockquote>

<p>I'm not sure what this error means...
I also tried to define a tf.keras model with only the feature_layer I defined, and then run .predict() on my dataset - instead of using ds.map(lambda x: feature_layer(x)):</p>

<pre><code>model = tf.keras.Sequential([feature_layer])
model.compile()
model.predict(ds)
</code></pre>

<p>However, this results exactly in the same error as above.
Does anybody have an idea what is going wrong? Is there maybe an easier way to achieve this?</p>
","['""""""How do I add a new feature column to a tf.data.Dataset object? \\n\\nI am building an input pipeline for proprietary data using Tensorflow 2.0\\\'s data module and using the tf.data.Dataset object to store my features. Here is my issue - the data source is a CSV file that has only 3 columns, a label column and then two columns which just hold strings referring to JSON files where that data is stored. I have developed functions that access all the data I need, and am able to use Dataset\\\'s map function on the columns to get the data, but I don\'t see how I can add a new column to my tf.data.Dataset object to hold the new data. So if anyone could help with the following questions, it would really help:\\n\\nHow can a new feature be appended to a tf.data.Dataset object? Should this process be done on the entire Dataset before iterating through it, or during (I think during iteration would allow utilization of the performance boost, but I don\'t know how this functionality works)? I have all the methods for taking the input as the elements from the columns and performing everything required to get the features for each element, I just don\'t understand how to get this data into the dataset. I could do ""hacky"" workarounds, using a Pandas Dataframe as a ""mediator"" or something along those lines, but I want to keep everything within the Tensorflow Dataset and pipeline process, for both performance gains and higher quality code. I have looked through the Tensorflow 2.0 documentation for the Dataset class , but haven\'t been able to find a method that can manipulate the structure of the object. Here is the function I use to load the original dataset:\\n\\ndef load_dataset(self): # TODO: Function to get max number of available CPU threads dataset = tf.data.experimental.make_csv_dataset(self.dataset_path, self.batch_size, label_name=\\\'score\\\', shuffle_buffer_size=self.get_dataset_size(), shuffle_seed=self.seed, num_parallel_reads=1) return dataset\\n\\nThen, I have methods which allow me to take a string input (column element) and return the actual feature data. And I am able to access the elements from the Dataset using a function like "".map"". But how do I add that as a column? \\n\\nWow, this is embarassing, but I have found the solution and it\\\'s simplicity literally makes me feel like an idiot for asking this.""""""']","### Description

This response addresses the question of how to apply feature columns directly to a `tf.data.Dataset` without using a `tf.Estimator` in TensorFlow 2.0.0-rc0. The user aims to preprocess and encode features from a CSV file using feature columns and then inspect the transformed data.

### Explanation

To achieve this, you can use TensorFlow's `tf.feature_column` module to define feature columns and then apply these columns to your dataset using a `tf.keras.layers.DenseFeatures` layer. The process involves several steps:

1. **Define Feature Columns**: Create feature columns for each feature in your dataset. For example, you can use `tf.feature_column.categorical_column_with_hash_bucket` to handle categorical features and then wrap them with `tf.feature_column.indicator_column` to convert them into one-hot encoded dense representations.

2. **Create a Feature Layer**: Use `tf.keras.layers.DenseFeatures` to create a feature layer that will apply the transformations defined by your feature columns.

3. **Process CSV Data**: Define a function to parse and process each line of the CSV file. This function should decode the CSV line into a dictionary of features.

4. **Create and Transform Dataset**: Use `tf.data.TextLineDataset` to read the CSV file and apply the processing function using the `map` method. Batch the dataset and then apply the feature layer to transform the features.

### Common Issues and Solutions

- **Data Type Compatibility**: Ensure that the data types of the columns in your dataset match the expected data types of the feature columns. The error ""Column dtype and SparseTensors dtype must be compatible"" indicates a mismatch between the data types. Verify that the data types specified in the `record_defaults` of the `tf.io.decode_csv` function align with the data types expected by the feature columns.

- **Batching and Mapping**: When applying the feature layer, ensure that the dataset is batched before mapping the feature layer. This allows the feature layer to process a batch of samples at once, which is necessary for the transformations to be applied correctly.

By following these steps, you can preprocess and encode features directly within a `tf.data.Dataset` pipeline, allowing you to inspect the transformed data without defining a model or an Estimator. This approach provides a clear and efficient way to handle feature transformations within the TensorFlow data pipeline.","Just found the issue:
tf.feature_column.categorical_column_with_hash_bucket() takes an optional argument dtype, which is set to tf.dtypes.string by default. However, the datatype of my columns is numerical (tf.dtypes.int32).

This solved the issue:
<pre><code>tf.feature_column.categorical_column_with_hash_bucket(key='UserID', hash_bucket_size=1000, dtype=tf.dtypes.int32)
</code></pre>",0.778538412,0.7916836943,0
"<p>I am trying to understand why there is a difference between calculating a dense layer operation directly and using the <code>keras</code> implementation.</p>
<p>Following the documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense</a>) <code>tf.keras.layers.Dense()</code> should implement the operation <code>output = activation(dot(input, kernel) + bias)</code> but <code>result</code> and <code>result1</code> below are not the same.</p>
<pre class=""lang-py prettyprint-override""><code>tf.random.set_seed(1)

bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32)
kernel = tf.Variable(tf.random.uniform(shape=(5,10)), dtype=tf.float32)
x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32))

result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x) + bias)
tf.print(result)

test = tf.keras.layers.Dense(units = 5, 
                            activation = 'relu',
                            use_bias = True, 
                            kernel_initializer = tf.keras.initializers.Constant(value=kernel), 
                            bias_initializer = tf.keras.initializers.Constant(value=bias), 
                            dtype=tf.float32)

result1 = test(tf.transpose(x))

print()
tf.print(result1)

</code></pre>
<p>output</p>
<pre class=""lang-py prettyprint-override""><code>
[[2.87080455]
 [3.25458574]
 [3.28776264]
 [3.14319134]
 [2.04760242]]

[[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]

</code></pre>
<p>Using <code>test.get_weights()</code> I can see that the kernel and bias (<code>b</code>) are getting set to the correct values. I am using TF version 2.12.0.</p>
","['""""""What exactly does tf.keras.layers.Dense do? \\n\\n I\'m using the Keras to build a convolutional neural network. I ran across the following:\\n\\nmodel = tf.keras.Sequential() model.add(layers.Dense(10*10*256, use_bias=False, input_shape=(100,)))\\n\\nI\'m curious - what exactly mathematically is going on here? My guess is that for input of size [100,N], the network will be evaluated N times, once for each training example. The Dense layer created by layers.Dense contains (10*10*256) * (100) parameters that will be updated during backpropagation. Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). Note: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel. # as first layer in a sequential model: model = Sequential() model.add(Dense(32, input_shape=(16,))) # now the model will take as input arrays of shape (*, 16) # and output arrays of shape (*, 32) # after the first layer, you don\\\'t need to specify # the size of the input anymore: model.add(Dense(32))\\n\\n> units: Positive integer, dimensionality of the output space. > activation: Activation function to use.\n\nUnderstanding tf.keras.layers.Dense()\\n\\nI am trying to understand why there is a difference between calculating a dense layer operation directly and using the keras implementation. Following the documentation tf.keras.layers.Dense() should implement the operation output = activation(dot(input, kernel) + bias) but result and result1 below are not the same. tf.random.set_seed(1) bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32) kernel = tf.Variable(tf.random.uniform(shape=(5,10)), dtype=tf.float32) x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32)) result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x) + bias) tf.print(result) test = tf.keras.layers.Dense(units = 5, activation = \\\'relu\\\', use_bias = True, kernel_initializer = tf.keras.initializers.Constant(value=kernel), bias_initializer = tf.keras.initializers.Constant(value=bias), dtype=tf.float32) result1 = test(tf.transpose(x)) print() tf.print(result1)\\n\\n[[2.87080455] [3.25458574] [3.28776264] [3.14319134] [2.04760242]] [[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]\\n\\nUsing test.get_weights() I can see that the kernel and bias (b) are getting set to the correct values. I am using TF version 2.12.0. After some experimentation I realized that the kernel for the dense layer needs to be of shape=(10,5) as apposed to (5,10) as in the code from the original question above. This is implicit because units=5 so a vector of size 10 needs to be passed (hence why input_shape=(10,) is commented out as a reminder). Below is the corrected code:\\n\\ntf.random.set_seed(1) bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32) kernel = tf.Variable(tf.random.uniform(shape=(10,5)), dtype=tf.float32) x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32)) result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x, transpose_a=True) + bias) tf.print(result) test = tf.keras.layers.Dense(units = 5, # input_shape=(10,), activation = \\\'relu\\\', use_bias = True, kernel_initializer = tf.keras.initializers.Constant(value=kernel), bias_initializer = tf.keras.initializers.Constant(value=bias), dtype=tf.float32) result1 = test(tf.transpose(x)) print() tf.print(result1)\\n\\n[[2.38769] [3.63470697] [2.62423944] [3.31286287] [2.91121125]] [[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]\\n\\nUltimately, I am not entirely sure what was happening under the hood and why keras did not raise an error. I will check with the tf.keras.layers.Dense() implementation but any thoughts or suggestions by someone who knows the code already are highly appreciated\n\nI just needed clarification of the Linear Algebra itself and how these Layers differ between Linears. The derivative is one example in how they differ ( as they\\\'re different linear algrabra operations ). I did state a \\\'guess\\\' of what TF\\\'s Dense layer is and if it\\\'s the same. So, I might make an edit to the title if other people think it\'ll help with clarification! Thank you for the explanation of the TF\'s Dense layer though! :D\\n\\n If you follow the references in its call function, it leads you to the definition of the operation used here, which is indeed a matrix multiplication of the inputs and weights plus a bias vector as expected:\\n\\noutputs = gen_math_ops.MatMul(a=inputs, b=kernel). outputs = nn_ops.bias_add(outputs, bias)\\n\\n4\\n\\nSo, the definition of the kernel variable is a weight matrix but of the opposite dimension of PyTorch\'s weight matrix? So, let\'s say I have A input features of batch size N, and B output features. The dimensionality of kernel would be [A, B] whereas in the case of PyTorch it\\\'d be [B, A] (because there\'s a transpose applied to it?)\\n\\n\\n\\nYes they store the weights slightly differently (W.T vs W) but the result is still the same. PyTorch\'s representation is closer to the notation in text books. You can check this quickly by printing out the shape of the Linear/Dense weights in torch and tf. In line 1192 of the first link to the TF source code above, the weights in are initialised with shape=[last_dim, self.units] (N_feats, N_out) and in PyTorch (source code link), the weights are initialised with Parameter(torch.Tensor(out_features, in_features)) (N_out, N_feats)\\n\\n\\n\\nAlright, so for a linear layer of input x of shape (N_samp, N_feats) the output for TF would be matmul(x, A) + b where A is (N_feats, N_out) and b is (N_out, ) and for PyTorch it\\\'s matmul(x, A^T) + b where A is now (N_out, N_feats) and b is (N_out, ). Alright, so it seems that the 2 libraries define their Linear layers differently!""""""']","### Understanding tf.keras.layers.Dense()

The question addresses the difference between calculating a dense layer operation directly and using the `tf.keras.layers.Dense` implementation. The `Dense` layer in TensorFlow implements the operation `output = activation(dot(input, kernel) + bias)`, where `activation` is the element-wise activation function passed as the `activation` argument, `kernel` is a weights matrix created by the layer, and `bias` is a bias vector created by the layer (if `use_bias` is `True`).

In the provided example, the discrepancy arises due to the shape of the `kernel` matrix. Initially, the `kernel` is defined with a shape of `(5, 10)`, which does not align with the expected shape for the `Dense` layer. The `Dense` layer expects the `kernel` to have a shape of `(input_dim, units)`, where `input_dim` is the dimensionality of the input and `units` is the dimensionality of the output space. In this case, the correct shape for the `kernel` should be `(10, 5)` because the input has a shape of `(10, 1)` and the `Dense` layer is configured with `units=5`.

When the `kernel` is correctly reshaped to `(10, 5)`, the operation `output = activation(dot(input, kernel) + bias)` produces the expected results, matching the direct calculation. The `Dense` layer performs a matrix multiplication between the input and the `kernel`, followed by the addition of the `bias` vector, and then applies the specified activation function.

The confusion stems from the implicit requirement that the `kernel` shape must match the expected dimensions based on the input and output sizes. TensorFlow does not raise an error when the `kernel` shape is incorrect, but the results will not be as expected. Ensuring that the `kernel` and `bias` initializers are set with the correct shapes resolves the discrepancy.

In summary, the difference observed is due to the incorrect shape of the `kernel` matrix. The `Dense` layer expects the `kernel` to have a shape of `(input_dim, units)`, and when this condition is met, the results from the `Dense` layer match the direct calculation.","After some experimentation I realized that the <code>kernel</code> for the dense layer needs to be of <code>shape=(10,5)</code> as apposed to <code>(5,10)</code> as in the code from the original question above. This is implicit because <code>units=5</code> so a vector of size <code>10</code> needs to be passed (hence why <code>input_shape=(10,)</code> is commented out as a reminder). 
Ultimately, I am not entirely sure what was happening under the hood and why <code>keras</code> did not raise an error. I will check with the <code>tf.keras.layers.Dense()</code> implementation but any thoughts or suggestions by someone who knows the code already are highly appreciated!",0.6465640314,0.8720748505,0.5
"<p>How can I understand the kernel of <code>tf.keras.layers.Dense</code> for rank &gt;2?</p>
<p>The official API doc states that:</p>
<blockquote>
<p>Note: If the input to the layer has a rank greater than 2, then Dense
computes the dot product between the inputs and the kernel along the
last axis of the inputs and axis 0 of the kernel (using tf.tensordot).
For example, if input has dimensions (batch_size, d0, d1), then we
create a kernel with shape (d1, units), and the kernel operates along
axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there
are batch_size * d0 such sub-tensors). The output in this case will
have shape (batch_size, d0, units).</p>
</blockquote>
<p>My understanding is that for a rank larger than 2 (for example rank 3) only <strong>one</strong> kernel is created and thus the same kernel is applied on all slices of the second dimension, like above.
That would consequently mean that the outputs for different indices of the second dimension are <strong>not independent</strong> of each other (especially during training).</p>
<p>Is my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?</p>
","['""""""Keras Dense layer\'s input is not flattened\\n\\nThis is my test code:\\n\\nfrom keras import layers input1 = layers.Input((2,3)) output = layers.Dense(4)(input1) print(output)\\n\\n<tf.Tensor \\\'dense_2/add:0\\\' shape=(?, 2, 4) dtype=float32>\\n\\nThe documentation says:\\n\\nNote: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel. While the output is reshaped?\n\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2? \\n\\nHow can I understand the kernel of tf.keras.layers.Dense for rank >2? The official API doc states that:\\n\\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 0 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units). My understanding is that for a rank larger than 2 (for example rank 3) only one kernel is created and thus the same kernel is applied on all slices of the second dimension, like above. That would consequently mean that the outputs for different indices of the second dimension are not independent of each other (especially during training). Is my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication? Yes, your understanding is correct. To achieve what you want, you need to define a custom keras layer. Let\\\'s suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original Dense layer (link: github), except that\\n\\nIn the build function, the shape of self.kernel is (d0, i0, units) instead. You can get the value of d0 as well as i0 from input_shape. In the call function, to do the specified tensor multiplication between inputs and self.kernel, use tf.einsum with this equation: tf.einsum(\\\'abc,bcg->abg\\\', inputs, self.kernel)""""""']","### Understanding the Kernel of `tf.keras.layers.Dense` for Rank > 2

When using the `tf.keras.layers.Dense` layer with inputs that have a rank greater than 2, the layer computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 0 of the kernel. For instance, if the input has dimensions `(batch_size, d0, d1)`, a kernel with shape `(d1, units)` is created. This kernel operates along axis 2 of the input, on every sub-tensor of shape `(1, 1, d1)`, resulting in an output shape of `(batch_size, d0, units)`.

Your understanding is correct: for inputs with a rank greater than 2, only one kernel is created, and this same kernel is applied to all slices of the second dimension. Consequently, the outputs for different indices of the second dimension are not independent of each other, especially during training.

To achieve independence across different indices of the second dimension, you would need to define a custom Keras layer. This custom layer would involve creating a stack of kernels instead of a single kernel and performing the specified tensor multiplication using `tf.einsum`.

This explanation addresses the question of how the kernel of `tf.keras.layers.Dense` operates for inputs with a rank greater than 2, confirming the understanding that a single kernel is applied across all slices of the second dimension and suggesting the creation of a custom layer for achieving independence across these slices.","Yes, your understanding is correct. To achieve what you want, you need to define a custom keras layer. Let's suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original <code>Dense</code> layer, except that

1. In the <code>build</code> function, the shape of <code>self.kernel</code> is (d0, i0, units) instead. You can get the value of <code>d0</code> as well as <code>i0</code> from <code>input_shape</code>.
2. In the <code>call</code> function, to do the specified tensor multiplication between <code>inputs</code> and <code>self.kernel</code>, use <code>tf.einsum</code> with this equation: <code>tf.einsum('abc,bcg-&gt;abg', inputs, self.kernel)</code>",0.565193495,0.8761585954,1
"<p>I seem to be having a misunderstanding on how <code>tf.cond</code> works. In the tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer"">documentation</a>, it gives the following example:</p>

<pre><code>z = tf.multiply(a, b)
result = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))
</code></pre>

<p>The result of the example, if <code>x&lt;y</code> is <code>True</code> is <code>tf.add(x,z)</code> else <code>tf.square(y)</code></p>

<p>Following this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation.</p>

<p>in my example, <code>deterministic_action = 4</code>, <code>random_action = 11</code>, <code>chose_random=False</code>. The <code>stochastic_action</code> should be <code>4</code>, instead it is <code>1</code>.
Where did the value 1 come from?</p>

<pre><code>#!/usr/bin/env python3

import tensorflow as tf
import numpy as np

with tf.Graph().as_default():
    with tf.device('/cpu:0'):
        stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"")
        eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0))
        with tf.variable_scope('test_cond') as sc:
            deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4
            random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11
            chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) &lt; eps # False because eps = 0
            stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1
            #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action)


    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init, feed_dict={stochastic_ph: True})
    print (""s_ph = "", stochastic_ph)
    d_action = sess.run(deterministic_action)
    print (""det_action= "", d_action)
    r_action = sess.run(random_action)
    print (""rand_action= "", r_action)
    e = sess.run(eps)
    c_action = sess.run(chose_random)
    print (""chose_rand= "", c_action)
    s_action = sess.run(stochastic_action)
    print (""s_action= "", s_action)
    #output = sess.run(output_action)
</code></pre>

<p>here is the output:</p>

<pre><code>python random_vec.py
2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  1
</code></pre>
","['""""""`tf.case` and `tf.cond` executes all the functions within in TensorFlow\\n\\n I\'m trying to execute some condition-dependent functions where each function needs to contract tensors differently depending on their shapes, for instance. However, I realised that tf.cond and tf.case is executing all functions regardless of the condition. Prepared the following code as an example;\\n\\ndef a(): print(""a"") return tf.constant(2) def b(): print(""b"") return tf.constant(3) def c(): print(""c"") return tf.constant(4) def d(): print(""default"") return tf.constant(1) x = tf.constant(1) @tf.function def f(): return tf.case([ (tf.equal(x,1), a), (tf.equal(x,2), b), (tf.equal(x,2), c) ], default=d, exclusive=True) @tf.function def f1(): def cond3(): return tf.cond(tf.equal(x,2), c, d) def cond2(): return tf.cond(tf.equal(x,2), b, cond3) return tf.cond(tf.equal(x,1), a, cond2) print(f()) print(f1()) # Output: # a # b # c # default # tf.Tensor(2, shape=(), dtype=int32) # a # b # c # default # tf.Tensor(2, shape=(), dtype=int32)\\n\\nas you can see for both of the cases, the result is as expected but each function is executed while reaching the conclusion. Hence in my particular case, since I\'m doing different calculations depending on the tensor\'s shape, I get a multitude of errors. I\\\'ve seen many such bug reports but haven\'t found a solution. Is there another way to do conditional execution that I\'m not aware of where different functions can be executed depending on the condition? Note that I tried simply using if tf.equal(x,2): ... but in that case, I\\\'m getting an error saying that tensor output can not be used as python boolean. Note that this example is much-simplified version of my problem, my conditions are based on tensor shapes such as tf.equal(tf.size(tensor), N) so I really need a way to execute different things for different cases. After @LaplaceRicky\'s answer I realised that the code that I provided was not representative enough so I\'m providing a better example showing what I need to do;\\n\\nx = tf.ones((3,2,1)) y = tf.ones((1,2,3)) z = tf.ones((4,3,5)) k = tf.ones((3,5,5)) def a(t): def exe(): return tf.einsum(""ijk,lmi"", t, y) return exe def b(t): def exe(): return tf.einsum(""ijk,ljm"", t, z) return exe def d(t): def exe(): return tf.einsum(""ijk,klm"", t, z) return exe c = tf.constant(1) @tf.function def f(t): y = tf.case([ (tf.equal(tf.shape(t)[0], 3), a(t)), (tf.equal(tf.shape(t)[1], 3), b(t)), ], default=d, exclusive=True) return y print(f(x))\\n\\nThis function will execute properly without tf.function decorator leading to\\n\\ntf.Tensor( [[[[3. 3.]]] [[[3. 3.]]]], shape=(2, 1, 1, 2), dtype=float32\\n\\nHowever, when the decorator is included I got a ValueError which shows that all the cases are executed. TensorFlow version: 2.4.1\\n\\nPython version: 3.8.2\\n\\nShort answer: use tf.print instead of print to check whether a particular branch is really being executed in tensorflow graph mode. Explanations: print does not work and won\\\'t print in graph mode but it will print during tracing. The printed messages actually implies all of the branches were added to the tensorflow graph but it does not imply all branches will be executed all the time in graph mode. tf.print should be used instead for the debugging. def a(): tf.print(\\\'a\\\') return tf.constant(10) def b(): tf.print(\\\'b\\\') return tf.constant(11) def c(): tf.print(\\\'c\\\') return tf.constant(12) @tf.function def cond_fn(x): return tf.switch_case(x, {0:a,1:b}, default=c) print(cond_fn(tf.constant(0))) print(cond_fn(tf.constant(1))) print(cond_fn(tf.constant(2)))\\n\\na tf.Tensor(10, shape=(), dtype=int32) b tf.Tensor(11, shape=(), dtype=int32) c tf.Tensor(12, shape=(), dtype=int32)\\n\\nThe ValueError error message is because tensorflow graph does not support this kind of feature very well, at least not with tf.einsum. One way of the workarounds is to have a graph that supports variable-shaped inputs by using tf.function(f).get_concrete_function(tf.TensorSpec(shape=[None,None,None])). Besides, tf.einsum is problematic in the process and have to be replaced by tf.transpose and tf.tensordot. x = tf.random.normal((3,2,1)) y = tf.random.normal((1,2,3)) z = tf.random.normal((4,3,5)) k = tf.random.normal((3,5,5)) #for checking the values def f2(t): p = tf.case([ (tf.equal(tf.shape(t)[0], 3), lambda:tf.einsum(""ijk,lmi"", t, y)), (tf.equal(tf.shape(t)[1], 3), lambda:tf.einsum(""ijk,ljm"", t, z)), ], default=lambda:tf.einsum(""ijk,klm"", t, k), exclusive=True) return p #work around def f(t): if tf.shape(t)[0] == 3: tf.print(\\\'branch a executed\\\') return tf.tensordot(tf.transpose(t,[1,2,0]), tf.transpose(y,[2,0,1]),1) elif tf.shape(t)[1] == 3: tf.print(\\\'branch b executed\\\') return tf.tensordot(tf.transpose(t,[0,2,1]), tf.transpose(z,[1,0,2]),1) else: tf.print(\\\'branch c executed\\\') return tf.tensordot(t, k,1) graph_f=tf.function(f).get_concrete_function(tf.TensorSpec(shape=[None,None,None])) print(np.allclose(graph_f(x),f2(x))) print(np.allclose(graph_f(y),f2(y))) print(np.allclose(graph_f(z),f2(z)))\\n\\nbranch a executed True branch c executed True branch b executed True\\n\\nSorry, you are correct I didn\'t provide a proper example, thanks for your answer. I updated the code that I provided which I believe exemplifies my situation better.\n\nConfused by the behavior of `tf.cond`\\n\\nI need a conditional control flow in my graph. If pred is True, the graph should call an op that updates a variable and then returns it, otherwise it returns the variable unchanged. A simplified version is:\\n\\npred = tf.constant(True) x = tf.Variable([1]) assign_x_2 = tf.assign(x, [2]) def update_x_2(): with tf.control_dependencies([assign_x_2]): return tf.identity(x) y = tf.cond(pred, update_x_2, lambda: tf.identity(x)) with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval())\\n\\nHowever, I find that both pred=True and pred=False lead to the same result y=[2], which means the assign op is also called when update_x_2 is not selected by tf.cond. How to explain this?\n\nTf.cond giving unexpected output\\n\\nI seem to be having a misunderstanding on how tf.cond works. In the tensorflow documentation, it gives the following example:\\n\\nz = tf.multiply(a, b) result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))\\n\\nThe result of the example, if x<y is True is tf.add(x,z) else tf.square(y)\\n\\nFollowing this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation. in my example, deterministic_action = 4, random_action = 11, chose_random=False.\n\nAnd how to solve this problem? TL;DR: If you want tf.cond() to perform a side effect (like an assignment) in one of the branches, you must create the op that performs the side effect inside the function that you pass to tf.cond(). The behavior of tf.cond() is a little unintuitive. Because execution in a TensorFlow graph flows forward through the graph, all operations that you refer to in either branch must execute before the conditional is evaluated. This means that both the true and the false branches receive a control dependency on the tf.assign() op, and so y always gets set to 2, even if pred is False. The solution is to create the tf.assign() op inside the function that defines the true branch. For example, you could structure your code as follows:\\n\\npred = tf.placeholder(tf.bool, shape=[]) x = tf.Variable([1]) def update_x_2(): with tf.control_dependencies([tf.assign(x, [2])]): return tf.identity(x) y = tf.cond(pred, update_x_2, lambda: tf.identity(x)) with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval(feed_dict={pred: False})) # ==> [1] print(y.eval(feed_dict={pred: True})) # ==> [2]\\n\\n\\n\\n 4\\n\\nYeah, that\'s the one that confuses me also. My understand is that before executing tf.cond, the runtime makes sure all the dependencies are executed. Dependencies of ops in True and False branches are also dependencies of cond, so even though an op in a branch may never be executed, all of it\'s dependencies are executed, does that that sound right? Yep - the graph pruning considers all potential dependencies (of either branch) for execution, and only inhibits their execution if they were defined inside one of the branches, because the CondContext adds a control dependency on the pivot and that dependency will be a dead tensor (preventing the op from executing) if it is in the branch not taken. What was the reasoning doing it this way? Why not prune the subgraph behind the non-active branch? The pruning happens before the value for pred has been computed. This enables TensorFlow to cache a single pruned graph based on a simple key (essentially the arguments to Session.run()), and makes the implementation of conditional execution simple and lightweight. The same mechanism is used to implement tf.while_loop(), where the advantages of performing the control flow at this level are more evident. pred = tf.constant(False) x = tf.Variable([1]) def update_x_2(): assign_x_2 = tf.assign(x, [2]) with tf.control_dependencies([assign_x_2]): return tf.identity(x) y = tf.cond(pred, update_x_2, lambda: tf.identity(x)) with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval())\\n\\nThis will get the result of [1]. \n\nWhere did the value 1 come from? #!/usr/bin/env python3 import tensorflow as tf import numpy as np with tf.Graph().as_default(): with tf.device(\\\'/cpu:0\\\'): stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"") eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0)) with tf.variable_scope(\\\'test_cond\\\') as sc: deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4 random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11 chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) < eps # False because eps = 0 stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1 #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action) init = tf.global_variables_initializer() sess = tf.Session() sess.run(init, feed_dict={stochastic_ph: True}) print (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) r_action = sess.run(random_action) print (""rand_action= "", r_action) e = sess.run(eps) c_action = sess.run(chose_random) print (""chose_rand= "", c_action) s_action = sess.run(stochastic_action) print (""s_action= "", s_action) #output = sess.run(output_action)\\n\\npython random_vec.py 2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA s_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 1\\n\\n\\n\\nThis is because you are evaluating again in a new sess.run. Since you are generating a random number for deterministic_action, the result turns out to be the next random number after 4, which is 1. Here is the result of your code, when I extract the value of deterministic_action as well in the last step. print (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) r_action = sess.run(random_action) print (""rand_action= "", r_action) e = sess.run(eps) c_action = sess.run(chose_random) print (""chose_rand= "", c_action) s_action, d_action = sess.run([stochastic_action, deterministic_action]) print (""s_action= "", s_action) print (""det_action= "", d_action)\\n\\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 1 det_action= 1\\n\\nNow all you need to do is run everything in one sess.run\\n\\nd_action, r_action, e, c_action, s_action = sess.run([deterministic_action, random_action, eps, chose_random, stochastic_action]) print (""det_action= "", d_action) print (""rand_action= "", r_action) print (""chose_rand= "", c_action) print (""s_action= "", s_action)\\n\\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 rand_action= 11 chose_rand= False s_action= 4\\n\\nI was not clear on why the random_uniform generates different values when seed is set. This is because the code is running with the same session object that it initialized the variables with. Modifying the code with a new session object, this is what happens:\\n\\nprint (""s_ph = "", stochastic_ph) d_action = sess.run(deterministic_action) print (""det_action= "", d_action) sess.close() sess = tf.Session() sess.run(init, feed_dict={stochastic_ph: True}) s_action = sess.run(stochastic_action) print (""s_action= "", s_action)\\n\\ns_ph = Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0) det_action= 4 s_action= 4\\n\\n 4\\n\\nThe seed is fixed. So, irrespective of the session you run in, the random value should always be the same. @tandem the sequence of numbers you get would be the same, but not the value every time. If you notice my first result, where I\\\'m just reading out the value of deterministic_action along with stochastic_action that you are getting in your last step. Your deterministic_action is 1, which means your result is also 1. When I put all the evaluations together in a single session run, your deterministic_action is 4, and hence your result is also 4. Seed should always give the same random number. as long as it is set. That explanation doesnt still make sense\\n\\n\\n\\n@tandem tensorflow computation graphs depend on your initialization. In the above example, you are still using the session object that is initialized at the beginning. This means tensorflow is keeping a track of the state of its variables in the session object, and thus does not reinitialize the random_uniform. Close the session object with sess.close() and then open a new session object and run your stochastic_action variable, it will produce the result 4 as expected. """"""']","### Explanation of Unexpected Output in `tf.cond`

The user is experiencing unexpected behavior when using `tf.cond` in TensorFlow. Specifically, the `stochastic_action` should be `4` but is instead `1`. This discrepancy arises from the way TensorFlow handles random number generation and session runs.

In TensorFlow, the `tf.cond` function is used to return the result of `true_fn` if the predicate `pred` is true, otherwise it returns the result of `false_fn`. However, it is important to note that any Tensors or Operations created outside of `true_fn` and `false_fn` will be executed regardless of which branch is selected at runtime. This behavior is consistent with TensorFlow's dataflow model but can be surprising to users expecting lazier semantics.

In the provided example, `deterministic_action` and `random_action` are both generated using `tf.random_uniform` with fixed seeds. The `chose_random` tensor is determined to be `False` because `eps` is initialized to `0`. Therefore, `stochastic_action` should be equal to `deterministic_action`.

The unexpected output (`s_action=1` instead of `4`) occurs because the random number generation is evaluated again in a new `sess.run` call. Each call to `sess.run` generates a new random number, which is why `deterministic_action` changes from `4` to `1` in subsequent evaluations. This behavior is due to the stateful nature of random number generation in TensorFlow, where the sequence of random numbers is maintained across session runs.

To ensure consistent results, all evaluations should be performed within a single `sess.run` call. This way, the state of the random number generator is preserved, and the expected values are produced.

In summary, the issue arises from the separate evaluations of `deterministic_action` and `stochastic_action` in different `sess.run` calls, leading to different random values being generated. To avoid this, combine all evaluations into a single `sess.run` call to maintain the state of the random number generator and achieve consistent results.","This is because you are evaluating again in a new sess.run. 
Since you are generating a random number for deterministic_action, the result turns out to be the next random number after 4, which is 1.
Here is the result of your code, when I extract the value of deterministic_action as well in the last step.",0.8844104311,0.8479485822,1
"<p>I am new to tensorflow and I was trying to follow the official documentation where I came across 
tf.feature_column.categorical_column_with_vocabulary_list</p>

<p>The code I tested is: </p>

<pre><code>key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), default_value=0)
columns = [[tfc.embedding_column(colors, 3)], ...]
features = tf.io.parse_example(..., features=tfc.make_parse_example_spec(columns))
dense_tensor = tfc.input_layer(features, columns)
</code></pre>

<p>However , when I run this sample code I get the following error : 
 ValueError: All feature_columns must be _FeatureColumn instances. Given: [EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), dtype=tf.string, default_value=0, num_oov_buckets=0), dimension=3, combiner='mean', initializer=, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)]</p>

<p>What I am doing wrong?  </p>
","['""""""Tensorflow feature column for variable list of values\\n\\nFrom the TensorFlow docs it\'s clear how to use tf.feature_column.categorical_column_with_vocabulary_list to create a feature column which takes as input some string and outputs a one-hot vector. For example\\n\\nvocabulary_feature_column = tf.feature_column.categorical_column_with_vocabulary_list( key=""vocab_feature"", vocabulary_list=[\'kitchenware\', \'electronics\', \'sports\'])\\n\\nLet\\\'s say \'kitchenware\' maps to [1,0,0] and ""electronics"" maps to [0,1,0]. My question is related to having a list of strings as a feature. For example, if the feature value was [\'kitchenware\',\'electronics\'] then the desired output would be [1,1,0]. The input list length is not fixed but the output dimension is. The use case is a straight bag-of-words type model (obviously with a much larger vocabulary list!). What is the correct way to implement this? \\n\\nHere is an example how to feed data to the indicator column:\\n\\nfeatures = {\\\'letter\\\': [[\\\'A\\\',\\\'A\\\'], [\\\'C\\\',\\\'D\\\'], [\\\'E\\\',\\\'F\\\'], [\\\'G\\\',\\\'A\\\'], [\\\'X\\\',\\\'R\\\']]} letter_feature = tf.feature_column.categorical_column_with_vocabulary_list( ""letter"", [""A"", ""B"", ""C""], dtype=tf.string) indicator = tf.feature_column.indicator_column(letter_feature) tensor = tf.feature_column.input_layer(features, [indicator]) with tf.Session() as session: session.run(tf.global_variables_initializer()) session.run(tf.tables_initializer()) print(session.run([tensor]))\\n\\n[array([[2., 0., 0.], [0., 0., 1.], [0., 0., 0.], [1., 0., 0.], [0., 0., 0.]], dtype=float32)]\\n\\n 3\\n\\nin above example the features is passed as dict. How do I get the same results when I have a column in csv file which is space separated and I need to multi-hot encode using the example above ? Can we use Embedding column here? In case we have large number of values in the column (a very common case), we may end up with a sparse column if we use indicator column. you should use tf.feature_column.indicator_column.\\n\\n Could you give an example of what the structure of the training data should look like in this case. The doc you post to show what the input data inso converted into but not what you feed it.""""""']","### Explanation

The question addresses an error encountered when using TensorFlow's `tf.feature_column.categorical_column_with_vocabulary_list` in conjunction with embedding columns. The error message ""All feature_columns must be _FeatureColumn instances"" indicates that the provided feature columns are not recognized as valid `_FeatureColumn` instances by TensorFlow.

The issue arises from the way the `columns` list is constructed. In the provided code, the `columns` list is nested within another list, which is incorrect. The `columns` list should directly contain the feature columns without any additional nesting.

To resolve this issue, ensure that the `columns` list is correctly structured by directly including the feature columns. This will allow TensorFlow to properly recognize and process the feature columns, avoiding the `_FeatureColumn` instance error.

Additionally, it is important to note that `tf.feature_column.categorical_column_with_vocabulary_list` is deprecated. The recommended approach is to use Keras preprocessing layers or the `tf.keras.utils.FeatureSpace` utility for feature preprocessing when training a Keras model. This modern approach provides a more streamlined and efficient way to handle feature columns.

In summary, the error is due to incorrect nesting of the `columns` list. Correcting the structure of the `columns` list and considering the use of Keras preprocessing layers or `tf.keras.utils.FeatureSpace` will resolve the issue and align with the latest TensorFlow practices.",<code>make_parse_example_spec</code> expects <code>FeatureColumn instances</code>. You can create the FeatureColumn instance using the below method for the category list.,0.8686169759,0.7744679035,0
"<p>I have read in answers to <a href=""https://stackoverflow.com/questions/36498127/how-to-apply-gradient-clipping-in-tensorflow"">this question here</a> that tf.clip_by_global_norm() handles None values by simply ignoring them (comment by danijar in comments to the answer by @danijar) but when i try to apply it i seem to be doing something wrong as it throws </p>

<p>ValueError: None values not supported.</p>

<pre><code>tf.reset_default_graph()
z = tf.get_variable(name = 'z', shape = [1])
b = tf.get_variable('b', [1])
c = b*b - 2*b + 1
optimizer = tf.train.AdamOptimizer(0.1)
gradients, variables = zip(*optimizer.compute_gradients(c))
gradients = tf.clip_by_global_norm(gradients, 2.5)
train_op = optimizer.apply_gradients(zip(gradients, variables))
</code></pre>

<p>Can somebody please tell me what am i doing wrong or if tf.clip_by_global_norm() does not handle None gradients and i have to take care of them manually</p>

<p>The official documentation seems to agree with @danijar's comments. see <a href=""https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/clip_by_global_norm"" rel=""nofollow noreferrer"">here</a></p>

<blockquote>
  <p>Any of the entries of t_list that are of type None are ignored.</p>
</blockquote>
","['""""""How to handle None in tf.clip_by_global_norm? I have read in answers to this question here that tf.clip_by_global_norm() handles None values by simply ignoring them (comment by danijar in comments to the answer by @danijar) but when i try to apply it i seem to be doing something wrong as it throws\\n\\nValueError: None values not supported. tf.reset_default_graph() z = tf.get_variable(name = \\\'z\\\', shape = [1]) b = tf.get_variable(\\\'b\\\', [1]) c = b*b - 2*b + 1 optimizer = tf.train.AdamOptimizer(0.1) gradients, variables = zip(*optimizer.compute_gradients(c)) gradients = tf.clip_by_global_norm(gradients, 2.5) train_op = optimizer.apply_gradients(zip(gradients, variables))\\n\\nCan somebody please tell me what am i doing wrong or if tf.clip_by_global_norm() does not handle None gradients and i have to take care of them manually\\n\\nThe official documentation seems to agree with @danijar\\\'s comments. see here\\n\\nAny of the entries of t_list that are of type None are ignored.""""""']","### Handling `None` Values in `tf.clip_by_global_norm`

The question addresses an issue encountered when using `tf.clip_by_global_norm` in TensorFlow, specifically regarding the handling of `None` values within the list of tensors to be clipped. The user reports receiving a `ValueError: None values not supported` despite the official documentation stating that `None` values are ignored.

According to the TensorFlow API documentation, `tf.clip_by_global_norm` is designed to clip the values of multiple tensors by the ratio of the sum of their norms. The function accepts a tuple or list of tensors (`t_list`), a clipping ratio (`clip_norm`), and optionally, a precomputed global norm (`use_norm`). The documentation explicitly states that any entries in `t_list` that are of type `None` are ignored during the clipping process.

However, the user's experience suggests that `None` values are not being ignored as expected, resulting in a `ValueError`. This discrepancy indicates that there might be an issue with how the `None` values are being handled in the specific context of the user's code.

To address this, it is important to ensure that the list of gradients passed to `tf.clip_by_global_norm` does not contain `None` values. This can be achieved by manually filtering out `None` values from the list of gradients before passing it to the function. This step ensures that only valid tensors are included in the clipping operation, thereby preventing the `ValueError`.

In summary, while `tf.clip_by_global_norm` is designed to ignore `None` values, practical implementation may require manual handling of `None` values to avoid errors. This involves filtering out `None` values from the list of tensors before invoking the function. This approach aligns with the official documentation and ensures smooth execution of the gradient clipping process.","There's a small problem in your code: you're assigning the return value of <code>tf.clip_by_global_norm</code> to a single variable, when this function returns a pair of values.
The documentation says:
<blockquote>
  Returns:
  list_clipped: A list of Tensors of the same type as list_t.
  global_norm: A 0-D (scalar) Tensor representing the global norm.
</blockquote>

Hence, the problem arises when you try to apply the gradients to the variables, in the next line.
You can easily fix your code ignoring the global_norm returned value.

<pre><code>gradients, _ = tf.clip_by_global_norm(gradients, 2.5)
</code></pre>",0.8356610249,0.8163283102,0.5714285714
"<p>In the documentation of tf.nn.conv1d, it is stated that</p>

<blockquote>
  <p>Internally, this op reshapes the input tensors and invokes tf.nn.conv2d. For example, if data_format does not start with ""NC"", a tensor of shape [batch, in_width, in_channels] is reshaped to [batch, 1, in_width, in_channels], and the filter is reshaped to [1, filter_width, in_channels, out_channels]. The result is then reshaped back to [batch, out_width, out_channels] (where out_width is a function of the stride and padding as in conv2d) and returned to the caller.</p>
</blockquote>

<p>I get that the operations are equivalent, but I am a bit confused about the implications of this implementation detail. </p>

<p>Does the reshaping create some computational overhead? 
The 3D convolution has its own implementation, so why not the 1D convolution?</p>

<p>Thanks for any explanation that helps me and others to understand this implementation detail of TensorFlow!</p>
","['"""""" What does tf.nn.conv2d do in tensorflow? \\n\\nI was looking at the docs of tensorflow about tf.nn.conv2d here. But I can\'t understand what it does or what it is trying to achieve. It says on the docs,\\n\\n#1 : Flattens the filter to a 2-D matrix with shape\\n\\n[filter_height * filter_width * in_channels, output_channels]. Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below :\\n\\n# 2: Extracts image patches from the the input tensor to form a virtual tensor of shape\\n\\n[batch, out_height, out_width, filter_height * filter_width * in_channels]. # 3: For each patch, right-multiplies the filter matrix and the image patch vector. It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this. I\\\'ve tried coding a small portion and printing out the shape of the operation. Still, I can\\\'t understand. I tried something like this:\\n\\nop = tf.shape(tf.nn.conv2d(tf.random_normal([1,10,10,10]), tf.random_normal([2,10,10,10]), strides=[1, 2, 2, 1], padding=\\\'SAME\\\')) with tf.Session() as sess: result = sess.run(op) print(result)\\n\\nI understand bits and pieces of convolutional neural networks. I studied them here. But the implementation on tensorflow is not what I expected. So it raised the question. EDIT: So, I implemented a much simpler code. But I can\\\'t figure out what\\\'s going on. I mean how the results are like this.""""""']","### Explanation of Why TensorFlow Uses 2D Convolutions for 1D Convolution Operations

In TensorFlow, the `tf.nn.conv1d` function is designed to perform 1D convolution operations on 3D input tensors. However, internally, TensorFlow reshapes the input tensors and invokes the `tf.nn.conv2d` function to perform the equivalent convolution operation. This implementation detail might seem confusing, but it is essential to understand the rationale behind it.

When `tf.nn.conv1d` is called, the input tensor of shape `[batch, in_width, in_channels]` (assuming the default data format `""NWC""`) is reshaped to `[batch, 1, in_width, in_channels]`. Similarly, the filter tensor of shape `[filter_width, in_channels, out_channels]` is reshaped to `[1, filter_width, in_channels, out_channels]`. This reshaping allows the 1D convolution to be treated as a special case of the 2D convolution, where one of the spatial dimensions has a size of 1. After the 2D convolution operation is performed, the result is reshaped back to `[batch, out_width, out_channels]`.

The primary reason for this approach is to leverage the existing optimized implementation of 2D convolutions in TensorFlow. By reusing the `tf.nn.conv2d` function, TensorFlow can avoid duplicating code and ensure that the 1D convolution benefits from the same performance optimizations and hardware acceleration as the 2D convolution.

While this reshaping process might introduce some computational overhead, it is generally minimal compared to the overall cost of the convolution operation. The benefits of code reuse and optimization typically outweigh the minor overhead introduced by reshaping the tensors.

In summary, TensorFlow uses 2D convolutions for 1D convolution operations to take advantage of the optimized implementation of `tf.nn.conv2d`. This approach ensures efficient computation while maintaining the flexibility to handle 1D convolution tasks.","Digging through the source code, I conclude that it's likely done for convenience and minimalism of implementation - details below.

First, there is no ""reshaping"", only expanding, squeezing, and re-ordering dims, which bears a tiny overhead; no array elements are actually being moved in memory - only the tensor object's indexing specifiers are changed.
Second, all <code>conv</code> ultimately route to <code>tf.nn_ops.convolution_internal</code>, which then routes to either <code>gen_nn_ops.conv2d</code> or <code>gen_nn_ops.conv3d</code>; a <code>conv1d</code> does not exist in <code>gen_nn_ops.py</code>. Note that for some reason, you won't find that file in the Git respository - but it should be in your local install, <code>/python/ops/gen_nn_ops.py</code>.

Lastly, to get a real answer on why there isn't a dedicated <code>conv1d</code> implementation, you'll need to ask the cuDNN developers behind the convolution algorithms found in <code>gen_nn_ops.py</code>; it's possible that they found no performance improvements, and that <code>conv2d</code> works just as fast. From a low-level standpoint, this makes sense, as the number of matrix multiplications in sliding a kernel with <code>N x 1</code> elements along an <code>M x 1</code> input is identical to that of <code>N</code> along <code>M</code> - again, the only difference is in indexing.

Unfortunately devs decided to encapsulate the ultimate call, that is to <code>_pywrap_tensorflow_internal.TFE_Py_FastPathExecute</code>; the module consists of a <code>.lib</code> and a <code>.pyd</code> file - basically, compiled C (Cython) code that requires disassembly for introspection.

(1) the ""reshaping"" has a trivial overhead; (2) lack of a dedicated <code>conv1d</code> implementation is likely per sparing redundancy as <code>conv2d</code> is just as fast; (3) I'm not a cuDNN expert, so if you need to be sure, better ask over at cuDNN, or read their SDK Documentation. Alternatively, a dev at TF Github may help. I haven't seen cuDNN devs answer on SO for years now, so posting here may not be the best bet.",0.6930531802,0.8697736965,0
"<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/manip/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd documentation</a> and run the example code for 1D and 3D tensors... and now I'm trying to do it for a 2D tensor.  I want to 'interleave' the columns of two tensors.  For 1D tensors, one can do this via</p>

<pre><code>'''
We want to interleave elements of 1D tensors arr1 and arr2, where
arr1 = [10, 11, 12]
arr2 = [1, 2, 3, 4, 5, 6]
such that
desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12]
'''

import tensorflow as tf

with tf.Session() as sess:

    updates1 = tf.constant([1,2,3,4,5,6])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)

    updates2 = tf.constant([10,11,12])
    indices2 = tf.constant([[2], [5], [8]])
    scatter2 = tf.scatter_nd(indices2, updates2, shape)

    result = scatter1 + scatter2

    print(sess.run(result))
</code></pre>

<p>(aside: is there a <em>better</em> way to do this?  I'm all ears.)</p>

<p>This gives the output</p>

<p><code>[ 1  2 10  3  4 11  5  6 12]</code></p>

<p>Yay! that worked!</p>

<p>Now lets' try to extend this to 2D.</p>

<pre><code>    '''
    We want to interleave the *columns* (not rows; rows would be easy!) of

    arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]
    arr2 = [[10 11 12], [10 11 12], [10 11 12]]
    such that
    desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]]
    '''

    updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([3, 9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)
</code></pre>

<p>This gives the error
<code>ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1
dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but
are 6 and 3. Shapes are [6] and [3]. for 'ScatterNd_2' (op: 'ScatterNd') with
input shapes: [6,1], [3,6], [2].</code></p>

<p>Seems like my <code>indices</code> is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean
I need to <em>explicitly</em> specify every single pair of indices for every element in <code>updates1</code>?
Or is there some kind of 'wildcard' specification I can use for the rows? (Note <code>indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]])</code> gives syntax errors, as it probably should.)</p>

<p>Would it be easier to just do a transpose, interleave the rows, then transpose back?
Because I tried that...</p>

<pre><code>scatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))
print(sess.run(tf.transpose(scatter1)))
</code></pre>

<p>...and got a <em>much</em> longer error message, that I don't feel like posting unless someone requests it. </p>

<p>PS- I searched to make sure this isn't a duplicate -- I find it hard to imagine that someone else hasn't asked this before -- but turned up nothing. </p>
","['""""""tensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)? \\n\\n I\'ve read the tf.scatter_nd documentation and run the example code for 1D and 3D tensors and now I\\\'m trying to do it for a 2D tensor. I want to \'interleave\' the columns of two tensors. For 1D tensors, one can do this via\\n\\n\\\'\\\'\\\' We want to interleave elements of 1D tensors arr1 and arr2, where arr1 = [10, 11, 12] arr2 = [1, 2, 3, 4, 5, 6] such that desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12] \\\'\\\'\\\' import tensorflow as tf with tf.Session() as sess: updates1 = tf.constant([1,2,3,4,5,6]) indices1 = tf.constant([[0], [1], [3], [4], [6], [7]]) shape = tf.constant([9]) scatter1 = tf.scatter_nd(indices1, updates1, shape) updates2 = tf.constant([10,11,12]) indices2 = tf.constant([[2], [5], [8]]) scatter2 = tf.scatter_nd(indices2, updates2, shape) result = scatter1 + scatter2 print(sess.run(result))\\n\\n(aside: is there a better way to do this? I\\\'m all ears.)\\n\\nThis gives the output\\n\\n[ 1 2 10 3 4 11 5 6 12]\\n\\nNow lets\\\' try to extend this to 2D. \\\'\\\'\\\' We want to interleave the *columns* (not rows; rows would be easy!) of arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]] arr2 = [[10 11 12], [10 11 12], [10 11 12]] such that desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]] \\\'\\\'\\\' updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]) indices1 = tf.constant([[0], [1], [3], [4], [6], [7]]) shape = tf.constant([3, 9]) scatter1 = tf.scatter_nd(indices1, updates1, shape)\\n\\nThis gives the error ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1 dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but are 6 and 3. Shapes are [6] and [3]. for \\\'ScatterNd_2\\\' (op: \\\'ScatterNd\\\') with input shapes: [6,1], [3,6], [2]. Seems like my indices is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean I need to explicitly specify every single pair of indices for every element in updates1? Or is there some kind of \\\'wildcard\\\' specification I can use for the rows? (Note indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]]) gives syntax errors, as it probably should.)\\n\\nWould it be easier to just do a transpose, interleave the rows, then transpose back? Because I tried that... scatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape)) print(sess.run(tf.transpose(scatter1)))\\n\\n...and got a much longer error message, that I don\\\'t feel like posting unless someone requests it. PS- I searched to make sure this isn\'t a duplicate -- I find it hard to imagine that someone else hasn\'t asked this before -- but turned up nothing. \\n\\nOk, the following lines actually work, but I have no idea why.. shape = tf.constant([9, 3]), scatter1 = tf.transpose(tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))) ...Particularly strange that I have to define shape as [9,3] and take its transpose, whereas just defining it as [3,9] and using it that way gives an error. I would have thought I need to use a shape of [9,3] if I\\\'m using the traspose, or else define [3,9] and then take its transpose. ? ...So, while I now have \\\'working code\\\', I\\\'d rather not \\\'answer my own question\\\': If you can either explain why this is necessary, or offer a better way to do what I want to do, then the prize is yours! \\n\\nThis is pure slicing but I didn\'t know that syntax like arr1[0:,:][:,:2] actually works. It seems it does but not sure if it is better. This may be the wildcard slicing mechanism you are looking for. arr1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,7],[1,2,3,4,5,8]]) arr2 = tf.constant([[10, 11, 12], [10, 11, 12], [10, 11, 12]]) with tf.Session() as sess : sess.run( tf.global_variables_initializer() ) print(sess.run(tf.concat([arr1[0:,:][:,:2], arr2[0:,:] [:,:1], arr1[0:,:][:,2:4],arr2[0:, :][:, 1:2], arr1[0:,:][:,4:6],arr2[0:, :][:, 2:3]],axis=1)))\\n\\n[[ 1 2 10 3 4 11 5 6 12] [ 1 2 10 3 4 11 5 7 12] [ 1 2 10 3 4 11 5 8 12]]\\n\\n[[1 2 3 4 5 6] [1 2 3 4 5 7] [1 2 3 4 5 8]]\\n\\nand arr1[0:,:][:,:2] returns the first two columns\\n\\n I was using scatter_nd instead of concatenate because I need the solution to scale up to hundreds of columns, which I can\'t count on being able to specify ""by hand"". Still, if there\'s a way to make this ""scale"", i.e. programatically specifying the columns (without a hundred concat operations which would be slow), then this answer wins. I also hit upon a different (non-scatter_nd) answer using permutation matrices, which I\\\'ll post in a bit... Some moderators might have regarded my question as a duplicate of this one, not because the questions are the same, but only because the answers contain parts one can use to answer this question -- i.e. specifying every index combination by hand. A totally different method would be to multiply by a permutation matrix as shown in the last answer to this question. Since my original question was about scatter_nd, I\\\'m going to post this solution but wait to see what other answers come in... (Alternatively, I or someone could edit the question to make it about reordering columns, not specific to scatter_nd --EDIT: I have just edited the question title to reflect this). Here, we concatenate the two different arrays/tensors... import numpy as np import tensorflow as tf sess = tf.Session() # the ultimate application is for merging variables which should be in groups, # e.g. in this example, [1,2,10] is a group of 3, and there are 3 groups of 3 n_groups = 3 vars_per_group = 3 # once the single value from arr2 (below) is included arr1 = 10+tf.range(n_groups, dtype=float) arr1 = tf.stack((arr1,arr1,arr1),0) arr2 = 1+tf.range(n_groups * (vars_per_group-1), dtype=float) arr2 = tf.stack((arr2,arr2,arr2),0) catted = tf.concat((arr1,arr2),1) # concatenate the two arrays together print(""arr1 = \\\\n"",sess.run(arr1)) print(""arr2 = \\\\n"",sess.run(arr2)) print(""catted = \\\\n"") """"""']","To interleave columns of two 2D tensors using TensorFlow, the `tf.scatter_nd` function can be employed, but it requires careful handling of indices and shapes. The goal is to interleave columns of two tensors, similar to how it is done for 1D tensors, but extended to 2D.

For 1D tensors, the interleaving can be achieved by specifying the indices where elements from each tensor should be placed and then using `tf.scatter_nd` to create the final interleaved tensor. This approach works well for 1D tensors, as demonstrated in the provided example where elements from two 1D tensors are interleaved to form a new tensor.

When extending this to 2D tensors, the challenge lies in specifying the correct indices for the columns rather than rows. The error encountered in the provided example arises because the indices are interpreted as row indices, leading to a mismatch in dimensions. To interleave columns, one must explicitly specify the indices for each element in the updates tensor, which can be cumbersome and error-prone.

An alternative approach involves transposing the tensors, interleaving the rows (which is straightforward), and then transposing back to achieve the desired column interleaving. This method, however, also encountered issues due to shape mismatches and the need for correct handling of transpositions.

A more scalable solution involves using slicing and concatenation. By slicing the tensors into the desired column groups and concatenating them along the appropriate axis, one can achieve the interleaving without manually specifying each index. This method is more efficient and easier to scale for tensors with a large number of columns.

In summary, while `tf.scatter_nd` can be used for interleaving columns of 2D tensors, it requires careful handling of indices and shapes. An alternative and more scalable approach is to use slicing and concatenation to achieve the desired interleaving. This method avoids the complexity of manually specifying indices and is more efficient for tensors with many columns.","This is pure slicing but I didn't know that syntax like <code>arr1[0:,:][:,:2]</code> actually works. It seems it does but not sure if it is better. This may be the <em>wildcard</em> slicing mechanism you are looking for.",0.7080769916,0.7452711212,1
"<p>I am using <code>tf.keras.utils.image_dataset_from_directory</code> to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the <code>validation_split</code> parameter), I want to split it into training, testing, and validation subsets.</p>
<p>I have tried using <code>dataset.skip()</code> and <code>dataset.take()</code> to further split one of the resulting subsets, but these functions return a <code>SkipDataset</code> and a <code>TakeDataset</code> respectively (by the way, contrary to <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#take"" rel=""nofollow noreferrer"">the documentation</a>, where it is claimed that these functions return a <code>Dataset</code>). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history.</p>
<p>So, my question is: is there a way to split a <code>Dataset</code> into three subsets for training, validation and testing, so that all three subsets are also <code>Dataset</code> objects?</p>
<p><strong>Code used to load the data</strong></p>
<pre><code>def load_data_tf(data_path: str, img_shape=(256,256), batch_size: int=8):
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.2,
        subset=&quot;training&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.3,
        subset=&quot;validation&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    return train_ds, val_ds

train_dataset, test_val_ds = load_data_tf('data_folder', img_shape = (256,256), batch_size=8)
test_dataset = test_val_ds.take(686)
val_dataset = test_val_ds.skip(686)
</code></pre>
<p><strong>Model compilation and fitting</strong></p>
<pre><code>model.compile(optimizer='sgd',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])
history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, verbose=1)
</code></pre>
<p><strong>When using a normal <code>Dataset</code>, <code>val_accuracy</code> and <code>val_loss</code> are present in the history of the model:</strong></p>
<p><a href=""https://i.stack.imgur.com/Qn1Yf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qn1Yf.png"" alt=""Expected behaviour: when using a Dataset, validation metrics are calculated"" /></a></p>
<p><strong>But when using a <code>SkipDataset</code>, they are not:</strong></p>
<p><a href=""https://i.stack.imgur.com/GMnBM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GMnBM.png"" alt=""Using the SkipDataset produced by test_val_ds.take() leads to validation metrics disappearing from model history"" /></a></p>
<p><a href=""https://i.stack.imgur.com/omU5U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/omU5U.png"" alt=""val_accuracy and val_loss are not present in history keys when using a SkipDataset or a TakeDataset"" /></a></p>
","['""""""How to use tf.keras.utils.image_dataset_from_directory to load test dataset? \\n\\nI am using tf.keras.utils.image_dataset_from_directory in my binary classification Mobilenet V2 model to split the dataset by defining training and validation subsets as following:\\n\\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(directory, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, validation_split=0.2, subset=\'training\', seed=42) validation_dataset = tf.keras.utils.image_dataset_from_directory(directory, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE, validation_split=0.2, subset=\'validation\', seed=42)\\n\\nNow, I want to use model.predict() on a set of images to look at the predictions. How can I use image_dataset_from_directory considering that there won\\\'t be two different folders containing the respective classes but only one folder for which I want the predictions? In addition, what should be the parameters of the image_dataset_from_directory function now? You need to have a separate directory containing test images. Then do the same thing you did for train/val datasets but with shuffle=False and without validation_split. As mentioned by @Djinn, You can do it in the same way and no need to define validation_split while accessing the folder. For example, Suppose I have a binary_data folder inside the dogs_cats/binary_data folder where I have stored multiple class images (5-5 images for each cats and dogs), then you can give the path till dogs_cats. This will automatically fetch all images inside the binary_data folder by stating class 1 where you can have multiple class images(binary - as per model) stored. After training the model, you can pass this dataset in model.predict() and can check the predictions for each image. Please check the below code:\\n\\ntest_dataset = tf.keras.utils.image_dataset_from_directory( ""/content/GoogleDrive/My Drive/MY WORK/dataset/dogs_cats/"", shuffle=True, #or False batch_size=BATCH_SIZE, image_size=IMG_SIZE)\\n\\nFound 10 files belonging to 1 classes. and the predictions part:\\n\\npredictions = model.predict(test_dataset) predictions = tf.where(predictions < 0.5,0, 1) print(\\\'Predictions:\\\\n\\\', predictions.numpy())\\n\\nPredictions: [[0] [1] [0] [0] [1] [1] [1] [1] [1] [1]]\\n\\nNote: Prediction\\\'s accuracy may depend on the model performance. \n\nI hope it better answers your question. use Keras - model.fit(dataset,.., validation.split=0.7, ...) see its all possible arguments\\n\\n Assuming you have all_dataset variable of tf.data.Dataset type:\\n\\ntest_dataset = all_dataset.take(1000) train_dataset = all_dataset.skip(1000)\\n\\nTest dataset now has first 1000 elements and the rest goes for training. 1,3\\n\\nAs also mentioned in ted\'s answer, adding all_dataset.shuffle() allows for a shuffled split. Possibly add as code comment in answer like so? # all_dataset = all_dataset.shuffle() # in case you want a shuffled split\\n\\n\\n\\nTensorFlow 2.10.0 will have a utility function for splitting, see my answer: stackoverflow.com/a/73591823/1389680\\n\\n\\n\\ntake and skip return TfTakeDatasets/SkipDatasets which have less functionality than TfDatasets. Does anyone know how to map those to tfDatasets or split into train test splits and get back TfDataset objects? You may use Dataset.take() and Dataset.skip():\\n\\ntrain_size = int(0.7 * DATASET_SIZE) val_size = int(0.15 * DATASET_SIZE) test_size = int(0.15 * DATASET_SIZE) full_dataset = tf.data.TFRecordDataset(FLAGS.input_file) full_dataset = full_dataset.shuffle() train_dataset = full_dataset.take(train_size) test_dataset = full_dataset.skip(train_size) val_dataset = test_dataset.skip(val_size) test_dataset = test_dataset.take(test_size)\\n\\nFor more generality, I gave an example using a 70/15/15 train/val/test split but if you don\'t need a test or a val set, just ignore the last 2 lines. Creates a Dataset with at most count elements from this dataset. Creates a Dataset that skips count elements from this dataset. You may also want to look into Dataset.shard():\\n\\nCreates a Dataset that includes only 1/num_shards of this dataset. Disclaimer I stumbled upon this question after answering this one so I thought I\\\'d spread the love\\n\\n 7\\n\\nThank you very much @ted! Is there a way to divide the dataset in a stratified way? Or, alternatively, how can we have an idea of the class proportions (suppose a binary problem) after the train/val/test split? Thanks a lot in advance! Have a look at this blogpost I wrote; eventhough it\\\'s for multilabel datasets, should be easily usable for single label, multiclass datasets -> vict0rs.ch/2018/06/17/multilabel-text-classification-tensorflow\\n\\n\\n\\nThis causes my train,validation and test datasets to have overlap between them. Is this supposed to happen and not a big deal? I would assume it\\\'s not a good idea to have the model train on validation and test data. @c_student I had the same problem and I figured out what I was missing: when you shuffle use the option reshuffle_each_iteration=False otherwise elements could be repeated in train, test and val\\n\\n\\n\\nThis is very true @xdola, and in particular when using list_files you should use shuffle=False and then shuffle with the .shuffle with reshuffle_each_iteration=False. \\n\\nMost of the answers here use take() and skip(), which requires knowing the size of your dataset before hand. This isn\\\'t always possible, or is difficult/intensive to ascertain. Instead what you can do is to essentially slice the dataset up so that 1 every N records becomes a validation record. To accomplish this, lets start with a simple dataset of 0-9:\\n\\ndataset = tf.data.Dataset.range(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\n\\nNow for our example, we\\\'re going to slice it so that we have a 3/1 train/validation split. Meaning 3 records will go to training, then 1 record to validation, then repeat. split = 3 dataset_train = dataset.window(split, split + 1).flat_map(lambda ds: ds) # [0, 1, 2, 4, 5, 6, 8, 9] dataset_validation = dataset.skip(split).window(1, split + 1).flat_map(lambda ds: ds) # [3, 7]\\n\\nSo the first dataset.window(split, split + 1) says to grab split number (3) of elements, then advance split + 1 elements, and repeat. That + 1 effectively skips the 1 element we\\\'re going to use in our validation dataset. The flat_map(lambda ds: ds) is because window() returns the results in batches, which we don\\\'t want. So we flatten it back out. Then for the validation data we first skip(split), which skips over the first split number (3) of elements that were grabbed in the first training window, so we start our iteration on the 4th element. The window(1, split + 1) then grabs 1 element, advances split + 1 (4), and repeats. Note on nested datasets: The above example works well for simple datasets, but flat_map() will generate an error if the dataset is nested. To address this, you can swap out the flat_map() with a more complicated version that can handle both simple and nested datasets:\\n\\n.flat_map(lambda *ds: ds[0] if len(ds) == 1 else tf.data.Dataset.zip(ds))\\n\\n 7\\n\\nDoesn\\\'t window just use skip under the hood? How does is the disadvantage The other disadvantage is that with skip() it has to read, and then discard, all the skipped records, which if your data source is slow means you might have a large spool-up time before results are emitted. adressed? If you have a dataset of 1000 records, and you want a 10% for validation, you would have to skip the first 900 records before a single validation record is emitted. With this solution, it only has to skip 9 records. It does end up skipping the same amount overall, but if you use dataset.prefetch(), it can read in the background while doing other things. The difference is just saving the initial spool-up time. Thinking about it a bit more, and I removed the statement.""""""']","To address the question of splitting a TensorFlow dataset into training, validation, and test subsets when using `tf.keras.utils.image_dataset_from_directory`, it is indeed possible to achieve this by leveraging the `skip` and `take` methods from the `tf.data.Dataset` API. The `image_dataset_from_directory` function allows splitting the dataset into two subsets using the `validation_split` parameter, but to further split one of these subsets into additional subsets, you can use the `skip` and `take` methods.

The `skip` method creates a dataset that skips a specified number of elements from the original dataset, while the `take` method creates a dataset that takes a specified number of elements from the original dataset. Although these methods return `SkipDataset` and `TakeDataset` objects respectively, they are still subclasses of `tf.data.Dataset` and can be used in the same way as any other dataset.

To split the dataset into three subsets, you can first use `image_dataset_from_directory` to create the initial training and combined validation/test datasets. Then, apply the `take` method to extract the test dataset from the combined validation/test dataset, and use the `skip` method to obtain the remaining validation dataset. This approach ensures that all three subsets are `tf.data.Dataset` objects, which can be used for model training, validation, and testing.

When fitting the model, ensure that the validation dataset is correctly passed to the `validation_data` parameter in the `model.fit` method. This will allow the model to calculate and report validation metrics such as `val_loss` and `val_accuracy` during training.

In summary, by using the `skip` and `take` methods, you can effectively split a dataset into training, validation, and test subsets while maintaining the integrity of the `tf.data.Dataset` objects, ensuring that the model's validation metrics are correctly calculated and reported.","The issue is that you are not taking and skipping samples when you do <code>test_val_ds.take(686)</code> and <code>test_val_ds.skip(686)</code>, but actually batches. Try running <code>print(val_dataset.cardinality())</code> and you will see how many batches you really have reserved for validation. I am guessing <code>val_dataset</code> is empty, because you do not have 686 batches for validation.",0.7984503516,0.7938014063,1
"<p>I'm just beginning to learn TensorFlow. Quoting from the <a href=""https://www.tensorflow.org/guide/low_level_intro#graph"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Let's build a simple computational graph. The most basic operation is a constant. The Python function that builds the operation takes a tensor value as input. The resulting operation takes no inputs. When run, it outputs the value that was passed to the constructor. We can create two floating point constants a and b as follows:</p>
</blockquote>

<pre><code>a = tf.constant(3.0, dtype=tf.float32)
b = tf.constant(4.0) # also tf.float32 implicitly
total = a + b
print(a)
print(b)
print(total)
</code></pre>

<p>The second constant is implicitly typed as a float32. Is that based on the explicit typing of the first constant? And does that imply that the first <code>dtype</code> is required? <a href=""https://www.tensorflow.org/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">tf.constant documentation</a> would imply that it does not:</p>

<blockquote>
  <p>If the argument dtype is not specified, then the type is inferred from the type of <code>value</code>.</p>
</blockquote>

<p>But then it would be unnecessary to explicitly type the 3.0 constant above.</p>

<p>I'm just looking for some clarification on this, since, like I said, I'm just starting out.</p>
","['""""""Why does tf.constant give a dtype error if we pass in a tensor? \\n\\na = tf.range(10) b = tf.constant(a, dtype=tf.float32)\\n\\ngives the following error:\\n\\nTypeError: Expected tensor with type tf.float32 not tf.int32\\n\\nAlthough from the documentation, setting dtype means that tf.constant is supposed to cast a to the specified data type. So I don\'t see why this should give a type error. a = np.arange(10) b = tf.constant(a, dtype=tf.float32)\\n\\ndoes not give an error. So actually, I\\\'m mainly wondering about what\\\'s happening under the hood here.\n\nIf you look at the source here, you will see that EagerTensor gets a special treatment. Basically, if the dtype of an EagerTensor doesn\'t match the new dtype, an error is raised. Here, tf.range() produces an EagerTensor. I\\\'m not sure why the special treatment for EagerTensors though. Could be a performance related restriction. """"""']","### Explanation of Explicit vs Implicit Type Definition in TensorFlow

In TensorFlow, when creating constants using `tf.constant`, the data type (`dtype`) of the constant can either be explicitly specified or implicitly inferred. The `dtype` argument is optional. If it is not provided, TensorFlow will infer the type from the value provided.

In the example given, `a = tf.constant(3.0, dtype=tf.float32)` explicitly sets the data type of the constant `a` to `tf.float32`. On the other hand, `b = tf.constant(4.0)` does not explicitly specify the `dtype`. In this case, TensorFlow infers the data type from the value `4.0`, which is a floating-point number, and thus `b` is also of type `tf.float32`.

The explicit specification of `dtype` is not required for the second constant `b` because TensorFlow can infer the type from the value. The documentation states that if the `dtype` is not specified, it will be inferred from the type of `value`. This means that the explicit typing of the first constant does not influence the typing of the second constant. Each constant's type is determined independently based on the provided value unless explicitly specified otherwise.

The confusion might arise from the assumption that the explicit typing of one constant affects the typing of another, which is not the case. The explicit `dtype` is only necessary if you want to ensure a specific type for a constant, especially when the value could be interpreted in multiple ways (e.g., integers vs. floating-point numbers).

In summary, the explicit `dtype` is not required if the type can be correctly inferred from the value. The explicit typing of one constant does not influence the implicit typing of another; each constant's type is determined based on its own value unless explicitly specified.","But then it would be unnecessary to explicitly type the 3.0 constant  above.
Absolutely correct. 
<pre><code>a = tf.constant(3.0, dtype=tf.float32)
</code></pre>

is equivalent to:
<pre><code>a = tf.constant(3.0)
</code></pre>
The documentation is just demonstrating the different overloads. We might choose to explicitly provide the type if we want a different numerical precision (or even just to aid human readability) but if you want the default data type TF infers, then it's entirely unnecessary.
",0.4423067768,0.8692271073,0.9
"<p>In the official <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">tf.custom_gradient</a> documentation it shows how to define custom gradients for <code>log(1 + exp(x))</code></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.math.log(1 + e), grad
</code></pre>
<p>When <code>y = log(1 + exp(x))</code>, analytically the derivative comes out to be <code>dy/dx = (1 - 1 / (1 + exp(x)))</code>.</p>
<p>However in the code <code>def grad</code> says its <code>dy * (1 - 1 / (1 + exp(x)))</code>.
<code>dy/dx = dy * (1 - 1 / (1 + exp(x)))</code> is not a valid equation. While <code>dx = dy * (1 - 1 / (1 + exp(x)))</code> is wrong as it should be the reciprocal.</p>
<p>What does the <code>grad</code> function equate to?</p>
","['""""""Does that mean in the custom gradient function, I need to return the same results as what tf.gradients should give, of which each elements are summed partial derivatives of dy/dx? @NathanExplosion Yes, that sounds right. I have added a snippet that (I hope) demonstrates how tf.gradients and gradient functions relate to each other. I tried tf.gradients(c[0,0], a), it will return dc[0,0]/da. But if we defined the returned gradients as the summation of partial gradients, how could it derive an individual gradient? @NathanExplosion In that case the flow goes like this. You have a slice operation that gives you a scalar, so you start the gradient computation with the scalar 1 as dc[0,0]/dc[0,0]. Then you compute dc[0,0]/dc, which is a matrix shaped like c with the gradient of c[0,0] wrt each element - so it is a matrix, grad_c, with all 0 except a 1 in the first value of the first row. Then you can get dc[0,0]/da, which is (dc[0,0]/dc)*(dc/da). We saw it ended up being grad_c * b.T, so you get a matrix the size of a where the first row is the first column of b and all other rows are 0. \n\nDefining custom gradient as a class method in Tensorflow\\n\\nI need to define a method to be a custom gradient as follows:\\n\\nclass CustGradClass: def __init__(self): pass @tf.custom_gradient def f(self,x): fx = x def grad(dy): return dy * 1 return fx, grad\\n\\nI am getting the following error:\\n\\nValueError: Attempt to convert a value (<main.CustGradClass object at 0x12ed91710>) with an unsupported type () to a Tensor. The reason is the custom gradient accepts a function f(*x) where x is a sequence of Tensors. And the first argument being passed is the object itself i.e., self. From the documentation:\\n\\nf: function f(*x) that returns a tuple (y, grad_fn) where: x is a sequence of Tensor inputs to the function. y is a Tensor or sequence of Tensor outputs of applying TensorFlow operations in f to x. grad_fn is a function with the signature g(*grad_ys)\\n\\nHow do I make it work? Do I need to inherit some python tensorflow class? I am using tf version 1.12.0 and eager mode.\n\nAlso, this attached image describes the solution as expected by manually calulation\\n\\nIf I do not use the @tf.custom_gradient then the TensorFlow gives the desired solution as expected. My question is that how can I provide custom gradient for y=Ax? We know that dy/dx = A^T as shown in the above attachment which shows steps of calculation that matches the TensorFlow output. import tensorflow as tf #I want to write custom gradient for this function f1 def f1(A,x): y=tf.matmul(A,x,name=\\\'y\\\') return y #for y= Ax, the derivative is: dy/dx= transpose(A) @tf.custom_gradient def f2(A,x): y=f1(A,x) def grad(dzByDy): # dz/dy = 2y reaches here correctly. dzByDx=tf.matmul(A,dzByDy,transpose_a=True) return dzByDx return y,grad x= tf.constant([[1.],[0.]],name=\\\'x\\\') A= tf.constant([ [1., 2.], [3., 4.]],name=\\\'A\\\') y=f1(A,x) # This works as desired #y=f2(A,x) #This line gives Error z=tf.reduce_sum(y*y,name=\\\'z\\\') g=tf.gradients(ys=z,xs=x) with tf.Session() as sess: print sess.run(g)\\n\\nSince your function f2() has two inputs, you have to provide a gradient to flow back to each of them. The error you see:\\n\\nNum gradients 2 generated for op name: ""IdentityN"" [...] do not match num inputs 3\\n\\nis admittedly quite cryptic, though. Supposing you never want to calculate dy/dA, you can just return None, dzByDx. The code below (tested):\\n\\nimport tensorflow as tf #I want to write custom gradient for this function f1 def f1(A,x): y=tf.matmul(A,x,name=\\\'y\\\') return y #for y= Ax, the derivative is: dy/dx= transpose(A) @tf.custom_gradient def f2(A,x): y=f1(A,x) def grad(dzByDy): # dz/dy = 2y reaches here correctly. dzByDx=tf.matmul(A,dzByDy,transpose_a=True) return None, dzByDx return y,grad x= tf.constant([[1.],[0.]],name=\\\'x\\\') A= tf.constant([ [1., 2.], [3., 4.]],name=\\\'A\\\') #y=f1(A,x) # This works as desired y=f2(A,x) #This line gives Error z=tf.reduce_sum(y*y,name=\\\'z\\\') g=tf.gradients(ys=z,xs=x) with tf.Session() as sess: print sess.run( g )\\n\\n[array([[20.], [28.]], dtype=float32)]""""""']","### Analytic Interpretation of TensorFlow Custom Gradient

In TensorFlow, the `tf.custom_gradient` decorator allows users to define custom gradients for functions, providing fine-grained control over the gradient computation. This can be particularly useful for improving numerical stability or efficiency in gradient calculations.

The question pertains to the analytic interpretation of the custom gradient function provided in the TensorFlow documentation for the function `log(1 + exp(x))`. When defining a custom gradient, the function `grad` is responsible for computing the gradient of the output with respect to the input.

For the function `y = log(1 + exp(x))`, the analytic derivative is `dy/dx = 1 - 1 / (1 + exp(x))`. In the custom gradient definition, the `grad` function returns `dy * (1 - 1 / (1 + exp(x)))`. Here, `dy` represents the upstream gradient, which is the gradient flowing from the subsequent layers or operations in the computational graph.

The expression `dy * (1 - 1 / (1 + exp(x)))` is the product of the upstream gradient `dy` and the local gradient `1 - 1 / (1 + exp(x))`. This follows the chain rule of differentiation, where the total gradient is the product of the upstream gradient and the local gradient. Therefore, the `grad` function correctly computes the gradient of `y` with respect to `x` by multiplying the upstream gradient `dy` with the local gradient `1 - 1 / (1 + exp(x))`.

In summary, the `grad` function in the custom gradient definition represents the chain rule application, where `dy` is the upstream gradient and `1 - 1 / (1 + exp(x))` is the local gradient. This ensures that the overall gradient computation is accurate and aligns with the mathematical principles of differentiation.","By chain rule we know that where <code>dx[i]/dx[i+1]</code> is the gradient of the current function. So <code>dy</code> is the product of all the gradients upstream before this function. So, if you forget to multiply the <code>dy</code> it is effectively the same as tf.stop_gradient.
",0.6331663479,0.8183796771,1
"<p>tf.compat.v1.layers.batch_normalization takes <code>trainable</code> as an input. The documentation says:</p>

<blockquote>
  <p>Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).</p>
</blockquote>

<p>I think only scaling factor (gamma) and offset (beta) should be added to trainable variables and I am skeptical if even moving averages will get added to GraphKeys.TRAINABLE_VARIABLES. Can somebody tell me how trainable input is influencing the behavior of batch_normalization</p>
","['""""""What\'s the differences between tf.GraphKeys.GLOBAL_VARIABLES and tf.GraphKeys.TRAINABLE_VARIABLES in tensorflow?\\n\\nGLOBAL_VARIABLES: the default collection of Variable objects, shared across distributed environment (model variables are subset of these).  Commonly, all TRAINABLE_VARIABLES variables will be in MODEL_VARIABLES, and all MODEL_VARIABLES variables will be in GLOBAL_VARIABLES\\n\\nTRAINABLE_VARIABLES: the subset of Variable objects that will be trained by an optimizer. See tf.compat.v1.trainable_variables for more details. So as I understand TRAINABLE_VARIABLES is subset of GLOBAL_VARIABLES, so what else GLOBAL_VARIABLES contain? Also for this simple example statement Commonly, all TRAINABLE_VARIABLES variables will be in MODEL_VARIABLES, and all MODEL_VARIABLES variables will be in GLOBAL_VARIABLES don\\\'t hold:\\n\\nIMAGE_HEIGHT = 5 IMAGE_WIDTH = 5 with tf.Graph().as_default(): with tf.variable_scope(\\\'my_scope\\\', reuse=tf.AUTO_REUSE): x_ph = tf.placeholder( dtype=tf.float32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 3], name=\\\'input\\\' ) x_tf = tf.layers.conv2d(x_ph, 32, 1, 1, padding=\\\'valid\\\') with tf.Session() as sess: sess.run(tf.global_variables_initializer()) x_np = np.random.rand(1, IMAGE_HEIGHT, IMAGE_WIDTH, 3) out_np = sess.run(x_tf, {x_ph:x_np}) print(\\\'out_np.shape\\\', out_np.shape) print(\\\'-\\\'*60) global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) print(\\\'len(global_vars)\\\', len(global_vars)) print(\\\'global_vars params:\\\', sum([np.prod(var.shape) for var in global_vars])) print(global_vars) print(\\\'-\\\'*60) model_vars = tf.get_collection(tf.GraphKeys.MODEL_VARIABLES) print(\\\'len(model_vars)\\\', len(model_vars)) print(\\\'model_vars params:\\\', sum([np.prod(var.shape) for var in model_vars])) print(model_vars) print(\\\'-\\\'*60) trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) print(\\\'len(trainable_vars)\\\', len(trainable_vars)) print(\\\'trainable_vars params:\\\', sum([np.prod(var.shape) for var in trainable_vars])) print(trainable_vars)\\n\\nout_np.shape (1, 5, 5, 32) ------------------------------------------------------------ len(global_vars) 2 global_vars params: 128 [<tf.Variable \\\'my_scope/conv2d/kernel:0\\\' shape=(1, 1, 3, 32) dtype=float32_ref>, <tf.Variable \\\'my_scope/conv2d/bias:0\\\' shape=(32,) dtype=float32_ref>] ------------------------------------------------------------ len(model_vars) 0 model_vars params: 0 [] ------------------------------------------------------------ len(trainable_vars) 2 trainable_vars params: 128 [<tf.Variable \\\'my_scope/conv2d/kernel:0\\\' shape=(1, 1, 3, 32) dtype=float32_ref>, <tf.Variable \\\'my_scope/conv2d/bias:0\\\' shape=(32,) dtype=float32_ref>]\\n\\nWhy Commonly, all TRAINABLE_VARIABLES variables will be in MODEL_VARIABLES, and all MODEL_VARIABLES variables will be in GLOBAL_VARIABLES don\\\'t hold for this example. What additional variables GLOBAL_VARIABLES contain besides TRAINABLE_VARIABLES? Is it true that TRAINABLE_VARIABLES will be always subset of GLOBAL_VARIABLES or they can just intersect partially? Note: All of this applies to TF version 1 only, as all variable collections have been deprecated and (IIRC) won\\\'t be in TF v2. Starting from question 2:\\n\\nWhat additional variables GLOBAL_VARIABLES contain besides TRAINABLE_VARIABLES? global_step, for example, is a global variable which is not trainable. It\\\'s a variable, because you update it at every step, it\\\'s not trainable because it\\\'s not part of the optimization process (as in, it\\\'s not a weight/bias that is altered in order to minimize the loss). Is it true that TRAINABLE_VARIABLES will be always subset of GLOBAL_VARIABLES or they can just intersect partially? In principle, the two groups can just intersect partially, although this would be very odd. An example I can think of would be something like a custom distributed training environment where each machine has its own optimizer and where some of the trainable variables are defined as local variables (i.e., each machine has its own copy and these copies are not kept in sync). Why would you do that?\n\nGraphKeys.TRAINABLE_VARIABLES vs tf.trainable_variables()\\n\\n Is GraphKeys.TRAINABLE_VARIABLES is the same as tf.trainable_variables() ? Is GraphKeys.TRAINABLE_VARIABLES actually tf.GraphKeys.TRAINABLE_VARIABLES? Looks like networks successfully trains with:\\n\\noptimizer = tf.train.AdamOptimizer(config.LEARNING_RATE) with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): self.train_op = optimizer.minimize(self.loss, var_list=tf.trainable_variables())\\n\\noptimizer = tf.train.AdamOptimizer(config.LEARNING_RATE) with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): self.train_op = optimizer.minimize(self.loss)\\n\\nAccording to documentation:\\n\\nvar_list: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.""""""']","### Description

In TensorFlow, a trainable parameter refers to a variable that is adjusted during the training process to minimize the loss function. The `tf.compat.v1.layers.batch_normalization` function includes a `trainable` argument, which is a boolean. When set to `True`, this argument ensures that the variables associated with the batch normalization layer are added to the `GraphKeys.TRAINABLE_VARIABLES` collection. This collection is a subset of `GraphKeys.GLOBAL_VARIABLES` and includes variables that will be trained by an optimizer.

The primary trainable parameters in batch normalization are the scaling factor (gamma) and the offset (beta). These parameters are adjusted during training to normalize the input data. The moving averages, such as moving mean and moving variance, are not considered trainable parameters because they are updated based on the statistics of the current batch and are not directly optimized to minimize the loss function.

The `trainable` argument influences the behavior of the batch normalization layer by determining whether the gamma and beta variables are included in the `GraphKeys.TRAINABLE_VARIABLES` collection. If `trainable` is set to `True`, these variables will be optimized during training. If set to `False`, they will not be included in the trainable variables collection, and their values will remain fixed during training.

In summary, the `trainable` argument in `tf.compat.v1.layers.batch_normalization` controls whether the gamma and beta parameters are added to the `GraphKeys.TRAINABLE_VARIABLES` collection, thereby influencing their optimization during the training process. The moving averages are not affected by this argument as they are not trainable parameters.","First of all, this function is deprecated and should not be used. <code>trainable</code> arguments means that scaling factor (gamma) and offset (beta) will be trainable and it's true by default. When it comes to moving averages, those <strong>are not trainable</strong>, they are only updated after each batch pass, those are not parameters (<code>tf.Variable</code> objects). Please notice, you can set <code>trainable</code> to false, in such case, if <code>beta</code> and <code>gamma</code> are set to defaults (zero and one respectively), they won't affect the moving averages. You can turn them off by issuing <code>center</code> (for <code>beta</code>) or <code>scale</code> (for <code>gamma</code>).
",0.7448731285,0.8925359922,1
"<p>I am trying to use tf.pad. Here is my attempt to pad the tensor to length 20, with values 10.</p>
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), paddings=20, constant_values=10)
</code></pre>
<p>I get this error message</p>
<pre><code>InvalidArgumentError: paddings must be a matrix with 2 columns: [2,1] [Op:PadV2]
</code></pre>
<p>I am looking at the documentation</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/pad</a></p>
<blockquote>
<p>paddings is an integer tensor with shape [n, 2], where n is the rank of tensor. For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension</p>
</blockquote>
<p>But I am unable to figure out how to shape the pad value</p>
","['""""""For processing, a static length makes them easier to work with. Yes. There is. Provided you do not need to change the rank of the tensor, it\\\'s very simple. tf.pad() accepts regular python lists with tensors. The format of the padding is a list of pairs of how much to pad on each side of that dimension. t = tf.constant([[1, 2], [3, 4]]) paddings = [[0, 0], [0, 4-tf.shape(t)[0]]] out = tf.pad(t, paddings, \\\'CONSTANT\\\', constant_values=-1) sess.run(out) # gives: # array([[ 1, 2, -1, -1], # [ 3, 4, -1, -1]], dtype=int32)\\n\\nIf you want to generalise this to a useful function, you could do something like:\\n\\ndef pad_up_to(t, max_in_dims, constant_values): diff = max_in_dims - tf.shape(t) paddings = tf.pad(diff[:, None], [[0, 0], [1, 0]]) return tf.pad(t, paddings, \\\'CONSTANT\\\', constant_values=constant_values) # (note: see edits for the solution referred to by other answers on this question)\\n\\nwhere max_in_dims is essentially the desired shape of the output. Note: this function will fail if you provide a shape that is strictly smaller than t in any dimension. You can use it like:\\n\\nt = tf.constant([[1, 2], [3, 4]]) # shape = [2, 2] t_padded = pad_up_to(t, [2, 4], -1) # shape = [2, 4], padded with -1s\\n\\nt = tf.placeholder(tf.float32, [None, None]) # shape = [?, ?] t_padded = pad_up_to(t, [5,5], -1) # shape = [5, 5], padded with -1s t_np = np.random.uniform(0, 1, [3,4]) # shape = [3,4], no padding t_padded_out = sess.run(t_padded, {t: t_np}) t_np2 = np.random.uniform(0, 1, [2,1]) # shape = [2,1], no padding t_padded_out2 = sess.run(t_padded, {t: t_np2})\\n\\nAlthough the dimension sizes are calculated dynamically, the number of dimensions is not, so make sure that max_in_dims has the same number of elements as t.shape. MultihunterMultihunter\\n\\n 12\\n\\nWhat if t has a dynamic size (e.g., its size is determined only after some placeholder is fed)? In my provided function, s is a tensor that is the shape of t, so the amount to pad is calculated dynamically. The number of dimensions is not calculated dynamically, so just make sure your max_in_dims is a vector with has the same number of elements as your t has dimensions. If you do this it will just work (I wrote the function with this use-case in mind). I didn\\\'t expect it to work with a dynamic size but to my surprise, it does! Thanks! Good reference to not waste time finding a more off the shelf solution. This didn\\\'t really work for me in TF 2.3 with dynamic sizes since m is evaluated to None which throws an error for the subtraction. However, the fix is to simply change the line to [[0, m - s[i]] if m != None else [0,0] for (i, m) in enumerate(max_in_dims)]. An extension of Multihunter\\\'s solution so that padding is only performed when necessary and does not yield an error for longer inputs:\\n\\nSuppose we have a sequential input called inp_seq, which is a tensor of rank 4 and should be padded in order to have a minimum length of filter_size in dimension 1. def dynamic_padding(inp, min_size): pad_size = min_size - tf.shape(inp)[1] paddings = [[0, 0], [0, pad_size], [0, 0], [0, 0]] # assign here, during graph execution return tf.pad(inp, paddings) # Pad only if necessary padded = tf.cond(tf.less(tf.shape(inp_seq)[1], filter_size), true_fn=lambda: dynamic_padding(inp_seq, filter_size), false_fn=lambda: inp_seq)\\n\\n1,2\\n\\nThe line creating a tf.Variable is redundant, since the subsequent line overwrites it with a python list. You can remove that line and it will function the same. (Also, a sequence is a class defined by the python base libraries, while a tensor is defined by tensorflow: I think you should clarify which of these your inp_seq actually is; I presume that what you\\\'re dealing with is actually a sequence (or list) of Tensors like inp_seq=[Tensor, Tensor, Tensor])\\n\\n\\n\\nI removed the redundant line, thank you for the suggestion. The input is simply a tensor; I used the term sequence with its broader meaning (to refer to data of high dimensionality which are sequential along one dimension, namely the one to pad), I was not referring to the python base libraries. I clarified this in the edit.""""""']","To pad a 1-dimensional vector in TensorFlow, you need to use the `tf.pad` function correctly by specifying the `paddings` argument as a matrix with 2 columns. The `paddings` argument should be an integer tensor with shape `[n, 2]`, where `n` is the rank of the tensor you are padding. For each dimension `D` of the input tensor, `paddings[D, 0]` indicates how many values to add before the contents of the tensor in that dimension, and `paddings[D, 1]` indicates how many values to add after the contents of the tensor in that dimension.

In your case, you are trying to pad a 1-dimensional tensor to a length of 20 with values of 10. The rank of your tensor is 1, so `paddings` should be a matrix with one row and two columns. The first column specifies the padding before the tensor, and the second column specifies the padding after the tensor. 

To achieve the desired padding, you need to calculate the appropriate values for the `paddings` matrix. For example, if your original tensor has a length of 4 and you want to pad it to a length of 20, you need to add 16 values in total. You can distribute these values before and after the tensor as needed.

Ensure that the `constant_values` argument is set to the value you want to pad with, which in this case is 10. The `mode` argument should be set to ""CONSTANT"" to use constant padding.

By following these guidelines, you can correctly pad your 1-dimensional tensor in TensorFlow without encountering the `InvalidArgumentError`.","You have to specify the padding at the beginning and the padding at the end of your vector by matrix of shape (1,2) :
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), [[ 0 , 20]], constant_values=10)
</code></pre>
if you have three-dimensional tensor (rank = 3 e.g : (225,225,3) ) the padding matrix has to be of shape (3, 2 ) where &quot;3&quot; is the rank, and &quot;2&quot; to specify the padding at the beginning and end of each dimension. For example, a padding matrix  = [ [0,2], [5,5], [2,0] ], means that we want to pad the first dimension by 0 at the beginning (=no padding) and 2 at the end .padding the second dimension by 5 at beginning and 5 at the end.
",0.8683014795,0.9017773464,0.8
"<p>I have a question regarding Tensorflow:</p>
<p>Which loss function is used in <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer""><code>tf.nn.sampled_softmax_loss</code></a>?</p>
<p>I believe it's <em><strong>cross-entropy</strong></em>, but it is not written on the official website. Can anyone confirm my guess?</p>
","['""""""The arguments weights and biases specify a separate fully-connected layer that is used to compute the logits for a chosen sample. Like above, labels are not one-hot encoded, but have the shape [batch_size, num_true]. Sampled functions are only suitable for training. In test time, it\\\'s recommended to use a standard softmax loss (either sparse or one-hot) to get an actual distribution. Another alternative loss is tf.nn.nce_loss, which performs noise-contrastive estimation (if you\\\'re interested, see this very detailed discussion). I\\\'ve included this function to the softmax family, because NCE guarantees approximation to softmax in the limit. 1\\n\\nMay I ask for a point of clarification regarding sigmoid cross entropy (sigCE)? If it solves for N binary classification tasks at once, is N = prod(output.shape), e.g. shape = [batch, examples, channels]; N = (batch * examples * channels)? If tf.losses expect ""logits"" (output of network), should I also return the probabilities for ease of use? \\n\\nHowever, for version 1.5, softmax_cross_entropy_with_logits_v2 must be used instead, while using its argument with the argument key=..., for example\\n\\nsoftmax_cross_entropy_with_logits_v2(_sentinel=None, labels=y, logits=my_prediction, dim=-1, name=None)\\n\\n\\n\\n 0\\n\\nWhile it is great that the accepted answer contains lot more info than what is asked, I felt that sharing a few generic thumb rules will make the answer more compact and intuitive:\\n\\nThere is just one real loss function. This is cross-entropy (CE). For a special case of a binary classification, this loss is called binary CE (note that the formula does not change) and for non-binary or multi-class situations the same is called categorical CE (CCE). Sparse functions are a special case of categorical CE where the expected values are not one-hot encoded but is an integer\\n\\nWe have the softmax formula which is an activation for multi-class scenario. For binary scenario, same formula is given a special name - sigmoid activation\\n\\nBecause there are sometimes numerical instabilities (for extreme values) when dealing with logarithmic functions, TF recommends combining the activation layer and the loss layer into one single function. This combined function is numerically more stable. TF provides these combined functions and they are suffixed with _with_logits\\n\\nWith this, let us now approach some situations. Say there is a simple binary classification problem - Is a cat present or not in the image? What is the choice of activation and loss function? It will be a sigmoid activation and a (binary)CE. So one could use sigmoid_cross_entropy or more preferably sigmoid_cross_entropy_with_logits. The latter combines the activation and the loss function and is supposed to be numerically stable. How about a multi-class classification. Say we want to know if a cat or a dog or a donkey is present in the image. What is the choice of activation and loss function? It will be a softmax activation and a (categorical)CE. So one could use softmax_cross_entropy or more preferably softmax_cross_entropy_with_logits. We assume that the expected value is one-hot encoded (100 or 010 or 001). If (for some weird reason), this is not the case and the expected value is an integer (either 1 or 2 or 3) you could use the \\\'sparse\\\' counterparts of the above functions. There could be a third case.\n\nIn Tensorflow, what is the difference between sampled_softmax_loss and softmax_cross_entropy_with_logits\\n\\nIn tensorflow, there are methods called softmax_cross_entropy_with_logits and sampled_softmax_loss. I read the tensorflow document and searched google for more information but I couldn\\\'t find the difference. It looks like to me both calculates the loss using softmax function. Using sampled_softmax_loss to calculate the loss\\n\\nloss = tf.reduce_mean(tf.nn.sampled_softmax_loss(...))\\n\\nUsing softmax_cross_entropy_with_logits to calculate the loss\\n\\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(P, Q))\\n\\nTo me, calculating softmax loss is same as calculating softmaxed cross entropy (e.g. cross_entropy(softmax(train_x)))\\n\\nCould somebody tell me the why there is two different methods and which method should I use in which case? \\n\\nIf your target vocabulary(or in other words amount of classes you want to predict) is really big, it is very hard to use regular softmax, because you have to calculate probability for every word in dictionary. By Using sampled_softmax_loss you only take in account subset V of your vocabulary to calculate your loss. Sampled softmax only makes sense if we sample(our V) less than vocabulary size. If your vocabulary(amount of labels) is small, there is no point using sampled_softmax_loss. Sequence to sequence translation in this example\\n\\n 2\\n\\nHi Farseer, I kind of struggle with the same issue as above. My output vocabulary is only around 100 labels. Now, tensorflow only provides the sampled_softmax_loss function. Instead of this one, could I use the tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases) function? Or can I just use the seq2seq_model.py as it is?""""""']","The `tf.nn.sampled_softmax_loss` function in TensorFlow computes and returns the sampled softmax training loss. This function is designed to provide a more efficient way to train a softmax classifier when dealing with a large number of classes. The loss function used in `tf.nn.sampled_softmax_loss` is indeed based on cross-entropy. 

In the context of sampled softmax, the cross-entropy loss is computed over a subset of the possible classes, rather than the full set. This subset is randomly sampled, which makes the computation more efficient, especially when the number of classes is very large. The sampled softmax loss is generally an underestimate of the full softmax loss, but it is much faster to compute during training. For evaluation or inference, it is recommended to use the full softmax loss to get an accurate measure of performance.

To summarize, the loss function used in `tf.nn.sampled_softmax_loss` is a form of cross-entropy loss, specifically adapted for efficient training with a large number of classes by sampling a subset of the classes.","Besides, the main difference between <code>sampled_softmax_loss</code> and <code>softmax_cross_entropy_with_logits</code> (the standard cross_entropy loss in TF) is that the first only takes into account a subset V of your vocabulary to calculate your loss, while the second takes into account your entire vocabulary.
",0.4371550907,0.8914775056,1
"<p>I'm trying to understand how <strong>tf.data.Dataset</strong> works.</p>
<p>It says on the documentation that <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""nofollow noreferrer"">take</a> returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch):</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds

# Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=True)

# Build your input pipeline
ds = ds.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)
# ...
</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([2 0 6 6 8 8 6 0 3 4 8 7 5 2 5 7 8 7 1 1 1 8 6 4 0 4 3 2 4 2 1 9], shape=(32,), dtype=int64)
</code></pre>
<p>However, iterating over it again, gives different labels: (continuation of last code)</p>
<pre class=""lang-py prettyprint-override""><code>for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([7 3 5 6 3 1 7 9 6 1 9 3 9 8 6 7 7 1 9 7 5 2 0 7 8 1 7 8 7 0 5 0], shape=(32,), dtype=int64)
tf.Tensor([1 3 6 1 8 8 0 4 1 3 2 9 5 3 8 7 4 2 1 8 1 0 8 5 4 5 6 7 3 4 4 1], shape=(32,), dtype=int64)
</code></pre>
<p>Shouldn't the labels be the same, given that the dataset is the same?</p>
","['""""""perhaps one could imagine a scenario where the shuffling is defined once and is used for every iteration. @jakub ah yes it makes sense from that perspective\\n\\nThis is because the data files are shuffled and the dataset is shuffled with dataset.shuffle(). With dataset.shuffle(), the data will be shuffled in a different way on each iteration by default. One can remove shuffle_files=True and set the argument reshuffle_each_iteration=False to prevent reshuffling on different iterations. The .take() function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them. # Construct a tf.data.Dataset ds = tfds.load(\\\'mnist\\\', split=\\\'train\\\', shuffle_files=False) # Build your input pipeline ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE) single_batch_dataset = ds.take(1) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label)\\n\\ntf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64) tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)\\n\\n\\n\\n""""""']","### Explanation of Why Iterations Over the Same `tf.data.Dataset` Give Different Data Each Iteration

The question addresses why iterating over the same `tf.data.Dataset` multiple times yields different data each iteration, even though the dataset itself remains unchanged.

The behavior observed is due to the shuffling mechanism applied to the dataset. When a `tf.data.Dataset` is shuffled using the `shuffle` method, it fills a buffer with a specified number of elements (`buffer_size`) and randomly samples elements from this buffer, replacing the selected elements with new ones. This process ensures that the elements are shuffled in a random order.

By default, the `shuffle` method reshuffles the dataset for each iteration. This is controlled by the `reshuffle_each_iteration` parameter, which is set to `True` by default. As a result, each time you iterate over the dataset, the elements are shuffled differently, leading to different outputs for each iteration.

In the provided context, the dataset is shuffled with `ds.shuffle(1024)`, and since `reshuffle_each_iteration` is not explicitly set to `False`, it defaults to `True`. Consequently, each iteration over the dataset results in a different order of elements, which explains why the labels printed in each iteration are different.

To achieve deterministic behavior where the dataset yields the same order of elements in each iteration, you can set `reshuffle_each_iteration=False` in the `shuffle` method. This will ensure that the dataset is shuffled only once, and the same order of elements is maintained across multiple iterations.

In summary, the different outputs in each iteration over the same `tf.data.Dataset` are due to the default behavior of the `shuffle` method, which reshuffles the dataset for each iteration. This can be controlled by setting the `reshuffle_each_iteration` parameter to `False` if deterministic behavior is desired.","This is because the data files are shuffled and the dataset is shuffled with <code>dataset.shuffle()</code>. With <code>dataset.shuffle()</code>, the data will be shuffled in a different way on each iteration by default. One can remove <code>shuffle_files=True</code> and set the argument <code>reshuffle_each_iteration=False</code> to prevent reshuffling on different iterations. The <code>.take()</code> function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them.",0.8188133844,0.830809093,0.6
"<p>I still don't after having read documentation about <code>tf.keras.Model.fit</code> and <code>tf.data.Dataset</code>, when passing <code>tf.data.Dataset</code> to fit function, should I call <code>repeat</code> and <code>batch</code> on the dataset object or should I provide the <code>batch_size</code> and <code>epochs</code> arguments to fit instead? or both? Should I apply the same treatment to the validation set?</p>

<p>And while I'm here, can I <code>shuffle</code> the dataset before the <code>fit</code>? (seems like it's an obvious yes)
If so, before, after calling <code>Dataset.batch</code> and <code>Dataset.repeat</code> (if calling them)?</p>

<p><strong>Edit:</strong> When using <code>batch_size</code> argument, and without having called <code>Dataset.batch(batch_size)</code> previously, I am getting the following error:</p>

<pre><code>ValueError: The `batch_size` argument must not be specified for the given input type.
Received input: &lt;MapDataset shapes: ((&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;), (&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;)), 
types: ((tf.float32, tf.float32, tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32))&gt;, 
batch_size: 1
</code></pre>

<p>Thanks</p>
","['""""""How does Model.fit() method\\\'s shuffle deals with Batches when using a tf.data.Dataset? \\n\\nI am using tensorflow 2. When using the Model.fit() method with a tf.data.Dataset, the argument \\\'batch_size\\\' is ignored. Thus to train my model on batches, I have to first change my dataset of samples into a dataset of batches of samples by calling tf.data.Dataset.batch(batch_size). Then, after reading the documentation, I don\\\'t understand clearly how the .fit() method will shuffle my dataset at each epoch. Since my dataset is a dataset of batches, will it shuffle the batches among each other (the batches remain unchanged) ? Or will it shuffle all the samples and then regroup them into new batches (which is the desired behaviour) ? Thanks a lot for your help. The shuffle parameter has no effect on the fit function when using the tf.data.Dataset API. If we read the documentation (emphasis is mine) :\\n\\nshuffle: Boolean (whether to shuffle the training data before each epoch) or str (for \\\'batch\\\'). This argument is ignored when x is a generator. \\\'batch\\\' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None. It\\\'s not super clear, but we can have a hint that the shuffle argument will be ignored when using a tf.data.Dataset, as it behave like a generator. To be certain, lets dive in the code.\n\nKeras model.fit() with tf.dataset API + validation_data\\n\\nSo I have got my keras model to work with a tf.Dataset through the following code:\\n\\n# Initialize batch generators(returns tf.Dataset) batch_train = build_features.get_train_batches(batch_size=batch_size) # Create TensorFlow Iterator object iterator = batch_train.make_one_shot_iterator() dataset_inputs, dataset_labels = iterator.get_next() # Create Model logits = ...(some layers) keras.models.Model(inputs=dataset_inputs, outputs=logits) # Train network model.compile(optimizer=train_opt, loss=model_loss, target_tensors=[dataset_labels]) model.fit(epochs=epochs, steps_per_epoch=num_batches, callbacks=callbacks, verbose=1)\\n\\nhowever when I try to pass validation_data parameter to the model. fit it tells me that I cannot use it with the generator. Is there a way to use validation while using tf.Dataset\\n\\nfor example in tensorflow I could do the following:\\n\\n# initialize batch generators batch_train = build_features.get_train_batches(batch_size=batch_size) batch_valid = build_features.get_valid_batches(batch_size=batch_size) # create TensorFlow Iterator object iterator = tf.data.Iterator.from_structure(batch_train.output_types, batch_train.output_shapes) # create two initialization ops to switch between the datasets init_op_train = iterator.make_initializer(batch_train) init_op_valid = iterator.make_initializer(batch_valid)\\n\\nthen just use sess.run(init_op_train) and sess.run(init_op_valid) to switch between the datasets\\n\\nI tried implementing a callback that does just that (switch to validation set, predict and back) but it tells me I can\\\'t use model.predict in a callback\\n\\ncan someone help me get validation working with Keras+Tf.Dataset\\n\\nedit: incorporate answer into the code\\n\\nso FINALLY what worked for me, thanks to the selected answer is:\\n\\n# Initialize batch generators(returns tf.Dataset) batch_train = # returns tf.Dataset batch_valid = # returns tf.Dataset # Create TensorFlow Iterator object and wrap it in a generator itr_train = make_iterator(batch_train) itr_valid = make_iterator(batch_train) # Create Model logits = # the keras model keras.models.Model(inputs=dataset_inputs, outputs=logits) # Train network model.compile(optimizer=train_opt, loss=model_loss, target_tensors=[dataset_labels]) model.fit_generator( generator=itr_train, validation_data=itr_valid, validation_steps=batch_size, epochs=epochs, steps_per_epoch=num_batches, callbacks=cbs, verbose=1, workers=0) def make_iterator(dataset): iterator = dataset.make_one_shot_iterator() next_val = iterator.get_next() with K.get_session().as_default() as sess: while True: *inputs, labels = sess.run(next_val) yield inputs, labels\\n\\nThis doesn\\\'t introduce any overhead\\n\\nAfter your change, how do you get dataset_inputs into model? I\\\'m not getting how line keras.models.Model(inputs=dataset_inputs, outputs=logits), and i\\\'m assuming this is the contents of the ""model"" variable, could you please complete the code, i have the exact same problem but can\'t seem to know how to apply your code, thanks in advance\\n\\n@mark rofail, I believe this line is incorrect and should receive batch_valid: itr_valid = make_iterator(batch_train)\\n\\n I solved the problem by using fit_genertor. I found the solution here. I applied @Dat-Nguyen\\\'s solution. You need simply to create two iterators, one for training and one for validation and then create your own generator where you will extract batches from the dataset and provide the data in form of (batch_data, batch_labels) . Finally in model.fit_generator you will pass the train_generator and validation_generator. \\n\\nso I have to wrap tensorflow iterators in a python generator like: iterator = ds.make_one_shot_iterator() while True: next_val = iterator.get_next() yield sess.run(next_val)\\n\\n.\n\nThere is described all the details of DataSet API. Your question is about iterating over the data several times. Here are two solutions for that:\\n\\nIterating all epochs at once, no information about end of individual epochs\\n\\nimport tensorflow as tf epoch = 10 dataset = tf.data.Dataset.range(100) dataset = dataset.repeat(epoch) iterator = dataset.make_one_shot_iterator() next_element = iterator.get_next() sess = tf.Session() num_batch = 0 j = 0 while True: try: value = sess.run(next_element) assert j == value j += 1 num_batch += 1 if j > 99: # new epoch j = 0 except tf.errors.OutOfRangeError: break print (""Num Batch: "", num_batch)\\n\\nSecond option inform you about ending each of epoch, so you can ex. check validation loss:\\n\\nimport tensorflow as tf epoch = 10 dataset = tf.data.Dataset.range(100) iterator = dataset.make_initializable_iterator() next_element = iterator.get_next() sess = tf.Session() num_batch = 0 for e in range(epoch): print (""Epoch: "", e) j = 0 sess.run(iterator.initializer) while True: try: value = sess.run(next_element) assert j == value j += 1 num_batch += 1 except tf.errors.OutOfRangeError: break print (""Num Batch: "", num_batch)\\n\\n\\n\\nIf your tensorflow version is 1.3+, I recommend the high-level API tf.train.MonitoredTrainingSession. The sess created by this API can automatically detect tf.errors.OutOfRangeError with sess.should_stop(). For most of training situations, you need to shuffle data and get a batch each step, I have added these in the following code. import tensorflow as tf epoch = 10 dataset = tf.data.Dataset.range(100) dataset = dataset.shuffle(buffer_size=100) # comment this line if you don\\\'t want to shuffle data dataset = dataset.batch(batch_size=32) # batch_size=1 if you want to get only one element per step dataset = dataset.repeat(epoch) iterator = dataset.make_one_shot_iterator() next_element = iterator.get_next() num_batch = 0 with tf.train.MonitoredTrainingSession() as sess: while not sess.should_stop(): value = sess.run(next_element) num_batch += 1 print(""Num Batch: "", num_batch)\\n\\n\\n\\n\\n\\nwhile True: try: print(sess.run(value)) except tf.errors.OutOfRangeError: break\\n\\nWhenever the dataset iterator reaches the end of the data, it will raise tf.errors.OutOfRangeError, you can catch it with except and start the dataset from the beginning. \\n\\nYou should explain your code or include comments as well\\n\\n\\n\\nSimilar to Toms answer, for tensorflow 2+, you can use the following high-level API calls (the code proposed in his answer is deprecated in tensorflow 2+):\\n\\nepoch = 10 batch_size = 32 dataset = tf.data.Dataset.range(100) dataset = dataset.shuffle(buffer_size=100) # comment this line if you don\\\'t want to shuffle data dataset = dataset.batch(batch_size=batch_size) dataset = dataset.repeat(epoch) num_batch = 0 for batch in dataset: num_batch += 1 print(""Num Batch: "", num_batch)\\n\\nA helpful call to track progress is the total number of batches that will be iterated over (to be used after the batch and the repeat calls):\\n\\nnum_batches = tf.data.experimental.cardinality(dataset)\\n\\nNote that currently (tensorflow 2.1), the cardinality method is still experimental. """"""']","When using the `tf.keras.Model.fit` function with a `tf.data.Dataset`, it is important to understand how to properly prepare your dataset for training. The `batch_size` argument in `fit` is ignored when the input is a `tf.data.Dataset`. Therefore, you must call `Dataset.batch(batch_size)` on your dataset before passing it to `fit`. This ensures that your data is processed in batches during training.

You should also call `Dataset.repeat(epochs)` on your dataset if you want to iterate over the dataset for multiple epochs. This is necessary because `fit` will not automatically repeat the dataset for multiple epochs if it is a `tf.data.Dataset`.

Regarding shuffling, you can and should shuffle your dataset to ensure that the training data is randomized. It is recommended to call `Dataset.shuffle(buffer_size)` before batching and repeating the dataset. This ensures that the data is shuffled at the sample level before being grouped into batches, providing a more randomized training process.

For the validation set, you should apply the same treatment: batch, repeat, and shuffle (if desired) the validation dataset before passing it to `fit` via the `validation_data` argument. This ensures consistency in how both the training and validation datasets are processed.

In summary, when using `tf.data.Dataset` with `fit`, you should call `Dataset.batch(batch_size)` and `Dataset.repeat(epochs)` on your dataset. Additionally, you should shuffle your dataset before batching and repeating it. This approach ensures that your data is properly prepared for training and validation, avoiding errors related to the `batch_size` argument and ensuring a randomized training process.","This way you have access to all the variables after model fitting as well, for example if you want to visualize the validation set, you can. This is not really possible with <code>validation_split</code>. If you remove <code>.batch(batch_size)</code>, you should remove the <code>// batch_size</code>s, but I would leave them, as it clearer what is happening now.",0.8769561083,0.7981470136,0.8571428571
"<p>I would like to use the function <code>tf.nn.conv2d()</code> on a <strong>single</strong> image example, but the TensorFlow documentation seems to only mention applying this transformation to a <strong>batch</strong> of images. </p>

<p>The docs mention that the input image must be of shape <code>[batch, in_height, in_width, in_channels]</code> and the kernel must be of shape <code>[filter_height, filter_width, in_channels, out_channels]</code>. However, what is the most straightforward way to achieve 2D convolution with input shape <code>[in_height, in_width, in_channels]</code>?</p>

<p>Here is an example of the current approach, where <code>img</code> has shape (height, width, channels):</p>

<pre><code>img = tf.random_uniform((10,10,3))  # a single image
img = tf.nn.conv2d([img], kernel)[0] # creating a batch of 1, then indexing the single example
</code></pre>

<p>I am reshaping the input as follows:</p>

<p><code>[in_height, in_width, in_channels]-&gt;[1, in_height, in_width, in_channels]-&gt;[in_height, in_width, in_channels]</code> </p>

<p>This feels like an unnecessary and costly operation when I am only interested in transforming one example.</p>

<p>Is there a simple/standard way to do this that doesn't involve reshaping?</p>
","['""""""It seems (here and here) that the first operation creates a copy (someone correct me if I\\\'m wrong). You may use tf.expand_dims instead though, it\\\'s IMO more readable because of it\\\'s verbosity. On the other hand, taking 0 element from the tensor should not perform a copy in this case and is almost free. Most importantly, except for a little inconvenience with syntax (e.g. [0]) those operations definitely are not costly, especially in the context of performing convolution. BTW. Other ready alternative layers like the ones in tf.keras, require batch as first dimension as well.\n\nIt would be extremely helpful if anyone could tell me what process yields this output. input = tf.Variable(tf.random_normal([1,2,2,1])) filter = tf.Variable(tf.random_normal([1,1,1,1])) op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding=\\\'SAME\\\') init = tf.initialize_all_variables() with tf.Session() as sess: sess.run(init) print(""input"") print(input.eval()) print(""filter"") print(filter.eval()) print(""result"") result = sess.run(op) print(result)\\n\\ninput [[[[ 1.60314465] [-0.55022103]] [[ 0.00595062] [-0.69889867]]]] filter [[[[-0.59594476]]]] result [[[[-0.95538563] [ 0.32790133]] [[-0.00354624] [ 0.41650501]]]]\\n\\n Actually cudnn is enabled by default on GPU in tf.nn.conv2d(), so the method in question is not used at all when we use TF with GPU support, unless use_cudnn_on_gpu=False is specified explicitly. n\\nOk I think this is about the simplest way to explain it all.\n\nHow can I apply a TensorFlow 2D Convolution (tf.nn.conv2d) to a single (non-batch) 2D image? \\n\\nI would like to use the function tf.nn.conv2d() on a single image example, but the TensorFlow documentation seems to only mention applying this transformation to a batch of images. The docs mention that the input image must be of shape [batch, in_height, in_width, in_channels] and the kernel must be of shape [filter_height, filter_width, in_channels, out_channels]. However, what is the most straightforward way to achieve 2D convolution with input shape [in_height, in_width, in_channels]? Here is an example of the current approach, where img has shape (height, width, channels):\\n\\nimg = tf.random_uniform((10,10,3)) # a single image img = tf.nn.conv2d([img], kernel)[0] # creating a batch of 1, then indexing the single example\\n\\nI am reshaping the input as follows:\\n\\n[in_height, in_width, in_channels]->[1, in_height, in_width, in_channels]->[in_height, in_width, in_channels]\\n\\nThis feels like an unnecessary and costly operation when I am only interested in transforming one example. Is there a simple/standard way to do this that doesn\\\'t involve reshaping?\n\n Now with ""SAME"" padding:\\n\\ninput = tf.Variable(tf.random_normal([1,5,5,5])) filter = tf.Variable(tf.random_normal([3,3,5,1])) op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding=\\\'SAME\\\')\\n\\nThis gives a 5x5 output image (size 1x5x5x1). This is done by centering the filter at each position on the image. Any of the 5-element dot products where the filter sticks out past the edge of the image get a value of zero. So the corners are only sums of 4, 5-element dot products. Now with multiple filters. input = tf.Variable(tf.random_normal([1,5,5,5])) filter = tf.Variable(tf.random_normal([3,3,5,7])) op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding=\\\'SAME\\\')\\n\\nThis still gives a 5x5 output image, but with 7 channels (size 1x5x5x7). Where each channel is produced by one of the filters in the set. Now with strides 2,2:\\n\\ninput = tf.Variable(tf.random_normal([1,5,5,5])) filter = tf.Variable(tf.random_normal([3,3,5,7])) op = tf.nn.conv2d(input, filter, strides=[1, 2, 2, 1], padding=\\\'SAME\\\')\\n\\nNow the result still has 7 channels, but is only 3x3 (size 1x3x3x7). This is because instead of centering the filters at every point on the image, the filters are centered at every other point on the image, taking steps (strides) of width 2. The x\\\'s below represent the filter center for each output pixel, on the input image. x.x.x ..... x.x.x ..... x.x.x\\n\\nAnd of course the first dimension of the input is the number of images so you can apply it over a batch of 10 images, for example:\\n\\ninput = tf.Variable(tf.random_normal([10,5,5,5])) filter = tf.Variable(tf.random_normal([3,3,5,7])) op = tf.nn.conv2d(input, filter, strides=[1, 2, 2, 1], padding=\\\'SAME\\\')\\n\\nThis performs the same operation, for each image independently, giving a stack of 10 images as the result (size 10x3x3x7)\\n\\n 7\\n\\n@ZijunLost No, the docs state that the first and last element must be 1. Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1]. Is this Toeplitz matrix-based implementation of convolution? Regarding this: ""This still gives a 5x5 output image, but with 7 channels (size 1x5x5x7). Where each channel is produced by one of the filters in the set."", I still have difficulty understanding where the 7 channels are from? what do you mean ""filters in the set""? Thanks. @mdaoust Hi, regarding your second example where the 3x3 image and the 1x1 filter each have 5 channels, I find the result is different from the manually calculated dot product. @derek I have the same question, does the ""output_channel"" same as ""number of filters""??? if so why are they named ""output_channel"" in the tensorflow docs? | Show 2 more comments\\n\\n2D convolution is computed in a similar way one would calculate 1D convolution: you slide your kernel over the input, calculate the element-wise multiplications and sum them up. But instead of your kernel/input being an array, here they are matrices. In the most basic example there is no padding and stride=1. Let\\\'s assume your input and kernel are:\\n\\nWhen you use your kernel you will receive the following output: , which is calculated in the following way:\\n\\n14 = 4 * 1 + 3 * 0 + 1 * 1 + 2 * 2 + 1 * 1 + 0 * 0 + 1 * 0 + 2 * 0 + 4 * 1\\n\\n6 = 3 * 1 + 1 * 0 + 0 * 1 + 1 * 2 + 0 * 1 + 1 * 0 + 2 * 0 + 4 * 0 + 1 * 1\\n\\n6 = 2 * 1 + 1 * 0 + 0 * 1 + 1 * 2 + 2 * 1 + 4 * 0 + 3 * 0 + 1 * 0 + 0 * 1\\n\\n12 = 1 * 1 + 0 * 0 + 1 * 1 + 2 * 2 + 4 * 1 + 1 * 0 + 1 * 0 + 0 * 0 + 2 * 1\\n\\nTF\\\'s conv2d function calculates convolutions in batches and uses a slightly different format. For an input it is [batch, in_height, in_width, in_channels] for the kernel it is [filter_height, filter_width, in_channels, out_channels]. So we need to provide the data in the correct format:\\n\\nimport tensorflow as tf k = tf.constant([ [1, 0, 1], [2, 1, 0], [0, 0, 1] ], dtype=tf.float32, name=\\\'k\\\') i = tf.constant([ [4, 3, 1, 0], [2, 1, 0, 1], [1, 2, 4, 1], [3, 1, 0, 2] ], dtype=tf.float32, name=\\\'i\\\') kernel = tf.reshape(k, [3, 3, 1, 1], name=\\\'kernel\\\') image = tf.reshape(i, [1, 4, 4, 1], name=\\\'image\\\')\\n\\nAfterwards the convolution is computed with:\\n\\nres = tf.squeeze(tf.nn.conv2d(image, kernel, [1, 1, 1, 1], ""VALID"")) # VALID means no padding with tf.Session() as sess: print sess.run(res)\\n\\nAnd will be equivalent to the one we calculated by hand. For examples with padding/strides, take a look here. Salvador DaliSalvador Dali\\n\\n 2\\n\\nNice example, however some links are broken.\n\nOne big motivation is to use dilate and stride at the same time, which seems to work with tf.nn.conv2d, but not with keras Conv2D. For some reasons, keras has not implemented that. """"""']","To apply a TensorFlow 2D Convolution (`tf.nn.conv2d`) to a single (non-batch) 2D image, you need to ensure that the input tensor conforms to the expected shape `[batch, in_height, in_width, in_channels]`. The TensorFlow documentation specifies that the input image must be in this 4-D format, where the first dimension represents the batch size.

For a single image, you can achieve this by reshaping the input tensor to include a batch dimension of size 1. This means converting the shape from `[in_height, in_width, in_channels]` to `[1, in_height, in_width, in_channels]`. After performing the convolution operation, you can remove the batch dimension to revert to the original shape.

Although this reshaping might seem unnecessary and costly, it is required to use the `tf.nn.conv2d` function, as it is designed to operate on batches of images. The reshaping operation itself is not computationally expensive, especially in the context of performing convolutions.

In summary, to apply `tf.nn.conv2d` to a single 2D image, you must reshape the image to include a batch dimension, perform the convolution, and then remove the batch dimension. This process ensures compatibility with the function's requirements without significant performance overhead.","AFAIK there is no way around it. It seems that the first operation creates a copy (someone correct me if I'm wrong). You may use <code>tf.expand_dims</code> instead though, it's IMO more readable because of it's verbosity. On the other hand, taking <code>0</code> element from the tensor should not perform a copy in this case and is almost free. <strong>Most importantly</strong>, except for a little inconvenience with syntax (e.g. <code>[0]</code>) those operations definitely <strong>are not costly</strong>, especially in the context of performing convolution. BTW. Other ready alternative layers like the ones in <code>tf.keras</code>, require batch as first dimension as well.
",0.7317847659,0.8271390635,0.8
"<p>I'm using a tf.data dataset containing my training data consisting of (lets say) 100k images.
I'm also using a tf.data dataset containing my validation set.
Since an epoch of all 100k images takes quite long (in my case approximately one hour) before I get any feedback on performance on the validation set, I set the <code>steps_per_epoch</code> parameter in tf.keras.Model <code>fit()</code> to <code>10000</code>.
Using a batch size of 1 this results into having 10 validation scores when reaching 100k of images.
In order to complete one epoch of 100k images of my entire training dataset, I set the <code>epochs</code> parameter to <code>10</code></p>
<p>However, I'm not sure if using <code>steps_per_epoch</code> and <code>epochs</code> this way has any other consequences. Is it correct to use these parameters in order to get more frequent feedback on performance?
And also a more specific question, does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'?
I already dug into the <a href=""https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">TensorFlow docs</a> and read several different stack overflow questions, but I couldn't find anything conclusive to answer my own question. Hope you can help!</p>
<p>Tensorflow version I'm using is 2.2.0.</p>
","['""""""If we look at the code of the fit method, you will see that the data is handled by a special class, DataHandler. Looking at the code of this class, we see that this is an Adapter class to handle different kind of data. We are interrested in the class that handle tf.data.Dataset, DatasetAdapter, and we can see that this class does not take into account the shuffle parameter :\\n\\ndef __init__(self, x, y=None, sample_weights=None, steps=None, **kwargs): super(DatasetAdapter, self).__init__(x, y, **kwargs) # Note that the dataset instance is immutable, its fine to reuse the user # provided dataset. self._dataset = x # The user-provided steps. self._user_steps = steps self._validate_args(y, sample_weights, steps)\\n\\nIf you want to shuffle your dataset, use the shuffle function from the tf.data.Dataset API. 2\\n\\nThank you for your explanation. To go further, If I want to shuffle my dataset each epoch during training. Would doing dataset.shuffle(len_dataset).batch(batch_size) and model.fit(num_epochs, ....) be enough ? Or do I have have to call, dataset.shuffle(len_dataset).batch(batch_size).repeat() and model.fit(steps_per_epoch=len_dataset//batch_size, ...). Or is it equivalent ? @Matt I am pretty certain that calling shuffle once should reshuffle the dataset between each epoch. It is something you could verify with a toy example.\n\nthere I start doubting this method. As I am currently experimenting with the tf.estimator API I would like to add my dewy findings here, too. I don\\\'t know yet if the usage of steps and epochs parameters is consistent throughout TensorFlow and therefore I am just relating to tf.estimator (specifically tf.estimator.LinearRegressor) for now. Training steps defined by num_epochs: steps not explicitly defined\\n\\nestimator = tf.estimator.LinearRegressor(feature_columns=ft_cols) train_input = tf.estimator.inputs.numpy_input_fn({\\\'x\\\':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True) estimator.train(input_fn=train_input)\\n\\nComment: I have set num_epochs=1 for the training input and the doc entry for numpy_input_fn tells me ""num_epochs: Integer, number of epochs to iterate over data. If None will run forever."". With num_epochs=1 in the above example the training runs exactly x_train.size/batch_size times/steps (in my case this was 175000 steps as x_train had a size of 700000 and batch_size was 4). Training steps defined by num_epochs: steps explicitly defined higher than number of steps implicitly defined by num_epochs=1\\n\\nestimator = tf.estimator.LinearRegressor(feature_columns=ft_cols) train_input = tf.estimator.inputs.numpy_input_fn({\\\'x\\\':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True) estimator.train(input_fn=train_input, steps=200000)\\n\\nComment: num_epochs=1 in my case would mean 175000 steps (x_train.size/batch_size with x_train.size=700,000 and batch_size=4) and this is exactly the number of steps estimator.train albeit the steps parameter was set to 200,000 estimator.train(input_fn=train_input, steps=200000). Training steps defined by steps\\n\\nestimator = tf.estimator.LinearRegressor(feature_columns=ft_cols) train_input = tf.estimator.inputs.numpy_input_fn({\\\'x\\\':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True) estimator.train(input_fn=train_input, steps=1000)\\n\\nComment: Although I have set num_epochs=1 when calling numpy_input_fnthe training stops after 1000 steps. This is because steps=1000 in estimator.train(input_fn=train_input, steps=1000) overwrites the num_epochs=1 in tf.estimator.inputs.numpy_input_fn({\\\'x\\\':x_train},y_train,batch_size=4,num_epochs=1,shuffle=True). Conclusion: Whatever the parameters num_epochs for tf.estimator.inputs.numpy_input_fn and steps for estimator.train define, the lower bound determines the number of steps which will be run through. 1,0\\n\\nEpoch: A training epoch represents a complete use of all training data for gradients calculation and optimizations(train the model). Step: A training step means using one batch size of training data to train the model. Number of training steps per epoch: total_number_of_training_examples / batch_size. Total number of training steps: number_of_epochs x Number of training steps per epoch. 1\\n\\nJust to add on to this, if there is a validation set of size V, then the number of training steps per epoch is (total_number_of_training_examples - V)/batch_size\\n\\n\\n\\nAccording to Google\\\'s Machine Learning Glossary, an epoch is defined as\\n\\n""A full training pass over the entire dataset such that each example has been seen once. Thus, an epoch represents N/batch_size training iterations, where N is the total number of examples.""\\n\\nIf you are training model for 10 epochs with batch size 6, given total 12 samples that means:\\n\\nthe model will be able to see whole dataset in 2 iterations ( 12 / 6 = 2) i.e. single epoch. overall, the model will have 2 X 10 = 20 iterations (iterations-per-epoch X no-of-epochs)\\n\\nre-evaluation of loss and model parameters will be performed after each iteration!  By default an epoch run over all your training data. In this case you have n steps, with n = Training_lenght / batch_size. If your training data is too big you can decide to limit the number of steps during an epoch.[https://www.tensorflow.org/tutorials/structured_data/time_series?_sm_byp=iVVF1rD6n2Q68VSN]\\n\\nWhen the number of steps reaches the limit that you\'ve set the process will start over, beginning the next epoch. When working in TF, your data is usually transformed first into a list of batches that will be fed to the model for training. At each step you process one batch. As to whether it\'s better to set 1000 steps for 1 epoch or 100 steps with 10 epochs I don\'t know if there\'s a straight answer. But here are results on training a CNN with both approaches using TensorFlow timeseries data tutorials :\\n\\nIn this case, both approaches lead to very similar prediction, only the training profiles differ. steps = 20 / epochs = 100\\n\\nsteps = 200 / epochs = 10\\n\\nYoan B. M.ScYoan B. M.Sc\\n\\n1,Divide the length of x_train by the batch size with\\n\\nsteps_per_epoch = x_train.shape[0] // batch_size\\n\\n\\n\\nWe split the training set into many batches. When we run the algorithm, it requires one epoch to analyze the full training set. An epoch is composed of many iterations (or batches). Iterations: the number of batches needed to complete one Epoch. Batch Size: The number of training samples used in one iteration. Epoch: one full cycle through the training dataset. A cycle is composed of many iterations. Number of Steps per Epoch = (Total Number of Training Samples) / (Batch Size)\\n\\nExample Training Set = 2,000 images Batch Size = 10\\n\\nNumber of Steps per Epoch = 2,000 / 10 = 200 steps\\n\\nHope this helps for better understanding. Tejas BadheTejas Badhe\\n\\nIn a nutshell the expression can be denoted as follows\\n\\n the above expression would be used calculate the steps\\n\\nand would be used conversantly to calculate the epochs.\n\nCombining Keras model.fit\\\'s `steps_per_epoch` with TensorFlow\\\'s Dataset API\\\'s `batch()`\\n\\n I\\\'m looking at the performance and GPU usage during training of a CNN model with Keras+TensorFlow. Similar to this question, I\\\'m having a hard time to understand the combined use of Keras model.fit\\\'s steps_per_epoch and TensorFlow\\\'s Dataset API\\\'s .batch(): I set a certain batch size on the input pipeline dataset = dataset.batch(batch_size) and later I use\\n\\nfit = model.fit(dataset, epochs=num_epochs, steps_per_epoch=training_set_size//batch_size)\\n\\nbut I see that one can actually set any number of steps per epoch, even more than training_set_size//batch_size. From the documentation I understand that on Keras an epoch is not necessarily a pass over the entire training set as usually, but anyway I\\\'m a bit confused and now I\\\'m not entirely sure if I\\\'m using it right. Is dataset.batch(batch_size) + steps_per_epoch=training_set_size//batch_size defining a minibatch SGD that runs over the entire training set by minibatches of batch_size samples? Are epochs larger than one pass over the training set if steps_per_epoch is set to more than training_set_size//batch_size? steps_per_epoch is the number of batches of your set batch size is ran through the network in one epoch. You have set your steps_per_epoch to be training_set_size//batch_size for a good reason. This ensures all data are trained upon in one epoch, providing the number divides exactly (if not it rounds by the // operator). That is to say if you had a batch size of 10 and a training set size of 30, then steps_per_epoch = 3 ensures all data are used. And to quote your question:\\n\\n""Are epochs larger than one pass over the training set if steps_per_epoch is set to more than training_set_size//batch_size?""\\n\\nYes. Some data will be passed through again in the same epoch.\n\nperhaps one could imagine a scenario where the shuffling is defined once and is used for every iteration. @jakub ah yes it makes sense from that perspective\\n\\n\\n\\nThis is because the data files are shuffled and the dataset is shuffled with dataset.shuffle(). With dataset.shuffle(), the data will be shuffled in a different way on each iteration by default. One can remove shuffle_files=True and set the argument reshuffle_each_iteration=False to prevent reshuffling on different iterations. The .take() function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them. # Construct a tf.data.Dataset ds = tfds.load(\\\'mnist\\\', split=\\\'train\\\', shuffle_files=False) # Build your input pipeline ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE) single_batch_dataset = ds.take(1) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label) for example in single_batch_dataset: image, label = example[""image""], example[""label""] print(label)\\n\\ntf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64) tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)""""""']","### Explanation of `steps_per_epoch` and `epochs` Parameters in `tf.keras.Model.fit()`

The question addresses the behavior of the `steps_per_epoch` and `epochs` parameters in the `tf.keras.Model.fit()` method, specifically in the context of training a model on a large dataset and obtaining more frequent feedback on validation performance.

#### Understanding `steps_per_epoch` and `epochs`

1. **`steps_per_epoch`**: This parameter defines the number of batches of samples to be used in one epoch. If you set `steps_per_epoch=10000` with a batch size of 1, it means that each epoch will consist of 10,000 steps, where each step processes one image. This allows you to get intermediate feedback on the model's performance without having to wait for the entire dataset to be processed.

2. **`epochs`**: This parameter defines the number of times the entire dataset is passed through the model. If you set `epochs=10`, it means the model will go through the dataset 10 times. However, since you have set `steps_per_epoch=10000`, the model will process 10,000 images per epoch, and over 10 epochs, it will process a total of 100,000 images.

#### Behavior and Consequences

- **Frequent Feedback**: By setting `steps_per_epoch=10000`, you effectively get validation feedback after every 10,000 images, rather than waiting for the entire 100,000 images to be processed. This can be useful for monitoring the model's performance more frequently and making adjustments if necessary.

- **Data Usage**: The `steps_per_epoch` parameter does not limit the model to using only the first 10,000 images repeatedly. Instead, it processes the dataset in batches, and if the dataset is shuffled (which is recommended), different subsets of the data will be used in each epoch. This ensures that the model sees the entire dataset over the course of training.

- **Shuffling**: If the dataset is shuffled using the `tf.data.Dataset.shuffle()` method, the model will see different batches of data in each epoch, even if `steps_per_epoch` is set to a smaller number than the total dataset size. This helps in better generalization and prevents the model from overfitting to a specific subset of the data.

#### Conclusion

Using `steps_per_epoch` and `epochs` in this manner is a valid approach to obtain more frequent feedback on the model's performance. It allows you to monitor validation metrics more frequently without having to wait for the entire dataset to be processed. Additionally, if the dataset is shuffled, the model will see different subsets of the data in each epoch, ensuring that it eventually processes the entire dataset over the course of training. This approach does not have any adverse consequences and can be beneficial for large datasets where training time per epoch is significant.","Is it correct to use these parameters in order to get more frequent feedback on performance? Yes, it is correct to use these parameters. Here is the code that i used to fit the model.
Does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'?
It use all images in your training data. For better understanding <code>Epoch</code> is the number times the learning algorithm will work through the entire training data set. Where as <code>steps_per_epoch</code> is the total number of samples in your training data set divided by the batch size. For example, if you have 100000 training samples and use a batch size of 100, one epoch will be equivalent to 1000 steps_per_epoch. Note: We generally observe batch size to be the power of 2, this is because of the effective work of optimized matrix operation libraries.",0.6221809962,0.8887239848,0.8123661327
"<p>I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop()</code></a>.</p>

<p>My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by</p>

<pre><code>import numpy as np
import tensorflow as tf
IMAGE_SHAPE = [960, 720]
CROP_SHAPE = [320, 240]
max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
</code></pre>

<p>and the condition is</p>

<pre><code>cond = tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)
</code></pre>

<p>Going over the documentation and examples of <code>tf.while_loop(cond, body, loop_vars, ...)</code>, what I understand is that both <code>cond</code> and <code>body</code> should take the same arguments given in <code>loop_vars</code>.
I don't see how I can have <code>cond</code> depend on <code>img_crop</code> which would be calculated inside <code>body</code>, and isn't provided in <code>loop_vars</code>.</p>

<p>I could equivalently compute <code>cond</code> using <code>crop_begin_index</code> without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem.</p>

<p>Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use <code>tf.while_loop()</code>?</p>
","['""""""TensorFlow while loop with condition dependent on body\\n\\n I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don\\\'t know how to accomplish this with tf.while_loop(). My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by\\n\\nimport numpy as np import tensorflow as tf IMAGE_SHAPE = [960, 720] CROP_SHAPE = [320, 240] max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE) crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index) img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])\\n\\nand the condition is\\n\\ncond = tf.count_nonzero(img_crop > 0) > 0.5 * tf.size(img_crop)\\n\\nGoing over the documentation and examples of tf.while_loop(cond, body, loop_vars, ...), what I understand is that both cond and body should take the same arguments given in loop_vars. I don\\\'t see how I can have cond depend on img_crop which would be calculated inside body, and isn\\\'t provided in loop_vars. I could equivalently compute cond using crop_begin_index without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem. Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use tf.while_loop()? The arguments that are passed on to the condition function are the arguments returned from your body function. So you just have to return that value that you want to base your condition on in the body function, then carry out the condition on that value in your cond function. Something like,\\n\\ndef body(image_shape, crop_shape, img_crop): max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE) crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index) img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1]) return (image_shape, crop_shape, img_crop) def cond(image_shape, crop_shape, img_crop): return tf.count_nonzero(img_crop > 0) > 0.5 * tf.size(img_crop) image_shape, crop_shape, img_crop = tf.while_loop(cond=cond, body=body, loop_vars=([960, 720], [320, 240], img_crop))\\n\\nDon\\\'t have access to an interpreter right now, so there might be some syntax problems there, but something like that. Also, if I recall correctly, the body and the condition need to be pure functions, you cannot alter the outer state from within the functions. Also note, you\\\'ll need to specify some initial value for img_crop in the loop vars. Moreover, by default, tf.while_loop expects the shapes of all the loop_vars to remain the same across all loop runs. You can modify this through the shape_invariants argument. \\n\\nGot it. I was also able to use just crop_begin_index as a variable like I wrote so the loop is more concise. \n\nCan anyone give me few more complex example? Also, in such case that if the future computation is based on the tensor output (ex: the RNN stop based on the output criterion), which is very common case. Is there an elegant way or better way instead of dynamic graph?. I\\\'m facing a problem where the gradients are None while implementing SimpleRNN using subclassing. I\\\'m wondering whether I need symbolic loop or can I manage without it? What is stopping you from adding more functionality to the body? You can build whatever complex computational graph you like in the body and take whatever inputs you like from the enclosing graph. Also, outside of the loop, you can then do whatever you want with whatever outputs you return. As you can see from the amount of \\\'whatevers\\\', TensorFlow\\\'s control flow primitives were built with much generality in mind. Below is another \\\'simple\\\' example, in case it helps. import tensorflow as tf import numpy as np def body(x): a = tf.random_uniform(shape=[2, 2], dtype=tf.int32, maxval=100) b = tf.constant(np.array([[1, 2], [3, 4]]), dtype=tf.int32) c = a + b return tf.nn.relu(x + c) def condition(x): return tf.reduce_sum(x) < 100 x = tf.Variable(tf.constant(0, shape=[2, 2])) with tf.Session(): tf.global_variables_initializer().run() result = tf.while_loop(condition, body, [x]) print(result.eval())\\n\\n My problem is the condition I need is calculated after run the body once. So it is like I need to use the return value of body as a parameter for condition. What you want is precisely what happens. The loop is while(condition(tensors)) { tensors = body(tensors); }, so the tensors you pass are updated to the tensors returned by the body each time, and then those updated tensors are passed to condition. The only time condition is called before body is the very first time, before the body of the pseudo-code loop above is entered. However, in that case, it\\\'s just about initializing the tensors you pass in loop_vars correctly. For example, you could pass the result of body as the loop_vars tensors to while_loop. oh, with loop_vars I refer to the function definition of while_loop, which is while_loop(condition, body, loop_vars) (its the tensors that are passed to condition and body)""""""']","To address the question of implementing a TensorFlow while loop with a condition dependent on a tensor computed within the loop body, it is essential to understand how `tf.while_loop` operates. The `tf.while_loop` function allows for the creation of a loop in which the body of the loop and the condition for continuation are defined by separate callable functions. Both the condition (`cond`) and the body (`body`) functions must take the same arguments, which are provided in the `loop_vars`.

In the given scenario, the user wants to perform random cropping on an image and continue this process until a crop of sufficient quality is obtained. The quality of the crop is determined by a condition that depends on the cropped image itself, which is computed within the loop body.

To achieve this, the key is to ensure that the tensor representing the cropped image (`img_crop`) is included in the `loop_vars` so that it can be passed between the `body` and `cond` functions. The `body` function will perform the random cropping and return the updated `img_crop` along with any other loop variables. The `cond` function will then evaluate the quality of the `img_crop` and determine whether the loop should continue.

The `tf.while_loop` function will call the `cond` function with the initial values of `loop_vars` before entering the loop. After each iteration, the updated values of `loop_vars` returned by the `body` function are passed back to the `cond` function to check if the loop should continue. This process ensures that the condition can depend on the tensor computed within the loop body.

Additionally, it is important to initialize `img_crop` with an appropriate initial value and ensure that the shapes of all loop variables remain consistent across iterations. If the shapes of the loop variables may change, the `shape_invariants` argument can be used to specify less specific shape invariants.

In summary, the solution involves defining the `body` function to perform the random cropping and return the updated `img_crop`, and the `cond` function to evaluate the quality of `img_crop`. By including `img_crop` in the `loop_vars`, the `tf.while_loop` can effectively manage the dependency of the condition on the tensor computed within the loop body.","The arguments that are passed on to the <code>condition</code> function are the arguments returned from your <code>body</code> function. So you just have to return that value that you want to base your condition on in the <code>body</code> function, then carry out the condition on that value in your <code>cond</code> function.
Also, if I recall correctly, the body and the condition need to be pure functions, you cannot alter the outer state from within the functions. Also note, you'll need to specify some initial value for <code>img_crop</code> in the loop vars. Moreover, by default, <code>tf.while_loop</code> expects the shapes of all the <code>loop_vars</code> to remain the same across all loop runs. You can modify this through the <code>shape_invariants</code> argument.
",0.9785714887,0.9142859547,1