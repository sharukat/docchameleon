QuestionId,Title,Body,CreationDate,AcceptedAnswer,UserID,User Reputation,QuestionURL,IssueType
38033079,Tensorflow understanding tf.train.shuffle_batch,"<p>I have a single file of training data, about 100K rows, and I'm running a straightforward <code>tf.train.GradientDescentOptimizer</code> on each training step.  The setup is essentially taken directly from Tensorflow's MNIST example.  Code reproduced below:</p>

<pre><code>x = tf.placeholder(tf.float32, [None, 21])
W = tf.Variable(tf.zeros([21, 2]))
b = tf.Variable(tf.zeros([2]))
y = tf.nn.softmax(tf.matmul(x, W) + b)

y_ = tf.placeholder(tf.float32, [None, 2])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
</code></pre>

<p>Given that I'm reading training data from a file, I'm using <code>tf.train.string_input_producer</code> and <code>tf.decode_csv</code> to read rows from the csv, and then <code>tf.train.shuffle_batch</code> to create batches that I then train on.</p>

<p>I'm confused as to what my parameters should be for <code>tf.train.shuffle_batch</code>.  I read Tensorflow's documentation, and yet I'm still not sure what the ""optimal"" batch_size, capacity, and min_after_dequeue values are.  Can anyone help shed some light on how I go about choosing proper values for these parameters, or link me to a resource where I can learn more?  Thanks-- </p>

<p>Here's the API link: <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#shuffle_batch"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#shuffle_batch</a></p>
",2016-06-25 20:59:00,"<p>There is a little bit about the number of threads to use at</p>

<p><a href=""https://www.tensorflow.org/versions/r0.9/how_tos/reading_data/index.html#batching"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.9/how_tos/reading_data/index.html#batching</a></p>

<p>Unfortunately, I don't think there is a simple answer for batch sizes.
The efficient batch size for a network depends on a lot of details
about the network. In practice, if you care about optimal performance
you're going to need to do a bunch of trial and error (maybe starting
from the values used by a similar network).</p>
",831938,5328,https://stackoverflow.com/questions/38033079,Requesting (Additional) Documentation/Examples
37979238,TensorFlow: product equivalent of tf.add_n,"<p>Is there a product equivalent of tf.add_n, returning the elementwise product of a list of tensors?</p>
",2016-06-22 22:24:18,"<p>Solution 1:</p>

<p>You can use higher order functions tf.foldl and tf.foldr. Here is an example:</p>

<pre><code>x = tf.constant([5, 2, 4, 3])
y = tf.constant([2, 2, 1, 6])
z = tf.constant([24, 2, 1, 6])

xyz=[x,y,z]
product = tf.foldl(tf.mul, xyz) 

with tf.Session() as sess:
    print product.eval()
</code></pre>

<p>Results:
[240   8   4 108]</p>

<p>Solution 2:
You can use tf.reduce_prod:</p>

<pre><code>x = tf.constant([5, 2, 4, 3])
y = tf.constant([2, 2, 1, 6])
z = tf.constant([24, 2, 1, 6])

x=tf.reshape(x,[1,-1])
y=tf.reshape(y,[1,-1])
z=tf.reshape(z,[1,-1])
xyz=tf.concat(concat_dim=0, values=[x,y,z])

product = tf.reduce_prod(xyz, reduction_indices=0) 

with tf.Session() as sess:
    print xyz.eval()
    print product.eval()
</code></pre>

<p>Results:</p>

<p>xyz
[[ 5  2  4  3]</p>

<p>[ 2  2  1  6]</p>

<p>[24  2  1  6]]</p>

<p>product
[240   8   4 108]</p>
",5640923,71,https://stackoverflow.com/questions/37979238,Documentation Replicability
37900780,In tensorflow what is the difference between tf.add and operator (+)?,"<p>In tensorflow tutorials, I see both codes like <code>tf.add(tf.matmul(X, W), b)</code> and <code>tf.matmul(X, W) + b</code>, what is the difference between using the math function <code>tf.add()</code>, <code>tf.assign()</code>, etc and the operators <code>+</code> and <code>=</code>, etc, in precision or other aspects? </p>
",2016-06-18 19:19:27,"<p>There's no difference in precision between <code>a+b</code> and <code>tf.add(a, b)</code>. The former translates to <code>a.__add__(b)</code> which gets mapped to <code>tf.add</code> by means of <a href=""https://github.com/tensorflow/tensorflow/blob/c43a32d5d0929170a057862e2cd0b59308421444/tensorflow/python/ops/math_ops.py#L845"" rel=""noreferrer"">following line</a> in math_ops.py </p>

<p><code>_OverrideBinaryOperatorHelper(gen_math_ops.add, ""add"")</code></p>

<p>The only difference is that node name in the underlying Graph is <code>add</code> instead of <code>Add</code>. You can generally compare things by looking at the underlying Graph representation like this</p>

<pre><code>tf.reset_default_graph()
dtype = tf.int32
a = tf.placeholder(dtype)
b = tf.placeholder(dtype)
c = a+b
print(tf.get_default_graph().as_graph_def())
</code></pre>

<p>You could also see this directly by inspecting the <code>__add__</code> method. There's an extra level of indirection because it's a closure, but you can get the underlying function as follows</p>

<pre><code>real_function = tf.Tensor.__add__.im_func.func_closure[0].cell_contents
print(real_function.__module__ + ""."" + real_function.__name__)
print(tf.add.__module__ + ""."" + tf.add.__name__)
</code></pre>

<p>And you'll see output below which means that they call same underlying function</p>

<pre><code>tensorflow.python.ops.gen_math_ops.add
tensorflow.python.ops.gen_math_ops.add
</code></pre>

<p>You can see from <code>tf.Tensor.OVERLOADABLE_OPERATORS</code> that following Python special methods are potentially overloaded by appropriate TensorFlow versions</p>

<pre><code>{'__abs__',
 '__add__',
 '__and__',
 '__div__',
 '__floordiv__',
 '__ge__',
 '__getitem__',
 '__gt__',
 '__invert__',
 '__le__',
 '__lt__',
 '__mod__',
 '__mul__',
 '__neg__',
 '__or__',
 '__pow__',
 '__radd__',
 '__rand__',
 '__rdiv__',
 '__rfloordiv__',
 '__rmod__',
 '__rmul__',
 '__ror__',
 '__rpow__',
 '__rsub__',
 '__rtruediv__',
 '__rxor__',
 '__sub__',
 '__truediv__',
 '__xor__'}
</code></pre>

<p>Those methods are described in <a href=""https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types"" rel=""noreferrer"">Python reference 3.3.7</a>: emulating numeric types. Note that Python data model does not provide a way to overload assignment operator <code>=</code> so assignment always uses native Python implementation.</p>
",1429955,847,https://stackoverflow.com/questions/37900780,Documentation Replicability
37376861,what does the tf.nn.lrn() method do?,"<p>Here is the code-snipped from the cifar10-tutorial. It's from the cifar10.py.</p>

<pre><code># conv1
with tf.variable_scope('conv1') as scope:
kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64],
                                     stddev=1e-4, wd=0.0)
conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')
biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))
bias = tf.nn.bias_add(conv, biases)
conv1 = tf.nn.relu(bias, name=scope.name)
_activation_summary(conv1)

# pool1
pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],
                     padding='SAME', name='pool1')
# norm1
norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,
                name='norm1')
</code></pre>

<p>What does the tf.nn.lrn-Method do? I can't find a definition in the API Documentation on <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/index.html"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.8/api_docs/python/index.html</a></p>
",2016-05-22 16:35:20,"<p><code>tf.nn.lrn</code> is a short for <code>tf.nn.local_response_normalization</code>.
Therefore, the documentation you may want to look at is: <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization</a></p>
",3019308,197,https://stackoverflow.com/questions/37376861,Lack of Alternative Solutions/Documentation
37234114,How to do gradient descent for not all variables in tensorflow,"<p>In tensorflow, tf.train.GradientDescentOptimizer does gradient descent for all variables in default. Can i just do gradient descent for only a few of my variables and 'lock' the others?</p>
",2016-05-15 02:59:57,"<p>To lock the ones that you don't want to train you can use <code>tf.Variable(..., trainable=False)</code></p>
",5881181,47,https://stackoverflow.com/questions/37234114,Documentation Replicability
37044006,Tensorflow conditional throwing value error,"<p>I am trying to use conditionals with tensorflow and I am getting the error:</p>

<pre><code>ValueError: Shapes (1,) and () are not compatible
</code></pre>

<p>Below is the code I use that is throwing the error.
It is saying the error is in the conditional</p>

<pre><code>import tensorflow as tf
import numpy as np

X = tf.constant([1, 0])
Y = tf.constant([0, 1])
BOTH = tf.constant([1, 1])
WORKING = tf.constant(1)

def create_mult_func(tf, amount, list):
    def f1():
        return tf.scalar_mul(amount, list)
    return f1

def create_no_op_func(tensor):
    def f1():
        return tensor
    return f1

def stretch(tf, points, dim, amount):
    """"""points is a 2 by ??? tensor, dim is a 1 by 2 tensor, amount is tensor scalor""""""
    x_list, y_list = tf.split(0, 2, points)
    x_stretch, y_stretch = tf.split(1, 2, dim)
    is_stretch_X = tf.equal(x_stretch, WORKING, name=""is_stretch_x"")
    is_stretch_Y = tf.equal(y_stretch, WORKING, name=""is_stretch_Y"")
    x_list_stretched = tf.cond(is_stretch_X,
                               create_mult_func(tf, amount, x_list), create_no_op_func(x_list))
    y_list_stretched = tf.cond(is_stretch_Y,
                               create_mult_func(tf, amount, y_list), create_no_op_func(y_list))
    return tf.concat(1, [x_list_stretched, y_list_stretched])

example_points = np.array([[1, 1], [2, 2], [3, 3]], dtype=np.float32)
example_point_list = tf.placeholder(tf.float32)

result = stretch(tf, example_point_list, X, 1)
sess = tf.Session()

with tf.Session() as sess:
    result = sess.run(result, feed_dict={example_point_list: example_points})
    print(result)
</code></pre>

<p>Stack trace:</p>

<pre><code>  File ""/path/test2.py"", line 36, in &lt;module&gt;
    result = stretch(tf, example_point_list, X, 1)
  File ""/path/test2.py"", line 28, in stretch
    create_mult_func(tf, amount, x_list), create_no_op_func(x_list))
  File ""/path/tensorflow/python/ops/control_flow_ops.py"", line 1142, in cond
    p_2, p_1 = switch(pred, pred)
  File ""/path/tensorflow/python/ops/control_flow_ops.py"", line 203, in switch
    return gen_control_flow_ops._switch(data, pred, name=name)
  File ""/path/tensorflow/python/ops/gen_control_flow_ops.py"", line 297, in _switch
    return _op_def_lib.apply_op(""Switch"", data=data, pred=pred, name=name)
  File ""/path/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)
  File ""/path/tensorflow/python/framework/ops.py"", line 2156, in create_op
    set_shapes_for_outputs(ret)
  File ""/path/tensorflow/python/framework/ops.py"", line 1612, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/path/tensorflow/python/ops/control_flow_ops.py"", line 2032, in _SwitchShape
    unused_pred_shape = op.inputs[1].get_shape().merge_with(tensor_shape.scalar())
  File ""/path/tensorflow/python/framework/tensor_shape.py"", line 554, in merge_with
    (self, other))
ValueError: Shapes (1,) and () are not compatible
</code></pre>

<p>I have tried changing the WORKING to be an array instead of a scalar.</p>

<p>I believe that the problem is that <code>tf.equal</code> is returning an <code>int32</code> instead of the bool that it is supposed to return according to the documentation</p>
",2016-05-05 06:37:10,"<p>The problem is in the first argument to <code>tf.cond</code>. From the documentation <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/control_flow_ops.html#cond"">here</a>, about the type of the first argument to <code>tf.cond</code> :</p>

<pre><code>pred: A scalar determining whether to return the result of fn1 or fn2.
</code></pre>

<p>Note that it has to be a <strong>scalar</strong>. You are using the result of comparing a tensor and a tensor, which gives you a <code>(1,)</code> <strong>tensor</strong>, <em>NOT</em> a scalar. You can convert it to a scalar using the <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#reshape""><code>tf.reshape</code></a> operator as follows :</p>

<pre><code>t = tf.equal(x_stretch, WORKING, name=""is_stretch_x"")
x_list_stretched = tf.cond(tf.reshape(t, []),
                           create_mult_func(tf, amount, x_list), create_no_op_func(x_list))
</code></pre>

<p>Complete working program :</p>

<pre><code>import tensorflow as tf
import numpy as np

X = tf.constant([1, 0])
Y = tf.constant([0, 1])
BOTH = tf.constant([1, 1])
WORKING = tf.constant(1)

def create_mult_func(tf, amount, list):
    def f1():
        return tf.scalar_mul(amount, list)
    return f1

def create_no_op_func(tensor):
    def f1():
        return tensor
    return f1

def stretch(tf, points, dim, amount):
    """"""points is a 2 by ??? tensor, dim is a 1 by 2 tensor, amount is tensor scalor""""""
    x_list, y_list = tf.split(0, 2, points)
    x_stretch, y_stretch = tf.split(0, 2, dim)
    is_stretch_X = tf.equal(x_stretch, WORKING, name=""is_stretch_x"")
    is_stretch_Y = tf.equal(y_stretch, WORKING, name=""is_stretch_Y"")
    x_list_stretched = tf.cond(tf.reshape(is_stretch_X, []),
                               create_mult_func(tf, amount, x_list), create_no_op_func(x_list))
    y_list_stretched = tf.cond(tf.reshape(is_stretch_Y, []),
                               create_mult_func(tf, amount, y_list), create_no_op_func(y_list))
    return tf.pack([x_list_stretched, y_list_stretched])

example_points = np.array([[1, 1], [2, 2]], dtype=np.float32)
example_point_list = tf.placeholder(tf.float32)

result = stretch(tf, example_point_list, X, 1)
sess = tf.Session()

with tf.Session() as sess:
    result = sess.run(result, feed_dict={example_point_list: example_points})
    print(result)
</code></pre>
",2187510,1596,https://stackoverflow.com/questions/37044006,Documentation Ambiguity
36620995,How to use L2 pooling in Tensorflow?,"<p>I am trying to implement one CNN architecture that uses L2 pooling. The reference paper particularly argues that L2 pooling was better than max pooling, so I would like to try L2 pooling after the activation function.</p>
<p>However, Tensorflow seems to provide only <code>tf.nn.avg_pool</code> / <code>tf.nn.max_pooling</code> / <code>tf.nn.max_pool_with_argmax</code>.</p>
<p>Is there a way to implement L2 pooling in Tensorflow?</p>
<pre><code>conv = tf.....
h = tf.nn.tanh(conv)
p = tf.pow(tf.nn.ave_pool(tf.pow(h,2)), 0.5)
</code></pre>
<p>Will this be equivalent? Will this work well in terms of backpropagation?</p>
",2016-04-14 10:55:25,"<p>For those who may wonder, when I tried as in Yaroslav Bulatov's response, I could see better performance:</p>
<pre><code>tf.sqrt(tf.nn.ave_pool(tf.square(h))
</code></pre>
",3251207,2148,https://stackoverflow.com/questions/36620995,Inadequate Examples
36223157,Set weight and bias tensors of tensorflow conv2d operation,"<p>I have been given a trained neural network in torch and I need to rebuild it exactly in tensorflow. I believe I have correctly defined the network's architecture in tensorflow but I am having trouble transferring the weight and bias tensors. Using a third party package, I converted all the weight and bias tensors from the torch network to numpy arrays then wrote them to disk. I can load them back into my python program but I cannot figure out a way to assign them to the corresponding layers in my tensorflow network. </p>

<p>For instance, I have a convolution layer defined in tensorflow as</p>

<pre><code>kernel_1 = tf.Variable(tf.truncated_normal([11,11,3,64], stddev=0.1))
conv_kernel_1 = tf.nn.conv2d(input, kernel_1, [1,4,4,1], padding='SAME')
biases_1 = tf.Variable(tf.zeros[64])
bias_layer_1 = tf.nn_add(conv_kernel_1, biases_1)
</code></pre>

<p>According to the tensorflow documentation, the tf.nn.conv2d operation uses the shape defined in the kernel_1 variable to construct the weight tensor. However, I cannot figure out how to access that weight tensor to set it to the weight array I have loaded from file. </p>

<p><strong>Is it possible to explicitly set the weight tensor? And if so, how?</strong> </p>

<p>(The same question applies to bias tensor.)</p>
",2016-03-25 15:45:42,"<p>If you have the weights and biases in a NumPy array, it should be easy to connect them into your TensorFlow network:</p>

<pre><code>weights_1_array = ...  # ndarray of weights for layer 1
biases_1_array = ...  # ndarray of biases for layer 1

conv_kernel_1 = tf.nn.conv2d(input, weights_1_array, [1, 4, 4, 1], padding='SAME')
bias_layer_1 = tf.nn.bias_add(conv_kernel_1, biases_1_array)
</code></pre>

<p>Note that you must ensure that <code>weights_1_array</code> and <code>biases_1_array</code> are in the correct data format. See the documentation for <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#conv2d"" rel=""noreferrer""><code>tf.nn.conv2d()</code></a> for an explanation of the required filter shape.</p>
",6114511,133,https://stackoverflow.com/questions/36223157,Lack of Alternative Solutions/Documentation
35958139,TensorFlow: how can I sum a list of tf.Variables?,"<p>I've got a 3D array that is of <code>tf.Variable</code> type. <code>tf.reduce_sum</code> only works on individual tensors. I've tried doing:</p>

<pre><code>tf.reduce_sum([tf.reduce_sum(mat) for mat in var_3Dlist])
</code></pre>

<p>...but <code>tf.reduce_sum()</code> expects a tensor and not a list. Can I convert it to a tensor somehow, or is there another, better way to do it? I haven't found anything in the API.</p>
",2016-03-12 13:19:15,"<p>The <a href=""https://www.tensorflow.org/versions/r0.7/api_docs/python/math_ops.html#reduce_sum"" rel=""noreferrer""><code>tf.reduce_sum()</code></a> op works on 3-D tensors and variables (and in general any rank or tensor or variable). However, if you have a <strong>list</strong> of 2-D tensors (or variables), you should use the <a href=""https://www.tensorflow.org/versions/r0.7/api_docs/python/math_ops.html#add_n"" rel=""noreferrer""><code>tf.add_n()</code></a> op to add together the values in the list:</p>

<pre><code>var_3Dlist = ...  # List of 3-D variables.
sum_list = [tf.reduce_sum(mat) for mat in var_3Dlist]
sum = tf.add_n(sum_list)
</code></pre>
",4829166,2248,https://stackoverflow.com/questions/35958139,Documentation Ambiguity
35689547,How to process single training file in parallel,"<p>I have a file <code>train.csv</code> that contains paths to images and their labels. ie:</p>

<pre><code>img1.jpg 3
img2.jpg 1
...
</code></pre>

<p>After going through the <a href=""https://www.tensorflow.org/versions/r0.7/how_tos/reading_data/index.html"" rel=""nofollow"">reading data tutorial</a> I came up with some code to go through each image, resize it and apply distortions:</p>

<pre><code>def apply_distortions(resized_image):
    # do a bunch of tf.image distortion...
    return float_image

def processing(filename):
    file_contents = tf.read_file(filename)
    image = tf.image.decode_jpeg(file_contents, channels=3)
    resized_image = tf.image.resize_images(image, 299, 299)
    distorted_image = apply_distortions(resized_image)
    return distorted_image

def parse_csv(filename_queue):
    line_reader = tf.TextLineReader()
    key, line = line_reader.read(filename_queue)
    filename, label = tf.decode_csv(line,     # line_batch or line (depending if you want to batch)
                               record_defaults=[tf.constant([],dtype=tf.string),
                                                tf.constant([],dtype=tf.int32)],
                               field_delim=' ')
    processed_image = processing(filename)
    return processed_image, label
</code></pre>

<p>The problem now is that I'm confused how to do these operations across the file in parallel. The documentation suggests either using <code>tf.train.batch_join</code> or <code>tf.train.batch</code> with num_threads=N.</p>

<p>I first tried following the example code using <code>tf.train.batch_join</code> but this seems to be intended for processing multiple files in parallel. In my case however I just have 1 file. </p>

<pre><code>filename_queue = tf.train.string_input_producer([""train.txt""], num_epochs=1, shuffle=True)    
example_list = [parse_csv(filename_queue) for _ in range(8)]
example_batch, label_batch = tf.train.batch_join(example_list, batch_size)
</code></pre>

<p>I also tried setting <code>tf.train.batch([example, label], batch_size, num_threads=8)</code> but its not clear to me if this is doing the right thing (although I can see more cpu cores in use)</p>

<pre><code>filename_queue = tf.train.string_input_producer([""train.txt""], num_epochs=1, shuffle=True)
example, label = parse_csv(filename_queue)
example_batch, label_batch = tf.train.batch([example, label], batch_size, num_threads=8)
</code></pre>

<p>Here is my code for executing the graph:</p>

<pre><code>sess.run(tf.initialize_all_variables())
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess,coord)
try:
    while not coord.should_stop():
        X, Y = sess.run([example_batch, label_batch])
        # Now run a training step
except tf.errors.OutOfRangeError:
    print('Done training -- epoch limit reached')
finally:
    # When done, ask the threads to stop.
    coord.request_stop()
coord.join(threads)
sess.close()
</code></pre>

<p>Whats the best way to process this file in parallel?</p>
",2016-02-28 23:19:26,"<p>Both seem like viable approaches. Using <code>batch</code> with <code>threads=N</code> will create <code>N</code> copies of your reader op connected to your queue so that they can run in parallel, while <code>batch_join</code> you have to create the copies manually.</p>

<p>In your usage with <code>batch_join</code> you are creating several copies of <code>TextLineReader</code> which (as you noticed) will only parallelize across files. To have several threads reading a single file, you could instead create one <code>TextLineReader</code> and have several <code>line_reader.read</code> ops using the same reader.</p>

<p>Here's an example with some textfiles containing numbers</p>

<p>Generate numbers:</p>

<pre><code>num_files=10
num_entries_per_file=10
file_root=""/temp/pipeline""
os.system('mkdir -p '+file_root)
for fi in range(num_files):
  fname = file_root+""/""+str(fi)
  dump_numbers_to_file(fname, fi*num_entries_per_file, (fi+1)*num_entries_per_file)
</code></pre>

<p>Read those numbers in batches of size 2, with parallelism of 2</p>

<pre><code>ops.reset_default_graph()
filename_queue = tf.train.string_input_producer([""/temp/pipeline/0"",
                                                 ""/temp/pipeline/1""],
                                                shuffle=False)
reader = tf.TextLineReader()
key, value = reader.read(filename_queue)
numeric_val1, = tf.decode_csv(value, record_defaults=[[-1]])
numeric_val2, = tf.decode_csv(value, record_defaults=[[-1]])
numeric_batch = tf.batch_join([[numeric_val1,], [numeric_val2]], 2)
# have to create session before queue runners because they use default session
sess = create_session()
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(coord=coord)

print '\n'.join([t.name for t in threads])
for i in range(20):
  print sess.run([numeric_batch])

coord.request_stop()
coord.join(threads)
</code></pre>

<p>You may see something like this:</p>

<pre><code>QueueRunner(input_producer:input_producer/input_producer_EnqueueMany)
QueueRunner(input_producer:input_producer/input_producer_Close_1)
QueueRunner(batch_join/fifo_queue:batch_join/fifo_queue_enqueue)
QueueRunner(batch_join/fifo_queue:batch_join/fifo_queue_enqueue_1)
QueueRunner(batch_join/fifo_queue:batch_join/fifo_queue_Close_1)
[array([0, 1], dtype=int32)]
[array([2, 3], dtype=int32)]
[array([4, 5], dtype=int32)]
[array([6, 7], dtype=int32)]
[array([8, 9], dtype=int32)]
[array([10, 11], dtype=int32)]
[array([12, 13], dtype=int32)]
[array([14, 15], dtype=int32)]
[array([16, 17], dtype=int32)]
[array([18, 19], dtype=int32)]
</code></pre>

<p>From the list of threads, you can see that there are 2 threads corresponding to read operations (<code>fifo_queue_enqueue</code> and <code>fifo_queue_enqueue_1</code> so you can do 2 reads in parallel)</p>
",2138200,4560,https://stackoverflow.com/questions/35689547,Documentation Replication on Other Examples
35565312,Is there a convolution function in Tensorflow to apply a Sobel filter?,"<p>Is there any <code>convolution</code> method in Tensorflow to apply a Sobel filter to an image <code>img</code> (tensor of type <code>float32</code> and rank 2)?</p>

<pre><code>sobel_x = tf.constant([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 'float32')
result = tf.convolution(img, sobel_x) # &lt;== TO DO THIS
</code></pre>

<p>I've already seen <code>tf.nn.conv2d</code> but I can't see how to use it for this operation. Is there some way to use <code>tf.nn.conv2d</code> to solve my problem?</p>
",2016-02-22 22:35:24,"<p>Perhaps I'm missing a subtlety here, but it appears that you could apply a Sobel filter to an image using <a href=""https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#expand_dims"" rel=""noreferrer""><code>tf.expand_dims()</code></a> and <a href=""https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#conv2d"" rel=""noreferrer""><code>tf.nn.conv2d()</code></a>, as follows:</p>

<pre><code>sobel_x = tf.constant([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], tf.float32)
sobel_x_filter = tf.reshape(sobel_x, [3, 3, 1, 1])
sobel_y_filter = tf.transpose(sobel_x_filter, [1, 0, 2, 3])

# Shape = height x width.
image = tf.placeholder(tf.float32, shape=[None, None])

# Shape = 1 x height x width x 1.
image_resized = tf.expand_dims(tf.expand_dims(image, 0), 3)

filtered_x = tf.nn.conv2d(image_resized, sobel_x_filter,
                          strides=[1, 1, 1, 1], padding='SAME')
filtered_y = tf.nn.conv2d(image_resized, sobel_y_filter,
                          strides=[1, 1, 1, 1], padding='SAME')
</code></pre>
",1112654,783,https://stackoverflow.com/questions/35565312,Documentation Replication on Other Examples
34877523,"In TensorFlow, what is tf.identity used for?","<p>I've seen <code>tf.identity</code> used in a few places, such as the official CIFAR-10 tutorial and the batch-normalization implementation on stackoverflow, but I don't see why it's necessary.</p>

<p>What's it used for? Can anyone give a use case or two?</p>

<p>One proposed answer is that it can be used for transfer between the CPU and GPU. This is not clear to me. Extension to the question, based on <a href=""https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py"" rel=""noreferrer"">this</a>: <code>loss = tower_loss(scope)</code> is under the GPU block, which suggests to me that all operators defined in <code>tower_loss</code> are mapped to the GPU. Then, at the end of <code>tower_loss</code>, we see <code>total_loss = tf.identity(total_loss)</code> before it's returned. Why? What would be the flaw with not using <code>tf.identity</code> here?</p>
",2016-01-19 13:01:41,"<p>After some stumbling I think I've noticed a single use case that fits all the examples I've seen. If there are other use cases, please elaborate with an example.</p>

<p>Use case:</p>

<p>Suppose you'd like to run an operator every time a particular Variable is evaluated. For example, say you'd like to add one to <code>x</code> every time the variable <code>y</code> is evaluated. It might seem like this will work:</p>

<pre><code>x = tf.Variable(0.0)
x_plus_1 = tf.assign_add(x, 1)

with tf.control_dependencies([x_plus_1]):
    y = x
init = tf.initialize_all_variables()

with tf.Session() as session:
    init.run()
    for i in xrange(5):
        print(y.eval())
</code></pre>

<p>It doesn't: it'll print 0, 0, 0, 0, 0. Instead, it seems that we need to add a new node to the graph within the <code>control_dependencies</code> block. So we use this trick:</p>

<pre><code>x = tf.Variable(0.0)
x_plus_1 = tf.assign_add(x, 1)

with tf.control_dependencies([x_plus_1]):
    y = tf.identity(x)
init = tf.initialize_all_variables()

with tf.Session() as session:
    init.run()
    for i in xrange(5):
        print(y.eval())
</code></pre>

<p>This works: it prints 1, 2, 3, 4, 5.</p>

<p>If in the CIFAR-10 tutorial we dropped <code>tf.identity</code>, then <code>loss_averages_op</code> would never run.</p>
",1441121,3056,https://stackoverflow.com/questions/34877523,Inadequate Examples
38111170,How is the input tensor for TensorFlow's tf.nn.dynamic_rnn operator structured?,"<p>I am trying to write a language model using word embeddings and recursive neural networks in TensorFlow 0.9.0 using the <code>tf.nn.dynamic_rnn</code> graph operation, but I don't understand how the <code>input</code> tensor is structured.</p>

<p>Let's say I have a corpus of <em>n</em> words. I embed each word in a vector of length <em>e</em>, and I want my RNN to unroll to <em>t</em> time steps. Assuming I use the default <code>time_major = False</code> parameter, what shape would my <code>input</code> tensor <code>[batch_size, max_time, input_size]</code> have?</p>

<p>Maybe a specific tiny example will make this question clearer. Say I have a corpus consisting of <em>n=8</em> words that looks like this.</p>

<pre><code>1, 2, 3, 3, 2, 1, 1, 2
</code></pre>

<p>Say I embed it in a vector of size <em>e=3</em> with the embeddings 1 -> [10, 10, 10], 2 -> [20, 20, 20], and 3 -> [30, 30, 30], what would my <code>input</code> tensor look like?</p>

<p>I've read the <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html#recurrent-neural-networks"" rel=""nofollow"">TensorFlow Recurrent Neural Network tutorial</a>, but that doesn't use <code>tf.nn.dynamic_rnn</code>. I've also read the documentation for <code>tf.nn.dynamic_rnn</code>, but find it confusing. In particular I'm not sure what ""max_time"" and ""input_size"" mean here.</p>

<p>Can anyone give the shape of the <code>input</code> tensor in terms of <em>n</em>, <em>t</em>, and <em>e</em>, and/or an example of what that tensor would look like initialized with data from the small corpus I describe?</p>

<p><em>TensorFlow 0.9.0, Python 3.5.1, OS X 10.11.5</em></p>
",2016-06-29 22:37:11,"<p>In your case, it looks like <code>batch_size = 1</code>, since you're looking at a single example.  So <code>max_time</code> is <code>n=8</code> and <code>input_size</code> is the input depth, in your case <code>e=3</code>.  So you would want to construct an <code>input</code> tensor which is shaped <code>[1, 8, 3]</code>.  It's batch_major, so the first dimension (the batch dimension) is <code>1</code>.  If, say, you had another input at the same time, with <code>n=6</code> words, then you would combine the two by padding this second example to <code>8</code> words (by padding zeros for the last 2 word embeddings) and you would have an <code>inputs</code> size of <code>[2, 8, 3]</code>.</p>
",1120370,16614,https://stackoverflow.com/questions/38111170,Documentation Replicability
34931121,Can cond support TF ops with side effects?,"<p>The (source code) documentation for <code>tf.cond</code> is unclear on whether the functions to be performed when the predicate is evaluated can have side effects or not. I've done some tests but I'm getting conflicting results. For example the code below does not work:</p>

<pre><code>import tensorflow as tf
from tensorflow.python.ops import control_flow_ops

pred = tf.placeholder(tf.bool, [])
count = tf.Variable(0)
adder = count.assign_add(1)
subtractor = count.assign_sub(2)

my_op = control_flow_ops.cond(pred, lambda: adder, lambda: subtractor)

sess = tf.InteractiveSession()
tf.initialize_all_variables().run()

my_op.eval(feed_dict={pred: True})
count.eval() # returns -1

my_op.eval(feed_dict={pred: False})
count.eval() # returns -2
</code></pre>

<p>I.e. no matter what value the predicate evaluates to, both functions are getting run, and so the net result is a subtraction of 1. On the other hand, this code snippet does work, where the only difference is that I add new ops to the graph every time <code>my_op</code> is called:</p>

<pre><code>pred = tf.placeholder(tf.bool, [])
count = tf.Variable(0)

my_op = control_flow_ops.cond(pred, lambda:count.assign_add(1), lambda:count.assign_sub(2))

sess = tf.InteractiveSession()
tf.initialize_all_variables().run()

my_op.eval(feed_dict={pred: False})
count.eval() # returns -2

my_op.eval(feed_dict={pred: True})
count.eval() # returns -1
</code></pre>

<p>Not sure why creating new ops every time works while the other case doesn't, but I'd obviously rather not be adding nodes as the graph will eventually become too big.</p>
",2016-01-21 18:07:23,"<p>Your second version&mdash;where the <code>assign_add()</code> and <code>assign_sub()</code> ops are creating inside the lambdas passed to <code>cond()</code>&mdash;is the correct way to do this. Fortunately, each of the two lambdas is only evaluated once, during the call to <code>cond()</code>, so your graph will not grow without bound.</p>

<p>Essentially what <code>cond()</code> does is the following:</p>

<ol>
<li><p>Create a <code>Switch</code> node, which forwards its input to only one of two outputs, depending on the value of <code>pred</code>. Let's call the outputs <code>pred_true</code> and <code>pred_false</code>. (They have the same value as <code>pred</code> but that's unimportant since this is never directly evaluated.)</p></li>
<li><p>Build the subgraph corresponding to the <code>if_true</code> lambda, where all of the nodes have a control dependency on <code>pred_true</code>.</p></li>
<li><p>Build the subgraph corresponding to the <code>if_false</code> lambda, where all of the nodes have a control dependency on <code>pred_false</code>.</p></li>
<li><p>Zip together the lists of return values from the two lambdas, and create a <code>Merge</code> node for each of these. A <code>Merge</code> node takes two inputs, of which only one is expected to be produced, and forwards it to its output.</p></li>
<li><p>Return the tensors that are the outputs of the <code>Merge</code> nodes.</p></li>
</ol>

<p>This means you can run your second version, and be content that the graph remains a fixed size, regardless of how many steps you run.</p>

<p>The reason your first version doesn't work is that, when a <code>Tensor</code> is captured (like <code>adder</code> or <code>subtractor</code> in your example), an additional <code>Switch</code> node is added to enforce the logic that the value of the tensor is only forwarded to the branch that actually executes. This is an artifact of how TensorFlow combines feed-forward dataflow and control flow  in its execution model. The result is that the captured tensors (in this case the results of the <code>assign_add</code> and <code>assign_sub</code>) will always be evaluated, even if they aren't used, and you'll see their side effects. This is something we need to document better, and <a href=""https://stackoverflow.com/a/34934516/3574081"">as Michael says</a>, we're going to make this more usable in future.</p>
",2615676,1557,https://stackoverflow.com/questions/34931121,Documentation Replicability
34870614,What does tf.nn.embedding_lookup function do?,"<pre><code>tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None)
</code></pre>

<p>I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)?</p>

<p>For instance, in the <code>skip-gram</code> model if we use <code>tf.nn.embedding_lookup(embeddings, train_inputs)</code>, then for each <code>train_input</code> it finds the correspond embedding?</p>
",2016-01-19 07:14:40,"<p><code>embedding_lookup</code> function retrieves rows of the <code>params</code> tensor. The behavior is similar to using indexing with arrays in numpy. E.g.</p>

<pre><code>matrix = np.random.random([1024, 64])  # 64-dimensional embeddings
ids = np.array([0, 5, 17, 33])
print matrix[ids]  # prints a matrix of shape [4, 64] 
</code></pre>

<p><code>params</code> argument can be also a list of tensors in which case the <code>ids</code> will be distributed among the tensors. For example, given a list of 3 tensors <code>[2, 64]</code>, the default behavior is that they will represent <code>ids</code>: <code>[0, 3]</code>, <code>[1, 4]</code>, <code>[2, 5]</code>. </p>

<p><code>partition_strategy</code> controls the way how the <code>ids</code> are distributed among the list. The partitioning is useful for larger scale problems when the matrix might be too large to keep in one piece.</p>
",5808490,2133,https://stackoverflow.com/questions/34870614,Documentation Ambiguity
34801342,tensorflow: how to rotate an image for data augmentation?,"<p>In tensorflow, I would like to rotate an image from a random angle, for data augmentation. But I don't find this transformation in the tf.image module. </p>
",2016-01-14 22:56:24,"<p><strong>Update</strong>: see @astromme's answer below. Tensorflow now supports rotating images natively.</p>

<p>What you can do while there is no native method in tensorflow is something like this:</p>

<pre><code>from PIL import Image
sess = tf.InteractiveSession()

# Pass image tensor object to a PIL image
image = Image.fromarray(image.eval())

# Use PIL or other library of the sort to rotate
rotated = Image.Image.rotate(image, degrees)

# Convert rotated image back to tensor
rotated_tensor = tf.convert_to_tensor(np.array(rotated))
</code></pre>
",3988198,1511,https://stackoverflow.com/questions/34801342,Inadequate Examples
34642595,Tensorflow Strides Argument,"<p>I am trying to understand the <strong>strides</strong> argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. </p>

<p>The <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#max_pool"" rel=""noreferrer"">documentation</a> repeatedly says </p>

<blockquote>
  <p>strides: A list of ints that has length >= 4. The stride of the sliding window for each dimension of the input tensor.</p>
</blockquote>

<p>My questions are:</p>

<ol>
<li>What do each of the 4+ integers represent?</li>
<li>Why must they have strides[0] = strides[3] = 1 for convnets?</li>
<li>In <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3%20-%20Neural%20Networks/convolutional_network.ipynb"" rel=""noreferrer"">this example</a> we see <code>tf.reshape(_X,shape=[-1, 28, 28, 1])</code>. Why -1?</li>
</ol>

<p>Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.</p>
",2016-01-06 20:56:26,"<p>The pooling and convolutional ops slide a ""window"" across the input tensor.  Using <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#conv2d""><code>tf.nn.conv2d</code></a> as an example: If the input tensor has 4 dimensions:  <code>[batch, height, width, channels]</code>, then the convolution operates on a 2D window on the <code>height, width</code> dimensions.</p>

<p><code>strides</code> determines how much the window shifts by in each of the dimensions.  The typical use sets the first (the batch) and last (the depth) stride to 1.</p>

<p>Let's use a very concrete example:  Running a 2-d convolution over a 32x32 greyscale input image.  I say greyscale because then the input image has depth=1, which helps keep it simple.  Let that image look like this:</p>

<pre><code>00 01 02 03 04 ...
10 11 12 13 14 ...
20 21 22 23 24 ...
30 31 32 33 34 ...
...
</code></pre>

<p>Let's run a 2x2 convolution window over a single example (batch size = 1).  We'll give the convolution an output channel depth of 8.</p>

<p>The input to the convolution has <code>shape=[1, 32, 32, 1]</code>.</p>

<p>If you specify <code>strides=[1,1,1,1]</code> with <code>padding=SAME</code>, then the output of the filter will be [1, 32, 32, 8].</p>

<p>The filter will first create an output for:</p>

<pre><code>F(00 01
  10 11)
</code></pre>

<p>And then for:</p>

<pre><code>F(01 02
  11 12)
</code></pre>

<p>and so on.  Then it will move to the second row, calculating:</p>

<pre><code>F(10, 11
  20, 21)
</code></pre>

<p>then</p>

<pre><code>F(11, 12
  21, 22)
</code></pre>

<p>If you specify a stride of [1, 2, 2, 1] it won't do overlapping windows.  It will compute:</p>

<pre><code>F(00, 01
  10, 11)
</code></pre>

<p>and then</p>

<pre><code>F(02, 03
  12, 13)
</code></pre>

<p>The stride operates similarly for the pooling operators.</p>

<p><strong>Question 2:  Why strides [1, x, y, 1] for convnets</strong></p>

<p>The first 1 is the batch:  You don't usually want to skip over examples in your batch, or you shouldn't have included them in the first place. :)</p>

<p>The last 1 is the depth of the convolution:  You don't usually want to skip inputs, for the same reason.</p>

<p>The conv2d operator is more general, so you <em>could</em> create convolutions that slide the window along other dimensions, but that's not a typical use in convnets.  The typical use is to use them spatially.</p>

<p><strong>Why reshape to -1</strong>  -1 is a placeholder that says ""adjust as necessary to match the size needed for the full tensor.""  It's a way of making the code be independent of the input batch size, so that you can change your pipeline and not have to adjust the batch size everywhere in the code.</p>
",3908247,1808,https://stackoverflow.com/questions/34642595,Documentation Replicability
40518132,regarding the usage of tf.tile and tf.pack,"<p>I saw a program including the following two lines of codes  </p>

<pre><code>exponential_map = tf.exp(output_map)
sum_exp = tf.reduce_sum(exponential_map, 3, keep_dims=True)
tensor_sum_exp = tf.tile(sum_exp, tf.pack([1, 1, 1, tf.shape(output)[3]]))
</code></pre>

<p>The <code>output_map</code> is of shape <code>(1,255,255,2)</code>. I can see <code>sum_exp</code> generally add the two channels for the <code>exponential_map</code> into one. So <code>sum_exp</code> should be of shape <code>(1,255,255,1)</code>. But I am confused about what does <code>tensor_sum_exp = tf.tile(sum_exp, tf.pack([1, 1, 1, tf.shape(output)[3]]))</code> aim to do?</p>
",2016-11-10 00:18:27,"<p>It makes multiple copies of the image along dimension 3.</p>

<p>It looks up the (dynaminc) number of output channels:</p>

<pre><code>outchannels = tf.shape(output)[3]
</code></pre>

<p>Then it builds a tensor containing the number of multiples along each dimension:</p>

<pre><code>multiples = tf.pack([1, 1, 1, outchanels])
</code></pre>

<p>And makes that many copies:</p>

<pre><code>tensor_sum_exp = tf.tile(sum_exp, multiples)
</code></pre>

<p>If for example, you know <code>outchannels</code> is always <code>n</code>, (check <code>sum_exp.get_shape()</code>), you could also:</p>

<pre><code>tensor_sum_exp = tf.concat(3,[sum_exp]*n)
</code></pre>
",785099,5403,https://stackoverflow.com/questions/40518132,Documentation Replicability
34619177,What does tf.nn.conv2d do in tensorflow?,"<p>I was looking at the docs of tensorflow about <code>tf.nn.conv2d</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""noreferrer"">here</a>. But I can't understand what it does or what it is trying to achieve. It says on the docs,</p>
<blockquote>
<p>#1 : Flattens the filter to a 2-D matrix with shape</p>
<p><code>[filter_height * filter_width * in_channels, output_channels]</code>.</p>
</blockquote>
<p>Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below :</p>
<blockquote>
<p># 2: Extracts image patches from the the input tensor to form a virtual tensor of shape</p>
<p><code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>.</p>
<p># 3: For each patch, right-multiplies the filter matrix and the image patch vector.</p>
</blockquote>
<p>It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this.</p>
<p>I've tried coding a small portion and printing out the shape of the operation. Still, I can't understand.</p>
<p>I tried something like this:</p>
<pre class=""lang-python prettyprint-override""><code>op = tf.shape(tf.nn.conv2d(tf.random_normal([1,10,10,10]), 
              tf.random_normal([2,10,10,10]), 
              strides=[1, 2, 2, 1], padding='SAME'))

with tf.Session() as sess:
    result = sess.run(op)
    print(result)
</code></pre>
<p>I understand bits and pieces of convolutional neural networks. I studied them <a href=""http://cs231n.github.io/convolutional-networks/"" rel=""noreferrer"">here</a>. But the implementation on tensorflow is not what I expected. So it raised the question.</p>
<p><strong>EDIT</strong>:
So, I implemented a much simpler code. But I can't figure out what's going on. I mean how the results are like this. It would be extremely helpful if anyone could tell me what process yields this output.</p>
<pre class=""lang-python prettyprint-override""><code>input = tf.Variable(tf.random_normal([1,2,2,1]))
filter = tf.Variable(tf.random_normal([1,1,1,1]))

op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')
init = tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)

    print(&quot;input&quot;)
    print(input.eval())
    print(&quot;filter&quot;)
    print(filter.eval())
    print(&quot;result&quot;)
    result = sess.run(op)
    print(result)
</code></pre>
<p>output</p>
<pre><code>input
[[[[ 1.60314465]
   [-0.55022103]]

  [[ 0.00595062]
   [-0.69889867]]]]
filter
[[[[-0.59594476]]]]
result
[[[[-0.95538563]
   [ 0.32790133]]

  [[-0.00354624]
   [ 0.41650501]]]]
</code></pre>
",2016-01-05 18:51:56,"<p>2D convolution is computed in a similar way one would calculate <a href=""http://www.riptutorial.com/tensorflow/example/30750/math-behind-1d-convolution-with-advanced-examples-in-tf"" rel=""noreferrer"">1D convolution</a>: you slide your kernel over the input, calculate the element-wise multiplications and sum them up. But instead of your kernel/input being an array, here they are matrices.</p>

<hr>

<p>In the most basic example there is no padding and stride=1. Let's assume your <code>input</code> and <code>kernel</code> are:
<a href=""https://i.stack.imgur.com/yTCl8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yTCl8.png"" alt=""enter image description here""></a></p>

<p>When you use your kernel you will receive the following output: <a href=""https://i.stack.imgur.com/TPhBi.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TPhBi.png"" alt=""enter image description here""></a>, which is calculated in the following way:</p>

<ul>
<li>14 = 4 * 1 + 3 * 0 + 1 * 1 + 2 * 2 + 1 * 1 + 0 * 0 + 1 * 0 + 2 * 0 + 4 * 1</li>
<li>6  = 3 * 1 + 1 * 0 + 0 * 1 + 1 * 2 + 0 * 1 + 1 * 0 + 2 * 0 + 4 * 0 + 1 * 1</li>
<li>6  = 2 * 1 + 1 * 0 + 0 * 1 + 1 * 2 + 2 * 1 + 4 * 0 + 3 * 0 + 1 * 0 + 0 * 1</li>
<li>12 = 1 * 1 + 0 * 0 + 1 * 1 + 2 * 2 + 4 * 1 + 1 * 0 + 1 * 0 + 0 * 0 + 2 * 1</li>
</ul>

<p>TF's <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""noreferrer"">conv2d</a> function calculates convolutions in batches and uses a slightly different format. For an input it is <code>[batch, in_height, in_width, in_channels]</code> for the kernel it is <code>[filter_height, filter_width, in_channels, out_channels]</code>. So we need to provide the data in the correct format:</p>

<pre><code>import tensorflow as tf
k = tf.constant([
    [1, 0, 1],
    [2, 1, 0],
    [0, 0, 1]
], dtype=tf.float32, name='k')
i = tf.constant([
    [4, 3, 1, 0],
    [2, 1, 0, 1],
    [1, 2, 4, 1],
    [3, 1, 0, 2]
], dtype=tf.float32, name='i')
kernel = tf.reshape(k, [3, 3, 1, 1], name='kernel')
image  = tf.reshape(i, [1, 4, 4, 1], name='image')
</code></pre>

<p>Afterwards the convolution is computed with:</p>

<pre><code>res = tf.squeeze(tf.nn.conv2d(image, kernel, [1, 1, 1, 1], ""VALID""))
# VALID means no padding
with tf.Session() as sess:
   print sess.run(res)
</code></pre>

<p>And will be equivalent to the one we calculated by hand. </p>

<hr>

<p>For <a href=""http://www.riptutorial.com/tensorflow/example/30755/some-padding--strides-1"" rel=""noreferrer"">examples with padding/strides, take a look here</a>.</p>
",4341948,10491,https://stackoverflow.com/questions/34619177,Documentation Replication on Other Examples
34589335,How does the distorted_inputs() function in the TensorFlow CIFAR-10 example tutorial get 128 images per batch?,"

<p>I was going through the CIFAR-10 example at <a href=""https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html"" rel=""nofollow noreferrer"">TensorFlow getting started guide for CNN</a></p>

<p>Now in the train function in <strong>cifar10_train.py</strong> we get images as </p>

<pre class=""lang-py prettyprint-override""><code>images,labels = cifar10.distorted_inputs()
</code></pre>

<p>In the <code>distorted_inputs()</code> function we generate the filenames in a queue and then read a single record as </p>

<pre class=""lang-py prettyprint-override""><code> # Create a queue that produces the filenames to read.
 filename_queue = tf.train.string_input_producer(filenames)

 # Read examples from files in the filename queue.
 read_input = cifar10_input.read_cifar10(filename_queue)
 reshaped_image = tf.cast(read_input.uint8image, tf.float32)
</code></pre>

<p>When I add debugging code, the <code>read_input</code> variable contains only 1 record with an image and its height, width, and label name.</p>

<p>The example then applies some distortion to the read image/record and then passes it to the <code>_generate_image_and_label_batch()</code> function.</p>

<p>This function then returns a 4D Tensor of shape <code>[batch_size, 32, 32, 3]</code>  where <code>batch_size = 128</code>. </p>

<p>The above function utilizes the <code>tf.train.shuffle_batch()</code> function when returns the batch. </p>

<p>My question is where do the extra records come from in the <code>tf.train.shuffle_batch()</code> function? We are not passing it any filename or  reader object. </p>

<p>Can someone shed some light on how we go from 1 record to 128 records? I looked into the documentation but didn't understand.</p>
",2016-01-04 10:43:25,"<p>The <a href=""https://www.tensorflow.org/versions/master/api_docs/python/io_ops.html#shuffle_batch""><code>tf.train.shuffle_batch()</code></a> function can be used to produce (one or more) tensors containing a batch of inputs. Internally, <code>tf.train.shuffle_batch()</code> creates a <a href=""https://www.tensorflow.org/versions/master/api_docs/python/io_ops.html#RandomShuffleQueue""><code>tf.RandomShuffleQueue</code></a>, on which it calls <a href=""https://www.tensorflow.org/versions/master/api_docs/python/io_ops.html#QueueBase.enqueue""><code>q.enqueue()</code></a> with the image and label tensors to enqueue a single element (image-label pair). It then returns the result of <a href=""https://www.tensorflow.org/versions/master/api_docs/python/io_ops.html#QueueBase.dequeue_many""><code>q.dequeue_many(batch_size)</code></a>, which concatenates <code>batch_size</code> randomly selected elements (image-label pairs) into a batch of images and a batch of labels.</p>

<p>Note that, although it looks from the code like <code>read_input</code> and <code>filename_queue</code> have a functional relationship, there is an additional wrinkle. Simply evaluating the result of <code>tf.train.shuffle_batch()</code> will block forever, because no elements have been added to the internal queue. To simplify this, when you call <code>tf.train.shuffle_batch()</code>, TensorFlow will add a <a href=""https://www.tensorflow.org/versions/master/api_docs/python/train.html#QueueRunner""><code>QueueRunner</code></a> to an internal collection in the graph. A later call to <a href=""https://www.tensorflow.org/versions/master/api_docs/python/train.html#start_queue_runners""><code>tf.train.start_queue_runners()</code></a> (e.g. <a href=""https://github.com/tensorflow/tensorflow/blob/2d5f0ac61fe4e4160fbb68d8031418528111dae9/tensorflow/models/image/cifar10/cifar10_train.py#L97"">here in <code>cifar10_train.py</code></a>) will start a thread that adds elements to the queue, and enables training to proceed. The <a href=""https://www.tensorflow.org/versions/master/how_tos/threading_and_queues/index.html"">Threading and Queues HOWTO</a> has more information on how this works.</p>
",628096,1843,https://stackoverflow.com/questions/34589335,Documentation Replication on Other Examples
38045785,Does the example for decaying the learning rate in TensorFlow website actually decay the learning rate?,"<p>I was reading the decaying learning rate and thought there might be a mistake in the docs and wanted to confirm. It says that the decay equation is:</p>

<blockquote>
  <p>decayed_learning_rate = learning_rate *
                          decay_rate ^ (global_step / decay_steps)</p>
</blockquote>

<p>however, if <code>global_step  = 0</code> I'd guess there is never a decay, right? However, look at the example:</p>

<pre><code>...
global_step = tf.Variable(0, trainable=False)
starter_learning_rate = 0.1
learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,
                                           100000, 0.96, staircase=True)
# Passing global_step to minimize() will increment it at each step.
learning_step = (
    tf.GradientDescentOptimizer(learning_rate)
    .minimize(...my loss..., global_step=global_step)
)
</code></pre>

<p>It has a <code>global_step = tf.Variable(0, trainable=False)</code> that is set equal to zero. Thus, no decay. Is this correct deduction?</p>

<p>I thought there might be a caveat due to integer division when staircase function is set to true, but even in integer division that still seems that there is no decay. Or is there a misunderstanding of what staircase does?</p>
",2016-06-27 04:07:26,"<p>The variable <code>global_step</code> is passed to the <code>minimize</code> function and will be incremented each time the training operation <code>learning_step</code> is run. </p>

<p>It is even written in the commentary of your code:</p>

<blockquote>
  <p><code># Passing global_step to minimize() will increment it at each step.</code></p>
</blockquote>
",1601580,5643,https://stackoverflow.com/questions/38045785,Documentation Replicability
40090943,How can the tf.Variable be shared in between-graph replication training?,"<p>I read the document of Distributed TensorFlow and have a question about between-graph replication.
<a href=""https://www.tensorflow.org/versions/master/how_tos/distributed/index.html"" rel=""nofollow"">https://www.tensorflow.org/versions/master/how_tos/distributed/index.html</a></p>

<p>In my understanding, between-graph replication training creates same number of graphs as workers and the graphs share <code>tf.Variables</code> on parameter servers.
That is, one worker creates one session and one graph, and all graphs share same <code>tf.Variable</code>.</p>

<p>However, I just thought two different sessions can not share the same <code>tf.Variable</code>.
Is it misunderstanding?</p>
",2016-10-17 16:03:43,"<p>For your last question:</p>

<p>""Can two different sessions share the same tf.Variable?""</p>

<ol>
<li>For distributed sessions(e.g. Session(""grpc://.."")), they can. </li>
<li>For direct sessions, they can't.</li>
</ol>

<p>In distributed training, variables are managed by tf.Server(), persistent across sessions. Remember? Server are created before sessions. It lives longer than tf.Sessions.</p>
",6472530,193,https://stackoverflow.com/questions/40090943,Documentation Replication on Other Examples
38114534,Basic 1d convolution in tensorflow,"<p>OK, I'd like to do a 1-dimensional convolution of time series data in Tensorflow. This is apparently supported using <code>tf.nn.conv2d</code>, according to <a href=""https://github.com/tensorflow/tensorflow/issues/2165"" rel=""noreferrer"">these</a> <a href=""https://github.com/tensorflow/tensorflow/issues/1136"" rel=""noreferrer"">tickets</a>, and <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#convolution"" rel=""noreferrer"">the manual</a>. the only requirement is to set <code>strides=[1,1,1,1]</code>. Sounds simple!</p>

<p>However, I cannot work out how to do this in even a very minimal test case. What am I doing wrong?</p>

<p>Let's set this up.</p>

<pre><code>import tensorflow as tf
import numpy as np
print(tf.__version__)
&gt;&gt;&gt; 0.9.0
</code></pre>

<p>OK, now generate a basic convolution test on two small arrays. I will make it easy by using a batch size of 1, and since time series are 1-dimensional, I will have an ""image height"" of 1. And since it's a univariate time series, clearly the number of ""channels"" is also 1, so this will be simple, right?</p>

<pre><code>g = tf.Graph()
with g.as_default():
    # data shape is ""[batch, in_height, in_width, in_channels]"",
    x = tf.Variable(np.array([0.0, 0.0, 0.0, 0.0, 1.0]).reshape(1,1,-1,1), name=""x"")
    # filter shape is ""[filter_height, filter_width, in_channels, out_channels]""
    phi = tf.Variable(np.array([0.0, 0.5, 1.0]).reshape(1,-1,1,1), name=""phi"")
    conv = tf.nn.conv2d(
        phi,
        x,
        strides=[1, 1, 1, 1],
        padding=""SAME"",
        name=""conv"")
</code></pre>

<p>BOOM. Error.</p>

<pre><code>ValueError: Dimensions 1 and 5 are not compatible
</code></pre>

<p>OK, For a start, I don't understand how this should happen with <em>any</em> dimension, since I've specified that I'm padding the arguments in the convolution OP. </p>

<p>but fine, maybe there are limits to that. I must have got the documentation confused and set up this convolution on the wrong axes of the tensor. I'll try all possible permutations:</p>

<pre><code>for i in range(4):
    for j in range(4):
        shape1 = [1,1,1,1]
        shape1[i] = -1
        shape2 = [1,1,1,1]
        shape2[j] = -1
        x_array = np.array([0.0, 0.0, 0.0, 0.0, 1.0]).reshape(*shape1)
        phi_array = np.array([0.0, 0.5, 1.0]).reshape(*shape2)
        try:
            g = tf.Graph()
            with g.as_default():
                x = tf.Variable(x_array, name=""x"")
                phi = tf.Variable(phi_array, name=""phi"")
                conv = tf.nn.conv2d(
                    x,
                    phi,
                    strides=[1, 1, 1, 1],
                    padding=""SAME"",
                    name=""conv"")
                init_op = tf.initialize_all_variables()
            sess = tf.Session(graph=g)
            sess.run(init_op)
            print(""SUCCEEDED!"", x_array.shape, phi_array.shape, conv.eval(session=sess))
            sess.close()
        except Exception as e:
            print(""FAILED!"", x_array.shape, phi_array.shape, type(e), e.args or e._message)
</code></pre>

<p>Result:</p>

<pre><code>FAILED! (5, 1, 1, 1) (3, 1, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (3, 1) Input: (1, 1)',)
FAILED! (5, 1, 1, 1) (1, 3, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (1, 3) Input: (1, 1)',)
FAILED! (5, 1, 1, 1) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 1 and 3 are not compatible',)
FAILED! (5, 1, 1, 1) (1, 1, 1, 3) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 5, 1, 1) (3, 1, 1, 1) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 5, 1, 1) (1, 3, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (1, 3) Input: (5, 1)',)
FAILED! (1, 5, 1, 1) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 1 and 3 are not compatible',)
FAILED! (1, 5, 1, 1) (1, 1, 1, 3) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 1, 5, 1) (3, 1, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (3, 1) Input: (1, 5)',)
FAILED! (1, 1, 5, 1) (1, 3, 1, 1) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 1, 5, 1) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 1 and 3 are not compatible',)
FAILED! (1, 1, 5, 1) (1, 1, 1, 3) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 1, 1, 5) (3, 1, 1, 1) &lt;class 'ValueError'&gt; ('Dimensions 5 and 1 are not compatible',)
FAILED! (1, 1, 1, 5) (1, 3, 1, 1) &lt;class 'ValueError'&gt; ('Dimensions 5 and 1 are not compatible',)
FAILED! (1, 1, 1, 5) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 5 and 3 are not compatible',)
FAILED! (1, 1, 1, 5) (1, 1, 1, 3) &lt;class 'ValueError'&gt; ('Dimensions 5 and 1 are not compatible',)
</code></pre>

<p>Hmm. OK, it looks like there are two problems now. Firstly, the <code>ValueError</code> is about applying the filter along the wrong axis, I guess, although there are two forms.</p>

<p>But then the axes along which I can apply the filter are confusing too - notice that it actually constructs the graph with input shape (5, 1, 1, 1)  and filter shape (1, 1, 1, 3). AFAICT from the documentation, this should be a filter that looks at on example from the batch, one ""pixel"" and one ""channel"" and outputs 3 ""channels"". Why does that one work, then, when others do not?</p>

<p>Anyway, sometimes it does not fail while constructing the graph.
Sometime it constructs the graph; then we get the <code>tensorflow.python.framework.errors.InvalidArgumentError</code>. From some <a href=""https://github.com/tensorflow/tensorflow/issues/524"" rel=""noreferrer"">confusing github tickets</a> I gather this is probably due to <del>the fact that I'm running on CPU instead of GPU, or vice versa</del> the fact that the convolution Op is only defined for 32 bit floats, not 64 bit floats. If anyone could throw some light on <em>which</em> axes I should be aligning <em>what</em> on, in order to convolve a time series with a kernel, I'd be very grateful.</p>
",2016-06-30 05:20:57,"<p>I am sorry to say that, but your first code was almost right. You just inverted <code>x</code> and <code>phi</code> in <code>tf.nn.conv2d</code>:</p>

<pre class=""lang-py prettyprint-override""><code>g = tf.Graph()
with g.as_default():
    # data shape is ""[batch, in_height, in_width, in_channels]"",
    x = tf.Variable(np.array([0.0, 0.0, 0.0, 0.0, 1.0]).reshape(1, 1, 5, 1), name=""x"")
    # filter shape is ""[filter_height, filter_width, in_channels, out_channels]""
    phi = tf.Variable(np.array([0.0, 0.5, 1.0]).reshape(1, 3, 1, 1), name=""phi"")
    conv = tf.nn.conv2d(
        x,
        phi,
        strides=[1, 1, 1, 1],
        padding=""SAME"",
        name=""conv"")
</code></pre>

<hr>

<p><strong>Update:</strong> TensorFlow now supports 1D convolution since version r0.11, using <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv1d"" rel=""noreferrer""><code>tf.nn.conv1d</code></a>. I previously made a guide to use them in the stackoverflow documentation (now extinct) that I'm pasting here:</p>

<hr>

<h2>Guide to 1D convolution</h2>

<p>Consider a basic example with an input of length <code>10</code>, and dimension <code>16</code>. The batch size is <code>32</code>. We therefore have a placeholder with input shape <code>[batch_size, 10, 16]</code>.</p>

<pre class=""lang-py prettyprint-override""><code>batch_size = 32
x = tf.placeholder(tf.float32, [batch_size, 10, 16])
</code></pre>

<p>We then create a filter with width 3, and we take <code>16</code> channels as input, and output also <code>16</code> channels.</p>

<pre class=""lang-py prettyprint-override""><code>filter = tf.zeros([3, 16, 16])  # these should be real values, not 0
</code></pre>

<hr>

<p>Finally we apply <code>tf.nn.conv1d</code> with a stride and a padding:
- <strong>stride</strong>: integer <code>s</code>
- <strong>padding</strong>: this works like in 2D, you can choose between <code>SAME</code> and <code>VALID</code>. <code>SAME</code> will output the same input length, while <code>VALID</code> will not add zero padding.</p>

<p>For our example we take a stride of 2, and a valid padding.
</p>

<pre><code>output = tf.nn.conv1d(x, filter, stride=2, padding=""VALID"")
</code></pre>

<p>The output shape should be <code>[batch_size, 4, 16]</code>.<br>
With <code>padding=""SAME""</code>, we would have had an output shape of <code>[batch_size, 5, 16]</code>.</p>
",11730,945,https://stackoverflow.com/questions/38114534,Documentation Ambiguity
38213440,How can you get length of a TensorFlow string?,"<p>Is there any way to get length of a TensorFlow string within TensorFlow? For example, is there any function that returns the length of <code>a = tf.constant(""Hello everyone"", tf.string)</code> as <code>14</code> without passing the string back to Python.</p>
",2016-07-05 21:55:42,"<p>This works for me:</p>

<pre><code>x = tf.constant(""Hello everyone"")

# Launch the default graph.
with tf.Session() as sess:
    print(tf.size(tf.string_split([x],"""")).eval())
</code></pre>
",4193504,53,https://stackoverflow.com/questions/38213440,Documentation Replicability
38601452,What is tf.nn.max_pool's ksize parameter used for?,"<p>In the definition of <code>tf.nn.max_pool</code>, what is <code>ksize</code> used for?</p>

<pre><code>tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)

Performs the max pooling on the input.

Args:

value: A 4-D Tensor with shape [batch, height, width, channels] and type    tf.float32.
ksize: A list of ints that has length &gt;= 4. The size of the window for each dimension of the input tensor.
</code></pre>

<p>For instance, if an <code>input value</code> is of <code>tensor :  [1, 64, 64, 3]</code> and <code>ksize=3</code>.what does that mean?</p>
",2016-07-26 23:28:21,"<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/max_pool"" rel=""noreferrer"">documentation</a> states:</p>

<blockquote>
  <p>ksize: A list of ints that has length >= 4. The size of the window for each dimension of the input tensor.</p>
</blockquote>

<p>In general for images, your input is of shape <code>[batch_size, 64, 64, 3]</code> for an RGB image of 64x64 pixels.</p>

<p>The kernel size <code>ksize</code> will typically be <code>[1, 2, 2, 1]</code> if you have a 2x2 window over which you take the maximum. On the batch size dimension and the channels dimension, <code>ksize</code> is <code>1</code> because we don't want to take the maximum over multiple examples, or over multiples channels.</p>
",288609,12685,https://stackoverflow.com/questions/38601452,Documentation Ambiguity
38646996,tf.nn.dynamic_rnn() returning error when used in Google Cloud Datalab,"<p>I'm trying to run an RNN on Google Cloud Datalab. The same network runs correctly on my computer, but when I run it on Datalab, I get the following error:</p>

<p>TypeError: dynamic_rnn() takes at least 3 arguments (3 given)</p>

<p>The use of dynamic_rnn() is as follows:</p>

<p>rnn_outputs, state = tf.nn.dynamic_rnn(cells, inputs, initial_state = initial_state_placeholder)</p>

<p>Is this a tensorflow version problem. On my computer I'm using tensorflow-0.9.0, from the latest GPU pip install for Ubuntu. Is Cloud Datalab perhaps using a different version, in which the use of dynamic_rnn() has changed slightly?</p>

<p>UPDATE: Have checked version of tensorflow imported into Datalab - it's 0.7.1. I assume there's no way of importing a newer version if this is what's causing the problem?</p>

<p>I'm following the directions for use here:
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md</a></p>

<p>Many thanks. </p>
",2016-07-28 21:23:27,"<p><code>dynamic_rnn</code> does not appear to be listed in the <a href=""https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html"" rel=""nofollow noreferrer"">api docs</a> of tensorflow 0.7.</p>

<p>As a next step, you could try one of the following options:</p>

<ul>
<li>Deploy a newer version of Datalab. This is the recommended option. The latest version uses tensorflow 0.9.0.</li>
<li>Upgrade tensorflow to version 0.9.0 by running the command below in a Datalab notebook. See my explanation in the following <a href=""https://stackoverflow.com/questions/37464668/tensorflow-upgrade-failed-on-google-datalab/37604626#37604626"">stackoverflow post</a> explaining why this is not recommended (but it may work). </li>
</ul>

<p>Note for tensorflow version 0.9.0 use: </p>

<pre><code>%%bash
wget https://storage.googleapis.com/cloud-datalab/deploy/tf/tensorflow-0.9.0-cp27-none-linux_x86_64.whl &amp;&amp; pip install --ignore-installed --no-deps tensorflow-0.9.0-cp27-none-linux_x86_64.whl &amp;&amp; rm tensorflow-0.9.0-cp27-none-linux_x86_64.whl
</code></pre>
",6483227,293,https://stackoverflow.com/questions/38646996,Documentation Replication on Other Examples
38958662,"Tensorflow: What are the ""output_node_names"" for freeze_graph.py in the model_with_buckets model?","<p>I trained a <code>tf.nn.seq2seq.model_with_buckets</code> with <code>seq2seq = tf.nn.seq2seq.embedding_attention_seq2seq</code> very similar to the example in the <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html"" rel=""noreferrer"">Tensorflow Tutorial</a>.</p>

<p>Now I would like to freeze the graph using <code>freeze_graph.py</code>. How can I find the ""output_node_names"" in my model?  </p>
",2016-08-15 16:05:28,"<p>You can choose names for the nodes in your model by passing the optional <code>name=""myname""</code> argument to pretty much any Tensorflow operator that builds a node. Tensorflow will pick names for graph nodes automatically if you don't specify them, but if you want to identify those nodes to a tool like freeze_graph.py, then it's best to choose the names yourself. Those names are what you pass to output_node_names.</p>
",6718251,153,https://stackoverflow.com/questions/38958662,Inadequate Examples
39008821,Tensorflow: When to use tf.expand_dims?,"<p>Tensorflow tutorials include the use of <code>tf.expand_dims</code> to add a &quot;batch dimension&quot; to a tensor. I have read the docs for this function but it still is rather mysterious to me. Does anyone know exactly under what circumstances this must be used?</p>
<p>My code is below. My intent is to calculate a loss based on the distance between the predicted and actual bins. (E.g. if <code>predictedBin = 10</code> and <code>truthBin = 7</code> then <code>binDistanceLoss = 3</code>).</p>
<pre class=""lang-py prettyprint-override""><code>batch_size = tf.size(truthValues_placeholder)
labels = tf.expand_dims(truthValues_placeholder, 1)
predictedBin = tf.argmax(logits)
binDistanceLoss = tf.abs(tf.sub(labels, logits))
</code></pre>
<p>In this case, do I need to apply <code>tf.expand_dims</code> to <code>predictedBin</code> and <code>binDistanceLoss</code>? Thanks in advance.</p>
",2016-08-18 01:57:21,"<p><code>expand_dims</code> will not add or reduce elements in a tensor, it just changes the shape by adding <code>1</code> to dimensions. For example, a vector with 10 elements could be treated as a 10x1 matrix.</p>

<p>The situation I have met to use <code>expand_dims</code> is when I tried to build a ConvNet to classify grayscale images. The grayscale images will be loaded as matrix of size <code>[320, 320]</code>. However, <code>tf.nn.conv2d</code> require input to be <code>[batch, in_height, in_width, in_channels]</code>, where the <code>in_channels</code> dimension is missing in my data which in this case should be <code>1</code>. So I used <code>expand_dims</code> to add one more dimension.</p>

<p>In your case, I do not think you need <code>expand_dims</code>.</p>
",3444294,2825,https://stackoverflow.com/questions/39008821,Documentation Replication on Other Examples
39133312,Why does setting an initialization value prevent placing a variable on a GPU in TensorFlow?,"<p>I get an exception when I try to run the following very simple TensorFlow code, although I virtually copied it from the documentation:</p>

<pre><code>import tensorflow as tf

with tf.device(""/gpu:0""):
  x = tf.Variable(0, name=""x"")

sess = tf.Session()
sess.run(x.initializer) # Bombs!
</code></pre>

<p>The exception is:</p>

<pre><code>tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to
node 'x': Could not satisfy explicit device specification '/device:GPU:0' because
no supported kernel for GPU devices is available.
</code></pre>

<p>If I change the variable's initial value to <code>tf.zeros([1])</code> instead, everything works fine:</p>

<pre><code>import tensorflow as tf

with tf.device(""/gpu:0""):
  x = tf.Variable(tf.zeros([1]), name=""x"")

sess = tf.Session()     
sess.run(x.initializer)  # Works fine
</code></pre>

<p>Any idea what's going on?</p>
",2016-08-24 21:28:59,"<p>This error arises because <code>tf.Variable(0, ...)</code> defines a variable of element type <code>tf.int32</code>, and there is no kernel that implements <code>int32</code> variables on GPU in the standard TensorFlow distribution. When you use <code>tf.Variable(tf.zeros([1]))</code>, you're defining a variable of element type <code>tf.float32</code>, which <em>is</em> supported on GPU.</p>

<p>The story of <code>tf.int32</code> on GPUs in TensorFlow is a long one. While it's technically easy to support integer operations running on a GPU, our experience has been that most integer operations actually take place on the <strong>metadata</strong> of tensors, and this metadata lives on the CPU, so it's more efficient to operate on it there. As a short-term workaround, several kernel registrations for <code>int32</code> on GPUs were removed. However, if these would be useful for your models, it would be possible to <a href=""https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html"" rel=""noreferrer"">add them as custom ops</a>.</p>

<hr>

<p><strong>Source:</strong> In TensorFlow 0.10, the Variable-related kernels are <a href=""https://github.com/tensorflow/tensorflow/blob/854146110b5ec6b087fc6f2e18f35cc3b98e7186/tensorflow/core/kernels/variable_ops.cc#L55"" rel=""noreferrer"">registered</a> using the <a href=""https://github.com/tensorflow/tensorflow/blob/854146110b5ec6b087fc6f2e18f35cc3b98e7186/tensorflow/core/framework/register_types.h#L162"" rel=""noreferrer""><code>TF_CALL_GPU_NUMBER_TYPES()</code></a> macro. The current ""GPU number types"" are <code>tf.float16</code>, <code>tf.float32</code>, and <code>tf.float64</code>.</p>
",38626,47323,https://stackoverflow.com/questions/39133312,Documentation Replicability
39210093,regarding the correct way to understand the result of tf.pad,"<p>When reading the document for <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#pad"" rel=""noreferrer"">tf.pad</a>, I feel quite confusing about the example given in the tutorial. For instance, padding is <code>[[1,1,],[2,2]]</code>, how does it cause the resulting tensor has the shape as shown in the figure. Besides, what's the mechanism to generate those padded values, e.g., the ones marked in red circle. It is not very clear how to connect the explanation with the example.</p>

<p><a href=""https://i.stack.imgur.com/7OHis.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7OHis.jpg"" alt=""enter image description here""></a></p>
",2016-08-29 15:38:03,"<p>the first pair in the padding tuple is the horizontal padding and the second pair is the vertical padding </p>

<p>the reflected padding seems to quite literally reflect as if you placed a mirror on the last digit before the padding </p>

<p>input: 123</p>

<p>output for [2,2] horizontal:  32 123 21</p>

<p>same logic for horizontal </p>

<p>Symmetric seems to do the same thing, except it also repeats the boundary number first</p>

<p>21 123 32</p>

<p>the diagonals (corners) apply the padding scheme to the vertical padding output</p>

<p>reflected with [1,1],[2,2] and input:</p>

<pre><code>123
456
</code></pre>

<p>output:</p>

<pre><code>65 456 54
32 123  23
65  456 54
32  123 21
</code></pre>
",785099,5403,https://stackoverflow.com/questions/39210093,Lack of Alternative Solutions/Documentation
41109652,How to read Ogg or MP3 audio files in a TensorFlow graph?,"<p>I've seen image decoders like <code>tf.image.decode_png</code> in TensorFlow, but how about reading audio files (WAV, Ogg, MP3, etc.)? Is it possible without <code>TFRecord</code>?</p>

<p>E.g. something like <a href=""https://stackoverflow.com/questions/33849617/how-do-i-convert-a-directory-of-jpeg-images-to-tfrecords-file-in-tensorflow"">this</a>:</p>

<pre><code>filename_queue = tf.train.string_input_producer(['my-audio.ogg'])
reader = tf.WholeFileReader()
key, value = reader.read(filename_queue)
my_audio = tf.audio.decode_ogg(value)
</code></pre>
",2016-12-12 21:11:34,"<p>The answer from @sygi is unfortunately not supported in TensorFlow 2.x. An alternative solution would be to use some external library (e.g. <a href=""https://github.com/jiaaro/pydub/blob/master/API.markdown"" rel=""noreferrer"">pydub</a> or <a href=""https://librosa.github.io/librosa/generated/librosa.core.load.html"" rel=""noreferrer"">librosa</a>) to implement the mp3 decoding step, and integrate it in the pipeline through the use of <code>tf.py_function</code>. So you can do something along the lines of:</p>

<pre><code>from pydub import AudioSegment
import tensorflow as tf

dataset = tf.data.Dataset.list_files('path/to/mp3s/*')

def decode_mp3(mp3_path):
    mp3_path = mp3_path.numpy().decode(""utf-8"")
    mp3_audio = AudioSegment.from_file(mp3_path, format=""mp3"")
    return mp3_audio.get_array_of_samples()

dataset = dataset.map(lambda path:
    tf.py_function(func=decode_mp3, inp=[path], Tout=tf.float32))

for features in dataset.take(3):
    data = features.numpy()
    plt.plot(data)
    plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/to46j.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/to46j.png"" alt=""enter image description here""></a></p>
",7287271,2712,https://stackoverflow.com/questions/41109652,Documentation Replicability
41283115,"Tensorflow, difference between tf.nn.softmax_cross_entropy_with_logits and tf.nn.sparse_softmax_cross_entropy_with_logits","<p>I have read the <a href=""https://www.tensorflow.org/api_docs/python/nn/classification"" rel=""nofollow noreferrer"">docs of both functions</a>, but as far as I know, for function <code>tf.nn.softmax_cross_entropy_with_logits(logits, labels, dim=-1, name=None)</code>, the result is the cross entropy loss, in which the dimensions of <code>logits</code> and <code>labels</code> are the same.</p>

<p>But, for function <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>, the dimensions of <code>logits</code> and <code>labels</code> are not the same?</p>

<p>Could you give a more detail example of <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>?</p>
",2016-12-22 12:23:08,"<p>The difference is that <code>tf.nn.softmax_cross_entropy_with_logits</code> doesn't assume that the classes are mutually exclusive: </p>

<blockquote>
  <p>Measures the probability error in discrete classification tasks in
  which each class is independent and not mutually exclusive. For
  instance, one could perform multilabel classification where a picture
  can contain both an elephant and a dog at the same time.</p>
</blockquote>

<p>Compare with <code>sparse_*</code>:</p>

<blockquote>
  <p>Measures the probability error in discrete classification tasks in
  which the classes are mutually exclusive (each entry is in exactly one
  class). For example, each CIFAR-10 image is labeled with one and only
  one label: an image can be a dog or a truck, but not both.</p>
</blockquote>

<p>As such, with sparse functions, the dimensions of <code>logits</code> and <code>labels</code> are not the same: <code>labels</code> contain one number per example, whereas <code>logits</code> the number of classes per example, denoting probabilities.</p>
",5046896,17040,https://stackoverflow.com/questions/41283115,Requesting (Additional) Documentation/Examples
41294094,Is there a difference between a placeholder and variable when not building a model?,"<p>I am trying to understand the difference between a placeholder and a variable in TensorFlow:</p>

<pre><code>X = tf.placeholder(""float"")
W = tf.Variable(rng.randn(), name=""weight"")
</code></pre>

<p>I also read the Stack Overflow's question below. I understand the difference when they are the input of a model.</p>

<blockquote>
  <p><a href=""https://stackoverflow.com/q/38296200/3924118"">InvalidArgumentError: You must feed a value for placeholder tensor Placeholder</a></p>
</blockquote>

<p>However, in general, if we are not building a model, is there still a difference between <code>tf.placeholder()</code> and <code>tf.Variable()</code>?</p>
",2016-12-23 01:39:36,"<h3>Placeholder</h3>

<p>A placeholder is used for feeding external data into a Tensorflow computation (stuff outside the graph). Here's some documentation: (<a href=""https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/#feeding"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/#feeding</a>)</p>

<blockquote>
  <p>TensorFlow's feed mechanism lets you inject data into any Tensor in a
  computation graph. A python computation can thus feed data directly
  into the graph.</p>
</blockquote>

<p>I personally would draw an analogy from placeholders to reading from standard input.</p>

<pre><code>x = raw_input()
X = tf.placeholder(""float"")
</code></pre>

<p>When you read from standard input, you need to ""inject data"" from an external source. Same with a placeholder. It lets you ""inject data"" that's external to the computation graph.</p>

<p>If you're training a learning algorithm, the clear use case of placeholder is to feed in your training data. The training data isn't stored in the computation graph. How are you going to get it into the graph? By injecting it through a placeholder. A placeholder is basically you telling the graph ""I don't have this for you yet. But I'll have it for you when I ask you to run.""</p>

<h3>Variable</h3>

<p>A variable is used to store state in your graph. It requires an initial value. One use case could be representing weights of a neural network or something similar. Here's documentation: (<a href=""https://www.tensorflow.org/api_docs/python/tf/Variable"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/Variable</a>)</p>

<blockquote>
  <p>A variable maintains state in the graph across calls to run(). You add
  a variable to the graph by constructing an instance of the class
  Variable.</p>
  
  <p>The Variable() constructor requires an initial value for the variable,
  which can be a Tensor of any type and shape. The initial value defines
  the type and shape of the variable. After construction, the type and
  shape of the variable are fixed. The value can be changed using one of
  the assign methods.</p>
</blockquote>

<p>I personally would draw an analogy between Tensorflow Variables and assigning a variable in Python to anything that is not dependent on external stuff. For example,</p>

<pre><code># Tensorflow:
W = tf.Variable(rng.randn(), name=""weight"")

# Standard python:
w = 5
w = ""hello""
w = [1, 2, 3, 4, 5]
</code></pre>

<p><code>W</code> represents some sort of result of your computation. Just like how you must initialize all your variables in Python (you can't just run a command <code>x</code> you have to say <code>x = ...something...</code>), you have to initialize all Variable objects in Tensorflow.</p>

<h3>Variable vs. Placeholder</h3>

<p>There's not much related between <code>tf.Variable</code> and <code>tf.placeholder</code> in my opinion. You use a <code>Variable</code> if you need to store state. You use a <code>placeholder</code> if you need to input external data.</p>

<p>If you are not building a model, you should still use <code>tf.placeholder</code> if you want to insert external data that you don't necessarily have while you're defining the graph. If you are not building a model, you still need <code>tf.Variable</code> if you want to store some kind of result of your computation while the graph is being run.</p>

<h3>Why have both?</h3>

<p>I'm not an expert in Tensorflow, so I can only speculate as to why the design has both. </p>

<p>A big difference between placeholders and variables is that placeholders can have variable size, but the shape of a <code>tf.Variable</code> must be specified while constructing the graph.</p>

<p>Variable size placeholders sense: maybe I only want to input a training batch of size 5 right now, but maybe I want to increase the batch size later on. Maybe I don't know ahead of time how many training examples I'm going to get.</p>

<p>Variable size variables don't make sense: <code>tf.Variable</code> holds the learned parameters of your model, and the number of parameters shouldn't change. Furthermore, Tensorflow extends to distributed computation. If you had <code>Variables</code> whose shape changed throughout the computation, it would be very difficult to keep it properly distributed among 1000 computers.</p>

<p>Usually, you build a model and all parameters are known ahead of time, so that's what <code>tf.Variable</code> is probably used to represent. <code>tf.placeholder</code> is probably for everything else outside of your model (or computation graph) and so that can be more flexible.</p>
",3993270,24326,https://stackoverflow.com/questions/41294094,Documentation Ambiguity
39493229,How to use tf.nn.max_pool_with_argmax correctly,"<p>currently I play a little bit around with tensorflow to create a better understanding of machine learning an tensorflow itself. Therefore I want to visualize the methods (as much as possible) of tensorflow. To visualize max_pool I loaded an image and perform the method. After that I displayed both: input and output image.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import cv2
import numpy as np

import matplotlib.pyplot as plt

image = cv2.imread('lena.png')
image_tensor = tf.expand_dims(tf.Variable(image, dtype=tf.float32), 0)

#output, argmax = tf.nn.max_pool_with_argmax(image_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')
output = tf.nn.max_pool(image_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')

init = tf.initialize_all_variables()
session = tf.Session()
session.run(init)

output = session.run(output)

session.close()

image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
plt.figure()
plt.imshow(image)
plt.show()

output = cv2.cvtColor(output[0], cv2.COLOR_RGB2BGR)
plt.figure()
plt.imshow(255-output)
plt.show() 
</code></pre>

<p>Everything works fine and I get this output (as expected)</p>

<p><a href=""https://i.stack.imgur.com/FFoKmm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FFoKmm.png"" alt=""image (input)""></a>
<a href=""https://i.stack.imgur.com/YZhogm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YZhogm.png"" alt=""enter image description here""></a></p>

<p>Now I wanted to test the method <code>tf.nn.max_pool_with_argmax</code> to get the argmax of the pooling operations. But if I uncomment the line </p>

<p><code>output, argmax = tf.nn.max_pool_with_argmax(image_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')
</code></p>

<p>Python crashes with </p>

<blockquote>
  <p>tensorflow.python.framework.errors.InvalidArgumentError: No OpKernel was registered to support Op 'MaxPoolWithArgmax' with these attrs
       [[Node: pool1 = MaxPoolWithArgmaxT=DT_FLOAT, Targmax=DT_INT64, ksize=[1, 2, 2, 1], padding=""SAME"", strides=[1, 2, 2, 1]]]</p>
</blockquote>

<p>I don't have an idea which argument is wrong because every argument should be correct (<a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#max_pool_with_argmax"" rel=""nofollow noreferrer"">tensorflow docs</a>) ...</p>

<p>Does anyone know what went wrong?</p>
",2016-09-14 14:37:22,"<p>From a look at <a href=""https://github.com/tensorflow/tensorflow/blob/bc64f05d4090262025a95438b42a54bfdc5bcc80/tensorflow/core/kernels/maxpooling_op.cc#L672"" rel=""noreferrer"">the implementation</a>, it appears that the <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#max_pool_with_argmax"" rel=""noreferrer""><code>tf.nn.max_pool_with_argmax()</code></a> is only implemented for GPU. If you are running the CPU-only build of TensorFlow, then you would get an error of the form <code>""No OpKernel was registered to support Op 'MaxPoolWithArgmax' with these attrs ...""</code>.</p>

<p>(This seems like a place where the documentation and the error message could be improved.)</p>
",3524844,3268,https://stackoverflow.com/questions/39493229,Documentation Replicability
41780655,What is the difference between tf.group and tf.control_dependencies?,"<p>Aside from <code>tf.control_dependencies</code> being a context manager (i.e. used with Python <code>with</code>), what's the difference between <code>tf.group</code> and <code>tf.control_dependencies</code>? </p>

<p>When should which be used? </p>

<p>Is it that <code>tf.group</code> doesn't have any particular order of operations? I'd assume <code>tf.group([op_1, op_2, op_3])</code> executes ops in the list's order, but maybe that's not the case? The docstring doesn't specify a behaviour.</p>
",2017-01-21 14:44:13,"<p>If you look at the graphdef, the <code>c=tf.group(a, b)</code> produces the same graph as </p>

<pre><code>with tf.control_dependencies([a, b]):
    c = tf.no_op() 
</code></pre>

<p>There's no specific order in which ops will run, TensorFlow tries to execute operations as soon as it can (i.e. in parallel).</p>
",7287271,2712,https://stackoverflow.com/questions/41780655,Documentation Replicability
40195549,tf.rank function in Tensorflow,"<p>I ma trying to understand tf.rank function in tensorflow. From the documentation <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#rank"" rel=""nofollow"">here</a>, I understood that rank should return the number of distinct elements in the tensor. </p>

<p>Here x and weights are 2 distinct 2*2 tensors with 4 distinct elemnts in each of them. However, rank() function outputs are:</p>

<blockquote>
  <p>Tensor(""Rank:0"", shape=(), dtype=int32) Tensor(""Rank_1:0"", shape=(),
  dtype=int32)</p>
</blockquote>

<p>Also, for the tensor x, I used tf.constant() with dtype = float to convert ndarray into float32 tensor but the rank() still outputs as int32.</p>

<pre><code>g = tf.Graph()
with g.as_default():
    weights = tf.Variable(tf.truncated_normal([2,2]))
    x = np.asarray([[1 , 2], [3 , 4]])
    x = tf.constant(x, dtype = tf.float32)
    y = tf.matmul(weights, x)
    print (tf.rank(x), tf.rank(weights))


with tf.Session(graph = g) as s:
    tf.initialize_all_variables().run()
    print (s.run(weights), s.run(x))
    print (s.run(y))
</code></pre>

<p>How should I interpret the output.</p>
",2016-10-22 18:23:37,"<p>Firstly, <code>tf.rank</code> returns the dimension of a tensor, not the number of elements. For instance, the output from <code>tf.rank</code> called for the 2x2 matrix would be 2. </p>

<p>To print the rank of a tensor, create an appropriate node, e.g. <code>rank = tf.rank(x)</code> and then evaluate this node using a <code>Session.run()</code>, as you've done for weights and x. Execution of <code>print (tf.rank(x), tf.rank(weights))</code> expectedly prints out description of tensors, as <code>tf.rank(x), tf.rank(weights)</code> are nodes of the graph, not the variables with defined values.</p>
",2289031,1183,https://stackoverflow.com/questions/40195549,Documentation Replication on Other Examples
39650169,SparseTensor equivalent of tf.tile?,"<p>There's a <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#tile"" rel=""nofollow"">tf.tile</a> function, which takes a tensor and copies it a given number of times.</p>

<pre><code>f = tf.tile([5], [3])
f.eval() == array([3, 3, 3], dtype=int32)
</code></pre>

<p>How to achieve something similar with <code>SparseTensor</code>s:</p>

<pre><code>g = tf.SparseTensorValue([[0, 0]], values=[5], shape=[1, 1])
tiled = tf.tile(g, [10, 1]) &lt;- gives ValueError: Argument must be a dense tensor
</code></pre>

<p>?</p>
",2016-09-22 23:06:51,"<p>Ok, I have found a solution (that works on <code>SparseTensor</code>s, but not on <code>SparseTensorValue</code>s):</p>

<pre><code>tiled = tf.sparse_concat(0, [g] * 10)
</code></pre>
",1951176,4607,https://stackoverflow.com/questions/39650169,Documentation Replication on Other Examples
40347742,Closed Tensorflow FIFO queue not throwing exception,"<p>I'm trying to run the following code on a CPU machine:</p>

<pre><code>import numpy as np
import tensorflow as tf


feature_dim = 3
nr_frames = 8

# create dummy features and targets
features = np.zeros((nr_frames, feature_dim))
targets = np.zeros((nr_frames))
for frame in range(nr_frames):
    for item in range(feature_dim):
        features[frame, item] = int(""%d%d"" % (frame, item))
    targets[frame] = int(""%d"" % (frame))


feature_input = tf.placeholder(tf.float32, shape=[None, feature_dim])
target_input = tf.placeholder(tf.int32, shape=[None])

fifo = tf.FIFOQueue(capacity=10000,
                    dtypes=[tf.float32, tf.int32],
                    shapes=[[feature_dim], []],
                    names=['features', 'targets'] )
enqueue_op = fifo.enqueue_many({'features' : feature_input,
                                'targets' :  target_input})

data = fifo.dequeue()

init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
coord = tf.train.Coordinator()

tf.train.start_queue_runners(sess, coord=coord)

# push dummy data to queue
sess.run(enqueue_op,
         feed_dict = {feature_input: features,
                      target_input : targets})
fifo.close()


for i in range(12):
    print ""\niteration %d"" % i
    print ""elements in databuffer: %d"" % sess.run(fifo.size())
    x = sess.run(data)
    print x
</code></pre>

<p>I'm basically creating some dummy features and targets (for a classification task), push the data to a TF FIFO queue and close the queue to simulate some behavior I will need in the fully-fledged system code.</p>

<p>Since the FIFO queue is closed, I'd expect the dequeue() function to throw an tf.errors.OutOfRangeError exception, once the queue gets empty, as explained in
<a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/io_ops.html#queues"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.11/api_docs/python/io_ops.html#queues</a></p>

<p>This should happen after iteration 7, since we have 8 elements in the features/target data. However, the program just keeps hanging - I guess due to some blocking call.</p>

<p>What am I doing wrong here?</p>

<p>RCB</p>
",2016-10-31 17:54:09,"<p>The <code>fifo.close()</code> call in your code doesn't actually close the queue: instead it returns a <code>tf.Operation</code> that, when run, will close the queue. If you replace it with the following code, you should see the exception as expected:</p>

<pre><code>close_op = fifo.close()
sess.run(close_op)

# Now `sess.run(data)` will return values until all of the elements in the queue
# have been dequeued.
</code></pre>
",7096427,3,https://stackoverflow.com/questions/40347742,Documentation Ambiguity
40879504,How to apply Drop Out in Tensorflow to improve the accuracy of neural network?,"<p>Drop-Out is regularization techniques. And I want to apply it to notMNIST data to reduce over-fitting to finish my Udacity Deep Learning Course Assignment.I have read the <a href=""https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/index.html"" rel=""noreferrer"">docs of tensorflow</a> on how to call the <code>tf.nn.dropout</code>. And here is my code</p>

<pre class=""lang-py prettyprint-override""><code># before proceeding further.
from __future__ import print_function
import numpy as np  
import tensorflow as tf
from six.moves import cPickle as pickle


pickle_file = 'notMNIST.pickle'

with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_dataset = save['train_dataset']
    train_labels = save['train_labels']
    valid_dataset = save['valid_dataset']
    valid_labels = save['valid_labels']
    test_dataset = save['test_dataset']
    test_labels = save['test_labels']
    del save  # hint to help gc free up memory
    print('Training set', train_dataset.shape, train_labels.shape)
    print('Validation set', valid_dataset.shape, valid_labels.shape)
    print('Test set', test_dataset.shape, test_labels.shape)


image_size = 28
num_labels = 10

def reformat(dataset, labels):
    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels

    train_dataset, train_labels = reformat(train_dataset, train_labels)
    valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
    test_dataset, test_labels = reformat(test_dataset, test_labels)
    print('Training set', train_dataset.shape, train_labels.shape)
    print('Validation set', valid_dataset.shape, valid_labels.shape)
    print('Test set', test_dataset.shape, test_labels.shape)

    def accuracy(predictions, labels):
        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))  / predictions.shape[0])


# ReLU neuron
# param
training_epochs = 30
batch_size = 521
display_step = 1
n_input = 784 # img shape: 28*28
n_classes = 10 # MNIST total classes (0-9 digits)

# hyper-parameter
n_hidden_1 = 256 
learning_rate = 0.05
lambda_term = 0.01


graph = tf.Graph()
with graph.as_default():
    # init weights
    weights_hiden =  tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=np.sqrt(n_input)))
    weights_out = tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=np.sqrt(n_hidden_1)))

    biases_hidden = tf.Variable(tf.random_normal([n_hidden_1]))
    biases_out = tf.Variable(tf.random_normal([n_classes]))

    x = tf.placeholder(""float"", [None, n_input])
    y = tf.placeholder(""float"", [None, n_classes])

    def model(x, weights_hiden, weights_out, biases_hidden, biases_out):
        # hidden layer with RELU activation
        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights_hiden), biases_hidden))
        # apply DropOut to hidden layer
        keep_prob = tf.placeholder(tf.float32)  # DROP-OUT here
        drop_out = tf.nn.dropout(layer_1, keep_prob)  # DROP-OUT here
        # output layer with linear activation
        out_layer = tf.matmul(layer_1, weights_out) + biases_out
        return out_layer

    # Construct model
    pred = model(x, weights_hiden, weights_out, biases_hidden, biases_out)

    # Define loss and optimizer
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y) +
                          lambda_term * tf.nn.l2_loss(weights_hiden) + 
                          lambda_term * tf.nn.l2_loss(weights_out) +
                          lambda_term * tf.nn.l2_loss(biases_hidden) + 
                          lambda_term * tf.nn.l2_loss(biases_out))
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)


# run the graph
with tf.Session(graph=graph) as sess:
    tf.initialize_all_variables().run()
    print('Initialized')
    # Training cycle
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(train_dataset.shape[0]/batch_size)
        # Loop over all batches
        for i in range(total_batch):
            batch_x = train_dataset[(i*batch_size):((i*batch_size) + batch_size), :]
            batch_y = train_labels[(i*batch_size):((i*batch_size) + batch_size), :]
            # Run optimization op (backprop) and cost op (to get loss value)
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})
            # Compute average loss
            avg_cost += c / total_batch
        # Display logs per epoch step
        if epoch % display_step == 0:
            print(""Epoch:"", '%04d' % (epoch+1), ""cost="", ""{:.9f}"".format(avg_cost))
    print(""Optimization Finished!"")

    # Test model
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
    print(""Test data accuracy:"", accuracy.eval({x: test_dataset, y: test_labels}))
    print(""Valid data accuracy:"", accuracy.eval({x: valid_dataset, y: valid_labels}))
</code></pre>

<p>The <code>tf.nn.dropout</code> is called in function <code>model()</code>, but after I applied the DropOut technique to the neural network, the accuracy did seem any change, here is the result:</p>

<pre><code>Epoch: 0001 cost= 579980.086977807
Epoch: 0002 cost= 238859.802382506
Epoch: 0003 cost= 90672.733752856
Epoch: 0004 cost= 32649.040985028
Epoch: 0005 cost= 11325.878361874
Epoch: 0006 cost= 3866.805511076
Epoch: 0007 cost= 1357.785540469
Epoch: 0008 cost= 519.381747333
Epoch: 0009 cost= 225.359804119
Epoch: 0010 cost= 110.099476707
Epoch: 0011 cost= 55.212384386
Epoch: 0012 cost= 28.469241683
Epoch: 0013 cost= 14.511494627
Epoch: 0014 cost= 6.567228943
Epoch: 0015 cost= 3.186372240
Epoch: 0016 cost= 1.701917576
Epoch: 0017 cost= 1.041632473
Epoch: 0018 cost= 0.843376874
Epoch: 0019 cost= 0.786183911
Epoch: 0020 cost= 0.775412846
Epoch: 0021 cost= 0.782965020
Epoch: 0022 cost= 0.796788171
Epoch: 0023 cost= 0.814522117
Epoch: 0024 cost= 0.832090579
Epoch: 0025 cost= 0.849197715
Epoch: 0026 cost= 0.867473578
Epoch: 0027 cost= 0.889561496
Epoch: 0028 cost= 0.921837020
Epoch: 0029 cost= 16.655304543
Epoch: 0030 cost= 1.421570476
Optimization Finished!
Test data accuracy: 0.8775
Valid data accuracy: 0.8069
</code></pre>

<p>How can I apply DropOut by Tensorflow to improve the accuracy of the network? Thank you!</p>
",2016-11-30 03:00:39,"<p>In the graph, I'd suggest to move <code>keep_prob = tf.placeholder(tf.float32)</code> outside of the <code>model</code> function to make it global.</p>

<pre class=""lang-py prettyprint-override""><code>with graph.as_default():
    ...
    x = tf.placeholder(""float"", [None, n_input])
    y = tf.placeholder(""float"", [None, n_classes])
    keep_prob = tf.placeholder(tf.float32)

    def model(x, weights_hiden, weights_out, biases_hidden, biases_out, keep_prob):
        # hidden layer with RELU activation
        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights_hiden), biases_hidden))
        # apply DropOut to hidden layer
        drop_out = tf.nn.dropout(layer_1, keep_prob)  # DROP-OUT here
        # output layer with linear activation
        out_layer = tf.matmul(drop_out, weights_out) + biases_out
        return out_layer
    ...
</code></pre>

<p>When running <code>session</code>, feed a desired <code>keep_prob</code> value during training time, and feed 1.0 to <code>keep_prob</code> during reference (validation and/or testing) time.</p>

<pre class=""lang-py prettyprint-override""><code># run the graph
with tf.Session(graph=graph) as sess:
    tf.initialize_all_variables().run()
    ...
    for epoch in range(training_epochs):
        ...
        for i in range(total_batch):
            batch_x = ...
            batch_y = ...
            # Run optimization op (backprop) and cost op (to get loss value)
            # Feed a value &lt; 1.0 for keep prob during training
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y, keep_prob : 0.5})
    ...
    # Feed 1.0 for keep prob during testing
    print(""Test data accuracy:"", accuracy.eval({x: test_dataset, y: test_labels, keep_prob : 1.0}))
    print(""Valid data accuracy:"", accuracy.eval({x: valid_dataset, y: valid_labels, keep_prob : 1.0}))
</code></pre>
",5046896,17040,https://stackoverflow.com/questions/40879504,Documentation Replication on Other Examples
40966850,Difference between tf.clip_by_average_norm and tf.clip_by_norm in tensorflow,"<p>I'm not completely sure of the difference between the two gradients clipping operator <code>clip_by_average_norm</code> and <code>clip_by_norm</code>. From the documentation, the difference seems to be that <code>clip_by_norm</code> uses <code>l2norm</code> instead of <code>l2norm_avg</code>.</p>

<p>I understand what the L2-norm of a gradient is but what does ""average L2-norm"" correspond too ?</p>

<p>Documentation reference</p>

<ul>
<li><a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#clip_by_norm"" rel=""nofollow noreferrer"">clip_by_norm</a></li>
<li><a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#clip_by_average_norm"" rel=""nofollow noreferrer"">clip_by_average_norm</a></li>
</ul>
",2016-12-05 03:56:14,"<p>The doc are a bit ambiguous, from <a href=""https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/python/kernel_tests/clip_ops_test.py#L280"" rel=""nofollow noreferrer"">test</a> and <a href=""https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/python/ops/clip_ops.py#L268"" rel=""nofollow noreferrer"">impl</a>, it seems <code>avg_norm</code> is <code>norm/len(vector)</code></p>

<pre><code> def testClipByAverageNormClipped(self):
    # Norm clipping when average clip_norm &lt; 0.83333333
    with self.test_session():
      x = tf.constant([-3.0, 0.0, 0.0, 4.0, 0.0, 0.0], shape=[2, 3])
      # Average norm of x = sqrt(3^2 + 4^2) / 6 = 0.83333333
</code></pre>
",4172685,5051,https://stackoverflow.com/questions/40966850,Documentation Replication on Other Examples
41125728,"When should tf.add(t, 0) be used instead of tensor t directly?","<p>In <a href=""https://github.com/tensorflow/tensorflow/blob/294442996b2aeff00b1bfdc7e7169f7cb35bbf3d/tensorflow/contrib/layers/python/layers/layers.py#L625"" rel=""nofollow noreferrer"">tensorflow.contrib.slim.batch_norm</a>, <code>math_ops.add(moving_mean, 0)</code> is used to copy the value of <code>moving_mean</code>, which is <a href=""https://github.com/tensorflow/tensorflow/blob/294442996b2aeff00b1bfdc7e7169f7cb35bbf3d/tensorflow/contrib/layers/python/layers/layers.py#L629"" rel=""nofollow noreferrer"">passed to <code>nn.moments</code></a> subsequently.</p>

<p>Would it be a problem if we just pass <code>moving_mean</code> to <code>nn.moments</code> directly?</p>

<p>Are there any guidelines on the use of <em>copy</em> operation (<code>tf.add(t, 0)</code>)?</p>
",2016-12-13 16:17:58,"<p>The problem is that the order in which moving_mean is updated, could cause the gradients to use the updated version of moving_mean, instead of the original moving_mean used as shift. So to make sure the same value is used in the forward pass and in the backward pass we make an explicit copy.</p>
",6770703,791,https://stackoverflow.com/questions/41125728,Documentation Completeness
41308515,Force copy of tensor when enqueuing,"<p>first, I'm not sure if the title is very good, but it was the best I could come up with given my understanding of the situation.</p>

<p>The background is that I'm trying to understand how queues work in tensorflow and ran into the following issue which puzzled me.</p>

<p>I have a variable <em>n</em>, which I enqueue to a <em>tf.FIFOQueue</em>, and then I increment the variable. This is repeated several times, and one would expect a result similar to 0, 1, 2, ... However, when emptying the queue all values are the same.</p>

<p>More precisely, the code is as follows:</p>

<pre><code>from __future__ import print_function

import tensorflow as tf

q = tf.FIFOQueue(10, tf.float32)

n = tf.Variable(0, trainable=False, dtype=tf.float32)
inc = n.assign(n+1)
enqueue = q.enqueue(n)

init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)

sess.run(enqueue)
sess.run(inc)

sess.run(enqueue)
sess.run(inc)

sess.run(enqueue)
sess.run(inc)

print(sess.run(q.dequeue()))
print(sess.run(q.dequeue()))
print(sess.run(q.dequeue()))
</code></pre>

<p>Which I expect would print:</p>

<pre><code>0.0
1.0
2.0
</code></pre>

<p>Instead I get the following result:</p>

<pre><code>3.0
3.0
3.0
</code></pre>

<p>It seems like I'm pushing some pointer to n to the queue, instead of the actual value, which is what I want. However, I don't really have any actual understanding of tensorflow internals, so maybe something else is going on?</p>

<p>I tried changing</p>

<pre><code>enqueue = q.enqueue(n)
</code></pre>

<p>to</p>

<pre><code>enqueue = q.enqueue(tf.identity(n))
</code></pre>

<p>since answers to <a href=""https://stackoverflow.com/questions/33717772/how-can-i-copy-a-variable-in-tensorflow"">How can I copy a variable in tensorflow</a> and <a href=""https://stackoverflow.com/questions/34877523/in-tensorflow-what-is-tf-identity-used-for"">In TensorFlow, what is tf.identity used for?</a> gives me the impression that it might help, but it does not change the result. I also tried adding a tf.control_dependencies(), but again, all values are the same when dequeueing.</p>

<p>Edit: The output above is from running the code on a computer with a single CPU, when trying to see if there was some difference between different versions of tensorflow, I noticed if I run the code on a computer with CPU and GPU I get the ""expected"" result. Indeed, if I run with CUDA_VISIBLE_DEVICES="""" I get the result above, and with CUDA_VISIBLE_DEVICES=""0"" I get the ""expected"" result.</p>
",2016-12-23 22:30:38,"<p>To force a non-caching read you can do</p>

<pre><code>q.enqueue(tf.add(q, 0))
</code></pre>

<p>This is what's <a href=""https://github.com/tensorflow/tensorflow/blob/294442996b2aeff00b1bfdc7e7169f7cb35bbf3d/tensorflow/contrib/layers/python/layers/layers.py#L625"" rel=""nofollow noreferrer"">currently done</a> by the batch-normalization layer to force a copy.</p>

<p>Semantics of how variables get read vs. referenced are in the process of getting revamped so they are temporarily non-intuitive. In particular, I expected <code>q.enqueue(v.read_value())</code> to force a non-caching read, but it doesn't fix your example on TF 0.12rc1</p>

<p>Using GPU machine puts variable on GPU, while Queue is CPU only, so <code>enqueue</code> op forces a GPU->CPU copy.</p>
",6850023,178,https://stackoverflow.com/questions/41308515,Documentation Replication on Other Examples
41482823,What do the functions tf.squeeze and tf.nn.rnn do?,"<p>What do the functions tf.squeeze and tf.nn.rnn do?</p>

<p>I searched these API, but I can't find argument, examples etc.
Also, what is the shape of <code>p_inputs</code> formed by the following code using <code>tf.squeeze</code>, and what is the meaning and case of using <code>tf.nn.rnn</code>?</p>

<pre><code>batch_num = 10
step_num = 2000
elem_num = 26

p_input = tf.placeholder(tf.float32, [batch_num, step_num, elem_num])
p_inputs = [tf.squeeze(t, [1]) for t in tf.split(1, step_num, p_input)]
</code></pre>
",2017-01-05 10:39:32,"<p>The best source of answers to questions like these is the <a href=""https://www.tensorflow.org/api_docs/"" rel=""noreferrer"">TensorFlow API documentation</a>. The two functions you mentioned create operations and symbolic tensors in a dataflow graph. In particular:</p>

<ul>
<li><p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/squeeze"" rel=""noreferrer""><code>tf.squeeze()</code></a> function returns a tensor with the same value as its first argument, but a different shape. It removes dimensions whose size is one. For example, if <code>t</code> is a tensor with shape <code>[batch_num, 1, elem_num]</code> (as in your question), <code>tf.squeeze(t, [1])</code> will return a tensor with the same contents but size <code>[batch_num, elem_num]</code>.</p></li>
<li><p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn"" rel=""noreferrer""><code>tf.nn.rnn()</code></a> function returns a pair of results, where the first element represents the outputs of a recurrent neural network for some given input, and the second element represents the final state of that network for that input. The TensorFlow website has a <a href=""https://www.tensorflow.org/tutorials/recurrent/"" rel=""noreferrer"">tutorial on recurrent neural networks</a> with more details.</p></li>
</ul>
",7378465,129,https://stackoverflow.com/questions/41482823,Documentation Replicability
41534593,Why would I ever use tf.concat instead of tf.stack?,"<p>Is there a good reason to use <code>tf.concat</code> instead of <code>tf.stack</code>? They seem very similar. Is it just to guarantee that the resulting tensor will have the same number of dimensions as the input list of tensors?</p>
",2017-01-08 15:55:47,"<p>Actually, I've misunderstood how <code>tf.stack</code> works. If the <code>axis</code> parameter is within the range of the existing dimensions, a new axis will be inserted at that index.</p>

<p>Example:</p>

<pre><code>import tensorflow as tf

t1 = tf.random_normal([1, 3])
t2 = tf.random_normal([1, 3])

tf.stack([t1, t2], axis=1).shape.as_list() == [1, 2, 3]
tf.concat([t1, t2], axis=1).shape.as_list() == [1, 6]
</code></pre>
",7287271,2712,https://stackoverflow.com/questions/41534593,Documentation Ambiguity
41937874,how to make tensorflow placeholder to work properly,"<p>I wrote simple code using tensorflow. I thought the code below should work. The goal of this code is simple. Just calculating simple addition. </p>

<pre><code>import tensorflow as tf
import numpy as np

data = np.array([[35.0, 40.0, 45.0]]).astype(np.float32)
print np.shape(data)
x = tf.placeholder(tf.float32, [1, 3], name='x')
y = tf.Variable(x + 5, name='y')

model = tf.initialize_all_variables()

with tf.Session() as sess:
    sess.run(model)
    result = sess.run(y, feed_dict={x: data})
    print result
</code></pre>

<p>But, the interpreter says something like this; </p>

<pre><code>Traceback (most recent call last):
File ""basic_compare.py"", line 12, in &lt;module&gt;
sess.run(model)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 717, in run
run_metadata_ptr)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 915, in _run
feed_dict_string, options, run_metadata)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _do_run
target_list, options, run_metadata)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 985, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: You must feed a value for placeholder tensor 'x' with dtype float and shape [1,3]
 [[Node: x = Placeholder[dtype=DT_FLOAT, shape=[1,3], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]

Caused by op u'x', defined at:
File ""basic_compare.py"", line 6, in &lt;module&gt;
x = tf.placeholder(tf.float32, [1, 3], name='x')
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1332, in placeholder
name=name)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 1748, in _placeholder
name=name)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 749, in apply_op
op_def=op_def)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2380, in create_op
original_op=self._default_original_op, op_def=op_def)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'x' with dtype float and shape [1,3]
 [[Node: x = Placeholder[dtype=DT_FLOAT, shape=[1,3], _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
</code></pre>

<p>I just copied what is essential to the error. I am spending lots of hours for this simple code, but I don't see any problem. Any tip would be greatly appreciated. </p>
",2017-01-30 14:06:41,"<p>You use <code>x</code> as a variable initializer, so you need to feed <code>x</code> when executing <code>tf.initialize_all_variables()</code>:</p>

<pre><code>import tensorflow as tf
import numpy as np

data = np.array([[35.0, 40.0, 45.0]]).astype(np.float32)
print np.shape(data)
x = tf.placeholder(tf.float32, [1, 3], name='x')
y = tf.Variable(x, name='y')

model = tf.initialize_all_variables()

with tf.Session() as sess:
    sess.run(model, feed_dict={x: data})
    result = sess.run(y)
    print result
</code></pre>

<p>Hope that helps!</p>
",5621202,409,https://stackoverflow.com/questions/41937874,Lack of Alternative Solutions/Documentation
40443951,Binary mask in Tensorflow,"<p>I would like to mask every other value along a particular dimension of a Tensor but don't see a good way to generate such a mask. For example</p>

<pre><code>#Masking on the 2nd dimension
a = [[1,2,3,4,5],[6,7,8,9,0]
mask = [[1,0,1,0,1],[1,1,1,1,1]]
b = a * mask #would return [[1,0,3,0,5],[6,0,8,0,0]]
</code></pre>

<p>Is there an easy way to generate such a mask?</p>

<p>Ideally I would like to do something like the following:</p>

<pre><code>mask = tf.ones_like(input_tensor)
mask[:,::2] = 0
mask * input_tensor
</code></pre>

<p>But slice assigning doesn't seem to be as simple as in Numpy.</p>
",2016-11-05 22:35:44,"<p>You can easily programmatically create such a tensor mask using python.  Then convert it to a tensor.  There's no such support in the TensorFlow API.  <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#tile"" rel=""nofollow noreferrer"">tf.tile([1,0], num_of_repeats)</a> might be a fast way to create such mask but not that great either if you have odd number of columns.</p>

<p>(Btw, if you end up creating a boolean mask, use <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#boolean_mask"" rel=""nofollow noreferrer"">tf.boolean_mask()</a>)</p>
",7120564,73,https://stackoverflow.com/questions/40443951,Documentation Ambiguity
40742947,How can I change the output of tf.cond to tuple in Tensorflow?,"<p>When I feed tuple of tensors into tf.cond, the output becomes a list not a tuple. </p>

<pre><code>output = tf.cond(sw_c, lambda: (a,b), lambda: (c,d))
</code></pre>

<p>How can  I change it to the list to the tuple?</p>
",2016-11-22 13:19:59,"<p>Convert it in a tuple</p>

<pre><code>output = tuple(tf.cond(sw_c, lambda: (a,b), lambda: (c,d)))
</code></pre>
",5522711,749,https://stackoverflow.com/questions/40742947,Documentation Replicability
40846881,"How do I finde the Code of ""tf.nn.dynamic_rnn"" in the tensorflow repository?","<p>I'm trying to understand the structure and the coding of tensorflow. While going through this tutorial ""<a href=""https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/"" rel=""nofollow noreferrer"">https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/</a>"" I have searched for the code that is used in the functions.
For example the line</p>

<pre><code>cell = GRUCell(num_neurons)  # Or LSTMCell(num_neurons)
</code></pre>

<p>uses the function GRUCell, which I can find in the file ""rnn_cell.py"" in the tensorflow repository. Furthermore the GRUCell is wrapped by a function called ""tf.nn.dynamic_rnn"" as follows:</p>

<pre><code>output, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)
</code></pre>

<p>Unfortunately I am not able to find the code for this function. Where do I find it? Everything I find is this documentation:</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md</a></p>

<p>Thanks for helping!</p>
",2016-11-28 14:39:21,"<p>You can find it and other related operations here:
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py</a></p>
",6716760,345,https://stackoverflow.com/questions/40846881,Lack of Alternative Solutions/Documentation
42022950,Which seeds have to be set where to realize 100% reproducibility of training results in tensorflow?,"<p>In a general tensorflow setup like</p>

<pre><code>model = construct_model()
with tf.Session() as sess:
    train_model(sess)
</code></pre>

<p>Where <code>construct_model()</code> contains the model definition including random initialization of weights (<code>tf.truncated_normal</code>) and <code>train_model(sess)</code> executes the training of the model -</p>

<p>Which seeds do I have to set where to ensure 100% reproducibility between repeated runs of the code snippet above? <a href=""https://www.tensorflow.org/api_docs/python/tf/random/set_random_seed"" rel=""nofollow noreferrer"">The documentation</a> for <code>tf.random.set_random_seed</code> may be concise, but left me a bit confused. I tried:</p>

<pre><code>tf.set_random_seed(1234)
model = construct_model()
    with tf.Session() as sess:
        train_model(sess)
</code></pre>

<p>But got different results each time. </p>
",2017-02-03 11:16:19,"<p>The best solution which works as of today with GPU is to install tensorflow-determinism with the following:</p>
<pre><code>pip install tensorflow-determinism
</code></pre>
<p>Then include the following code to your code</p>
<pre><code>import tensorflow as tf
import os
os.environ['TF_DETERMINISTIC_OPS'] = '1'
</code></pre>
<p>source: <a href=""https://github.com/NVIDIA/tensorflow-determinism"" rel=""nofollow noreferrer"">https://github.com/NVIDIA/tensorflow-determinism</a></p>
",1934212,9203,https://stackoverflow.com/questions/42022950,Documentation Replicability
41685279,Distributed TensorFlow: about the using of tf.train.Supervisor.start_queue_runners,"<p>I'm looking into the code for <a href=""https://github.com/tensorflow/models/tree/master/inception"" rel=""nofollow noreferrer"">distributed inception model in TF</a>, in which I have below questions about the use of <code>tf.train.Supervisor.start_queue_runners</code> in <a href=""https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py"" rel=""nofollow noreferrer"">inception_distributed_train.py</a>:</p>

<ol>
<li><p>Why do we need to explicitly call <code>sv.start_queue_runners()</code> in line
<code>264</code> and line <code>269</code> in <a href=""https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py"" rel=""nofollow noreferrer"">inception_distributed_train.py</a>? In <a href=""https://www.tensorflow.org/api_docs/python/train/distributed_execution#Supervisor.start_queue_runners"" rel=""nofollow noreferrer"">API
doc. of start_queue_runners</a>, I see there is no need for such
calls due to:</p>

<blockquote>
  <p>Note that the queue runners collected in the graph key QUEUE_RUNNERS
  are already started automatically when you create a session with the
  supervisor, so unless you have non-collected queue runners to start
  you do not need to call this explicitly.</p>
</blockquote></li>
<li><p>I noticed the values of <code>queue_runners</code> in calling
<code>sv.start_queue_runners</code> are different in line <code>264</code> and line
<code>269</code> in <a href=""https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py"" rel=""nofollow noreferrer"">inception_distributed_train.py</a>. But aren't
<code>chief_queue_runners</code> also in the collection of
<code>tf.GraphKeys.QUEUE_RUNNERS</code> (all <code>QUEUE_RUNNERS</code> are obtained in line <code>263</code>)? If
so, then there is no need for line <code>269</code> since the <code>chief_queue_runners</code> has already
been started in line <code>264</code>.</p></li>
<li><p>Besides, could you please explain to me or show me some references about what queues are created in <code>tf.train.Supervisor</code>?</p></li>
</ol>

<p>Thanks for your time!</p>
",2017-01-16 21:30:11,"<p>Not an answer, but some general notes how to find an answer :)</p>

<p>First of all, using github's blame, inception_distributed was checked in on April 13, while that comment in <code>start_queue_runners</code> was added on Apr 15th, so it's possible that functionality was changed but didn't get updated in all the places that use it.</p>

<p>You could comment-out that line and see if things still work. And if not, you could add <code>import pdb; pdb.set_trace()</code> in the place where queue runner gets created (ie <a href=""https://github.com/tensorflow/tensorflow/blob/dd01113c2b9efb16c3c3ea0a4aaa340a0494696f/tensorflow/python/training/queue_runner_impl.py#L47"" rel=""nofollow noreferrer"">here</a>) and see who is creating those extra unattended queue runners.</p>

<p>Also, Supervisor development seems to have slowed down and things are getting moved over to FooSession (from comment <a href=""https://github.com/tensorflow/tensorflow/issues/6604#issuecomment-270950324"" rel=""nofollow noreferrer"">here</a>). Those provide a more robust training architecture (your workers won't crash because of temporary network error), but there are not many examples on how to use them yet.</p>
",4394807,1217,https://stackoverflow.com/questions/41685279,Documentation Replicability
42049256,TensorFlow: Does queuing examples in parallel speed up a batch creation if tf.train.batch is already dequeuing examples in parallel?,"<p>In the TensorFlow-slim documentation, there is a ParallelReader object that can read TFRecords data in parallel through having multiple readers to take in example strings into queue. However, if I am not mistaken tf.train.batch dequeues examples from a queue and is able to do so in parallel with the argument <code>num_threads</code>. If that is the case, is it necessary that both the batch creation and reading of data must have the same speed otherwise one will cause a slower creation of a batch?</p>

<p>I am actually not very sure whether the dequeue operation happens in tf.train.batch or when a queue_runner is manually created to dequeue the examples since I believe tf.train.batch can effectively replace the queue_runner operation. Is this correct?</p>
",2017-02-05 06:38:39,"<p>As far as your first paragraph goes: the answer is that each step in your input pipeline should have enough bandwidth to feed the model (and probably not too much more). You can easily run into bottlenecks if e.g. moving from one queue to another is fast but not enough data is being fed to the first queue (because of I/O issues, for example). You could also imagine having huge numbers of parallel readers and plenty of I/O bandwidth but only a single thread responsible for collecting all of those examples being the bottleneck. Some back-of-the-envelope computation may help, but ultimately you should be <a href=""https://stackoverflow.com/questions/40191367/tensorflow-get-amount-of-samples-in-queue"">monitoring the number of examples in each queue</a>.</p>

<p>As for the second question, <a href=""https://www.tensorflow.org/api_docs/python/io_ops/input_pipeline#batch"" rel=""nofollow noreferrer"">tf.train.batch does start its own queue runner</a>. The actual enqueuing/dequeuing happens asynchronously, which is why it's important to monitor queue sizes to make sure that training isn't consuming data faster than the input pipeline is producing it.</p>
",5107084,2893,https://stackoverflow.com/questions/42049256,Documentation Replication on Other Examples
41764199,Row-wise Histogram,"<p>Given a 2-dimensional tensor <code>t</code>, what's the fastest way to compute a tensor <code>h</code> where</p>

<pre><code>h[i, :] = tf.histogram_fixed_width(t[i, :], vals, nbins)
</code></pre>

<p>I.e. where <code>tf.histogram_fixed_width</code> is called per row of the input tensor <code>t</code>?</p>

<p>It seems that <code>tf.histogram_fixed_width</code> is missing an <code>axis</code> parameter that works like, e.g., <code>tf.reduce_sum</code>'s <code>axis</code> parameter.</p>
",2017-01-20 12:53:32,"<p>Inspired by keveman's answer and because the number of rows of <code>t</code> is fixed and rather small, I chose to use a combination of <code>tf.gather</code> to split rows and <code>tf.pack</code> to join rows. It looks simple and works, will see if it is efficient...</p>

<pre><code>t_histo_rows = [
        tf.histogram_fixed_width(
            tf.gather(t, [row]),
            vals, nbins)
        for row in range(t_num_rows)]

t_histo = tf.pack(t_histo_rows, axis=0)
</code></pre>
",482601,8388,https://stackoverflow.com/questions/41764199,Documentation Replication on Other Examples
41780344,Gradient of tf.floor is None,"<p><code>tf.floor</code> return None gradient<br>
it means the weights before <code>floor</code> operation won't be update, right?<br>
but I still need the gradient to update weights<br>
Isn't it weird? sometimes we use <code>floor</code>, <code>ceil</code>... but they can't deliver gradient or this is right action<br>
as well as <code>tf.cast</code>, the gradient return none </p>

<p>Here is an issue <a href=""https://github.com/tensorflow/tensorflow/issues/897"" rel=""nofollow noreferrer"">#897</a> to discuss the gradient, but I don't understand why return None is good</p>

<pre><code>numa = tf.constant(5.3)  
res1 = numa * 2   
res2 = tf.floor(numa *2)   
print tf.gradients(res1, numa)
# &lt;type 'list'&gt;: [&lt;tf.Tensor 'gradients/mul_grad/Reshape:0' shape=() dtype=float32&gt;]

print tf.gradients(res2, numa)
# None
</code></pre>

<p>Can I modify <code>math_grad.py</code> directly? need other action?<br>
something like delete <code>math_grad.pyc</code>?</p>

<pre><code>@ops.RegisterGradient(""Floor"")
def _FloorGrad(_, unused_grad):
  # return [None]  # before
  return [unused_grad] # I want to modify
</code></pre>
",2017-01-21 14:11:59,"<p>TensorFlow uses <code>None</code> to represent <code>0</code> for implementation reasons. I don't think it would help you if the returned gradient was true gradient <code>0</code>, since you want to train. You could use <code>gradient_override_map</code> to substitute gradient of <code>Identity</code> op for <code>Floor</code> as follows</p>

<pre><code>tf.reset_default_graph()
x = tf.Variable(10.)
with tf.get_default_graph().gradient_override_map({""Floor"": ""Identity""}):
    x2 = tf.floor(x)
loss = tf.square(x2)
opt = tf.train.GradientDescentOptimizer(0.1)
train_op = opt.minimize(loss)
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for i in range(10):
    print(sess.run([loss, train_op]))
</code></pre>
",6306884,1430,https://stackoverflow.com/questions/41780344,Documentation Replication on Other Examples
42095625,What does the function control_dependencies do?,"<p>I would like to have an example illustrating the use of the function <a href=""https://www.tensorflow.org/api_docs/python/tf/control_dependencies"" rel=""noreferrer""><code>tf.control_dependencies</code></a>. For example, I want to create two tensors <code>X</code> and <code>Y</code> and if they are equal do or print something. </p>

<pre><code>import tensorflow as tf

session = tf.Session()

X = tf.constant(5)
Y = tf.constant(50)

with tf.control_dependencies([tf.assert_equal(X, Y)]):
    print('X and Y are equal!')
</code></pre>

<p>In the code above, <code>X</code> is clearly not equal to <code>Y</code>. What is <code>tf.control_dependencies</code> doing in this case?</p>
",2017-02-07 16:56:35,"<p><code>control_dependencies</code> is not a conditional. It is a mechanism to add dependencies to whatever ops you create in the <code>with</code> block. More specifically, what you specify in the argument to <code>control_dependencies</code> is ensured to be evaluated before anything you define in the <code>with</code> block. </p>

<p>In your example, you don't add any (TensorFlow) operations in the <code>with</code> block, so the block does nothing.</p>

<p><a href=""https://stackoverflow.com/a/33950177/524436"">This answer</a> has an example of how to use <code>control_dependencies</code>, where it is used to make sure the assignments happen before the batchnorm operations are evaluated.</p>
",5540159,922,https://stackoverflow.com/questions/42095625,Documentation Replication on Other Examples
41602374,tf.zeros doesn't return a 1D tensor?,"<p>I'm trying to duplicate a tensor across a new axis, like this:</p>

<pre><code>original_tensor = tf.constant([1,2,3,4,5])
made_copies_tensor = tf.tile(original_tensor, 5)
final_result = tf.reshape([5,5])
</code></pre>

<p>However I'm getting this error:</p>

<pre><code>File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 650, in with_rank
raise ValueError(""Shape %s must have rank %d"" % (self, rank))
ValueError: Shape () must have rank 1
</code></pre>

<p>In the documentation it says the way I wrote tf.constant is supposed to have it return a 1D tensor but when I checked its shape with get_shape(), it has (5,) as its shape. I tried reshaping it but nothing changed.</p>

<p>Why am I getting this error? Thanks.</p>
",2017-01-11 23:05:35,"<p>Your ""5"" parameter has shape <code>()</code> but needs to have rank 1</p>

<pre><code>original_tensor = tf.constant([1,2,3,4,5])
made_copies_tensor = tf.tile(original_tensor, [5])
sess.run(made_copies_tensor)
</code></pre>
",3873000,41,https://stackoverflow.com/questions/41602374,Documentation Replicability
41673889,TensorFlow: does tf.train.batch automatically load the next batch when the batch has finished training?,"<p>For instance, after I have created my operations, fed the batch data through the operation and run the operation, does tf.train.batch automatically feed in another batch of data to the session?</p>

<p>I ask this because tf.train.batch has an attribute of <code>allow_smaller_final_batch</code> which makes it possible for the final batch to be loaded as a size lesser than the indicated batch size. Does this mean even without a loop, the next batch could be automatically fed? From the tutorial codes I am rather confused. When I load a single batch, I get literally a single batch size of shape [batch_size, height, width, num_channels], but the <a href=""https://www.tensorflow.org/api_docs/python/io_ops/input_pipeline#batch"" rel=""noreferrer"">documentation</a> says it <code>Creates batches of tensors in tensors.</code> Also, when I read the tutorial code in the <a href=""https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb"" rel=""noreferrer"">tf-slim walkthrough tutorial</a>, where there is a function called load_batch, there are only 3 tensors returned: <code>images, images_raw, labels</code>. Where are 'batches' of data as explained in the documentation?</p>

<p>Thank you for your help.</p>
",2017-01-16 10:09:04,"<blockquote>
  <p>... does tf.train.batch automatically feeds in another batch of data to the session?</p>
</blockquote>

<p>No. Nothing happens automatically. You must call <code>sess.run(...)</code> again to load a new batch.</p>

<blockquote>
  <p>Does this mean even without a loop, the next batch could be automatically fed?</p>
</blockquote>

<p>No. <code>tf.train.batch(..)</code> will always load <code>batch_size</code> tensors. If you have for example 100 images and a <code>batch_size=30</code> then you will have 3*30 batches as in you can call <code>sess.run(batch)</code> three times before the input queue will start from the beginning (or stop if <code>epoch=1</code>). This means that you miss out <code>100-3*30=10</code> samples from training. In case you do not want to miss them you can do <code>tf.train.batch(..., allow_smaller_final_batch=True)</code> so now you will have 3x 30-sample-batches and 1x 10-sample-batch before the input queue will restart.</p>

<p>Let me also elaborate with a code sample:</p>

<pre class=""lang-py prettyprint-override""><code>queue = tf.train.string_input_producer(filenames,
        num_epochs=1) # only iterate through all samples in dataset once

reader = tf.TFRecordReader() # or any reader you need
_, example = reader.read(queue)

image, label = your_conversion_fn(example)

# batch will now load up to 100 image-label-pairs on sess.run(...)
# most tf ops are tuned to work on batches
# this is faster and also gives better result on e.g. gradient calculation
batch = tf.train.batch([image, label], batch_size=100)

with tf.Session() as sess:
    # ""boilerplate"" code
    sess.run([
        tf.local_variables_initializer(),
        tf.global_variables_initializer(),
    ])
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)

    try:
        # in most cases coord.should_stop() will return True
        # when there are no more samples to read
        # if num_epochs=0 then it will run for ever
        while not coord.should_stop():
            # will start reading, working data from input queue
            # and ""fetch"" the results of the computation graph
            # into raw_images and raw_labels
            raw_images, raw_labels = sess.run([images, labels])
    finally:
        coord.request_stop()
        coord.join(threads)
</code></pre>
",5107084,2893,https://stackoverflow.com/questions/41673889,Lack of Alternative Solutions/Documentation
41789133,What are c_state and m_state in Tensorflow LSTM?,"<p>Tensorflow r0.12's documentation for tf.nn.rnn_cell.LSTMCell describes this as the init:</p>

<pre><code>tf.nn.rnn_cell.LSTMCell.__call__(inputs, state, scope=None)
</code></pre>

<p>where <code>state</code> is as follows:</p>

<blockquote>
  <p>state: if state_is_tuple is False, this must be a state Tensor, 2-D, batch x state_size. If state_is_tuple is True, this must be a tuple of state Tensors, both 2-D, with column sizes c_state and m_state.</p>
</blockquote>

<p>What aare <code>c_state</code> and <code>m_state</code> and how do they fit into LSTMs? I cannot find reference to them anywhere in the documentation.</p>

<p><a href=""https://web.archive.org/web/20170223030652/https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods"" rel=""noreferrer"">Here is a link to that page in the documentation.</a></p>
",2017-01-22 09:04:46,"<p>I've stumbled upon same question, here's how I understand it! Minimalistic LSTM example:</p>

<pre><code>import tensorflow as tf

sample_input = tf.constant([[1,2,3]],dtype=tf.float32)

LSTM_CELL_SIZE = 2

lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(LSTM_CELL_SIZE, state_is_tuple=True)
state = (tf.zeros([1,LSTM_CELL_SIZE]),)*2

output, state_new = lstm_cell(sample_input, state)

init_op = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init_op)
print sess.run(output)
</code></pre>

<p>Notice that <code>state_is_tuple=True</code> so when passing <code>state</code> to this <code>cell</code>, it needs to be in the <code>tuple</code> form. <code>c_state</code> and <code>m_state</code> are probably ""Memory State"" and ""Cell State"", though I honestly am NOT sure, as these terms are only mentioned in the docs. In the code and papers about <code>LSTM</code> - letters <code>h</code> and <code>c</code> are commonly used to denote ""output value"" and ""cell state"".
<a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""noreferrer"">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>
Those tensors represent combined internal state of the cell, and should be passed together. Old way to do it was to simply concatenate them, and new way is to use tuples.</p>

<p>OLD WAY:</p>

<pre><code>lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(LSTM_CELL_SIZE, state_is_tuple=False)
state = tf.zeros([1,LSTM_CELL_SIZE*2])

output, state_new = lstm_cell(sample_input, state)
</code></pre>

<p>NEW WAY:</p>

<pre><code>lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(LSTM_CELL_SIZE, state_is_tuple=True)
state = (tf.zeros([1,LSTM_CELL_SIZE]),)*2

output, state_new = lstm_cell(sample_input, state)
</code></pre>

<p>So, basically all we did, is changed <code>state</code> from being 1 tensor of length <code>4</code> into two tensors of length <code>2</code>. The content remained the same. <code>[0,0,0,0]</code> becomes <code>([0,0],[0,0])</code>. (This is supposed to make it faster)</p>
",5299052,315,https://stackoverflow.com/questions/41789133,Lack of Alternative Solutions/Documentation
41857602,sorting of 2d array min to max in tensorflow,"<p>I have an array </p>

<pre><code>x1 = tf.Variable([[0.51, 0.52, 0.53, 0.94, 0.35],
             [0.32, 0.72, 0.83, 0.74, 0.55],
             [0.23, 0.72, 0.63, 0.64, 0.35],
             [0.11, 0.02, 0.03, 0.14, 0.15],
             [0.01, 0.72, 0.73, 0.04, 0.75]],tf.float32)
</code></pre>

<p>I want to sort the elements in each row from min to max. Is there any function for doing such ?</p>

<p>In the example here they are using <code>tf.nn.top_k</code><a href=""https://stackoverflow.com/questions/40784941/sorting-an-array-in-tensorflow"">2d array</a>,using this I can loop to create the max to min. </p>

<pre><code>def sort(instance):
   sorted = []
   rows = tf.shape(instance)[0]
   col = tf.shape(instance)[1]
   for i in range(rows.eval()):
       matrix.append([tf.gather(instance[i], tf.nn.top_k(instance[i], k=col.eval()).indices)])
   return matrix
</code></pre>

<p>Is there any thing similar for finding the min to max or how to reverse the array in each row ?</p>
",2017-01-25 17:13:22,"<p>As suggested by @Yaroslav you can just use the <code>top_k</code> values.</p>

<pre><code>a = tf.Variable([[0.51, 0.52, 0.53, 0.94, 0.35],
             [0.32, 0.72, 0.83, 0.74, 0.55],
             [0.23, 0.72, 0.63, 0.64, 0.35],
             [0.11, 0.02, 0.03, 0.14, 0.15],
             [0.01, 0.72, 0.73, 0.04, 0.75]],tf.float32)

row_size = a.get_shape().as_list()[-1]
top_k = tf.nn.top_k(-a, k=row_size)
sess.run(-top_k.values)
</code></pre>

<p>this prints for me</p>

<pre><code>array([[ 0.34999999,  0.50999999,  0.51999998,  0.52999997,  0.94      ],
       [ 0.31999999,  0.55000001,  0.72000003,  0.74000001,  0.82999998],
       [ 0.23      ,  0.34999999,  0.63      ,  0.63999999,  0.72000003],
       [ 0.02      ,  0.03      ,  0.11      ,  0.14      ,  0.15000001],
       [ 0.01      ,  0.04      ,  0.72000003,  0.73000002,  0.75      ]], dtype=float32)
</code></pre>
",7457101,1706,https://stackoverflow.com/questions/41857602,Inadequate Examples
41900775,"TensorFlow: Value Error Shape and Rank Do Not Match: ValueError: Shape (?, 128, 128, 2) must have rank 2","<p>I'm getting this exception when running a prediction using a deconv neural network. The rank and shape seem to be the same so I'm not sure what the issue is.</p>

<pre><code>File ""/home/Workspace/image-recognition/app/model/per_pixel_deconv.py"", line 141, in Model
    softmax = tf.nn.softmax(output, name=None)

File ""/home/anaconda2/envs/image-recognition/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1396, in softmax
    result = _op_def_lib.apply_op(""Softmax"", logits=logits, name=name)

File ""/home/anaconda2/envs/image-recognition/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
    op_def=op_def)

File ""/home/anaconda2/envs/image-recognition/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2319, in create_op
    set_shapes_for_outputs(ret)

File ""/home/anaconda2/envs/image-recognition/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1711, in set_shapes_for_outputs
    shapes = shape_func(op)

File ""/home/anaconda2/envs/image-recognition/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py"", line 45, in _ShapeFunction
    return [op.inputs[0].get_shape().with_rank(rank)]

File ""/home/anaconda2/envs/image-recognition/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py"", line 641, in with_rank
    raise ValueError(""Shape %s must have rank %d"" % (self, rank))
ValueError: Shape (?, 128, 128, 2) must have rank 2
</code></pre>
",2017-01-27 18:35:05,"<p>Shape <code>(?, 128, 128, 2)</code> is a rank 4 tensor, but the softmax operator apparently expected a rank 2 tensor.</p>

<p>It looks to me like you are using an old version of Tensorflow. In version 0.10, <code>tf.nn.softmax</code> required that its input be rank 2:
<a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/classification#softmax"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/classification#softmax</a></p>

<p>In current versions of Tensorflow, <code>tf.nn.softmax</code> accepts inputs of any rank:
<a href=""https://www.tensorflow.org/api_docs/python/nn/classification#softmax"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/nn/classification#softmax</a></p>

<p>Try using a newer version of Tensorflow.</p>

<p>Hope that helps!</p>
",1628635,668,https://stackoverflow.com/questions/41900775,Documentation Completeness
42787903,Feeding inputs to tf.contrib.learn estimators directly,"<p>I am having trouble using the DNNRegressor estimator from TensorFlow's tf.contrib.learn. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNRegressor"" rel=""nofollow noreferrer"" title=""DNNRegressor"">documentation page</a> of the estimator two methods for providing inputs are presented.</p>

<p>The first method uses the <em>input_fn</em> function, which, as described, should be used for pre-processing and feeding the input to the estimators, and the second method feeds the input directly. Examples:</p>

<pre><code>def input_function:
    ...
    return feature_cols, label

estimator.fit(input_fn=input_function, steps=...)
</code></pre>

<p>In this case <em>feature_cols</em> is a <em>dict</em> with:</p>

<ul>
<li>Key: <em>string</em> specifying the column name,</li>
<li>Value: <em>tf.constant</em> specifying the column values,</li>
</ul>

<p><em>label</em> is a single <em>tf.constant</em> column containing the labels.</p>

<p>This worked.</p>

<pre><code>X_train = ...
y_train = ...
estimator.fit(x=X_train, y=y_train, steps=...)
</code></pre>

<p>In this case I don't know what to feed in as <em>X</em> and <em>y</em>. I have tried the following:</p>

<ul>
<li>Ordinary numpy arrays. This was a long shot and did not work with. The error message is: <code>KeyError: 'my_column0'</code></li>
<li>Pandas DataFrame with columns corresponding to the defined column names (defined on initialization of the estimator). I would again get the same KeyError, even though the key should be there now.</li>
<li>Pass X=<em>feature_cols</em> and y=<em>label</em> defined in the same way as in the case of the input_fn above. This yields: <code>ValueError: Inputs cannot be tensors. Please provide input_fn.</code></li>
</ul>

<p>I also tried other combinations with <em>dict</em> and numpy arrays, but nothing worked. I would like to be able to make this work using the second method, since this is also useful for passing the objects to the <em>evaluate</em> and <em>predict</em>. Does anyone know the correct format for this?
Also, is there a way to simply pass numpy arrays?</p>

<p>Thank you! </p>

<p>tl;dr
What should the inputs to tf.contrib.learn estimator be in order to feed them directly using <code>estimator.fit(x=X_train, y=y_train, steps=...)</code>?</p>
",2017-03-14 13:53:51,"<p>I guess I missed the following warning:
SOME ARGUMENTS ARE DEPRECATED. They will be removed after 2016-12-01. Instructions for updating: Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn. Example conversion: est = Estimator(...) -> est = SKCompat(Estimator(...))</p>

<p>It is strange that the arguments are still there after 2016-12-01 though.</p>
",4330168,380,https://stackoverflow.com/questions/42787903,Documentation Replication on Other Examples
42127505,Tensorflow dense_to_sparse,"<p>I am trying to convert a uncompressed sparse array into a format accepted by tf.SparseTensor. There is an inbuilt function tf.sparse_to_dense that does exactly the opposite I am trying to do. So my question is there any inbuilt function in Tensorflow or Python to do this conversion?</p>
",2017-02-09 03:02:13,"<p><a href=""https://stackoverflow.com/questions/39838234/sparse-matrix-from-a-dense-one-tensorflow"">according to this question:</a> </p>

<p>you can do it with this:</p>

<p>You can use tf.where and tf.gather_nd to do that:</p>

<pre><code>a = np.reshape(np.arange(24), (3, 4, 2))
with tf.Session() as sess:
    a_t = tf.constant(a)
    idx = tf.where(tf.not_equal(a_t, 0))
    # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic
    sparse = tf.SparseTensor(idx, tf.gather_nd(a_t, idx), a_t.get_shape())
    dense = tf.sparse_tensor_to_dense(sparse)
    b = sess.run(dense)
np.all(a == b)
&gt;&gt;&gt; True
</code></pre>
",7537870,495,https://stackoverflow.com/questions/42127505,Documentation Replicability
42404564,What does tf.train.get_global_step() do in TensorFlow?,"<p>What is the use of the function <code>tf.train.get_global_step()</code> in TensorFlow? 
In machine learning concepts what is it equivalent to?</p>
",2017-02-22 23:56:00,"<p>You could use it to restart training exactly where you left off when the training procedure has been stopped for some reason. Of course you can always restart training without knowing the <code>global_step</code> (if you save checkpoints regularly in your code, that is), but unless you somehow keep track of how many iterations you already performed, you will not know how many iterations are left after the restart. Sometimes you really want your model to be trained exactly <code>n</code> iterations and not <code>n</code> plus <code>unknown amount before crash</code>. So in my opinion, this is more of a practicality than a theoretical machine learning concept.</p>
",775755,1799,https://stackoverflow.com/questions/42404564,Lack of Alternative Solutions/Documentation
42897816,"Why does ""tf.Variable([.3], tf.float32)"" work in tensorflow?","<p>The standard usage should be </p>

<p><code>tf.Variable([.3], dtype=tf.float32)</code>, isn't it?</p>

<p>I saw the <code>tf.Variable([.3], tf.float32)</code> in official documentation. The constructor function prototype of  <code>tf.Variable</code> is </p>

<p><code>__init__(self, initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None)</code>. </p>

<p>If we pass the parameter <code>tf.float32</code> instead of <code>dtype=tf.float32</code> (key parameter), how does it know the <code>tf.float32</code> is employed for <code>dtype</code>. Does python interpreter check the parameter type?</p>
",2017-03-20 07:17:54,"<p>From the documentation of <code>tf.Variable</code>:</p>

<blockquote>
  <p>dtype: If set, initial_value will be converted to the given type.
          If <code>None</code>, either the datatype will be kept (if <code>initial_value</code> is
          a Tensor), or <code>convert_to_tensor</code> will decide.</p>
</blockquote>

<p>And the documentation from <code>convert_to_tensor(value, dtype=None, ...)</code>:</p>

<blockquote>
  <p>dtype: Optional element type for the returned tensor. If missing, the
        type is inferred from the type of <code>value</code>.</p>
</blockquote>

<p>Also, there is an example given in the documentation to <code>convert_to_tensor</code>:</p>

<pre><code>import numpy as np

def my_func(arg):
    arg = tf.convert_to_tensor(arg, dtype=tf.float32)
    return tf.matmul(arg, arg) + arg

# The following calls are equivalent.
value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))
value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])
value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))
</code></pre>

<p>So, coming back to your question - Tensorflow does not know that you intended to use <code>tf.float32</code>, it just happened to be the datatype that was chosen by default by the <code>convert_to_tensor</code> function. Hence, it was mere coincidence that the returned Tensor had the datatype that you expected. If e.g. you call <code>tf.Variable([.3], tf.float64)</code> the resulting Tensor has the same <code>dtype</code> as when calling <code>tf.Variable([.3], tf.float32)</code>.</p>

<p>In fact, I believe that both calls <code>tf.Variable([.3], tf.float32)</code> and <code>tf.Variable([.3], tf.float64)</code> are equivalent, as the second argument to <code>tf.Variable</code> is a boolean and thus <code>tf.floatX</code> is being converted to a boolean which always returns <code>True</code>. </p>
",4810225,301,https://stackoverflow.com/questions/42897816,Documentation Replicability
41941940,TensorFlow: Understanding the `collections` argument in tf.summary.scalar,"<p>I am working with TensorBoard, specifically <code>tf.summary.scalar</code>. In the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/summary.md#tfsummaryscalarname-tensor-collectionsnone-scalar"" rel=""nofollow noreferrer"">documentation</a> it has an arugment <code>collections=None</code>, which is described as:</p>

<blockquote>
  <p><code>collections</code>: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to <code>[GraphKeys.SUMMARIES]</code>.</p>
</blockquote>

<p>I don't understand this description, and what <code>collections</code> is used for. Can someone please explain this to me, and perhaps point me towards a good example use-case?</p>
",2017-01-30 17:32:28,"<p>It's a hidden gem! You can provide it a list of strings of your choice that label the summary node, e.g.</p>

<pre><code>tf.summary.scalar('learning_rate', p_lr, collections=['train'])
tf.summary.scalar('loss', t_loss, collections=['train', 'test'])
</code></pre>

<p>and then fetch the summaries by their label, e.g. like so:</p>

<pre><code>s_training = tf.summary.merge_all('train')
s_test = tf.summary.merge_all('test')
</code></pre>

<p>I'm doing it like that because I often want to log extra information during the validation phase; in the above example, I don't have to provide a value for the learning rate placeholder <code>p_lr</code> when evaluating (and writing) the accuracy, for example - or anything really that the inference part of the graph relies on.</p>

<p>Providing (only) custom categories also has the nice side effect of hiding the node from <code>Supervisor</code>, for example. If you really want to have control over when exactly you write a summary (e.g. using <code>sv.summary_computed()</code> in case of <code>Supervisor</code>), that's an easy way to go.</p>
",3747801,5858,https://stackoverflow.com/questions/41941940,Documentation Replicability
42728235,Tensorflow: Why is tf.case giving me the wrong result?,"<p>I'm trying to use <code>tf.case</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/case"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/case</a>) to conditionally update a Tensor. As shown, I'm trying to update <code>learning_rate</code> to <code>0.01</code> when <code>global_step == 2</code>, and to <code>0.001</code> when <code>global_step == 4</code>.</p>

<p>However, when <code>global_step == 2</code>, I already get <code>learning_rate = 0.001</code>. Upon further inspection, it looks like <code>tf.case</code> is giving me the wrong result when <code>global_step == 2</code> (I get <code>0.001</code> instead of <code>0.01</code>). This is happening even though the predicate for <code>0.01</code> is evaluating to True, and the predicate for <code>0.001</code> is evaluating to False.</p>

<p>Am I doing something wrong, or is this a bug?</p>

<p><strong>TF Version: 1.0.0</strong></p>

<p>Code:</p>

<pre><code>import tensorflow as tf

global_step = tf.Variable(0, dtype=tf.int64)
train_op = tf.assign(global_step, global_step + 1)
learning_rate = tf.Variable(0.1, dtype=tf.float32, name='learning_rate')

# Update the learning_rate tensor conditionally
# When global_step == 2, update to 0.01
# When global_step == 4, update to 0.001
cases = []
case_tensors = []
for step, new_rate in [(2, 0.01), (4, 0.001)]:
    pred = tf.equal(global_step, step)
    fn_tensor = tf.constant(new_rate, dtype=tf.float32)
    cases.append((pred, lambda: fn_tensor))
    case_tensors.append((pred, fn_tensor))
update = tf.case(cases, default=lambda: learning_rate)
updated_learning_rate = tf.assign(learning_rate, update)

print tf.__version__
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for _ in xrange(6):
        print sess.run([global_step, case_tensors, update, updated_learning_rate])
        sess.run(train_op)
</code></pre>

<p>Results:</p>

<pre><code>1.0.0
[0, [(False, 0.0099999998), (False, 0.001)], 0.1, 0.1]
[1, [(False, 0.0099999998), (False, 0.001)], 0.1, 0.1]
[2, [(True, 0.0099999998), (False, 0.001)], 0.001, 0.001]
[3, [(False, 0.0099999998), (False, 0.001)], 0.001, 0.001]
[4, [(False, 0.0099999998), (True, 0.001)], 0.001, 0.001]
[5, [(False, 0.0099999998), (False, 0.001)], 0.001, 0.001]
</code></pre>
",2017-03-10 21:45:31,"<p>This was answered in <a href=""https://github.com/tensorflow/tensorflow/issues/8776"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/8776</a></p>

<p>It turns out that <code>tf.case</code> behavior is undefined if, in <code>fn_tensors</code>, the lambdas return a tensor that was created outside of the lambda.  The correct usage is to define the lambdas such that they return a newly-created tensor.</p>

<p>According to the linked Github issue, this usage is required because <code>tf.case</code> must create the tensor itself in order to hook up the tensor's inputs to the correct branch of the predicate.</p>
",1232944,706,https://stackoverflow.com/questions/42728235,Documentation Replicability
42773379,tf.nn.relu vs. tf.contrib.layers.relu?,"<p>I see this ""tf.nn.relu"" documented here: <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/relu"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/relu</a></p>

<p>But then I also see usage of tf.contrib.layers.relu on this page in ""model_fn"":
<a href=""https://www.tensorflow.org/extend/estimators"" rel=""nofollow noreferrer"">https://www.tensorflow.org/extend/estimators</a></p>

<p>It seems like the latter isn't described like the first one in an API-like fashion, but only presented in use.</p>

<p>Why is this? Are the docs out of date? Why have two - is one old and no longer supported/going to be removed?</p>
",2017-03-13 21:03:42,"<p>They are not the same thing.</p>

<p>The latter is not an activation function but a <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected"" rel=""noreferrer""><code>fully_connected</code></a> <strong>layer</strong> that has its activation function preset as <code>nn.relu</code>:</p>

<pre><code>relu = functools.partial(fully_connected, activation_fn=nn.relu)
# ^                                                     |&lt;   &gt;|
# |_ tf.contrib.layers.relu                     tf.nn.relu_|
</code></pre>

<hr>

<p>If you read the docs for <a href=""https://www.tensorflow.org/api_guides/python/contrib.layers"" rel=""noreferrer""><code>contrib.layers</code></a>, you'll find:</p>

<blockquote>
  <p>Aliases for <code>fully_connected</code> which set a default activation function
  are available: <code>relu</code>, <code>relu6</code> and <code>linear</code>.</p>
</blockquote>

<p>Summarily, <a href=""https://github.com/tensorflow/tensorflow/blob/4433079e7f317724eaa92ec120c6b1c3c0c52f2f/tensorflow/contrib/layers/python/layers/layers.py#L2137"" rel=""noreferrer""><code>tf.contrib.layers.relu</code></a> is an alias for a <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected"" rel=""noreferrer""><code>fully_connected</code></a> layer with <em>relu</em> activation while <code>tf.nn.relu</code> is the REctified Linear Unit activation function itself.</p>
",384137,3271,https://stackoverflow.com/questions/42773379,Documentation Ambiguity
42777388,optimizer.applyGradient doesnt work in tensorflow,"<p>I am trying to change gradients in tensorflow and after that trying to update with applyGradient() function. This is my code and it doesnt work </p>

<pre><code>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))
optimizer = tf.train.AdamOptimizer(LEARNING_RATE)
        for j in range(n_rounds):
            sample = np.random.randint(row, size=int(batch_size))
            batch_xs = temp[sample][:]
            batch_ys = output[sample][:]
            vars_with_grads = sess.run(optimizer.compute_gradients(cross_entropy), feed_dict={x: batch_xs, y_: batch_ys})
            noiseAddedGradient = []
            print(vars_with_grads)
            for var in vars_with_grads:
                gaussianNoise = [np.random.normal(MEAN_FOR_AUTOENCODER, SCALE_FOR_AUTOENCODER, var[0].shape) for i in range(int(batch_size))]
                totalGaussianNoise = [sum(x) for x in zip(*gaussianNoise)]
                averageGaussianNoise = [x1 / float(batch_size) for x1 in totalGaussianNoise]
                averageGaussianNoiseList = np.array(averageGaussianNoise).flatten().reshape(var[0].shape)
                noiseAddedGradient.append((tf.Variable(np.add(var[0], averageGaussianNoiseList), dtype=np.float32), var[1]))
            appliedGradient = sess.run(optimizer.apply_gradients(noiseAddedGradient))
</code></pre>

<p>it returns error function:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/Downloads/objectPerturbation.py"", line 214, in &lt;module&gt;
    appliedGradient = sess.run(optimizer.apply_gradients(noiseAddedGradient))
  File ""/home/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 384, in apply_gradients
    p = _get_processor(v)
  File ""/home/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 98, in _get_processor
    if v.op.type == ""ReadVariableOp"":
AttributeError: 'numpy.ndarray' object has no attribute 'op'
</code></pre>

<p>Can you please help me?</p>
",2017-03-14 03:42:02,"<p>Try chaging it like this.  The gradients should be computed in the graph.</p>

<pre><code>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))
optimizer = tf.train.AdamOptimizer(LEARNING_RATE)
grads = optimizer.compute_gradients(cross_entropy)
grad_placeholder = [(tf.placeholder(""float"", shape=grad[1].get_shape()), grad[1] for grad in grads]
apply_placeholder_op = opt.apply_gradients(grad_placeholder)

#added in case you don't do this
sess.run(tf.initialize_all_variables())

    for j in range(n_rounds):
        sample = np.random.randint(row, size=int(batch_size))
        batch_xs = temp[sample][:]
        batch_ys = output[sample][:]
        vars_with_grads = sess.run(grads, feed_dict={x: batch_xs, y_: batch_ys})
        #Add gaussian noise to gradients
        feed_dict = {}
        for i in range(len(grad_placeholder)):
            feed_dict[grad_placeholder[i][0]] = add_gaussian_noise_fn(grad_vals[i])
        sess.run(apply_placeholder_op, feed_dict=feed_dict)

#separate function to make it more general to do whatever you want with grads
def add_gaussian_noise_fn(x):
  return x + np.random.normal(size=x.shape)
</code></pre>

<p>Idea is similar to this previous post:
<a href=""https://stackoverflow.com/questions/34687761/efficiently-grab-gradients-from-tensorflow"">Efficiently grab gradients from TensorFlow?</a></p>
",3104352,1120,https://stackoverflow.com/questions/42777388,Inadequate Examples
42785026,tf.nn.conv2d vs tf.layers.conv2d,"<p>Is there any advantage in using <code>tf.nn.*</code> over <code>tf.layers.*</code>?</p>

<p>Most of the examples in the doc use <code>tf.nn.conv2d</code>, for instance, but it is not clear why they do so.</p>
",2017-03-14 11:41:17,"<p>For convolution, they are the same. More precisely, <code>tf.layers.conv2d</code> (actually <code>_Conv</code>) uses <code>tf.nn.convolution</code> as the backend. You can follow the calling chain of: <code>tf.layers.conv2d&gt;Conv2D&gt;Conv2D.apply()&gt;_Conv&gt;_Conv.apply()&gt;_Layer.apply()&gt;_Layer.\__call__()&gt;_Conv.call()&gt;nn.convolution()...</code></p>
",326849,36704,https://stackoverflow.com/questions/42785026,Documentation Replicability
43411738,tf.image.pad_to_bounding_box VS tf.pad and tf.image.crop_to_bounding_box VS tf.slice,"<p>I'd like to understand why does the two functions <code>tf.image.crop_to_bounding_box</code> and <code>tf.image.pad_to_bounding_box</code> exists, since the behaviour of these two functions can be done really simply with respectively <code>tf.slice</code> and <code>tf.pad</code>.</p>

<p>They are not so much easier to understand, and their scope is narrow since they accept only 3D and 4D tensors. Furthermore, they tend to be slower in terms of time of execution. </p>

<p>Is there something I miss here ? </p>
",2017-04-14 12:42:35,"<p>Mostly you use them <code>tf.image.*</code> for easiness of use.</p>

<p>Both <code>crop_to_bounding_box</code> and <code>pad_to_bounding_box</code> use <code>slice</code> and <code>pad</code>underneath, but also add checkings and constraints to make sure you don't spend hours trying to debug your slice/pad indices and offsets.</p>
",7370153,11028,https://stackoverflow.com/questions/43411738,Documentation Replicability
42818819,What is the difference between tensorflow conv2d_transpose and conv2d_backprop_filter?,"<p>Can someone please explain in simple terms and examples on how these work after performing the conv2d forward pass.</p>

<p>Let me add to this question - What is the difference between conv2d_backprop_filter and tf.nn.conv2d_backprop_input?</p>
",2017-03-15 19:18:04,"<p>For an explanation of conv2d_transpose I would look at other stack overflow questions such as this one: <a href=""https://stackoverflow.com/questions/39373230/what-does-tensorflows-conv2d-transpose-operation-do"">conv2d_transpose</a></p>

<p>As for conv2d_backprop_filter:
this is what is computed during backpropagation to be passed to the previous layer.  It has been used for things such as Deep Dream and creation of adversarial examples.</p>
",7716958,25,https://stackoverflow.com/questions/42818819,Documentation Ambiguity
42933599,Slice a tensor in half in tensorflow,"<p>I have a tensor of shape <code>(32, 32, 32, 1)</code> and I want to slice it into two tensors, along the first dimension, containing the first and second halves like so</p>

<pre><code>half1  with shape = (16, 32, 32, 1)
half2  with shape = (16, 32, 32, 1)
</code></pre>

<p>I am trying to use tf.slice but I don't know how to use the begin and end indices, and the documentation is anything but clear. </p>
",2017-03-21 16:57:04,"<p>Here is how you do it:</p>

<pre><code>import tensorflow as tf
import numpy as np
t = t = tf.pack(np.random.randint(1,10,[32,32,32,1]))
half1 = tf.slice(t,[0,0,0,0],[16,32,32,1])
half2 = tf.slice(t,[16,0,0,0],[16,32,32,1])
</code></pre>

<p><code>[0,0,0,0]</code> means start from the very first element in each dimension, [<code>16,32,32,1]</code> means size in first dimension is 16 and for the others 32, 32, 1. It basically means get the first half with regard to first dimension and for all other dimensions get all elements.</p>
",5016028,4293,https://stackoverflow.com/questions/42933599,Documentation Ambiguity
42940451,How can I get the file name of a tf.summary.FileWriter in TensorFlow?,"<p>How can I get the file name of a <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter"" rel=""nofollow noreferrer""><code>tf.summary.FileWriter</code></a> (<a href=""https://web.archive.org/web/20170321224015/https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter"" rel=""nofollow noreferrer"">mirror</a>) in TensorFlow?</p>

<p>I am aware that I can use  <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter#get_logdir"" rel=""nofollow noreferrer""><code>get_logdir()</code></a> but  I don't see any similar method to access the file name.</p>
",2017-03-22 00:05:55,"<p>There is no way to get the filename as it's built on the other side of the C++/python interface.</p>

<p>That said, listing the logdir after writing will let you see what is there.</p>
",395857,79396,https://stackoverflow.com/questions/42940451,Documentation Replicability
42430331,TensorFlow: construct a tensor with recursively defined elements?,"<p>I would like to do something like:</p>

<pre><code>x = tf.Variable(tf.ones([100], dtype=tf.float32))
x0 = tf.Variable(tf.ones([1], dtype=tf.float32))
def f(x):
    return tf.sin(x)
x[0] = x0
for i in range(1,100):
    x[i+1] = f(x[i])
</code></pre>

<p>to construct a tensor 'x'. Is such a construct possible? I looked at 'tf.while_loop' but does not seem to help.</p>
",2017-02-24 03:39:23,"<pre><code>def f(x):
    return tf.sin(x)
x = []
x.append(tf.constant(1.0))
for i in range(1,100):
    x.append(f(i))
</code></pre>

<p>Is this what you want?
This is not recursive by the way. It's iterative.</p>
",5721911,165,https://stackoverflow.com/questions/42430331,Documentation Replication on Other Examples
42560143,Cannot find documentary for many things in tf.nn.seq2seq class in the tensorflow,"<p><strong>tf.nn.seq2seq.embedding_attention_seq2seq</strong></p>

<p>I was searching  the above function in tensorflow api. But I counln't find that by the exact name. But similar function was there in the class <strong>tf.contrib.legacy_seq2seq.embedding_attention_seq2seq</strong>  .
Why they moved few from nn class to contrib class ?</p>
",2017-03-02 15:59:47,"<p>Google <a href=""https://www.tensorflow.org/install/migration"" rel=""nofollow noreferrer"">just released TensorFlow 1.0 last week</a>, where the API has been updated, so things have been moved around. Because of the various contributions from the TF community, the contrib directory is reserved for this code, which has not yet been finalized for acceptance into core TF. </p>

<p>Check out the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/README.md"" rel=""nofollow noreferrer"">README</a> on contrib:</p>

<blockquote>
  <p>The contrib directory contains project directories, each of which has
  designated owners. It is meant to contain features and contributions
  that eventually should get merged into core TensorFlow, but whose
  interfaces may still change, or which require some testing to see
  whether they can find broader acceptance. We are trying to keep
  duplication within contrib to a minimum, so you may be asked to
  refactor code in contrib to use some feature inside core or in another
  project in contrib rather than reimplementing the feature.</p>
</blockquote>
",5915270,4001,https://stackoverflow.com/questions/42560143,Documentation Replicability
42569921,tf.contrib.slim.get_variables_to_restore() does not return value,"<p>Running below code tf.contrib.slim.get_variables_to_restore() return empty value [] for all_vars, and then causing failure when calling tf.train.Saver. Detail error message shows below.</p>

<p>Am I missing anything?</p>

<pre><code>&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; inception_exclude_scopes = ['InceptionV3/AuxLogits', 'InceptionV3/Logits', 'global_step', 'final_ops']
&gt;&gt;&gt; inception_checkpoint_file = '/Users/morgan.du/git/machine-learning/projects/capstone/yelp/model/inception_v3_2016_08_28.ckpt'
&gt;&gt;&gt; with tf.Session(graph=tf.Graph()) as sess:
...     init_op = tf.global_variables_initializer()
...     sess.run(init_op)
...     reader = tf.train.NewCheckpointReader(inception_checkpoint_file)
...     var_to_shape_map = reader.get_variable_to_shape_map()
...     all_vars = tf.contrib.slim.get_variables_to_restore(exclude=inception_exclude_scopes)
...     inception_saver = tf.train.Saver(all_vars)
...     inception_saver.restore(sess, inception_checkpoint_file)
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 7, in &lt;module&gt;
  File ""/Users/morgan.du/miniconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1051, in __init__
    self.build()
  File ""/Users/morgan.du/miniconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1072, in build
    raise ValueError(""No variables to save"")
ValueError: No variables to save
</code></pre>
",2017-03-03 03:07:07,"<p>The problem here seems to be that your graph is emptyi.e. it does not contain any variables. You create a new graph on the line <code>with tf.Session(graph=tf.Graph()):</code>, and none of the following lines creates a <code>tf.Variable</code> object.</p>

<p>To restore a pre-trained TensorFlow model, you need to do one of three things:</p>

<ol>
<li>Rebuild the model graph, by executing the same Python graph building code that was used to train the model in the first place.</li>
<li>Load a ""MetaGraph"" that contains information about how to reconstruct the graph structure and model variables. See <a href=""https://www.tensorflow.org/programmers_guide/meta_graph"" rel=""nofollow noreferrer"">this tutorial</a> for more details on how to create and use a MetaGraph. MetaGraphs are often created alongside checkpoint files, and typically have the extension <code>.meta</code>.</li>
<li>Load a ""SavedModel"", which contains a ""MetaGraph"". See the documentation <a href=""https://github.com/tensorflow/tensorflow/blob/1f09d0c1ee7648288b3454d64e72e8760b9acc9c/tensorflow/python/saved_model/README.md"" rel=""nofollow noreferrer"">here</a> for more details.</li>
</ol>
",7397552,61,https://stackoverflow.com/questions/42569921,Documentation Replicability
42675391,tf.nn.sigmoid_cross_entropy_with_logits companies about arguments from documentation,"<p>So I have the following model that I am wanting to test out an idea with. I am particularly interested in <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits"" rel=""nofollow noreferrer"">tf.nn.sigmoid_cross_entropy_with_logits()</a>  because my labels are not mutually exclusive. </p>

<pre><code>import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

x  = tf.placeholder(tf.float32, shape=[None, 784])
y_ = tf.placeholder(tf.float32, shape=[None, 10])

w1 = tf.get_variable(""w1"", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())
b1 = tf.Variable(tf.zeros([512], dtype=tf.float32))
w2 = tf.Variable(tf.zeros([512, 10], dtype=tf.float32))
b2 = tf.Variable(tf.zeros([10], dtype=tf.float32))

h = tf.nn.relu(tf.matmul(x, w1) + b1)
y = tf.matmul(h, w2) + b2

cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y)
train_step = tf.train.AdamOptimizer().minimize(cross_entropy)

with tf.Session() as sess:

    sess.run(tf.initialize_all_variables())
    start = time.time()

    for i in range(20000):
        batch = mnist.train.next_batch(50)
        train_step.run(feed_dict={x: batch[0], y_: batch[1]})
</code></pre>

<p>However, I am getting the following error repeatedly, which seems to be contradicting the tensor flow documentation. </p>

<pre><code>Traceback (most recent call last):
File ""mnist_test.py"", line 19, in &lt;module&gt;
cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y)
TypeError: sigmoid_cross_entropy_with_logits() got an unexpected keyword argument 'labels'
</code></pre>

<p>Please help!!</p>
",2017-03-08 15:29:37,"<p>The  keyword argument <code>labels</code>  <a href=""https://web.archive.org/web/20170308155130/https://github.com/carpedm20/DCGAN-tensorflow/issues/84"" rel=""nofollow noreferrer"">only exists</a> in TensorFlow 1.0.0 and above. I guess you're using 0.12 or below. Use <code>pip freeze</code> or <code>print('TensorFlow version: {0}'.format(tf.__version__))</code> to check.</p>

<hr>

<p>The documentation for prior versions can be found at <a href=""https://www.tensorflow.org/versions/"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/</a></p>

<p>To search for some information in the documentation of a previous version you can use: <a href=""https://www.google.com/search?q=site:https://www.tensorflow.org/versions/r0.12+sigmoid_cross_entropy_with_logits()"" rel=""nofollow noreferrer"">https://www.google.com/search?q=site:https://www.tensorflow.org/versions/r0.12+sigmoid_cross_entropy_with_logits()</a></p>
",4500078,906,https://stackoverflow.com/questions/42675391,Documentation Replicability
43681154,Shapes in tf.test.compute_gradient_error,"<p>In addition to 2 tensors, tf.test.compute_gradient_error takes two shapes. Why are they required? Why can't we just evaluate the tensors and calculate their shapes?</p>
",2017-04-28 13:15:34,"<p>It was written very early in TensorFlow's development, at a point where shape information was not associated with Tensors. Since it's not useful in model code, updating it for usability was never a priority.</p>
",250560,10270,https://stackoverflow.com/questions/43681154,Documentation Replicability
43422949,CTC Loss InvalidArgumentError: sequence_length(b) <= time,"<p>I am running into this error while trying to use tf.nn.ctc_loss through keras (ctc_batch_cost):</p>

<blockquote>
  <p>InvalidArgumentError (see above for traceback): sequence_length(4) &lt;= 471</p>
</blockquote>

<p>According to the documentation for tf.nn.ctc_loss, Input requirements are:</p>

<blockquote>
  <p>sequence_length(b) &lt;= time for all b</p>
  
  <p>max(labels.indices(labels.indices[:, 1] == b, 2))   &lt;=
  sequence_length(b) for all b.</p>
</blockquote>

<p>I am having a hard time understanding what this means-- what is <code>b</code> and what is <code>sequence_length(b)</code>? </p>
",2017-04-15 06:24:56,"<p>In this case <code>b</code> is each example in a minibatch. <code>sequence_length(b)</code> is the number of time stamps you have for that example. This is specified in the <code>sequence_length</code> argument passed to <code>tf.nn.ctc_loss</code> which is a 1-d tensor of sequence lengths.</p>
",740857,475,https://stackoverflow.com/questions/43422949,Documentation Ambiguity
43472077,How to use tf.summary.text?,"<p>TensorFlow 1.1.0rc2 has support for Text in its dashboard but how do I actually log something that will show up there? TensorFlow master branch has a reference to <code>tf.summary.text</code> but nothing called that is available in 1.1.0rc2.</p>
",2017-04-18 12:24:40,"<p><a href=""https://github.com/tensorflow/tensorflow/releases"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/releases</a></p>

<p>Patch notes says that it was only added in v1.2.0</p>

<p>Perhaps the code is there in previous versions, but when it's installed/built, it's not included?</p>
",3504575,535,https://stackoverflow.com/questions/43472077,Lack of Alternative Solutions/Documentation
44162432,Analysis of the output from tf.nn.dynamic_rnn tensorflow function,"<p>I am not able to understand the output from <code>tf.nn.dynamic_rnn</code> tensorflow function. The document just tells about the size of the output, but it doesn't tell what does each row/column means. From the documentation:</p>

<blockquote>
  <p><strong>outputs</strong>: The RNN output <code>Tensor</code>.</p>
  
  <p>If time_major == False (default), this will be a <code>Tensor</code> shaped:
      <code>[batch_size, max_time, cell.output_size]</code>.</p>
  
  <p>If time_major == True, this will be a <code>Tensor</code> shaped:
      <code>[max_time, batch_size, cell.output_size]</code>.</p>
  
  <p>Note, if <code>cell.output_size</code> is a (possibly nested) tuple of integers
  or <code>TensorShape</code> objects, then <code>outputs</code> will be a tuple having the<br>
  same structure as <code>cell.output_size</code>, containing Tensors having shapes
  corresponding to the shape data in <code>cell.output_size</code>.</p>
  
  <p><strong>state</strong>: The final state.  If <code>cell.state_size</code> is an int, this   will
  be shaped <code>[batch_size, cell.state_size]</code>.  If it is a<br>
  <code>TensorShape</code>, this will be shaped <code>[batch_size] + cell.state_size</code>.<br>
  If it is a (possibly nested) tuple of ints or <code>TensorShape</code>, this will
  be a tuple having the corresponding shapes.</p>
</blockquote>

<p>The <code>outputs</code> tensor is a 3-D matrix but what does each row/column represent?</p>
",2017-05-24 15:15:08,"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""noreferrer""><code>tf.dynamic_rnn</code></a> provides two outputs, <code>outputs</code> and <code>state</code>.</p>

<ul>
<li><code>outputs</code> contains the output of the RNN cell at every time instant. Assuming the default <code>time_major == False</code>, let's say you have an input composed of 10 examples with 7 time steps each and a feature vector of size 5 for every time step. Then your input would be 10x7x5 (<code>batch_size</code>x<code>max_time</code>x<code>features</code>). Now you give this as an input to a RNN cell with output size 15. Conceptually, each time step of each example is input to the RNN, and you would get a 15-long vector for each of those. So that is what <code>outputs</code> contains, a tensor in this case of size 10x7x15 (<code>batch_size</code>x<code>max_time</code>x<code>cell.output_size</code>) with the output of the RNN cell at each time step. If you are only interested in the last output of the cell, you can just slice the time dimension to pick just the last element (e.g. <code>outputs[:, -1, :]</code>).</li>
<li><code>state</code> contains the state of the RNN after processing all the inputs. Note that, unlike <code>outputs</code>, this doesn't contain information about every time step, but only about the last one (that is, the state <em>after</em> the last one). Depending on your case, the state may or may not be useful. For example, if you have very long sequences, you may not want/be able to processes them in a single batch, and you may need to split them into several subsequences. If you ignore the <code>state</code>, then whenever you give a new subsequence it will be as if you are beginning a new one; if you remember the state, however (e.g. outputting it or storing it in a variable), you can feed it back later (through the <code>initial_state</code> parameter of <code>tf.nn.dynamic_rnn</code>) in order to correctly keep track of the state of the RNN, and only reset it to the initial state (generally all zeros) after you have completed the whole sequences. The shape of <code>state</code> can vary depending on the RNN cell that you are using, but, in general, you have some state for each of the examples (one or more tensors with size <code>batch_size</code>x<code>state_size</code>, where <code>state_size</code> depends on the cell type and size).</li>
</ul>
",333125,7837,https://stackoverflow.com/questions/44162432,Documentation Completeness
43175272,check if tensorflow placeholder is filled,"<p>Suppose I have two placeholder quantities in tensorflow: placeholder_1 and placeholder_2. Essentially I would like the following computational functionality: ""if placeholder_1 is defined (ie is given a value in the feed_dict of sess.run()), compute X as f(placeholder_1), otherwise, compute X as g(placeholder_2)."" Think of X as being a hidden layer in a neural network that can optionally be computed in these two different ways. Eventually I would use X to produce an output, and I'd like to backpropagate error to the parameters of f or g depending on which placeholder I used. </p>

<p>One could accomplish this using the tf.where(condition, x, y) function if there was a way to make the condition ""placeholder_1 has a value"", but after looking through the tensorflow documentation on booleans and asserts I couldn't find anything that looked applicable.</p>

<p>Any ideas? I have a vague idea of how I could accomplish this basically by copying part of the network, sharing parameters and syncing the networks after updates, but I'm hoping for a cleaner way to do it.</p>
",2017-04-03 01:21:00,"<p>You can create a third placeholder variable of type boolean to select which branch to use and feed that in at run time.</p>

<p>The logic behind it is that since you are feeding in the placholders at runtime anyways you can determine outside of tensorflow which placeholders will be fed.</p>
",4938706,135,https://stackoverflow.com/questions/43175272,Lack of Alternative Solutions/Documentation
43396525,How to display the code of an image after tf.decode_image,"<p>I'm trying to see how the tf decode images so I try</p>

<pre><code>import tensorflow as tf
image1 = tf.image.decode_png('/usr/src/pycharm-2017.1/bin/pycharm.png')
print(image1.shape)
with tf.Session() as sess:
    img = sess.run(image1)
    print(img.shape, img)
</code></pre>

<p>but it raise the error</p>

<pre><code>InvalidArgumentError (see above for traceback): Invalid PNG header, data size 39
 [[Node: DecodePng = DecodePng[channels=0, dtype=DT_UINT8, _device=""/job:localhost/replica:0/task:0/cpu:0""](DecodePng/contents)]]
</code></pre>

<p>Also I've tried  tf.image.decode_image, but it didn't work either.
What's wrong ? How can I fix it?
Thank u</p>
",2017-04-13 15:41:15,"<p><code>tf.image.decode_png</code> accepts a Tensor of type string, so you need to read the png with Tensorflow before passing it to the function:</p>

<pre><code>import tensorflow as tf
image1 = tf.image.decode_png(tf.read_file('/usr/src/pycharm-2017.1/bin/pycharm.png'))
print(image1.shape)
with tf.Session() as sess:
    img = sess.run(image1)
    print(img.shape, img)
</code></pre>
",7765466,27,https://stackoverflow.com/questions/43396525,Documentation Replication on Other Examples
43623121,Shape assertions and declarations in tensroflow,"<p>I use tf.strided_slice to get one value out of the 1d tensor. Unfortunately, inferred shape is ?. How can I assert/declare that it has shape [1]?</p>

<p>P.S. I used reshape, but it might have performance implications in some cases</p>
",2017-04-26 00:39:52,"<p>Use <a href=""https://www.tensorflow.org/api_docs/python/tf/Tensor#set_shape"" rel=""nofollow noreferrer""><code>x.set_shape()</code></a> to provide additional information about the shape of this tensor that cannot be inferred from the graph alone.</p>

<p>You can get more information from the <a href=""https://www.tensorflow.org/programmers_guide/faq#tensor_shapes"" rel=""nofollow noreferrer"">FAQ</a>:</p>

<blockquote>
  <p>The tf.Tensor.set_shape method updates the static shape of a Tensor
  object, and it is typically used to provide additional shape
  information when this cannot be inferred directly. It does not change
  the dynamic shape of the tensor.</p>
</blockquote>
",250560,10270,https://stackoverflow.com/questions/43623121,Documentation Replicability
43755609,TensorFlow Estimator : model_fn has following not expected args: ['self'],"<p>I'm using TensorFlow (1.1) high-level API <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer"">Estimators</a> to create my neural net. But I'm using it into a class and I have to call an instance of my class to generate the model of the neural network. (Here <code>self.a</code>)</p>

<pre><code>class NeuralNetwork(object):
  def __init__(self):
    """""" Create neural net """"""
    regressor = tf.estimator.Estimator(model_fn=self.my_model_fn,
                                       model_dir=""/tmp/data"")
    // ...

  def my_model_fn(self, features, labels, mode):
  """""" Generate neural net model """"""
    self.a = a
    predictions = ...
    loss = ...
    train_op = ...
    return tf.estimator.EstimatorSpec(
      mode=mode,
      predictions=predictions,
      loss=loss,
      train_op=train_op)
</code></pre>

<p>But I get the error :
<code>ValueError: model_fn [...] has following not expected args: ['self']</code>. 
I tried to remove the <code>self</code> for the args of my model but got another error <a href=""https://stackoverflow.com/questions/18950054/class-method-generates-typeerror-got-multiple-values-for-keyword-argument"">TypeError:  got multiple values for keyword argument</a>.
Is there any way to use these EstimatorSpec into a class ?</p>
",2017-05-03 09:02:38,"<p>It looks like the <code>Estimator</code>'s argument checking is a bit overzealous. As a workaround, you can wrap the member-function <code>model_fn</code> in a <code>lambda</code> like so:</p>

<pre><code>import tensorflow as tf

class ModelClass(object):

  def __init__(self):
    self._constant = 2.
    self.regressor = tf.estimator.Estimator(
        model_fn=lambda features, labels, mode: self._model_fn(
            features, labels, mode))

  def _model_fn(self, features, labels, mode):
    loss = tf.constant(self._constant)
    train_op = tf.no_op()
    return tf.estimator.EstimatorSpec(
      mode=mode,
      loss=loss,
      train_op=train_op)

ModelClass()
</code></pre>

<p>However, this is rather annoying. Would you mind <a href=""https://github.com/tensorflow/tensorflow/issues/new"" rel=""nofollow noreferrer"">filing a feature request on Github</a> to relax this argument checking for member functions?</p>

<p><em>Update</em>: Should be fixed in TensorFlow 1.3+. Thanks, Yuan!</p>
",7900457,90,https://stackoverflow.com/questions/43755609,Documentation Replicability
43792961,Understanding the while loop in Tensorflow,"<p>I am using the <a href=""https://www.tensorflow.org/api_docs/python/"" rel=""noreferrer"">Python API for Tensorflow</a>. I am trying to implement the <a href=""https://www.sfu.ca/~ssurjano/rosen.html"" rel=""noreferrer"">Rosenbrock function</a> given below without the use of a Python loop:</p>

<p><a href=""https://i.stack.imgur.com/9AdOH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/9AdOH.png"" alt=""Rosenbrock function""></a></p>

<p>My current implementation is as follows:</p>

<pre><code>def rosenbrock(data_tensor):
    columns = tf.unstack(data_tensor)

    summation = 0
    for i in range(1, len(columns) - 1):
        first_term = tf.square(tf.subtract(columns[i + 1], tf.square(columns[i])))
        second_term = tf.square(tf.subtract(columns[i], 1.0))
        summation += tf.add(tf.multiply(100.0, first_term), second_term)

    return summation
</code></pre>

<p>I have tried implementing the summation in a <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""noreferrer""><code>tf.while_loop()</code></a>; however, I found the API somewhat unintuitive when it comes to using an index integer that is meant to remain separate from the data. The example given in the <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""noreferrer"">documentation</a> uses the data as the index (or vice-versa):</p>

<pre><code>i = tf.constant(0)
c = lambda i: tf.less(i, 10)
b = lambda i: tf.add(i, 1)
r = tf.while_loop(c, b, [i])
</code></pre>
",2017-05-04 21:21:01,"<p>This can be achieved using the <code>tf.while_loop()</code> and standard <a href=""https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences"" rel=""noreferrer"">tuples</a> as per the second example in the <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""noreferrer"">documentation</a>. </p>

<pre><code>def rosenbrock(data_tensor):
    columns = tf.unstack(data_tensor)

    # Track both the loop index and summation in a tuple in the form (index, summation)
    index_summation = (tf.constant(1), tf.constant(0.0))

    # The loop condition, note the loop condition is 'i &lt; n-1'
    def condition(index, summation):
        return tf.less(index, tf.subtract(tf.shape(columns)[0], 1))

    # The loop body, this will return a result tuple in the same form (index, summation)
    def body(index, summation):
        x_i = tf.gather(columns, index)
        x_ip1 = tf.gather(columns, tf.add(index, 1))

        first_term = tf.square(tf.subtract(x_ip1, tf.square(x_i)))
        second_term = tf.square(tf.subtract(x_i, 1.0))
        summand = tf.add(tf.multiply(100.0, first_term), second_term)

        return tf.add(index, 1), tf.add(summation, summand)

    # We do not care about the index value here, return only the summation
    return tf.while_loop(condition, body, index_summation)[1]
</code></pre>

<p>It is important to note that the index increment should occur in the body of the loop similar to a standard while loop. In the solution given, it is the first item in the tuple returned by the <code>body()</code> function. </p>

<p>Additionally, the loop condition function must allot a parameter for the summation although it is not used in this particular example.</p>
",1309401,4368,https://stackoverflow.com/questions/43792961,Documentation Replication on Other Examples
43827792,How do I use strided_slice to select all the element in tensorflow?,"<p>I read the examples in document:</p>

<pre><code># 'input' is [[[1, 1, 1], [2, 2, 2]],
#             [[3, 3, 3], [4, 4, 4]],
#             [[5, 5, 5], [6, 6, 6]]]
tf.strided_slice(input, [1, 0, 0], [2, 1, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3]]]
tf.strided_slice(input, [1, 0, 0], [2, 2, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3],
                                                               [4, 4, 4]]]
tf.strided_slice(input, [1, -1, 0], [2, -3, 3], [1, -1, 1]) ==&gt;[[[4, 4, 4],
                                                                 [3, 3, 3]]] 
</code></pre>

<p>It seems like that I can not simply use <code>input[:,:]</code> to select all the element, instead I have to use the syntax like <code>input[:-1, :-1]</code>. However in this way <code>input[:-1, :-1]</code> , I will miss the last row or last column. What should I do?</p>

<p>I take an example:</p>

<pre><code>ph = tf.placeholder(shape=[None, 3], dtype=tf.int32)
x = tf.strided_slice(ph, [0,0],[-1,-1],[1,1])
input_ = np.array([[1,2,3],
                  [3,4,5],
                  [7,8,9]])
sess = tf.InteractiveSession()
sess.run(x,feed_dict={ph:input_})
</code></pre>

<p>output:</p>

<pre><code>array([[1, 2],
       [3, 4]])
</code></pre>

<p>I read a lot of material and I found that I can use <code>tf.shape(ph)</code>,let see:</p>

<pre><code>ph = tf.placeholder(shape=[None, 3], dtype=tf.int32)
x = tf.strided_slice(ph, [0,0],tf.shape(ph),[1,1])
input_ = np.array([[1,2,3],
                  [3,4,5],
                  [7,8,9]])
sess = tf.InteractiveSession()
sess.run(x,feed_dict={ph:input_})
</code></pre>

<p>out:</p>

<pre><code>array([[1, 2, 3],
       [3, 4, 5],
       [7, 8, 9]])
</code></pre>

<p>However, if I want to get the result like this:</p>

<pre><code>[[1, 2],
 [3, 4],
 [7, 8]]
</code></pre>

<p>What can I do?</p>
",2017-05-07 04:23:40,"<p>I having trouble understanding your question, but here's my attempt at answering it:</p>

<p>You can use the <code>x[:, :, :]</code> syntax to select all elements of an array:</p>

<pre><code>sess = tf.Session()
inp = tf.constant([[[1, 1, 1], [2, 2, 2]],
                   [[3, 3, 3], [4, 4, 4]],
                   [[5, 5, 5], [6, 6, 6]]])
print(inp.shape)

x = inp[:, :, :]
print(sess.run(x))
</code></pre>

<p>To get the last output you wanted, it's certainly possible with some manual  dimension calculations:</p>

<pre><code>sess = tf.Session()
x = tf.constant([[1,2,3],
                 [3,4,5],
                 [7,8,9]])
y = tf.shape(x)
bounds = tf.concat([y[:-1], [-1]], axis=0)
out = tf.strided_slice(x, [0,0], bounds, [1,1])
print(sess.run(out))
</code></pre>

<p>In general the Tensorflow slicing syntax follows numpy's slicing syntax, which is documented here:
<a href=""https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html</a></p>

<p>Hope that helps!</p>
",6080827,493,https://stackoverflow.com/questions/43827792,Documentation Replicability
44887367,"TensorFlow, tf.one_hot why the shape of output is defined by the value of axis?","<p>I read the <a href=""https://www.tensorflow.org/api_docs/python/tf/one_hot"" rel=""nofollow noreferrer"">docs of tf.one_hot</a> and found that </p>

<blockquote>
  <p>... . The new axis is created at dimension axis (default: the new axis is appended at the end).</p>
</blockquote>

<p>What is <code>The new axis</code>?</p>

<blockquote>
  <p>If indices is a vector of length features, the output shape will be:</p>
  
  <blockquote>
    <p>features x depth if axis == -1</p>
    
    <p>depth x features if axis == 0</p>
  </blockquote>
  
  <p>If indices is a matrix (batch) with shape [batch, features], the output shape will be:</p>
  
  <blockquote>
    <p>batch x features x depth if axis == -1</p>
    
    <p>batch x depth x features if axis == 1</p>
    
    <p>depth x batch x features if axis == 0</p>
  </blockquote>
</blockquote>

<p>Why the shape of output is defined by axis? </p>
",2017-07-03 13:53:16,"<p><code>tf.one_hot()</code> transforms a list of indices (e.g. <code>[0, 2, 1]</code>) and transforms it into a list of one-hot vectors of length <code>depth</code>.</p>

<p>For instance, if <code>depth = 3</code>, </p>

<ul>
<li>index 0 in the input will be replaced by [1, 0, 0]</li>
<li>index 1 in the input will be replaced by [0, 1, 0]</li>
<li>index 2 in the input will be replaced by [0, 0, 1]</li>
</ul>

<p>So <code>[0, 2, 1]</code> would be encoded as <code>[[1, 0, 0], [0, 0, 1], [0, 1, 0]]</code></p>

<p>As you can see, the output has one more dimension than the input (since each index is replaced by a vector). </p>

<p>By default (<strong>and what you usually need</strong>), the new dimension is created as the last one, so if your input is of shape <code>(d1, d2, .., dn)</code>, your output will be of shape <code>(d1, d2, .., dn, depth)</code>. But if you change the input parameter <em>axis</em>, you may choose to put the new dimension elsewhere, for instance if <code>axis=0</code> your output will be of shape <code>(depth, d1, d2, .., dn)</code>.</p>

<p>Changing the order of the dimensions is basically the n-dimensional version of transposing: you have the same data, but switch the order of the indices to access them (equivalent to switching the columns and the rows in a 2D matrix).</p>
",5046896,17040,https://stackoverflow.com/questions/44887367,Lack of Alternative Solutions/Documentation
43865115,Why tf.nn.separable_conv2d uses with_space_to_batch?,"<p>The implementation of <code>tf.nn.separable_conv2d</code> uses <code>tf.nn.with_space_to_batch</code>. But I don't see any difference in the outputs when <code>tf.nn.with_space_to_batch</code> is removed. What is the purpose of space to batch here? Is this a low-level optimization?</p>

<p>Related code;</p>

<pre><code>import unittest

import numpy as np
import tensorflow as tf


class TestTensorflowSeparableWithoutSpaceToBatch(unittest.TestCase):
    def setUp(self):
        #  filter height, filter width, in_channels, channel_multiplier
        self.depthwise_weights = np.random.rand(3, 3, 16, 3).astype(np.float32)
        self.pointwise_weights = np.random.rand(1, 1, 48, 64).astype(np.float32)
        #  batch size, height, width, channels
        self.input_batch = np.random.rand(20, 224, 224, 16).astype(np.float32)

    def __test_with_configuration(self, strides=1, padding=""SAME""):
        with tf.Session() as sess:
            input_placeholder = tf.placeholder(tf.float32, shape=[None, 224, 224, 16])

            # separable without space to batch
            depthwise_nostb = tf.nn.depthwise_conv2d_native(input=input_placeholder,
                                                            filter=self.depthwise_weights,
                                                            strides=[1, strides, strides, 1],
                                                            padding=padding,
                                                            name=""depthwise_only"")
            separable_nostb = tf.nn.conv2d(depthwise_nostb,
                                           filter=self.pointwise_weights,
                                           strides=[1, 1, 1, 1],
                                           padding=padding)

            # separable with space to batch
            separable_stb = tf.nn.separable_conv2d(input_placeholder,
                                                   depthwise_filter=self.depthwise_weights,
                                                   pointwise_filter=self.pointwise_weights,
                                                   strides=[1, strides, strides, 1],
                                                   padding=padding)
            tf.nn.with_space_to_batch()
            sess.run(tf.global_variables_initializer())

            nostb, stb, = sess.run([separable_nostb, separable_stb], feed_dict={input_placeholder: self.input_batch})

            np.testing.assert_array_equal(stb, nostb,
                                          err_msg=""output of tensorflow implementation is different than ours"")
            sess.close()

    def test_stride_1_padding_same(self):
        self.__test_with_configuration(strides=1, padding=""SAME"")

    def test_stride_2_padding_same(self):
        self.__test_with_configuration(strides=2, padding=""SAME"")

    def test_stride_3_padding_same(self):
        self.__test_with_configuration(strides=3, padding=""SAME"")

    def test_stride_1_padding_valid(self):
        self.__test_with_configuration(strides=1, padding=""VALID"")

    def test_stride_2_padding_valid(self):
        self.__test_with_configuration(strides=2, padding=""VALID"")

    def test_stride_3_padding_valid(self):
        self.__test_with_configuration(strides=3, padding=""VALID"")


if __name__ == '__main__':
    unittest.main()
</code></pre>
",2017-05-09 08:43:34,"<p>All the details you could ever want are in the comment describing <code>with_space_to_batch</code>
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L149"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L149</a></p>

<p>Note in particular the following text:
""In the special case that <code>dilation_rate</code> is uniformly 1, this simply returns: <code>op(input, num_spatial_dims, padding)</code>"".</p>

<p>So <code>space_to_batch</code> does nothing unless you have a dilated convolution.</p>
",545089,470,https://stackoverflow.com/questions/43865115,Documentation Replicability
43885770,Clarification of tf.name_scope in TensorFlow documentation,"<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/name_scope"" rel=""nofollow noreferrer"">TensorFlow documentation</a> mentions the following for <code>tf.name_scope</code></p>

<pre><code>This context manager validates that the given values are from the same
graph, makes that graph the default graph, and pushes a name scope in 
that graph.
</code></pre>

<p>What is the meaning of <code>given values are from the same graph, makes that graph the default graph</code> ? </p>

<p><code>Same graph</code> refers to which graph ?</p>

<p>Also, what is the use of <code>values</code> parameter in <code>tf.name_scope</code> ? </p>
",2017-05-10 07:12:52,"<p>This simply checks that <code>values</code> are Tensors built as part of the same <code>tf.Graph()</code> object. You can have more than one <code>tf.Graph()</code> in your program, and confusing things can happen if you mix nodes from two graphs. Most programs don't do this though, but often it is a good thing to check when writing library code.</p>

<p>There is no additional value to the <code>values</code> parameter than providing this error checking.</p>

<p>Hope this helps!</p>
",6842947,1859,https://stackoverflow.com/questions/43885770,Documentation Ambiguity
44526763,How to perform tf.image.per_image_standardization on a batch of images in tensorflow,"<p>I would like to know how to perform image whitening on a batch of images. </p>

<p>According to the documentation in <a href=""https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization</a>, it is said that <code>tf.image.per_image_standardization</code> takes as input a 3D tensor, that is an image, of shape: <code>[height, width, channels]</code>. </p>

<p>Is it a missing feature or there is a different method?</p>

<p>Any help is much appreciated. </p>
",2017-06-13 16:17:30,"<p>This is how to perform this operation on a batch of images.</p>

<p><code>tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), frames)</code></p>
",7886651,2282,https://stackoverflow.com/questions/44526763,Documentation Replicability
43604608,Why is my tf_gradients returning None?,"<pre><code># Defining the tf ops
prob_placeholder = tf.placeholder(tf.float32, shape=(2))
log_placeholder = tf.log(prob_placeholder)
grads_placeholder = tf.gradients(ys=tf.log(prob_placeholder), xs=model.weights)


# t is some index into the holders (which are lists)
# s is some state || p_a is some list of [p_1, 1 - p_1] || a_ is either 0 or 1 ||  r is 1

prob_ = tf_sess.run(prob_placeholder, {prob_placeholder: p_a})
log_ = tf_sess.run(log_placeholder, {prob_placeholder: prob_})
print(prob_, log_)
grads_ = tf_sess.run(grads_placeholder, {prob_placeholder: prob_})
</code></pre>

<p>Basically I'm not sure why it's returning None. </p>

<pre><code>TypeError: Fetch argument None has invalid type &lt;type 'NoneType'&gt;
</code></pre>

<p>I've tried adding print statements and I can see prob_ and log_ come out just fine but I'm not sure what's happening in tf.gradients that is causing the issue above.</p>

<p>model.weights are basically the weights of the model that I'm using.</p>
",2017-04-25 07:45:36,"<p><code>prob_placeholder</code> does not have any explicit dependence on <code>model.weights</code>, i.e. it is not functionally dependent on <code>model.weights</code> the way you have defined them.</p>

<p>Hence, though technically the gradient should be zero, it is computed as <code>None</code> due to <a href=""https://github.com/tensorflow/tensorflow/issues/783"" rel=""noreferrer"">technical reasons in TensorFlow</a>.</p>
",3532564,1895,https://stackoverflow.com/questions/43604608,Documentation Ambiguity
44540673,TypeError('The value of a feed cannot be a tf.Tensor object....) though I am providing it a numpy array,"<p>I am providing to my feed_dict a numpy array but it still gives this error that the feed need to be a tf.Tensor object.</p>

<pre><code>index = tf.placeholder(tf.int32, shape=[None], name='index')
dontknow = np.random.choice(range(1,200), 180)
_, summary = sess.run([train, merged], feed_dict={
            input_placeholder:train_batch_x,
            attr_placeholder:train_class_attr,
            label_placeholder:train_batch_y,
            index:dontknow
            })
</code></pre>

<p>Is this a bug in the tensorflow library since I wanted to post as an issue but wasn't sure. Any help is highly appreciated. 
Thanks</p>
",2017-06-14 09:25:17,"<p>I think that your problem is not with <code>dontknow</code> variable, it is with one of these:</p>

<pre><code>input_placeholder:train_batch_x,
attr_placeholder:train_class_attr,
label_placeholder:train_batch_y,  
</code></pre>

<p>When I remove them, I can execute your stuff without any error:</p>

<pre><code>import tensorflow as tf
import numpy as np

index = tf.placeholder(tf.int32, shape=[None], name='index')
dontknow = np.random.choice(range(1,200), 180)

with tf.Session() as sess:
    print sess.run(index, {index:dontknow})
</code></pre>

<p>Print each of them before doing your <code>sess.run</code> to find which one is the tensor</p>
",7289393,38,https://stackoverflow.com/questions/44540673,Documentation Replicability
44226932,Difference between tf.nn_conv2d and tf.nn.depthwise_conv2d,"<p>What is the difference between <code>tf.nn_conv2d</code> and <code>tf.nn.depthwise_conv2d</code> in Tensorflow?</p>
",2017-05-28 11:43:06,"<p>I am no expert on this, but as far as I understand the difference is this:</p>

<p>Lets say you have an input colour image with length 100, width 100. So the dimensions are 100x100x3. For both examples we use the same filter of width and height 5. Lets say we want the next layer to have a depth of 8.</p>

<p>In tf.nn.conv2d you define the kernel shape as [width, height, in_channels, out_channels]. In our case this means the kernel has shape [5,5,3,out_channels].
The weight-kernel that is strided over the image has a shape of 5x5x3, and it is strided over the whole image 8 times to produce 8 different feature maps.</p>

<p>In tf.nn.depthwise_conv2d you define the kernel shape as [width, height, in_channels, channel_multiplier]. Now the output is produced differently. Separate filters of 5x5x1 are strided over each dimension of the input image, one filter per dimension, each producing one feature map per dimension. So here, a kernel size [5,5,3,1] would produce an output with depth 3. The channel_multiplier tells you how many different filters you want to apply <strong>per dimension</strong>. So the original desired output of depth 8 is not possible with 3 input dimensions. Only multiples of 3 are possible.</p>
",7994456,1380,https://stackoverflow.com/questions/44226932,Lack of Alternative Solutions/Documentation
44395547,tensorflow: what's the difference between tf.nn.dropout and tf.layers.dropout,"<p>I'm quite confused about whether to use tf.nn.dropout or tf.layers.dropout.</p>

<p>many MNIST CNN examples seems to use tf.nn.droput, with keep_prop as one of params. </p>

<p>but how is it different with tf.layers.dropout? is the ""rate"" params in tf.layers.dropout similar to tf.nn.dropout?</p>

<p>Or generally speaking, is the difference between tf.nn.dropout and tf.layers.dropout applies to all other similar situations, like similar functions in tf.nn and tf.layers. </p>
",2017-06-06 16:44:30,"<p>A quick glance through 
<a href=""https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/layers/core.py"" rel=""noreferrer"">tensorflow/python/layers/core.py</a> and <a href=""https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/ops/nn_ops.py"" rel=""noreferrer"">tensorflow/python/ops/nn_ops.py</a>
reveals that <code>tf.layers.dropout</code> is a wrapper for <code>tf.nn.dropout</code>.</p>

<p>The only differences in the two functions are:</p>

<ol>
<li>The <code>tf.nn.dropout</code> has parameter <code>keep_prob</code>: ""Probability that each element is kept""<br> <code>tf.layers.dropout</code> has parameter <code>rate</code>: ""The dropout rate""<br> Thus, <code>keep_prob = 1 - rate</code> as defined <a href=""https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/layers/core.py#L256"" rel=""noreferrer"">here</a></li>
<li>The <code>tf.layers.dropout</code> has <code>training</code> parameter: ""Whether to return the output in training mode (apply dropout) or in inference mode (return the input untouched).""</li>
</ol>
",8102163,273,https://stackoverflow.com/questions/44395547,Documentation Replicability
44871248,Nonlinear input to output mapping (undefined range) using tensorflow,"<p>I have an array of 1D input data (30,1). I m trying to map this to output data (30,1) (with noise). I have plotted the data and it is definitely non-linear and continuous. </p>

<p>I want to train a neural network to reproduce this mapping. I am currently trying to complete this task using tensorflow.</p>

<p>My problem right now is that the output data is in an undefined range (e.g. -2.74230671e+01, 1.00000000e+03, 6.34566772e+02 etc), and non-linear tensorflow activation functions seem to all between -1 and 1? 
<a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_</a></p>

<p>I am rather new to tensorflow etc, so my question is, how do I approach this problem? </p>

<p>I thought I could mean-normalize the data, but since I don't actually know the range of the output values (possibly unbounded). </p>

<p>Is this possible using tensorflow functions or will I need to build my own? The approach I am using is below, where I tried different functions for tf.nn.relu:</p>

<pre><code>tf_x = tf.placeholder(tf.float32, x.shape)     # input x
tf_y = tf.placeholder(tf.float32, y.shape)     # output y

# neural network layers
l1 = tf.layers.dense(tf_x, 50, tf.nn.relu)     # tried different activation functions here
output = tf.layers.dense(l1, 1)                # tried here too

loss = tf.losses.mean_squared_error(tf_y, output)   
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)
train_op = optimizer.minimize(loss)

#train
for step in range(30):
_, l, pred = sess.run([train_op, loss, output], {tf_x: x, tf_y: y})
print(sess.run(loss, feed_dict={tf_x: x, tf_y: y}))
</code></pre>
",2017-07-02 13:19:45,"<p>You definitely have to normalize your data for it to work and it does not necessarily have to be in the range [-1, 1]. </p>

<p>Take a Computer Vision (CV) problem as an example. What some papers do is simply divide by <code>255.0</code>. Other papers, compute the <code>mean</code> and <code>standard_deviation</code> of each RGB channel from all the images. To normalize the images, we simply do <code>(x-mu)/sigma</code> over each channel.</p>

<p>Since your data is unbounded like what you said, then we can't simply divide by a scalar. Perhaps the best approach is to normalize based on the data statistics. Specific to your case, you could perhaps find the <code>mean</code> and <code>standard_deviation</code> of each of your 30 dimensions.</p>

<p>This <a href=""http://cs231n.github.io/neural-networks-2/"" rel=""nofollow noreferrer"">post</a> is more detailed and will potentially help you.</p>
",5919010,1264,https://stackoverflow.com/questions/44871248,Inadequate Examples
44871420,TensorFlow dynamic_rnn input for regression,"<p>I'm stuck trying to convert an existing tensorflow sequence to sequence classifier to a regressor.</p>

<p>Currently I'm stuck in handling the input for <code>tf.nn.dynamic_rnn()</code>. According to the documentation and other answers, input should be in the shape of <code>(batch_size, sequence_length, input_size)</code>. However my input data has only two dimensions: <code>(sequence_length, batch_size)</code>.</p>

<p>The original solution uses <code>tf.nn.embedding_lookup()</code> as an intermediate step before feeding input to <code>dynamic_rnn()</code>. If I understand correctly, I believe I don't need this step since I'm working on a regression problem, not a classification problem.</p>

<p>Do I need the embedding_lookup step? If so, why? If not, how can I fit my <code>encoder_inputs</code> directly into <code>dynamic_rnn()</code>?</p>

<p>Below is a working minimalized example of the general idea:</p>

<pre><code>import numpy as np
import tensorflow as tf

tf.reset_default_graph()
sess = tf.InteractiveSession()

PAD = 0
EOS = 1
VOCAB_SIZE = 10 # Don't think I should need this for regression?
input_embedding_size = 20

encoder_hidden_units = 20
decoder_hidden_units = encoder_hidden_units

LENGTH_MIN = 3
LENGTH_MAX = 8
VOCAB_LOWER = 2
VOCAB_UPPER = VOCAB_SIZE
BATCH_SIZE = 10

def get_random_sequences():
    sequences = []
    for j in range(BATCH_SIZE):
        random_numbers = np.random.randint(3, 10, size=8)
        sequences.append(random_numbers)
    sequences = np.asarray(sequences).T
    return(sequences)

def next_feed():
    batch = get_random_sequences()

    encoder_inputs_ = batch
    eos = np.ones(BATCH_SIZE)
    decoder_targets_ = np.hstack((batch.T, np.atleast_2d(eos).T)).T
    decoder_inputs_ = np.hstack((np.atleast_2d(eos).T, batch.T)).T

    #print(encoder_inputs_)
    #print(decoder_inputs_)

    return {
        encoder_inputs: encoder_inputs_,
        decoder_inputs: decoder_inputs_,
        decoder_targets: decoder_targets_,
    }

### ""MAIN""

# Placeholders
encoder_inputs = tf.placeholder(shape=(LENGTH_MAX, BATCH_SIZE), dtype=tf.int32, name='encoder_inputs')
decoder_targets = tf.placeholder(shape=(LENGTH_MAX + 1, BATCH_SIZE), dtype=tf.int32, name='decoder_targets')
decoder_inputs = tf.placeholder(shape=(LENGTH_MAX + 1, BATCH_SIZE), dtype=tf.int32, name='decoder_inputs')

# Don't think I should need this for regression problems
embeddings = tf.Variable(tf.random_uniform([VOCAB_SIZE, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)

# Encoder RNN
encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_inputs_embedded, # Throws 'ValueError: Shape (8, 10) must have rank at least 3' if encoder_inputs is used
    dtype=tf.float32, time_major=True,
)

# Decoder RNN
decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)
decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(
    decoder_cell, decoder_inputs_embedded, 
    initial_state=encoder_final_state,
    dtype=tf.float32, time_major=True, scope=""plain_decoder"",
)
decoder_logits = tf.contrib.layers.linear(decoder_outputs, VOCAB_SIZE)
decoder_prediction = tf.argmax(decoder_logits, 2)

# Loss function
loss = tf.reduce_mean(tf.squared_difference(decoder_logits, tf.one_hot(decoder_targets, depth=VOCAB_SIZE, dtype=tf.float32)))
train_op = tf.train.AdamOptimizer().minimize(loss)


sess.run(tf.global_variables_initializer())

max_batches = 5000
batches_in_epoch = 500

print('Starting train')
try:
    for batch in range(max_batches):
        feed = next_feed()
        _, l = sess.run([train_op, loss], feed)

        if batch == 0 or batch % batches_in_epoch == 0:
            print('batch {}'.format(batch))
            print('  minibatch loss: {}'.format(sess.run(loss, feed)))
            predict_ = sess.run(decoder_prediction, feed)
            for i, (inp, pred) in enumerate(zip(feed[encoder_inputs].T, predict_.T)):
                print('  sample {}:'.format(i + 1))
                print('    input     &gt; {}'.format(inp))
                print('    predicted &gt; {}'.format(pred))
                if i &gt;= 2:
                    break
            print()
except KeyboardInterrupt:
    print('training interrupted')
</code></pre>

<p>I have read similar questions here on stackoverflow but find my self still puzzled as to how to solve this.</p>

<p>EDIT:
I think I should clarify that the code above works well, however the real desired output should mimic a noisy signal (text to speech for example) which is why I think I need continuous output values instead of words or letters.</p>
",2017-07-02 13:40:52,"<p>If you are trying to do continuous why can't you just reshape your input placeholders to be of shape <code>[BATCH, TIME_STEPS, 1]</code> and add that one extra dimension into your input via <code>tf.expand_dims(input, 2)</code>. This way, your input would match the dimensions that <code>dynamic_rnn</code> expects (actually in your case, since you are doing <code>time_major=True</code> your input should be of shape <code>[TIME_STEPS, BATCH, 1])</code></p>

<p>I'd be curious to know how you'd then handle the switch of the output dimension from your cell size to 1. Right now you have this line:</p>

<pre><code>decoder_logits = tf.contrib.layers.linear(decoder_outputs, VOCAB_SIZE)
</code></pre>

<p>But since you are no longer doing a classification, then <code>VOCAB_SIZE</code> is just 1? I asked a similar question here a few days ago, but didn't get any responses. I'm doing it this way (using 1), but not sure whether it's appropriate (seems to sort-of work in practice, but not perfectly).</p>
",1145023,2097,https://stackoverflow.com/questions/44871420,Documentation Replication on Other Examples
44939540,How to get tensorflow to do a convolution on a 2 x 2 matrix with a 1 x 2 kernel?,"<p>I have the following matrix:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D&space;0&space;&amp;&space;1%5C%5C&space;2&space;&amp;&space;3&space;%5Cend%7Bbmatrix%7D"" title=""\begin{bmatrix} 0 &amp; 1\\ 2 &amp; 3 \end{bmatrix}"" /></p>

<p>and the following kernel:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D&space;1&space;&amp;&space;2&space;%5Cend%7Bbmatrix%7D"" title=""\begin{bmatrix} 1 &amp; 2 \end{bmatrix}"" /></p>

<p>If I do a convolution with no padding and slide by 1 row, I should get the following answer:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D&space;2&space;%5C%5C&space;8&space;%5Cend%7Bbmatrix%7D"" title=""\begin{bmatrix} 2 \\ 8 \end{bmatrix}"" /></p>

<p>Because:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?2&space;=&space;(0%5Ctimes1)&space;&plus;&space;(1%5Ctimes&space;2)"" title=""2 = (0\times1) + (1\times 2)"" /></p>

<p><img src=""https://latex.codecogs.com/gif.latex?8&space;=&space;(2%5Ctimes&space;1)&space;&plus;&space;(3&space;%5Ctimes&space;2)"" title=""8 = (2\times 1) + (3 \times 2)"" /></p>

<p>Based the documentation of  <code>tf.nn.conv2d</code>, I thought this code expresses what I just described above:</p>

<pre><code>import tensorflow as tf

input_batch = tf.constant([
    [
        [[.0], [1.0]],
        [[2.], [3.]]
    ]
])

kernel = tf.constant([
    [
        [[1.0, 2.0]]
    ]
])

conv2d = tf.nn.conv2d(input_batch, kernel, strides=[1, 1, 1, 1], padding='VALID')
sess = tf.Session()

print(sess.run(conv2d))
</code></pre>

<p>But it produces this output:</p>

<pre><code>[[[[ 0.  0.]
   [ 1.  2.]]

  [[ 2.  4.]
   [ 3.  6.]]]]
</code></pre>

<p>And I have no clue how that is computed. I've tried experimenting with different values for the strides padding parameter but still am not able to produce the result I expected.</p>
",2017-07-06 03:56:31,"<p>You have not correctly read my explanation in the tutorial you linked. After a straight-forward modification of <code>no-padding, strides=1</code> you suppose to get the following code.</p>

<pre><code>import tensorflow as tf
k = tf.constant([
    [1, 2],
], dtype=tf.float32, name='k')
i = tf.constant([
    [0, 1],
    [2, 3],
], dtype=tf.float32, name='i')
kernel = tf.reshape(k, [1, 2, 1, 1], name='kernel')
image  = tf.reshape(i, [1, 2, 2, 1], name='image')

res = tf.squeeze(tf.nn.conv2d(image, kernel, [1, 1, 1, 1], ""VALID""))
# VALID means no padding
with tf.Session() as sess:
   print sess.run(res)
</code></pre>

<p>Which gives you the result you expected: <code>[2., 8.]</code>. Here I got a vector instead of the column because of squeeze operator. </p>

<hr>

<p>One problem I see with your code (there might be other) is that your kernel is of the shape <code>(1, 1, 1, 2)</code>, but it suppose to be <code>(1, 2, 1, 1)</code>.</p>
",3775778,1108,https://stackoverflow.com/questions/44939540,Documentation Ambiguity
44949292,Is there a difference between activations functions in tensorflow? tf.nn.tanh vs tf.tanh,"<p>I want to setup a neuronal network and I am asking myself if there is a difference between those two functions? </p>

<p>tf.nn.tanh vs tf.tanh</p>
",2017-07-06 12:45:55,"<p>No, there's no difference.</p>

<p>In the <code>tensorflow/tensorflow/python/ops/nn.py</code> file (that's where <code>tf.nn</code> is defined) we can find the definition of <code>tanh</code>:</p>

<pre><code>from tensorflow.python.ops.math_ops import tanh
</code></pre>

<p>also, there's this <code>TODO</code> <a href=""https://github.com/tensorflow/tensorflow/blob/0b723590631432584d0761c03285eabb55116c6d/tensorflow/python/ops/nn.py"" rel=""nofollow noreferrer"">here</a></p>

<pre><code># TODO(cwhipkey): sigmoid and tanh should not be exposed from tf.nn.
</code></pre>

<p>Thus, probably tanh will be removed from the <code>tf.nn</code> package.</p>

<p>Hence <code>tf.tanh</code> (that's defined <a href=""https://github.com/tensorflow/tensorflow/blob/cf18c6d384a96a53b448bd51a90c117af0ed7c96/tensorflow/python/ops/math_ops.py"" rel=""nofollow noreferrer"">here</a>) is the one to use.</p>
",4355878,1375,https://stackoverflow.com/questions/44949292,Documentation Replicability
44478812,What kind of calculation does tf.nn.dynamic_rnn do with its input parameters?,"<p>What kind of calculation does <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer""><code>tf.nn.dynamic_rnn</code></a> perform? How does it use the parameters <code>cell</code> and <code>inputs</code> (to create the result)? </p>

<p>I have looked up in the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">documentation</a>, but I have not found an explanation.</p>
",2017-06-10 22:53:18,"<h2><code>tf.nn.static_rnn</code> vs. <code>tf.nn.dynamic_rnn</code></h2>

<p>Internally, <code>tf.nn.static_rnn</code> creates an unrolled graph for a fixed RNN length. That means that, if you call <code>tf.nn.static_rnn</code> with inputs having 200 time-steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, youre unable to pass in longer sequences (> 200) than you've originally specified.</p>

<p><code>tf.nn.dynamic_rnn</code> solves this. It uses a <code>tf.while_loop</code> to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size. </p>

<h3>What about performance?</h3>

<p>You may think the <code>tf.nn.static_rnn</code> is faster than its dynamic counterpart because it pre-builds the graph.</p>

<blockquote>
  <p>Please note, it is strongly encouraged to use <code>tf.nn.dynamic_rnn</code>.</p>
</blockquote>

<p>Reference: <a href=""http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/"" rel=""nofollow noreferrer"">http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/</a></p>
",6183280,175,https://stackoverflow.com/questions/44478812,Documentation Completeness
45067801,tensorflow: how to calculate the zero-mean and of the rgb values and uni-variance,"<p>I want to calculate the zero mean and univariance of an image. </p>

<p>I have already read in a pair of images in a list as tensors with the dimensions (m, n, 3)</p>

<p>The zero-mean is calculated by taking the mean of all red, green, blue values of all images in the list and substract the per image. </p>

<p>For this task, can I use the moments method? if yes, which axes are correct?</p>

<pre><code>mean, var = tf.nn.moments(input, axes=[0,1,2])
</code></pre>
",2017-07-12 21:27:37,"<p>For getting <code>mean</code> and <code>variance</code> using <code>tf.nn.moments</code> is the right thing. The axes parameter tells which axes to be included fro aggregating.</p>

<p>If you want a single mean\var for the entire <code>RGB</code> you can use:</p>

<pre><code> mean, var = tf.nn.moments(RGB, axes=[0,1,2])
</code></pre>

<p>if you want to get a mean/var for each of the channels(R,G,B), you can use:</p>

<pre><code> mean, var = tf.nn.moments(RGB, axes=[0,1])
</code></pre>
",4355878,1375,https://stackoverflow.com/questions/45067801,Lack of Alternative Solutions/Documentation
44206534,Why is tf.transpose so important in a RNN?,"<p>I've been reading the docs to learn TensorFlow and have been struggling on when to use the following functions and their purpose.</p>

<pre><code>tf.split()
tf.reshape()
tf.transpose()
</code></pre>

<p>My guess so far is that:</p>

<p>tf.split() is used because inputs must be a sequence.</p>

<p>tf.reshape() is used to make the shapes compatible (Incorrect shapes tends to be a common problem / mistake for me). I used numpy for this before. I'll probably stick to tf.reshape() now. I am not sure if there is a difference between the two. </p>

<p>tf.transpose() swaps the rows and columns from my understanding. If I don't use tf.transpose() my loss doesn't go down. If the parameter values are incorrect the loss doesn't go down. So the purpose of me using tf.transpose() is so that my loss goes down and my predictions become more accurate. </p>

<p>This bothers me tremendously because I'm using tf.transpose() because I have to and have no understanding why it's such an important factor. I'm assuming if it's not used correctly the inputs and labels can be in the wrong position. Making it impossible for the model to learn. If this is true how can I go about using tf.transpose() so that I am not so reliant on figuring out the parameter values via trial and error?  </p>
",2017-05-26 16:48:07,"<h1>Question</h1>

<ol>
<li>Why do I need <strong>tf.transpose()</strong>?</li>
<li>What is the purpose of <strong>tf.transpose()</strong>?</li>
</ol>

<h1>Answer</h1>

<ol>
<li><p>Why do I need <strong>tf.transpose()</strong>?  I can't imagine why you would need it unless you coded your solution from the beginning to require it.  For example, suppose I have 120 student records with 50 stats per student and I want to use that to try and make a linear association with their chance of taking 3 classes.  I'd state it like so</p>

<p>c = r x m</p></li>
</ol>

<p>r = records, a matrix with a shape if [120x50]
m = the induction matrix.  it has a shape of [50x3]
c = the chance of all students taking one of three courses, a matrix with a shape of [120x3]</p>

<p>Now if instead of making m [50x3], we goofed and made m [3x50], then we'd have to transpose it before multiplication.</p>

<ol start=""2"">
<li>What is the purpose of <strong>tf.transpose()</strong>?</li>
</ol>

<p>Sometimes you just need to swap rows and columns, like above.  <a href=""https://en.wikipedia.org/wiki/Transpose"" rel=""nofollow noreferrer"">Wikipedia</a> has a fantastic page on it.  The transpose function has some excellent properties for matrix math function, like associativeness and associativeness with the inverse function.</p>

<h1>Summary</h1>

<p>I don't think I've ever used <strong>tf.transpose</strong> in any <strong>CNN</strong> I've written.</p>
",4005959,318,https://stackoverflow.com/questions/44206534,Documentation Replicability
44681810,Is tf.train.GradientDescentOptimizer a vanilla GradientDescent?,"<p>Is <code>tf.train.GradientDescentOptimizer</code> a vanilla gradient descent? i.e. not SGD, so it is equivalent to a gradient update implemented in numpy.</p>
",2017-06-21 16:42:26,"<p>Yes, it's the vanilla gradient descent. You can't say it is not SGD. Because it depends on your number of examples you consider when training a single epoch. </p>

<p>Which means if you use only a mini-batch (ideally one single instance, but mini-batch is also fine) of data for a single epoch, We call it SGD. </p>

<p>Yes functionally it should be equivalent to <code>numpy</code> implementation. </p>
",3851003,1957,https://stackoverflow.com/questions/44681810,Documentation Replication on Other Examples
44232566,Add L2 regularization when using high level tf.layers,"<p>Is it possible to add an L2 regularization when using the layers defined in tf.layers? </p>

<p>It seems to me that since tf.layers is an high level wrapper, there is no easy way to get access to the filter weights.</p>

<p><strong>With tf.nn.conv2d</strong></p>

<pre><code>regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)

weights = tf.get_variable(
    name=""weights"",
    regularizer=regularizer
)

#Previous layers

...

#Second layer 
layer 2 = tf.nn.conv2d(
input,
weights,
[1,1,1,1],
[1,1,1,1])

#More layers
...

#Loss
loss = #some loss

reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)
loss += reg_term
</code></pre>

<p><strong>Now what would that look like with tf.layers.conv2d?</strong></p>

<p>Thanks!</p>
",2017-05-28 22:21:57,"<p>You can pass them into <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/conv2d"" rel=""noreferrer""><code>tf.layers.conv2d</code> as arguments:</a></p>

<pre><code>regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)
layer2 = tf.layers.conv2d(
    inputs,
    filters,
    kernel_size,
    kernel_regularizer=regularizer)
</code></pre>

<p>Then you should add the regularization loss to your loss like this:</p>

<pre><code>l2_loss = tf.losses.get_regularization_loss()
loss += l2_loss
</code></pre>

<p><em>Edit: Thanks Zeke Arneodo, Tom and srcolinas I added, the last bit on your feedback so that the accepted answer provides the complete solution.</em></p>
",6255101,609,https://stackoverflow.com/questions/44232566,Documentation Replication on Other Examples
44244763,TensorFlow tf.group ignoring dependencies?,"<p>Following on from an <a href=""https://stackoverflow.com/questions/44244275/tensorflow-fifoqueue-not-fifo"">earlier question</a>, it seems <code>tf.group</code> is indeed ignoring dependencies. Here's a simple stand-alone example (I have run it on Python 2.7 with TensorFlow 1.1):</p>

<pre><code>import tensorflow as tf
from tensorflow.python.ops import control_flow_ops

xs = [tf.constant(x) for x in range(10)]
xs = [tf.Print(x, [x]) for x in xs]
dependency = None
dxs = []

for x in xs:
    if dependency is None:
        dependency = x
    else:
        dependency = control_flow_ops.with_dependencies([dependency], x)

    dxs.append(dependency)

print_all_op = tf.group(*dxs)

with tf.Session() as session:
    session.run(print_all_op)
</code></pre>

<p>Expected output:</p>

<pre><code>2017-05-29 15:11:53.961221: I tensorflow/core/kernels/logging_ops.cc:79] [0]
2017-05-29 15:11:53.961236: I tensorflow/core/kernels/logging_ops.cc:79] [1]
2017-05-29 15:11:53.961255: I tensorflow/core/kernels/logging_ops.cc:79] [2]
2017-05-29 15:11:53.961237: I tensorflow/core/kernels/logging_ops.cc:79] [3]
2017-05-29 15:11:53.961262: I tensorflow/core/kernels/logging_ops.cc:79] [4]
2017-05-29 15:11:53.961263: I tensorflow/core/kernels/logging_ops.cc:79] [5]
2017-05-29 15:11:53.961268: I tensorflow/core/kernels/logging_ops.cc:79] [6]
2017-05-29 15:11:53.961272: I tensorflow/core/kernels/logging_ops.cc:79] [7]
2017-05-29 15:11:53.961274: I tensorflow/core/kernels/logging_ops.cc:79] [8]
2017-05-29 15:11:53.961221: I tensorflow/core/kernels/logging_ops.cc:79] [9]
</code></pre>

<p>Actual output (different each time the code is run):</p>

<pre><code>2017-05-29 15:16:26.279655: I tensorflow/core/kernels/logging_ops.cc:79] [0]
2017-05-29 15:16:26.279655: I tensorflow/core/kernels/logging_ops.cc:79] [9]
2017-05-29 15:16:26.279697: I tensorflow/core/kernels/logging_ops.cc:79] [3]
2017-05-29 15:16:26.279660: I tensorflow/core/kernels/logging_ops.cc:79] [1]
2017-05-29 15:16:26.279711: I tensorflow/core/kernels/logging_ops.cc:79] [8]
2017-05-29 15:16:26.279713: I tensorflow/core/kernels/logging_ops.cc:79] [4]
2017-05-29 15:16:26.279723: I tensorflow/core/kernels/logging_ops.cc:79] [5]
2017-05-29 15:16:26.279663: I tensorflow/core/kernels/logging_ops.cc:79] [2]
2017-05-29 15:16:26.279724: I tensorflow/core/kernels/logging_ops.cc:79] [7]
2017-05-29 15:16:26.279728: I tensorflow/core/kernels/logging_ops.cc:79] [6]
</code></pre>

<p>There's nothing in the <a href=""https://www.tensorflow.org/api_docs/python/tf/group"" rel=""nofollow noreferrer""><code>tf.group</code></a> documentation to indicate why dependencies are ignored.</p>

<p>Is there an alternative to <code>tf.group</code> that does consider dependencies?</p>

<p>Switching to use <code>tf.control_dependencies</code> instead of <code>tensorflow.python.ops.control_flow_ops.with_dependencies</code> doesn't help:</p>

<pre><code>import tensorflow as tf

xs = [tf.constant(x) for x in range(10)]
xs = [tf.Print(x, [x]) for x in xs]
dependency = None
dxs = []

for x in xs:
    if dependency is None:
        dependency = x
    else:
        with tf.control_dependencies([dependency]):
            dependency = x

    dxs.append(dependency)

print_all_op = tf.group(*dxs)

with tf.Session() as session:
    session.run(print_all_op)
</code></pre>
",2017-05-29 14:20:22,"<p>I think the problem is that the initial code creates dependencies between dummy identity ops implicitly created by <code>control_flow_ops.with_dependencies</code> and not the actual <code>tf.Print</code> ops. Tensorflow seems to only ensure that the ops in the dependency list have been already executed but the order of other preceding ops is not fixed. In the above example, the dependencies are created on the dummy identity ops created by <code>control_flow_ops.with_dependencies</code>:</p>

<pre><code>    dependency = control_flow_ops.with_dependencies([dependency], x)
</code></pre>

<p>which should be equivalent to:</p>

<pre><code>        with tf.control_dependencies([dependency]):
            dependency = tf.identity(x)
</code></pre>

<p>Thus, the dependencies here are created between the <code>tf.identity</code> ops and not the <code>tf.Print</code> ops. The <code>tf.Print</code> ops can be run in any order, the strict ordering is only on the <code>tf.identity</code> ops. I don't think it is possible to achieve the desired behavior with <code>control_flow_ops.with_dependencies</code>. Instead one has to use <code>with tf.control_dependencies</code> instead (as already suggested by the op):</p>

<pre><code>xs = [tf.constant(x) for x in range(10)]
dependency = None
dxs = []

for x in xs:
    if dependency is None:
        dependency = tf.Print(x, [x])
    else:
        with tf.control_dependencies([dependency]):
            dependency = tf.Print(x, [x])

    dxs.append(dependency)
</code></pre>
",127480,33899,https://stackoverflow.com/questions/44244763,Lack of Alternative Solutions/Documentation
44315874,Is adding a dimension broadcasting?,"<p>Given</p>

<pre><code>a = tf.constant([[1, 2, 3], [10, 20, 30], [100, 200, 300], [1000, 2000, 3000]])
</code></pre>

<p>all of the following are equivalent</p>

<pre><code>b = tf.constant([100000, 200000, 300000])
print((a+b).eval())

bb = tf.constant([[100000, 200000, 300000]])
print((a+bb).eval())

bbb = tf.constant([[100000, 200000, 300000], [100000, 200000, 300000], [100000, 200000, 300000], [100000, 200000, 300000]])
print((a+bbb).eval())
</code></pre>

<p>and produce</p>

<pre><code>[[100001 200002 300003]
 [100010 200020 300030]
 [100100 200200 300300]
 [101000 202000 303000]]
</code></pre>

<p>I understand that <code>bb</code> is ""broadcast"" to the value corresponding to <code>bbb</code> by <code>tf.add</code> (here <code>+</code>). Is the addition of a dimension that transforms <code>b</code> to the value of <code>bbb</code> all broadcasting, or is it something else?</p>
",2017-06-01 19:43:33,"<p>As you mentioned in the comments, <code>b</code>, <code>bb</code> are both valid forms of broadcasting. As mentioned in the <code>numpy</code> <a href=""https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html#general-broadcasting-rules"" rel=""nofollow noreferrer"">documentation</a>,</p>

<blockquote>
  <p>Arrays do not need to have the same number of dimensions.</p>
</blockquote>
",656912,46191,https://stackoverflow.com/questions/44315874,Documentation Replication on Other Examples
45107068,How to fine-tune model using `MonitoredTrainingSession` / `Scaffold`,"<p>I want to restore model parameters of VGG_19 which is used as feature extractor for an appended newly initialised graph and train everything in a distributed setup. </p>

<p>Everything works if I use <code>slim.learning.train</code>, but I am not able to get it to work with the <code>Scaffold</code> required by  <code>tf.train.MonitoredTrainingSession</code>. If I pass a <code>restore_fn</code> (created using <code>tf.contrib.framework.assign_from_checkpoint_fn</code> <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/training/python/training/training.py#L139"" rel=""nofollow noreferrer"">as in documentaiton</a>)as <code>init_fn</code> into the <code>Scaffold</code> I am getting 
<code>
TypeError: callback() takes 1 positional argument but 2 were given
</code></p>

<p>I tried ""fixing"" it by passing a <code>lambda scaffold, sess: restore_fn(sess)</code>.</p>

<p>If I try to create a restore operator and pass it in as <code>init_op</code> (created with <code>tf.contrib.slim.assign_from_checkpoint</code> I am getting </p>

<pre><code>INFO:tensorflow:Create CheckpointSaverHook.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, fetches, contraction_fn)
    267         self._unique_fetches.append(ops.get_default_graph().as_graph_element(
--&gt; 268             fetch, allow_tensor=True, allow_operation=True))
    269       except TypeError as e:

/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operatio
n)
   2608     if self._finalized:
-&gt; 2609       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
   2610

/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_
operation)
   2700       raise TypeError(""Can not convert a %s into a %s.""
-&gt; 2701                       % (type(obj).__name__, types_str))
   2702

TypeError: Can not convert a ndarray into a Tensor or Operation.

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/ScanAvoidanceML/ScanAvoidanceML/datasets/project_daphnis/train.py in &lt;module&gt;()
    129         )
    130         FLAGS, unparsed = parser.parse_known_args()
--&gt; 131         tf.app.run(main=train, argv=[sys.argv[0]] + unparsed)

/opt/conda/lib/python3.6/site-packages/tensorflow/python/platform/app.py in run(main, argv)
     46   # Call the main function, passing through any arguments
     47   # to the final program.
---&gt; 48   _sys.exit(main(_sys.argv[:1] + flags_passthrough))
     49
     50

/ScanAvoidanceML/ScanAvoidanceML/datasets/project_daphnis/train.py in train(_)
     83                 scaffold=tf.train.Scaffold(
     84                     init_op=restore_op,
---&gt; 85                     summary_op=tf.summary.merge_all())) as mon_sess:
     86             while not mon_sess.should_stop():
     87                 # Run a training step asynchronously.

/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in MonitoredTrainingSession(master, is_chief,
checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grac
e_period_secs, log_step_count_steps)
    351     all_hooks.extend(hooks)
    352   return MonitoredSession(session_creator=session_creator, hooks=all_hooks,
--&gt; 353                           stop_grace_period_secs=stop_grace_period_secs)
    354
    355

/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in __init__(self, session_creator, hooks, stop
_grace_period_secs)
    654     super(MonitoredSession, self).__init__(
    655         session_creator, hooks, should_recover=True,
--&gt; 656         stop_grace_period_secs=stop_grace_period_secs)
    657
    658

/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in __init__(self, session_creator, hooks, shou
ld_recover, stop_grace_period_secs)
    476         stop_grace_period_secs=stop_grace_period_secs)
    477     if should_recover:
--&gt; 478       self._sess = _RecoverableSession(self._coordinated_creator)
    479     else:
    480       self._sess = self._coordinated_creator.create_session()


/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in __init__(self, sess_creator)
    828     """"""
    829     self._sess_creator = sess_creator
--&gt; 830     _WrappedSession.__init__(self, self._create_session())
    831
    832   def _create_session(self):

/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in _create_session(self)
    833     while True:
    834       try:
--&gt; 835         return self._sess_creator.create_session()
    836       except _PREEMPTION_ERRORS as e:
    837         logging.info('An error was raised while a session was being created. '

/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in create_session(self)
    537       """"""Creates a coordinated session.""""""
    538       # Keep the tf_sess for unit testing.
--&gt; 539       self.tf_sess = self._session_creator.create_session()
    540       # We don't want coordinator to suppress any exception.
    541       self.coord = coordinator.Coordinator(clean_stop_exception_types=[])

/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py in create_session(self)
    411         init_op=self._scaffold.init_op,
    412         init_feed_dict=self._scaffold.init_feed_dict,
--&gt; 413         init_fn=self._scaffold.init_fn)
    414
    415

/opt/conda/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py in prepare_session(self, master, init_op, saver,
 checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)
    277                            ""init_fn or local_init_op was given"")
    278       if init_op is not None:
--&gt; 279         sess.run(init_op, feed_dict=init_feed_dict)
    280       if init_fn:
    281         init_fn(sess)

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    894     try:
    895       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 896                          run_metadata_ptr)
    897       if run_metadata:
    898         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_met
adata)
   1107     # Create a fetch handler to take care of the structure of fetches.
   1108     fetch_handler = _FetchHandler(
-&gt; 1109         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
   1110
   1111     # Run request and get response.

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, graph, fetches, feeds, feed_handles)
    409     """"""
    410     with graph.as_default():
--&gt; 411       self._fetch_mapper = _FetchMapper.for_fetch(fetches)
    412     self._fetches = []
    413     self._targets = []

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in for_fetch(fetch)
    229     elif isinstance(fetch, (list, tuple)):
    230       # NOTE(touts): This is also the code path for namedtuples.
--&gt; 231       return _ListFetchMapper(fetch)
    232     elif isinstance(fetch, dict):
    233       return _DictFetchMapper(fetch)

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, fetches)
    336     """"""
    337     self._fetch_type = type(fetches)
--&gt; 338     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
    339     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    340

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in &lt;listcomp&gt;(.0)
    336     """"""
    337     self._fetch_type = type(fetches)
--&gt; 338     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
    339     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    340

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in for_fetch(fetch)
    231       return _ListFetchMapper(fetch)
    232     elif isinstance(fetch, dict):
--&gt; 233       return _DictFetchMapper(fetch)
    234     else:
    235       # Look for a handler in the registered expansions.

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, fetches)
    369     self._keys = fetches.keys()
    370     self._mappers = [_FetchMapper.for_fetch(fetch)
--&gt; 371                      for fetch in fetches.values()]
    372     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    373

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in for_fetch(fetch)
    237         if isinstance(fetch, tensor_type):
    238           fetches, contraction_fn = fetch_fn(fetch)
--&gt; 239           return _ElementFetchMapper(fetches, contraction_fn)
    240     # Did not find anything.
    241     raise TypeError('Fetch argument %r has invalid type %r' %

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, fetches)
    369     self._keys = fetches.keys()
    370     self._mappers = [_FetchMapper.for_fetch(fetch)
--&gt; 371                      for fetch in fetches.values()]
    372     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    373

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in &lt;listcomp&gt;(.0)
    369     self._keys = fetches.keys()
    370     self._mappers = [_FetchMapper.for_fetch(fetch)
--&gt; 371                      for fetch in fetches.values()]
    372     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    373

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in for_fetch(fetch)
    237         if isinstance(fetch, tensor_type):
    238           fetches, contraction_fn = fetch_fn(fetch)
--&gt; 239           return _ElementFetchMapper(fetches, contraction_fn)
    240     # Did not find anything.
    241     raise TypeError('Fetch argument %r has invalid type %r' %

/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py in __init__(self, fetches, contraction_fn)
    270         raise TypeError('Fetch argument %r has invalid type %r, '
    271                         'must be a string or Tensor. (%s)'
--&gt; 272                         % (fetch, type(fetch), str(e)))
    273       except ValueError as e:
    274         raise ValueError('Fetch argument %r cannot be interpreted as a '

TypeError: Fetch argument array([[[[ 0.39416704, -0.08419707, -0.03631314, ..., -0.10720515,
          -0.03804016,  0.04690642],
         [ 0.46418372,  0.03355668,  0.10245045, ..., -0.06945956,
          -0.04020201,  0.04048637],
         [ 0.34119523,  0.09563112,  0.0177449 , ..., -0.11436455,
          -0.05099866, -0.00299793]],

        [[ 0.37740308, -0.07876257, -0.04775979, ..., -0.11827433,
          -0.19008617, -0.01889699],
         [ 0.41810837,  0.05260524,  0.09755926, ..., -0.09385028,
          -0.20492788, -0.0573062 ],
         [ 0.33999205,  0.13363543,  0.02129423, ..., -0.13025227,
          -0.16508926, -0.06969624]],

        [[-0.04594866, -0.11583115, -0.14462094, ..., -0.12290562,
          -0.35782176, -0.27979308],
         [-0.04806903, -0.00658076, -0.02234544, ..., -0.0878844 ,
          -0.3915486 , -0.34632796],
         [-0.04484424,  0.06471398, -0.07631404, ..., -0.12629718,
          -0.29905206, -0.28253639]]],


       [[[ 0.2671299 , -0.07969447,  0.05988706, ..., -0.09225675,
           0.31764674,  0.42209673],
         [ 0.30511212,  0.05677647,  0.21688674, ..., -0.06828708,
           0.3440761 ,  0.44033417],
         [ 0.23215917,  0.13365699,  0.12134422, ..., -0.1063385 ,
           0.28406844,  0.35949969]],

        [[ 0.09986369, -0.06240906,  0.07442063, ..., -0.02214639,
           0.25912452,  0.42349899],
         [ 0.10385381,  0.08851637,  0.2392226 , ..., -0.01210995,
           0.27064082,  0.40848857],
         [ 0.08978214,  0.18505956,  0.15264879, ..., -0.04266965,
           0.25779948,  0.35873157]],

        [[-0.34100872, -0.13399366, -0.11510294, ..., -0.11911335,
          -0.23109646, -0.19202407],
         [-0.37314063, -0.00698938,  0.02153259, ..., -0.09827439,
          -0.2535741 , -0.25541356],
         [-0.30331427,  0.08002605, -0.03926321, ..., -0.12958746,
          -0.19778992, -0.21510386]]],


       [[[-0.07573577, -0.07806503, -0.03540679, ..., -0.1208065 ,
           0.20088433,  0.09790061],
         [-0.07646758,  0.03879711,  0.09974211, ..., -0.08732687,
           0.2247974 ,  0.10158388],
         [-0.07260918,  0.10084777,  0.01313597, ..., -0.12594968,
           0.14647409,  0.05009392]],

        [[-0.28034249, -0.07094654, -0.0387974 , ..., -0.08843154,
           0.18996507,  0.07766484],
         [-0.31070709,  0.06031388,  0.10412455, ..., -0.06832542,
           0.20279962,  0.05222717],
         [-0.246675  ,  0.1414054 ,  0.02605635, ..., -0.10128672,
           0.16340195,  0.02832468]],

        [[-0.41602272, -0.11491341, -0.14672887, ..., -0.13079506,
          -0.1379628 , -0.26588449],
         [-0.46453714, -0.00576723, -0.02660675, ..., -0.10017379,
          -0.15603794, -0.32566148],
         [-0.33683276,  0.06601517, -0.08144748, ..., -0.13460518,
          -0.1342358 , -0.27096185]]]], dtype=float32) has invalid type &lt;class 'numpy.ndarray'&gt;, must be a string or Tensor. (Can not
 convert a ndarray into a Tensor or Operation.)
</code></pre>

<p>I tried using a <code>local_init_op</code>, too, which did not work. 
My code:</p>

<pre><code>import sys
import tensorflow as tf
slim = tf.contrib.slim
import argparse
import model as M
import decoder as D


FLAGS = None


def train(_):
    vgg_19_ckpt_path='/media/data/projects/project_daphnis/pretrained_models/vgg_19.ckpt'
    train_log_dir = ""/media/data/projects/project_daphnis/train_log_dir""

    ps_hosts = FLAGS.ps_hosts.split("","")
    worker_hosts = FLAGS.worker_hosts.split("","")

    # Create a cluster from the parameter server and worker hosts.
    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})

    # Create and start a server for the local task.
    server = tf.train.Server(cluster,
                             job_name=FLAGS.job_name,
                             task_index=FLAGS.task_index)

    if FLAGS.job_name == ""ps"":
        server.join()
    elif FLAGS.job_name == ""worker"":
        if not tf.gfile.Exists(train_log_dir):
            tf.gfile.MakeDirs(train_log_dir)

        # Assigns ops to the local worker by default.
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster)):

            # Set up the data loading:
            image, c, p, s = \
                D.get_training_dataset_data_provider()

            image, c, p, s = \
                tf.train.batch([image, c, p, s],
                               batch_size=16)

            # Define the model:
            predictions, loss, end_points = M.model_as_in_paper(
                image, c, p, s
            )

            restore_fn = tf.contrib.framework.assign_from_checkpoint_fn(
                vgg_19_ckpt_path,
                var_list=slim.get_variables_to_restore(include=[""vgg_19""],
                                                       exclude=[
                                                           'vgg_19/conv4_3_X',
                                                           'vgg_19/conv4_4_X']
                                                       )
            )


            # Specify the optimization scheme:
            optimizer = tf.train.AdamOptimizer(learning_rate=.00001)

            # create_train_op that ensures that when we evaluate it to get the loss,
            # the update_ops are done and the gradient updates are computed.
            train_op = slim.learning.create_train_op(loss, optimizer)
        tf.summary.scalar(""losses/total_loss"", loss)

        # The StopAtStepHook handles stopping after running given steps.
        hooks = [tf.train.StopAtStepHook(last_step=1000000)]

        # The MonitoredTrainingSession takes care of session initialization,
        # restoring from a checkpoint, saving to a checkpoint, and closing when done
        # or an error occurs.
        with tf.train.MonitoredTrainingSession(
                master=server.target,
                is_chief=(FLAGS.task_index == 0),
                checkpoint_dir=train_log_dir,
                hooks=hooks,
                scaffold=tf.train.Scaffold(
                    init_fn=restore_fn,
                    summary_op=tf.summary.merge_all())) as mon_sess:
            while not mon_sess.should_stop():
                # Run a training step asynchronously.
                # See `tf.train.SyncReplicasOptimizer` for additional details on how to
                # perform *synchronous* training.
                # mon_sess.run handles AbortedError in case of preempted PS.
                mon_sess.run(train_op)
        #
        # # Actually runs training.
        # slim.learning.train(train_tensor,
        #                     train_log_dir,
        #                     init_fn=restore_fn,
        #                     summary_op=tf.summary.merge_all(),
        #                     is_chief=False)

if __name__ == ""__main__"":
    if __name__ == ""__main__"":
        parser = argparse.ArgumentParser()
        parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")
        # Flags for defining the tf.train.ClusterSpec
        parser.add_argument(
            ""--ps_hosts"",
            type=str,
            default="""",
            help=""Comma-separated list of hostname:port pairs""
        )
        parser.add_argument(
            ""--worker_hosts"",
            type=str,
            default="""",
            help=""Comma-separated list of hostname:port pairs""
        )
        parser.add_argument(
            ""--job_name"",
            type=str,
            default="""",
            help=""One of 'ps', 'worker'""
        )
        # Flags for defining the tf.train.Server
        parser.add_argument(
            ""--task_index"",
            type=int,
            default=0,
            help=""Index of task within the job""
        )
        FLAGS, unparsed = parser.parse_known_args()
        tf.app.run(main=train, argv=[sys.argv[0]] + unparsed)
</code></pre>
",2017-07-14 15:49:59,"<p>The answer is to use a saver to restore the parameters and to wrap the <code>saver.restore</code> function so it can be used as a <code>init_fn</code> of the <code>Scaffold</code>. This wrapper has to take two arguments: <code>scaffold</code> and <code>sess</code>, of which <code>sess</code> is used to restore the parameters and the <code>scaffold</code> is thrown away.</p>

<p>Complete code:</p>

<pre><code>import sys
import tensorflow as tf
slim = tf.contrib.slim
import argparse
import model as M
import decoder as D


FLAGS = None


def train(_):
    vgg_19_ckpt_path='/media/data/projects/project_daphnis/pretrained_models/vgg_19.ckpt'
    train_log_dir = ""/media/data/projects/project_daphnis/train_log_dir""

    ps_hosts = FLAGS.ps_hosts.split("","")
    worker_hosts = FLAGS.worker_hosts.split("","")

    # Create a cluster from the parameter server and worker hosts.
    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})

    # Create and start a server for the local task.
    server = tf.train.Server(cluster,
                             job_name=FLAGS.job_name,
                             task_index=FLAGS.task_index)

    if FLAGS.job_name == ""ps"":
        server.join()
    elif FLAGS.job_name == ""worker"":
        if not tf.gfile.Exists(train_log_dir):
            tf.gfile.MakeDirs(train_log_dir)

        # Assigns ops to the local worker by default.
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster)):

            # Set up the data loading:
            image, c, p, s = \
                D.get_training_dataset_data_provider()

            image, c, p, s = \
                tf.train.batch([image, c, p, s],
                               batch_size=16)

            # Define the model:
            predictions, loss, end_points = M.model_as_in_paper(
                image, c, p, s
            )

            values_to_restore = slim.get_variables_to_restore(
                include=[""vgg_19""],
                exclude=[
                    'vgg_19/conv4_3_X',
                    'vgg_19/conv4_4_X']
        )


            # Specify the optimization scheme:
            optimizer = tf.train.AdamOptimizer(learning_rate=.00001)

            # create_train_op that ensures that when we evaluate it to get the loss,
            # the update_ops are done and the gradient updates are computed.
            train_op = slim.learning.create_train_op(loss, optimizer)
        tf.summary.scalar(""losses/total_loss"", loss)

        # The StopAtStepHook handles stopping after running given steps.
        hooks = [tf.train.StopAtStepHook(last_step=1000000)]

        pre_train_saver = tf.train.Saver(values_to_restore)

        def load_pretrain(scaffold, sess):
            pre_train_saver.restore(sess,
                                    vgg_19_ckpt_path)

        # The MonitoredTrainingSession takes care of session initialization,
        # restoring from a checkpoint, saving to a checkpoint, and closing when done
        # or an error occurs.
        with tf.train.MonitoredTrainingSession(
                master=server.target,
                is_chief=(FLAGS.task_index == 0),
                checkpoint_dir=train_log_dir,
                hooks=hooks,
                scaffold=tf.train.Scaffold(
                    init_fn=load_pretrain,
                    summary_op=tf.summary.merge_all())) as mon_sess:

            while not mon_sess.should_stop():
                # Run a training step asynchronously.
                # See `tf.train.SyncReplicasOptimizer` for additional details on how to
                # perform *synchronous* training.
                # mon_sess.run handles AbortedError in case of preempted PS.
                mon_sess.run(train_op)

if __name__ == ""__main__"":
    if __name__ == ""__main__"":
        parser = argparse.ArgumentParser()
        parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")
        # Flags for defining the tf.train.ClusterSpec
        parser.add_argument(
            ""--ps_hosts"",
            type=str,
            default="""",
            help=""Comma-separated list of hostname:port pairs""
        )
        parser.add_argument(
            ""--worker_hosts"",
            type=str,
            default="""",
            help=""Comma-separated list of hostname:port pairs""
        )
        parser.add_argument(
            ""--job_name"",
            type=str,
            default="""",
            help=""One of 'ps', 'worker'""
        )
        # Flags for defining the tf.train.Server
        parser.add_argument(
            ""--task_index"",
            type=int,
            default=0,
            help=""Index of task within the job""
        )
        FLAGS, unparsed = parser.parse_known_args()
        tf.app.run(main=train, argv=[sys.argv[0]] + unparsed)
</code></pre>
",2156909,3815,https://stackoverflow.com/questions/45107068,Documentation Replication on Other Examples
45151015,How does tf.gradients behave when passed a list of `ys` tensors?,"<p>How exactly does <code>tf.gradients</code> behave when passed a list of tensors as its first argument? Take this very small example:</p>

<pre><code>a = tf.constant(5)
b = tf.constant(7)
c = a + 2 * b
</code></pre>

<p>If I compute the gradients of a single tensor, <code>c</code>, with respect to <code>[a,b]</code>, I get the expected answer:</p>

<pre><code>grads = tf.gradients(c, [a, b])
with tf.Session() as sess:
    sess.run(grads) # returns (1, 2)
</code></pre>

<p>According to the Tensorflow documentation, if you pass in a <em>list</em> of tensors as your first argument <code>ys</code>, <code>tf.gradients</code> will sum the gradients over that list, returning <code>sum_over_ys(dy/dx)</code> for each <code>x</code> in your second argument. So I would expect:</p>

<pre><code>tf.gradients([a, b, c], [a, b])
</code></pre>

<p>to behave the same way as:</p>

<pre><code>tf.gradients(a + b + c, [a, b])
</code></pre>

<p>Am I reading the docs wrong? When I test this code, I get the expected result <code>[2, 3]</code> for the second expression (explicitly summing <code>a + b + c</code>), but <code>[2, 1]</code> for the first. Where is this <code>[2, 1]</code> coming from?</p>
",2017-07-17 18:16:35,"<p>This is due to the fact that you are using <code>tf.constant</code>, which in theory shouldn't be affected by inputs.</p>

<p>If you replace your experiments with anything else (e.g. <code>Variables</code>) it works as expected.</p>

<p>When you apply an operator to the constant (be it addition, or even identity), you obtain a new tensor that is not <code>constant</code>, even though they depand on <code>constant</code>s only -- and therefore you obtain the expected behavior.</p>
",1931098,2074,https://stackoverflow.com/questions/45151015,Documentation Ambiguity
45172725,Tensorflow - Why are there so many similar or even duplicate functions in tf.nn and tf.layers / tf.losses / tf.contrib.layers etc?,"<p>In Tensorflow (as of v1.2.1), it seems that there are (at least) two parallel APIs to construct computational graphs. There are functions in tf.nn, like conv2d, avg_pool, relu, dropout and then there are similar functions in tf.layers, tf.losses and elsewhere, like tf.layers.conv2d, tf.layers.dense, tf.layers.dropout.</p>

<p>Superficially, it seems that this situation only serves to confuse: for example, tf.nn.dropout uses a 'keep rate' while tf.layers.dropout uses a 'drop rate' as an argument.</p>

<p>Does this distinction have any practical purpose for the end-user / developer?
If not, is there any plan to cleanup the API? </p>
",2017-07-18 16:45:54,"<p>Tensorflow proposes on the one hand a low level API (<code>tf.</code>, <code>tf.nn.</code>...), and on the other hand, a higher level API (<code>tf.layers.</code>, <code>tf.losses.</code>,...).</p>

<p>The goal of the higher level API is to provide functions that greatly simplify the design of the most common neural nets. The lower level API is there for people with special needs, or who wishes to keep a finer control of what is going on.</p>

<p>The situation is a bit confused though, because some functions have the same or similar names, and also, there is no clear way to distinguish at first sight which namespace correspond to which level of the API.</p>

<p>Now, let's look at <code>conv2d</code> for example. A striking difference between <code>tf.nn.conv2d</code> and <code>tf.layers.conv2d</code> is that the later takes care of all the variables needed for  weights and biases. A single line of code, and <em>voil</em>, you just created a convolutional layer. With <code>tf.nn.conv2d</code>, you have to take declare the weights variable yourself before passing it to the function. And as for the biases, well, they are actually not even handled: you need to add them yourself later.</p>

<p>Add to that that <code>tf.layers.conv2d</code> also proposes to add regularization and activation in the same function call, you can imagine how this can reduce code size when one's need is covered by the higher-level API.</p>

<p>The higher level also makes some decisions by default that could be considered as best practices. For example, losses in <code>tf.losses</code> are added to the <code>tf.GraphKeys.LOSSES</code> collection by default, which makes recovery and summation of the various component easy and somewhat standardized. If you use the lower level API, you would need to do all of that yourself. Obviously, you would need to be careful when you start mixing low and high level API functions there.</p>

<p>The higher-level API is also an answer to a great need from people that have been otherwise used to similarly high-level function in other frameworks, Theano aside. This is rather obvious when one ponders the number of alternative higher level APIs built on top of tensorflow, such as keras 2 (now <a href=""https://blog.keras.io/introducing-keras-2.html"" rel=""noreferrer"">part of the official tensorflow API</a>), slim (in <code>tf.contrib.slim</code>), tflearn, tensorlayer, and the likes.</p>

<p>Finally, if I may add an advice: if you are beginning with tensorflow and do not have a preference towards a particular API, I would personnally encourage you to stick to the <code>tf.keras.*</code> API:</p>

<ul>
<li>Its API is friendly and at least as good as the other high-level APIs built on top of the low-level tensorflow API</li>
<li>It has a clear namespace within tensorflow (although it can -- and sometimes should -- be used with parts from other namespaces, such as <code>tf.data</code>)</li>
<li>It is now a first-class citizen of tensorflow (it used to be in <code>tf.contrib.keras</code>), and care is taken to make new tensorflow features (such as <code>eager</code>) compatible with keras.</li>
<li>Its generic implementation can use other toolkits such as CNTK, and so does not lock you to tensorflow.</li>
</ul>
",5615276,736,https://stackoverflow.com/questions/45172725,Documentation Replicability
45701681,Determining the Epoch Number with tf.train.string_input_producer in tensorflow,"<p>I have some doubts on how <code>tf.train.string_input_producer</code> works. So suppose I fed filename_list as an input parameter to the <code>string_input_producer</code>. Then, according to the documentation <a href=""https://www.tensorflow.org/programmers_guide/reading_data"" rel=""nofollow noreferrer"">https://www.tensorflow.org/programmers_guide/reading_data</a>, this will create a <code>FIFOQueue</code>, where I can set epoch number, shuffle the file names and so on. Therefore, in my case, I have 4 file names (""db1.tfrecords"", ""db2.tfrecords""...). And I used <code>tf.train.batch</code> to feed the network batch of images. In addition, each file_name/database, contain a set of images for one person. The second database is for the second person and so on. So far I have the following code: </p>

<pre><code>tfrecords_filename_seq = [(common + ""P16_db.tfrecords""), (common + ""P17_db.tfrecords""), (common + ""P19_db.tfrecords""),
                          (common + ""P21_db.tfrecords"")]

filename_queue = tf.train.string_input_producer(tfrecords_filename_seq, num_epochs=num_epoch, shuffle=False, name='queue')
reader = tf.TFRecordReader()

key, serialized_example = reader.read(filename_queue)
features = tf.parse_single_example(
    serialized_example,
    # Defaults are not specified since both keys are required.
    features={
        'height': tf.FixedLenFeature([], tf.int64),
        'width': tf.FixedLenFeature([], tf.int64),
        'image_raw': tf.FixedLenFeature([], tf.string),
        'annotation_raw': tf.FixedLenFeature([], tf.string)
    })

image = tf.decode_raw(features['image_raw'], tf.uint8)
height = tf.cast(features['height'], tf.int32)
width = tf.cast(features['width'], tf.int32)

image = tf.reshape(image, [height, width, 3])

annotation = tf.cast(features['annotation_raw'], tf.string)

min_after_dequeue = 100
num_threads = 4
capacity = min_after_dequeue + num_threads * batch_size
label_batch, images_batch = tf.train.batch([annotation, image],
                                                        shapes=[[], [112, 112, 3]],
                                                        batch_size=batch_size,
                                                        capacity=capacity,
                                                        num_threads=num_threads)
</code></pre>

<p>Finally, when trying to view out the reconstructed image at the output of the autoencoder, I got the first the images from the 1st database, then I start viewing images from the second database and so on. </p>

<p>My question: How can i know if I'm within the same epoch? And if I'm within the sane epoch, how can i merge a batch of images from all the file_names that I have?</p>

<p>Finally, I tried to print out the value of the epoch by evaluating the local variable within the <code>Session</code> as follows: </p>

<pre><code>epoch_var = tf.local_variables()[0]
</code></pre>

<p>Then: </p>

<pre><code>with tf.Session() as sess:
    print(sess.run(epoch_var.eval())) # Here I got 9 as output. don't know y.
</code></pre>

<p>Any help is much appreciated!!</p>
",2017-08-15 21:16:29,"<p>So what I figured out is that using <code>tf.train.shuffle_batch_join</code> solves my issue as it starts shuffling images from different data sets. In other words, every batch is now containing images from all the datasets/file_names. Here is an example:</p>

<pre><code>def read_my_file_format(filename_queue):
    reader = tf.TFRecordReader()
    key, serialized_example = reader.read(filename_queue)
    features = tf.parse_single_example(
        serialized_example,
        # Defaults are not specified since both keys are required.
        features={
            'height': tf.FixedLenFeature([], tf.int64),
            'width': tf.FixedLenFeature([], tf.int64),
            'image_raw': tf.FixedLenFeature([], tf.string),
            'annotation_raw': tf.FixedLenFeature([], tf.string)
        })

    # This is how we create one example, that is, extract one example from the database.
    image = tf.decode_raw(features['image_raw'], tf.uint8)
    # The height and the weights are used to
    height = tf.cast(features['height'], tf.int32)
    width = tf.cast(features['width'], tf.int32)

    # The image is reshaped since when stored as a binary format, it is flattened. Therefore, we need the
    # height and the weight to restore the original image back.
    image = tf.reshape(image, [height, width, 3])

    annotation = tf.cast(features['annotation_raw'], tf.string)
    return annotation, image

def input_pipeline(filenames, batch_size, num_threads, num_epochs=None):
    filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epoch, shuffle=False,
                                                    name='queue')
    # Therefore, Note that here we have created num_threads readers to read from the filename_queue.
    example_list = [read_my_file_format(filename_queue=filename_queue) for _ in range(num_threads)]
    min_after_dequeue = 100
    capacity = min_after_dequeue + num_threads * batch_size
    label_batch, images_batch = tf.train.shuffle_batch_join(example_list,
                                                            shapes=[[], [112, 112, 3]],
                                                            batch_size=batch_size,
                                                            capacity=capacity,
                                                            min_after_dequeue=min_after_dequeue)
    return label_batch, images_batch, example_list

label_batch, images_batch, input_ann_img = \
    input_pipeline(tfrecords_filename_seq, batch_size, num_threads, num_epochs=num_epoch)
</code></pre>

<p>And now this is going to create a number of readers to read from the <code>FIFOQueue</code>, and after each reader will have a different decoder. Finally, after decoding the images, they will fed into another <code>Queue</code> that is created after calling <code>tf.train.shuffle_batch_join</code> to feed the network a batch of images. </p>
",7886651,2282,https://stackoverflow.com/questions/45701681,Documentation Replicability
45203872,How tf.train.shuffle_batch works?,"<p>Does it do one shuffling in one epoch, or else?</p>

<p>What is the difference of tf.train.shuffle_batch and tf.train.batch?</p>

<p>Could someone explain it? Thanks.</p>
",2017-07-20 02:02:41,"<p>First take a look at the documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/train/batch"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/train/batch</a> ). Internally batch is build around a FIFOQueue and shuffle_batch is build around a RandomShuffleQueue.</p>

<p>Consider the following toy example, it puts 1 to 100 in a constant which gets fed through tf.train.shuffle_batch and tf.train.batch and later on this prints the results. </p>

<pre><code>import tensorflow as tf
import numpy as np

data = np.arange(1, 100 + 1)
data_input = tf.constant(data)

batch_shuffle = tf.train.shuffle_batch([data_input], enqueue_many=True, batch_size=10, capacity=100, min_after_dequeue=10, allow_smaller_final_batch=True)
batch_no_shuffle = tf.train.batch([data_input], enqueue_many=True, batch_size=10, capacity=100, allow_smaller_final_batch=True)

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    for i in range(10):
        print(i, sess.run([batch_shuffle, batch_no_shuffle]))
    coord.request_stop()
    coord.join(threads)
</code></pre>

<p>Which yields:</p>

<pre><code>0 [array([23, 48, 15, 46, 78, 89, 18, 37, 88,  4]), array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])]
1 [array([80, 10,  5, 76, 50, 53,  1, 72, 67, 14]), array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20])]
2 [array([11, 85, 56, 21, 86, 12,  9,  7, 24,  1]), array([21, 22, 23, 24, 25, 26, 27, 28, 29, 30])]
3 [array([ 8, 79, 90, 81, 71,  2, 20, 63, 73, 26]), array([31, 32, 33, 34, 35, 36, 37, 38, 39, 40])]
4 [array([84, 82, 33,  6, 39,  6, 25, 19, 19, 34]), array([41, 42, 43, 44, 45, 46, 47, 48, 49, 50])]
5 [array([27, 41, 21, 37, 60, 16, 12, 16, 24, 57]), array([51, 52, 53, 54, 55, 56, 57, 58, 59, 60])]
6 [array([69, 40, 52, 55, 29, 15, 45,  4,  7, 42]), array([61, 62, 63, 64, 65, 66, 67, 68, 69, 70])]
7 [array([61, 30, 53, 95, 22, 33, 10, 34, 41, 13]), array([71, 72, 73, 74, 75, 76, 77, 78, 79, 80])]
8 [array([45, 52, 57, 35, 70, 51,  8, 94, 68, 47]), array([81, 82, 83, 84, 85, 86, 87, 88, 89, 90])]
9 [array([35, 28, 83, 65, 80, 84, 71, 72, 26, 77]), array([ 91,  92,  93,  94,  95,  96,  97,  98,  99, 100])]
</code></pre>
",7424342,129,https://stackoverflow.com/questions/45203872,Requesting (Additional) Documentation/Examples
44946189,TypeError: __init__() got an unexpected keyword argument 'shape',"<p>I am new to Tensorflow and I met an error while trying to run some sample codes.</p>

<pre><code>import tensorflow as tf

g1 = tf.Graph()
with g1.as_default():
    v = tf.get_variable(""v"", initializer=tf.zeros_initializer(shape=[1]))
</code></pre>

<p>Running the code above gives the error:
TypeError: __init__() got an unexpected keyword argument 'shape'.</p>

<p>The comment below says that tf.zeros_initializer does not accept 'shape' argument according to the documentation. I tried</p>

<pre><code>v = tf.get_variable(""v"", initializer=tf.zeros_initializer())
</code></pre>

<p>and it says ValueError: Shape of a new variable (v) must be fully defined, but instead was .</p>

<p>So, what kind of argument/expression should I use to define the shape without causing a type error?</p>

<p>I cannot find how to solve it online. Please help. Thank you</p>
",2017-07-06 10:19:08,"<p>It appears that the book I'm using is a bit out of date. The following code appears to work so far.</p>

<pre><code>v = tf.get_variable(""v"", shape=[1], initializer=tf.zeros_initializer)
</code></pre>

<p>I will check if it actually works with more code added later.</p>
",2504541,93,https://stackoverflow.com/questions/44946189,Documentation Replication on Other Examples
45208516,K-means example(tf.expand_dims),"<p>In Example code of Kmeans of Tensorflow,</p>

<p>When use the function 'tf.expand_dims'(Inserts a dimension of 1 into a tensor's shape.) in point_expanded, centroids_expanded 
before calculate tf.reduce_sum.</p>

<p>why is these have different indexes(0, 1) in second parameter?</p>

<pre><code>import numpy as np
import tensorflow as tf
points_n = 200
clusters_n = 3
iteration_n = 100
points = tf.constant(np.random.uniform(0, 10, (points_n, 2)))
centroids = tf.Variable(tf.slice(tf.random_shuffle(points), [0, 0],[clusters_n, -1]))
points_expanded = tf.expand_dims(points, 0)
centroids_expanded = tf.expand_dims(centroids, 1)
distances = tf.reduce_sum(tf.square(tf.subtract(points_expanded, centroids_expanded)), 2)
assignments = tf.argmin(distances, 0)
means = []
for c in range(clusters_n):
    means.append(tf.reduce_mean(tf.gather(points,tf.reshape(tf.where(tf.equal(assignments, c)), [1, -1])), reduction_indices=[1]))
new_centroids = tf.concat(means,0)
update_centroids = tf.assign(centroids, new_centroids)
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    for step in range(iteration_n):
    [_, centroid_values, points_values, assignment_values] = sess.run([update_centroids, centroids, points, assignments])
    print(""centroids"" + ""\n"", centroid_values)
plt.scatter(points_values[:, 0], points_values[:, 1], c=assignment_values, s=50, alpha=0.5)
plt.plot(centroid_values[:, 0], centroid_values[:, 1], 'kx', markersize=15)
plt.show()
</code></pre>
",2017-07-20 07:46:00,"<p>This is done to subtract each centroid from each point. First, make sure you understand the notion of broadcasting (<a href=""https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a>)
 that is linked from tf.subtract (<a href=""https://www.tensorflow.org/api_docs/python/tf/subtract"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/subtract</a>). Then, you just need to draw the shapes of <code>points</code>, <code>expanded_points</code>, <code>centroids</code>, and <code>expanded_centroids</code> and understand what values get ""broadcast"" where. Once you do that you will see that broadcasting allows you to compute exactly what you want - subtract each point from each centroid.</p>

<p>As a sanity check, since there are 200 points, 3 centroids, and each is 2D, we should have 200*3*2 differences. This is exactly what we get:  </p>

<pre><code>In [53]: points
Out[53]: &lt;tf.Tensor 'Const:0' shape=(200, 2) dtype=float64&gt;

In [54]: points_expanded
Out[54]: &lt;tf.Tensor 'ExpandDims_4:0' shape=(1, 200, 2) dtype=float64&gt;

In [55]: centroids
Out[55]: &lt;tf.Variable 'Variable:0' shape=(3, 2) dtype=float64_ref&gt;

In [56]: centroids_expanded
Out[56]: &lt;tf.Tensor 'ExpandDims_5:0' shape=(3, 1, 2) dtype=float64&gt;

In [57]: tf.subtract(points_expanded, centroids_expanded)
Out[57]: &lt;tf.Tensor 'Sub_5:0' shape=(3, 200, 2) dtype=float64&gt;
</code></pre>

<p>If you are having trouble drawing the shapes, you can think of broadcasting the <code>expanded_points</code> with dimension <code>(1, 200, 2)</code> to dimension <code>(3, 200, 2)</code> as copying the 200x2 matrix 3 times along the first dimension. The 3x2 matrix in <code>centroids_expanded</code> (of shape (3, 1, 2)) get copied 200 times along the second dimension.</p>
",8282898,937,https://stackoverflow.com/questions/45208516,Documentation Replicability
44963306,Cannot printout concatenated tensor by tf.concat() (tensorflow 1.2.1 - gpu / py36),"<p>Learning <strong><a href=""https://www.tensorflow.org/api_docs/python/"" rel=""nofollow noreferrer"">Tensorflow</a></strong> (Python bindings) since the last month. I've been reading the docs on <code>tf.concat()</code>, but cannot resolve the problem as shown below, so I'm asking for your help!</p>

<p>What I want to do is to see the contents of the concatenated tensor.
I tried <code>Tensor.eval()</code>.</p>

<pre><code>import tensorflow as tf 
import numpy as np 

a=np.zeros([3,3])
a_trail=np.ones([3,3])

with tf.Session() as sess:
    concatenated=tf.concat([a, a_trail], axis=0) 
    print(concatenated)
    print(type(concatenated)) 
    concatenated.eval() 
    sess.run(concatenated) 
    sess.run(tf.constant(concatenated)) 
</code></pre>

<p><strong>Output:</strong>   </p>

<pre><code>Tensor(""concat_2:0"", shape=(6, 3), dtype=float64)
&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;
(nothing prints)
(nothing shows up either meh =/)
Error: List of Tensors when single Tensor expected    
</code></pre>

<p><code>tf.concat()</code> supposed to return <code>Tensor</code> and looks like it does. But why aren't <code>T.eval()</code> and <code>sess.run()</code> not working?</p>
",2017-07-07 05:43:27,"<p>Most of TF functions take tensors as parameters. This applies to <a href=""https://www.tensorflow.org/api_docs/python/tf/concat"" rel=""nofollow noreferrer"">tf.concat</a>. As you can see from the documentation:</p>

<blockquote>
  <p>values: A list of Tensor objects or a single Tensor.</p>
</blockquote>

<p>And you provide numpy arrays and not tensors. Here is a working example.</p>

<pre><code>import tensorflow as tf 
import numpy as np 

a=np.zeros([3,3])
b=np.ones([3,3])

A = tf.constant(a)
B = tf.constant(b)
concatenated=tf.concat([A, B], axis=0) 

with tf.Session() as sess:
    print sess.run(concatenated)
</code></pre>

<p>Not related to your question, but separate a graph definition from a graph execution (you do everything in <code>tf.session</code>, I define everything outside and only execute in the session</p>
",8063281,23,https://stackoverflow.com/questions/44963306,Documentation Replication on Other Examples
45030619,Detecting out-of-bounds slicing with tf.slice like in numpy,"<p>In tensorflow, I'm trying to use tf.slice, but <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/array_ops/slicing_and_joining"" rel=""nofollow noreferrer"">as its documentation states</a>, it requires the slice to fit in the input array. For instance, if you try to slice the first 5 positions of the tensor [1,2,3,4] it will crash. I want to have the same functionality we get with python lists or numpy arrays where slicing gets you the intersection of the original array and the slice you asked for. For instance if you ask for positions 2 to 6 of [1,2,3,4] you'll get [2,3,4].</p>

<p>How can I do that in tensorflow?</p>

<p>Thanks!</p>
",2017-07-11 09:30:53,"<p>You could use tensorflow's python slicing operator, which is slightly more powerful than <code>tf.slice</code> and in particular does some bound checking to behave similarly to <code>numpy</code>.</p>

<pre><code>x = tf.zeros((10,))
y = tf.slice(x, [5], [15])
print(y.shape)
# (15,)
z = x[5:42]
print(z.shape)
# (5,)
</code></pre>
",4189580,13484,https://stackoverflow.com/questions/45030619,Documentation Replication on Other Examples
44640357,Does Tensorflow's tf.while_loop automatically capture dependencies when executing in parallel?,"<p>I am interested in implementing a Recursive Neural Network in Tensorflow, like what has been done in <a href=""https://stackoverflow.com/questions/37054188/how-can-i-implement-a-recursive-neural-network-in-tensorflow"">How can I implement a recursive neural network in TensorFlow?</a>. </p>

<p>However, in his implementation, the <code>parallel_iterations</code> of the <code>tf.while_loop</code> statement was fixed to be 1. I fear that this might be too slow. Since the tree I am going to feed into tensorflow have parts that are not dependent on each other, I would hope that I could set <code>parallel_iterations</code> to a higher value. However, it is inevitable that there are some dependencies required in the tree I feed in as input to tensorflow, and I am afraid that setting it to higher value may break the dependency property. </p>

<p>So my question is, had Tensorflow's <code>tf.while_loop</code> automatically captured dependencies already, in order to only use paralleism on placed that are not dependent on each other?</p>

<p>The tensorflow documentation says the following:</p>

<blockquote>
  <p>For correct programs, while_loop should return the same result for any
  parallel_iterations > 0.</p>
</blockquote>

<p>But I am not sure what they mean by ""correct programs"".</p>
",2017-06-19 21:36:50,"<p>You can.</p>

<p>according to this <a href=""https://github.com/tensorflow/tensorflow/issues/1984"" rel=""nofollow noreferrer"">issue</a>, ops will run in parallel once all their inputs nodes are computed:</p>

<blockquote>
  <p>while_loop implements non-strict semantics. An iteration can start as soon as one of the ops for this iteration is ready (i.e., all its inputs are available.) for execution. So a while_loop can easily have multiple iterations running in parallel. For example, for scan, even if the accumulated value is not available in a step, the step can still start and execute any ops that don't depend on the accumulated value. </p>
</blockquote>

<p>so you shouldn't encounter any issues.</p>
",3608412,501,https://stackoverflow.com/questions/44640357,Documentation Ambiguity
44796793,Difference between tf.clip_by_value and tf.clip_by_global_norm for RNN's and how to decide max value to clip on?,"<p>Want to understand the difference in roles of <code>tf.clip_by_value</code> and <code>tf.clip_by_global_norm</code> during the implementation of Gradient Clipping in TensorFlow. Which one is preferred and how to decide the max value to clip on?</p>
",2017-06-28 08:00:03,"<p><strong>TL;DR</strong>: use <code>tf.clip_by_global_norm</code> for gradient clipping, with &quot;some high value&quot; as max value.</p>
<h3>clip_by_value</h3>
<p><code>tf.clip_by_value</code> clips each value inside one tensor, regardless of the other values in the tensor. For instance,</p>
<pre><code>tf.clip_by_value([-1, 2, 10], 0, 3)  -&gt; [0, 2, 3]  # Only the values below 0 or above 3 are changed
</code></pre>
<p>Consequently, it can change the direction of the tensor, so it should be used if the values in the tensor are decorrelated one from another (which is not the case for gradient clipping), or to avoid zero / infinite values in a tensor that could lead to Nan / infinite values elsewhere (by clipping with a minimum of epsilon=1e-8 and a very big max value for instance).</p>
<h3>clip_by_norm</h3>
<p><code>tf.clip_by_norm</code> rescales one tensor if necessary, so that its L2 norm does not exceed a certain threshold. It's useful typically to avoid exploding gradient on one tensor, because you keep the gradient direction. For instance:</p>
<pre><code>tf.clip_by_norm([-2, 3, 6], 5)  -&gt; [-2, 3, 6]*5/7  # The original L2 norm is 7, which is &gt;5, so the final one is 5
tf.clip_by_norm([-2, 3, 6], 9)  -&gt; [-2, 3, 6]  # The original L2 norm is 7, which is &lt;9, so it is left unchanged
</code></pre>
<p>However, <code>clip_by_norm</code> works on only one gradient, so if you use it on all your gradient tensors, you'll unbalance them (some will be rescaled, others not, and not all with the same scale).</p>
<p>Note that the two first ones work on only one tensor, while the last one is used on a list of tensors.</p>
<h3>clip_by_global_norm</h3>
<p><code>tf.clip_by_global_norm</code> rescales a list of tensors so that the total norm of the vector of all their norms does not exceed a threshold. The goal is the same as  <code>clip_by_norm</code> (avoid exploding gradient, keep the gradient directions), but it works on all the gradients at once rather than on each one separately (that is, all of them are rescaled by the same factor if necessary, or none of them are rescaled). This is better, because the balance between the different gradients is maintained.</p>
<p>For instance:</p>
<pre><code>tf.clip_by_global_norm([tf.constant([-2, 3, 6]),tf.constant([-4, 6, 12])] , 14.5)
</code></pre>
<p>will rescale both tensors by a factor <code>14.5/sqrt(49 + 196)</code>, because the first tensor has a L2 norm of 7, the second one 14, and <code>sqrt(7^2+ 14^2)&gt;14.5</code></p>
<p>This (<code>tf.clip_by_global_norm</code>) is the one that you should use for gradient clipping. See <a href=""https://arxiv.org/pdf/1211.5063.pdf"" rel=""nofollow noreferrer"">this</a> for instance for more information.</p>
<h3>Choosing the value</h3>
<p>Choosing the max value is the hardest part. You should use the biggest value such that you don't have exploding gradient (whose effects can be <code>Nan</code>s or <code>infinite</code> values appearing in your tensors, constant loss /accuracy after a few training steps). The value should be bigger for <code>tf.clip_by_global_norm</code> than for the others, since the global L2 norm will be mechanically bigger than the other ones due to the number of tensors implied.</p>
",7730199,417,https://stackoverflow.com/questions/44796793,Documentation Replicability
45774938,tensorflow: tf.split is given weird parameters,"<p>Here is code(<a href=""https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/"" rel=""nofollow noreferrer"">from here</a>):</p>

<pre><code>import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.ops import rnn, rnn_cell
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot = True)

hm_epochs = 3
n_classes = 10
batch_size = 128
chunk_size = 28
n_chunks = 28
rnn_size = 128


x = tf.placeholder('float', [None, n_chunks,chunk_size])
y = tf.placeholder('float')
def recurrent_neural_network(x):
    layer = {'weights':tf.Variable(tf.random_normal([rnn_size,n_classes])),
             'biases':tf.Variable(tf.random_normal([n_classes]))}

    x = tf.transpose(x, [1,0,2])
    x = tf.reshape(x, [-1, chunk_size])
    x = tf.split(0, n_chunks, x)

    lstm_cell = rnn_cell.BasicLSTMCell(rnn_size,state_is_tuple=True)
    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)

    output = tf.matmul(outputs[-1],layer['weights']) + layer['biases']

    return output
def train_neural_network(x):
    prediction = recurrent_neural_network(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )
    optimizer = tf.train.AdamOptimizer().minimize(cost)


    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())

        for epoch in range(hm_epochs):
            epoch_loss = 0
            for _ in range(int(mnist.train.num_examples/batch_size)):
                epoch_x, epoch_y = mnist.train.next_batch(batch_size)
                epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))

                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})
                epoch_loss += c

            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)

        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))

        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        print('Accuracy:',accuracy.eval({x:mnist.test.images.reshape((-1, n_chunks, chunk_size)), y:mnist.test.labels}))

train_neural_network(x)
</code></pre>

<p>I have issue understanding <code>x = tf.split(0, n_chunks, x)</code> , more specificaly third parameter(<code>x</code>-input). By <a href=""https://www.tensorflow.org/api_docs/python/tf/split"" rel=""nofollow noreferrer"">documenation</a> this should be axis...but that can't be, right? Isn't <code>x</code> one dimensional?
I apologise if it's trivial, I'm beginner and can't sem to get it. Maybe it's just formality, but if it is I don't understand how it works...</p>
",2017-08-19 18:19:09,"<blockquote>
  <p>By documenation this should be axis...but that can't be, right? </p>
</blockquote>

<p>From tensorflow 1.0 onwards, the first argument of <code>tf.split</code> is not the axis, but I assume that the code was written using an older version where the first argument is indeed the axis. </p>

<blockquote>
  <p>Isn't x one dimensional?</p>
</blockquote>

<p><code>x</code> is not one dimensional. Right before the call to <code>tf.split</code>, <code>x</code> is reshaped from 3 to 2 dimensions with this statement:</p>

<pre><code>x = tf.reshape(x, [-1, chunk_size])
</code></pre>

<p>The statement reshapes <code>x</code> into a tensor with two dimensions: the size of the second dimension is <code>chunk_size</code> and the size of the first dimension is inferred (that is what the <code>-1</code> denotes here).</p>
",8081595,515,https://stackoverflow.com/questions/45774938,Documentation Ambiguity
45879776,TensorFlow how to make results reproducible for `tf.nn.sampled_softmax_loss`,"<p>I would like to get reproducible results for my tensorflow runs. The way I'm trying to make this happen is to set up the numpy and tensorflow seeds:</p>

<pre><code>import numpy as np
rnd_seed = 1
np.random.seed(rnd_seed)

import tensorflow as tf
tf.set_random_seed(rnd_seed)
</code></pre>

<p>As well as make sure that the weights of the neural network, that I initialized with <code>tf.truncated_normal</code> also use that seed: <code>tf.truncated_normal(..., seed=rnd_seed)</code></p>

<p>For reasons that are beyond the scope of this question, I'm using the sampled softmax loss function, <code>tf.nn.sampled_softmax_loss</code>, and unfortunately, I'm not able to control the stochasticity of this function with a random seed.</p>

<p>By a look at the TensorFlow documentation of this function (<a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss</a>), I can see that parameter <code>sampled_values</code> should be the only parameter that affects randomization, but I'm not able to understand how to actually use a seed.</p>

<p>[EDITED]
This is (part of) my script</p>

<pre><code>import numpy as np
# set a seed so that the results are consistent
rnd_seed = 1
np.random.seed(rnd_seed)

import tensorflow as tf
tf.set_random_seed(rnd_seed)

embeddings_ini = np.random.uniform(low=-1, high=1, size=(self.vocabulary_size, self.embedding_size))

with graph.as_default(), tf.device('/cpu:0'):

    train_dataset = tf.placeholder(tf.int32, shape=[None, None])
    train_labels = tf.placeholder(tf.int32, shape=[None, 1])
    valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)

    # Variables.
    initial_embeddings = tf.placeholder(tf.float32, shape=(self.vocabulary_size, self.embedding_size))
    embeddings = tf.Variable(initial_embeddings)

    softmax_weights = tf.Variable(
        tf.truncated_normal([self.vocabulary_size, self.embedding_size],
                            stddev=1.0 / math.sqrt(self.embedding_size), seed=rnd_seed))
    softmax_biases = tf.Variable(tf.zeros([self.vocabulary_size]))

    # Model.
    # Look up embeddings for inputs.
    if self.model == ""skipgrams"":
        # Skipgram model
        embed = tf.nn.embedding_lookup(embeddings, train_dataset)
    elif self.model == ""cbow"":
        # CBOW Model
        embeds = tf.nn.embedding_lookup(embeddings, train_dataset)
        embed = tf.reduce_mean(embeds, 1, keep_dims=False)

    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights,
                                                     biases=softmax_biases,
                                                     inputs=embed,
                                                     labels=train_labels,
                                                     num_sampled=self.num_sampled,
                                                     num_classes=self.vocabulary_size))
</code></pre>
",2017-08-25 10:52:21,"<p>I finally, found out how to make results reproducible. Just like @Anis suggested I should've set the graph seed and this can be done by:</p>

<pre><code>with graph.as_default(), tf.device('/cpu:0'):
    tf.set_random_seed(1234)
</code></pre>
",863713,14296,https://stackoverflow.com/questions/45879776,Documentation Replicability
45229165,How can I serve the Faster RCNN with Resnet 101 model with tensorflow serving,"<p>I am trying to serve the Faster RCNN with Resnet 101 model with tensorflow serving.</p>

<p>I know I need to use tf.saved_model.builder.SavedModelBuilder to export the model definition as well as variables, then I need a script like inception_client.py provided by tensorflow_serving. </p>

<p>while I am going through the examples and documentation and experimenting, I think someone may have done the same thing. So plase help if you have done the same or know how to get it done. Thanks in advance.</p>
",2017-07-21 04:47:55,"<p>Tensorflow Object Detection API has its own exporter script that is more sophisticated than the outdated examples found under Tensorflow Serving. </p>

<p>While building Tensorflow Serving, make sure you pull the latest master commit of tensorflow/tensorflow (>r1.2) and tensorflow/models</p>

<p><strong>Build Tensorflow Serving for GPU</strong></p>

<p><code>bazel build -c opt --config=cuda tensorflow_serving/...</code></p>

<p>If you face errors regarding crosstool and nccl, follow the solutions at 
<a href=""https://github.com/tensorflow/serving/issues/186#issuecomment-251152755"" rel=""nofollow noreferrer"">https://github.com/tensorflow/serving/issues/186#issuecomment-251152755</a>
<a href=""https://github.com/tensorflow/serving/issues/327#issuecomment-305771708"" rel=""nofollow noreferrer"">https://github.com/tensorflow/serving/issues/327#issuecomment-305771708</a></p>

<p><strong>Usage</strong></p>

<p><code>python tf_models/object_detection/export_inference_graph.py \
--pipeline_config_path=/path/to/ssd_inception_v2.config \
--trained_checkpoint_prefix=/path/to/trained/checkpoint/model.ckpt \
--output_directory /path/to/output/1 \
--export_as_saved_model \
--input_type=image_tensor</code></p>

<p>Note that during export all variables are converted into constants and baked into the protobuf binary. Don't be panicked if you don't find any files under saved_model/variables directory</p>

<p>To start the server,</p>

<p><code>bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=inception_v2 --model_base_path=/path/to/output --enable_batching=true</code></p>

<p>As for the client, the examples under Tensorflow Serving work well</p>
",5566610,109,https://stackoverflow.com/questions/45229165,Documentation Replicability
45247909,Tensorflow - How to get the gradients of the output w.r.t the model parameters,"<p>I would like to know if it is possible to compute the gradients of the output of a model with respect to the model parameters.  In other words I would like to compute <code>dy / d theta</code>.</p>

<p>Here is a short example of what I mean:</p>

<pre><code>import keras
import tensorflow as tf

# Dummy input
test = np.random.rand(1, 32, 32, 1)

x = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))

model = keras.layers.Conv2D(16, 5, padding = 'same', activation='elu') (x)
model = keras.layers.Flatten() (model)
model = keras.layers.Dense(128, activation='relu') (model)
predictions = keras.layers.Dense(1) (model)

with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    y = sess.run(predictions, feed_dict={x: test})

    # Get gradients of y w.r.t model parameters.
    gradients = sess.run(tf.gradients(y, model_parameters))
</code></pre>

<p>I have looked at the documentation of <code>tf.gradients()</code> and it states</p>

<blockquote>
  <p><code>ys</code> and <code>xs</code> are each a <code>Tensor</code> or a list of tensors. <code>grad_ys</code> is a list of <code>Tensor</code>, holding the gradients received by the <code>ys</code>. The list must be the same length as <code>ys</code>.</p>
</blockquote>

<p>So I do understand that both args need to be a tensor. However, when I try </p>

<p><code>model_parameters = tf.trainable_variables()</code></p>

<p><code>model_parameters</code> is a list of elements of type <code>tensorflow.python.ops.variables.Variable</code></p>

<p>Is there a way to get the parameters of the model as a tensor to use for differentiation?</p>
",2017-07-21 23:01:51,"<p>Okay, so I figured it out.  If I want to compute the gradients of the output with respect to the variables of the network it goes like this.</p>

<pre><code>import keras
import tensorflow as tf

# Dummy input
test = np.random.rand(1, 32, 32, 1)

x = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))

model = keras.layers.Conv2D(16, 5, padding = 'same', activation='elu') (x)
model = keras.layers.Flatten() (model)
model = keras.layers.Dense(128, activation='relu') (model)
predictions = keras.layers.Dense(1) (model)

# This was the part that I was missing.
============================================================
opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)
gradient_step = opt.compute_gradients(predictions, tf.trainable_variables())
============================================================

with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)

    # This part changed too.
    ==========================================================
    gradients = sess.run(gradient_step, feed_dict={x: test})
    ==========================================================
</code></pre>

<p>I had to define an optimizer <code>tf.train.GradientDescentOptimizer</code> and then feed the <code>predictions</code> to the <code>gradient_step</code> operation to find the gradient for my output. It was actually pretty simple!  </p>

<p>Thank you all for your help ^.^ </p>
",7476324,161,https://stackoverflow.com/questions/45247909,Documentation Replicability
45347275,What is the difference between tf.gradients and tf.train.Optimizer.compute_gradient?,"<p>It seems that <code>tf.gradients</code> allows to compute also Jacobians, i.e. the partial derivatives of each entry of one tensor wrt. each entry of another tensor, while <code>tf.train.Optimizer.compute_gradient</code> only computes actual gradients, e.g. the partial derivatives of a scalar value wrt. each entry of a particular tensor or wrt. one particular scalar. Why is there a separate function if <code>tf.gradients</code> also implements that functionality?</p>
",2017-07-27 09:53:36,"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/gradients"" rel=""noreferrer""><code>tf.gradients</code></a> does not allow you to compute Jacobians, it aggregates the gradients of each input for every output (something like the summation of each column of the actual Jacobian matrix). In fact, there is no ""good"" way of computing Jacobians in TensorFlow (basically you have to call <code>tf.gradients</code> once per output, <a href=""https://github.com/tensorflow/tensorflow/issues/675"" rel=""noreferrer"">see this issue</a>).</p>

<p>With respect to <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Optimizer#compute_gradients"" rel=""noreferrer""><code>tf.train.Optimizer.compute_gradients</code></a>, yes, its result is basically the same, but taking care of some details automatically and with slightly more convenient output format. If you look at <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py"" rel=""noreferrer"">the implementation</a>, you will see that, at its core, is a call to <code>tf.gradients</code> (in this case aliased to <code>gradients.gradients</code>), but it is useful for optimizer implementations to have the surrounding logic already implemented. Also, having it as a method allows for extensible behaviour in subclasses, either to implement some kind of optimization strategy (not very likely at the <code>compute_gradients</code> step, really) or for auxiliary purposes, like tracing or debugging.</p>
",852592,6051,https://stackoverflow.com/questions/45347275,Documentation Replication on Other Examples
45373740,Tensorflow ReLU normalizes strangely,"<p>in my opinion the rectified linear unit is supposed to execute the following function:</p>

<pre><code>relu(x) = max(x, 0)
</code></pre>

<p>However, this seems not to be the case with <code>tf.nn.relu</code>:</p>

<pre><code>import tensorflow as tf
import numpy as np
rand_large = np.random.randn(10, 3)*100
X = tf.placeholder(tf.float32, [10, 3])
sess = tf.Session()
sess.run(tf.nn.relu(X), feed_dict={X:rand_large})
</code></pre>

<p>The random matrix looks like this:</p>

<pre><code>&gt;&gt;&gt; rand_large
array([[  21.94064161,  -82.16632876,   16.25152777],
   [  55.54897693,  -93.15235155,  118.99166126],
   [ -13.36452239,   39.36508285,   65.42844521],
   [-193.34041145,  -97.08632376,   99.22162259],
   [  87.02924619,    2.04134891,  -27.29975745],
   [-181.11406687,   43.55952393,   42.29312993],
   [ -29.81242188,   93.5764354 , -165.62711447],
   [  17.78380711, -171.30536766, -197.20709038],
   [ 105.94903623,   34.07995616,   -7.27568839],
   [-100.59533697, -189.88957685,   -7.52421816]])
</code></pre>

<p>And the output from the relu function like this:</p>

<pre><code>&gt;&gt;&gt; sess.run(tf.nn.relu(X), feed_dict={X:rand_large})array([[ 1. ,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5]], dtype=float32)
</code></pre>

<p><strong>So, if I see it correctly, <code>tf.nn.relu</code> does some sort of normalization, right? If yes, why isn't it mentioned in the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/relu"" rel=""nofollow noreferrer"">docs</a>?</strong></p>

<p>Okay, I found out that the whole issue was related to my tensorflow installtion which seemed to be corrupt. On another machine, I did get the expected results.
Thank you for the help and helpful comments.</p>
",2017-07-28 12:36:46,"<p><code>tf.nn.relu</code> does not normalize the data. For example, if I run</p>

<pre><code>import tensorflow as tf
import numpy as np
X = tf.placeholder(tf.float32, [2, 3])
relu_X=tf.nn.relu(X)

sess = tf.Session()
mat = np.array([[-1,2,3],[2,-5,1]])
sess.run(relu_X, feed_dict={X:mat})
</code></pre>

<p>the result is</p>

<pre><code>array([[ 0.,  2.,  3.],
       [ 2.,  0.,  1.]], dtype=float32)
</code></pre>
",5631237,270,https://stackoverflow.com/questions/45373740,Lack of Alternative Solutions/Documentation
45077445,How to use method recover_last_checkpoints of tf.train.Saver()?,"<p>The documentation writes that a list of checkpoint paths should be passed to it, but how to get the list? By hard coding? No, it's a silly practice. By parsing the protocol buffer file (a file named as <code>checkpoint</code> in your model directory)? But tensorflow does not implement a parser, does it? So do I have to implement one by myself? <strong>Do you have a good practice to get the checkpoint paths list?</strong></p>

<p>I raise this question because these days I am troubled by one thing. As you know, a days-long training may crash for some reason, and I have to recover it from the latest checkpoint. Recovering training is easy, since I just need to write the following code:</p>

<pre><code>restorer = tf.train.Saver()
restorer.restore(sess, latest_checkpoint)
</code></pre>

<p>I can hard code <code>latest_checkpoint</code>, or somewhat wiser, use <code>tf.train.latest_checkpoint()</code>.</p>

<p>However, a problem arises after I recover the training. Those old checkpoints files that are created before crash are left there. The Saver only manages the checkpoint files created in one run. I hope it could also manage the previously created checkpoints files so they would be automatically deleted, and I don't have to manually delete them every time. I think such repeating work is really silly.</p>

<p>Then I find the <code>recover_last_checkpoints</code> method in class <code>tf.train.Saver()</code>, which allows Saver to manage old checkpoints. But it's not handy to use. So is there any good solution?</p>
",2017-07-13 10:03:26,"<p>As mentioned by <a href=""https://stackoverflow.com/users/1946465/isarandi"">@isarandi</a> in a comment, the easiest way is to first recover all checkpoint paths using <code>get_checkpoint_state</code> followed by <code>all_model_checkpoint_paths</code>, which is basically an undocumented feature. You can then restore your latest state as such:</p>

<pre><code>states = tf.train.get_checkpoint_state(your_checkpoint_dir)
checkpoint_paths = states.all_model_checkpoint_paths
saver.recover_last_checkpoints(checkpoint_paths)
</code></pre>
",8269216,37,https://stackoverflow.com/questions/45077445,Lack of Alternative Solutions/Documentation
45428557,Tensorflow: How to make return value of tf.unique same size as input,"<p>According to <a href=""https://www.tensorflow.org/api_docs/python/tf/unique"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/unique</a>, <code>tf.unique(x)</code> returns a tuple <code>(y, idx)</code>,  The <strong>shape of y is (?, )</strong> is not known during build time. Is there anyway I can pad <code>y</code> to match the input size <code>x</code>?.</p>

<p>For example, </p>

<pre><code># tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx = unique(x)
y ==&gt; [1, 2, 4, 7, 8]
idx ==&gt; [0, 0, 1, 2, 2, 2, 3, 4, 4]
</code></pre>

<p>I wanna make y = [1, 2, 4, 7, 8, 0, 0, 0, 0]</p>
",2017-08-01 03:37:18,"<p>You can use <code>tf.pad</code> to pad the zeros after calling <code>tf.unique</code>.</p>

<pre><code>x = tf.placeholder(tf.int32, shape=(None))
y, idx = tf.unique(x)
y = tf.pad(y,[[0,(tf.shape(x) - tf.shape(y))[0]]])

sess = tf.InteractiveSession()
print(sess.run(y, {x:np.random.randint(0,10, (10), dtype=np.int32)}))
</code></pre>
",7765065,93,https://stackoverflow.com/questions/45428557,Inadequate Examples
45549251,Tensorflow Dataset API doubles graph protobuff filesize,"<p><strong>Summary</strong>: Using the new tf.contrib.data.Dataset doubles the size of my graph protobuff file and I'm unable to visualize the graph in Tensorboard.</p>

<p><strong>The details:</strong></p>

<p>I'm trying out the new TensorFlow <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md"" rel=""nofollow noreferrer""><code>tf.contrib.data.Dataset</code></a> functionality together with the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"" rel=""nofollow noreferrer""><code>tf.contrib.learn.Experiment</code></a> framework. My input data is defined as <a href=""https://www.tensorflow.org/get_started/input_fn"" rel=""nofollow noreferrer"">input functions</a> which return tensors of features and labels.</p>

<p>If I create my input function with the <a href=""https://www.tensorflow.org/api_docs/python/tf/train/slice_input_producer"" rel=""nofollow noreferrer""><code>tf.train.slice_input_producer</code></a> function like in the following codeblock (full code <a href=""https://gist.github.com/peterroelants/bbb67fb4e7dad69c79102fec90737a53#file-mnist_old_data-py"" rel=""nofollow noreferrer"">here</a>), then my resulting <code>graph.pbtxt</code> file is 620M and the <code>.meta</code> files are around 165M in size.</p>

<pre><code>def train_inputs():
    with tf.name_scope('Training_data'):
        x = tf.constant(mnist.train.images.reshape([-1, 28, 28, 1]))
        y = tf.constant(mnist.train.labels)
        sliced_input = tf.train.slice_input_producer(
            tensor_list=[x, y], shuffle=True)
        return tf.train.shuffle_batch(
            sliced_input, batch_size=batch_size,
            capacity=10000, min_after_dequeue=batch_size*10)
</code></pre>

<p>Now if I create my input function with the new <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset#from_tensor_slices"" rel=""nofollow noreferrer""><code>tf.contrib.data.Dataset.from_tensor_slices</code></a> like in the following codeblock (full code <a href=""https://gist.github.com/peterroelants/bbb67fb4e7dad69c79102fec90737a53#file-mnist_new_dataset-py"" rel=""nofollow noreferrer"">here</a>), then my resulting <code>graph.pbtxt</code> file doubles in size to 1.3G and the <code>.meta</code> files double in size to 330M. </p>

<pre><code>def train_inputs():
    with tf.name_scope('Training_data'):
        images = mnist.train.images.reshape([-1, 28, 28, 1])
        labels = mnist.train.labels
        dataset = tf.contrib.data.Dataset.from_tensor_slices(
            (images, labels))
        dataset = dataset.repeat(None)  # Infinite
        dataset = dataset.shuffle(buffer_size=10000)
        dataset = dataset.batch(batch_size)
        iterator = dataset.make_one_shot_iterator()
        next_example, next_label = iterator.get_next()
        return next_example, next_label
</code></pre>

<p>Now because the <code>graph.pbtxt</code> file is so big TensorBoard takes ages to parse this file, and I'm unable to debug my model graph visually.
I found in the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md"" rel=""nofollow noreferrer"">Dataset documentation</a> that this increase in size comes from: <em>""the contents of the array will be copied multiple times""</em> and the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/datasets.md#consuming-numpy-arrays"" rel=""nofollow noreferrer"">solution</a> would be to use placeholders. However, in this case, I would need to feed in the numpy arrays into the placeholders with an active session to initialize the iterator:</p>

<pre><code>sess.run(iterator.initializer, feed_dict={features_placeholder: features, labels_placeholder: labels})
</code></pre>

<p>This seems, however, to be out of my control when using the <code>tf.contrib.learn.Experiment</code> framework.</p>

<p>How can I initialize the iterator's initialiser with the Experiment framework? Or find a workaround to using the Dataset API without increasing my graph size?</p>
",2017-08-07 14:22:40,"<p>I found a solution to my problem using <a href=""https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook"" rel=""nofollow noreferrer""><code>tf.train.SessionRunHook</code></a>. I create a <code>SessionRunHook</code> object that initialises the iterator after the session is created:</p>

<pre><code>class IteratorInitializerHook(tf.train.SessionRunHook):
    def __init__(self):
        super(IteratorInitializerHook, self).__init__()
        self.iterator_initiliser_func = None

    def after_create_session(self, session, coord):
        self.iterator_initiliser_func(session)
</code></pre>

<p>The initializer function is set when creating the Dataset Iterator:</p>

<pre><code>iterator_initiliser_hook.iterator_initiliser_func = \
    lambda sess: sess.run(
        iterator.initializer,
        feed_dict={images_placeholder: images,
                   labels_placeholder: labels})
</code></pre>

<p>And I pass in the hook objects to <code>train_monitors</code> and <code>eval_hooks</code> parameters of <code>tf.contrib.learn.Experiment</code>.</p>

<p>The resulting <code>graph.pbtxt</code> file is now only 500K while the <code>.meta</code> files are only 244K.</p>

<p><a href=""https://gist.github.com/peterroelants/6a7b3cc802f7f855744e3a74a1fab354#file-mnist_new_dataset_hook-py"" rel=""nofollow noreferrer""><strong>Full example here.</strong></a></p>
",919431,776,https://stackoverflow.com/questions/45549251,Documentation Replication on Other Examples
45595419,Is it possible to have multiple conditions defined in tf.while_loop,"<p>Is it possible to define to multiple conditions for termination of a tf.while_loop in tensorflow? For example depending on the two tensor values achieving two specific values. eg. <code>i==2</code> and <code>j==3</code> ?</p>

<p>Also can I have several blocks of code in the body? In all the examples in the documentation, it seems that the body is more like a single statement returning a value or a tuple. I want to execute a set of several ""<strong>sequential</strong>"" statements in the body.</p>
",2017-08-09 15:53:32,"<p><code>tf.while_loop</code> accepts a generic callable (python functions defined with <code>def</code>) or lambdas) that must return a boolean tensor.</p>

<p>You can, therefore, chain multiple conditions within the body of the condition using the <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/logical_operators"" rel=""nofollow noreferrer"">logical operators</a>, like <code>tf.logical_and</code>, <code>tf.logical_or</code>, ...</p>

<p>Even <code>body</code> is a general python callable, thus you're not limited to lambdas and single statement functions.</p>

<p>Something like that is perfectly acceptable and works well:</p>

<pre><code>import tensorflow as tf
import numpy as np


def body(x):
    a = tf.random_uniform(shape=[2, 2], dtype=tf.int32, maxval=100)
    b = tf.constant(np.array([[1, 2], [3, 4]]), dtype=tf.int32)
    c = a + b
    return tf.nn.relu(x + c)


def condition(x):
    x = tf.Print(x, [x])
    return tf.logical_or(tf.less(tf.reduce_sum(x), 1000), tf.equal(x[0, 0], 15))


x = tf.Variable(tf.constant(0, shape=[2, 2]))
result = tf.while_loop(condition, body, [x])

init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    print(sess.run(result))
</code></pre>
",8348464,305,https://stackoverflow.com/questions/45595419,Inadequate Examples
45705070,how to load and use a saved model on tensorflow?,"<p>I have found 2 ways to save a model in Tensorflow: <code>tf.train.Saver()</code> and <code>SavedModelBuilder</code>. However, <strong>I can't find documentation on using the model</strong> after it being loaded  the second way.</p>

<p>Note: I want to use <code>SavedModelBuilder</code> way because I train the model in Python and will use it at serving time in another language (Go), and it seems that <code>SavedModelBuilder</code> is the only way in that case.</p>

<p>This works great with <code>tf.train.Saver()</code> (first way):</p>

<pre><code>model = tf.add(W * x, b, name=""finalnode"")

# save
saver = tf.train.Saver()
saver.save(sess, ""/tmp/model"")

# load
saver.restore(sess, ""/tmp/model"")

# IMPORTANT PART: REALLY USING THE MODEL AFTER LOADING IT
# I CAN'T FIND AN EQUIVALENT OF THIS PART IN THE OTHER WAY.

model = graph.get_tensor_by_name(""finalnode:0"")
sess.run(model, {x: [5, 6, 7]})
</code></pre>

<p><code>tf.saved_model.builder.SavedModelBuilder()</code> is defined in the <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/saved_model/"" rel=""noreferrer"">Readme</a>  but after loading the model with <code>tf.saved_model.loader.load(sess, [], export_dir)</code>), I can't find documentation on getting back at the nodes (see <code>""finalnode""</code> in the code above)</p>
",2017-08-16 04:21:25,"<p>What was missing was the <code>signature</code></p>

<pre><code># Saving
builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
builder.add_meta_graph_and_variables(sess, [""tag""], signature_def_map= {
        ""model"": tf.saved_model.signature_def_utils.predict_signature_def(
            inputs= {""x"": x},
            outputs= {""finalnode"": model})
        })
builder.save()

# loading
with tf.Session(graph=tf.Graph()) as sess:
    tf.saved_model.loader.load(sess, [""tag""], export_dir)
    graph = tf.get_default_graph()
    x = graph.get_tensor_by_name(""x:0"")
    model = graph.get_tensor_by_name(""finalnode:0"")
    print(sess.run(model, {x: [5, 6, 7, 8]}))
</code></pre>
",2210667,8590,https://stackoverflow.com/questions/45705070,Lack of Alternative Solutions/Documentation
46315774,Why should one use pure TensorFlow over Keras?,"<p>Recently I was watching an introduction to TensorFlow and during that course Keras came up as an High Level API that could use either TensorFlow or Theano in the back end. </p>

<p>I played around with both of them and did a MNIST LeNet-5 implementation with first pure TensorFlow and after that with Keras. At least on my Machine I wasn't able to see any significant boost in performance, when using TensorFlow directly.</p>

<p>So my question now is: What are the advantages, of using TensorFlow directly over Keras? Maybe it's a matter of scale, will I see significant boost in perfomance on a larger scale project?</p>
",2017-09-20 07:28:31,"<p>Keras just hides the complexity of Tensorflow (and other backends), that's all.</p>

<p>If you need/want a better control on what's going on under the hood, you could use Tensorflow directly (you can control almost everything and you can be sure that your implementation effectively does what you want).</p>

<p>If you just want to fast prototype and develop your models, then Keras is fine because it allows to do it faster. The trade-off is that you loose a bit of control because Keras does a lot of things for you that you wouldn't. For instance, you can't change the variable name/scope of your convolutional layers because they are fixed within the keras-layer definition (the kernel will always be 'kernel' the bias will always be 'bias').</p>

<p>However, since Tensorflow first builds a graph and then executes it, the execution speed of the same graph defined in Keras or in Tensorflow is the same (Keras slows down only the graph construction, that's just Python code).</p>

<p>Update: from Tensorflow 2.0 the default will be to use <code>tf.keras</code>: <a href=""https://pgaleone.eu/tensorflow/gan/2018/11/04/tensorflow-2-models-migration-and-new-design/"" rel=""nofollow noreferrer"">https://pgaleone.eu/tensorflow/gan/2018/11/04/tensorflow-2-models-migration-and-new-design/</a></p>
",3874585,115,https://stackoverflow.com/questions/46315774,Documentation Ambiguity
45886201,Tensorflow: Can't use tf.case with input argument,"<p>I need to create a variable <code>epsilon_n</code> that changes definition (and value) based on the current <code>step</code>. Since I have more than two cases, it seems that I can't use <code>tf.cond</code> . I am trying to use <code>tf.case</code> as follows:</p>

<pre><code>import tensorflow as tf

####
EPSILON_DELTA_PHASE1 = 33e-4
EPSILON_DELTA_PHASE2 = 2.5
####
step = tf.placeholder(dtype=tf.float32, shape=None)


def fn1(step):
    return tf.constant([1.])

def fn2(step):
    return tf.constant([1.+step*EPSILON_DELTA_PHASE1])

def fn3(step):
    return tf.constant([1.+step*EPSILON_DELTA_PHASE2])

epsilon_n = tf.case(
        pred_fn_pairs=[
            (tf.less(step, 3e4), lambda step: fn1(step)),
            (tf.less(step, 6e4), lambda step: fn2(step)),
            (tf.less(step, 1e5), lambda step: fn3(step))],
            default=lambda: tf.constant([1e5]),
        exclusive=False)
</code></pre>

<p>However, I keep getting this error message: </p>

<pre><code>TypeError: &lt;lambda&gt;() missing 1 required positional argument: 'step'
</code></pre>

<p>I tried the following:</p>

<pre><code>epsilon_n = tf.case(
        pred_fn_pairs=[
            (tf.less(step, 3e4), fn1),
            (tf.less(step, 6e4), fn2),
            (tf.less(step, 1e5), fn3)],
            default=lambda: tf.constant([1e5]),
        exclusive=False)
</code></pre>

<p>Still I would the same error. The examples in Tensorflow documentation weigh in on cases where no input argument is passed to the callable functions. I couldn't find enough info about tf.case on the internet! Please any help?</p>
",2017-08-25 16:53:55,"<p>Here's few changes you need to make.
For consistency you can set all return values as variable.</p>

<pre><code># Since step is a scalar, scalar shape [() or [], not None] much be provided 
step = tf.placeholder(dtype=tf.float32, shape=())


def fn1(step):
    return tf.constant([1.])

# Here you need to use Variable not constant, since you are modifying the value using placeholder
def fn2(step):
    return tf.Variable([1.+step*EPSILON_DELTA_PHASE1])

def fn3(step):
    return tf.Variable([1.+step*EPSILON_DELTA_PHASE2])

epsilon_n = tf.case(
    pred_fn_pairs=[
        (tf.less(step, 3e4), lambda : fn1(step)),
        (tf.less(step, 6e4), lambda : fn2(step)),
        (tf.less(step, 1e5), lambda : fn3(step))],
        default=lambda: tf.constant([1e5]),
    exclusive=False)
</code></pre>
",5650892,175,https://stackoverflow.com/questions/45886201,Lack of Alternative Solutions/Documentation
46418686,tf.nn.dynamic_rnn shape error in seq2seq,"<p>I am attempting to write my own basic seq2seq classifier. Im doing this by using <code>tf.nn.dynamic_rnn</code> and the code is shown below. However, there seems to be a problem with the shape of the tensor I'm sending to <code>tf.nn.dynamic_rnn</code>. The reason I'm doing this is because tensorflow's documentation when it comes to seq2seq is very much all over the place.</p>

<p>Running </p>

<pre><code>import numpy as np
source_batch = np.random.randint(x_letters, size=[batch_size, x_seq_length])
target_batch = np.random.randint(y_letters, size=[batch_size, y_seq_length+1])

sess.run(tf.global_variables_initializer())
loss = sess.run([loss],
            feed_dict = {inputs: source_batch, 
                         outputs: target_batch[:, :-1], 
                         targets: target_batch[:, 1:]})
</code></pre>

<p>gives me the error: <code>ValueError: Cannot feed value of shape (128, 10) for Tensor 'decoding/rnn/transpose:0', which has shape '(128, 10, 32)'</code>.</p>

<p><strong>The graph</strong> is shown below:</p>

<pre><code>import tensorflow as tf

x_seq_length = 29
y_seq_length = 10

x_letters = 60
y_letters = 13

epochs = 2
batch_size = 128
nodes = 32
embed_size = 10

####################
# Tensorflow Graph
####################
tf.reset_default_graph()
sess = tf.InteractiveSession()

inputs = tf.placeholder(tf.int32, (batch_size, x_seq_length), 'inputs')
outputs = tf.placeholder(tf.int32, (batch_size, y_seq_length), 'output')
targets = tf.placeholder(tf.int32, (batch_size, y_seq_length), 'targets')

input_embedding = tf.Variable(tf.random_uniform((x_letters, embed_size), -1, 1), name='enc_embedding')
output_embedding = tf.Variable(tf.random_uniform((y_letters, embed_size), -1, 1), name='dec_embedding')

date_input_embed = tf.nn.embedding_lookup(input_embedding, inputs)
date_output_embed = tf.nn.embedding_lookup(output_embedding, outputs)

with tf.variable_scope(""encoding"") as encoding_scope:
    lstm_enc = tf.contrib.rnn.BasicLSTMCell(nodes)
    _, last_state = tf.nn.dynamic_rnn(lstm_enc, dtype=tf.float32,inputs=date_input_embed)

with tf.variable_scope(""decoding"") as decoding_scope:
    lstm_dec = tf.contrib.rnn.BasicLSTMCell(nodes)
    outputs, _ = tf.nn.dynamic_rnn(lstm_dec, inputs=date_output_embed, initial_state=last_state)

logits = tf.contrib.layers.fully_connected(outputs, num_outputs=y_letters, activation_fn=None) 

with tf.name_scope(""optimization""):
    loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, y_seq_length]))
    optimizer = tf.train.AdamOptimizer().minimize(loss)
</code></pre>
",2017-09-26 05:48:29,"<p>You have two variables by the name <code>outputs</code>, i.e., the placeholder and the decoder outputs. Change the name of one variable.</p>
",2530674,9753,https://stackoverflow.com/questions/46418686,Documentation Replicability
46046145,TensorFlow: How to get sub array for each row in tensor,"<p>I have following code:</p>

<pre><code>import numpy as np
import tensorflow as tf

series = tf.placeholder(tf.float32, shape=[None, 5])
series_length = tf.placeholder(tf.int32, shape=[None])
useful_series = tf.magic_slice_function(series, series_length)

with tf.Session() as sess:
    input_x = np.array([[1, 2, 3, 0, 0],
                        [2, 3, 0, 0, 0],
                        [1, 0, 0, 0, 0]])
    input_y = np.array([[3], [2], [1]])
    print(sess.run(useful_series, feed_dict={series: input_x, series_length: input_y}))
</code></pre>

<p>Expected output as following</p>

<p>[[1,2,3],[2,3],[1]]</p>

<p>I have tried several functions, etc tf.gather, tf.slice. All of them do not work.
What is the <strong>magic_slice_function</strong>?</p>
",2017-09-05 02:24:13,"<p>It's a little tricky:</p>

<pre><code>import numpy as np
import tensorflow as tf

series = tf.placeholder(tf.float32, shape=[None, 5])
series_length = tf.placeholder(tf.int64)

def magic_slice_function(input_x, input_y):
    array = []
    for i in range(len(input_x)):
        temp = [input_x[i][j] for j in range(input_y[i])]
        array.extend(temp)
    return [array]

with tf.Session() as sess:
    input_x = np.array([[1, 2, 3, 0, 0],
                        [2, 3, 0, 0, 0],
                        [1, 0, 0, 0, 0]])

    input_y = np.array([3, 2, 1], dtype=np.int64)

    merged_series =  tf.py_func(magic_slice_function, [series, series_length], tf.float32, name='slice_func')

    out = tf.split(merged_series, input_y)
    print(sess.run(out, feed_dict={series: input_x, series_length: input_y}))
</code></pre>

<p>The output will be:</p>

<pre><code>[array([ 1.,  2.,  3.], dtype=float32), array([ 2.,  3.], dtype=float32), array([ 1.], dtype=float32)]
</code></pre>
",1114397,2182,https://stackoverflow.com/questions/46046145,Documentation Replicability
45401311,What are channels in tf.nn.conv2D?,"<p>I've looked through some <a href=""https://stackoverflow.com/questions/34619177/what-does-tf-nn-conv2d-do-in-tensorflow"">great explanations</a> on what different arguments of tf.nn.conv2D represent, but I still can't understand what exactly in_channels and out_channels represent.</p>

<p>Could someone please clarify this for me?</p>
",2017-07-30 15:12:49,"<p>Lets say you have a image of size <code>64x64</code>. It is composed of <code>R-G-B</code> of <code>64x64</code> each, so the input size is <code>64x64x3</code> and <code>3</code> is the input channel in this case. Now you want to convolve this input with a <code>kernel</code> of <code>5x5x3</code>, you get an output of <code>64x64x1</code> (with padding). Suppose you have <code>100</code> such kernels and convolve each one of them with the input, you get <code>64x64x100</code>. Here the output channels are <code>100</code>.</p>
",4588128,2904,https://stackoverflow.com/questions/45401311,Documentation Replicability
45517940,What's the difference between tf.cond and if-else?,"<p>What difference between tf.cond and if-else?</p>

<h2>Scenario 1</h2>

<pre><code>import tensorflow as tf

x = 'x'
y = tf.cond(tf.equal(x, 'x'), lambda: 1, lambda: 0)
with tf.Session() as sess:
    print(sess.run(y))
x = 'y'
with tf.Session() as sess:
    print(sess.run(y))
</code></pre>

<h2>Scenario 2</h2>

<pre><code>import tensorflow as tf

x = tf.Variable('x')
y = tf.cond(tf.equal(x, 'x'), lambda: 1, lambda: 0)
init = tf.global_variables_initializer()
with tf.Session() as sess:
    init.run()
    print(sess.run(y))

tf.assign(x, 'y')
with tf.Session() as sess:
    init.run()
    print(sess.run(y))
</code></pre>

<p>The outputs are both <code>1</code>.</p>

<p>Does it mean only tf.placeholder can work, and not all the tensor, such as tf.variable? When should I choose if-else condition and when to use tf.cond? What are the diffences between them?</p>
",2017-08-05 03:41:15,"<p><code>tf.cond</code> is evaluated at the runtime, whereas <code>if-else</code> is evaluated at the graph construction time. </p>

<p>If you want to evaluate your condition depending on the value of the tensor at the runtime, <code>tf.cond</code> is the best option.</p>
",8165066,1227,https://stackoverflow.com/questions/45517940,Documentation Replicability
47167409,Using weights initializer with tf.nn.conv2d,"<p>When using <code>tf.layers.conv2d</code>, setting the initializer is easy, it can be done through its parameter. But what if I use <code>tf.nn.conv2d</code>? I use this code. Is this equivalent to setting the <code>kernel_initializer</code> parameter in <code>tf.layers.conv2d</code>? Although the program runs without errors, I don't know how to verify whether it does what it is expected do.</p>

<pre><code> with tf.name_scope('conv1_2') as scope:
        kernel = tf.get_variable(initializer=tf.contrib.layers.xavier_initializer(), 
                                 shape=[3, 3, 32, 32], name='weights')
        conv = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[32], dtype=tf.float32),
                             trainable=True, name='biases')
        out = tf.nn.bias_add(conv, biases)
        self.conv1_2 = tf.nn.relu(out, name=scope)
        self.parameters += [kernel, biases]
</code></pre>
",2017-11-07 21:03:46,"<p>The operation underneath is the same (see <a href=""https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d"">here</a>).</p>

<p>As for the kernel and its initialization, I took a glimpse in the code and it <em>looked</em> the same... the <code>layers.conv2d</code> call a <code>tf.get_variable</code> at the end of the day.</p>

<p>But I wanted to see it empirically, so here is a test code that declares a conv2d using each method (<code>tf.layers.conv2d</code> and <code>tf.nn.conv2d</code>), evaluates the initialized kernels and compares them.</p>

<p>I've arbitrarily set the things that shouldn't interfere in the comparison, such as an input tensor and the strides.</p>

<pre><code>import tensorflow as tf
import numpy as np


# the way you described in your question
def _nn(input_tensor, initializer, filters, size):
    kernel = tf.get_variable(
        initializer=initializer, 
        shape=[size, size, 32, filters],
        name='kernel')

    conv = tf.nn.conv2d(
        input=input_tensor,
        filter=kernel,
        strides=[1, 1, 1, 1],
        padding='SAME')

    return kernel

# the other way
def _layer(input_tensor, initializer, filters, size):
    tf.layers.conv2d(
        inputs=input_tensor,
        filters=filters,
        kernel_size=size,
        kernel_initializer=initializer)

    # 'conv2d/kernel:0' is the name of the generated kernel
    return tf.get_default_graph().get_tensor_by_name('conv2d/kernel:0')

def _get_kernel(method):
    # an isolated context for each conv2d
    graph = tf.Graph()
    sess = tf.Session(graph=graph)

    with graph.as_default(), sess.as_default():
        # important so that same randomness doesnt play a role
        tf.set_random_seed(42)

        # arbitrary input tensor with compatible shape
        input_tensor = tf.constant(1.0, shape=[1, 64, 64, 32])

        initializer = tf.contrib.layers.xavier_initializer()

        kernel = method(
            input_tensor=input_tensor,
            initializer=initializer,
            filters=32,
            size=3)

        sess.run(tf.global_variables_initializer())
        return sess.run(kernel)

if __name__ == '__main__':
    kernel_nn = _get_kernel(_nn)
    kernel_layer = _get_kernel(_layer)

    print('kernels are ', end='')
    # compares shape and values
    if np.array_equal(kernel_layer, kernel_nn):
        print('exactly the same')
    else:
        print('not the same!')
</code></pre>

<p>And the output is... <strong>kernels are exactly the same</strong>.</p>

<p>The docs, btw: <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""nofollow noreferrer"">tf.nn.conv2d</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/conv2d"" rel=""nofollow noreferrer"">tf.layers.conv2d</a>.</p>
",6204637,1999,https://stackoverflow.com/questions/47167409,Documentation Replicability
46139202,Tensorflow: TypeError with numpy_input_fn,"<p>I am coding a Convolutional Neural Network to classify images in TensorFlow but there is a problem:</p>

<p>When I try to feed my NumPy array of flattened  images (3 channels with RGB values from 0 to 255) to a tf.estimator.inputs.numpy_input_fn I get the following error:</p>

<pre><code>  TypeError: Failed to convert object of type &lt;class 'dict'&gt; to Tensor. 
  Contents: {'x': &lt;tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(8, 
  196608) dtype=uint8&gt;}. Consider casting elements to a supported type.
</code></pre>

<p>My numpy_imput_fn looks like this:</p>

<pre><code>train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={'x': train_x},
    y=train_y,
    batch_size=8,
    num_epochs=None,
    shuffle=True)
</code></pre>

<p>In the documentation for the function it is said that x should be a dict of NumPy array:</p>

<blockquote>
  <p>x: dict of numpy array object.</p>
</blockquote>
",2017-09-10 09:29:56,"<p>Nevermind, for those having the same problem I fixed it. In my model function i had:</p>

<pre><code>input_layer = tf.reshape(features, [-1, 256, 256, 1])
</code></pre>

<p>Which raised the type error. To fix it you have to access the 'x' key in the features dictionary:</p>

<pre><code>input_layer = tf.reshape(features['x'], [-1, 256, 256, 1]) 
</code></pre>
",5423940,3,https://stackoverflow.com/questions/46139202,Documentation Replicability
46370159,Outputting batch/epoch training loss during `tf.train.MonitoredTrainingSession`,"<p>I would like to output my loss with <code>MonitoredTrainingSession</code> every epoch or batch.
Ideally I would love to get a flag that the epoch is ended or be able to provide a callback like in keras. I see that I can also do it by manually counting steps, but I want to use the tf functionality, which seems still poorly documented.</p>

<p>From what I could find in their documentation, one can use <code>tf.train.LoggingTensorHook</code> to print the tensors every <code>n</code> steps. </p>

<p>The problem however is that it prints with frequency different from what I request. When I run following with <code>every_n_iter=4</code> I get output every 2nd iteration:</p>

<pre><code>tf.reset_default_graph()
with g.as_default():
    loghook = tf.train.LoggingTensorHook([tf.reduce_mean(loss, name='m_loss')],
                                         every_n_iter=4,
                                         formatter=lambda x: ""LOSS\t%.4f"" % [tt for kk,tt in x.items() if kk.name.startswith('m_loss')][-1]
                                        )
    optimizer = get_optimizer(lr=lr, opt_name = opt_name)
    training_op = optimizer.minimize(loss)
    init_op = tf.global_variables_initializer()
    with tf.Session(graph=g) as sess:    
        sess.run(init_op)
    with tf.train.MonitoredTrainingSession(log_step_count_steps=1, hooks=[loghook]) as sess:
        losslist = []
        while not sess.should_stop():
            print('.')
            loss_ = sess.run(loss, feed_dict={K.learning_phase():1})
            sess.run(training_op)
            losslist.append(np.mean(loss_))
</code></pre>

<p>I am getting output like:</p>

<pre><code>.
INFO:tensorflow:LOSS    2.2416
.
.
INFO:tensorflow:LOSS    2.1547
.
.
INFO:tensorflow:LOSS    2.1186
.
.
</code></pre>

<p>etc. That is it outputs every 2nd step, not every 4th. </p>

<p>The documentation says: </p>

<pre><code>every_n_iter: `int`, print the values of `tensors` once every N local
      steps taken on the current worker.
</code></pre>

<p>I am running it on one local machine. Why one ""local step"" equals two loop python iterations? Why two and not five?</p>

<p>Looking at the Python source does not seem helping. Any Google folks aware of what it is doing?</p>
",2017-09-22 17:26:44,"<p>""local step"" is incremented on every call to <code>sess.run()</code>. You are calling <code>sess.run()</code> twice within your while loop.</p>

<p>Here are some pointers to relevant code:
<a href=""https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/training/basic_session_run_hooks.py#L255"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/training/basic_session_run_hooks.py#L255</a> - increment _iter_count after every call to <code>sess.run()</code>.</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/training/basic_session_run_hooks.py#L228"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/training/basic_session_run_hooks.py#L228</a> - If <code>_iter_count</code> should trigger logging, add the current tensors to be run in the following call to <code>sess.run()</code> so that their values can be logged next.</p>
",1716733,12800,https://stackoverflow.com/questions/46370159,Lack of Alternative Solutions/Documentation
45917464,Tensorflow: What's the difference between tf.nn.dropout and tf.contrib.rnn.DropoutWrapper?,"<p>How's the following codes different?</p>

<ul>
<li><p>with tf.contrib.rnn.DropoutWrapper</p>

<pre><code>enc_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(rnn_sizes, output_keep_prob=1-keep_prob) for _ in range(num_layers)])

_, encoding_state = tf.nn.dynamic_rnn(enc_cell, rnn_inputs, dtype=tf.float32)
</code></pre></li>
<li><p>with tf.nn.droupout</p>

<pre><code>enc_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(rnn_size) for _ in range(num_layers)])

_, encoding_state = tf.nn.dynamic_rnn(enc_cell, tf.nn.dropout(rnn_inputs, 1 - keep_prob), dtype=tf.float32)
</code></pre></li>
</ul>

<p>It seems that there is a difference in the number of states we get from tf.nn.dynamic_rnn. len(encoding state) is greater with tf.nn.dropout. </p>

<p>An explanation will be highly appreciated.</p>

<p>Thank you.</p>
",2017-08-28 11:03:01,"<p>The idea behind both is the same and it is dropout: the network ""drops"" (i.e does not use) some of its nodes in the prediction. This means reducing during training the capacity of the model to prevent overfitting. Thanks to dropout, the network learns not to rely exclusively on particular nodes for its prediction.</p>

<p>The difference between the two methods is that:</p>

<ul>
<li><p><code>tf.nn.droput</code>is a generic function to perform droput to a given input tensor. Looking at the documentation:</p>

<blockquote>
  <p>Computes dropout.</p>
  
  <p>With probability <code>keep_prob</code>, outputs the input element scaled up by 1 /
  <code>keep_prob</code>, otherwise outputs 0. The scaling is so that the expected
  sum is unchanged.</p>
</blockquote></li>
<li><p><code>tf.contrib.rnn.DropoutWrapper</code> or <code>tf.nn.rnn_cell.DropoutWrapper</code> is a specific class to define Recurrent Neural Network cells with dropout applied both at the input and the output of the cell. Looking at the documentation: </p>

<blockquote>
  <p>Operator adding dropout to inputs and outputs of the given cell.</p>
</blockquote>

<p>In particular, it <a href=""https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/rnn_cell_impl.py#L770"" rel=""nofollow noreferrer"">uses</a> <code>tf.nn.droput</code> to mask the input to the cell, the state and the output.</p></li>
</ul>

<p>The difference between your two pieces of code is that when you are using <code>tf.nn.dropout</code> you are masking the inputs of the first layer only. In the wrapper case, layer per layer, you are masking the outputs of the cells (since you are providing only the output probabilities )</p>
",8409009,43,https://stackoverflow.com/questions/45917464,Requesting (Additional) Documentation/Examples
47768298,How to understand using tf.cond with tf.Print?,"<p>Look at the code:</p>

<pre><code>import tensorflow as tf

x = tf.constant(1.0)
y = tf.constant(2.0)
z = tf.constant(3.0)
def f1():
    return tf.Print(x, [x])

def f2():
    return tf.Print(z, [z])
op = tf.cond(x&gt;y, f1, f2)
with tf.Session() as sess:
    sess.run(op)
</code></pre>

<p>I'm very puzzled, the output of tf.Print is 3.0</p>

<p>As we know, tf.Print(z, [z]) will output the value of <code>z</code> only when <code>z</code> is evaluated, but I don't think I have evaluated <code>z</code>.</p>

<p>Another question is about <code>tf.cond</code>, how does it add node to graph, for example how does add <code>tf.Print</code> to graph, I think it should relate some tensor with the return of <code>tf.Print</code>, otherwise <code>tf.Print</code> won't be executed.</p>

<p>I'm so puzzled.</p>
",2017-12-12 08:39:07,"<p>I think you might have got the order of the arguments to <code>tf.cond</code> confused. The call:</p>

<pre><code>tf.cond(predicate, f, g)
</code></pre>

<p>is equivalent to ""if <code>predicate</code> is true then evaluate <code>f</code>, otherwise evaluate <code>g</code>""</p>

<p>In your example, since your predicate <code>x &gt; y</code> is false, <code>f2</code> is evaluated</p>

<hr>

<p><strong>Note</strong></p>

<p>Since tensorflow 1.4, <code>tf.cond</code> will accept key-word arguments <code>true_fn</code> and <code>false_fn</code>, so you can avoid any confusion by writing:</p>

<pre><code>tf.cond(predicate, true_fn=f, false_fn=g)

# Or equivalently...
tf.cond(predicate, false_fn=g, true_fn=f)
</code></pre>
",8165066,1227,https://stackoverflow.com/questions/47768298,Documentation Replicability
45955241,How do I create padded batches in Tensorflow for tf.train.SequenceExample data using the DataSet API?,"<p>For training an <strong>LSTM model</strong> in <strong>Tensorflow</strong>, I have structured my data into a <strong>tf.train.SequenceExample</strong> format and stored it into a <strong>TFRecord file</strong>. I would now like to use the new DataSet API to <strong>generate padded batches for training</strong>. In <a href=""https://www.tensorflow.org/programmers_guide/datasets"" rel=""noreferrer"">the documentation</a> there is an example for using padded_batch, but for my data I can't figure out what the value of <em>padded_shapes</em> should be.</p>

<p>For reading the TFrecord file into the batches I have written the following Python code:</p>

<pre><code>import math
import tensorflow as tf
import numpy as np
import struct
import sys
import array

if(len(sys.argv) != 2):
  print ""Usage: createbatches.py [RFRecord file]""
  sys.exit(0)


vectorSize = 40
inFile = sys.argv[1]

def parse_function_dataset(example_proto):
  sequence_features = {
      'inputs': tf.FixedLenSequenceFeature(shape=[vectorSize],
                                           dtype=tf.float32),
      'labels': tf.FixedLenSequenceFeature(shape=[],
                                           dtype=tf.int64)}

  _, sequence = tf.parse_single_sequence_example(example_proto, sequence_features=sequence_features)

  length = tf.shape(sequence['inputs'])[0]
  return sequence['inputs'], sequence['labels']

sess = tf.InteractiveSession()

filenames = tf.placeholder(tf.string, shape=[None])
dataset = tf.contrib.data.TFRecordDataset(filenames)
dataset = dataset.map(parse_function_dataset)
# dataset = dataset.batch(1)
dataset = dataset.padded_batch(4, padded_shapes=[None])
iterator = dataset.make_initializable_iterator()

batch = iterator.get_next()

# Initialize `iterator` with training data.
training_filenames = [inFile]
sess.run(iterator.initializer, feed_dict={filenames: training_filenames})

print(sess.run(batch))
</code></pre>

<p>The code works well if I use <code>dataset = dataset.batch(1)</code> (no padding needed in that case), but when I use the <code>padded_batch</code> variant, I get the following error:</p>

<blockquote>
  <p>TypeError: If shallow structure is a sequence, input must also be a
  sequence. Input has type: .</p>
</blockquote>

<p>Can you help me figuring out what I should pass for the <em>padded_shapes</em> parameter?</p>

<p>(I know there is lots of example code using threading and queues for this, but I'd rather use the new DataSet API for this project)</p>
",2017-08-30 08:12:35,"<p>You need to pass a tuple of shapes.
In your case you should pass </p>

<pre><code>dataset = dataset.padded_batch(4, padded_shapes=([vectorSize],[None]))
</code></pre>

<p>or try </p>

<pre><code>dataset = dataset.padded_batch(4, padded_shapes=([None],[None]))
</code></pre>

<p>Check this <a href=""https://github.com/tensorflow/tensorflow/blob/d236d19f7753ae23f91d794701efa70ace1629da/tensorflow/python/util/nest.py#L427"" rel=""noreferrer"">code</a> for more details. I had to debug this method to figure out why it wasn't working for me.  </p>
",8536501,183,https://stackoverflow.com/questions/45955241,Documentation Replication on Other Examples
46534796,Properly concatenate feature maps in Tensorflow,"<p>I am attempting to reproduce a Convolution Neural Network from a research paper using Tensorflow. <a href=""https://i.stack.imgur.com/1qLP2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1qLP2.png"" alt=""Image of architecture""></a></p>

<p>There are many times in the diagram where the results of convolutions are concatenated. Currently I am using tf.concat(<a href=""https://www.tensorflow.org/api_docs/python/tf/concat"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/concat</a>) along the last axis (representing channels) to concatenate these feature maps. I originally believed that I would want to concatenate along all axes, but this does not seem to be an option in tensorflow. Now I am facing the problem where the paper indicates that tensors(feature maps) of different sizes should be concatenated. tf.concat does not support concatenations of different sizes, so I am wondering if this was the correct command to use in the first place. In summary, <strong>what is the correct way to concatenate feature maps(sometimes of different sizes) in tensorflow?</strong></p>

<p>Thank you.</p>
",2017-10-02 22:48:51,"<p>It's impossible and meaningless to concatenate features maps with different sizes.</p>

<p>If you want to concatenate 2 tensors, every dimension except the concatenation one must be equal.</p>

<p>From the image you posted, in fact, you can see that every feature map that gets concatenated, has the same spatial extent (but different depth) of the other one.
If you can't concatenate in that way, probabily that's something wrong in your code, and probably the problem is the lack of padding = valid in the convolution operation.</p>
",8015346,767,https://stackoverflow.com/questions/46534796,Documentation Replication on Other Examples
46647805,building a tf.estimator input_fn: feature is not in features dictionary,"<p>I have a corpus of records that represent matchups in a video game. I want to feed this to a <code>tf.estimator.DNNClassifier</code>. </p>

<p>The records contain text representations of the 5 heroes on team 0 and the 5 heroes on team 1, the map the game was played on, and the winner of the game. I want to represent these three features as three sparse vectors.</p>

<p>I'm not using pandas or numpy right now. I would prefer to keep it as simple as possible for the time being, until I can elaborate my tf knowledge. (But no simpler!). </p>

<p>Perhaps the best way to ask the question is to show what I have and ask for help filling in the blank, at <code>make_input_fn</code></p>

<pre><code>import tensorflow as tf
import packunpack as source
import tempfile
from collections import namedtuple

GameRecord = namedtuple('GameRecord', 'team_0 team_1 game_map winner')
def parse(line):
    parts = line.rstrip().split(""\t"")
    return GameRecord(
        game_map = parts[1], 
        team_0 = parts[2].split("",""), 
        team_1 = parts[3].split("",""), 
        winner = int(parts[4]))

def conjugate(record):
    return GameRecord(
        team_0 = record.team_1, 
        team_1 = record.team_0, 
        game_map = record.game_map, 
        winner = 0 if record.winner == 1 else 1)

def sparse_team(team):
    return tf.SparseTensor(indices=team, values = [1] * len(team), dense_shape=[len(source.heroes_array)])

def sparse_map(i):
    return tf.SparseTensor(indices=[i], values = [1], dense_shape=[len(source.maps_array)])

def make_input_fn(filename, shuffle = True, add_conjugate_games = True):
    def _fn():
        records = []
        with open(filename, ""r"") as raw:
            i = 0
            for line in raw:
                record = parse(line)
                records.append(record)
                if add_conjugate_games:
                    # the team_0 and team_1 designations are arbitrary, and so the same inference should be drawn from a game and its ""conjugate"" game
                    records.append(conjugate(record))

        team_0s = map(lambda r: sparse_team(r.team_0), records)
        team_1s = map(lambda r: sparse_team(r.team_1), records)
        maps = map(lambda r: sparse_map(r.game_map), records)
        winners = map(lambda r: tf.constant([r.winner]), records)

        return ({
                    team_0: team_0s,
                    team_1: team_1s,
                    game_map: maps,
                }, 
                winners)
        #Please help me finish this function?

    return _fn

team_0 = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(""team_0"", source.heroes_array), 1)
team_1 = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(""team_1"", source.heroes_array), 1)
game_map = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(""game_map"", source.maps_array), 1)

model_dir = tempfile.mkdtemp()
m = tf.estimator.DNNClassifier(
    model_dir=model_dir,
    hidden_units = [1024, 512, 256], 
    feature_columns=[team_0, team_1, game_map])

def main():
    m.train(input_fn=make_input_fn(""validation.txt""))

if __name__ == ""__main__"":
    main()
</code></pre>

<p>I've been all over the docs today, but all of the code examples I can find show how to feed pandas and numpy data structures into input_fn, and obsure the underlying mechanics of the process by calling out to helper functions that don't work for me.</p>

<p>(e.g.,   <a href=""https://www.tensorflow.org/get_started/input_fn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/get_started/input_fn</a>
and 
<a href=""https://www.tensorflow.org/tutorials/wide"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/wide</a>)</p>

<p>tf version 1.4.0-dev20171008</p>

<p>When I run I get this stack trace. I think it doesn't like the return value from <code>_fn</code>. But that dictionary does have the names of the features that I gave to the model AFAICT.</p>

<pre><code> File ""estimator.py"", line 72, in &lt;module&gt;
    main()
  File ""estimator.py"", line 69, in main
    m.train(input_fn=make_input_fn(""validation.txt""))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/canned/dnn.py"", line 334, in _model_fn
    config=config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/canned/dnn.py"", line 190, in _dnn_model_fn
    logits = logit_fn(features=features, mode=mode)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/canned/dnn.py"", line 89, in dnn_logit_fn
    features=features, feature_columns=feature_columns)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 230, in input_lay
er
    trainable=trainable)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 1834, in _get_den
se_tensor
    inputs, weight_collections=weight_collections, trainable=trainable)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 2119, in _get_spa
rse_tensors
    return _CategoricalColumn.IdWeightPair(inputs.get(self), None)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 1533, in get
    transformed = column._transform_feature(self)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 2087, in _transfo
rm_feature
    input_tensor = _to_sparse_input(inputs.get(self.key))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 1529, in get
    raise ValueError('Feature {} is not in features dictionary.'.format(key))
ValueError: Feature team_0 is not in features dictionary.
</code></pre>
",2017-10-09 13:35:10,"<p>I think you should check your data and make sure the field you are missing (team_0) is showing up correctly. It could be many things like ill formed data or the field name might be incorrectly spelt in the training data source.</p>
",86432,9350,https://stackoverflow.com/questions/46647805,Documentation Replicability
46658607,where is tf.nn.l2_loss defined?,"<p>According to this documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss</a> it says</p>

<pre><code>Defined in tensorflow/python/ops/gen_nn_ops.py.
</code></pre>

<p>But when I go to tensorflow/python/ops/gen_nn_ops.py there is no l2_loss defined.</p>

<p>I'm trying to see what would be the difference between using <code>tf.nn.l2_loss(W)</code> or just using <code>tf.reduce_sum(tf.square(W))</code>.</p>
",2017-10-10 04:25:53,"<p><a href=""https://github.com/tensorflow/tensorflow/blob/512d3d0868c5131a9a88ad4af0243051bdaff51c/tensorflow/core/kernels/l2loss_op.cc#L36"" rel=""nofollow noreferrer"">Here</a> is the used CPU implementation of it.</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/d2d42ee8b3e134de4c11055b695c0e16e6186b73/tensorflow/cc/gradients/nn_grad.cc#L98"" rel=""nofollow noreferrer"">Here</a> is the according gradient implementation.</p>
",3907250,21227,https://stackoverflow.com/questions/46658607,Documentation Ambiguity
46062649,tensorflow slim concurrent train and evaluation loops; single device,"<p>I am interested in using the tensorflow slim library (tf.contrib.slim) to do evaluation of a model performance on a(n) (entire) test set periodically during training. The documentation is pretty clear that slim.evaluation.evaluation_loop is the way to go, and it looks promising. The issue is that I don't have a second gpu to spare, this model parameters take up an entire gpu's worth of memory, and I would like to do concurrent evaluation.</p>

<p>For example, if I had 2 GPUs, I could run a python script that terminated with ""slim.learning.train()"" on the first gpu, and another that terminated with ""slim.evaluation.evaluation_loop()"" on the second gpu.</p>

<p>Is there an approach that can manage 1 gpu's resources for both tasks? tf.train.Supervisor comes to mind, but I don't honestly know.</p>
",2017-09-05 20:08:19,"<p>You can partition the GPU usage using the following code.</p>

<p>You can set the fraction of the GPU to be used for training and evaluation separately. The code below means that the process is given 30% of the memory.
<code>gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3000)
  sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
  sess.run(tf.app.run())</code></p>
",3391229,808,https://stackoverflow.com/questions/46062649,Documentation Replication on Other Examples
46904972,How to create a tf.feature_column by multiplying two other tf.feature_columns?,"<p>In Tensorflow there is already a function to create feature by crossing columns <code>tf.feature_column.crossed_column</code> , but it is more for category data. How about numeric data? </p>

<p>For example, there are 2 columns already</p>

<pre><code>age = tf.feature_column.numeric_column(""age"")
education_num = tf.feature_column.numeric_column(""education_num"")
</code></pre>

<p>if i want to create a third and fourth feature columns base on age and education_num like this</p>

<pre><code>my_feature = age * education_num
my_another_feature = age * age
</code></pre>

<p>How can it be done?</p>
",2017-10-24 07:49:12,"<p>You can declare a custom numerical column and add it to the dataframe in your <a href=""https://www.tensorflow.org/get_started/input_fn"" rel=""nofollow noreferrer"">input function</a>:</p>

<pre><code># Existing features
age = tf.feature_column.numeric_column(""age"")
education_num = tf.feature_column.numeric_column(""education_num"")
# Declare a custom column just like other columns
my_feature = tf.feature_column.numeric_column(""my_feature"")

...
# Add to the list of features
feature_columns = { ... age, education_num, my_feature, ... }

...
def input_fn():
  df_data = pd.read_csv(""input.csv"")
  df_data = df_data.dropna(how=""any"", axis=0)
  # Manually update the dataframe
  df_data[""my_feature""] = df_data[""age""] * df_data[""education_num""]

  return tf.estimator.inputs.pandas_input_fn(x=df_data,
                                             y=labels,
                                             batch_size=100,
                                             num_epochs=10)

...
model.train(input_fn=input_fn())
</code></pre>
",4049331,3299,https://stackoverflow.com/questions/46904972,Documentation Replicability
46920307,"Is tf.contrib.layers.flatten(x) the same as tf.reshape(x, [n, 1])?","<p>I was just wondering whether <code>tf.contrib.layers.flatten(x)</code> is  the same as <code>tf.reshape(x, [n, 1])</code>? Anyone has ideas about this?</p>
",2017-10-24 21:26:47,"<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/15bd614fedc20d229f3e97a99e0748136ba4852c/tensorflow/python/layers/core.py#L361"" rel=""nofollow noreferrer"">implementation</a> of <a href=""https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/layers/flatten"" rel=""nofollow noreferrer""><code>tf.layers.flatten()</code></a> reveals the answer. It is equivalent to the following:</p>

<pre><code>flattened = tf.reshape(x, [tf.shape(x)[0], -1])
</code></pre>
",1272683,45989,https://stackoverflow.com/questions/46920307,Documentation Replicability
47205160,Tensorflow v1.4: Layer.input not supported in Eager mode,"<p>I understand that Eager mode is a new alpha feature on the nightly builds and that it is not perfect yet, but I do not know if there are any tf.keras workarounds for this problem.</p>

<p>The error <code>Layer.input not supported in Eager mode.</code> triggers on the block</p>

<pre><code>model = tf.keras.models.Sequential()
model.add(tf.layers.Dense(2, input_shape = (None, 1)))
model.add(tf.layers.Dense(units = 1))
model.compile(optimizer = ""sgd"", loss = ""mean_squared_error"")
</code></pre>

<p>I do not know anything about keras or the keras tensorflow API and I was wondering if there was a way to avoid <code>Layer.input</code> with keras techniques so as to stay within Eager mode. Following a tutorial in the tf.Eager docs I have confirmed that <code>model = tf.layers.Dense(1)</code> works but I don't know how to add another layer.</p>

<p>Any help is very much appreciated.</p>

<p><strong>EDIT</strong>
As of tensorflow v1.10, keras is supported in eager mode.</p>
",2017-11-09 15:05:41,"<p>Keras <code>Model</code>s are not yet supported with eager execution, but Keras layers are. Which means that while you can't use <code>tf.keras.models.Sequential</code> yet, you could combine layers yourself. See <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/g3doc/guide.md#using-keras-and-the-layers-api"" rel=""nofollow noreferrer"">the user guide</a>.</p>

<p>Hope that helps.</p>
",5496253,152,https://stackoverflow.com/questions/47205160,Documentation Replication on Other Examples
47319390,Why does this TensorFlow code behave differently when inside a test case?,"<p>I have a function (<code>foo</code> below) which is behaving differently when it's run directly vs when it is run inside a <code>tf.test.TestCase</code>.</p>

<p>The code is supposed to create a dataset with elems [1..5] and shuffle it. Then it repeats 3 times: create an iterator from the data and use that to print the 5 elements.</p>

<p>When run on its own it gives output where all the lists are shuffled e.g.:</p>

<pre><code>[4, 0, 3, 2, 1]
[0, 2, 1, 3, 4]
[2, 3, 4, 0, 1]
</code></pre>

<p>but when run inside a test case they are always the same, even between runs:</p>

<pre><code>[0, 4, 2, 3, 1]
[0, 4, 2, 3, 1]
[0, 4, 2, 3, 1]
</code></pre>

<p>I imagine it's something to do with how test cases handle random seeds but I can't see anything about that in the TensorFlow docs. Thanks for any help!</p>

<hr>

<h2>Code:</h2>

<pre><code>import tensorflow as tf

def foo():
    sess = tf.Session()
    dataset = tf.data.Dataset.range(5)
    dataset = dataset.shuffle(5, reshuffle_each_iteration=False)

    for _ in range(3):
        data_iter = dataset.make_one_shot_iterator()
        next_item = data_iter.get_next()
        with sess.as_default():
            data_new = [next_item.eval() for _ in range(5)]
        print(data_new)


class DatasetTest(tf.test.TestCase):
    def testDataset(self):
        foo()

if __name__ == '__main__':
    foo()
    tf.test.main()
</code></pre>

<p>I am running it with Python 3.6 and TensorFlow 1.4. No other modules should be needed.</p>
",2017-11-16 00:04:09,"<p>I think you are right; <code>tf.test.TestCase</code> is being <a href=""https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/framework/test_util.py#L242"" rel=""nofollow noreferrer"">setup</a> to use fixed seed.</p>

<pre><code>class TensorFlowTestCase(googletest.TestCase):
# ...
def setUp(self):
  self._ClearCachedSession()
  random.seed(random_seed.DEFAULT_GRAPH_SEED)
  np.random.seed(random_seed.DEFAULT_GRAPH_SEED)
  ops.reset_default_graph()
  ops.get_default_graph().seed = random_seed.DEFAULT_GRAPH_SEED
</code></pre>

<p>and 
<code>DEFAULT_GRAPH_SEED = 87654321</code> see <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/random_seed.py#L27"" rel=""nofollow noreferrer"">this</a> line in <code>tensorflow/tensorflow/python/framework/random_seed.py</code>.</p>
",1015528,149,https://stackoverflow.com/questions/47319390,Documentation Replication on Other Examples
47665314,how can we get benefit from sharding the data to speed the training time?,"<p>My main issue is : I have 204 GB training tfrecords for 2 million images, and 28GB for validation tf.records files, of 302900 images. it takes 8 hour to train one epoch and this will take 33 day for training. I want to speed that by using multiple threads and shards but I am little bit confused about couple of things.</p>

<p>In <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">tf.data.Dataset API</a> there is shard function , So in the documentation they mentioned the following about shard function : </p>

<blockquote>
  <p>Creates a Dataset that includes only 1/num_shards of this dataset.</p>
  
  <p>This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.</p>
  
  <p>When reading a single input file, you can skip elements as follows:</p>
</blockquote>

<pre><code>d = tf.data.TFRecordDataset(FLAGS.input_file)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
d = d.repeat(FLAGS.num_epochs)
d = d.shuffle(FLAGS.shuffle_buffer_size)
d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)
</code></pre>

<blockquote>
  <p>Important caveats:</p>
  
  <p>Be sure to shard before you use any randomizing operator (such as shuffle).
  Generally it is best if the shard operator is used early in the dataset pipeline. >For example, when reading from a set of TFRecord files, shard before converting >the dataset to input samples. This avoids reading every file on every worker. The >following is an example of an efficient sharding strategy within a complete >pipeline:</p>
</blockquote>

<pre><code>d = Dataset.list_files(FLAGS.pattern)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
d = d.repeat(FLAGS.num_epochs)
d = d.shuffle(FLAGS.shuffle_buffer_size)
d = d.repeat()
d = d.interleave(tf.data.TFRecordDataset,
             cycle_length=FLAGS.num_readers, block_length=1)

d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)
</code></pre>

<p>So my question regarding the code above is when I try to makes d.shards of my data using shard function, if I set the number of shards (num_workers)to 10 , I will have 10 splits of my data , then should I set the num_reader in d.interleave function to 10 to guarantee that each reader take one split from the 10 split? </p>

<p>and how I can control which split the function interleave will take? because if I set the shard_index (worker_index) in shard function to 1 it will give me the first split. Can anyone give me an idea how can I perform this distributed training using the above functions? </p>

<p>then what about the num_parallel_call . should I set it to 10 as well? </p>

<p>knowing that I have single tf.records file for training and another one for validation , I don't split the tf.records files into multiple files.</p>
",2017-12-06 01:16:41,"<p>First of all, how come dataset is 204GB for only 2million images? I think your image is way too large. Try to resize the image. After all, you would probably need to resize it to 224 x 224 in the end.</p>

<p>Second, try to reduce the size of your model. your model could be either too deep or not efficient enough.</p>

<p>Third, try to parallelize your input reading process. It could the bottleneck.</p>
",8262057,818,https://stackoverflow.com/questions/47665314,Documentation Replication on Other Examples
47898147,Tensorflow Module Import error: AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell',"<p>When attempting to pass my RNN call, I call tf.nn.rnn_cell and I receive the following error: </p>

<pre><code>AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'
</code></pre>

<p>Which is odd, because I'm sure I imported everything correctly: </p>

<pre><code>from __future__ import print_function, division
from tensorflow.contrib import rnn
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
</code></pre>

<p>But looking at the docs, things have moved around between tensorflow versions. </p>

<p>what would you all recommend to fix this?? </p>

<p>Line, I'm getting the error against: </p>

<pre><code>state_per_layer_list = tf.unstack(init_state, axis=0)
rnn_tuple_state = tuple(
    [tf.nn.rnn_cell.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])
     for idx in range(num_layers)]
)
</code></pre>

<p>Specifically: </p>

<pre><code>tf.nn.rnn_cell
</code></pre>

<p>I'm using anaconda 3 to manage all of this so, the dependancies should all be taken care of. I have already tried working around a damn rank/shape error with Tensor shapes which took ages to resolve. </p>

<p>Cheers in advance. </p>
",2017-12-20 02:56:05,"<p>Replace <code>tf.nn.rnn_cell</code> with <code>tf.contrib.rnn</code></p>

<p>Since version 1.0, <code>rnn</code> implemented as part of the contrib module.</p>

<p>More information can be found here
<a href=""https://www.tensorflow.org/api_guides/python/contrib.rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_guides/python/contrib.rnn</a></p>
",5552621,117,https://stackoverflow.com/questions/47898147,Documentation Replication on Other Examples
47947629,"Tensorflow: Keras, Estimators and custom input function","<p>TF1.4 made Keras an integral part. 
When trying to create Estimators from Keras models with propratery input function (I.e., not using the tf.estimator.inputs.numpy_input_fn) things are not working as Tensorflow can not fuse the model with the Input function.</p>

<p>I am using tf.keras.estimator.model_to_estimator</p>

<pre><code>keras_estimator = tf.keras.estimator.model_to_estimator(
            keras_model = keras_model,
            config = run_config)

train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, 
                                    max_steps=self.train_steps)
eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn,
                                  steps=None)

tf.estimator.train_and_evaluate(keras_estimator, train_spec, eval_spec)
</code></pre>

<p>and I get the following error message:</p>

<pre><code>    Cannot find %s with name ""%s"" in Keras Model. It needs to match '
              'one of the following:
</code></pre>

<p>I found some reference for this topic <a href=""https://www.tensorflow.org/versions/master/programmers_guide/estimators"" rel=""nofollow noreferrer"">here</a> (strangely enough its hidden in the TF docs in the master branch - compare to <a href=""https://www.tensorflow.org/programmers_guide/estimators"" rel=""nofollow noreferrer"">this</a>)</p>

<p>If you have the same issue - see my answer below. Might save you several hours. </p>
",2017-12-22 21:32:10,"<p>So here is the deal. You must make sure that your custom Input Function returns a dictionary of {inputs} and a dictionary of {outputs}.
The dictionary keys must match your Keras input/output layers name.</p>

<p>From TF docs:</p>

<p><em>First, recover the input name(s) of Keras model, so we can use them as the
feature column name(s) of the Estimator input function</em></p>

<p>This is correct. 
Here is how I did this:</p>

<pre><code># Get inputs and outout Keras model name to fuse them into the infrastructure.
keras_input_names_list = keras_model.input_names
keras_target_names_list = keras_model.output_names
</code></pre>

<p>Now, that you have the names, you need to go to your own input function and change it so it will deliver two dictionaries with the corresponding input and output names. </p>

<p>In my example, before the change, the input function returned [image_batch],[label_batch]. This is basically a bug because it is stated that the inputfn returns a dictionary and not a list. </p>

<p>To solve this, we need to wrap it up into a dict:</p>

<pre><code>image_batch_dict = dict(zip(keras_input_names_list , [image_batch]))
label_batch_dict = dict(zip(keras_target_names_list , [label_batch]))
</code></pre>

<p>Only now, TF will be able to connect the input function to the Keras input layers. </p>
",8096451,546,https://stackoverflow.com/questions/47947629,Documentation Replicability
47327256,Understanding Tensor Inputs & Transformations for use in an LSTM (dynamic RNN),"<p>I am building an LSTM style neural network in Tensorflow and am having some difficulty understanding exactly what input is needed and the subsequent transformations made by <em>tf.nn.dynamic_rnn</em> before it is passed to the <em>sparse_softmax_cross_entropy_with_logits</em> layer.</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a></p>

<h2>Understanding the input</h2>

<p>The input function is sending a feature tensor in the form </p>

<p><strong>[batch_size, max_time]</strong></p>

<p>However the manual states that input tensors must be in the form</p>

<p><strong>[batch_size, max_time, ...]</strong></p>

<p>I have therefore expanded the input with a 1d tensor to take the form </p>

<p><strong>[batch_size, max_time, 1]</strong></p>

<p>At this point the input does not break upon running, but I don't understand exactly what we have done here and suspect it may be causing the problems when calculating loss (see below).</p>

<h2>Understanding the Transformations</h2>

<p>This expanded tensor is then the 'features' tensor used in the code below</p>

<pre><code>LSTM_SIZE = 3
lstm_cell = rnn.BasicLSTMCell(LSTM_SIZE, forget_bias=1.0)
outputs, _ = tf.nn.dynamic_rnn(lstm_cell, features, dtype=tf.float64)

#slice to keep only the last cell of the RNN
outputs = outputs[-1]

#softmax layer

with tf.variable_scope('softmax'):
   W = tf.get_variable('W', [LSTM_SIZE, n_classes], dtype=tf.float64)
   b = tf.get_variable('b', [n_classes], initializer=tf.constant_initializer(0.0), dtype=tf.float64)

logits = tf.matmul(outputs, W) + b

loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))
</code></pre>

<p>This throws a value error at <em>loss</em></p>

<p>dimensions must be equal, but are <strong>[max_time, num_classes] and [batch_size]</strong></p>

<p>from <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/classification"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/classification</a> -</p>

<p><em>A common use case is to have logits of shape [batch_size, num_classes] and labels of shape [batch_size]. But higher dimensions are supported.</em></p>

<p>At some point in the process max_time and batch_size have been mixed up and I'm uncertain if its at input or during the LSTM. I'm grateful for any advice!</p>
",2017-11-16 10:28:49,"<p>That is because of the shape of the output of the tf.nn.dynamic_rnn. From its documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a>:</p>

<p>outputs: The RNN output Tensor.</p>

<p>If time_major == False (default), this will be a Tensor shaped: [batch_size, max_time, cell.output_size].</p>

<p>If time_major == True, this will be a Tensor shaped: [max_time, batch_size, cell.output_size].</p>

<p>you are in the default case, so your <code>outputs</code> gas shape <code>[batch_size, max_time, output_size]</code>, and when performing <code>outputs[-1]</code> you obtain a tensor with shape <code>[max_time, output_size]</code>. Probably slicing with <code>outputs[:, -1]</code> should fix it.</p>
",7822780,173,https://stackoverflow.com/questions/47327256,Lack of Alternative Solutions/Documentation
47568998,Tensorflow: Load data in multiple threads on cpu,"<p>I have a python class <code>SceneGenerator</code> which has multiple member functions for preprocessing and a generator function <code>generate_data()</code>. The basic structure is like this:</p>

<pre><code>class SceneGenerator(object):
    def __init__(self):
       # some inits

    def generate_data(self):
        """"""
        Generator. Yield data X and labels y after some preprocessing
        """"""
        while True:
            # opening files, selecting data
            X,y = self.preprocess(some_params, filenames, ...)            

            yield X, y
</code></pre>

<p>I used the class member function sceneGenerator.generate_data() in keras model.fit_generator() function to read the data from disk, preprocess it and yield it. In keras, this is done on multiple CPU threads, if the <code>workers</code> parameter of <code>model.fit_generator()</code> is set to something > 1.</p>

<p>I now want to use the same <code>SceneGenerator</code> class in tensorflow. My current approach is this:</p>

<pre><code>sceneGenerator = SceneGenerator(some_params)
for X, y in sceneGenerator.generate_data():

    feed_dict = {ops['data']: X,
                 ops['labels']: y,
                 ops['is_training_pl']: True
                 }
    summary, step, _, loss, prediction = sess.run([optimization_op, loss_op, pred_op],
                                                  feed_dict=feed_dict)
</code></pre>

<p>This, however, is slow and does not use multiple threads. I found the <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/data/Dataset"" rel=""noreferrer""><code>tf.data.Dataset</code></a> api with some <a href=""https://www.tensorflow.org/versions/master/programmers_guide/datasets"" rel=""noreferrer"">documentation</a>, but I fail to implement the methods.</p>

<p><strong>Edit:</strong> Notice that I do not work with images so that the image loading mechanisms with file paths etc. do not work here.
My <code>SceneGenerator</code> loads data from hdf5 files. But not complete datasets but - depending on the initialization parameters - only parts of a dataset. I would love to keep the generator function as it is and learn how this generator can be directly used as input for tensorflow and runs on multiple threads on the CPU. Rewriting the data from the hdf5 files to csv is not a good option because it duplicated lots of data.</p>

<p><strong>Edit 2:</strong>: I think something similar to this could help: <a href=""https://stackoverflow.com/questions/47086599/parallelising-tf-data-dataset-from-generator"">parallelising tf.data.Dataset.from_generator</a></p>
",2017-11-30 08:34:45,"<p>Assuming you're using the latest Tensorflow (1.4 at the time of this writing), you can keep the generator and use the <a href=""https://www.tensorflow.org/api_docs/python/tf/data"" rel=""noreferrer""><code>tf.data.*</code></a> API as follows (I chose arbitrary values for the thread number, prefetch buffer size, batch size and output data types):</p>

<pre><code>NUM_THREADS = 5
sceneGen = SceneGenerator()
dataset = tf.data.Dataset.from_generator(sceneGen.generate_data, output_types=(tf.float32, tf.int32))
dataset = dataset.map(lambda x,y : (x,y), num_parallel_calls=NUM_THREADS).prefetch(buffer_size=1000)
dataset = dataset.batch(42)
X, y = dataset.make_one_shot_iterator().get_next()
</code></pre>

<p>To show that it's actually multiple threads extracting from the generator, I modified your class as follows:</p>

<pre><code>import threading    
class SceneGenerator(object):
  def __init__(self):
    # some inits
    pass

  def generate_data(self):
    """"""
    Generator. Yield data X and labels y after some preprocessing
    """"""
    while True:
      # opening files, selecting data
      X,y = threading.get_ident(), 2 #self.preprocess(some_params, filenames, ...)            
      yield X, y
</code></pre>

<p>This way, creating a Tensorflow session and getting one batch shows the thread IDs of the threads getting the data. On my pc, running:</p>

<pre><code>sess = tf.Session()
print(sess.run([X, y]))
</code></pre>

<p>prints</p>

<pre><code>[array([  8460.,   8460.,   8460.,  15912.,  16200.,  16200.,   8460.,
         15912.,  16200.,   8460.,  15912.,  16200.,  16200.,   8460.,
         15912.,  15912.,   8460.,   8460.,   6552.,  15912.,  15912.,
          8460.,   8460.,  15912.,   9956.,  16200.,   9956.,  16200.,
         15912.,  15912.,   9956.,  16200.,  15912.,  16200.,  16200.,
         16200.,   6552.,  16200.,  16200.,   9956.,   6552.,   6552.], dtype=float32),
 array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])]
</code></pre>

<p><strong>Note</strong>: You might want to experiment removing the <code>map</code> call (that we only use to have the multiple threads) and checking if the <code>prefetch</code>'s buffer is enough to remove the bottleneck in your input pipeline (even with only one thread, often the input preprocessing is faster than the actual graph execution, so the buffer is enough to have the preprocessing go as fast as it can).</p>
",3971621,1761,https://stackoverflow.com/questions/47568998,Documentation Replication on Other Examples
46759271,Image pixel value normalized for tf.image.decode_jpeg and tf.train.shuffle_batch?,"<p>I am trying to use the tf.train.shuffle_batch function from tensorflow, then I need to first load the images using tf.image.decode_jpeg(or other similar functions to load png and jpg). But I just found out that the images are loaded as probability map, which means the max of the value of pixel is 1, and the min of the value of the pixel is 0. Below is my code updated from a github repo. I don't know why the values of pixels are normalized to [0,1], and I don't find related documentation on tensorflow. Could anyone help me? Thanks.  </p>

<pre><code>def load_examples(self, input_dir,  flip, scale_size, batch_size, min_queue_examples):
    input_paths = get_image_paths(input_dir)
    with tf.name_scope(""load_images""):
        path_queue = tf.train.string_input_producer(input_paths)
        reader = tf.WholeFileReader()
        paths, contents = reader.read(path_queue)
        # note this is important for truncated images
        raw_input = tf.image.decode_jpeg(contents,try_recover_truncated = True, acceptable_fraction=0.5)
        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)
        raw_input.set_shape([None, None, 3])

        # break apart image pair and move to range [-1, 1]
        width = tf.shape(raw_input)[1]  # [height, width, channels]
        a_images = preprocess(raw_input[:, :width // 2, :])
        b_images = raw_input[:, width // 2:, :]

    inputs, targets = [a_images, b_images]

    def transform(image):
        r = image

        r = tf.image.resize_images(r, [self.image_height, self.image_width], method=tf.image.ResizeMethod.AREA)
        return r
    def transform_gaze(image):
        r = image
        r = tf.image.resize_images(r, [self.gaze_height, self.gaze_width], method=tf.image.ResizeMethod.AREA)
        return r
    with tf.name_scope(""input_images""):
        input_images = transform(inputs)

    with tf.name_scope(""target_images""):
        target_images = transform(targets)
    total_image_count = len(input_paths)
    # target_images = tf.image.per_image_standardization(target_images)
    target_images = target_images[:,:,0]
    target_images = tf.expand_dims(target_images, 2)
    inputs_batch, targets_batch = tf.train.shuffle_batch([input_images, target_images],
                                         batch_size=batch_size,
                                         num_threads=1,
                                         capacity=min_queue_examples + 3 * batch_size,
                                         min_after_dequeue=min_queue_examples)
    # inputs_batch, targets_batch = tf.train.batch([input_images, target_images],batch_size=batch_size)
    return inputs_batch, targets_batch, total_image_count
</code></pre>
",2017-10-15 19:44:25,"<p>Values are into [0,1] because is what <code>tf.image.decode_*</code> methods do.</p>

<p>In general, when a method returns a float tensor, its values are supposed to be in the [0,1] range, whilst if the returned tensor is a uint8 the values are supposed to be in the [0,255] range.</p>

<p>Also, when you use the <code>tf.image.convert_image_dtype</code> method, to convert the dtype of the input image, you're applying that conversion rules.</p>

<p>If your input image is a uint8 image and you convert it to a float32, the values are scaled in the [0,1] range. If your image is already a float, its values are supposed to be in that range and nothing is done.</p>
",3173450,505,https://stackoverflow.com/questions/46759271,Documentation Replication on Other Examples
47657835,"In Tensorflow, is there a direct way to construct an array tensor from a scalar tensor?","<p>For example, something similar to:</p>

<pre><code>x = 12
y = np.array([[x, x], [0, 0]])
</code></pre>

<p>Is there a direct to construct the array tensor y given scalar tensor x in Tensorflow? Without using tf.expand_dims and tf.pad.</p>

<pre><code>x = tf.placeholder(tf.int32)
y = ?
</code></pre>

<p>Thank you very much.</p>
",2017-12-05 16:03:13,"<p>You can use the <a href=""https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor"" rel=""nofollow noreferrer""><code>tf.convert_to_tensor()</code></a> function to do this:</p>

<pre><code>x = tf.placeholder(tf.int32)  # NOTE: You should probably pass `shape=[]` as well.
y = tf.convert_to_tensor([[x, x], [0, 0]])
</code></pre>

<p>Also note that TensorFlow will implicitly call <code>tf.convert_to_tensor()</code> on the inputs to any function that expects a <code>tf.Tensor</code> as input, so you may be able to pass <code>[[x, x], [0, 0]]</code> directly to many functions.</p>
",7586189,1249,https://stackoverflow.com/questions/47657835,Requesting (Additional) Documentation/Examples
47658691,Replace tf.squeeze using as tf.reshape,"<p>I need to train a mobile net using tensorflow. The tf.squeeze layer is not supported. Can I replace it with the tf.reshape? </p>

<p>Is the operation:</p>

<pre><code>tf.squeeze(net, [1, 2], name='squeeze')
</code></pre>

<p>the same as:</p>

<pre><code>tf.reshape(net, [50,1000], name='reshape')
</code></pre>

<p>where net has the shape [50,1,1,1000].</p>
",2017-12-05 16:50:22,"<p>Why do you say <a href=""https://www.tensorflow.org/api_docs/python/tf/squeeze"" rel=""nofollow noreferrer"">tf.squeeze</a> is not supported? In order to remove 1 dimensional axis from tensor, <code>tf.squeeze</code> is the correct operation. But you can achieve your desired work with <code>tf.reshape</code> as well though I will suggest you to make use of <code>tf.squeeze</code>.</p>
",4888520,63,https://stackoverflow.com/questions/47658691,Documentation Replication on Other Examples
48697799,Tensorflow feature column for variable list of values,"<p>From the TensorFlow docs it's clear how to use <code>tf.feature_column.categorical_column_with_vocabulary_list</code> to create a feature column which takes as input some string and outputs a one-hot vector. For example</p>

<pre><code>vocabulary_feature_column =
    tf.feature_column.categorical_column_with_vocabulary_list(
        key=""vocab_feature"",
        vocabulary_list=[""kitchenware"", ""electronics"", ""sports""])
</code></pre>

<p>Let's say <code>""kitchenware""</code> maps to <code>[1,0,0]</code> and <code>""electronics""</code> maps to <code>[0,1,0]</code>. My question is related to having a <strong>list of strings</strong> as a feature. For example, if the feature value was <code>[""kitchenware"",""electronics""]</code> then the desired output would be <code>[1,1,0]</code>. The input list length is not fixed but the output dimension is.</p>

<p>The use case is a straight bag-of-words type model (obviously with a much larger vocabulary list!).</p>

<p>What is the correct way to implement this?</p>
",2018-02-09 02:18:42,"<p>you should use tf.feature_column.indicator_column
see <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/feature_column/indicator_column"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/master/api_docs/python/tf/feature_column/indicator_column</a></p>
",7978198,807,https://stackoverflow.com/questions/48697799,Documentation Replication on Other Examples
48825785,How can I filter tf.data.Dataset by specific values?,"<p>I create a dataset by reading the TFRecords, I map the values and I want to filter the dataset for specific values, but since the result is a dict with tensors, I am not able to get the actual value of a tensor or to check it with <code>tf.cond()</code> / <code>tf.equal</code>. How can I do that?</p>

<pre><code>def mapping_func(serialized_example):
    feature = { 'label': tf.FixedLenFeature([1], tf.string) }
    features = tf.parse_single_example(serialized_example, features=feature)
    return features

def filter_func(features):
    # this doesn't work
    #result = features['label'] == 'some_label_value'
    # neither this
    result = tf.reshape(tf.equal(features['label'], 'some_label_value'), [])
    return result

def main():
    file_names = [""/var/data/file1.tfrecord"", ""/var/data/file2.tfrecord""]
    dataset = tf.contrib.data.TFRecordDataset(file_names)
    dataset = dataset.map(mapping_func)
    dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.filter(filter_func)
    dataset = dataset.repeat()
    iterator = dataset.make_one_shot_iterator()
    sample = iterator.get_next()
</code></pre>
",2018-02-16 11:27:07,"<p>I am answering my own question. I found the issue!</p>

<p>What I needed to do is <code>tf.unstack()</code> the label like this:</p>

<pre><code>label = tf.unstack(features['label'])
label = label[0]
</code></pre>

<p>before I give it to <code>tf.equal()</code>:</p>

<pre><code>result = tf.reshape(tf.equal(label, 'some_label_value'), [])
</code></pre>

<p>I suppose the problem was that the label is defined as an array with one element of type string <code>tf.FixedLenFeature([1], tf.string)</code>, so in order to get the first and single element I had to unpack it (which creates a list) and then get the element with index 0, correct me if I'm wrong.</p>
",4137497,7300,https://stackoverflow.com/questions/48825785,Documentation Replication on Other Examples
48914952,num_buckets as a parameter in a tensorflow feature column,"<p>Currently Tensorflow documentation define a categorical vocabulary column this way:</p>

<pre><code>vocabulary_feature_column =
tf.feature_column.categorical_column_with_vocabulary_list(
    key=""feature_name_from_input_fn"",
    vocabulary_list=[""kitchenware"", ""electronics"", ""sports""]) 
</code></pre>

<p>However this suppose that we input manually the vocabulary list.
In case of large dataset with many columns and many unique values I would like to automate the process this way:</p>

<pre><code>for k in categorical_feature_names:
    vocabulary_feature_column =
        tf.feature_column.categorical_column_with_vocabulary_list(
        key=""feature_name_from_input_fn"",
        vocabulary_list=list_of_unique_values_in_the_column) 
</code></pre>

<p>To do so I need to retrieve the parameter <code>list_of_unique_values_in_the_column</code>.
Is there anyway to do that with Tensorflow? </p>

<p>I know there is tf.unique that could return unique values in a tensor but I don't get how I could feed the column to it so it returns the right vocabulary list.</p>
",2018-02-21 20:31:51,"<p>If <code>list_of_unique_values_in_the_column</code> is known, you can save them in one file and read by <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file"" rel=""nofollow noreferrer""> tf.feature_column.categorical_column_with_vocabulary_file</a>. If unknown, you can use <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket"" rel=""nofollow noreferrer""> tf.feature_column.categorical_column_with_hash_bucket </a> with a large enough size.</p>
",2175173,2187,https://stackoverflow.com/questions/48914952,Documentation Replication on Other Examples
48048297,Explicit CPU placement in TensorFlow,"<p>I found there are a piece of code in official model sample which confused me. </p>

<pre><code>with tf.device(""/cpu:0""):
  embedding = tf.get_variable(
      ""embedding"", [vocab_size, size], dtype=data_type())
  inputs = tf.nn.embedding_lookup(embedding, input_.input_data)
</code></pre>

<p>Why using <code>tf.device(""/cpu:0"")</code> here? Except the case GPU memory leak, is there any other situation which we need to designate CPU operations explicitly?</p>
",2018-01-01 08:46:22,"<p>The reason to place the embedding matrix on a CPU is that <code>tf.nn.embedding_lookup</code> <a href=""https://github.com/tensorflow/tensorflow/issues/2502"" rel=""nofollow noreferrer"">isn't supported</a> on a GPU yet:</p>

<blockquote>
  <p>So, given the basic word2vec example being bound to CPU (#514), we can
  see that <code>tf.nn.embedding_lookup</code> doesn't work on GPU. Therefore, ops
  that use <code>embedding_lookup</code> internally doesn't support GPU either (for
  example, <code>nce_loss</code>).</p>
</blockquote>

<p>This means that GPU placement of <code>embedding</code> variable will only lead to unnecessary transfer of data from the main memory to the GPU memory and vice versa. Hence, it would be more efficient to explicitly place the variable on a CPU.</p>
",3134227,63,https://stackoverflow.com/questions/48048297,Documentation Replication on Other Examples
47389988,How to control GPU memory size with tf.estimator,"<p>I'm trying to control the size of GPU memory allocated for one tensorflow estimator tf.estimator.Estimator. The purpose is to only allocate half to run other tensorflow net on the same GPU. I found for the contrib version but not for the official. Someone knows if it's possible?</p>
",2017-11-20 10:35:43,"<p>When you create an <code>Estimator</code> instance, you can pass in the constructor's <code>config</code> a <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"" rel=""noreferrer""><code>tf.estimator.RunConfig</code></a> instance.
The <code>RunConfig</code> has a <code>session_config</code> attribute you can use to set a <code>tf.ConfigProto</code> with the session's parameters.</p>



<p>In code, this translates to:</p>

<pre class=""lang-python prettyprint-override""><code>session_config = tf.ConfigProto()
session_config.gpu_options.per_process_gpu_memory_fraction = 0.5
estimator_config = tf.estimator.RunConfig(session_config=session_config)
my_estimator = tf.estimator.Estimator(..., config=estimator_config)
</code></pre>
",7007134,165,https://stackoverflow.com/questions/47389988,Documentation Replication on Other Examples
48169791,How to change the value of a tensor which is not a tf.Variable in TensorFlow?,"<p>I know that there is a <code>tf.assign</code> function in TensorFlow, but this function is mainly aimed at mutable tensor (<code>tf.Variable</code>). How to modify the value of the tensor? For example, the following code,</p>

<pre><code>import numpy as np
import tensorflow as tf

X = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])

conv1 = tf.layers.conv2d(X, filters=64, kernel_size=(3, 3), padding='same',name='conv1')
relu1 = tf.nn.relu(conv1)

conv2 = tf.layers.conv2d(relu1, filters=64, kernel_size=(3, 3), padding='same',name='conv2')
relu2 = tf.nn.relu(conv2)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)

tensor = sess.graph.get_tensor_by_name(u'conv2/Conv2D:0')
feature_map = tf.reduce_mean(tensor[:,:,:,24])

image = np.random.uniform(size=(1,32,32,3))
sess.run([feature_map], feed_dict={X: image})
</code></pre>

<p>How to modify the value of <code>feature_map</code> and do not affect its derivation?</p>

<p>More specifically, when I change the value of <code>feature_map</code>, it does not affect its derivation process. 
For example, <code>y = a^2</code>, <code>y'= 2a</code>, I just need to change <code>a = 1</code> to <code>a = 2</code>. </p>

<p><code>Other_op = tf.gradients(feature_map, X)</code></p>

<p>Different <code>feature_map</code> would achieve the different values, but it does not destroy the graph structures of operation.</p>
",2018-01-09 13:57:16,"<p>In your example <code>feature_map</code> doesn't have a value as it's an operation. Therefore you can't change it's value as such. What you can do, is pass another value in as part of the <code>feed_dict</code> parameter of <code>session.run</code>.</p>

<p>So for example if your feature_map is followed by an operation like this:</p>

<pre><code>other_op = tf.gradient(feature_map, X)
</code></pre>

<p>Then you can change the value passed in to that op (<code>gradient</code> in this case) via <code>feed_dict</code> like so:</p>

<pre><code>session.run(other_op, feed_dict={feature_map: &lt;new value&gt;})
</code></pre>
",3571955,13,https://stackoverflow.com/questions/48169791,Documentation Replicability
48243764,Concat two padded senteces and insert to conv1d i tensorflow?,"<p>What dimensions are required in tf.nn.conv1d ? and how to perform max pooling afterwards?</p>
",2018-01-13 19:57:18,"<p>A simple example snip:</p>

<pre><code>               filter = tf.zeros([3, 16, 16])
                W = tf.Variable(tf.truncated_normal(filter, stddev=0.1), name=""W"")
                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")
                conv = tf.nn.conv1d(
                    input_values,
                    W,
                    strides=2,
                    padding=""VALID"",
                    name=""conv"")
                # nonlinearity operation
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")
                # Maxpooling over the outputs
                pooled = tf.nn.max_pool(
                    h,
                    ksize=[1, sequence_length - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1],
                    padding='VALID',
                    name=""pool"")
                pooled_outputs.append(pooled)
</code></pre>

<p>Check this <a href=""https://stackoverflow.com/a/38117279/1363495"">answer</a> as well.</p>
",7905379,25,https://stackoverflow.com/questions/48243764,Documentation Replicability
48471926,In Tensorflow's Dataset API how do you map one element into multiple elements?,"<p>In the tensorflow <code>Dataset</code> pipeline I'd like to define a custom map function which takes a single input element (data sample) and returns multiple elements (data samples).</p>

<p>The code below is my attempt, along with the desired results. </p>

<p>I could not follow the documentation on <code>tf.data.Dataset().flat_map()</code> well enough to understand if it was applicable here or not.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

input = [10, 20, 30]

def my_map_func(i):
  return [[i, i+1, i+2]]       # Fyi [[i], [i+1], [i+2]] throws an exception

ds = tf.data.Dataset.from_tensor_slices(input)
ds = ds.map(map_func=lambda input: tf.py_func(
  func=my_map_func, inp=[input], Tout=[tf.int64]
))
element = ds.make_one_shot_iterator().get_next()

with tf.Session() as sess:
  for _ in range(9):
    print(sess.run(element))
</code></pre>

<p>Results:</p>

<pre><code>(array([10, 11, 12]),)
(array([20, 21, 22]),)
(array([30, 31, 32]),)
</code></pre>

<p>Desired results:</p>

<pre><code>(10)
(11)
(12)
(20)
(21)
(22)
(30)
(31)
(32)
</code></pre>
",2018-01-27 02:08:26,"<p>Two more steps were required to achieve this. First, the map function needs to return a numpy array, not a list.</p>

<p>Then you can use <code>flat_map</code> combined with <code>Dataset().from_tensor_slices()</code> to flatten them. The code below now produces the desired result:</p>

<p>Tested in Tensorflow 1.5 (copy/paste runnable example)</p>

<pre><code>import tensorflow as tf
import numpy as np

input = [10, 20, 30]

def my_map_func(i):
  return np.array([i, i + 1, i + 2])

ds = tf.data.Dataset.from_tensor_slices(input)
ds = ds.map(map_func=lambda input: tf.py_func(
  func=my_map_func, inp=[input], Tout=[tf.int64]
))
ds = ds.flat_map(lambda x: tf.data.Dataset().from_tensor_slices(x))

element = ds.make_one_shot_iterator().get_next()

with tf.Session() as sess:
  for _ in range(9):
    print(sess.run(element))
</code></pre>

<hr>

<p>Here is a method of doing this if you have multiple variables to return, in this example I input a string (such as a filename) and output multiples of both strings and integers. In this case I repeat the string for each of the integers of [10, 20, 30]. </p>

<p>Copy/paste runnable example:</p>

<pre><code>import tensorflow as tf
import numpy as np

input = [b'testA', b'testB', b'testC']

def my_map_func(input):
  return np.array([input, input, input]), np.array([10, 20, 30])

ds = tf.data.Dataset.from_tensor_slices(input)
ds = ds.map(map_func=lambda input: tf.py_func(
    func=my_map_func, inp=[input], Tout=[tf.string, tf.int64]))
ds = ds.flat_map(lambda mystr, myint: tf.data.Dataset().zip((
  tf.data.Dataset().from_tensor_slices(mystr),
  tf.data.Dataset().from_tensor_slices(myint))
))

element = ds.make_one_shot_iterator().get_next()

with tf.Session() as sess:
  for _ in range(9):
    print(sess.run(element))
</code></pre>
",4790871,31007,https://stackoverflow.com/questions/48471926,Documentation Replicability
48543654,Why should one use tf.train.Server to execute multiple tf.Session() in parallel?,"<p>The official way to execute multiple <code>tf.Session()</code> in parallel is to use <code>tf.train.Server</code> as described in <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">Distributed TensorFlow
</a>. On the other hand, the following works for Keras and can be modified to Tensorflow presumably without using tf.train.Server according to <a href=""https://stackoverflow.com/questions/42504669/keras-tensorflow-and-multiprocessing-in-python"">Keras + Tensorflow and Multiprocessing in Python</a>.  </p>

<pre><code>def _training_worker(train_params):
    import keras
    model = obtain_model(train_params)
    model.fit(train_params)
    send_message_to_main_process(...)

def train_new_model(train_params):
    training_process = multiprocessing.Process(target=_training_worker, args = train_params)
    training_process.start()
    get_message_from_training_process(...)
    training_process.join()
</code></pre>

<p>Is the first method faster than the second method? I have a code written in the second way, and due to the nature of my algorithm (AlphaZero) a single GPU is supposed to run many processes, each of which performs prediction of tiny minibatch.   </p>
",2018-01-31 13:45:18,"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/train/Server"" rel=""nofollow noreferrer""><code>tf.train.Server</code></a> is designed for distributed computation within a <em>cluster</em>, when there is a need to <em>communicate</em> between different nodes. This is especially useful when training is distributed across multiple machines or in some cases across multiple GPUs on a single machine. From the documentation:</p>
<blockquote>
<p>An in-process TensorFlow server, for use in distributed training.</p>
<p>A <code>tf.train.Server</code> instance encapsulates a set of devices and a <code>tf.Session</code> target that can participate in distributed training. A server belongs to a cluster (specified by a <code>tf.train.ClusterSpec</code>), and corresponds to a particular task in a named job. <strong>The server can communicate with any other server in the same cluster</strong>.</p>
</blockquote>
<p>Spawning multiple processes with <code>multiprocessing.Process</code> isn't a cluster in Tensorflow sense, because the child processes aren't interacting with each other. This method is easier to setup, but it's <strong>limited to a single machine</strong>. Since you say you have just one machine, this might not be a strong argument, but if you ever plan to scale to a cluster of machines, you'll have to redesign the whole approach.</p>
<p><code>tf.train.Server</code> is thus a more universal and scalable solution. Besides, it allows to organize complex training with some non-trivial communications, e.g., async gradient updates. Whether it is faster to train or not greatly depends on a task, I don't think there will be a significant difference on one shared GPU.</p>
<p>Just for the reference, here's how the code looks like with the server (between graph replication example):</p>

<pre class=""lang-py prettyprint-override""><code># specify the cluster's architecture
cluster = tf.train.ClusterSpec({
  'ps': ['192.168.1.1:1111'],
  'worker': ['192.168.1.2:1111',
             '192.168.1.3:1111']
})

# parse command-line to specify machine
job_type = sys.argv[1]  # job type: &quot;worker&quot; or &quot;ps&quot;
task_idx = sys.argv[2]  # index job in the worker or ps list as defined in the ClusterSpec

# create TensorFlow Server. This is how the machines communicate.
server = tf.train.Server(cluster, job_name=job_type, task_index=task_idx)

# parameter server is updated by remote clients.
# will not proceed beyond this if statement.
if job_type == 'ps':
  server.join()
else:
  # workers only
  with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:' + task_idx,
                                                cluster=cluster)):
    # build your model here as if you only were using a single machine
    pass

  with tf.Session(server.target):
    # train your model here
    pass
</code></pre>
",2830610,199,https://stackoverflow.com/questions/48543654,Documentation Replicability
49418325,"Use ""tf.contrib.factorization.KMeansClustering""","<p>Referring to this Link, <a href=""https://github.com/tensorflow/tensorflow/issues/17002"" rel=""nofollow noreferrer"">(the Link)</a>
I try to practice using tf.contrib.factorization.KMeansClustering for clustering. The simple codes as follow works okay:</p>

<pre><code>import numpy as np
import tensorflow as tf

# ---- Create Data Sample -----
k = 5
n = 100
variables = 5
points = np.random.uniform(0, 1000, [n, variables])

# ---- Clustering -----
input_fn=lambda: tf.train.limit_epochs(tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)
kmeans=tf.contrib.factorization.KMeansClustering(num_clusters=6)
kmeans.train(input_fn=input_fn)
centers = kmeans.cluster_centers()

# ---- Print out -----
cluster_indices = list(kmeans.predict_cluster_index(input_fn))
for i, point in enumerate(points):
  cluster_index = cluster_indices[i]
  print ('point:', point, 'is in cluster', cluster_index, 'centered at', centers[cluster_index])
</code></pre>

<p>My question is why would this ""input_fn"" code does the trick?
If I change the code to this, it will run into an infinite loop. Why??</p>

<pre><code>input_fn=lambda:tf.convert_to_tensor(points, dtype=tf.float32)
</code></pre>

<p>From the document <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering"" rel=""nofollow noreferrer"">(here)</a>, it seems that train() is expecting argument of input_fn, which is simply a A 'tf.data.Dataset' object , like Tensor(X). So, why do I have to do all these tricky things regarding lambda: tf.train.limit_epochs()?</p>

<p>Can anyone who is familiar with the fundamental of tensorflow estimators help to explain? Many Thanks!</p>
",2018-03-21 23:47:33,"<blockquote>
  <p>My question is why would this ""input_fn"" code does the trick? If I change the code to this, it will run into an infinite loop. Why??</p>
</blockquote>

<p>The documentation states that <code>input_fn</code> is called repeatedly until it returns a <code>tf.errors.OutOfRangeError</code>. Adorning your tensor with <code>tf.train.limit_epochs</code> ensures that the error is eventually raised, which signals to KMeans that it should stop training.</p>
",7270211,159,https://stackoverflow.com/questions/49418325,Inadequate Examples
49949463,Does tf.transpose also change the memory (like np.ascontiguousarray)?,"<p>If I use <code>tf.transpose</code> does it also change the memory layout? </p>

<p>In numpy it is used the function <code>np.ascontiguousarray</code>. </p>

<p>I mean this would be important if I use cuda. Because it makes a difference if the memory layout is [N C H W] or [N H W C]. (N ... Nr of samples, H ... array height, W ... array width, C ... array depth, e.g. RGB)</p>

<p>How to check this?</p>
",2018-04-20 20:44:04,"<p>If you read carefully <a href=""https://www.tensorflow.org/api_docs/python/tf/transpose"" rel=""nofollow noreferrer"">the documentation</a> you can find the answer:</p>

<blockquote>
  <p>Numpy Compatibility</p>
  
  <p>In numpy transposes are memory-efficient constant time operations as they simply return a new view of the same data with adjusted strides.</p>
  
  <p>TensorFlow does not support strides, so transpose returns a new tensor with the items permuted.</p>
</blockquote>

<p>Hence <code>tf.transpose</code> returns a new tensor with the desired shape (and therefore is inefficient), so yes, it changes the memory layout.</p>

<p>However, instead of using <code>tf.trasnpose</code> you could use <a href=""https://www.tensorflow.org/api_docs/python/tf/reshape"" rel=""nofollow noreferrer""><code>tf.reshape</code></a> for changing the tensor shape without creating a new one</p>
",4355878,1375,https://stackoverflow.com/questions/49949463,Documentation Replication on Other Examples
48675932,How to automatically add zero padding in a image to expected shape?,"<p>I have an image with the shape of <code>200x250x3</code>. I want to add the zero padding on top, left, right, bottom to the image to achieve a target shape of <code>256x256x3</code>. How could I do it in tensorflow? I found the function tf.pad, but it needs to compute the padding size, while my task have to compute it automatically</p>

<p><a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#pad"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#pad</a></p>

<pre><code>tf.pad(tensor, paddings, mode='CONSTANT', name=None)
</code></pre>
",2018-02-08 01:10:12,"<p>For padding images to a target shape, you can use <a href=""https://www.tensorflow.org/api_docs/python/tf/image/resize_image_with_crop_or_pad"" rel=""nofollow noreferrer"">tf.image.resize_image_with_crop_or_pad()</a>. This op crops the image if it is larger than target size, and pads (evenly on all sides) with zeros if it is smaller.</p>

<pre><code>&gt;&gt;&gt; a = tf.ones([3, 4, 3])
&gt;&gt;&gt; tf.image.resize_image_with_crop_or_pad(a, 5, 5)
&lt;tf.Tensor 'Squeeze:0' shape=(5, 5, 3) dtype=float32&gt;
</code></pre>

<p>If you want to use padding, you can define a function to calculate padding amount using the difference between desired size and the shape of the tensor (<a href=""https://www.tensorflow.org/api_docs/python/tf/shape"" rel=""nofollow noreferrer"">tf.shape()</a>) and pad the difference, check <a href=""https://stackoverflow.com/a/48535322/826970"">this answer</a> for padding.</p>
",2938494,3766,https://stackoverflow.com/questions/48675932,Documentation Replicability
49564318,Issue with fine-tuning inceptionv3 in slim tensorflow and tf record batches,"<p>I am trying to fine-tune inceptionv3 model using slim tensorflow library. 
I am unable to understand certain things while writing the code for it. I tried to read source code (no proper documentation) and figured out few things and I am able to fine-tune it and save the check point. Here are the steps I followed 
 1. I created a tf.record for my training data which is fine, now I am reading the data using the below code. </p>

<pre><code>import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import tensorflow.contrib.slim as slim
import matplotlib.pyplot as plt
import numpy as np

# get the data and labels here

data_path = '/home/sfarkya/nvidia_challenge/datasets/detrac/train1.tfrecords'

# Training setting
num_epochs = 100
initial_learning_rate = 0.0002
learning_rate_decay_factor = 0.7
num_epochs_before_decay = 5
num_classes = 5980

# load the checkpoint
model_path = '/home/sfarkya/nvidia_challenge/datasets/detrac/inception_v3.ckpt'

# log directory
log_dir = '/home/sfarkya/nvidia_challenge/datasets/detrac/fine_tuned_model'

with tf.Session() as sess:
    feature = {'train/image': tf.FixedLenFeature([], tf.string),
               'train/label': tf.FixedLenFeature([], tf.int64)}

    # Create a list of filenames and pass it to a queue
    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)

    # Define a reader and read the next record
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)

    # Decode the record read by the reader
    features = tf.parse_single_example(serialized_example, features=feature)

    # Convert the image data from string back to the numbers
    image = tf.decode_raw(features['train/image'], tf.float32)

    # Cast label data into int32
    label = tf.cast(features['train/label'], tf.int32)

    # Reshape image data into the original shape
    image = tf.reshape(image, [128, 128, 3])

    # Creates batches by randomly shuffling tensors
    images, labels = tf.train.shuffle_batch([image, label], batch_size=64, capacity=128, num_threads=2,
                                            min_after_dequeue=64)
</code></pre>

<p>Now I am finetuning the model using slim and this is the code. </p>

<pre><code>  init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    sess.run(init_op)

    # Create a coordinator and run all QueueRunner objects
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    # load model

    # load the inception model from the slim library - we are using inception v3
    #inputL = tf.placeholder(tf.float32, (64, 128, 128, 3))

    img, lbl = sess.run([images, labels])
    one_hot_labels = slim.one_hot_encoding(lbl, num_classes)

    with slim.arg_scope(slim.nets.inception.inception_v3_arg_scope()):
        logits, inceptionv3 = nets.inception.inception_v3(inputs=img, num_classes=5980, is_training=True,
                                                          dropout_keep_prob=.6)

    # Restore convolutional layers:

    variables_to_restore = slim.get_variables_to_restore(exclude=['InceptionV3/Logits', 'InceptionV3/AuxLogits'])
    init_fn = slim.assign_from_checkpoint_fn(model_path, variables_to_restore)

    # loss function
    loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits = logits)
    total_loss = tf.losses.get_total_loss()

    # train operation
    train_op = slim.learning.create_train_op(total_loss + loss, optimizer= tf.train.AdamOptimizer(learning_rate=1e-4))

    print('Im here')
    # Start training.
    slim.learning.train(train_op, log_dir, init_fn=init_fn, save_interval_secs=20, number_of_steps= 10)
</code></pre>

<p>Now I have few questions about the code, which I am quite unable to figure out. Once, the code reaches <strong>slim.learning.train</strong> I don't see anything printing however, it's training, I can see in the log. Now, 
1. How do I give the number of epochs to the code? Right now it's running step by step with each step has batch_size = 64.<br>
2. How do I make sure that in the code <strong>tf.train.shuffle_batch</strong> I am not repeating my images and I am training over the whole dataset? 
3. How can I print the loss values while it's training?</p>
",2018-03-29 20:14:52,"<p>Here are answers to your questions.</p>

<ol>
<li><p>You cannot give epochs directly to <code>slim.learning.train</code>. Instead, you give the number of batches as the argument. It is called <code>number_of_steps</code>. It is used to set an operation called <code>should_stop_op</code> on <a href=""https://github.com/tensorflow/tensorflow/blob/7a0def60d45c1841a4e79a0ddf6aa9d50bf551ac/tensorflow/contrib/slim/python/slim/learning.py#L709"" rel=""nofollow noreferrer"">line 709</a>. I assume you know how to convert number of epochs to batches.</p></li>
<li><p>I don't think the <code>shuffle_batch</code> function will repeat images because internally it uses the <a href=""https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue"" rel=""nofollow noreferrer"">RandomShuffleQueue</a>. According to <a href=""https://stackoverflow.com/a/43190902/1586200"">this answer</a>, the <code>RandomShuffleQueue</code> enqueues elements using a background thread as:</p>

<ul>
<li>While <code>size(queue) &lt; capacity</code>:

<ul>
<li>Add an element to the queue</li>
</ul></li>
</ul></li>
</ol>

<p>It dequeues elements as:</p>

<ul>
<li>While the <code>number of elements dequeued &lt; batch_size</code>:

<ul>
<li>Wait until the <code>size(queue) &gt;= min_after_dequeue + 1</code> elements.</li>
<li>Select an element from the queue uniformly at random, remove it from the queue, and add it the output batch.</li>
</ul></li>
</ul>

<p>So in my opinion, there is very little chance that the elements would be repeated, because in the <code>dequeuing</code> operation, the chosen element is removed from the queue. So it is sampling without replacement.</p>

<p><strong>Will a new queue be created for every epoch?</strong></p>

<p>The tensors being inputted to <code>tf.train.shuffle_batch</code> are <code>image</code> and <code>label</code> which ultimately come from the <code>filename_queue</code>. If that queue is producing TFRecord filenames indefinitely, then I don't think a new queue will be created by <code>shuffle_batch</code>. You can also create a toy code like <a href=""https://stackoverflow.com/a/45207025/1586200"">this</a> to understand how <code>shuffle_batch</code> works.</p>

<p>Coming to the next point, how to train over the whole dataset? In your code, the following line gets the list of TFRecord filenames.</p>

<pre><code>filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)
</code></pre>

<p>If <code>filename_queue</code> covers all TFRecords that you have, then you are surely training over the entire dataset. Now, how to shuffle the entire dataset is another question. As mentioned <a href=""https://github.com/tensorflow/tensorflow/issues/14857"" rel=""nofollow noreferrer"">here</a> by @mrry, there is no support (yet, AFAIK) to shuffle out-of-memory datasets. So the best way is to prepare many shards of your dataset such that each shard contains about 1024 examples. Shuffle the list of TFRecord filenames as:</p>

<pre><code>filename_queue = tf.train.string_input_producer([data_path], shuffle=True, capacity=1000)
</code></pre>

<p>Note that I removed the <code>num_epochs = 1</code> argument and set <code>shuffle=True</code>. This way it will produce the <em>shuffled</em> list of TFRecord filenames indefinitely. Now on each file, if you use <code>tf.train.shuffle_batch</code>, you will get a near-to-uniform shuffling. Basically, as the number of examples in each shard tend to 1, your shuffling will get more and more uniform. I like to not set <code>num_epochs</code> and instead terminate the training using the <code>number_of_steps</code> argument mentioned earlier.</p>

<ol start=""3"">
<li>To print the loss values, you could probably just edit the <a href=""https://github.com/tensorflow/tensorflow/blob/024aecf414941e11eb643e29ceed3e1c47a115ad/tensorflow/contrib/slim/python/slim/learning.py#L768"" rel=""nofollow noreferrer""><code>training.py</code></a> and introduce <code>logging.info('total loss = %f', total_loss)</code>. I don't know if there is any simpler way. Another way without changing the code is to view summaries in Tensorboard.</li>
</ol>

<p>There are very helpful articles on how to view summaries in Tensorboard, including the link at the end of this answer. Generally, you need to do the following things.</p>

<ol>
<li>Create <code>summary</code> object.</li>
<li>Write variables of interest into <code>summary</code>.</li>
<li>Merge all individual summaries.</li>
<li>Create a <code>summary</code> op.</li>
<li>Create a summary file writer.</li>
<li>Write the summaries throughout the training at a desired frequency.</li>
</ol>

<p>Now steps 5 and 6 are already done automatically for you if you use <code>slim.learning.train</code>.</p>

<p>For first 4 steps, you could check the file <a href=""https://github.com/tensorflow/models/blob/46377f8842e512805a3b9cbbfab026e39a82d346/research/slim/train_image_classifier.py#L472"" rel=""nofollow noreferrer""><code>train_image_classifier.py</code></a>. Line 472 shows you how to create a <code>summaries</code> object. Lines 490, 512 and 536 write the relevant variables into <code>summaries</code>. Line 549 merges all summaries and the line 553 creates an op. You can pass this op to <code>slim.learning.train</code> and you can also specify how frequently you want to write summaries. In my opinion, do not write anything apart from loss, total_loss, accuracy and learning rate into the summaries, unless you want to do specific debugging. If you write histograms, then the tensorboard file could take tens of hours to load for networks like ResNet-50 (my tensorboard file once was 28 GB, which took 12 hours to load the progress of 6 days!). By the way, you could actually use <code>train_image_classifier.py</code> file to finetune and you will skip most of the steps above. However, I prefer this as you get to learn a lot of things.</p>

<p>See the <a href=""https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard"" rel=""nofollow noreferrer"">launching tensorboard</a> section on how to view the progress in a browser.</p>

<p><strong>Additional remarks:</strong></p>

<ul>
<li><p>Instead of minimizing <code>total_loss + loss</code>, you could do the following:</p>

<pre><code>loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits = logits)
tf.losses.add_loss(loss)
total_loss = tf.losses.get_total_loss()
train_op = slim.learning.create_train_op(total_loss, optimizer=tf.train.AdamOptimizer(learning_rate=1e-4))
</code></pre></li>
<li><p>I found <a href=""https://kwotsin.github.io/tech/2017/02/11/transfer-learning.html"" rel=""nofollow noreferrer"">this</a> post to be very useful when I was learning Tensorflow.</p></li>
</ul>
",7776604,962,https://stackoverflow.com/questions/49564318,Documentation Replication on Other Examples
48101576,TensorFlow - Read video frames from TFRecords file,"<p><strong>TLDR;</strong> <em>my question is on how to load compressed video frames from TFRecords.</em></p>

<p>I am setting up a data pipeline for training deep learning models on a large video dataset (<a href=""https://deepmind.com/research/open-source/open-source-datasets/kinetics/"" rel=""noreferrer"">Kinetics</a>). For this I am using TensorFlow, more specifically the <code>tf.data.Dataset</code> and <code>TFRecordDataset</code> structures. As the dataset contains ~300k videos of 10 seconds, there is a large amount of data to deal with. During training, I want to randomly sample 64 consecutive frames from a video, therefore fast random sampling is important. For achieving this there are a number of data loading scenarios possible during training:</p>

<ol>
<li><strong>Sample from Video.</strong> Load the videos using <code>ffmpeg</code> or <code>OpenCV</code> and sample frames. Not ideal as seeking in videos is tricky, and decoding video streams is much slower than decoding JPG.</li>
<li><strong>JPG Images.</strong> Preprocess the dataset by extracting all video frames as JPG. This generates a huge amount of files, which is probably not going to be fast due to random access.</li>
<li><strong>Data Containers.</strong> Preprocess the dataset to <code>TFRecords</code> or <code>HDF5</code> files. Requires more work getting the pipeline ready, but most likely to be the fastest of those options.</li>
</ol>

<p>I have decided to go for option (3) and use <code>TFRecord</code> files to store a preprocessed version of the dataset. However, this is also not as straightforward as it seems, for example:</p>

<ol>
<li><strong>Compression.</strong> Storing the video frames as uncompressed byte data in TFRecords will require a huge amount of disk space. Therefore, I extract all the video frames, apply JPG compression and store the compressed bytes as TFRecords. </li>
<li><strong>Video Data.</strong> We are dealing with video, so each example in the TFRecords file will be quite large and contains several video frames (typically 250-300 for 10 seconds of video, depending on the frame rate). </li>
</ol>

<p>I have wrote the following code to preprocess the video dataset and write the video frames as TFRecord files (each of ~5GB in size):</p>

<pre><code>def _int64_feature(value):
    """"""Wrapper for inserting int64 features into Example proto.""""""
    if not isinstance(value, list):
        value = [value]
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

def _bytes_feature(value):
    """"""Wrapper for inserting bytes features into Example proto.""""""
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


with tf.python_io.TFRecordWriter(output_file) as writer:

  # Read and resize all video frames, np.uint8 of size [N,H,W,3]
  frames = ... 

  features = {}
  features['num_frames']  = _int64_feature(frames.shape[0])
  features['height']      = _int64_feature(frames.shape[1])
  features['width']       = _int64_feature(frames.shape[2])
  features['channels']    = _int64_feature(frames.shape[3])
  features['class_label'] = _int64_feature(example['class_id'])
  features['class_text']  = _bytes_feature(tf.compat.as_bytes(example['class_label']))
  features['filename']    = _bytes_feature(tf.compat.as_bytes(example['video_id']))

  # Compress the frames using JPG and store in as bytes in:
  # 'frames/000001', 'frames/000002', ...
  for i in range(len(frames)):
      ret, buffer = cv2.imencode("".jpg"", frames[i])
      features[""frames/{:04d}"".format(i)] = _bytes_feature(tf.compat.as_bytes(buffer.tobytes()))

  tfrecord_example = tf.train.Example(features=tf.train.Features(feature=features))
  writer.write(tfrecord_example.SerializeToString())
</code></pre>

<p>This works fine; the dataset is nicely written as TFRecord files with the frames as compressed JPG bytes. My question regards, how to read the TFRecord files during training, randomly sample 64 frames from a video and decode the JPG images. </p>

<p>According to <a href=""https://www.tensorflow.org/programmers_guide/datasets"" rel=""noreferrer"">TensorFlow's documentation</a> on <code>tf.Data</code> we need to do something like: </p>

<pre><code>filenames = tf.placeholder(tf.string, shape=[None])
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(...)  # Parse the record into tensors.
dataset = dataset.repeat()  # Repeat the input indefinitely.
dataset = dataset.batch(32)
iterator = dataset.make_initializable_iterator()
training_filenames = [""/var/data/file1.tfrecord"", ""/var/data/file2.tfrecord""]
sess.run(iterator.initializer, feed_dict={filenames: training_filenames})
</code></pre>

<p>There are many example on how to do this with images, and that is quite straightforward. However, for video and random sampling of frames I am stuck. The <code>tf.train.Features</code> object stores the frames as <code>frame/00001</code>, <code>frame/000002</code> etc. My first question is how to randomly sample a set of consecutive frames from this inside the <code>dataset.map()</code> function? Considerations are that each frame has a variable number of bytes due to JPG compression and need to be decoded using <code>tf.image.decode_jpeg</code>.</p>

<p>Any help how to best setup reading video sampels from TFRecord files would be appreciated! </p>
",2018-01-04 18:39:07,"<p>Encoding each frame as a separate feature makes it difficult to select frames dynamically, because the signature of <code>tf.parse_example()</code> (and <code>tf.parse_single_example()</code>) requires that the set of parsed feature names be fixed at graph construction time. However, you could try encoding the frames as a <em>single</em> feature that contains a list of JPEG-encoded strings:</p>

<pre><code>def _bytes_list_feature(values):
    """"""Wrapper for inserting bytes features into Example proto.""""""
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=values))

with tf.python_io.TFRecordWriter(output_file) as writer:

  # Read and resize all video frames, np.uint8 of size [N,H,W,3]
  frames = ... 

  features = {}
  features['num_frames']  = _int64_feature(frames.shape[0])
  features['height']      = _int64_feature(frames.shape[1])
  features['width']       = _int64_feature(frames.shape[2])
  features['channels']    = _int64_feature(frames.shape[3])
  features['class_label'] = _int64_feature(example['class_id'])
  features['class_text']  = _bytes_feature(tf.compat.as_bytes(example['class_label']))
  features['filename']    = _bytes_feature(tf.compat.as_bytes(example['video_id']))

  # Compress the frames using JPG and store in as a list of strings in 'frames'
  encoded_frames = [tf.compat.as_bytes(cv2.imencode("".jpg"", frame)[1].tobytes())
                    for frame in frames]
  features['frames'] = _bytes_list_feature(encoded_frames)

  tfrecord_example = tf.train.Example(features=tf.train.Features(feature=features))
  writer.write(tfrecord_example.SerializeToString())
</code></pre>

<p>Once you have done this, it will be possible to slice the <code>frames</code> feature dynamically, using a modified version of <a href=""https://gist.github.com/tomrunia/7ef5d40639f2ae41fb71d3352a701e4a"" rel=""nofollow noreferrer"">your parsing code</a>:</p>

<pre><code>def decode(serialized_example, sess):
  # Prepare feature list; read encoded JPG images as bytes
  features = dict()
  features[""class_label""] = tf.FixedLenFeature((), tf.int64)
  features[""frames""] = tf.VarLenFeature(tf.string)
  features[""num_frames""] = tf.FixedLenFeature((), tf.int64)

  # Parse into tensors
  parsed_features = tf.parse_single_example(serialized_example, features)

  # Randomly sample offset from the valid range.
  random_offset = tf.random_uniform(
      shape=(), minval=0,
      maxval=parsed_features[""num_frames""] - SEQ_NUM_FRAMES, dtype=tf.int64)

  offsets = tf.range(random_offset, random_offset + SEQ_NUM_FRAMES)

  # Decode the encoded JPG images
  images = tf.map_fn(lambda i: tf.image.decode_jpeg(parsed_features[""frames""].values[i]),
                     offsets)

  label  = tf.cast(parsed_features[""class_label""], tf.int64)

  return images, label
</code></pre>

<p>(Note that I haven't been able to run your code, so there may be some small errors, but hopefully it is enough to get you started.)</p>
",3419427,1297,https://stackoverflow.com/questions/48101576,Documentation Replicability
49294329,How to get the default session from a tf.estimator?,"<p>I am trying the high API <code>tf.estimator</code>, but I find it hardly to get the session to debug some inter-result such as global step.</p>

<pre><code>cls = tf.estimator.Estimator(
    model_fn=my_model,
    params={
        'feature_columns': fcs,
        'hidden_units': [10, 10],
        'n_classes': 3,
    })
</code></pre>

<p>The example from <a href=""https://www.tensorflow.org/versions/master/get_started/custom_estimators"" rel=""noreferrer"">https://www.tensorflow.org/versions/master/get_started/custom_estimators</a></p>

<p>I have try <code>sess = tf.get_default_session</code> and <code>with tf.Session() as sess</code>, but can't get the defut session.</p>
",2018-03-15 08:10:41,"<p>The easiest thing would be to use <code>tf.Print</code> like:</p>

<pre><code>...
global_step = tf.Print(global_step, [global_step], message='Value of global step"")
...
</code></pre>

<p>You can replace global_step with any tensor you want printed. Then when you run the training it will print the values every time the tensor is evaluated.</p>

<p>Another, more complicated way is to export the model then load it back in using your own session (not the estimator api). Once you do this you can call <code>session.run</code> for any operation defined. You can get operations with <code>tf.get_operation_by_name</code> or <code>tf.get_tensor_by_name</code>. You can also feed whatever values you want as a input.</p>
",8000475,271,https://stackoverflow.com/questions/49294329,Inadequate Examples
49899526,Tensorflow input pipeline where multiple rows correspond to a single observation?,"<p>So I've just started using Tensorflow, and I'm struggling to properly understand input pipelines. </p>

<p>The problem I'm working on is sequence classification.
I'm trying to read in a CSV file with shape (100000, 4). First 3 columns are features, 4th column is the label. BUT - the data represents sequences of length 10 i.e. rows 1-10 are sequence 1, rows 11-20 are sequence 2 etc. This also means each label is repeated 10 times.</p>

<p>So at some point in this input pipeline, I'll need to reshape my feature tensor like tf.reshape(features, [batch_size_, rows_per_ob, input_dim]). 
And only take every 10th row of my label tensor like label[::rows_per_ob]</p>

<p>Another thing I should point out is that my actual dataset is in the billions of rows so I have to think about performance.</p>

<p>I've put together the below code from documentation and other posts on here, but I don't think I fully understand this because I'm seeing the following error:</p>

<blockquote>
  <p>INFO:tensorflow:Error reported to Coordinator: , Attempting to use uninitialized value input_producer_2/limit_epochs/epochs</p>
</blockquote>

<p>There seems to be an out of range error.</p>

<p>I also can't figure out what to do with these batches once I get them working. Initially, I thought I would reshape them then just feed them into ""feed_dict"", but then I read that this is really bad, and I should be using a tf.data.Dataset object. But I'm not sure how to feed these batches into a Dataset. I'm also not entirely sure when would be the optimal time in this process to reshape my data?</p>

<p>And a final point of confusion - when you use an Iterator with a Dataset object, I see that we use the get_next() method. Does this mean that each element in the Dataset represent a full batch of data? And does this then mean that if we want to change the batch size, we need rebuild the entire Dataset object?</p>

<p>I'm really struggling to fit all the pieces together. If anyone has any pointers for me, it would be very much appreciated! Thanks!</p>

<pre><code># import
import tensorflow as tf

# constants
filename = ""tensorflow_test_data.csv""
num_rows = 100000
rows_per_ob = 10
batch_size_ = 5
num_epochs_ = 2
num_batches = int(num_rows * num_epochs_ / batch_size_ / rows_per_ob)

# read csv line
def read_from_csv(filename_queue):
    reader = tf.TextLineReader(skip_header_lines=1)
    _, value = reader.read(filename_queue)
    record_defaults = [[0.0], [0.0], [0.0], [0.0]]
    a, b, c, d = tf.decode_csv(value, record_defaults=record_defaults)
    features = tf.stack([a, b, c])
    return features, d

def input_pipeline(filename=filename, batch_size=batch_size_, num_epochs=num_epochs_):
    filename_queue = tf.train.string_input_producer([filename],
                                                    num_epochs=num_epochs,
                                                    shuffle=False)
    x, y = read_from_csv(filename_queue)
    x_batch, y_batch = tf.train.batch([x, y],
                                      batch_size = batch_size * rows_per_ob,
                                      num_threads=1,
                                      capacity=10000)
    return x_batch, y_batch

###
x, y = input_pipeline(filename, batch_size=batch_size_,
                      num_epochs = num_epochs_)

# I imagine using lists is wrong here - this was more just for me to
# see the output
x_list = []
y_list = []
with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    for _ in range(num_batches):
        x_batch, y_batch = sess.run([x, y])
        x_list.append(x_batch)
        y_list.append(y_batch)
    coord.request_stop()
    coord.join(threads)
</code></pre>
",2018-04-18 12:17:41,"<p>You can express the entire pipeline using <code>tf.data.Dataset</code> objects, which might make things slightly easier:</p>

<pre><code>dataset = tf.data.TextLineDataset(filename)

# Skip the header line.
dataset = dataset.skip(1)

# Combine 10 lines into a single observation.   
dataset = dataset.batch(rows_per_ob)

def parse_observation(line_batch):
  record_defaults = [[0.0], [0.0], [0.0], [0.0]]
  a, b, c, d = tf.decode_csv(value, record_defaults=record_defaults)
  features = tf.stack([a, b, c])
  label = d[-1]  # Take the label from the last row.
  return features, label

# Parse each observation into a `row_per_ob X 2` matrix of features and a
# scalar label.
dataset = dataset.map(parse_observation)

# Batch multiple observations.
dataset = dataset.batch(batch_size)

# Optionally add a prefetch for performance.
dataset = dataset.prefetch(1)
</code></pre>

<p>To use the values from the dataset, you can make a <code>tf.data.Iterator</code> to get the next element as a pair of <code>tf.Tensor</code> objects, then use these as the input to your model.</p>

<pre><code>iterator = dataset.make_one_shot_iterator()

features_batch, label_batch = iterator.get_next()

# Use the `features_batch` and `label_batch` tensors as the inputs to
# the model, rather than fetching them and feeding them via the `Session`
# interface.
train_op = build_model(features_batch, label_batch)
</code></pre>
",5323535,143,https://stackoverflow.com/questions/49899526,Documentation Replication on Other Examples
49987839,How to handle None in tf.clip_by_global_norm?,"<p>I have read in answers to <a href=""https://stackoverflow.com/questions/36498127/how-to-apply-gradient-clipping-in-tensorflow"">this question here</a> that tf.clip_by_global_norm() handles None values by simply ignoring them (comment by danijar in comments to the answer by @danijar) but when i try to apply it i seem to be doing something wrong as it throws </p>

<p>ValueError: None values not supported.</p>

<pre><code>tf.reset_default_graph()
z = tf.get_variable(name = 'z', shape = [1])
b = tf.get_variable('b', [1])
c = b*b - 2*b + 1
optimizer = tf.train.AdamOptimizer(0.1)
gradients, variables = zip(*optimizer.compute_gradients(c))
gradients = tf.clip_by_global_norm(gradients, 2.5)
train_op = optimizer.apply_gradients(zip(gradients, variables))
</code></pre>

<p>Can somebody please tell me what am i doing wrong or if tf.clip_by_global_norm() does not handle None gradients and i have to take care of them manually</p>

<p>The official documentation seems to agree with @danijar's comments. see <a href=""https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/clip_by_global_norm"" rel=""nofollow noreferrer"">here</a></p>

<blockquote>
  <p>Any of the entries of t_list that are of type None are ignored.</p>
</blockquote>
",2018-04-23 18:38:06,"<p>There's a small problem in your code: you're assigning the return value of <code>tf.clip_by_global_norm</code> to a single variable, when this function returns a pair of values.</p>

<p>The documentation says:</p>

<blockquote>
  <p>Returns:</p>
  
  <p>list_clipped: A list of Tensors of the same type as list_t.</p>
  
  <p>global_norm: A 0-D (scalar) Tensor representing the global norm.</p>
</blockquote>

<p>Hence, the problem arises when you try to apply the gradients to the variables, in the next line.</p>

<p>You can easily fix your code ignoring the global_norm returned value.</p>

<pre><code>gradients, _ = tf.clip_by_global_norm(gradients, 2.5)
</code></pre>
",6546694,5020,https://stackoverflow.com/questions/49987839,Documentation Replicability
50243230,Unable to understand tf.nn.raw_rnn,"<p>In the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn"" rel=""nofollow noreferrer"">official documentation</a> of <code>tf.nn.raw_rnn</code> we have emit structure as the third output of <code>loop_fn</code> when the <code>loop_fn</code> is run for the first time.</p>

<p>Later on the emit_structure is used to copy <code>tf.zeros_like(emit_structure)</code> to the minibatch entries that are finished by <code>emit = tf.where(finished, tf.zeros_like(emit_structure), emit)</code>.</p>

<p>my lack of understanding or lousy documentation on google's part is: emit structure is <code>None</code> so <code>tf.where(finished, tf.zeros_like(emit_structure), emit)</code> is going to throw a ValueError as <code>tf.zeros_like(None)</code> does so. Can somebody please fill in what I am missing here?</p>
",2018-05-08 22:45:02,"<p>Yep, the doc is rather confusing in this place. If you look at the internals of <code>tf.nn.raw_rnn</code>, the key term there is <strong>""in pseudo-code""</strong>, so the example in the doc isn't accurate.</p>

<p>The exact source code looks like this (may differ depending on your tensorflow version):</p>



<pre class=""lang-py prettyprint-override""><code>if emit_structure is not None:
  flat_emit_structure = nest.flatten(emit_structure)
  flat_emit_size = [emit.shape if emit.shape.is_fully_defined() else
                    array_ops.shape(emit) for emit in flat_emit_structure]
  flat_emit_dtypes = [emit.dtype for emit in flat_emit_structure]
else:
  emit_structure = cell.output_size
  flat_emit_size = nest.flatten(emit_structure)
  flat_emit_dtypes = [flat_state[0].dtype] * len(flat_emit_size)
</code></pre>

<p>So it handles the case when <code>emit_structure is None</code> and simply takes the value <code>cell.output_size</code>. That's why nothing really breaks.</p>
",6546694,5020,https://stackoverflow.com/questions/50243230,Documentation Replication on Other Examples
49555016,Compute gradients for each time step of tf.while_loop,"<p>Given a TensorFlow <code>tf.while_loop</code>, how can I calculate the gradient of <code>x_out</code> with respect to all weights of the network for each time step?</p>

<pre><code>network_input = tf.placeholder(tf.float32, [None])
steps = tf.constant(0.0)

weight_0 = tf.Variable(1.0)
layer_1 = network_input * weight_0

def condition(steps, x):
    return steps &lt;= 5

def loop(steps, x_in):
    weight_1 = tf.Variable(1.0)
    x_out = x_in * weight_1
    steps += 1
    return [steps, x_out]

_, x_final = tf.while_loop(
    condition,
    loop,
    [steps, layer_1]
)
</code></pre>

<p><em>Some notes</em></p>

<ol>
<li>In my network the condition is dynamic. Different runs are going to run the while loop a different amount of times.</li>
<li>Calling <code>tf.gradients(x, tf.trainable_variables())</code> crashes with <code>AttributeError: 'WhileContext' object has no attribute 'pred'</code>. It seems like the only possibility to use <code>tf.gradients</code> within the loop is to calculate the gradient with respect to <code>weight_1</code> and the current value of <code>x_in</code> / time step only without backpropagating through time.</li>
<li>In each time step, the network is going to output a probability distribution over actions. The gradients are then needed for a policy gradient implementation.</li>
</ol>
",2018-03-29 11:34:58,"<p>You can't ever call <code>tf.gradients</code> inside <code>tf.while_loop</code> in Tensorflow based on <a href=""https://github.com/tensorflow/tensorflow/issues/14101"" rel=""nofollow noreferrer"">this</a> and <a href=""https://github.com/tensorflow/tensorflow/issues/9450"" rel=""nofollow noreferrer"">this</a>, I found this out the hard way when I was trying to create conjugate gradient descent entirely into the <code>Tensorflow</code> graph.</p>

<p>But if I understand your model correctly, you could make your own version of an <code>RNNCell</code> and wrap it in a <code>tf.dynamic_rnn</code>, but the actual cell 
 implementation will be a little complex since you need to evaluate a condition dynamically at runtime.</p>

<p>For starters, you can take a look at Tensorflow's <code>dynamic_rnn</code> code <a href=""https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/ops/rnn.py"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Alternatively, dynamic graphs have never been <code>Tensorflow</code>'s strong suite, so consider using other frameworks like <code>PyTorch</code> or you can try out <code>eager_execution</code> and see if that helps.</p>
",2393597,579,https://stackoverflow.com/questions/49555016,Documentation Replicability
50263822,Which one to use tf.layers or tf.keras.layers for high level OO code in tensorflow?,"<p>Tensorflow have seemingly similar packages: tf.layers and tf.keras.layers. Which one is preferrable? It seems that tf.keras.layers extends tf.layers.</p>
",2018-05-10 00:26:06,"<p>They are essentially the same, and it doesn't matter which one you use. The <code>tf.keras.layers</code> layers have guaranteed compatibility with the keras API, but that's the only difference.</p>
",250560,10270,https://stackoverflow.com/questions/50263822,Documentation Replicability
50036568,compute the gradient (dy/dx) in tensorflow with the same size of y,"<p>I want to calculate the gradient of dy/dx and get the gradient in size
of y plus the dimensions of x to indicate diffrentiation wrt variable in x.
for example if Y[100x1]=A[100x50]X[50x1] so return Y[100x1x50x1].
(for each parameter in x give me the diffrentiation of Y)</p>

<p>I've tried the tf.gradients operator:
<a href=""https://www.tensorflow.org/api_docs/python/tf/gradients"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/gradients</a></p>

<p>However, it returns the sum(dy/dx) instead of dy/dx</p>
",2018-04-26 06:52:22,"<p>You do not actually want to calculate a gradient, you want to calculate the Jacobian. A gradient is the application of the nabla operator to a scalar function with respect to a vector while the Jacobian is simply a matrix where every element J_ij is the derivative of y_i with respect to x_j, which is what you want.</p>

<p>This is still not implemented in Tensorflow, but has been discussed in detail <a href=""https://github.com/tensorflow/tensorflow/issues/675"" rel=""nofollow noreferrer"">here</a>. You'll probably find a working way for you there.</p>

<p>The basic Idea is to call the <a href=""https://www.tensorflow.org/api_docs/python/tf/gradients"" rel=""nofollow noreferrer""><code>tf.gradients</code></a> with respect to only one variable at a time and then constructing the Jacobian from there manually.</p>
",9651398,27,https://stackoverflow.com/questions/50036568,Documentation Replication on Other Examples
49686860,Side effect in tf.while_loop,"<p>I am currently having a hard time trying to understand how tensorflow works, and I feel like the python interface is somehow obscure.</p>

<p>I recently tried to run a simple print statement inside a tf.while_loop, and there are many things that remains unclear to me:</p>

<pre><code>import tensorflow as tf

nb_iter = tf.constant(value=10)
#This solution does not work at all
#nb_iter = tf.get_variable('nb_iter', shape=(1), dtype=tf.int32, trainable=False)
i = tf.get_variable('i', shape=(), trainable=False,
                     initializer=tf.zeros_initializer(), dtype=nb_iter.dtype)

loop_condition = lambda i: tf.less(i, nb_iter)
def loop_body(i):
    tf.Print(i, [i], message='Another iteration')
    return [tf.add(i, 1)]

i = tf.while_loop(loop_condition, loop_body, [i])

initializer_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(initializer_op)
    res = sess.run(i)
    print('res is now {}'.format(res))
</code></pre>

<p>Notice that if I initialize nb_iter with</p>

<pre><code>nb_iter = tf.get_variable('nb_iter', shape=(1), dtype=tf.int32, trainable=False)
</code></pre>

<p>I got the following error:</p>

<blockquote>
  <p>ValueError: Shape must be rank 0 but is rank 1 for 'while/LoopCond'
  (op: 'LoopCond') with input shapes: [1].</p>
</blockquote>

<p>It get even worse when I try to use the 'i' index for indexing a tensor (example not shown here), I then get the following error</p>

<blockquote>
  <p>alueError: Operation 'while/strided_slice' has been marked as not
  fetchable.</p>
</blockquote>

<p>Can someone point me to a documentation that explains how tf.while_loop works when used with tf.Variables, and if it possible to use side_effects (like print) inside the loop, as well as indexing tensor with the loop variable ?</p>

<p>Thank you in advance for your help</p>
",2018-04-06 06:39:39,"<p>There were actually many things wrong with my first example:</p>

<p>tf.Print is not executed if the operator has no side effect (ie i = tf.Print())</p>

<p>If the boolean, is a scalar, it is then a rank-0 tensor, not a rank-1 tensor. ...</p>

<p>Here is the code that works:</p>

<pre><code>import tensorflow as tf

#nb_iter = tf.constant(value=10)
#This solution does not work at all
nb_iter = tf.get_variable('nb_iter', shape=(), dtype=tf.int32, trainable=False,
                          initializer=tf.zeros_initializer())
nb_iter = tf.add(nb_iter,10)
i = tf.get_variable('i', shape=(), trainable=False,
                     initializer=tf.zeros_initializer(), dtype=nb_iter.dtype)
v = tf.get_variable('v', shape=(10), trainable=False,
                     initializer=tf.random_uniform_initializer, dtype=tf.float32)

loop_condition = lambda i: tf.less(i, nb_iter)
def loop_body(i):
    i = tf.Print(i, [v[i]], message='Another vector element: ')
    return [tf.add(i, 1)]

i = tf.while_loop(loop_condition, loop_body, [i])

initializer_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(initializer_op)
    res = sess.run(i)
    print('res is now {}'.format(res))
</code></pre>

<p>output:</p>

<pre><code>Another vector element: [0.203766704]
Another vector element: [0.692927241]
Another vector element: [0.732221603]
Another vector element: [0.0556482077]
Another vector element: [0.422092319]
Another vector element: [0.597698212]
Another vector element: [0.92387116]
Another vector element: [0.590101123]
Another vector element: [0.741415381]
Another vector element: [0.514917374]
res is now 10
</code></pre>
",2697831,485,https://stackoverflow.com/questions/49686860,Requesting (Additional) Documentation/Examples
49688134,TensorFlow session inside Keras custom loss function,"<p>After going through some Stack questions and the Keras documentation, I manage to write some code trying to evaluate the gradient of the output of a neural network w.r.t its inputs, the purpose being a simple exercise of approximating a bivariate function (<code>f(x,y) = x^2+y^2</code>)  using as loss the difference between analytical and automatic differentiation.</p>

<p>Combining answers from two questions (<a href=""https://stackoverflow.com/questions/46464549"">Keras custom loss function: Accessing current input pattern
</a> and <a href=""https://stackoverflow.com/questions/39561560"">Getting gradient of model output w.r.t weights using Keras
</a>), I came up with this:</p>

<pre><code>import tensorflow as tf
from keras import backend as K
from keras.models import Model
from keras.layers import Dense, Activation, Input

def custom_loss(input_tensor):

    outputTensor = model.output       
    listOfVariableTensors = model.input      
    gradients = K.gradients(outputTensor, listOfVariableTensors)

    sess = tf.InteractiveSession()
    sess.run(tf.initialize_all_variables())
    evaluated_gradients = sess.run(gradients,feed_dict={model.input:input_tensor})

    grad_pred = K.add(evaluated_gradients[0], evaluated_gradients[1])
    grad_true = k.add(K.scalar_mul(2, model.input[0][0]), K.scalar_mul(2, model.input[0][1])) 

    return K.square(K.subtract(grad_pred, grad_true))

input_tensor = Input(shape=(2,))
hidden = Dense(10, activation='relu')(input_tensor)
out = Dense(1, activation='sigmoid')(hidden)
model = Model(input_tensor, out)
model.compile(loss=custom_loss_wrapper(input_tensor), optimizer='adam')
</code></pre>

<p>Which yields the error: <code>TypeError: The value of a feed cannot be a tf.Tensor object.</code> because of <code>feed_dict={model.input:input_tensor}</code>. I understand the error, I just don't know how to fix it. </p>

<p>From what I gathered, I can't simply pass input data into the loss function, it must be a tensor. I realized Keras would 'understand' it when I call <code>input_tensor</code>. This all just leads me to think I'm doing things the wrong way, trying to evaluate the gradient like that. Would really appreciate some enlightenment.</p>
",2018-04-06 08:02:36,"<p>I don't really understand why you want this loss function, but I will provide an answer anyway. Also, there is no need to evaluate the gradient within the function (in fact, you would be ""disconnecting"" the computational graph). The loss function could be implemented as follows:</p>

<pre><code>from keras import backend as K
from keras.models import Model
from keras.layers import Dense, Input

def custom_loss(input_tensor, output_tensor):
    def loss(y_true, y_pred):
        gradients = K.gradients(output_tensor, input_tensor)
        grad_pred = K.sum(gradients, axis=-1)
        grad_true = K.sum(2*input_tensor, axis=-1)
        return K.square(grad_pred - grad_true)
    return loss

input_tensor = Input(shape=(2,))
hidden = Dense(10, activation='relu')(input_tensor)
output_tensor = Dense(1, activation='sigmoid')(hidden)
model = Model(input_tensor, output_tensor)
model.compile(loss=custom_loss(input_tensor, output_tensor), optimizer='adam')
</code></pre>
",1058309,438,https://stackoverflow.com/questions/49688134,Documentation Replicability
49701918,tf.layers.batch_normalization parameters,"<p>I am not sure if it is only me who thinks that tensorflow documentation is a bit weak.</p>

<p>I was planing to use the tf.nn.batch_normalization function to implement batch normalization but later recognized the  tf.layers.batch_normalization function which seemingly should be the one to use for its simplicity. But the documentation is really poor if I may say it.</p>

<p>I am trying to understand how to <em>correctly</em> use it but with the information provided on the Web page is it really not easy. I am hoping that maybe some other people have experience and help me (and possibly many others) to understand it.. </p>

<p>Let me share the interface first:</p>

<pre><code>tf.layers.batch_normalization(
    inputs,
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer=tf.zeros_initializer(),
    gamma_initializer=tf.ones_initializer(),
    moving_mean_initializer=tf.zeros_initializer(),
    moving_variance_initializer=tf.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    training=False,
    trainable=True,
    name=None,
    reuse=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=None,
    virtual_batch_size=None,
    adjustment=None
)
</code></pre>

<p>Q1) beta values are initialized to zero and gamma values are initialized to 1. But it does not say why. When batch normalization used, I understand that the ordinary bias parameter of the neural network becomes obsolete and beta parameter in the batch normalization step kind of does the same thing. From that angle, setting beta to zero is understandable. But why are gamma values initialized to 1? Is that really the most efficient way?</p>

<p>Q2) I see a momentum parameter there as well. The documentation just says "" Momentum for the moving average."". I assume that this parameter is used when calculating the ""mean"" value for a certain mini batch in the corresponding hidden layer. With other words, the mean value used in batch normalization is NOT the mean of current mini batch, it is rather primarily the mean of the last 100 mini batches (since momentum = 0.99). But it is very unclear how this parameter affects the execution in testing, or if I am just validating my model on the dev set by calculating cost and accuracy. My <em>assumption</em> is that anytime I deal with test and dev sets, I set the parameter ""training"" to False so that momentum parameter becomes obsolete for that particular execution and the ""mean"" and ""variance"" values that were calculated during the training are used now instead of calculating new mean and variance values. It is how it should be if I am mistaken but I do not see anything in the documentation if it is the case. Could anyone confirm that my understanding correct? If not, I would really appreciate further explanation on this.</p>

<p>Q3) I am having difficulties to give a meaning to the trainable parameter. I assume beta and gamma params are meant here. Why would they not be trainable?</p>

<p>Q4) The ""reuse"" parameter. What is it really?</p>

<p>Q5) adjustment parameter. Another mistery..</p>

<p>Q5) A kind of summary question.. Here is my overall assumption that needs confirmation and feedback.. Important params here are:
- inputs
- axis
- momentum
- center
- scale
- training
And I assume that as long as the training=True when training, we are safe. And as long as training=False when validating dev set or test set or even when using the model in real life, we are safe too.</p>

<p>Any feedback will really be appreciated.</p>

<p>ADDENDUM:</p>

<p>Confusion continues. Help!</p>

<p>I am trying to use this function instead of implementing a batch normalizer manually. I have the following forward propagation function that loops through layers of the NN.</p>

<pre><code>def forward_propagation_with_relu(X, num_units_in_layers, parameters, 
                                  normalize_batch, training, mb_size=7):

    L = len(num_units_in_layers)

    A_temp = tf.transpose(X)

    for i in range (1, L):
        W = parameters.get(""W""+str(i))
        b = parameters.get(""b""+str(i))
        Z_temp = tf.add(tf.matmul(W, A_temp), b)

        if normalize_batch:
            if (i &lt; (L-1)):  
                with tf.variable_scope(""batch_norm_scope"", reuse=tf.AUTO_REUSE):
                    Z_temp = tf.layers.batch_normalization(Z_temp, axis=-1, 
                                                           training=training)

        A_temp = tf.nn.relu(Z_temp)

    return Z_temp   #This is the linear output of last layer
</code></pre>

<p>The tf.layers.batch_normalization(..) function wants to have static dimensions but I do not have it in my case.</p>

<p>Since I apply mini batches rather than training the entire train set each time before I run the optimizer, 1 dimension of the X appears to be unknown.</p>

<p>If I write:</p>

<pre><code>print(X.shape)
</code></pre>

<p>I get:</p>

<pre><code>(?, 5)
</code></pre>

<p>And when this is the case, when I run the whole program I get the following error below.</p>

<p>I saw in some other threads that some people say that they could solve the problem by using tf.reshape function. I try it.. Forward prop goes fine but later on it crashes in the Adam Optimizer..</p>

<p>Here is what I get when I run the code above (without using tf.reshape):</p>

<p>How do I solve this???</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-191-990fb7d7f7f6&gt; in &lt;module&gt;()
     24 parameters = nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs,
     25                       normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers,
---&gt; 26                       lambd, print_progress)
     27 
     28 print(parameters)

&lt;ipython-input-190-59594e979129&gt; in nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs, normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers, lambd, print_progress)
     34         # Forward propagation: Build the forward propagation in the tensorflow graph
     35         ZL = forward_propagation_with_relu(X_mini_batch, num_units_in_layers, 
---&gt; 36                                            parameters, normalize_batch, training)
     37 
     38     with tf.name_scope(""calc_cost""):

&lt;ipython-input-187-8012e2fb6236&gt; in forward_propagation_with_relu(X, num_units_in_layers, parameters, normalize_batch, training, mb_size)
     15                 with tf.variable_scope(""batch_norm_scope"", reuse=tf.AUTO_REUSE):
     16                     Z_temp = tf.layers.batch_normalization(Z_temp, axis=-1, 
---&gt; 17                                                            training=training)
     18 
     19         A_temp = tf.nn.relu(Z_temp)

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py in batch_normalization(inputs, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, training, trainable, name, reuse, renorm, renorm_clipping, renorm_momentum, fused, virtual_batch_size, adjustment)
    775       _reuse=reuse,
    776       _scope=name)
--&gt; 777   return layer.apply(inputs, training=training)
    778 
    779 

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py in apply(self, inputs, *args, **kwargs)
    805       Output tensor(s).
    806     """"""
--&gt; 807     return self.__call__(inputs, *args, **kwargs)
    808 
    809   def _add_inbound_node(self,

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    676           self._defer_regularizers = True
    677           with ops.init_scope():
--&gt; 678             self.build(input_shapes)
    679           # Create any regularizers added by `build`.
    680           self._maybe_create_variable_regularizers()

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py in build(self, input_shape)
    251       if axis_to_dim[x] is None:
    252         raise ValueError('Input has undefined `axis` dimension. Input shape: ',
--&gt; 253                          input_shape)
    254     self.input_spec = base.InputSpec(ndim=ndims, axes=axis_to_dim)
    255 

ValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(6), Dimension(None)]))
</code></pre>

<p>This is so hopeless.. </p>

<p>ADDENDUM(2)</p>

<p>I am adding more information:</p>

<p>The following simply means that there are 5 units in input layer, 6 units in each hidden layer, and 2 units in output layer.</p>

<pre><code>num_units_in_layers = [5,6,6,2] 
</code></pre>

<p>Here is the updated version of forward prop function with tf.reshape</p>

<pre><code>def forward_propagation_with_relu(X, num_units_in_layers, parameters, 
                                  normalize_batch, training, mb_size=7):

    L = len(num_units_in_layers)
    print(""X.shape before reshape: "", X.shape)             # ADDED LINE 1
    X = tf.reshape(X, [mb_size, num_units_in_layers[0]])   # ADDED LINE 2
    print(""X.shape after reshape: "", X.shape)              # ADDED LINE 3
    A_temp = tf.transpose(X)

    for i in range (1, L):
        W = parameters.get(""W""+str(i))
        b = parameters.get(""b""+str(i))
        Z_temp = tf.add(tf.matmul(W, A_temp), b)

        if normalize_batch:
            if (i &lt; (L-1)):  
                with tf.variable_scope(""batch_norm_scope"", reuse=tf.AUTO_REUSE):
                    Z_temp = tf.layers.batch_normalization(Z_temp, axis=-1, 
                                                           training=training)

        A_temp = tf.nn.relu(Z_temp)

    return Z_temp   #This is the linear output of last layer
</code></pre>

<p>When I do this, I can run the forward prop function. But it seems to be crashing in later execution. Here is the error that I get. (Note that I print out the shape of input X before and after reshaping in the forward prop function).</p>

<pre><code>X.shape before reshape:  (?, 5)
X.shape after reshape:  (7, 5)

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1349     try:
-&gt; 1350       return fn(*args)
   1351     except errors.OpError as e:

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1328                                    feed_dict, fetch_list, target_list,
-&gt; 1329                                    status, run_metadata)
   1330 

~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    515             compat.as_text(c_api.TF_Message(self.status.status)),
--&gt; 516             c_api.TF_GetCode(self.status.status))
    517     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: Incompatible shapes: [7] vs. [2]
     [[Node: forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub = Sub[T=DT_FLOAT, _class=[""loc:@batch_norm_scope/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward_prop/batch_norm_scope/batch_normalization/cond_2/Switch_1:1, forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1:1)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-222-990fb7d7f7f6&gt; in &lt;module&gt;()
     24 parameters = nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs,
     25                       normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers,
---&gt; 26                       lambd, print_progress)
     27 
     28 print(parameters)

&lt;ipython-input-221-59594e979129&gt; in nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs, normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers, lambd, print_progress)
     88                                                                         cost_mini_batch,
     89                                                                         accuracy_mini_batch],
---&gt; 90                                                                         feed_dict={training: True})
     91                       nr_of_minibatches += 1
     92                       sum_minibatch_costs += minibatch_cost

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1126     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1127       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1128                              feed_dict_tensor, options, run_metadata)
   1129     else:
   1130       results = []

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1342     if handle is None:
   1343       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-&gt; 1344                            options, run_metadata)
   1345     else:
   1346       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1361         except KeyError:
   1362           pass
-&gt; 1363       raise type(e)(node_def, op, message)
   1364 
   1365   def _extend_graph(self):

InvalidArgumentError: Incompatible shapes: [7] vs. [2]
     [[Node: forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub = Sub[T=DT_FLOAT, _class=[""loc:@batch_norm_scope/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward_prop/batch_norm_scope/batch_normalization/cond_2/Switch_1:1, forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1:1)]]

Caused by op 'forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub', defined at:
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py"", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py"", line 478, in start
    self.io_loop.start()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2850, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-222-990fb7d7f7f6&gt;"", line 26, in &lt;module&gt;
    lambd, print_progress)
  File ""&lt;ipython-input-221-59594e979129&gt;"", line 36, in nn_model
    parameters, normalize_batch, training)
  File ""&lt;ipython-input-218-62e4c6126c2c&gt;"", line 19, in forward_propagation_with_relu
    training=training)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 777, in batch_normalization
    return layer.apply(inputs, training=training)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 807, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 697, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 602, in call
    lambda: self.moving_mean)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/utils.py"", line 211, in smart_cond
    return control_flow_ops.cond(pred, true_fn=fn1, false_fn=fn2, name=name)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1985, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1839, in BuildCondBranch
    original_result = fn()
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 601, in &lt;lambda&gt;
    lambda: _do_update(self.moving_mean, new_mean),
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 597, in _do_update
    var, value, self.momentum, zero_debias=False)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/training/moving_averages.py"", line 87, in assign_moving_average
    update_delta = (variable - value) * decay
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 778, in _run_op
    return getattr(ops.Tensor, operator)(a._AsTensor(), *args)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 934, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4819, in _sub
    ""Sub"", x=x, y=y, name=name)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3267, in create_op
    op_def=op_def)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Incompatible shapes: [7] vs. [2]
     [[Node: forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub = Sub[T=DT_FLOAT, _class=[""loc:@batch_norm_scope/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward_prop/batch_norm_scope/batch_normalization/cond_2/Switch_1:1, forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1:1)]]
</code></pre>

<p>Regarding the question why the shape of X is not static.. I don't know...
HEre is how I setup the dataset.</p>

<pre><code>with tf.name_scope(""next_train_batch""):
    filenames = tf.placeholder(tf.string, shape=[None])
    dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv))
    dataset = dataset.shuffle(buffer_size=1000)
    dataset = dataset.batch(minibatch_size)
    iterator = dataset.make_initializable_iterator()
    X_mini_batch, Y_mini_batch = iterator.get_next()
</code></pre>

<p>I have 2 csv files that include the train data.</p>

<pre><code>train_path1 = ""train1.csv""
train_path2 = ""train2.csv""
train_input_paths = [train_path1, train_path2]
</code></pre>

<p>And I use the initializable iterator as following:</p>

<pre><code>sess.run(iterator.initializer, 
         feed_dict={filenames: train_input_paths})
</code></pre>

<p>During the training, I keep getting mini batches from the train set. Everything works fine when I disable batch normalization. If I enable batch norm, it requires static shape of the input X (mini batch). I reshape it but this time it crashes later in the execution as seen above. </p>

<p>ADDENDUM(3)</p>

<p>I guess I figured out where it crashes. It probably crashes when I run the optimizer after calculating the cost.</p>

<p>First the sequence of commands:
First forward prop, then compute cost, then run optimizer. First 2 seems to be working but not the optimizer.</p>

<p>HEre is how I define the optimizer:</p>

<pre><code>with tf.name_scope(""train""):
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):        
        # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.
        optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_mini_batch)
</code></pre>

<p>I have the update_ops there to be able to update the moving averages. If I interpret it right, it is just crashing when it tries to update moving averages. I might be misinterpreting the error msg as well.. </p>

<p>ADDENDUM(4)</p>

<p>I tried to normalize based on the known dimension and it worked! But that's not the dimension I would like to normalize, which is now confusing. Let me elaborate:</p>

<p>nr of units in input layer: 5
nr of units in layer 1 (first hidden layer): 6
so weight1 is (6, 5) matrix
Assume that mini batch size is 7.
Shape of A[0] (or X_mini_batch) in my case is: (7, 5), where 7 is the # training samples in mini batch, and 5 is the # units in input layer.</p>

<p>When calculating Z[1]...
Z[1] = weight1 * A[0].transpose
... then shape of Z[1] is (6, 7) matrix, where each column gives 6 features for each train sample.</p>

<p>The question is then which column do we want to normalize in Z[1]? What makes sense to me is that you normalize each feature from all given train samples. This means that I need to normalize each row bcz I have different feature values for different train examples in each row. And since Z[1] has the shape (6, 7), if I set axis=0, it should refer to normalization in each row. And 7 is the unknown number in my case so it doesn't hurt. Based on this logic, it works! But I am totally puzzled if axis=0 really refers to each row here... Let me show another example about this axis issue, which has bothered me for a long time now..</p>

<p>The <em>irrelevant from this topic</em> code example:</p>

<pre><code>cc = tf.constant([[1.,2.,3.], 
                  [4.,5.,6.]])

with tf.Session() as sess:
    print(sess.run(tf.reduce_mean(cc, axis=0)))
    print(sess.run(tf.reduce_mean(cc, axis=1)))  
</code></pre>

<p>This gives the following output:</p>

<pre><code>[2.5 3.5 4.5]
[2. 5.]
</code></pre>

<p>When I set axis to 0, it is giving the average of each column. And if axis=1, it is giving the average of each row.</p>

<p>(Note that cc.shape gives (2,3))</p>

<p>Now the million dollar question: In a 2 dimensional matrix, is axis 0 or 1 when I want to address each row?</p>

<p>ADDENDUM(5)
I guess I get it now correctly. Let me summarize my axis understanding here. Hopefully I am getting it right now...</p>

<p>Here is the Z[1] matrix representation with the shape (6,7):</p>

<p>t_ex :   train example
f:       feature</p>

<pre><code>t_ex1   t_ex2   t_ex3   t_ex4   t_ex5   t_ex6   t_ex7
  f1      f1      f1      f1      f1      f1      f1
  f2      f2      f2      f2      f2      f2      f2
  f3      f3      f3      f3      f3      f3      f3
  f4      f4      f4      f4      f4      f4      f4
  f5      f5      f5      f5      f5      f5      f5
  f6      f6      f6      f6      f6      f6      f6
</code></pre>

<p>In this mini batch above, there are 7 train examples and each train ex has 6 features (since there are 6 units in layer 1). When we say ""tf.layers.batch_normalization(..,axis=0)"", we mean that the normalization has to be done per row for each feature to eliminate the high variance between - say - f1 values in the first row.</p>

<p>With other words, we do NOT normalize f1,f2,f3,f4,f5,f6 with each other. We normalize f1:s with each other, and f2:s with each other, and so on..</p>
",2018-04-06 22:44:37,"<p>Q1) Initializing gamma as 1, beta as 0 means directly using the normalized inputs. Since there is no prior information about what the variance of a layer output should be, it is fair enough to assume standard Gaussian.</p>

<p>Q2) During training phase (<code>training=True</code>), the batch is normalized with their own mean and var, assuming that training data are randomly sampled. During test (<code>training=False</code>), since the test data could be arbitrarily sampled, we cannot use their mean and var. Thus, we use, as you said, the moving averaging estimations from the last ""100"" training iterations.</p>

<p>Q3) Yes, trainable refers to <code>beta</code> and <code>gamma</code>. There are cases to set <code>trainable=False</code>, e.g. if a novel method is used to update the parameters, or if the batch_norm layer is pre-trained and needs to be frozen. </p>

<p>Q4) You may have noticed <code>reuse</code> parameters in other <code>tf.layers</code> functions as well. In general, if you wanna call a layer more than once (e.g. training and validation) and you do not wanna TensorFlow to think that you are creating a new layer, you set <code>reuse=True</code>. I prefer <code>with tf.variable_scope(..., reuse=tf.AUTO_REUSE):</code> to achieve the same purpose.</p>

<p>Q5) I am not sure about this one. I guess it is for users who want to design new tricks to adjust the scale and bias.</p>

<p>Q6) Yes, you are right.</p>
",9328846,2063,https://stackoverflow.com/questions/49701918,Lack of Alternative Solutions/Documentation
49370940,One hot encoding characters,"<p>Is there a possibilty to one-hot encode characters of a text in Tensorflow or Keras?</p>

<ul>
<li><code>tf.one_hot</code> seem to take only integers.</li>
<li><code>tf.keras.preprocessing.text.one_hot</code> seems to one-hot encode sentences
to words, but not to characters.</li>
</ul>

<p>Beside that, <code>tf.keras.preprocessing.text.one_hot</code> works really strange, since the response does not really seem one-hot encoded, since the following code:</p>

<pre><code>text = ""ab bba bbd""
res = tf.keras.preprocessing.text.one_hot(text=text,n=3)
print(res)
</code></pre>

<p>Lead to this result:</p>

<pre><code>[1,2,2]
</code></pre>

<p>Every time I run this program, the output is a different 3d vector, sometimes it is <code>[1,1,1]</code> or <code>[2,1,1]</code>. The documentation says, that unicity is not guaranteed, but this seems really senseless to me.</p>
",2018-03-19 19:30:14,"<p>I found a nice answer based on pure python, unfortunately I do not find the source anymore. It first converts every char to an int, and then replaces the int with an one-hot array. It has unicity over the whole program, even over all programms if the alphabet is the same length and the same order.</p>

<pre><code>    # Is the alphabet of all possible chars you want to convert
    alphabet = ""abcdefghijklmnopqrstuvwxyz0123456789""

    def convert_to_onehot(data):
        #Creates a dict, that maps to every char of alphabet an unique int based on position
        char_to_int = dict((c,i) for i,c in enumerate(alphabet))
        encoded_data = []
        #Replaces every char in data with the mapped int
        encoded_data.append([char_to_int[char] for char in data])
        print(encoded_data) # Prints the int encoded array

        #This part now replaces the int by an one-hot array with size alphabet
        one_hot = []
        for value in encoded_data:
            #At first, the whole array is initialized with 0
            letter = [0 for _ in range(len(alphabet))]
            #Only at the number of the int, 1 is written
            letter[value] = 1
            one_hot.append(letter)
        return one_hot

   print(convert_to_onehot(""hello world""))
</code></pre>
",3921232,607,https://stackoverflow.com/questions/49370940,Documentation Ambiguity
49845685,Tensorflow: NaN for custom softmax,"<p>Simply exchanging the <strong>nn.softmax</strong> function for a combination which uses <strong>tf.exp</strong>, keeping everything else like it was, causes not only the gradients to contain NaN but also the intermediate variable <strong>s</strong>. I have no idea why this is. </p>

<pre><code>tempX = x
tempW = W
tempMult = tf.matmul(tempX, W)
s = tempMult + b

#! ----------------------------
#p = tf.nn.softmax(s)
p = tf.exp(s) / tf.reduce_sum(tf.exp(s), axis=1)
#!------------------------------


myTemp = y*tf.log(p)
cost = tf.reduce_mean(-tf.reduce_sum(myTemp, reduction_indices=1)) + mylambda*tf.reduce_sum(tf.multiply(W,W))

grad_W, grad_b = tf.gradients(xs=[W, b], ys=cost)

new_W = W.assign(W - tf.multiply(learning_rate, grad_W))
new_b = b.assign(b - tf.multiply(learning_rate, grad_b))
</code></pre>
",2018-04-15 19:00:19,"<h1>Answer</h1>

<p><code>tf.exp(s)</code> easily overflows for large <strong>s</strong>.  That's the main reason that <code>tf.nn.softmax</code> doesn't actually use that equation but does <em>something equilivent</em> to it (according to the docs).</p>

<h1>Discussion</h1>

<p>When I rewrote your softmax function to </p>

<pre><code>p = tf.exp(s) / tf.reshape( tf.reduce_sum(tf.exp(s), axis=1), [-1,1] )
</code></pre>

<p>It worked without a problem.</p>

<p>Here is a fully working python 2.7 implementation that uses a hand-crafted softmax and works (using the reshape function)</p>

<pre><code># -- imports --
import tensorflow as tf
import numpy as np

# np.set_printoptions(precision=1) reduces np precision output to 1 digit
np.set_printoptions(precision=2, suppress=True)

# -- constant data --
x = [[0., 0.], [1., 1.], [1., 0.], [0., 1.]]
y_ = [[1., 0.], [1., 0.], [0., 1.], [0., 1.]]

# -- induction --
# 1x2 input -&gt; 2x3 hidden sigmoid -&gt; 3x1 sigmoid output

# Layer 0 = the x2 inputs
x0 = tf.constant(x, dtype=tf.float32)
y0 = tf.constant(y_, dtype=tf.float32)

# Layer 1 = the 2x3 hidden sigmoid
m1 = tf.Variable(tf.random_uniform([2, 3], minval=0.1, maxval=0.9, dtype=tf.float32))
b1 = tf.Variable(tf.random_uniform([3], minval=0.1, maxval=0.9, dtype=tf.float32))
h1 = tf.sigmoid(tf.matmul(x0, m1) + b1)

# Layer 2 = the 3x2 softmax output
m2 = tf.Variable(tf.random_uniform([3, 2], minval=0.1, maxval=0.9, dtype=tf.float32))
b2 = tf.Variable(tf.random_uniform([2], minval=0.1, maxval=0.9, dtype=tf.float32))
h2 = tf.matmul(h1, m2) + b2
y_out = tf.exp(h2) / tf.reshape( tf.reduce_sum(tf.exp(h2), axis=1) , [-1,1] )


# -- loss --

# loss : sum of the squares of y0 - y_out
loss = tf.reduce_sum(tf.square(y0 - y_out))

# training step : gradient decent (1.0) to minimize loss
train = tf.train.GradientDescentOptimizer(1.0).minimize(loss)


# -- training --
# run 500 times using all the X and Y
# print out the loss and any other interesting info
#with tf.Session() as sess:
sess = tf.Session()
sess.run(tf.global_variables_initializer())
print ""\nloss""
for step in range(500):
    sess.run(train)
    if (step + 1) % 100 == 0:
        print sess.run(loss)

results = sess.run([m1, b1, m2, b2, y_out, loss])
labels = ""m1,b1,m2,b2,y_out,loss"".split("","")
for label, result in zip(*(labels, results)):
    print """"
    print label
    print result

print """"
</code></pre>

<p>Perhaps your initial values for M and b are too <em>large</em>.  I tried re-running my above code but with with weights initialized to large numbers and I was able to reproduce your NaN issue.</p>
",4308982,429,https://stackoverflow.com/questions/49845685,Documentation Replicability
49969957,Tensorflow Adam Optimizer,"<p>Okey I have been reading some of the posts regarding AdamOptimizer in tensorflow. I think there is some confusion around, at least among beginners in NNs like me.</p>

<p>If I understood correctly, tf.train.AdamOptimizer keeps a so-called ""adaptative learning rate"". I thought that this learning rate would grow smaller as time increases.</p>

<p>However, when I plot the function by which the learning rate is scaled, taken from the <a href=""https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/train/AdamOptimizer"" rel=""nofollow noreferrer"">docs</a>,</p>

<pre><code>t &lt;- t + 1
lr_t &lt;- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)
</code></pre>

<p>this is what I get:</p>

<pre><code>t = np.arange(200)
result = np.sqrt(1-0.999**t)/(1-0.9**t)
plt.plot(result)
plt.show
</code></pre>

<p><a href=""https://i.stack.imgur.com/G2BEK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G2BEK.png"" alt=""enter image description here""></a></p>

<p>So, for t = 1, the value for the user-selected learning rate is multiplied by 0.3 Then it decreases quite fast until 0.15 of its value, and then increases with time, slowly approaching the limit = user-selected learning rate. </p>

<p>Isn't it a bit weird? I guess somewhere I am wrong, but I would've expected the learning rate to start at a higher value and then progressively decreasing towards smaller values. </p>
",2018-04-22 19:35:02,"<p>You can not really plot the Adam learning rate like this, since Adam is a momentum optimizer. The applied gradient for each steps depends on a moving average of the mean and standard deviation of the gradients of previous steps.</p>

<p>In general there is no guarantee for the learning to converge, the raw learning rate <code>alpha</code> itself is not directly changed by Adams. It is only rescaled using the momentums of the gradient. The learning only converges well if mean and standard deviation of the gradient decrease over time when reaching the global minimum, which is often the case for simple neural networks. </p>

<p>For highly stochastic problems however one might still need to implement some form of learning rate decay to suppress 'oscillations' around the optimal parameters, or at least make them smaller to make sure there really is convergence.</p>

<p>If you really want to understand how exactly this works you might want to read the Adam <a href=""https://arxiv.org/abs/1412.6980"" rel=""nofollow noreferrer"">paper</a>, it is much simpler than it seems on first sight.</p>
",9527947,487,https://stackoverflow.com/questions/49969957,Documentation Ambiguity
50246535,Tensorflow estimator input function: defining each feature or not?,"<p>With <code>x</code> is a 120 x 4 feature matrix of Iris data (4 features) and <code>y</code> is a label, I can make an input function for <code>tf.estimator</code> like below </p>

<pre><code>def input_function(x, y):
    dict_x = {
        ""sepal_length"" : x[:,0],
        ""sepal_width"" :  x[:,1],
        ""petal_length"" : x[:,2],
        ""petal_width"" :  x[:,3]
    }

    dataset = tf.data.Dataset.from_tensor_slices((
        dict_x, y
    ))

    return dataset
</code></pre>

<p>then define the feature column like below:</p>

<pre><code>feature_columns = [
    tf.feature_column.numeric_column(key=""sepal_length""),
    tf.feature_column.numeric_column(key=""sepal_width""),
    tf.feature_column.numeric_column(key=""petal_length""),
    tf.feature_column.numeric_column(key=""petal_width"")
]
</code></pre>

<p>But, I found in the internet (I forget the source, still searching) that I also can define the input function like below. The difference with previous method is all four features now defined with only one key, <code>""x""</code>.</p>

<pre><code>def input_function(x, y):
    dict_x = {
        ""x"" : x,
    }

    dataset = tf.data.Dataset.from_tensor_slices((
        dict_x, y
    ))

    return dataset
</code></pre>

<p>then define the feature column like below:</p>

<pre><code>feature_columns = [
    tf.feature_column.numeric_column(key=""x"",shape=4),
]
</code></pre>

<p>I've run both method and both give almost same result. <strong>My question</strong>: I can't find any documentation that explain the difference between both method, because at a glance <code>dict_x</code> have different shape. Are they still treated equally at input layer on neural networks?</p>

<p>I'm new using <code>tf.estimator</code>, Thank You</p>

<p>My estimator code if needed:</p>

<pre><code>classifier = tf.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[10],
    n_classes=3,
    optimizer=tf.train.GradientDescentOptimizer(0.001),
    activation_fn=tf.nn.relu
)

# Train the model
classifier.train(
    input_fn=lambda:input_function(xtrain, ytrain, True)
)
</code></pre>
",2018-05-09 06:07:16,"<p>In case of <code>numeric_column</code> with same <code>dtype</code>'s the only difference is shape of the resultant input:</p>

<p>Option 1 creates input of shape: <code>[120,4,1]</code>: 120 samples, each represented by 4 vectors of 1 number.</p>

<p>Whereas option 2 creates input of shape: <code>[120,1,4]</code>: 120 samples, each represented by a single vector consisting of 4 numbers.</p>

<p>In the end, it does not really matter because both get flattened to <code>[120,4]</code> before being fed to the network.</p>

<hr>

<p>First I created the features.</p>

<pre><code>features1 = {
    'sepal_length' : np.random.rand(120),
    'sepal_width': np.random.rand(120),
    'petal_length': np.random.rand(120),
    'petal_width': np.random.rand(120)
}

features2 = {
    'everything' : np.random.rand(120, 4)
}
</code></pre>

<p>
Then I prepared the feature columns -- same as you did.</p>

<pre><code>feature_columns1 = [
    tf.feature_column.numeric_column(key=""sepal_length""),
    tf.feature_column.numeric_column(key=""sepal_width""),
    tf.feature_column.numeric_column(key=""petal_length""),
    tf.feature_column.numeric_column(key=""petal_width"")
]

feature_columns2 = [
    tf.feature_column.numeric_column(key=""everything"", shape=4),
]
</code></pre>

<p>
Now, to see what exactly is done with them when they're fed to the network we can use the <code>feature_column.input_layer()</code>.</p>

<pre><code>inputs1 = tf.feature_column.input_layer(features1, feature_columns1)
inputs2 = tf.feature_column.input_layer(features2, feature_columns2)
</code></pre>

<p>
And as we can see, both ways produced the same shape.</p>

<pre><code>with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    res1 = sess.run(inputs1)
    res2 = sess.run(inputs2)
</code></pre>

 

<pre><code>print(res1.shape)
print(res2.shape)
(120, 4)
(120, 4)
</code></pre>
",2147347,3145,https://stackoverflow.com/questions/50246535,Lack of Alternative Solutions/Documentation
50967885,"tf.gradients, how can I understand `grad_ys` and use it?","<p>In <code>tf.gradients</code>,  there is a keyword argument <code>grad_ys</code></p>

<blockquote>
  <p><code>grad_ys</code> is a list of tensors of the same length as <code>ys</code> that holds the initial gradients for each <code>y</code> in <code>ys</code>. When <code>grad_ys</code> is None, we fill in a tensor of 1s of the shape of <code>y</code> for each <code>y</code> in <code>ys</code>. A user can provide their own initial <code>grad_ys</code> to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).</p>
</blockquote>

<p>Why is <code>grads_ys</code> needed here? The docs here is implicit. Could you please give some specific purpose and code?</p>

<p>And my example code for <code>tf.gradients</code> is</p>

<pre class=""lang-py prettyprint-override""><code>In [1]: import numpy as np

In [2]: import tensorflow as tf

In [3]: sess = tf.InteractiveSession()

In [4]: X = tf.placeholder(""float"", shape=[2, 1])

In [5]: Y = tf.placeholder(""float"", shape=[2, 1])

In [6]: W = tf.Variable(np.random.randn(), name='weight')

In [7]: b = tf.Variable(np.random.randn(), name='bias')

In [8]: pred = tf.add(tf.multiply(X, W), b)

In [9]: cost = 0.5 * tf.reduce_sum(tf.pow(pred-Y, 2))

In [10]: grads = tf.gradients(cost, [W, b])

In [11]: sess.run(tf.global_variables_initializer())

In [15]: W_, b_, pred_, cost_, grads_ = sess.run([W, b, pred, cost, grads], 
                                    feed_dict={X: [[2.0], [3.]], Y: [[3.0], [2.]]})
</code></pre>
",2018-06-21 11:49:15,"<p><code>grad_ys</code> is only needed for advanced use cases. Here is how you can think about it.</p>

<p><code>tf.gradients</code> allows you to compute <code>tf.gradients(y, x, grad_ys) = grad_ys * dy/dx</code>. In other words, <code>grad_ys</code> is the multiplier of each <code>y</code>. In this notation, it seems silly to provide this argument because one should be able to just multiple himself, i.e. <code>tf.gradients(y, x, grad_ys) = grad_ys * tf.gradients(y, x)</code>. Unfortunately, this equality does not hold because when computing gradients backwards, we perform reduction (typically summation) after each step to get ""intermediate loss"".</p>

<p>This functionality can be useful in many cases. One is mentioned in the doc string. Here is another. Remember the chain rule - <code>dz/dx = dz/dy * dy/dx</code>. Let's say that we wanted to compute <code>dz/dx</code> but <code>dz/dy</code> is not differentiable and we can only approximate it. Let's say we compute the approximation somehow and call it <code>approx</code>. Then, <code>dz/dx = tf.gradients(y, x, grad_ys=approx)</code>.</p>

<p>Another use case can be when you have a model with a ""huge fan-in"". Let's say you have 100 input sources that go through a few layers (call these ""100 branches""), get combined at <code>y</code>, and go through 10 more layers until you get to a <code>loss</code>. It might be that computing all the gradients (which requires remembering many activations) for the whole model at once does not fit in memory. One way to do this would be to compute <code>d(loss)/dy</code> first. Then, compute the gradients for variables in <code>branch_i</code> with respect to <code>loss</code> using <code>tf.gradients(y, branch_i_variables, grad_ys=d(loss)/dy)</code>. Using this (and a few more details I am skipping) you can reduce the peak memory requirement.</p>
",5046896,17040,https://stackoverflow.com/questions/50967885,Documentation Replicability
49997294,Moving away from tf.contrib.learn: distributed training with dedicated evaluator process,"<p>In TF 1.8's upcoming release, <code>tf.contrib.learn.*</code> will be deprecated.
The <code>tf.contrib.learn.Experiment</code> class recommends switching to <code>tf.estimator.train_and_evaluate</code> instead, so I'm trying to port my code to that framework.</p>

<p>What I want to do is set up distributed training on two machines' GPUs, plus a third CPU-only process that does continuous evaluation on a small validation set.</p>

<p>Following the examples in <a href=""https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer"">the documentation of <code>train_and_evaluate</code></a> and the <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">Distributed Tensorflow</a> guide, I managed to set up the training half of my desired architecture, but I can't find a way to set up an estimator.</p>

<p>So far, what I have looks as follows:</p>

<pre><code>def input_fn(mode, num_classes, batch_size):  
  # [...] build input pipeline
  return {'input': images}, labels

def model_fn(features, labels, num_classes, mode):
  # [...] build model
  return tf.estimator.EstimatorSpec(
    mode=mode,
    predictions=predictions,
    loss=total_loss,
    train_op=train_op,
    eval_metric_ops=metrics,
    export_outputs=export_outputs)

def distributed_main_v2(unused_argv):
  """"""Expects `unused_argv` to be a list ['&lt;task_type&gt;', '&lt;task_id&gt;']""""""  
  import json
  # Set up environment variables according to the parameters passed to the process
  TF_CONFIG = {
    'cluster': {
        ""ps"": [
            ""host1:2222"",
        ],
        ""chief"": [
            ""host1:2223"",
            ],
        ""worker"": [
            ""host2:2224""
            ]
    },
    'environment': 'cluster',    
    'task': {
        'type': unused_argv[1].strip(),
        'index': unused_argv[2].strip() if len(unused_argv) &gt; 2 else 0
        }
  }
  os.environ['TF_CONFIG'] = json.dumps(TF_CONFIG)
  if unused_argv[1].strip() not in ['worker', 'chief']:
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # leave the GPU to the worker process

  # create the estimator
  # define warm start configuration
  regex = '^(?!.*final_layer*|.*aux_logits*)'
  ws_settings = tf.estimator.WarmStartSettings('checkpoint_path', regex)

  gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.95) # fix for cuDNN fatal memory error with tf.contrib.learn.Experiment (TODO: still necessary?)
  sess_conf = tf.ConfigProto(gpu_options=gpu_opts)
  run_conf = tf.estimator.RunConfig(session_config=sess_conf)

  # Create the Estimator
  estimator = tf.estimator.Estimator(
    model_fn=lambda features, labels, mode: model_fn(features, labels, NUM_CLASSES, mode),
    model_dir=model_dir,
    config=run_conf,
    warm_start_from=ws_settings)

  # Set up input functions for training and evaluation
  train_input_fn = lambda : input_fn(tf.estimator.ModeKeys.TRAIN, NUM_CLASSES, batch_size)
  eval_input_fn = lambda : input_fn(tf.estimator.ModeKeys.EVAL, NUM_CLASSES, batch_size)

  train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=steps)
  eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)

  # start distributed training
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

if __name__ == '__main__':
  # set up globals and parse known arguments
  distributed_main_v2(unused_argv)
</code></pre>

<p>This code works, although my understanding of it is still pretty limited. I get what the PS and workers do, but from the specification of <code>chief</code> I understand this should be the ""master"" worker that also logs summaries and saves checkpoints. What I'm missing now is the periodic evaluation... and I'm at a loss. From the <code>train_and_evaluate</code> codebase I see there's some ""evaluator"" support but I don't understand how to set it up properly.</p>
",2018-04-24 08:51:46,"<p><strong>Note:</strong> While writing the question I eventually realised my mistake (i.e., being blind and not seeing code and documentation that by now I think I looked at 20 times at least), but I believe the question and matching answer might be useful to others so I decided to finish the question and self-answer it.</p>

<p><strong>If I were to read the whole <a href=""https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer"">docs</a> as they are written</strong>, I would have noticed the following: </p>

<blockquote>
  <p>Example of TF_CONFIG for evaluator task. Evaluator is a special task
  that is not part of the training cluster. There could be only one. It
  is used for model evaluation.</p>
</blockquote>

<pre><code># This should be a JSON string, which is set as environment variable. Usually
# the cluster manager handles that.
TF_CONFIG='{
    ""cluster"": {
        ""chief"": [""host0:2222""],
        ""worker"": [""host1:2222"", ""host2:2222"", ""host3:2222""],
        ""ps"": [""host4:2222"", ""host5:2222""]
    },
    ""task"": {""type"": ""evaluator"", ""index"": 0}
}'
</code></pre>

<p>As it turns out, yes, there is indeed support for the evaluation task <em>and using it is a lot easier than I expected</em>.</p>

<p>Just set the <code>""task""</code>part of <code>TF_CONFIG</code> to <code>{""type"": ""evaluator"", ""index"": 0}</code> as shown above and there you have evaluation running. The confusing part for me was <em>""Evaluator is a special task that is not part of the training cluster""</em>. This is, I believe, because the chief worker waits for all workers to register with him when starting the distributed session, so leaving the evaluator out of the cluster keeps training and evaluation independent of each other and makes training agnostic of evaluation.</p>
",3214872,18784,https://stackoverflow.com/questions/49997294,Documentation Replication on Other Examples
50276275,KeyError: u'NearestNeighbors' on loading saved model from tf.contrib.factorization.KMeansClustering,"<p>I am trying to do the following:</p>

<ul>
<li>Run kmeans clustering using tensorflow (1.8.0)</li>
<li>Save the model using <code>kmeans.export_savedmodel</code></li>
<li>Use the model using <code>tf.saved_model.loader.load</code></li>
</ul>

<p>I am using the exact script at: <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering</a></p>

<p>I am using following code for saving the model:</p>

<p>Input Reciever:</p>

<pre><code>def serving_input_receiver_fn():
    feature_spec = {""x"": tf.FixedLenFeature(dtype=tf.float32, shape=[2])}
    model_placeholder = tf.placeholder(dtype=tf.string,shape=[None],name='input')
    receiver_tensors = {""model_inputs"": model_placeholder}
    features = tf.parse_example(model_placeholder, feature_spec)
    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)
</code></pre>

<p>Export:</p>

<pre><code>kmeans.export_savedmodel(""/path/"", serving_input_receiver_fn)
</code></pre>

<p>To import I use:</p>

<pre><code>tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],""/path"")
</code></pre>

<p>On last step I run into this issue:</p>

<p><code>
Traceback (most recent call last):
  File ""restore_model.py"", line 6, in &lt;module&gt;
    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], ""/Users/z001t3k/work/codebase/ContentPipeline/cep-scripts/cep/datacollection/algorithms/cluster_model/1525963476"")
  File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 219, in load
    saver = tf_saver.import_meta_graph(meta_graph_def_to_load, **saver_kwargs)
  File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1955, in import_meta_graph
    **kwargs)
  File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py"", line 743, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 460, in import_graph_def
    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
  File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 227, in _RemoveDefaultAttrs
    op_def = op_dict[node.op]
KeyError: u'NearestNeighbors'</code></p>
",2018-05-10 15:19:04,"<p>Tensorflow is having trouble locating the <code>NearestNeighbors</code> op, which is part of the graph you're loading. Ops defined in contrib are loaded dynamically when you import the corresponding contrib package in Python.</p>

<p>So just add</p>

<pre><code>import tensorflow.contrib.factorization
</code></pre>

<p>before loading the SavedModel.</p>
",781469,189,https://stackoverflow.com/questions/50276275,Documentation Replicability
50329855,How to use the Tensorflow Dataset Pipeline for Variable Length Inputs?,"<p>I am training a Recurrent Neural Network in Tensorflow over a dataset of sequence of numbers of varying lengths and have been trying to use the <code>tf.data</code> API to create an efficient pipeline. However I can't seem to get this thing to work</p>

<h3>My approach</h3>

<p>My data set is a NumPy array of shape <code>[10000, ?, 32, 2]</code> which is saved on my disk as a file in the <code>.npy</code> format. Here the <code>?</code> denotes that elements have variable length in the second dimension. 10000 denotes the number of minibatches in the dataset and 32 denotes the size of a mini-batch.</p>

<p>I am using <code>np.load</code> to open this data set and I am trying to create a <code>tf.data.Dataset</code> object using the <code>from_tensor_slices</code> method but it seems that this only works if all input Tensors have the same shape!</p>

<p>I tried reading the <a href=""https://www.tensorflow.org/programmers_guide/datasets#batching_tensors_with_padding"" rel=""noreferrer"">docs</a> but they have only given a very simple example.</p>

<h3>My code</h3>

<p>So the numpy files have been generated as follows - </p>

<pre><code>dataset = []
for i in xrange(num_items):
  #add an element of shape [?, 32, 2] to the list where `?` takes
  # a random value between [1, 40]
  dataset.append(generate_random_rnn_input())

with open('data.npy', 'w') as f:
  np.save(f, dataset)
</code></pre>

<p>The code given below is my attempt to create a <code>tf.data.Dataset</code> object</p>

<pre><code># dataset_list is a list containing `num_items` number of itesm
# and each item has shape [?, 32, 2]
dataset_list = np.load('data.npy')

# error, this doesn't work!
dataset = tf.data.Dataset.from_tensor_slices(dataset_list)
</code></pre>

<p>The error I get is ""TypeError: Expected binary or unicode string, got array([[[0.0875, 0.    ], ...""</p>

<h3>Continued, still need help!</h3>

<p>So I tried @mrry's answer and I am now able to created a Dataset object. <strong>However</strong>, I am not able to iterate through this dataset using iterators as said in the tutorial. This is what my code looks like now - </p>

<pre><code>dataset_list = np.load('data.npy')

dataset = tf.data.Dataset.from_generator(lambda: dataset_list, 
                                         dataset_list[0].dtype,
                                         tf.TensorShape([None, 32, 2]))

dataset = dataset.map(lambda x : tf.cast(x, tf.float32))

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
  print sess.run(next_element) # The code fails on this line
</code></pre>

<p>The error I get is <code>AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype'</code>. I have no absolutely no clue what this means.</p>

<p>This is the complete stack trace -</p>

<pre><code>2018-05-15 04:19:25.559922: W tensorflow/core/framework/op_kernel.cc:1261] Unknown: exceptions.AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype'
Traceback (most recent call last):

  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/ops/script_ops.py"", line 147, in __call__
    ret = func(*args)

  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 378, in generator_py_func
    nest.flatten_up_to(output_types, values), flattened_types)

AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype'


2018-05-15 04:19:25.559989: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at iterator_ops.cc:891 : Unknown: exceptions.AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype'
Traceback (most recent call last):

  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/ops/script_ops.py"", line 147, in __call__
    ret = func(*args)

  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 378, in generator_py_func
    nest.flatten_up_to(output_types, values), flattened_types)

AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype'


     [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_DOUBLE], token=""pyfunc_1""](arg0)]]
Traceback (most recent call last):
  File ""pipeline_test.py"", line 320, in &lt;module&gt;
    tf.app.run()
  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 126, in run
    _sys.exit(main(argv))
  File ""pipeline_test.py"", line 316, in main
    train(FLAGS.num_training_iterations, FLAGS.report_interval, FLAGS.report_interval_verbose)
  File ""pipeline_test.py"", line 120, in train
    print(sess.run(next_element))
  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 905, in run
    run_metadata_ptr)
  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
    run_metadata)
  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnknownError: exceptions.AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype'
Traceback (most recent call last):

  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/ops/script_ops.py"", line 147, in __call__
    ret = func(*args)

  File ""/home/vastolorde95/virtualenvs/thesis/local/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 378, in generator_py_func
    nest.flatten_up_to(output_types, values), flattened_types)

AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype'


     [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_DOUBLE], token=""pyfunc_1""](arg0)]]
     [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,32,2]], output_types=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
</code></pre>
",2018-05-14 12:04:15,"<p>As you have noticed, <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices"" rel=""noreferrer""><code>tf.data.Dataset.from_tensor_slices()</code></a> only works on objects that can be converted to a (dense) <code>tf.Tensor</code> or a <code>tf.SparseTensor</code>. The easiest way to get variable-length NumPy data into a <code>Dataset</code> is to use <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""noreferrer""><code>tf.data.Dataset.from_generator()</code></a>, as follows:</p>

<pre><code>dataset = tf.data.Dataset.from_generator(lambda: dataset_list, 
                                         tf.as_dtype(dataset_list[0].dtype),
                                         tf.TensorShape([None, 32, 2]))
</code></pre>
",3760132,1819,https://stackoverflow.com/questions/50329855,Documentation Replication on Other Examples
50340211,Calculate values in a vector based on the elements in another vector in Tensorflow,"<p>I have two vectors: time and event. If one event is 1, the time at the same index should be assigned to <code>func_for_event1</code>. Otherwise, it goes to <code>func_for_event0</code>. </p>

<pre><code>import tensorflow as tf

def func_for_event1(t):
    return t + 1

def func_for_event0(t):
    return t - 1

time = tf.placeholder(tf.float32, shape=[None])  # [3.2, 4.2, 1.0, 1.05, 1.8]
event = tf.placeholder(tf.int32, shape=[None])  # [0, 1, 1, 0, 1]

# result: [2.2, 5.2, 2.0, 0.05, 2.8]
# For example, 3.2 should be sent to func_for_event0 because the first element in event is 0.
</code></pre>

<p>How should I implement this logic in Tensorflow? Say <code>tf.cond</code> or <code>tf.where</code>?</p>
",2018-05-14 23:06:27,"<p>This is exactly what <a href=""https://www.tensorflow.org/api_docs/python/tf/where"" rel=""nofollow noreferrer""><code>tf.where()</code></a> is for. This code (tested):</p>



<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

def func_for_event1(t):
    return t + 1

def func_for_event0(t):
    return t - 1

time = tf.placeholder(tf.float32, shape=[None])  # [3.2, 4.2, 1.0, 1.05, 1.8]
event = tf.placeholder(tf.int32, shape=[None])  # [0, 1, 1, 0, 1]

result = tf.where( tf.equal( 1, event ), func_for_event1( time ), func_for_event0( time ) )
# result: [2.2, 5.2, 2.0, 0.05, 2.8]
# For example, 3.2 should be sent to func_for_event0 because the first element in event is 0.

with tf.Session() as sess:
    res = sess.run( result, feed_dict = {
        time : np.array( [3.2, 4.2, 1.0, 1.05, 1.8] ),
        event : np.array( [0, 1, 1, 0, 1] )
    } )
    print ( res )
</code></pre>

<p>outputs:</p>

<blockquote>
  <p>[2.2        5.2        2.         0.04999995 2.8       ]</p>
</blockquote>

<p>as desired.</p>
",1118236,3921,https://stackoverflow.com/questions/50340211,Documentation Replicability
50368759,Reshape one 4D-tensor into 2D whose dimension is None,"<p>I have one question about <code>tf.reshape</code></p>

<p>E.g. one tensor <code>t1</code> whose shape is <code>[None, h, c, w]</code>. I want to reshape the tensor into 2D, just like:</p>

<pre><code>shape = t1.get_shape().as_list()
t2 = tf.reshape(t1, [shape[0]*shape[1], shape[2]*shape[3]])
</code></pre>

<p>However, the first dimension of <code>t1</code> is None. </p>

<p>How could I process this case, any suggestions?</p>
",2018-05-16 10:42:52,"<p>You can do:</p>

<pre><code>shape = tf.shape(t1)
t2 = tf.reshape(t1, [-1, shape[2]*shape[3]])
</code></pre>
",3387141,253,https://stackoverflow.com/questions/50368759,Documentation Replicability
50383462,how to randomly initialize weights in tensorflow?,"<p>in tensorflow, I learned from the tutorial that one would initialize the variables with something like
<code>
sess.run(tf.global_variables_initializer())
</code></p>

<p>however I found that every time I run this with the same input dataset, the loss value starts with the same value.</p>

<p>I presume this is due to the fact that the initialization is always setting up the variables with the same values. (probably zero)</p>

<p>I wish to randomize the values of weights. I've tried searching for this but  tensorflow docs doesn't give a clear answer if the initialization is done with zero values by default or random values.</p>

<p>How can I specify the initializaing to setup random values?</p>

<hr>

<p><strong>update</strong></p>

<p>my network is first a bunch of CNNs and pooling layers like below:
```
conv1 = tf.layers.conv2d(inputs=input_layer, filters=32, kernel_size=[3,3], padding=""same"", activation=tf.nn.relu, name=""conv_chad_1"")</p>

<pre><code>    pool1 = tf.layers.max_pooling2d(inputs=conv1,pool_size=[2,2],strides=2)

    conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3,3], padding=""same"", activation=tf.nn.relu, name=""conv_chad_2"")

    pool2 = tf.layers.max_pooling2d(inputs=conv2,pool_size=[2,2],strides=2, name=""pool_chad_2"")
</code></pre>

<p>```</p>

<p>AFAIK, the weights are defined inside these predefined layers. How do I specify these layers to initialize their weight variables randomly??</p>
",2018-05-17 04:42:33,"<p>You should provide more information. For example, how do you initialize your variables in your graph? For initializing your weights in a neural network, you must initialize them randomly (biases are ok to be initialized all as zero). Thus you must use a code like the following for defining them with proper initialization:</p>

<pre><code># initialize weights randomly from a Gaussian distribution
# step 1: create the initializer for weights
weight_initer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)
# step 2: create the weight variable with proper initialization
W = tf.get_variable(name=""Weight"", dtype=tf.float32, shape=[784, 200], initializer=weight_initer)

# initialize biases as zero
# step 1: create the initializer for biases
bias_initer =tf.constant(0., shape=[200], dtype=tf.float32)
# step 2: create the bias variable with proper initialization
b = tf.get_variable(name=""Bias"", dtype=tf.float32, initializer=bias_initer)
</code></pre>
",3044831,807,https://stackoverflow.com/questions/50383462,Documentation Replication on Other Examples
49711324,tf.contrib.summary.generic or tf.summary.text in eager mode,"<p>It looks like only tf.contrib.summary.scalar is supported when using eager mode. Is there a workaround to use tf.contrib.summary.generic or tf.summary.text?</p>
",2018-04-07 19:45:19,"<p>I believe you are mistaken. <strong>All</strong> the summary methods in <code>tf.contrib.summary</code> are supported for both eager execution and graph construction. For example, something like this seems to work:</p>

<pre><code>import tensorflow as tf

tf.enable_eager_execution()

with tf.contrib.summary.create_file_writer('/tmp/logdir').as_default(), tf.contrib.summary.always_record_summaries():
  for step in range(3):
    tf.contrib.summary.generic(""generic"", tf.pow(10., step), step=step)
    tf.contrib.summary.histogram(""histogram"", tf.random_uniform([10]), step=step)
  tf.contrib.summary.flush()
</code></pre>

<p>With <code>tensorboard --logdir /tmp/logdir --inspect</code> you can see that 3 events each are written, and of course the histogram can be visualized in <code>tensorboard</code>.</p>

<p>Hope that helps.</p>
",1375548,87,https://stackoverflow.com/questions/49711324,Documentation Replication on Other Examples
49808208,"What's the difference between tf.constant([1,2,3]) and tf.constant([[1,2,3]])","<pre><code>&gt;&gt;&gt; tf.constant([1,2,3])
&lt;tf.Tensor 'Const:0' shape=(3,) dtype=int32&gt;
&gt;&gt;&gt; tf.constant([[1,2,3]])
&lt;tf.Tensor 'Const_1:0' shape=(1, 3) dtype=int32&gt;
</code></pre>

<p>Is <code>tf.constant([1,2,3])</code> creating a scalar and <code>tf.constant([[1,2,3]])</code> creating an array?</p>
",2018-04-13 01:59:19,"<p>Not quite. <code>tf.constant([1, 2, 3])</code> creates a rank 1 constant tensor (a vector). Thus the shape is <code>(3,)</code>.</p>

<pre><code>&gt;&gt;&gt; sess = tf.InteractiveSession()

&gt;&gt;&gt; tf.constant([1, 2, 3]).eval()
array([1, 2, 3], dtype=int32)
</code></pre>

<p>While <code>tf.constant([[1, 2, 3]])</code> creates a rank 2 constant tensor (a matrix), with 1 row and 3 columns.. so its shape is <code>(1, 3)</code>. </p>

<pre><code>&gt;&gt;&gt; tf.constant([[1, 2, 3]]).eval()
array([[1, 2, 3]], dtype=int32)
</code></pre>

<p>If you really wanted a scalar (rank 0), you wouldn't construct it with a sequence but rather just a scalar value. </p>

<pre><code>&gt;&gt;&gt; tf.constant(3)
&lt;tf.Tensor 'Const_5:0' shape=() dtype=int32&gt;
</code></pre>

<p>Notice the empty shape here, making it clear it is rank 0. </p>

<p>See <a href=""https://www.tensorflow.org/programmers_guide/tensors#rank"" rel=""nofollow noreferrer"">Tensor/Rank</a> in the documentation. </p>
",9639109,43,https://stackoverflow.com/questions/49808208,Documentation Replicability
50153291,Clip to maximum / minimum finite int/float/double?,"<p>I'd like to clip some Tensorflow variables with tf.clip_by_value to the most positive / negative <strong>finite</strong> value possible.
I'd like to reach a similar behavior like numpy.nan_to_num()</p>

<p>How do I get these values, given some data type?
Does this clipping affect the optimizer?</p>
",2018-05-03 10:53:22,"<p>All <a href=""https://www.tensorflow.org/api_docs/python/tf/DType#max"" rel=""noreferrer"">TensorFlow datatypes have the min and max properties</a>, that return the maximum and minimum values the type can hold. E.g.,</p>

<pre><code>import tensorflow as tf

print( tf.float32.max )
</code></pre>

<p>Outputs:</p>

<blockquote>
  <p>3.4028235e+38</p>
</blockquote>
",2219819,666,https://stackoverflow.com/questions/50153291,Documentation Replicability
50203668,Using tf.custom_gradient in tensorflow r1.8,"<h3>System information</h3>

<ul>
<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Y</li>
<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>
<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>
<li><strong>TensorFlow version (use command below)</strong>: r1.8</li>
<li><strong>Python version</strong>: 2.7.14</li>
<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4</li>
<li><strong>CUDA/cuDNN version</strong>: 8.0/7.0</li>
<li><strong>GPU model and memory</strong>: GTX1080, 8G</li>
<li><strong>Bazel version</strong>: N/A</li>
<li><strong>Exact command to reproduce</strong>: python test_script.py</li>
</ul>

<h3>Describe the problem</h3>

<p>Hello, I'm trying to make a custom_gradient op using the function of <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">tf.custom_gradient</a>. I made my test code based on the API explanation online. However, it seems there is a problem in the custom_gradient function. Thanks!</p>

<h3>Source code / logs</h3>

<pre><code>import tensorflow as tf
import numpy as np

@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.log(1 + e), grad

x = tf.constant(100.)
f = tf.custom_gradient(log1pexp)

y, dy = f(x)

sess = tf.Session()
print (y.eval(session=sess), y.eval(session=sess).shape)

File ""/home/local/home/research/DL/unit_tests/tf_test_custom_grad.py"", line 14, in &lt;module&gt;
    y, dy = f(x)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated
    return _graph_mode_decorator(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 132, in _graph_mode_decorator
    result, grad_fn = f(*args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 439, in __iter__
""Tensor objects are not iterable when eager execution is not ""
TypeError: Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn.
</code></pre>
",2018-05-06 19:23:12,"<p>If you just want to test the code in the documentation, here is the way.</p>

<p>The following code will give the instable <code>[nan]</code> result:</p>

<pre><code>import tensorflow as tf

def log1pexp(x):
    return tf.log(1 + tf.exp(x))

x = tf.constant(100.)
y = log1pexp(x)
dy = tf.gradients(y, x)

with tf.Session() as sess:
    print(sess.run(dy))
</code></pre>

<p>And the following code will give the correct result <code>[1.0]</code>:</p>

<pre><code>import tensorflow as tf

@tf.custom_gradient
def log1pexp(x):
    e = tf.exp(x)
    def grad(dy):
        return dy * (1 - 1 / (1 + e))
    return tf.log(1 + e), grad

x = tf.constant(100.)
y = log1pexp(x)
dy = tf.gradients(y, x)

with tf.Session() as sess:
    print(sess.run(dy))
</code></pre>

<h1>Details:</h1>

<p>The main problem here is that you are trying to decorate <code>log1pexp</code> twice in your code: once with <code>@tf.custom_gradient</code> and once with <code>f = tf.custom_gradient(log1pexp)</code>. In <a href=""https://docs.python.org/3/glossary.html#term-decorator"" rel=""nofollow noreferrer"">python</a>, <code>@tf.custom_gradient</code> here is equivalent to <code>log1pexp = tf.custom_gradient(log1pexp)</code>. You should do this only once, especially here for the following reason.</p>

<p><code>tf.custom_gradient</code> needs to call the function being pass to it to get both the function output and the gradient, i.e. expecting two returns. During decoration, everything works as expected because <code>log1pexp</code> returns <code>tf.log(1 + e)</code> and <code>grad</code>. After decorating <code>log1pexp</code>, <code>log1pexp</code> (returned by <code>tf.custom_gradient</code>) becomes a new function which returns only one tensor <code>tf.log(1 + e)</code>. When you do <code>f = tf.custom_gradient(log1pexp)</code> after decorating <code>log1pexp</code>, <code>tf.custom_gradient</code> can only get one return which is the single tensor <code>tf.log(1 + e)</code>. It will try to split this tensor into two by iterating this returned tensor. But it is wrong and is not allowed as the error message stated:</p>

<blockquote>
  <p>Tensor objects are not iterable when eager execution is not enabled.</p>
</blockquote>

<p>You should not decorate <code>log1pexp</code> twice anyway. But this is why you got this error. One more thing to mention, your code will trigger another error for the same reason even if you removed <code>@tf.custom_gradient</code>. After removing <code>@tf.custom_gradient</code>, the line <code>f = tf.custom_gradient(log1pexp)</code> should work as expected. But <code>f</code> is a function returning only one tensor. <code>y, dy = f(x)</code> is wrong and will not work.</p>
",9749663,13,https://stackoverflow.com/questions/50203668,Documentation Ambiguity
50820781,quesion about the axis of tf.stack(),"<p>I read the doc of <code>tf.stack()</code> on <a href=""https://www.tensorflow.org/api_docs/python/tf/stack"" rel=""nofollow noreferrer"">tensorflow stack </a>. There is an example on the page:</p>

<pre><code>&gt;&gt;&gt; x = tf.constant([1, 4])
&gt;&gt;&gt; y = tf.constant([2, 5])
&gt;&gt;&gt; z = tf.constant([3, 6])
&gt;&gt;&gt; sess=tf.Session()
&gt;&gt;&gt; sess.run(tf.stack([x, y, z]))
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)
&gt;&gt;&gt; sess.run(tf.stack([x, y, z], axis=1))
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)
</code></pre>

<p>what I don't understand is the second example where <code>axis=1</code>.</p>

<p>From the result it seems it converts the three inputs rows to columns first </p>

<p>and then put them toghter along the <code>axis=1</code>, but </p>

<p>I think the result should be </p>

<pre><code>array([[1,4, 2, 5, 3, 6 ]] dtype=int32 )
</code></pre>

<p>can anyone help explain this?</p>

<p>Thanks!</p>
",2018-06-12 15:27:38,"<p><code>tf.stack</code> always adds a new dimension, and always concatenates the given tensor along that new dimension. In your case, you have three tensors with shape <code>[2]</code>. Setting <code>axis=0</code> is the same as adding a new first dimension, so each tensor would now have shape <code>[1, 2]</code>, and concatenating across that dimension, so the final shape would be <code>[3, 2]</code>. That is, each tensor would be a ""row"" of the final tensor. With <code>axis=1</code> the shapes of each individual tensor would be extended to <code>[2, 1]</code>, and the result would have shape <code>[2, 3]</code>. So each given tensor would be a ""column"" of the resulting tensor.</p>

<p>In other words, <code>tf.stack</code> is functionally equivalent to this:</p>

<pre><code>def tf.stack(tensors, axis=0):
    return tf.concatenate([tf.expand_dims(t, axis=axis) for t in tensors], axis=axis)
</code></pre>

<p>But the result that you expect would be obtained with something like this:</p>

<pre><code>tf.concatenate([tf.expand_dims(t, axis=0) for t in tensors], axis=1)
</code></pre>

<p>Note that the added dimension and the concatenation dimension are different in this case.</p>
",1347796,609,https://stackoverflow.com/questions/50820781,Inadequate Examples
51069173,What exactly qualifies as a 'Tensor' in TensorFlow?,"<p>I am new to TensorFlow and just went through the eager execution tutorial and came across the tf.decode_csv function. Not knowing about it, I read the documentation.  <a href=""https://www.tensorflow.org/api_docs/python/tf/decode_csv"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/decode_csv</a></p>

<p>I don't really understand it. </p>

<p>The documentation says 'records: A Tensor of type string.' 
<strong>So, my question is: What qualifies as a 'Tensor'?</strong> </p>

<p>I tried the following code:</p>

<pre><code>dec_res = tf.decode_csv('0.1,0.2,0.3', [[0.0], [0.0], [0.0]])
print(dec_res, type(dec_res))



l = [[1,2,3],[4,5,6],[7,8,9]]
r = tf.reshape(l, [9,-1])
print(l, type(l))
print(r, type(r))
</code></pre>

<p>So the list <code>dec_res</code> contains tf.tensor objects. That seems reasonable to me. But is an ordinary string also a 'Tensor' according to the documentation?</p>

<p>Then I tried something else with the <code>tf.reshape</code> function. In the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/reshape"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/reshape</a> it says that 'tensor: A Tensor.' So, <code>l</code> is supposed to be a tensor. But it is not of type <code>tf.tensor</code> but simply a python <code>list</code>. This is confusing.</p>

<p>Then the documentation says </p>

<blockquote>
  <p>Returns:</p>
  
  <p>A Tensor. Has the same type as tensor.</p>
</blockquote>

<p>But the type of <code>l</code> is <code>list</code> where the type of <code>r</code> is <code>tensorflow.python.framework.ops.Tensor</code>. So the types are not the same. </p>

<p>Then I thought that TensorFlow is very generous with things being a tensor. So I tried:</p>

<pre><code>class car(object):
def __init__(self, color):
    self.color = color


red_car = car('red')
#test_reshape = tf.reshape(red_car, [1, -1])
print(red_car.color) # to check, that red_car exists.
</code></pre>

<p>Now, the line in comments results in an error. </p>

<p>So, can anyone help me to find out, what qualifies as a 'Tensor'?</p>

<p>P.S.: I tried to read the source code of <code>tf.reshape</code> as given in the documentation </p>

<blockquote>
  <p>Defined in tensorflow/python/ops/gen_array_ops.py.</p>
</blockquote>

<p>But this file does not exist in the Github repo. Does anyone know how to read it?</p>
",2018-06-27 18:20:58,"<p><a href=""https://www.tensorflow.org/programmers_guide/tensors"" rel=""nofollow noreferrer"">https://www.tensorflow.org/programmers_guide/tensors</a></p>

<blockquote>
  <p>TensorFlow, as the name indicates, is a framework to define and run
  computations involving tensors. A tensor is a generalization of
  vectors and matrices to potentially higher dimensions. Internally,
  TensorFlow represents tensors as n-dimensional arrays of base
  datatypes.</p>
</blockquote>

<p>What you are observing commes from the fact that tensorflow operations (like reshape) can be built from various python types using the function tf.convert_to_tensor:</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor</a></p>

<blockquote>
  <p>All standard Python op constructors apply this function to each of
  their Tensor-valued inputs, which allows those ops to accept numpy
  arrays, Python lists, and scalars in addition to Tensor objects</p>
</blockquote>
",6810233,153,https://stackoverflow.com/questions/51069173,Requesting (Additional) Documentation/Examples
51824310,Difference between Keras and tensorflow implementation of LSTM with dropout,"<p>I was reviewing the documentation for the LSTM cell in tensorflow and Keras. In particular, I want to apply dropout as well. Here is what I have in Keras and would like to apply the same LSTM cell in tensorflow:</p>

<pre><code>cell = LSTM(num_units_2, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)(net)
</code></pre>

<p>Therefore, I know that I need to use <code>tf.nn.rnn_cell.LSTMCell</code> in tensorflow with <code>num_units = num_units_2</code>. Second, I need a <code>DropoutWrapper</code> as:</p>

<pre><code>cell = tf.nn.rnn_cell.DropoutWrapper(cell)
</code></pre>

<p>Now, I want to apply <code>dropout</code> and <code>recurrent_dropout</code> similar to the Keras code. Therefore, I found that tensorflow's implementation of dropout will apply a different dropout mask at every time step unless <code>variational_recurrent</code> is set to True (Yet I'm not sure how variational_recurrent works in details). </p>

<p>Additionally, I'm not sure if the LSTM in Keras apply different Mask at each time step as well. </p>

<p>Second, I was confused about the difference between the <code>output_keep_prob</code> and the <code>state_keep_prob</code> as both mention: </p>

<p><em>output_keep_prob</em>: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added...</p>

<p>Any help is much appreciated!!</p>
",2018-08-13 14:08:09,"<h2>What variational dropout does</h2>

<p>As far as I know, the main novelty of variational dropout is using the same dropout mask for all unrolled steps (as you said).</p>

<h2>Difference between <code>output_keep_prob</code> and the <code>state_keep_prob</code></h2>

<p><code>output_keep_prob</code> is the dropout rate applied to the output (h) of the LSTM cell where <code>state_keep_prob</code> is the dropout rate applied to the cell (c) of the LSTM state.</p>

<h2>Dropout choice in Keras</h2>

<p>Looking at the <code>_generate_dropout_mask</code> method in the <a href=""https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L2220-L2232"" rel=""nofollow noreferrer"">LSTM source code</a> and <a href=""https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L1855-L1868"" rel=""nofollow noreferrer"">its use for the LSTMCell</a> of Keras, I think Keras LSTM uses variational recurrent dropout only for the recurrent connections (i.e. <code>self._recurrent_dropout_mask</code>) . But I'm not 100% confident about this.</p>
",7886651,2282,https://stackoverflow.com/questions/51824310,Documentation Replication on Other Examples
51194912,How do I know which version of the Keras API is implemented in tf.keras?,"<p>In the standard implementation of Keras, one can get the API version using <a href=""https://stackoverflow.com/q/46086030/1735003""><code>keras.__version__</code></a>.</p>

<p>However, there is no <code>tf.keras.__version__</code>.</p>

<p>So how do I check the version of the Keras API implemented in <code>tf.keras</code>?</p>
",2018-07-05 15:19:35,"<p>You can simply run this code</p>

<pre><code>from tensorflow.python import keras
print(keras.__version__)
</code></pre>

<p>which is compatible with TF v1.8.0.</p>
",1735003,23635,https://stackoverflow.com/questions/51194912,Documentation Ambiguity
51586693,"Tensor has shape [?, 0] -- how to reshape to [?,]","<p>When <code>src</code> has shape <code>[?]</code>, <code>tf.gather(src, tf.where(src != 0))</code> returns a tensor with shape <code>[?, 0]</code>. I'm not sure how a dimension can have size 0, and I'm especially unsure how to change the tensor back. I didn't find anything in the documentation to explain this, either.</p>

<p>I tried to <code>tf.transpose(tensor)[0]</code>, but the first dimension of the transposed tensor has size 0 and cannot be accessed! What's wrong?</p>
",2018-07-30 03:41:10,"<p>I think you should use <a href=""https://www.tensorflow.org/api_docs/python/tf/not_equal"" rel=""nofollow noreferrer""><code>tf.not_equal</code></a> to perform elementwise comparison on the tensor.</p>

<pre><code>src = tf.constant([0, 1, 1, 0], dtype=tf.int8)
tf.gather(src, tf.where(tf.not_equal(src, 0))).eval(session=tf.Session())

array([[1],
       [1]], dtype=int8)
</code></pre>

<p>You can also shorten this a bit and use <a href=""https://www.tensorflow.org/api_docs/python/tf/boolean_mask"" rel=""nofollow noreferrer""><code>tf.boolean_mask</code></a> instead of <code>tf.where</code> and <code>tf.gather</code>:</p>

<pre><code>tf.boolean_mask(src, tf.not_equal(src, 0)).eval(session=tf.Session())
array([1, 1], dtype=int8)
</code></pre>

<p>Note the difference in the shape of the outputs.</p>
",6772171,1574,https://stackoverflow.com/questions/51586693,Documentation Replication on Other Examples
50083475,"what we do wtih tf.stack([tf.range(tf.shape(self.a)[0], dtype=tf.int32), self.a], axis=1)","<p>What is done with this command? What does <code>tf.stack</code> stands for what?</p>

<pre><code>tf.stack([tf.range(tf.shape(self.a)[0], dtype=tf.int32), self.a], axis=1)
</code></pre>
",2018-04-29 04:47:58,"<p>Generally speaking, I try the NumPy equivalents to the TensorFlow functions when I'm working things out.  Initially, the TensorFlow API had some quirky differences to the NumPy API, but enough users want the two packages to behave the same that TensorFlow is making changes.</p>

<p>You say that the array <code>self.a</code> is guaranteed to be 1D.  All right then:</p>

<pre><code>import numpy as np
arr = np.random.randint(-9,9,(10,))
print(arr)
result = np.stack([np.arange(np.shape(arr)[0], dtype=np.int32), arr], axis=1)
print(result)
</code></pre>

<p>Here's a sample output:</p>

<pre><code>array([-5,  1,  0, -3, -9, -8,  3, -1,  0, -2])

array([[ 0, -5],
       [ 1,  1],
       [ 2,  0],
       [ 3, -3],
       [ 4, -9],
       [ 5, -8],
       [ 6,  3],
       [ 7, -1],
       [ 8,  0],
       [ 9, -2]])
</code></pre>

<p>So, it looks like the original 1D array is enlarged into a 2D array with a numerical index in the 0th column.</p>
",9708375,49,https://stackoverflow.com/questions/50083475,Documentation Replicability
50457247,Tensorflow finding matching strings in tensor,"<p>I'm trying to find variables that end in <code>train_step</code> from <code>tf.report_uninitialized_variables(),</code> but you can't iterate over tensors without <code>eager execution.</code> I get that you need to use <code>tf.map_fn,</code> but I do not understand it well enough.</p>

<p>This is what I have:</p>

<pre><code>variables = []
for s, t in zip(tf.report_uninitialized_variables().eval(session=sess), 
                tf.report_uninitialized_variables()):
    if 'train_step' in s:
        variables.append(t)
train_step_init = tf.variables_initializer(variables, name='train_step_init')
</code></pre>
",2018-05-21 22:14:42,"<p>Turns out I could do:</p>

<pre><code>variables = []
for i, v in enumerate(tf.global_variables()):
    name = v.name.split(':')[0].encode('ASCII')
    if name in sess.run(tf.report_uninitialized_variables()[0]):
        if b'train_step' in name:
            variables.append(v)

train_step_init = tf.variables_initializer(variables)
sess.run(train_step_init)
</code></pre>
",3938049,1,https://stackoverflow.com/questions/50457247,Documentation Replicability
50606178,TensorFlow tf.data.Dataset and bucketing,"<p>For an LSTM network, I've seen great improvements with bucketing.</p>

<p>I've come across the <a href=""https://www.tensorflow.org/api_guides/python/contrib.training#Bucketing"" rel=""noreferrer"">bucketing section in the TensorFlow docs</a> which (tf.contrib).</p>

<p>Though in my network, I am using the <code>tf.data.Dataset</code> API, specifically I'm working with TFRecords, so my input pipeline looks something like this</p>

<pre><code>dataset = tf.data.TFRecordDataset(TFRECORDS_PATH)
dataset = dataset.map(_parse_function)
dataset = dataset.map(_scale_function)
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.padded_batch(batch_size, padded_shapes={.....})
</code></pre>

<p>How can I incorporate the bucketing method into a the <code>tf.data.Dataset</code> pipeline?</p>

<p>If it matters, in every record in the TFRecords file I have the sequence length saved as an integer.</p>
",2018-05-30 13:37:22,"<p>Various <code>bucketing</code> use cases using <code>Dataset API</code> are explained well <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/kernel_tests/bucketing_test.py"" rel=""nofollow noreferrer"">here</a>.</p>

<p><strong><code>bucket_by_sequence_length()</code> example:</strong></p>

<pre><code>def elements_gen():
   text = [[1, 2, 3], [3, 4, 5, 6, 7], [1, 2], [8, 9, 0, 2]]
   label = [1, 2, 1, 2]
   for x, y in zip(text, label):
       yield (x, y)

def element_length_fn(x, y):
   return tf.shape(x)[0]

dataset = tf.data.Dataset.from_generator(generator=elements_gen,
                                     output_shapes=([None],[]),
                                     output_types=(tf.int32, tf.int32))

dataset =   dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=element_length_fn,
                                                              bucket_batch_sizes=[2, 2, 2],
                                                              bucket_boundaries=[0, 8]))

batch = dataset.make_one_shot_iterator().get_next()

with tf.Session() as sess:

   for _ in range(2):
      print('Get_next:')
      print(sess.run(batch))
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Get_next:
(array([[1, 2, 3, 0, 0],
   [3, 4, 5, 6, 7]], dtype=int32), array([1, 2], dtype=int32))
Get_next:
(array([[1, 2, 0, 0],
   [8, 9, 0, 2]], dtype=int32), array([1, 2], dtype=int32))
</code></pre>
",5368083,12085,https://stackoverflow.com/questions/50606178,Documentation Replicability
51806852,Can't save custom subclassed model,"<p>Inspired by <a href=""https://www.tensorflow.org/guide/keras#model_subclassing"" rel=""noreferrer"">tf.keras.Model subclassing</a> I created custom model.<br>
I can train it and get successfull results, but <strong>I can't save it</strong>.<br>
I use python3.6 with tensorflow v1.10 (or v1.9)  </p>

<p>Minimal complete code example here:</p>

<pre><code>import tensorflow as tf
from tensorflow.keras.datasets import mnist


class Classifier(tf.keras.Model):
    def __init__(self):
        super().__init__(name=""custom_model"")

        self.batch_norm1 = tf.layers.BatchNormalization()
        self.conv1 = tf.layers.Conv2D(32, (7, 7))
        self.pool1 = tf.layers.MaxPooling2D((2, 2), (2, 2))

        self.batch_norm2 = tf.layers.BatchNormalization()
        self.conv2 = tf.layers.Conv2D(64, (5, 5))
        self.pool2 = tf.layers.MaxPooling2D((2, 2), (2, 2))

    def call(self, inputs, training=None, mask=None):
        x = self.batch_norm1(inputs)
        x = self.conv1(x)
        x = tf.nn.relu(x)
        x = self.pool1(x)

        x = self.batch_norm2(x)
        x = self.conv2(x)
        x = tf.nn.relu(x)
        x = self.pool2(x)

        return x


if __name__ == '__main__':
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = x_train.reshape(*x_train.shape, 1)[:1000]
    y_train = y_train.reshape(*y_train.shape, 1)[:1000]

    x_test = x_test.reshape(*x_test.shape, 1)
    y_test = y_test.reshape(*y_test.shape, 1)

    y_train = tf.keras.utils.to_categorical(y_train)
    y_test = tf.keras.utils.to_categorical(y_test)

    model = Classifier()

    inputs = tf.keras.Input((28, 28, 1))

    x = model(inputs)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(10, activation=""sigmoid"")(x)

    model = tf.keras.Model(inputs=inputs, outputs=x)
    model.compile(optimizer=""adam"", loss=""binary_crossentropy"", metrics=[""accuracy""])
    model.fit(x_train, y_train, epochs=1, shuffle=True)

    model.save(""./my_model"")
</code></pre>

<p>Error message:  </p>

<pre><code>1000/1000 [==============================] - 1s 1ms/step - loss: 4.6037 - acc: 0.7025
Traceback (most recent call last):
  File ""/home/user/Data/test/python/mnist/mnist_run.py"", line 62, in &lt;module&gt;
    model.save(""./my_model"")
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1278, in save
    save_model(self, filepath, overwrite, include_optimizer)
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 101, in save_model
    'config': model.get_config()
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1049, in get_config
    layer_config = layer.get_config()
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1028, in get_config
    raise NotImplementedError
NotImplementedError

Process finished with exit code 1
</code></pre>

<p>I looked into the error line and found out that <strong>get_config</strong> method checks <strong>self._is_graph_network</strong></p>

<p>Do anybody deal with this problem?</p>

<p>Thanks!</p>

<p><strong>Update 1:</strong><br>
On the keras 2.2.2 (not tf.keras)<br>
Found comment (for model saving)<br>
file: keras/engine/network.py<br>
Function: get_config  </p>

<blockquote>
  <p># Subclassed networks are not serializable<br>
  # (unless serialization is implemented by<br>
  # the author of the subclassed network).  </p>
</blockquote>

<p>So, obviously it won't work...<br>
I wonder, why don't they point it out in the <a href=""https://www.tensorflow.org/guide/keras"" rel=""noreferrer"">documentation</a> (Like: ""Use subclassing without ability to save!"")</p>

<p><strong>Update 2:</strong><br>
Found in <a href=""https://keras.io/models/about-keras-models/"" rel=""noreferrer"">keras documentation</a>:  </p>

<blockquote>
  <p>In subclassed models, the model's topology is defined as Python code<br>
  (rather than as a static graph of layers). That means the model's<br>
  topology cannot be inspected or serialized. As a result, the following<br>
  methods and attributes are not available for subclassed models:  </p>
  
  <p>model.inputs        and model.outputs.<br>
  model.to_yaml()     and model.to_json()<br>
  model.get_config()  and model.save().  </p>
</blockquote>

<p><strong>So, there is no way to save model by using subclassing.</strong><br>
It's possible to only use <code>Model.save_weights()</code></p>
",2018-08-12 07:53:41,"<h3>TensorFlow 2.2</h3>

<p>Thanks for @cal for noticing me that the new TensorFlow has supported saving the custom models!</p>

<blockquote>
  <p>By using model.save to save the whole model and by using load_model to restore previously stored subclassed model. The following code snippets describe how to implement them.</p>
</blockquote>

<pre><code>class ThreeLayerMLP(keras.Model):

  def __init__(self, name=None):
    super(ThreeLayerMLP, self).__init__(name=name)
    self.dense_1 = layers.Dense(64, activation='relu', name='dense_1')
    self.dense_2 = layers.Dense(64, activation='relu', name='dense_2')
    self.pred_layer = layers.Dense(10, name='predictions')

  def call(self, inputs):
    x = self.dense_1(inputs)
    x = self.dense_2(x)
    return self.pred_layer(x)

def get_model():
  return ThreeLayerMLP(name='3_layer_mlp')

model = get_model()
# Save the model
model.save('path_to_my_model',save_format='tf')

# Recreate the exact same model purely from the file
new_model = keras.models.load_model('path_to_my_model')
</code></pre>

<p>See: <a href=""https://www.tensorflow.org/guide/keras/save_and_serialize#part_ii_saving_and_loading_of_subclassed_models"" rel=""noreferrer"">Save and serialize models with Keras - Part II: Saving and Loading of Subclassed Models</a></p>

<h3>TensorFlow 2.0</h3>

<p>TL;DR:</p>

<ol>
<li>do not use <code>model.save()</code> for custom subclass keras model;</li>
<li>use <code>save_weights()</code> and <code>load_weights()</code> instead.</li>
</ol>

<hr>

<p>With the help of the Tensorflow Team, it turns out the best practice of saving a Custom Sub-Class Keras Model is to save its weights and load it back when needed.</p>

<p>The reason that we can not simply save a Keras custom subclass model is that it contains custom codes, which can not be serialized safely. However, the weights can be saved/loaded when we have the same model structure and custom codes without any problem.</p>

<p>There has a great tutorial written by Francois Chollet who is the author of Keras, for how to save/load Sequential/Functional/Keras/Custom Sub-Class Models in Tensorflow 2.0 in Colab at <a href=""https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=mJqOn0snzCRy"" rel=""noreferrer"">here</a>. In <strong>Saving Subclassed Models</strong> section, it said that:</p>

<blockquote>
  <p>Sequential models and Functional models are datastructures that represent a DAG of layers. As such, they can be safely serialized and deserialized.</p>
  
  <p>A subclassed model differs in that it's not a datastructure, it's a
  piece of code. The architecture of the model is defined via the body
  of the call method. This means that the architecture of the model
  cannot be safely serialized. To load a model, you'll need to have
  access to the code that created it (the code of the model subclass).
  Alternatively, you could be serializing this code as bytecode (e.g.
  via pickling), but that's unsafe and generally not portable.</p>
</blockquote>
",3049753,2102,https://stackoverflow.com/questions/51806852,Requesting (Additional) Documentation/Examples
50997477,Keras and Tensorflow: Saving non-sequential model weights,"<p>(I'm using Tensorflow 1.8.0 ...)</p>

<p>The <a href=""https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model"" rel=""nofollow noreferrer"">documentation from Keras on how to save a model</a> mentions no difference between saving a sequential model vs. one created from the functional API. But, all of the following blocks of code fail:</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
net.save('file')
</code></pre>

<p>or</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
print(net.to_json())
</code></pre>

<p>or</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
print(net.to_yaml())
</code></pre>

<p>or</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
print(net.get_config())
</code></pre>

<p>They raise a <code>NotImplementedError</code>. In the Keras module, the relevant lines are</p>

<pre><code>if not self._is_graph_network:
  raise NotImplementedError
</code></pre>

<p>which shows up in <code>.save</code> and <code>get_config</code> (the latter is also called by <code>to_json</code> and <code>to_yaml</code>.</p>

<p>The only thing that DOES work is the following</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
net.save_weights('file')
</code></pre>

<p>in which case the weights are saved successfully and can be successfully loaded with <code>net.load_weights</code>.</p>

<p>However, replacing the second line of the above blocks of code, <code>net = tf.keras.models.Model()</code>, with <code>net = tf.keras.models.Sequential()</code>, making net a sequential model, allows everything above to work.</p>

<p>Is it really not possible to save the structure of a Keras model made with the functional API (using <code>Model</code> rather than <code>Sequential</code>)? Right now, can we only save weights?</p>
",2018-06-23 02:33:26,"<p>Of course its possible to save Model, all your examples have an empty Model, whcih makes no sense to save. Keras' author simply did not implement that case.</p>

<p>If you test with a non-empty Model you will see that saving works perfectly. We use it every day.</p>
",9878957,603,https://stackoverflow.com/questions/50997477,Lack of Alternative Solutions/Documentation
51089334,What is the difference between tf.keras.layers versus tf.layers?,"<p>What is the difference between tf.keras.layers versus tf.layers?<br>
E.g. both of them have Conv2d, do they provide different outputs?<br>
 Is there any benefits if you mix them (something like a tf.keras.layers.Conv2d in one hidden layer and in the next, tf.layers.max_pooling2d)?</p>
",2018-06-28 18:35:11,"<p>Since TensorFlow 1.12, <code>tf.layers</code> are merely wrappers around <code>tf.keras.layers</code>.</p>

<p>A few examples:</p>

<p>Convolutional <code>tf.layers</code> just inherit from the convolutional <code>tf.keras.layers</code>, see source code <a href=""https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/layers/convolutional.py#L217"" rel=""noreferrer"">here</a>:</p>

<pre><code>@tf_export('layers.Conv2D')
class Conv2D(keras_layers.Conv2D, base.Layer):
</code></pre>

<p>The same is true for all <a href=""https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/layers/core.py"" rel=""noreferrer"">core <code>tf.layers</code></a>, e.g.:</p>

<pre><code>@tf_export('layers.Dense')
class Dense(keras_layers.Dense, base.Layer):
</code></pre>

<p>With the integration of Keras into TensorFlow, it would make little sense to maintain several different layer implementations. <code>tf.keras</code> is becoming the de-facto high-level API for TensorFlow, therefore <code>tf.layers</code> are now just wrappers around <code>tf.keras.layers</code>.</p>
",9262788,401,https://stackoverflow.com/questions/51089334,Documentation Replication on Other Examples
51859776,lambda layer function definition without tf.keras.backend (Python Keras Package),"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda"" rel=""nofollow noreferrer"">tf.keras.layers.Lambda</a> documentation explains how a function can be defined in a lambda layer. That document provides the following function as an example,</p>

<pre><code>def antirectifier(x):

    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)

    pos = K.relu(x)
    neg = K.relu(-x)

    return K.concatenate([pos, neg], axis=1)

model.add(Lambda(antirectifier))
</code></pre>

<p>But according to that, <code>tf.keras.backend</code> must be used to conduct operations on the input Tensor object.</p>

<p>Is there any way we can use default python packages and user-defined function to define the steps of a lambda function.</p>

<p>If it's possible, please be kind enough to provide some examples.</p>
",2018-08-15 13:34:42,"<p>If you're not using <code>import tensorflow</code> and its functions, there is absolutely no problem. </p>

<p>The code is perfect and that's it. </p>

<p>Just <code>import keras.backend as K</code></p>

<p>Example <code>rounded = K.round(x)</code></p>

<hr>

<p>This is Keras independent documentation: <a href=""https://keras.io/layers/core/#lambda"" rel=""nofollow noreferrer"">https://keras.io/layers/core/#lambda</a></p>
",261433,432,https://stackoverflow.com/questions/51859776,Inadequate Examples
51278422,Interpreting the FLOPs profile result of tensorflow,"<p>I want to profile the FLOPs of a very simple neural network model, which is used to classify the MNIST dataset, and the batch size is 128. As I followed the official tutorials, I got the result of the following model, but I cannot understand some parts of the output.</p>

<pre><code>w1 = tf.Variable(tf.random_uniform([784, 15]), name='w1')
w2 = tf.Variable(tf.random_uniform([15, 10]), name='w2')
b1 = tf.Variable(tf.zeros([15, ]), name='b1')
b2 = tf.Variable(tf.zeros([10, ]), name='b2')

hidden_layer = tf.add(tf.matmul(images_iter, w1), b1)
logits = tf.add(tf.matmul(hidden_layer, w2), b2)

loss_op = tf.reduce_sum(\
    tf.nn.softmax_cross_entropy_with_logits(logits=logits, 
                                            labels=labels_iter))
opetimizer = tf.train.AdamOptimizer(learning_rate=0.01)
train_op = opetimizer.minimize(loss_op)
</code></pre>

<p>The <code>images_iter</code> and the <code>labels_iter</code> are the iterators of tf.data, which are similar to the placeholder. </p>

<pre><code>tf.profiler.profile(
    tf.get_default_graph(),
    options=tf.profiler.ProfileOptionBuilder.float_operation())
</code></pre>

<p>I used this code, which equals to <code>scope -min_float_ops 1 -select float_ops -account_displayed_op_only</code> in tfprof comments line tool, to profile the FLOPs and got the below result.</p>

<pre><code>Profile:
node name | # float_ops
_TFProfRoot (--/23.83k flops)
  random_uniform (11.76k/23.52k flops)
    random_uniform/mul (11.76k/11.76k flops)
    random_uniform/sub (1/1 flops)
  random_uniform_1 (150/301 flops)
    random_uniform_1/mul (150/150 flops)
    random_uniform_1/sub (1/1 flops)
  Adam/mul (1/1 flops)
  Adam/mul_1 (1/1 flops)
  softmax_cross_entropy_with_logits_sg/Sub (1/1 flops)
  softmax_cross_entropy_with_logits_sg/Sub_1 (1/1 flops)
  softmax_cross_entropy_with_logits_sg/Sub_2 (1/1 flops)
</code></pre>

<p>My questions are   </p>

<ol>
<li>What do the numbers in the parentheses mean? For example, <code>random_uniform_1 (150/301 flops)</code>, what are 150 and 301?</li>
<li>Why is the first number in the parentheses of _TFProfRoot ""--""?</li>
<li>Why are the flops of Adam/mul and softmax_cross_entropy_with_logits_sg/Sub 1?</li>
</ol>

<p>I know it is discouraging to read a question so long, but a desperate boy who cannot find relating information from the official document needs your guys to help.</p>
",2018-07-11 06:17:03,"<p>I'll give it a try:</p>

<p>(1) From this example, looks like the first number is the ""self"" flops, the second number means the ""total"" flops under the naming scope. For example: for the 3 nodes respectively named random_uniform (if there is such a node), random_uniform/mul, random_uniform/sub, they respectively take 11.76k, 11.76k, and 1 flops, and in total 23.52k flops. </p>

<p>For another example: 23.83k = 23.52k + 300.</p>

<p>Does this make sense?</p>

<p>(2) The root node is a ""virtual"" top-level node added by the profiler, which doesn't have a ""self"" flops , or in other words, it has zero self flops.</p>

<p>(3) Not sure why it is 1. It would help if you can print the GraphDef and find out what this node really is, with print(sess.graph_def)</p>

<p>Hope this helps.</p>
",6156468,33,https://stackoverflow.com/questions/51278422,Lack of Alternative Solutions/Documentation
51882846,tensorflow: how to use tf.nn.leaky_relu with alpha as activation_fn for tf.contrib.layers.fully_connected?,"<p>The default <code>activation_fn</code> for <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected"" rel=""nofollow noreferrer"">tf.contrib.layers.fully_connected</a> is <code>tf.nn.relu</code>.</p>

<p>If I want to change the <code>activation_fn</code> for a certain fully connected layer to <code>tf.nn.tanh</code> then I can call it as: <code>tf.contrib.layers.fully_connected(inputs, num_outputs, activation_fn=tf.nn.tanh)</code></p>

<p>Now if I want to use <code>tf.nn.leaky_relu</code> with <code>alpha=0.01</code>, I can't do that. I can only use <code>tf.nn.leaky_relu</code> with the default value of <code>alpha</code>.</p>

<p>Is there any elegant way to do that ? Or should I make <code>activation_fn=None</code> and then manually call <code>tf.nn.leaky_relu</code> after that ?</p>
",2018-08-16 17:57:53,"<p>You can just define a function based on <code>leaky_relu</code>. </p>

<p>For example you could use <code>activation_fn=lambda x: tf.nn.leaky_relu(x, alpha=0.01)</code>.</p>

<p>This is the same as <code>def lrelu_01(x): return tf.nn.leaky_relu(x, alpha=0.01)</code> and then passing <code>activation_fn=lrelu_01</code>.</p>
",3142049,4880,https://stackoverflow.com/questions/51882846,Documentation Replicability
51612489,tensorflow tf.edit_distance explanation required?,"<p>How does tensorflow <code>tf.edit_distance</code> function works?
How it compares string stored in two different sparse matrix equivalent of 2d or 3d dense matrix. </p>

<p>Example given on tensorflow web page <a href=""https://www.tensorflow.org/api_docs/python/tf/edit_distance"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/edit_distance</a> is not so obvious. Please provide explanation using some other examples. </p>

<p>Also this example is not clear.</p>

<pre><code>#'hypothesis' is a tensor of shape [2, 1] with variable-length values:
#(0,0) = [""a""] and (1,0) = [""b""]

hypothesis = tf.SparseTensor([[0, 0, 0],[1, 0, 0]],[""a"", ""b""],(2, 1, 1))

#'truth' is a tensor of shape `[2, 2]` with variable-length values:
#(0,0) = [], (0,1) = [""a""], (1,0) = [""b"", ""c""],(1,1) = [""a""]

truth = tf.SparseTensor([[0, 1, 0],[1, 0, 0],[1, 0, 1],[1, 1, 0]],[""a"", ""b"", 
""c"", ""a""],(2, 2, 2))

normalize = True

#'output' is a tensor of shape [2, 2] with edit distances normalized by 
#'truth' lengths.

output ==&gt; [[inf, 1.0],[0.5, 1.0]],

(0,0): no truth, (0,1): no hypothesis, (1,0): addition, (1,1): no hypothesis
</code></pre>

<p>How output is of dimension [2,2]?</p>

<p>What normalization is doing here?</p>
",2018-07-31 11:37:21,"<p>hypothesis in dense form looks like this</p>

<pre><code>[[['a']],
 [['b']]] # (2, 1, 1)
</code></pre>

<p>truth is this</p>

<pre><code>[[[],['a']],
 [['b', 'c'], ['a']]] # (2, 2, 2)
</code></pre>

<p>We are trying to find the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a> between hypothesis and truth value.
So, here is what is happening:</p>

<p>at (0,0,0) - how far is ['a'] in hypothesis from [] - no truth in that position so can't calculate distance</p>

<p>at (0,0,1) - since there is nothing in that position at hypothesis we return 1. Unlike the case above, the distance is 1 because in theory the hypothesis can be made same as truth by inserting one character (See Levenshtein distance calculations)</p>

<p>at (1,0,0) - how far is ['b'] in hyp from ['b', 'c'] in truth. This is again 1, since we can insert a character to make hyp same as truth. But, we selected to normalize the output distance. So we divide by length of truth segment, which is 2. So you get 0.5</p>

<p>at (1,0,1) - how far is [] in hyp from ['a'], since there is nothing in that position at hyp, we return 1</p>

<p>Output is (2,2) because rank of hyp and truth is 3. The function returns tensor with rank (rank-1) </p>

<p>It helps by imagining what we are trying to do here. You have 2 sequences in hypothesis and 2 sequences in the truth. So your output score will be such that you get scores for each position in each sequence.</p>

<p>Here is an example where we try to match 4 hypotheses to a truth value. I think you have to do this for each truth sequence for the use case that you describe in your comment - let me know if you find something more efficient :-)</p>

<pre><code>import tensorflow as tf

hypothesis = tf.SparseTensor(
            [[0, 0, 0],
             [1, 0, 0],
             [2, 0, 0],
             [3, 0, 0]],
             [""a"", ""b"", ""c"", ""d""],
            (4, 1, 1))

truth = tf.SparseTensor([[0, 0, 0], [0, 0, 1], [0, 1, 0]], [""b"", ""c"", ""a""], (1,2,2))
num_hyp = 4
truth = tf.sparse_concat(0, [truth] * num_hyp)

d = tf.edit_distance(hypothesis, truth)

with tf.Session() as sess:
    print(sess.run(d))
</code></pre>

<p>Output:</p>

<pre><code>[[1.  1. ]
 [0.5 1. ]
 [0.5 1. ]
 [1.  1. ]]
</code></pre>
",7930290,397,https://stackoverflow.com/questions/51612489,Lack of Alternative Solutions/Documentation
51690095,how to gather element with index in tensorflow,"<p>For example,</p>

<pre><code>import tensorflow as tf

index = tf.constant([[1],[1]])
values = tf.constant([[0.2, 0.8],[0.4, 0.6]])
</code></pre>

<p>if I use <code>extract = tf.gather_nd(values, index)</code>
the return is </p>

<pre><code>[[0.4 0.6]
 [0.4 0.6]]
</code></pre>

<p>However, I want the result is</p>

<pre><code>[[0.8], [0.6]]
</code></pre>

<p>where the index is along axis = 1, however, there is no axis parameter setting in tf.gather_nd.</p>

<p>What should I do? Thanks!</p>
",2018-08-04 22:46:29,"<p>Concatenate a range to <code>index</code>:</p>

<pre><code>index = tf.stack([tf.range(index.shape[0])[:, None], index], axis=2)
result = tf.gather_nd(values, index)
</code></pre>

<p></p>

<pre><code>result.eval(session=tf.Session())
array([[0.8],
       [0.6]], dtype=float32)
</code></pre>
",5020881,1117,https://stackoverflow.com/questions/51690095,Documentation Ambiguity
51691199,How does tf.create_partitioned_variables work?,"<p>I am trying to figure out how to use <a href=""https://www.tensorflow.org/api_docs/python/tf/create_partitioned_variables"" rel=""nofollow noreferrer"">tf.create_partitioned_variables</a>
I am reading the documentation but I am having a hard time understanding.</p>

<p>Could anyone explain how it works and give some examples of its usage? </p>

<p>From what I understand I can use it to get a list of slices from a variable.
I just dont understand how I get the slices</p>

<p>ex:
how would i get a list of <code>[[1.],[3.]]</code> from <code>tf.Variable(np.array([[1.0],[3.0]]), dtype=tf.float32)</code></p>

<p>or list of </p>

<pre><code>[[[1 0] [3 0]], [[0 5] [0 7]]]
</code></pre>

<p>from</p>

<pre><code>[[[1 0]
  [3 0]]

 [[0 5]
  [0 7]]]
</code></pre>
",2018-08-05 03:39:55,"<p>The first 3 arguments are required. The first is the shape of the input tensor. The second is the split specification. The API currently supports splits along one dimension only. The split specification has the same number of dimensions as the shape, with one split being >= 1 and the others being 1. The last argument is the tensor itself, or a callable that returns it.</p>

<p>First example:</p>

<pre><code>tf.create_partitioned_variables(v.shape, [2, 1], v)
</code></pre>

<p>Second example:</p>

<pre><code>[tf.squeeze(v) 
    for v in tf.create_partitioned_variables(
        v.shape, [2, 1, 1], v)]
</code></pre>
",774972,5999,https://stackoverflow.com/questions/51691199,Inadequate Examples
50514454,End of Sequence Error when using tf.estimator and tf.data,"<p>I am using <code>tf.estimator.train_and_evaluate</code> and <code>tf.data.Dataset</code> to feed data to the estimator:</p>

<p>Input Data function:</p>

<pre><code>    def data_fn(data_dict, batch_size, mode, num_epochs=10):
        dataset = {}
        if mode == tf.estimator.ModeKeys.TRAIN:
            dataset = tf.data.Dataset.from_tensor_slices(data_dict['train_data'].astype(np.float32))
            dataset = dataset.cache()
            dataset = dataset.shuffle(buffer_size= batch_size * 10).repeat(num_epochs).batch(batch_size)
        else:
            dataset = tf.data.Dataset.from_tensor_slices(data_dict['valid_data'].astype(np.float32))
            dataset = dataset.cache()
            dataset = dataset.batch(batch_size)

        iterator = dataset.make_one_shot_iterator()
        next_element = iterator.get_next()

    return next_element
</code></pre>

<p>Train Function:</p>

<pre><code>def train_model(data):
    tf.logging.set_verbosity(tf.logging.INFO)
    config = tf.ConfigProto(allow_soft_placement=True,
                            log_device_placement=False)
    config.gpu_options.allow_growth = True
    run_config = tf.contrib.learn.RunConfig(
        save_checkpoints_steps=10,
        keep_checkpoint_max=10,
        session_config=config
    )

    train_input = lambda: data_fn(data, 100, tf.estimator.ModeKeys.TRAIN, num_epochs=1)
    eval_input = lambda: data_fn(data, 1000, tf.estimator.ModeKeys.EVAL)
    estimator = tf.estimator.Estimator(model_fn=model_fn, params=hps, config=run_config)
    train_spec = tf.estimator.TrainSpec(train_input, max_steps=100)
    eval_spec = tf.estimator.EvalSpec(eval_input,
                                      steps=None,
                                      throttle_secs = 30)

    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
</code></pre>

<p>The training goes fine, but when it comes to evaluation I get this error:</p>

<pre><code>OutOfRangeError (see above for traceback): End of sequence 
</code></pre>

<p>If I don't use <code>Dataset.batch</code> on evaluation dataset (by omitting the line <code>dataset[name] = dataset[name].batch(batch_size)</code> in <code>data_fn</code>) I get the same error but after a much longer time.</p>

<p>I can only avoid this error if I don't batch the data and use <code>steps=1</code> for evaluation, but does that perform the evaluation on the whole dataset?</p>

<p>I don't understand what causes this error as the documentation suggests I should be able to evaluate on batches too.</p>

<p>Note: I get the same error when using <code>tf.estimator.evaluate</code> on data batches.</p>
",2018-05-24 16:50:01,"<p>I posted this question as a github issue and here is the response from the Tensorflow team:</p>
<p><a href=""https://github.com/tensorflow/tensorflow/issues/19541"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/19541</a></p>
<p>Copying from &quot;xiejw&quot; for completeness:</p>
<blockquote>
<p>If I understand correctly, this issue is &quot;once give estimator an input_fn with dataset inside, the evaluate process will error out with OutOfRangeError.&quot;</p>
<p>Estimator can handle this correctly actually. However, a known common root cause for this is metrics defined in model_fn have bug. We need to rule that part out first.</p>
<p>@mrezak if possible, can you show the code about the model_fn? Or if you have a minimal reproducible script, that will be extremely helpful. -- Thanks in advance.</p>
<p>A common problem for this is: metric in tensorflow should return two Ops: update_op and value_op. Estimator calls the update_op for each batch of the data in input source and, once it is exhausted, it call the value_op to get the metric values. The value_op here should have dependency back to variables reading only.</p>
<p>Many model_fn puts the dependency of value_op with the input pipeline, so, estimator.evaluate will thereby trigger the input pipeline one more time, which errors out with OutOfRangeError</p>
</blockquote>
<p>The problem was indeed how I defined the <code>eval_metric</code> in <code>model_fn</code>. In my actual code my total loss to be optimized was composed of multiple losses (reconstruction + L2 + KL) and in the evaluation part I wanted to get the reconstruction loss (on the validation data), which depended on the input data pipeline. My actual reconstruction cost was more complex than MSE (none of the other tf.metric functions as well) which was not straightforward to be implemented using tf.metric basic functions.</p>
<p>This is &quot;xiejw&quot;'s suggestion which fixed the issue:</p>
<pre><code>my_total_loss =  ... # the loss you care. Pay attention to how you reduce the loss. 
eval_metric_ops = {'total_loss: tf.metrics.mean(my_total_loss)}
</code></pre>
",8636859,273,https://stackoverflow.com/questions/50514454,Documentation Replication on Other Examples
53003208,How to save a Tensorflow Checkpoint file from Google Colaboratory in when using TPU mode?,"<p>When I use <code>saver = tf.train.Saver()</code> and <code>save_path = saver.save(session, ""checkpointsFolder/checkpoint.ckpt"")</code></p>

<p>I get a <code>UnimplementedError (see above for traceback): File system scheme '[local]' not implemented</code> error</p>

<p>Here is the full error</p>

<pre><code>---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-&gt; 1334       return fn(*args)
   1335     except errors.OpError as e:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-&gt; 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-&gt; 1407         run_metadata)
   1408 

UnimplementedError: File system scheme '[local]' not implemented (file: 'checkpointsBook2Vec5Inputs')
     [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:tpu_worker/replica:0/task:0/device:CPU:0""](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, embeddings, embeddings/Shampoo, embeddings/Shampoo_1, embeddings/Shampoo_2, epochCount, softmax_biases, softmax_weights, softmax_weights/Shampoo, softmax_weights/Shampoo_1, softmax_weights/Shampoo_2)]]

During handling of the above exception, another exception occurred:

UnimplementedError                        Traceback (most recent call last)
&lt;ipython-input-22-ca87cd5e5739&gt; in &lt;module&gt;()
     48             print('recEpoch_indexA is', recEpoch_indexA)
     49 
---&gt; 50             save_path = saver.save(session, ""checkpointsBook2Vec5Inputs/Research2VecCS4.ckpt"") #Save checkpoint
     51             print( 'epochCount.eval() is ', epochCount.eval() )
     52 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py in save(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)
   1439           model_checkpoint_path = sess.run(
   1440               self.saver_def.save_tensor_name,
-&gt; 1441               {self.saver_def.filename_tensor_name: checkpoint_file})
   1442 
   1443         model_checkpoint_path = compat.as_str(model_checkpoint_path)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-&gt; 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-&gt; 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

UnimplementedError: File system scheme '[local]' not implemented (file: 'checkpointsBook2Vec5Inputs')
     [[node save/SaveV2 (defined at &lt;ipython-input-15-c14caac2081d&gt;:45)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:tpu_worker/replica:0/task:0/device:CPU:0""](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, embeddings, embeddings/Shampoo, embeddings/Shampoo_1, embeddings/Shampoo_2, epochCount, softmax_biases, softmax_weights, softmax_weights/Shampoo, softmax_weights/Shampoo_1, softmax_weights/Shampoo_2)]]

Caused by op 'save/SaveV2', defined at:
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py"", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-15-c14caac2081d&gt;"", line 45, in &lt;module&gt;
    saver = tf.train.Saver()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1102, in __init__
    self.build()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1114, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1151, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 792, in _build_internal
    save_tensor = self._AddSaveOps(filename_tensor, saveables)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 284, in _AddSaveOps
    save = self.save_op(filename_tensor, saveables)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 202, in save_op
    tensors)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1690, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

UnimplementedError (see above for traceback): File system scheme '[local]' not implemented (file: 'checkpointsBook2Vec5Inputs')
     [[node save/SaveV2 (defined at &lt;ipython-input-15-c14caac2081d&gt;:45)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:tpu_worker/replica:0/task:0/device:CPU:0""](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, embeddings, embeddings/Shampoo, embeddings/Shampoo_1, embeddings/Shampoo_2, epochCount, softmax_biases, softmax_weights, softmax_weights/Shampoo, softmax_weights/Shampoo_1, softmax_weights/Shampoo_2)]]
</code></pre>

<p>Looking up this error, I found the following:</p>

<p>From Google official TPU debugging guide</p>

<p><a href=""https://cloud.google.com/tpu/docs/troubleshooting"" rel=""nofollow noreferrer"">https://cloud.google.com/tpu/docs/troubleshooting</a></p>

<blockquote>
  <p>Error Message</p>
  
  <p>InvalidArgumentError: Unimplemented: File system scheme '[local]' not
  implemented</p>
  
  <p>Details</p>
  
  <p>All input files and the model directory must use a cloud storage
  bucket path (gs://bucket-name/...), and this bucket must be accessible
  from the TPU server. Note that all data processing and model
  checkpointing is performed on the TPU server, not the local machine.
  For information on how to properly configure cloud storage for use
  with the TPU, see the guide Connecting to Cloud Storage Buckets.</p>
</blockquote>

<p>Someone else with a similar issue</p>

<p><a href=""https://stackoverflow.com/questions/52703047/tpu-local-filesystem-doesnt-exist"">TPU local Filesystem doesn&#39;t exist?</a></p>

<blockquote>
  <p>The local filesystem is not available on Cloud TPU's. Model
  directories (checkpoints etc) and input data should be stored in
  Google Cloud Storage (and prefixed with ""gs://"").</p>
  
  <p>More details here</p>
  
  <p><a href=""https://cloud.google.com/tpu/docs/storage-buckets"" rel=""nofollow noreferrer"">https://cloud.google.com/tpu/docs/storage-buckets</a></p>
</blockquote>

<p>However, I do not have a Google Cloud service, I am just using Google Colab. Is there a way to save a Tensorflow checkpoint when in TPU mode? </p>
",2018-10-26 07:02:30,"<p>Another way to do this is to rewrite the model using Keras and use tf.contrib.tpu.keras_to_tpu_model(..) with tf.contrib.tpu.TPUDistributionStrategy(...). Here is small code snippet for this: </p>

<pre><code>def get_model():
  return keras.Sequential([
    keras.layers.Dense(10, input_shape=(4,), activation=tf.nn.relu, name = ""Dense_1""),
    keras.layers.Dense(10, activation=tf.nn.relu, name = ""Dense_2""),
    keras.layers.Dense(3, activation=None, name = ""logits""),
    keras.layers.Dense(3, activation=tf.nn.softmax, name = ""softmax"")
  ])

dnn_model = get_model()

dnn_model.compile(optimizer=tf.train.AdagradOptimizer(learning_rate=0.1), 
              loss='sparse_categorical_crossentropy',
              metrics=['sparse_categorical_crossentropy'])

tpu_model = tf.contrib.tpu.keras_to_tpu_model(
    dnn_model,
    strategy=tf.contrib.tpu.TPUDistributionStrategy(
        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))

# Train the model
tpu_model.fit(
  train_x, train_y,
  steps_per_epoch = steps_per_epoch,
  epochs=epochs,
)

tpu_model.save_weights('./saved_weights.h5', overwrite=True)
</code></pre>
",3259896,5853,https://stackoverflow.com/questions/53003208,Documentation Replication on Other Examples
51706848,How does tf.reshape() work internally ?,"<p>I'm trying to understand how tf.reshape works. Let's have an example:</p>

<pre><code>embeddings = tf.placeholder(tf.float32, shape=[N0,N1])
M_2D = tf.placeholder(tf.float32, shape=[N0,None])
M_3D = tf.reshape(M_2D, [-1,N0,1])
weighted_embeddings = tf.multiply(embeddings, M_3D)
</code></pre>

<p>Here I have a 2D tensor M_2D whose columns represent coefficients for the N0 embeddings of dimension N1. I want to create a 3D tensor where each column of M_2D is placed in the first dimension of M_3D, and columns are keep in the same order. My final goal is to create a 3D tensor of 2D embeddings, each weighted by the columns of M_2D. </p>

<p>How can I be sure that reshape actually place each column in the new dimension of M_3D. Is it possible that it places the rows instead ? Is there somewhere in tensorflow documentation a clear explanation on the internal working process of tf.reshape, particularly when -1 is provided ?    </p>
",2018-08-06 11:43:20,"<p>Tensor before and after <code>tf.reshape</code> have the <strong>same flatten order</strong>.</p>

<p>In tensorflow runtime, a Tensor is consists of raw data(byte array), shape, and dtype, <code>tf.reshape</code> only change shape, with raw data and dtype not changed. <code>-1</code> or <code>None</code> in <code>tf.reshape</code> means that this value can be calculated.</p>

<p>For example,</p>

<pre><code># a tensor with 6 elements, with shape [3,2]
a = tf.constant([[1,2], [3,4], [5,6]]) 
# reshape tensor to [2, 3, 1], 2 is calculated by 6/3/1
b = tf.reshape(a, [-1, 3, 1])
</code></pre>

<p>In this example, <code>a</code> and <code>b</code> have the same flatten order, namely <code>[1,2,3,4,5,6]</code>, <code>a</code> has the shape <code>[3,2]</code>, its value is <code>[[1,2], [3,4], [5,6]]</code>, <code>b</code> has the shape <code>[2,3,1]</code>, its value is <code>[[[1],[2],[3]],[[4],[5],[6]]]</code>.</p>
",6084245,1551,https://stackoverflow.com/questions/51706848,Documentation Completeness
51762406,"What is the Tensorflow loss equivalent of ""Binary Cross Entropy""?","<p>I'm trying to rewrite a Keras graph into a Tensorflow graph, but wonder which loss function is the equivalent of ""Binary Cross Entropy"". Is it tf.nn.softmax_cross_entropy_with_logits_v2?</p>

<p>Thanks a lot!</p>
",2018-08-09 08:38:04,"<p>No, the implementation of the <code>binary_crossentropy</code> with tensorflow backend is defined <a href=""https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/keras/backend.py"" rel=""nofollow noreferrer"">here</a> as</p>

<pre class=""lang-py prettyprint-override""><code>@tf_export('keras.backend.binary_crossentropy')
def binary_crossentropy(target, output, from_logits=False):
    """"""Binary crossentropy between an output tensor and a target tensor.
    Arguments:
      target: A tensor with the same shape as `output`.
      output: A tensor.
      from_logits: Whether `output` is expected to be a logits tensor.
          By default, we consider that `output`
          encodes a probability distribution.
    Returns:
      A tensor.
    """"""
    # Note: nn.sigmoid_cross_entropy_with_logits
    # expects logits, Keras expects probabilities.
    if not from_logits:
        # transform back to logits
        epsilon_ = _to_tensor(epsilon(), output.dtype.base_dtype)
        output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)
        output = math_ops.log(output / (1 - output))
    return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)
</code></pre>

<p>Therefore, it uses <code>sigmoid_crossentropy</code> and not <code>softmax_crossentropy</code>.</p>
",3337758,115,https://stackoverflow.com/questions/51762406,Documentation Replication on Other Examples
51858970,"tf.gradients() sums over ys, does it?","<p><a href=""https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients"" rel=""noreferrer"">https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients</a></p>

<p>In the documentation for tf.gradients(ys, xs) it states that </p>

<blockquote>
  <p>Constructs symbolic derivatives of sum of ys w.r.t. x in xs</p>
</blockquote>

<p>I am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example:</p>

<pre><code>x_dims = 3
batch_size = 4

x = tf.placeholder(tf.float32, (None, x_dims))

y = 2*(x**2)

grads = tf.gradients(y,x)

sess = tf.Session()

x_val = np.random.randint(0, 10, (batch_size, x_dims))
y_val, grads_val = sess.run([y, grads], {x:x_val})

print('x = \n', x_val)
print('y = \n', y_val)
print('dy/dx = \n', grads_val[0])
</code></pre>

<p>This gives the following output:</p>

<pre><code>x = 
 [[5 3 7]
 [2 2 5]
 [7 5 0]
 [3 7 6]]
y = 
 [[50. 18. 98.]
 [ 8.  8. 50.]
 [98. 50.  0.]
 [18. 98. 72.]]
dy/dx = 
 [[20. 12. 28.]
 [ 8.  8. 20.]
 [28. 20.  0.]
 [12. 28. 24.]]
</code></pre>

<p>This is the output I would expect, simply the derivative dy/dx for every element in the batch. I don't see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: <a href=""https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html"" rel=""noreferrer"">https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html</a>). Why is this necessary?</p>

<p>I am using Tensorflow 1.6 and Python 3.</p>
",2018-08-15 12:45:16,"<p>If y and x have the same shape then the sum over the dy/dx is the sum over exactly one value. However, if you have more than one y for each x, then the gradients are summed.</p>

<pre><code>import numpy as np
import tensorflow as tf

x_dims = 3
batch_size = 4

x = tf.placeholder(tf.float32, (None, x_dims))
y = 2*(x**2)
z = tf.stack([y, y]) # There are twice as many z's as x's

dy_dx = tf.gradients(y,x)
dz_dx = tf.gradients(z,x)

sess = tf.Session()

x_val = np.random.randint(0, 10, (batch_size, x_dims))
y_val, z_val, dy_dx_val, dz_dx_val = sess.run([y, z, dy_dx, dz_dx], {x:x_val})

print('x.shape =', x_val.shape)
print('x = \n', x_val)
print('y.shape = ', y_val.shape)
print('y = \n', y_val)
print('z.shape = ', z_val.shape)
print('z = \n', z_val)
print('dy/dx = \n', dy_dx_val[0])
print('dz/dx = \n', dz_dx_val[0])
</code></pre>

<p>Produces the following output:</p>

<pre><code>x.shape = (4, 3)
x = 
 [[1 4 8]
 [0 2 8]
 [2 8 1]
 [4 5 2]]

y.shape =  (4, 3)
y = 
 [[  2.  32. 128.]
 [  0.   8. 128.]
 [  8. 128.   2.]
 [ 32.  50.   8.]]

z.shape =  (2, 4, 3)
z = 
 [[[  2.  32. 128.]
  [  0.   8. 128.]
  [  8. 128.   2.]
  [ 32.  50.   8.]]

 [[  2.  32. 128.]
  [  0.   8. 128.]
  [  8. 128.   2.]
  [ 32.  50.   8.]]]

dy/dx = 
 [[ 4. 16. 32.]
 [ 0.  8. 32.]
 [ 8. 32.  4.]
 [16. 20.  8.]]
dz/dx = 
 [[ 8. 32. 64.]
 [ 0. 16. 64.]
 [16. 64.  8.]
 [32. 40. 16.]]
</code></pre>

<p>In particular, notice that the values of dz/dx are twice those of dy/dz since they are summed over the inputs to the stack.</p>
",8820311,109,https://stackoverflow.com/questions/51858970,Documentation Ambiguity
52711895,How to run define Tensorflow graph were all variables are in float16 instead instead of float32,"<p>By default, the variables Tensorflow is in float32. To save memory, I'm trying to run in float16. In my graph, every place where I could define the datatype as float16, I did. However, I get an error when I run the code </p>

<p>Here's my code below. </p>

<pre><code>import math
import numpy as np
import tensorflow as tf

vocabulary_size = 10
batch_size = 64 
embedding_size = 100 
num_inputs =4
num_sampled = 128 

graph = tf.Graph()

with graph.as_default(): #took out "" , tf.device('/cpu:0')""


    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, num_inputs ])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])

    embeddings = tf.get_variable( 'embeddings', dtype=tf.float16,
        initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, dtype=tf.float16) )

    softmax_weights = tf.get_variable( 'softmax_weights', dtype=tf.float16,
        initializer= tf.truncated_normal([vocabulary_size, embedding_size],
                             stddev=1.0 / math.sqrt(embedding_size), dtype=tf.float16 ) )

    softmax_biases = tf.get_variable('softmax_biases', dtype=tf.float16,
        initializer= tf.zeros([vocabulary_size], dtype=tf.float16),  trainable=False )

    embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is

    embed_reshaped = tf.reshape( embed, [batch_size*num_inputs, embedding_size] )

    segments= np.arange(batch_size).repeat(num_inputs)

    averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)

    sam_sof_los = tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)

    loss = tf.reduce_mean( sam_sof_los )

    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) 

    saver = tf.train.Saver()
</code></pre>

<p>And this is this is the error message</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    509                 as_ref=input_arg.is_ref,
--&gt; 510                 preferred_dtype=default_dtype)
    511           except TypeError as err:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1143     if ret is None:
-&gt; 1144       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1145 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    980         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
--&gt; 981         (dtype.name, t.dtype.name, str(t)))
    982   return t

ValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: 'Tensor(""sampled_softmax_loss/Log:0"", shape=(64, 1), dtype=float32)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-2-12d508b9e5d7&gt; in &lt;module&gt;()
     46 
     47     sam_sof_los = tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
---&gt; 48                                    labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)
     49 
     50     loss = tf.reduce_mean( sam_sof_los )

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py in sampled_softmax_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name, seed)
   1347       partition_strategy=partition_strategy,
   1348       name=name,
-&gt; 1349       seed=seed)
   1350   labels = array_ops.stop_gradient(labels, name=""labels_stop_gradient"")
   1351   sampled_losses = nn_ops.softmax_cross_entropy_with_logits_v2(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py in _compute_sampled_logits(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name, seed)
   1126     if subtract_log_q:
   1127       # Subtract log of Q(l), prior probability that l appears in sampled.
-&gt; 1128       true_logits -= math_ops.log(true_expected_count)
   1129       sampled_logits -= math_ops.log(sampled_expected_count)
   1130 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    860     with ops.name_scope(None, op_name, [x, y]) as name:
    861       if isinstance(x, ops.Tensor) and isinstance(y, ops.Tensor):
--&gt; 862         return func(x, y, name=name)
    863       elif not isinstance(y, sparse_tensor.SparseTensor):
    864         try:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in sub(x, y, name)
   8316   if _ctx is None or not _ctx._eager_context.is_eager:
   8317     _, _, _op = _op_def_lib._apply_op_helper(
-&gt; 8318         ""Sub"", x=x, y=y, name=name)
   8319     _result = _op.outputs[:]
   8320     _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    544                   ""%s type %s of argument '%s'."" %
    545                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
--&gt; 546                    inferred_from[input_arg.type_attr]))
    547 
    548           types = [values.dtype]

TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type float16 of argument 'x'.
</code></pre>

<p>The error comes from line <code>tf.nn.sampled_softmax_loss</code>.</p>

<p>At first I thought perhaps tf.segment_mean may cast the output as a float32, so I tried casting averaged_embeds to float16 but I still get the same error. </p>

<p>From the documentation, there doesn't seem to be a way to define any data types in sampled_softmax_loss</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss</a></p>
",2018-10-09 00:53:37,"<p>As far as I can tell, you can only do it using a hack.</p>

<p>The issue comes from the call to:</p>

<pre><code>  if sampled_values is None:
      sampled_values = candidate_sampling_ops.log_uniform_candidate_sampler(
          true_classes=labels,
          num_true=num_true,
          num_sampled=num_sampled,
          unique=True,
          range_max=num_classes,
          seed=seed)
</code></pre>

<p>which outputs an object of this type:</p>

<pre><code>LogUniformCandidateSampler(
    sampled_candidates=&lt;tf.Tensor 'LogUniformCandidateSampler:0' shape=(128,) dtype=int64&gt;,
    true_expected_count=&lt;tf.Tensor 'LogUniformCandidateSampler:1' shape=(64, 1) dtype=float32&gt;,
    sampled_expected_count=&lt;tf.Tensor 'LogUniformCandidateSampler:2' shape=(128,) dtype=float32&gt;
)
</code></pre>

<p>The hack would be to generate yourself the <code>LogUniformCandidateSampler</code>, to cast its result as <code>tf.float16</code> and pass it to <code>tf.nn.sampled_softmax_loss</code>.</p>

<pre><code># Redefine it as the tensorflow one is not exposed.
LogUniformCandidateSampler = namedtuple(""namedtuple"", [""sampled_candidates"", ""true_expected_count"", ""sampled_expected_count""]) 
sampled_values = tf.nn.log_uniform_candidate_sampler(
      true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled,
      num_true=1,
      unique=True,
      range_max=vocabulary_size,
      seed=None)

sampled_value_16 = LogUniformCandidateSampler(
    sampled_values.sampled_candidates,
    tf.cast(sampled_values.true_expected_count, tf.float16),
    tf.cast(sampled_values.sampled_expected_count, tf.float16))

sam_sof_los = tf.nn.sampled_softmax_loss(
    weights=softmax_weights,
    biases=softmax_biases,
    inputs=averaged_embeds,
    labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size, 
    sampled_values=sampled_value_16)
</code></pre>

<p>But this is really a hack and it might have unexpected consequences (an expected one would be that the <code>tf.cast</code> operation is not differentiable).</p>
",3259896,5853,https://stackoverflow.com/questions/52711895,Lack of Alternative Solutions/Documentation
52878311,How to extract rows and columns from a 3D array in Tensorflow,"<p>I wanted to do the following indexing operation on a TensorFlow tensor.
What should be the equivalent operations in TensorFlow to get <code>b</code> and <code>c</code> as output? Although <code>tf.gather_nd</code> documentation has several examples but I could not generate equivalent <code>indices</code> tensor to get these results.</p>
<pre><code>import tensorflow as tf
import numpy as np

a=np.arange(18).reshape((2,3,3))

idx=[2,0,1] #it can be any validing re-ordering index list

#These are the two numpy operations that I want to do in Tensorflow
b=a[:,idx,:]
c=a[:,:,idx] 

# TensorFlow operations

aT=tf.constant(a)
idxT=tf.constant(idx)

# what should be these two indices  
idx1T=tf.reshape(idxT, (3,1)) 
idx2T=tf.reshape(idxT, (1,1,3))

bT=tf.gather_nd(aT, idx1T ) #does not work
cT=tf.gather_nd(aT, idx2T)  #does not work

with tf.Session() as sess:
    b1,c1=sess.run([bT,cT])

print(np.allclose(b,b1))
print(np.allclose(c,c1))
</code></pre>
<p>I am not restricted to <code>tf.gather_nd</code> Any other suggestion to achieve the same operations on GPU will be helpful.</p>
<h1>Edit: I have updated the question for a typo:</h1>
<p>old statement: <code>c=a[:,idx]</code>,</p>
<p>New statement: <code>c=a[:,:,idx]</code>
What I wanted to achieve was re-ordering of columns as well.</p>
",2018-10-18 16:11:39,"<p>That can be done with <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer""><code>tf.gather</code></a>, using the <code>axis</code> parameter:</p>

<pre><code>import tensorflow as tf
import numpy as np

a = np.arange(18).reshape((2,3,3))
idx = [2,0,1]
b = a[:, idx, :]
c = a[:, :, idx]

aT = tf.constant(a)
idxT = tf.constant(idx)
bT = tf.gather(aT, idxT, axis=1)
cT = tf.gather(aT, idxT, axis=2)

with tf.Session() as sess:
    b1, c1=sess.run([bT, cT])

print(np.allclose(b, b1))
print(np.allclose(c, c1))
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>True
True
</code></pre>
",4082304,95,https://stackoverflow.com/questions/52878311,Inadequate Examples
52959039,Using tf.layer.conv proper usage - activation and activity_regularizer,"<p>In tensorflow's tf.layer.conv1d webpage <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/conv1d"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/layers/conv1d</a>, they provide the option to set the ""activation"" and ""activity_regularizer"" for your model. I already have a model built, so it would have been nice if I could have just set activity_regularizer = tf.layer.batch_normalization(training=True) or activation=tf.nn.leaky_relu(alpha = 0.001).However, if i try to put any inputs into tf.layer.batch_normalization or tf.nn.leaky_relu they give errors saying there are arguments missing (""inputs"" for former and ""features"" for latter).</p>

<p>TLDR:</p>

<pre><code>          conv1 = tf.layers.conv1d(inputs=_inputs, filters=6, kernel_size=4, strides=1,
                                  padding='same', activation=tf.nn.leaky_relu, activity_regularizer = tf.layers.batch_normalization)
</code></pre>

<p>seems to ""work"" (it runs at least), but this:</p>

<pre><code>          conv1 = tf.layers.conv1d(inputs=_inputs, filters=6, kernel_size=4, strides=1,
                                  padding='same', activation=tf.nn.leaky_relu(alpha = 0.001), activity_regularizer = tf.layers.batch_normalization(training = True, trainable = True))
</code></pre>

<p>says there are missing arguments for activation and activity_regularizer.</p>

<p>I may be using activation and activity_regularizer completely wrong so feel free to correct me. I am hoping there is an easy fix to this, otherwise the only option for me is to write extra lines of code to separate the activation and batch normalizations from conv1d. Although, I do not see the purpose of having activation and activity_regularizer built-in tf.layers.conv1d if I can't change their parameters. </p>

<p>Last point: I am particularly worried about tf.layer.batch_normalization because I am assuming it is set at the default training=False and trainable = True which should not always be the case in the block of code that ""worked"".</p>
",2018-10-23 23:20:30,"<p>The argument to <code>activation</code> should be a function that takes a tensor and returns a tensor. <code>tf.nn.leaky_relu</code> is such a function.</p>

<p><code>tf.nn.leaky_relu(alpha = 0.001)</code> is not valid Python. You are not specifying the only required argument <code>features</code>. Python does not know how to run a function without a required argument. What you want is something like this:</p>

<pre><code>def my_relu(features):
    return tf.nn.leaky_relu(features, alpha=0.001)

tf.layers.conv1d(..., activation=my_relu, ...)
</code></pre>

<p>The argument to <code>activity_regularizer</code> is a function taking your layer's activity (i.e. output) and computing a penalty for it. Usually you give high penalty for large activations. This function should output a scalar. I don't know why <code>activity_regularizer = tf.layers.batch_normalization</code> does not complain but it is probably doing not what you expect. Typical regularizers are <code>l1</code> and <code>l2</code> norms (see <a href=""https://keras.io/regularizers/"" rel=""nofollow noreferrer"">https://keras.io/regularizers/</a>). Batch normalization is a layer, not a regularizer.</p>
",9912116,142,https://stackoverflow.com/questions/52959039,Documentation Replication on Other Examples
53079436,tensorflow Tf.cond giving unexpected output,"<p>I seem to be having a misunderstanding on how <code>tf.cond</code> works. In the tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer"">documentation</a>, it gives the following example:</p>

<pre><code>z = tf.multiply(a, b)
result = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))
</code></pre>

<p>The result of the example, if <code>x&lt;y</code> is <code>True</code> is <code>tf.add(x,z)</code> else <code>tf.square(y)</code></p>

<p>Following this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation.</p>

<p>in my example, <code>deterministic_action = 4</code>, <code>random_action = 11</code>, <code>chose_random=False</code>. The <code>stochastic_action</code> should be <code>4</code>, instead it is <code>1</code>.
Where did the value 1 come from?</p>

<pre><code>#!/usr/bin/env python3

import tensorflow as tf
import numpy as np

with tf.Graph().as_default():
    with tf.device('/cpu:0'):
        stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"")
        eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0))
        with tf.variable_scope('test_cond') as sc:
            deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4
            random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11
            chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) &lt; eps # False because eps = 0
            stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1
            #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action)


    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init, feed_dict={stochastic_ph: True})
    print (""s_ph = "", stochastic_ph)
    d_action = sess.run(deterministic_action)
    print (""det_action= "", d_action)
    r_action = sess.run(random_action)
    print (""rand_action= "", r_action)
    e = sess.run(eps)
    c_action = sess.run(chose_random)
    print (""chose_rand= "", c_action)
    s_action = sess.run(stochastic_action)
    print (""s_action= "", s_action)
    #output = sess.run(output_action)
</code></pre>

<p>here is the output:</p>

<pre><code>python random_vec.py
2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  1
</code></pre>
",2018-10-31 08:46:24,"<p>This is because you are evaluating again in a new sess.run. 
Since you are generating a random number for deterministic_action, the result turns out to be the next random number after 4, which is 1.
Here is the result of your code, when I extract the value of deterministic_action as well in the last step.</p>

<p>Modification:</p>

<pre><code>print (""s_ph = "", stochastic_ph)
d_action = sess.run(deterministic_action)
print (""det_action= "", d_action)
r_action = sess.run(random_action)
print (""rand_action= "", r_action)
e = sess.run(eps)
c_action = sess.run(chose_random)
print (""chose_rand= "", c_action)
s_action, d_action = sess.run([stochastic_action, deterministic_action])
print (""s_action= "", s_action)
print (""det_action= "", d_action)
</code></pre>

<p>Result:</p>

<pre><code>s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  1
det_action=  1
</code></pre>

<p>Now all you need to do is run everything in one sess.run</p>

<pre><code>d_action, r_action, e,  c_action, s_action = sess.run([deterministic_action, random_action, eps, chose_random, stochastic_action])
print (""det_action= "", d_action)
print (""rand_action= "", r_action)
print (""chose_rand= "", c_action)
print (""s_action= "", s_action)
</code></pre>

<p>Result:</p>

<pre><code>s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  4
</code></pre>

<p>Update:</p>

<p>I was not clear on why the random_uniform generates different values when seed is set. This is because the code is running with the same session object that it initialized the variables with.
Modifying the code with a new session object, this is what happens:</p>

<pre><code>print (""s_ph = "", stochastic_ph)
d_action = sess.run(deterministic_action)
print (""det_action= "", d_action)
sess.close()
sess = tf.Session()
sess.run(init, feed_dict={stochastic_ph: True})
s_action = sess.run(stochastic_action)
print (""s_action= "", s_action)
</code></pre>

<p>Result:</p>

<pre><code>s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
s_action=  4
</code></pre>
",1059860,2080,https://stackoverflow.com/questions/53079436,Documentation Replication on Other Examples
52471675,Check if Tensor is Placeholder?,"<p>Placeholders are recognized as Tensors in TensorFlow. </p>

<p><code>isinstance(tf.placeholder(""float"", []), tf.Tensor)</code> returns <code>True</code></p>

<p>Is there a way to check if a Tensor is a placeholder specifically? Something like:</p>

<pre><code>isinstance(tf.placeholder(""float"", []), tf.Placeholder)
</code></pre>

<p>Unfortunately for the API I'm building, <code>tf.Placeholder</code> is not an actual instance type in TensorFlow.</p>
",2018-09-24 01:22:20,"<p>You can check it with <code>op.type</code>:</p>

<pre><code>assert tf.placeholder(""float"", []).op.type == 'Placeholder'
</code></pre>
",10210261,301,https://stackoverflow.com/questions/52471675,Documentation Replication on Other Examples
53206900,Sound way of managing multiple sessions and graphs,"<p>I'd like to manage multiple Keras models in multiple sessions. My application is constructed such that models can be live at the same time, in addition to creating, saving and loading them.</p>

<p><strong>What is the proper way of managing this situation?</strong></p>

<p>Currently one model is represented by an instance of a wrapper class. This is used in the training, saving, loading and prediction. One <code>tf.Graph</code> and <code>tf.Session</code> is created per instance, and they are used in every function requiring the actual model.</p>

<pre><code>class NN:
    def __init__(self):
        self.graph = tf.Graph()
        self.session = tf.Session(graph=self.graph)

    def predict(self, x):
        with self.graph.as_default():
            with self.session.as_default():
                return self.model.predict(x)
</code></pre>

<p>Similar functions using the <code>with</code> statements are created for compiling the network, fitting, saving (weights to .h5 and model to JSON) and loading. So whenever the model is needed, the graph and session are brought to context.</p>

<p>This resulted in a strange error (<a href=""https://stackoverflow.com/questions/53002518/poor-exit-code-when-managing-multiple-sessions"">Q</a> for further context), and I was left wondering, what is the standard way of dealing with this. I tried to release all possible resources before creating or loading a model, but it hasn't helped. This function is just a compilation of all possible routines scraped off the internet, and is purely guesswork.</p>

<pre><code>def _new_session(self):
    if self.session is not None:
        self.session.close()
    k.clear_session()
    gc.collect()
    self.graph = tf.Graph()
    self.session = tf.Session(graph=self.graph)
</code></pre>

<p>I've not found good documentation of a similar situation. So I'd very much appreciate any real insight into this.</p>

<hr>

<p>I might need to delete the old question, as it's quite all over the place. At the time of asking I had no idea what was going on. But it's there for now.</p>

<hr>

<p>Some specific questions have arisen.</p>

<ul>
<li>Loading and making predictions on a model works, compiling and fitting doesn't, although just compiling does. Do the two contexts differ in any way? Is the loaded model exactly the same?</li>
<li>At which points should a new context be created when manipulating the models? (e.g. at load, compilation, fitting, probably not with every prediction)</li>
<li>Which actions are necessary to take when releasing the resources of previous contexts? Either when a network is disposed of or when creating a new context.</li>
<li>Why exactly is the context switch needed for multiple models?</li>
<li>What are the roles of graph vs. session, given that different things are executed on the graph and session?</li>
</ul>

<h3>Updates</h3>

<ul>
<li>Compiling, fitting and saving one network works without any context trickery. Doing the same for another model in the same context works too (or at least does not produce an error).</li>
<li>In addition to above, <strong>loading the saved model and predicting works too</strong>, right after the training and for both models! Now I'm not sure whether the prediction is made correctly, but again, no error. This only begs the question I posed above even more: <em>why are the different contexts needed?</em></li>
</ul>

<hr>

<p>The underlying issue with the error has been finally (and somewhat embarassingly) <a href=""https://stackoverflow.com/questions/53002518/poor-exit-code-when-managing-multiple-sessions"">resolved</a> by updating all packages.</p>
",2018-11-08 11:31:55,"<p>EDIT: Actually, seeing again how <code>K.get_session()</code> works, it should return the current default session, so I'm not sure <code>set_session</code> is doing anything meaningful there. I'll leave the answer just in case you want to try but probably this won't help.</p>

<hr>

<p>Maybe you can get it to work with something like this:</p>

<pre><code>from contextlib import contextmanager

class NN:
    def __init__(self):
        self.graph = tf.Graph()
        self.session = tf.Session(graph=self.graph)

    def predict(self, x):
        with self._context():
            return self.model.predict(x)

    @contextmanager
    def _context(self):
        prev_sess = K.get_session()
        K.set_session(self.session)
        with self.graph.as_default(), self.session.as_default():
            yield
        K.set_session(prev_sess)
</code></pre>

<p>Note that the Keras session object is a global variable, so I suppose this should work as long as you don't try to use these contexts from multiple threads.</p>
",7089239,2588,https://stackoverflow.com/questions/53206900,Lack of Alternative Solutions/Documentation
52572275,tensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)?,"<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/manip/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd documentation</a> and run the example code for 1D and 3D tensors... and now I'm trying to do it for a 2D tensor.  I want to 'interleave' the columns of two tensors.  For 1D tensors, one can do this via</p>

<pre><code>'''
We want to interleave elements of 1D tensors arr1 and arr2, where
arr1 = [10, 11, 12]
arr2 = [1, 2, 3, 4, 5, 6]
such that
desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12]
'''

import tensorflow as tf

with tf.Session() as sess:

    updates1 = tf.constant([1,2,3,4,5,6])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)

    updates2 = tf.constant([10,11,12])
    indices2 = tf.constant([[2], [5], [8]])
    scatter2 = tf.scatter_nd(indices2, updates2, shape)

    result = scatter1 + scatter2

    print(sess.run(result))
</code></pre>

<p>(aside: is there a <em>better</em> way to do this?  I'm all ears.)</p>

<p>This gives the output</p>

<p><code>[ 1  2 10  3  4 11  5  6 12]</code></p>

<p>Yay! that worked!</p>

<p>Now lets' try to extend this to 2D.</p>

<pre><code>    '''
    We want to interleave the *columns* (not rows; rows would be easy!) of

    arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]
    arr2 = [[10 11 12], [10 11 12], [10 11 12]]
    such that
    desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]]
    '''

    updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([3, 9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)
</code></pre>

<p>This gives the error
<code>ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1
dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but
are 6 and 3. Shapes are [6] and [3]. for 'ScatterNd_2' (op: 'ScatterNd') with
input shapes: [6,1], [3,6], [2].</code></p>

<p>Seems like my <code>indices</code> is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean
I need to <em>explicitly</em> specify every single pair of indices for every element in <code>updates1</code>?
Or is there some kind of 'wildcard' specification I can use for the rows? (Note <code>indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]])</code> gives syntax errors, as it probably should.)</p>

<p>Would it be easier to just do a transpose, interleave the rows, then transpose back?
Because I tried that...</p>

<pre><code>scatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))
print(sess.run(tf.transpose(scatter1)))
</code></pre>

<p>...and got a <em>much</em> longer error message, that I don't feel like posting unless someone requests it. </p>

<p>PS- I searched to make sure this isn't a duplicate -- I find it hard to imagine that someone else hasn't asked this before -- but turned up nothing. </p>
",2018-09-29 20:24:35,"<p>This is pure slicing but I didn't know that syntax like <code>arr1[0:,:][:,:2]</code> actually works. It seems it does but not sure if it is better.</p>

<p>This may be the <em>wildcard</em> slicing mechanism you are looking for.</p>

<pre><code>arr1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,7],[1,2,3,4,5,8]])
arr2 = tf.constant([[10, 11, 12], [10, 11, 12], [10, 11, 12]])

with tf.Session() as sess :
    sess.run( tf.global_variables_initializer() )
    print(sess.run(tf.concat([arr1[0:,:][:,:2], arr2[0:,:] [:,:1],
                              arr1[0:,:][:,2:4],arr2[0:, :][:, 1:2],
                              arr1[0:,:][:,4:6],arr2[0:, :][:, 2:3]],axis=1)))
</code></pre>

<p>Output is</p>

<pre><code>[[ 1  2 10  3  4 11  5  6 12]
 [ 1  2 10  3  4 11  5  7 12]
 [ 1  2 10  3  4 11  5  8 12]]
</code></pre>

<p>So, for example,</p>

<p><code>arr1[0:,:]</code> returns</p>

<pre><code>[[1 2 3 4 5 6]
 [1 2 3 4 5 7]
 [1 2 3 4 5 8]]
</code></pre>

<p>and <code>arr1[0:,:][:,:2]</code> returns the first two columns</p>

<pre><code>[[1 2]
 [1 2]
 [1 2]]
</code></pre>

<p>axis is 1.</p>
",4259243,1472,https://stackoverflow.com/questions/52572275,Documentation Replication on Other Examples
53612973,TensorFlow Sigmoid Cross Entropy with Logits for 1D data,"<h1>Context</h1>

<p>Suppose we have some 1D data (e.g. time series), where all series have fixed length <em>l</em>:</p>

<pre><code>        # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11] index
example = [ 0,  1,  1,  0, 23, 22, 20, 14,  9,  2,  0,  0] # l = 12
</code></pre>

<p>and we want to perform semantic segmentation, with <em>n</em> classes:</p>

<pre><code>          # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]    index            
labeled = [
            [ 0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0], # class 1
            [ 0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  0], # class 2
            [ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  0,  0], # class 3
           #[                     ...                      ],
            [ 1,  1,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1], # class n
 ]
</code></pre>

<p>then the output for a single example has shape <code>[n, l]</code> (i.e. the <code>data_format</code> is not <code>""channels_last""</code>) and the batched output has shape <code>[b, n, l]</code>, where <code>b</code> is the number of examples in the batch.</p>

<p>These classes are independent, so it is my understanding that the use <em>sigmoid</em> cross entropy is applicable here as the loss rather than softmax cross entropy.</p>

<hr>

<h1>Question</h1>

<p>I have a few small related questions in regards to the expected format for and use of <code>tf.nn.sigmoid_cross_entropy_with_logits</code>:</p>

<ol>
<li><p>since the network outputs a tensor in the same shape as the batched labels, should I train the network under the assumption that it outputs logits, or take the keras approach (see keras's <code>binary_crossentropy</code>) and assume it outputs probabilities?</p></li>
<li><p>given the 1d segmentation problem, should I call <code>tf.nn.sigmoid_cross_entropy_with_logits</code> on:</p>

<ul>
<li><code>data_format='channels_first'</code> (as shown above), or</li>
<li><code>data_format='channels_last'</code>  (example.T)</li>
</ul>

<p>if I want the labels to be assigned individually per channel?</p></li>
<li><p>should the loss operation passed to the optimizer be:</p>

<ul>
<li><code>tf.nn.sigmoid_cross_entropy_with_logits(labels, logits)</code>, </li>
<li><code>tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels, logits))</code>, or</li>
<li><code>tf.losses.sigmoid_cross_entropy</code>?</li>
</ul></li>
</ol>

<hr>

<h1>Code</h1>

<p>This <a href=""https://colab.research.google.com/drive/12EiVRy8Tdt4UyvSuuBr3wcXfCmXPVFku"" rel=""nofollow noreferrer"">Colab</a>, highlights my confusion and demonstrates that the <code>data_format</code> does in fact matter..., but the documentation does not explicitly state which is expected.</p>

<h2>Dummy data</h2>

<pre><code>c = 5  # number of channels (label classes)
p = 10 # number of positions ('pixels')


# data_format = 'channels_first', shape = [classes, pixels]
# 'logits' for 2 examples
pred_1 = np.array([[random.random() for v in range(p)]for n in range(c)]).astype(float)
pred_2 = np.array([[random.random() for v in range(p)]for n in range(c)]).astype(float)

# 'ground truth' for the above 2 examples
targ_1 = np.array([[0 if random.random() &lt; 0.8 else 1 for v in range(p)]for n in range(c)]).astype(float)
targ_2 = np.array([[0 if random.random() &lt; 0.8 else 1 for v in range(p)]for n in range(c)]).astype(float)

# batched form of the above examples
preds = np.array([pred_1, pred_2])
targs = np.array([targ_1, targ_2])


# data_format = 'channels_last', shape = [pixels, classes]
t_pred_1 = pred_1.T
t_pred_2 = pred_2.T
t_targ_1 = targ_1.T
t_targ_2 = targ_2.T

t_preds = np.array([t_pred_1, t_pred_2])
t_targs = np.array([t_targ_1, t_targ_2])
</code></pre>

<h2>losses</h2>

<h3>tf.nn</h3>

<pre><code># calculate individual losses for 'channels_first'
loss_1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=targ_1, logits=pred_1)
loss_2 = tf.nn.sigmoid_cross_entropy_with_logits(labels=targ_2, logits=pred_2)
# calculate batch loss for 'channels_first'
b_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targs, logits=preds)

# calculate individual losses for 'channels_last'
t_loss_1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=t_targ_1, logits=t_pred_1)
t_loss_2 = tf.nn.sigmoid_cross_entropy_with_logits(labels=t_targ_2, logits=t_pred_2)
# calculate batch loss for 'channels_last'
t_b_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=t_targs, logits=t_preds)
# get actual tensors
with tf.Session() as sess:
  # loss for 'channels_first'
  l1   = sess.run(loss_1)
  l2   = sess.run(loss_2)
  # batch loss for 'channels_first'
  bl   = sess.run(b_loss)

  # loss for 'channels_last'
  t_l1 = sess.run(t_loss_1)
  t_l2 = sess.run(t_loss_2)

  # batch loss for 'channels_last'
  t_bl = sess.run(t_b_loss)
</code></pre>

<h3>tf.reduced_mean(tf.nn)</h3>

<pre><code># calculate individual losses for 'channels_first'
rm_loss_1 = tf.reduce_mean(loss_1)
rm_loss_2 = tf.reduce_mean(loss_2)
# calculate batch loss for 'channels_first'
rm_b_loss = tf.reduce_mean(b_loss)

# calculate individual losses for 'channels_last'
rm_t_loss_1 = tf.reduce_mean(t_loss_1)
rm_t_loss_2 = tf.reduce_mean(t_loss_2)
# calculate batch loss for 'channels_last'
rm_t_b_loss = tf.reduce_mean(t_b_loss)
# get actual tensors
with tf.Session() as sess:
  # loss for 'channels_first'
  rm_l1   = sess.run(rm_loss_1)
  rm_l2   = sess.run(rm_loss_2)
  # batch loss for 'channels_first'
  rm_bl   = sess.run(rm_b_loss)

  # loss for 'channels_last'
  rm_t_l1 = sess.run(rm_t_loss_1)
  rm_t_l2 = sess.run(rm_t_loss_2)

  # batch loss for 'channels_last'
  rm_t_bl = sess.run(rm_t_b_loss)
</code></pre>

<h3>tf.losses</h3>

<pre><code># calculate individual losses for 'channels_first'
tf_loss_1 = tf.losses.sigmoid_cross_entropy(multi_class_labels=targ_1, logits=pred_1)
tf_loss_2 = tf.losses.sigmoid_cross_entropy(multi_class_labels=targ_2, logits=pred_2)
# calculate batch loss for 'channels_first'
tf_b_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=targs, logits=preds)

# calculate individual losses for 'channels_last'
tf_t_loss_1 = tf.losses.sigmoid_cross_entropy(multi_class_labels=t_targ_1, logits=t_pred_1)
tf_t_loss_2 = tf.losses.sigmoid_cross_entropy(multi_class_labels=t_targ_2, logits=t_pred_2)
# calculate batch loss for 'channels_last'
tf_t_b_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=t_targs, logits=t_preds)
# get actual tensors
with tf.Session() as sess:
  # loss for 'channels_first'
  tf_l1   = sess.run(tf_loss_1)
  tf_l2   = sess.run(tf_loss_2)
  # batch loss for 'channels_first'
  tf_bl   = sess.run(tf_b_loss)

  # loss for 'channels_last'
  tf_t_l1 = sess.run(tf_t_loss_1)
  tf_t_l2 = sess.run(tf_t_loss_2)

  # batch loss for 'channels_last'
  tf_t_bl = sess.run(tf_t_b_loss)
</code></pre>

<h2>Test equivalency</h2>

<h3>data_format equivalency</h3>

<pre><code># loss _should_(?) be the same for 'channels_first' and 'channels_last' data_format
# test example_1
e1 = (l1 == t_l1.T).all()
# test example 2
e2 = (l2 == t_l2.T).all()

# loss calculated for each example and then batched together should be the same 
# as the loss calculated on the batched examples
ea = (np.array([l1, l2]) == bl).all()
t_ea = (np.array([t_l1, t_l2]) == t_bl).all()

# loss calculated on the batched examples for 'channels_first' should be the same
# as loss calculated on the batched examples for 'channels_last'
eb = (bl == np.transpose(t_bl, (0, 2, 1))).all()


e1, e2, ea, t_ea, eb
# (True, False, False, False, True) &lt;- changes every time, so True is happenstance
</code></pre>

<h3>equivalency between tf.reduce_mean and tf.losses</h3>

<pre><code>l_e1 = tf_l1 == rm_l1
l_e2 = tf_l2 == rm_l2
l_eb = tf_bl == rm_bl

l_t_e1 = tf_t_l1 == rm_t_l1
l_t_e2 = tf_t_l2 == rm_t_l2
l_t_eb = tf_t_bl == rm_t_bl

l_e1, l_e2, l_eb, l_t_e1, l_t_e2, l_t_eb
# (False, False, False, False, False, False)
</code></pre>
",2018-12-04 12:25:23,"<p>Both <code>tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(...))</code> and <code>tf.losses.sigmoid_cross_entropy(...)</code> (with default arguments) are computing the same thing. The problem is in your tests where you use <code>==</code> to compare two floating-point numbers. Instead, use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.isclose.html"" rel=""nofollow noreferrer""><code>np.isclose</code></a> method to check whether two floating-point numbers are equal or not:</p>

<pre><code># loss _should_(?) be the same for 'channels_first' and 'channels_last' data_format
# test example_1
e1 = np.isclose(l1, t_l1.T).all()
# test example 2
e2 = np.isclose(l2, t_l2.T).all()

# loss calculated for each example and then batched together should be the same 
# as the loss calculated on the batched examples
ea = np.isclose(np.array([l1, l2]), bl).all()
t_ea = np.isclose(np.array([t_l1, t_l2]), t_bl).all()

# loss calculated on the batched examples for 'channels_first' should be the same
# as loss calculated on the batched examples for 'channels_last'
eb = np.isclose(bl, np.transpose(t_bl, (0, 2, 1))).all()


e1, e2, ea, t_ea, eb
# (True, True, True, True, True)
</code></pre>

<p>And:</p>

<pre><code>l_e1 = np.isclose(tf_l1, rm_l1)
l_e2 = np.isclose(tf_l2, rm_l2)
l_eb = np.isclose(tf_bl, rm_bl)

l_t_e1 = np.isclose(tf_t_l1, rm_t_l1)
l_t_e2 = np.isclose(tf_t_l2, rm_t_l2)
l_t_eb = np.isclose(tf_t_bl, rm_t_bl)

l_e1, l_e2, l_eb, l_t_e1, l_t_e2, l_t_eb
# (True, True, True, True, True, True)
</code></pre>
",5623899,4978,https://stackoverflow.com/questions/53612973,Lack of Alternative Solutions/Documentation
53649402,Disable TensorBoard logging on tf.Estimator methods,"<p>Is there a way disable the automatic TensorBoard logging when using the <code>tf.estimator.Estimator</code> class?</p>

<pre><code># Classifier
classifier = tf.estimator.Estimator(
    model_fn=lambda features, labels, mode, params: model(features, labels, mode, params, word_embeddings),
    params=params,
    model_dir=str(MODEL_DIR),
    tensorboard=Fasle)  # &lt;---- Something like that?

for _ in range(params['epochs']):
    classifier.train(input_fn=lambda: input_fn(generator, example_generator._data['train'] ,batch_size=params['batch_size']))
    classifier.evaluate(input_fn=lambda: input_fn(generator, example_generator._data['validation'], batch_size=params['batch_size']))
</code></pre>

<p>I read through the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"" rel=""nofollow noreferrer""><code>tf.estimator.RunConfig</code></a> and couldn't find a solution.</p>
",2018-12-06 10:33:07,"<p>Instantiating RunConfig for Estimator with <code>save_summary_steps=None</code> should fix this</p>
",5368083,12085,https://stackoverflow.com/questions/53649402,Lack of Alternative Solutions/Documentation
52671481,Why are variables defined with 'self' automatically given a ListWrapper() while inheriting from tf.keras.Model?,"<p>I am not familiar with <code>ListWrapper()</code>, but it is being applied to all list variables created with <code>self</code> when my class inherits from <code>tf.keras.Model</code>. 
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/models/Model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/models/Model</a></p>

<p>This is bad because it is causing an <code>IndexError</code> when I use it in certain functions, or even by just passing it through my Tensorflow model. (I am using eager execution)</p>

<p>A small reproduction of the problem can be seen with this code:</p>

<pre><code>import tensorflow as tf

class my_class(tf.keras.Model):

    def __init__(self):
        super(my_class, self).__init__()

        self.x = [0]
        print(self.x)

model = my_class()
</code></pre>

<p>Output:</p>

<pre><code>ListWrapper([0])
</code></pre>

<p>Switching the inheritance to be from <code>object</code> solves the issue, which is how I know its <code>tf.keras.Model</code> that is causing this.</p>

<p>I tried looking it up but can't find anything on this. Any tips? Thanks!</p>
",2018-10-05 18:34:50,"<p>Turns out this was a bug in Tensorflow between tf.keras.Model and eager execution. 
This was not ""how the tensorflow has 'patched' setting attributes"", as suggested by a comment.</p>

<p>This is a link to the closed issue on Tensorflow:
<a href=""https://github.com/tensorflow/tensorflow/issues/22853"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/22853</a></p>

<p>If you have this problem, it should be fixed in the next Tensorflow update.
This bug was in version 1.11.0</p>
",10458815,763,https://stackoverflow.com/questions/52671481,Documentation Replication on Other Examples
52872239,Can tf.scatter_update or tf.scatter_nd_update be used to update column slices of a tensor?,"<p>I want to implement a function that takes in a variable as input, mutate some of its rows or columns and replaces them back in the original variable. I am able to implement it for row slices using tf.gather and tf.scatter_update but unable to do so for column slices since apparently tf.scatter_update only updates the row slices and does not have an axis feature. I am not an expert in tensorflow therefore I may be missing something. Can someone help?</p>
<pre><code>def matrix_reg(t, percent_t, beta):
    
    ''' Takes a variable tensor t as input and regularizes some of its rows.
    The number of rows to be regularized are specified by the percent_t. Returns the original tensor by updating its rows indexed by row_ind.
    
    Arguments:
        t -- input tensor
        percent_t -- percentage of the total rows
        beta -- the regularization factor
    Output:
        the regularized tensor
        '''
    row_ind = np.random.choice(int(t.shape[0]), int(percent_t*int(t.shape[0])), replace = False)
    t_ = tf.gather(t,row_ind)
    t_reg = (1+beta)*t_-beta*(tf.matmul(tf.matmul(t_,tf.transpose(t_)),t_))
    return tf.scatter_update(t, row_ind, t_reg)
</code></pre>
",2018-10-18 10:43:25,"<p>Here is a small demonstration of how to update rows or columns. The idea is that you specify the row and column indices of the variables where you want each element in the update to end up. That is easy to do with <a href=""https://www.tensorflow.org/api_docs/python/tf/meshgrid"" rel=""nofollow noreferrer""><code>tf.meshgrid</code></a>.</p>

<pre><code>import tensorflow as tf

var = tf.get_variable('var', [4, 3], tf.float32, initializer=tf.zeros_initializer())
updates = tf.placeholder(tf.float32, [None, None])
indices = tf.placeholder(tf.int32, [None])
# Update rows
var_update_rows = tf.scatter_update(var, indices, updates)
# Update columns
col_indices_nd = tf.stack(tf.meshgrid(tf.range(tf.shape(var)[0]), indices, indexing='ij'), axis=-1)
var_update_cols = tf.scatter_nd_update(var, col_indices_nd, updates)
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    print('Rows updated:')
    print(sess.run(var_update_rows, feed_dict={updates: [[1, 2, 3], [4, 5, 6]], indices: [3, 1]}))
    print('Columns updated:')
    print(sess.run(var_update_cols, feed_dict={updates: [[1, 5], [2, 6], [3, 7], [4, 8]], indices: [0, 2]}))
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>Rows updated:
[[0. 0. 0.]
 [4. 5. 6.]
 [0. 0. 0.]
 [1. 2. 3.]]
Columns updated:
[[1. 0. 5.]
 [2. 5. 6.]
 [3. 0. 7.]
 [4. 2. 8.]]
</code></pre>
",9936406,63,https://stackoverflow.com/questions/52872239,Documentation Replication on Other Examples
51895395,How to pass tf.glorot_uniform_initializer() into tf.Variable(0 not get_variable(),"<p>For <code>tensorflow</code> I would like to create a <code>tf</code> variable using <code>tf.Variable</code></p>

<pre><code>test = tf.Variable()
</code></pre>

<p>and using<code>tf.glorot_uniform_initializer()</code>with the <code>shape = [ 2 , 3 ]</code></p>

<p>I know I can use</p>

<pre><code>w2 =tf.get_variable( ""w2"" , shape = [ hiddensize ,
    outputsize ] , initializer =
    tf.glorot_uniform_initializer() , dtype =
    tf.float32
    )
</code></pre>

<p>but it may potential gets scope error, <code>tf.Variable()</code> won't.</p>

<p>If anyone can help me, I would much appreciate it.</p>
",2018-08-17 12:22:56,"<p>You can get an initial value from the initializer just using it as a callable:</p>

<pre><code>test = tf.Variable(tf.glorot_uniform_initializer()((2, 3)))
</code></pre>
",8461300,155,https://stackoverflow.com/questions/51895395,Documentation Replication on Other Examples
52969867,make tensorflow dataset from huge number of images(*.jpg) and labels(*.mat),"<p>I have a huge number of images with their labels (.mat) file (cannot use <code>tf.data.Dataset.from_tensor_slices()</code>) and I want to use <code>tf.data</code> API to make a tensorflow dataset out of it.</p>

<p>As I read in the documentation, I can use <code>tf.data.TextLineDataset</code> for large number of data(I have to have a txt file with the address of all the images and send the path of the txt file as <code>tf.data.TextLineDataset</code> argument).
Then, I can use <code>map</code> method to read txt file (<code>tf.read_file</code>) decode jpg image (<code>tf.image.decode_jpeg</code>) and do some basic transformation on the image.</p>

<p>However, I cannot use <code>scipy.io.loadmat</code> in any part of <code>map</code> method because I have no string indicating the path to the mat file. All I have is <code>tf.Tensor</code>.</p>

<p>I don't think that reading all images and making a TFRecord out of it is that much efficient in this case because then I am basically doing every thing two times. Once, reading the whole images and making TFRecord, and once again, reading TFRecord to make tensorflow dataset.</p>

<p>Any idea how I can resolve this issue?</p>

<p>This is my code:</p>

<pre><code>dataset = tf.data.TextLineDataset(txt_file).map(read_img_and_mat)
</code></pre>

<p>and then:</p>

<pre><code>def read_img_and_mat(path):
    image_string = tf.read_file(path)
    image_decoded = tf.image.decode_jpeg(image_string, channels=3)
    label = ... # get label from mat file
    return image_decoded, label
</code></pre>
",2018-10-24 13:08:06,"<p>I found a way to do it using <code>tf.data.from_generator</code>
The trick I found was to make two separate Dataset (one for mat file and one for the jpg file) and then to combine them using <code>tf.data.Dataset.zip</code></p>

<p>Here is how it works:</p>

<pre><code>mat_dataset = tf.data.Dataset.from_generator(read_mat_file, tf.int64)

def read_mat_file():
    while True:
        with open('mat_addresses.txt', 'r') as input_:
            for line in input_:
                # open file and extract info from it as np.array
                yield tuple(label)  # why tuple? https://github.com/tensorflow/tensorflow/issues/13101
</code></pre>

<p>in order to get the next batch one just have to do:</p>

<pre><code>iter = mat_dataset.make_one_shot_iterator()
sess.run(iter.get_next())
</code></pre>

<p>however, one can make <code>img_dataset</code> and combine it with <code>mat_dataset</code> like this:</p>

<pre><code>img_dataset = tf.data.TextLineDataset('img_addresses.txt').map(read_img)

def read_img(path):
    image_string = tf.read_file(path)
    image_decoded = tf.image.decode_jpeg(image_string, channels=3)
    return image_decoded

dataset = tf.data.Dataset.zip((mat_dataset, img_dataset))
</code></pre>

<p>and now, getting next next batch like mentioned.</p>

<p>PS. I have no idea about how efficient the code works in comparison to <code>feed_dict</code></p>
",10354279,148,https://stackoverflow.com/questions/52969867,Documentation Replication on Other Examples
52976606,Global step not incrementing with batch norm and custom estimator,"<p>I have a customer estimator that has several layers that look like the following in the model function:</p>

<pre><code>natural_layer = tf.layers.dense(inputs = natural_layer, 
                                units = units, 
                                activation = None,
                                use_bias = False,
                                kernel_regularizer = params['regularizer'],
                                name = 'pre_batch_norm_layer_' + str(i + 1))

natural_layer = tf.layers.batch_normalization(natural_layer,
                                              axis = 1,
                                              center = True,
                                              scale = True,
                                              training = (mode == tf.estimator.ModeKeys.TRAIN),
                                              name = 'batch_norm_layer_' + str(i + 1))

natural_layer = params['natural_layer_activation'](natural_layer, name = 'activation_layer_' + str(i + 1))
</code></pre>

<p>Because I'm using batch norm, the training op is set up like this:</p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    optimizer = tf.contrib.opt.MultitaskOptimizerWrapper(params['optimization_algorithm'](params['training_rate']))
    train_op = optimizer.minimize(loss, global_step = tf.train.get_global_step())
</code></pre>

<p>Where the optimizer is usually tf.train.AdamOptimizer.</p>

<p>However, when I go to train the estimator the global step never increments (so training will run forever), and I get this:</p>

<p>WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.</p>

<p>I am passing tf.train.get_global_step() to minimize, so I'm not sure why it never gets updated. My hunch is that it has something to do with the batch normalization because when I remove that or replace it with dropout, everything works fine (even when keeping the update ops lines that are required for batch normalization per the documentation).</p>

<p>Anyone know what is going on? Happy to post more code if helpful. </p>
",2018-10-24 19:29:33,"<p>I couldn't figure out why the global step was not incrementing automatically, but manually incrementing the global step as follows by adding it to the train_op with tf.group is a nice work around. </p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        optimizer = tf.contrib.opt.MultitaskOptimizerWrapper(params['optimization_algorithm'](params['training_rate']))
        train_op = optimizer.minimize(loss)

        global_step = tf.train.get_global_step()
        update_global_step = tf.assign(global_step, global_step + 1, name = 'update_global_step')

        return tf.estimator.EstimatorSpec(mode, loss = loss, train_op = tf.group(train_op, update_global_step))
</code></pre>
",9226290,111,https://stackoverflow.com/questions/52976606,Documentation Replication on Other Examples
52219099,Is there an equivalent to tf.convert_to_tensor in tensorflow c++?,"<p>I have mean.npy file which I have to convert into tensor. I found that tf.convert_to_tensor does that but could not find the equivalent for it in C++.
Does anybody know the equivalent function in C++ ?</p>
",2018-09-07 09:05:09,"<p>There is not a provided way to read a <code>.npy</code> file into a <code>tensorflow::Tensor</code>. First you would need to read the file, which is not trivial but it is not too hard either, checkout the <a href=""https://www.numpy.org/devdocs/reference/generated/numpy.lib.format.html"" rel=""nofollow noreferrer"">NPY format documentation</a>. Once you have that, the easiest thing would be to copy the data to a tensor:</p>

<pre><code>// Existing NPY reading function, assuming float type
bool read_npy(const char* file, std::vector&lt;float&gt;&amp; npy_values, std::vector&lt;int64_t&gt;&amp; shape);
// Read file
std::vector&lt;float&gt; npy_values;
std::vector&lt;int64_t&gt; shape;
if (!read_npy(""data.npy"", npy_values, shape))
{
    // error...
}
// Make tensor
tensorflow::TensorShape tensorShape;
for (int64_t dim : shape)
{
    tensorShape.AddDim(dim);
}
tensorflow::Tensor tensor(DT_FLOAT, tensorShape);
// Copy data
std::copy(npy_values.begin(), npy_values.end(), tensor.flat&lt;float&gt;().data());
// Or with memcpy
std::memcpy(tensor.flat&lt;float&gt;().data(), npy_values.data(), tensor.NumElements() * sizeof(float));
</code></pre>

<p>Note that this assumes that the NPY data buffer is in row-major order like TensorFlow tensors, and I suppose <a href=""https://www.tensorflow.org/api_docs/cc/class/tensorflow/tensor#classtensorflow_1_1_tensor_1aec1fa00d85d569b344a55ad89f915ce9"" rel=""nofollow noreferrer""><code>IsAligned()</code></a> should be true for the tensor, although afaik that should always be true for new tensors.</p>

<p>Another option would be to first create the tensor and then use its buffer (<code>tensor.flat&lt;float&gt;().data()</code>) to write the read values. This however requires a bit more of work, because you would need to first read the shape of the tensor in the file (or fix it beforehand), create the tensor and then read the file into its buffer (in this case the reading function would receive a pointer and not allocate any memory).</p>

<p>EDIT: I just realised you said ""Assuming I have a utility function to read the .npy file and it returns a float pointer to the array"", not a vector. Well the idea should be the same, you can still use <code>memcpy</code> or <code>copy</code> like:</p>

<pre><code>std::copy(npy_ptr, npy_ptr + tensor.NumElements(), tensor.flat&lt;float&gt;().data());
</code></pre>
",3719483,175,https://stackoverflow.com/questions/52219099,Documentation Replicability
53033780,Indexing Ops in TF,"<p>Is there a way to index an operation in tensorflow? In particular, I'm interested in indexing by the iterator variable of <code>tf.while_loop</code>.</p>

<p>More concretely, let's say I have <code>my_ops = [op1, op2]</code>. I would like to have:</p>

<pre><code>my_ops = [...]
i = tf.constant(0)
c = lambda i: tf.less(i, 10)
b = lambda i: my_ops[i](...)
r = tf.while_loop(c, b, [i])
</code></pre>

<p>which unfortunately will not work, since a python array supports only integer indexing.</p>
",2018-10-28 16:40:41,"<p>I believe this is not possible. However, you could instead use <a href=""https://www.tensorflow.org/api_docs/python/tf/stack"" rel=""nofollow noreferrer"">tf.stack</a> to stack the operations' output tensors, and then use <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer"">tf.gather</a> to obtain the desired output.</p>

<p>Here you have an example:</p>

<pre><code>import tensorflow as tf


def condition(i, x):
    return tf.less(i, 10)


def body_1(my_ops):
    def b(i, x):
        stacked_results = tf.stack([op(x) for op in my_ops])
        gather_idx = tf.mod(i, 2)
        return [i + 1, tf.gather(stacked_results, gather_idx)]

    return b


def body_2(my_ops):
    def b(i, x):
        nb_ops = len(my_ops)
        pred_fn_pairs = [(tf.equal(tf.mod(i, nb_ops), 0), lambda: my_ops[0](x)),
                         (tf.equal(tf.mod(i, nb_ops), 1), lambda: my_ops[1](x))]
        result = tf.case(pred_fn_pairs)
        return [i + 1, result]

    return b


my_ops = [lambda x: tf.Print(x + 1, [x, 1]),
          lambda x: tf.Print(x + 2, [x, 2])]
i = tf.constant(0)
x = tf.constant(0)
r = tf.while_loop(condition, body_2(my_ops), [i, x])  # See the difference with body_1

with tf.Session() as sess:
    i, x = sess.run(r)
    print(x)  # Prints 15 = 5*2 + 5*1
</code></pre>
",868829,1210,https://stackoverflow.com/questions/53033780,Documentation Replicability
53549352,minimize a function in Tensorflow,"<p>How can I get the gradients of a function using tf.gradients? the below is working when I use GradientDescentOptimizer.minimize(), tf.gradients seems to be evaluating 1 at the deriv of x^2+2 which is 2x</p>

<p>What am I missing ?</p>

<pre><code>x = tf.Variable(1.0, trainable=True)
y = x**2 + 2

grad = tf.gradients(y, x)
#grad = tf.train.GradientDescentOptimizer(0.1).minimize(y)

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    grad_value = sess.run(grad)
    print(grad_value)
</code></pre>
",2018-11-30 00:00:58,"<p>If I understand your question correctly, you want to find the value of <code>x</code> which minimizes <code>x^2 + 2</code>. </p>

<p>To do so, you need to repeatedly call <code>GradientDescentOptimizer</code> until <code>x</code> converges to the value which minimizes the function. This is because gradient descent is an iterative technique. </p>

<p>Also, in tensorflow, the method  <code>minimize</code> of <code>GradientDescentOptimizer</code> does both computing the gradients and then applying them to the relevant variables (<code>x</code> in your case). So the code should look like this (Notice I commented the <code>grad</code> variable, which is not required unless you want to look at the gradient values): </p>

<pre><code>x = tf.Variable(1.0, trainable=True)
y = x**2 + 2

# grad = tf.gradients(y, x)
grad_op = tf.train.GradientDescentOptimizer(0.2).minimize(y)

init = tf.global_variables_initializer()

n_iterations = 10
with tf.Session() as sess:
    sess.run(init)
    for i in range(n_iterations):
        _, new_x = sess.run([grad_op, x])
        print('Iteration:', i,', x:', new_x)
</code></pre>

<p>and you get: </p>

<pre><code>Iteration: 0 , x: 1.0
Iteration: 1 , x: 0.6
Iteration: 2 , x: 0.36
Iteration: 3 , x: 0.216
Iteration: 4 , x: 0.07776
Iteration: 5 , x: 0.07776
Iteration: 6 , x: 0.046656
Iteration: 7 , x: 0.01679616
Iteration: 8 , x: 0.010077696
Iteration: 9 , x: 0.010077696
</code></pre>

<p>which you see in converging to the true answer which is 0.</p>

<p>If you increase the learning rate of <code>GradientDescentOptimizer</code>, from 0.2 to 0.4, it will converge to 0 much faster.</p>

<p><strong>EDIT</strong></p>

<p>OK, based on my new understanding of the question, to manually implement gradient descent, you cannot do <code>x = x - alpha * gradient</code> because this is python operation which simply replaces the object <code>x</code>. You need to tell tensorflow to add the op to the graph, and this can be done using <code>x.assign</code>. It will look like: </p>

<pre><code>x = tf.Variable(1.0, trainable=True)
y = x**2 + 2

grad = tf.gradients(y, x)
# grad_op = tf.train.GradientDescentOptimizer(0.5).minimize(y)

update_op = x.assign(x - 0.2*grad[0])

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    for i in range(10):
        new_x = sess.run([update_op, x])
        print('Iteration:', i,', x:', new_x)
</code></pre>

<p>and we get the same answer as the native <code>GradientDescentOptimizer</code>:</p>

<pre><code>Iteration: 0 , x: 1.0
Iteration: 1 , x: 0.6
Iteration: 2 , x: 0.36
Iteration: 3 , x: 0.1296
Iteration: 4 , x: 0.1296
Iteration: 5 , x: 0.077759996
Iteration: 6 , x: 0.046655998
Iteration: 7 , x: 0.027993599
Iteration: 8 , x: 0.01679616
Iteration: 9 , x: 0.010077696
</code></pre>
",9650073,97,https://stackoverflow.com/questions/53549352,Documentation Replicability
53307954,TensorFlow Custom Estimator predict throwing value error,"<p>Note: this question has an accompanying, documented <a href=""https://colab.research.google.com/drive/1GKAqEo7qSr6kMAgxPrOrudA5eLndQ6Ub"" rel=""nofollow noreferrer"">Colab</a> notebook.</p>
<p>TensorFlow's documentation can, at times, leave a lot to be desired. Some of the older docs for lower level apis seem to have been expunged, and most newer documents point towards using higher level apis such as TensorFlow's subset of <code>keras</code> or <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>estimators</code></a>. This would not be so problematic if the higher level apis did not so often rely closely on their lower levels. Case in point, <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>estimators</code></a> (especially the <code>input_fn</code> when using TensorFlow Records).</p>
<p>Over the following Stack Overflow posts:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel"">Tensorflow v1.10: store images as byte strings or per channel?</a></li>
<li><a href=""https://stackoverflow.com/questions/52064866/tensorflow-1-10-tfrecorddataset-recovering-tfrecords"">Tensorflow 1.10 TFRecordDataset - recovering TFRecords</a></li>
<li><a href=""https://stackoverflow.com/questions/52874647/tensorflow-v1-10-why-is-an-input-serving-receiver-function-needed-when-checkpoi"">Tensorflow v1.10+ why is an input serving receiver function needed when checkpoints are made without it?</a></li>
<li><a href=""https://stackoverflow.com/questions/52641737/tensorflow-1-10-custom-estimator-early-stopping-with-train-and-evaluate"">TensorFlow 1.10+ custom estimator early stopping with train_and_evaluate</a></li>
<li><a href=""https://stackoverflow.com/questions/53226898/tensorflow-custom-estimator-stuck-when-calling-evaluate-after-training"">TensorFlow custom estimator stuck when calling evaluate after training</a></li>
</ul>
<p>and with the gracious assistance of the TensorFlow / StackOverflow community, we have moved closer to doing what the TensorFlow <a href=""https://www.tensorflow.org/guide/custom_estimators"" rel=""nofollow noreferrer"">&quot;Creating Custom Estimators&quot; guide</a> has not, demonstrating how to make an estimator one might actually use in practice (rather than toy example) e.g. one which:</p>
<ul>
<li>has a validation set for early stopping if performance worsen,</li>
<li>reads from TF Records because many datasets are larger than the TensorFlow recommend 1Gb for in memory, and</li>
<li>that saves its best version whilst training</li>
</ul>
<p>While I still have many questions regarding this (from the best way to encode data into a TF Record, to what exactly the <code>serving_input_fn</code> expects), there is one question that stands out more prominently than the rest:</p>
<p>How to predict with the custom estimator we just made?</p>
<p>Under the documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#predict"" rel=""nofollow noreferrer"">predict</a>, it states:</p>
<blockquote>
<p><code>input_fn</code>: A function that constructs the features. Prediction continues until <code>input_fn</code> raises an end-of-input exception (<code>tf.errors.OutOfRangeError</code> or <code>StopIteration</code>). See Premade Estimators for more information. The function should construct and return one of the following:</p>
<ul>
<li>A tf.data.Dataset object: Outputs of Dataset object must have same constraints as below.</li>
<li>features: A tf.Tensor or a dictionary of string feature name to Tensor. features are consumed by model_fn. They should satisfy the expectation of model_fn from inputs.</li>
<li>A tuple, in which case the first item is extracted as features.</li>
</ul>
</blockquote>
<p>(perhaps) Most likely, if one is using <code>estimator.predict</code>, they are using data in memory such as a dense tensor (because a held out test set would likely go through <code>evaluate</code>).</p>
<p>So I, in the accompanying <a href=""https://colab.research.google.com/drive/1GKAqEo7qSr6kMAgxPrOrudA5eLndQ6Ub"" rel=""nofollow noreferrer"">Colab</a>, create a single dense example, wrap it up in a <code>tf.data.Dataset</code>, and call <code>predict</code> to get a <code>ValueError</code>.</p>
<p>I would greatly appreciate it if someone could explain to me how I can:</p>
<ol>
<li>load my saved estimator</li>
<li>given a dense, in memory example, predict the output with the estimator</li>
</ol>
",2018-11-14 20:02:54,"<pre><code>to_predict = random_onehot((1, SEQUENCE_LENGTH, SEQUENCE_CHANNELS))\
        .astype(tf_type_string(I_DTYPE))
pred_features = {'input_tensors': to_predict}

pred_ds = tf.data.Dataset.from_tensor_slices(pred_features)
predicted = est.predict(lambda: pred_ds, yield_single_examples=True)

next(predicted)
</code></pre>

<blockquote>
  <p>ValueError: Tensor(""IteratorV2:0"", shape=(), dtype=resource) must be from the same graph as Tensor(""TensorSliceDataset:0"", shape=(), dtype=variant).</p>
</blockquote>

<p>When you use the <code>tf.data.Dataset</code> module, it actually defines an input graph which is independant from the model graph. What happens here is that you first created a small graph by calling <code>tf.data.Dataset.from_tensor_slices()</code>, then the estimator API created a second graph by calling <code>dataset.make_one_shot_iterator()</code> automatically. These 2 graphs can't communicate so it throws an error.</p>

<p>To circumvent this, you should never create a dataset outside of estimator.train/evaluate/predict. This is why everything data related is wrapped inside input functions.</p>

<pre><code>def predict_input_fn(data, batch_size=1):
  dataset = tf.data.Dataset.from_tensor_slices(data)
  return dataset.batch(batch_size).prefetch(None)

predicted = est.predict(lambda: predict_input_fn(pred_features), yield_single_examples=True)
next(predicted)
</code></pre>

<p>Now, the graph is not created outside of the predict call.</p>

<p>I also added <code>dataset.batch()</code> because the rest of your code expect batched data and it was throwing a shape error. Prefetch just speed things up.</p>
",5623899,4978,https://stackoverflow.com/questions/53307954,Requesting (Additional) Documentation/Examples
53523729,How to select a proper API/implementation of an LSTM cell in TensorFlow?,"<p>In Tensorflow there are many different implementations of the same task under different APIs. For LSTM cell one can find many implementations, e.g.,</p>

<ol>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell#class_lstmcell"" rel=""nofollow noreferrer"">tf.nn.rnn_cell.LSTMCell</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMBlockCell"" rel=""nofollow noreferrer"">tf.contrib.rnn.LSTMBlockCell</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell"" rel=""nofollow noreferrer"">tf.keras.layers.LSTMCell</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"" rel=""nofollow noreferrer"">tf.keras.layers.LSTM</a></li>
</ol>

<p>Which of the above implementation shall I select? Is there a general guideline? For example always use tf.nn > tf.keras > tf.layers > tf. contrib.</p>

<p>In a slightly related question <a href=""https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow"">here</a> (for batch normalization), the approved answer says tf.contrib is not a good choice as it is for early implementation. Therein, the KERAS API also uses tf.nn. So it looks like tf.nn > tf.keras > tf.contrib.</p>
",2018-11-28 16:12:44,"<p>Here <a href=""https://www.tensorflow.org/guide/performance/overview#rnn_performance"" rel=""nofollow noreferrer"">RNN Performance</a> are some recommendations for TensorFlow. A couple of important points from the doc:</p>

<ul>
<li>use <code>tf.contrib.cudnn_rnn()</code> on NVIDIA GPUs;</li>
<li>use <code>tf.nn.dynamic_rnn()</code> instead of <code>tf.nn.static_rnn()</code>. Probably they mean, that we need to add <code>sequence_length</code> to avoid extra computations.</li>
</ul>
",3484477,1491,https://stackoverflow.com/questions/53523729,Documentation Replicability
53032922,TensorFlow while loop with condition dependent on body,"<p>I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop()</code></a>.</p>

<p>My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by</p>

<pre><code>import numpy as np
import tensorflow as tf
IMAGE_SHAPE = [960, 720]
CROP_SHAPE = [320, 240]
max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
</code></pre>

<p>and the condition is</p>

<pre><code>cond = tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)
</code></pre>

<p>Going over the documentation and examples of <code>tf.while_loop(cond, body, loop_vars, ...)</code>, what I understand is that both <code>cond</code> and <code>body</code> should take the same arguments given in <code>loop_vars</code>.
I don't see how I can have <code>cond</code> depend on <code>img_crop</code> which would be calculated inside <code>body</code>, and isn't provided in <code>loop_vars</code>.</p>

<p>I could equivalently compute <code>cond</code> using <code>crop_begin_index</code> without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem.</p>

<p>Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use <code>tf.while_loop()</code>?</p>
",2018-10-28 15:05:05,"<p>The arguments that are passed on to the <code>condition</code> function are the arguments returned from your <code>body</code> function. So you just have to return that value that you want to base your condition on in the <code>body</code> function, then carry out the condition on that value in your <code>cond</code> function. Something like, </p>

<pre><code>def body(image_shape, crop_shape, img_crop):
    max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
    crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
    img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
    return (image_shape, crop_shape, img_crop)

def cond(image_shape, crop_shape, img_crop):
    return tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)

image_shape, crop_shape, img_crop = tf.while_loop(cond=cond, body=body, loop_vars=([960, 720], [320, 240], img_crop))
</code></pre>

<p>Don't have access to an interpreter right now, so there might be some syntax problems there, but something like that. </p>

<p>Also, if I recall correctly, the body and the condition need to be pure functions, you cannot alter the outer state from within the functions.</p>

<p>Also note, you'll need to specify some initial value for <code>img_crop</code> in the loop vars.</p>

<p>Moreover, by default, <code>tf.while_loop</code> expects the shapes of all the <code>loop_vars</code> to remain the same across all loop runs. You can modify this through the <code>shape_invariants</code> argument. </p>
",2260153,2642,https://stackoverflow.com/questions/53032922,Documentation Replication on Other Examples
58398790,Does Tensorflow have an inverse of tf.unravel_index?,"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/unravel_index"" rel=""nofollow noreferrer""><code>tf.unravel_index</code></a> takes a flat index into an array and a shape, and returns the tuple that represents that index in the array.</p>

<pre><code>with tf.Session() as sess: 
    t = tf.unravel_index(tf.constant(15), [2, 3, 4]) 
    print(sess.run(t)) # [1 0 3]
</code></pre>

<p>Is there an inverse? <code>tf.ravel_index</code> doesn't exist, but maybe it's under a different name?</p>
",2019-10-15 16:10:22,"<p>Here is how you can do the equivalent to <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel_multi_index.html"" rel=""nofollow noreferrer""><code>np.ravel_multi_index</code></a> in TensorFlow:</p>

<pre><code>import tensorflow as tf

# multi_index is 2D (num dimensions x num indices), dims is 1D
# Does not check for out of bounds indices
def tf_ravel_multi_index(multi_index, dims):
    strides = tf.cumprod(dims, exclusive=True, reverse=True)
    return tf.reduce_sum(multi_index * tf.expand_dims(strides, 1), axis=0)

# Test
with tf.Graph().as_default(), tf.Session() as sess:
    # Shape tensor
    shape = tf.constant([4, 5, 6])
    # Some flat indices
    idx = tf.constant([23, 56, 4, 17])
    # Unravel indices
    unravel = tf.unravel_index(idx, shape)
    # Ravel indices again
    idx2 = tf_ravel_multi_index(unravel, shape)
    # Check result
    print(*sess.run((unravel, idx2)), sep='\n')
    # [[0 1 0 0]
    #  [3 4 0 2]
    #  [5 2 4 5]]
    # [23 56  4 17]
</code></pre>
",635523,3912,https://stackoverflow.com/questions/58398790,Documentation Replicability
53568337,Print accuracy when training tf.estimator.DNNClassifier,"<p>I am new to tensorflow, using <a href=""https://www.tensorflow.org/guide/custom_estimators"" rel=""nofollow noreferrer"">official tutorial</a> tf.estimator.DNNClassifier and custom estimator to build simple NN to solve classification problem.</p>

<p>While training :</p>

<pre><code>dnn_model = tf.estimator.DNNClassifier(hidden_units=[10,10,10],
                                       feature_columns = my_features_column,
                                       n_classes=5,
                                       optimizer = tf.train.AdamOptimizer()
                                      )

dnn_model.train(input_fn=train_input_func)
</code></pre>

<p>It will report loss at specific time as following:</p>

<pre><code>INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/tmphwkvj5le/model.ckpt-150
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 150 into /tmp/tmphwkvj5le/model.ckpt.
INFO:tensorflow:loss = 133.04277, step = 150
INFO:tensorflow:global_step/sec: 115.114
INFO:tensorflow:loss = 128.15938, step = 250 (0.872 sec)
INFO:tensorflow:global_step/sec: 134.317
INFO:tensorflow:loss = 123.093094, step = 350 (0.743 sec)
INFO:tensorflow:global_step/sec: 133.573
INFO:tensorflow:loss = 117.80729, step = 450 (0.748 sec)
INFO:tensorflow:global_step/sec: 135.081
INFO:tensorflow:loss = 114.07168, step = 550 (0.741 sec)
INFO:tensorflow:Saving checkpoints for 650 into /tmp/tmphwkvj5le/model.ckpt.
INFO:tensorflow:Loss for final step: 118.19583.
</code></pre>

<p>I want to print classification accuracy every batch or epoch, likes the log Info in keras:</p>

<pre><code>Epoch 1/20
5000/5000 [==============================] - 1s 157us/step - loss: 1.4885 - acc: 0.3276 - val_loss: 1.4397 - val_acc: 0.3620
Epoch 2/20
5000/5000 [==============================] - 0s 66us/step - loss: 1.3792 - acc: 0.3922 - val_loss: 1.4001 - val_acc: 0.3768
.
.
</code></pre>

<p>How can I find the tutorial on this problem ? All I find were talking about more lower API (tensor, session, etc.).</p>
",2018-12-01 06:26:11,"<p>You want to use the 'hooks' option in the estimator train.</p>

<p>The particular hook you want to use is this one: <a href=""https://www.tensorflow.org/api_docs/python/tf/train/LoggingTensorHook"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/train/LoggingTensorHook</a></p>

<p>A similar problem was solved here:
<a href=""https://stackoverflow.com/a/45716062/10498246"">https://stackoverflow.com/a/45716062/10498246</a>
where they used the Logging Tensor Hook on a different training function.</p>
",10083164,55,https://stackoverflow.com/questions/53568337,Documentation Replicability
53569622,Difference between tf.train.Checkpoint and tf.train.Saver,"<p>I found there are different ways to save/restore models and variables in <code>Tensorflow</code>. These ways including:</p>

<ul>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/simple_save"" rel=""nofollow noreferrer"">tf.saved_model.simple_save</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint"" rel=""nofollow noreferrer"">tf.train.Checkpoint</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/train/Saver"" rel=""nofollow noreferrer"">tf.train.Saver</a></li>
</ul>

<p>In tensorflow's documentations, I found some differences between them:</p>

<ol>
<li><code>tf.saved_model</code> is a thin wrapper around <code>tf.train.Saver</code></li>
<li><code>tf.train.Checkpoint</code> support eager execution but <code>tf.train.Saver</code> <strong>not</strong>.</li>
<li><code>tf.train.Checkpoint</code> not creating <code>.meta</code> file but still can load graph structure (here is a big question! how it can do that?)</li>
</ol>

<p>How <code>tf.train.Checkpoint</code> can load graph without <code>.meta</code> file? or more generally What is the difference between <code>tf.train.Saver</code> and <code>tf.train.Checkpoint</code>?</p>
",2018-12-01 09:56:07,"<p>According to Tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint"" rel=""nofollow noreferrer"">docs</a>:</p>

<blockquote>
  <p><code>Checkpoint.save</code> and <code>Checkpoint.restore</code> write and read object-based
  checkpoints, in contrast to <code>tf.train.Saver</code> which writes and reads
  variable.name based checkpoints. Object-based checkpointing saves a
  graph of dependencies between Python objects (Layers, Optimizers,
  Variables, etc.) with named edges, and this graph is used to match
  variables when restoring a checkpoint. It can be more robust to
  changes in the Python program, and helps to support restore-on-create
  for variables when executing eagerly. <strong>Prefer <code>tf.train.Checkpoint</code> over
  <code>tf.train.Saver</code> for new code</strong>.</p>
</blockquote>
",1462770,16282,https://stackoverflow.com/questions/53569622,Documentation Ambiguity
53578484,tf.gather with indices of higher dimention than input data?,"<p>Reading <a href=""https://github.com/WangYueFt/dgcnn"" rel=""nofollow noreferrer"">Dynamic Graph CNN for Learning on Point Clouds</a> code, I came across this snippet:</p>

<pre><code>  idx_ = tf.range(batch_size) * num_points
  idx_ = tf.reshape(idx_, [batch_size, 1, 1]) 

  point_cloud_flat = tf.reshape(point_cloud, [-1, num_dims])
  point_cloud_neighbors = tf.gather(point_cloud_flat, nn_idx+idx_)  &lt;--- what happens here?
  point_cloud_central = tf.expand_dims(point_cloud_central, axis=-2)
</code></pre>

<p>debugging the line I made sure that the dims are</p>

<pre><code>point_cloud_flat:(32768,3) nn_idx:(32,1024,20), idx_:(32,1,1) 
// indices are (32,1024,20) after broadcasting
</code></pre>

<p>Reading the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer"">tf.gather doc</a> I couldn't understand what the function does with dimensions higher that the input dimensions</p>
",2018-12-02 08:04:32,"<p>An equivalent function in numpy is <code>np.take</code>, a simple example:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np

params = np.array([4, 3, 5, 7, 6, 8])

# Scalar indices; (output is rank(params) - 1), i.e. 0 here.
indices = 0
print(params[indices])

# Vector indices; (output is rank(params)), i.e. 1 here.
indices = [0, 1, 4]
print(params[indices])  # [4 3 6]

# Vector indices; (output is rank(params)), i.e. 1 here.
indices = [2, 3, 4]
print(params[indices])  # [5 7 6]

# Higher rank indices; (output is rank(params) + rank(indices) - 1), i.e. 2 here
indices = np.array([[0, 1, 4], [2, 3, 4]])
print(params[indices])  # equivalent to np.take(params, indices, axis=0)
# [[4 3 6]
# [5 7 6]]
</code></pre>

<p>In your case, the rank of <code>indices</code> is higher than <code>params</code>, so output is rank(<code>params</code>) + rank(<code>indices</code>) - 1 (i.e. 2 + 3 - 1 = 4, i.e. (32, 1024, 20, 3)). The <code>- 1</code> is because the <code>tf.gather(axis=0)</code> and <code>axis</code> must be rank 0 (so a scalar) at this moment. So the <code>indices</code> takes the elements of the first dimension (<code>axis=0</code>) in a ""fancy"" indexing way.</p>

<p><strong>EDITED</strong>:</p>

<p>In brief, in your case, (if I didn't misunderstand the code)</p>

<ul>
<li><code>point_cloud</code> is (32, 1024, 3), 32 batches 1024 points which have 3
coordinates. </li>
<li><code>nn_idx</code> is (32, 1024, 20), indices of 20 neighbors of
32 batches 1024 points. The indices are for indexing in <code>point_cloud</code>. </li>
<li><code>nn_idx+idx_</code> (32, 1024, 20), indices of 20 neighbors of
32 batches 1024 points. The indices are for indexing in <code>point_cloud_flat</code>.</li>
<li><code>point_cloud_neighbors</code> finally is (32, 1024,
20, 3), the same as <code>nn_idx+idx_</code> except that <code>point_cloud_neighbors</code> are their 3 coordinates while <code>nn_idx+idx_</code> are just their indices.</li>
</ul>
",7034613,2349,https://stackoverflow.com/questions/53578484,Documentation Replicability
53760992,Difference between tf.layers.dense and tf.nn.xw_plus_b,"<p>What is the difference between <code>tf.layers.dense</code> and <code>tf.nn.xw_plus_b</code> in <code>TF</code>? 
What is the default activation used in <code>tf.layers.dense</code> when ""activation"" argument is passed as <code>None</code>?                                </p>
",2018-12-13 11:36:37,"<p><code>tf.nn.xw_plus_b</code> is a low-level operation that only computes <code>x*W+b</code> and requires existing variables.</p>

<p><code>tf.layers.dense</code> is a high-level ""layer"" that creates variables, apply activation can set constrains and apply regularization.</p>

<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/dense"" rel=""noreferrer"">documentation</a> default activation is linear (no activation).</p>

<blockquote>
  <p><strong>activation</strong>: Activation function (callable). Set it to None to maintain
  a linear activation.</p>
</blockquote>

<p><strong>Update</strong></p>

<p>In Tensorflow 1.12 <code>Dense</code> layer inherits  <code>keras.layers.Dense</code> (<a href=""https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/layers/core.py#L32"" rel=""noreferrer"">code</a>):</p>

<pre class=""lang-py prettyprint-override""><code>@tf_export('layers.Dense')
class Dense(keras_layers.Dense, base.Layer):
</code></pre>

<p>Keras implementation of this layer does the following (<a href=""https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/keras/layers/core.py#L958"" rel=""noreferrer"">code</a>):</p>

<pre class=""lang-py prettyprint-override""><code>  def call(self, inputs):
    inputs = ops.convert_to_tensor(inputs, dtype=self.dtype)
    rank = common_shapes.rank(inputs)
    if rank &gt; 2:
      # Broadcasting is required for the inputs.
      outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]])
      # Reshape the output back to the original ndim of the input.
      if not context.executing_eagerly():
        shape = inputs.get_shape().as_list()
        output_shape = shape[:-1] + [self.units]
        outputs.set_shape(output_shape)
    else:
      outputs = gen_math_ops.mat_mul(inputs, self.kernel)
    if self.use_bias:
      outputs = nn.bias_add(outputs, self.bias)
    if self.activation is not None:
      return self.activation(outputs)  # pylint: disable=not-callable
    return outputs
</code></pre>

<p>So it is not implemented using <code>tf.nn.xw_plus_b</code> but uses two separate operations.</p>

<p>To answer your question: <code>Dense</code> layer without activation, constraints and regularization should do the same as <code>tf.nn.xw_plus_b</code>.</p>
",8238383,183,https://stackoverflow.com/questions/53760992,Documentation Replicability
53772787,tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) in tensorflow,"<p>What is purpose of <code>tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS))</code> in tensorflow?</p>

<p>With more context:</p>

<pre><code>    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)
    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
        train_op = optimizer.minimize(loss_fn, var_list=tf.trainable_variables())
</code></pre>
",2018-12-14 02:35:40,"<p>The method <code>tf.control_dependencies</code> allow to ensure that the operations used as inputs of the context manager are run before the operations defined inside the context manager.</p>

<p>For example: </p>

<pre><code>count = tf.get_variable(""count"", shape=(), initializer=tf.constant_initializer(1), trainable=False)
count_increment = tf.assign_add(count, 1)
c = tf.constant(2.)
with tf.control_dependencies([count_increment]):
    d = c + 3
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(""eval count"", count.eval())
    print(""eval d"", d.eval())
    print(""eval count"", count.eval())
</code></pre>

<p>This prints:</p>

<pre><code>eval count 1
eval d 5.0 # Running d make count_increment operation being run
eval count 2 # count_increment operation has be run and now count hold 2.
</code></pre>

<p>So in your case, each time you run the <code>train_op</code> operation it will first run all the operations defined in the <code>tf.GraphKeys.UPDATE_OPS</code> collection.</p>
",1179925,20633,https://stackoverflow.com/questions/53772787,Requesting (Additional) Documentation/Examples
53919290,tensorflow sparse categorical cross entropy with logits,"<p>I am a novice programmer trying to follow <a href=""https://www.tensorflow.org/tutorials/sequences/text_generation"" rel=""nofollow noreferrer"">this</a> guide.
However, I ran across an issue. The guide says to define the loss function as:</p>

<pre><code>def loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)
</code></pre>

<p>This gives me the following error:</p>

<blockquote>
  <p>sparse_categorical_crossentropy() got an unexpected keyword argument
  'from_logits'</p>
</blockquote>

<p>which I take to mean that <code>from_logits</code> is an argument not specified in the function, which is supported by the documentation, which that <code>tf.keras.losses.sparse_categorical_crossentropy()</code> has only two possible inputs. </p>

<p>Is there a way to specify that logits are being used or is that even necesarry?</p>
",2018-12-25 04:10:02,"<p>The <code>from_logits</code> parameter is introduced in Tensorflow 1.13.</p>

<p>You can compare 1.12 and 1.13 with these urls:</p>

<pre><code>https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/keras/losses.py
https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/losses.py
</code></pre>

<p>1.13 is not released at the time of writing. This is why the tutorial starts with the line</p>

<pre><code>!pip install -q tf-nightly
</code></pre>
",9185745,175,https://stackoverflow.com/questions/53919290,Documentation Replication on Other Examples
53541803,Rows or elements selection on sparse tensor,"<p>In tensorflow, how can we do tf.gather or tf.gather_nd in sparse tensor? How can we extract select specific rows or specific elements from sparse tensor without converting it into dense tensor?</p>
",2018-11-29 14:57:47,"<p>This is one possible solution, although it is still expensive in time and memory so it is probably not feasible for a big use case:</p>

<pre><code>import tensorflow as tf

def sparse_select_indices(sp_input, indices, axis=0):
    # Only necessary if indices may have non-unique elements
    indices, _ = tf.unique(indices)
    n_indices = tf.size(indices)
    # Only necessary if indices may not be sorted
    indices, _ = tf.math.top_k(indices, n_indices)
    indices = tf.reverse(indices, [0])
    # Get indices for the axis
    idx = sp_input.indices[:, axis]
    # Find where indices match the selection
    eq = tf.equal(tf.expand_dims(idx, 1), tf.cast(indices, tf.int64))
    # Mask for selected values
    sel = tf.reduce_any(eq, axis=1)
    # Selected values
    values_new = tf.boolean_mask(sp_input.values, sel, axis=0)
    # New index value for selected elements
    n_indices = tf.cast(n_indices, tf.int64)
    idx_new = tf.reduce_sum(tf.cast(eq, tf.int64) * tf.range(n_indices), axis=1)
    idx_new = tf.boolean_mask(idx_new, sel, axis=0)
    # New full indices tensor
    indices_new = tf.boolean_mask(sp_input.indices, sel, axis=0)
    indices_new = tf.concat([indices_new[:, :axis],
                             tf.expand_dims(idx_new, 1),
                             indices_new[:, axis + 1:]], axis=1)
    # New shape
    shape_new = tf.concat([sp_input.dense_shape[:axis],
                           [n_indices],
                           sp_input.dense_shape[axis + 1:]], axis=0)
    return tf.SparseTensor(indices_new, values_new, shape_new)
</code></pre>

<p>Here is an example of use:</p>

<pre><code>import tensorflow as tf

with tf.Session() as sess:
    # Input
    sp1 = tf.SparseTensor([[0, 1], [2, 3], [4, 5]], [10, 20, 30], [6, 7])
    print(sess.run(tf.sparse.to_dense(sp1)))
    # [[ 0 10  0  0  0  0  0]
    #  [ 0  0  0  0  0  0  0]
    #  [ 0  0  0 20  0  0  0]
    #  [ 0  0  0  0  0  0  0]
    #  [ 0  0  0  0  0 30  0]
    #  [ 0  0  0  0  0  0  0]]

    # Select rows 0, 1, 2
    sp2 = sparse_select_indices(sp1, [0, 1, 2])
    print(sess.run(tf.sparse.to_dense(sp2)))
    # [[ 0 10  0  0  0  0  0]
    #  [ 0  0  0  0  0  0  0]
    #  [ 0  0  0 20  0  0  0]]

    # Select columns 4, 5
    sp3 = sparse_select_indices(sp1, [4, 5], axis=1)
    print(sess.run(tf.sparse.to_dense(sp3)))
    # [[ 0  0]
    #  [ 0  0]
    #  [ 0  0]
    #  [ 0  0]
    #  [ 0 30]
    #  [ 0  0]]
</code></pre>
",10596177,95,https://stackoverflow.com/questions/53541803,Documentation Replication on Other Examples
68939916,"TensorFlow text generation RNN example failing on TF 2.6, tf.sparse.to_dense(), Invalid argument: indices[1] = [0] is repeated","<p>I am trying to run through the TensorFlow text generation RNN example,</p>
<p><a href=""https://github.com/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb</a></p>
<p>Running on a local Windows computer with TensorFlow 2.6 installed.</p>
<p>I was able to run through and train the RNN model successfully. I was getting a &quot;Tensor' object has no attribute 'numpy&quot; error but added,</p>
<pre><code>tf.compat.v1.enable_eager_execution()
</code></pre>
<p>and this resolved it.</p>
<p>But now trying to test the model with some text I am getting the error,</p>
<pre><code>Invalid argument: indices[1] = [0] is repeated
</code></pre>
<p>This occurs in  <code>tf.sparse.to_dense</code> inside the OneStep function.</p>
<pre><code>class OneStep(tf.keras.Model):
  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):
    super().__init__()
    self.temperature = temperature
    self.model = model
    self.chars_from_ids = chars_from_ids
    self.ids_from_chars = ids_from_chars

    print(len(ids_from_chars.get_vocabulary()))
    # Create a mask to prevent &quot;[UNK]&quot; from being generated.
    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]
    sparse_mask = tf.SparseTensor(
        # Put a -inf at each bad index.
        values=[-float('inf')]*len(skip_ids),
        indices=skip_ids,
        # Match the shape to the vocabulary
        dense_shape=[len(ids_from_chars.get_vocabulary())])
    print(sparse_mask)
    self.prediction_mask = tf.sparse.to_dense(sparse_mask)
</code></pre>
<p>I added some debug to print the ids_from_chars</p>
<pre><code>76
SparseTensor(indices=tf.Tensor(
[[0]
[0]], shape=(2, 1), dtype=int64), values=tf.Tensor([-inf -inf], shape=(2,), dtype=float32), dense_shape=tf.Tensor([76], shape=(1,), dtype=int64))
2021-08-25 15:28:23.935194: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at sparse_to_dense_op.cc:162 : Invalid argument: indices[1] = [0] is repeated
Traceback (most recent call last):
File &quot;app.py&quot;, line 1041, in test_nlp_text_generation
result = text_generation.predictionfunction(text, analytic_id)
File &quot;D:\Projects\python-run-2\text_generation.py&quot;, line 238, in predictionfunction
one_step_model = OneStep(model, chars_from_ids, ids_from_chars)
File &quot;D:\Projects\python-run-2\text_generation.py&quot;, line 166, in __init__
self.prediction_mask = tf.sparse.to_dense(sparse_mask)
File &quot;D:\Users\james\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\sparse_ops.py&quot;, line 1721, in sparse_tensor_to_dense
name=name)
File &quot;D:\Users\james\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gen_sparse_ops.py&quot;, line 3161, in sparse_to_dense
_ops.raise_from_not_ok_status(e, name)
File &quot;D:\Users\james\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 6941, in raise_from_not_ok_status
six.raise_from(core._status_to_exception(e.code, message), None)
File &quot;&lt;string&gt;&quot;, line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[1] = [0] is repeated [Op:SparseToDense]
</code></pre>
<p>Also, I had this example running fine on my computer previously. Just had reinstalled TensorFlow and was trying the demo from scratch again.</p>
<p>Any idea what is causing this error, or how to fix it?</p>
",2021-08-26 13:56:12,"<p>I reproduced this error through the code below</p>
<pre><code>import tensorflow as tf

#there are same values in the tensor
skip_ids = tf.constant([[0], [0]], dtype=tf.int64)

sparse_mask = tf.SparseTensor(
    # Put a -inf at each bad index.
    values=[-float('inf')] * len(skip_ids),
    indices=skip_ids,
    # Match the shape to the vocabulary
    dense_shape=[76])
print(sparse_mask)

prediction_mask = tf.sparse.to_dense(sparse_mask)
</code></pre>
<p>Your indices has same value, it does not allow to assign values in the same position. Just get unique values in indices tensor before:</p>
<pre><code>import tensorflow as tf

skip_ids = tf.constant([[0], [0]], dtype=tf.int64)

# get unique indices
tmp1 = tf.reshape(skip_ids, shape=(-1,))
uniques, idx, counts = tf.unique_with_counts(tmp1)
uniques_ids = tf.expand_dims(uniques, axis=1)


sparse_mask = tf.SparseTensor(
    # Put a -inf at each bad index.
    values=[-float('inf')] * len(uniques_ids),
    indices=uniques_ids,
    # Match the shape to the vocabulary
    dense_shape=[76])
print(sparse_mask)

prediction_mask = tf.sparse.to_dense(sparse_mask)

print(prediction_mask)
</code></pre>
<p>My tensorflow version is 2.1.0</p>
",416206,18025,https://stackoverflow.com/questions/68939916,Documentation Replication on Other Examples
61720708,How do you save a Tensorflow dataset to a file?,"<p>There are at least two more questions like this on SO but not a single one has been answered.</p>

<p>I have a dataset of the form:</p>

<pre><code>&lt;TensorSliceDataset shapes: ((512,), (512,), (512,), ()), types: (tf.int32, tf.int32, tf.int32, tf.int32)&gt;
</code></pre>

<p>and another of the form:</p>

<pre><code>&lt;BatchDataset shapes: ((None, 512), (None, 512), (None, 512), (None,)), types: (tf.int32, tf.int32, tf.int32, tf.int32)&gt;
</code></pre>

<p>I have looked and looked but I can't find the code to save these datasets to files that can be loaded later. The closest I got was <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter"" rel=""noreferrer"">this page in the TensorFlow docs</a>, which suggests serializing the tensors using <code>tf.io.serialize_tensor</code> and then writing them to a file using <code>tf.data.experimental.TFRecordWriter</code>.</p>

<p>However, when I tried this using the code:</p>

<pre><code>dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter('mydata.tfrecord')
writer.write(dataset)
</code></pre>

<p>I get an error on the first line:</p>

<blockquote>
  <p>TypeError: serialize_tensor() takes from 1 to 2 positional arguments but 4 were given</p>
</blockquote>

<p>How can I modify the above (or do something else) to accomplish my goal?</p>
",2020-05-11 01:00:29,"<p><code>TFRecordWriter</code> seems to be the most convenient option, but unfortunately it can only write datasets with a single tensor per element. Here are a couple of workarounds you can use. First, since all your tensors have the same type and similar shape, you can concatenate them all into one, and split them back later on load:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

# Write
a = tf.zeros((100, 512), tf.int32)
ds = tf.data.Dataset.from_tensor_slices((a, a, a, a[:, 0]))
print(ds)
# &lt;TensorSliceDataset shapes: ((512,), (512,), (512,), ()), types: (tf.int32, tf.int32, tf.int32, tf.int32)&gt;
def write_map_fn(x1, x2, x3, x4):
    return tf.io.serialize_tensor(tf.concat([x1, x2, x3, tf.expand_dims(x4, -1)], -1))
ds = ds.map(write_map_fn)
writer = tf.data.experimental.TFRecordWriter('mydata.tfrecord')
writer.write(ds)

# Read
def read_map_fn(x):
    xp = tf.io.parse_tensor(x, tf.int32)
    # Optionally set shape
    xp.set_shape([1537])  # Do `xp.set_shape([None, 1537])` if using batches
    # Use `x[:, :512], ...` if using batches
    return xp[:512], xp[512:1024], xp[1024:1536], xp[-1]
ds = tf.data.TFRecordDataset('mydata.tfrecord').map(read_map_fn)
print(ds)
# &lt;MapDataset shapes: ((512,), (512,), (512,), ()), types: (tf.int32, tf.int32, tf.int32, tf.int32)&gt;
</code></pre>

<p>But, more generally, you can simply have a separate file per tensor and then read them all:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

# Write
a = tf.zeros((100, 512), tf.int32)
ds = tf.data.Dataset.from_tensor_slices((a, a, a, a[:, 0]))
for i, _ in enumerate(ds.element_spec):
    ds_i = ds.map(lambda *args: args[i]).map(tf.io.serialize_tensor)
    writer = tf.data.experimental.TFRecordWriter(f'mydata.{i}.tfrecord')
    writer.write(ds_i)

# Read
NUM_PARTS = 4
parts = []
def read_map_fn(x):
    return tf.io.parse_tensor(x, tf.int32)
for i in range(NUM_PARTS):
    parts.append(tf.data.TFRecordDataset(f'mydata.{i}.tfrecord').map(read_map_fn))
ds = tf.data.Dataset.zip(tuple(parts))
print(ds)
# &lt;ZipDataset shapes: (&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;), types: (tf.int32, tf.int32, tf.int32, tf.int32)&gt;
</code></pre>

<p>It is possible to have the whole dataset in a single file with multiple separate tensors per element, namely as a file of TFRecords containing <code>tf.train.Example</code>s, but I don't know if there is a way to create those within TensorFlow, that is, without having to get the data out of the dataset into Python and then write it to the records file.</p>
",424306,1174,https://stackoverflow.com/questions/61720708,Documentation Replication on Other Examples
74359221,Tensorflow v2.10 mutate output of signature function to be a map of label to results,"<p>I'm trying to save my model so that when called from tf-serving the output is:</p>
<pre><code>{
   &quot;results&quot;: [
      { &quot;label1&quot;: x.xxxxx, &quot;label2&quot;: x.xxxxx },
      { &quot;label1&quot;: x.xxxxx, &quot;label2&quot;: x.xxxxx }
   ]
}
</code></pre>
<p>where <code>label1</code> and <code>label2</code> are my labels and <code>x.xxxxx</code> are the probability of that label.</p>
<p>This is what I'm trying:</p>
<pre class=""lang-py prettyprint-override""><code>class TFModel(tf.Module):

    def __init__(self, model: tf.keras.Model) -&gt; None:
        self.labels = ['label1', 'label2']
        self.model = model
            
    @tf.function(input_signature=[tf.TensorSpec(shape=(1, ), dtype=tf.string)])
    def prediction(self, pagetext: str):

        return
        { 'results': tf.constant([{k: v for dct in [{self.labels[c]: f&quot;{x:.5f}&quot;} for (c,x) in enumerate(results[i])] for k, v in dct.items()}
         for i in range(len(results.numpy()))])}


# and then save it:
tf_model_wrapper = TFModel(classifier_model)
tf.saved_model.save(tf_model_wrapper.model, 
                      saved_model_path,
                      signatures={'serving_default':tf_model_wrapper.prediction}
                   )
</code></pre>
<p><em><strong>Side Note:</strong> Apparently in TensorFlow v2.0 if <code>signatures</code> is omitted it should scan the object for the first <code>@tf.function</code> (according to this: <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/save"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/saved_model/save</a>) but in reality that doesn't seem to work. Instead, the model saves successfully with no errors and the <code>@tf.function</code> is not called, but default output is returned instead.</em></p>
<p>The error I get from the above is:</p>
<pre><code>ValueError: Got a non-Tensor value &lt;tf.Operation 'PartitionedCall' type=PartitionedCall&gt; for key 'output_0' in the output of the function __inference_prediction_125493 used to generate the SavedModel signature 'serving_default'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor.
</code></pre>
<p>I wrapped the result in <code>tf.constant</code> above because of this error, thinking it might be a quick fix, but I think it's me just being naive and not understanding Tensors properly.</p>
<p>I tried a bunch of other things before learning that [all outputs must be return values].<a href=""https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values"" rel=""nofollow noreferrer"">1</a></p>
<p>How can I change the output to be as I want it to be?</p>
",2022-11-08 10:30:09,"<p>You can see a Tensor as a multidimensional vector, i.e a structure with a fixed size and dimension and containing elements sharing the same type. Your return value is a map between a string and a list of dictionaries. A list of dictionaries cannot be converted to a tensor, because there is no guarantee that the number of dimensions and their size is constant, nor a guarantee that each element is sharing the same type.</p>
<p>You could instead return the raw output of your network, which should be a tensor and do your post processing outside of tensorflow-serving.</p>
<hr />
<p>If you really want to do something like in your question, you can use a Tensor of strings instead, and you could use some code like that:</p>
<pre><code>labels = tf.constant(['label1', 'label2'])
# if your batch size is dynamic, you can use tf.shape on your results variable to find it at runtime
batch_size = 32
# assuming your model returns something with the shape (N,2)
results = tf.random.uniform((batch_size,2))
res_as_str = tf.strings.as_string(results, precision=5)
return {
    &quot;results&quot;: tf.stack(
        [tf.tile(labels[None, :], [batch_size, 1]), res_as_str], axis=-1
    )
} 
</code></pre>
<p>The output will be a dictionary mapping the value &quot;results&quot; to a Tensor of dimensions <code>(Batch, number of labels, 2)</code>, the last dimension containing the label name and its corresponding value.</p>
",498391,3492,https://stackoverflow.com/questions/74359221,Documentation Replication on Other Examples
54055707,Keras model.fit() with tf.dataset fails while using tf.train works fine,"<p>Summary: according to the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit"" rel=""nofollow noreferrer"">documentation</a>, Keras <code>model.fit()</code> should accept tf.dataset as input (I am using TF version 1.12.0). I can train my model if I manually do the training steps but using <code>model.fit()</code> on the same model, I get an error I cannot resolve.</p>

<p>Here is a sketch of what I did: my dataset, which is too big to fit in the memory, consists of many files each with different number of rows of (100 features, label). I'd like to use <code>tf.data</code> to build my data pipeline:</p>

<pre class=""lang-py prettyprint-override""><code>def data_loader(filename):
    '''load a single data file with many rows'''
    features, labels = load_hdf5(filename)
    ...
    return features, labels

def make_dataset(filenames, batch_size):
    '''read files one by one, pick individual rows, batch them and repeat'''
    dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.map(      # Problem here! See edit for solution
        lambda filename: tuple(tf.py_func(data_loader, [filename], [float32, tf.float32])))
    dataset = dataset.flat_map(
        lambda features, labels: tf.data.Dataset.from_tensor_slices((features, labels)))
    dataset = dataset.batch(batch_size)
    dataset = dataset.repeat()
    dataset = dataset.prefetch(1000)
    return dataset

_BATCH_SIZE = 128
training_set = make_dataset(training_files, batch_size=_BATCH_SIZE)
</code></pre>

<p>I'd like to try a very basic logistic regression model:</p>

<pre class=""lang-py prettyprint-override""><code>inputs = tf.keras.layers.Input(shape=(100,))
outputs = tf.keras.layers.Dense(1, activation='softmax')(inputs)
model = tf.keras.Model(inputs, outputs)
</code></pre>

<p>If I train it <em>manually</em> everything works fine, e.g.:</p>

<pre class=""lang-py prettyprint-override""><code>labels = tf.placeholder(tf.float32)
loss = tf.reduce_mean(tf.keras.backend.categorical_crossentropy(labels, outputs))
train_step = tf.train.GradientDescentOptimizer(.05).minimize(loss)

iterator = training_set.make_one_shot_iterator()
next_element = iterator.get_next()
init_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init_op)
    for i in range(training_size // _BATCH_SIZE):
        x, y = sess.run(next_element)
        train_step.run(feed_dict={inputs: x, labels: y})
</code></pre>

<p>However, if I instead try to use <code>model.fit</code> like this:</p>

<pre class=""lang-py prettyprint-override""><code>model.compile('adam', 'categorical_crossentropy', metrics=['acc'])
model.fit(training_set.make_one_shot_iterator(),
          steps_per_epoch=training_size // _BATCH_SIZE,
          epochs=1,
          verbose=1)
</code></pre>

<p>I get an error message <code>ValueError: Cannot take the length of Shape with unknown rank.</code> inside the keras'es <code>_standardize_user_data</code> function.</p>

<p>I have tried quite a few things but could not resolve the issue. Any ideas?</p>

<p><strong>Edit:</strong> based on @kvish's answer, the solution was to change the map from a lambda to a function that would specify the correct tensor dimensions, e.g.:</p>

<pre class=""lang-py prettyprint-override""><code>def data_loader(filename):
    def loader_impl(filename):
        features, labels, _ = load_hdf5(filename)
        ...
        return features, labels

    features, labels = tf.py_func(loader_impl, [filename], [tf.float32, tf.float32])
    features.set_shape((None, 100))
    labels.set_shape((None, 1))
    return features, labels
</code></pre>

<p>and now, all needed to do is to call this function from <code>map</code>:</p>

<pre class=""lang-py prettyprint-override""><code>dataset = dataset.map(data_loader)
</code></pre>
",2019-01-05 19:55:30,"<p>Probably tf.py_func produces an unknown shape which Keras cannot infer. We can set the shape of the tensor returned by it using set_shape(your_shape) method and that would help Keras infer the shape of the result.</p>
",628654,1085,https://stackoverflow.com/questions/54055707,Documentation Ambiguity
58550146,How to use the tf.keras.layers.BatchNormalization() in custom training loop?,"<p>I went back to tensorflow after quite a while and it seems the landscape is completely changed.</p>

<p>However, previously I used to use <code>tf.contrib....batch_normalization</code> with the following in the training loop:</p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(cnn.loss, global_step=global_step)
</code></pre>

<p>But it seems, <code>contrib</code> is nowhere to be found and <code>tf.keras.layers.BatchNormalization</code> does not work the same way. Also, I couldn't find any training instruction in their <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"" rel=""nofollow noreferrer"">documentation</a>.</p>

<p>So, any information of help is appreciated.</p>
",2019-10-24 22:55:53,"<p>I started using pyTorch. It solved the problem.</p>
",817824,447,https://stackoverflow.com/questions/58550146,Lack of Alternative Solutions/Documentation
62877768,Input shape of tf.data.Dataset not accepted by model.fit(),"<p>I would like to feed with data my model by applying a <code>tf.data.Dataset</code>.</p>
<p>Having checked the documentation of TF 2.0 I found that the <code>.fit()</code> function (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit</a>) accepts:</p>
<blockquote>
<p>x - A tf.data dataset. Should return a tuple of either (inputs, targets)
or (inputs, targets, sample_weights).</p>
</blockquote>
<p>So, I wrote the following minial proof of concept code:</p>
<pre><code>from sklearn.datasets import make_blobs
import tensorflow as tf
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.metrics import Accuracy, AUC

X, Y = make_blobs(n_samples=500, n_features=2, cluster_std=3.0, random_state=1)

def define_model():
    model = Sequential()
    model.add(Dense(units=1, activation=&quot;sigmoid&quot;, input_shape=(2,)))
    model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[AUC(), Accuracy()])
    return model

model = define_model()

X_ds = tf.data.Dataset.from_tensor_slices(X)
Y_ds = tf.data.Dataset.from_tensor_slices(Y)
dataset = tf.data.Dataset.zip((X_ds, Y_ds))

for elem in dataset.take(1):
    print(type(elem))
    print(elem)

model.fit(x=dataset) #&lt;-- does not work
#model.fit(x=X, y=Y) &lt;-- does work without any problems....
</code></pre>
<p>As mentioned in the second comment, the code that does not apply a <code>tf.data.Dataset</code> works fine.</p>
<p>However, when applying the Dataset object I get the following error message:</p>
<pre><code>&lt;class 'tuple'&gt;
(&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([-10.42729974,  -0.85439721])&gt;, &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;)
... other output here...
ValueError: Error when checking input: expected dense_19_input to have
shape (2,) but got array with shape (1,)
</code></pre>
<p>From my understanding of the documentation, the dataset I have constructed should be exactly the tuple object the fit method expects.</p>
<p>I do not understand this error message.</p>
<p>What am I doing wrong here?</p>
",2020-07-13 14:14:11,"<p>When you pass a dataset to <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""noreferrer""><code>fit</code></a>, it is expected that it will directly generate batches, not individual examples. You just need to batch your dataset before training.</p>
<pre class=""lang-py prettyprint-override""><code>dataset = dataset.batch(batch_size)
model.fit(x=dataset)
</code></pre>
",1020704,13036,https://stackoverflow.com/questions/62877768,Documentation Ambiguity
62611459,How to support mixed precision in custom Tensorflow layers?,"<p><strong>When developing my own custom layers for <code>tf.keras</code>: how am I supposed to support mixed precision?</strong></p>
<p>The <a href=""https://www.tensorflow.org/guide/mixed_precision"" rel=""nofollow noreferrer"">documentation of mixed precision</a> - a feature which is currently marked as experimental in Tensorflow 2.2 - only explains how to use it from a consumers perspective with predefined layers such as the <code>tf.keras.layers.Dense</code> one.</p>
<p>I already tried to guess it myself and found two - maybe relevant - details:</p>
<ul>
<li><p>The <code>dtype</code> property stays as <code>float32</code> by default when using 16-bit mixed precision.</p>
</li>
<li><p>There is a <code>mixed_precision.get_layer_policy(layer)</code> method (see <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/get_layer_policy"" rel=""nofollow noreferrer"">docs</a>) and a <code>mixed_precision.global_policy()</code> method (see <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/global_policy"" rel=""nofollow noreferrer"">docs</a>) which could be used to retrieve the configured <code>compute_dtype</code> and <code>variable_dtype</code>.</p>
</li>
</ul>
<p>Am I supposed to use the above <code>get_layer_policy</code>-method and just cast my variables into <code>compute_dtype</code> within the <code>call(...)</code> method of my layer? (And pass <code>variable_dtype</code> in my layers <code>build(...)</code> method to <code>add_weight(...)</code> when creating variables?)</p>
<p>For example, here is naive sample implementation of a standard dense neuron layer:</p>
<pre class=""lang-py prettyprint-override""><code>  def call(self, input):
    policy = mixed_precision.get_layer_policy(self)
    bias = tf.cast(self._bias, policy.compute_dtype)
    weights = tf.cast(self._weights, policy.compute_dtype)
    y = tf.nn.bias_add(tf.matmul(input, weights), bias)
    outputs = self._activation(y)
    return outputs
</code></pre>
<p>Sure, nobody would implement such basic stuff themselves, that one is just for demonstration. But, would this be the way the Tensorflow team expects us to implement the <code>call(...)</code> methods of our custom layers?</p>
",2020-06-27 14:44:15,"<p><a href=""https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s91029-automated-mixed-precision-tools-for-tensorflow-training-v2.pdf"" rel=""nofollow noreferrer"">This</a> guide (slide 13-14) from nvidia mentions custom layers for mixed precision training.</p>
<p>You have to implement the method <code>cast_input()</code>. In this example the layer is casted to float16 when mixed precision is enabled:</p>
<pre class=""lang-py prettyprint-override""><code>class CustomBiasLayer(tf.keras.layers.Layer):

 def build(self, _):
   self.v = self.add_weight('v', ())
   self.built = True
  
 def call(self, inputs):
   return inputs + self.v

 def cast_inputs(self, inputs):
   # Casts to float16, the policy's lowest-precision dtype
   return self._mixed_precision_policy.cast_to_lowest(inputs)
</code></pre>
<p>I have not tried this myself, so please let me know if this works for you.</p>
",1037200,11855,https://stackoverflow.com/questions/62611459,Documentation Replication on Other Examples
68136894,how to construct object of the same type that `tf.one_hot` returns?,"<p>I have a function <code>input_preprocess</code> that I am using in the data pipeline:</p>
<pre><code>def input_preprocess(image, label):
    if label == 1:
        return tf.zeros(NUM_CLASSES)
    else:
        label = tf.one_hot(label, NUM_CLASSES)
    return image, label
</code></pre>
<p>The problem is that whatever <code>tf.one_hot</code> returns is a <code>sequence</code> but what <code>tf.zeros</code> returns is not.</p>
<p>I get the following error:</p>
<pre><code> The two structures don't have the same nested structure.
    
    First structure: type=Tensor str=Tensor(&quot;cond/zeros_like:0&quot;, shape=(28,), dtype=float32)
    
    Second structure: type=tuple str=(&lt;tf.Tensor 'args_0:0' shape=(224, 224, 3) dtype=float32&gt;, &lt;tf.Tensor 'cond/one_hot:0' shape=(28,) dtype=float32&gt;)
    
    More specifically: Substructure &quot;type=tuple str=(&lt;tf.Tensor 'args_0:0' shape=(224, 224, 3) dtype=float32&gt;, &lt;tf.Tensor 'cond/one_hot:0' shape=(28,) dtype=float32&gt;)&quot; is a sequence, while substructure &quot;type=Tensor str=Tensor(&quot;cond/zeros_like:0&quot;, shape=(28,), dtype=float32)&quot; is not
    Entire first structure:
    .
    Entire second structure:
    (., .)
</code></pre>
<p>How can I manually construct something that could stand in for what <code>tf.one_hot</code> returns?</p>
",2021-06-25 20:46:07,"<p><code>tf.one_hot</code> returns an <code>EagerTensor</code> just like <code>tf.zeros</code>:</p>
<pre><code>a = tf.one_hot(1,2)
print(type(a))
b = tf.zeros(2)
print(type(b))
# tensorflow.python.framework.ops.EagerTensor
# tensorflow.python.framework.ops.EagerTensor
</code></pre>
<p>I think your issue is that your <em>function</em> <code>input_preprocess</code> returns a <em>single value</em> if <code>label == 1</code> (first return) and a <em>tuple</em> otherwise (second return).</p>
",1105837,1703,https://stackoverflow.com/questions/68136894,Documentation Replicability
55005915,How to resize image to put into tf.train.Example,"<p>I have an image (JPEG or PNG) as a byte buffer (read from the internet), and this is the way I was putting it in a <code>tf.train.Example</code> before:</p>

<pre><code>record = tf.train.Example(features=tf.train.Features(feature={
    'image/encoded': dataset_util.bytes_feature(image_bytes)
    # there are more features but they're not relevant
}))
</code></pre>

<p>However, for my usecase, the images are too big, so I'd like to resize them either before I put them in the <code>tf.train.Example</code> or just after (whichever is easiest).</p>

<p>Here's what I'm trying:</p>

<pre><code># predeclared
# - image_bytes
# - image_format
# - height
# - width

# resize image
if image_format == b'jpeg':
    image = tf.image.decode_jpeg(image_bytes, None, tf.float32)
elif image_format == b'png':
    image = tf.image.decode_png(image_bytes, None, tf.float32)

image = tf.image.resize_images(image, (int(height), int(width)))

image = tf.image.convert_image_dtype(image, tf.uint8)
record = tf.train.Example(features=tf.train.Features(feature={
    'image/encoded': dataset_util.bytes_feature(tf.image.encode_jpeg(image))
    # there are more features but they're not relevant
}))
</code></pre>

<p>I suspect this is valid right up until I actually try to put it in the <code>tf.train.Example</code>, at which point it tells me <code>TypeError: &lt;tf.Tensor 'EncodeJpeg:0' shape=() dtype=string&gt; has type Tensor, but expected one of: bytes</code>. I've tried figuring out how to get the <code>Tensor</code> into a <code>BytesList</code> or something like it, but I haven't been able to find any documentation for this. I suspect there may be a better way to approach the entire process however.</p>

<p>How can I do this the right way?</p>
",2019-03-05 15:11:05,"<p>You can resize prior to encoding. </p>

<pre><code>def int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
</code></pre>

<p>Convert from string and resize </p>

<pre><code>image = numpy.fromstring(byte_arr).reshape((height, width, channels))
image_raw = image.tostring()
</code></pre>

<p>Then serialize as tfrecords file</p>

<pre><code>writer = tf.python_io.TFRecordWriter(tfr_name)
example = tf.train.Example(features=tf.train.Features(feature{'height':int64_feature(height),
                                                              'width': int64_feature(width),
                                                              'channels': int64_feature(channels),
                                                              'image_raw': bytes_feature(image_raw),

writer.write(example.SerializeToString())
writer.close()
</code></pre>
",1129436,3136,https://stackoverflow.com/questions/55005915,Lack of Alternative Solutions/Documentation
63096162,How to expand_dims an unknown number of times in TensorFlow?,"<p>If the number of dimensions of an element is known then I can call <code>tf.expand_dims</code>. How do I put <code>tf.expand_dims</code> in a loop? The following code works in eager but not in graph mode.</p>
<pre class=""lang-py prettyprint-override""><code># @tf.function
def broadcast_multiply(x, y):
    print(tf.shape(x)) # [2, 2, ?, ?, ... ?]
    print(tf.shape(y)) # [2, 2]

    # Doesnt work in graph mode but works in eager
    rank_diff = tf.rank(x) - tf.rank(y)
    for _ in tf.range(rank_diff):
        y = tf.expand_dims(y, -1)

    return x * y
</code></pre>
",2020-07-26 04:42:54,"<p>You should check out <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/broadcast_to"" rel=""nofollow noreferrer""><code>tf.broadcast_to</code></a></p>
<pre><code>def broadcast_multiply(x, y):
    y = tf.broadcast_to(y, tf.shape(x))
    return x * y
</code></pre>
",1217998,3176,https://stackoverflow.com/questions/63096162,Documentation Replicability
67847893,Understanding true_classes in log_uniform_candidate_sampler,"<p><a href=""https://www.tensorflow.org/tutorials/text/word2vec"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/word2vec</a> uses <code>tf.random.log_uniform_candidate_sampler</code> for negative sampling.</p>
<p>The tutorial sets true_classes to context_class.</p>
<p>My experiment shows no matter what I set for true_classes, the function always yields good results.</p>
<pre><code>&gt; tf.random.log_uniform_candidate_sampler( true_classes=[[1]],
                num_true=1, num_sampled=num_ns, 
                unique=True, range_max=vocab_size)
[0, 1, 7, 5]

&gt; tf.random.log_uniform_candidate_sampler( true_classes=[[2]],
                num_true=1, num_sampled=num_ns, 
                unique=True, range_max=vocab_size)
[0, 6, 2, 5]
</code></pre>
<p>What does <code>true_classes</code> mean in this function?</p>
",2021-06-05 08:49:48,"<p>The line in the tutorial:</p>
<blockquote>
<p>You can call the function on one skip-grams's target word and pass the
context word as a true class to exclude it from being sampled</p>
</blockquote>
<p>That's misleading.</p>
<blockquote>
<p>What does true_classes mean in this function?</p>
</blockquote>
<p>Function returns <code>true_expected_count</code> which is defined in this <a href=""https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/ops/candidate_sampling_ops.py#L180"" rel=""nofollow noreferrer"">line of the source code.</a>.</p>
<p><code>true_classes</code> seems only used to calculate <code>true_expected_count</code>. So this function <strong>does not exclude</strong> negative classes. Every label has a probability to get sampled.</p>
<p>I copy an example code that can be experimented on (in case something happens to the link), <a href=""https://github.com/tensorflow/tensorflow/issues/44758#issuecomment-828114423"" rel=""nofollow noreferrer"">taken from this GitHub issue</a>:</p>
<pre><code># Do sampling 1000 times using true_classes [0, 8]
sample_func = lambda ii: tf.random.log_uniform_candidate_sampler(true_classes=[[ii]], num_true=1, num_sampled=4, unique=True, range_max=8, seed=42)
dd = {ii : np.stack([sample_func(ii)[0].numpy() for jj in range(1000)]) for ii in range(8)}
# Calculate the distribution in each true_class
for ii in dd:
    print(&quot;true_class:&quot;, ii, &quot;, negative value_counts:&quot;, pd.value_counts(dd[ii].flatten()).to_dict())
# true_class: 0 , negative value_counts: {0: 871, 1: 722, 2: 584, 3: 466, 4: 402, 5: 329, 7: 319, 6: 307}
# true_class: 1 , negative value_counts: {0: 867, 1: 695, 2: 571, 3: 485, 4: 411, 5: 380, 6: 316, 7: 275}
# true_class: 2 , negative value_counts: {0: 869, 1: 716, 2: 541, 3: 488, 4: 389, 5: 357, 6: 321, 7: 319}
# true_class: 3 , negative value_counts: {0: 877, 1: 715, 2: 582, 3: 482, 4: 394, 5: 355, 6: 318, 7: 277}
# true_class: 4 , negative value_counts: {0: 883, 1: 716, 2: 566, 3: 489, 4: 394, 5: 367, 6: 316, 7: 269}
# true_class: 5 , negative value_counts: {0: 862, 1: 717, 2: 583, 3: 496, 4: 376, 5: 357, 6: 315, 7: 294}
# true_class: 6 , negative value_counts: {0: 859, 1: 725, 2: 575, 3: 482, 4: 413, 5: 356, 6: 302, 7: 288}
# true_class: 7 , negative value_counts: {0: 880, 1: 724, 2: 555, 3: 488, 4: 425, 5: 324, 7: 302, 6: 302}

# Result of `true_expected_count`
print({ii : np.mean([sample_func(ii)[1].numpy() for jj in range(1000)]) for ii in range(8)})
# {0: 0.99967235, 1: 0.7245632, 2: 0.5737029, 3: 0.47004792, 4: 0.3987442, 5: 0.34728608, 6: 0.3084587, 7: 0.27554017}
</code></pre>
",746461,5985,https://stackoverflow.com/questions/67847893,Documentation Replication on Other Examples
63146831,What is the analytic interpretation for Tensorflow custom gradient?,"<p>In the official <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">tf.custom_gradient</a> documentation it shows how to define custom gradients for <code>log(1 + exp(x))</code></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.math.log(1 + e), grad
</code></pre>
<p>When <code>y = log(1 + exp(x))</code>, analytically the derivative comes out to be <code>dy/dx = (1 - 1 / (1 + exp(x)))</code>.</p>
<p>However in the code <code>def grad</code> says its <code>dy * (1 - 1 / (1 + exp(x)))</code>.
<code>dy/dx = dy * (1 - 1 / (1 + exp(x)))</code> is not a valid equation. While <code>dx = dy * (1 - 1 / (1 + exp(x)))</code> is wrong as it should be the reciprocal.</p>
<p>What does the <code>grad</code> function equate to?</p>
",2020-07-29 05:26:15,"<p>I finally figured it out. The <code>dy</code> should be called <code>upstream_gradient</code> or <code>upstream_dy_dx</code>.</p>
<p>By chain rule we know that</p>
<p><a href=""https://i.stack.imgur.com/7g3aZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7g3aZ.png"" alt=""chain rule"" /></a></p>
<p>where <code>dx[i]/dx[i+1]</code> is the gradient of the current function.</p>
<p>So <code>dy</code> is the product of all the gradients upstream before this function.</p>
<p><a href=""https://i.stack.imgur.com/nu4Z8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nu4Z8.png"" alt=""enter image description here"" /></a></p>
<p>So, if you forget to multiply the <code>dy</code> it is effectively the same as <a href=""https://www.tensorflow.org/api_docs/python/tf/stop_gradient"" rel=""nofollow noreferrer"">tf.stop_gradient</a></p>
<p>Here is a code which demos this. Full notebook <a href=""https://github.com/Ghost---Shadow/differentiable-programming-handbook/blob/master/notebooks/custom-gradient.ipynb"" rel=""nofollow noreferrer"">here</a></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def foo(x):
    tf.debugging.assert_rank(x, 0)

    def grad(dy_dx_upstream):
        dy_dx = 2 * x
        dy_dx_downstream = dy_dx * dy_dx_upstream
        tf.print(f'x={x}\tupstream={dy_dx_upstream}\tcurrent={dy_dx}\t\tdownstream={dy_dx_downstream}')
        return dy_dx_downstream
    
    y = x ** 2
    tf.print(f'x={x}\ty={y}')
    
    return y, grad


x = tf.constant(2.0, dtype=tf.float32)

with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    y = foo(foo(foo(x))) # y = x ** 8

tf.print(f'\nfinal dy/dx={tape.gradient(y, x)}')
</code></pre>
<p>Output</p>
<pre><code>x=2.0   y=4.0
x=4.0   y=16.0
x=16.0  y=256.0
x=16.0  upstream=1.0    current=32.0        downstream=32.0
x=4.0   upstream=32.0   current=8.0     downstream=256.0
x=2.0   upstream=256.0  current=4.0     downstream=1024.0

final dy/dx=1024.0
</code></pre>
",1217998,3176,https://stackoverflow.com/questions/63146831,Documentation Replication on Other Examples
54155481,TensorFlow per_image_standardization vs mean standardization across full dataset,"<p>I am curious about the difference between standardizing each image individually vs standardizing across the full data set.</p>

<p>I am using <a href=""https://github.com/tensorflow/models/tree/master/official/resnet"" rel=""nofollow noreferrer"">tensorflow/models/official/resnet</a> which is built using tf.estimator.  The tf estimator supports an input pipeline function that produces a tf Dataset.  The Dataset object applies the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization"" rel=""nofollow noreferrer"">tf.image.per_image_standardization</a> op that standardizes by subtracting the mean of the image itself from each pixel and enforces unit variance.  </p>

<p>This is different from other ML preprocessing that standardizes the image based on the mean across the whole dataset, such as with <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""nofollow noreferrer"">sklearn.preprocessing.StandardScaler</a>.</p>

<p>I'm confused as to whether any aspect of this input pipeline is persisted in the tf SavedModel exported from the tf.estimator.Estimator.</p>

<p>So I'm wondering if I need to still apply feature standardization when serving the model, either via tf.contrib.predictor or when deploying the model in any other dnn format.</p>

<p>Should I be applying standardization across the dataset even though I'm using the per_image_standardization?  If so, should I just export the mean value from the whole image set somehow so that when serving the model the server can just pick up the mean value from the whole dataset and apply standardization that way?</p>
",2019-01-11 23:45:32,"<p>In StandardScaler, we do feature-wise normalization. In case of images, we can do pixel-wise normalization, by considering the entire data-distribution, but that is not helpful, because of variability in the distribution. So <strong>per_image_standardization</strong> is preferred, which normalize the entire image to mean zero and std 1. <strong>It also make learning fast.</strong></p>

<p>Further <a href=""https://stats.stackexchange.com/questions/211436/why-normalize-images-by-subtracting-datasets-image-mean-instead-of-the-current"">this-link</a> can be helpful. There is <a href=""https://becominghuman.ai/image-data-pre-processing-for-neural-networks-498289068258"" rel=""nofollow noreferrer"">another-link</a>, where author has explained this by taking an example.</p>
",779837,3033,https://stackoverflow.com/questions/54155481,Documentation Replication on Other Examples
57296471,How can one use TensorFlow.js tf.data.generator for remote data sources since generators can't use callbacks,"<p>When introducing the tf.data.Dataset API, the <a href=""https://www.manning.com/books/deep-learning-with-javascript"" rel=""nofollow noreferrer"">Deep Learning with JavaScript book</a> says:</p>

<blockquote>
  <p>Large applications require technology for accessing data from a remote source, piece by piece, on demand.</p>
</blockquote>

<p>But the documentation I've read about generators says a generator can't produce values via callbacks. But how else can one access remote sources? I don't see how one can use <a href=""https://js.tensorflow.org/api/latest/#data.generator"" rel=""nofollow noreferrer"">tf.data.generator</a> in such cases. <a href=""https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/yield"" rel=""nofollow noreferrer"">MDN documentation on yield</a> states:</p>

<blockquote>
  <p>yield can only be called directly from the generator function that contains it. It can't be called from nested functions or from callbacks.</p>
</blockquote>
",2019-07-31 17:54:30,"<p>You can pass an <code>async</code> function (or a function returning a Promise) to the generator. It is then okay to use <code>await</code> inside the function (even inside a loop) to handle any asynchronous tasks.</p>

<p><strong>Code Sample</strong></p>

<pre class=""lang-js prettyprint-override""><code>const dataset = tf.data.generator(async function* () {
    const dataToDownload = await fetch(/* ... */);
    while (/* ... */) {
        const moreData = await fetch(/* ... */);
        yield otherData;
    }
});
</code></pre>

<p>This example uses <a href=""https://github.com/bitinn/node-fetch"" rel=""nofollow noreferrer""><code>node-fetch</code></a>, of course any other method of downloading data also works fine.</p>

<h3>Async Generators</h3>

<p>Regarding the MDN documentation, generators can be defined as <code>async</code>, but this changes the way they work. Instead of returning the value right away, they will return a Promise that you have to await for. So, instead of calling <code>iterator.next()</code>, you have to call <code>await iterator.next()</code> to read the value.</p>

<p><strong>Code Sample</strong></p>

<pre class=""lang-js prettyprint-override""><code>async function* foo(index) {
  while (true) {
    yield index++;
  }
}

(async () =&gt; {
  const iterator = foo(0);
  console.log((await iterator.next()).value); // 0
  console.log((await iterator.next()).value); // 1
})();
</code></pre>

<p>Luckily, Tensorflow.js is able to handle <code>async</code> functions/Promises in generators.</p>
",1230710,458,https://stackoverflow.com/questions/57296471,Documentation Ambiguity
57014236,How to use the Embedding Projector in Tensorflow 2.0,"<p>With the tf.contrib module gone from Tensorflow, and with tf.train.Saver() also gone, I cannot find a way to store a set of embeddings and their corresponding thumbnails, so that the Tensorboard Projector can read them.</p>

<p>The <a href=""https://www.tensorflow.org/tensorboard/r2/get_started"" rel=""noreferrer"">Tensorboard documentation</a> for Tensorflow 2.0 explains how to create plots and summaries, and how to use the summary tool in general, but nothing about the projector tool. Has anyone found how to store datasets for visualization?</p>

<p>If possible, I would appreciate a (minimal) code example.</p>
",2019-07-12 21:30:31,"<p>It seems there are some issues left in tensorboard. However, there are some
workarounds (for now) for preparing embeddings for projector with tensorflow2:
(bug report at: <a href=""https://github.com/tensorflow/tensorboard/issues/2471"" rel=""noreferrer"">https://github.com/tensorflow/tensorboard/issues/2471</a>)</p>

<p>tensorflow1 code would look something like that:</p>

<pre><code>embeddings = tf.compat.v1.Variable(latent_data, name='embeddings')
CHECKPOINT_FILE = TENSORBOARD_DIR + '/model.ckpt'
# Write summaries for tensorboard
with tf.compat.v1.Session() as sess:
    saver = tf.compat.v1.train.Saver([embeddings])
    sess.run(embeddings.initializer)
    saver.save(sess, CHECKPOINT_FILE)
    config = projector.ProjectorConfig()
    embedding = config.embeddings.add()
    embedding.tensor_name = embeddings.name
    embedding.metadata_path = TENSORBOARD_METADATA_FILE

projector.visualize_embeddings(tf.summary.FileWriter(TENSORBOARD_DIR), config)
</code></pre>

<p>when using eager mode in tensorflow2 this should (?) look somehow like this:</p>

<pre><code>embeddings = tf.Variable(latent_data, name='embeddings')
CHECKPOINT_FILE = TENSORBOARD_DIR + '/model.ckpt'
ckpt = tf.train.Checkpoint(embeddings=embeddings)
ckpt.save(CHECKPOINT_FILE)

config = projector.ProjectorConfig()
embedding = config.embeddings.add()
embedding.tensor_name = embeddings.name
embedding.metadata_path = TENSORBOARD_METADATA_FILE

writer = tf.summary.create_file_writer(TENSORBOARD_DIR)
projector.visualize_embeddings(writer, config)
</code></pre>

<p>however, there are 2 issues:</p>

<ul>
<li>the <code>writer</code> created with <code>tf.summary.create_file_writer</code> does not have the function <code>get_logdir()</code> required by <code>projector.visualize_embeddings</code>, a simple workaround is to patch the <code>visualize_embeddings</code> function to take the logdir as parameter.</li>
<li>the checkpoint format has changed, when reading the checkpoint with load_checkpoint (which seems to be the tensorboard way of loading the file), the variable names change. e.g. <code>embeddings</code> changes to something like <code>embeddings/.ATTRIBUTES/VARIABLE_VALUE</code> (also there are additional variables in the map extracted by <code>get_variable_to_shape_map()</code>but they are empty anyways).</li>
</ul>

<p>the second issue was solved with the following quick-and-dirty workaround (and logdir is now a parameter of <code>visualize_embeddings()</code>)</p>

<pre><code>embeddings = tf.Variable(latent_data, name='embeddings')
CHECKPOINT_FILE = TENSORBOARD_DIR + '/model.ckpt'
ckpt = tf.train.Checkpoint(embeddings=embeddings)
ckpt.save(CHECKPOINT_FILE)

reader = tf.train.load_checkpoint(TENSORBOARD_DIR)
map = reader.get_variable_to_shape_map()
key_to_use = """"
for key in map:
    if ""embeddings"" in key:
        key_to_use = key

config = projector.ProjectorConfig()
embedding = config.embeddings.add()
embedding.tensor_name = key_to_use
embedding.metadata_path = TENSORBOARD_METADATA_FILE

writer = tf.summary.create_file_writer(TENSORBOARD_DIR)
projector.visualize_embeddings(writer, config,TENSORBOARD_DIR)
</code></pre>

<p>I did not find any examples on how to use tensorflow2 to directly write the embeddings for tensorboard, so I am not sure if this is the right way, but if it is, then those two issues would need to be addressed, and at least for now there is a workaround.</p>
",1357690,425,https://stackoverflow.com/questions/57014236,Documentation Replication on Other Examples
54606302,tf.data.Dataset from tf.keras.preprocessing.image.ImageDataGenerator.flow_from_directory?,"<p>How do I create a <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer""><code>tf.data.Dataset</code></a> from <a href=""https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory"" rel=""noreferrer""><code>tf.keras.preprocessing.image.ImageDataGenerator.flow_from_directory</code></a>?</p>

<p>I'm considering <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""noreferrer""><code>tf.data.Dataset.from_generator</code></a>, but it's unclear how to acquire the <code>output_types</code> keyword argument for it, given the return type:</p>

<blockquote>
  <p>A <code>DirectoryIterator</code> yielding tuples of <code>(x, y)</code> where <code>x</code> is a numpy array containing a batch of images with shape <code>(batch_size, *target_size, channels)</code> and <code>y</code> is a numpy array of corresponding labels.</p>
</blockquote>
",2019-02-09 12:40:03,"<p>Both <a href=""https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/keras/_impl/keras/preprocessing/image.py#L1351"" rel=""noreferrer"">batch_x</a> and <a href=""https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/keras/_impl/keras/preprocessing/image.py#L1383"" rel=""noreferrer"">batch_y</a> in <code>ImageDataGenerator</code> are of type <code>K.floatx()</code>, so must be <code>tf.float32</code> by default.</p>

<p>Similar question was discussed already at <a href=""https://stackoverflow.com/questions/52636127/how-to-use-keras-generator-with-tf-data-api"">How to use Keras generator with tf.data API</a>. Let me copy-paste the answer from there:</p>

<pre><code>def make_generator():
    train_datagen = ImageDataGenerator(rescale=1. / 255)
    train_generator = 
    train_datagen.flow_from_directory(train_dataset_folder,target_size=(224, 224), class_mode='categorical', batch_size=32)
    return train_generator

train_dataset = tf.data.Dataset.from_generator(make_generator,(tf.float32, tf.float32))
</code></pre>

<p>The author faced another issue with the graph scope, but I guess it is unrelated to your question.</p>

<p>Or as a one liner:</p>

<pre><code>tf.data.Dataset.from_generator(lambda:
    ImageDataGenerator().flow_from_directory('folder_path'),(tf.float32, tf.float32))
</code></pre>
",587021,13244,https://stackoverflow.com/questions/54606302,Documentation Replication on Other Examples
54985037,How to convert TF Tensor holding value into Tensor holding categorical values,"<p>I'm paring TFRecords which provide me a label as numerical value. But I need to convert this value into categorical vector while I'm reading proto records. How can I do that. Here is code snippet for reading the proto records:</p>

<pre><code> def parse(example_proto):
     features={'label':: tf.FixedLenFeature([], tf.int64), ...}
     parsed_features = tf.parse_single_example(example_proto, features)
     label = tf.cast(parsed_features['label'], tf.int32)
     # at this point label is a Tensor which holds numerical value
     # but I need to return a Tensor which holds categorical vector
     # for instance, if my label is 1 and I have two classes
     # I need to return a vector [1,0] which represents categorical values
</code></pre>

<p>I know that there is <code>tf.keras.utils.to_categorical</code> function but it does not take a Tensor as an input.</p>
",2019-03-04 14:14:48,"<p>You simply need to convert the label to its one-hot representation (that's the representation you described):</p>

<pre><code>label = tf.cast(parsed_features['label'], tf.int32)
num_classes = 2
label = tf.one_hot(label, num_classes)
</code></pre>
",779856,1542,https://stackoverflow.com/questions/54985037,Documentation Replication on Other Examples
58843164,Use `tf.contrib.predictor` to predict on batches from `tf.estimator.export_savedmodel` for TF 1.13,"<p>I found several examples to load my saved estimator, <code>my_estimator.export_savedmodel(export_dir, export_input_fn)</code> model, as a predictor like so, <code>predictor = tf.contrib.predictor.from_saved_model(export_dir)</code>. This works great when my <code>tf.train.Example</code> has only one item. How can I make it work for a batch for TF 1.13?</p>

<pre class=""lang-py prettyprint-override""><code>model_input= tf.train.Example(features=tf.train.Features(feature={
      'browser_name': tf.train.Feature(bytes_list=tf.train.BytesList(value=[b""chrome"", b""ie""])),
      'version': tf.train.Feature(float_list=tf.train.FloatList(value=[8.0, 11.0]))     
  })).SerializeToString()
</code></pre>

<pre class=""lang-py prettyprint-override""><code>predictor({""inputs"":[model_input]})
</code></pre>

<p>call fails when there are multiple inputs per feature.</p>
",2019-11-13 18:12:08,"<p>Here's a working example with tensorflow 1.13.1 :</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import pandas as pd 
import numpy as np


prod_export_dir = 'my_model_dir'
data =  pd.read_csv('my_data.csv')

predictor = tf.contrib.predictor.from_saved_model(prod_export_dir)

model_input = {}
for k, v in predictor.feed_tensors.items():
    model_input[k] = np.array(data[k].tolist(), dtype=v.dtype.as_numpy_dtype)

prediction = predictor(model_input)
</code></pre>
",1585523,7377,https://stackoverflow.com/questions/58843164,Documentation Replication on Other Examples
58631390,What is the purpose of tf.compat?,"<p>What's the purpose of tf.compat module? It looks like just the entire Tensorflow API is replicated inside this module.
The documentation states</p>

<blockquote>
  <p>Functions for Python 2 vs. 3 compatibility.</p>
</blockquote>

<p>So why there is a ""v1"" and a ""v2"" submodule? What are the compatibility problems address by tf.compat specifically?</p>
",2019-10-30 18:08:57,"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/compat"" rel=""noreferrer""><code>tf.compat</code></a> allows you to write code that works both in TensorFlow 1.x and 2.x. For example, the following piece of code:</p>

<pre><code>import tensorflow as tf

tf.compat.v1.disable_v2_behavior()
with tf.compat.v1.Session() as sess:
    x = tf.compat.v1.placeholder(tf.float32, [2])
    x2 = tf.square(x)
    print(sess.run(x2, feed_dict={x: [2, 3]}))
    # [4. 9.]
</code></pre>

<p>Runs the same on TensorFlow 1.15.0 and 2.0.0, even though session and placeholders were deprecated in 2.x. Likewise, <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v2"" rel=""noreferrer""><code>tf.compat.v2</code></a> allows you to use things introduced in 2.x from 1.x. Also, these APIs provide also backwards compatibility for the future too, so if at some point a 3.x version is released, the mechanism to write version-independent code will already be there since the first version of 2.x.</p>

<p>EDIT: The documentation for the module about Python should actually be changed. Originally, <code>tf.compat</code> only held functions for that purpose (and it was like that until 1.13, <a href=""https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/compat"" rel=""noreferrer"">see all module documentation</a>). However, it was later repurposed for TensorFlow version compatibility.</p>
",1719931,3428,https://stackoverflow.com/questions/58631390,Documentation Completeness
55213452,Tensorflow: stacking subarrays in a tensor,"<p>I have a tensor that looks like:</p>

<pre><code>array([[[  1,   2,   3],
        [  3,   4,   5]],
       [[11, 22, 33],
        [33, 44, 55]]], dtype=int32)
</code></pre>

<p>I would like to concatenate/stack the values at each index in the inner array so it looks like:</p>

<pre><code>array([[[1, 3], [2, 4], [3, 5]],
       [[11, 33], [22, 44], [33, 55]]], dtype=int32)
</code></pre>

<p>I've tried various forms of tf.concat and tf.stack/tf.unstack, but can't seem to get it right. Does anyone know how to do this?</p>
",2019-03-18 00:52:21,"<p>You can use <code>tf.transpose()</code>:</p>

<pre><code># t
# array([[[ 1,  2,  3],
#         [ 3,  4,  5]],

#        [[11, 22, 33],
#         [33, 44, 55]]])

tf.transpose(t, perm=[0, 2, 1])
# array([[[ 1,  3],
#         [ 2,  4],
#         [ 3,  5]],

#        [[11, 33],
#         [22, 44],
#         [33, 55]]])
</code></pre>
",1733587,399,https://stackoverflow.com/questions/55213452,Documentation Replicability
64474463,Custom f1_score metric in tensorflow,"<p>I want to implement the f1_score metric for tf.keras.</p>
<pre><code>from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.metrics import Accuracy, BinaryAccuracy
from sklearn.metrics import accuracy_score
import numpy as np
import tensorflow as tf

class F1_Score(tf.keras.metrics.Metric):

    def __init__(self, name='f1_score', **kwargs):
        super().__init__(name=name, **kwargs)
        self.f1 = self.add_weight(name='f1', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        p = Precision(thresholds=0.5)(y_true, y_pred)
        r = Recall(thresholds=0.5)(y_true, y_pred)
        self.f1 = 2 * ((p * r) / (p + r + 1e-6))

    def result(self):
        return self.f1

    def reset_states(self):
        self.f1.assign(0)
        
model = Sequential([
  Dense(64, activation='relu', input_shape=(784,)),
  Dense(64, activation='relu'),
  Dense(4, activation='sigmoid'),
])
x = np.random.normal(size=(10, 784))
y = np.random.choice(2, size=(10, 4))
model.compile(optimizer=Adam(0.001), loss='binary_crossentropy',
                  metrics=['accuracy', , F1_Score()])
model.fit(x[:1], y[:1], batch_size=1, epochs=1, verbose=1)
</code></pre>
<p>I got an error:</p>
<blockquote>
<p>ValueError: tf.function-decorated function tried to create variables
on non-first call.</p>
</blockquote>
",2020-10-22 02:40:16,"<p>You get this error because you want to instantiate some <code>tf.Variable</code>s during the update_state function. When instantiate object from the class Precision and Recall, you are creating some <code>tf.Variable</code>s.</p>
<p>Instantiate the objects in the constructor, and call them in the update_state function:</p>
<pre class=""lang-py prettyprint-override""><code>class F1_Score(tf.keras.metrics.Metric):

    def __init__(self, name='f1_score', **kwargs):
        super().__init__(name=name, **kwargs)
        self.f1 = self.add_weight(name='f1', initializer='zeros')
        self.precision_fn = Precision(thresholds=0.5)
        self.recall_fn = Recall(thresholds=0.5)

    def update_state(self, y_true, y_pred, sample_weight=None):
        p = self.precision_fn(y_true, y_pred)
        r = self.recall_fn(y_true, y_pred)
        # since f1 is a variable, we use assign
        self.f1.assign(2 * ((p * r) / (p + r + 1e-6)))

    def result(self):
        return self.f1

    def reset_states(self):
        # we also need to reset the state of the precision and recall objects
        self.precision_fn.reset_states()
        self.recall_fn.reset_states()
        self.f1.assign(0)
</code></pre>
<hr />
<p><strong>Explanation of the behavior :</strong></p>
<p>Tensorflow allow to create Variable only on the first call of a <code>tf.function</code>, see the <a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""nofollow noreferrer"">documentation</a> :</p>
<blockquote>
<p>tf.function only allows creating new tf.Variable objects when it is called for the first time</p>
</blockquote>
<p>Keras metrics are wrapped in a tf.function to allow compatibility with tensorflow v1. You can find this comment in the <a href=""https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/metrics.py#L81-L314"" rel=""nofollow noreferrer"">code</a></p>
<blockquote>
<p>If <code>update_state</code> is not in eager/tf.function and it is not from a
built-in metric, wrap it in <code>tf.function</code>. This is so that users writing
custom metrics in v1 need not worry about control dependencies and
return ops.</p>
</blockquote>
<p>You also have another bug in your class, is that you override the <code>f1 tf.Variable</code> that you created with the calculation of your f1 score. To update the value of a variable, you need to use <code>assign</code>. And we must not forget to reset the states of the Precision and Recall Metrics objects in use!</p>
",2130515,1662,https://stackoverflow.com/questions/64474463,Documentation Replicability
61743921,can we build object detection model using Tensorflow or it is only possible with the help f tf.keras,"<p>Is there any way to build object detection model using Tensorflow without any help of tf.keras module?</p>

<p>From Tensorflow documentation I'm  not able to find any example which helps to create model without Keras.</p>
",2020-05-12 04:36:22,"<p>Keras is a high level API. But if you want to use only Tensorflow then you have to implement the architecture using low level API. You can certainly implement but you have to code it yourself to build all the convolutional layers and dense layer by yourself. </p>
",1490940,1485,https://stackoverflow.com/questions/61743921,Lack of Alternative Solutions/Documentation
56900599,Tensorflow 2: Getting Tensor Value,"<p>I am switching to TF2 and I just followed this <a href=""https://www.tensorflow.org/beta/tutorials/quickstart/advanced"" rel=""nofollow noreferrer"">tutorial</a>,
where the train and step functions are defined now as ""@tf.function"".</p>

<p>How can I print the values of the Tensors y_pred and loss?</p>

<pre><code>@tf.function
def train_step(images, labels):
  with tf.GradientTape() as tape:
    predictions = model(images)
    loss = loss_object(labels, predictions)

    print(""train preds: "", y_pred)
    print(""train loss: "", loss)

  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_loss(loss)
  train_accuracy(labels, predictions)
</code></pre>
",2019-07-05 09:47:23,"<p><code>print</code> executes in the Python world (not in graph), so it will only print the tensors once while <code>tf.function</code> is tracing your function to construct a graph. If you want to print in-graph, use <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/print"" rel=""nofollow noreferrer""><code>tf.print</code></a>.</p>
",2335224,1382,https://stackoverflow.com/questions/56900599,Documentation Replicability
64380057,TF 2.3.0 training keras model using tf dataset with sample weights does not apply to metrics,"<p>I am passing in sample_weight as the 3rd tuple in tf.data.Dataset (using it in the context of mask, so my sample_weight are either 0, or 1. The problem is that this sample_weight doesn't seem to get applied to metrics calculation. (Ref: <a href=""https://www.tensorflow.org/guide/keras/train_and_evaluate#sample_weights"" rel=""noreferrer"">https://www.tensorflow.org/guide/keras/train_and_evaluate#sample_weights</a>)</p>
<p>Here's code snippet:</p>
<pre><code>train_ds = tf.data.Dataset.from_tensor_slices((imgs, labels, masks))
train_ds = train_ds.shuffle(1024).repeat().batch(32).prefetch(buffer_size=AUTO)

model.compile(optimizer = Adam(learning_rate=1e-4),
             loss = SparseCategoricalCrossentropy(),
             metrics = ['sparse_categorical_accuracy'])

model.fit(train_ds, steps_per_epoch = len(imgs)//32, epochs = 20)
</code></pre>
<p>The loss after training is very close to zero, but sparse_categorical_accuracy is not (about 0.89). So I highly suspect whatever sample_weight (masks) that's passed in to construct the tf.dataset, does NOT get applied when the metrics is reported during training, while loss seems to be correct. I further confirmed by running prediction on the subset that are not masked separately, and confirmed the accuracy is 1.0</p>
<p>Also, according to documentation:</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy</a></p>
<p>the metric has 3 args: y_true, y_pred, sample_weight</p>
<p>So how does one pass the sample_weight during metric computation? Is this the responsibility of model.fit(...) within the keras framework? I can't find any example googling around so far.</p>
",2020-10-15 21:40:39,"<p>Upon some debugging and doc reading, i found there's weighted_metrics argument in .compile, which i should use instead of metrics=. I confirmed this fixed my test case in the shared colab.</p>
<pre><code>model.compile(optimizer = Adam(learning_rate=1e-4),
             loss = SparseCategoricalCrossentropy(),
             weighted_metrics = [SparseCategoricalAccuracy()])
</code></pre>
",1762295,3759,https://stackoverflow.com/questions/64380057,Inadequate Examples
66874943,Why iterations over the same tf.data.Dataset give different data each iteration?,"<p>I'm trying to understand how <strong>tf.data.Dataset</strong> works.</p>
<p>It says on the documentation that <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""nofollow noreferrer"">take</a> returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch):</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds

# Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=True)

# Build your input pipeline
ds = ds.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)
# ...
</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([2 0 6 6 8 8 6 0 3 4 8 7 5 2 5 7 8 7 1 1 1 8 6 4 0 4 3 2 4 2 1 9], shape=(32,), dtype=int64)
</code></pre>
<p>However, iterating over it again, gives different labels: (continuation of last code)</p>
<pre class=""lang-py prettyprint-override""><code>for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([7 3 5 6 3 1 7 9 6 1 9 3 9 8 6 7 7 1 9 7 5 2 0 7 8 1 7 8 7 0 5 0], shape=(32,), dtype=int64)
tf.Tensor([1 3 6 1 8 8 0 4 1 3 2 9 5 3 8 7 4 2 1 8 1 0 8 5 4 5 6 7 3 4 4 1], shape=(32,), dtype=int64)
</code></pre>
<p>Shouldn't the labels be the same, given that the dataset is the same?</p>
",2021-03-30 16:35:49,"<p>This is because the data files are shuffled and the dataset is shuffled with <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=ru#shuffle"" rel=""nofollow noreferrer""><code>dataset.shuffle()</code></a>.</p>
<p>With <code>dataset.shuffle()</code>, the data will be shuffled in a different way on each iteration by default.</p>
<p>One can remove <code>shuffle_files=True</code> and set the argument <code>reshuffle_each_iteration=False</code> to prevent reshuffling on different iterations.</p>
<p>The <code>.take()</code> function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them.</p>
<pre class=""lang-py prettyprint-override""><code># Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=False)

# Build your input pipeline
ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
    
for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)
tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)
</code></pre>
",2076973,1151,https://stackoverflow.com/questions/66874943,Documentation Replicability
59408065,tf.print from TF2 not printing to output,"<p>I'm using TF2 and I would like to print the tensor inside a function running in a tf.data.Dataset pipeline.</p>

<p>This is my code:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import sys

sys.stdout = open('tf.log', 'w')

def main():

    ## Dataset generator
    #
    numRows= 100

    indx = tf.reshape([i+1 for i in range(numRows)], [numRows,1])
    features = tf.random.uniform([numRows, 2], minval=1, maxval=10, dtype=tf.int32)

    myData = tf.concat([indx, features], 1)

    ## tf.data.Dataset
    #
    dataset = tf.data.Dataset.from_tensor_slices(myData)

    ## Pipeline
    #
    dataset.map(myFunc)

    ## Run pipeline
    #
    for d in dataset:
        print('--')

def myFunc(t):
    tf.print(t, output_stream=sys.stdout)
    return t

if __name__ == ""__main__"":
    main()
</code></pre>

<p>But as result, I'm getting only this:</p>

<pre><code>--
--
--
--
--
--
--
--
--
--
--
</code></pre>

<p>How can I use <code>tf.print</code> to print the tensors to the console ?</p>
",2019-12-19 10:44:19,"<p>You're not assigning the dataset returned by <code>map</code> to anything. Simply do,</p>

<pre><code>dataset = tf.data.Dataset.from_tensor_slices(myData)
dataset = dataset.map(myFunc)
</code></pre>
",2135819,2647,https://stackoverflow.com/questions/59408065,Documentation Replicability
62850250,Understanding the input_shape parameter of hub.KerasLayer,"<p>When transfer learning is done, one could use a model from the tf hub. Like MobilNetV2 or Inception. These models expects the inputs, the images in a certain size. So one has to resize the images into this size before applying the models. In this <a href=""https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub"" rel=""nofollow noreferrer"">tutorial</a> the following is used:</p>
<pre><code>feature_extractor_url = &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&quot; 

feature_extractor_layer = hub.KerasLayer(feature_extractor_url,
                                         input_shape=(224,224,3))
</code></pre>
<p>In this example the images were already resized to 224,224 before. I am wondering about the <code>input_shape=(224,224,3)</code>. In this <a href=""https://www.tensorflow.org/tutorials/images/transfer_learning"" rel=""nofollow noreferrer"">tutorial</a> the pretrained model is not loaded with the hub-KerasLayer, but instead using</p>
<pre><code>base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
</code></pre>
<p>Where IMG_SHAPE is</p>
<pre><code>IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)
</code></pre>
<p>and img_size is 160. So here the input_shape is input_shape=(160,160,3).</p>
<p>Now coming back to the:</p>
<pre><code>feature_extractor_layer = hub.KerasLayer(feature_extractor_url,
                                         input_shape=(224,224,3))
</code></pre>
<p>I was wondering what exactly the input_shape parameter  tells me or does? So I do not need to enter 224,224 here, right? I could enter another size, like 160, because my images were resized to this size? So MobilNetV2 does expect 224,224, but with this option I can specify something else? For <code>tf.keras.applications.MobileNetV2</code> I found the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2"" rel=""nofollow noreferrer"">documentation</a> where it exactly explains it:</p>
<blockquote>
<p>Optional shape tuple, to be specified if you would like to use a model
with an input image resolution that is not (224, 224, 3). It should
have exactly 3 inputs channels (224, 224, 3). You can also omit this
option if you would like to infer input_shape from an input_tensor. If
you choose to include both input_tensor and input_shape then
input_shape will be used if they match, if the shapes do not match
then we will throw an error. E.g. (160, 160, 3) would be one valid
value.</p>
</blockquote>
<p>So when I resized my images to 300,300 and I want to use MobileNetV2, can I use the following code:</p>
<pre><code> feature_extractor_url = &quot;https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2&quot; 
    
    feature_extractor_layer = hub.KerasLayer(feature_extractor_url,
                                             input_shape=(300,300,3))
</code></pre>
<p>Or do I have to resize to 224,224 and enter the 224,224 here?</p>
<p>When I check an <a href=""https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l06c03_exercise_flowers_with_transfer_learning_solution.ipynb#scrollTo=wVII2H9ZNNQf"" rel=""nofollow noreferrer"">implementation</a> for inception the images are resized to 299,299 and then the following code is used:</p>
<pre><code>IMAGE_RES = 299

feature_extractor = hub.KerasLayer(URL,
  input_shape=(IMAGE_RES, IMAGE_RES, 3),
  trainable=False)
</code></pre>
<p>Is it necessary to do this exactly to 299? Or could I also resize to another size, like 250 and give this as an input:</p>
<pre><code>   IMAGE_RES = 250

feature_extractor = hub.KerasLayer(URL,
  input_shape=(IMAGE_RES, IMAGE_RES, 3),
  trainable=False)
</code></pre>
<p>So the pretrained models do expect a certain fixed size and this input_shape parameter exists in order to make it flexible in case the user wants to use another size, right? But then why all these examples resize to exactly the size the models assume? I could also have done this to another size, right? So in all the examples it says like the models expects this and I understand it in the way that therefore we do have to resize to exactly what the model expects. But the input_shape parameter is exactly existing for this to make it flexible so that I do not have to resize to exactly what the model expects, but instead just resize to whatever I want and with the input_shape parameter I tell this to the model? As in the mentioned example with 160 image size. Or is this just possible in case I use <code>tf.keras.applications.MobileNetV2</code> loading the pretrained models, but when using <code>hub.KerasLayer</code> I cannot do it?</p>
",2020-07-11 14:13:10,"<p>This is a good observation.</p>
<p><strong>TLDR</strong>, different <code>Input Shapes</code> can be passed for <code>Models</code> of <code>tf.keras.applications</code> with the argument, <code>include_top = False</code> but that is not possible when we use <code>tf.keras.applications</code> with the argument, <code>include_top = True</code> and when we use <code>Models</code> of <code>Tensorflow Hub</code>.</p>
<p><strong>Detailed Explanation</strong>:</p>
<p>This <a href=""https://www.tensorflow.org/hub/common_signatures/images#image_input"" rel=""nofollow noreferrer"">Tensorflow Hub Documentation</a> states</p>
<pre><code>&gt; The height and width dimensions are fixed to the expected size of
&gt; input images. (Future work may remove that restriction for fully
&gt; convolutional modules.)
</code></pre>
<p>That's the reason, if we pass the <code>Image Shape</code> other than the Expected Shape, it raises an error,</p>
<pre><code> Expected these arguments to match one of the following 4 option(s):
    
    Option 1:
      Positional arguments (4 total):
        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')
        * True
        * False
        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')
      Keyword arguments: {}
    
    Option 2:
      Positional arguments (4 total):
        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')
        * True
        * True
        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')
      Keyword arguments: {}
    
    Option 3:
      Positional arguments (4 total):
        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')
        * False
        * True
        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')
      Keyword arguments: {}
    
    Option 4:
      Positional arguments (4 total):
        * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')
        * False
        * False
        * TensorSpec(shape=(), dtype=tf.float32, name='batch_norm_momentum')
      Keyword arguments: {}
</code></pre>
<p>Similarly, when we pass different <code>Input Shape</code> while using the <code>Pre-Trained Models</code> of <code>tf.keras.applications</code> with the argument, <strong><code>include_top = True</code></strong> (including the Dense Layers at the Top as well), it raises an error,</p>
<pre><code>ValueError: When setting `include_top=True` and loading `imagenet` 
weights, `input_shape` should be (224, 224, 3).
</code></pre>
<p>But if we set the value of argument, <strong><code>include_top = False</code></strong> while using the <code>Pre-Trained Models</code> from <code>tf.keras.applications</code>, the <code>Input_Shape</code> can be <strong>flexible</strong> i.e., for MobileNetV2, we can pass any of the shapes from the list, <code>[96, 128, 160, 192, 224]</code>) and for Models like <code>ResNet</code> or <code>VGGNet</code>, we can pass any <code>Input Shape</code>.</p>
",2165335,833,https://stackoverflow.com/questions/62850250,Documentation Ambiguity
53966148,Tensorflow: How to use boolean_mask in a way that perserves 2D tensor shape,"<p>When I use tf.boolean_mask, the result is flattened. </p>

<pre><code>tensor7 = tf.constant( [[ 0,  1,  2,  3, -1],[ 2,  3,  4, -1, -1],[ 3,  6,  5,  4,  3]], tf.int64)
mask7 = tf.constant([[ True,  True,  True,  True, False],  [ True,  True,  True, False, False], [ True,  True,  True,  True,  True]], tf.bool)
result7=tf.boolean_mask(tensor7, mask7, axis=0)  

with tf.Session() as sess:
    print(sess.run([ result7 ]))
</code></pre>

<blockquote>
  <p>array([0, 1, 2, 3, 2, 3, 4, 3, 6, 5, 4, 3])]</p>
</blockquote>

<p>Is there a way to use it that preserves the original 3 array shape? The shapes of the individual arrays should change since they're shorter now. I'm looking for something like this</p>

<blockquote>
  <p>[array([[ 0,  1,  2,  3 ],
         [ 2,  3,  4 ],
         [ 3,  6,  5,  4,  3]])</p>
</blockquote>
",2018-12-29 02:11:03,"<p>Your expectation may not be logically sound, as pointed out further <a href=""https://github.com/tensorflow/tensorflow/issues/18238#issuecomment-378646160"" rel=""nofollow noreferrer"">in this feature request</a> that resembles your question. You would need to have dynamic shapes for tensors, which has only limited support in TensorFlow (e.g. <a href=""https://www.tensorflow.org/guide/ragged_tensors"" rel=""nofollow noreferrer"">Ragged Tensors</a>).</p>
",3259896,5853,https://stackoverflow.com/questions/53966148,Documentation Replicability
63004540,How to pad 1 dimensinal vector in tensorflow? Getting InvalidArgumentError: paddings must be a matrix with 2 columns with tf.pad,"<p>I am trying to use tf.pad. Here is my attempt to pad the tensor to length 20, with values 10.</p>
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), paddings=20, constant_values=10)
</code></pre>
<p>I get this error message</p>
<pre><code>InvalidArgumentError: paddings must be a matrix with 2 columns: [2,1] [Op:PadV2]
</code></pre>
<p>I am looking at the documentation</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/pad</a></p>
<blockquote>
<p>paddings is an integer tensor with shape [n, 2], where n is the rank of tensor. For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension</p>
</blockquote>
<p>But I am unable to figure out how to shape the pad value</p>
",2020-07-20 22:12:17,"<p>You have to specify the padding at the beginning and the padding at the end of your vector by matrix of shape (1,2) :</p>
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), [[ 0 , 20]], constant_values=10)
</code></pre>
<p>if you have three-dimensional tensor (rank = 3 e.g : (225,225,3) ) the padding matrix has to be of shape (3, 2 ) where &quot;3&quot; is the rank, and &quot;2&quot; to specify the padding at the beginning and end of each dimension.</p>
<p>For example, a padding matrix  = [ [0,2], [5,5], [2,0] ], means that we want to pad the first dimension by 0 at the beginning (=no padding) and 2 at the end .padding the second dimension by 5 at beginning and 5 at the end.</p>
",3259896,5853,https://stackoverflow.com/questions/63004540,Documentation Replicability
58520594,"tf.Data.Dataset - On each Epoch, only train with a sub sample of the full dataset","<p>I have an image dataset with a large imbalance of positive and negatives samples (many more negatives). I would like to create a tf.data.Dataset where each epoch it will train with all of the positive samples but only (ratio * len(positive) ) of the negative samples. </p>

<p>I am currently using a datagen inherited from keras.util.Sequence to achieve this and using this subsampling policy is performing much better than training on all data.</p>

<p>However reading the docs on Dataset, I cannot seem to find a way to do it, is it possible?</p>

<p>In my existing data generator, I am doing this:</p>

<pre class=""lang-py prettyprint-override""><code># List if indicies of the positive and negative samples
positives = np.where(self.labels == 1)[0]
negatives = np.where(self.labels == 0)[0]
# How many of the negatives do we want to use?
n_negatives = np.clip(int(len(positives) * self.config.DATASET_NEGSUBSAMPLE_RATIO), 1, len(negatives))
# Choose random negatives
subsampled_negatives = np.random.choice(negatives, n_negatives, replace=False)
# Create the incidies array from the positive and subsamples negative indicies
self.indexes = np.concatenate((positives, subsampled_negatives))
# Shuffle them together
np.random.shuffle(self.indexes)
</code></pre>
",2019-10-23 10:16:33,"<p><code>positives</code> and <code>negatives</code> are defined as in the question.</p>

<pre><code>positives = [(0,1),(1,1),(2,1),(3,1),(4,1)]
negatives = [(10,0),(11,0),(12,0),(13,0),(14,0),(15,0),(16,0)]

NEGATIVE_SAMPLES = 3

pos_ds = tf.data.Dataset.from_tensor_slices(positives)
neg_ds = tf.data.Dataset.from_tensor_slices(negatives).shuffle(1000)

ds = pos_ds.concatenate(neg_ds.take(NEGATIVE_SAMPLES)).shuffle(1000)

els = [v.numpy().tolist() for v in list(ds)]
</code></pre>

<p>printing <code>els</code> for a sample execution gives:</p>

<pre><code>[[0, 1], [4, 1], [12, 0], [16, 0], [1, 1], [10, 0], [3, 1], [2, 1]]
</code></pre>

<p><strong>Note:</strong> you might need to experiment with the buffer sizes for the shuffle at the end of the <code>ds</code> definition.</p>
",2252698,115,https://stackoverflow.com/questions/58520594,Lack of Alternative Solutions/Documentation
74005009,How to create output_signature for tensorflow.dataset.from_generator,"<p>I have a generator yielding data and labels <code>yield data, labels</code> where the data is
an <code>numpy.ndarray</code> with variable rows and 500 columns of type <code>dtype=float32</code> and the labels are integers of <code>numpy.int64</code>.</p>
<p>I'm trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: <code>tf.data.Dataset.from_generator</code></p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">docs</a> say that the from_generator function needs a parameter <code>output_signature</code> as an input. But I'm having trouble understanding how to build this output_signature.</p>
<p>How can I make the output_signature for the generator I described?</p>
<p>Thank you!</p>
<p>Edit:
I used <code>tf.type_spec_from_value</code> to get this:</p>
<pre><code>dataset = tf.data.Dataset.from_generator(
   datagen_row,
   output_signature=(
      tf.TensorSpec(shape=(None, 512), dtype=tf.float32, name=None),
      tf.TensorSpec(shape=(), dtype=tf.int64, name=None)
   )
)
</code></pre>
<p>But is it correct to use None when the number of rows is varying for the first data type?</p>
",2022-10-09 13:04:41,"<p>if your datagen_row() function yields input_data, label with format 500 and 1
than your output_signature should be:</p>
<pre><code>  output_signature=(
  tf.TensorSpec(shape=(None, 500), dtype=tf.float32, name=None),
  tf.TensorSpec(shape=(), dtype=tf.int64, name=None))
</code></pre>
<p>where the first TensorSpec is for the data format and the second one for the label format.
But it would be helpful if you post the function + maybe data examples or data shape here. Otherwise it is hard to help.</p>
",2300622,1104,https://stackoverflow.com/questions/74005009,Documentation Replicability
60684241,How to shape TFRecordDataset to meet Model API?,"<p>I am building a model based on <a href=""https://github.com/xiph/rnnoise/blob/master/training/rnn_train.py"" rel=""nofollow noreferrer"">this code for noise suppression</a>. My problem with the vanilla implementation is that it loads all data at once, which is not the best idea when the training data gets really large; my input file, denoted in the linked code as <code>training.h5</code>, is over 30 GB.</p>

<p>I decided to instead go with <code>tf.data</code> interface that should allow me to work with large data sets; my problem here is that I don't know how to properly shape <code>TFRecordDataset</code> so that it meets what's required by the Model API.</p>

<p>If you check <code>model.fit(x_train, [y_train, vad_train]</code>, it essentially requires the following:</p>

<ul>
<li>x_train, shape <code>[nb_sequences, window, 42]</code></li>
<li>y_train, shape <code>[nb_sequences, window, 22]</code></li>
<li>vad_train, shape <code>[nb_sequences, window, 1]</code></li>
</ul>

<p><code>window</code> one typically fixes (in the code: <code>2000</code>), so the only variable <code>nb_sequences</code> that stems from how large is your data set. However, with <code>tf.data</code>, we don't supply <code>x</code> and <code>y</code>, but only <code>x</code> (see <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">Model API docs</a>).</p>

<h1>Saving tfrecord to file</h1>

<p>In an effort to make the code reproducible, I created the input file with the following code:</p>

<pre><code>writer = tf.io.TFRecordWriter(path='example.tfrecord')
for record in data:
    feature = {}
    feature['X'] = tf.train.Feature(float_list=tf.train.FloatList(value=record[:42]))
    feature['y'] = tf.train.Feature(float_list=tf.train.FloatList(value=record[42:64]))
    feature['vad'] = tf.train.Feature(float_list=tf.train.FloatList(value=[record[64]]))
    example = tf.train.Example(features=tf.train.Features(feature=feature))
    serialized = example.SerializeToString()
    writer.write(serialized)
writer.close()
</code></pre>

<p><code>data</code> is our training data with shape <code>[10000, 65]</code>. My <code>example.tfrecord</code> is available <a href=""https://drive.google.com/open?id=1hG5N2MdN9ziMkfnCz42kr3D1CRM9hn5o"" rel=""nofollow noreferrer"">here</a>. It's 3 MB, in reality it would be 30 GB+.</p>

<p>You might notice that in the linked code, numpy array has shape <code>[x, 87]</code>, while mine is <code>[x, 65]</code>. That's OK - the remainder is not used anywhere.</p>

<h1>Loading the dataset with tf.data.TFRecordDataset</h1>

<p>I would like to use <code>tf.data</code> to load ""on demand"" the data with some prefetching, there's no need to keep it all in memory. My attempt:</p>

<pre><code>import datetime
import numpy as np
import h5py
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import GRU
from tensorflow.keras import regularizers
from tensorflow.keras.constraints import Constraint
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras import backend as K
from tensorflow.keras.layers import concatenate

def load_dataset(path):
    def _parse_function(example_proto):
        keys_to_features = {
            'X': tf.io.FixedLenFeature([42], tf.float32),
            'y': tf.io.FixedLenFeature([22], tf.float32),
            'vad': tf.io.FixedLenFeature([1], tf.float32)
        }
        features = tf.io.parse_single_example(example_proto, keys_to_features)
        return (features['X'], (features['y'], features['vad']))

    dataset = tf.data.TFRecordDataset(path).map(_parse_function)
    return dataset


def my_crossentropy(y_true, y_pred):
    return K.mean(2 * K.abs(y_true - 0.5) * K.binary_crossentropy(y_pred, y_true), axis=-1)


def mymask(y_true):
    return K.minimum(y_true + 1., 1.)


def msse(y_true, y_pred):
    return K.mean(mymask(y_true) * K.square(K.sqrt(y_pred) - K.sqrt(y_true)), axis=-1)


def mycost(y_true, y_pred):
    return K.mean(mymask(y_true) * (10 * K.square(K.square(K.sqrt(y_pred) - K.sqrt(y_true))) + K.square(
        K.sqrt(y_pred) - K.sqrt(y_true)) + 0.01 * K.binary_crossentropy(y_pred, y_true)), axis=-1)


def my_accuracy(y_true, y_pred):
    return K.mean(2 * K.abs(y_true - 0.5) * K.equal(y_true, K.round(y_pred)), axis=-1)


class WeightClip(Constraint):
    '''Clips the weights incident to each hidden unit to be inside a range
    '''

    def __init__(self, c=2.0):
        self.c = c

    def __call__(self, p):
        return K.clip(p, -self.c, self.c)

    def get_config(self):
        return {'name': self.__class__.__name__,
                'c': self.c}

def build_model():
    reg = 0.000001
    constraint = WeightClip(0.499)
    main_input = Input(shape=(None, 42), name='main_input')
    tmp = Dense(24, activation='tanh', name='input_dense', kernel_constraint=constraint, bias_constraint=constraint)(
        main_input)
    vad_gru = GRU(24, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, name='vad_gru',
                  kernel_regularizer=regularizers.l2(reg), recurrent_regularizer=regularizers.l2(reg),
                  kernel_constraint=constraint, recurrent_constraint=constraint, bias_constraint=constraint)(tmp)
    vad_output = Dense(1, activation='sigmoid', name='vad_output', kernel_constraint=constraint,
                       bias_constraint=constraint)(vad_gru)
    noise_input = concatenate([tmp, vad_gru, main_input])
    noise_gru = GRU(48, activation='relu', recurrent_activation='sigmoid', return_sequences=True, name='noise_gru',
                    kernel_regularizer=regularizers.l2(reg), recurrent_regularizer=regularizers.l2(reg),
                    kernel_constraint=constraint, recurrent_constraint=constraint, bias_constraint=constraint)(noise_input)
    denoise_input = concatenate([vad_gru, noise_gru, main_input])

    denoise_gru = GRU(96, activation='tanh', recurrent_activation='sigmoid', return_sequences=True, name='denoise_gru',
                      kernel_regularizer=regularizers.l2(reg), recurrent_regularizer=regularizers.l2(reg),
                      kernel_constraint=constraint, recurrent_constraint=constraint, bias_constraint=constraint)(
        denoise_input)

    denoise_output = Dense(22, activation='sigmoid', name='denoise_output', kernel_constraint=constraint,
                           bias_constraint=constraint)(denoise_gru)

    model = Model(inputs=main_input, outputs=[denoise_output, vad_output])

    model.compile(loss=[mycost, my_crossentropy],
                  metrics=[msse],
                  optimizer='adam', loss_weights=[10, 0.5])
    return model

model = build_model()
dataset = load_dataset('example.tfrecord')
</code></pre>

<p>My dataset has now the following shape:</p>

<pre><code>&lt;MapDataset shapes: ((42,), ((22,), (1,))), types: (tf.float32, (tf.float32, tf.float32))&gt;
</code></pre>

<p>which I thought is what Model API expects (spoiler: it doesn't). </p>

<pre><code>model.fit(dataset.batch(10))
</code></pre>

<p>gives following error:</p>

<pre><code>ValueError: Error when checking input: expected main_input to have 3 dimensions, but got array with shape (None, 42)
</code></pre>

<p>Makes sense, I don't have the <code>window</code> here. At the same time it seems like it's not getting correct shape expected by <code>Model(inputs=main_input, outputs=[denoise_output, vad_output])</code>.</p>

<p>How to modify <code>load_dataset</code> so that it matches what's expected by the Model API for the <code>tf.data</code>?</p>
",2020-03-14 15:17:33,"<p>Given that your model has 1 input and 2 outputs, your <code>tf.data.Dataset</code> should have two entries:<br>
1) Input array of shape <code>(window, 42)</code><br>
2) Tuple of two arrays each of shape <code>(window, 22)</code> and <code>(window, 1)</code>  </p>

<h3>EDIT: Updated answer - you already return two element tuple</h3>

<p>I just noticed that your dataset has these two entries (similar to those described above) and the only thing that differs is the shape.<br>
The only operations you need to perfom is to batch your data twice:<br>
First - to restore the window parameter.
Second - to pass a batch to a model.</p>

<pre class=""lang-py prettyprint-override""><code>window_size = 1
batch_size = 10
dataset = load_dataset('example.tfrecord')
model.fit(dataset.batch(window_size).batch(batch_size)
</code></pre>

<p>And that should work.  </p>

<p>Below is an old answer, where I wrongfully assumed your dataset shape:</p>

<h3>Old Answer, where I assumed you are returning three element tuple:</h3>

<p>Assuming that you are starting from three element tuple of shapes <code>(42,)</code>, <code>(22,)</code> and <code>(1,)</code>, this can be achieved in the same batching operations, enriched with a <code>custom_reshape</code> function to return two-element tuple:</p>

<pre class=""lang-py prettyprint-override""><code>window_size = 1
batch_size = 10
dataset = load_dataset('example.tfrecord')
dataset = dataset.batch(window_size).batch(batch_size)

# Change output format
def custom_reshape(x, y, vad):
    return x, (y, vad)

dataset = dataset.map(custom_reshape)
</code></pre>

<p>In short, given this dataset shape, you could just call:
<code>model.fit(dataset.batch(window_size).batch(10).map(custom_reshape)</code><br>
and it should work too.</p>

<p>Best of luck. And sorry again for the fuss.</p>
",1397946,11007,https://stackoverflow.com/questions/60684241,Documentation Replication on Other Examples
55916743,How to get gradients with respect to input and change input (rather than trainable vars) to minimize loss in TF 2?,"<p>I want to use a trained model to change the input so it minimizes the loss (rather than changing the trainable variables) a la Deep Dreaming in Tensorflow 2.0 but I am not having success.</p>

<p>Say I have a basic NN as the one in the docs</p>

<pre><code>class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
    self.d2 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)

model = MyModel()

</code></pre>

<p>Which I train using a simple tf.GradientTape function</p>

<pre><code>@tf.function
def train_step(image, label):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
</code></pre>

<p>What's the idiomatic way to create a function that will instead calculate and apply the gradients to the input - images.</p>

<p>I assumed it will be as simple as</p>

<pre><code>def train_step(image, label):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  gradients = tape.gradient(loss, image)
  optimizer.apply_gradients(zip(gradients, image))
</code></pre>

<p>However, that doesn't work.</p>
",2019-04-30 08:25:08,"<p><code>tf.GradientTape.gradients</code> can only differentiate wrt to a watched tensor. Variables are automatically watched on first access. In order to differentiate wrt an arbitrary tensor, you have to explicitly <code>watch</code> it:</p>

<pre><code>&gt;&gt;&gt; x = tf.constant([4.0])
&gt;&gt;&gt; y = tf.constant([2.0])
&gt;&gt;&gt; with tf.GradientTape() as tape:
...     tape.watch([x, y])
...     z = x * y
...     
&gt;&gt;&gt; tape.gradient(z, [x, y])
[&lt;tf.Tensor: id=9, shape=(1,), dtype=float32, numpy=array([ 2.], dtype=float32)&gt;, 
 &lt;tf.Tensor: id=10, shape=(1,), dtype=float32, numpy=array([ 4.], dtype=float32)&gt;]
</code></pre>
",1540616,41,https://stackoverflow.com/questions/55916743,Documentation Replication on Other Examples
66858658,"In Keras with Tensorflow, how can I reindex an axis of nd Tensor?","<p>Like this, except without the errors:</p>
<pre><code>input = tf.convert_to_tensor(np.random.rand(500,100,5))
new_order = [0,4,1,3,2]
output = input[:,:,new_order]
</code></pre>
<p>Closest I found is <code>tf.gather</code>, but I haven't been able to make it work.</p>
",2021-03-29 17:15:24,"<p>I find it easy to transpose the tensor so that the indexing dimension is the first dimension, gather the element and then put it back to the original shape.</p>
<pre><code>output = tf.transpose(
    tf.gather(
        tf.transpose(input, [2,0,1]), 
        new_order
    ), [1,2,0]
)
</code></pre>
",2364295,2168,https://stackoverflow.com/questions/66858658,Documentation Replicability
60590333,Increasing each element of a tensor by the predecessor in Tensorflow 2.0,"<p>I'm new to <em>tensorflow 2.0</em>, and haven't done much except designing and training some artificial neural networks from boilerplate code. I'm trying to solve an <em>exercise for newcomers</em> into the new tensorflow. I created some code, but it doesn't work. Below is the <strong><em>problem definition</em></strong>:</p>

<hr>

<p>Assuming we have tensor <code>M</code> of rational numbers in shape of <code>(a, b, c)</code> and scalar <code>p  (0, 1)</code> (memory factor), lets create a function that will return tensor <code>N</code> in shape of <code>(a, b, c)</code>. Each element of <code>N</code> tensors moving along axis <em>c</em> should be increased by the value of predecessor multiplied by <code>p</code>.</p>

<p>Assuming we have tensor:</p>

<pre><code>T = [x1, x2, x3, x4]
</code></pre>

<p>in shape of <code>(1, 1, 4)</code>, we would like to get vector:</p>

<pre><code>[x1, x2+x1p, x3+(x2+x1p)p, x4+(x3+(x2+x1p)p)*p] 
</code></pre>

<p>Solution should be created in <em>Tensorflow 2.0</em> and should be focused on delivering the shortest execution time on CPU. Created graph should allow to efficiently calculate derivative both on tensor <code>M</code> and value <code>p</code>.</p>

<hr>

<p>This is the <strong>code I created till now</strong>:</p>

<pre><code>import tensorflow as tf

@tf.function
def vectorize_predec(t, p):
    last_elem = 0
    result = []
    for el in t:
        result.append(el + (p * last_elem))
        last_elem = el + (p * last_elem)
    return result

p = tf.Variable(0.5, dtype='double')

m = tf.constant([[0, 1, 2, 3, 4],
          [1, 3, 5, 7, 10],
          [1, 1, 1, -1, 0]])

vectorize_predec(m, p)
</code></pre>

<p>But it throws a <code>TypeError</code>.</p>

<p>I looked around documentation, I've seen functions like <code>cumsum</code> and <code>polyeval</code>, but I'm not sure they fit my needs. To my understanding, I need to write my own customer function annotated with <code>@tf.function</code>. I'm also not sure how to handle 3-dimension tensors properly according to the problem definition (adding the predecessor should happen on the last (<em>""c""</em>) axis). </p>

<p>I've seen in documentation (here: <a href=""https://www.tensorflow.org/tutorials/customization/performance"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/customization/performance</a>) that there are ways to measure size of the produced graph. Although, I'm not sure how ""graph"" allows to efficiently calculate <em>derivative</em> both on tensor <code>M</code> and value <code>p</code>. ELI5 answers appreciated, or at least some materials I can read to educate myself better.</p>

<p>Thanks a lot! </p>
",2020-03-08 17:46:06,"<p>I'll give you a couple of different methods to implement that. I think the most obvious solution is to use <a href=""https://www.tensorflow.org/api_docs/python/tf/scan"" rel=""noreferrer""><code>tf.scan</code></a>:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

def apply_momentum_scan(m, p, axis=0):
    # Put axis first
    axis = tf.convert_to_tensor(axis, dtype=tf.int32)
    perm = tf.concat([[axis], tf.range(axis), tf.range(axis + 1, tf.rank(m))], axis=0)
    m_t = tf.transpose(m, perm)
    # Do computation
    res_t = tf.scan(lambda a, x: a * p + x, m_t)
    # Undo transpose
    perm_t = tf.concat([tf.range(1, axis + 1), [0], tf.range(axis + 1, tf.rank(m))], axis=0)
    return tf.transpose(res_t, perm_t)
</code></pre>

<p>However, you can also implement this as a particular matrix product, if you build a matrix of exponential factors:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

def apply_momentum_matmul(m, p, axis=0):
    # Put axis first and reshape
    m = tf.convert_to_tensor(m)
    p = tf.convert_to_tensor(p)
    axis = tf.convert_to_tensor(axis, dtype=tf.int32)
    perm = tf.concat([[axis], tf.range(axis), tf.range(axis + 1, tf.rank(m))], axis=0)
    m_t = tf.transpose(m, perm)
    shape_t = tf.shape(m_t)
    m_tr = tf.reshape(m_t, [shape_t[0], -1])
    # Build factors matrix
    r = tf.range(tf.shape(m_tr)[0])
    p_tr = tf.linalg.band_part(p ** tf.dtypes.cast(tf.expand_dims(r, 1) - r, p.dtype), -1, 0)
    # Do computation
    res_tr = p_tr @ m_tr
    # Reshape back and undo transpose
    res_t = tf.reshape(res_tr, shape_t)
    perm_t = tf.concat([tf.range(1, axis + 1), [0], tf.range(axis + 1, tf.rank(m))], axis=0)
    return tf.transpose(res_t, perm_t)
</code></pre>

<p>This can also be rewritten to avoid the first transposing (which in TensorFlow is expensive) with <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""noreferrer""><code>tf.tensordot</code></a>:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

def apply_momentum_tensordot(m, p, axis=0):
    # Put axis first and reshape
    m = tf.convert_to_tensor(m)
    # Build factors matrix
    r = tf.range(tf.shape(m)[axis])
    p_mat = tf.linalg.band_part(p ** tf.dtypes.cast(tf.expand_dims(r, 1) - r, p.dtype), -1, 0)
    # Do computation
    res_t = tf.linalg.tensordot(m, p_mat, axes=[[axis], [1]])
    # Transpose
    last_dim = tf.rank(res_t) - 1
    perm_t = tf.concat([tf.range(axis), [last_dim], tf.range(axis, last_dim)], axis=0)
    return tf.transpose(res_t, perm_t)
</code></pre>

<p>The three functions would be used in a similar way:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

p = tf.Variable(0.5, dtype=tf.float32)
m = tf.constant([[0, 1, 2, 3, 4],
                 [1, 3, 5, 7, 10],
                 [1, 1, 1, -1, 0]], tf.float32)
# apply_momentum is one of the functions above
print(apply_momentum(m, p, axis=0).numpy())
# [[ 0.    1.    2.    3.    4.  ]
#  [ 1.    3.5   6.    8.5  12.  ]
#  [ 1.5   2.75  4.    3.25  6.  ]]
print(apply_momentum(m, p, axis=1).numpy())
# [[ 0.      1.      2.5     4.25    6.125 ]
#  [ 1.      3.5     6.75   10.375  15.1875]
#  [ 1.      1.5     1.75   -0.125  -0.0625]]
</code></pre>

<p>Using a matrix product is more asymptotically complex, but it can be faster than scanning. Here is a small benchmark:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

# Make test data
tf.random.set_seed(0)
p = tf.constant(0.5, dtype=tf.float32)
m = tf.random.uniform([100, 30, 50], dtype=tf.float32)

# Axis 0
print(np.allclose(apply_momentum_scan(m, p, 0).numpy(), apply_momentum_matmul(m, p, 0).numpy()))
# True
print(np.allclose(apply_momentum_scan(m, p, 0).numpy(), apply_momentum_tensordot(m, p, 0).numpy()))
# True
%timeit apply_momentum_scan(m, p, 0)
# 11.5 ms  610 s per loop (mean  std. dev. of 7 runs, 100 loops each)
%timeit apply_momentum_matmul(m, p, 0)
# 1.36 ms  18.3 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
%timeit apply_momentum_tensordot(m, p, 0)
# 1.62 ms  7.39 s per loop (mean  std. dev. of 7 runs, 1000 loops each)

# Axis 1
print(np.allclose(apply_momentum_scan(m, p, 1).numpy(), apply_momentum_matmul(m, p, 1).numpy()))
# True
print(np.allclose(apply_momentum_scan(m, p, 1).numpy(), apply_momentum_tensordot(m, p, 1).numpy()))
# True
%timeit apply_momentum_scan(m, p, 1)
# 4.27 ms  60.4 s per loop (mean  std. dev. of 7 runs, 100 loops each)
%timeit apply_momentum_matmul(m, p, 1)
# 1.27 ms  36.4 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
%timeit apply_momentum_tensordot(m, p, 1)
# 1.2 ms  11.6 s per loop (mean  std. dev. of 7 runs, 1000 loops each)

# Axis 2
print(np.allclose(apply_momentum_scan(m, p, 2).numpy(), apply_momentum_matmul(m, p, 2).numpy()))
# True
print(np.allclose(apply_momentum_scan(m, p, 2).numpy(), apply_momentum_tensordot(m, p, 2).numpy()))
# True
%timeit apply_momentum_scan(m, p, 2)
# 6.29 ms  64.6 s per loop (mean  std. dev. of 7 runs, 100 loops each)
%timeit apply_momentum_matmul(m, p, 2)
# 1.41 ms  21.8 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
%timeit apply_momentum_tensordot(m, p, 2)
# 1.05 ms  26 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
</code></pre>

<p>So, matrix product seems to win. Let's see if this scales:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

# Make test data
tf.random.set_seed(0)
p = tf.constant(0.5, dtype=tf.float32)
m = tf.random.uniform([1000, 300, 500], dtype=tf.float32)

# Axis 0
print(np.allclose(apply_momentum_scan(m, p, 0).numpy(), apply_momentum_matmul(m, p, 0).numpy()))
# True
print(np.allclose(apply_momentum_scan(m, p, 0).numpy(), apply_momentum_tensordot(m, p, 0).numpy()))
# True
%timeit apply_momentum_scan(m, p, 0)
# 784 ms  6.78 ms per loop (mean  std. dev. of 7 runs, 1 loop each)
%timeit apply_momentum_matmul(m, p, 0)
# 1.13 s  76.9 ms per loop (mean  std. dev. of 7 runs, 1 loop each)
%timeit apply_momentum_tensordot(m, p, 0)
# 1.3 s  27 ms per loop (mean  std. dev. of 7 runs, 1 loop each)

# Axis 1
print(np.allclose(apply_momentum_scan(m, p, 1).numpy(), apply_momentum_matmul(m, p, 1).numpy()))
# True
print(np.allclose(apply_momentum_scan(m, p, 1).numpy(), apply_momentum_tensordot(m, p, 1).numpy()))
# True
%timeit apply_momentum_scan(m, p, 1)
# 852 ms  12.7 ms per loop (mean  std. dev. of 7 runs, 1 loop each)
%timeit apply_momentum_matmul(m, p, 1)
# 659 ms  10.7 ms per loop (mean  std. dev. of 7 runs, 1 loop each)
%timeit apply_momentum_tensordot(m, p, 1)
# 741 ms  19.5 ms per loop (mean  std. dev. of 7 runs, 1 loop each)

# Axis 2
print(np.allclose(apply_momentum_scan(m, p, 2).numpy(), apply_momentum_matmul(m, p, 2).numpy()))
# True
print(np.allclose(apply_momentum_scan(m, p, 2).numpy(), apply_momentum_tensordot(m, p, 2).numpy()))
# True
%timeit apply_momentum_scan(m, p, 2)
# 1.06 s  16.2 ms per loop (mean  std. dev. of 7 runs, 1 loop each)
%timeit apply_momentum_matmul(m, p, 2)
# 924 ms  17 ms per loop (mean  std. dev. of 7 runs, 1 loop each)
%timeit apply_momentum_tensordot(m, p, 2)
# 483 ms  10.1 ms per loop (mean  std. dev. of 7 runs, 1 loop each)
</code></pre>

<p>Well, now it's not so clear anymore. Scanning is still not super fast, but matrix products are sometimes slower. As you can imagine if you go to even bigger tensors the complexity of matrix products will dominate the timings.</p>

<p>So, if you want the fastest solution and know your tensors are not going to get huge, use one of the matrix product implementations. If you're fine with okay speed but want to make sure you don't run out of memory (matrix solution also takes much more) and timing is predictable, you can use the scanning solution.</p>

<p>Note: Benchmarks above were carried out on CPU, results may vary significantly on GPU.</p>
",1554153,855,https://stackoverflow.com/questions/60590333,Lack of Alternative Solutions/Documentation
62668694,"How to create an _Arg, _Retval, _If operation in tensorflow?","<p>I'm trying to test all the operations available in tensorflow. For example, we can find 'Conv2d' in tf.nn module.
There are some operations started with an '_', e.g, '_Arg', '_ArrayToList', '_Retval'. I looked into the tensorflow source code, but still can't find how to create an operation '_Arg'. Please give me some instructions of how to find these operations, or what does these operations do?</p>
",2020-07-01 02:21:17,"<p>Those operations are for an internal purpose, they are implemented in c++ so you'll need to download the source code, code (in c++) your own tests, compile and run them, since most of those operations do not have a Python wrapper.</p>
<p>Here you can find the <a href=""https://www.tensorflow.org/api_docs/cc"" rel=""nofollow noreferrer"">c++ api</a>.</p>
<p>This <a href=""https://www.tensorflow.org/guide/create_op"" rel=""nofollow noreferrer"">tutorial</a> may help you if you are starting with tf operation. It does not do what you want, as it works with custom public operations.</p>
<p>You may have a look to the tests already implemented in tf code, fore example <a href=""https://github.com/tensorflow/tensorflow/blob/abbeddb86f458c79910e1546fbb76694140c9362/tensorflow/core/common_runtime/function_test.cc"" rel=""nofollow noreferrer"">this</a> test file.</p>
<p>However, I will strongly recommend that you reconsider if you really need to test those functions. Testing every single function from TensorFlow, even the internal ones, is going to be a hard job.</p>
",2525479,571,https://stackoverflow.com/questions/62668694,Requesting (Additional) Documentation/Examples
70686756,Tensorflow tf.data.Dataset.cache seems do not take the expected effect,"<p>I am trying to improve my model training performance following the <a href=""https://www.tensorflow.org/guide/data_performance"" rel=""nofollow noreferrer"">Better performance with the tf.data API</a> guideline. However, I have observed that the performance using <code>.cache()</code> is almost the same or even worse if compared to same settings without <code>.cache()</code>.</p>
<pre><code>datafile_list = load_my_files()
RAW_BYTES = 403*4
BATCH_SIZE = 32

raw_dataset = tf.data.FixedLengthRecordDataset(filenames=datafile_list, record_bytes=RAW_BYTES, num_parallel_reads=10, buffer_size=1024*RAW_BYTES)
raw_dataset = raw_dataset.map(tf.autograph.experimental.do_not_convert(decode_and_prepare),
    num_parallel_calls=tf.data.AUTOTUNE)
raw_dataset = raw_dataset.cache()
raw_dataset = raw_dataset.shuffle(buffer_size=1024)
raw_dataset = raw_dataset.batch(BATCH_SIZE)
raw_dataset = raw_dataset.prefetch(tf.data.AUTOTUNE)
</code></pre>
<p>The data in <code>datafile_list</code> hold 9.92GB which fairly fits the system total physical RAM available (100GB). System swap is disabled.</p>
<p>By training the model using the dataset:</p>
<pre><code>model = build_model()
model.fit(raw_dataset, epochs=5, verbose=2)
</code></pre>
<p>results in:</p>
<pre><code>Epoch 1/5
206247/206247 - 126s - loss: 0.0043 - mae: 0.0494 - mse: 0.0043
Epoch 2/5
206247/206247 - 125s - loss: 0.0029 - mae: 0.0415 - mse: 0.0029
Epoch 3/5
206247/206247 - 129s - loss: 0.0027 - mae: 0.0397 - mse: 0.0027
Epoch 4/5
206247/206247 - 125s - loss: 0.0025 - mae: 0.0386 - mse: 0.0025
Epoch 5/5
206247/206247 - 125s - loss: 0.0024 - mae: 0.0379 - mse: 0.0024
</code></pre>
<p>This result is frustrating. By the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p>The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.</p>
</blockquote>
<p>And from <a href=""https://www.tensorflow.org/datasets/performances#caching_the_dataset"" rel=""nofollow noreferrer"">this guide</a>:</p>
<blockquote>
<p>When iterating over this dataset, the second iteration will be much faster than the first one thanks to the caching.</p>
</blockquote>
<p>However, the elapsed time took by all epochs are almost the same. In addition, during the training both CPU and GPU usage are very low (see images below).</p>
<p>By commenting out the line <code>raw_dataset = raw_dataset.cache()</code> the results do not show any notable difference:</p>
<pre><code>Epoch 1/5
206067/206067 - 129s - loss: 0.0042 - mae: 0.0492 - mse: 0.0042
Epoch 2/5
206067/206067 - 127s - loss: 0.0028 - mae: 0.0412 - mse: 0.0028
Epoch 3/5
206067/206067 - 134s - loss: 0.0026 - mae: 0.0393 - mse: 0.0026
Epoch 4/5
206067/206067 - 127s - loss: 0.0024 - mae: 0.0383 - mse: 0.0024
Epoch 5/5
206067/206067 - 126s - loss: 0.0023 - mae: 0.0376 - mse: 0.0023
</code></pre>
<p>As pointed out in the docs, my expectations were using cache would result in a much fast training time. I would like to know what I am doing wrong.</p>
<p><strong>Attachments</strong></p>
<p>GPU usage during training using cache:</p>
<p><a href=""https://i.stack.imgur.com/ZrWvj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZrWvj.png"" alt=""enter image description here"" /></a></p>
<p>GPU usage during training WITHOUT cache:</p>
<p><a href=""https://i.stack.imgur.com/u1Ir1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u1Ir1.png"" alt=""enter image description here"" /></a></p>
<p>System Stats (Memory, CPU etc) during training using cache:</p>
<p><a href=""https://i.stack.imgur.com/M6Myf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M6Myf.png"" alt=""enter image description here"" /></a></p>
<p>System Stats (Memory, CPU etc) during training WITHOUT cache:</p>
<p><a href=""https://i.stack.imgur.com/HrQTd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HrQTd.png"" alt=""enter image description here"" /></a></p>
",2022-01-12 18:38:35,"<p>Just a small observation using Google Colab. According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#cache"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p>Note: For the cache to be finalized, the input dataset must be iterated through in its entirety. Otherwise, subsequent iterations will not use cached data.</p>
</blockquote>
<p>And</p>
<blockquote>
<p>Note: cache will produce exactly the same elements during each
iteration through the dataset. If you wish to randomize the iteration
order, make sure to call shuffle after calling cache.</p>
</blockquote>
<p>I did notice a few differences when using caching and iterating over the dataset beforehand. Here is an example.</p>
<p><strong>Prepare data</strong>:</p>
<pre><code>import random
import struct
import tensorflow as tf
import numpy as np

RAW_N = 2 + 20*20 + 1

bytess = random.sample(range(1, 5000), RAW_N*4)
with open('mydata.bin', 'wb') as f:
  f.write(struct.pack('1612i', *bytess))
def decode_and_prepare(register):
  register = tf.io.decode_raw(register, out_type=tf.float32)
  inputs = register[2:402]
  label = tf.random.uniform(()) + register[402:]
  return inputs, label

raw_dataset = tf.data.FixedLengthRecordDataset(filenames=['/content/mydata.bin']*7000, record_bytes=RAW_N*4)
raw_dataset = raw_dataset.map(decode_and_prepare)
</code></pre>
<p><strong>Train model <em>without</em> caching and iterating beforehand</strong>:</p>
<pre class=""lang-py prettyprint-override""><code>total_data_entries = len(list(raw_dataset.map(lambda x, y: (x, y))))
train_ds = raw_dataset.shuffle(buffer_size=total_data_entries).batch(32).prefetch(tf.data.AUTOTUNE)
inputs = tf.keras.layers.Input((400,))
x = tf.keras.layers.Dense(200, activation='relu', kernel_initializer='normal')(inputs)
x = tf.keras.layers.Dense(100, activation='relu', kernel_initializer='normal')(x)
outputs = tf.keras.layers.Dense(1, kernel_initializer='normal')(x)
model = tf.keras.Model(inputs, outputs)
model.compile(optimizer='adam', loss='mse')
model.fit(train_ds, epochs=5)
</code></pre>
<pre><code>Epoch 1/5
875/875 [==============================] - 4s 3ms/step - loss: 0.1425
Epoch 2/5
875/875 [==============================] - 4s 3ms/step - loss: 0.0841
Epoch 3/5
875/875 [==============================] - 4s 3ms/step - loss: 0.0840
Epoch 4/5
875/875 [==============================] - 4s 3ms/step - loss: 0.0840
Epoch 5/5
875/875 [==============================] - 4s 3ms/step - loss: 0.0840
&lt;keras.callbacks.History at 0x7fc41be037d0&gt;
</code></pre>
<p><strong>Training model <em>with</em> caching but <em>no</em> iterating</strong>:</p>
<pre class=""lang-py prettyprint-override""><code>total_data_entries = len(list(raw_dataset.map(lambda x, y: (x, y))))
train_ds = raw_dataset.shuffle(buffer_size=total_data_entries).cache().batch(32).prefetch(tf.data.AUTOTUNE)
inputs = tf.keras.layers.Input((400,))
x = tf.keras.layers.Dense(200, activation='relu', kernel_initializer='normal')(inputs)
x = tf.keras.layers.Dense(100, activation='relu', kernel_initializer='normal')(x)
outputs = tf.keras.layers.Dense(1, kernel_initializer='normal')(x)
model = tf.keras.Model(inputs, outputs)
model.compile(optimizer='adam', loss='mse')
model.fit(train_ds, epochs=5)
</code></pre>
<pre><code>Epoch 1/5
875/875 [==============================] - 4s 2ms/step - loss: 0.1428
Epoch 2/5
875/875 [==============================] - 2s 2ms/step - loss: 0.0841
Epoch 3/5
875/875 [==============================] - 2s 2ms/step - loss: 0.0840
Epoch 4/5
875/875 [==============================] - 2s 2ms/step - loss: 0.0840
Epoch 5/5
875/875 [==============================] - 2s 3ms/step - loss: 0.0840
&lt;keras.callbacks.History at 0x7fc41fa87810&gt;
</code></pre>
<p><strong>Training model <em>with</em> caching and iterating</strong>:</p>
<pre class=""lang-py prettyprint-override""><code>total_data_entries = len(list(raw_dataset.map(lambda x, y: (x, y))))
train_ds = raw_dataset.shuffle(buffer_size=total_data_entries).cache().batch(32).prefetch(tf.data.AUTOTUNE)
_ = list(train_ds.as_numpy_iterator()) # iterate dataset beforehand
inputs = tf.keras.layers.Input((400,))
x = tf.keras.layers.Dense(200, activation='relu', kernel_initializer='normal')(inputs)
x = tf.keras.layers.Dense(100, activation='relu', kernel_initializer='normal')(x)
outputs = tf.keras.layers.Dense(1, kernel_initializer='normal')(x)
model = tf.keras.Model(inputs, outputs)
model.compile(optimizer='adam', loss='mse')
model.fit(train_ds, epochs=5)
</code></pre>
<pre><code>Epoch 1/5
875/875 [==============================] - 3s 3ms/step - loss: 0.1427
Epoch 2/5
875/875 [==============================] - 2s 2ms/step - loss: 0.0841
Epoch 3/5
875/875 [==============================] - 2s 2ms/step - loss: 0.0840
Epoch 4/5
875/875 [==============================] - 2s 2ms/step - loss: 0.0840
Epoch 5/5
875/875 [==============================] - 2s 2ms/step - loss: 0.0840
&lt;keras.callbacks.History at 0x7fc41ac9c850&gt;
</code></pre>
<p>Conclusion: The caching and the prior iteration of the dataset seem to have an effect on training, but in this example only 7000 files were used.</p>
",3055724,2493,https://stackoverflow.com/questions/70686756,Documentation Replication on Other Examples
54686895,Tensorflow dilation behave differently than morphological dilation,"<p>As the following piece of code shows, the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d"" rel=""noreferrer"">tensorflow <code>tf.nn.dilation2D</code> function</a> doesn't behave as a <a href=""https://homepages.inf.ed.ac.uk/rbf/HIPR2/dilate.htm"" rel=""noreferrer"">conventional dilation operator</a>. </p>

<pre><code>import tensorflow as tf
tf.InteractiveSession()
A = [[0, 0, 0, 0, 0, 0, 0],
     [0, 0, 0, 0, 1, 0, 0],
     [0, 0, 0, 1, 1, 1, 0],
     [0, 0, 0, 0, 1, 0, 0],
     [0, 0, 0, 0, 0, 0, 0],
     [0, 0, 0, 0, 0, 0, 0]]
kernel = tf.ones((3,3,1))
input4D = tf.cast(tf.expand_dims(tf.expand_dims(A, -1), 0), tf.float32)
output4D = tf.nn.dilation2d(input4D, filter=kernel, strides=(1,1,1,1), rates=(1,1,1,1), padding=""SAME"")
print(tf.cast(output4D[0,:,:,0], tf.int32).eval())
</code></pre>

<p>Returns the following tensor:</p>

<pre><code>array([[1, 1, 1, 2, 2, 2, 1],
       [1, 1, 2, 2, 2, 2, 2],
       [1, 1, 2, 2, 2, 2, 2],
       [1, 1, 2, 2, 2, 2, 2],
       [1, 1, 1, 2, 2, 2, 1],
       [1, 1, 1, 1, 1, 1, 1]], dtype=int32)
</code></pre>

<p>I don't understand neither <strong>why</strong> it behaves like that, neither <strong>how</strong> I should use <code>tf.nn.dilation2d</code> to retrieve the expected output:</p>

<pre><code>array([[0, 0, 0, 1, 1, 1, 0],
       [0, 0, 1, 1, 1, 1, 1],
       [0, 0, 1, 1, 1, 1, 1],
       [0, 0, 1, 1, 1, 1, 1],
       [0, 0, 0, 1, 1, 1, 0],
       [0, 0, 0, 0, 0, 0, 0]], dtype=int32)
</code></pre>

<p>Can someone enlighten the succinct documentation of tensorflow and give an explanation of what the the <code>tf.nn.dilation2D</code> function does ?</p>
",2019-02-14 09:19:44,"<p>As mentioned in the documentation page linked,</p>

<blockquote>
  <p>Computes the grayscale dilation of 4-D input and 3-D filter tensors.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>In detail, the grayscale morphological 2-D dilation is the max-sum correlation [...]</p>
</blockquote>

<p>What this means is that the kernel's values are added to the image's values at each position, then the maximum value is taken as the output value.</p>

<p>Compare this to correlation, replacing the multiplication with an addition, and the integral (or sum) with the maximum:</p>

<p>&nbsp; &nbsp; &nbsp; convolution: <em>g</em>(<em>t</em>) =  <em>f</em>() <em>h</em>(-<em>t</em>) d</p>

<p>&nbsp; &nbsp; &nbsp; dilation: <em>g</em>(<em>t</em>) = max<sub></sub> { <em>f</em>() + <em>h</em>(-<em>t</em>) }</p>

<p>Or in the discrete world:</p>

<p>&nbsp; &nbsp; &nbsp; convolution: <em>g</em>[<em>n</em>] = <sub><em>k</em></sub> <em>f</em>[<em>k</em>] <em>h</em>[<em>k</em>-<em>n</em>]</p>

<p>&nbsp; &nbsp; &nbsp; dilation: <em>g</em>[<em>n</em>] = max<sub><em>k</em></sub> { <em>f</em>[<em>k</em>] + <em>h</em>[<em>k</em>-<em>n</em>] }</p>

<hr>

<p>The dilation with a binary structuring element (kernel, what the question refers to as a conventional dilation) uses a structuring element (kernel) that contains only 1s and 0s. These indicate included and excluded. That is, the 1s determine the domain of the structuring element.</p>

<p>To recreate the same behavior with a grey-value dilation, set the included pixels to 0 and the excluded pixels to minus infinity.</p>

<p>For example, the 3x3 square structuring element used in the question should be a 3x3 matrix of zeros.</p>
",1782553,1477,https://stackoverflow.com/questions/54686895,Documentation Ambiguity
56286350,tf.keras.metrics.SpecificityAtSensitivity num_thresholds interpretation,"<p>I'm trying to get my head around <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/metrics/SensitivityAtSpecificity"" rel=""nofollow noreferrer"">tf.keras.metrics.SensitivityAtSpecificity</a>. I'm fine with the concept of sensity and specificity in isolation, but I'm unsure how the two are related in this single metric.</p>

<p>More specifically, I'm unsure how to interpret the <code>num_thresholds</code> argument. The example in documentation has <code>num_thresholds=1</code>. Setting <code>num_thresholds</code> greater than 1 with the same input data seems to always return a metric value of 1.0.</p>

<pre class=""lang-py prettyprint-override""><code>def print_metric_value(num_thresholds):
    # other values based on docs example
    m = tf.keras.metrics.SensitivityAtSpecificity(
        0.4, num_thresholds=num_thresholds)
    m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
    print('Result with num_thresholds = %d: %.1f' %
          (num_thresholds, m.result().numpy()))

print_metric_value(1)    # 0.5 - same as docs
print_metric_value(2)    # 1.0
print_metric_value(200)  # 1.0
</code></pre>
",2019-05-24 05:29:09,"<p>The <code>num_thresholds</code> refers to the number of thresholds. But you might ask: what is a threshold (in this context)? And the answer is that the threshold, which is in the range [0,1], is actually the value which all the predictions greater than that will be considered as positive (i.e. 1) and all the prediction lower than that will be considered as negative (i.e. 0). </p>

<p>For example, consider the prediction vector as <code>[0, 0.5, 0.3, 0.9]</code> which are actually confidences scores (e.g. probabilities). Now if we apply the threshold value of <code>0.1</code>, we get <code>[0, 1, 1, 1]</code>; or if we apply threshold value of <code>0.6</code> we get <code>[0, 0, 0, 1]</code> (i.e. only the confidence of last prediction is higher than <code>0.6</code>).   </p>

<p>Now suppose you want to monitor the changes to specificity at a fixed sensitivity. What <code>SensitivityAtSpecificity</code> metric does is that, to compute the value of sensitivity, it would first compute the specificity at different thresholds and then chooses the threshold which has the closest specificity to the specificity value you have provided (for example, in your question you have given <code>0.4</code> as the specificity value). Then the sensitivity is computed at that threshold and will be returned as the value of this metric. The same thing applies to <code>SpecificityAtSensitivity</code> metric, just swap ""specificity"" and ""sensitivity"" in this paragraph.</p>

<p>You might also ask: what are the threshold values? The answer is if <code>num_thresholds=1</code> then the only threshold is 0.5. If <code>num_thresholds &gt; 1</code> then, besides 0 and 1 as thresholds, the interval (0,1) will be split into <code>num_thresholds - 1</code> equal sub-intervals and the split points are chosen as additional threshold values. For example:</p>

<pre><code>num_threshold  |  thresholds
=============================
1              | [0.5]
2              | [0, 1]
3              | [0, 0.5, 1]
4              | [0, 0.33, 0.66, 1]
5              | [0, 0.25, 0.5, 0.75, 1]
...
</code></pre>
",3098092,4098,https://stackoverflow.com/questions/56286350,Documentation Replication on Other Examples
74702527,Tensorflow - constructing a tensor from particular values extracted from two different tensors,"<p>I'm trying to construct a single tensor using values from two different tensors and an array of two dimensional indices, in a manner compatible with TensorFlow autodiff.</p>
<p>In a first step I want to extract the elements of a tensor <code>D</code> of shape <code>(n,n)</code> whose values are the same as those in another tensor <code>a</code>. In particular, I'm looking for a better way to implement the following loop:</p>
<pre><code>a = []
for i in range(len(f)):
    a.append(tf.where(tf.experimental.numpy.isclose(f[I], D, atol=1e-6))[0])
P_x = tf.gather(D,a)
</code></pre>
<p>In the append step, I'm just using the first instance where the values are equal because the function I'm interested in is independent of this choice. I need to use isclose because the two arrays are float32 arrays and are not exactly equal to one another.</p>
<p>Then in a second step I want to combine <code>P_x</code> with <code>P_y = tf.gather(g, indices)</code> to construct a tensor <code>P</code>. Assume that <code>P_x</code> and <code>P_y</code> are both of shape <code>(n, )</code>. Then,</p>
<pre><code>P = [[P_x[0], P_y[0]],[P_x[1], P_y[1]], ..., [P_x[n], P_y[n]] ]
</code></pre>
<p>I'm pretty new to TensorFlow, so despite looking through the docs I don't see a way to do all of these operations using gather, scatter etc., which seems to be necessary to make autodiff work. When I use loops and other methods, I get gradients = none.</p>
",2022-12-06 12:28:47,"<p>For the first step, you can reduce the loop into matrix operation by finding the closest match using <code>broadcasting</code>.</p>
<pre><code>indices = tf.reduce_sum(tf.math.abs(D[:,None] - a), 2)     
#numpy is_close

tf.gather(D,tf.where(indices &lt; 1e-6)[:,0])
</code></pre>
<p>Example:</p>
<pre><code>D = tf.random.normal(shape=(5,3))
a = tf.concat([tf.random.normal(shape=(2,3)), D[:2],], axis=0)

#Here a last 2 rows of `a` are made same as first two rows of D.
#D is array([[ 0.6221494 ,  0.39071774,  0.5728211 ],
   [ 0.926828  ,  0.8460992 ,  0.08634651],
   [-0.39511812, -0.02012417,  1.0490925 ],
   [-0.31207308,  0.41652176,  0.85152763],
   [-1.27271   , -0.09542792, -0.16090107]]
#a is array([[ 0.9826471 ,  0.25055575, -0.4920534 ],
   [-0.3222343 ,  0.91883016,  1.2904693 ],
   [ 0.6221494 ,  0.39071774,  0.5728211 ],
   [ 0.926828  ,  0.8460992 ,  0.08634651]]
</code></pre>
<p>numpy.is_close() operation</p>
<pre><code>indices = tf.reduce_sum(tf.math.abs(D[:,None] - a), 2)  
#this compares each row of D with each row of a. So we get a (5,4) matrix for the above example.
</code></pre>
<p>Gather D close to a:</p>
<pre><code>tf.gather(D,tf.where(indices &lt; 1e-6)[:,0])
#output
array([[0.6221494 , 0.39071774, 0.5728211 ],
       [0.926828  , 0.8460992 , 0.08634651]],
</code></pre>
",3131493,53,https://stackoverflow.com/questions/74702527,Documentation Replication on Other Examples
57349824,"Recurrent neural network, time series prediction with newer Tensorflow 1.14","<p>How to use new tf.keras API with recurrent neural network? I have checked the documentation but there is no example of such a situation.
There is this great book Hands on machine learning from 2017. Since that year the API of tensorflow has evolved and I am trying to rewrite recurrent neural network for time series prediction with using version <code>1.14</code> code.
The code from the book is using older <code>tf.nn.dynamic_rnn</code> and <code>tf.nn.rnn_cell.BasicRNNCell</code>:</p>

<pre><code>n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1
learning_rate = 0.001

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])
cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)
rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
loss = tf.reduce_mean(tf.square(outputs - y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()
n_iterations = 500
batch_size = 50

with tf.Session() as sess:
    init.run()
        for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})
</code></pre>

<p>And this code works just fine (except that it throws warnings about deprecation left and right). I wanted to use <code>tf.keras</code> API as suggested in warning. My code is the same except:</p>

<pre><code>cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)  
rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
</code></pre>

<p>But this yields following exception:</p>

<pre><code>InvalidArgumentError: Input to reshape is a tensor with 50 values, but the requested shape requires a multiple of 20
 [[node Reshape_1 (defined at &lt;ipython-input-9-879361be49dd&gt;:3) ]]
</code></pre>

<p>so I understand that the problematic line is</p>

<pre><code>outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
</code></pre>

<p>After checking and comparing documentation for both cells <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a> and 
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN</a> I can't find the culprit.</p>

<p><strong>What is the difference with these two cells? How to use tf.keras API with time series?</strong></p>

<p>Full old code: <a href=""https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb"" rel=""nofollow noreferrer"">https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb</a></p>

<p>Full ""my"" code:</p>

<pre><code>import numpy as np
import tensorflow as tf
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd
from utils import shuffle_batch, variable_summaries
import os


dir_path = os.getcwd()

now = datetime.utcnow().strftime(""%Y%m%d%H%M%S"")
root_logdir = ""tf_logs""
logdir = ""{}/run-{}/"".format(root_logdir, now)
print(dir_path)


t_min, t_max = -5, 5
section_start = (t_max + t_min) / 2
resolution = 0.1
n_steps = 20

def time_series(t):
    return np.sin(t)

def next_batch(batch_size, n_steps):
    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)
    Ts = t0 + np.arange(0., n_steps + 1) * resolution
    ys = time_series(Ts)
    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)


t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))

t_instance = np.linspace(start = section_start, stop = section_start + resolution * (n_steps + 1),num = n_steps + 1)

plt.figure(figsize=(11,4))
plt.subplot(121)
plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"")
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)
#plt.axis([-10, 10, -17, 13])
plt.xlabel(""Time"")
plt.ylabel(""Value"")

plt.subplot(122)
plt.title(""A training instance"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""c*"", markersize=10, label=""target"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")


# In[6]:


n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])


# In[7]:


cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)                        


rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
print(rnn_outputs.get_shape())


stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons], name='reshape1')
stacked_outputs = tf.keras.layers.Dense(n_outputs,name=""hidden2"")(stacked_rnn_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs], name='reshape2')


learning_rate = 0.001

loss = tf.reduce_mean(tf.square(outputs - y)) # MSE
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()

n_iterations = 1500
batch_size = 50
save_path =os.path.join(dir_path,""model"",""recurrent_sinus_model"")

with tf.Session() as sess:
    init.run()
    for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    saver.save(sess, save_path)


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})


plt.title(""Testing the model"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""w*"", markersize=10, label=""target"")
plt.plot(t_instance[1:], y_pred[0,:,0], ""r."", markersize=10, label=""prediction"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")

plt.show()


# In[ ]:


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t.reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})



plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"",linewidth=5,c='r')
plt.plot(t[:-1], time_series(t[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)

plt.xlabel(""Time"")
plt.ylabel(""Value"")
</code></pre>
",2019-08-04 20:00:21,"<p>So the answer is:</p>

<pre><code>rnn_outputs, rnn_states  = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"", return_state=True, return_sequences=True)(X)
</code></pre>

<p>instead of </p>

<pre><code>rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
</code></pre>

<p>so the parameter <code>return_sequences=True</code> make the RNN return the time series as well, and well, this is the point.</p>
",2710943,435,https://stackoverflow.com/questions/57349824,Documentation Replication on Other Examples
67361081,Tensorflow 2 - what is 'index depth' in tensor_scatter_nd_update?,"<p>Please explain what is index depth of <a href=""https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"" rel=""nofollow noreferrer"">tf.tensor_scatter_nd_update</a>.</p>
<pre><code>tf.tensor_scatter_nd_update(
    tensor, indices, updates, name=None
)
</code></pre>
<p>Why indices is 2D for 1D tensor?</p>
<blockquote>
<p>indices has at least two axes, the last axis is the depth of the index vectors.
For a higher rank input tensor scalar updates can be inserted by using an index_depth that matches tf.rank(tensor):</p>
</blockquote>
<pre><code>tensor = [0, 0, 0, 0, 0, 0, 0, 0]    # tf.rank(tensor) == 1
indices = [[1], [3], [4], [7]]       # num_updates == 4, index_depth == 1   # &lt;--- what is depth and why 2D for 1D tensor?
updates = [9, 10, 11, 12]            # num_updates == 4
print(tf.tensor_scatter_nd_update(tensor, indices, updates))
</code></pre>
<p><a href=""https://i.stack.imgur.com/eNdogl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eNdogl.png"" alt=""enter image description here"" /></a></p>
<pre><code>tensor = [[1, 1], [1, 1], [1, 1]]    # tf.rank(tensor) == 2
indices = [[0, 1], [2, 0]]           # num_updates == 2, index_depth == 2
updates = [5, 10]                    # num_updates == 2
print(tf.tensor_scatter_nd_update(tensor, indices, updates))
</code></pre>
",2021-05-02 21:01:33,"<p>For <code>indices</code>, the <code>index depth</code> is the size or <strong>length</strong> of the <strong>index vectors</strong>. For example:</p>
<pre><code>indicesA = [[1], [3], [4], [7]] # index vector with 1 element: index_depth = 1
indicesB = [[0, 1], [2, 0]]     # index vector with 2 element: index_depth = 2
</code></pre>
<hr />
<p>The reason for indices is <code>2D</code> is to hold two information, one is the <strong>length of the updates (<code>num_updates</code>)</strong> and the <strong>length of the index vector</strong>. Two things need to be fulfilled:</p>
<ul>
<li>The <code>index depth</code> of <code>indices</code> must equal the <strong>rank</strong> of the <code>input</code> tensor</li>
<li>The length of <code>updates</code> must equal the <strong>length</strong> of the <code>indices</code></li>
</ul>
<p>So, in the example code</p>
<pre><code># tf.rank(tensor) == 1
tensor = [0, 0, 0, 0, 0, 0, 0, 0]    

# num_updates == 4, index_depth == 1 | tf.rank(indices).numpy() == 2 
indices = [[1], [3], [4], [7]]    

# num_updates == 4 | tf.rank(output).numpy() == 1  
updates = [9, 10, 11, 12]        

output = tf.tensor_scatter_nd_update(tensor, indices, updates)
tf.Tensor([ 0  9  0 10 11  0  0 12], shape=(8,), dtype=int32)
</code></pre>
<p>Also</p>
<pre><code># tf.rank(tensor) == 2
tensor = [[1, 1], [1, 1], [1, 1]]    

 # num_updates == 2, index_depth == 2 | tf.rank(indices).numpy() == 2
indices = [[0, 1], [2, 0]]          

# num_updates == 2 | tf.rank(output).numpy() == 2
updates = [5, 10]       
             
output = tf.tensor_scatter_nd_update(tensor, indices, updates)
tf.Tensor(
[[ 1  5]
 [ 1  1]
 [10  1]], shape=(3, 2), dtype=int32)

num_updates, index_depth = tf.convert_to_tensor(indices).shape.as_list()
[num_updates, index_depth]
[2, 2]
</code></pre>
",4281353,20088,https://stackoverflow.com/questions/67361081,Documentation Replicability
67362672,tensorflow 2 - how to directly update elements in tf.Variable X at indices?,"<p>Is there a way to directly update the elements in <code>tf.Variable</code> X at indices without creating a new tensor having the same shape as X?</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"" rel=""nofollow noreferrer"">tf.tensor_scatter_nd_update</a> create a new tensor hence it appears not updateing the original tf.Variable.</p>
<blockquote>
<p>This operation creates a new tensor by applying sparse updates to the input tensor.</p>
</blockquote>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/Variable#assign"" rel=""nofollow noreferrer"">tf.Variable assign</a> apparently needs a new tensor <code>value</code> which has the same shape of X to update the tf.Variable X.</p>
<pre><code>assign(
    value, use_locking=False, name=None, read_value=True
)
</code></pre>
<blockquote>
<p>value A Tensor. The new value for this variable.</p>
</blockquote>
",2021-05-03 01:54:03,"<p>About the <a href=""https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"" rel=""nofollow noreferrer""><code>tf.tensor_scatter_nd_update</code></a>, you're right that it returns a new <a href=""https://www.tensorflow.org/guide/tensor"" rel=""nofollow noreferrer""><strong>tf.tensor</strong></a> (and not <a href=""https://www.tensorflow.org/guide/variable"" rel=""nofollow noreferrer""><strong>tf.Variable</strong></a>). But about the <a href=""https://www.tensorflow.org/api_docs/python/tf/Variable#assign"" rel=""nofollow noreferrer""><code>assign</code></a> which is an attribute of <a href=""https://www.tensorflow.org/guide/variable"" rel=""nofollow noreferrer""><strong>tf.Variable</strong></a>, I think you somewhat misread the document; the <code>value</code> is just the new item that you want to assign in particular indices of your old variable.</p>
<p>AFAIK, in <a href=""/questions/tagged/tensorflow"" class=""post-tag"" title=""show questions tagged &#39;tensorflow&#39;"" rel=""tag"">tensorflow</a> all tensors are immutable like <a href=""/questions/tagged/python"" class=""post-tag"" title=""show questions tagged &#39;python&#39;"" rel=""tag"">python</a> numbers and strings; you can never update the contents of a tensor, only create a new one, <a href=""https://www.tensorflow.org/guide/tensor"" rel=""nofollow noreferrer"">source</a>. And directly updating or manipulating of <code>tf.tensor</code> or <code>tf.Variable</code> such as <a href=""/questions/tagged/numpy"" class=""post-tag"" title=""show questions tagged &#39;numpy&#39;"" rel=""tag"">numpy</a> like item assignment is still not supported. Check the following Github issues to follow up the discussions: <a href=""https://github.com/tensorflow/tensorflow/issues/33131"" rel=""nofollow noreferrer"">#33131</a>, <a href=""https://github.com/tensorflow/tensorflow/issues/14132"" rel=""nofollow noreferrer"">#14132</a>.</p>
<hr />
<p>In <a href=""/questions/tagged/numpy"" class=""post-tag"" title=""show questions tagged &#39;numpy&#39;"" rel=""tag"">numpy</a>, we can do an in-place item assignment that you showed in the comment box.</p>
<pre><code>import numpy as np

a = np.array([1,2,3])  
print(a) # [1 2 3]
a[1] = 0
print(a) # [1 0 3] 
</code></pre>
<p>A similar result can be achieved in <strong>tf.Variable</strong> with <code>assign</code> attribute.</p>
<pre><code>import tensorflow as tf 

b = tf.Variable([1,2,3])
b.numpy() # array([1, 2, 3], dtype=int32)

b[1].assign(0)
b.numpy() # array([1, 0, 3], dtype=int32)
</code></pre>
<p>Later, we can convert it to <strong>tf. tensor</strong> as follows.</p>
<pre><code>b_ten = tf.convert_to_tensor(b)
b_ten.numpy() # array([1, 0, 3], dtype=int32)
</code></pre>
<p>We can do such item assignment in <strong>tf.tensor</strong> too but we need to convert it to <strong>tf.Variable</strong> first, (I know, not very intuitive).</p>
<pre><code>tensor = [[1, 1], [1, 1], [1, 1]]    # tf.rank(tensor) == 2
indices = [[0, 1], [2, 0]]           # num_updates == 2, index_depth == 2
updates = [5, 10]                    # num_updates == 2
x = tf.tensor_scatter_nd_update(tensor, indices, updates)
x
&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 1,  5],
       [ 1,  1],
       [10,  1]], dtype=int32)&gt;
</code></pre>
<pre><code>x = tf.Variable(x)
x
&lt;tf.Variable 'Variable:0' shape=(3, 2) dtype=int32, numpy=
array([[ 1,  5],
       [ 1,  1],
       [10,  1]], dtype=int32)&gt;

x[0].assign([5,1])
x
&lt;tf.Variable 'Variable:0' shape=(3, 2) dtype=int32, numpy=
array([[ 5,  1],
       [ 1,  1],
       [10,  1]], dtype=int32)&gt;
</code></pre>
<pre><code>x = tf.convert_to_tensor(x)
x
&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 5,  1],
       [ 1,  1],
       [10,  1]], dtype=int32)&gt;
</code></pre>
",4281353,20088,https://stackoverflow.com/questions/67362672,Documentation Replication on Other Examples
75482826,What is negative_slope argument of tf.keras.layers.ReLU?,"<p><a href=""https://keras.io/api/layers/activation_layers/relu/"" rel=""nofollow noreferrer"">tf.keras.layers.ReLU</a> has <code>negative_slope</code> argument which is explained as <code>Float &gt;= 0. Negative slope coefficient. Default to 0</code>.</p>
<pre><code>tf.keras.layers.ReLU(
    max_value=None, 
    negative_slope=0.0, 
    threshold=0.0, 
    **kwargs
)
</code></pre>
<p>Is this to make it as Leaky ReLU? If so, is it the same with the <code>alpha</code> argument of the <a href=""https://keras.io/api/layers/activation_layers/leaky_relu/"" rel=""nofollow noreferrer"">tf.keras.layers.LeakyReLU</a>?</p>
<pre><code>tf.keras.layers.LeakyReLU(alpha=0.3, **kwargs)

* alpha: Float &gt;= 0. Negative slope coefficient. Default to 0.3.
</code></pre>
",2023-02-17 10:08:49,"<p><strong>Short answer:</strong></p>
<blockquote>
<p>Is this to make it as Leaky ReLU?</p>
</blockquote>
<p>Yes, the <code>negative_slope</code> parameter of <code>tf.keras.layers.ReLU</code> plays the same role as <code>alpha</code> does in <code>tf.keras.layers.LeakyReLU</code>. For example, <code>tf.keras.layers.ReLU(negative_slope=0.5)</code> and <code>tf.keras.layers.LeakyReLU(alpha=0.5)</code> have the same behavior.</p>
<hr />
<p>Here is a visualization of their behavior:</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

relu=tf.keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)
relu_neg_half=tf.keras.layers.ReLU(max_value=None, negative_slope=0.5, threshold=0.0)
relu_neg_1tenth=tf.keras.layers.ReLU(max_value=None, negative_slope=0.1, threshold=0.0)
relu_neg_1tenth_thresh_1=tf.keras.layers.ReLU(max_value=None, negative_slope=0.1, threshold=1.)
relu_neg_1tenth_thresh_2=tf.keras.layers.ReLU(max_value=None, negative_slope=0.1, threshold=2.)

lrelu_alph_half=tf.keras.layers.LeakyReLU(alpha=0.5)
lrelu_alph_1tenth=tf.keras.layers.LeakyReLU(alpha=0.1)

x=np.linspace(-5,5,101)

fig = plt.figure(figsize=(6,6), dpi=150)
markevery=0.05
plt.plot(x, relu(x), '-o', markevery=markevery, label=&quot;ReLU | negative_slope=0.0&quot;)
plt.plot(x, relu_neg_half(x), '--s', markevery=markevery, label=&quot;ReLU | negative_slope=0.5&quot;)
plt.plot(x, relu_neg_1tenth(x), '--p', markevery=markevery, label=&quot;ReLU | negative_slope=0.1&quot;)
plt.plot(x, relu_neg_1tenth_thresh_2(x), '--d', markevery=markevery, label=&quot;ReLU | negative_slope=0.1 | threshold=2.0&quot;)

plt.plot(x, lrelu_alph_half(x), '--v', markevery=markevery*1.2, label=&quot;LeakyReLU | alpha=0.5&quot;)
plt.plot(x, lrelu_alph_1tenth(x), '--^', markevery=markevery*1.2, label=&quot;LeakyReLU | alpha=0.5&quot;)

plt.legend(frameon=False)
plt.savefig('relu.png', bbox_inches='tight')
</code></pre>
<p>Output:
<a href=""https://i.stack.imgur.com/MpmVB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MpmVB.png"" alt=""enter image description here"" /></a></p>
",4281353,20088,https://stackoverflow.com/questions/75482826,Documentation Replicability
59962253,How do I use hparams with estimators?,"<p>To log hparams <a href=""https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/hparams/api.py#L38"" rel=""noreferrer"">without using Keras</a>, I'm doing the following as suggested in the tf code <a href=""https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/hparams/api.py#L55"" rel=""noreferrer"">here</a>:</p>

<pre class=""lang-py prettyprint-override""><code>with tf.summary.create_file_writer(model_dir).as_default():
    hp_learning_rate = hp.HParam(""learning_rate"", hp.RealInterval(0.00001, 0.1))
    hp_distance_margin = hp.HParam(""distance_margin"", hp.RealInterval(0.1, 1.0))
    hparams_list = [
        hp_learning_rate,
        hp_distance_margin
    ]
    metrics_to_monitor = [
        hp.Metric(""metrics_standalone/auc"", group=""validation""),
        hp.Metric(""loss"", group=""train"", display_name=""training loss""),
    ]
    hp.hparams_config(hparams=hparams_list, metrics=metrics_to_monitor)
    hparams = {
        hp_learning_rate: params.learning_rate,
        hp_distance_margin: params.distance_margin,
    }
    hp.hparams(hparams)
</code></pre>

<p>Note that <code>params</code> is a dictionary object here that I'll pass to the estimator.</p>

<p>Then I train the estimator as usual,</p>

<pre><code>config = tf.estimator.RunConfig(model_dir=params.model_dir)
estimator = tf.estimator.Estimator(model_fn, params=params, config=config)
train_spec = tf.estimator.TrainSpec(...)
eval_spec = tf.estimator.EvalSpec(...)

tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)
</code></pre>

<p>After training, when I launch tensorboard, I do have the hparams logged, but I do not see any metrics logged against them</p>

<p><a href=""https://i.stack.imgur.com/mYAYP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mYAYP.png"" alt=""enter image description here""></a></p>

<p>I further confirmed that they show up in the <code>scalars</code> page with the same tag name for both train and validation i.e. <code>.</code> and <code>./eval</code>, but the hparams page doesn't see those logged tensors.</p>

<p><strong>How do I use hparams with estimators?</strong></p>

<hr>

<p>I'm using</p>

<pre><code>tensorboard              2.1.0
tensorflow               2.1.0
tensorflow-estimator     2.1.0
tensorflow-metadata      0.15.2
</code></pre>

<p>on <code>Python 3.7.5</code></p>

<hr>

<p><em>Attempt 1:</em></p>

<p>After some googling, I saw some older tf code where they passed <code>hparams</code> to <code>params</code> argument of Estimator, so just to make sure if tf2 logs those hparams by itself when given, I checked the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""noreferrer"">Estimator</a> docs and it says:</p>

<blockquote>
  <p>The <code>params</code> argument contains hyperparameters. It is passed to the
  <code>model_fn</code>, if the <code>model_fn</code> has a parameter named ""params"", and to the
  input functions in the same manner. <code>Estimator</code> only passes params
  along, it does not inspect it. The structure of <code>params</code> is therefore
  entirely up to the developer.</p>
</blockquote>

<p>So using hparams as params will not be useful.</p>

<hr>

<p><em>Attempt 2:</em></p>

<p>I doubt that since estimators use <code>tensorflow.python.summary</code>  instead of <code>tf.summary</code> which is the default in v2, tensors logged by v1 was probably not accessible and so, I also tried to use </p>

<pre><code>with tensorflow.python.summary.FileWriter(model_dir).as_default()
</code></pre>

<p>However that failed with <code>RuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead</code>.</p>

<p><em>Update</em>: I ran it with eager execution disabled. Now, even the hparam initial logging did not happen. There was no <code>hparams</code> tab in tensorboard as it failed with error </p>

<pre><code>E0129 13:03:07.656290 21584 hparams_plugin.py:104] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.
</code></pre>

<p>Is there a way to make tensorboard read already logged metric tensors and link them with hparams?</p>
",2020-01-29 07:25:07,"<p>The culprit seems to be </p>

<pre><code># This doesn't seem to compatible with Estimator API
hp.hparams_config(hparams=hparams_list, metrics=metrics_to_monitor)
</code></pre>

<p>Simply calling hparams logs all metrics logged with <code>tf.summary</code>. Then in tensorboard, you can filter only the metrics you need and then compare trials.</p>

<pre><code>with tf.summary.create_file_writer(train_folder).as_default():
    # params is a dict which contains
    # { 'learning_rate': 0.001, 'distance_margin': 0.5,...}
    hp.hparams(hparams=params))

</code></pre>
",3211422,8631,https://stackoverflow.com/questions/59962253,Documentation Replication on Other Examples
59723003,From .tfrecord to tf.data.Dataset to tf.keras.model.fit,"<p>I am attemping to use Tensorflow (v2.0)'s Datasets API to pass large amounts of data to a <code>tf.keras.model</code>.  Here is a simplified version of my dataset:</p>

<pre><code>for rec in my_dataset:
    print(repr(rec))

$ {'feature0': &lt;tf.Tensor: id=528, shape=(), dtype=float32, numpy=0.2963&gt;,
'feature1': &lt;tf.Tensor: id=618, shape=(), dtype=int64, numpy=0&gt;,
'feature2': &lt;tf.Tensor: id=620, shape=(), dtype=string, numpy=b'Inst1'&gt;,
'target': &lt;tf.Tensor: id=621, shape=(), dtype=int64, numpy=2&gt;}
{'feature0': &lt;tf.Tensor: id=528, shape=(), dtype=float32, numpy=0.4633&gt;,
'feature1': &lt;tf.Tensor: id=618, shape=(), dtype=int64, numpy=1&gt;,
'feature2': &lt;tf.Tensor: id=620, shape=(), dtype=string, numpy=b'Inst4'&gt;,
'target': &lt;tf.Tensor: id=621, shape=(), dtype=int64, numpy=0&gt;}
</code></pre>

<p>...and so on.  Each record in the <code>my_dataset</code> object is a dictionary with the features' (and target's) names as the keys and associated tensors as the values.  I created the dataset from several .tfrecord files, so I'm constrained in the sense that each tensor corresponds to a <code>tf.train.Example</code> (wrapper) object.  The dataset precisely matches the format seen in tensorflow documentation (see, for example, the last code example in <a href=""https://www.tensorflow.org/tutorials/load_data/tfrecord#reading_a_tfrecord_file"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/load_data/tfrecord#reading_a_tfrecord_file</a>).</p>

<p>I would like to use this dataset with keras.  The <code>tf.keras.model</code> objects I'm working with all seem, for their <code>fit</code> function, to take as input a tuple representing the feature vector (X) and the target (y).  I think I could figure out how to transform the tensors from my dataset into numpy arrays and pass them into the model that way, or iterate over the dataset using an iterator, but if I understand correctly that seems to defeat the whole purpose of using the Datasets API to begin with (see, for example, <a href=""https://www.tensorflow.org/guide/keras/overview#train_from_tfdata_datasets"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/overview#train_from_tfdata_datasets</a>).</p>

<p>My question:  what is the appropriate way to transform <code>my_dataset</code> into some form that <code>tf.keras.model.fit()</code> will receive?  Or if this is the wrong question, what fundamental concepts am I missing that keep me from asking the right one?  (For example, should the .tfrecord Examples be structured differently?  Or, am I required to use an iterator instead of directly passing <code>my_dataset</code> to the model as I'd prefer?)</p>
",2020-01-13 19:30:04,"<p>Unfortunately I was only able to find a workaround instead of an outright solution.  Because <code>tf.stack</code> will only work on items of the same data type, I need to transform all data into floats during processing of the Examples (including one-hot encoding for all strings), and then use <code>tf.stack</code> on the resulting tensor:</p>

<pre><code>def proces_example(serialized_example):
    feature_description = get_feature_desc()  # dictionary describing features and dtypes
    target_name = get_target_name()  # so we don't include the target in our feature vector
    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)
    tensor_list = []
    for tensor in parsed_example:
        if tensor != target_name:
            parsed_example[tensor] = tf.dtypes.cast(parsed_example[tensor], tf.float32)
            tensor_list.append(parsed_example[tensor])
    X = tf.stack(tensor_list)
    y = parsed_example[target_name]
    return X, y
</code></pre>
",3397173,506,https://stackoverflow.com/questions/59723003,Documentation Replicability
57116074,TensorFlow - return distinct sub-tensors of multidimensional tensor,"<p>In TensorFlow, the <code>tf.unique</code> function can be used to return the distinct elements of a 1-dimensional <code>Tensor</code>. How can I get the distinct sub-<code>Tensor</code>s along the axis 0 of a higher-dimensional <code>Tensor</code>? For example, given the following <code>Tensor</code>, the desired <code>distinct</code> function would return the specified result:</p>

<pre><code>input = tf.constant([
    [0,3],
    [0,1],
    [0,4],
    [0,1],
    [1,5],
    [3,9],
    [3,2],
    [3,6],
    [3,5],
    [3,3]])

distinct(input) == tf.constant([
    [0,3],
    [0,1],
    [0,4],
    [1,5],
    [3,9],
    [3,2],
    [3,6],
    [3,5],
    [3,3]])
</code></pre>

<p>How can the distinct multidimensional elements be generated for <code>Tensor</code>s of any number of dimensions?</p>
",2019-07-19 16:03:51,"<p>One approach would be to look for elements for which a previous sub-<code>Tensor</code> along the axis 0 is equal, then filter those out:</p>

<ol>
<li>Use <code>tf.equal</code> to get the pairwise equality of the individual axis -1 elements of the input crossed with itself along axis 0.</li>
<li>Use <code>tf.math.reduce_all</code> to aggregate the pairwise equalities until you have a 2-D equality matrix for the axis 0 elements of the input.</li>
<li>Generate an upper-triangular matrix of False values</li>
<li>Use that triangular matrix to restrict our equality comparisons to one direction along axis 0.</li>
<li>Use <code>tf.reduce_any</code> to find which axis 0 elements are equal to any later elements; they are the duplicates that will be removed.</li>
<li>Use <code>tf.math.logical_not</code> and <code>tf.boolean_mask</code> to obtain only the non-duplicate elements of axis 0.</li>
</ol>

<p>This process is implement in the following Python code, tested in TensorFlow 2.0 beta:</p>

<pre><code>def distinct(input:tf.Tensor) -&gt; tf.Tensor:
    """"""Returns only the distinct sub-Tensors along the 0th dimension of the provided Tensor""""""
    is_equal = tf.equal(input[:,tf.newaxis], input[tf.newaxis,:])
    while len(is_equal.shape) &gt; 2:
        is_equal = tf.math.reduce_all(is_equal, axis=2)
    all_true = tf.constant(True, shape=is_equal.shape)
    true_upper_tri = tf.linalg.band_part(all_true, 0, -1)
    false_upper_tri = tf.math.logical_not(true_upper_tri)
    is_equal_one_way = tf.math.logical_and(is_equal, false_upper_tri)
    is_duplicate = tf.reduce_any(is_equal_one_way, axis=1)
    is_distinct = tf.math.logical_not(is_duplicate)
    distinct_elements = tf.boolean_mask(input, is_distinct, 0)
    return distinct_elements
</code></pre>
",3582363,121,https://stackoverflow.com/questions/57116074,Documentation Replication on Other Examples
56553579,How to export Estimator's best model?,"<p>I am training a simple CNN based on a Custom Estimator with TF Records.
I am trying to export the best model in terms of validation loss during the <code>train_and_evaluate</code> phase. </p>

<p>According to the documentation of the <code>tf.estimator.BestExporter</code>, I should feed a function that returns a <code>ServingInputReceiver</code> but after doing so, the <code>train_and_evaluate</code> phase crashes with a <code>NotFoundError: model/m01/eval; No such file or directory</code>.</p>

<p>Seems like if the BestExporter does not permit saving the evaluation results as it would do without the exporter. I tried with different <code>ServingInputReceiver</code> but I keep getting the same error.</p>

<p>As defined <a href=""https://www.tensorflow.org/guide/saved_model#using_savedmodel_with_estimators"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>feature_spec = {
        'shape': tf.VarLenFeature(tf.int64),
        'image_raw': tf.FixedLenFeature((), tf.string),
        'label_raw': tf.FixedLenFeature((43), tf.int64)
    }

def serving_input_receiver_fn():
  serialized_tf_example = tf.placeholder(dtype=tf.string,
                                         shape=[120, 120, 3],
                                         name='input_example_tensor')
  receiver_tensors = {'image': serialized_tf_example}
  features = tf.parse_example(serialized_tf_example, feature_spec)
  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)
</code></pre>

<p>and <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/BestExporter#__init__"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>def serving_input_receiver_fn():
    feature_spec = {
            'image': tf.FixedLenFeature((), tf.string)
        }
    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)
</code></pre>

<p>Here are my exporter and training procedure:</p>

<pre><code>exporter = tf.estimator.BestExporter(
    name=""best_exporter"",
    serving_input_receiver_fn=serving_input_receiver_fn,
    exports_to_keep=5)

train_spec = tf.estimator.TrainSpec(
    input_fn=lambda: imgs_input_fn(train_path, True, epochs, batch_size))

eval_spec = tf.estimator.EvalSpec(
    input_fn=lambda: imgs_input_fn(eval_path, perform_shuffle=False, batch_size=1),
    exporters=exporter)

tf.estimator.train_and_evaluate(ben_classifier, train_spec, eval_spec)
</code></pre>

<p><a href=""https://gist.github.com/hichameyessou/f2710391066f6ed5786693892ac93dbe"" rel=""nofollow noreferrer"">This is a gist</a> with the output.
What's the correct way to define a <code>ServingInputReceiver</code> for the <code>BestExporter</code>?</p>
",2019-06-12 01:39:22,"<p>Can you try the code shown below:</p>

<pre><code>def serving_input_receiver_fn():
    """"""
    This is used to define inputs to serve the model.
    :return: ServingInputReciever
    """"""
    reciever_tensors = {
        # The size of input image is flexible.
        'image': tf.placeholder(tf.float32, [None, None, None, 1]),
    }

    # Convert give inputs to adjust to the model.
    features = {
        # Resize given images.
        'image': tf.reshape(reciever_tensors[INPUT_FEATURE], [-1, INPUT_SHAPE])
    }
    return tf.estimator.export.ServingInputReceiver(receiver_tensors=reciever_tensors,
                                                    features=features)
</code></pre>

<p>Then use <code>tf.estimator.BestExporter</code> as shown below:</p>

<pre><code>best_exporter = tf.estimator.BestExporter(
        serving_input_receiver_fn=serving_input_receiver_fn,
        exports_to_keep=1)
    exporters = [best_exporter]
    eval_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={input_name: eval_data},
        y=eval_labels,
        num_epochs=1,
        shuffle=False)
    eval_spec = tf.estimator.EvalSpec(
        input_fn=eval_input_fn,
        throttle_secs=10,
        start_delay_secs=10,
        steps=None,
        exporters=exporters)

    # Train and evaluate the model.
    tf.estimator.train_and_evaluate(classifier, train_spec=train_spec, eval_spec=eval_spec)
</code></pre>

<p>For more info, refer the link:
<a href=""https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_keras_estimator.py"" rel=""nofollow noreferrer"">https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_keras_estimator.py</a></p>
",3674176,2688,https://stackoverflow.com/questions/56553579,Documentation Ambiguity
59074659,Best practice for allocating GPU and CPU resources in TensorFlow,"<p>I'm wondering what is the correct way to set devices for creating/training a model in order to optimize resource usage for speedy training in TensorFlow with the Keras API? I have 1 CPU and 2 GPUs at my disposal. I was initially using a <code>tf.device</code> context to create my model and train on GPUs only, but then I saw in the TensorFlow documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model"" rel=""nofollow noreferrer""><code>tf.keras.utils.multi_gpu_model</code></a>, they suggest explicitly instantiating the model on the CPU:</p>

<pre><code># Instantiate the base model (or ""template"" model).
# We recommend doing this with under a CPU device scope,
# so that the model's weights are hosted on CPU memory.
# Otherwise they may end up hosted on a GPU, which would
# complicate weight sharing.
with tf.device('/cpu:0'):
    model = Xception(weights=None,
                     input_shape=(height, width, 3),
                     classes=num_classes)

# Replicates the model on 8 GPUs.
# This assumes that your machine has 8 available GPUs.
parallel_model = multi_gpu_model(model, gpus=8)
parallel_model.compile(loss='categorical_crossentropy',
                       optimizer='rmsprop')
</code></pre>

<p>I did this, and now when I train I see my CPU usage go way up with all 8 cores at about 70% usage each, and my GPU memory is maxed out. Would things go faster if the model were created on one of the GPUs? Even if I have just 1 GPU, is it still better to create model on CPU and use <code>tf.device</code> context to train the model on the GPU?</p>
",2019-11-27 16:31:37,"<p>Many TensorFlow operations are accelerated using the GPU for computation. Without any annotations, TensorFlow automatically decides whether to use the GPU or CPU for an operationcopying the tensor between CPU and GPU memory, if necessary. Tensors produced by an operation are typically backed by the memory of the device on which the operation executed.</p>

<p>Tensorflow will only allocate memory and place operations on visible physical devices, as otherwise no LogicalDevice will be created on them. By default all discovered devices are marked as visible. </p>

<p>Also GPU utilization depends on the <code>batch_size</code>. The utilization may change with varying <code>batch_size</code>. </p>

<p>You can also compare your current results(time taken and utilization) with model using the <code>Example 3</code> from <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model"" rel=""nofollow noreferrer"">multi_gpu_model</a>.</p>

<p>Also if you go into the link, it states -</p>

<blockquote>
  <p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2020-04-01. Instructions for updating: Use tf.distribute.MirroredStrategy instead.</p>
</blockquote>

<p>There should be performance improvement and GPU Utilization using <code>tf.distribute.MirroredStrategy</code>. This strategy is typically used for training on one machine with multiple GPUs. The <code>tf.distribute.Strategy</code> API provides an abstraction for distributing your training across multiple processing units. The goal is to allow users to enable distributed training using existing models and training code, with minimal changes.</p>

<p>For example, a variable created under a <code>MirroredStrategy</code> is a <code>MirroredVariable</code>. If no devices are specified in the constructor argument of the strategy then it will use all the available <code>GPUs</code>. If no <code>GPUs</code> are found, it will use the available <code>CPUs</code>. Note that TensorFlow treats all <code>CPUs</code> on a machine as a single device, and uses threads internally for parallelism.</p>

<p>Would recommend to go through <a href=""https://www.tensorflow.org/tutorials/distribute/custom_training"" rel=""nofollow noreferrer"">Custom training with tf.distribute.Strategy</a> tutorial that demonstrates on how to use tf.distribute.Strategy with custom training loops. They will train a simple CNN model on the fashion MNIST dataset.</p>

<p>Hope this answers your question. Happy Learning.</p>
",3711266,4334,https://stackoverflow.com/questions/59074659,Documentation Replication on Other Examples
57526164,What is the relationship between `tf.function` and `autograph.to_graph` in Tensorflow?,"<p>Similar results can be obtained via <code>tf.function</code> and <code>autograph.to_graph</code>.<br>
However this seems to be version dependant.</p>

<p>For example, the function (taken from the official guide):</p>

<pre><code>def square_if_positive(x):
  if x &gt; 0:
    x = x * x
  else:
    x = 0.0
  return x
</code></pre>

<p>Can be evaluated in graph mode using:</p>

<ul>
<li><code>autograph.to_graph</code> in TF 1.14</li>
</ul>

<pre><code>tf_square_if_positive = autograph.to_graph(square_if_positive)

with tf.Graph().as_default():
  g_out = tf_square_if_positive(tf.constant( 9.0))
  with tf.Session() as sess:
    print(sess.run(g_out))
</code></pre>

<ul>
<li><code>tf.function</code> in TF2.0</li>
</ul>

<pre><code>@tf.function
def square_if_positive(x):
  if x &gt; 0:
    x = x * x
  else:
    x = 0.0
  return x

square_if_positive(tf.constant( 9.0))
</code></pre>

<p>So:</p>

<ul>
<li>What is the relationship between <code>tf.function</code> and <code>autograph.to_graph</code>? One can assumes <code>tf.function</code> is using <code>autograph.to_graph</code> (as well as <code>autograph.to_code</code>) internally, but this is far from obvious.</li>
<li>Is the <code>autograph.to_graph</code> snippet still supported in TF2.0 (since it requires <code>tf.Session</code>)? It is present in the <a href=""https://www.tensorflow.org/guide/autograph#automatically_convert_python_control_flow"" rel=""nofollow noreferrer"">autograph doc</a> in TF 1.14, but not in the corresponding doc of TF 2.0</li>
</ul>
",2019-08-16 14:13:02,"<p>I covered and answered all your questions in a three-part article: ""Analyzing tf.function to discover AutoGraph strengths and subtleties"": <a href=""https://pgaleone.eu/tensorflow/tf.function/2019/03/21/dissecting-tf-function-part-1/"" rel=""noreferrer"">part 1</a>, <a href=""https://pgaleone.eu/tensorflow/tf.function/2019/04/03/dissecting-tf-function-part-2/"" rel=""noreferrer"">part 2</a>, <a href=""https://pgaleone.eu/tensorflow/tf.function/2019/05/10/dissecting-tf-function-part-3/"" rel=""noreferrer"">part 3</a>.</p>

<p>To summarize and answer your 3 questions:</p>

<ul>
<li>What is the relationship between <code>tf.function</code> and <code>autograph.to_graph</code>?</li>
</ul>

<p><code>tf.function</code> uses AutoGraph by default. What happens when you <strong>invoke</strong> the first time a tf.function-decorated function is that:</p>

<ol>
<li>The function body is executed (in TensorFlow 1.x like, thus without eager mode) and its execution is traced (now tf.function knows which nodes are present, which branch of the <code>if</code> to keep and so on)</li>
<li>At the same time, AutoGraph starts and tries to convert to <code>tf.*</code> calls, the Python statements it knows (<code>while</code> -> <code>tf.while</code>, <code>if</code> -> <code>tf.cond</code>, ...)-.</li>
</ol>

<p>Merging the information from points 1 and 2 a new graph is built, and based on the function name and the type of the parameters it is cached in a map (see the articles for a better understanding).</p>

<ul>
<li>Is the autograph.to_graph snippet still supported in TF2.0?</li>
</ul>

<p>Yes, <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/to_graph"" rel=""noreferrer""><code>tf.autograph.to_graph</code></a> is still present and it creates a session internally for you (in TF2 you don't have to worry about them).</p>

<p>At any rate, I suggest you read the three articles linked since they cover in detail this and other peculiarities of <code>tf.function</code>.</p>
",3860928,1697,https://stackoverflow.com/questions/57526164,Documentation Replicability
57138511,How to addin new Tensorflow layers to TensorRT engine?,"<p>I have tensorflow freezed model from which TensorRT engine is produced.</p>

<p>I can't retrain model since I don't have all those required images.</p>

<p>But Tensorflow process has some post processing layers and I like to add into TensorRT engine.</p>

<p>What would be the best approach?</p>

<p>Can I create plugin layer using TensorRT layers?</p>

<p>Those Tensorflow layers are mostly available in TensorRT as follows.</p>

<pre><code>self.tensor_heatMat_up = tf.image.resize_area(self.tensor_output[:, :, :, :19], self.upsample_size,
                                                      align_corners=False, name='upsample_heatmat')
        self.tensor_pafMat_up = tf.image.resize_area(self.tensor_output[:, :, :, 19:], self.upsample_size,
                                                     align_corners=False, name='upsample_pafmat')
        if trt_bool is True:
            smoother = Smoother({'data': self.tensor_heatMat_up}, 25, 3.0, 19)
        else:
            smoother = Smoother({'data': self.tensor_heatMat_up}, 25, 3.0)
        gaussian_heatMat = smoother.get_output()

        max_pooled_in_tensor = tf.nn.pool(gaussian_heatMat, window_shape=(3, 3), pooling_type='MAX', padding='SAME')
        self.tensor_peaks = tf.where(tf.equal(gaussian_heatMat, max_pooled_in_tensor), gaussian_heatMat,
tf.zeros_like(gaussian_heatMat))
</code></pre>

<p>TensorRT has scale for resize_area, conv for Smoother.
Not sure tf.equal in TensorRT.</p>

<p>How to addin those layers to TensorRT?
Possible to use graphsurgeon or UFF model?</p>
",2019-07-22 02:30:03,"<p>The following steps add a custom plugin layer in C++ for TensorFlow networks: </p>

<ol>
<li>Implement the IPluginV2 and IPluginCreator classes as shown in: <a href=""https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#example1_add_custlay_c"" rel=""nofollow noreferrer"">Adding A Custom Layer Using C++ For Caffe</a>.</li>
<li>Map the TensorFlow operation to the plugin operation. You can use <a href=""https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/graphsurgeon/graphsurgeon.html"" rel=""nofollow noreferrer"">GraphSurgeon</a> for this. </li>
<li><p>Call the <a href=""https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/uff/uff.html"" rel=""nofollow noreferrer"">UFF converter</a> with the preprocess -p flag set. This will generate a UFF file with the TensorFlow operations replaced by TensorRT plugin nodes.<code>
convert-to-uff frozen_inference_graph.pb -p config.py -t</code></p></li>
<li><p>Run the pre-processed and converted UFF file with TensorRT using the UFF parser. For details, see <a href=""https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#using_custom_layer"" rel=""nofollow noreferrer"">Using Custom Layers When Importing A Model From A Framework</a>. <a href=""https://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html#uffssd_sample"" rel=""nofollow noreferrer"">The Object Detection With A TensorFlow SSD Network</a> sample illustrates how to add a custom layer that is not supported in UFF using C++. See config.py in the sample folder for a demonstration of how to pre-process the graph.</p></li>
</ol>

<p>Although the C++ API is the preferred language to implement custom layers; due to easily accessing libraries like CUDA and cuDNN, you can also work with custom layers in Python applications. You can follow <a href=""https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#add_custom_layer_python"" rel=""nofollow noreferrer"">Adding Custom Layers Using The Python API</a> guide. </p>
",2467772,7142,https://stackoverflow.com/questions/57138511,Documentation Replication on Other Examples
57246091,How to cast int32 tensor to float32,"<p>How can I cast an <code>int32</code> tensor to <code>float32</code> in tensorflow. I don't understand what <code>tf.cast</code> does. It does not seem to do anything.</p>

<pre><code>import tensorflow as tf
import numpy as np

tf.enable_eager_execution()

a = tf.constant([[1, 2, 3, 4], [1, 2, 3, 4]])
b = tf.cast(a, dtype=tf.float32)

print(tf.shape(a))
print(tf.shape(b))
</code></pre>

<p>outputs;</p>

<pre><code>tf.Tensor([2 4], shape=(2,), dtype=int32) #a   
tf.Tensor([2 4], shape=(2,), dtype=int32) #b
</code></pre>
",2019-07-29 00:54:54,"<p>If you just use;</p>

<pre><code>print(a)
print(b)
</code></pre>

<p>you shall get the correct result;</p>

<pre><code>tf.Tensor(
[[1 2 3 4]
 [1 2 3 4]], shape=(2, 4), dtype=int32) #a
tf.Tensor(
[[1. 2. 3. 4.]
 [1. 2. 3. 4.]], shape=(2, 4), dtype=float32) #b
</code></pre>

<p>So the <code>tf.cast()</code> works as intended!</p>

<hr>

<p>With <a href=""https://www.tensorflow.org/api_docs/python/tf/shape"" rel=""noreferrer""><code>tf.shape()</code></a> you are getting a result that explains the shape details of input. </p>

<blockquote>
  <p>Returns: A Tensor of type out_type.</p>
  
  <p>out_type: (Optional) The specified output type of the operation (int32
  or int64). Defaults to tf.int32</p>
</blockquote>

<p>So the <code>dtype</code> of <code>tf.shape()</code> result, is the <code>dtype</code> of the resulting ""<em>shape detailing tensor</em>"", not of <code>a</code>, or <code>b</code></p>
",3926152,1040,https://stackoverflow.com/questions/57246091,Documentation Replicability
66980404,Tensorflow 2 - How to conditionally update values directly in tf.Variable,"<h1>Question</h1>
<p>Please advise how to conditionally update the original tf.Variable. This is a different question from <a href=""https://stackoverflow.com/questions/65449671/conditional-assignment-of-tf-variable-in-tensorflow-2"">conditional assignment of tf.variable in Tensorflow 2</a>.</p>
<h1>Background</h1>
<p><code>tf.Variable</code> is mutable and the <code>assign</code> method would update the same memory area. It looks the <code>assign</code> method does not have an option to incorporate condition when assigning values. Hence I suppose <a href=""https://www.tensorflow.org/api_docs/python/tf/where"" rel=""nofollow noreferrer"">tf.where</a> is to update tf.Variable conditionally.</p>
<ul>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/where"" rel=""nofollow noreferrer"">tf.where</a></li>
</ul>
<pre><code>Returns:
    If x and y are provided: 
        A Tensor with the same type as x and y, and shape that is broadcast from the condition, x, and y.
    Otherwise: 
        A Tensor with shape (num_true, dim_size(condition)).
</code></pre>
<p>In numpy, indexing can be used to directly update the numpy array but there looks no such way in Tensorflow.</p>
<pre><code># Numpy conditional uddate with boolean indexing
x = np.random.uniform(-1, 1, size=(3, 4))
x[x &gt; 0] = 0
</code></pre>
<h3>Problem</h3>
<p>As <code>tf.Variable</code> is mutable, expected that the <code>tf.where</code> will mutate the original Variable, however as below, the original Variable <code>x</code> has not been updated.</p>
<pre><code>x = tf.Variable(np.random.uniform(-1, 1, size=(3,4)), dtype=tf.float32)
print(f&quot;x:{x}\n&quot;)
print(f&quot;x &gt; 0:\n{x &gt; 0}\n&quot;)
print(f&quot;tf.where(x&gt;0, 1, x):\n{tf.where(x&gt;0, 1, x)}&quot;)

x  # check if updated
</code></pre>
<p>Result:</p>
<pre><code># --------------------------------------------------------------------------------
# Original tf.Variable x
# --------------------------------------------------------------------------------
x is &lt;tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=
array([[ 0.8015974 ,  0.8223503 , -0.2704468 ,  0.01874248],
       [ 0.46989247,  0.4753061 , -0.06808566, -0.57646054],
       [ 0.07082719,  0.2924774 , -0.12741995,  0.3168819 ]],
      dtype=float32)&gt;

x &gt; 0 is [[ True  True False  True]
 [ True  True False False]
 [ True  True False  True]]

# --------------------------------------------------------------------------------
# Update using tf.where
# --------------------------------------------------------------------------------
tf.where(x&gt;0, 1, x)=
[[ 1.          1.         -0.2704468   1.        ]
 [ 1.          1.         -0.06808566 -0.57646054]
 [ 1.          1.         -0.12741995  1.        ]]

# --------------------------------------------------------------------------------
# The x is the same as before.
# --------------------------------------------------------------------------------
&lt;tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=
array([[ 0.8015974 ,  0.8223503 , -0.2704468 ,  0.01874248],
       [ 0.46989247,  0.4753061 , -0.06808566, -0.57646054],
       [ 0.07082719,  0.2924774 , -0.12741995,  0.3168819 ]],
      dtype=float32)&gt;
</code></pre>
<p>Please help me understand if there is a way to directly update <code>x</code>.</p>
<hr />
<h1>Notes</h1>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/issues/36559"" rel=""nofollow noreferrer"">How to efficiently update a tensor slice? #36559</a></li>
</ul>
<blockquote>
<p>First, the return value from the slide op is a Tensor, which is not the original variable.</p>
<pre><code>var_slice = var[4:5]
var_slice.assign(math_ops.sub(var, const))
</code></pre>
<p>With a tensor, it doesn't have methods like &quot;assign&quot; or &quot;assign_add&quot; etc.</p>
<p>I think the most feasible way to create a TensorArray, which contains
your slides of mean/variance value, and read/write to it within the
call() body for cell. Once the layer goes over all the timesteps, you
can do a stack for the TensorArray and assign the value back to the
variable itself. You don't have to continuously write to the variable
when processing the timesteps, since timestep t shouldn't affect the
result in t+1 (if I understand your problem correctly)</p>
</blockquote>
<ul>
<li><a href=""https://stackoverflow.com/questions/62092147/how-to-efficiently-assign-to-a-slice-of-a-tensor-in-tensorflow"">How to efficiently assign to a slice of a tensor in TensorFlow</a></li>
</ul>
",2021-04-07 06:07:26,"<p>To update a <code>tf.Variable</code>, you need to call <code>assign</code>. Read more in the <a href=""https://www.tensorflow.org/guide/variable"" rel=""nofollow noreferrer"">Introduction to Variables</a> guide.</p>
<pre><code>&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; tf.random.set_seed(0)
&gt;&gt;&gt; x = tf.Variable(tf.random.uniform((3,4),-1,1), dtype=tf.float32)
&gt;&gt;&gt; x
&lt;tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=
array([[-0.41604972, -0.5868671 ,  0.07078147,  0.12251496],
       [-0.16665101,  0.6156559 , -0.0135498 ,  0.9962585 ],
       [ 0.3934703 , -0.7492528 ,  0.4196334 ,  0.32483125]],
      dtype=float32)&gt;
&gt;&gt;&gt; x.assign(tf.where(x&gt;0,x,0))
&lt;tf.Variable 'UnreadVariable' shape=(3, 4) dtype=float32, numpy=
array([[0.        , 0.        , 0.07078147, 0.12251496],
       [0.        , 0.6156559 , 0.        , 0.9962585 ],
       [0.3934703 , 0.        , 0.4196334 , 0.32483125]], dtype=float32)&gt;
&gt;&gt;&gt; x
&lt;tf.Variable 'Variable:0' shape=(3, 4) dtype=float32, numpy=
array([[0.        , 0.        , 0.07078147, 0.12251496],
       [0.        , 0.6156559 , 0.        , 0.9962585 ],
       [0.3934703 , 0.        , 0.4196334 , 0.32483125]], dtype=float32)&gt;
</code></pre>
",4281353,20088,https://stackoverflow.com/questions/66980404,Documentation Replicability
60013980,tf.nn.embedding_lookup_sparse 3D sparse tensor input,"<p>I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup_sparse</code></a> I found it only supports 2D sparse tensors,</p>

<blockquote>
  <p>sp_ids: N x M SparseTensor of int64 ids where N is typically batch size and M is arbitrary.</p>
</blockquote>

<p>My example code here</p>

<pre><code>import numpy as np
import tensorflow as tf
tf.enable_eager_execution()

# [feature number, embedding dim] 
w = tf.get_variable(""w"", [4, 4], initializer=tf.random_normal_initializer())

z = np.array(
     [
      [
        [0, 1, 2, 3],   # get the vector of row 0, 1, 2, 3 of the embedding matrix w and get the sum
        [2, 3]
      ],

      [
        [1, 3],
        [2]
      ],

      [
        [0, 1, 3],
        [1, 2]
      ]
     ])

sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],
                     indices=[[0,0,0],[0,0,1],[0,0,2],[0,0,3],[0,1,2],
                              [0,1,3],[1,0,1],[1,0,3],[1,1,2],[2,0,0],
                              [2,0,1],[2,0,3],[2,1,1],[2,1,2]],
                     dense_shape=[3, 2, 4])

tf.nn.embedding_lookup_sparse(w, sp, None, combiner='sum')
# the outputs
&lt;tf.Tensor: id=970, shape=(3, 4), dtype=float32, numpy=
array([[-5.8729677 , -1.3900641 ,  0.8126096 , -3.1223912 ],
       [-1.0788026 , -1.1324122 ,  0.34160078,  0.23714277],
       [-2.497394  , -2.7855003 ,  3.0201516 , -1.8009453 ]],
      dtype=float32)&gt;

print(w)
&lt;tf.Variable 'w:0' shape=(4, 4) dtype=float32, numpy=
array([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],
       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],
      dtype=float32)&gt;
</code></pre>

<p>But the expected output is a matrix with a dimension of <code>3x2x4</code>, not <code>3x4</code>. Does <code>tf.nn.embedding_lookup_sparse</code> support this operation?</p>
",2020-02-01 04:23:54,"<p>The most easier way to do so is to make the sparse tensor as a 2D tensor and get the weights of the embedding matrix and then reshape.</p>

<pre class=""lang-py prettyprint-override""><code># First make the z as a 2D arr and create a sparse tensor 
z = np.array([
        [0, 1, 2, 3],  # get the row 0,1,2,3 of the embedding matrix w and get the sum
        [2, 3],
        [1, 3],
        [2],
        [0, 1, 3],
        [1, 2]
      ])

sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],
                     indices=[[0,0],[0,1],[0,2],[0,3],[1,2],[1,3],[2,1],
                              [2,3],[3,2],[4,0],[4,1],[4,3],[5,1],[5,2]],
                     dense_shape=[6, 4])

res = tf.nn.embedding_lookup_sparse(w, sp, None, combiner='sum')

res.numpy()
# the output
array([[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],
       [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ],
       [ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],
       [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]],

# reshape
tf.reshape(res, [-1, 2, 4])
# that is exacly what I want.
array([[[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],
        [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ]],

       [[ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],
        [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532]],

       [[-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],
        [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]]])

# print w, and the above result is right
w.numpy()

array([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],
       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],
      dtype=float32)
</code></pre>

<p>So, forget the 3D sparse tensor, simply convert it to 2D tensor. Because you only care about the values (index of rows, which are used to get the corresponding rows of the embedding matrix) in the sparse tensor.</p>
",5046896,17040,https://stackoverflow.com/questions/60013980,Documentation Replicability
55740550,"Tensorflow, iterate tensors","<p>I defined my loss function and I want to iterative each item of the batch to calculate the loss function. I used <code>tf.map_fn</code> however, I found it is very slow. Are there any suggestions?</p>

<pre class=""lang-py prettyprint-override""><code>def loss(phi, mu, sigma, t_phi, t_mu, t_sigma):
    _loss = 0.0
    for i in range(phi.shape[0]):
        for j in range(phi.shape[0]):
            _loss += phi[i] * phi[j] * pdf(mu[i], mu[j], tf.sqrt(sigma[i]**2 + sigma[j]**2))
            _loss += t_phi[i] * t_phi[j] * pdf(t_mu[i], t_mu[j], tf.sqrt(t_sigma[i]**2 + t_sigma[j]**2))
            _loss += -2 * phi[i] * t_phi[j] * pdf(mu[i], t_mu[j], tf.sqrt(sigma[i]**2 + t_sigma[j]**2))
    return tf.sqrt(_loss)

def reduce_loss(phi, mu, sigma, t_phi, t_mu, t_sigma):
    with tf.variable_scope('loss') as loss:
        stacked = tf.stack([phi, mu, sigma, t_phi, t_mu, t_sigma], 1)
        return tf.map_fn(lambda x: loss(x[0], x[1], x[2], x[3], x[4], x[5]), stacked,
                         parallel_iterations=4)

def pdf(x, mu, sigma):
    return tf.exp(-0.5*(x-mu)**2/sigma**2) / ((2*np.pi*sigma**2)**0.5)
</code></pre>

<p>The batch size is 1024. </p>
",2019-04-18 06:51:32,"<p>You can eliminate the loops in your <code>loss</code> function. This is done by vectorising everything. For example you iterate through <code>i</code> and <code>j</code> to calculate <code>phi[i]*phi[j]</code> But this is the ij'th element of <code>tf.matmul(phi[:, None], phi[None, :])</code>. Doing this should be faster than the implementation with loops.</p>

<p>Also, because tensorflow builds the graph statically, your function could take a loooong time to even build the graph. You should generally avoid big nested for loops in tensorflow for this reason.</p>

<p>I have made an example with part of your loss function vectorised, it should be easy to do the other parts.</p>

<pre><code>import tensorflow as tf
from numpy import pi as PI
from time import time


# some random vectors
size = 10
phi = tf.random.uniform([size])
mu = tf.random.uniform([size])
sigma = tf.random.uniform([size])


####################################
# Your original loss
####################################

def pdf(x, m, s):
    return tf.exp(-0.5*(x-m)**2/s**2) / ((2*PI*s**2)**0.5)


def loss():
    _loss = 0.0
    for i in range(phi.shape[0]):
        for j in range(phi.shape[0]):
            _loss += phi[i] * phi[j] * pdf(mu[i], mu[j], tf.sqrt(sigma[i]**2 + sigma[j]**2))
    return tf.sqrt(_loss)


####################################
# vectorised loss
####################################

def vector_pdf(x, s):
    return tf.exp(-0.5*x**2/s**2) / ((2*PI*s**2)**0.5)


def vectorised_loss():
    phi_ij = tf.matmul(phi[:, None], phi[None, :])
    difference = mu[:, None] - mu[None, :]
    sigma_squared = sigma**2
    sigma_sum = tf.sqrt(sigma_squared[:, None] + sigma_squared[None, :])

    loss_array = phi_ij*vector_pdf(difference, sigma_sum)
    return tf.sqrt(tf.reduce_sum(loss_array))


#######################################
# Time the functions and show they are the same
#######################################

with tf.Session() as sess:
    loop_loss = loss()
    vector_loss = vectorised_loss()
    # init = tf.global_variables_initializer()
    # sess.run(init)

    t = 0.
    for _ in range(100):
        st = time()
        loop_loss_val = sess.run(loop_loss)
        t += time() - st
    print('loop took {}'.format(t/100))

    t = 0.
    for _ in range(100):
        st = time()
        vector_val = sess.run(vector_loss)
        t += time() - st
    print('vector took {}'.format(t / 100))

    l_val, v_val = sess.run([loop_loss, vector_loss])
    print(l_val, v_val)
</code></pre>

<p>This prints</p>

<pre><code>loop took 0.01740453243255615
vector took 0.004280190467834472
4.6466274 4.6466274
</code></pre>

<p>By vectorising the loss function, your reduce function should be straightforward to vectorise as well. Now you will want to batch matmul, and slightly change the indices of the subtractions. For example:</p>

<pre><code>mu[:, None] - mu[None, :]
# becomes
mu[: ,:, None] - mu[:, None, :]
</code></pre>
",5046896,17040,https://stackoverflow.com/questions/55740550,Documentation Replicability
58897066,How to print one example of a dataset from tf.data?,"<p>I have a dataset in <code>tf.data</code>. How can I easily print (or grab) one element in my dataset?</p>

<p>Similar to:</p>

<pre class=""lang-py prettyprint-override""><code>print(dataset[0])
</code></pre>
",2019-11-17 02:41:21,"<p>In TF 1.x you can use the following. There are different iterators provided (some might be deprecated in future versions).</p>

<pre><code>import tensorflow as tf

d = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])
diter = d.make_one_shot_iterator()
e1 = diter.get_next()

with tf.Session() as sess:
  print(sess.run(e1))
</code></pre>

<p>Or in TF 2.x</p>

<pre><code>import tensorflow as tf

d = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])
print(next(iter(d)).numpy())

## You can also use loops as follows to traverse the full set one item at a time
for elem in d:
    print(elem)

</code></pre>
",4313927,3611,https://stackoverflow.com/questions/58897066,Documentation Replication on Other Examples
63376657,Should I be using tf.keras or keras in 2020?,"<p>A while back I read this <a href=""https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/"" rel=""nofollow noreferrer"">post on PyImageSearch</a> and was satisfied enough to switch completely to <code>tf.keras</code>.</p>
<p>But since then I noticed the <a href=""https://keras.io/"" rel=""nofollow noreferrer"">Keras website</a> got an overhaul and seems to be expanding on its ambitions.</p>
<p>So I'm a bit confused.</p>
<ul>
<li>Is it still true that <em>&quot;the latest release of the keras package (v2.3.0) will be the last release to support multiple backends and feature updates. Moving forward, the keras package will receive only bug fixes.&quot;</em> from <a href=""https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/"" rel=""nofollow noreferrer"">PyImageSearch</a>?</li>
<li>If I stick with <code>tf.keras</code> could I miss out on anything? I'm talking about specific features (to make this a &quot;fact based&quot; question)</li>
</ul>
",2020-08-12 12:35:22,"<p>An <a href=""https://datascience.stackexchange.com/a/60408"">answer on the Datascience Stack Exchange site</a> seems to answer this question.</p>
<p>Franois Chollet, the creator of Keras, recommends that users switch to <code>tf.keras</code> (<a href=""https://twitter.com/fchollet/status/1174018651449544704?s=19"" rel=""nofollow noreferrer"">source</a>)</p>
<blockquote>
<p>New release of multi-backend Keras: 2.3.0</p>
<p>https://github.com/keras-team/keras/releases/tag/2.3.0</p>
<ul>
<li>First release of multi-backend Keras with full TF 2 support</li>
<li>Continued support for Theano/CNTK</li>
<li>Will be the last major release of multi-backend Keras</li>
</ul>
<p>We recommend you switch your Keras code to tf.keras.</p>
</blockquote>
<p>And in two replies:</p>
<blockquote>
<p>Both Theano and CNTK are out of development. Meanwhile, as Keras backends they represent less than 4% of Keras usage. The other 96% of users (of which more than half are already on tf.keras) are better served with tf.keras.</p>
<p>Keras development will focus on tf.keras going forward.</p>
</blockquote>
<blockquote>
<p>Importantly, we will seek to start developing tf.keras in its own standalone GitHub repository at keras-team/keras in order to make it much easier for 3rd party folks to contribute.</p>
<p>Keras has never been moving faster than now :)</p>
</blockquote>
",4391249,2921,https://stackoverflow.com/questions/63376657,Documentation Ambiguity
58225926,Tensorflow Gradient Tape returning None,"<p>I'm using TensorFlow 1.14.0 with Python(3.6.8). I'm trying to use tensorflow_probability's lbfgs optimizer implementation(<a href=""https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer/lbfgs_minimize"" rel=""nofollow noreferrer"">documentation/example</a>).</p>

<p>If I run the example code provided in the documentation it works fine. I tried to follow the same procedure for my own code which uses the <code>tf.GradientTape()</code> approach for computing the objective function. When doing it that way, the gradients come back as <code>None</code> type.</p>

<p>I'm not seeing why one is working, but the other is not.</p>

<p>Edit: I realized that running eager execution using the gradients wouldn't work, so I adjusted the example to be able to be run with eager execution.</p>

<p>Non-working example(using GradientTape) with eager execution</p>

<pre><code>import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

tf.enable_eager_execution()

# A high-dimensional quadratic bowl.
ndims = 3
minimum = np.ones([ndims], dtype='float64')
scales = np.arange(ndims, dtype='float64') + 1.0

# The objective function and the gradient.
def quadratic(x):
    with tf.GradientTape() as g:
        value = tf.reduce_sum(scales * (x - minimum) ** 2)
    grads = g.gradient(value, x)
    print('Gradients: ')
    print(grads)
    return value, grads

start = np.arange(ndims, 0, -1, dtype='float64')
optim_results = tfp.optimizer.lbfgs_minimize(quadratic, initial_position=start, num_correction_pairs=10,tolerance=1e-8)

print('results')
print(optim_results)
# Check that the search converged
assert(optim_results.converged)
# Check that the argmin is close to the actual value.
np.testing.assert_allclose(optim_results.position, minimum)
</code></pre>
",2019-10-03 19:44:56,"<p>You need to watch x. For the operations inside this context manager, it's required at least one of their inputs is being watched. </p>

<pre class=""lang-py prettyprint-override""><code>with tf.GradientTape() as g:
    g.watch(x)
    value = tf.reduce_sum(scales * (x - minimum) ** 2)
</code></pre>
",3667142,1133,https://stackoverflow.com/questions/58225926,Documentation Replication on Other Examples
65745053,Tensorflow softmax does not ignore masking value,"<p>I am reviving this github <a href=""https://github.com/tensorflow/tensorflow/issues/27010"" rel=""nofollow noreferrer"">issue</a> because I believe it is valid and needs to be explained. tf.keras has a masking layer with docs that reads</p>
<blockquote>
<p>For each timestep in the input tensor (dimension #1 in the tensor), if
all values in the input tensor at that timestep are equal to
mask_value, then the timestep will be masked (skipped) in all
downstream layers (as long as they support masking).</p>
<p>If any downstream layer does not support masking yet receives such an
input mask, an exception will be raised.</p>
</blockquote>
<pre><code>
# create padded zeros and change two valid entries.
inputs = np.zeros([1,5])
inputs[0,1] = 0.5
inputs[0,2] = 0.1
inputs = tf.Variable(inputs)
masked_inputs = tf.keras.layers.Masking(mask_value=0.0)(inputs)
with_masking = tf.keras.layers.Softmax()(masked_inputs)
without_masking = tf.keras.layers.Softmax()(inputs)
</code></pre>
<p>The two results are virtually identical</p>
<pre><code>with_masking
&lt;tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.1737954 , 0.28654018, 0.19207363, 0.1737954 , 0.1737954 ]],
      dtype=float32)&gt;
without_masking
&lt;tf.Tensor: shape=(1, 5), dtype=float64, numpy=array([[0.1737954 , 0.28654017, 0.19207362, 0.1737954 , 0.1737954 ]])&gt;
</code></pre>
<h1>Expected behavior</h1>
<p>I expected to just take softmax of the valid entries, similiar to</p>
<pre><code>#Assign one large value 
inputs = np.zeros([1,2])
inputs[0,0] = 0.5
inputs[0,1] = 0.1
inputs = tf.Variable(inputs)
without_masking = tf.keras.layers.Softmax()(inputs)

without_masking
&lt;tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[0.59868766, 0.40131234]])&gt;
</code></pre>
<p>padded at the correct positions</p>
<pre><code>with_masking
&lt;tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0 , 0.59868766, 0.40131234, 0, 0 ]],
      dtype=float32)&gt;
</code></pre>
<p>To ignore 0's in a softmax function, we could switch out massively negative numbers?</p>
<p>Related: <a href=""https://stackoverflow.com/questions/39091432/tensorflow-softmax-ignore-negative-labels-just-like-caffe"">tensorflow - softmax ignore negative labels (just like caffe)</a></p>
<pre><code>from tensorflow import __version__
__version__
'2.3.1'
</code></pre>
",2021-01-15 23:55:04,"<p>I think this is already explained well in the <a href=""https://github.com/tensorflow/tensorflow/issues/27010"" rel=""noreferrer"">Github issue</a> you have linked. Underlying problem is that irrespective of whether an array is masked or not, <code>softmax()</code> still operates on <code>0.0</code> values and returns a <code>non-zero</code> value as mathematically expected (<a href=""https://en.wikipedia.org/wiki/Softmax_function"" rel=""noreferrer"">link</a>).</p>
<p>The only way to get a zero output from a <code>softmax()</code> is to pass a <strong>very small float value</strong>. If you set the masked values to the minimum possible machine limit for <code>float64</code>, <code>Softmax()</code> of this value will be zero.</p>
<p>To get machine limit on float64 you need <code>tf.float64.min</code> which is equal to <code>-1.7976931348623157e+308</code>. More info about machine limits on this <a href=""https://stackoverflow.com/questions/65657086/how-can-i-express-this-custom-loss-function-in-tensorflow/65658222#65658222"">post</a>.</p>
<p>Here is an implementation for your reference on <code>tf.boolean_mask</code> only, and the correct method of using <code>tf.where</code> for creating the mask and passing it to <code>softmax()</code> -</p>
<pre><code>import tensorflow as tf

inputs = np.zeros([1,5])
inputs[0,1] = 0.5
inputs[0,2] = 0.1
inputs = tf.Variable(inputs)

#Returns only the elements that are not masked (2,)
with_boolmask = tf.boolean_mask(inputs, inputs!=0)
with_boolmask = tf.keras.layers.Softmax()(with_boolmask)

#Correct way to do it!
masked_inp = tf.where(inputs!=0, inputs, tf.float64.min) #&lt;----
with_where = tf.keras.layers.Softmax()(masked_inp)

print('BOOLEAN MASK (NOT EXPECTED)')
print(with_boolmask)

print('')
print('MASKED INPUT - ')
print(masked_inp)
print('')
print('SOFTMAX OUTPUT')
print(with_where)
</code></pre>
<pre><code>BOOLEAN MASK (NOT EXPECTED)
tf.Tensor([0.59868765 0.40131232], shape=(2,), dtype=float32)

MASKED INPUT - 
tf.Tensor(
[[-1.79769313e+308  5.00000000e-001  1.00000000e-001 -1.79769313e+308
  -1.79769313e+308]], shape=(1, 5), dtype=float64)

SOFTMAX OUTPUT
tf.Tensor([[0.         0.59868765 0.40131232 0.         0.        ]], shape=(1, 5), dtype=float32)
</code></pre>
",2984051,2237,https://stackoverflow.com/questions/65745053,Documentation Replication on Other Examples
63919438,TensorFlow keras model fit() parameters steps_per_epoch and epochs behavior on train set,"<p>I'm using a tf.data dataset containing my training data consisting of (lets say) 100k images.
I'm also using a tf.data dataset containing my validation set.
Since an epoch of all 100k images takes quite long (in my case approximately one hour) before I get any feedback on performance on the validation set, I set the <code>steps_per_epoch</code> parameter in tf.keras.Model <code>fit()</code> to <code>10000</code>.
Using a batch size of 1 this results into having 10 validation scores when reaching 100k of images.
In order to complete one epoch of 100k images of my entire training dataset, I set the <code>epochs</code> parameter to <code>10</code></p>
<p>However, I'm not sure if using <code>steps_per_epoch</code> and <code>epochs</code> this way has any other consequences. Is it correct to use these parameters in order to get more frequent feedback on performance?
And also a more specific question, does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'?
I already dug into the <a href=""https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">TensorFlow docs</a> and read several different stack overflow questions, but I couldn't find anything conclusive to answer my own question. Hope you can help!</p>
<p>Tensorflow version I'm using is 2.2.0.</p>
",2020-09-16 11:49:04,"<blockquote>
<p>Is it correct to use these parameters in order to get more frequent
feedback on performance?</p>
</blockquote>
<p>Yes, it is correct to use these parameters. Here is the code that i used to fit the model.</p>
<pre><code>model.fit(
train_data,
steps_per_epoch = train_samples//batch_size,
epochs = epochs,
validation_data = test_data,
verbose = 1,
validation_steps = test_samples//batch_size)
</code></pre>
<blockquote>
<p>does it use all 100k images or does it use the same first 10k images of my
training set at every 'epoch'?</p>
</blockquote>
<p>It use all images in your training data.</p>
<p>For better understanding <code>Epoch</code> is the number times the learning algorithm will work through the entire training data set.</p>
<p>Where as <code>steps_per_epoch</code> is the total number of samples in your training data set divided by the batch size.</p>
<p>For example, if you have 100000 training samples and use a batch size of 100, one epoch will be equivalent to 1000 steps_per_epoch.</p>
<p><em>Note: We generally observe batch size to be the power of 2, this is because of the effective work of optimized matrix operation libraries.</em></p>
",3908025,589,https://stackoverflow.com/questions/63919438,Inadequate Examples
54989442,"RNN in Tensorflow vs Keras, depreciation of tf.nn.dynamic_rnn()","<p>My question is: Are the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""noreferrer""><code>tf.nn.dynamic_rnn</code></a> and <code>keras.layers.RNN(cell)</code> truly identical as stated in docs? </p>

<p>I am planning on building an RNN, however, it seems that <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""noreferrer""><code>tf.nn.dynamic_rnn</code></a> is depricated in favour of Keras.</p>

<p>In particular, it states that:</p>

<blockquote>
  <p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future
  version. Instructions for updating: Please use keras.layers.RNN(cell),
  which is equivalent to this API</p>
</blockquote>

<p>But I don't see how the APIs are equivalent, in the case of variable sequence lengths!</p>

<p>In raw TF, we can specify a tensor of shape <code>(batch_size, seq_lengths)</code>. This way, if our sequence is <code>[0, 1, 2, 3, 4]</code> and the longest sequence in the batch is of size 10, we can pad it with 0s and <code>[0, 1, 2, 3, 4, 0, 0, 0, 0, 0]</code>, we can say <code>seq_length=5</code> to process <code>[0, 1, 2, 3, 4]</code>.</p>

<p>However, in Keras, this is not how it works! What we can do, is specify the <code>mask_zero=True</code> in previous Layers, e.g. the Embedding Layer. This will also mask the 1st zero!</p>

<p>I can go around it by adding ones to the whole vector, but then thats extra preprocessing that I need to do after processing using <code>tft.compute_vocabulary()</code>, which maps vocabulary words to 0 indexed vector.</p>
",2019-03-04 18:33:48,"<p>No, but they are (or can be made to be) not so different either. </p>

<h2>TL;DR</h2>

<p><code>tf.nn.dynamic_rnn</code> replaces elements after the sequence end with 0s. This cannot be replicated with <code>tf.keras.layers.*</code> as far as I know, but you can get a similar behaviour with <code>RNN(Masking(...)</code> approach: it simply stops the computation and carries the last outputs and states forward. You will get the same (non-padding) outputs as those obtained from <code>tf.nn.dynamic_rnn</code>.</p>

<h2>Experiment</h2>

<p>Here is a minimal working example demonstrating the differences between <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""noreferrer""><code>tf.nn.dynamic_rnn</code></a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU"" rel=""noreferrer""><code>tf.keras.layers.GRU</code></a> with and without the use of <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking"" rel=""noreferrer""><code>tf.keras.layers.Masking</code></a> layer.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf

test_input = np.array([
    [1, 2, 1, 0, 0],
    [0, 1, 2, 1, 0]
], dtype=int)
seq_length = tf.constant(np.array([3, 4], dtype=int))

emb_weights = (np.ones(shape=(3, 2)) * np.transpose([[0.37, 1, 2]])).astype(np.float32)
emb = tf.keras.layers.Embedding(
    *emb_weights.shape,
    weights=[emb_weights],
    trainable=False
)
mask = tf.keras.layers.Masking(mask_value=0.37)
rnn = tf.keras.layers.GRU(
    1,
    return_sequences=True,
    activation=None,
    recurrent_activation=None,
    kernel_initializer='ones',
    recurrent_initializer='zeros',
    use_bias=True,
    bias_initializer='ones'
)


def old_rnn(inputs):
    rnn_outputs, rnn_states = tf.nn.dynamic_rnn(
        rnn.cell,
        inputs,
        dtype=tf.float32,
        sequence_length=seq_length
    )
    return rnn_outputs


x = tf.keras.layers.Input(shape=test_input.shape[1:])
m0 = tf.keras.Model(inputs=x, outputs=emb(x))
m1 = tf.keras.Model(inputs=x, outputs=rnn(emb(x)))
m2 = tf.keras.Model(inputs=x, outputs=rnn(mask(emb(x))))

print(m0.predict(test_input).squeeze())
print(m1.predict(test_input).squeeze())
print(m2.predict(test_input).squeeze())

sess = tf.keras.backend.get_session()
print(sess.run(old_rnn(mask(emb(x))), feed_dict={x: test_input}).squeeze())
</code></pre>

<p>The outputs from <code>m0</code> are there to show the result of applying the embedding layer.
Note that there are no zero entries at all:</p>

<pre><code>[[[1.   1.  ]    [[0.37 0.37]
  [2.   2.  ]     [1.   1.  ]
  [1.   1.  ]     [2.   2.  ]
  [0.37 0.37]     [1.   1.  ]
  [0.37 0.37]]    [0.37 0.37]]]
</code></pre>

<p>Now here are the actual outputs from the <code>m1</code>, <code>m2</code> and <code>old_rnn</code> architectures:</p>

<pre><code>m1: [[  -6.  -50. -156. -272.7276 -475.83362]
     [  -1.2876 -9.862801 -69.314 -213.94202 -373.54672 ]]
m2: [[  -6.  -50. -156. -156. -156.]
     [   0.   -6.  -50. -156. -156.]]
old [[  -6.  -50. -156.    0.    0.]
     [   0.   -6.  -50. -156.    0.]]
</code></pre>

<h2>Summary</h2>

<ul>
<li>The old <code>tf.nn.dynamic_rnn</code> used to mask padding elements with zeros.</li>
<li>The new RNN layers <em>without masking</em> run over the padding elements as if they were data.</li>
<li>The new <code>rnn(mask(...))</code> approach simply stops the computation and carries the last outputs and states forward. Note that the (non-padding) outputs that I obtained for this approach are exactly the same as those from <code>tf.nn.dynamic_rnn</code>.</li>
</ul>

<p>Anyway, I cannot cover all possible edge cases, but I hope that you can use this script to figure things out further.</p>
",4729764,2866,https://stackoverflow.com/questions/54989442,Documentation Replication on Other Examples
55176818,How to support masking in custom tf.keras.layers.Layer,"<p>I'm implementing a custom <code>tf.keras.layers.Layer</code> that needs to support masking.</p>

<p>Consider the following scenario</p>

<pre class=""lang-py prettyprint-override""><code>embedded = tf.keras.layer.Embedding(input_dim=vocab_size + 1, 
                                    output_dim=n_dims, 
                                    mask_zero=True)
x = MyCustomKerasLayers(embedded)
</code></pre>

<p>Now per the documentation</p>

<blockquote>
  <p><code>mask_zero</code>: Whether or not the input value 0 is a special ""padding"" value that should be masked out. This is useful when using recurrent layers which may take variable length input. <strong>If this is True then all subsequent layers in the model need to support masking or an exception will be raised</strong>. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).</p>
</blockquote>

<p>I wonder, what does that mean? Looking through <a href=""https://www.tensorflow.org/tutorials/eager/custom_layers"" rel=""noreferrer"">TensorFlow's custom layers guide</a> and the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer"" rel=""noreferrer"">tf.keras.layer.Layer</a> documentation it is not clear what should be done to support masking</p>

<ol>
<li><p>How do I support masking?</p></li>
<li><p>How do I access the mask from the past layer? </p></li>
<li><p>Assuming input of <code>(batch, time, channels)</code> or `(batch, time) would the masks look different? What will be their shapes?</p></li>
<li><p>How do I pass it on to the next layer? </p></li>
</ol>
",2019-03-15 06:31:12,"<ol>
<li><p>To support masking one should implement the <code>compute_mask</code> method inside the custom layer</p></li>
<li><p>To access the mask, simply add as the second positional argument in the <code>call</code> method the argument <code>mask</code>, and it will be accessible (ex. <code>call(self, inputs, mask=None)</code>)</p></li>
<li><p>This cannot be guessed, it is the layer's before responsible to calculate the mask </p></li>
<li><p>Once you implemented the <code>compute_mask</code> passing the mask to the next layer happens automatically - excluding the case of model subclassing, which in this case it is up to you to calculate masks and pass them on.</p></li>
</ol>

<p>Example:</p>

<pre class=""lang-py prettyprint-override""><code>class MyCustomKerasLayers(tf.keras.layers.Layer):
    def __init__(self, .......):
        ...

    def compute_mask(self, inputs, mask=None):
        # Just pass the received mask from previous layer, to the next layer or 
        # manipulate it if this layer changes the shape of the input
        return mask

    def call(self, input, mask=None):
        # using 'mask' you can access the mask passed from the previous layer
</code></pre>

<p>Notice that this example just passes on the mask, if the layer will output a shape different than the one received, you should change the mask accordingly in <code>compute_mask</code> to pass on the correct one</p>

<h3>EDIT</h3>

<p>Now explanation is also included in the <a href=""https://www.tensorflow.org/beta/guide/keras/masking_and_padding"" rel=""nofollow noreferrer""><code>tf.keras</code> masking and padding documentation</a>.</p>
",5368083,12085,https://stackoverflow.com/questions/55176818,Lack of Alternative Solutions/Documentation
55109696,TensorFlow - Difference between tf.keras.layers.Layer vs tf.keras.Model,"<p>Reading through the <a href=""https://www.tensorflow.org/tutorials/eager/custom_layers"" rel=""noreferrer"">documentation of implementing custom layers</a> with <code>tf.keras</code>, they specify two options to inherit from, <code>tf.keras.Layer</code> and <code>tf.keras.Model</code>.</p>

<p>Under the context of <em>creating custom layers</em>, I'm asking myself what is the difference between these two? Technically what is different?</p>

<p>If I were to implement the transformer encoder for example, which one would be more suitable? (assuming the transformer is a only a ""layer"" in my full model)</p>
",2019-03-11 20:14:39,"<p>In the documentation:</p>

<blockquote>
  <p>The Model class has the same API as Layer, with the following
  differences: - It exposes built-in training, evaluation, and
  prediction loops (model.fit(), model.evaluate(), model.predict()). -
  It exposes the list of its inner layers, via the model.layers
  property. - It exposes saving and serialization APIs.</p>
  
  <p>Effectively, the ""Layer"" class corresponds to what we refer to in the
  literature as a ""layer"" (as in ""convolution layer"" or ""recurrent
  layer"") or as a ""block"" (as in ""ResNet block"" or ""Inception block"").</p>
  
  <p>Meanwhile, the ""Model"" class corresponds to what is referred to in the
  literature as a ""model"" (as in ""deep learning model"") or as a
  ""network"" (as in ""deep neural network"").</p>
</blockquote>

<p>So if you want to be able to call <code>.fit()</code>, <code>.evaluate()</code>, or <code>.predict()</code> on those blocks or you want to be able to save and load those blocks separately or something you should use the Model class. The Layer class is leaner so you won't bloat the layers with unnecessary functionality...but I would guess that that generally wouldn't be a big problem.  </p>
",5368083,12085,https://stackoverflow.com/questions/55109696,Documentation Replication on Other Examples
59361689,Redundancies in tf.keras.backend and tensorflow libraries,"<p>I have been working in TensorFlow for about a year now, and I am transitioning from TF 1.x to TF 2.0, and I am looking for some guidance on how to use the <code>tf.keras.backend</code> library in TF 2.0. I understand that the transition to TF 2.0 is supposed to remove a lot of redundancies in modeling and building graphs, since there were many ways to create equivalent layers in earlier TensorFlow versions (and I'm insanely grateful for that change!), but I'm getting stuck on understanding when to use <code>tf.keras.backend</code>, because the operations appear redundant with other TensorFlow libraries. </p>

<p>I see that some of the functions in <code>tf.keras.backend</code> are redundant with other TensorFlow libraries. For instance, <code>tf.keras.backend.abs</code> and <code>tf.math.abs</code> are not aliases (or at least, they're not listed as aliases in the documentation), but both take the absolute value of a tensor. After examining the source code, it looks like <code>tf.keras.backend.abs</code> calls the <code>tf.math.abs</code> function, and so I really do not understand why they are not aliases. Other <code>tf.keras.backend</code> operations don't appear to be duplicated in TensorFlow libraries, but it looks like there are TensorFlow functions that can do equivalent things. For instance, <code>tf.keras.backend.cast_to_floatx</code> can be substituted with <code>tf.dtypes.cast</code> as long as you explicitly specify the dtype. I am wondering two things:</p>

<ol>
<li>when is it best to use the <code>tf.keras.backend</code> library instead of the equivalent TensorFlow functions?</li>
<li>is there a difference in these functions (and other equivalent <code>tf.keras.backend</code> functions) that I am missing?</li>
</ol>
",2019-12-16 17:36:52,"<p>Short answer: Prefer tensorflow's native API such as <code>tf.math.*</code> to the<code>tf.keras.backend.*</code> API wherever possible.</p>

<p>Longer answer:</p>

<ul>
<li>The <code>tf.keras.backend.*</code> API can be <em>mostly</em> viewed as a remnant of the <code>keras.backend.*</code> API. The latter is a design that serves the ""exchangeable backend"" design of the original (non-TF-specific) keras. This relates to the historical aspect of keras, which supports multiple backend libraries, among which tensorflow used to be just one of them. Back in 2015 and 2016, other backends, such as Theano and MXNet were quite popular too. But going into 2017 and 2018, tensorflow became the dominant backend of keras users. Eventually keras became a part of the tensorflow API (in 2.x and later minor versions of 1.x). In the old multi-backend world, the <code>backend.*</code> API provides a backend-independent abstraction over the myriad of supported backend. But in the tf.keras world, the value of the backend API is much more limited.</li>
<li>The various functions in <code>tf.keras.backend.*</code> can be divided into a few categories:

<ol>
<li>Thin wrappers around the equivalent or mostly-equivalent tensorflow native API. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2363"" rel=""noreferrer"">tf.keras.backend.less</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2431"" rel=""noreferrer"">tf.keras.backend.sin</a></li>
<li>Slightly thicker wrappers around tensorflow native APIs, with more features included. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2589"" rel=""noreferrer"">tf.keras.backend.batch_normalization</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869"" rel=""noreferrer"">tf.keras.backend.conv2d</a>(<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869</a>). They often perform proprocessing and implement other logics, which make your life easier than using native tensorflow API.</li>
<li>Unique functions that don't have equivalent in the native tensorflow API. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L3809"" rel=""noreferrer"">tf.keras.backend.rnn</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L342"" rel=""noreferrer"">tf.keras.backend.set_learning_phase</a></li>
</ol></li>
</ul>

<p>For category 1, use native tensorflow APIs. For categories 2 and 3, you may want to use the <code>tf.keras.backend.*</code> API, as long as you can find it in the documentation page: <a href=""https://www.tensorflow.org/api_docs/python/"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/</a>, because the documented ones have backward compatibility guarantees, so that you don't need to worry about a future version of tensorflow removing it or changing its behavior.</p>
",4982425,485,https://stackoverflow.com/questions/59361689,Documentation Replication on Other Examples
57414387,Meaning of tf.keras.layers.LSTM parameters,"<p>I am having trouble understanding some of the parameters of <code>LSTM</code> layers in the <code>tf.keras.layers</code> API. </p>

<p>I am investigating using <code>CuDNNLSTM</code> layers instead of <code>LSTM</code> layers (to speed up training), but before I commit to <code>CuDNN</code> layers, I would like to have a full understanding of the parameters that I lose by using a <code>CuDNNLSTM</code> instead of a <code>LSTM</code> layer. I have read the docs, but they seem to assume some prior knowledge of <code>LSTM</code>s that I do not have.</p>

<p>I have listed the pararameters that <code>CuDNNLSTM</code> does not have (that <code>LSTM</code> has) and interspersed with my questions about them, respectively.</p>

<ul>
<li><code>activation</code></li>
<li><code>recurrent_activation</code>

<ol>
<li>What is the difference between <code>activation</code> and <code>recurrent_activation</code>? I am assuming it has something to do with the activation for a cell vs. the activation for the full <code>LSTM</code> layer, but am unsure.</li>
</ol></li>
<li><code>use_bias</code>

<ol start=""2"">
<li>If <code>use_bias</code> is True, where is this bias applied?</li>
</ol></li>
<li><code>dropout</code></li>
<li><code>recurrent_dropout</code>

<ol start=""3"">
<li>Again, what is the difference between <code>dropout</code> and <code>recurrent_dropout</code>? If <code>recurrent_dropout</code> is dropout between the LSTM cells, that does not make sense to me, because you would be ignoring the previous output, which I thought would defeat the purpose of having an RNN.</li>
<li>Can either of these dropout parameters be substituted with a dropout layer before/after the LSTM layer (i.e. <code>tf.keras.models.sequential([Input(...), LSTM(...), Dropout(0.5)])</code> or <code>tf.keras.models.sequential([Input(...), Dropout(0.5), LSTM(...)])</code> instead of <code>tf.keras.models.sequential([Input(...), LSTM(..., dropout=0.5)])</code>)</li>
</ol></li>
<li><code>implementation</code>

<ol start=""5"">
<li>I understand why this parameter is not in <code>CuDNN</code> layers, since it would probably make it harder to parallelize. However, in <code>LSTM</code>s, does this impact the result (i.e. with the same seed, will <code>implementation=1</code> converge to the same or different result as <code>implementation=2</code>)?</li>
</ol></li>
<li><code>unroll</code></li>
</ul>

<p>I've read a lot about <code>LSTM</code>s, and am at a point where I've decided to start training things, otherwise I won't absorb much more hypothetical knowledge. I've tried a lot of things in modeling, too, but the network I'm training is really simple so nothing seems to impact the results.</p>
",2019-08-08 13:59:50,"<h2><code>activation</code> vs <code>recurrent_activation</code></h2>

<p>If you look at the <a href=""https://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow noreferrer"">LSTM equations</a>. <code>activation</code> (defaults to <code>sigmoid</code>) refers to the activations used for the gates (i.e. input/forget/output), and <code>recurrent_activation</code> (defaults to <code>tanh</code>) refers to the activation used for other things (e.g. the cell output h). </p>

<p>I can explain why the need to two intuitively. For a gate, a range between 0-1 sounds intuitive because a gate can be either on or off or in the middle, but not negative (thus <code>sigmoid</code>). However the cell output, will be more expressive and leads to less saturation as it ranges between -1 and 1 (thus <code>tanh</code>). It might also help with solving vanishing gradient. But I'm not entirely sure about that.</p>

<h2><code>use_bias</code></h2>

<p>If <code>use_bias</code> is True, there will be a <code>+b</code> (e.g. <code>i_t = sigma(x_t Ui + h_t-1 Wi + bi)</code>) in the equations. If not there will be no bias (e.g. <code>i_t = sigma(x_t Ui + h_t-1 Wi)</code>). Personally, I always use a bias.</p>

<h2><code>dropout</code> vs <code>recurrent_dropout</code></h2>

<p>The need for <code>dropout</code> and <code>recurrent_dropout</code> is that, applying dropout on time-dimension can be quite disasterious, as you are influencing the memory of the model. However applying <code>dropout</code> on input data, is pretty much what we do day to day with feed-forward models. So,</p>

<ul>
<li><code>dropout</code>: Applies a dropout mask on the input data (<code>x</code>)</li>
<li><code>recurrent_dropout</code>: Applices a dropout mask on the previous state data (<code>h_t-1</code>)</li>
</ul>

<h2><code>implementation</code></h2>

<p>The implementation gives different ways to compute the same thing. The need for the differences might be the different memory requirements.</p>

<ul>
<li><code>implementation=1</code>

<ul>
<li>Here, the computations are done as if you would have written the following equations. In other words, do those in four steps.

<ul>
<li><code>i_t = sigma(x_t Ui + h_t-1 Wi + bi)</code></li>
<li><code>f_t = sigma(x_t Uf + h_t-1 Wf + bf)</code></li>
<li><code>o_t = sigma(x_t Uo + h_t-1 Wo + bo)</code></li>
<li><code>tilde{c}_t = tanh(x_c Uc + h_t-1 Wc + bc)</code></li>
</ul></li>
</ul></li>
<li><code>implementation=anything else</code>

<ul>
<li>You do the above computations at one go as,

<ul>
<li><code>z = x_t concat(Ui, Uf, Uo, Uc)</code></li>
<li><code>z += h_t-1 concat(Wi, Wf, Wo, Wc)</code></li>
<li><code>z += concat(bi, bf, bo, bc)</code></li>
<li>apply activations</li>
</ul></li>
</ul></li>
</ul>

<p>So the second implementation is much efficiant as there's only two matrix multiplications taking place.</p>

<h2><code>unroll</code></h2>

<p>If true, it will unroll the RNN on the time dimension and do computations without a loop (which will be memory intensive). If false, this will be done with a <code>for</code> loop, which will take longer but less memory intensive.</p>

<p>The source code I referred is found <a href=""https://github.com/tensorflow/tensorflow/blob/e19c354920c3b246dda6598229210a582caaa1a9/tensorflow/python/keras/layers/recurrent.py#L2028"" rel=""nofollow noreferrer"">here</a>. Hope this clarifies it.</p>
",4982425,485,https://stackoverflow.com/questions/57414387,Documentation Replicability
62249084,What is the numpy equivalent of TensorFlow Xavier initializer for CNN?,"<p>I would like to re-create the Xavier initialization in NumPy (using basic functions) in the same way that TensorFlow2 does for CNN. 
Here is how I learned to do Xavier initialization in NumPy:</p>

<pre><code># weights.shape = (2,2)
np.random.seed(0)
nodes_in = 2*2
weights = np.random.rand(2,2) * np.sqrt(1/nodes_in)

&gt;&gt;&gt;array([[0.27440675, 0.35759468],
          [0.30138169, 0.27244159]])
</code></pre>

<p>This is the way I learned Xavier initialization for the logistic regression model. It seems that for Convolution Neural Network it should be different but I don't know how.</p>

<pre><code>initializer = tf.initializers.GlorotUniform(seed=0)
tf.Variable(initializer(shape=[2,2],dtype=tf.float32))

&gt;&gt;&gt;&lt;tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
   array([[-0.7078647 ,  0.50461936],
          [ 0.73500216,  0.6633029 ]], dtype=float32)&gt;
</code></pre>

<p>I'm confused by the TensorFlow <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform"" rel=""noreferrer"">documentation</a> when they explain the ""fan_in"" and ""fan_out"". I'm guessing this is where the problem is. Can somebody dumb it down for me, please? </p>

<p>Much appreciate it!</p>

<p><em>[UPDATE]:</em></p>

<p>When I follow the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform"" rel=""noreferrer"">tf.keras.initializers.GlorotUniform</a> documentation I still don't come to the same results:</p>

<pre><code># weights.shape = (2,2)
np.random.seed(0)
fan_in = 2*2
fan_out = 2*2
limit = np.sqrt(6/(fan_in + fan_out))
np.random.uniform(-limit,limit,size=(2,2))
&gt;&gt;&gt;array([[0.08454747, 0.37271892],
          [0.17799139, 0.07773995]])
</code></pre>
",2020-06-07 17:25:25,"<p><strong>Using Tensorflow</strong></p>

<pre><code>initializer = tf.initializers.GlorotUniform(seed=0)
tf.Variable(initializer(shape=[2,2],dtype=tf.float32))
&lt;tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
array([[-0.7078647 ,  0.50461936],
       [ 0.73500216,  0.6633029 ]], dtype=float32)&gt;
</code></pre>

<p><strong>Same logic in Numpy</strong></p>

<pre><code>import math
np.random.seed(0)
scale = 1/max(1., (2+2)/2.)
limit = math.sqrt(3.0 * scale)
weights = np.random.uniform(-limit, limit, size=(2,2))
print(weights)
array([[0.11956818, 0.52710415],
       [0.25171784, 0.1099409 ]])
</code></pre>

<p>If you observe, the above two are not the same because of random number generators. Internally tensorflow uses the stateless random generator as below and if you observe, we got the same output. </p>

<pre><code>tf.random.stateless_uniform(shape=(2,2),seed=[0, 0], minval=-limit, maxval=limit)
&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[-0.7078647 ,  0.50461936],
       [ 0.73500216,  0.6633029 ]], dtype=float32)&gt;
</code></pre>

<p>If you need to know more about internal implementation, you can check <a href=""https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/init_ops_v2.py#L525"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/init_ops_v2.py#L525</a></p>
",5040482,115,https://stackoverflow.com/questions/62249084,Documentation Replication on Other Examples
68550779,Slicing a 2D tensor similar to numpy np.ix_,"<p>I have learned how to slice a tensor on one dimension <a href=""https://stackoverflow.com/questions/64081367/slicing-a-tensor-with-a-tensor-of-indices-and-tf-gather"">here</a>.</p>
<p>I have learned how to slice a 2D tensor giving a 1D tensor of specific values <a href=""https://stackoverflow.com/questions/38492608/tensorflow-indexing-into-2d-tensor-with-1d-tensor"">here</a>.</p>
<p>Both use <code>tf.gather()</code> but I'm pretty sure I need <code>tf.gather_nd()</code> though I'm obviously using it wrong.</p>
<p>In numpy, I have a 5x5 2D array, and I can slice a 2x2 array by using <code>np.ix_()</code> with row and column indices (I always need the same indices for rows and columns, resulting in a squared matrix):</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

a = np.array([[1,2,3,4,5],[2,1,6,7,8],[3,6,1,9,10],[4,7,9,1,11],[5,8,10,11,1]])

a
</code></pre>
<blockquote>
<pre><code>array([[ 1,  2,  3,  4,  5],
      [ 2,  1,  6,  7,  8],
      [ 3,  6,  1,  9, 10],
      [ 4,  7,  9,  1, 11],
      [ 5,  8, 10, 11,  1]])
</code></pre>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>a[np.ix_([1,3], [1,3])]
</code></pre>
<blockquote>
<pre><code>array([[1, 7],
      [7, 1]])
</code></pre>
</blockquote>
<p>Reading over the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather_nd"" rel=""nofollow noreferrer""><code>tf.gather_nd()</code></a> docs I assumed this is the way to do it in TF, but I'm using it wrong:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

a = tf.constant([[1,2,3,4,5],[2,1,6,7,8],[3,6,1,9,10],[4,7,9,1,11],[5,8,10,11,1]])

tf.gather_nd(a, [[1,3], [1,3]])
</code></pre>
<blockquote>
<pre><code>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([7, 7])&gt;
</code></pre>
</blockquote>
<p>I would have to do something like:</p>
<pre class=""lang-py prettyprint-override""><code>tf.gather_nd(a, [[[1,1], [1,3]],[[3,1],[3,3]]])
</code></pre>
<blockquote>
<pre><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1, 7],
      [7, 1]])&gt;
</code></pre>
</blockquote>
<p>Which leads me down another rabbit hole I'm not keen on. My indices vector is a lot longer of course.</p>
<p>My indices, BTW, are 1D integer tensors themselves. So bottom-line I want to slice <code>a</code> with the same indices for rows and columns as I do with <code>np._ix()</code>, and my indices are something like:</p>
<pre class=""lang-py prettyprint-override""><code>idx = tf.constant([1, 3])

# tf.gather_nd(a, indices = &quot;something with idx&quot;)
</code></pre>
",2021-07-27 19:29:17,"<p>To slice a nxn 2D array with a 1D tensor of length d that results in a dxd 2D array with the specified indices, it can be done by using <code>tf.repeat</code>, <code>tf.tile</code> and then <code>tf.stack</code>:</p>
<pre><code>n = 5
a = tf.constant(np.arange(n * n).reshape(n, n)) # 2D nxn array
idx = [1,2,4] # 1D tensor with length d
d = tf.shape(idx)[0]
ix_ = tf.reshape(tf.stack([tf.repeat(idx,d),tf.tile(idx,[d])],1),[d,d,2])
target = tf.gather_nd(a,ix_) # 2D dxd array
print(a)
print(target)
</code></pre>
<p>Expected outputs:</p>
<pre><code>tf.Tensor(
[[ 0  1  2  3  4]
 [ 5  6  7  8  9]
 [10 11 12 13 14]
 [15 16 17 18 19]
 [20 21 22 23 24]], shape=(5, 5), dtype=int64)
tf.Tensor(
[[ 6  7  9]
 [11 12 14]
 [21 22 24]], shape=(3, 3), dtype=int64)
</code></pre>
",4095235,3557,https://stackoverflow.com/questions/68550779,Documentation Replication on Other Examples
55422537,Testing TF serving model fails with bytes as strings and strings as bytes confusion,"<p>I'm having a problem serving my text classification model on <code>Tensorflow 1.12</code>. I'm using <code>tf.estimator.inputs.pandas_input_fn</code> to read in my data, and <code>tf.estimator.DNNClassifier</code> to train/evaluate. I'd then like to serve my model.
(Apologies in advance, it's tough to provide a full working example here, but it's very much like the example TF provides at <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier</a>  )</p>

<p>I'm currently saving my model with ...</p>

<pre class=""lang-py prettyprint-override""><code>...
estimator.export_savedmodel(""./TEST_SERVING/"", self.serving_input_receiver_fn, strip_default_attrs=True)
...
def serving_input_receiver_fn(self):
      """"""An input receiver that expects a serialized tf.Example.""""""

      # feature spec dictionary  determines our input parameters for the model
      feature_spec = {
          'Headline': tf.VarLenFeature(dtype=tf.string),
          'Description': tf.VarLenFeature(dtype=tf.string)
      }

      # the inputs will be initially fed as strings with data serialized by
      # Google ProtoBuffers
      serialized_tf_example = tf.placeholder(
          dtype=tf.string, shape=None, name='input_example_tensor')
      receiver_tensors = {'examples': serialized_tf_example}

      # deserialize input
      features = tf.parse_example(serialized_tf_example, feature_spec)
      return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)


</code></pre>

<p>This actually fails to run with the error:</p>

<pre class=""lang-sh prettyprint-override""><code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensor'&gt; to Tensor. Contents: SparseTensor(indices=Tensor(""ParseExample/ParseExample:0"", shape=(?, 2), 
dtype=int64), values=Tensor(""ParseExample/ParseExample:2"", shape=(?,), dtype=string), dense_shape=Tensor(""ParseExample/ParseExample:4"", shape=(2,), dtype=int64)). Consider casting elements to a supported type.

</code></pre>

<p>I tried to save a second way doing:</p>

<pre class=""lang-py prettyprint-override""><code>def serving_input_receiver_fn(self):
  """"""Build the serving inputs.""""""
  INPUT_COLUMNS = [""Headline"",""Description""]
  inputs = {}
  for feat in INPUT_COLUMNS:
    inputs[feat] = tf.placeholder(shape=[None], dtype=tf.string, name=feat)
  return tf.estimator.export.ServingInputReceiver(inputs, inputs)
</code></pre>

<p>This actually works, until I try testing it with the <code>saved_model_cli</code>.
Some output for <code>saved_model_cli show --all --dir TEST_SERVING/1553879255/</code>:</p>

<pre class=""lang-sh prettyprint-override""><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['predict']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['Description'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Description:0
    inputs['Headline'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Headline:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['class_ids'] tensor_info:
        dtype: DT_INT64
        shape: (-1, 1)
        name: dnn/head/predictions/ExpandDims:0
    outputs['classes'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: dnn/head/predictions/str_classes:0
    outputs['logits'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/logits/BiasAdd:0
    outputs['probabilities'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/head/predictions/probabilities:0
  Method name is: tensorflow/serving/predict

</code></pre>

<p>But now I can't seem to test it.</p>

<pre class=""lang-sh prettyprint-override""><code>&gt;&gt;&gt; saved_model_cli run --dir TEST_SERVING/1553879255/ --tag_set serve --signature_def predict --input_examples 'inputs=[{""Description"":[""What is going on""],""Headline"":[""Help me""]}]'
Traceback (most recent call last):
 ...
  File ""/Users/Josh/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 489, in _create_example_string
    feature_list)
TypeError: 'What is going on' has type str, but expected one of: bytes

</code></pre>

<p>Ok, lets turn it into a bytes object by changing to <code>b[""What is going on""]</code> and <code>b[""Help me""]</code>...</p>

<pre class=""lang-sh prettyprint-override""><code>ValueError: Type &lt;class 'bytes'&gt; for value b'What is going on' is not supported for tf.train.Feature.
</code></pre>

<p>Any ideas/thoughts??
Thanks!</p>
",2019-03-29 17:18:28,"<p>Ok, so eventually I found the answer, quoted in <a href=""https://stackoverflow.com/questions/51482730/tensorflow-how-to-export-estimator-using-tensorhub-module"">TensorFlow: how to export estimator using TensorHub module?</a> </p>

<p>The problem was with serialization stuff I don't really understand. The solution allows to pass raw strings to <code>tf.estimator.export.build_raw_serving_input_receiver_fn</code> instead.</p>

<p>My saving funciton now looks like this:</p>

<pre class=""lang-py prettyprint-override""><code>  def save_serving_model(self,estimator):
      feature_placeholder = {'Headline': tf.placeholder('string', [1], name='headline_placeholder'),
      'Description': tf.placeholder('string', [1], name='description_placeholder')}
      serving_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholder)

      estimator.export_savedmodel(""TEST_SERVING/"", serving_input_fn)
</code></pre>

<p>where using the <code>saved_model_cli</code> works. I.e.:</p>

<pre class=""lang-sh prettyprint-override""><code>saved_model_cli run --dir /path/to/model/ --tag_set serve --signature_def predict --input_exprs=""Headline=['Finally, it works'];Description=['Yay, it works']"" 

</code></pre>

<pre class=""lang-sh prettyprint-override""><code>Result for output key class_ids:
[[2]]
Result for output key classes:
[[b'2']]
Result for output key logits:
[[-0.56755465  0.31625098  0.39260274]]
Result for output key probabilities:
[[0.16577701 0.40119565 0.4330274 ]]
</code></pre>
",5088987,276,https://stackoverflow.com/questions/55422537,Documentation Replication on Other Examples
58748202,Difference between feature_column.embedding_column and keras.layers.Embedding in TensorFlow,"<p>I have been using <strong>keras.layers.Embedding</strong> for almost all of my projects. But, recently I wanted to fiddle around with tf.data and found <strong>feature_column.embedding_column</strong>.</p>

<p>From the documentation:</p>

<p><strong>feature_column.embedding_column</strong> - 
<code>DenseColumn</code> that converts from sparse, categorical input.
  Use this when your inputs are sparse, but you want to convert them to a dense
  representation (e.g., to feed to a DNN).</p>

<p><strong>keras.layers.Embedding</strong> - Turns positive integers (indexes) into dense vectors of fixed size.
  e.g. <code>[[4], [20]] -&gt; [[0.25, 0.1], [0.6, -0.2]]</code>
  This layer can only be used as the first layer in a model.</p>

<p>My question is, is both of the api doing similar thing on different type of input data(for ex. input - [0,1,2] for keras.layers.Embedding and its one-hot-encoded rep. [[1,0,0],[0,1,0],[0,0,1] for feature_column.embedding_column)?</p>
",2019-11-07 11:52:23,"<p>After reviewing source code for both operations here is what I found:</p>

<ul>
<li>both operations rely on <code>tensorflow.python.ops.embedding_ops</code> funcitonality;</li>
<li>keras.layers.Embedding uses <strong>dense</strong> representations and contains generic keras code for fiddling with shapes, init variables etc;</li>
<li>feature_column.embedding_column relies on <strong>sparse</strong> and contains functionality to cache results.</li>
</ul>

<p>So, your guess seems to be right: these 2 are doing similar things, rely on distinct input representations, contain some logic that doesn't change the essense of what they do.</p>
",4402524,354,https://stackoverflow.com/questions/58748202,Documentation Replication on Other Examples
54945641,"keras, model still setting expected input shape from training despite input_shape(None, ...)","<p>I have a simple CNN model written in the tf.keras framework, which I wish to use with variable input size.</p>

<p>According to <a href=""https://github.com/keras-team/keras/issues/1920#issuecomment-193883690"" rel=""nofollow noreferrer"">this</a> ""documentation"" I can use variable input size by setting <code>input_shape=(None, None, n_channels)</code>, and I have used a <code>GlobalMaxPooling2D</code> layer before my dense layer to standardize the input to the dense layer.</p>

<p>Yet when I train the model with one size of image and try to predict on a different size I get the error:</p>

<pre><code>  File ""multilabel_384.py"", line 180, in main
probabilities = model.predict(test_data)
File ""/usr/local/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1471, in predict
x, check_steps=True, steps_name='steps', steps=steps)
File ""/usr/local/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 868, in _standardize_user_data
exception_prefix='input')
File ""/usr/local/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 191, in standardize_input_data
' but got array with shape ' + str(data_shape))
ValueError: Error when checking input: expected sequential_input to have shape (16, 24, 1) but got array with shape (32, 48, 1)
</code></pre>

<p>This is the code used to define my model:</p>

<pre><code>from tensorflow.keras import layers
import tensorflow as tf

def make_model(num_classes=8):
    # type (int) -&gt; tf.keras.model
    """"""implementation of SimpleNet in keras""""""
    model = tf.keras.Sequential()
    # conv layers
    model.add(layers.ZeroPadding2D(2))
    model.add(layers.Conv2D(input_shape=(None, None, 1),
                            filters=32, kernel_size=5, activation=""relu""))
    model.add(layers.BatchNormalization())
    model.add(layers.ZeroPadding2D(2))
    model.add(layers.Conv2D(filters=64,  kernel_size=5, activation=""relu""))
    model.add(layers.Conv2D(filters=128, kernel_size=3, activation=""relu""))
    model.add(layers.Conv2D(filters=256, kernel_size=3, activation=""relu""))
    model.add(layers.Conv2D(filters=128, kernel_size=3, activation=""relu""))
    model.add(layers.GlobalMaxPooling2D())
    # dense layers
    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation=""relu""))
    model.add(layers.Dropout(0.25))
    model.add(layers.Dense(256, activation=""relu""))
    model.add(layers.Dropout(0.25))
    # use sigmoid for multiclass problems
    model.add(layers.Dense(num_classes, activation=""sigmoid""))
    return model
</code></pre>

<p>So in essence my question is why is keras <em>still</em> defining an expected input shape, and is there any way to disable this implicit <code>standardize_input_data</code> that's going on?</p>
",2019-03-01 13:27:01,"<p>The input shape should be in the first layer of your model, but you are putting it in the second. So Keras is assuming a shape from your training data.</p>
",5445177,1028,https://stackoverflow.com/questions/54945641,Documentation Replicability
61522019,Is it still necessary to implement `compute_output_shape()` when defining a custom tf.keras Layer?,"<p>I have implemented a custom <code>Layer</code> in <code>tf.keras</code>, using TensorFlow 2.1.0.</p>

<p>In the past, when using the stand-alone Keras, it was important to define the <code>compute_output_shape(input_shape)</code> method in any custom layer so that the computational graph could be created. </p>

<p>Now, having moved to TF2, I found out that even if I remove that method from my custom implementation the layer still works as expected. Apparently, it works both in eager and graph mode.
This is an example of what I mean: </p>

<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.layers import Layer, Input
from tensorflow.keras.models import Sequential
import numpy as np


class MyLayer(Layer):
    def call(self, inputs):
        return inputs[:, :-1]  # Do something that changes the shape


m = Sequential([MyLayer(), MyLayer()])
m.predict(np.ones((10, 3)))  # This would not have worked in the past
</code></pre>

<p>Is it safe to say that <code>compute_output_shape()</code> is not necessary anymore? Am I missing something important?</p>

<p>In the documentation there's no explicit mention of removing <code>compute_output_shape()</code>, although none of the examples implements it explicitly. </p>

<p>Thanks</p>
",2020-04-30 11:10:29,"<p>It is not mentioned in the Tensorflow Documentation but in <strong>Chapter 12</strong>,  <strong>Custom Models and Training with TensorFlow</strong> of the book, <strong>Hands-on Machine Learning using Scikit-Learn and Tensorflow (2nd Edition Updated for Tensorflow 2)</strong> of O'REILLY Publications, written by Aurelien Geron, it is mentioned as shown in the screenshot below:</p>
<p><a href=""https://i.stack.imgur.com/OqSfX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OqSfX.png"" alt=""enter image description here"" /></a></p>
<p>To answer your question, yes, it is safe to say <code>compute_output_shape</code> is not needed unless the Layer is Dynamic.</p>
<p>This is evident from this <a href=""https://www.tensorflow.org/tutorials/customization/custom_layers#implementing_custom_layers"" rel=""nofollow noreferrer"">Tensorflow Tutorial on Custom Layer</a> where <code>compute_output_shape</code> is not used.</p>
<p>Hope this helps. Happy Learning!</p>
",5499527,1517,https://stackoverflow.com/questions/61522019,Inadequate Examples
58113387,What can be used to replace tf.train.GradientDescentOptimizer in TensorFlow 2,"<p>I'm trying to convert my TensorFlow 1.4 code to TensorFlow 2 but in there's no more <code>tf.train.GradientDescentOptimizer</code> in TF 2.</p>

<p>What should be the replacement for tf.train.GradientDescentOptimizer?</p>

<p>I found the same thing in <code>tf.compat.v1.train.</code> but I shouldn't use it as those stuff in <code>tf.compat</code> will be removed sooner or later.</p>
",2019-09-26 09:12:29,"<p>You can replace it with, <code>tf.keras.optimizers.SGD()</code> defined <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/SGD"" rel=""noreferrer"">here</a>.
<a href=""https://www.tensorflow.org/beta/guide/migration_guide#keras_optimizers"" rel=""noreferrer"">Here</a> (skip to third point) is the official message, where TF team mentioned to use this keras optimizer. </p>
",5581893,7837,https://stackoverflow.com/questions/58113387,Documentation Completeness
60215970,What's the cleanest and most efficient way to pass two stereo images to a loss function in Keras?,"<p>First off, why am I using Keras? I'm trying to stay as high level as possible, which doesn't mean I'm scared of low-level Tensorflow; I just want to see how far I can go while keeping my code as simple and readable as possible.</p>

<p>I need my Keras model (custom-built using the Keras functional API) to read the left image from a stereo pair and minimize a loss function that needs to access both the right and left images. I want to store the data in a <code>tf.data.Dataset</code>.</p>

<p>What I tried:</p>

<ol>
<li>Reading the dataset as <code>(left image, right image)</code>, i.e. as tensors with shape <code>((W, H, 3), (W, H, 3))</code>, then use function closure: define a <code>keras_loss(left_images)</code> that returns a <code>loss(y_true, y_pred)</code>, with <code>y_true</code> being a <code>tf.Tensor</code> that holds the right image. The problem with this approach is that <code>left_images</code> is a <code>tf.data.Dataset</code> and Tensorflow complains (rightly so) that I'm trying to operate on a dataset instead of a tensor. </li>
<li><p>Reading the dataset as <code>(left image, (left image, right image))</code>, which should make <code>y_true</code> a <code>tf.Tensor</code> with shape <code>((W, H, 3), (W, H, 3))</code> that holds both the right and left images. The problem with this approach is that it...does not work and raises the following error:</p>

<pre><code>ValueError: Error when checking model target: the list of Numpy arrays 
that you are passing to your model is not the size the model expected. 
Expected to see 1 array(s), for inputs ['tf_op_layer_resize/ResizeBilinear'] 
but instead got the following list of 2 arrays: [&lt;tf.Tensor 'args_1:0' 
shape=(None, 512, 256, 3) dtype=float32&gt;, &lt;tf.Tensor 'args_2:0' 
shape=(None, 512, 256, 3) dtype=float32&gt;]...
</code></pre></li>
</ol>

<p>So, is there anything I did not consider? I read the documentation and found nothing about what gets considered as <code>y_pred</code> and what as <code>y_true</code>, nor about how to convert a dataset into a tensor smartly and without loading it all in memory. </p>

<p>My model is designed as such:</p>

<pre><code> def my_model(input_shape):
     width = input_shape[0]
     height = input_shape[1]
     inputs = tf.keras.Input(shape=input_shape)
     # &lt; a few more layers &gt;
     outputs = tf.image.resize(tf.nn.sigmoid(tf.slice(disp6, [0, 0, 0, 0], [-1, -1, -1, 2])), tf.Variable([width, height]))
     model = tf.keras.Model(inputs=inputs, outputs=outputs)
     return model
</code></pre>

<p>And my dataset is built as such (in case 2, while in case 1 only the function <code>read_stereo_pair_from_line()</code> changes):</p>

<pre><code>def read_img_from_file(file_name):
    img = tf.io.read_file(file_name)
    # convert the compressed string to a 3D uint8 tensor
    img = tf.image.decode_png(img, channels=3)
    # Use `convert_image_dtype` to convert to floats in the [0,1] range.
    img = tf.image.convert_image_dtype(img, tf.float32)
    # resize the image to the desired size.
    return tf.image.resize(img, [args.input_width, args.input_height])


def read_stereo_pair_from_line(line):
    split_line = tf.strings.split(line, ' ')
    return read_img_from_file(split_line[0]), (read_img_from_file(split_line[0]), read_img_from_file(split_line[1]))

# Dataset loading
list_ds = tf.data.TextLineDataset('test/files.txt')
images_ds = list_ds.map(lambda x: read_stereo_pair_from_line(x))
images_ds = images_ds.batch(1)
</code></pre>
",2020-02-13 20:29:51,"<p>Solved. I just needed to read the dataset as <code>(left image, [left image, right image])</code> instead of <code>(left image, (left image, right image))</code> i.e. make the second item a list and not a tuple. I can then access the images as <code>input_r = y_true[:, 1, :, :]</code> and <code>input_l = y_true[:, 0, :, :]</code></p>
",5623016,47,https://stackoverflow.com/questions/60215970,Lack of Alternative Solutions/Documentation
61461381,The well-defined dimension of a tf.tensor is inexplicably `None`,"<p>The example below is extracted from <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">the official TensorFlow tutorial</a> on data pipelines. Basically, one resizes a bunch of JPGs to be <code>(128, 128, 3)</code>. For some reason, when applying the <code>map()</code> operation, the colour dimension, namely 3, is turned into a <code>None</code> when examining the shape of the dataset. Why is that third dimension singled out? (I checked to see if there were any images that weren't <code>(128, 128, 3)</code> but didn't fid any.)</p>

<p>If anything, <code>None</code> should only show up for the very first dimension, i.e., that which counts the number of examples, and should not affect the individual dimensions of the examples, since---as nested structures---they're supposed to have the same shape anyway so as to be stored as <code>tf.data.Dataset</code>s.</p>

<p>The code in TensorFlow 2.1 is</p>

<pre><code>import pathlib
import tensorflow as tf

# Download the files.
flowers_root = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar=True)
flowers_root = pathlib.Path(flowers_root)

# Compile the list of files.
list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))

# Reshape the images.
# Reads an image from a file, decodes it into a dense tensor, and resizes it
# to a fixed shape.
def parse_image(filename):
  parts = tf.strings.split(file_path, '\\') # Use the forward slash on Linux
  label = parts[-2]

  image = tf.io.read_file(filename)
  image = tf.image.decode_jpeg(image)
  image = tf.image.convert_image_dtype(image, tf.float32)
  image = tf.image.resize(image, [128, 128])
  print(""Image shape:"", image.shape)
  return image, label

print(""Map the parse_image() on the first image only:"")
file_path = next(iter(list_ds))
image, label = parse_image(file_path)

print(""Map the parse_image() on the whole dataset:"")
images_ds = list_ds.map(parse_image)
</code></pre>

<p>and yields</p>

<pre><code>Map the parse_image() on the first image only:
Image shape: (128, 128, 3)
Map the parse_image() on the whole dataset:
Image shape: (128, 128, None)
</code></pre>

<p>Why <code>None</code> in that last line?</p>
",2020-04-27 14:38:48,"<p>From the tutorial you are missing this part</p>

<pre><code>for image, label in images_ds.take(5):
    show(image, label)
</code></pre>

<p>The line</p>

<pre><code>images_ds = list_ds.map(parse_image)
</code></pre>

<p>only creates a placeholder
and there is no image being passed to the function
if you put prints the file_path is blank
But if your use </p>

<pre><code>for image, label in images_ds.take(5)
</code></pre>

<p>it iterates over each image passing it through the parse_image function.</p>
",5640161,791,https://stackoverflow.com/questions/61461381,Documentation Replication on Other Examples
54929570,Not able to use tf.stop_gradient,"<p>I am currently trying to understand how tf.stop_gradient works and to that end i used this small code snippet</p>

<pre><code>tf.reset_default_graph()
w1 = tf.get_variable(name = 'w1',initializer=tf.constant(10, dtype=tf.float32))
w2 = tf.get_variable(name = 'w2',initializer=tf.constant(3,dtype=tf.float32), trainable=True)
inter = w1*w2
inter=tf.stop_gradient(inter)
loss = w1*w1 - inter  - 10
opt = tf.train.GradientDescentOptimizer(learning_rate = 0.0001)


gradients = opt.compute_gradients(loss)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(gradients))
</code></pre>

<p>Error: TypeError: Fetch argument None has invalid type </p>

<p>If i comment out the line using tf.stop_gradient the code  runs fine and as expected. Please guide me on how to use tf.stop_gradient</p>
",2019-02-28 15:53:11,"<p>You used <code>tf.stop_gradient</code> correctly. However, TensorFlow stops the gradients at <code>inter</code> by removing all graph connections leading to <code>loss</code>. As a result, it will return <code>None</code> if you calculate <code>dLoss/dw2</code> using <code>tf.gradients</code> or <code>opt.compute_gradients</code> because <a href=""https://github.com/tensorflow/tensorflow/issues/783"" rel=""nofollow noreferrer"">[1]</a></p>

<blockquote>
  <p>Returning <code>None</code> makes it explicit that there is no graph connection between the two.</p>
</blockquote>

<p>That's how the <code>TypeError</code> appears (<code>dLoss/dw1</code> does not have this problem).
Many users (including myself) thought this kind of gradient should be <code>0</code> instead of <code>None</code>, but TensorFlow engineers insist that this is intended behavior.</p>

<p>Fortunately there're workarounds, try the codes below:</p>

<pre><code>import tensorflow as tf

w1 = tf.get_variable(name='w1', initializer=tf.constant(10, dtype=tf.float32))
w2 = tf.get_variable(name='w2', initializer=tf.constant(3, dtype=tf.float32))
inter = w1 * w2
inter = tf.stop_gradient(inter)
loss = w1*w1 - inter - 10
dL_dW = tf.gradients(loss, [w1, w2])
# Replace None gradient with 0 manully
dL_dW = [tf.constant(0) if grad is None else grad for grad in dL_dW]

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(dL_dW))
</code></pre>
",6546694,5020,https://stackoverflow.com/questions/54929570,Documentation Replication on Other Examples
57449484,What is trainable parameter in tensorflow?,"<p>tf.compat.v1.layers.batch_normalization takes <code>trainable</code> as an input. The documentation says:</p>

<blockquote>
  <p>Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).</p>
</blockquote>

<p>I think only scaling factor (gamma) and offset (beta) should be added to trainable variables and I am skeptical if even moving averages will get added to GraphKeys.TRAINABLE_VARIABLES. Can somebody tell me how trainable input is influencing the behavior of batch_normalization</p>
",2019-08-11 11:11:50,"<p>First of all, this function is <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">deprecated</a> and should not be used.</p>

<p><code>trainable</code> arguments means that scaling factor (gamma) and offset (beta) will be trainable and it's true by default.</p>

<p>When it comes to moving averages, those <strong>are not trainable</strong>, they are only updated after each batch pass, those are not parameters (<code>tf.Variable</code> objects).</p>

<p>Please notice, you can set <code>trainable</code> to false, in such case, if <code>beta</code> and <code>gamma</code> are set to defaults (zero and one respectively), they won't affect the moving averages. You can turn them off by issuing <code>center</code> (for <code>beta</code>) or <code>scale</code> (for <code>gamma</code>).</p>
",6546694,5020,https://stackoverflow.com/questions/57449484,Documentation Replicability
63140320,How to use sequence/generator on tf.data.Dataset object to fit partial data into memory?,"<p>I am doing image classification with Keras on Google Colab. I load images with the tf.keras.preprocessing.image_dataset_from_directory() function (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory</a>) which returns a tf.data.Dataset object:</p>
<pre><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=1234,
  image_size=(img_height, img_width),
  batch_size=batch_size,
  label_mode=&quot;categorical&quot;)
</code></pre>
<p>I found that when the data contains thousands of images, model.fit() will use all memory after training a number of batches (I am using Google Colab and can see RAM usage grow during the first epoch).
Then I try to use Keras Sequence, which is a suggested solution of loading partial data into RAM (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence</a>):</p>
<pre><code>  class DatasetGenerator(tf.keras.utils.Sequence):
      def __init__(self, dataset):
          self.dataset = dataset

      def __len__(self):
          return tf.data.experimental.cardinality(self.dataset).numpy()

      def __getitem__(self, idx):
          return list(self.dataset.as_numpy_iterator())[idx]
</code></pre>
<p>And I train the model with:</p>
<p><code>history = model.fit(DatasetGenerator(train_ds), ...)</code></p>
<p>The problem is that <strong>getitem</strong>() must return a batch of data with index. However, the list() function I use has to put the whole dataset into RAM and thus hit memory limit when a DatasetGenerator object instantiates (tf.data.Dataset object does not support indexing with []).</p>
<p>My questions:</p>
<ol>
<li>Is there any way to implement <strong>getitem</strong>() (get a specific batch from the dataset object) without putting the whole object into memory?</li>
<li>If item 1 is not possible, is there any workaround?</li>
</ol>
<p>Thanks in advance!</p>
",2020-07-28 18:09:45,"<p>I understand that you are concerned about having your complete dataset in the memory.</p>
<p>Do not worry, the <code>tf.data.Dataset</code> API is very efficient and it does not load your complete dataset in the memory.</p>
<p>Internally it just creates a sequence of functions and when called with <code>model.fit()</code> it will load only the batch in the memory and not the complete dataset.</p>
<p>You can read more in this <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">link</a>, I am pasting the important part from the documentation.</p>
<blockquote>
<p>The tf.data.Dataset API supports writing descriptive and efficient
input pipelines. Dataset usage follows a common pattern:</p>
<p>Create a source dataset from your input data. Apply dataset
transformations to preprocess the data. Iterate over the dataset and
process the elements. Iteration happens in a streaming fashion, so the
full dataset does not need to fit into memory.</p>
</blockquote>
<p>From the last line you can understand that the <code>tf.data.Dataset</code> API does not load the complete dataset in the memory but one batch at a time.</p>
<p>You will have to do the following to create batches of your dataset.</p>
<p><code>train_ds.batch(32)</code></p>
<p>This will create the batch of size <code>32</code>. Also you can use prefetch to prepare one batch berore it heads for training. This removes the bottleneck where the model is idle after training one batch and waiting for another batch.</p>
<p><code>train_ds.batch(32).prefetch(1)</code></p>
<p>You can also use the <code>cache</code> API to make your data pipeline even faster. It will cache your dataset and make the training much faster.</p>
<p><code>train_ds.batch(32).prefetch(1).cache()</code></p>
<p>So to answer in short, you do not need the <code>generator</code> if you are concerned about loading the whole dataset into memory, the <code>tf.data.Dataset</code> API takes care of it.</p>
<p>I hope my answer finds you well.</p>
",5222932,43,https://stackoverflow.com/questions/63140320,Documentation Replication on Other Examples
62348605,How to parse tensor without giving out_type in tensorflow?,"<p>When parsing a serialized tensor, the <code>tf.io.parse_tensor</code> has a required kwarg &quot;out_type&quot;. However it seems that tf does not need this to know the type of the serialized tensor because when one gives a wrong type, it manages to print out the good one.</p>
<p>How could then I parse without this arg?</p>
<p>MWE:</p>
<pre class=""lang-py prettyprint-override""><code>tf.io.parse_tensor(tf.io.serialize_tensor(tf.constant([1])), tf.int32)

tf.io.parse_tensor(tf.io.serialize_tensor(tf.constant([1])))
Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;
TypeError: parse_tensor() missing 1 required positional argument: 'out_type'

tf.io.parse_tensor(tf.io.serialize_tensor(tf.constant([1])), tf.float32)
Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/Users/clementwalter/.pyenv/versions/keras_fsl/lib/python3.6/site-packages/tensorflow/python/ops/gen_parsing_ops.py&quot;, line 2160, in parse_tensor
    _ops.raise_from_not_ok_status(e, name)
  File &quot;/Users/clementwalter/.pyenv/versions/keras_fsl/lib/python3.6/site-packages/tensorflow/python/framework/ops.py&quot;, line 6653, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Type mismatch between parsed tensor (int32) and dtype (float) [Op:ParseTensor]
</code></pre>
",2020-06-12 16:33:53,"<p>EDIT:</p>

<p>I worked out a kind of ""hacky"" solution that can read tensors from different types, casting them to a given type, and works with <code>@tf.function</code> (interestingly, it does <em>not</em> work without <code>@tf.function</code>). The idea is to read the second byte of the <code>TensorProto</code> message, which <em>should</em> indicate the data type, and then make a <code>tf.switch_case</code> to convert from a range of possible source data types. Here is how it could work:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

# Different sets of data types you could use
INTEGER_DTYPES = frozenset({tf.bool, tf.uint8, tf.uint16, tf.uint32, tf.uint64,
                            tf.int8, tf.int16, tf.int32, tf.int64})
FLOAT_DTYPES = frozenset({tf.float16, tf.bfloat16, tf.float32, tf.float64})
COMPLEX_DTYPES = frozenset({tf.complex64, tf.complex128})
REAL_DTYPES = INTEGER_DTYPES | FLOAT_DTYPES
NUMERICAL_DTYPES = REAL_DTYPES | COMPLEX_DTYPES

@tf.function
def parse_tensor_cast(tensor_proto, out_dtype, possible_dtypes=REAL_DTYPES):
    # Prepare branches
    branches = {}
    dtype_idx = [0] * 128
    for i, dtype in enumerate(possible_dtypes):
        dtype_idx[dtype.as_datatype_enum] = i
        branches[i] = lambda: tf.dtypes.cast(
            tf.io.parse_tensor(tensor_proto, dtype), out_dtype)
    dtype_idx = tf.constant(dtype_idx, tf.int32)
    # Extract dtype byte (""hacky"" part of the solution)
    dtype_code = tf.strings.substr(tensor_proto, 1, 1)
    dtype_num = tf.io.decode_raw(dtype_code, tf.uint8)[0]
    dtype_num_idx = dtype_idx[tf.dtypes.cast(dtype_num, tf.int32)]
    # Switch operation
    return tf.switch_case(dtype_num_idx, branches)

# Test
serialized_tensors = [
    tf.io.serialize_tensor(tf.constant([1, 2, 3], tf.int32)),
    tf.io.serialize_tensor(tf.constant([1, 2, 3], tf.float64))
]
for t in serialized_tensors:
    tf.print(parse_tensor_cast(t, tf.float32))
# [1 2 3]
# [1 2 3]
</code></pre>

<hr>

<p>Unfortunately you cannot skip this argument. If there was only eager mode it wouldn't be necessary, but if you want to ""graph"" this operation (e.g. within a <code>@tf.function</code>) then the data type needs to be known in advance, before the actual parsing happens.</p>

<p>If you are only interested in eager mode, then it is not difficult to work around this:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf

input_tensor = tf.constant([1, 2, 3], tf.int32)

# A scalar tf.string tensor containing the serialized input_tensor
serialized_tensor = tf.io.serialize_tensor(input_tensor)

# Create a TensorProto from serialized_tensor content
tensor_proto = tf.core.framework.tensor_pb2.TensorProto()
tensor_proto.ParseFromString(serialized_tensor.numpy())

# At this point, this is equivalent to tf.make_tensor_proto
tf.make_tensor_proto(input_tensor)

# Read data back from tensor_proto
tensor_parsed = tf.io.parse_tensor(serialized_tensor.numpy(),
                                   tf.dtypes.as_dtype(tensor_proto.dtype))
tf.debugging.assert_equal(input_tensor, tensor_parsed)

# You can also just directly create the tensor from the extracted message
numpy_parsed = tf.make_ndarray(tensor_proto)
np.testing.assert_array_equal(input_tensor.numpy(), numpy_parsed)
</code></pre>
",4444546,4964,https://stackoverflow.com/questions/62348605,Documentation Replicability
65649660,Combine arbitrary shaped tensors,"<p>I'd like to combine two variable length tensors.</p>
<p>Since they don't match in shape I can't use tf.concat or tf.stack.</p>
<p>So I thought I'd flatten one and then append it to each element of the other - but I don't see how to do that.</p>
<p>For example,</p>
<pre><code>a = [ [1,2], [3,4] ]
flat_b = [5, 6]

combine(a, flat_b) would be [ [ [1,5,6], [2,5,6] ],
                              [ [3,5,6], [4,5,6] ] ]
</code></pre>
<p>Is there a method like this?</p>
",2021-01-10 02:33:09,"<p>Using <code>tf.map_fn</code> with <code>tf.concat</code>, Example code:</p>
<pre><code>import tensorflow as tf

a = tf.constant([ [1,2], [3,4] ])
flat_b = [5, 6]
flat_a = tf.reshape(a, (tf.reduce_prod(a.shape).numpy(), ))[:, tf.newaxis]
print(flat_a)
c = tf.map_fn(fn=lambda t: tf.concat([t, flat_b], axis=0), elems=flat_a)
c = tf.reshape(c, (-1, a.shape[1], c.shape[1]))
print(c)
</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor(
[[1]
 [2]
 [3]
 [4]], shape=(4, 1), dtype=int32)
tf.Tensor(
[[[1 5 6]
  [2 5 6]]

 [[3 5 6]
  [4 5 6]]], shape=(2, 2, 3), dtype=int32)
</code></pre>
",5239473,308,https://stackoverflow.com/questions/65649660,Inadequate Examples
55703097,Training while loop in Tensorflow,"<p>I've attempted converting a Python-side training loop to Tensorflow to (hypothetically) make the code run faster - not having to pass control over to cpu constantly. However, I can't manage using <code>tf.while_loop</code>.</p>

<p>Here's the code that works:</p>

<pre><code>import numpy as np
import tensorflow as tf

from tqdm import tqdm
from sklearn.datasets import load_iris
from sklearn.preprocessing import RobustScaler

x, y = load_iris(True)
x = RobustScaler().fit_transform(x)

shape = (10, 10)
max_epochs = 1000


graph = tf.Graph()
sess = tf.Session(graph=graph)

x = x.astype(np.float64)


# Construct graph
with graph.as_default():
    weights = tf.get_variable(
        'weights', shape, initializer=tf.constant_initializer, dtype=tf.float64
    )
    curr_epoch = tf.placeholder(dtype=tf.int64, shape=())

    with tf.name_scope('data'):
        data = tf.data.Dataset.from_tensor_slices(x)
        data = data.shuffle(buffer_size=10000)
        data = data.repeat(max_epochs)
        data = data.batch(1)
        data = data.make_one_shot_iterator().get_next()

    with tf.name_scope('update'):
        update_op = make_update_op(weights)

    init = tf.global_variables_initializer()


sess.run(init)

for i in tqdm(range(max_epochs)):
    for _ in range(x.shape[0]):
        sess.run(update_op, feed_dict={
            curr_epoch: i
        })

np_weights = sess.run(weights)
print(np_weights) # Correctly prints an array of 150's.
</code></pre>

<p>Now, if I create an update function to pass <code>tf.while_loop</code>, an error is thrown.</p>

<pre><code>def make_update_op(w):
    return w.assign(
        w + 0.001
    )

# In the code above:
update_op = tf.while_loop(lambda _: True, make_update_op, (weights,), maximum_iterations=x.shape[0])

# No inner loop:
for i in tqdm(range(max_epochs)):
    sess.run(update_op, feed_dict={
        curr_epoch: i
    })
</code></pre>

<blockquote>
  <p>Line 22, in make_update_op
      <code>return w.assign(</code>
  AttributeError: 'Tensor' object has no attribute 'assign'</p>
</blockquote>

<p>I don't quite understand what is happening even after reading the documentation. <code>weights</code> is a <code>Variable</code> after all. What could be done to correctly make the training loop?</p>
",2019-04-16 07:48:26,"<p>Turns out, all that was missing was the fact that one cannot assign to a variable inside a loop as <a href=""https://stackoverflow.com/a/55707514/7089239"">Vlad pointed out</a>. Instead, one can return the new value of a variable.</p>

<pre><code>def make_update_op(w):
    return w + 0.001

new_w = tf.while_loop(lambda _: True, make_update_op, (weights,), maximum_iterations=x.shape[0])
update_op = weights.assign(new_w)
</code></pre>

<p>To use more variables one would need to return the same amount from the function and unpack them in Python, but the principle is the same.</p>

<pre><code>def make_update_op(w, d):
    return w + 0.001, d

new_w, _ = tf.while_loop(lambda *_: True, make_update_op, (weights, data), maximum_iterations=x.shape[0])
update_op = weights.assign(new_w)
</code></pre>
",7089239,2588,https://stackoverflow.com/questions/55703097,Documentation Replication on Other Examples
62411242,tf.keras.backend.clip not giving correct results,"<p><code>tf.keras.backend.clip</code> is not clipping the tensors</p>

<p>When I use <code>tf.keras.backend.clip</code> inside this function</p>

<pre><code>def grads_ds(model_ds, ds_inputs,y_true,cw):
    print(y_true)
    with tf.GradientTape() as ds_tape:
        y_pred = model_ds(ds_inputs)
        print(y_pred.numpy())
        logits_1 = -1*y_true*K.log(y_pred)*cw[:,0]
        logits_0 = -1*(1-y_true)*K.log(1-y_pred)*cw[:,1]
        loss = logits_1 + logits_0
        loss_value_ds = K.sum(loss)

    ds_grads = ds_tape.gradient(loss_value_ds,model_ds.trainable_variables,unconnected_gradients=tf.UnconnectedGradients.NONE)
    for g in ds_grads:
        g = tf.keras.backend.clip(g,min_grad,max_grad)
    return loss_value_ds, ds_grads
</code></pre>

<p>THe value of the gradients remain the same (unclipped).</p>

<p>When I use <code>tf.keras.backend.clip</code> inside the custom training loop, same way</p>

<pre><code>for g in ds_grads:
    g = tf.keras.backend.clip(g,min_grad,max_grad)
</code></pre>

<p>it doesn't work. The gradient applied to the variables are not clipped.</p>

<p>However, if I print <code>g</code> within the loop, then it shows the clipped value.</p>

<p>Can't understand where the problem is.</p>
",2020-06-16 14:45:04,"<p>This is because the <code>g</code> in your example is a reference to the value in the list. When you assign to it you are merely changing the value to which it points (ie you are not modifying the current value it points to).
Consider this example, I want to set all the values in <code>lst</code> to 5. Guess what happens when you run this code sample?</p>
<pre class=""lang-py prettyprint-override""><code>lst = [1,2,3,4]
for ele in lst:
    ele = 5
print(lst)
</code></pre>
<p>Nothing! You get the exact same list back. However within the loop you will see that ele is now 5, as you have already found out in your case. This was the case where the values in the list are immutable (tensors are immutable).</p>
<p>However, you can modify mutable objects in place:</p>
<pre class=""lang-py prettyprint-override""><code>lst = [[2], [2], [2]]
for ele in lst:
    ele.append(3)
print(lst)
</code></pre>
<p>The above code will make each element <code>[2, 3]</code> as expected.</p>
<p>One way of solving your problem is:</p>
<pre><code>lst = [1,2,3,4]
for itr in range(len(lst)):
    lst[itr] = 5
print(lst)
</code></pre>
",5337505,1166,https://stackoverflow.com/questions/62411242,Documentation Replicability
62670041,batch_size in tf model.fit() vs. batch_size in tf.data.Dataset,"<p>I have a large dataset that can fit in host memory. However, when I use tf.keras to train the model, it yields GPU out-of-memory problem. Then I look into tf.data.Dataset and want to use its batch() method to batch the training dataset so that it can execute the model.fit() in GPU. According to its documentation, an example is as follows:</p>
<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))

BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
test_dataset = test_dataset.batch(BATCH_SIZE)
</code></pre>
<p>Is the BATCH_SIZE in dataset.from_tensor_slices().batch() the same as the batch_size in the tf.keras modelt.fit()?</p>
<p>How should I choose BATCH_SIZE so that GPU has sufficient data to run efficiently and yet its memory is not overflown?</p>
",2020-07-01 05:04:38,"<p>You do not need to pass the <code>batch_size</code> parameter in <code>model.fit()</code> in this case. It will automatically use the BATCH_SIZE that you use in <code>tf.data.Dataset().batch()</code>.</p>
<p>As for your other question : the batch size hyperparameter indeed needs to be carefully tuned. On the other hand, if you see OOM errors, you should decrease it until you do not get OOM (normally (but not necessarily) in this manner 32 --&gt; 16 --&gt; 8 ...). In fact you can try non-power of two batch sizes for the decrease purposes.</p>
<p>In your case I would start with a batch_size of 2 an increase it gradually (<code>3-4-5-6...</code>).</p>
<p>You do not need to provide the <code>batch_size</code> parameter if you use the <code>tf.data.Dataset().batch()</code> method.</p>
<p>In fact, even the official <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">documentation</a> states this:</p>
<blockquote>
<p>batch_size : Integer or None. Number of samples per gradient update.
If unspecified, batch_size will default to 32. Do not specify the
batch_size if your data is in the form of datasets, generators, or
keras.utils.Sequence instances (since they generate batches).</p>
</blockquote>
",6227592,1185,https://stackoverflow.com/questions/62670041,Documentation Replication on Other Examples
67882420,How tf.keras.preprocessing.image_dataset_from_directory display output to console,"<p>I am using the below function to read images from a directory</p>
<pre><code>   train_ds = tf.keras.preprocessing.image_dataset_from_directory(directory=image_dataset_path,
                                                                   validation_split=0.2,
                                                                    subset='training',
                                                                    batch_size=32,
                                                                    color_mode='rgb',
                                                                    seed=1)
</code></pre>
<p>and it display below text in the output</p>
<pre><code>Found 284 files belonging to 5 classes.
Using 228 files for training.
</code></pre>
<p>After exploring the above function <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/preprocessing/image_dataset.py"" rel=""nofollow noreferrer"">here</a> I am not able to find out how it is displaying the text in the console. One thing that I noticed is that the output of the function is a dataset but how does it generate the text in console?</p>
<p>Please help me understand How tf.keras is showing this output in the console. What is the exact code behind this?</p>
",2021-06-08 06:25:04,"<p>Those two statements are the result of two helper functions used by <code>tf.keras.preprocessing.image_dataset_from_directory</code>.</p>
<p>See the relevant part of those functions below:</p>
<ul>
<li><p><a href=""https://github.com/tensorflow/tensorflow/blob/401dc1b3db4e7412f4fc50f96c25fd564d593f4c/tensorflow/python/keras/preprocessing/dataset_utils.py#L28"" rel=""nofollow noreferrer"">dataset_utils.index_directory</a>.</p>
<pre><code>if labels is None:
 print('Found %d files.' % (len(filenames),))
 else:
   print('Found %d files belonging to %d classes.' %
       (len(filenames), len(class_names)))
</code></pre>
</li>
<li><p><a href=""https://github.com/tensorflow/tensorflow/blob/401dc1b3db4e7412f4fc50f96c25fd564d593f4c/tensorflow/python/keras/preprocessing/dataset_utils.py#L166"" rel=""nofollow noreferrer"">dataset_utils.get_training_or_validation_split</a></p>
<pre><code>if subset == 'training':
    print('Using %d files for training.' % (len(samples) - num_val_samples,))
    samples = samples[:-num_val_samples]
    labels = labels[:-num_val_samples]
</code></pre>
</li>
</ul>
<p>Those two helper functions simply print those messages in the standard output as a way to provide information to the developer.</p>
",6244166,888,https://stackoverflow.com/questions/67882420,Documentation Replication on Other Examples
59531864,Why does TensorFlow calculate 2D convolutions when 1D convolution is called?,"<p>In the documentation of tf.nn.conv1d, it is stated that</p>

<blockquote>
  <p>Internally, this op reshapes the input tensors and invokes tf.nn.conv2d. For example, if data_format does not start with ""NC"", a tensor of shape [batch, in_width, in_channels] is reshaped to [batch, 1, in_width, in_channels], and the filter is reshaped to [1, filter_width, in_channels, out_channels]. The result is then reshaped back to [batch, out_width, out_channels] (where out_width is a function of the stride and padding as in conv2d) and returned to the caller.</p>
</blockquote>

<p>I get that the operations are equivalent, but I am a bit confused about the implications of this implementation detail. </p>

<p>Does the reshaping create some computational overhead? 
The 3D convolution has its own implementation, so why not the 1D convolution?</p>

<p>Thanks for any explanation that helps me and others to understand this implementation detail of TensorFlow!</p>
",2019-12-30 13:42:28,"<p>Digging through the source code, I conclude that it's likely done for convenience and minimalism of implementation - details below. </p>

<p>First, there is no ""reshaping"", only expanding, squeezing, and re-ordering dims, which bears a tiny overhead; no array elements are actually being moved in memory - only the tensor object's indexing specifiers are changed.</p>

<p>Second, all <code>conv</code> ultimately route to <a href=""https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/nn_ops.py#L942"" rel=""nofollow noreferrer""><code>tf.nn_ops.convolution_internal</code></a>, which then routes to either <code>gen_nn_ops.conv2d</code> or <code>gen_nn_ops.conv3d</code>; a <code>conv1d</code> does not exist in <code>gen_nn_ops.py</code>. Note that for some reason, you won't find that file in the Git respository - but it should be in your local install, <code>/python/ops/gen_nn_ops.py</code>.</p>

<p>Lastly, to get a real answer on why there isn't a dedicated <code>conv1d</code> implementation, you'll need to ask the cuDNN developers behind the convolution algorithms found in <code>gen_nn_ops.py</code>; it's possible that they found no performance improvements, and that <code>conv2d</code> works just as fast. From a low-level standpoint, this makes sense, as the number of matrix multiplications in sliding a kernel with <code>N x 1</code> elements along an <code>M x 1</code> input is identical to that of <code>N</code> along <code>M</code> - again, the only difference is in indexing.</p>

<p>Unfortunately devs decided to encapsulate the ultimate call, that is to <code>_pywrap_tensorflow_internal.TFE_Py_FastPathExecute</code>; the module consists of a <code>.lib</code> and a <code>.pyd</code> file - basically, compiled C (Cython) code that requires disassembly for introspection. </p>

<hr>

<p>TL;DR (1) the ""reshaping"" has a trivial overhead; (2) lack of a dedicated <code>conv1d</code> implementation is likely per sparing redundancy as <code>conv2d</code> is just as fast; (3) I'm not a cuDNN expert, so if you need to be sure, better ask over at <a href=""https://developer.nvidia.com/cudnn"" rel=""nofollow noreferrer"">cuDNN</a>, or read their <a href=""https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html"" rel=""nofollow noreferrer"">SDK Documentation</a>. Alternatively, a dev at <a href=""https://github.com/tensorflow/tensorflow/issues"" rel=""nofollow noreferrer"">TF Github</a> may help. I haven't seen cuDNN devs answer on SO for years now, so posting here may not be the best bet.</p>

<hr>

<p><strong>Dim reordering performance demo</strong>:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
from time import time

x = np.random.randn(700, 800, 900) # 504,000,000 elements

t0 = time()
for i in range(1000):
    if i % 2 == 0:
        x = x.reshape(700, 900, 800)
    else:
        x = x.reshape(700, 800, 900)
print(time() - t0)
</code></pre>

<pre><code>0.0009968280792236328
</code></pre>
",6251391,1056,https://stackoverflow.com/questions/59531864,Documentation Replicability
58176215,Tensorflow difference between tf.stop_gradient and feed variables to optimizer?,"<p>I'm trying to train a model in <strong>self-supervised learning</strong>. The flow chart is something like the following:
<a href=""https://i.stack.imgur.com/g2yMn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g2yMn.jpg"" alt=""enter image description here""></a></p>

<p>Let's assume that <code>N1</code> is already trained and we want to train just <code>N2</code>. This is my current implementation:</p>

<pre><code>x_1 = tf.placeholder(tf.float32, [None, 128, 128, 1])
x_2 = tf.placeholder(tf.float32, [None, 128, 128, 1])

s_t1 = tf.stop_gradient(N1(x_1))  # treat s_t1 as a constant
s_t2_pred = N2(s_t1)) 
s_t2 = tf.stop_gradient(N1(x_2))  # treat s_t2 as a constant

loss = some_loss_function(s_t2, s_t2_pred)
train_op = tf.train.AdamOptimizer(lr).minimize(loss)
</code></pre>

<p>In this way, I should be optimizing only <code>N2</code>. What makes me confused is the fact that if I were to use the following code I would obtain very different results (much better than the above): </p>

<pre><code># treat everything as a variable:
s_t1 = N1(x_1)
s_t2_pred = N2(s_t1)
s_t2 = N1(x_2)

loss = some_loss_function(s_t2, s_t2_pred)
var_list = take_all_variables_in_N2()
train_op = tf.train.AdamOptimizer(lr).minimize(loss, var_list)
</code></pre>

<p>I wonder what is the problem with the first implementation. What is exactly the behaviour of <code>tf.stop_gradient</code> (the documentation is a bit poor)? How does this differ from the second approach?</p>

<p>From a practical perspective in semi-supervised learning: what is the difference between the two? Which one is the correct approach?</p>

<p>Thank you :) </p>

<hr>

<hr>

<p><strong>I added a possible solution to the problem in the comments below. I would still be happy to receive any feedback from more experienced users and to share some opinions on the best approach to structure a self-supervised learning problem in tensorflow.</strong></p>

<p>Bye, G.</p>
",2019-09-30 22:58:10,"<p>I found a possible solution to my question and I'm posting it here, in case someone may find it useful.</p>

<p>Apparently, <code>tf.stop_gradients()</code> only stops the new gradients to be back-propagated through the layers, but: if we have a momentum term (e.g. when using Adam or RMSProp) the variables of such layers could still be updated due to some gradients cumulated in the past (contained in the momentum term). Let's have a look at the simple case of SGD + Momentum; the formula would be:</p>

<pre><code>w1 = w0 - a*grad(loss) - b*v0
</code></pre>

<p>where <code>w1</code> and <code>w0</code> are the weights at time 0 and 1, <code>a</code> is the learning rate <code>v0</code> is the accumulated velocity (a function of the past gradients). Using <code>tf.stop_gradients()</code> is equivalent to multiplying the second term for zero. Then, the update rule becomes:</p>

<pre><code>w1 = w0 - b*v0
</code></pre>

<p>e.g. we still have a momentum component that can update the weights. </p>

<p>A workaround to this problem would be to explicitly passing the variables to be updated to the optimizer. For example:</p>

<pre><code>var_list = take_all_variables_in_N2()
train_op = tf.train.AdamOptimizer(lr).minimize(loss, var_list)
</code></pre>

<hr>

<p><strong>References:</strong></p>

<p>[1] <a href=""http://ruder.io/optimizing-gradient-descent/"" rel=""nofollow noreferrer"">http://ruder.io/optimizing-gradient-descent/</a></p>

<p>[2] <a href=""https://stackoverflow.com/questions/51158399/using-stop-gradient-with-adamoptimizer-in-tensorflow"">Using stop_gradient with AdamOptimizer in TensorFlow</a> </p>
",6470174,809,https://stackoverflow.com/questions/58176215,Documentation Replication on Other Examples
55627995,Keras2 ImageDataGenerator or TensorFlow tf.data?,"<p>With Keras2 being implemented into TensorFlow and TensorFlow 2.0 on the horizon, should you use Keras <code>ImageDataGenerator</code>  with e.g, <code>flow_from_directory</code> or <code>tf.data</code> from TensorFlow which also can be used with <code>fit_genearator</code> of Keras now? </p>

<p>Will both methods will have their place by serving a different purpose or will <code>tf.data</code> be the new way to go and Keras generators deprecated in the future? </p>

<p>Thanks, I would like to take the path which keeps me up to date a bit longer in this fast moving field. </p>
",2019-04-11 08:29:24,"<h3>Update 2022</h3>
<p>On visiting the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"" rel=""nofollow noreferrer""><code>ImageDataGenerator</code> documentation</a>, there is now a deprecation message that says the following:</p>
<blockquote>
<p><strong>Deprecated:</strong> <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"" rel=""nofollow noreferrer""><code>tf.keras.preprocessing.image.ImageDataGenerator</code></a> is not recommended for new code. Prefer loading images with <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"" rel=""nofollow noreferrer""><code>tf.keras.utils.image_dataset_from_directory</code></a> and transforming the output <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code></a> with preprocessing layers. For more information, see the tutorials for <a href=""https://www.tensorflow.org/tutorials/load_data/images"" rel=""nofollow noreferrer"">loading images</a> and <a href=""https://www.tensorflow.org/tutorials/images/data_augmentation"" rel=""nofollow noreferrer"">augmenting images</a>, as well as the <a href=""https://www.tensorflow.org/guide/keras/preprocessing_layers"" rel=""nofollow noreferrer"">preprocessing layer guide</a>.</p>
</blockquote>
",6510273,2039,https://stackoverflow.com/questions/55627995,Documentation Replicability
60469970,How does tf.function compile a python function with autograph?,"<p>How does <code>tf.function</code> compile a python function operating on tensors into a graph, especially wrt autograph? The <a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""nofollow noreferrer"">docs</a> don't go into detail</p>

<blockquote>
  <p><code>tf.function</code> constructs a callable that executes a TensorFlow graph (<code>tf.Graph</code>) created by trace-compiling the TensorFlow operations in <code>func</code>, effectively executing <code>func</code> as a TensorFlow graph.</p>
</blockquote>

<p>Does it use the special methods called by conditionals (<code>__bool__</code>) and loops (<code>__iter__</code>) to 'trace' the function's implementation? For example</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

@tf.function
def op(t: tf.Tensor) -&gt; tf.Tensor:
    if tf.reduce_sum(t) == 0:
        for _ in t:
            ...
</code></pre>

<p>could use the fact that the <code>if</code> results in <code>Tensor.__bool__(...)</code> and <code>for _ in t</code> results in <code>Tensor.__iter__(...)</code></p>
",2020-02-29 21:57:46,"<p>Well - it's more complicated than that. <code>autograph</code> is an entire ""subpackage"" on its own, so a lot happens under the hood when you call <code>@tf.function</code>. A nice place to start might be here: <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/autograph/g3doc/reference"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/autograph/g3doc/reference</a>. You could also dive right into the code if you're really curious.</p>
",5986907,6932,https://stackoverflow.com/questions/60469970,Documentation Replicability
61988657,Why does tensorflow.rank always return shape with null value,"<p>Being a beginner to TensorFlow <strong>I couldn't get why does tensorflow.rank always return shape with null value?</strong></p>

<p><strong>This is what I am working on:</strong></p>

<pre><code>import tensorflow as tf
%tensorflow_version 2.x

list_2d = [[1,2,3,4],
             [5,6,7,8],
             [9,10,11,12]
]
tensor_2d = tf.Variable(list_2d)

print(tensor_2d.shape)
print(tf.rank(tensor_2d))
</code></pre>

<p><strong>and the output is</strong> </p>

<pre><code>(3, 4)
tf.Tensor(2, shape=(), dtype=int32)
</code></pre>

<p>So <strong>my question is what is this <code>shape=()</code> from <code>tf.rank</code> output</strong>?</p>

<p>I couldn't get much from here - <a href=""https://www.tensorflow.org/api_docs/python/tf/rank"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/rank</a></p>
",2020-05-24 16:06:16,"<p>It's because the output of <code>tf.rank()</code> is a tensor itself. Instead of returning the rank as an integer, the output is a tensor with a single int32 value that represents the rank of the given input tensor. The shape <code>()</code> is the shape of the output value, not the input tensor (single value tensors have shape <code>()</code>).</p>
",5176364,5151,https://stackoverflow.com/questions/61988657,Documentation Replicability
62962147,TensorFlow - Fashion MNIST Steps Per Epoch,"<p>I'm working with the Kera's Fashion MNIST dataset. When I fit my model, I noticed to complete one epoch it would have to go through 1500 steps.</p>
<pre><code>history = model.fit(x_train, y_train, epochs=30, validation_split=0.2)

Epoch 3/30
1500/1500 [==============================] - 3s 2ms/step - loss: 0.4494 - sparse_categorical_accuracy: 0.8438 - val_loss: 0.4691 - val_sparse_categorical_accuracy: 0.8308
Epoch 4/30
964/1500 [==================&gt;...........] - ETA: 0s - loss: 0.4294 - sparse_categorical_accuracy: 0.8504
</code></pre>
<p>I was looking at the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">docs</a> for the fit function, but couldn't understand why the default steps were set to 1500. I understand when the <code>steps_per_epoch</code> is <code>None</code> the behavior is dependent on the data type of the dataset, but how can I check if the data type is a tensor or tf.data?</p>
",2020-07-17 21:43:59,"<p>The number of steps per epoch is equal to <code>ceil(samples / batch_size)</code>. The default batch size in <code>model.fit</code> is 32 (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">documentation</a>). If the MNIST training data has 60000 samples, then each epoch would take 60000 / 32 = 1875 steps.</p>
<p><code>model.fit</code> also supports splitting your data into training and validation sets. You have done this with <code>validation_split=0.2</code>, so only 80% of the samples are part of the training set (20% are for validation). The new calculation would be 0.8 * 60000 / 32 = 1500. This is why you see 1500 steps per epoch.</p>
",6179818,2563,https://stackoverflow.com/questions/62962147,Documentation Replication on Other Examples
60761888,Chaining custom Keras layers in functional style,"<p>I want to build a model using <code>tf.keras</code>' functional API. My model is quite large, hence I would like to create custom layers by inheriting from <code>tf.keras.layers.Layer</code>. Below is my attempt, inspired by <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_layer_class"" rel=""nofollow noreferrer"">TensorFlow's documentation</a>.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

class Conv2D(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

        input_layer = tf.keras.layers.Input(
            shape=(256, 256, 3)
        )
        self.conv = tf.keras.layers.Conv2D(
            filters=16,
            kernel_size=3,
            strides=(1, 1),
            padding=""same""
        )(input_layer)

    def call(self, inputs):
        return self.conv(inputs)

outer_input_layer = tf.keras.layers.Input(
    shape=(256, 256, 3)
)
x = Conv2D()(outer_input_layer)
</code></pre>

<p>This code crashes with the following error.</p>

<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File ""c:\Users\user\.vscode\extensions\ms-python.python-2020.2.64397\pythonFiles\ptvsd_launcher.py"", line 48, in &lt;module&gt;
    main(ptvsdArgs)
  File ""c:\Users\user\.vscode\extensions\ms-python.python-2020.2.64397\pythonFiles\lib\python\old_ptvsd\ptvsd\__main__.py"", line 432, in main
    run()
  File ""c:\Users\user\.vscode\extensions\ms-python.python-2020.2.64397\pythonFiles\lib\python\old_ptvsd\ptvsd\__main__.py"", line 316, in run_file
    runpy.run_path(target, run_name='__main__')
  File ""C:\Users\user\code\.env\lib\runpy.py"", line 263, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""C:\Users\user\code\.env\lib\runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""C:\Users\user\code\.env\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""c:\Users\user\code\tests.py"", line 23, in &lt;module&gt;
    x = Conv2D()(outer_input_layer)
  File ""C:\Users\user\code\.env\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 773, in __call__        
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""C:\Users\user\code\.env\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 237, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:

    c:\Users\user\code\tests.py:18 call  *
        return self.conv(inputs)
    C:\Users\user\code\.env\lib\site-packages\tensorflow_core\python\autograph\impl\api.py:447 converted_call
        f in m.__dict__.values() for m in (collections, pdb, copy, inspect, re)):
    C:\Users\user\code\.env\lib\site-packages\tensorflow_core\python\autograph\impl\api.py:447 &lt;genexpr&gt;
        f in m.__dict__.values() for m in (collections, pdb, copy, inspect, re)):
    C:\Users\user\code\.env\lib\site-packages\tensorflow_core\python\ops\math_ops.py:1351 tensor_equals
        return gen_math_ops.equal(self, other, incompatible_shape_error=False)
    C:\Users\user\code\.env\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py:3240 equal
        name=name)
    C:\Users\user\code\.env\lib\site-packages\tensorflow_core\python\framework\op_def_library.py:477 _apply_op_helper
        repr(values), type(values).__name__, err))

    TypeError: Expected float32 passed to parameter 'y' of op 'Equal', got 'collections' of type 'str' instead. Error: Expected float32, got 'collections' 
of type 'str' instead.
</code></pre>

<p>What's wrong with my approach?</p>
",2020-03-19 17:06:25,"<p>No custom layer has ""input"" layers. That doesn't make much sense. The input is what you pass to the layer when you call it. </p>

<p>So:</p>

<pre><code>import tensorflow as tf

class ConvBN(tf.keras.layers.Layer):
    def __init__(self, activation, name):
        super().__init__()

        #here you just ""store"" the layers, you don't use them
        #you also store any other property you find necessary for the call
        self.conv = tf.keras.layers.Conv2D(
            filters=16,
            kernel_size=3,
            strides=(1, 1),
            padding=""same"",
            name = name+'_conv'
        )
       self.bn = tf.keras.layers.BatchNormalization(name = name + ""_bn"")
       self.activation = tf.keras.layers.Activation(activation, name = name + ""_act"")

    def call(self, inputs):
        #here you ""use"" the layers with the given input to produce an output
        out = self.conv(inputs)
        out = self.bn(out)
        out = self.activation(out)

        return out
</code></pre>

<p>You could also, if you're not going to use ""the same layer"" more than once, create simpler bloks:</p>

<pre><code>def convGroup(input_tensor, activation, name):
    out = tf.keras.layers.Conv2D(
            filters=16,
            kernel_size=3,
            strides=(1, 1),
            padding=""same"",
            name = name+'_conv'
        )(input_tensor)
    out = tf.keras.layers.BatchNormalization(name = name + ""_bn"")(out)
    out = tf.keras.layers.Activation(activation, name = name + ""_act"")(out)

    return out
</code></pre>
",5248987,1741,https://stackoverflow.com/questions/60761888,Documentation Replication on Other Examples
62028248,What are all the random seeds for TensorFlow V2.x and tf.keras?,"<p>I only use tf.keras in TensorFlow V2.x. What are all the seeds that I can set? I have only found tf.random.set_seed(). Are there other seeds?</p>
",2020-05-26 17:46:10,"<p>Here are the experiments that we have tried. The results for <code>tf.random.set_seed</code> are identical.</p>

<p><strong>Experiment 1  :</strong> <code>tf.random.set_seed(1234)</code> set only once. </p>

<pre><code>import tensorflow as tf

for i in range(5):
  print(""Iteration Number :"", i)
  tf.random.set_seed(1234)
  print(tf.random.uniform([1]))  # generates 'A1'
  print(tf.random.uniform([1]))  # generates 'A2'
  print(tf.random.uniform([1]))  # generates 'A3'
</code></pre>

<p><strong>Output -</strong> Generates same value for every iteration for <code>A1</code>, <code>A2</code> and <code>A3</code>.</p>

<pre><code>Iteration Number : 0
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
Iteration Number : 1
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
Iteration Number : 2
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
Iteration Number : 3
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
Iteration Number : 4
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
</code></pre>

<p>Lets restart the runtime or kernel and verify the result.</p>

<p><strong>Output -</strong> Generates same value for every iteration for <code>A1</code>, <code>A2</code> and <code>A3</code>. And also the results match with the previous run results.</p>

<pre><code>Iteration Number : 0
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
Iteration Number : 1
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
Iteration Number : 2
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
Iteration Number : 3
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
Iteration Number : 4
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.3253647], shape=(1,), dtype=float32)
tf.Tensor([0.59750986], shape=(1,), dtype=float32)
</code></pre>

<p><strong>Experiment 2 :</strong> <code>tf.random.set_seed(1234)</code> set for every operation.</p>

<pre><code>for i in range(5):
  print(""Iteration Number :"", i)
  tf.random.set_seed(1234)
  print(tf.random.uniform([1]))  # generates 'A1'
  tf.random.set_seed(1234)
  print(tf.random.uniform([1]))  # generates 'A1'
  tf.random.set_seed(1234)
  print(tf.random.uniform([1]))  # generates 'A1'
</code></pre>

<p><strong>Output -</strong> All the values are identical as <code>tf.random.set_seed</code> is set for every operation.</p>

<pre><code>Iteration Number : 0
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
Iteration Number : 1
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
Iteration Number : 2
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
Iteration Number : 3
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
Iteration Number : 4
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
tf.Tensor([0.5380393], shape=(1,), dtype=float32)
</code></pre>

<p>The values remain the same even after you restart the kernel.</p>

<p>If you still have any doubts, kindly share the reproducible code with your expectations.</p>

<p>Hope this answers your question. Happy Learning.</p>
",6227592,1185,https://stackoverflow.com/questions/62028248,Lack of Alternative Solutions/Documentation
56047272,Explicit vs implicit type definition in TensorFlow,"<p>I'm just beginning to learn TensorFlow. Quoting from the <a href=""https://www.tensorflow.org/guide/low_level_intro#graph"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Let's build a simple computational graph. The most basic operation is a constant. The Python function that builds the operation takes a tensor value as input. The resulting operation takes no inputs. When run, it outputs the value that was passed to the constructor. We can create two floating point constants a and b as follows:</p>
</blockquote>

<pre><code>a = tf.constant(3.0, dtype=tf.float32)
b = tf.constant(4.0) # also tf.float32 implicitly
total = a + b
print(a)
print(b)
print(total)
</code></pre>

<p>The second constant is implicitly typed as a float32. Is that based on the explicit typing of the first constant? And does that imply that the first <code>dtype</code> is required? <a href=""https://www.tensorflow.org/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">tf.constant documentation</a> would imply that it does not:</p>

<blockquote>
  <p>If the argument dtype is not specified, then the type is inferred from the type of <code>value</code>.</p>
</blockquote>

<p>But then it would be unnecessary to explicitly type the 3.0 constant above.</p>

<p>I'm just looking for some clarification on this, since, like I said, I'm just starting out.</p>
",2019-05-08 18:51:39,"<blockquote>
  <p>But then it would be unnecessary to explicitly type the 3.0 constant
  above.</p>
</blockquote>

<p>Absolutely correct. </p>

<pre><code>a = tf.constant(3.0, dtype=tf.float32)
</code></pre>

<p>is equivalent to:</p>

<pre><code>a = tf.constant(3.0)
</code></pre>

<p>The documentation is just demonstrating the different overloads. We might choose to explicitly provide the type if we want a different numerical precision (or even just to aid human readability) but if you want the default data type TF infers, then it's entirely unnecessary.</p>
",7287543,1821,https://stackoverflow.com/questions/56047272,Documentation Replication on Other Examples
64687375,Get labels from dataset when using tensorflow image_dataset_from_directory,"<p>I wrote a simple CNN using tensorflow (v2.4) + keras in python (v3.8.3). I am trying to optimize the network, and I want more info on what it is failing to predict. I am trying to add a confusion matrix, and I need to feed tensorflow.math.confusion_matrix() the test labels.</p>
<p>My problem is that I cannot figure out how to access the labels from the dataset object created by tf.keras.preprocessing.image_dataset_from_directory()</p>
<p>My images are organized in directories having the label as the name. The documentation says the function returns a tf.data.Dataset object.</p>
<blockquote>
<pre><code>If label_mode is None, it yields float32 tensors of shape (batch_size, image_size[0], image_size[1], num_channels), encoding
</code></pre>
<p>images (see below for rules regarding num_channels).
Otherwise, it yields a tuple (images, labels), where images has shape (batch_size, image_size[0], image_size[1], num_channels), and
labels follows the format described below.</p>
</blockquote>
<p>Here is the code:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers
#import matplotlib.pyplot as plt
import numpy as np
import random

import PIL
import PIL.Image

import os
import pathlib

#load the IMAGES
dataDirectory = '/p/home/username/tensorflow/newBirds'

dataDirectory = pathlib.Path(dataDirectory)
imageCount = len(list(dataDirectory.glob('*/*.jpg')))
print('Image count: {0}\n'.format(imageCount))

#test display an image
# osprey = list(dataDirectory.glob('OSPREY/*'))
# ospreyImage = PIL.Image.open(str(osprey[random.randint(1,100)]))
# ospreyImage.show()

# nFlicker = list(dataDirectory.glob('NORTHERN FLICKER/*'))
# nFlickerImage = PIL.Image.open(str(nFlicker[random.randint(1,100)]))
# nFlickerImage.show()

#set parameters
batchSize = 32
height=224
width=224

(trainData, trainLabels) = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    validation_split=0.2,
    subset='training',
    seed=324893,
    image_size=(height,width),
    batch_size=batchSize)

testData = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    validation_split=0.2,
    subset='validation',
    seed=324893,
    image_size=(height,width),
    batch_size=batchSize)

#class names and sampling a few images
classes = trainData.class_names
testClasses = testData.class_names
#plt.figure(figsize=(10,10))
# for images, labels in trainData.take(1):
#     for i in range(9):
#         ax = plt.subplot(3, 3, i+1)
#         plt.imshow(images[i].numpy().astype(&quot;uint8&quot;))
#         plt.title(classes[labels[i]])
#         plt.axis(&quot;off&quot;)
# plt.show()

#buffer to hold the data in memory for faster performance
autotune = tf.data.experimental.AUTOTUNE
trainData = trainData.cache().shuffle(1000).prefetch(buffer_size=autotune)
testData = testData.cache().prefetch(buffer_size=autotune)

#augment the dataset with zoomed and rotated images
#use convolutional layers to maintain spatial information about the images
#use max pool layers to reduce
#flatten and then apply a dense layer to predict classes
model = tf.keras.Sequential([
    #layers.experimental.preprocessing.RandomFlip('horizontal', input_shape=(height, width, 3)),
    #layers.experimental.preprocessing.RandomRotation(0.1),
    #layers.experimental.preprocessing.RandomZoom(0.1),
    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(height, width, 3)),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(256, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    # layers.Conv2D(512, 3, padding='same', activation='relu'),
    # layers.MaxPooling2D(),
    #layers.Conv2D(1024, 3, padding='same', activation='relu'),
    #layers.MaxPooling2D(),
    #dropout prevents overtraining by not allowing each node to see each datapoint
    #layers.Dropout(0.5),
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dense(len(classes))
    ])

model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.summary()
    
epochs=2
history = model.fit(
    trainData,
    validation_data=testData,
    epochs=epochs
    )

#create confusion matrix
predictions = model.predict_classes(testData)
confusionMatrix = tf.math.confusion_matrix(labels=testClasses, predictions=predictions).numpy()
</code></pre>
<p>I have tried using (foo, foo1) = tf.keras.preprocessing.image_dataset_from_directory(dataDirectory, etc), but I get
(trainData, trainLabels) = tf.keras.preprocessing.image_dataset_from_directory(
ValueError: too many values to unpack (expected 2)</p>
<p>And if I try to return as one variable and then split it as so:</p>
<pre><code>train = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    validation_split=0.2,
    subset='training',
    seed=324893,
    image_size=(height,width),
    batch_size=batchSize)
trainData = train[0]
trainLabels = train[1]
</code></pre>
<p>I get TypeError: 'BatchDataset' object is not subscriptable</p>
<p>I can access the labels via testClasses = testData.class_names, but I get:</p>
<blockquote>
<p>2020-11-03 14:15:14.643300: W
tensorflow/core/framework/op_kernel.cc:1740] OP_REQUIRES failed at
cast_op.cc:121 : Unimplemented: Cast string to int64 is not supported
Traceback (most recent call last):   File &quot;birdFake.py&quot;, line 115, in

confusionMatrix = tf.math.confusion_matrix(labels=testClasses, predictions=predictions).numpy()   File
&quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py&quot;,
line 201, in wrapper
return target(*args, **kwargs)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/confusion_matrix.py&quot;,
line 159, in confusion_matrix
labels = math_ops.cast(labels, dtypes.int64)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py&quot;,
line 201, in wrapper
return target(*args, **kwargs)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py&quot;,
line 966, in cast
x = gen_math_ops.cast(x, base_type, name=name)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py&quot;,
line 1827, in cast
_ops.raise_from_not_ok_status(e, name)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py&quot;,
line 6862, in raise_from_not_ok_status
six.raise_from(core._status_to_exception(e.code, message), None)   File &quot;&quot;, line 3, in raise_from
tensorflow.python.framework.errors_impl.UnimplementedError: Cast
string to int64 is not supported [Op:Cast]</p>
</blockquote>
<p>I am open to any method to get those labels into the confusion matrix. Any ideas as to why what I am doing is not working would also be appreciated.</p>
<p>UPDATE: I tried the method proposed by Alexandre Catalano, and I get the following error</p>
<blockquote>
<p>Traceback (most recent call last):   File &quot;./birdFake.py&quot;, line 118,
in 
labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])   File &quot;&lt;<strong>array_function</strong> internals&gt;&quot;, line 5, in concatenate
ValueError: all the input arrays must have same number of dimensions,
but the array at index 0 has 1 dimension(s) and the array at index 1
has 0 dimension(s)</p>
</blockquote>
<p>I printed the first element of the labels array, and it is zero</p>
",2020-11-04 20:37:05,"<p>If I were you, I'll iterate over the entire testData, I'll save the predictions and labels along the way and I'll build the confusion matrix at the end.</p>
<pre><code>testData = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    seed=324893,
    image_size=(height,width),
    batch_size=32)


predictions = np.array([])
labels =  np.array([])
for x, y in testData:
  predictions = np.concatenate([predictions, model.predict_classes(x)])
  labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])

tf.math.confusion_matrix(labels=labels, predictions=predictions).numpy()
</code></pre>
<p>and the result is</p>
<pre><code>Found 4 files belonging to 2 classes.
array([[2, 0],
       [2, 0]], dtype=int32)
</code></pre>
",6419985,307,https://stackoverflow.com/questions/64687375,Documentation Replication on Other Examples
56231695,When should tf.losses.add_loss() be used in TensorFlow?,"<p>I cannot find an answer to this question in the TensorFlow documentation. I once read that one should add losses from <code>tf.nn</code> functions but it isn't necessary for functions from <code>tf.losses</code>. Therefore:</p>

<p>When should I use <code>tf.losses.add_loss()</code>?</p>

<p>Example:</p>

<pre><code>loss = tf.reduce_mean(tf.nn.sparse_softmax_corss_entropy_with_logits
                       (labels=ground_truth, logits=predictions))

tf.losses.add_loss(loss) &lt;-- when is this required?
</code></pre>

<p>Thank yoou.</p>
",2019-05-21 05:36:59,"<p>One would use this method to register the loss defined by user.</p>

<p>Namely, if you have created a tensor that defines your loss, for example as <code>my_loss = tf.mean(output)</code> you can use this method to add it to loss collection. You might want to do that if you are not tracking all your losses manually. For example if you are using a method like <code>tf.losses.get_total_loss()</code>.</p>

<p>Inside <code>tf.losses.add_loss</code> is very much straightforward:</p>

<pre><code>def add_loss(loss, loss_collection=ops.GraphKeys.LOSSES):
  if loss_collection and not context.executing_eagerly():
    ops.add_to_collection(loss_collection, loss)
</code></pre>
",7353970,10660,https://stackoverflow.com/questions/56231695,Lack of Alternative Solutions/Documentation
55310900,TensorFlow: How to find minimum/maximum coordinates of segmentations in a tensor excluding zeros?,"<p>For the computation of Intersection over Union (IoU) I want to find coordinates of minimum and maximum values (the border pixels) in a segmentation image <code>image_pred</code> that is represented by a float32 3D tensor. In particular, I aim at finding top left and bottom right corner coordinates of objects in an image. The image is entirely comprised of black pixels (value 0.0) except where the object is located, I have color pixels (0.0 &lt; values &lt; 1.0). Here's an example for such a bounding box (in my case, the object is the traffic sign and the environment is blacked out):</p>

<p><a href=""https://i.stack.imgur.com/QU04v.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QU04v.jpg"" alt=""enter image description here""></a></p>

<p>My approach so far is to <code>tf.boolean_mask</code> for setting every pixel to False except for the color pixels:</p>

<pre><code>zeros = tf.zeros_like(image_pred)
mask = tf.greater(image_pred, zeros)
boolean_mask_pred = tf.boolean_mask(image_pred, mask)
</code></pre>

<p>and then use <code>tf.where</code> to find the coordinates of the masked image. To determine the <strong>horizontal and vertical coordinate values</strong> of the top left and bottom right corners of the rectangle, I thought about using <code>tf.recude_max</code> and <code>tf.reduce_min</code>, but since these do not return a single value if I provide an <code>axis</code>, I am unsure if this is the correct function to use. According to the docs, if I do not specify <code>axis</code>, the function will reduce all dimensions which is not what I want either. Which is the correct function to do this? The IoU in the end is a single 1D float value.</p>

<pre><code>coordinates_pred = tf.where(boolean_mask_pred)
x21 = tf.reduce_min(coordinates_pred, axis=1)
y21 = tf.reduce_min(coordinates_pred, axis=0)
x22 = tf.reduce_max(coordinates_pred, axis=1)
y22 = tf.reduce_max(coordinates_pred, axis=0)
</code></pre>
",2019-03-23 05:27:27,"<p>All you need to do is not use <code>tf.boolean_mask</code>. First, I customized a similar picture.</p>

<pre><code>import numpy as np
from matplotlib import pyplot as plt

image = np.zeros(shape=(256,256))
np.random.seed(0)
image[12:76,78:142] = np.random.random_sample(size=(64,64))
plt.imshow(image)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/T6jPR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T6jPR.png"" alt=""enter image description here""></a></p>

<p>Then get its the coordinates of maximum and minimum by tensorflow.</p>

<pre><code>import tensorflow as tf

image_pred = tf.placeholder(shape=(256,256),dtype=tf.float32)
zeros = tf.zeros_like(image_pred)
mask = tf.greater(image_pred, zeros)

coordinates_pred = tf.where(mask)
xy_min = tf.reduce_min(coordinates_pred, axis=0)
xy_max = tf.reduce_max(coordinates_pred, axis=0)

with tf.Session() as sess:
    print(sess.run(xy_min,feed_dict={image_pred:image}))
    print(sess.run(xy_max,feed_dict={image_pred:image}))

[12 78]
[ 75 141]
</code></pre>
",7353970,10660,https://stackoverflow.com/questions/55310900,Documentation Replication on Other Examples
58412668,Hparams plugin with tf.keras (tensorflow 2.0),"<p>I try to follow the example from the <a href=""https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams"" rel=""noreferrer"">tensorflow docs</a> and setup hyperparameter logging. It also mentions that, if you use <code>tf.keras</code>, you can just use the callback <code>hp.KerasCallback(logdir, hparams)</code>. However, if I use the callback I don't get my metrics (only the outcome). </p>
",2019-10-16 11:53:54,"<p>The trick is to define the Hparams config with the path in which TensorBoard saves its validation logs.</p>
<p>So, if your TensorBoard callback is set up as:</p>
<pre><code>log_dir = 'path/to/training-logs'
tensorboard_cb = TensorBoard(log_dir=log_dir)
</code></pre>
<p>Then you should set up Hparams like this:</p>
<pre><code>hparams_dir = os.path.join(log_dir, 'validation')

with tf.summary.create_file_writer(hparams_dir).as_default():
    hp.hparams_config(
        hparams=HPARAMS,
        metrics=[hp.Metric('epoch_accuracy')]  # metric saved by tensorboard_cb
    )

hparams_cb = hp.KerasCallback(
    writer=hparams_dir,
    hparams=HPARAMS
)
</code></pre>
",7441757,4779,https://stackoverflow.com/questions/58412668,Documentation Ambiguity
59998335,Constantly update tf.cond based on bool value,"<p>I am using <code>tf.cond</code> for controlling the flow of the Tensorflow graph. I went through the documentation and was able to implement <code>tf.cond</code> based branching successfully. But my concern is that while the graph is being loaded the value of the <code>bool</code> variable is checked and the branching decision is made at the initialization step itself. Any further changes in the <code>bool</code> is not tracked. Following is the MWE that better describes the problem:</p>

<pre class=""lang-py prettyprint-override""><code>def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    global foo
    if i &gt; 10:
        foo = False
    print(sess.run(x))    
</code></pre>

<p>This prints only <code>32</code>s. </p>

<p>I tried with <code>eager_execution</code> too with the following code:</p>

<pre class=""lang-py prettyprint-override""><code>tf.enable_eager_execution()
def funa():
    return tf.constant(32)

def funb():
    return tf.constant(21)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(x)
</code></pre>

<p>Still the same result.</p>

<p>So my question is how can I write code such that one part of the graph is chosen dynamically, based on the updates to the <code>bool</code> variable (if possible)? Thanks. I am using Tensorflow v1.14.</p>
",2020-01-31 05:24:12,"<p>You can make a placeholder for <code>foo</code> and feed it's value while running the session. Modified code:</p>

<pre><code>import tensorflow as tf

def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
foo_p = tf.placeholder(tf.bool)

sess = tf.Session()

x = tf.cond(foo_p, lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(sess.run(x, {foo_p:foo}))
</code></pre>
",6997665,3238,https://stackoverflow.com/questions/59998335,Documentation Replication on Other Examples
56038372,Does wrapping tf.data.Dataset into tf.function improve performance?,"<p>Given the two examples below, is there a performance improvement when autographing the <code>tf.data.Dataset</code>?</p>

<p>Dataset not in tf.function</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf


class MyModel(tf.keras.Model):

    def call(self, inputs):
        return tf.ones([1, 1]) * inputs


model = MyModel()
model2 = MyModel()


@tf.function
def train_step(data):
    output = model(data)
    output = model2(output)
    return output


dataset = tf.data.Dataset.from_tensors(tf.ones([1, 1]))

for data in dataset:
    train_step(data)
</code></pre>

<p>Dataset in tf.function</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf


class MyModel(tf.keras.Model):

    def call(self, inputs):
        return tf.ones([1, 1]) * inputs


model = MyModel()
model2 = MyModel()


@tf.function
def train():
    dataset = tf.data.Dataset.from_tensors(tf.ones([1, 1]))
    def train_step(data):
        output = model(data)
        output = model2(output)
        return output
    for data in dataset:
        train_step(data)


train()
</code></pre>
",2019-05-08 10:15:26,"<p>Adding <code>@tf.function</code> does add significant speedup. Take a look at this:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

data = tf.random.normal((1000, 10, 10, 1))
dataset = tf.data.Dataset.from_tensors(data).batch(10)

def iterate_1(dataset):
    for x in dataset:
        x = x

@tf.function
def iterate_2(dataset):
    for x in dataset:
        x = x

%timeit -n 1000 iterate_1(dataset) # 1.46 ms  8.2 s per loop
%timeit -n 1000 iterate_2(dataset) # 239 s  10.2 s per loop
</code></pre>

<p>As you can see iterating with <code>@tf.function</code> is more than 6 times faster.</p>
",7128838,405,https://stackoverflow.com/questions/56038372,Documentation Replicability
70424291,How to use tf.gather similar to the numpy slicing,"<p>I have a list of indices representing the rows and columns that I'd like to access.</p>
<pre><code>r= np.array([0, 1, 0, 1, 0, 1, 2, 3, 2, 3, 2, 3])
c = np.array([0, 0, 1, 1, 4, 4, 1, 1, 3, 3, 5, 5])
</code></pre>
<p>When I use them as <code>[r,c]</code>, I am able to obtain the corresponding elements in a <code>numpy</code> array.</p>
<pre><code>arr = np.array([[517.0, 1876.0, 4716.0, 2725.0, 2138.0, 2213.0],
                [517.0, 1876.0, 4716.0, 2725.0, 2138.0, 2213.0],
                [3745.0, 3103.0, 885.0, 3482.0, 4196.0, 1802.0], 
                [3745.0, 3103.0, 885.0, 3482.0, 4196.0, 1802.0],
                [1548.0, 610.0, 3936.0, 905.0, 3791.0, 3657.0], 
                [1548.0, 610.0, 3936.0, 905.0, 3791.0, 3657.0], 
                [971.0, 573.0, 4756.0, 2137.0, 1407.0, 4388.0], 
                [971.0, 573.0, 4756.0, 2137.0, 1407.0, 4388.0], 
                [2786.0, 4769.0, 3391.0, 940.0, 2188.0, 1823.0], 
                [2786.0, 4769.0, 3391.0, 940.0, 2188.0, 1823.0], 
                [3225.0, 3262.0, 3444.0, 783.0, 3931.0, 1546.0], 
                [3225.0, 3262.0, 3444.0, 783.0, 3931.0, 1546.0]])
print(mst_array[r, c])
[ 517.  517. 1876. 1876. 2138. 2138. 3103. 3103. 3482. 3482. 1802. 1802.]
</code></pre>
<p>I would like to perform the same operation in <code>tf</code> and am using <code>tf.gather</code>.</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/gather</a></p>
<pre><code>t_r = tf.convert_to_tensor(r, dtype= tf.int32)
t_c =  tf.convert_to_tensor(c, dtype= tf.int32)
t_arr = tf.convert_to_tensor(arr, dtype= tf.int32)
print(t_r, &quot;\n&quot;, t_c, &quot;\n&quot;)
tf.Tensor([0 1 0 1 0 1 2 3 2 3 2 3], shape=(12,), dtype=int32) 
tf.Tensor([0 0 1 1 4 4 1 1 3 3 5 5], shape=(12,), dtype=int32) 
</code></pre>
<p>I have tried different commands, but could not manage to get the same results as I do in <code>numpy</code>.</p>
<pre><code>tf.gather(t_arr, [t_r, t_c], axis=-1)
tf.gather(t_arr, [t_r, t_c], axis=-1).shape_as_list()
</code></pre>
",2021-12-20 15:40:34,"<p>Try <code>tf.stack</code> with <code>tf.gather_nd</code>:</p>
<pre class=""lang-py prettyprint-override""><code>z = tf.gather_nd(t_arr, tf.stack([t_r, t_c], axis=1))
</code></pre>
<pre><code>tf.Tensor([ 517  517 1876 1876 2138 2138 3103 3103 3482 3482 1802 1802], shape=(12,), dtype=int32)
</code></pre>
",7447976,632,https://stackoverflow.com/questions/70424291,Documentation Replication on Other Examples
55764694,How to use gradient_override_map in Tensorflow 2.0?,"<p>I'm trying to use <code>gradient_override_map</code> with Tensorflow 2.0. There is an <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Graph#gradient_override_map"" rel=""noreferrer"">example in the documentation</a>, which I will use as the example here as well.</p>

<p>In 2.0, <code>GradientTape</code> can be used to compute gradients as follows:</p>

<pre><code>import tensorflow as tf
print(tf.version.VERSION)  # 2.0.0-alpha0

x = tf.Variable(5.0)
with tf.GradientTape() as tape:
    s_1 = tf.square(x)
print(tape.gradient(s_1, x))
</code></pre>

<p>There is also the <code>tf.custom_gradient</code> decorator, which can be used to define the gradient for a <em>new</em> function (again, using the <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/custom_gradient"" rel=""noreferrer"">example from the docs</a>):</p>

<pre><code>import tensorflow as tf
print(tf.version.VERSION)  # 2.0.0-alpha

@tf.custom_gradient
def log1pexp(x):
    e = tf.exp(x)

    def grad(dy):
        return dy * (1 - 1 / (1 + e))

    return tf.math.log(1 + e), grad

x = tf.Variable(100.)

with tf.GradientTape() as tape:
    y = log1pexp(x)

print(tape.gradient(y, x))
</code></pre>

<p>However, I would like to replace the gradient for standard functions such as <code>tf.square</code>. I tried to use the following code:</p>

<pre><code>@tf.RegisterGradient(""CustomSquare"")
def _custom_square_grad(op, grad):
  return tf.constant(0)

with tf.Graph().as_default() as g:
    x = tf.Variable(5.0)
    with g.gradient_override_map({""Square"": ""CustomSquare""}):
        with tf.GradientTape() as tape:
            s_2 = tf.square(x, name=""Square"")

    with tf.compat.v1.Session() as sess:
        sess.run(tf.compat.v1.global_variables_initializer())            
        print(sess.run(tape.gradient(s_2, x)))
</code></pre>

<p>However, there are two issues: The gradient replacement does not seem to work (it is evaluated to <code>10.0</code> instead of <code>0.0</code>) and I need to resort to <code>session.run()</code> to execute the graph. Is there a way to achieve this in ""native"" TensorFlow 2.0?</p>

<p>In TensorFlow 1.12.0, the following produces the desired output:</p>

<pre><code>import tensorflow as tf
print(tf.__version__)  # 1.12.0

@tf.RegisterGradient(""CustomSquare"")
def _custom_square_grad(op, grad):
  return tf.constant(0)

x = tf.Variable(5.0)

g = tf.get_default_graph()
with g.gradient_override_map({""Square"": ""CustomSquare""}):
    s_2 = tf.square(x, name=""Square"")
grad = tf.gradients(s_2, x)

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  print(sess.run(grad))
</code></pre>
",2019-04-19 16:12:29,"<p>There is no built-in mechanism in TensorFlow 2.0 to override all gradients for a built-in operator within a scope. However, if you are able to modify the call-site for each call to the built-in operator, you can use the <code>tf.custom_gradient</code> decorator as follows:</p>

<pre><code>@tf.custom_gradient
def custom_square(x):
  def grad(dy):
    return tf.constant(0.0)
  return tf.square(x), grad

with tf.Graph().as_default() as g:
  x = tf.Variable(5.0)
  with tf.GradientTape() as tape:
    s_2 = custom_square(x)

  with tf.compat.v1.Session() as sess:
    sess.run(tf.compat.v1.global_variables_initializer())            
    print(sess.run(tape.gradient(s_2, x)))
</code></pre>
",7469434,2559,https://stackoverflow.com/questions/55764694,Documentation Replication on Other Examples
58730460,Freeze sublayers in tensorflow 2,"<p>I have a model which is composed of custom layers. Each custom layer contains many tf.keras.layers. The problem is that if I want to freeze those layers after defining my model, the loop:</p>

<pre><code>for i, layer in enumerate(model.layers):
    print(i, layer.name)
</code></pre>

<p>only prints the ""outer"" custom layers and not those who exist inside. Is there any way to access the inner layers so I can freeze them?</p>

<p>an example of a custom layer from the <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models"" rel=""nofollow noreferrer"">official tf docs</a>:</p>

<pre><code>class MLPBlock(layers.Layer):

  def __init__(self):
    super(MLPBlock, self).__init__()
    self.linear_1 = Linear(32)
    self.linear_2 = Linear(32)
    self.linear_3 = Linear(1)

  def call(self, inputs):
    x = self.linear_1(inputs)
    x = tf.nn.relu(x)
    x = self.linear_2(x)
    x = tf.nn.relu(x)
    return self.linear_3(x)
</code></pre>
",2019-11-06 12:42:48,"<p>Ok i came up with a solution.
An ""update"" function must be implemented inside the custom layer, which updates the inner layers so that they become non trainable.
Here is a sample code:</p>

<pre><code>import tensorflow as tf
import numpy as np

layers = tf.keras.layers

seq_model = tf.keras.models.Sequential


class MDBlock(layers.Layer):

    def __init__(self):
        super(MDBlock, self).__init__()
        self.dense1 = layers.Dense(784, name=""first"")
        self.dense2 = layers.Dense(32, name=""second"")
        self.dense3 = layers.Dense(32, name=""third"")
        self.dense4 = layers.Dense(1, activation='sigmoid', name=""outp"")

    def call(self, inputs):
        x = self.dense1(inputs)
        x = tf.nn.relu(x)
        x = self.dense2(x)
        x = tf.nn.relu(x)
        x = self.dense3(x)
        x = tf.nn.relu(x)
        x = self.dense4(x)
        return x

    def updt(self):
        self.dense1.trainable = False

    def __str__(self):
        return ""\nd1:{0}\nd2:{1}\nd3:{2}\nd4:{3}"".format(self.dense1.trainable, self.dense2.trainable,
                                                         self.dense3.trainable, self.dense4.trainable)


# define layer block
layer = MDBlock()

model = seq_model()
model.add(layers.Input(shape=(784,)))
model.add(layer)

# Use updt function to make layers non-trainable
for i, layer in enumerate(model.layers):
    layer.updt()

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Generate dummy data
data = np.random.random((1000, 784))
labels = np.random.randint(2, size=(1000, 1))

# Train the model, iterating on the data in batches of 32 samples
model.fit(data, labels, epochs=10, batch_size=32)

# print block's layers state
for i, layer in enumerate(model.layers):
    print(i, layer)


</code></pre>
",7248145,844,https://stackoverflow.com/questions/58730460,Documentation Replication on Other Examples
56969703,How to use `tf.scatter_nd` with multi-dimensional tensors,"<p>I'm trying to create a new tensor (<code>output</code>) with the values of another tensor (<code>updates</code>) placed according to <code>idx</code> tensor. The shape of <code>output</code> should be <code>[batch_size, 1, 4, 4]</code> (like an image of 2x2 pixels and one channel) and <code>update</code> has shape <code>[batch_size, 3]</code>.</p>

<p>I've read Tensorflow documentation (I'm working with gpu version 1.13.1) and found <code>tf.scatter_nd</code> should work for my problem. The issue is that I cannot make it work, I think I'm having problems understanding how I have to arange <code>idx</code>. </p>

<p>Let's consider <code>batch_size = 2</code>, so what I'm doing is:</p>

<pre class=""lang-py prettyprint-override""><code>updates = tf.constant([[1, 2, 3], [4, 5, 6]])  # shape [2, 3]
output_shape = tf.constant([2, 1, 4, 4])
idx = tf.constant([[[1, 0], [1, 1], [1, 0]], [[0, 0], [0, 1], [0, 2]]])  # shape [2, 3, 2]
idx_expanded = tf.expand_dims(idx, 1)  # so I have shape [2, 1, 3, 2]
output = tf.scatter_nd(idx_expanded, updates, output_shape)
</code></pre>

<p>I expect it to work, but it doesn't, it gives me this error:</p>

<p><code>ValueError: The outer 3 dimensions of indices.shape=[2,1,3,2] must match the outer 3 dimensions of updates.shape=[2,3]: Shapes must be equal rank, but are 3 and 2 for 'ScatterNd_7' (op: 'ScatterNd') with input shapes: [2,1,3,2], [2,3], [4]</code></p>

<p>I don't understand why it's expecting <code>updates</code> to have dimension 3. I thought <code>idx</code> has to make sense with <code>output_shape</code> (that's why I used <code>expand_dims</code>) and also with <code>updates</code> (specify the two indices for the three points), but it's obvious I'm missing something here.</p>

<p>Any help would be appreciated.</p>
",2019-07-10 11:25:01,"<p>I've been playing around with the function and I have found my mistake. If anyone is facing this problem, this is what I did to solve it:</p>

<p>Considering <code>batch_size=2</code> and <code>3</code> points, <code>idx</code> tensor must have shape <code>[2, 3, 4]</code>, where first dimension correspond to the batch from where we are taking <code>update</code>value, second dimension must be equal to the second dimension of <code>updates</code> (number of points per batch) and the third dimension is <code>4</code> because we need <code>4</code> indices: [batch_number, channel, row, col]. Following the example in the question:</p>

<pre class=""lang-py prettyprint-override""><code>updates = tf.constant([[1., 2., 3.], [4., 5., 6.]])  # [2, 3]
idx = tf.constant([[[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 1, 0]], [[1, 0, 1, 1], [1, 0, 0, 0], [1, 0, 1, 0]]])  # [2, 3, 4]
output = tf.scatter_nd(idx, updates, [2, 1, 4, 4])

sess = tf.Session()
print(sess.run(output))

[[[[2. 1. 0. 0.]
   [3. 0. 0. 0.]
   [0. 0. 0. 0.]
   [0. 0. 0. 0.]]]


 [[[5. 0. 0. 0.]
   [6. 4. 0. 0.]
   [0. 0. 0. 0.]
   [0. 0. 0. 0.]]]]

</code></pre>

<p>This way it's possible to place specific numbers in a new tensor.</p>
",7327257,337,https://stackoverflow.com/questions/56969703,Documentation Replication on Other Examples
59493127,Does tf.keras.losses.categorical_crossentropy return an array or a single value?,"<p>I'm using a custom training loop. The loss that is returned by <code>tf.keras.losses.categorical_crossentropy</code> is an array of I'm assuming <code>(1,batch_size)</code>. Is this what it is supposed to return or a single value?</p>

<p>In the latter case, any idea what I could be doing wrong?</p>
",2019-12-26 20:22:54,"<p>If you have a prediction shape of <code>(samples of batch, classes)</code> <code>tf.keras.losses.categorical_crossentropy</code> returns the losses in the shape of <code>(samples of batch,)</code>.</p>

<p>So, if your labels are:</p>

<pre><code>[[1. 0. 0.]
 [0. 1. 0.]
 [0. 0. 1.]]
</code></pre>

<p>And your predictions are:</p>

<pre><code>[[0.9  0.05 0.05]
 [0.5  0.89 0.6 ]
 [0.05 0.01 0.94]]
</code></pre>

<p>You will get a loss like:</p>

<pre><code>[0.10536055 0.8046684  0.06187541]
</code></pre>

<p>In most case your model will use these value's <strong>mean</strong> for the update of your model parameters. So if you manually do the updates you can use:</p>

<pre><code>loss = tf.keras.backend.mean(losses)
</code></pre>
",7694977,403,https://stackoverflow.com/questions/59493127,Documentation Replicability
55560676,How to use tf.while_loop with eager execution?,"<p>In the documentation, the body of a tf.while_loop needs to be a python callable.</p>

<pre><code>i = tf.constant(0)
b = lambda i: tf.add(i,1)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>works but</p>

<pre><code>def b(i):
    tf.add(i,1)

i = tf.constant(0)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>throws a ValueError: Attempt to convert a value (None) with an unsupported type() to a Tensor</p>

<p>In 2.0, eager execution is default, I wonder what's the problem?!</p>
",2019-04-07 15:33:15,"<p>You forgot to add return statement to your function:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

def b(i):
    return tf.add(i, 1)

i = tf.constant(0)
c = lambda i: tf.less(i, 10)
tf.while_loop(c, b, [i]) # &lt;tf.Tensor: id=51, shape=(), dtype=int32, numpy=10&gt;
</code></pre>

<p>Note that in your first example function <code>b</code> does return incremented value:</p>

<pre class=""lang-py prettyprint-override""><code>i = tf.constant(0)
b = lambda i: tf.add(i,1)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
print(b(1).numpy()) # 2
</code></pre>
",7779411,366,https://stackoverflow.com/questions/55560676,Documentation Replicability
64826405,Tensorflow: 'axis' argument in dot product,"<p>Can someone show me the way I should use the <code>axis</code> argument in <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>?</p>
<p>I read the documentation but it was complicated and I'm still confused. I saw <a href=""https://stackoverflow.com/questions/48082900/in-tensorflow-what-is-the-argument-axis-in-the-function-tf-one-hot"">another question</a> that asks about <code>axis</code> in <code>tf.one_hot</code> and in the answers were some good insights about the matter, but that didn't help me with <code>tf.tensordot</code>. I thought you can give me some insights on this too.</p>
<p>For example, I know I can dot product a vector and a tensor like this:</p>
<pre><code>my_vector = tf.random.uniform(shape=[n])
my_tensor = tf.random.uniform(shape=[m, n])

dp = tf.tensordot(my_tensor, my_vector, 1)
</code></pre>
<p>But when I <em><strong>batch</strong></em> them and add one dimension to them to be of the shape <code>(b, n)</code> and <code>(b, m, n)</code> to obtain a <code>(b, m, 1)</code>, now I don't know how to dot product every batch.</p>
",2020-11-13 18:46:00,"<p>The operation that you want to do cannot be done (in an effective way) with <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>. There is, however, a dedicated function for that operation, <a href=""https://www.tensorflow.org/api_docs/python/tf/linalg/matvec"" rel=""nofollow noreferrer""><code>tf.linalg.matvec</code></a>, which will work with batches out of the box. And you can also do the same thing with <a href=""https://www.tensorflow.org/api_docs/python/tf/einsum"" rel=""nofollow noreferrer""><code>tf.einsum</code></a>, like <code>tf.einsum('bmn,bn-&gt;bm', my_tensors, my_vectors)</code>.</p>
<p>With respect to <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>, in general it computes an &quot;all vs all&quot; product of the two given tensors, but matching and reducing some axes. When no axes are given (you have to explicitly pass <code>axes=[[], []]</code> to do this), it creates a tensor with the dimensions of both inputs concatenated. So, if you have <code>my_tensors</code> with shape <code>(b, m, n)</code> and <code>my_vectors</code> with shape <code>(b, n)</code> and you do:</p>
<pre class=""lang-py prettyprint-override""><code>res = tf.tensordot(my_tensors, my_vectors, axes=[[], []])
</code></pre>
<p>You get <code>res</code> with shape <code>(b, m, n, b, n)</code>, such that <code>res[p, q, r, s, t] == my_tensors[p, q, r] * my_vectors[s, t]</code>.</p>
<p>The <code>axes</code> argument is used to specify dimensions in the input tensors that are &quot;matched&quot;. Values along matched axes are multiplied and summed (like a dot product), so those matched dimensions are reduced from the output. <code>axes</code> can take two different forms:</p>
<ul>
<li>If it is a single integer, <code>N</code> then the last <code>N</code> dimensions of the first parameter are matched against the first <code>N</code> dimensions of <code>b</code>. In your example, that corresponds to the dimensions with <code>n</code> elements in <code>my_tensor</code> and <code>my_vector</code>.</li>
<li>If it is a list, it must contain two sublists, <code>axes_a</code> and <code>axes_b</code>, each with the same number <code>N</code> of integers. In this form, you are explicitly indicating which dimensions of the given values are matched. So, in your example, you could have passed <code>axes=[[1], [0]]</code>, which means &quot;match the dimension <code>1</code> of the first parameter (<code>my_tensor</code>) to the dimension <code>0</code> of the second parameter (<code>my_vector</code>)&quot;.</li>
</ul>
<p>If you have now <code>my_tensors</code>  with shape <code>(b, m, n)</code> and <code>my_vectors</code> with shape <code>(b, n)</code>, then you would want to match the dimension <code>2</code> of the first one to the dimension <code>1</code> of the second one, so you could pass <code>axes=[[2], [1]]</code>. However, that will give you a result <code>res</code> with shape <code>(b, m, b)</code> such that <code>res[i, :, j]</code> is the product of matrix <code>my_tensors[i]</code> and vector <code>my_vectors[j]</code>. You could take then only the results that you want (those where <code>i == j</code>), with something more or less convoluted like <code>tf.transpose(tf.linalg.diag_part(tf.transpose(res, [1, 0, 2])))</code>, but you would be doing far more computation than you need to get the same result.</p>
",7339624,3432,https://stackoverflow.com/questions/64826405,Documentation Ambiguity
75371111,"when trying to load external tfrecord with TFDS, given tf.train.Example, how to get tfds.features?","<p><strong>What I need help with / What I was wondering</strong></p>
<p>Hi, I am trying to load external tfrecord files with TFDS. I have read the official doc <a href=""https://www.tensorflow.org/datasets/external_tfrecord"" rel=""nofollow noreferrer"">here</a>, and find I need to define the feature structure using <code>tfds.features</code>. However, since the tfrecords files are alreay generated, I do not have control the generation pipeline. I do, however, know the <code>tf.train.Example</code> structre used in <code>TFRecordWriter</code> during generation, shown as follows.</p>
<pre><code>from tensorflow.python.training.training import BytesList, Example, Feature, Features, Int64List

dict(Example=Features({
'image': Feature(bytes_list=BytesList(value=[img_str])), # img_str is jpg encoded image raw bytes
'caption': Feature(bytes_list=BytesList(value=[caption])), # caption is a string
'height': Feature(bytes_list=Int64List(value=[caption])), 
'width': Feature(bytes_list=Int64List(value=[caption])), 
})
</code></pre>
<p>The doc only describes how to translate <a href=""https://www.tensorflow.org/datasets/api_docs/python/tfds/features"" rel=""nofollow noreferrer"">tfds.features</a> into the human readable structure of the <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Example"" rel=""nofollow noreferrer"">tf.train.Example</a>. But nowhere does it mention how to translate a tf.train.Example into tfds.features, which is needed to automatically add the proper metadata fileswith <a href=""https://www.tensorflow.org/datasets/api_docs/python/tfds/folder_dataset/write_metadata"" rel=""nofollow noreferrer"">tfds.folder_dataset.write_metadata</a>.</p>
<p>I wonder how to translate the above tf.train.Example into tfds.features? Thanks a lot!</p>
<p>BTW, while I understand that it is possible to directly read the data as it is in TFRecord with <code>tf.data.TFRecordDataset</code> and then use <code>map(decode_fn)</code> for decoding as suggested <a href=""https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset"" rel=""nofollow noreferrer"">here</a>, it seems to me this approach lacks necessary metadata like <code>num_shards</code> or <code>shard_lengths</code>. In this case, I am not sure if it is still ok to use common operations like <code>cache/repeat/shuffle/map/batch</code> on that <code>tf.data.TFRecordDataset</code>. So I think it is better to stick to the tfds approach.</p>
<p><strong>What I've tried so far</strong></p>
<p>I have searched the official doc for quite some time but cannot find the answer. There is a <code>Scalar</code> class in <code>tfds.features</code>, which I assume could be used to decode <code>Int64List</code>. But How can I decode the <code>BytesList</code>?</p>
<p><strong>Environment information</strong></p>
<ul>
<li><code>tensorflow-datasets</code> version: 4.8.2</li>
<li><code>tensorflow</code> version: 2.11.0</li>
</ul>
",2023-02-07 09:13:40,"<p>After some searching, I find the simplest solution is</p>
<pre><code>features = tfds.features.FeaturesDict({
    'image': tfds.features.Image(),  # &lt;&lt; Ideally best if add the `shape=(h, w, c)` info
    'caption': tfds.features.Text(),
    'height': tf.int32,
    'width': tf.int32,
})
</code></pre>
<p>Then I can load the data either with <code>tfds.folder_dataset.write_metadata</code> or directly with:</p>
<pre><code>ds = tf.data.TFRecordDataset()
ds = ds.map(features.deserialize_example)
</code></pre>
<p><code>batch, shuffle ,...</code> will work as expected in both cases.</p>
<p>The TFDS metadata would be helpful if fine-grained split control is needed.</p>
",7811775,41,https://stackoverflow.com/questions/75371111,Documentation Completeness
64611137,port TensorFlow 1 code to TensorFlow 2 (model learning process without sess.run),"<p>I have this piece of tf1 code, which was taken from nice book &quot;Deep Learning&quot; by S. Nikolenko.</p>
<p>It's a simple linear regression that learns <code>k</code> and <code>b</code> to 2 and 1 respectively.</p>
<pre><code>%tensorflow_version 1.x

import numpy as np,tensorflow as tf
import pandas as pd

n_samples, batch_size, num_steps = 1000, 100, 20000 #set learning constants
X_data = np.random.uniform(1, 10, (n_samples, 1)) #generate array x from 1 to 10 of shape (1000,1)
print(X_data.shape)
y_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1)) #generate right answer and add noise to it (to make it scatter)

X = tf.placeholder(tf.float32, shape=(batch_size, 1)) #defining placeholders to put into session.run
y = tf.placeholder(tf.float32, shape=(batch_size, 1))

with tf.variable_scope('linear-regression'):
  k = tf.Variable(tf.random_normal((1, 1)), name='slope') #defining 2 variables with shape (1,1)
  b = tf.Variable(tf.zeros((1,)), name='bias') # and (1,)
  print(k.shape,b.shape)

y_pred = tf.matmul(X, k) + b # all predicted y in batch, represents linear formula k*x + b
loss = tf.reduce_sum((y - y_pred) ** 2)  # mean square
optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)
display_step = 100

with tf.Session() as sess:
  sess.run(tf.initialize_variables([k,b]))
  for i in range(num_steps):
    indices = np.random.choice(n_samples, batch_size) # taking random indices
    X_batch, y_batch = X_data[indices], y_data[indices] # taking x and y from generated examples
    _, loss_val, k_val, b_val = sess.run([optimizer, loss, k, b ],
      feed_dict = { X : X_batch, y : y_batch })
    if (i+1) % display_step == 0:
      print('Epoch %d: %.8f, k=%.4f, b=%.4f' %
        (i+1, loss_val, k_val, b_val))

</code></pre>
<p>I'm striving to port it on TensorFlow 2</p>
<p>And for long time I can't wrap my head what should I use instead of <code>sess.run()</code> and <code>feed_dict</code>, which doing magic behind the scenes, official documentation go into to details with writing model class and so on, but I'm want to keep this as flat as possible.</p>
<p>Also it's suggested to calculate derivatives with <code>tf.GradientTape</code>, but I'm struggling with applying it right to my example</p>
<pre><code>%tensorflow_version 2.x

import numpy as np,tensorflow as tf
import pandas as pd

n_samples, batch_size, num_steps = 1000, 100, 200
X_data = np.random.uniform(1, 10, (n_samples, 1))
y_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1))

X = tf.Variable(tf.zeros((batch_size, 1)), dtype=tf.float32, shape=(batch_size, 1))
y = tf.Variable(tf.zeros((batch_size, 1)), dtype=tf.float32, shape=(batch_size, 1))

k = tf.Variable(tf.random.normal((1, 1)), name='slope')
b = tf.Variable(tf.zeros((1,)), name='bias')

loss = lambda: tf.reduce_sum((y - (tf.matmul(X, k) + b)) ** 2)
optimizer = tf.keras.optimizers.SGD(0.01).minimize(loss, [k, b, X, y])
display_step = 100


for i in range(num_steps):
  indices = np.random.choice(n_samples, batch_size)
  X_batch, y_batch = X_data[indices], y_data[indices]
  
</code></pre>
<p>I need SGD optimizer minimize that given loss function and learn k and b values, how can I achieve it from this point?</p>
",2020-10-30 15:19:16,"<p>After plenty of manuals I got how to do that was hiding under the hood of <code>sess.run</code> in tf1, but without an optimizer:</p>
<ol>
<li>Counting loss</li>
<li>Conunting gradients with respect to variables trained</li>
<li>Adjust grow speed of function relative to each trained var to learning rate</li>
<li>Assing new values to <code>k</code> and <code>b</code></li>
</ol>
<pre><code>X_batch, y_batch = X_data[indices], y_data[indices]
X.assign(tf.convert_to_tensor(X_batch))
y.assign(tf.convert_to_tensor(y_batch))
with tf.GradientTape(persistent=True) as tape:
  loss_val = loss()

dy_dk = tape.gradient(loss_val, k)
dy_db = tape.gradient(loss_val, b)

k.assign_sub(dy_dk * learn_rate)
b.assign_sub(dy_db * learn_rate)
if (i+1) % display_step == 0:
  print('Epoch %d: %.8f, k=%.4f, b=%.4f' %
        (i+1, loss_val, k.numpy(), b.numpy()))
</code></pre>
",6634635,41,https://stackoverflow.com/questions/64611137,Documentation Replicability
59555206,keras to tf.keras Conversion: Dense layer dimensions not defined?,"<p>So I've built a convnet using pure <code>keras</code>. It compiles and operates exactly as intended, but I need to convert it to use <code>tf.keras</code> so that I can make use of <code>tfmot</code>. Having read documentation, I attempted to convert it, only to get the following error:</p>

<p><code>The last dimension of the inputs to Dense should be defined. Found None.</code> </p>

<p>Any idea what I'm doing wrong?</p>

<p>Thanks!</p>

<p>Original <code>keras</code> model:</p>

<pre><code>input_layer = keras.layers.Input(shape=(100,))
reshape_layer = keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = keras.layers.Flatten()(conv_layer_5)
label_layer = keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = keras.layers.Dense(1, activation=""linear"")(label_layer)

model = keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>Converted <code>tf.keras</code> model:</p>

<pre><code>input_layer = tf.keras.layers.InputLayer(input_shape=(100,))
reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)
label_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>EDIT 1:</p>

<p>I thought maybe I could get around the issue by saving the <code>keras</code> model after creation and loading it as a <code>tf.keras</code> model immediately before compilation / training. That throws the same error! </p>
",2020-01-01 18:51:03,"<p>There's few issues with your code. Fix them and you should be good to go,</p>

<h2>Issue 1: Use <code>Input</code> instead of <code>InputLayer</code></h2>

<p>The standard is to use <code>Input</code> layer instead of <code>InputLayer</code> (which infact uses <code>InputLayer</code> internally). You also need to change <code>input_shape</code> to <code>shape</code> if you are using <code>Input</code> layer.</p>

<pre><code>input_layer = tf.keras.layers.Input(shape=(100,))
</code></pre>

<h2>Issue 2: 2 <code>None</code> dimensions in the output</h2>

<p>When you execute the following line, you get two <code>None</code> dimensions in your output. </p>

<pre><code>reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)
</code></pre>

<p>Which is why you are getting the above error. When defining the <code>Reshape</code> layer you don't define the <code>batch</code> dimension, which will be None. And that's the only dimension you can have as <code>None</code> if you want to use a <code>Dense</code> layer. Otherwise, the <code>Dense</code> layer cannot infer the shape of its weights (which is why you get the error). So change that to,</p>

<pre><code>reshape_layer = tf.keras.layers.Reshape((1, 100, 1))(input_layer)
</code></pre>

<p>The rest stays the same.</p>

<pre><code>conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)
label_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.summary()
</code></pre>
",6639365,57,https://stackoverflow.com/questions/59555206,Documentation Replication on Other Examples
60398554,"Should we apply repeat, batch shuffle to tf.data.Dataset when passing it to fit function?","<p>I still don't after having read documentation about <code>tf.keras.Model.fit</code> and <code>tf.data.Dataset</code>, when passing <code>tf.data.Dataset</code> to fit function, should I call <code>repeat</code> and <code>batch</code> on the dataset object or should I provide the <code>batch_size</code> and <code>epochs</code> arguments to fit instead? or both? Should I apply the same treatment to the validation set?</p>

<p>And while I'm here, can I <code>shuffle</code> the dataset before the <code>fit</code>? (seems like it's an obvious yes)
If so, before, after calling <code>Dataset.batch</code> and <code>Dataset.repeat</code> (if calling them)?</p>

<p><strong>Edit:</strong> When using <code>batch_size</code> argument, and without having called <code>Dataset.batch(batch_size)</code> previously, I am getting the following error:</p>

<pre><code>ValueError: The `batch_size` argument must not be specified for the given input type.
Received input: &lt;MapDataset shapes: ((&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;), (&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;)), 
types: ((tf.float32, tf.float32, tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32))&gt;, 
batch_size: 1
</code></pre>

<p>Thanks</p>
",2020-02-25 15:55:09,"<p>There's different ways to do what you want here, but the one I always use is: </p>

<pre><code>batch_size = 32
ds = tf.Dataset()
ds = ds.shuffle(len_ds)
train_ds = ds.take(0.8*len_ds)
train_ds = train_ds.repeat().batch(batch_size)
validation_ds = ds.skip(0.8*len_ds)
validation_ds = train_ds.repeat().batch(batch_size)
model.fit(train_ds,
          steps_per_epoch = len_train_ds // batch_size,
          validation_data = validation_ds,
          validation_steps = len_validation_ds // batch_size,
          epochs = 5)
</code></pre>

<p>This way you have access to all the variables after model fitting as well, for example if you want to visualize the validation set, you can. This is not really possible with <code>validation_split</code>. If you remove <code>.batch(batch_size)</code>, you should remove the <code>// batch_size</code>s, but I would leave them, as it clearer what is happening now.</p>

<p>You always have to provide epochs.</p>

<p>Calculating the length of your train/validation sets requires you to loop over them: </p>

<pre><code>len_train_ds = 0
for i in train_ds:
  len_train_ds += 1
</code></pre>

<p>if in <code>tf.Dataset</code> form.</p>
",7483509,1057,https://stackoverflow.com/questions/60398554,Documentation Replication on Other Examples
56201490,What happens when @tf.function decorator compiles a function into graph? Why is it faster than that in eager mode?,"<p>According to the TensorFlow <a href=""https://www.tensorflow.org/alpha/guide/autograph"" rel=""nofollow noreferrer"">document</a>, <code>@tf.function</code> compiles a function into a graph, and ""makes you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel.""</p>

<p>The example in the document demonstrates such a benefit:</p>

<pre><code>lstm_cell = tf.keras.layers.LSTMCell(10)

@tf.function
def lstm_fn(input, state):
  return lstm_cell(input, state)

input = tf.zeros([10, 10])
state = [tf.zeros([10, 10])] * 2
# warm up
lstm_cell(input, state); lstm_fn(input, state)
print(""eager lstm:"", timeit.timeit(lambda: lstm_cell(input, state), number=10))
print(""function lstm:"", timeit.timeit(lambda: lstm_fn(input, state), number=10))
</code></pre>

<p>output:</p>

<pre><code>eager lstm: 0.032440788112580776 
function lstm: 0.004768412094563246
</code></pre>

<p>What's the difference between a 'compiled graph' and a 'function in eager mode'? Why is the former faster when executed?</p>
",2019-05-18 17:53:15,"<p>Eager execution, used by default in TF 2.0, evaluates operations immediately, without building graphs. Graph on the other hand, has certain <a href=""https://www.tensorflow.org/guide/graphs#why_dataflow_graphs"" rel=""nofollow noreferrer"">advantages</a>. However, execution time will depend on particular code. The more code uses c++ backend, the less the difference will be. All the overhead comes from program in eager mode using python interpreter. For example basic matrix multiplication operation will not get as much benefit as more complex DNN applications.</p>
",7238787,491,https://stackoverflow.com/questions/56201490,Documentation Replicability
62086367,Does tf.data API make use of tf.placeholders under the hood?,"<p>I am wondering if <code>tf.data</code> API is using <code>tf.placeholders</code> under the hood, and if creating <code>tf.data.Dataset</code> is like creating a separate computational graph in which we load preprocess data and then merge with computational graph that our model represents?</p>
",2020-05-29 12:46:34,"<p><code>tf.data</code> does not use <code>tf.placeholders</code> under the hood. <code>tf.data.Dataset</code> uses the same mechanisms as other Tensorflow ops. The graph it creates in graph mode is not separate.</p>

<p>Check out the below video to learn more about how <code>tf.data</code> works under the hood. I linked to the part that explains what happens when you call <code>next()</code> on a dataset iterator.</p>

<p><a href=""https://youtu.be/kVEOCfBy9uY?t=1404"" rel=""nofollow noreferrer"">https://youtu.be/kVEOCfBy9uY?t=1404</a></p>
",8560600,1499,https://stackoverflow.com/questions/62086367,Documentation Replicability
64807842,Using tf.data.experimental.make_csv_dataset for time series data,"<p>How do I use <code>tf.data.experimental.make_csv_dataset</code> with CSV files containing time series data?</p>
<pre><code>building_dataset = tf.data.experimental.make_csv_dataset(file_pattern=csv_file,
                                                        batch_size=5,num_epochs=1, shuffle=False,select_columns=feature_columns)
</code></pre>
",2020-11-12 16:31:46,"<p>It is assumed that the CSV file is already sorted w.r.t. time. First, read the CSV file using:</p>
<pre><code>building_dataset = tf.data.experimental.make_csv_dataset(file_pattern=csv_file,
                                                        batch_size=5,num_epochs=1, shuffle=False,select_columns=feature_columns)
</code></pre>
<p>Then define a <code>pack_features_vector</code> to convert to a features vector and unbatch using flat_map(). The tensors are also cast to float32.</p>
<pre><code>def pack_features_vector(features):
    &quot;&quot;&quot;Pack the features into a single array.&quot;&quot;&quot;
    
    features = tf.stack([tf.cast(x,tf.float32) for x in list(features.values())], axis=1)
    return features

   
building_dataset = building_dataset.map(pack_features_vector)
building_dataset = building_dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))
for feature in building_dataset.take(1):
    print('Stacked tensor:',feature)
</code></pre>
<p>Then use the window and flat map method.</p>
<pre><code>building_dataset = building_dataset.window(window_size, shift=1, drop_remainder=True)
building_dataset = building_dataset.flat_map(lambda window: window.batch(window_size))
</code></pre>
<p>Then use map method to separate features and labels.</p>
<pre><code>building_dataset = building_dataset.map(lambda window: (window[:,:-1], window[-1:,-1]))
for feature, label in building_dataset.take(5):
    print(feature.shape)
    print('feature:',feature[:,0:4])
    print('label:',label)
</code></pre>
<p>Finally create batches using batch() and use as inputs to model training.</p>
<pre><code>building_dataset = building_dataset.batch(32)
</code></pre>
",7656080,757,https://stackoverflow.com/questions/64807842,Documentation Replicability
66879748,What is the difference between tf.keras.model and tf.keras.sequential?,"<p>In some <code>tf. keras</code> tutorials, I've seen them instantiated their model class like this:</p>
<p><code>model = tf.keras.Sequential()</code></p>
<p>While in some places, they use something like this:</p>
<p><code>model = tf.keras.Model(inputs=input, outputs=output)</code></p>
<p>But seeing here in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model"" rel=""noreferrer"">docs</a>, they do seem the same, but I am not sure nor is it explicitly mentioned. What are the differences between the two?</p>
",2021-03-30 23:31:57,"<p>There are two class API to define a model in <code>tf. keras</code>. According to the doc</p>
<ul>
<li><p><a href=""https://keras.io/api/models/sequential/"" rel=""noreferrer""><code>Sequential class</code></a>: Sequential groups a linear stack of layers into a <code>tf. keras.Model</code>.</p>
</li>
<li><p><a href=""https://keras.io/api/models/model/"" rel=""noreferrer""><code>Model class</code></a>: <code>Model</code> group's layers into an object with training and inference features.</p>
</li>
</ul>
<hr />
<p>An <code>Sequential</code> model is the simplest type of model, a linear stack of layers. But there are some flaws in using the <code>sequential</code> model API, it's limited in certain points. We can't build complex networks such as multi-input or multi-output networks using this API.</p>
<p>But using <a href=""https://keras.io/api/models/model/"" rel=""noreferrer""><code>Model class</code></a>, we can instantiate a Model with the <strong>Functional API</strong> (and also with <strong>Subclassing the Model class</strong>) that allows us to create arbitrary graphs of layers. From this, we can get more flexibility and easily define models where each layer can connect not just with the previous and next layers but also share feature information with other layers in the model, for example, model-like <code>ResNet</code>, <code>EfficientNet</code>.</p>
<p>In fact, most of the SOTA model that you can get from <code>tf.keras.applications</code> is basically implemented using the <strong>Functional API</strong>. However, in subclassing API, we define our layers in <code>__init__</code> and we implement the model's forward pass in the <code>call</code> method.</p>
<p>Generally speaking, all the model definitions using Sequential API, can be achieved in Functional API or Model Subclassing API. And in Functional API or Model Subclassing API, we can create complex layers that not possible to achieve in Sequential API. If you wondering which one to choose, the answer is, it totally depends on your need. However, check out the following blog post where we have discussed the various model strategies in <code>tf. keras</code> with more examples. <a href=""https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e"" rel=""noreferrer"">Model Sub-Classing and Custom Training Loop from Scratch in TensorFlow 2</a></p>
",8648710,1198,https://stackoverflow.com/questions/66879748,Documentation Ambiguity
59260563,"TypeError from ""tf.keras.layers.concatenate"": The added layer must be an instance of class Layer. Found: Tensor","<p>I'm trying to merge three layers and add it to model, but I got a <code>Tensor</code> from <code>tf.keras.layers.concatenate</code>, not a layer? How to solve this?</p>

<pre><code>...
ipt = tf.keras.Input(shape=[10, 5])
convs = []
fs= [1, 2, 3]
for f in fs:
    conv = tf.keras.layers.Conv1D(activation='tanh', kernel_size=f, filters=200)(ipt)
    pool = tf.keras.layers.MaxPooling1D(10 - fsz + 1, padding=""same"")(conv)
    pool = tf.keras.layers.Flatten()(pool)
    convs.append(pool)
merge = tf.keras.layers.concatenate(convs, axis=1)

model = tf.keras.models.Sequential()
model.add(ipt)
model.add(merge)
...
</code></pre>

<pre><code>TypeError: The added layer must be an instance of class Layer. Found: Tensor(""concatenate/Identity:0"", shape=(None, 600), dtype=float32)
</code></pre>
",2019-12-10 05:05:00,"<p>I think you're not using the Model in the correct way. Try changing your code the following way.</p>

<pre><code>from tensorflow.keras import layers, models

ipt = layers.Input(shape=[10, 5])
convs = []
fsz = 8
fs= [1, 2, 3]
for f in fs:
    conv = layers.Conv1D(activation='tanh', kernel_size=f, filters=200)(ipt)
    pool = layers.MaxPooling1D(10 - fsz + 1, padding=""same"")(conv)
    pool = layers.Flatten()(pool)
    convs.append(pool)
merge = layers.Concatenate(axis=1)(convs)

model = models.Model(inputs=ipt, outputs=merge)
model.summary()
</code></pre>
",7815609,35,https://stackoverflow.com/questions/59260563,Documentation Replication on Other Examples
61875170,tf.keras custom metric is giving incorrect results,"<p>I have implemented a custom metric in tf.keras for a multi label classification problem.</p>

<pre><code>def multilabel_TP(y_true, y_pred, thres = 0.4):
  return (      
                  tf.math.count_nonzero(
                                         tf.math.logical_and(tf.cast(y_true, tf.bool),
                                         tf.cast(y_pred &gt;= thres, tf.bool))
                                       )
         )
</code></pre>

<p>count_zero function produces integer results but while running the model it gives me float values. The custom function gives me correct results when tried outside the scope of the keras model.</p>

<pre><code> 8/33 [======&gt;.......................] - ETA: 27s - loss: 0.4294 - multilabel_TP: **121.6250** 
</code></pre>

<pre><code>model.compile(loss = 'binary_crossentropy', metrics = multilabel_TP, optimizer= 'adam')
model.fit(train_sentences, y_train, batch_size= 128, epochs = 20, validation_data= (test_sentences, y_test))
</code></pre>

<p>Why is this happenning?</p>
",2020-05-18 17:12:13,"<p>What is presented in the keras progress bar is a running mean of your loss/metrics over batches, since the model is being trained on batches and the weights are changing after each batch. This is why you get a floating point value.</p>

<p>Your metric should also return a floating point value, maybe by taking a division over the number of elements in the batch. Then the metric values will make more sense.</p>
",8410166,73,https://stackoverflow.com/questions/61875170,Documentation Replication on Other Examples
55300544,TF 2.0 @tf.function example,"<p>In the tensorflow documentation at the <a href=""https://www.tensorflow.org/alpha/guide/autograph"" rel=""nofollow noreferrer"">autograph</a> section we have the following code snippet</p>

<pre class=""lang-py prettyprint-override""><code>@tf.function
def train(model, optimizer):
  train_ds = mnist_dataset()
  step = 0
  loss = 0.0
  accuracy = 0.0
  for x, y in train_ds:
    step += 1
    loss = train_one_step(model, optimizer, x, y)
    if tf.equal(step % 10, 0):
      tf.print('Step', step, ': loss', loss, '; accuracy', compute_accuracy.result())
  return step, loss, accuracy

step, loss, accuracy = train(model, optimizer)
print('Final step', step, ': loss', loss, '; accuracy', compute_accuracy.result())
</code></pre>

<p>I have a small question concerning the <code>step</code> variable, it's an integer and not a tensor, autograph supports built-in python type such as integer. Therefore the <code>tf.equal(step%10,0)</code> could be changed to simply <code>step%10 == 0</code> right ? </p>
",2019-03-22 13:20:45,"<p>Yes, you're right. The integer variable step remains a Python variable even when converted to its graph representation. You can see the conversion result by calling <code>tf.autograph.to_code(train.python_function)</code>.</p>

<p>Without reporting all the code but only the <code>step</code> variable related part, you'll see that</p>

<pre><code>  def loop_body(loop_vars, loss_1, step_1):
    with ag__.function_scope('loop_body'):
      x, y = loop_vars
      step_1 += 1
</code></pre>

<p>Is still a python operation (otherwise it will be <code>step_1.assign_add(1)</code> if step 1 was a <code>tf.Tensor</code>).</p>

<p>For more information about autograph and tf.function I suggest reading the article <a href=""https://pgaleone.eu/tensorflow/tf.function/2019/03/21/dissecting-tf-function-part-1/"" rel=""nofollow noreferrer"">https://pgaleone.eu/tensorflow/tf.function/2019/03/21/dissecting-tf-function-part-1/</a> that explains easily what happens when a function is converted.</p>
",8593338,1558,https://stackoverflow.com/questions/55300544,Documentation Replication on Other Examples
55711355,How to restore dangling tf.py_func within the tf.data.Dataset() with tf.saved_model API?,"<p>After doing a research for restoring the <code>tf.py_func()</code> when using saved_model API in vain, I couldn't find other information than documented in <a href=""https://www.tensorflow.org/api_docs/python/tf/py_func"" rel=""nofollow noreferrer"">tensorflow</a>:</p>

<blockquote>
  <p>The operation must run in the same address space as the Python program that calls <code>tf.py_func()</code>. If you are using distributed TensorFlow, you must run a <code>tf.train.Server</code> in the same process as the program that calls <code>tf.py_func()</code> and you must pin the created operation to a device in that server (e.g. using with <code>tf.device()</code>:)</p>
</blockquote>

<p>Two save/load snippets help to illustrate the situation. </p>

<p><strong>Save part:</strong></p>

<pre><code>def wrapper(x, y):
    with tf.name_scope('wrapper'):
        return tf.py_func(Copy, [x, y], [tf.float32, tf.float32])

def Copy(x, y):
    return x, y

x_ph = tf.placeholder(tf.float32, [None], 'x_ph')
y_ph = tf.placeholder(tf.float32, [None], 'y_ph')

with tf.name_scope('input'):
    ds = tf.data.Dataset.from_tensor_slices((x_ph, y_ph))
    ds = ds.map(wrapper)
    ds = ds.batch(1)
    it = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)
    it_init_op = it.make_initializer(ds, name='it_init_op')

x_it, y_it = it.get_next()

# Simple operation
with tf.name_scope('add'):
    res = tf.add(x_it, y_it)

with tf.Session() as sess:
    sess.run([tf.global_variables_initializer(), it_init_op], feed_dict={y_ph: [10] * 10, x_ph: [i for i in range(10)]})
    sess.run([res])
    tf.saved_model.simple_save(sess, './dummy/test', {'x_ph': x_ph, 'y_ph': y_ph}, {'res': res})
</code></pre>

<p><strong>Load part:</strong></p>

<pre><code>graph = tf.Graph()
graph.as_default()
with tf.Session(graph=graph) as sess:
    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], './dummy/test')

    res = graph.get_tensor_by_name('add/Add:0')
    it_init_op = graph.get_operation_by_name('input/it_init_op')
    x_ph = graph.get_tensor_by_name('x_ph:0')
    y_ph = graph.get_tensor_by_name('y_ph:0')
    sess.run([it_init_op], feed_dict={x_ph: [5] * 5, y_ph: [i for i in range(5)]})

    for _ in range(5):
        sess.run([res])
</code></pre>

<p><strong>Error:</strong></p>

<blockquote>
  <p>ValueError: callback pyfunc_0 is not found</p>
</blockquote>

<p>It's well known that the function wrapped by the <code>tf.py_func()</code> isn't saved with the model. Does anybody has a solution to restore this by using the small hint given by the tf doc applying <code>tf.train.Server</code></p>
",2019-04-16 15:08:09,"<p>As long as no answer, I will suggest mine, which contour the pb instead of solving it. Struggling for a long time, I finally neglected it by pruning it. Then graft the new input/ouput to it with a simpler way the placeholder. Moreover, this <strong>py_func is deprecated in TF2.0</strong>.</p>
",9217178,533,https://stackoverflow.com/questions/55711355,Lack of Alternative Solutions/Documentation
57098652,Check if a tf.Tensor is mutable,"<p>How to check if a tf.Tensor is mutable?</p>

<p>I want to assert the arguments of a function have the correct types.</p>

<p>A tf.Tensor can be mutable:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np
x = tf.get_variable('x', shape=(2,), dtype=np.float32)
print(x[1])  # x[1] is a tf.Tensor
tf.assign(x[1], 1.0)
</code></pre>
",2019-07-18 16:06:29,"<p>This is not part of the public API, but looking at how <code>tf.assign</code> is implemented, I think you can just do:</p>

<pre><code>import tensorflow as tf

def is_assignable(x):
    return x.dtype._is_ref_dtype or (isinstance(x, tf.Tensor) and hasattr(x, 'assign'))
</code></pre>
",8704463,2044,https://stackoverflow.com/questions/57098652,Documentation Replicability
56547737,Nested tf.function is horribly slow,"<p>Within a function decorated with <code>tf.function</code>, I try to call another function decorated with <code>tf.function</code>. The result is horribly slow.</p>

<p>Is that because I am not suppose to use python native types in the function?
<a href=""https://stackoverflow.com/questions/55711115/tensorflow-2-0-model-using-tf-function-very-slow-and-is-recompiling-every-time-t"">Tensorflow 2.0 model using tf.function very slow and is recompiling every time the train count changes. Eager runs about 4x faster</a></p>

<p>Test:</p>

<pre><code>import numpy as np
import tensorflow as tf


@tf.function
def loop(x, y):
    for i in range(1000):
        x.assign_add(y)
    return x


@tf.function
def loop2(x, y):
    for i in range(1000):
        loop(x, y)
    return x


def main():
    print(""TensorFlow version: {}"".format(tf.__version__))
    print(""Eager execution: {}"".format(tf.executing_eagerly()))

    x = tf.Variable(initial_value=0, dtype=np.float32)
    y = tf.Variable(initial_value=1, dtype=np.float32)

    # print(loop2(x, y))  # horribly slow

    for i in range(1000):  # faster
        loop(x, y)


main()
</code></pre>
",2019-06-11 16:04:55,"<p>You should read <a href=""https://pgaleone.eu/tensorflow/tf.function/2019/05/10/dissecting-tf-function-part-3/"" rel=""noreferrer"">part 3</a> of the article cited in the answer you linked.</p>

<p>In part 3, you can see that the problem is not only when using Python native types, but also when using Python constructs (like <code>for</code>) that operate on Python types and not on <code>tf.Tensor</code> objects.</p>

<p>In particular, when looping over a <code>range</code> and not on a <code>tf.range</code> you're building a huge graph since you're repeating <code>1000</code> times the body loop (you're unrolling the loop.</p>

<p>If you replace <code>range</code> with <code>tf.range</code> everything goes way faster.</p>

<p>Proof.</p>

<p>Your code (with time measurements and 100 instead of 1000):</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf
from time import time

@tf.function
def loop(x, y):
    for i in range(100):
        x.assign_add(y)
    return x


@tf.function
def loop2(x, y):
    for i in range(100):
        loop(x, y)
    return x


def main():
    print(""TensorFlow version: {}"".format(tf.__version__))
    print(""Eager execution: {}"".format(tf.executing_eagerly()))

    x = tf.Variable(initial_value=0, dtype=np.float32)
    y = tf.Variable(initial_value=1, dtype=np.float32)
    print(""one"")
    start = time()
    print(loop2(x, y))  # horribly slow
    print(""end: "", time() - start)
    print(""second: "")
    start = time()
    for i in range(100):  # faster
        loop(x, y)
    print(""end: "", time() - start)


main()
</code></pre>

<p>The output:</p>

<pre><code>TensorFlow version: 2.0.0-beta0
Eager execution: True
one
tf.Tensor(10000.0, shape=(), dtype=float32)
end:  86.44128751754761
second: 
end:  0.08476066589355469
</code></pre>

<p>Updated code using only TensorFlow methods:</p>

<pre class=""lang-py prettyprint-override""><code>@tf.function
def loop__(x, y):
    for i in tf.range(100):
        x.assign_add(y)
    return x


@tf.function
def loop2__(x, y):
    for i in tf.range(100):
        loop__(x, y)
    return x


def main():
    print(""TensorFlow version: {}"".format(tf.__version__))
    print(""Eager execution: {}"".format(tf.executing_eagerly()))

    x = tf.Variable(initial_value=0, dtype=np.float32)
    y = tf.Variable(initial_value=1, dtype=np.float32)
    print(""one"")
    start = time()
    print(loop2__(x, y))  # horribly slow
    print(""end: "", time() - start)
    print(""second: "")
    start = time()
    for i in tf.range(100):  # faster
        loop__(x, y)
    print(""end: "", time() - start)


main()
</code></pre>

<p>The output:</p>

<pre><code>TensorFlow version: 2.0.0-beta0
Eager execution: True
one
tf.Tensor(10000.0, shape=(), dtype=float32)
end:  0.4946322441101074
second: 
end:  0.24096465110778809
</code></pre>
",8704463,2044,https://stackoverflow.com/questions/56547737,Documentation Replicability
57813806,Apply feature columns without tf.Estimator (Tensorflow 2.0.0-rc0),"<p>In the Tensorflow tf.Estimator and tf.feature_column docs it is well documented, how to use feature columns together with an Estimator e.g. in order to one-hot encode the categorical features in the dataset being used.</p>

<p>However, I want to ""apply"" my feature columns directly to a tf.dataset which I create from a .csv file (with two columns: UserID, MovieID), without even defining a model or an Estimator. (Reason: I want to check what's happening exactly in my datapipeline, i.e. I'd like to be able to run a batch of samples through my the pipeline, and then see in the output how the features got encoded.)</p>

<p>This is what I have tried so far:</p>

<pre><code>column_names = ['UserID', 'MovieID']

user_col = tf.feature_column.categorical_column_with_hash_bucket(key='UserID', hash_bucket_size=1000)
movie_col = tf.feature_column.categorical_column_with_hash_bucket(key='MovieID', hash_bucket_size=1000)
feature_columns = [tf.feature_column.indicator_column(user_col), tf.feature_column.indicator_column(movie_col)]

feature_layer = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)

def process_csv(line):
  fields = tf.io.decode_csv(line, record_defaults=[tf.constant([], dtype=tf.int32)]*2, field_delim="";"")
  features = dict(zip(column_names, fields))

  return features 

ds = tf.data.TextLineDataset(csv_filepath)
ds = ds.map(process_csv, num_parallel_calls=4)
ds = ds.batch(10)
ds.map(lambda x: feature_layer(x))
</code></pre>

<p>However the last line with the map call raises the following error:</p>

<blockquote>
  <p>ValueError: Column dtype and SparseTensors dtype must be compatible.
  key: MovieID, column dtype: , tensor dtype: </p>
</blockquote>

<p>I'm not sure what this error means...
I also tried to define a tf.keras model with only the feature_layer I defined, and then run .predict() on my dataset - instead of using ds.map(lambda x: feature_layer(x)):</p>

<pre><code>model = tf.keras.Sequential([feature_layer])
model.compile()
model.predict(ds)
</code></pre>

<p>However, this results exactly in the same error as above.
Does anybody have an idea what is going wrong? Is there maybe an easier way to achieve this?</p>
",2019-09-05 22:50:19,"<p>Just found the issue:
tf.feature_column.categorical_column_with_hash_bucket() takes an optional argument dtype, which is set to tf.dtypes.string by default.
However, the datatype of my columns is numerical (tf.dtypes.int32).
This solved the issue:</p>

<pre><code>tf.feature_column.categorical_column_with_hash_bucket(key='UserID', hash_bucket_size=1000, dtype=tf.dtypes.int32)
</code></pre>
",8725045,724,https://stackoverflow.com/questions/57813806,Documentation Replication on Other Examples
55363728,How to feed .h5 files in tf.data pipeline in tensorflow model,"<p>I'm trying to optimize the input pipeline for .h5 data with tf.data. But I encountered a <code>TypeError: expected str, bytes or os.PathLike object, not Tensor</code>. I did a research but can't find anything about converting a tensor of string to string.</p>

<p>This simplified code is executable and return the same error:</p>

<pre><code>batch_size = 1000
conv_size = 3
nb_conv = 32
learning_rate = 0.0001

# define parser function
def parse_function(fname):
    with h5py.File(fname, 'r') as f: #Error comes from here
        X = f['X'].reshape(batch_size, patch_size, patch_size, 1)
        y = f['y'].reshape(batch_size, patch_size, patch_size, 1)
        return X, y

# create a list of files path
flist = []
for dirpath, _, fnames in os.walk('./proc/'):
    for fname in fnames:
        if fname.startswith('{}_{}'.format(patch_size, batch_size)) and fname.endswith('h5'):
            flist.append(fname)

# prefetch data
dataset = tf.data.Dataset.from_tensor_slices((flist))
dataset = dataset.shuffle(len(flist))
dataset = dataset.map(parse_function, num_parallel_calls=4)
dataset = dataset.batch(1)
dataset = dataset.prefetch(3)

# simplest model that I think of
X_ph = tf.placeholder(tf.float32, shape=None)
y_ph = tf.placeholder(tf.float32, shape=None)
W = tf.get_variable('w', shape=[conv_size, conv_size, 1, 1], initializer=tf.contrib.layers.xavier_initializer())
loss = tf.reduce_mean(tf.losses.mean_squared_error(tf.nn.softmax(labels=y_ph, predictions=tf.matmul(X_ph, W))))
train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

# start session
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(train_op, feed_dict={X_ph: dataset[0], y_ph: dataset[1]}))
</code></pre>

<p>Apparently the <code>fname</code> is a tensor of string but the positional argument waits for only a string. I can't find any documentation on this. And the answer of <a href=""https://stackoverflow.com/questions/45246764/how-to-convert-a-string-tensor-to-a-python-string-in-tensorflow"">another post</a> doesn't solve this problem. In my case, I work only with h5 where one h5 store one batch.</p>

<hr>

<p><strong>Update Solution:</strong>
Thanks to the comment of @kvish, the part of loading .h5 file is solved.
The code is upgraded with a simple conv layer, the placeholders have been taken. <strong>Each .h5 is one batch.</strong> I want to prefetch in parallele multiple batches(h5py doesn't support multithread reading so I write batches into multiple files). One can <strong>copy-paste-and-launch</strong>:</p>

<pre><code>import h5py
import threading
import numpy as np
import tensorflow as tf

# generate some img data
for i in range(5):
    with h5py.File('./test_{}.h5'.format(i), 'w') as f:
        f.create_dataset('X', shape=(1000, 100, 100), dtype='float32', data=np.random.rand(10**7).reshape(1000, 100, 100))
        f.create_dataset('y', shape=(1000, 100, 100), dtype='float32', data=np.random.rand(10**7).reshape(1000, 100, 100))
        print(threading.get_ident())

# params
num_cores = 3
shuffle_size = 1
batch_size = 1

# read .h5 file
def parse_file(f):
    print(f.decode('utf-8'))
    with h5py.File(f.decode(""utf-8""), 'r') as fi:
        X = fi['X'][:].reshape(1000, 100, 100, 1)
        y = fi['y'][:].reshape(1000, 100, 100, 1)
        print(threading.get_ident())  # to see the thread id
        return X, y

# py_func wrapper
def parse_file_tf(filename):
    return tf.py_func(parse_file, [filename], [tf.float32, tf.float32])

# tf.data input pipeline
files = tf.data.Dataset.list_files('./test_*.h5')
dataset = files.map(parse_file_tf, num_parallel_calls=num_core)
dataset = dataset.batch(batch_size).shuffle(shuffle_size).prefetch(3)
it = dataset.make_initializable_iterator()
iter_init_op = it.initializer
X_it, y_it = it.get_next()

# simplest model that I can think of 
with tf.name_scope(""Conv1""):
    W = tf.get_variable(""W"", shape=[3, 3, 1, 1],
                         initializer=tf.contrib.layers.xavier_initializer())
    b = tf.get_variable(""b"", shape=[1], initializer=tf.contrib.layers.xavier_initializer())
    layer1 = tf.nn.conv2d(X_it, W, strides=[1, 1, 1, 1], padding='SAME') + b
    out = tf.nn.relu(layer1)

loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_it, predictions=out))
train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)

# session
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(iter_init_op)
sess.run([train_op])
sess.close()
</code></pre>

<hr>

<p>Somehow there will be another cudnn issue which isn't related to this post.</p>

<p>tensorflow-cpu v1.12: work fine</p>

<p>tensorflow-gpu v1.12: <strong>runtime</strong> issue happens</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"",
  line 1334, in _do_call
      return fn(*args)   File ""/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"",
  line 1319, in _run_fn
      options, feed_dict, fetch_list, target_list, run_metadata)   File ""/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"",
  line 1407, in _call_tf_sessionrun
      run_metadata) tensorflow.python.framework.errors_impl.NotFoundError: No algorithm
  worked!    [[{{node Conv1/Conv2D}} = Conv2D[T=DT_FLOAT,
  data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"",
  strides=[1, 1, 1, 1], use_cudnn_on_gpu=true,
  _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/Conv1/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer,
  W/read)]]      [[{{node
  mean_squared_error/num_present/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_2/_37}}
  = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"",
  send_device=""/job:localhost/replica:0/task:0/device:GPU:0"",
  send_device_incarnation=1, tensor_name=""edge_63_me...t/Switch_2"",
  tensor_type=DT_INT32,
  _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]
  tensorflow-cpu v1.12: works fine!</p>
</blockquote>
",2019-03-26 18:10:29,"<p>Here is an example of how you can wrap the function with the help of <a href=""https://www.tensorflow.org/api_docs/python/tf/py_func"" rel=""nofollow noreferrer"">py_func</a>. Do note that this is deprecated in TF V2. You can follow the documentation for further details.</p>

<pre><code>def parse_function_wrapper(filename):
   # Assuming your data and labels are float32
   # Your input is parse_function, who arg is filename, and you get X and y as output
   # whose datatypes are indicated by the tuple argument  
   features, labels = tf.py_func(
       parse_function, [filename], (tf.float32, tf.float32)) 
   return features, labels

# Create dataset of filenames.
dataset = tf.data.Dataset.from_tensor_slices(flist)
dataset = dataset.shuffle(len(flist))
dataset = dataset.map(parse_function_wrapper)
</code></pre>
",9217178,533,https://stackoverflow.com/questions/55363728,Lack of Alternative Solutions/Documentation
55160136,Tensorflow 2.0 and image processing,"<p>I'm just converting some code to tensorflow v2, but i'm facing an issue.
I used to use some tf.contrib.image functions in tensorflow v1 (dense_image_warp and rotate). I don't know how to replace them in v2. 
Those functions are not part of the tf.image module.</p>

<p>The tf_upgrade_v2 script can't convert those methods.</p>

<p>Does someone know how to replace them to be v2 compatible ?</p>

<p>Thank you</p>
",2019-03-14 10:25:58,"<p>Many of the image processing functions that were historically housed in <code>tf.contrib</code> have been migrated to other locations in TensorFlow 2.0. <code>dense_image_warp</code> and <code>rotate</code> are now part of the <a href=""https://github.com/tensorflow/addons/tree/master/tensorflow_addons/image"" rel=""nofollow noreferrer""><code>tfa.image</code></a> module in TensorFlow Addons.</p>
",9266202,78,https://stackoverflow.com/questions/55160136,Documentation Replicability
55638989,Eager tf.GradientTape() returns only Nones,"<p>I try to calculate the gradients with Tensorflow in the eager mode, but
tf.GradientTape () returns only None values. I can not understand why.
The gradients are calculated in the update_policy () function.</p>

<p>The output of the line:</p>

<pre class=""lang-py prettyprint-override""><code>grads = tape.gradient(loss, self.model.trainable_variables)
</code></pre>

<p>is</p>

<pre class=""lang-py prettyprint-override""><code>{list}&lt;class 'list'&gt;:[None, None, ... ,None]
</code></pre>

<p>Here is the code.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from keras.backend.tensorflow_backend import set_session

import numpy as np

tf.enable_eager_execution()
print(tf.executing_eagerly())

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
set_session(sess)


class PGEagerAtariNetwork:
    def __init__(self, state_space, action_space, lr, gamma):
        self.state_space = state_space
        self.action_space = action_space
        self.gamma = gamma

        self.model = tf.keras.Sequential()
        # Conv
        self.model.add(
            tf.keras.layers.Conv2D(filters=32, kernel_size=[8, 8], strides=[4, 4], activation='relu',
                                   input_shape=(84, 84, 4,),
                                   name='conv1'))
        self.model.add(
            tf.keras.layers.Conv2D(filters=64, kernel_size=[4, 4], strides=[2, 2], activation='relu', name='conv2'))
        self.model.add(
            tf.keras.layers.Conv2D(filters=128, kernel_size=[4, 4], strides=[2, 2], activation='relu', name='conv3'))
        self.model.add(tf.keras.layers.Flatten(name='flatten'))

        # Fully connected
        self.model.add(tf.keras.layers.Dense(units=512, activation='relu', name='fc1'))
        self.model.add(tf.keras.layers.Dropout(rate=0.4, name='dr1'))
        self.model.add(tf.keras.layers.Dense(units=256, activation='relu', name='fc2'))
        self.model.add(tf.keras.layers.Dropout(rate=0.3, name='dr2'))
        self.model.add(tf.keras.layers.Dense(units=128, activation='relu', name='fc3'))
        self.model.add(tf.keras.layers.Dropout(rate=0.1, name='dr3'))

        # Logits
        self.model.add(tf.keras.layers.Dense(units=self.action_space, activation=None, name='logits'))

        self.model.summary()

        # Optimizer
        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)

    def get_probs(self, s):
        s = s[np.newaxis, :]
        logits = self.model.predict(s)
        probs = tf.nn.softmax(logits).numpy()
        return probs

    def update_policy(self, s, r, a):
        with tf.GradientTape() as tape:
            logits = self.model.predict(s)
            policy_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=a, logits=logits)
            policy_loss = policy_loss * tf.stop_gradient(r)
            loss = tf.reduce_mean(policy_loss)
        grads = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))
</code></pre>
",2019-04-11 18:26:20,"<p>You don't have a forward pass in your model. The <code>Model.predict()</code> method returns <code>numpy()</code> array without taping the forward pass. Take a look at this example:</p>

<p>Given a following data and model:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

x_train = tf.convert_to_tensor(np.ones((1, 2), np.float32), dtype=tf.float32)
y_train = tf.convert_to_tensor([[0, 1]])

model = tf.keras.models.Sequential([tf.keras.layers.Dense(2, input_shape=(2, ))])
</code></pre>

<p>First we use <code>predict()</code>:</p>

<pre class=""lang-py prettyprint-override""><code>with tf.GradientTape() as tape:
    logits = model.predict(x_train)
    print('`logits` has type {0}'.format(type(logits)))
    # `logits` has type &lt;class 'numpy.ndarray'&gt;
    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_train, logits=logits)
    reduced = tf.reduce_mean(xentropy)
    grads = tape.gradient(reduced, model.trainable_variables)
    print('grads are: {0}'.format(grads))
    # grads are: [None, None]
</code></pre>

<p>Now we use model's input:</p>

<pre class=""lang-py prettyprint-override""><code>with tf.GradientTape() as tape:
    logits = model(x_train)
    print('`logits` has type {0}'.format(type(logits)))
    # `logits` has type &lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;
    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_train, logits=logits)
    reduced = tf.reduce_mean(xentropy)
    grads = tape.gradient(reduced, model.trainable_variables)
    print('grads are: {0}'.format(grads))
    # grads are: [&lt;tf.Tensor: id=2044, shape=(2, 2), dtype=float32, numpy=
    # array([[ 0.77717704, -0.777177  ],
    #        [ 0.77717704, -0.777177  ]], dtype=float32)&gt;, &lt;tf.Tensor: id=2042, 
    # shape=(2,), dtype=float32, numpy=array([ 0.77717704, -0.777177  ], dtype=float32)&gt;]

</code></pre>

<p>So use model's <code>__call__()</code> (i.e. <code>model(x)</code>) for forward pass and not <code>predict()</code>.</p>
",8807447,176,https://stackoverflow.com/questions/55638989,Documentation Replicability
64081367,Slicing a tensor with a tensor of indices and tf.gather,"<p>I am trying to slice a tensor with a indices tensor. For this purpose I am trying to use <code>tf.gather</code>.
However, I am having a hard time understanding the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer"">documentation</a> and don't get it to work as I would expect it to:</p>
<p>I have two tensors. An <code>activations</code> tensor with a shape of <code>[1,240,4]</code> and an <code>ids</code> tensor with the shape <code>[1,1,120]</code>. I want to slice the second dimension of the <code>activations</code> tensor with the indices provided in the third dimension of the <code>ids</code> tensor:</p>
<pre><code>downsampled_activations = tf.gather(activations, ids, axis=1)
</code></pre>
<p>I have given it the <code>axis=1</code> option since that is the axis in the <code>activations</code> tensor I want to slice.</p>
<p>However, this does not render the expected result and only gives me the following error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0,0,1] = 1 is not in [0, 1)
</code></pre>
<p>I have tried various combinations of the <code>axis</code> and <code>batch_dims</code> options, but to no avail so far and the documentation doesn't really help me on my path. Anybody care to explain the parameters in more detail or on the example above would be very helpful!</p>
<p><strong>Edit:</strong>
The IDs are precomputed before runtime and come in through an input pipeline as such:</p>
<pre><code>features = tf.io.parse_single_example(
            serialized_example,
            features={ 'featureIDs': tf.io.FixedLenFeature([], tf.string)}
</code></pre>
<p>They are then reshaped into the previous format:</p>
<pre><code>feature_ids_raw = tf.decode_raw(features['featureIDs'], tf.int32)
feature_ids_shape = tf.stack([batch_size, (num_neighbours * 4)])
feature_ids = tf.reshape(feature_ids_raw, feature_ids_shape)
feature_ids = tf.expand_dims(feature_ids, 0)
</code></pre>
<p>Afterwards they have the previously mentioned shape (<code>batch_size = 1</code> and <code>num_neighbours = 30</code> -&gt; <code>[1,1,120]</code>) and I want to use them to slice the <code>activations</code> tensor.</p>
<p><strong>Edit2:</strong> I would like the output to be <code>[1,120,4]</code>. (So I would like to gather the entries along the second dimension of the <code>activations</code> tensor in accordance with the IDs stored in my <code>ids</code> tensor.)</p>
",2020-09-26 18:48:20,"<blockquote>
<p><code>tf.gather</code> Gather slices from <code>params</code> axis <code>axis</code> according to indices.</p>
</blockquote>
<p>Granted that the documentation is not the most expressive, and the emphasis should be placed on the <strong>slices</strong> (since you index slices from the <code>axis</code> and not elements, which is what I suppose you mistakenly took it for).</p>
<p><strong>Let's take a much smaller example:</strong></p>
<pre class=""lang-py prettyprint-override""><code>activations_small = tf.convert_to_tensor([[[1, 2, 3, 4], [11, 22, 33, 44]]])
print(activations_small.shape) # [1, 2, 4]
</code></pre>
<p>Let's picture this tensor:</p>
<pre><code>    XX 4  XX 44 XX XX
  XX  3 XX  33 X  XX
XXX 2 XX   22XX  XX
X-----X-----+X  XX
|  1  |  11 | XX
+-----+-----+X
</code></pre>
<p><code>tf.gather(activations1, [0, 0], axis=1)</code> will return</p>
<pre><code>&lt;tf.Tensor: shape=(1, 2, 4), dtype=int32, numpy=
array([[[1, 2, 3, 4],
        [1, 2, 3, 4]]], dtype=int32)&gt;
</code></pre>
<p>What <code>tf.gather</code> did was to <em>look from</em> axis 1, and picks up index 0 (ofc, two times i.e. <code>[0, 0]</code>). If you were to run <code>tf.gather(activations1, [0, 0, 0, 0, 0], axis=1).shape</code>, you'd get <code>TensorShape([1, 5, 4])</code>.</p>
<p><strong>Your Error</strong>
Now let's try to trigger the error that you're getting.</p>
<p><code>tf.gather(activations1, [0, 2], axis=1)</code></p>
<blockquote>
<p>InvalidArgumentError: indices[1] = 2 is not in [0, 2) [Op:GatherV2]</p>
</blockquote>
<p>What happened here was that when <code>tf.gather</code> looks from axis 1 perspective, there's no item (column if you will) with index = 2.</p>
<p>I guess this is what the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather#args"" rel=""nofollow noreferrer"">documentation</a> is hinting at by</p>
<blockquote>
<p><code>param:&lt;indices&gt;</code> The index Tensor. Must be one of the following types: int32, int64. Must be in range [0, params.shape[axis]).</p>
</blockquote>
<p><strong>Your (potential) solution</strong></p>
<p>From the dimensions of <code>indices</code>, and that of the expected result from your question, I am not sure if the above was very obvious to you.</p>
<p><code>tf.gather(activations, indices=[0, 1, 2, 3], axis=2)</code> or anything with indices within the range of indices in <code>[0, activations.shape[2])</code> i.e. <code>[0, 4)</code> would work. Anything else would give you the error that you're getting.</p>
<p>There's a verbatim answer below in case that's your expected result.</p>
",8183516,443,https://stackoverflow.com/questions/64081367,Documentation Ambiguity
61885570,Reading a tfrecord: DecodeError: Error parsing message,"<p>I am using colab to run a <a href=""https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb"" rel=""nofollow noreferrer"">tutorial</a> on tensorflow ranking. It uses wget to fetch the tfrecord:</p>

<pre><code>!wget -O ""/tmp/train.tfrecords"" ""http://ciir.cs.umass.edu/downloads/Antique/tf-ranking/ELWC/train.tfrecords""
</code></pre>

<p>I am using this code to try to look at the structure of the tfrecord:</p>

<pre><code>for example in tf.compat.v1.python_io.tf_record_iterator(""/tmp/train.tfrecords""):
    print(tf.train.Example.FromString(example))
    break
</code></pre>

<p>And I am getting:</p>

<pre><code>DecodeError: Error parsing message
</code></pre>

<p>How to generally look at the structure of tfrecords instead?</p>

<p>A second question: Where to find documentation on classes like <code>tf.train.Example</code>? I just find this <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Example"" rel=""nofollow noreferrer"">empty page</a>.</p>
",2020-05-19 07:34:24,"<p>The insight of the problem is that the records are serialized using another schema: the <code>ExampleListWithContext</code> Schema, instead of the basic <code>tf.train.Example</code> schema. Updating the right deserialization solves the problem.</p>

<pre><code>filenames = ['/tmp/train.tfrecords']
raw_dataset = tf.data.TFRecordDataset(filenames)
for e in raw_dataset.take(1):
    ELWC = input_pb2.ExampleListWithContext()
    v = ELWC.FromString(e.numpy())
    print(v.context)
    for e in v.examples:
        print(e)
</code></pre>

<p>outputs:</p>

<pre><code>features {
  feature {
    key: ""query""
    value {
      bytes_list {
        value: ""why do ...""
      }
    }
  }
  feature {
    key: ""query_bert_encoder_outputs""
    value {
      float_list {
...
}}
</code></pre>
",8183621,605,https://stackoverflow.com/questions/61885570,Requesting (Additional) Documentation/Examples
58213561,"Under Tensorflow 2.0, how to use tf.summary?","<p>It seems the use of tf.summary under TF2.0 is different from TF1.x. If I want to visualize multiple scalars (e.g., train_loss, val_loss, etc.) in TensorBoard, how to implement tf.summary in the code. I thought it might be:</p>

<pre><code>writer = tf.summary.create_file_writer('summaries')
for epoch in range(epochs):
    ...
    train_loss = ...
    val_loss = ...
    tf.summary.scalar('train_loss', train_loss)
    tf.summary.scalar('val_loss', val_loss)
    writer.flush()
</code></pre>

<p>But it doesn't work. I think the problem may be from the use of write.flush().</p>
",2019-10-03 06:39:09,"<p>In tensorflow 2.0, you need to set default writer via <code>.as_default()</code>. Also, <code>step</code> argument must be passed in summary ops. More details can be seen <a href=""https://www.tensorflow.org/tensorboard/migrate"" rel=""nofollow noreferrer"">here</a>. </p>

<pre class=""lang-py prettyprint-override""><code>writer = tf.summary.create_file_writer(""summaries"")

for epoch in range(epochs):
    train_loss = ...
    val_loss = ...
    with writer.as_default():
      tf.summary.scalar('train_loss', train_loss, step=epoch)
      tf.summary.scalar('val_loss', val_loss, step=epoch)
    writer.flush()
</code></pre>
",9151176,145,https://stackoverflow.com/questions/58213561,Documentation Replicability
55909188,How can I apply a TensorFlow 2D Convolution (tf.nn.conv2d) to a single (non-batch) 2D image?,"<p>I would like to use the function <code>tf.nn.conv2d()</code> on a <strong>single</strong> image example, but the TensorFlow documentation seems to only mention applying this transformation to a <strong>batch</strong> of images. </p>

<p>The docs mention that the input image must be of shape <code>[batch, in_height, in_width, in_channels]</code> and the kernel must be of shape <code>[filter_height, filter_width, in_channels, out_channels]</code>. However, what is the most straightforward way to achieve 2D convolution with input shape <code>[in_height, in_width, in_channels]</code>?</p>

<p>Here is an example of the current approach, where <code>img</code> has shape (height, width, channels):</p>

<pre><code>img = tf.random_uniform((10,10,3))  # a single image
img = tf.nn.conv2d([img], kernel)[0] # creating a batch of 1, then indexing the single example
</code></pre>

<p>I am reshaping the input as follows:</p>

<p><code>[in_height, in_width, in_channels]-&gt;[1, in_height, in_width, in_channels]-&gt;[in_height, in_width, in_channels]</code> </p>

<p>This feels like an unnecessary and costly operation when I am only interested in transforming one example.</p>

<p>Is there a simple/standard way to do this that doesn't involve reshaping?</p>
",2019-04-29 18:47:54,"<p>AFAIK there is no way around it. It seems (<a href=""https://stackoverflow.com/questions/50779869/does-tensorflow-tf-slice-incur-allocation-and-or-memory-copy"">here</a> and <a href=""https://stackoverflow.com/questions/53398721/tensorflow-can-reshape-create-a-copy"">here</a>) that the first operation creates a copy (someone correct me if I'm wrong). You may use <a href=""https://www.tensorflow.org/api_docs/python/tf/expand_dims"" rel=""nofollow noreferrer""><code>tf.expand_dims</code></a> instead though, it's IMO more readable because of it's verbosity.</p>

<p>On the other hand, taking <code>0</code> element from the tensor should not perform a copy in this case and is almost free.</p>

<p><strong>Most importantly</strong>, except for a little inconvenience with syntax (e.g. <code>[0]</code>) those operations definitely <strong>are not costly</strong>, especially in the context of performing convolution.</p>

<p>BTW. Other ready alternative layers like the ones in <code>tf.keras</code>, require batch as first dimension as well.</p>
",9672143,422,https://stackoverflow.com/questions/55909188,Documentation Replication on Other Examples
56802840,What exactly tensorflow.gather() does?,"<p>I saw code for triplet loss that contains the function tf.gather(). What this function does?</p>

<p>I have gone through the tensorflow's official website for definition but still unable to get it.</p>

<pre><code>def margin_triplet_loss(y_true, y_pred, margin, batch_size):
    anchor = tf.gather(y_pred, tf.range(0, batch_size, 3))
    positive = tf.gather(y_pred, tf.range(1, batch_size, 3))
    negative = tf.gather(y_pred, tf.range(2, batch_size, 3))

    loss = K.maximum(margin
                 + K.sum(K.square(anchor-positive), axis=1)
                 - K.sum(K.square(anchor-negative), axis=1),
                 0.0)
    return K.mean(loss)
</code></pre>
",2019-06-28 07:41:59,"<p>tf.gather is a function to index an array. You gather the elements which you specify by the index argument. This is not natively posible for tensorflow tensors. </p>

<p>tf.gather(y_pred, tf.range(0, batch_size, 3)) is equivalent in numpy to y_pred[0:batch_size:3], which means that you return every third element starting from the first one. </p>
",9678047,437,https://stackoverflow.com/questions/56802840,Lack of Alternative Solutions/Documentation
54701429,Tensorflow: why tf.nn.conv2d runs faster than tf.layers.conv2d?,"<p>I am writing a simple implementation of AlexNet. I tried with using tf.nn.conv2d and tf.layers.conv2d, and the results turn out that the loss dropped faster when using tf.nn.conv2d, even the structure is exactly the same. Does anyone know any explanation for that?</p>
",2019-02-15 01:18:31,"<p>If you try to follow the chain of function calls, you will find that <code>tf.layers.conv2D()</code> makes calls to <code>tf.nn.conv2D()</code> so no matter what you use, <code>tf.nn.conv2d()</code> will be called, it will be just faster if you call it yourself. You can use <code>traceback.print_stack()</code> method to verify that for yourself.</p>

<p><strong>NOTE</strong> This does not mean that they are one and the same, select the function based on your need as there are various other tasks undertaken by <code>tf.layers.conv2D()</code>.</p>
",8859758,23,https://stackoverflow.com/questions/54701429,Documentation Replication on Other Examples
73264100,Use tf.nn.local_response_normalization in keras layers,"<p>I am using keras to add layers, for example:</p>
<pre><code>model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', padding=&quot;same&quot;, input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu', padding=&quot;same&quot;, input_shape=(16, 16, 32)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu', padding=&quot;same&quot;))
model.add(BatchNormalization())
model.add(layers.Dropout(0.5))

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(10))
</code></pre>
<p>Now I am implementing LRN. However, the keras library does not have LRN to my limited knowledge. The old tf.nn library does have a LRN function called tf.nn.local_response_normalization.</p>
<p>Is it possible to mix tf.nn with keras?</p>
",2022-08-07 00:18:06,"<p>Yes, <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization"" rel=""nofollow noreferrer""><code>tf.nn.local_response_normalization</code></a> can be used in a lambda layer. See the code below:</p>
<pre><code>...
model.add(BatchNormalization())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(10))
model.add(layers.Lambda(tf.nn.local_response_normalization))
...
</code></pre>
",9274598,137,https://stackoverflow.com/questions/73264100,Documentation Replicability
64734480,How to implent tf.nn.in_top_k in pytorch,"<p>I want to implent <code>tf.nn.in_top_k</code> in pytorch. Here is the link of <code>tf.nn.in_top_k</code>,</p>
<pre><code>tf.math.in_top_k(
    targets, predictions, k, name=None
)
</code></pre>
<p>It computed precision at k as a bool Tensor and will return a Tensor of type bool.</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/math/in_top_k"" rel=""nofollow noreferrer"">tf.nn.in_top_k</a></p>
<p>I wonder whether there are similar api in pytorch?</p>
",2020-11-08 03:12:27,"<p>AFAIK there is no equivalent <code>in_top_k</code> function built into pytorch. It's relatively straightforward to write one. For example</p>
<pre class=""lang-py prettyprint-override""><code>def in_top_k(targets, preds, k):
    topk = preds.topk(k)[1]
    return (targets.unsqueeze(1) == topk).any(dim=1)
</code></pre>
",9276708,1827,https://stackoverflow.com/questions/64734480,Documentation Replicability
65464181,An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?,"<ol>
<li><p>task : object_detection</p>
</li>
<li><p>environment: AWS sagemaker</p>
</li>
<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1</p>
</li>
<li><p>Main file to be run: <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/model_main_tf2.py"" rel=""nofollow noreferrer"">original</a></p>
</li>
<li><p>Problematic code segment from the main file:</p>
<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
    FLAGS.tpu_name)
    tf.config.experimental_connect_to_cluster(resolver)
    tf.tpu.experimental.initialize_tpu_system(resolver)
    strategy = tf.distribute.experimental.TPUStrategy(resolver)
    elif FLAGS.num_workers &gt; 1:
        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    else:
        strategy = tf.compat.v2.distribute.MirroredStrategy()
</code></pre>
</li>
<li><p>Problem : Can't find the proper value to be given as <code>tpu_name</code> argument.</p>
</li>
<li><p>My research on the problem:</p>
</li>
</ol>
<p>According to the tensorflow documentation in <a href=""https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver"" rel=""nofollow noreferrer"">tf.distribute.cluster_resolver.TPUClusterResolver</a>, it says that this resolver works only on Google Cloud platform.</p>
<blockquote>
<p>This is an implementation of cluster resolvers for the Google Cloud
TPU service.</p>
<p>TPUClusterResolver supports the following distinct environments:
Google Compute Engine Google Kubernetes Engine Google internal</p>
<p>It can be passed into tf.distribute.TPUStrategy to support TF2
training on Cloud TPUs.</p>
</blockquote>
<p>But from <a href=""https://github.com/tensorflow/tensorflow/issues/39721"" rel=""nofollow noreferrer"">this issue in github</a>, I found out that a similar code also works in Azure.</p>
<ol start=""8"">
<li>My question :</li>
</ol>
<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker</strong> ?</p>
<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?</p>
",2020-12-27 08:55:38,"<p>Let me clarify some confusion here. TPUs are only offered on Google Cloud and the <code>TPUClusterResolver</code> implementation queries GCP APIs to get the cluster config for the TPU node. Thus, no you can't use <code>TPUClusterResolver</code> with AWS sagemaker, but you should try it out with TPUs on GCP instead or try find some other documentation on Sagemaker's end on how they enable cluster resolving on their end (if they do).</p>
",9279666,75,https://stackoverflow.com/questions/65464181,Documentation Replicability
71059361,How to I train a keras functional API model with batched tf Dataset objects? (BatchDataset),"<p>I am constructing a tf keras model using the functional API. This model will train fine on large memory mapped arrays. However, for numerous reasons it can be advantageous to work with tensorflow Dataset objects. Therefore, I use from_tensor_slices() to convert my arrays to Dataset objects.
The problem is that the model will no longer train.</p>
<p>The keras docs: <a href=""https://keras.io/api/models/model_training_apis/"" rel=""nofollow noreferrer"">Model training APIs</a> indicate that dataset objects are acceptable.</p>
<p>The guide I'm following on how to train is found here: <a href=""https://www.tensorflow.org/guide/data#using_tfdata_with_tfkeras"" rel=""nofollow noreferrer"">Using tf.data with tf keras</a></p>
<p>Guides on how to use the keras functional API are <a href=""https://keras.io/guides/functional_api/"" rel=""nofollow noreferrer"">here</a>. However, training a functional API model with a tf Dataset object is not outlined.</p>
<p>A MWE is provided here:</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers

print('numpy version: {}'.format(np.__version__))
print('keras version: {}'.format(keras.__version__))
print('tensorflow version: {}'.format(tf.__version__))
</code></pre>
<blockquote>
<p>numpy version: 1.21.4<br />
keras version: 2.6.0<br />
tensorflow version: 2.6.0</p>
</blockquote>
<pre><code>X = np.random.uniform(size=(1000,75))
Y = np.random.uniform(size=(1000))

data = tf.data.Dataset.from_tensor_slices((X, Y))
print(data.cardinality().numpy())
</code></pre>
<blockquote>
<p>1000</p>
</blockquote>
<pre><code>data.batch(batch_size=100, drop_remainder=True)
</code></pre>
<blockquote>
<p>&lt;BatchDataset shapes: ((100, 75), (100,)), types: (tf.float64, tf.float64)&gt;</p>
</blockquote>
<pre><code>def API_Model(input_shape, name=&quot;test_model&quot;):

    inputs = layers.Input(shape=input_shape)
    x = layers.Dense(1)(inputs)
    outputs = layers.Activation('relu')(x)

    return keras.Model(inputs=inputs, outputs=outputs, name=name)

api_model = API_Model(input_shape=(X.shape[1],))
api_model.compile()
api_model.summary()
</code></pre>
<blockquote>
<pre><code>Model: &quot;test_model&quot;
_________________________________________________________________  
Layer (type)                 Output Shape              Param #     
=================================================================  
input_2 (InputLayer)         [(None, 75)]              0           
_________________________________________________________________  
dense_1 (Dense)              (None, 1)                 76          
_________________________________________________________________  
activation_1 (Activation)    (None, 1)                 0           
=================================================================  
Total params: 76  
Trainable params: 76  
Non-trainable params: 0  
_________________________________________________________________
</code></pre>
</blockquote>
<pre><code>api_model.fit(data, epochs=10)
</code></pre>
<blockquote>
<p>Epoch 1/10
WARNING:tensorflow:Model was constructed with shape (None, 75) for input
KerasTensor(type_spec=TensorSpec(shape=(None, 75), dtype=tf.float32, name='input_2'),
name='input_2', description=&quot;created by layer 'input_2'&quot;), but it was called on an input with
incompatible shape (75, 1).</p>
</blockquote>
<p>The error I receive is: <code>ValueError: Input 0 of layer dense_1 is incompatible with the layer: expected axis -1 of input shape to have value 75 but received input with shape (75, 1)</code></p>
<p>In addition, the error from my actual model I'm trying to train is slightly different but seems to be malfunctioning under the same principle. It is the following:</p>
<pre><code>ValueError: Input 0 is incompatible with layer pfn_base: expected shape=(None, 1086, 5), found shape=(1086, 5)
</code></pre>
<p>What is the proper way to train a keras functional API model on a BatchDataset object?</p>
",2022-02-10 03:13:43,"<p>You need to assign the batched dataset to a variable and you should also use a loss function in <code>model.compile</code> because the default value is <code>None</code> and you can't learn anything with it. Here is a working example:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers

print('numpy version: {}'.format(np.__version__))
print('keras version: {}'.format(keras.__version__))
print('tensorflow version: {}'.format(tf.__version__))
X = np.random.uniform(size=(1000,75))
Y = np.random.uniform(size=(1000))

data = tf.data.Dataset.from_tensor_slices((X, Y))
print(data.cardinality().numpy())
data = data.batch(batch_size=100, drop_remainder=True)

def API_Model(input_shape, name=&quot;test_model&quot;):

    inputs = layers.Input(shape=input_shape)
    x = layers.Dense(1)(inputs)
    outputs = layers.Activation('relu')(x)

    return keras.Model(inputs=inputs, outputs=outputs, name=name)

api_model = API_Model(input_shape=(X.shape[1],))
api_model.compile(loss='mse')
api_model.summary()
api_model.fit(data, epochs=10)
</code></pre>
",9008259,13,https://stackoverflow.com/questions/71059361,Documentation Replication on Other Examples
64100466,How to use tf.Dataset in Keras model.fit without specifying targets?,"<p>I want to use an AutoEncoder model with Keras functional API. Also I want to use <code>tf.data.Dataset</code> as an input pipeline for the model. However, there is limitation that I can pass the dataset to the <code>keras.model.fit</code> only with tuple <code>(inputs, targets)</code> accroding to the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p>Input data. It could be: A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights).</p>
</blockquote>
<p>So here is the question: can I pass the <code>tf.data.Dataset</code> without repeating inputs like that <code>(inputs, inputs)</code> and more like <code>(inputs, None)</code>. And if I can't, will the repeated inputs double the GPU memory for my model?</p>
",2020-09-28 10:31:37,"<p>You can use <code>map()</code> to return your input twice:</p>
<pre><code>import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Conv2DTranspose, Reshape
from functools import partial

(xtrain, _), (xtest, _) = tf.keras.datasets.mnist.load_data()

ds = tf.data.Dataset.from_tensor_slices(
    tf.expand_dims(tf.concat([xtrain, xtest], axis=0), axis=-1))

ds = ds.take(int(1e4)).batch(4).map(lambda x: (x/255, x/255))

custom_convolution = partial(Conv2D, kernel_size=(3, 3),
                             strides=(1, 1),
                             activation='relu',
                             padding='same')
custom_pooling = partial(MaxPool2D, pool_size=(2, 2))

conv_encoder = Sequential([
    custom_convolution(filters=16, input_shape=(28, 28, 1)),
    custom_pooling(),
    custom_convolution(filters=32),
    custom_pooling(),
    custom_convolution(filters=64),
    custom_pooling()
    ])

# conv_encoder(next(iter(ds))[0].numpy().astype(float)).shape
custom_transpose = partial(Conv2DTranspose,
                           padding='same',
                           kernel_size=(3, 3),
                           activation='relu',
                           strides=(2, 2))

conv_decoder = Sequential([
    custom_transpose(filters=32, input_shape=(3, 3, 64), padding='valid'),
    custom_transpose(filters=16),
    custom_transpose(filters=1, activation='sigmoid'),
    Reshape(target_shape=[28, 28, 1])
    ])

conv_autoencoder = Sequential([
    conv_encoder,
    conv_decoder
    ])

conv_autoencoder.compile(loss='binary_crossentropy', optimizer='adam')

history = conv_autoencoder.fit(ds)
</code></pre>
<pre><code>2436/2500 [============================&gt;.] - ETA: 0s - loss: 0.1282
2446/2500 [============================&gt;.] - ETA: 0s - loss: 0.1280
2456/2500 [============================&gt;.] - ETA: 0s - loss: 0.1279
2466/2500 [============================&gt;.] - ETA: 0s - loss: 0.1278
2476/2500 [============================&gt;.] - ETA: 0s - loss: 0.1277
2487/2500 [============================&gt;.] - ETA: 0s - loss: 0.1275
2497/2500 [============================&gt;.] - ETA: 0s - loss: 0.1274
2500/2500 [==============================] - 14s 6ms/step - loss: 0.1273
</code></pre>
",9048235,23,https://stackoverflow.com/questions/64100466,Documentation Replication on Other Examples
65436819,Keras: How to use `image_dataset_from_directory` to load test set?,"<p>I am using <code>tf.keras.preprocessing.image_dataset_from_directory</code> to load dataset as follows,</p>
<pre><code>train_dataset = tf.keras.preprocessing.image_dataset_from_directory(train_dir, 
                                                                    labels='inferred', 
                                                                    label_mode='categorical',
                                                                    batch_size=32,
                                                                    image_size=(224, 224))


val_dataset = tf.keras.preprocessing.image_dataset_from_directory(val_dir, 
                                                                  labels='inferred', 
                                                                  label_mode='categorical',
                                                                  batch_size=32,
                                                                  image_size=(224, 224))

</code></pre>
<p>However, when I check the document looks like this argument <code>labels</code> seem to be a must-have one,  but my test data has no labels, so how can I load test data? Is there a convenient and unified way to do this?</p>
",2020-12-24 10:08:53,"<p>If your data isn't labeled, I don't think you can call it the <em>test set</em>, since you won't be able to evaluate the performance of your algorithm using it.</p>
<p>The argument you're looking for is <code>label_mode</code>, see the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer""><code>documentation</code></a>. If you set it to <code>label_model=None</code>, it will not return a target;</p>
<blockquote>
<p><strong>label_mode</strong>: <strong>'int'</strong>: means that the labels are encoded as integers (e.g. for sparse_categorical_crossentropy loss).
<strong>'categorical'</strong> means that the labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss).
<strong>'binary'</strong> means that the labels (there can be only 2) are encoded as float32 scalars with values 0 or 1 (e.g. for binary_crossentropy).
<strong>None</strong> (no labels).</p>
</blockquote>
",9444831,1150,https://stackoverflow.com/questions/65436819,Documentation Replicability
57835609,pycharm says taht it cant find tensorflow.keras but it works,"<p>it always says tat it tf.keras is an ""Unresolved References"" but it works. why and how can i get to reconize it?</p>

<p>I tryed using </p>

<pre><code>import tensorflow as tf
from tensorflow import keras
</code></pre>

<p>and</p>

<pre><code>import tensorflow as tf
import tensorflow.keras
</code></pre>

<p>but then it just wont work at all</p>
",2019-09-07 16:36:31,"<p>Which <code>tensorflow</code> version do you use? There is a known issue with 2.0.0 <a href=""https://youtrack.jetbrains.com/issue/PY-37589"" rel=""nofollow noreferrer"">https://youtrack.jetbrains.com/issue/PY-37589</a></p>
",9560068,5,https://stackoverflow.com/questions/57835609,Documentation Replication on Other Examples
55573670,Unexpected output for tf.nn.sparse_softmax_cross_entropy_with_logits,"<p>The TensorFlow documentation for <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> explicitly declares that I should not apply softmax to the inputs of this op:</p>

<blockquote>
  <p>This op expects unscaled logits, since it performs a softmax on logits
  internally for efficiency. Do not call this op with the output of
  softmax, as it will produce incorrect results.</p>
</blockquote>

<p>However if I use cross entropy without softmax it gives me unexpected results. According to <a href=""https://cs231n.github.io/neural-networks-3/#sanitycheck"" rel=""nofollow noreferrer"">CS231n course</a> the expected loss value is around 2.3 for CIFAR-10:</p>

<blockquote>
  <p>For example, for CIFAR-10 with a Softmax classifier we would expect
  the initial loss to be 2.302, because we expect a diffuse probability
  of 0.1 for each class (since there are 10 classes), and Softmax loss
  is the negative log probability of the correct class so: -ln(0.1) =
  2.302.</p>
</blockquote>

<p>However without softmax I get much bigger values, for example 108.91984.</p>

<p>What exactly am I doing wrong with <code>sparse_softmax_cross_entropy_with_logits</code>? The TF code is shown below.</p>

<pre><code>import tensorflow as tf
import numpy as np
from tensorflow.python import keras


(_, _), (x_test, y_test) = keras.datasets.cifar10.load_data()
x_test = np.reshape(x_test, [-1, 32, 32, 3])

y_test = np.reshape(y_test, (10000,))
y_test = y_test.astype(np.int32)

x = tf.placeholder(dtype=tf.float32, shape=(None, 32, 32, 3))
y = tf.placeholder(dtype=tf.int32, shape=(None,))

layer = tf.layers.Conv2D(filters=16, kernel_size=3)(x)
layer = tf.nn.relu(layer)
layer = tf.layers.Flatten()(layer)
layer = tf.layers.Dense(units=1000)(layer)
layer = tf.nn.relu(layer)
logits = tf.layers.Dense(units=10)(layer)

# If this line is uncommented I get expected value around 2.3
# logits = tf.nn.softmax(logits)

loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,
                                                      logits=logits)
loss = tf.reduce_mean(loss, name='cross_entropy')

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    res = sess.run(loss, feed_dict={x: x_test[0:256], y: y_test[0:256]})
    print(""loss: "", res)
    # Expected output is value close to 2.3
    # Real outputs are 108.91984, 72.82324, etc.

</code></pre>
",2019-04-08 12:36:53,"<p>The issue is not in the lines </p>

<pre><code># If this line is uncommented I get expected value around 2.3
# logits = tf.nn.softmax(logits)
</code></pre>

<p>Images in cifar10 dataset are in RGB, thus pixel values are in range [0, 256). If you divide your <code>x_test</code> by 255 </p>

<pre><code>x_test = np.reshape(x_test, [-1, 32, 32, 3]).astype(np.float32) / 255
</code></pre>

<p>the values will be rescaled to [0,1] and <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> will return expected values</p>
",9565342,1086,https://stackoverflow.com/questions/55573670,Documentation Replication on Other Examples
62620694,What is difference between tf.keras.models.sequential vs tf.keras.sequential?,"<p>What is difference between <code>tf.keras.models.Sequential()</code> vs <code>tf.keras.Sequential()</code>? I don't understand differences between them quite well. Can somebody explain it to me? I am new to TensorFlow but have some basic understanding on machine learning.</p>
",2020-06-28 09:33:02,"<pre><code>tf.keras.models.Sequential
</code></pre>
<p>and</p>
<pre><code>tf.keras.Sequential
</code></pre>
<p>Do the same thing but they are from different versions of tensorflow. By the documentation (TensorFlow 2.0), <code>tf.keras.Sequential</code> is the most recent way of called this function.</p>
",9685065,456,https://stackoverflow.com/questions/62620694,Documentation Ambiguity
65712409,How to convert tf2 model so it will run on tflite interpreter,"<p><strong>Background:</strong>
I am trying to convert the tf2 model for SSD MobileNet V2 FPNLite 320x320 (for example) from the official tf zoo. The model should run eventually on raspberry pi, so I would like it to run on the tflite interpreter (without full tf). The docs imply that ssd model conversion is supported.</p>
<p><strong>Whats happening:</strong>
the process is detailed in <a href=""https://colab.research.google.com/drive/1P7T-ghA68mrNIRXjDPc_A59-lLxfq5yw?usp=sharing"" rel=""nofollow noreferrer"">this colab notebook</a>. It is failing with the error:</p>
<pre><code>ConverterError: &lt;unknown&gt;:0: error: loc(callsite(callsite(&quot;Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519&quot; at &quot;StatefulPartitionedCall@__inference_signature_wrapper_25508&quot;) at &quot;StatefulPartitionedCall&quot;)): 'tf.Size' op is neither a custom op nor a flex op
&lt;unknown&gt;:0: note: loc(&quot;StatefulPartitionedCall&quot;): called from
&lt;unknown&gt;:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
    tf.Size {device = &quot;&quot;}
</code></pre>
<p>if I add the flag tf.lite.OpsSet.SELECT_TF_OPS, it works but wont run on the rpi, as it does not have the ops.</p>
<p>Can this be done? Has anyone succeeded?</p>
",2021-01-14 02:07:52,"<p>Since TF.Size is not natively supported on TFLite you can use TF Select mode which fallbacks to TF for the missing op, which during conversion is enabled using &quot;SELECT_TF_OPS&quot; that you tried.
When you run inference you will need to use Interpreter which have Select ops linked.
See the <a href=""https://www.tensorflow.org/lite/guide/ops_select#run_inference"" rel=""nofollow noreferrer"">guide</a> on running inference.</p>
",9694304,400,https://stackoverflow.com/questions/65712409,Documentation Replication on Other Examples
60453533,Tensorflow what is the tf.contrib.nccl.allsum in new version?,"<p>It seems that from tensorflow 1.13, there is no api such as tf.contrib.nccl.allsum. However, in the Nvidia official GitHub <a href=""https://github.com/tkarras/progressive_growing_of_gans"" rel=""nofollow noreferrer"">https://github.com/tkarras/progressive_growing_of_gans</a>, which uses this old API to reduce sum from different gpu devices as the following. </p>

<pre><code># Sum gradients across devices.
            if len(devices) &gt; 1:
                with tf.name_scope('SumAcrossGPUs'), tf.device(None):
                    for var_idx, grad_shape in enumerate(self._grad_shapes):
                        g = [dev_grads[dev][var_idx][0] for dev in devices]
                        if np.prod(grad_shape): # nccl does not support zero-sized tensors
                            g = tf.contrib.nccl.all_sum(g)
                        for dev, gg in zip(devices, g):
                            dev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])
</code></pre>

<p>I am not sure if there is similar api which can achieve the same collective operation cross different devices. I have checked the Tensorflow official website and it seems that programmers prefer to use <code>tf.distribute.MirroredStrategy</code> which hides the raw operation of <code>NCCL</code>. Thanks a lot.</p>
",2020-02-28 14:16:17,"<p>I think the same API is <code>nccl_ops.all_sum</code>. I have demoed this API by the following code. </p>

<pre><code>import tensorflow as tf 
from tensorflow.python.ops import nccl_ops

a = []

with tf.device(""/GPU:0""):
    g = tf.constant([1,1])
    print(g)
    a.append(g)
with tf.device(""/GPU:1""):
    g = tf.constant([2,2])
    a.append(g)

b = nccl_ops.all_sum(a)
with tf.Session() as sess:
    print(sess.run(b))
</code></pre>

<p>I am not sure what tensorflow team will do in the future. But now we can use it to do the collective operations.</p>
",9881203,961,https://stackoverflow.com/questions/60453533,Documentation Replication on Other Examples
54557468,"In tf.keras.layers.Embedding, why it is important to know the size of dictionary?","<p>Same as the title, in tf.keras.layers.Embedding, why it is important to know the size of dictionary as input dimension?</p>
",2019-02-06 15:47:07,"<p>In such setting, the dimensions/shapes of the tensors are the following:</p>

<ul>
<li>The input tensor has size <code>[batch_size, max_time_steps]</code> such that each element of that tensor can have a value in the range <code>0 to vocab_size-1</code>.</li>
<li>Then, each of the values from the input tensor pass through an embedding layer, that has a shape <code>[vocab_size, embedding_size]</code>. The output of the embedding layer is of shape <code>[batch_size, max_time_steps, embedding_size]</code>.</li>
<li>Then, in a typical seq2seq scenario, this <code>3D</code> tensor is the input of a recurrent neural network.</li>
<li>...</li>
</ul>

<p>Here's how this is implemented in Tensorflow so you can get a better idea:</p>

<pre><code>inputs = tf.placeholder(shape=(batch_size, max_time_steps), ...)
embeddings = tf.Variable(shape=(vocab_size, embedding_size], ...)
inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
</code></pre>

<p>Now, the output of the embedding lookup table has the <code>[batch_size, max_time_steps, embedding_size]</code> shape.</p>
",9890698,461,https://stackoverflow.com/questions/54557468,Documentation Replicability
71335830,What is the difference between tf.keras.layers.Input() and tf.keras.layers.Flatten(),"<p>I have seen multiple uses of both <code>tf.keras.layers.Flatten()</code> (ex. <a href=""https://www.tensorflow.org/tutorials/generative/autoencoder#first_example_basic_autoencoder"" rel=""nofollow noreferrer"">here</a>) and <code>tf.keras.layers.Input()</code> (ex. <a href=""https://www.tensorflow.org/tutorials/generative/autoencoder#define_a_convolutional_autoencoder"" rel=""nofollow noreferrer"">here</a>). After reading the documentation, it is not clear to me</p>
<ol>
<li>whether either of them uses the other</li>
<li>whether both can be used interchangeably when introducing to a model an input layer (let's say with dimensions <code>(64, 64)</code>)</li>
</ol>
",2022-03-03 10:46:49,"<p>I think the confusion comes from using a <code>tf.keras.Sequential</code> model, which does not need an explicit <code>Input</code> layer. Consider the following two models, which are equivalent:</p>
<pre><code>import tensorflow as tf

model1 = tf.keras.Sequential([
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(5, activation='relu'),
    ])

model1.build((1, 28, 28, 1))
</code></pre>
<pre><code>model2 = tf.keras.Sequential([
      tf.keras.layers.Input((28, 28, 1)),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(5, activation='relu'),
    ])
</code></pre>
<p>The difference is that I explicitly set the input shape of <code>model2</code> using an <code>Input</code> layer. In <code>model1</code>, the input shape will be inferred when you pass real data to it or call <code>model.build</code>.</p>
<p>Now regarding the <code>Flatten</code> layer, this layer simply converts a n-dimensional tensor (for example <code>(28, 28, 1)</code>) into a 1D tensor <code>(28 x 28 x 1)</code>. The <code>Flatten</code> layer and <code>Input</code> layer can coexist in a <code>Sequential</code> model but do not depend on each other.</p>
",9758352,427,https://stackoverflow.com/questions/71335830,Documentation Ambiguity
62236460,How to set bounds and constraints on Tensorflow Variables (tf.Variable),"<p>I am using Tensorflow to minimize a function. The function takes about 10 parameters. Every single parameter has bounds, e.g. a minimum and a maximum value the parameter is allowed to take. For example, the parameter x1 needs to be between 1 and 10.</p>

<p>I also have a pair of parameters that need to have the following constraint x2 > x3. In other words, x2 must always be bigger than x3. (In addition to this, x2 and x3 also have bounds, similarly to the example of x1 above.)</p>

<p>I know that tf.Variable has a ""constraint"" argument, however I can't really find any examples or documentation on how to use this to achieve the bounds and constraints as mentioned above.</p>

<p>Thank you!</p>
",2020-06-06 18:45:27,"<p>It seems to me (I can be mistaken) that <strong>constrained optimization</strong> (you can google for it in tensorflow) is not exactly the case for which tensroflow was designed. You may want to take a look at <a href=""https://github.com/google-research/tensorflow_constrained_optimization"" rel=""noreferrer"">this repo</a>, it may satisfy your needs, but as far as I understand, it's still not solving arbitrary constrained optimization, just some classification problems with labels and features, compatible with precision/recall scores. </p>

<p>If you want to use constraints on the tensorflow variable (i.e. some function applied after gradient step - which you can do manually also - by taking variable values, doing manipulations, and reassigning then), it means that you will be cutting variables after each step done using gradient in <em>general space</em>. It's a question whether you will successfully reach the right optimization goal this way, or your variables will stuck at boundaries, because <code>general</code> gradient will point somewhere outside.</p>

<p><strong>My approach 1</strong></p>

<p>If your problem is simple enough. you can try to parametrize your <code>x2</code> and <code>x3</code> as <code>x2 = x3 + t</code>, and then try to do cutting in the graph:</p>

<pre><code>x3 = tf.get_variable('x3',
                   dtype=tf.float32,
                   shape=(1,),
                   initializer=tf.random_uniform_initializer(minval=1., maxval=10.),
                   constraint=lambda z: tf.clip_by_value(z, 1, 10))
t = tf.get_variable('t',
                   dtype=tf.float32,
                   shape=(1,),
                   initializer=tf.random_uniform_initializer(minval=1., maxval=10.),
                   constraint=lambda z: tf.clip_by_value(z, 1, 10))
x2 = x3 + t
</code></pre>

<p>Then, on a separate call additionally do</p>

<p><code>sess.run(tf.assign(x2, tf.clip_by_value(x2, 1.0, 10.0)))</code></p>

<p>But my opinion is that it won't work well. </p>

<p><strong>My approach 2</strong></p>

<p>I would also try to invent some loss terms to keep variables within constraints, which is more likely to work. For example, constraint for x2 to be in the interval <code>[1,10]</code> will be:</p>

<p><code>loss += alpha*tf.abs(tf.math.tan(((x-5.5)/4.5)*pi/2))</code></p>

<p>Here the expression under <code>tan</code> is brought to <code>-pi/2,pi/2</code> and then <code>tan</code> function is used to make it grow very rapidly when it reaches boundaries. In this case I think you're more likely to find your optimum, but again the loss weight <code>alpha</code> might be too big and training will stuck somewhere nearby, if required value of <code>x2</code> lies near the boundary. In this case you can try to use smaller <code>alpha</code>.</p>
",9762137,1678,https://stackoverflow.com/questions/62236460,Documentation Replicability
75305478,Can tf.gradienttape() calculate gradient of other library's function,"<p>If I include inside the <code>tf.GradientTape()</code> some functions from other Python libraries, like `sklearn.decomposition.PCA.inverse_transform()', can TensorFlow calculate gradients from that function?</p>
<p>Specifically, can tf automatically differetiate <code>pca_inverse_tranform = pca.inverse_transform(h2)</code>?</p>
<pre><code>...
from sklearn.decomposition import PCA
pca = PCA(n_components=10)
pca.fit(x)
...
with tf.GradientTape() as tape:
    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])
    h1 = tf.nn.relu(h1)
    h2 = h1@w2 + tf.broadcast_to(b2, [x.shape[0], 10])
    h2 = tf.nn.relu(h2)

    pca_inverse_tranform = pca.inverse_transform(h2)

    loss = tf.square(pca_inverse_tranform - target)
    loss = tf.reduce_mean(loss)
[dl_dw1, dl_db1, dl_dw2, dl_db2] = tape.gradient(loss, [w1,b1,w2,b2])
</code></pre>
",2023-02-01 03:53:11,"<p>I found the answer from <a href=""https://www.tensorflow.org/guide/autodiff#2_did_calculations_outside_of_tensorflow"" rel=""nofollow noreferrer"">tf docs</a>. It says <code>The tape can't record the gradient path if the calculation exits TensorFlow. For example:</code></p>
<pre><code>x = tf.Variable([[1.0, 2.0],
                 [3.0, 4.0]], dtype=tf.float32)

with tf.GradientTape() as tape:
  x2 = x**2

  # This step is calculated with NumPy
  y = np.mean(x2, axis=0)

  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor
  # using `tf.convert_to_tensor`.
  y = tf.reduce_mean(y, axis=0)

print(tape.gradient(y, x))
</code></pre>
<p>So, the answer to the question is &quot;No, tf can't calculate gradient of other library's functions&quot;.</p>
",9919423,380,https://stackoverflow.com/questions/75305478,Documentation Replicability
55253299,How to log a tensorflow layer output in tf.estimator.Estimator(),"<p>In this <a href=""https://www.tensorflow.org/tutorials/estimators/cnn"" rel=""nofollow noreferrer"">tutorial</a>, they successfully log the softmax function by giving a name to the <code>tf.nn.softmax</code> node.</p>

<pre><code>tf.nn.softmax(logits, name=""softmax_tensor"") # giving name to the node
.
.
.
tensors_to_log = {""probadfabilities"": ""softmax_tensor""} # logging the node

logging_hook = tf.train.LoggingTensorHook(
    tensors=tensors_to_log, every_n_iter=50)
eval_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"": eval_data},
    y=eval_labels,
    num_epochs=1,
    shuffle=False)
</code></pre>

<p>Now, instead of the softmax, I would like to also log the output of the last Dense layer.</p>

<pre><code>logits = tf.layers.dense(inputs=dropout, units=10, name='logits')
.
.
.
tensors_to_log = {""last_layer"": ""logits""}
</code></pre>

<p>But  it gives me the following error</p>

<blockquote>
  <p>KeyError: ""The name 'logits:0' refers to a Tensor which does not
  exist. The operation, 'logits', does not exist in the graph.""</p>
</blockquote>

<p><strong>My question is</strong>: how to log the layer output in tensorflow?</p>

<h2>My code</h2>

<pre><code>import tensorflow as tf
import numpy as np
import os

tf.logging.set_verbosity(tf.logging.INFO)

def cnn_model_fn(features, labels, mode):
    """"""Model function for CNN.""""""
    # Input Layer
    input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])

    # Convolutional Layer #1
    conv1 = tf.layers.conv2d(
      inputs=input_layer,
      filters=128,
      kernel_size=[7, 7],
      padding=""same"",
      activation=tf.nn.relu)

    # Pooling Layer #1
    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

    # Convolutional Layer #2 and Pooling Layer #2
    conv2 = tf.layers.conv2d(
      inputs=pool1,
      filters=256,
      kernel_size=[5, 5],
      padding=""same"",
      activation=tf.nn.relu)
    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

    # Dense Layer
    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 256])
    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
    dropout = tf.layers.dropout(
      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

    # Logits Layer
    logits = tf.layers.dense(inputs=dropout, units=10, name='logits')

    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

    # Calculate Loss (for both TRAIN and EVAL modes)
    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

    # Configure the Training Op (for TRAIN mode)
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
        train_op = optimizer.minimize(
            loss=loss,
            global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

    # Add evaluation metrics (for EVAL mode)
    eval_metric_ops = {
      ""accuracy"": tf.metrics.accuracy(
          labels=labels, predictions=predictions[""classes""])
    }
    return tf.estimator.EstimatorSpec(
      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)

((train_data, train_labels),
 (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()

train_data = train_data/np.float32(255)
train_labels = train_labels.astype(np.int32)  # not required

eval_data = eval_data/np.float32(255)
eval_labels = eval_labels.astype(np.int32)

mnist_classifier = tf.estimator.Estimator(
    model_fn=cnn_model_fn, model_dir=""./mnist_convnet_model"")

# Set up logging for predictions
tensors_to_log = {""last_layer"": ""logits""}

logging_hook = tf.train.LoggingTensorHook(
    tensors=tensors_to_log, every_n_iter=50)

# Train the model
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"": train_data},
    y=train_labels,
    batch_size=100,
    num_epochs=None,
    shuffle=True)

# train one step and display the probabilties
mnist_classifier.train(
    input_fn=train_input_fn,
    steps=10,
    hooks=[logging_hook])
</code></pre>
",2019-03-20 03:49:48,"<p>In the <a href=""https://github.com/tensorflow/models/tree/master/official/resnet"" rel=""nofollow noreferrer"">tf.official ResNet implementation</a>, they use <code>tf.identity</code> for this purpose:</p>

<pre><code>logits = tf.identity(logits, 'logits')
</code></pre>
",9793316,2953,https://stackoverflow.com/questions/55253299,Documentation Replication on Other Examples
54059805,How to convert tensor dtype=tf.float32_ref to dtype=tf.float32?,"<p>I want to use the modify word_embeddings dtype from  <code>float32_ref</code> to <code>float32</code> through the function <code>tf.cast()</code>:</p>

<pre><code>   word_embeddings_modify=tf.cast(word_embeddings,dtype=tf.float32)
</code></pre>

<p>But it did not work as expected and word_embeddings_modify dtype still tf.float32_ref.</p>

<pre><code>   word_embeddings = tf.scatter_nd_update(var_output, error_word_f,sum_all)
   word_embeddings_modify=tf.cast(word_embeddings,dtype=tf.float32)
   word_embeddings_dropout = tf.nn.dropout(word_embeddings_2, dropout_pl)
</code></pre>
",2019-01-06 08:19:27,"<p>You can dereference a <code>_ref</code> type using <a href=""https://www.tensorflow.org/api_docs/python/tf/identity"" rel=""nofollow noreferrer""><code>tf.identity</code></a></p>

<pre><code>word_embeddings = tf.identity(word_embeddings)
</code></pre>
",9799714,75,https://stackoverflow.com/questions/54059805,Documentation Replicability
64046604,How to make tf.data.Dataset.map function executed only once in first epoch?,"<p>I try to implement some transformation on dataset by using tf.data.Dataset.</p>
<p>I found the transformation was executed in every epoch. Is it possible that the map function is executed in first epoch?</p>
",2020-09-24 12:37:42,"<p>You can just use different datasets. That's easy in a custom training loop. Just like that:</p>
<pre><code>def transformation(inputs, labels):
    tf.print('With transformation!')
    return inputs, labels

def no_transformation(inputs, labels):
    tf.print('No transformation!')
    return inputs, labels

data_with_transform = data.take(4).map(transformation).batch(4)
data_no_transform = data.take(4).map(no_transformation).batch(4)
</code></pre>
<p>And then later:</p>
<pre><code>if epoch &lt; 1:
    ds = data_with_transform
else:
    ds = data_no_transform

for X_train, y_train in ds:
    train_step(X_train, y_train)
</code></pre>
<p>Full example:</p>
<pre><code>import tensorflow_datasets as tfds
import tensorflow as tf

data, info = tfds.load('iris', split='train', as_supervised=True,
                       with_info=True)

def transformation(inputs, labels):
    tf.print('With transformation!')
    return inputs, labels

def no_transformation(inputs, labels):
    tf.print('No transformation!')
    return inputs, labels

data_with_transform = data.take(4).map(transformation).batch(4)
data_no_transform = data.take(4).map(no_transformation).batch(4)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(info.features['label'].num_classes)
])

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

train_loss = tf.keras.metrics.Mean()
train_acc = tf.keras.metrics.SparseCategoricalAccuracy()

opt = tf.keras.optimizers.Adam(learning_rate=1e-3)


@tf.function
def train_step(inputs, labels):
    with tf.GradientTape() as tape:
        logits = model(inputs)
        loss = loss_object(labels, logits)

    gradients = tape.gradient(loss, model.trainable_variables)
    opt.apply_gradients(zip(gradients, model.trainable_variables))
    train_loss(loss)
    train_acc(labels, logits)


def main(epochs=5):

    for epoch in range(epochs):

        train_loss.reset_states()
        train_acc.reset_states()

        if epoch &lt; 1:
            ds = data_with_transform
        else:
            ds = data_no_transform

        for X_train, y_train in ds:
            train_step(X_train, y_train)

if __name__ == '__main__':
    main()
</code></pre>
<pre><code>With transformation!
With transformation!
With transformation!
With transformation!

No transformation!
No transformation!
No transformation!
No transformation!

No transformation!
No transformation!
No transformation!
No transformation!

No transformation!
No transformation!
No transformation!
No transformation!

No transformation!
No transformation!
No transformation!
No transformation!
</code></pre>
",10088497,179,https://stackoverflow.com/questions/64046604,Documentation Replication on Other Examples
55996869,Tensorflow Matrix square Root code source,"<p>Where can we get the source code of the tf.linalg.sqrtm and can we modify it ?</p>
",2019-05-05 22:12:02,"<p><code>tf.linalg.sqrtm</code> is just an alias for <code>matrix_square_root</code>. The source for which is hosted on github and can be found <a href=""https://github.com/tensorflow/tensorflow/blob/df3a3375941b9e920667acfe72fb4c33a8f45503/tensorflow/contrib/opt/python/training/matrix_functions.py"" rel=""nofollow noreferrer"">here</a>.</p>

<p>TensorFlow code is open source under the Apache Licence so, sure, if you know what you are doing you can fork it and amend it to your own requirements. Maybe if your use-case is specific to your own requirements you could also just build your own alternative <code>sqrtm</code> function.</p>
",10112379,17,https://stackoverflow.com/questions/55996869,Documentation Replicability
58126494,How to Translate CSV Data into TFRecord Files,"<p>Currently I am working on a system that can take data from a CSV file and import it into a TFRecord file, However I have a few questions.</p>

<p>For starters, I need to know what type a TFRecord file can take, when using CSV types are removed.</p>

<p>Secondly, How can I convert data type:object into a type that a TFRecord can take?</p>

<p>I have two columns (will post example below) of two objects types that are strings, How can I convert that data to the correct type for TFRecords?</p>

<p>When importing Im hoping to append data from each row at a time into the TFRecord file, any advice or documentation would be great, I have been looking for some time at this problem and it seems there can only be ints,floats inputted into a TFRecord but what about a list/array of Integers?</p>

<p>Thankyou for reading!</p>

<p>Quick Note, I am using PANDAS to create a dataframe of the CSV file</p>

<p>Some Example Code Im using </p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from ast import literal_eval
import numpy as np
import tensorflow as tf


tf.compat.v1.enable_eager_execution()


def Start():
    db = pd.read_csv(""I:\Github\ClubKeno\Keno Project\Database\..\LotteryDatabase.csv"")

    pd.DataFrame = db
    print(db['Winning_Numbers'])
    print(db.dtypes)

    training_dataset = (
        tf.data.Dataset.from_tensor_slices(
            (
                tf.cast(db['Draw_Number'].values, tf.int64),
                tf.cast(db['Winning_Numbers'].values, tf.int64),
                tf.cast(db['Extra_Numbers'].values, tf.int64),
                tf.cast(db['Kicker'].values, tf.int64)
            )
        )
    )

    for features_tensor, target_tensor in training_dataset:
        print(f'features:{features_tensor} target:{target_tensor}')
</code></pre>

<p>Error Message:</p>

<p><img src=""https://cdn.discordapp.com/attachments/279786369902051328/626967249395122213/Capture.PNG"" alt=""Error Message""></p>

<p><a href=""https://cdn.discordapp.com/attachments/502661247809093673/626946732239880194/LotteryDatabase.csv"" rel=""nofollow noreferrer"">CSV Data</a></p>

<p>Update:
Got Two Columns of dating working using the following function...</p>

<pre class=""lang-py prettyprint-override""><code>dataset = tf.data.experimental.make_csv_dataset(
        file_pattern=databasefile,
        column_names=['Draw_Number', 'Kicker'],
        column_defaults=[tf.int64, tf.int64],
    )
</code></pre>

<p>However when trying to include my two other column object types
(What data looks like in both those columns)
<code>""3,9,11,16,25,26,28,29,36,40,41,46,63,66,67,69,72,73,78,80""</code></p>

<p>I get an error, here is the function I tried for that</p>

<pre class=""lang-py prettyprint-override""><code>    dataset = tf.data.experimental.make_csv_dataset(
        file_pattern=databasefile,
        column_names=['Draw_Number', 'Winning_Numbers', 'Extra_Numbers', 'Kicker'],
        column_defaults=[tf.int64, tf.compat.as_bytes, tf.compat.as_bytes, tf.int64],
        header=True,
        batch_size=100,
        field_delim=',',
        na_value='NA'
    )
</code></pre>

<p>This Error Appears:</p>

<pre><code>TypeError: Failed to convert object of type &lt;class 'function'&gt; to Tensor. Contents: &lt;function as_bytes at 0x000000EA530908C8&gt;. Consider casting elements to a supported type.
</code></pre>

<p>Should I try to Cast those two types outside the function and try combining it later into the TFRecord file alongside the tf.data from the <code>make_csv_dataset</code> function? </p>
",2019-09-27 01:06:27,"<blockquote>
<p>For starters, I need to know what type a TFRecord file can take, when using CSV types are removed.</p>
</blockquote>
<p>TFRecord accepts following datatypes-
string, byte, float32, float 64, bool, enum, int32, int64, uint32, uint64
Talked <a href=""https://www.tensorflow.org/beta/tutorials/load_data/tf_records#data_types_for_tfexample"" rel=""nofollow noreferrer"">here</a>.</p>
<blockquote>
<p>Secondly, How can I convert data type:object into a type that a TFRecord can take?</p>
</blockquote>
<p><a href=""https://www.tensorflow.org/beta/tutorials/load_data/tf_records#creating_a_tfexample_message"" rel=""nofollow noreferrer"">Here</a> is an example from TF, it is a bit complicated to digest it at once but if you read it carefully it is easy.</p>
<blockquote>
<p>have two columns (will post example below) of two objects types that are strings, How can I convert that data to the correct type for TFRecords?</p>
</blockquote>
<p>For string type data, you require <code>tf.train.BytesList</code> which returns a bytes_list from a string.</p>
<blockquote>
<p>When importing Im hoping to append data from each row at a time into the TFRecord file, any advice or documentation would be great, I have been looking for some time at this problem and it seems there can only be ints,floats inputted into a TFRecord but what about a list/array of Integers?</p>
<p>Quick Note, I am using PANDAS to create a dataframe of the CSV file</p>
</blockquote>
<p>Instead of reading csv file using Pandas, I would recommend you to use <code>tf.data.experimental.make_csv_dataset</code> defined <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/make_csv_dataset"" rel=""nofollow noreferrer"">here</a>. This will make this conversion process very faster than Pandas and will give you less compatibility issues to work with TF classes. If you use this function, then you will not need to read the csv file row by row but all at once using <code>map()</code> which uses <code>eager execution</code>. <a href=""https://www.tensorflow.org/beta/tutorials/load_data/tf_records#tfrecord_files_using_tfdata"" rel=""nofollow noreferrer"">This</a> is a good tutorial to get started.</p>
<p>Accidentally edited wrong section of the post</p>
",9873122,175,https://stackoverflow.com/questions/58126494,Requesting (Additional) Documentation/Examples
57857325,How to load custom op library when using c++?,"<p>I have build a very simple custom op <code>zero_out.dll</code> with bazel, it works when using python.</p>

<pre><code>import tensorflow as tf
zero_out_module = tf.load_op_library('./zero_out.dll')
with tf.Session(''):
  zero_out_module.zero_out([[1, 2], [3, 4]]).eval()
</code></pre>

<p>However I have to run the inference using C++, is there any c++ api which has the similar function as <code>tf.load_op_library</code>, as it seems that a lot of registeration work has been done in <code>tf.load_op_library</code>, TF has not counterpart c++ API?</p>
",2019-09-09 15:50:00,"<p>While there does not seem to be a public API for that in C++, the library loading functions are exposed in the <a href=""https://www.tensorflow.org/install/lang_c"" rel=""nofollow noreferrer"">TensorFlow API for C</a> (which is the API that <a href=""https://www.tensorflow.org/api_docs/python/tf/load_library"" rel=""nofollow noreferrer""><code>tf.load_library</code></a> uses). There is no ""nice"" documentation for it, but you can find them in <a href=""https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/c/c_api.h#L1626-L1655"" rel=""nofollow noreferrer""><code>c/c_api.h</code></a>:</p>

<pre class=""lang-c prettyprint-override""><code>// --------------------------------------------------------------------------
// Load plugins containing custom ops and kernels

// TF_Library holds information about dynamically loaded TensorFlow plugins.
typedef struct TF_Library TF_Library;

// Load the library specified by library_filename and register the ops and
// kernels present in that library.
//
// Pass ""library_filename"" to a platform-specific mechanism for dynamically
// loading a library. The rules for determining the exact location of the
// library are platform-specific and are not documented here.
//
// On success, place OK in status and return the newly created library handle.
// The caller owns the library handle.
//
// On failure, place an error status in status and return NULL.
TF_CAPI_EXPORT extern TF_Library* TF_LoadLibrary(const char* library_filename,
                                                 TF_Status* status);

// Get the OpList of OpDefs defined in the library pointed by lib_handle.
//
// Returns a TF_Buffer. The memory pointed to by the result is owned by
// lib_handle. The data in the buffer will be the serialized OpList proto for
// ops defined in the library.
TF_CAPI_EXPORT extern TF_Buffer TF_GetOpList(TF_Library* lib_handle);

// Frees the memory associated with the library handle.
// Does NOT unload the library.
TF_CAPI_EXPORT extern void TF_DeleteLibraryHandle(TF_Library* lib_handle);
</code></pre>

<p>These functions do actually call C++ code (see source in <a href=""https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/c/c_api.cc#L977-L995"" rel=""nofollow noreferrer""><code>c/c_api.cc</code></a>). However, the called functions, defined in <a href=""https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/core/framework/load_library.cc"" rel=""nofollow noreferrer""><code>core/framework/load_library.cc</code></a> does not have a header to include. The workaround to use it in C++ code, which they use in <a href=""https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/c/c_api.cc#L751-L753"" rel=""nofollow noreferrer""><code>c/c_api.cc</code></a>, is to declare the function yourself, and link the TensorFlow library.</p>

<pre class=""lang-cpp prettyprint-override""><code>namespace tensorflow {
// Helpers for loading a TensorFlow plugin (a .so file).
Status LoadLibrary(const char* library_filename, void** result,
                   const void** buf, size_t* len);
}
</code></pre>

<p>As far as I can tell there is no API to unload the library. The C API allows you only to delete the library handle object. This done just by freeing the pointer, but if you want to avoid trouble you should probably use the freeing function given by TensorFlow, <code>tensorflow::port:free</code>, declared in <a href=""https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/core/platform/mem.h#L34"" rel=""nofollow noreferrer""><code>core/platform/mem.h</code></a>. Again, if you cannot not or don't want to include that, you can declare the function yourself and it should work as well.</p>

<pre class=""lang-cpp prettyprint-override""><code>namespace tensorflow {
namespace port {
void Free(void* ptr);
}
}
</code></pre>
",9873377,75,https://stackoverflow.com/questions/57857325,Lack of Alternative Solutions/Documentation
55964427,tf.keras HDF5 Model and Keras HDF5 Model,"<p>I want to convert a Keras model to Tensorflow Lite model. When I examined the documentation, it is stated that we can use tf.keras HDF5 models as input. Does it mean I can use my saved HDF5 Keras model as input to it or tf.keras HDF5 model and Keras HDF5 models are different things?</p>

<p>Documentation: <a href=""https://www.tensorflow.org/lite/convert"" rel=""nofollow noreferrer"">https://www.tensorflow.org/lite/convert</a></p>

<p>Edit: I could convert my Keras model to Tensorflow Lite model with using this API, but I didn't test it yet. My code:</p>

<pre><code>converter = tf.lite.TFLiteConverter.from_keras_model_file(path + 'plant- 
recognition-model.h5')
tflite_model = converter.convert()

with open('plant-recognition-model.tflite', 'wb') as f:
   f.write(tflite_model)
</code></pre>
",2019-05-03 06:34:27,"<p>tf.keras HDF5 model and Keras HDF5 models are not different things, except for inevitable software version update synchronicity. <a href=""https://www.tensorflow.org/guide/keras"" rel=""nofollow noreferrer"">This is what the official docs say</a>:</p>

<blockquote>
  <p>tf.keras is TensorFlow's implementation of the Keras API specification. This is a high-level API to build and train models that includes first-class support for TensorFlow-specific functionality</p>
</blockquote>

<p>If the convertor can convert a keras model to tf.lite, it will deliver same results. But tf.lite functionality is more limited than tf.keras. If this feature set is not enough for you, you can still work with tensorflow, and enjoy its other advantages. </p>

<p>May be, it won't take too long before your models can run on a smartphone.</p>
",10170533,315,https://stackoverflow.com/questions/55964427,Documentation Replication on Other Examples
66711706,"Jax, jit and dynamic shapes: a regression from Tensorflow?","<p>The <a href=""https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#to-jit-or-not-to-jit"" rel=""noreferrer"">documentation for JAX</a> says,</p>
<blockquote>
<p>Not all JAX code can be JIT compiled, as it requires array shapes to be static &amp; known at compile time.</p>
</blockquote>
<p>Now I am somewhat surprised because tensorflow has operations like <code>tf.boolean_mask</code> that does what JAX seems incapable of doing when compiled.</p>
<ol>
<li>Why is there such a regression from Tensorflow? I was under the assumption that the underlying XLA representation was shared between the two frameworks, but I may be mistaken. I don't recall Tensorflow ever having troubles with dynamic shapes, and functions such as <code>tf.boolean_mask</code> have been around forever.</li>
<li>Can we expect this gap to close in the future? If not, why makes it impossible to do in JAX' jit what Tensorflow (among others) enables?</li>
</ol>
<p><strong>EDIT</strong></p>
<p>The gradient passes through <code>tf.boolean_mask</code> (obviously not on mask values, which are discrete); case in point here using TF1-style graphs where values are unknown, so TF cannot rely on them:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

x1 = tf.placeholder(tf.float32, (3,))
x2 = tf.placeholder(tf.float32, (3,))
y = tf.boolean_mask(x1, x2 &gt; 0)
print(y.shape)  # prints &quot;(?,)&quot;
dydx1, dydx2 = tf.gradients(y, [x1, x2])
assert dydx1 is not None and dydx2 is None
</code></pre>
",2021-03-19 16:07:35,"<p><strong>Currently, you can't</strong> (<em>as discussed here</em>)</p>
<p>This is not a limitation of JAX jit vs TensorFlow, but a limitation of XLA or rather how the two compile.</p>
<p>JAX uses simply XLA to compile the function. XLA <em>needs to know</em> the static shape. That's an inherent design choice <em>within XLA</em>.</p>
<p>TensorFlow uses the <code>function</code>: this creates a graph which can have shapes that are not statically known. This is not as efficient as using XLA, but still fine. However, <code>tf.function</code> offers an option <a href=""https://www.tensorflow.org/xla/tutorials/jit_compile"" rel=""nofollow noreferrer""><code>jit_compile</code></a>, which will compile the graph inside the function with XLA. While this offers often a decent speedup (for free), it comes with restrictions: shapes need to be statically known (surprise, surprise,...)</p>
<p>This is overall not too surprising behavior: computations in computers are in general faster (given a decent optimizer went over it) <em>the more is previously known</em> as more parameters (memory layout,...) can be optimally scheduled. The less is known, the slower the code (on this end is normal Python).</p>
",9973879,1817,https://stackoverflow.com/questions/66711706,Documentation Replication on Other Examples
55264696,Tensorflow dynamic_rnn deprecation,"<p>It seems that the <code>tf.nn.dynamic_rnn</code> has been deprecated: </p>

<blockquote>
  <p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use keras.layers.RNN(cell), which is equivalent to this API</p>
</blockquote>

<p>I have checked out keras.layers.RNN(cell) and it says that it can use masking which I assume can act as a replacement for <code>dynamic_rnn</code>'s <code>sequence_length</code> parameter? </p>

<blockquote>
  <p>This layer supports masking for input data with a variable number of timesteps. To introduce masks to your data, use an Embedding layer with the mask_zero parameter set to True.</p>
</blockquote>

<p>But there is no further information even in the Embedding docs for how I can use <code>mask_zero=True</code> to accommodate variable sequence lengths. Also, if I am using an embedding layer just to add a mask, how do I prevent the Embedding from changing my input and being trained? </p>

<p>Similar to this question <a href=""https://stackoverflow.com/questions/54989442/rnn-in-tensorflow-vs-keras-depreciation-of-tf-nn-dynamic-rnn"">RNN in Tensorflow vs Keras, depreciation of tf.nn.dynamic_rnn()</a> but I want to know how to use the mask to replace <code>sequence_length</code></p>
",2019-03-20 15:39:31,"<p>I needed an answer to this too, and figured out what I needed through the link at the bottom of your question.</p>

<p>In short, you do as the answer in the link says, but you 'simply' leave out the embedding layer if you're not interested in using one. I'd highly recommend reading and understanding the <a href=""https://stackoverflow.com/questions/54989442/rnn-in-tensorflow-vs-keras-depreciation-of-tf-nn-dynamic-rnn"">linked answer</a> as it goes into more detail, and the docs on <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking"" rel=""nofollow noreferrer"">Masking</a>, but here's a modified version which uses a masking layer over the sequence inputs to replace 'sequence_length':</p>

<pre><code>import numpy as np
import tensorflow as tf

pad_value = 0.37
# This is our input to the RNN, in [batch_size, max_sequence_length, num_features] shape
test_input = np.array(
[[[1.,   1.  ],
  [2,    2.  ],
  [1.,   1.  ],
  [pad_value, pad_value], # &lt;- a row/time step which contains all pad_values will be masked through the masking layer
  [pad_value, pad_value]],

 [[pad_value, pad_value],
  [1.,   1.  ],
  [2,    2.  ],
  [1.,   1.  ],
  [pad_value, pad_value]]])

# Define the mask layer, telling it to mask all time steps that contain all pad_value values
mask = tf.keras.layers.Masking(mask_value=pad_value)
rnn = tf.keras.layers.GRU(
    1,
    return_sequences=True,
    activation=None, # &lt;- these values and below are just used to initialise the RNN in a repeatable way for this example
    recurrent_activation=None,
    kernel_initializer='ones',
    recurrent_initializer='zeros',
    use_bias=True,
    bias_initializer='ones'
)

x = tf.keras.layers.Input(shape=test_input.shape[1:])
m0 = tf.keras.Model(inputs=x, outputs=rnn(x))
m1 = tf.keras.Model(inputs=x, outputs=mask(x))
m2 = tf.keras.Model(inputs=x, outputs=rnn(mask(x)))

print('raw inputs\n', test_input)
print('raw rnn output (no mask)\n', m0.predict(test_input).squeeze())
print('masked inputs\n', m1.predict(test_input).squeeze())
print('masked rnn output\n', m2.predict(test_input).squeeze())
</code></pre>

<p>out:</p>

<pre><code>raw inputs
 [[[1.   1.  ]
  [2.   2.  ]
  [1.   1.  ]
  [0.37 0.37]
  [0.37 0.37]]

 [[0.37 0.37]
  [1.   1.  ]
  [2.   2.  ]
  [1.   1.  ]
  [0.37 0.37]]]
raw rnn output (no mask)
 [[  -6.        -50.       -156.       -272.7276   -475.83362 ]
 [  -1.2876     -9.862801  -69.314    -213.94202  -373.54672 ]]
masked inputs
 [[[1. 1.]
  [2. 2.]
  [1. 1.]
  [0. 0.]
  [0. 0.]]

 [[0. 0.]
  [1. 1.]
  [2. 2.]
  [1. 1.]
  [0. 0.]]]
masked rnn output
 [[  -6.  -50. -156. -156. -156.]
 [   0.   -6.  -50. -156. -156.]]
</code></pre>

<p>Notice how with the mask applied, the calculations are not performed on a time step where the mask is active (i.e. where the sequence is padded out). Instead, state from the previous time step is carried forward.</p>

<p>A few other points to note:</p>

<ul>
<li>In the linked (and this) example, the RNN is created with various activation and initializer parameters. I assume this is to initialize the RNN to a known state for repeatability for the example. In practice, you would initialize the RNN how you would like.</li>
<li>The pad value can be any value you specify. Typically, padding using zeros is used. In the linked (and this) example, a value of 0.37 is used. I can only assume it is an arbitrary value to show the difference in the raw and masked RNN outputs, as a zero input value with this example RNN initialisation gives little/no difference in output, therefore 'some' value (i.e. 0.37) demonstrates the effect of the masking.</li>
<li>The <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Masking"" rel=""nofollow noreferrer"">Masking</a> docs state that rows/time steps are masked only if <em>all</em> of the values for that time step contain the mask value. For example, in the above, a time step of <code>[0.37, 2]</code> would still be fed to the network with those values, however, a time step of <code>[0.37, 0.37]</code> would be skipped over.</li>
<li>An alternative approach  to this problem instead of masking would be to train several times by batching the different sequence lengths together. For example, if you have a mix of sequence lengths of 10, 20, and 30, instead of padding them all out to 30 and masking, train using all your 10 sequence lengths, then your 20s, then 30s. Or if you have say lots of 100 sequence lengths, and also lots of 3, 4, 5 sequence lengths, you may want to pad your smaller ones to all 5 length and train twice using 100s and padded/masked 5s. You will likely gain training speed, but at the trade-off of less accuracy as you won't be able to shuffle between batches of different sequence lengths.</li>
</ul>
",10083000,119,https://stackoverflow.com/questions/55264696,Documentation Replication on Other Examples
71947836,Merge two tensorflow datasets into one dataset with inputs and labels,"<p>I have two tensorflow datasets that are generated using <code>timeseries_dataset_from_array</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array"" rel=""nofollow noreferrer"">docs</a>). One corresponds to the input of my network and the other one to the output. I guess we can call them the <em>inputs</em> dataset and the <em>targets</em> dataset, which are both the same shape (a timeseries window of a fixed size).</p>
<p>The code I'm using to generate these datasets goes like this:</p>
<pre><code>train_x = timeseries_dataset_from_array(
    df_train['x'],
    None,
    sequence_length,
    sequence_stride=sequence_stride,
    batch_size=batch_size
)
train_y = timeseries_dataset_from_array(
    df_train['y'],
    None,
    sequence_length,
    sequence_stride=sequence_stride,
    batch_size=batch_size
)
</code></pre>
<p>The problem is that when calling <code>model.fit</code>, tf.keras expects that if a <code>tf.data.Dataset</code> is given in the <code>x</code> argument, it has to provide both the <em>inputs</em> and <em>targets</em>. That is why I need to combine these two datasets into one, setting one as <em>inputs</em> and the other one as <em>targets</em>.</p>
",2022-04-21 01:08:30,"<p>Simplest way would be to use <code>tf.data.Dataset.zip</code>:</p>
<pre><code>import tensorflow as tf
import numpy as np

X = np.arange(100)
Y = X*2

sample_length = 20
input_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  X, None, sequence_length=sample_length, sequence_stride=sample_length)
target_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
  Y, None, sequence_length=sample_length, sequence_stride=sample_length)

dataset = tf.data.Dataset.zip((input_dataset, target_dataset))

for x, y in dataset:
  print(x.shape, y.shape)
</code></pre>
<pre><code>(5, 20) (5, 20)
</code></pre>
<p>You can then feed <code>dataset</code> directly to your model.</p>
",10649437,1296,https://stackoverflow.com/questions/71947836,Documentation Replication on Other Examples
63222770,Backpropagating through multiple forward passes,"<p>In usual backprop, we forward-prop once, compute gradients, then apply them to update weights. But suppose we wish to forward-prop <em>twice</em>, and backprop through <em>both</em>, and apply gradients only then (skip on first).</p>
<p>Suppose the following:</p>
<pre class=""lang-py prettyprint-override""><code>x = tf.Variable([2.])
w = tf.Variable([4.])

with tf.GradientTape(persistent=True) as tape:
    w.assign(w * x)
    y = w * w  # w^2 * x
print(tape.gradient(y, x))  # &gt;&gt;None
</code></pre>
<p>From <a href=""https://www.tensorflow.org/guide/autodiff#5_took_gradients_through_a_stateful_object"" rel=""nofollow noreferrer"">docs</a>, a <code>tf.Variable</code> is a <em>stateful</em> object, which <em>blocks gradients</em>, and weights are <code>tf.Variable</code>s.</p>
<p>Examples are differentiable hard attention (as opposed to RL), or simply passing a hidden state between layers in subsequent forward passes, as in diagram below. Neither TF nor Keras have an API-level support for stateful gradients, including <code>RNN</code>s, which only keep a stateful state tensor; gradient does <em>not</em> flow beyond one batch.</p>
<p>How can this be accomplished?</p>
<img src=""https://i.stack.imgur.com/Kq4zs.png"" width=""300"">
",2020-08-03 01:54:22,"<p>We'll need to elaborately apply <code>tf.while_loop</code>; from <a href=""https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/python/ops/tensor_array_ops.py#L950"" rel=""nofollow noreferrer""><code>help(TensorArray)</code></a>:</p>
<blockquote>
<p>This class is meant to be used with dynamic iteration primitives such as <code>while_loop</code> and <code>map_fn</code>.  It  supports gradient back-propagation via special &quot;flow&quot; control flow dependencies.</p>
</blockquote>
<p>We thus seek to write a loop such that all outputs we are to backpropagate through are written to a <code>TensorArray</code>. Code accomplishing this, and its high-level description, below. At bottom is a validating example.</p>
<hr>
<p><strong>Description</strong>:</p>
<ul>
<li>Code borrows from <a href=""https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/python/keras/backend.py#L3996"" rel=""nofollow noreferrer""><code>K.rnn</code></a>, rewritten for simplicity and relevance</li>
<li>For better understanding, I suggest  inspecting <code>K.rnn</code>, <a href=""https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/python/keras/layers/recurrent.py#L1362"" rel=""nofollow noreferrer""><code>SimpleRNNCell.call</code></a>, and <a href=""https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/python/keras/layers/recurrent.py#L722"" rel=""nofollow noreferrer""><code>RNN.call</code></a>.</li>
<li><code>model_rnn</code> has a few needless checks for sake of case 3; will link cleaner version</li>
<li>The idea's as follows: we traverse the network <em>first</em> bottom-to-top, <em>then</em> left-to-right, and write the entire forward pass to a <em>single</em> <code>TensorArray</code> under a single <code>tf.while_loop</code>; this ensures TF caches tensor ops throughout for backpropagation.</li>
</ul>
<hr>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.python.util import nest
from tensorflow.python.ops import array_ops, tensor_array_ops
from tensorflow.python.framework import ops


def model_rnn(model, inputs, states=None, swap_batch_timestep=True):
    def step_function(inputs, states):
        out = model([inputs, *states], training=True)
        output, new_states = (out if isinstance(out, (tuple, list)) else
                              (out, states))
        return output, new_states

    def _swap_batch_timestep(input_t):
        # (samples, timesteps, channels) -&gt; (timesteps, samples, channels)
        # iterating dim0 to feed (samples, channels) slices expected by RNN
        axes = list(range(len(input_t.shape)))
        axes[0], axes[1] = 1, 0
        return array_ops.transpose(input_t, axes)

    if swap_batch_timestep:
        inputs = nest.map_structure(_swap_batch_timestep, inputs)

    if states is None:
        states = (tf.zeros(model.inputs[0].shape, dtype='float32'),)
    initial_states = states
    input_ta, output_ta, time, time_steps_t = _process_args(model, inputs)

    def _step(time, output_ta_t, *states):
        current_input = input_ta.read(time)
        output, new_states = step_function(current_input, tuple(states))

        flat_state = nest.flatten(states)
        flat_new_state = nest.flatten(new_states)
        for state, new_state in zip(flat_state, flat_new_state):
            if isinstance(new_state, ops.Tensor):
                new_state.set_shape(state.shape)

        output_ta_t = output_ta_t.write(time, output)
        new_states = nest.pack_sequence_as(initial_states, flat_new_state)
        return (time + 1, output_ta_t) + tuple(new_states)

    final_outputs = tf.while_loop(
        body=_step,
        loop_vars=(time, output_ta) + tuple(initial_states),
        cond=lambda time, *_: tf.math.less(time, time_steps_t))

    new_states = final_outputs[2:]
    output_ta = final_outputs[1]
    outputs = output_ta.stack()
    return outputs, new_states


def _process_args(model, inputs):
    time_steps_t = tf.constant(inputs.shape[0], dtype='int32')

    # assume single-input network (excluding states)
    input_ta = tensor_array_ops.TensorArray(
        dtype=inputs.dtype,
        size=time_steps_t,
        tensor_array_name='input_ta_0').unstack(inputs)

    # assume single-input network (excluding states)
    # if having states, infer info from non-state nodes
    output_ta = tensor_array_ops.TensorArray(
        dtype=model.outputs[0].dtype,
        size=time_steps_t,
        element_shape=model.outputs[0].shape,
        tensor_array_name='output_ta_0')

    time = tf.constant(0, dtype='int32', name='time')
    return input_ta, output_ta, time, time_steps_t
</code></pre>
<hr>
<p><strong>Examples &amp; validating</strong>:</p>
<p>Case design: we feed the same input twice, which enables certain stateful vs stateless comparisons; results also hold for differing inputs.</p>
<ul>
<li><strong>Case 0</strong>: control; other cases must match this.</li>
<li><strong>Case 1</strong>: fail; gradients don't match, even though outputs and loss do. Backprop fails when feeding the halved sequence.</li>
<li><strong>Case 2</strong>: gradients match case 1. It may seem we've used only one <code>tf.while_loop</code>, but SimpleRNN uses one of its own for the 3 timesteps, and writes to a <code>TensorArray</code> that's discarded; this won't do. A workaround is to implement the SimpleRNN logic ourselves.</li>
<li><strong>Case 3</strong>: perfect match.</li>
</ul>
<p>Note that there's no such thing as a stateful RNN cell; statefulness is implemented in the <code>RNN</code> base class, and we've recreated it in <code>model_rnn</code>. This is likewise how any other layer is to be handled - feeding one step slice at a time for every forward pass.</p>
<pre class=""lang-py prettyprint-override""><code>import random
import numpy as np
import tensorflow as tf

from tensorflow.keras.layers import Input, SimpleRNN, SimpleRNNCell
from tensorflow.keras.models import Model

def reset_seeds():
    random.seed(0)
    np.random.seed(1)
    tf.compat.v1.set_random_seed(2)  # graph-level seed
    tf.random.set_seed(3)  # global seed

def print_report(case, model, outs, loss, tape, idx=1):
    print(&quot;\nCASE #%s&quot; % case)
    print(&quot;LOSS&quot;, loss)
    print(&quot;GRADS:\n&quot;, tape.gradient(loss, model.layers[idx].weights[0]))
    print(&quot;OUTS:\n&quot;, outs)


#%%# Make data ###############################################################
reset_seeds()
x0 = y0 = tf.constant(np.random.randn(2, 3, 4))
x0_2 = y0_2 = tf.concat([x0, x0], axis=1)
x00  = y00  = tf.stack([x0, x0], axis=0)

#%%# Case 0: Complete forward pass; control case #############################
reset_seeds()
ipt = Input(batch_shape=(2, 6, 4))
out = SimpleRNN(4, return_sequences=True)(ipt)
model0 = Model(ipt, out)
model0.compile('sgd', 'mse')
#%%#############################################################
with tf.GradientTape(persistent=True) as tape:
    outs = model0(x0_2, training=True)
    loss = model0.compiled_loss(y0_2, outs)
print_report(0, model0, outs, loss, tape)

#%%# Case 1: Two passes, stateful RNN, direct feeding ########################
reset_seeds()
ipt = Input(batch_shape=(2, 3, 4))
out = SimpleRNN(4, return_sequences=True, stateful=True)(ipt)
model1 = Model(ipt, out)
model1.compile('sgd', 'mse')
#%%#############################################################
with tf.GradientTape(persistent=True) as tape:
    outs0 = model1(x0, training=True)
    tape.watch(outs0)  # cannot even diff otherwise
    outs1 = model1(x0, training=True)
    tape.watch(outs1)
    outs = tf.concat([outs0, outs1], axis=1)
    tape.watch(outs)
    loss = model1.compiled_loss(y0_2, outs)
print_report(1, model1, outs, loss, tape)

#%%# Case 2: Two passes, stateful RNN, model_rnn #############################
reset_seeds()
ipt = Input(batch_shape=(2, 3, 4))
out = SimpleRNN(4, return_sequences=True, stateful=True)(ipt)
model2 = Model(ipt, out)
model2.compile('sgd', 'mse')
#%%#############################################################
with tf.GradientTape(persistent=True) as tape:
    outs, _ = model_rnn(model2, x00, swap_batch_timestep=False)
    outs = tf.concat(list(outs), axis=1)
    loss = model2.compiled_loss(y0_2, outs)
print_report(2, model2, outs, loss, tape)

#%%# Case 3: Single pass, stateless RNN, model_rnn ###########################
reset_seeds()
ipt  = Input(batch_shape=(2, 4))
sipt = Input(batch_shape=(2, 4))
out, state = SimpleRNNCell(4)(ipt, sipt)
model3 = Model([ipt, sipt], [out, state])
model3.compile('sgd', 'mse')
#%%#############################################################
with tf.GradientTape(persistent=True) as tape:
    outs, _ = model_rnn(model3, x0_2)
    outs = tf.transpose(outs, (1, 0, 2))
    loss = model3.compiled_loss(y0_2, outs)
print_report(3, model3, outs, loss, tape, idx=2)
</code></pre>
<hr>
<p><strong>Vertical flow</strong>: we've validated horizontal, <em>timewise</em>-backpropagation; what about vertical?</p>
<p>To this end, we implement a stacked stateful RNN; results below. All outputs on my machine, <a href=""https://pastebin.com/bMN6WGMt"" rel=""nofollow noreferrer"">here</a>.</p>
<p>We've hereby validated both <strong>vertical</strong> and <strong>horizontal</strong> stateful backpropagation. This can be used to implement arbitrarily complex forward-prop logic with correct backprop. Applied example <a href=""https://stackoverflow.com/a/63222778/10133797"">here</a>.</p>
<pre class=""lang-py prettyprint-override""><code>#%%# Case 4: Complete forward pass; control case ############################
reset_seeds()
ipt = Input(batch_shape=(2, 6, 4))
x   = SimpleRNN(4, return_sequences=True)(ipt)
out = SimpleRNN(4, return_sequences=True)(x)
model4 = Model(ipt, out)
model4.compile('sgd', 'mse')
#%%
with tf.GradientTape(persistent=True) as tape:
    outs = model4(x0_2, training=True)
    loss = model4.compiled_loss(y0_2, outs)
print(&quot;=&quot; * 80)
print_report(4, model4, outs, loss, tape, idx=1)
print_report(4, model4, outs, loss, tape, idx=2)

#%%# Case 5: Two passes, stateless RNN; model_rnn ############################
reset_seeds()
ipt = Input(batch_shape=(2, 6, 4))
out = SimpleRNN(4, return_sequences=True)(ipt)
model5a = Model(ipt, out)
model5a.compile('sgd', 'mse')

ipt  = Input(batch_shape=(2, 4))
sipt = Input(batch_shape=(2, 4))
out, state = SimpleRNNCell(4)(ipt, sipt)
model5b = Model([ipt, sipt], [out, state])
model5b.compile('sgd', 'mse')
#%%
with tf.GradientTape(persistent=True) as tape:
    outs = model5a(x0_2, training=True)
    outs, _ = model_rnn(model5b, outs)
    outs = tf.transpose(outs, (1, 0, 2))
    loss = model5a.compiled_loss(y0_2, outs)
print_report(5, model5a, outs, loss, tape)
print_report(5, model5b, outs, loss, tape, idx=2)
</code></pre>
",10133797,19301,https://stackoverflow.com/questions/63222770,Documentation Ambiguity
63715707,Does image_dataset_from_directory load all the images into memory at once?,"<p>I'm new to machine learning, and I am trying to create an image classifier, I want to load the dataset, but I want to do it in a way such that it does not take up all of my memory. Reading the tensorflow documentation, it says that iteration of a dataset happens in streaming fashion, and I am wondering if tf.keras.preprocessing.image_dataset_from_directory will load the images at once or &quot;stream&quot; it a batch at a time. If not I was thinking of making a generator to read file names one at a time and load them when the batches are ready with keras.utils.Sequence.</p>
",2020-09-03 01:55:37,"<pre><code>tf.keras.preprocessing.image_dataset_from_directory(
    directory, labels='inferred', label_mode='int', class_names=None,
    color_mode='rgb', batch_size=32, image_size=(256, 256), shuffle=True, seed=None,
    validation_split=None, subset=None, interpolation='bilinear', follow_links=False
)
</code></pre>
<p>If you define batch size it will generate data <code>according to batch size</code> otherwise <code>default batch size is 32</code>. <strong>It is never possible to load the whole data in a single batch in normal computer.</strong> For more details read <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer"">documentation</a>.</p>
",10171841,669,https://stackoverflow.com/questions/63715707,Documentation Replication on Other Examples
66144466,Tensorflow Serve SignatureDefs for classifier,"<p>I trained a BERT text classifier following these steps, with own texts and some modifications:</p>
<p><a href=""https://www.tensorflow.org/tutorials/text/classify_text_with_bert"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/classify_text_with_bert</a></p>
<p>To export the model and run it with Tensorflow Serve works well:</p>
<p><a href=""https://www.tensorflow.org/tfx/serving/docker"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/serving/docker</a></p>
<p>Unfortunately, I can not really figure out how to define the SignatureDefs for a classifier, so that the classifier endpoint for Tensorflow Serve is defined. The <code>:predict</code> endpoint works well and seems to be the default signature.</p>
<p>Obviously, I have to define the signatures, when I save the model. Since the documentation is not very exhaustive on this topic, I am not sure how to define the classifier signature.</p>
<p><a href=""https://www.tensorflow.org/tfx/serving/signature_defs"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/serving/signature_defs</a></p>
<p>In the above example for BERT, the <code>serving_results</code> just define the <code>reloaded_model</code> with <code>tf.constant(examples)</code> and instantiates it with <code>serving_results = tf.sigmoid(serving_results['classifier')]</code>.</p>
<p>So, I assume I have to give the activation function and the <code>classifier</code> signature as arguments, when calling the <code>model.save</code> method.</p>
<p><code>predict</code>endpoint works, <code>classify</code> endpoint gives the error:</p>
<p><code>{&quot;error&quot;: &quot;No classification inputs found in SignatureDef: inputs {\n  key: \&quot;text\&quot;\n  value {\n    name: \&quot;serving_default_text:0\&quot;\n    dtype: DT_STRING\n    tensor_shape {\n      dim {\n        size: -1\n      }\n    }\n  }\n}\noutputs {\n  key: \&quot;classifier\&quot;\n  value {\n    name: \&quot;StatefulPartitionedCall_2:0\&quot;\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: -1\n      }\n      dim {\n        size: 1\n      }\n    }\n  }\n}\nmethod_name: \&quot;tensorflow/serving/predict\&quot;\n&quot;}</code></p>
<p>I would be grateful for any hints.</p>
",2021-02-10 20:23:16,"<p>The right documentation for defining the Signatures can be found here.</p>
<p><a href=""https://www.tensorflow.org/tfx/serving/serving_basic?hl=en"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/serving/serving_basic?hl=en</a></p>
<pre><code>builder.add_meta_graph_and_variables(
    sess, [tf.compat.v1.saved_model.tag_constants.SERVING],
    signature_def_map={
        'predict_images':
            prediction_signature,
        tf.compat.v1.saved_model.signature_constants
            .DEFAULT_SERVING_SIGNATURE_DEF_KEY:
            classification_signature,
    },
</code></pre>
",10263591,43,https://stackoverflow.com/questions/66144466,Lack of Alternative Solutions/Documentation
63383594,How does Tensorflow build() work from tf.keras.layers.Layer,"<p>I was wondering if anyone knew how the <code>build()</code> function works from the <code>tf.keras.layers.Layer</code> class under the hood. According to the <a href=""https://www.tensorflow.org/tutorials/customization/custom_layers"" rel=""noreferrer"">documentation</a>:</p>
<blockquote>
<p><em>build is called when you know the shapes of the input tensors and can
do the rest of the initialization</em></p>
</blockquote>
<p>so to me it seems like the class is behaving similar to this:</p>
<pre><code>class MyDenseLayer:
  def __init__(self, num_outputs):
    self.num_outputs = num_outputs

  def build(self, input_shape):
    self.kernel = self.add_weight(&quot;kernel&quot;,
                                  shape=[int(input_shape[-1]), self.num_outputs])

  def __call__(self, input):
    self.build(input.shape) ## build is called here when input shape is known
    return tf.matmul(input, self.kernel)
</code></pre>
<p>I can't imagine <code>build()</code> would be called for ever <code>__call__</code>, but it is the only place where the input is passed in. Does anyone know how exactly this works under the hood?</p>
",2020-08-12 19:35:33,"<p>The <code>Layer.build()</code> method is typically used to instantiate the weights of the layer. See the <a href=""https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/layers/core.py#L1159-L1190"" rel=""noreferrer"">source code for <code>tf.keras.layers.Dense</code></a> for an example, and note that the weight and bias tensors are created in that function. The <code>Layer.build()</code> method takes an <code>input_shape</code> argument, and the shape of the weights and biases often depend on the shape of the input.</p>
<p>The <code>Layer.call()</code> method, on the other hand, implements the forward-pass of the layer. You do not want to overwrite <code>__call__</code>, because that is implemented in the base class <code>tf.keras.layers.Layer</code>. In a custom layer, you should implement <code>call()</code>.</p>
<p><code>Layer.call()</code> does not call <code>Layer.build()</code>. However, <code>Layer().__call__()</code> <em>does</em> call it if the layer has not been built yet (<a href=""https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/keras/engine/base_layer.py#L981-L982"" rel=""noreferrer"">source</a>), and that will set an attribute <code>self.built = True</code> to prevent <code>Layer.build()</code> from being called again. In other words, <code>Layer.__call__()</code> only calls <code>Layer.build()</code> the first time it is called.</p>
",11065415,467,https://stackoverflow.com/questions/63383594,Documentation Replication on Other Examples
60639731,Tensorboard for custom training loop in Tensorflow 2,"<p>I want to create a custom training loop in tensorflow 2 and use tensorboard for visualization. Here is an example I've created based on tensorflow documentation:</p>

<pre><code>import tensorflow as tf
import datetime

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""    # which gpu to use

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))

train_dataset = train_dataset.shuffle(60000).batch(64)
test_dataset = test_dataset.batch(64)


def create_model():
    return tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28), name='Flatten_1'),
        tf.keras.layers.Dense(512, activation='relu', name='Dense_1'),
        tf.keras.layers.Dropout(0.2, name='Dropout_1'),
        tf.keras.layers.Dense(10, activation='softmax', name='Dense_2')
    ], name='Network')


# Loss and optimizer
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

# Define our metrics
train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')
test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')

@tf.function
def train_step(model, optimizer, x_train, y_train):
    with tf.GradientTape() as tape:
        predictions = model(x_train, training=True)
        loss = loss_object(y_train, predictions)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    train_loss(loss)
    train_accuracy(y_train, predictions)

@tf.function
def test_step(model, x_test, y_test):
    predictions = model(x_test)
    loss = loss_object(y_test, predictions)

    test_loss(loss)
    test_accuracy(y_test, predictions)


current_time = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
train_log_dir = '/NAS/Dataset/logs/gradient_tape/' + current_time + '/train'
test_log_dir = '/NAS/Dataset/logs/gradient_tape/' + current_time + '/test'
train_summary_writer = tf.summary.create_file_writer(train_log_dir)
test_summary_writer = tf.summary.create_file_writer(test_log_dir)

model = create_model()  # reset our model

EPOCHS = 5


for epoch in range(EPOCHS):
    for (x_train, y_train) in train_dataset:
        train_step(model, optimizer, x_train, y_train)
    with train_summary_writer.as_default():
        tf.summary.scalar('loss', train_loss.result(), step=epoch)
        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)

    for (x_test, y_test) in test_dataset:
        test_step(model, x_test, y_test)
    with test_summary_writer.as_default():
        tf.summary.scalar('loss', test_loss.result(), step=epoch)
        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)

    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
    print(template.format(epoch + 1,
                          train_loss.result(),
                          train_accuracy.result() * 100,
                          test_loss.result(),
                          test_accuracy.result() * 100))

    # Reset metrics every epoch
    train_loss.reset_states()
    test_loss.reset_states()
    train_accuracy.reset_states()
    test_accuracy.reset_states()
</code></pre>

<p>I am accessing tensorboard with the following command on terminal:</p>

<pre><code>tensorboard --logdir=.....
</code></pre>

<p>The code above produce summaries for losses and metrics. My question is:</p>

<ul>
<li><strong>How can i produce the graph of this process?</strong></li>
</ul>

<p>I've tried to use the recommended commands from tensorflow: <strong>tf.summary.trace_on()</strong> and <strong>tf.summary.trace_export()</strong>, but I haven't managed to plot the graph. Maybe I am using them wrong. I whould really appreciate any suggestion on how to do this.</p>
",2020-03-11 15:43:28,"<p>As answered <a href=""https://stackoverflow.com/a/61173028/2110869"">here</a>, I'm sure there's a better way, but a simple workaround is to just use the existing tensorboard callback logic:</p>

<pre><code>tb_callback = tf.keras.callbacks.TensorBoard(LOG_DIR)
tb_callback.set_model(model) # Writes the graph to tensorboard summaries using 
an internal file writer
</code></pre>
",10687511,184,https://stackoverflow.com/questions/60639731,Documentation Replication on Other Examples
62956096,Is it possible to extract trained class names from tflite model?,"<p>I have tried to search everywhere, tried everything in <code>tflite_interpreter = tf.lite.Interpreter(model_path='model.tflite')</code>, read tflite documentation but I cannot find the method to extract the class names from the model.</p>
<p>Is it possible?</p>
",2020-07-17 14:28:57,"<p>When you create data for training, you encode the labels to numerical representations. After that the labels are lost, if you know the data with which the model was trained, then, you can know the label_names. Information of label_names is not stored in the model.</p>
",10688345,140,https://stackoverflow.com/questions/62956096,Lack of Alternative Solutions/Documentation
59607363,Tensorflow 2.0 dataset batching not working properly,"<p>Tensorflow 2.0 dataset api's batch is not working as I expected it to work.</p>

<p>I've made a dataset like this.</p>

<pre><code>self.train_dataset = tf.data.Dataset.from_generator(generator=train_generator, output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6]), tf.TensorShape([])))
</code></pre>

<p>This yields DatasetV1Adapter shapes: ((6,), ()), types: (tf.float32, tf.float32),
and to this dataset I applied batch function from tf.data.Dataset.</p>

<pre><code>self.train_dataset.batch(1024)
</code></pre>

<p>yields DatasetV1Adapter shapes: ((None, 6), (None,)), types: (tf.float32, tf.float32), and changing the batch size doesn't help at all. </p>

<p>From official description of the batch, </p>

<blockquote>
  <p>The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.</p>
</blockquote>

<p>The way I thought this function would work, was to make [batch, 6], [batch,] but didn't work out well. </p>

<p>I originally used pytorch, and started using TF 2.0 recently, and need some help on proper batching. Thanks in advance.</p>
",2020-01-06 06:01:59,"<p>You can get the desired result by setting,</p>

<pre><code>train_dataset = train_dataset.batch(2, drop_remainder=True)
</code></pre>

<p><code>drop_remainder=False</code> by default. When that's the case, the first dimension <strong>must</strong> be <code>None</code> as there will (most probably) be a batch with <code>&lt; batch_size</code> elements at the end of the dataset, because the number of samples is not divisible by <code>batch_size</code>.</p>
",10690874,1027,https://stackoverflow.com/questions/59607363,Documentation Replication on Other Examples
55094952,Understanding Tensorflow control dependencies,"<p>I am trying to gain a stronger grasp of TensorFlow. I came across the concept of control dependencies. I understand that the order of ops as specified by us is not really relevant to Tensorflow during execution. In order to optimise the speed of execution TensorFlow decides its own order of calculating nodes. 
But we can customise order of execution by using tf.control_dependencies.
I am not able to understand the use cases of the function. Can anyone direct me to some resource(other than the documentation) or explain the working of this function?
An example:</p>

<pre><code>tf.reset_default_graph()
x = tf.Variable(5)
y=tf.Variable(3)
assign = tf.assign(x,x+y)
z = x+assign
with tf.Session() as sess:
   sess.run(tf.global_variables_initializer())
   with tf.control_dependencies([assign]):
        z_out = sess.run(z)

print(z_out)
</code></pre>

<p>The output of the code is 8. So I infer that since z=x+y,the assign node has not been evaluated(right?). But doesn't this mean that the result of tensorflow may be erroneous? This means we need to create new nodes during every operation to force TensorFlow to calculate all the nodes leading up to the result. But in say training a neural network with 10000 steps if each step creates a new set of 1000 weights/parameters won't the space complexity explode?</p>
",2019-03-11 03:42:04,"<p>In the snippet you have posted, <a href=""https://www.tensorflow.org/api_docs/python/tf/control_dependencies"" rel=""noreferrer""><code>tf.control_dependencies</code></a> is not having any effect. The function creates a context where <strong>new operations</strong> are created with a control dependency to the given operations, but in your code there are no new operations within the context, just evaluation of previously existing operations.</p>

<p>In most cases, control flow in TensorFlow is ""obvious"", in the sense that there is only one way to make a computation correctly. However, when stateful objects (i.e. variables) are involved, there are situations that may be ambiguous. Consider the following example:</p>

<pre><code>import tensorflow as tf

v1 = tf.Variable(0)
v2 = tf.Variable(0)
upd1 = tf.assign(v1, v2 + 1)
upd2 = tf.assign(v2, v1 + 1)
init = tf.global_variables_initializer()
</code></pre>

<p><code>v1</code> and <code>v2</code> are both variables initialized to <code>0</code> and then updated. However, each use the value of the other variable in the update. In a regular Python program things would run sequentially, so <code>upd1</code> would run first (so <code>v1</code> would be <code>1</code>) and <code>upd2</code> after (so <code>v2</code> would be <code>2</code>, because <code>v1</code> was <code>1</code>). But TensorFlow does not record the order in which operations are created, only their dependencies. So it may also happen that <code>upd2</code> runs before <code>upd1</code> (so <code>v1</code> would be <code>2</code> and <code>v2</code> would be <code>1</code>) or that both update values (<code>v2 + 1</code> and <code>v1 + 1</code>) are computed before the assignments (so both <code>v1</code> and <code>v2</code> would be <code>1</code> in the end). Indeed, if I run it several times:</p>

<pre><code>for i in range(10):
    with tf.Session() as sess:
        sess.run(init)
        sess.run([upd1, upd2])
        print(*sess.run([v1, v2]))
</code></pre>

<p>I do not always get the same result (personally I get <code>1 1</code> and <code>2 1</code>, although technically <code>1 2</code> would also be possible). If for example you wanted to compute the new value for <code>v2</code> after <code>v1</code> has been updated, you could just do the following:</p>

<pre><code>import tensorflow as tf

v1 = tf.Variable(0)
v2 = tf.Variable(0)
upd1 = tf.assign(v1, v2 + 1)
upd2 = tf.assign(v2, upd1 + 1)
init = tf.global_variables_initializer()
</code></pre>

<p>Here the new value <code>v2</code> is computed using <code>upd1</code>, which is guaranteed to be the value of the variable after the update. So here <code>upd2</code> would have an implicit dependency to the assignment, and so things would work as expected.</p>

<p>But what if you wanted to always compute the new values for <code>v1</code> and <code>v2</code> using the non-updated variable values (that is, consistently end up with both <code>v1</code> and <code>v2</code> being <code>1</code>)? In that case you can use <a href=""https://www.tensorflow.org/api_docs/python/tf/control_dependencies"" rel=""noreferrer""><code>tf.control_dependencies</code></a>:</p>

<pre><code>import tensorflow as tf

v1 = tf.Variable(0)
v2 = tf.Variable(0)
new_v1 = v2 + 1
new_v2 = v1 + 1
with tf.control_dependencies([new_v1, new_v2]):
    upd1 = tf.assign(v1, new_v1)
    upd2 = tf.assign(v2, new_v2)
init = tf.global_variables_initializer()
</code></pre>

<p>Here, the assignment operations cannot happen until the new values for <code>v1</code> and <code>v2</code> have been computed, so their final values will always be <code>1</code> in both cases.</p>
",10726895,115,https://stackoverflow.com/questions/55094952,Inadequate Examples
61136605,Why tf2 can't save a tf_function model as .pb file?,"<p>I tried to saved a model like the official code of transformer on <a href=""https://www.tensorflow.org/tutorials/text/transformer?hl=zh-cn"" rel=""nofollow noreferrer"">official website</a>, but when i want to save the train_step graph or trace on it with tf.summary.trace_on ,it errors.the error is as </p>

<pre><code>    ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in convert(x)
    936         try:
--&gt; 937           x = ops.convert_to_tensor_or_composite(x)
    938         except (ValueError, TypeError):

15 frames
TypeError: Can't convert Operation 'PartitionedFunctionCall' to Tensor (target dtype=None, name=None, as_ref=False)

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in convert(x)
    941               ""must return zero or more Tensors; in compilation of %s, found ""
    942               ""return value of type %s, which is not a Tensor."" %
--&gt; 943               (str(python_func), type(x)))
    944       if add_control_dependencies:
    945         x = deps_ctx.mark_as_return(x)

TypeError: To be compatible with tf.contrib.eager.defun, Python functions must return zero or more Tensors; in compilation of &lt;function canonicalize_signatures.&lt;locals&gt;.signature_wrapper at 0x7fcf794b47b8&gt;, found return value of type &lt;class 'tensorflow.python.framework.ops.Operation'&gt;, which is not a Tensor.
</code></pre>

<p>I supposed it was some error on tensor operation and write an other demo to confirm my idea:</p>

<pre><code>    import tensorflow as tf
sig=[tf.TensorSpec(shape=(None, None), dtype=tf.int64),tf.TensorSpec(shape=(None, None), dtype=tf.int64)]
@tf.function(input_signature=sig)
def cal(a,d):
  b=a[1:]
root=tf.Module()
root.func = cal

# 
concrete_func = root.func.get_concrete_function(
      tf.TensorSpec(shape=(None, None), dtype=tf.int64),tf.TensorSpec(shape=(None, None), dtype=tf.int64)
)
tf.saved_model.save(root, '/correct', concrete_func)
</code></pre>

<p>the error occurs as supposed. But how can i fixed it? the positional_encoding requires and i have no idea about how to replace this operation.</p>
",2020-04-10 08:30:59,"<p>I figure it because i didn't save the transformer model in the pb file</p>
",10796214,43,https://stackoverflow.com/questions/61136605,Documentation Replicability
57711285,How can I get global step in tensorflow.estimator?,"<p>Does anyone know, how can I get the global step count in estimator definition?<br>
I need it to adjust the learning rate, when the optimizer created.<br>
like the example below: </p>

<pre><code>def estimator_fn(features, labels, mode):
  if mode == tf.estimator.ModeKeys.TRAIN:
    optimizer=xxx(learning_rate=GLOBAL_STEP*some process)
</code></pre>

<p>And does <code>tf.train.get_global_step</code> work?</p>
",2019-08-29 13:28:49,"<p>You can also use this:</p>

<pre><code>if mode == tf.estimator.ModeKeys.TRAIN:
   global_step = tf.train.get_or_create_global_step()
   learning_rate = learning_rate_fn(global_step)
</code></pre>

<p>where learning_rate_fn is a function that you can modify your learning rate.</p>

<p>For more info look at <a href=""https://github.com/tensorflow/models/blob/57e075203f8fba8d85e6b74f17f63d0a07da233a/official/resnet/resnet_run_loop.py#L367"" rel=""nofollow noreferrer"">here</a>.</p>
",10846167,195,https://stackoverflow.com/questions/57711285,Documentation Replication on Other Examples
60708695,"How can I make ""element wise"" comparsion inside of the tf.function?","<p>I try to make my own activation function in TensorFlow 2 and the function looks like this:</p>

<pre><code>@tf.function
def f(x):
  r = 2
  if x&gt;=0:
    return (r**2 * x + 1)**(1/r) - 1/r
  else:
    return K.exp(r*x) - 1/r
</code></pre>

<p>The problem is that it cant take as argument <code>tf.constant([2.0, 3.0])</code>because there is an issue with conditions. I have tried <code>tf.math.qreater_equal(x, 0)</code> which lead to same output also <code>tf.cond()</code>. I have had no luck with documentation examples either.
It returns error:</p>

<pre><code>InvalidArgumentError:  The second input must be a scalar, but it has shape [2]
     [[{{node cond/switch_pred/_2}}]] [Op:__inference_f_7469065]
</code></pre>

<p>Thanks!</p>
",2020-03-16 15:24:16,"<p><code>if</code> statements are converted to <code>cond</code>, but that only takes scalar arguments for the predicate (and does no broadcasting).  Try <a href=""https://www.tensorflow.org/api_docs/python/tf/where"" rel=""nofollow noreferrer""><code>where</code></a> instead: </p>

<pre><code>return tf.where(x &gt;= 0, (r**2 * x + 1)**(1/r) - 1/r, K.exp(r*x) - 1/r))
</code></pre>

<p>(Can't test this currently with TensorFlow, but that's at least how Numpy behaves...)</p>
",10962934,3,https://stackoverflow.com/questions/60708695,Documentation Ambiguity
67542939,tensorflow function in graph mode (tf.function) is slow when return object,"<p>I have a function wrapped by <code>@tf.function</code>, the function was quick when it doen't return any object. But it becomes significant slower when it return an object.</p>
<p>Any way to boost the performance even when it has to return an object?</p>
<p>Below is a toy example</p>
<pre><code>from tqdm import tqdm
import tensorflow as tf
import numpy as np

model = tf.keras.Sequential([
    tf.keras.layers.Input((4,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10000)
])

model.compile(
        optimizer=&quot;rmsprop&quot;,
        loss=&quot;sparse_categorical_crossentropy&quot;,
        metrics=[&quot;sparse_categorical_accuracy&quot;],
)

np.random.seed(0)
xy = tf.data.Dataset.from_tensor_slices((np.random.random((200000,4)), np.random.randint(1000, size=(200000,1)))).batch(200)

@tf.function
def _no_return(batch):
    x, y = batch
    y_pred = model(x, training=False)
    r = tf.math.top_k(y_pred, k=20).indices
    # True indicate that item is the correct prediction
    r = tf.cast(tf.equal(r, tf.expand_dims(tf.cast(y, tf.int32), 1)), tf.float32)
    # rank of the correct prediction, rank = 9999999+1 if no correction prediction within topk
    r = tf.add((tf.reduce_sum(r, 1)-1) * -9999999, tf.cast(tf.argmax(r, 1) + 1, tf.float32))

@tf.function
def _return(batch):
    x, y = batch
    y_pred = model(x, training=False)
    r = tf.math.top_k(y_pred, k=20).indices
    # True indicate that item is the correct prediction
    r = tf.cast(tf.equal(r, tf.expand_dims(tf.cast(y, tf.int32), 1)), tf.float32)
    # rank of the correct prediction, rank = 9999999+1 if no correction prediction within topk
    r = tf.add((tf.reduce_sum(r, 1)-1) * -9999999, tf.cast(tf.argmax(r, 1) + 1, tf.float32))
    return r

#
# this run 1428.27 iterations/s 
#
for batch in tqdm(xy):
    _no_return(batch)

#
# this run 135.61 iterations/s 
#
for batch in tqdm(xy):
    _return(batch)

</code></pre>
",2021-05-15 03:08:29,"<p>It is happening because of <code>autograph</code>. In the case of the <code>_return</code> function, because the <code>r</code> value is returned, it is generating the complete graph (with <code>r</code> as the leaf node). But, in the case of <code>_no_return</code> function, there is no return value, and the autograph does not set <code>r</code> as the leaf node, and because there is no leaf node the overall graph has nothing in it.</p>
<p>Thus, you observe this speed-up because the function is not computing anything.</p>
<p>Add any leaf nodes (as shown in the demo below)</p>
<pre><code>def _no_return(batch):
    # your old code
    tf.print(r) # this will create another leaf node, 
    # or you can try
    tf.summary.scalar(r)
</code></pre>
<p>you will observe a similar performance between <code>_return</code> and <code>_no_return</code>. Also, if you remove <code>@tf.function</code> decorator, then also you will see the same performance.</p>
<p>lemme know if you have any more doubts.</p>
",10545499,91,https://stackoverflow.com/questions/67542939,Documentation Replication on Other Examples
54047604,How to assign custom gradient to TensorFlow op with multiple inputs,"<p>I'm trying to use TensorFlow's <code>@tf.custom_gradient</code> functionality to assign a custom gradient to a function with multiple inputs.  I can put together a working setup for only one input, but not for two or more.</p>

<p>I've based my code on <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">TensorFlow's custom_gradient documentation</a>, which works just fine for one input, as in this example:</p>

<pre><code>import tensorflow as tf
import os

# Suppress Tensorflow startup info
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

# Custom gradient decorator on a function,
# as described in documentation
@tf.custom_gradient
def my_identity(x):

    # The custom gradient
    def grad(dy):
        return dy

    # Return the result AND the gradient
    return tf.identity(x), grad

# Make a variable, run it through the custom op
x = tf.get_variable('x', initializer=1.)
y = my_identity(x)

# Calculate loss, make an optimizer, train the variable
loss = tf.abs(y)
opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)
train = opt.minimize(loss)

# Start a TensorFlow session, initialize variables, train
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(train)
</code></pre>

<p>This example runs silently, then closes.  No issues, no errors.  The variable optimizes as expected.  However, in my application, I need to do such a calculation with multiple inputs, so something of this form:</p>

<pre><code>@tf.custom_gradient
def my_identity(x, z):

    def grad(dy):
        return dy

    return tf.identity(x*z), grad
</code></pre>

<p>Running this in place of the example (and adding another variable input to the call of <code>my_identify</code>) results in the following error output.  Best as I can tell, the last parts of the error are from the dynamic generation of the op -- the information format matches the C++ formatting required in the op establishment (though that's about all I know about it).</p>

<pre><code>Traceback (most recent call last):
  File ""testing.py"", line 27, in &lt;module&gt;
    train = opt.minimize(loss)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 400, in minimize
    grad_loss=grad_loss)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 519, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 630, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 821, in _GradientsHelper
    _VerifyGeneratedGradients(in_grads, op)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 323, in _VerifyGeneratedGradients
    ""inputs %d"" % (len(grads), op.node_def, len(op.inputs)))
ValueError: Num gradients 2 generated for op name: ""IdentityN""
op: ""IdentityN""
input: ""Identity""
input: ""x/read""
input: ""y/read""
attr {
  key: ""T""
  value {
    list {
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
    }
  }
}
attr {
  key: ""_gradient_op_type""
  value {
    s: ""CustomGradient-9""
  }
}
 do not match num inputs 3
</code></pre>

<p>Based on other custom gradient options, I surmised that the issue was a lack of supplied gradient for the second input argument.  So, I changed my function to this:</p>

<pre><code>@tf.custom_gradient
def my_identity(x, z):

    def grad(dy):
        return dy

    return tf.identity(x*z), grad, grad
</code></pre>

<p>This results in the following more familiar error:</p>

<pre><code>Traceback (most recent call last):
  File ""testing.py"", line 22, in &lt;module&gt;
    y = my_identity(x, z)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated
    return _graph_mode_decorator(f, *args, **kwargs)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 132, in _graph_mode_decorator
    result, grad_fn = f(*args)
ValueError: too many values to unpack (expected 2)
</code></pre>

<p>The <code>@custom_gradient</code> decorator is only identifying the last returned element as a gradient.  So, I tried putting the two gradients into a tuple as <code>(grad, grad)</code> such that there would only be ""two"" outputs for the function.  TensorFlow rejected this too, this time because it can't call a tuple like it would a Tensor -- entirely reasonable, in hindsight.</p>

<p>I've fussed around with the example some more, but to no avail.  No matter what I try, I can't get the custom-defined gradient to deal with multiple inputs.  I'm hoping that somebody with more knowledge than I regarding custom ops and gradients will have a better idea on this -- thanks in advance for the help!</p>
",2019-01-04 23:45:09,"<p>If we use multiple variables as input, the number of gradients return from ""grad"" function should be equals to number of input variables, though we maybe don't care about some of them. </p>

<p>For example:</p>

<pre><code>@tf.custom_gradient
def my_multiple(x,z):

def grad(dy):
    # return two gradients, one for 'x' and one for 'z'
    return (dy*z, dy*x)

return tf.identity(x*z), grad
</code></pre>

<p>Note that the second output of ""my_multiple"" is a function, not a gradient tensor. </p>
",10554082,119,https://stackoverflow.com/questions/54047604,Documentation Replication on Other Examples
70735454,How to flatten a gradient (list of tensors) in tf.function (graph mode),"<p>I want to do some linear algebra (e.g. tf.matmul) using the gradient. By default the gradient is returned as a list of tensors, where the tensors may have different shapes. My solution has been to reshape the gradient into a single vector. This works in eager mode, but now I want to compile my code using tf.function. It seems there is no way to write a function which can 'flatten' the gradient in graph mode (tf.function).</p>
<pre><code>grad = [tf.ones((2,10)), tf.ones((3,))]  # an example of what a gradient from tape.gradient can look like

# this works for flattening the gradient in eager mode only
def flatten_grad(grad):
    return tf.concat([tf.reshape(grad[i], tf.math.reduce_prod(tf.shape(grad[i]))) for i in range(len(grad))], 0)
</code></pre>
<p>I tried converting it like this, but it doesn't work with tf.function either.</p>
<pre><code>@tf.function
def flatten_grad1(grad):
    temp = [None]*len(grad)
    for i in tf.range(len(grad)):
        i = tf.cast(i, tf.int32)
        temp[i] = tf.reshape(grad[i], tf.math.reduce_prod(tf.shape(grad[i])))
    return tf.concat(temp, 0)
</code></pre>
<p>I tried TensorArrays, but it also does not work.</p>
<pre><code>@tf.function
def flatten_grad2(grad):
    temp = tf.TensorArray(tf.float32, size=len(grad), infer_shape=False)
    for i in tf.range(len(grad)):
        i = tf.cast(i, tf.int32)
        temp = temp.write(i, tf.reshape(grad[i], tf.math.reduce_prod(tf.shape(grad[i]))))
    return temp.concat()
</code></pre>
",2022-01-17 00:51:55,"<p>Maybe you could try directly iterating over your <code>list</code> of tensors instead of getting individual tensors by their index:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

grad = [tf.ones((2,10)), tf.ones((3,))]  # an example of what a gradient from tape.gradient can look like

@tf.function
def flatten_grad1(grad):
    temp = [None]*len(grad)
    for i, g in enumerate(grad):
        temp[i] = tf.reshape(g, (tf.math.reduce_prod(tf.shape(g)), ))
    return tf.concat(temp, axis=0)
print(flatten_grad1(grad))
</code></pre>
<pre><code>tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(23,), dtype=float32)
</code></pre>
<p>With <code>tf.TensorArray</code>:</p>
<pre class=""lang-py prettyprint-override""><code>@tf.function
def flatten_grad2(grad):
    temp = tf.TensorArray(tf.float32, size=0, dynamic_size=True, infer_shape=False)
    for g in grad:
        temp = temp.write(temp.size(), tf.reshape(g, (tf.math.reduce_prod(tf.shape(g)), )))
    return temp.concat()

print(flatten_grad2(grad))
</code></pre>
",10565197,507,https://stackoverflow.com/questions/70735454,Documentation Replicability
54175038,Creating a serving graph separately from training in tensorflow for Google CloudML deployment?,"<p>I am trying to deploy a <code>tf.keras</code> image classification model to Google CloudML Engine. Do I have to include code to create serving graph separately from training to get it to serve my models in a web app? I already have my model in SavedModel format (<code>saved_model.pb</code> &amp; variable files), so I'm not sure if I need to do this extra step to get it to work. </p>

<p>e.g. this is code directly from GCP Tensorflow Deploying models <a href=""https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models"" rel=""nofollow noreferrer"">documentation</a></p>

<pre><code>def json_serving_input_fn():
  """"""Build the serving inputs.""""""
  inputs = {}
  for feat in INPUT_COLUMNS:
    inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

  return tf.estimator.export.ServingInputReceiver(inputs, inputs)
</code></pre>
",2019-01-14 02:14:36,"<p>You are probably training your model with actual image files, while it is best to send images as encoded byte-string to a model hosted on CloudML. Therefore you'll need to specify a <code>ServingInputReceiver</code> function when exporting the model, as you mention. Some boilerplate code to do this for a Keras model:</p>

<pre><code># Convert keras model to TF estimator
tf_files_path = './tf'
estimator =\
    tf.keras.estimator.model_to_estimator(keras_model=model,
                                          model_dir=tf_files_path)

# Your serving input function will accept a string
# And decode it into an image
def serving_input_receiver_fn():
    def prepare_image(image_str_tensor):
        image = tf.image.decode_png(image_str_tensor,
                                    channels=3)
        return image  # apply additional processing if necessary

    # Ensure model is batchable
    # https://stackoverflow.com/questions/52303403/
    input_ph = tf.placeholder(tf.string, shape=[None])
    images_tensor = tf.map_fn(
        prepare_image, input_ph, back_prop=False, dtype=tf.float32)

    return tf.estimator.export.ServingInputReceiver(
        {model.input_names[0]: images_tensor},
        {'image_bytes': input_ph})

# Export the estimator - deploy it to CloudML afterwards
export_path = './export'
estimator.export_savedmodel(
    export_path,
    serving_input_receiver_fn=serving_input_receiver_fn)
</code></pre>

<p>You can refer to <a href=""https://stackoverflow.com/questions/51432589/how-do-i-get-a-tensorflow-keras-model-that-takes-images-as-input-to-serve-predic"">this very helpful answe</a>r for a more complete reference and other options for exporting your model. </p>

<p><strong>Edit:</strong> If this approach throws a <code>ValueError: Couldn't find trained model at ./tf.</code> error, you can try it the workaround solution that I documented in <a href=""https://stackoverflow.com/questions/54615708/exporting-a-keras-model-as-a-tf-estimator-couldnt-find-trained-model"">this answer</a>.</p>
",10647086,134,https://stackoverflow.com/questions/54175038,Documentation Ambiguity
61305781,Using Tensorflow embedded columns raises All feature_columns must be _FeatureColumn instances error,"<p>I am new to tensorflow and I was trying to follow the official documentation where I came across 
tf.feature_column.categorical_column_with_vocabulary_list</p>

<p>The code I tested is: </p>

<pre><code>key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), default_value=0)
columns = [[tfc.embedding_column(colors, 3)], ...]
features = tf.io.parse_example(..., features=tfc.make_parse_example_spec(columns))
dense_tensor = tfc.input_layer(features, columns)
</code></pre>

<p>However , when I run this sample code I get the following error : 
 ValueError: All feature_columns must be _FeatureColumn instances. Given: [EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), dtype=tf.string, default_value=0, num_oov_buckets=0), dimension=3, combiner='mean', initializer=, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)]</p>

<p>What I am doing wrong?  </p>
",2020-04-19 14:17:44,"<p><code>make_parse_example_spec</code> expects <code>FeatureColumn instances</code>. You can create the FeatureColumn instance using the below method for the category list.</p>

<pre><code>colors = feature_column.categorical_column_with_vocabulary_list(key='colors',vocabulary_lis=('R', 'G', 'B', 'Y'),num_oov_buckets=2)
my_feature_columns = [feature_column.indicator_column(colors)]
feature_column.make_parse_example_spec(my_feature_columns)
</code></pre>

<p>Output :</p>

<pre><code>{'colors': VarLenFeature(dtype=tf.string)}  
</code></pre>

<p>If you want to create a dense embedding tensor on your categorical column, you can follow the below example.  </p>

<pre><code>data = {'colors': ['X', 'R', 'G', 'B', 'Y']}

df = pd.DataFrame(data)

colors = feature_column.categorical_column_with_vocabulary_list('colors', df['colors'].unique())

colors_embedding = feature_column.embedding_column(colors, dimension=4)

dense_tensor = tf.keras.layers.DenseFeatures(colors_embedding)(data)
</code></pre>

<p>Result: </p>

<pre><code>tf.Tensor(
[[ 0.17071894  0.29407692 -0.26661882  0.07768019]
 [ 0.26196313  0.14372464 -0.41102907 -0.7207164 ]
 [-0.7888006  -0.07049363 -0.49007863  0.45744416]
 [ 0.56329435 -0.7051675   0.04742934 -0.69377   ]
 [-0.52031726  0.488502   -0.37031132 -0.44338205]], shape=(5, 4), dtype=float32)
</code></pre>
",11041539,23,https://stackoverflow.com/questions/61305781,Documentation Replication on Other Examples
71129505,"Is it possible to split a tensorflow dataset into train, validation AND test datasets when using image_dataset_from_directory?","<p>I am using <code>tf.keras.utils.image_dataset_from_directory</code> to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the <code>validation_split</code> parameter), I want to split it into training, testing, and validation subsets.</p>
<p>I have tried using <code>dataset.skip()</code> and <code>dataset.take()</code> to further split one of the resulting subsets, but these functions return a <code>SkipDataset</code> and a <code>TakeDataset</code> respectively (by the way, contrary to <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#take"" rel=""nofollow noreferrer"">the documentation</a>, where it is claimed that these functions return a <code>Dataset</code>). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history.</p>
<p>So, my question is: is there a way to split a <code>Dataset</code> into three subsets for training, validation and testing, so that all three subsets are also <code>Dataset</code> objects?</p>
<p><strong>Code used to load the data</strong></p>
<pre><code>def load_data_tf(data_path: str, img_shape=(256,256), batch_size: int=8):
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.2,
        subset=&quot;training&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.3,
        subset=&quot;validation&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    return train_ds, val_ds

train_dataset, test_val_ds = load_data_tf('data_folder', img_shape = (256,256), batch_size=8)
test_dataset = test_val_ds.take(686)
val_dataset = test_val_ds.skip(686)
</code></pre>
<p><strong>Model compilation and fitting</strong></p>
<pre><code>model.compile(optimizer='sgd',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])
history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, verbose=1)
</code></pre>
<p><strong>When using a normal <code>Dataset</code>, <code>val_accuracy</code> and <code>val_loss</code> are present in the history of the model:</strong></p>
<p><a href=""https://i.stack.imgur.com/Qn1Yf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qn1Yf.png"" alt=""Expected behaviour: when using a Dataset, validation metrics are calculated"" /></a></p>
<p><strong>But when using a <code>SkipDataset</code>, they are not:</strong></p>
<p><a href=""https://i.stack.imgur.com/GMnBM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GMnBM.png"" alt=""Using the SkipDataset produced by test_val_ds.take() leads to validation metrics disappearing from model history"" /></a></p>
<p><a href=""https://i.stack.imgur.com/omU5U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/omU5U.png"" alt=""val_accuracy and val_loss are not present in history keys when using a SkipDataset or a TakeDataset"" /></a></p>
",2022-02-15 15:56:25,"<p>The issue is that you are not taking and skipping samples when you do <code>test_val_ds.take(686)</code> and <code>test_val_ds.skip(686)</code>, but actually batches. Try running <code>print(val_dataset.cardinality())</code> and you will see how many batches you really have reserved for validation. I am guessing <code>val_dataset</code> is empty, because you do not have 686 batches for validation. Here is a working example:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import pathlib

dataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=123,
  image_size=(180, 180),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;validation&quot;,
  seed=123,
  image_size=(180, 180),
  batch_size=batch_size)

test_dataset = val_ds.take(5)
val_ds = val_ds.skip(5)

print('Batches for testing --&gt;', test_dataset.cardinality())
print('Batches for validating --&gt;', val_ds.cardinality())

model = tf.keras.Sequential([
  tf.keras.layers.Rescaling(1./255, input_shape=(180, 180, 3)),
  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(5)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

epochs=1
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=1
)
</code></pre>
<pre><code>Found 3670 files belonging to 5 classes.
Using 2936 files for training.
Found 3670 files belonging to 5 classes.
Using 734 files for validation.
Batches for testing --&gt; tf.Tensor(5, shape=(), dtype=int64)
Batches for validating --&gt; tf.Tensor(18, shape=(), dtype=int64)
92/92 [==============================] - 96s 1s/step - loss: 1.3516 - accuracy: 0.4489 - val_loss: 1.1332 - val_accuracy: 0.5645
</code></pre>
<p>In this example, with a <code>batch_size</code> of 32, you can clearly see that the validation set reserved 23 batches. Afterwards, 5 batches were given to the test set and 18 batches remained for the validation set.</p>
",11212528,45,https://stackoverflow.com/questions/71129505,Documentation Replication on Other Examples
55681290,Feeding Dataset Iterator to Tensorflow,"<p>Can i get a full example somewhere where they feed tf.data.Dataset iterator to a model? I'm trying to feed this data into a model without the help of tf.Estimators.</p>

<pre><code>def preprocess_image(image):
  image = tf.image.decode_jpeg(image, channels=1)
  image = tf.image.resize_images(image, [224, 224])
  image = tf.image.random_flip_left_right(image)
  image /= 255.0
  image = tf.cast(image, tf.float32)
  image = tf.train.shuffle_batch([image],batch_size=16, num_threads=10, capacity=100000, min_after_dequeue=15)
  return image

def load_and_preprocess_image(path):
  image = tf.read_file(path)
  return preprocess_image(image)




train_data_dx = tf.data.Dataset.from_tensor_slices(xray_data_train['full_path'].values)
train_data_dx = train_data_dx.map(load_and_preprocess_image, num_parallel_calls=8)
train_data_dy = xray_data_train['Finding_strings']
print(train_data_dx.output_shapes)
print(train_data_dx.output_types)

test_data_dx = tf.data.Dataset.from_tensor_slices(xray_data_test['full_path'].values)
test_data_dx = test_data_dx.map(load_and_preprocess_image, num_parallel_calls=8)
test_data_dy = xray_data_test['Finding_strings']
</code></pre>
",2019-04-15 00:34:56,"<p>Here's a full example. </p>

<p><strong>Note</strong></p>

<ul>
<li>Iterator must be initialized at the beginning</li>
<li>We can set number of epochs to perform by using <code>repeat()</code> method of number of epochs and <code>batch()</code> method for batch size. Note that I use first <code>repeat()</code> and then <code>batch()</code>.</li>
<li>At each iteration we're using <code>tf.Session()</code> interface to access the next batch.</li>
<li>We use <code>try-except</code> since when repetition of data ends it raises <code>tf.error.OutOfRangeError</code>.</li>
</ul>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from sklearn.datasets import make_blobs

# generate dummy data for illustration
x_train, y_train = make_blobs(n_samples=25,
                              n_features=2,
                              centers=[[1, 1], [-1, -1]],
                              cluster_std=0.5)
n_epochs = 2
batch_size = 10

with tf.name_scope('inputs'):
    x = tf.placeholder(tf.float32, shape=[None, 2])
    y = tf.placeholder(tf.int32, shape=[None])

with tf.name_scope('logits'):
    logits = tf.layers.dense(x,
                             units=2,
                             name='logits')

with tf.name_scope('loss'):
    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)
    loss_tensor = tf.reduce_mean(xentropy)

with tf.name_scope('optimizer'):
    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss_tensor)

# create dataset `from_tensor_slices` and create iterator
dataset = tf.data.Dataset.from_tensor_slices({'x':x_train, 'y':y_train})
dataset = dataset.repeat(n_epochs).batch(10)
iterator = dataset.make_initializable_iterator()

with tf.Session() as sess:
    sess.run([tf.global_variables_initializer(), 
              iterator.initializer]) # &lt;-- must be initialized!
    next_batch = iterator.get_next()

    while True:
        try:
            batch = sess.run(next_batch) # &lt;-- extract next batch
            loss_val, _ = sess.run([loss_tensor, train_op], 
                                   feed_dict={x:batch['x'], y:batch['y']})
            print(loss_val)
        except tf.errors.OutOfRangeError:
            break 
</code></pre>
",11360841,23,https://stackoverflow.com/questions/55681290,Lack of Alternative Solutions/Documentation
57316557,"tf.keras.layers.pop() doesn't work, but tf.keras._layers.pop() does","<p>I want to pop the last layer of the model. So I use the <code>tf.keras.layers.pop()</code>, but it doesn't work.</p>

<pre><code>base_model.summary()
</code></pre>

<p><a href=""https://i.stack.imgur.com/wRz02.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wRz02.png"" alt=""enter image description here""></a></p>

<pre><code>base_model.layers.pop()

base_model.summary()
</code></pre>

<p><a href=""https://i.stack.imgur.com/msGnY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/msGnY.png"" alt=""enter image description here""></a></p>

<p>When I use <code>tf.keras._layers.pop()</code>, it works.</p>

<pre><code>base_model.summary()
</code></pre>

<p><a href=""https://i.stack.imgur.com/CAq45.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CAq45.png"" alt=""enter image description here""></a></p>

<pre><code>base_model._layers.pop()
base_model.summary()
</code></pre>

<p><a href=""https://i.stack.imgur.com/viCxT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/viCxT.png"" alt=""enter image description here""></a></p>

<p>I don't find docs about this usage. Could someone help explain this?</p>
",2019-08-01 20:27:44,"<p>I agree this is confusing. The reason is that <code>model.layers</code> returns a shallow copy of the layers list so:</p>

<p>The tldr is dont use <code>model.layers.pop()</code> to remove the last layer. Instead we should create a new model with all but the last layer. Perhaps something like this:</p>

<pre class=""lang-py prettyprint-override""><code>new_model = tf.keras.models.Sequential(base_model.layers[:-1])
</code></pre>

<p>Checkout this <a href=""https://github.com/tensorflow/tensorflow/issues/22479"" rel=""noreferrer"">github issue</a> for more details</p>
",11255365,731,https://stackoverflow.com/questions/57316557,Documentation Replication on Other Examples
62223016,Single Prediction Image doesn't need to be rescaled?,"<p>I followed a tutorial to make my first Convolutional Neural Network using Keras and I have a small question regarding the rescaling step.</p>

<p>So when we are importing the training set and test set, we create an instance of  the <code>tf.keras.preprocessing.image.ImageDataGenerator</code> class and use it as:</p>

<pre class=""lang-py prettyprint-override""><code>train_datagen = ImageDataGenerator(rescale=1/255)
</code></pre>

<p>Along with some other augmentation parameters. My understanding is that we use the <code>rescale</code> parameter to normalize the pixel values of the images imported.</p>

<p>But when we load up a single image to run through the CNN, we write something like (code from keras docs):</p>

<pre class=""lang-py prettyprint-override""><code>image = tf.keras.preprocessing.image.load_img(image_path)
input_arr = keras.preprocessing.image.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
</code></pre>

<p>My question is, I cannot see the single input image being <code>rescaled</code> anywhere. Is it being done implicitly, or is there no need to actually perform rescaling? If the latter, then why is it so?</p>

<p>Thanks!</p>
",2020-06-05 19:40:28,"<p>The image should be normalized that it should be divided by 255, if it's done during the training. Network will not be able to interpret that.</p>

<p>Also, when we use test_datagen, we apply Rescaling by 1/255 for the predict generator. </p>

<p>Normalization, mean subtraction and std deviation needs to be done at the testing time, if that has been applied during the training stage.</p>
",11319137,476,https://stackoverflow.com/questions/62223016,Documentation Replicability
73469081,Impact of batch_size in predict method of tf.keras.Model,"<p>The <code>predict</code> method of a <code>tf.keras.Model</code> takes the following arguments:</p>
<pre><code>predict(
    x,
    batch_size=None,
    verbose='auto',
    steps=None,
    callbacks=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False
)
</code></pre>
<p>What is the point of specifying the batch_size? What are the ways in which it impacts the predictions?</p>
",2022-08-24 07:30:36,"<p>Batching only affects the CPU/GPU RAM memory needs and not the predicted values.</p>
",11330010,346,https://stackoverflow.com/questions/73469081,Documentation Replication on Other Examples
72165812,Connecting BatchDataset with Keras VGG16 preprocess_input,"<p>I am using <code>tf.keras.preprocessing.image_dataset_from_directory</code> to get a <code>BatchDataset</code>, where the dataset has 10 classes.</p>
<p>I am trying to integrate this <code>BatchDataset</code> with a Keras <code>VGG16</code> (<a href=""https://keras.io/api/applications/vgg/"" rel=""nofollow noreferrer"">docs</a>) network.  From the docs:</p>
<blockquote>
<p>Note: each Keras Application expects a specific kind of input preprocessing. For VGG16, call <code>tf.keras.applications.vgg16.preprocess_input</code> on your inputs before passing them to the model.</p>
</blockquote>
<p>However, I am struggling to get this <code>preprocess_input</code> working with a <code>BatchDataset</code>.  <strong>Can you please help me figure out how to connect these two dots?</strong></p>
<p>Please see the below code:</p>
<pre class=""lang-py prettyprint-override""><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_data_dir, image_size=(224, 224))
train_ds = tf.keras.applications.vgg16.preprocess_input(train_ds)
</code></pre>
<p>This will throw <code>TypeError: 'BatchDataset' object is not subscriptable</code>:</p>
<pre><code>Traceback (most recent call last):
  ...
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/vgg16.py&quot;, line 232, in preprocess_input
    return imagenet_utils.preprocess_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 117, in preprocess_input
    return _preprocess_symbolic_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 278, in _preprocess_symbolic_input
    x = x[..., ::-1]
TypeError: 'BatchDataset' object is not subscriptable
</code></pre>
<p>From <a href=""https://github.com/tensorflow/tensorflow/issues/39338"" rel=""nofollow noreferrer"">TypeError: 'DatasetV1Adapter' object is not subscriptable</a> (from <a href=""https://stackoverflow.com/q/61642569/11163122"">BatchDataset not subscriptable when trying to format Python dictionary as table</a>) the suggestion was to use:</p>
<pre class=""lang-py prettyprint-override""><code>train_ds = tf.keras.applications.vgg16.preprocess_input(
    list(train_ds.as_numpy_iterator())
)
</code></pre>
<p>However, this also fails:</p>
<pre><code>Traceback (most recent call last):
  ...
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/vgg16.py&quot;, line 232, in preprocess_input
    return imagenet_utils.preprocess_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 117, in preprocess_input
    return _preprocess_symbolic_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 278, in _preprocess_symbolic_input
    x = x[..., ::-1]
TypeError: list indices must be integers or slices, not tuple
</code></pre>
<p>This is all using <code>Python==3.10.3</code> with <code>tensorflow==2.8.0</code>.</p>
<p>How can I get this working?  Thank you in advance.</p>
",2022-05-08 23:44:35,"<p>Okay I figured it out.  I needed to pass a <code>tf.Tensor</code>, not a <code>tf.data.Dataset</code>.  One can get a <code>Tensor</code> out by iterating over the <code>Dataset</code>.</p>
<p>This can be done in a few ways:</p>
<pre class=""lang-py prettyprint-override""><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(...)

# Option 1
batch_images = next(iter(train_ds))[0]
preprocessed_images = tf.keras.applications.vgg16.preprocess_input(batch_images)

# Option 2:
for batch_images, batch_labels in train_ds:
    preprocessed_images = tf.keras.applications.vgg16.preprocess_input(batch_images)
</code></pre>
<p>If you convert option 2 into a generator, it can be directly passed into the downstream <code>model.fit</code>. Cheers!</p>
",11163122,2513,https://stackoverflow.com/questions/72165812,Documentation Replication on Other Examples
68431633,tf.image.stateless_random_crop VS. tf.image.random_crop. Shouldn't these be the same thing?,"<p>In tf 2.5, there are two functions for cropping an image: <code>tf.image.stateless_random_crop</code>, and <code>tf.image.random_crop</code>. The documentation states that <code>stateless_random_crop</code> is deterministic (always returns the same crop given one seed). However, <code>random_crop</code> has a seed parameter and is also deterministic, one would think. What is the actual difference between these two functions? I cannot find information about statelessness in Tensorflow anywhere.</p>
<p>The differences between <code>tf.image.stateless_random_crop</code>, and <code>tf.image.random_crop</code> are one line where stateless_random_uniform is used instead of a random_uniform:
stateless_random_crop: <a href=""https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465</a>
random_crop: <a href=""https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412</a></p>
<p>I always thought that <code>random_crop</code> would always return the same crop given a seed, but it looks like maybe that wasn't always true? Any enlightenment about statelessness in Tensorflow is greatly appreciated!</p>
",2021-07-18 17:39:57,"<p><code>random_crop</code> always return the same sequence of crops only when <strong>both</strong> global seed <strong>and</strong> operation seed are set.</p>
<ol>
<li>global seed is set using <code>tf.random.set_seed(global_seed)</code></li>
<li>operation seed is set by passing the seed argument into the operation, i.e., <code>tf.image.random_crop(value, size, seed=ops_seed)</code></li>
</ol>
<p>whereas what <code>stateless_random_crop</code> returns is totally determined by the seed you pass into it when the device and tensorflow version are unchanged.</p>
<p>And you are correct that the functions look redundant and duplicate but actually <code>tf.image.random_crop</code> is from the old RNGs API and it may be buggy in graph mode. The new RNGs API is <code>tf.random.Generator</code> and the stateless RNGs. For more information, see <a href=""https://www.tensorflow.org/guide/random_numbers"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/random_numbers</a></p>
<p>Using <code>tf.random.Generator</code> in combination with <code>stateless_random_crop</code>:</p>
<pre><code>class new_RNGs_random_crop:
  def __init__(self,seed,size):
    self.rand_generator=tf.random.Generator.from_seed(seed)
    self.size=size
  def random_crop(self,x):
    return tf.image.stateless_random_crop(x,self.size,
           seed=self.rand_generator.uniform_full_int([2],dtype=tf.int32))

dummy_dataset=tf.data.Dataset.from_tensor_slices(np.arange(2*3*3).reshape((2,3,3))).batch(1)
cropper=new_RNGs_random_crop(88883,(1,2,2))
dummy_dataset=dummy_dataset.map(cropper.random_crop)

for image in dummy_dataset:
  print(image)
</code></pre>
<p>Example outputs:</p>
<pre><code>tf.Tensor(
[[[3 4]
  [6 7]]], shape=(1, 2, 2), dtype=int64)
tf.Tensor(
[[[ 9 10]
  [12 13]]], shape=(1, 2, 2), dtype=int64)
</code></pre>
",11632499,185,https://stackoverflow.com/questions/68431633,Documentation Ambiguity
71596616,Predicting Data using an Untrained Keras Model,"<p>Essentially, I want to propagate data through a Keras model, without first training the Keras model. I tried using both predict() and feeding in raw tensors into the model.</p>
<p>The data is a 2D Numpy float64 array with shape (3, 3), filled entirely with zeros.</p>
<p>The model itself is outlined below:</p>
<pre><code>inputs = keras.Input(shape=(3,), batch_size=1)
FFNNlayer1 = keras.layers.Dense(100, activation='relu')(inputs)
FFNNlayer2 = keras.layers.Dense(100, activation='relu')(FFNNlayer1)
numericalOutput = keras.layers.Dense(3, activation='sigmoid')(FFNNlayer2)
categoricalOutput = keras.layers.Dense(9, activation='softmax')(FFNNlayer2)
outputs = keras.layers.concatenate([numericalOutput, categoricalOutput])
hyperparameters = keras.Model(inputs=inputs, outputs=outputs, name=&quot;hyperparameters&quot;)
hyperparameters.summary()
</code></pre>
<p>The model needed two different activation functions in it's output layer, hence why I used Functional API.</p>
<p>I first attempted to use <code>hyperparameter.predict(data[0])</code>, but kept getting the following error:</p>
<pre><code>WARNING:tensorflow:Model was constructed with shape (1, 3) for input KerasTensor(type_spec=TensorSpec(shape=(1, 3), dtype=tf.float32, name='input_15'), name='input_15', description=&quot;created by layer 'input_15'&quot;), but it was called on an input with incompatible shape (None,).
Traceback (most recent call last):

  File &quot;&lt;ipython-input-144-4c4a629eaefa&gt;&quot;, line 1, in &lt;module&gt;
    mainNet.hyperparameters.predict([dataset_info[0]])

  File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File &quot;C:\Users\hudso\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\func_graph.py&quot;, line 1129, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)

ValueError: in user code:

    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1621, in predict_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1611, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1604, in run_step  **
        outputs = model.predict_step(data)
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1572, in predict_step
        return self(x, training=False)
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\input_spec.py&quot;, line 227, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer &quot;{layer_name}&quot; '

    ValueError: Exception encountered when calling layer &quot;hyperparameters&quot; (type Functional).
    
    Input 0 of layer &quot;dense_20&quot; is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)
    
    Call arguments received:
       inputs=('tf.Tensor(shape=(None,), dtype=float32)',)
       training=False
       mask=None
</code></pre>
<p>I fiddled around with array dimensions a bit, but the model continued to give the same error. I then tried feeding raw tensors into the model, with the following code:</p>
<pre><code>tensorflow_dataset_info =  tf.data.Dataset.from_tensor_slices([dataset_info[0]]).batch(1)
aaaaa = enumerate(tensorflow_dataset_info)
predictions = mainNet.hyperparameters(aaaaa)
</code></pre>
<p>This code continued to give the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-143-df51fe8fd203&gt;&quot;, line 1, in &lt;module&gt;
    hyperparameters = mainNet.hyperparameters(enumerate(tensorflow_dataset_info))

  File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\input_spec.py&quot;, line 196, in assert_input_compatibility
    raise TypeError(f'Inputs to a layer should be tensors. Got: {x}')

TypeError: Inputs to a layer should be tensors. Got: &lt;enumerate object at 0x000001F60081EA40&gt;
</code></pre>
<p>I've looked online for a while, and I've searched through the tf.data documentation, but I'm still not sure how to fix this. Again, I've tried multiple variations of this code, and I continue to get mostly the same errors.</p>
",2022-03-24 03:10:28,"<p>If <code>data.shape = (3, 3)</code>, when you pass <code>data[0]</code> to <code>model.predict()</code>, you are actually sending a vector of shape <code>(3, )</code>, but your model is expecting shape <code>(1, 3)</code> which means 1 example of size 3.</p>
<p>Try slicing your data instead:</p>
<pre><code>model.predict(data[:1])
</code></pre>
<p>This way your tensor will have shape (1, 3).</p>
",11442631,49,https://stackoverflow.com/questions/71596616,Documentation Ambiguity
69195950,Problem with inputs when building a model with TFBertModel and AutoTokenizer from HuggingFace's transformers,"<p>I'm trying to build the model illustrated in this picture:
<a href=""https://i.stack.imgur.com/4eiAK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4eiAK.png"" alt=""enter image description here"" /></a></p>
<p>I obtained a pre-trained BERT and respective tokenizer from HuggingFace's <code>transformers</code> in the following way:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, TFBertModel
model_name = &quot;dbmdz/bert-base-italian-xxl-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
bert = TFBertModel.from_pretrained(model_name)
</code></pre>
<p>The model will be fed a sequence of italian tweets and will need to determine if they are ironic or not.</p>
<p>I'm having problems building the initial part of the model, which takes the inputs and feeds them to the tokenizer in order to get a representation I can feed to BERT.</p>
<p>I can do it outside of the model-building context:</p>
<pre><code>my_phrase = &quot;Ciao, come va?&quot;
# an equivalent version is tokenizer(my_phrase, other parameters)
bert_input = tokenizer.encode(my_phrase, add_special_tokens=True, return_tensors='tf', max_length=110, padding='max_length', truncation=True) 
attention_mask = bert_input &gt; 0
outputs = bert(bert_input, attention_mask)['pooler_output']
</code></pre>
<p>but I'm having troubles building a model that does this. Here is the code for building such a model (the problem is in the first 4 lines ):</p>
<pre><code>def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = tokenizer(text_input, return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  
  X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(net)
  X = tf.keras.layers.Concatenate(axis=-1)([X, input_layer])
  X = tf.keras.layers.MaxPooling1D(20)(X)
  X = tf.keras.layers.SpatialDropout1D(0.4)(X)
  X = tf.keras.layers.Flatten()(X)
  X = tf.keras.layers.Dense(128, activation=&quot;relu&quot;)(X)
  X = tf.keras.layers.Dropout(0.25)(X)
  X = tf.keras.layers.Dense(2, activation='softmax')(X)

  model = tf.keras.Model(inputs=text_input, outputs = X) 
  
  return model
</code></pre>
<p>And when I call the function for creating this model I get this error:</p>
<blockquote>
<p>text input must of type <code>str</code> (single example), <code>List[str]</code> (batch or single pretokenized example) or <code>List[List[str]]</code> (batch of pretokenized examples).</p>
</blockquote>
<p>One thing I thought was that maybe I had to use the <code>tokenizer.batch_encode_plus</code> function which works with lists of strings:</p>
<pre class=""lang-py prettyprint-override""><code>class BertPreprocessingLayer(tf.keras.layers.Layer):
  def __init__(self, tokenizer, maxlength):
    super().__init__()
    self._tokenizer = tokenizer
    self._maxlength = maxlength
  
  def call(self, inputs):
    print(type(inputs))
    print(inputs)
    tokenized = tokenizer.batch_encode_plus(inputs, add_special_tokens=True, return_tensors='tf', max_length=self._maxlength, padding='max_length', truncation=True)
    return tokenized

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = BertPreprocessingLayer(tokenizer, 100)(text_input)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  # ... same as above
</code></pre>
<p>but I get this error:</p>
<blockquote>
<p>batch_text_or_text_pairs has to be a list (got &lt;class 'keras.engine.keras_tensor.KerasTensor'&gt;)</p>
</blockquote>
<p>and beside the fact I haven't found a way to convert that tensor to a list with a quick google search, it seems weird that I have to go in and out of tensorflow in this way.</p>
<p>I've also looked up on the huggingface's <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""noreferrer"">documentation</a> but there is only a single usage example, with a single phrase, and what they do is analogous at my &quot;out of model-building context&quot; example.</p>
<p>EDIT:</p>
<p>I also tried with <code>Lambda</code>s in this way:</p>
<pre><code>tf.executing_eagerly()

def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  return tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='text')
  
  encoder_inputs = tf.keras.layers.Lambda(tokenize_tensor, name='tokenize')(text_input)
  ...
  
  outputs = bert(encoder_inputs)
</code></pre>
<p>but I get the following error:</p>
<blockquote>
<p>'Tensor' object has no attribute 'numpy'</p>
</blockquote>
<p>EDIT 2:</p>
<p>I also tried the approach suggested by @mdaoust of wrapping everything in a <code>tf.py_function</code> and got this error.</p>
<pre class=""lang-py prettyprint-override""><code>def py_func_tokenize_tensor(tensor):
  return tf.py_function(tokenize_tensor, [tensor], Tout=[tf.int32, tf.int32, tf.int32])
</code></pre>
<blockquote>
<p>eager_py_func() missing 1 required positional argument: 'Tout'</p>
</blockquote>
<p>Then I defined Tout as the type of the value returned by the tokenizer:</p>
<p><code>transformers.tokenization_utils_base.BatchEncoding</code></p>
<p>and got the following error:</p>
<blockquote>
<p>Expected DataType for argument 'Tout' not &lt;class
'transformers.tokenization_utils_base.BatchEncoding'&gt;</p>
</blockquote>
<p>Finally I unpacked the value in the BatchEncoding in the following way:</p>
<pre class=""lang-py prettyprint-override""><code>def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  dictionary = tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)
  #unpacking
  input_ids = dictionary['input_ids']
  tok_type = dictionary['token_type_ids']
  attention_mask = dictionary['attention_mask']
  return input_ids, tok_type, attention_mask
</code></pre>
<p>And get an error in the line below:</p>
<pre class=""lang-py prettyprint-override""><code>...
outputs = bert(encoder_inputs)
</code></pre>
<blockquote>
<p>ValueError: Cannot take the length of shape with unknown rank.</p>
</blockquote>
",2021-09-15 15:28:40,"<p>For now I solved by taking the tokenization step out of the model:</p>
<pre class=""lang-py prettyprint-override""><code>def tokenize(sentences, tokenizer):
    input_ids, input_masks, input_segments = [],[],[]
    for sentence in sentences:
        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=128, pad_to_max_length=True, return_attention_mask=True, return_token_type_ids=True)
        input_ids.append(inputs['input_ids'])
        input_masks.append(inputs['attention_mask'])
        input_segments.append(inputs['token_type_ids'])        
        
    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')
</code></pre>
<p>The model takes two inputs which are the first two values returned by the tokenize funciton.</p>
<pre class=""lang-py prettyprint-override""><code>def build_classifier_model():
   input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')
   input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') 

   embedding_layer = bert(input_ids_in, attention_mask=input_masks_in)[0]
...
   model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)

   for layer in model.layers[:3]:
     layer.trainable = False
   return model
</code></pre>
<p>I'd still like to know if someone has a solution which integrates the tokenization step inside the model-building context so that an user of the model can simply feed phrases to it to get a prediction or to train the model.</p>
",11579184,1644,https://stackoverflow.com/questions/69195950,Inadequate Examples
56452714,"replacement of ""tf.gather_nd""","<p>I am doing a project but their tensorflow version does not support  tf.gather_nd. I am asking if possible that use tf.gather, tf.slice or tf.strided_slice to rewrite a function of tf.gather_nd?</p>

<p>tf.gather_nd is used to gather slices from a tensor into a Tensor with shape specified by indices. details can be found in <a href=""https://www.tensorflow.org/api_docs/python/tf/gather_nd"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/gather_nd</a></p>

<p>Thanks,</p>
",2019-06-04 23:58:27,"<p>This function should do an equivalent work:</p>

<pre><code>import tensorflow as tf
import numpy as np

def my_gather_nd(params, indices):
    idx_shape = tf.shape(indices)
    params_shape = tf.shape(params)
    idx_dims = idx_shape[-1]
    gather_shape = params_shape[idx_dims:]
    params_flat = tf.reshape(params, tf.concat([[-1], gather_shape], axis=0))
    axis_step = tf.cumprod(params_shape[:idx_dims], exclusive=True, reverse=True)
    indices_flat = tf.reduce_sum(indices * axis_step, axis=-1)
    result_flat = tf.gather(params_flat, indices_flat)
    return tf.reshape(result_flat, tf.concat([idx_shape[:-1], gather_shape], axis=0))

# Test
np.random.seed(0)
with tf.Graph().as_default(), tf.Session() as sess:
    params = tf.constant(np.random.rand(10, 20, 30).astype(np.float32))
    indices = tf.constant(np.stack([np.random.randint(10, size=(5, 8)),
                                    np.random.randint(20, size=(5, 8))], axis=-1))
    result1, result2 = sess.run((tf.gather_nd(params, indices),
                                 my_gather_nd(params, indices)))
    print(np.allclose(result1, result2))
    # True
</code></pre>
",11601605,13,https://stackoverflow.com/questions/56452714,Documentation Replicability
56767246,How to wrap tf.cond function with keras.layers.Lambda?,"<p>I'm trying to define a custom layer in keras,but I can't find a way to warp tf.cond with layers.Lambda function</p>

<pre class=""lang-py prettyprint-override""><code>        matches = tf.cond(
            tf.greater(N, 0),
            lambda: match_boxes(
                anchors, groundtruth_boxes,
                positives_threshold=positives_threshold,
                negatives_threshold=negatives_threshold,
                force_match_groundtruth=True
            ),
            lambda: only_background
        )
</code></pre>
",2019-06-26 07:17:00,"<p>Since the body of your true function is very big, you could create a custom layer like this:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

class CustomLayer(tf.keras.layers.Layer):

  def __init__(self, **kwargs):
    super(CustomLayer, self).__init__()
    self.pred = kwargs.get('pred', False)

  def call(self, inputs):
    def true_fn(x):
      return x + 1.

    return tf.cond(self.pred,
                   true_fn=lambda: true_fn(inputs),
                   false_fn=lambda: tf.identity(inputs))
</code></pre>

<p>Testing:</p>

<pre class=""lang-py prettyprint-override""><code>inputs = tf.placeholder(tf.float32, shape=(None, 1))
pred = tf.placeholder(tf.bool, shape=())

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(1, kernel_initializer=tf.initializers.ones))
model.add(CustomLayer(pred=pred))

outputs = model(inputs)

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  print(outputs.eval({inputs: [[1.]], pred: False})) # [[1.]]
  print(outputs.eval({inputs: [[1.]], pred: True})) # [[2.]]
</code></pre>
",11701537,35,https://stackoverflow.com/questions/56767246,Documentation Replicability
60047705,Repeated use of GradientTape for multiple Jacobian calculations,"<p>I am attempting to compute the Jacobian of a TensorFlow neural network's outputs with respect to its inputs. This is easily achieved with the <code>tf.GradientTape.jacobian</code> method. The trivial example provided in the TensorFlow documentation is as follows:</p>

<pre><code>with tf.GradientTape() as g:
  x  = tf.constant([1.0, 2.0])
  g.watch(x)
  y = x * x
jacobian = g.jacobian(y, x)
</code></pre>

<p>This is fine if I want only want to compute the Jacobian of a single instance of the input tensor <code>x</code>. However, I need to repeatedly evaluate this Jacobian many, many times for various instances of <code>x</code>. For a non-trivial Jacobian calculation (e.g. for a deep convolutional neural network with non-linear activation functions), this is incredibly expensive to repeatedly rerun the GradientTape calculation and evaluate the <code>jacobian</code> method. I know from the <a href=""https://www.tensorflow.org/tutorials/customization/autodiff"" rel=""nofollow noreferrer"">TensorFlow documentation</a> that the gradients (and hence the Jacobian) are computed via automatic differentiation.  I have to imagine there is some internal storage of the analytical gradient of the network (computed by automatic differentiation) which is evaluated at the given inputs. </p>

<p>My question: am I correct in assuming that TensorFlow builds and stores (at least parts of) the analytical gradients needed to compute the Jacobian? And if so, is there a way to save this analytical gradient and re-evaluate the Jacobian with new inputs without having to reconstruct it via the GradientTape method?</p>

<p>A ""persistent"" GradientTape does not seem to solve this issue: it only allows for the repeated evaluation of a single GradientTape instance with respect to multiple internal arguments of the computation.</p>
",2020-02-03 21:54:37,"<p>Maybe you find this helpful:</p>

<p>I needed to compute the jacobian of an arbitrary function many, many times. My problem was that I was using <code>GradientTape</code> inappropriately, but the code I posted might help you or give you some insight. I posted a self contained example of calculating the jacobian using both the session based <code>tf.gradient()</code> function and the modern <code>GriadientTape</code> approach. With help, I got them to run within the same order of magnitude of each other.</p>

<ul>
<li>If your question is focused on trying to reuse the intermediate calculations between calls for a speed boost, then I think Nick's answer is more applicable.</li>
<li>If your question is focused on trying to make GradientTape as fast as a static graph, then make sure you wrap it in <code>@tf.function</code> since it does just that.</li>
</ul>

<p>See my question: <a href=""https://stackoverflow.com/questions/61810094/abysmal-tf-gradienttape-performance-compared-to-tf-gradients-for-computing-jac"">Abysmal tf.GradientTape performance compared to tf.gradients() for computing jacobians</a></p>
",11737392,23,https://stackoverflow.com/questions/60047705,Documentation Replication on Other Examples
66231467,How to set a minimum number of epoch in Optuna SuccessiveHalvingPruner()?,"<p>I'm using Optuna 2.5 to optimize a couple of hyperparameters on a tf.keras CNN model. I want to use pruning so that the optimization skips the less promising corners of the hyperparameters space. I'm using something like this:</p>
<pre><code>study0 = optuna.create_study(study_name=study_name,
                             storage=storage_name,
                             direction='minimize', 
                             sampler=TPESampler(n_startup_trials=25, multivariate=True, seed=123),
                             pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource='auto',
                             reduction_factor=4, min_early_stopping_rate=0),
                             load_if_exists=True)
</code></pre>
<p>Sometimes the model stops after 2 epochs, some other times it stops after 12 epochs, 48 and so forth. What I want is to ensure that the model always trains at least 30 epochs before being pruned. I guess that the parameter <code>min_early_stopping_rate</code> might have some control on this but I've tried to change it from 0 to 30 and then  the models never get pruned. Can someone explain me a bit better than the Optuna documentation, what these parameters in the  <code>SuccessiveHalvingPruner()</code> really do (specially <code>min_early_stopping_rate</code>)?
Thanks</p>
",2021-02-16 20:08:37,"<p><code>min_resource</code>'s explanation on <a href=""https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.SuccessiveHalvingPruner.html"" rel=""nofollow noreferrer"">the documentation</a> says</p>
<blockquote>
<p>A trial is never pruned until it executes <code>min_resource * reduction_factor ** min_early_stopping_rate</code> steps.</p>
</blockquote>
<p>So, I suppose that we need to replace the value of <code>min_resource</code> with a specific number depending on <code>reduction_factor</code> and <code>min_early_stopping_rate</code>.</p>
",11433296,13,https://stackoverflow.com/questions/66231467,Requesting (Additional) Documentation/Examples
70686521,"Slice of 20 elements of rank1 tensor then reshaping throws ""Input to reshape is tensor with 10272 values, but requested shape requires multiple of 20""","<p>My input tensor <code>Data = Input(shape=(856,))</code> is a vector of float32 values concatenated from many different devices. I am trying to apply different TensorFlow functions to different subslices of each input chunk. Some of these functions include a 1D Convolution which requires a reshape.</p>
<pre><code>slice = Data[:20]
reshape = tf.reshape(slice, (-1, 20, 1))
...
</code></pre>
<p>Doing this crashes after trying to fit my model. It throws the following errors:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input to reshape is a tensor with 10272 values, but the requested shape requires a multiple of 20
         [[node model/tf.reshape_1/Reshape
 (defined at /home/.local/lib/python3.8/site-packages/keras/layers/core/tf_op_layer.py:261)
]] [Op:__inference_train_function_1858]

Errors may have originated from an input operation.
Input Source operations connected to node model/tf.reshape_1/Reshape:
In[0] model/tf.__operators__.getitem_1/strided_slice:
In[1] model/tf.reshape_1/Reshape/shape:
</code></pre>
<p>I am not sure how slicing 20 elements from a tensor of 856 could result in a tensor of 10272 values.</p>
<p>I have also tried using the <code>tf.slice</code> function a couple of different ways; both fail. Referencing the docs: <a href=""https://www.tensorflow.org/guide/tensor_slicing"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/tensor_slicing</a></p>
<pre><code>slice = tf.slice(Data, begin=[0], size=[20]) 
...
</code></pre>
<p>And fails, stating:</p>
<pre><code>Shape must be rank 1 but is rank 2 for '{{node tf.slice/Slice}} = Slice[Index=DT_INT32, T=DT_FLOAT](Placeholder, tf.slice/Slice/begin, tf.slice/Slice/size)' with input shapes: [?,856], [1], [1].
</code></pre>
<p>For reference, here is what some of the values look like in the input data</p>
<pre><code>array([-9.55784683e+01, -1.70557899e+01,  2.95967350e+01,  7.81378937e+00,
        9.02729130e+00,  5.49621725e+00,  4.19811630e+00,  5.84186697e+00,
        4.90438080e+00,  3.73845983e+00,  5.12300587e+00,  2.61530232e+00,
        2.67061424e+00,  3.91038632e+00,  2.31110978e+00,  4.20644665e+00,
        4.50000000e+00,  9.87345278e-01,  1.59740388e+00,  6.30727148e+00,
...
</code></pre>
",2022-01-12 18:20:52,"<p>When you slice data like in <code>Data[:20]</code> it will produce a sequence with length <code>min(20, len(Data))</code>. So I guess your data has length less than 20.</p>
<p>Other message says it has rank 2, so I guess it has one of the following shapes</p>
<pre class=""lang-txt prettyprint-override""><code>       1   10272
       2    5136
       3    3424
       4    2568
       6    1712
       8    1284
      12     856
      16     642
</code></pre>
<p>Any of those result in a tensor with <code>10272</code> elements as your first message shows, and that's not a multiple of 20.</p>
",11590962,37,https://stackoverflow.com/questions/70686521,Documentation Replicability
65260831,Why does tf.image.ssim always return AttributeError: 'numpy.ndarray' object has no attribute 'get_shape',"<p>I am trying to use <code>tf.image.ssim</code> to get the similarity between 2 images, however, it returns an attribute error. Since I am just directly using the TensorFlow code, I don't see any way to debug this issue.</p>
<pre><code>import tensorflow as tf
from sklearn import datasets
import matplotlib.pyplot as plt
iris = datasets.load_iris()
x_train, y= tf.keras.datasets.mnist.load_data(
    path='mnist.npz'
)

tf.image.ssim(
    x_train[0][0], x_train[0][0], 255
)
</code></pre>
",2020-12-12 01:57:03,"<p>MNIST returns a grayscale image which is in 2D, SSIM requires the image to be in 3D. So just expand the dims of the returned image that you want to compare and apply SSIM on it.</p>
<pre><code>import numpy as np

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(
    path='mnist.npz'
)

x_train_expanded = np.expand_dims(x_train[0], axis=2)
print(tf.image.ssim(x_train_expanded, x_train_expanded, 255))
</code></pre>
<p>It returns the following:</p>
<pre><code>tf.Tensor(1.0, shape=(), dtype=float32)
</code></pre>
<p>The returned tensor contains an MS-SSIM value for each image in batch. The values are in the range [0, 1] and the example returns a value of 1 indicating both images are identical.</p>
",11628437,1341,https://stackoverflow.com/questions/65260831,Documentation Replicability
71090570,How do I create a tf.Tensor from a pandas DataFrame containing arrays?,"<p>I have a pandas DataFrame like below.</p>
<pre><code>import pandas as pd
import numpy as np
import tensorflow as tf  # Version 2.8.0
df = pd.DataFrame({&quot;id&quot;: 
                   [&quot;i123&quot;, &quot;i456&quot;],  
                   &quot;col&quot;: [np.array([&quot;igh&quot;, &quot;ghdd&quot;, &quot;yu&quot;]),
                           np.array([&quot;uh&quot;, &quot;lkk&quot;, &quot;nj&quot;])]})
print(df)
</code></pre>
<p>Output:</p>
<pre><code>    id      col
0   i123    [igh, ghdd, yu]
1   i456    [uh, lkk, nj]
</code></pre>
<p>I would to create a <code>Tensor</code> from the values of the <code>col</code> column, in order to use them in a specific use case. I have tried converting the values like</p>
<pre><code>values = df[&quot;col&quot;].to_numpy()
values
</code></pre>
<p>Which looks like:</p>
<pre><code>array([array(['igh', 'ghdd', 'yu'], dtype='&lt;U4'),
       array(['uh', 'lkk', 'nj'], dtype='&lt;U3')], dtype=object)
</code></pre>
<p>When I try to convert this to a Tensor, by</p>
<pre><code>tf.constant(values)
</code></pre>
<p>I get an exception:</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).
</code></pre>
<p>I can see from the <a href=""https://www.tensorflow.org/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">documentation</a> that the tf.constant method should work on a very similar array
<a href=""https://i.stack.imgur.com/zscVM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zscVM.png"" alt=""TF docs"" /></a></p>
<p>The <code>values</code> variable I create have <code>.shape</code> like <code>(2,)</code> while the image below have <code>(2, 3)</code>, which might be the problem.
I can't seem to get the dtype and/or shape to match exactly, and I'm unsure how to get it to work. Any ideas?</p>
",2022-02-12 09:27:37,"<p>Try:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
import tensorflow as tf  # Version 2.8.0
df = pd.DataFrame({&quot;id&quot;: 
                   [&quot;i123&quot;, &quot;i456&quot;],  
                   &quot;col&quot;: [np.array([&quot;igh&quot;, &quot;ghdd&quot;, &quot;yu&quot;]),
                           np.array([&quot;uh&quot;, &quot;lkk&quot;, &quot;nj&quot;])]})

values = df[&quot;col&quot;].to_list()
print(tf.constant(values))
</code></pre>
<pre><code>tf.Tensor(
[[b'igh' b'ghdd' b'yu']
 [b'uh' b'lkk' b'nj']], shape=(2, 3), dtype=string)
</code></pre>
<p>Or</p>
<pre><code>values = np.stack(df[&quot;col&quot;].to_numpy())
print(tf.constant(values))
</code></pre>
",11764097,943,https://stackoverflow.com/questions/71090570,Documentation Replicability
55640642,Error when attempting to change tensor shape in keras model,"<p>I want to change the shape and the content of the tensor in a keras model. Tensor is the output of a layer and has </p>

<p><code>shape1=(batch_size, max_sentences_in_doc, max_tokens_in_doc, embedding_size)</code> </p>

<p>and I want to convert to </p>

<p><code>shape2=(batch_size, max_documents_length, embedding_size)</code> </p>

<p>suitable as input of the next layer. Here sentences are made of tokens, and are zero-padded so every sentence has <code>length=max_tokens_in_sentence</code>. 
In detail:</p>

<ol>
<li>I wanto to concatenate all the sentences of a batch taking only the nonzero part of the sentences;</li>
<li>then I zero-pad this concatenation to a <code>length=max_document_length</code>.</li>
</ol>

<p>So passing from <code>shape1</code> to <code>shape2</code> is not only a reshape as mathematical operations are involved.</p>

<p>I created the function <code>embedding_to_docs(x)</code> that iterates on the tensor of shape1 to transform it into shape2. I call the function using a Lambda layer in the model, it works in debug with fictious data, but when I try to call it during the build of the model an error is raised: </p>

<p><code>Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.</code></p>

<pre><code>def embedding_to_docs(x):
    new_output = []
    for doc in x:
        document = []
        for sentence in doc:
            non_zero_indexes = np.nonzero(sentence[:, 0])
            max_index = max(non_zero_indexes[0])
            if max_index &gt; 0:
                document.extend(sentence[0:max_index])
        if MAX_DOCUMENT_LENGTH-len(document) &gt; 0:
            a = np.zeros((MAX_DOCUMENT_LENGTH-len(document), 1024))
            document.extend(a)
        else:
            document = document[0:MAX_DOCUMENT_LENGTH]
        new_output.append(document)

    return np.asarray(new_output)

...

# in the model:
tensor_of_shape2 = Lambda(embedding_to_docs)(tensor_of_shape1)

</code></pre>

<p>How to fix this?</p>
",2019-04-11 20:27:52,"<p>You can use <a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""nofollow noreferrer""><code>py_function</code></a>, which allows you to switch from the graph mode (used by Keras) to the eager mode (where it is possible to iterate over tensors like in your function).</p>

<pre><code>def to_docs(x):
  return tf.py_function(embedding_to_docs, [x], tf.float32)

tensor_of_shape2 = Lambda(to_docs)(tensor_of_shape1)
</code></pre>

<p>Note that the code run within your <code>embedding_to_docs</code> must be written in tensorflow eager instead of numpy. This means that you'd need to replace some of the numpy calls with tensorflow. You'd surely need to replace the return line with:</p>

<pre><code>return tf.convert_to_tensor(new_output)
</code></pre>

<p>Using numpy arrays will stop the gradient computation, but you are likely not interested in gradient flowing through the input data anyway.</p>
",11348106,13,https://stackoverflow.com/questions/55640642,Documentation Replication on Other Examples
57279754,"What are the Tensorflow qint8, quint8, qint32, qint16, and quint16 datatypes?","<p>I'm looking at the Tensorflow tf.nn.quantized_conv2d function and I'm wondering what exactly the qint8, etc. dataypes are, particularly if they are the datatypes used for the ""fake quantization nodes"" in tf.contrib.quantize or are actually stored using 8 bits (for qint8) in memory.</p>

<p>I know that they are defined in tf.dtypes.DType, but that doesn't have any information about what they actually are.</p>
",2019-07-30 20:43:43,"<p>These are the data types of the <code>output Tensor</code> of the function, <code>tf.quantization.quantize()</code>. This corresponds to the Argument, <code>T</code> of the function.</p>

<p>Mentioned below is the underlying code, which converts/quantizes a Tensor from one Data Type (e.g. <code>float32</code>) to another (<code>tf.qint8, tf.quint8, tf.qint32, tf.qint16, tf.quint16</code>).</p>

<pre><code>out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
</code></pre>

<p>Then, they can be passed to functions like <code>tf.nn.quantized_conv2d</code>, etc.., whose input is a Quantized Tensor, explained above. </p>

<p><strong>TLDR</strong>, to answer your question in short, they are actually stored 8 bits (for <code>qint8</code>) in memory. </p>

<p>You can find more information about this topic in the below links:</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/quantization/quantize"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/quantization/quantize</a></p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/quantized_conv2d"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/quantized_conv2d</a></p>

<p><a href=""https://www.tensorflow.org/lite/performance/post_training_quantization"" rel=""noreferrer"">https://www.tensorflow.org/lite/performance/post_training_quantization</a></p>

<p>If you feel this answer is useful, kindly accept this answer and/or up vote it. Thanks.</p>
",11860666,103,https://stackoverflow.com/questions/57279754,Documentation Replication on Other Examples
67749813,TFRecord parsing for 3-D features,"<p>I have similar question to <a href=""https://stackoverflow.com/questions/49588382/how-to-convert-float-array-list-to-tfrecord"">this</a>, but what if my feature shape is 3-D? Instead of prices <code>(1,288)</code>, it is <code>(1,288,3)</code> for example. What should I put as the shape of <code>tf.io.FixedLenFeature()</code>? Is it <code>tf.io.FixedLenFeature(shape=[288,3], tf.float32)</code> or <code>tf.io.FixedLenFeature(shape=[864], tf.float32)</code> or anything else? Thanks!</p>
",2021-05-29 09:31:30,"<p>There's a few ways you can do this. One is using a <a href=""https://www.tensorflow.org/api_docs/python/tf/train/BytesList"" rel=""nofollow noreferrer"">BytesList</a> feature</p>
<pre class=""lang-py prettyprint-override""><code>def _bytes_feature(value):
  return tf.train.Feature(
    bytes_list=tf.train.BytesList(value=[value]))
</code></pre>
<p>and the other is using a <a href=""https://www.tensorflow.org/api_docs/python/tf/train/FloatList"" rel=""nofollow noreferrer"">FloatList</a> feature</p>
<pre class=""lang-py prettyprint-override""><code>def _float_feature(value):
  return tf.train.Feature(
    float_list=tf.train.FloatList(value=value))
</code></pre>
<p><strong>Example</strong>:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf


# make some data
img = np.random.normal(size=(5, 3))
img = img.astype(np.float32)

writer = tf.io.TFRecordWriter(&quot;/tmp/data.tfrec&quot;)

example = tf.train.Example(
  features=tf.train.Features(
    feature = {
      &quot;img_b&quot;: _bytes_feature(img.tobytes()),
      &quot;img_f&quot;: _float_feature(img.flatten()),
    }))

writer.write(example.SerializeToString())
writer.close()

def parse_fn(example):
  features = {
    &quot;img_b&quot;: tf.io.FixedLenFeature([], tf.string),
    &quot;img_f&quot;: tf.io.FixedLenFeature([5, 3], tf.float32),
  }
  parsed_example = tf.io.parse_single_example(example, features)
  img_b = tf.io.decode_raw(
      parsed_example['img_b'],
      out_type=tf.float32)
  img_b = tf.reshape(img_b, (5, 3))
  img_f = parsed_example['img_f']
  return img_b, img_f
</code></pre>
<p>Let's import the data and see if it worked</p>
<pre class=""lang-py prettyprint-override""><code>dataset = tf.data.TFRecordDataset([&quot;/tmp/data.tfrec&quot;])
dataset = dataset.map(parse_fn).batch(1)

arr_b, arr_f = next(iter(dataset))

np.testing.assert_almost_equal(arr_b.numpy(), arr_f.numpy())
# passes
</code></pre>
<p>This assumes that you know the shape of your images and that they are all the same shape.</p>
",11970084,985,https://stackoverflow.com/questions/67749813,Documentation Replicability
64096624,what is the difference between using softmax as a sequential layer in tf.keras and softmax as an activation function for a dense layer?,"<p>what is the difference between using softmax as a sequential layer in tf.keras and softmax as an activation function for a dense layer?</p>
<pre><code>tf.keras.layers.Dense(10, activation=tf.nn.softmax)
</code></pre>
<p>and</p>
<pre><code>tf.keras.layers.Softmax(10)
</code></pre>
",2020-09-28 05:52:14,"<p>they are the same, you can test it on your own</p>
<pre><code># generate data
x = np.random.uniform(0,1, (5,20)).astype('float32')

# 1st option
X = Dense(10, activation=tf.nn.softmax)
A = X(x)

# 2nd option
w,b = X.get_weights()
B = Softmax()(tf.matmul(x,w) + b)

tf.reduce_all(A == B)
# &lt;tf.Tensor: shape=(), dtype=bool, numpy=True&gt;
</code></pre>
<p>Pay attention also when using <code>tf.keras.layers.Softmax</code>, it doesn't require to specify the units, it's a simple activation</p>
<p>by default, the softmax is computed on the -1 axis, you can change this if you have tensor outputs &gt; 2D and want to operate softmax on other dimensionalities. You can change this easily in the second option</p>
",11995630,322,https://stackoverflow.com/questions/64096624,Documentation Replication on Other Examples
57823210,How to combine tf.data.Dataset and tf.estimator.DNNRegressor properly,"<p>I am currently learning to use tensorflow and have troubles getting started.
I would like to use the newest API, namely estimator and dataset. But if I run the code presented below I get an Error.</p>

<p>On the tensorflow page <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor</a> I found, that ""The function should construct and return one of the following: * A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below.""</p>

<p>I thought my code would provide that, but there seems to be a problem and I am out of ideas.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
def input_evaluation_set():
    data = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
    labels = []
    for d in data:
        labels.append(1)
    return tf.data.Dataset.from_tensor_slices((tf.constant(data), tf.constant(labels)))

point = tf.feature_column.numeric_column('points')
estimator = tf.estimator.DNNRegressor(feature_columns = [point],hidden_units = [100,100,100])

estimator.train(input_fn = input_evaluation_set)
</code></pre>

<p>I expect to run a training session on a deep neural network with 3 hidden layers a' 100 neurons in order to approximate the 'constant 1' function;
instead I get the Error ""ValueError: features should be a dictionary of 'Tensor's. Given type: class, 'tensorflow.python.framework.ops.Tensor'</p>
",2019-09-06 13:42:53,"<p>You need to use .batch on your database in order to have the right format.</p>

<p>The following is working on my computer:</p>

<pre><code>import tensorflow as tf
import numpy as np

def basic_dataset(numPoints):
    data = np.linspace(0,1,numPoints)
    dataset = dict({'points': data})
    labels = []
    for d in data:
        labels.append(1)
    return tf.data.Dataset.from_tensor_slices((dataset, np.array(labels)))

def input_train_set():
    dataset = basic_dataset(11)
    return dataset.repeat(100).shuffle(1000).batch(1)

point = tf.feature_column.numeric_column('points')
estimator = tf.estimator.DNNRegressor(feature_columns = [point],hidden_units = [100,100,100], label_dimension = 1)

estimator.train(input_fn = input_train_set)
</code></pre>
",12030773,39,https://stackoverflow.com/questions/57823210,Documentation Replication on Other Examples
70747499,Using tf.map_fn when the function has multiple outputs,"<p>I can easily use tf.map_fn when the function has one output:</p>
<pre><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    return x[0]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p><strong>output:</strong></p>
<pre><code>tf.Tensor([1. 4.], shape=(2,), dtype=float32)
</code></pre>
<p>But, when the function has two outputs:</p>
<pre><code>def my_fun(x):
    return [x[0],x[1]]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p>I get an error. Not sure what is going on. I read the information about tf.map_fn in here <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/map_fn</a>, but not sure how to fix this:</p>
<p>map_fn also supports functions with multi-arity inputs and outputs:</p>
<p><em>If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems).
If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures.</em></p>
<p><strong>Output:</strong></p>
<pre><code>~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    317     _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types,
--&gt; 318                                            expand_composites)
    319   except (ValueError, TypeError) as e:

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-36-5b11c7fef461&gt; in &lt;module&gt;
      5     return [x[0],x[1]]
      6 
----&gt; 7 print(tf.map_fn(my_fun,tensaki))

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)
    266         back_prop=back_prop,
    267         swap_memory=swap_memory,
--&gt; 268         maximum_iterations=n)
    269     results_flat = [r.stack() for r in r_a]
    270 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   2712                                               list(loop_vars))
   2713       while cond(*loop_vars):
-&gt; 2714         loop_vars = body(*loop_vars)
   2715         if try_to_pack and not isinstance(loop_vars, (list, _basetuple)):
   2716           packed = True

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in &lt;lambda&gt;(i, lv)
   2703         cond = lambda i, lv: (  # pylint: disable=g-long-lambda
   2704             math_ops.logical_and(i &lt; maximum_iterations, orig_cond(*lv)))
-&gt; 2705         body = lambda i, lv: (i + 1, orig_body(*lv))
   2706       try_to_pack = False
   2707 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in compute(i, tas)
    256       packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])
    257       packed_fn_values = fn(packed_values)
--&gt; 258       nest.assert_same_structure(dtype or elems, packed_fn_values)
    259       flat_fn_values = output_flatten(packed_fn_values)
    260       tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    323                   &quot;Entire first structure:\n%s\n&quot;
    324                   &quot;Entire second structure:\n%s&quot;
--&gt; 325                   % (str(e), str1, str2))
    326 
    327 

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not
Entire first structure:
.
Entire second structure:
[., .]```
</code></pre>
",2022-01-17 21:04:57,"<p>You should make sure you are returning a tensor. Maybe concatenate or stack the list of values:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    x = tf.stack([x[0], x[1]], axis=0)
    return x

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<pre><code>tf.Tensor(
[[1. 2.]
 [4. 5.]], shape=(2, 2), dtype=float32)
</code></pre>
<p>Of course, it all depends on the output you are expecting.</p>
",11861082,399,https://stackoverflow.com/questions/70747499,Documentation Replication on Other Examples
57301699,Why we always use seed =1234 in tf.compat.v1.random.set_random_seed(1234).Is there any specific reason?,"<p>Why we always use ,
seed = 1234 in tf.compat.v1.random.set_random_seed(seed).Is there any specific reason?""</p>
",2019-08-01 04:06:41,"<p>Yeah, to make the random sequences generated by all ops be repeatable across sessions. By, the way <code>seed=1234</code> is random. You can select any value.</p>

<p>For example:</p>

<pre><code>a = tf.random.uniform([1])
b = tf.random.normal([1])

# Repeatedly running this block with the same graph will generate the 
# different sequences of 'a' and 'b' across sessions.
print(""Session 1"")
with tf.Session() as sess1:
  print(sess1.run(a))  # generates 'A1'
  print(sess1.run(a))  # generates 'A2'
  print(sess1.run(b))  # generates 'B1'
  print(sess1.run(b))  # generates 'B2'

print(""Session 2"")
with tf.Session() as sess2:
  print(sess2.run(a))  # generates 'A3'
  print(sess2.run(a))  # generates 'A4'
  print(sess2.run(b))  # generates 'B3'
  print(sess2.run(b))  # generates 'B4'
</code></pre>

<p>Output:</p>

<pre><code>Session 1
[0.35214436]
[0.61644566]
[-0.2290629]
[0.8414659]
Session 2
[0.62713313]
[0.5924448]
[-0.5366475]
[-0.36064562]
</code></pre>

<p>But, after setting a graph-level seed using <code>tf.random.set_random_seed(1234)</code>:</p>

<pre><code>tf.random.set_random_seed(1234)
a = tf.random.uniform([1])
b = tf.random.normal([1])

# Repeatedly running this block with the same graph will generate the same
# sequences of 'a' and 'b'.
print(""Session 1"")
with tf.Session() as sess1:
  print(sess1.run(a))  # generates 'A1'
  print(sess1.run(a))  # generates 'A2'
  print(sess1.run(b))  # generates 'B1'
  print(sess1.run(b))  # generates 'B2'

print(""Session 2"")
with tf.Session() as sess2:
  print(sess2.run(a))  # generates 'A1'
  print(sess2.run(a))  # generates 'A2'
  print(sess2.run(b))  # generates 'B1'
  print(sess2.run(b))  # generates 'B2'
</code></pre>

<p>Output:</p>

<pre><code>Session 1
[0.53202796]
[0.91749656]
[-1.3118125]
[-0.44506428]
Session 2
[0.53202796]
[0.91749656]
[-1.3118125]
[-0.44506428]
</code></pre>

<p>You can also set op-level seed like <code>a = tf.random.uniform([1], seed=1)</code>. Read more about it from official documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/random/set_random_seed"" rel=""nofollow noreferrer"">here</a>.</p>
",11867096,31,https://stackoverflow.com/questions/57301699,Documentation Replicability
59422616,Get ValueError when calling tf.image.non_max_suppression,"<p>I want to use tensorflow tf.image.non_max_suppression function.</p>

<p>I tried both snippets below:</p>

<pre><code>indices = tf.image.non_max_suppression(
    boxes=anchors_fit, 
    scores=rpn_cls_prob,
    max_output_size=self.max_outputs_num, 
)
</code></pre>

<pre><code>indices,scores = tf.image.non_max_suppression_with_scores(
    boxes=anchors_fit, 
    scores=rpn_cls_prob,
    max_output_size=self.max_outputs_num, 
)
</code></pre>

<p>where <code>anchors_fit.shape</code> is <code>[36864,4]</code>
and <code>rpn_cls_prob.shape</code> is <code>[36864]</code></p>

<p>Both calls raise:</p>

<pre><code>ValueError: Shape must be rank 0 but is rank 1 for 'non_max_suppression_with_scores/NonMaxSuppressionV5' (op: 'NonMaxSuppressionV5') with input shapes: [36864,4], [36864], [1], [], [], [].
</code></pre>

<p>What should i do?</p>
",2019-12-20 09:15:49,"<p>What is the shape of <code>self.max_outputs_num</code>?</p>

<p>From <a href=""https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression?version=stable"" rel=""nofollow noreferrer"">the documentation</a>:</p>

<blockquote>
  <p><strong><code>max_output_size</code></strong>: A scalar integer Tensor representing the maximum number of boxes to be selected by non max suppression.</p>
</blockquote>

<p>Here, it seems to be an 1-D array, as the error message report it between brackets.
Moreover, it would match the error message, as a scalar has a rank 0 and a 1-D array as a rank 1.</p>

<p>Thus, you should probably convert your <code>self.max_outputs_num</code> from an array to a scalar.</p>
",11919966,5,https://stackoverflow.com/questions/59422616,Documentation Replication on Other Examples
69941348,does tf.data.Dataset .from_tensor_slices() preserve the order of examples?,"<p>If I have a set of tfrecords, using <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave"" rel=""nofollow noreferrer"">.from_tensor_slices()</a> here, will <code>dataset</code> created preserve the order of the data? For example, if I have 3 tfrecords (the first one contains 40 examples, the second one contains 30 examples, the third one contains 70 examples) called <code>1.tfrecord</code>, <code>2.tfrecord</code>, <code>3.tfrecord</code> respectively, then I construct <code>dataset = tf.data.Dataset.from_tensor_slices(['1.tfrecord', '2.tfrecord', '3.tfrecord'])</code>. During loading, will the order of these examples preserved?</p>
",2021-11-12 10:24:07,"<p>If I understood your question correctly, yes, the order of examples is preserved when using <code>tf.data.Dataset.from_tensor_slices</code> with <code>tfrecord</code>. Here is a simple example:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

with tf.io.TFRecordWriter(&quot;sample1.tfrecord&quot;) as w:
    w.write(b&quot;Record A&quot;)
    w.write(b&quot;Record B&quot;)

with tf.io.TFRecordWriter(&quot;sample2.tfrecord&quot;) as w:
    w.write(b&quot;Record C&quot;)
    w.write(b&quot;Record D&quot;)
    w.write(b&quot;Record E&quot;)
    w.write(b&quot;Record F&quot;)

with tf.io.TFRecordWriter(&quot;sample3.tfrecord&quot;) as w:
    w.write(b&quot;Record G&quot;)
    w.write(b&quot;Record H&quot;)
    w.write(b&quot;Record I&quot;)
    w.write(b&quot;Record J&quot;)
    w.write(b&quot;Record K&quot;)
    w.write(b&quot;Record L&quot;)

dataset = tf.data.Dataset.from_tensor_slices([&quot;sample1.tfrecord&quot;,
                                              &quot;sample2.tfrecord&quot;,
                                              &quot;sample3.tfrecord&quot;])
for record in dataset:
   for item in tf.data.TFRecordDataset(record):
     tf.print('Record:', record, 'Item --&gt;', item)
</code></pre>
<pre><code>Record: &quot;sample1.tfrecord&quot; Item --&gt; &quot;Record A&quot;
Record: &quot;sample1.tfrecord&quot; Item --&gt; &quot;Record B&quot;
Record: &quot;sample2.tfrecord&quot; Item --&gt; &quot;Record C&quot;
Record: &quot;sample2.tfrecord&quot; Item --&gt; &quot;Record D&quot;
Record: &quot;sample2.tfrecord&quot; Item --&gt; &quot;Record E&quot;
Record: &quot;sample2.tfrecord&quot; Item --&gt; &quot;Record F&quot;
Record: &quot;sample3.tfrecord&quot; Item --&gt; &quot;Record G&quot;
Record: &quot;sample3.tfrecord&quot; Item --&gt; &quot;Record H&quot;
Record: &quot;sample3.tfrecord&quot; Item --&gt; &quot;Record I&quot;
Record: &quot;sample3.tfrecord&quot; Item --&gt; &quot;Record J&quot;
Record: &quot;sample3.tfrecord&quot; Item --&gt; &quot;Record K&quot;
Record: &quot;sample3.tfrecord&quot; Item --&gt; &quot;Record L&quot;
</code></pre>
<p>Or:</p>
<pre class=""lang-py prettyprint-override""><code>dataset = tf.data.Dataset.from_tensor_slices([&quot;sample1.tfrecord&quot;,
                                              &quot;sample2.tfrecord&quot;,
                                              &quot;sample3.tfrecord&quot;])
for item in tf.data.TFRecordDataset(dataset):
  tf.print('Item --&gt;', item)
</code></pre>
<pre><code>Item --&gt; &quot;Record A&quot;
Item --&gt; &quot;Record B&quot;
Item --&gt; &quot;Record C&quot;
Item --&gt; &quot;Record D&quot;
Item --&gt; &quot;Record E&quot;
Item --&gt; &quot;Record F&quot;
Item --&gt; &quot;Record G&quot;
Item --&gt; &quot;Record H&quot;
Item --&gt; &quot;Record I&quot;
Item --&gt; &quot;Record J&quot;
Item --&gt; &quot;Record K&quot;
Item --&gt; &quot;Record L&quot;
</code></pre>
",11970084,985,https://stackoverflow.com/questions/69941348,Documentation Replicability
73198391,import import tensorflow_docs error on Colab,"<p>I've been using colab for deep learning for a over a month and all of a sudden <code>import tensorflow_docs as tfdocs</code> stopped working. Is anyone encountering the same issues???</p>
<p>I'm running tf.<strong>version</strong> 2.8.2</p>
<pre><code>&gt; !pip install git+https://github.com/tensorflow/docs
&gt; 
&gt; import tensorflow_docs as tfdocs   
&gt; import tensorflow_docs.modeling   
&gt; import tensorflow_docs.plots
</code></pre>
<p>Error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-10-8469b6568dc0&gt; in &lt;module&gt;()
      2 get_ipython().system('pip install git+https://github.com/tensorflow/docs')
      3 
----&gt; 4 import tensorflow_docs as tfdocs
      5 import tensorflow_docs.modeling
      6 import tensorflow_docs.plots

4 frames
/usr/local/lib/python3.7/dist-packages/tensorflow_docs/api_generator/reference_resolver.py in ReferenceResolver()
     86       py_module_names: Union[list[str], dict[str, str]],
     87       link_prefix: Optional[str] = None,
---&gt; 88       physical_path: Optional[dict[str, str]] = None,
     89   ):
     90     &quot;&quot;&quot;Initializes a Reference Resolver.

TypeError: 'type' object is not subscriptable
</code></pre>
<p><strong>UPDATE 01-Sep-2022</strong></p>
<p>I updated python in colab to 3.9.1 using @Kor suggestion and confirmed it. Then ran:</p>
<pre><code>&gt; !pip install git+https://github.com/tensorflow/docs
&gt; 
&gt; import tensorflow_docs as tfdocs 
</code></pre>
<p>The tensorflow doc completed installation but still threw an error on import:</p>
<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
    &lt;ipython-input-15-2bc3446903cd&gt; in &lt;module&gt;
    ----&gt; 1 import tensorflow_docs as tfdocs
          2 import tensorflow_docs.modeling
          3 import tensorflow_docs.plots
          4 
          5 print(&quot;Version: &quot;, tf.__version__)

    ModuleNotFoundError: No module named 'tensorflow_docs'
</code></pre>
",2022-08-01 18:59:51,"<p>You can try this:</p>
<pre><code>!pip install -q git+https://github.com/MJAHMADEE/docs
import tensorflow_docs as tfdocs   
import tensorflow_docs.modeling   
import tensorflow_docs.plots
</code></pre>
",11963507,102,https://stackoverflow.com/questions/73198391,Documentation Replicability
58728086,Passing `training=true` when using Tensorflow 2's Keras Functional API,"<p>When operating in graph mode in TF1, I believe I needed to wire up <code>training=True</code> and <code>training=False</code> via feeddicts when I was using the functional-style API. What is the proper way to do this in TF2?</p>

<p>I believe this is automatically handled when using <code>tf.keras.Sequential</code>. For example, I don't need to specify <code>training</code> in the following example from the <a href=""https://www.tensorflow.org/guide/migrate"" rel=""noreferrer"">docs</a>:</p>

<pre><code>model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu',
                           kernel_regularizer=tf.keras.regularizers.l2(0.02),
                           input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Model is the full model w/o custom layers
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_data, epochs=NUM_EPOCHS)
loss, acc = model.evaluate(test_data)
print(""Loss {:0.4f}, Accuracy {:0.4f}"".format(loss, acc))
</code></pre>

<p>Can I also assume that keras automagically handles this when training with the functional api? Here is the same model, rewritten using the function api:</p>

<pre><code>inputs = tf.keras.Input(shape=((28,28,1)), name=""input_image"")
hid = tf.keras.layers.Conv2D(32, 3, activation='relu',
                           kernel_regularizer=tf.keras.regularizers.l2(0.02),
                           input_shape=(28, 28, 1))(inputs)
hid = tf.keras.layers.MaxPooling2D()(hid)
hid = tf.keras.layers.Flatten()(hid)
hid = tf.keras.layers.Dropout(0.1)(hid)
hid = tf.keras.layers.Dense(64, activation='relu')(hid)
hid = tf.keras.layers.BatchNormalization()(hid)
outputs = tf.keras.layers.Dense(10, activation='softmax')(hid)
model_fn = tf.keras.Model(inputs=inputs, outputs=outputs)

# Model is the full model w/o custom layers
model_fn.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model_fn.fit(train_data, epochs=NUM_EPOCHS)
loss, acc = model_fn.evaluate(test_data)
print(""Loss {:0.4f}, Accuracy {:0.4f}"".format(loss, acc))
</code></pre>

<p>I'm unsure if <code>hid = tf.keras.layers.BatchNormalization()(hid)</code> needs to be <code>hid = tf.keras.layers.BatchNormalization()(hid, training)</code>?</p>

<p>A colab for these models can be found <a href=""https://gist.github.com/justincosentino/d36d66cddb1d606f72818c63d90d5147"" rel=""noreferrer"">here</a>.</p>
",2019-11-06 10:32:34,"<p>I realized that there is a bug in the <code>BatchNormalization</code> documentation [1] where the <code>{{TRAINABLE_ATTRIBUTE_NOTE}}</code> isn't actually replaced with the intended note [2]:</p>

<p><strong>About setting <code>layer.trainable = False</code> on a <code>BatchNormalization</code> layer:</strong>
  The meaning of setting <code>layer.trainable = False</code> is to freeze the layer,
  i.e. its internal state will not change during training:
  its trainable weights will not be updated
  during <code>fit()</code> or <code>train_on_batch()</code>, and its state updates will not be run.
  Usually, this does not necessarily mean that the layer is run in inference
  mode (which is normally controlled by the <code>training</code> argument that can
  be passed when calling a layer). ""Frozen state"" and ""inference mode""
  are two separate concepts.</p>

<p>However, in the case of the <code>BatchNormalization</code> layer, <strong>setting
  <code>trainable = False</code> on the layer means that the layer will be
  subsequently run in inference mode</strong> (meaning that it will use
  the moving mean and the moving variance to normalize the current batch,
  rather than using the mean and variance of the current batch).
  This behavior has been introduced in TensorFlow 2.0, in order
  to enable <code>layer.trainable = False</code> to produce the most commonly
  expected behavior in the convnet fine-tuning use case.
  Note that:</p>

<ul>
<li>This behavior only occurs as of TensorFlow 2.0. In 1.*,
  setting <code>layer.trainable = False</code> would freeze the layer but would
  not switch it to inference mode.</li>
<li>Setting <code>trainable</code> on an model containing other layers will
  recursively set the <code>trainable</code> value of all inner layers.</li>
<li>If the value of the <code>trainable</code>
  attribute is changed after calling <code>compile()</code> on a model,
  the new value doesn't take effect for this model
  until <code>compile()</code> is called again.</li>
</ul>

<p>[1] <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=stable"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=stable</a></p>

<p>[2] <a href=""https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/normalization_v2.py#L26-L65"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/normalization_v2.py#L26-L65</a></p>
",12250264,364,https://stackoverflow.com/questions/58728086,Documentation Ambiguity
58069572,Workaround for lack of broadcast in TFLite,"<p>I would like to run a TFLite model that requires me to produce a 3d output (the sample code is a minimum example generating the error). Is there a tensorflow equivalent to gather_nd that does not reduce the dimension by one?</p>

<p>I've tried looking through the documentation for related functions that I can think of and haven't found a good option.</p>

<pre><code>import tensorflow.compat.v1 as tf
import numpy as np

tf.disable_v2_behavior()
initial_input = tf.placeholder(dtype=tf.float32, shape=(None,5,1024))
cap_i = tf.gather_nd(initial_input, [[0,1]]) #[0,2],[0,3],[0,4],[0,5]
cap_i_broadcast = tf.broadcast_to(cap_i, [1,5,1024])
cap_iT = tf.transpose(cap_i_broadcast, perm=[0,2,1])

sess = tf.Session()
sess.run(tf.global_variables_initializer())
tf.io.write_graph(sess.graph_def, '', 'train.pbtxt')
converter = tf.lite.TFLiteConverter.from_session(sess, [initial_input], [cap_iT])
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open('converted_model.tflite', ""wb"").write(tflite_model)
sess.close()
</code></pre>

<p>Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: GATHER_ND, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BroadcastTo.</p>
",2019-09-23 20:11:46,"<p>The following code has a solution using strided slice with dimensionality reduction and then reshape to get back the correct dimension.</p>

<pre><code>import tensorflow.compat.v1 as tf
import numpy as np

tf.disable_v2_behavior()
initial_input = tf.placeholder(dtype=tf.float32, shape=(None,5,1024))
cap_i = tf.strided_slice(initial_input, [0,0,0], [0,5,1024], [1,1,1], 
shrink_axis_mask=1)
cap_i_reshaped =tf.reshape(cap_i,[1,5,1024])
cap_iT = tf.transpose(cap_i_reshaped, perm=[0,2,1])

sess = tf.Session()
sess.run(tf.global_variables_initializer())
tf.io.write_graph(sess.graph_def, '', 'train.pbtxt')
converter = tf.lite.TFLiteConverter.from_session(sess, [initial_input], 
[cap_iT])
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, 
tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open('converted_model.tflite', ""wb"").write(tflite_model)
sess.close()
</code></pre>

<p>Previously thought slice was supported in TFLite but only strided_slice is.</p>
",12092080,33,https://stackoverflow.com/questions/58069572,Documentation Replication on Other Examples
58096095,How does tf.audio.decode_wav get its contents?,"<p>I'm trying to pull some audio files into Tensorflow by using <code>tf.audio.decode_wav</code>.</p>

<p>I can see someone is looking into providing more info in the docs, but does anyone have any examples of how this should work?</p>

<pre><code>tf.audio.decode_wav(
 contents,
 desired_channels=-1,
 desired_samples=-1,
 name=None
)
</code></pre>

<p><strong>Args:</strong></p>

<ul>
<li>contents: A Tensor of type string. The WAV-encoded audio, usually from a file.</li>
<li>desired_channels: An optional int. Defaults to -1. Number of sample channels wanted.</li>
<li>desired_samples: An optional int. Defaults to -1. Length of audio requested.</li>
<li>name: A name for the operation (optional).</li>
</ul>

<p>I'm guessing the contents is a tensor which has already been pulled from a file rather than a path?</p>
",2019-09-25 10:15:21,"<p>You're right, <code>tf.audio.decode_wav()</code> requires a tensor. You can provide one with 
<code>tf.io.read_file()</code> which reads wav file into tensor of type string.</p>

<pre><code>raw_audio = tf.io.read_file(filename)
waveform = tf.audio.decode_wav(raw_audio)
</code></pre>
",12118244,53,https://stackoverflow.com/questions/58096095,Inadequate Examples
58986077,AttributeError: module 'tensorflow._api.v1.compat.v1.nn' has no attribute 'avg_pool2d',"<p>can some help me with this?</p>

<pre><code>  File ""C:\tensorflow1\models\research\slim\nets\mobilenet\mobilenet.py"", line 397, in &lt;module&gt;
    def global_pool(input_tensor, pool_op=tf.compat.v1.nn.avg_pool2d):
AttributeError: module 'tensorflow._api.v1.compat.v1.nn' has no attribute 'avg_pool2d'
</code></pre>

<p>here's the code </p>

<pre><code>def global_pool(input_tensor, pool_op=tf.nn.avg_pool2d):
  shape = input_tensor.get_shape().as_list()
  if shape[1] is None or shape[2] is None:
    kernel_size = tf.convert_to_tensor(
        [1, tf.shape(input_tensor)[1],
         tf.shape(input_tensor)[2], 1])
  else:
    kernel_size = [1, shape[1], shape[2], 1]
  output = pool_op(
      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID')
  # Recover output shape, for unknown shape.
  output.set_shape([None, 1, 1, None])
  return output
</code></pre>

<p>it should be tf.nn.avg_pool2d(...) like this is it? i did search for solution. but i dont really understand it. </p>
",2019-11-22 01:16:07,"<p>Have you tried changing it to <code>tf.nn.avg_pool</code>?
It seemed to work for me.</p>
",12212094,47,https://stackoverflow.com/questions/58986077,Documentation Replicability
63008991,"why implementing the ""call"" method when subclassing a tf.keras layer(or model) class makes the layer(model) object callable?","<p>When writing customized tf.keras layers, we have to implement the &quot;<code>call</code>&quot; method, since a object of a class can be called like a function with &quot;()&quot; only(?) if the object has a valid &quot;<code>__call__</code>&quot; method. while I didn't find something like</p>
<pre><code>class tf.keras.model():
def __call__(self, input):
    return self.call(input)
</code></pre>
<p>in the keras.model source, how could all this work?</p>
",2020-07-21 06:54:26,"<pre><code>from keras.models import Model
import inspect

inspect.getmro(Model)
# (keras.engine.training.Model, keras.engine.network.Network, keras.engine.layer._Layer)
</code></pre>
<p><code>inspect.getmro(CLS)</code> returns a tuple of class CLS's base classes, including CLS, in method resolution order.</p>
<p>The <code>__call__</code> method inside <code>Model</code> infact comes from <code>keras.engine.layer._Layer</code> class. You can refer the code <a href=""https://github.com/tensorflow/tensorflow/blob/4343d75c5153470198d970166497fb5b9c4961f5/tensorflow/python/keras/engine/base_layer.py#L103"" rel=""nofollow noreferrer"">here</a></p>
<p>On line <a href=""https://github.com/tensorflow/tensorflow/blob/4343d75c5153470198d970166497fb5b9c4961f5/tensorflow/python/keras/engine/base_layer.py#L1107"" rel=""nofollow noreferrer"">996</a>, inside <code>__call__</code> method <code>call_fn</code> is assigned as <code>call</code> &amp; is indeed called on line <a href=""https://github.com/tensorflow/tensorflow/blob/4343d75c5153470198d970166497fb5b9c4961f5/tensorflow/python/keras/engine/base_layer.py#L979"" rel=""nofollow noreferrer"">979</a>.</p>
<p>So, essentially, in a way I guess, the following holds true -</p>
<pre><code>def __call__(self, input):
    return self.call(input)
</code></pre>
<p>Let's discuss further!</p>
",12589108,23,https://stackoverflow.com/questions/63008991,Inadequate Examples
68839011,Python/Keras: LeakyRelu using tensorflow,"<p>I am having problems installing keras. The following are giving me too much trouble to get around (even when doing updates on the terminal):</p>
<pre><code>from keras.layers import Dense, Activation
from keras.models import Sequential
</code></pre>
<p>So instead of initialising a ANN with <code>ann = Sequential()</code>, I do <code>ann = tf.keras.models.Sequential()</code>. This by importing:</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
</code></pre>
<p>I would like to use LeakyReLU as an activation function. However, this one seems to be different to implement and the keras documentation is not helping me that much compared to how others tend to do.</p>
<p>I've seen that ann.add(LeakyReLU(alpha=0.05)) is needed. However, what about the other parameters like unit or input_dim? How can I implement this using my code?</p>
<pre><code># Initialising the ANN
ann = tf.keras.models.Sequential()

# Adding the input layer and the first hidden layer
ann.add(tf.keras.layers.Dense(units=32, activation='relu'))

# Adding the second hidden layer
ann.add(tf.keras.layers.Dense(units=32, activation='relu'))

# Adding the output layer
ann.add(tf.keras.layers.Dense(units=1))
</code></pre>
",2021-08-18 20:35:30,"<p>First of all you can import <code>Sequential</code>, <code>Dense</code> and <code>Activation</code> directly by using <code>from tensorflow.keras.models import Sequential</code> and <code>from tensorflow.keras.layers import Dense, Activation</code></p>
<p>You can implement <code>LeakyReLU</code> like this:</p>
<pre><code>from tensorflow import keras

model = keras.models.Sequential([
    keras.layers.Dense(10),
    keras.layers.LeakyReLU(alpha=0.05)
])
</code></pre>
<p>You can specify the <code>LeakuReLU</code> activation function after you declare the layer as given in <a href=""https://keras.io/api/layers/activations/#about-advanced-activation-layers"" rel=""nofollow noreferrer"">keras documentation</a>.</p>
",12463547,1059,https://stackoverflow.com/questions/68839011,Lack of Alternative Solutions/Documentation
67197448,How to extract multiple rows from tensor at the same time?,"<p>TL;DR:
TensorFlow tensor is of shape <code>(50, 50, 6)</code>, want these indices (:, :, (0, 2, 3)). How to extract them?</p>
<p>Here is an example array I am working with:</p>
<pre><code>import numpy as np

a = np.random.randint(0,10, (50, 50, 6))
</code></pre>
<p>I want to extract the the first, third, and fourth row; in other words I need all these entries <code>(:, :, (1, 3))</code>, which works for numpy arrays:</p>
<pre><code>out = a[:,:, [0, 2, 3]]
out.shape #(50, 50, 3)

</code></pre>
<p>Working with a tensor <code>t = tf.convert_to_tensor(a)</code> and then calling the index like</p>
<pre><code>t[:,:, [0, 2, 3]]
</code></pre>
<p>throws an error:</p>
<pre><code>TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got [0, 1, 3]
</code></pre>
<p>For numpy I have found the following relevant questions, but they naturally focus on numpy arrays:</p>
<p><a href=""https://stackoverflow.com/questions/46227095/how-to-slice-a-2d-array-non-consecutively-in-python?noredirect=1&amp;lq=1"">How to slice a 2D array non-consecutively in Python</a></p>
<p><a href=""https://stackoverflow.com/questions/24398708/slicing-a-numpy-array-along-a-dynamically-specified-axis"">Slicing a numpy array along a dynamically specified axis</a></p>
<p>Looking at the TF documentation I found <code>gather_nd</code> and <code>boolean_mask</code>, which I feel are helpful, but I must freely admit that I have not understood the docs at this part. On SO I found this question <a href=""https://stackoverflow.com/questions/58052967/how-to-select-elements-of-a-tensor-along-a-specific-axis-in-tensorflow"">How to select elements of a tensor along a specific axis in TensorFlow</a>, which focuses on single elements; I am looking for complete dimensions (if that's the right wording here).</p>
<p>How can I do the numpy thing in TensorFlow?</p>
",2021-04-21 14:02:33,"<p>If you want the 0,2 and 3rd element of the last axis in the tensor, you can use tf.gather as follows: tf.gather(t,indices=[0, 2, 3],axis=-1))</p>
",12859833,310,https://stackoverflow.com/questions/67197448,Documentation Replication on Other Examples
61219907,Failed to create control dependencies with tf.control_dependencies(),"<p>I tried to understand <code>tf.control_dependencies()</code>, and wanted to verify it does create control dependencies. Here is the code</p>

<pre><code>import tensorflow as tf

a = tf.get_variable('a', shape = [2, 3])
b = tf.get_variable('b', shape = [2, 3])
c = tf.scalar_mul(2, a)
d = tf.scalar_mul(3, b)

with tf.control_dependencies([d, c]):
  f = d-c

print (f.op.control_inputs)
</code></pre>

<p>It returned <code>[]</code>, which was not what I expected. If I added the control dependencies in the following way</p>

<pre><code>f = d-c
f.op._add_control_inputs([c.op, d.op])
print (f.op.control_inputs)
</code></pre>

<p>It gave back what I expected <code>[&lt;tf.Operation 'Mul' type=Mul&gt;, &lt;tf.Operation 'Mul_1' type=Mul&gt;]</code>. </p>

<p>So my question is, does <code>tf.control_dependencies()</code> really add control dependencies? Or does <code>f.op.control_inputs</code> return all the control inputs?</p>
",2020-04-15 01:34:28,"<p>It's solved <a href=""https://github.com/tensorflow/tensorflow/issues/38556"" rel=""nofollow noreferrer"">here</a>. Just changing <code>tf.get_variable()</code> to <code>tf.Varibble()</code>.</p>
",13160838,1,https://stackoverflow.com/questions/61219907,Documentation Replication on Other Examples
75851842,tensorflow map function to mulitple tensors,"<p>I am using the following function in a custom layer in TensorFlow to rearrange query, key values:</p>
<pre><code>q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h = self.heads), (q, k, v)) 
</code></pre>
<p>and it throws this warning:</p>
<p><code>WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23. Instructions for updating: Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089</code></p>
<p>Is there a more TensorFlowic way of doing this?</p>
<p>I tried using map_fn as follows and it throws the the same warning and an error:</p>
<pre><code>import tensorflow as tf
from einops import rearrange
a = tf.random.uniform((1, 196, 196))
b, c, d = tf.map_fn(lambda t: rearrange(t, 'b (h n) d -&gt; b h n d', h=14), [a, a, a])
</code></pre>
<p>From documentation, it seems <code>tf.map_fn</code> but it seems to work on a stack of tensors. Will it be better to stack the tensors?</p>
",2023-03-27 03:11:32,"<p>Couple of mistakes-
First you need to use the <code>einops tensorflow layer</code> for <code>rearrange</code>. The below changes should work:</p>
<pre><code>from einops.layers.tensorflow import Rearrange

a = tf.random.uniform(shape=(1, 196, 196))
q, k, v = tf.map_fn(fn= Rearrange('b (h n) d -&gt; b h n d', h=14), elems=tf.stack([a, a, a]))
print(q.shape, k.shape, v.shape)
# (1, 14, 14, 196) (1, 14, 14, 196) (1, 14, 14, 196)
</code></pre>
",13262692,309,https://stackoverflow.com/questions/75851842,Documentation Replication on Other Examples
71791115,Nan Loss when training Deep neural Recommender model using tensorflow,"<p>I am trying to follow <a href=""https://www.tensorflow.org/recommenders/examples/deep_recommenders"" rel=""nofollow noreferrer"">tensorflow documentation</a> and applying same technique to one of toy dataset.</p>
<p>During training I am getting all loss as Nan. I have tried to debug the same using Debugger V2 and I could see that <code>tf.keras.layers.GlobalAveragePooling1D</code> is giving Nan due to division by 0, which is causing all values to be Nan during backpropagation. But what is not clear from the debugger V2 GUI why the sum is becoming 0. I did try to reduce the number of features and the size of the dataset, but each of this activity is giving me new error (probably I shall start a separate question thread for each issues at a later point ).</p>
<p>Below is the code for reference. I am providing the dataset as well <a href=""https://drive.google.com/file/d/1z954Djz8IntSzMP6velSdMGWTW_yBUAn/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>. I had tried below code on Google Colab.</p>
<pre><code>import os
import pprint
import tempfile

from typing import Dict, Text

import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_datasets as tfds

tf.debugging.experimental.enable_dump_debug_info(
    &quot;./tfdbg2_logdir&quot;,
    tensor_debug_mode=&quot;FULL_HEALTH&quot;,
    circular_buffer_size=-1)

!pip install -q tensorflow-recommenders
import tensorflow_recommenders as tfrs  
</code></pre>
<p>Preparing Data</p>
<pre><code>ds=pd.read_csv('train_recom.csv')
ds['year'].replace(0,1,inplace=True)
ds_song=ds.groupby(['song_id','title','release','artist_name','year']).size().reset_index().rename(columns={0:'count'})
ds_song.to_csv('songs_details.csv')
ds.to_csv('train_recom_transformed.csv')
</code></pre>
<p>Reading data to tensorflow dataset</p>
<pre><code>ratings = tf.data.experimental.make_csv_dataset(
    &quot;./train_recom_transformed.csv&quot;,
    batch_size=5,
    select_columns=['user_id', 'song_id', 'listen_count', 'title', 'release', 'artist_name',
       'year'],
    header=True,
    num_epochs=1,
    ignore_errors=False,)
songs = tf.data.experimental.make_csv_dataset(
    &quot;./songs_details.csv&quot;,
    batch_size=128,
    select_columns=['song_id','title','release','artist_name','year'],
    num_epochs=1,
    ignore_errors=True,)
ratings = ratings.unbatch().map(lambda x: {
    &quot;song_id&quot;: x[&quot;song_id&quot;],
    &quot;user_id&quot;: x[&quot;user_id&quot;],
    &quot;release&quot; : x[&quot;release&quot;],
    &quot;artist_name&quot; : x[&quot;artist_name&quot;],
    &quot;title&quot; : x[&quot;title&quot;],
    &quot;year&quot; : x[&quot;year&quot;],
    &quot;listen_count&quot;: x[&quot;listen_count&quot;]
})
songs = songs.unbatch().map(lambda x: x[&quot;song_id&quot;]) 
</code></pre>
<p>Preparing train and test dataset</p>
<pre><code>tf.random.set_seed(42)
shuffled = ratings.shuffle(16000, seed=42, reshuffle_each_iteration=False)

train = shuffled.take(12000)
test = shuffled.skip(12000).take(4000)
cached_train = train.shuffle(100_000).batch(1200).cache()
cached_test = test.batch(400).cache()

title = songs.batch(1000)
user_ids = ratings.batch(1_000_000).map(lambda x: x[&quot;user_id&quot;])
unique_song_titles = np.unique(np.concatenate(list(title)))
unique_user_ids = np.unique(np.concatenate(list(user_ids)))
year_data=np.concatenate(list(ratings.map(lambda x: x['year']).batch(4000)))
</code></pre>
<p>User model class</p>
<pre><code>class UserModel(tf.keras.Model):

    def __init__(self):
        super().__init__()

        max_tokens = 1_000_000

        embedding_dimension = 32
        self.user_embedding = tf.keras.Sequential([
            tf.keras.layers.StringLookup(
                vocabulary=unique_user_ids, mask_token=None),
            tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)
          ])

        self.release_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            max_tokens=max_tokens)
        
        self.release_text_embedding = tf.keras.Sequential([
          self.release_vectorizer,
          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True,input_length=144),
          tf.keras.layers.GlobalAveragePooling1D(),
        ])

        self.release_vectorizer.adapt(np.concatenate(list(ratings.map(lambda x: x['release']).batch(4000))))

        self.artist_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            max_tokens=max_tokens)
        self.artist_text_embedding = tf.keras.Sequential([
          self.artist_vectorizer,
          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),
          tf.keras.layers.GlobalAveragePooling1D(),
        ])
        self.artist_vectorizer.adapt(np.concatenate(list(ratings.map(lambda x: x['artist_name']).batch(4000))))
        
        self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            max_tokens=max_tokens)
        self.title_text_embedding = tf.keras.Sequential([
          self.title_vectorizer,
          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),
          tf.keras.layers.GlobalAveragePooling1D(),
        ])
        self.title_vectorizer.adapt(np.concatenate(list(ratings.map(lambda x: x['title']).batch(4000))))
        
        self.year_embedding = tf.keras.Sequential([
              tf.keras.layers.Embedding(len(year_data) + 1, 32),
            ])

    def call(self, inputs):
      return tf.concat([
          self.user_embedding(inputs['user_id']),
          self.release_text_embedding(inputs['release'])
          ,
          self.year_embedding(inputs['year']), 
          self.artist_text_embedding(inputs['artist_name']),
          self.title_text_embedding(inputs['title']),
             ], axis=1)
</code></pre>
<p>Item model</p>
<pre><code>class ItemModel(tf.keras.Model):

    def __init__(self):
        super().__init__()

        max_tokens = 10_000

        embedding_dimension = 32

        ## embed title from unique_song_titles
        self.title_embedding = tf.keras.Sequential([
        tf.keras.layers.StringLookup(
            vocabulary=unique_song_titles, mask_token=None),
        tf.keras.layers.Embedding(len(unique_song_titles) + 1, embedding_dimension)
      ])

    def call(self, inputs):
      return self.title_embedding(inputs)
</code></pre>
<p>Query model . Creating Deep model</p>
<pre><code>class QueryModel(tf.keras.Model):
  &quot;&quot;&quot;Model for encoding user queries.&quot;&quot;&quot;

  def __init__(self, layer_sizes):
    &quot;&quot;&quot;Model for encoding user queries.

    Args:
      layer_sizes:
        A list of integers where the i-th entry represents the number of units
        the i-th layer contains.
    &quot;&quot;&quot;
    super().__init__()

    # We first use the user model for generating embeddings.
    self.embedding_model = UserModel()

    # Then construct the layers.
    self.dense_layers = tf.keras.Sequential()

    # Use the ReLU activation for all but the last layer.
    for layer_size in layer_sizes[:-1]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=&quot;relu&quot;))

    # No activation for the last layer.
    for layer_size in layer_sizes[-1:]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size))

  def call(self, inputs):
    feature_embedding = self.embedding_model(inputs)
    return self.dense_layers(feature_embedding)
</code></pre>
<p>Creating deep model for the Item model</p>
<pre><code>class CandidateModel(tf.keras.Model):
  &quot;&quot;&quot;Model for encoding movies.&quot;&quot;&quot;

  def __init__(self, layer_sizes):
    &quot;&quot;&quot;Model for encoding movies.

    Args:
      layer_sizes:
        A list of integers where the i-th entry represents the number of units
        the i-th layer contains.
    &quot;&quot;&quot;
    super().__init__()

    self.embedding_model = ItemModel()

    # Then construct the layers.
    self.dense_layers = tf.keras.Sequential()

    # Use the ReLU activation for all but the last layer.
    for layer_size in layer_sizes[:-1]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=&quot;relu&quot;))

    # No activation for the last layer.
    for layer_size in layer_sizes[-1:]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size))

  def call(self, inputs):
    feature_embedding = self.embedding_model(inputs)
    return self.dense_layers(feature_embedding)
</code></pre>
<p>Combining both query and candidate model</p>
<pre><code>class SongModel(tfrs.models.Model):

    def __init__(self, layer_sizes):
        super().__init__()
        self.query_model = QueryModel(layer_sizes)
        self.candidate_model = CandidateModel(layer_sizes)
        self.task = tfrs.tasks.Retrieval(
          metrics=tfrs.metrics.FactorizedTopK(
              candidates=songs.batch(128).map(self.candidate_model),
          ),
      )

    def compute_loss(self, features, training=False):
        print('type of feature ----',type(features))

        query_embeddings = self.query_model({
            &quot;user_id&quot;: features[&quot;user_id&quot;]
            ,
                &quot;release&quot; : features[&quot;release&quot;]
                ,
                &quot;artist_name&quot; : features[&quot;artist_name&quot;],
                &quot;title&quot;: features[&quot;title&quot;],
                &quot;year&quot; : features[&quot;year&quot;],
        })

        item_embeddings = self.candidate_model(features[&quot;song_id&quot;])

        return self.task(query_embeddings, item_embeddings)
</code></pre>
<p>training the model</p>
<pre><code>model = SongModel([32])
model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))
model_hist = model.fit(cached_train, epochs=9)
</code></pre>
<p>Below id the outout that I got</p>
<pre><code>WARNING:tensorflow:Failed to read source code from path: /content/&lt;ipython-input-26-fdc864fc30cf&gt;. Reason: Source path neither exists nor can be loaded as a .par file: /content/&lt;ipython-input-26-fdc864fc30cf&gt;
WARNING:tensorflow:Failed to read source code from path: /content/&lt;ipython-input-25-e3009db55439&gt;. Reason: Source path neither exists nor can be loaded as a .par file: /content/&lt;ipython-input-25-e3009db55439&gt;
Epoch 1/9
type of feature ---- &lt;class 'dict'&gt;
WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_10_input'), name='embedding_10_input', description=&quot;created by layer 'embedding_10_input'&quot;), but it was called on an input with incompatible shape (None,).
type of feature ---- &lt;class 'dict'&gt;
WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_10_input'), name='embedding_10_input', description=&quot;created by layer 'embedding_10_input'&quot;), but it was called on an input with incompatible shape (None,).
10/10 [==============================] - 63s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0022 - factorized_top_k/top_10_categorical_accuracy: 0.0033 - factorized_top_k/top_50_categorical_accuracy: 0.0073 - factorized_top_k/top_100_categorical_accuracy: 0.0103 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 2/9
10/10 [==============================] - 9s 945ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 3/9
10/10 [==============================] - 10s 953ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 4/9
10/10 [==============================] - 9s 948ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 5/9
10/10 [==============================] - 10s 966ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 6/9
10/10 [==============================] - 10s 955ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 7/9
10/10 [==============================] - 10s 955ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 8/9
10/10 [==============================] - 10s 958ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 9/9
10/10 [==============================] - 10s 971ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
</code></pre>
",2022-04-08 02:46:02,"<p>I got a similar error when using tfrs on a custom dataset. And it turns out that I had some none print characters and sysmbols in the data. I simply searched and removed the symbols (manually, some regex) and i also limit the text columns in the dataframe to printable characters only.</p>
<pre><code>from string import printable as pt

allowed_set = set(pt)
df[col] = df[col].apply(lambda x:  ''.join([' ' if  s not in  allowed_set else s for s in x]))
</code></pre>
<p>I hope it helps.</p>
",12271381,939,https://stackoverflow.com/questions/71791115,Documentation Replication on Other Examples
61767803,Tensorflow 1.x to Tensorflow 2.1.0,"<p>I am trying to update code written in Tensorflow 1.x to code in Tensorflow 2.1.0. I have been converting codes using Tensorflow 2.1.0 documentation, and I had no problems until this code.</p>

<pre><code>loss = tf.losses.softmax_cross_entropy(one_hot_labels, logits)
</code></pre>

<p>Above code is Tensorflow 1.x version, and I think, according to Tensorflow 2.1.0 documentation, the properly updated code is </p>

<pre><code>loss = tf.nn.softmax_cross_entropy_with_logits(one_hot_labels, logits)
</code></pre>

<p>Then, when I run</p>

<pre><code>return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)
</code></pre>

<p>I get the following error.</p>

<pre><code>Loss must be scalar, given: Tensor(""softmax_cross_entropy_with_logits/Reshape_2:0"", shape=(512,), dtype=float32)**
</code></pre>

<p>So, I am guessing in Tensorflow 1.x version, the loss was passed as 'tensor' to tf.estimator.EstimatorSpec, but in Tensorflow 2.1.0, the loss has to be passed as <code>scalar</code> to <code>tf.estimator.EstimatorSpec</code>? Loss (the way it is defined here) in both Tensorflow 1.x and 2.1.0 is tensor if I remember it correctly.</p>

<p>So, does anyone know how to convert tensor to scalar (which I don't think will be sufficient nor efficient in building the CNN model) or better yet, how to solve this dilemma?</p>

<p>Or did I convert the original code the wrong way?</p>

<p>I would very much appreciate if compat.v1. is not used unless absolutely necessary (i.e. no other way to use the code in Tensorflow 2.1.0 than compat.v1.)</p>
",2020-05-13 06:30:32,"<p>You can just average the result (which is what <code>tf.losses.softmax_cross_entropy</code> did anyway through <code>tf.losses.compute_weighted_loss</code>):</p>

<pre class=""lang-py prettyprint-override""><code>loss = tf.math.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(one_hot_labels, logits))
</code></pre>
",12997689,81,https://stackoverflow.com/questions/61767803,Documentation Replication on Other Examples
61790621,List of tensors and just tensors,"<p>I am updating codes from tensorflow 1.x to 2.1.0.</p>

<p>I changed tensorflow 1.x code</p>

<pre><code>labels = tf.cast(labels, tf.int64)
predict = tf.argmax(input=logits, axis=1)
tf.metrics.accuracy(labels=labels, predictions=predict)
</code></pre>

<p>to tensorflow 2.1.0 code.</p>

<pre><code>labels = tf.cast(labels, tf.int64)
predict = tf.argmax(input=logits, axis=1)
tf.keras.metrics.Accuracy.update_state(labels, predict) #updated code
</code></pre>

<p>But, when I run the updated code, I got the following error.</p>

<pre><code>TypeError: update_state() missing 1 required positional argument: 'y_pred'
</code></pre>

<p>So, I checked the tensorflow 2.1.0 document, and parameters for <code>tf.keras.metrics.Accuracy.update_state()</code> seem to be a list (in form of [ , , , ]). Then, I searched for a way to convert tensor to a list, which is</p>

<pre><code>labels = tf.make_tensor_proto(labels)
labels = tf.make_ndarray(labels)
</code></pre>

<p>After I run this code, it gives the following error.</p>

<pre><code>TypeError: List of Tensors when single Tensor expected
</code></pre>

<p>So, I tried to turn a list of Tensors into Tensors with</p>

<pre><code>labels = tf.stack(labels)
#or
labels = torch.stack(labels)
</code></pre>

<p><code>tf.stack()</code> did not work, as it gave the same initial TypeError saying 'y_pred' is missing at the updated code.</p>

<p><code>torch.stack()</code>, however, gave the following error.</p>

<pre><code>TypeError: stack() : argument 'tensors' (position 1) must be tuple of Tensors, not Tensor
</code></pre>

<p>So, I am guessing <code>torch.stack()</code> only accepts a tuple, <strong>NOT a list</strong>.
But, <code>tf.stack()</code> seems to accept a list, but it does not turn it into a Tensor?</p>

<p>Are my labels and predict even a list of Tensors in the first place? If so, why would tf.stack() not turn them into Tensors? How can I correctly convert labels and predict so that they can be passed into <code>tf.keras.metrics.Accuracy.update_state()</code>?</p>

<p>I would very much appreciate if not using <code>compat.v1.</code> unless absolutely necessary.</p>
",2020-05-14 06:22:20,"<p>try in this way:</p>

<pre><code>labels = [0,1]
logits = np.asarray([[0.9,0.1],[0.1,0.9]])

labels = tf.cast(labels, tf.int64)
predict = tf.argmax(input=logits, axis=1)
acc = tf.keras.metrics.Accuracy()
acc = acc.update_state(y_true=labels, y_pred=predict)
acc
</code></pre>
",12997689,81,https://stackoverflow.com/questions/61790621,Documentation Replicability
64652064,Finding intersection of two tensors,"<p>I have used that the dice coefficient is calculated by 2xintersection/union in semantic segmentation neural networks. And the intersection of y_true and y_pred is found by tf.math.reduce_sum(y_pred*y_true). Please someone can help me figure out, how the multiplication of two tensors are equal to the intersection?</p>
",2020-11-02 19:28:03,"<p>It's a special case of multiplication that becomes intersection.</p>
<p>Consider,</p>
<pre><code>y_true = [
[0,1,0],
[1,1,0],
[1,0,1]
]
</code></pre>
<p>and</p>
<pre><code>y_pred = [
[0,1,1],
[1,0,0],
[1,0,1]
]
</code></pre>
<p>then <code>y_true * y_pred</code> will be,</p>
<pre><code>res = [
[0,1,0],
[1,0,0],
[1,0,1]
</code></pre>
<p>Next <code>tf.reduce_sum()</code> gives the sum of all the ones in <code>res</code> (which is intersection). In other words <code>res</code> will have an element set to 1, only if both <code>y_true</code> and <code>y_pred</code> has 1 for that position.</p>
<pre><code>intersection = 4
</code></pre>
",14042046,15,https://stackoverflow.com/questions/64652064,Documentation Replication on Other Examples
72928149,Difference between Experimental Preprocessing layers and normal preprocessing layers in Tensorflow,"<pre><code>import tensorflow as tf
import keras
import tensorflow.keras.layers as tfl
from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation
</code></pre>
<p>I am trying to figure out which I should use for Data Augmentation. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers"" rel=""nofollow noreferrer"">documentation</a>, there is:</p>
<p>tf.keras.layers.RandomFlip and RandomRotation</p>
<p>Then we have in <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing"" rel=""nofollow noreferrer"">tf.keras.layers.experimental.preprocessing</a> the same things, randomFlip and RandomRotation.</p>
<p>Which should I use? I've seen <a href=""https://www.tensorflow.org/guide/keras/preprocessing_layers"" rel=""nofollow noreferrer"">guides</a> that use both.</p>
<p>This is my current code:</p>
<pre><code>def data_augmenter():
data_augmentation = tf.keras.Sequential([
    tfl.RandomFlip(),
    tfl.RandomRotation(0.2)
])
return data_augmentation
</code></pre>
<p>and this is a part of my model:</p>
<pre><code>def ResNet50(image_shape = IMG_SIZE, data_augmentation=data_augmenter()):

input_shape = image_shape + (3,)

# Remove top layer in order to put mine with the correct classification labels, get weights for imageNet
base_model = tf.keras.applications.resnet_v2.ResNet50V2(input_shape=input_shape, include_top=False, weights='imagenet')

# Freeze base model
base_model.trainable = False

# Define input layer
inputs = tf.keras.Input(shape=input_shape)

# Apply Data Augmentation
x = data_augmentation(inputs)
</code></pre>
<p>I am a bit confused here..</p>
",2022-07-10 11:28:50,"<p>If you find something in an <code>experimental</code> module and something in the same package by the same name, these will typically be aliases of one another. For the sake of backwards compatibility, they don't remove the experimental one (at least not for a few iterations.)</p>
<p>You should generally use the non-experimental one if it exists, since this is considered stable and should not be removed or changed later.</p>
<p>The following page shows Keras preprocessing exerimental. If it redirects to the preprocessing module, it's an alias. <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing</a></p>
",14072615,85,https://stackoverflow.com/questions/72928149,Documentation Replication on Other Examples
67561368,Difference between tf.nn.leaky_relu and tf.keras.layers.LeakyReLU?,"<p>I understand that <code>tf.nn.leaky_relu</code> is used to compute the function while <code>tf.keras.layers.LeakyReLU</code> is the layer version. But how would they differ while adding to a Functional API neural network model?</p>
",2021-05-16 20:36:51,"<p>If you go <a href=""https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/backend.py#L4718"" rel=""nofollow noreferrer"">deep enough</a>, you might find that <code>tf.keras.layers.LeakyReLU</code> is actually calling <code>tf.nn.leaky_relu</code> under the hood.</p>
<p>Therefore, I don't think there should be any difference in using the latter or former.</p>
<p>Although, from my experience I would suggest to stick with Keras Layers as much as possible, if you are using Keras API, to avoid any serialization issues later on.</p>
",13316200,3,https://stackoverflow.com/questions/67561368,Documentation Ambiguity
73165980,Tensorflow: how to feed a variable-time-step input to a RNN,"<p>I have a simple X_train and Y_train data:</p>
<pre><code>x_train = [
  array([ 6,  1,  9, 10,  7,  7,  1,  9, 10,  3, 10,  1,  4]), 
  array([ 2,  8,  8,  1,  1,  4,  2,  5,  1,  2,  7,  2,  1,  1, 4,  5, 10, 4])
]
y_train = [23, 17]
</code></pre>
<p>Arrays are numpy arrays.
I am now trying to use the <code>tf.data.Dataset</code> class to load these as tensors.
Before I have done a similar thing successfully using the following code:</p>
<pre><code>    dataset = data.Dataset.from_tensor_slices((x_train, y_train))
</code></pre>
<p>As this input is fed into a RNN, I have used the expand_dims method in the first RNN layer (the expand_dimension is passed as a function to overcome an apparent bug in tensorflow: see <a href=""https://github.com/keras-team/keras/issues/5298#issuecomment-281914537"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/issues/5298#issuecomment-281914537</a>):</p>
<pre><code>def expand_dimension(x):
    from tensorflow import expand_dims
    return expand_dims(x, axis=-1)

model = models.Sequential(
    [
        layers.Lambda(expand_dimension,
                      input_shape=[None]),
        layers.LSTM(units=64, activation='tanh'),
        layers.Dense(units=1)
    ]
)
</code></pre>
<p>This worked although because I had arrays of equal length. In the example I posted instead the 1st array has 13 numbers and the 2nd one 18.
In this case the method above doesn't work, and the recommended method seems to be using <code>tf.data.Dataset.from_generator</code>.
Reading this <a href=""https://stackoverflow.com/questions/50329855/how-to-use-the-tensorflow-dataset-pipeline-for-variable-length-inputs"">How to use the Tensorflow Dataset Pipeline for Variable Length Inputs?</a>, the accepted solution shows something like the following would work (where I am not caring here about <code>y_train</code> for simplicity):</p>
<pre><code>dataset = tf.data.Dataset.from_generator(lambda: x_train, 
                                         tf.as_dtype(x_train[0].dtype),
                                         tf.TensorShape([None, ]))
</code></pre>
<p>However, the syntax in tensorflow has changed since this answer, and now it requires to use the <code>output_signature</code> argument (see <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator</a>).</p>
<p>I've tried different ways but I'm finding hard to understand from tensorflow documentation what the <code>output_signature</code> should exactly be in my case.
Any help would be much appreciated.</p>
",2022-07-29 11:26:49,"<p>Short answer is, you can define <code>output_signature</code> as follows.</p>
<pre><code>import tensorflow as tf
import numpy as np
x_train = [
  np.array([ 6,  1,  9, 10,  7,  7,  1,  9, 10,  3, 10,  1,  4]), 
  np.array([ 2,  8,  8,  1,  1,  4,  2,  5,  1,  2,  7,  2,  1,  1, 4,  5, 10, 4])
]
y_train = [23, 17]

dataset = tf.data.Dataset.from_generator(
    lambda: x_train, 
    output_signature=tf.TensorSpec(
        [None, ], 
        dtype=tf.as_dtype(x_train[0].dtype)
    )
)
</code></pre>
<p>I'll also expand and improve on some things you're doing here to improve your pipeline.</p>
<h2>Using both inputs and labels</h2>
<pre><code>dataset = tf.data.Dataset.from_generator(
    lambda: zip(x_train, y_train), 
    output_signature=(
        tf.TensorSpec([None, ], dtype=tf.as_dtype(x_train[0].dtype)),
        tf.TensorSpec([], dtype=tf.as_dtype(y_train.dtype))
    )
)

for x in dataset:
  print(x)
</code></pre>
<p>Which would output,</p>
<pre><code>(&lt;tf.Tensor: shape=(13,), dtype=int64, numpy=array([ 6,  1,  9, 10,  7,  7,  1,  9, 10,  3, 10,  1,  4])&gt;, &lt;tf.Tensor: shape=(), dtype=int64, numpy=23&gt;)
(&lt;tf.Tensor: shape=(18,), dtype=int64, numpy=
array([ 2,  8,  8,  1,  1,  4,  2,  5,  1,  2,  7,  2,  1,  1,  4,  5, 10,
        4])&gt;, &lt;tf.Tensor: shape=(), dtype=int64, numpy=17&gt;)
</code></pre>
<p><strong>Caveat</strong>: This can get slightly more complicated if you try to <code>tf.data.Dataset.batch()</code> items. Then you need to use <code>RaggedTensorSpec</code> instead of <code>TensorSpec</code>. Also, I haven't experimented too much with feeding in ragged tensors into a RNN. But I think those are out of scope for the question you've asked.</p>
",13454852,151,https://stackoverflow.com/questions/73165980,Documentation Ambiguity
63210672,What Does tf.estimator.LinearClassifier() Do?,"<p>In TensorFlow library, what does the <code>tf.estimator.LinearClassifier</code> class do in linear regression models? (In other words, what is it used for?)</p>
",2020-08-01 22:32:43,"<p>Linear Classifier is nothing but Logistic Regression.</p>
<p>According to Tensorflow documentation, <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier"" rel=""nofollow noreferrer"">tf.estimator.LinearClassifier</a> is used to</p>
<blockquote>
<p>Train a linear model to classify instances into one of multiple
possible classes. When number of possible classes is 2, this is binary
classification</p>
</blockquote>
<p>Linear regression predicts a value while the linear classifier predicts a class. Classification aims at predicting the probability of each class given a set of inputs.</p>
<p>For implementation of <code>tf.estimator.LinearClassifier</code>, please follow <a href=""https://www.guru99.com/linear-classifier-tensorflow.html#7"" rel=""nofollow noreferrer"">this</a> tutorial by guru99.</p>
<p>To know about the linear classifiers, read <a href=""https://cs231n.github.io/linear-classify/"" rel=""nofollow noreferrer"">this</a> article.</p>
",13744618,29,https://stackoverflow.com/questions/63210672,Documentation Replicability
71149271,"How to remove single feature from tensorflow dataset, how to use apply on single feture?","<p>I created dataset from csv file with dataset = tf.data.experimental.make_csv_dataset() function but My dataset has categorical and numeric features.</p>
<pre><code>dataset=
color  price weight
red    120    1.2
blue    80     2.0
green   90     3
</code></pre>
<p>Question 1:
The question is how can I  modify  only single feature, for example weight +2, to:</p>
<pre><code>dataset=
color  price weight
red    120    3.2
blue    80     4.0
green   90     5
</code></pre>
<p>I try to do something like:</p>
<pre><code>dataset = dataset.apply(lambda x: x['weight']+2)
</code></pre>
<p>but the error is: &quot;TypeError: 'FilterDataset' object is not subscriptable&quot;</p>
<p>Example from the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply</a> doesn't show it.</p>
<p>Question 2:
How can I remove single feature ? Is there any equivalent to pandas drop column?</p>
",2022-02-16 21:06:40,"<p>You can remove features by only filtering the features that you want. This how you can modify only one feature:</p>
<pre><code>import tensorflow as tf
import pandas as pd

df = pd.DataFrame(data={'color': ['red', 'blue','green'], 'price': [120, 80, 90], 'weight': [3.2, 4.0, 5]})
df.to_csv('data.csv', index=False)

dataset = tf.data.experimental.make_csv_dataset('/content/data.csv', batch_size=1, num_epochs = 1, shuffle=False)
dataset = dataset.map(lambda x: (x['color'], x['price'], x['weight']+2))

for x in dataset:
  print(x[0], x[1], x[2])
</code></pre>
<pre><code>tf.Tensor([b'red'], shape=(1,), dtype=string) tf.Tensor([120], shape=(1,), dtype=int32) tf.Tensor([5.2], shape=(1,), dtype=float32)
tf.Tensor([b'blue'], shape=(1,), dtype=string) tf.Tensor([80], shape=(1,), dtype=int32) tf.Tensor([6.], shape=(1,), dtype=float32)
tf.Tensor([b'green'], shape=(1,), dtype=string) tf.Tensor([90], shape=(1,), dtype=int32) tf.Tensor([7.], shape=(1,), dtype=float32)
</code></pre>
",13824257,142,https://stackoverflow.com/questions/71149271,Documentation Replication on Other Examples
61411259,Which one should I ues for preprocessing image?,"<p>which one should I use for preprocessing  image. using ImageDataGenerator from keras or using tf.data from tensorflow?
Which one has better performance?</p>
",2020-04-24 14:55:48,"<p>As per my experience tf.data should be used because there is a lot of scope in optimization by tuning the number of parallel calls when doing the preprocessing using the <code>map</code> on the dataset.</p>

<p>For example:  <a href=""https://www.tensorflow.org/guide/data#decoding_image_data_and_resizing_it"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/data#decoding_image_data_and_resizing_it</a></p>

<p>Parallelizing data transformation: <a href=""https://www.tensorflow.org/guide/data_performance#parallelizing_data_transformation"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/data_performance#parallelizing_data_transformation</a></p>
",13399061,13,https://stackoverflow.com/questions/61411259,Documentation Replicability
66772069,Eager execution disabled in Keras model on `predict_step`,"<p>Why does tensorflow disable eager execution inside the <code>predict_step</code> function of a <code>tf.keras.Model</code>? Maybe I am getting wrong something but here's an example:</p>
<pre><code>from __future__ import annotations
from functools import wraps
import tensorflow as tf

def print_execution(func):
    @wraps(func)
    def wrapper(self: SimpleModel, data):
        print(tf.executing_eagerly())  # Prints False
        return func(self, data)
    return wrapper

class SimpleModel(tf.keras.Model):
    def __init__(self):
        super().__init__()

    def call(self, inputs, training=None, mask=None):
        return inputs

    @print_execution
    def predict_step(self, data):
        return super().predict_step(data)

if __name__ == &quot;__main__&quot;:
    x = tf.random.uniform((2, 2))
    print(tf.executing_eagerly())  # Prints True
    model = SimpleModel()
    pred = model.predict(x)
</code></pre>
<p>Is this the intended behavior? Is there a way to force the <code>predict_step</code> to run in eager mode?</p>
",2021-03-23 22:14:44,"<p>If you want to run the <code>predict_step</code> function in eager mode, you can do it as follows. Please note, it will set everything in eager mode.</p>
<pre><code>import tensorflow as tf
tf.config.run_functions_eagerly(True)
</code></pre>
<p>Typically <code>tf.function</code> are in <code>Graph</code> mode. Using the above statement, they can be set to <code>Eager</code> mode too, <a href=""https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly"" rel=""nofollow noreferrer"">src</a>.</p>
<p>As per your comment, AFAIK, there should not be any difference if you set <code>run_eagerly</code> while compiling the model. Here is from the official statement, <a href=""https://keras.io/api/models/model_training_apis/"" rel=""nofollow noreferrer"">src - model.compile</a>.</p>
<blockquote>
<p><strong>run_eagerly</strong>: Bool. Defaults to False. If True, this Model's logic will not be wrapped in a <strong>tf. function</strong>. Recommended to leave this as None unless your Model cannot be run inside a <strong>tf. function</strong>.</p>
</blockquote>
<hr />
<p>About your first query, why does <code>TensorFlow</code> disable eager execution inside the <code>predict_step</code> function of a <code>tf.keras.Model</code>?</p>
<p>One of the main reasons is to deliver the best performance of your model. And it's not only with <code>predict_step</code> but also <code>train_step</code> and <code>test_step</code>. Basically <code>tf. keras</code> models are compiled to a static graph. In order to make run them in eager mode, the above approaches need to be done. But note that, using eager mode in such cases may slow down your training. For the collective of good, <code>tf. keras</code> models are compiled in graph mode.</p>
",13637002,501,https://stackoverflow.com/questions/66772069,Documentation Ambiguity
62752605,Loss function in tf.nn.sampled_softmax_loss,"<p>I have a question regarding Tensorflow:</p>
<p>Which loss function is used in <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer""><code>tf.nn.sampled_softmax_loss</code></a>?</p>
<p>I believe it's <em><strong>cross-entropy</strong></em>, but it is not written on the official website. Can anyone confirm my guess?</p>
",2020-07-06 09:01:18,"<p>Based on <a href=""https://stackoverflow.com/questions/35241251/in-tensorflow-what-is-the-difference-between-sampled-softmax-loss-and-softmax-c"">this other question</a>, it looks like it is cross entropy.</p>
<p>Besides, the main difference between <code>sampled_softmax_loss</code> and <code>softmax_cross_entropy_with_logits</code> (the standard cross_entropy loss in TF) is that the first only takes into account a subset V of your vocabulary to calculate your loss, while the second takes into account your entire vocabulary.</p>
",13874745,393,https://stackoverflow.com/questions/62752605,Documentation Replicability
62767445,What's default initial_state in tf.nn.dynamic_rnn,"<p>Usually, we would use <code>cell.zero_state</code> as the <code>initial_state</code> of <code>tf.nn.dynamic_rnn</code>.</p>
<p>Now, I'm wondering what's default <code>initial_state</code> in <code>tf.nn.dynamic_rnn</code> if we don't set <code>initial_state</code>.</p>
<p>The most similar question I can find is <a href=""https://stackoverflow.com/questions/56140870/setting-initial-state-in-dynamic-rnn"">Setting initial state in dynamic RNN</a></p>
<p>But I can't understand what does scratch mean in the answer:</p>
<blockquote>
<p>If you don't set the initial_state, it will be trained from scratch as
other weight matrices do.</p>
</blockquote>
",2020-07-07 02:55:20,"<p>If there is no <code>initial_state</code>, dynamic_rnn will try to call <code>cell.get_initial_state(inputs=None, batch_size=batch_size, dtype=dtype)</code> to set <code>initial_state</code>, and if <code>cell.get_initial_state</code> is not defined, <code>cell.zero_state</code> is used. See the source code of dynamic_rnn <a href=""https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/rnn.py#L671"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/rnn.py#L671</a></p>
<p>For most default cell implementations, <code>cell.get_initial_state</code> function is defined the same as <code>cell.zero_state</code> if <code>inputs is none</code>. For example <a href=""https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/rnn_cell_impl.py#L281-L309"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/rnn_cell_impl.py#L281-L309</a></p>
<p>In conclusion, <code>cell.zero_state</code> is used whether you set the <code>initial_state</code> or not. But you can build your own cell and re-implement <code>cell.get_initial_state</code>.</p>
",13874745,393,https://stackoverflow.com/questions/62767445,Documentation Ambiguity
67698111,Show version of tensorflow 2.4.1 in python 3.7,"<p>Using python 3.7, how do you print the version of tensorflow if you have tensorflow 2.4.1. None of the documented styles: 'tf. __ version__', tf.version, nor tf.version.VERSION seem to work.</p>
",2021-05-26 03:22:18,"<p>This one seems to be working for me:</p>
<p><code>tf.__version__</code></p>
<p>Run with Python 3.7.10 and Tensorflow 2.4.1 in a Colab Notebook</p>
<p>Alternatively, if you used pip to install Tensorflow, you could also use:</p>
<pre><code>pip show tensorflow
</code></pre>
",13402234,18,https://stackoverflow.com/questions/67698111,Documentation Ambiguity
64642944,Steps of tf.summary.* operations in TensorBoard are always 0,"<p>When I'm training my model with TensorFlow 2.3, I want to visualize some intermediate tensors calculated using the weight in the computation graph of my customized <code>tf.keras.layers.Layer</code>.</p>
<p>So I use <code>tf.summary.image()</code> to record these tensors and visualize them as images like this:</p>
<pre><code>class CustomizedLayer(tf.keras.layers.Layer):
    def call(self, inputs, training=None):
        # ... some code ...
        tf.summary.image(name=&quot;some_weight_map&quot;, data=some_weight_map)
        # ... some code ...
</code></pre>
<p>But in TensorBoard, no matter how many steps passed, there is only one image of step 0 shown.</p>
<p>And I tried to set the parameter <em><strong>step</strong></em> of <code>tf.summary.image()</code> to the value obtained from <code>tf.summary.experimental.get_step()</code>:</p>
<pre><code>tf.summary.image(name=&quot;weight_map&quot;, data=weight_map, step=tf.summary.experimental.get_step())
</code></pre>
<p>And update the step by calling <strong>tf.summary.experimental.set_step</strong> from a customized Callback using a tf.Variable like codes shown below:</p>
<pre><code>class SummaryCallback(tf.keras.callbacks.Callback):
def __init__(self, step_per_epoch):
    super().__init__()
    self.global_step = tf.Variable(initial_value=0, trainable=False, name=&quot;global_step&quot;)
    self.global_epoch = 0
    self.step_per_epoch = step_per_epoch
    tf.summary.experimental.set_step(self.global_step)

def on_batch_end(self, batch, logs=None):
    self.global_step = batch + self.step_per_epoch * self.global_epoch
    tf.summary.experimental.set_step(self.global_step)  
    # whether the line above is commented, calling tf.summary.experimental.get_step() in computation graph code always returns 0.
    # tf.print(self.global_step)

def on_epoch_end(self, epoch, logs=None):
    self.global_epoch += 1
</code></pre>
<p>This Callback's instance is passed in the argument <em><strong>callbacks</strong></em> in <code>model.fit()</code> function.</p>
<p>But the value <code>tf.summary.experimental.get_step()</code> returned is still 0.</p>
<p>The TensorFlow document of &quot;<code>tf.summary.experimental.set_step()</code>&quot; says:</p>
<blockquote>
<p>when using this with @tf.functions, the step value will be captured at the time the function is traced, so changes to the step outside the function will not be reflected inside the function unless using a tf.Variable step.</p>
</blockquote>
<p>Accroding to the document, I am already using a Variable to store the steps, but it's changes are still not reflected inside the function (or keras.Model).</p>
<p>Note: My code produces expected results in TensorFlow 1.x with just a simple line of <code>tf.summary.image()</code> before I migrate it to TensorFlow 2.</p>
<p>So I want to know if my approach is wrong in TensorFlow 2?</p>
<p>In TF2, how can I <strong>get training steps inside the computation graph</strong>?</p>
<p>Or there is other solution to <strong>summarize tensors (as scalar, image, etc.) inside a model in TensorFlow 2</strong>?</p>
",2020-11-02 09:33:43,"<p>I found this issue has been reported on Github repository of Tensorflow: <a href=""https://github.com/tensorflow/tensorflow/issues/43568"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/43568</a></p>
<p>This is caused by using <strong>tf.summary</strong> in model while <strong>tf.keras.callbacks.TensorBoard</strong> callback is also enabled, and the step will always be zero. The issue reporter gives a temporary solution.</p>
<p>To fix it, inherit the <strong>tf.keras.callbacks.TensorBoard</strong> class and overwrite the <strong>on_train_begin</strong> method and <strong>on_test_begin</strong> method like this:</p>
<pre><code>class TensorBoardFix(tf.keras.callbacks.TensorBoard):
&quot;&quot;&quot;
This fixes incorrect step values when using the TensorBoard callback with custom summary ops
&quot;&quot;&quot;

def on_train_begin(self, *args, **kwargs):
    super(TensorBoardFix, self).on_train_begin(*args, **kwargs)
    tf.summary.experimental.set_step(self._train_step)


def on_test_begin(self, *args, **kwargs):
    super(TensorBoardFix, self).on_test_begin(*args, **kwargs)
    tf.summary.experimental.set_step(self._val_step)
</code></pre>
<p>And use this fixed callback class in <strong>model.fit()</strong>:</p>
<pre><code>tensorboard_callback = TensorBoardFix(log_dir=log_dir, histogram_freq=1, write_graph=True, update_freq=1)
model.fit(dataset, epochs=200, callbacks=[tensorboard_callback])
</code></pre>
<p>This solve my problem and now I can get proper step inside my model by calling <strong>tf.summary.experimental.get_step()</strong>.</p>
<p>(This issue may be fixed in later version of TensorFlow)</p>
",14562728,31,https://stackoverflow.com/questions/64642944,Documentation Replication on Other Examples
64799299,Use TensorBoard to visualize graph from tf_agents,"<p>I'm quite new to RL and currently teaching myself how to implement different algorithms and hyper-parameters using tf_agents library.</p>
<p>I've been playing around with the code provided from this tutorial <a href=""https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb</a>.</p>
<p>After learning how to use TensorBoard I've come to wonder how I can visualize a graph from tf_agents library. Every TensorBoard tutorials/posts seems to implement its own model or define tf.function to log graph. However, I just can't apply such methods to the tutorial above.</p>
<p>If someone can help me visualize a model graph using tf_agents in TensorBoard, it will be very much appreciated. Thanks!</p>
",2020-11-12 07:01:01,"<p>Consider that this colab notebook is a very simple version of how TF-Agents actually works. In reality you should use the Driver to sample trajectories instead of you manually calling</p>
<pre><code>agent.action(state)
env.step(action)
</code></pre>
<p>at every iteration. The other advantage of the Driver is that it provides easy compatibility with all the metrics in TF-Agents.</p>
<p>As to your question here is how:</p>
<p>At the beginning of your training define a summary_writer with something like:</p>
<pre><code>train_dir = os.path.join(root_dir, 'train')    
train_summary_writer = tf.summary.create_file_writer(
            train_dir, flush_millis=10000)
train_summary_writer.set_as_default()
</code></pre>
<p>Now everytime you call agent.train it will flush to this summary writer and its tensorboard folder <code>train_dir</code>.</p>
<p>To add some metrics into the mix simply define them with something like:</p>
<pre><code>train_metrics = [
        tf_metrics.NumberOfEpisodes(),
        tf_metrics.EnvironmentSteps(),
        tf_metrics.AverageReturnMetric(buffer_size=collect_episodes_per_epoch),
        tf_metrics.AverageEpisodeLengthMetric(buffer_size=collect_episodes_per_epoch),
    ]
</code></pre>
<p>Pass them to the Driver as observers together with your Replay Buffer like this:</p>
<pre><code>dynamic_episode_driver.DynamicEpisodeDriver(
            tf_env,
            collect_policy,
            observers=replay_observer + train_metrics,
            num_episodes=collect_episodes_per_epoch).run()
</code></pre>
<p>And after this log them to your summaries with:</p>
<pre><code>for train_metric in train_metrics:
    train_metric.tf_summaries(train_step=epoch_counter, step_metrics=train_metrics[:2])
</code></pre>
<p>In case you're wondering, the <code>step_metrics</code> arg is to plot the last two metrics against the first two.</p>
",14622788,107,https://stackoverflow.com/questions/64799299,Documentation Replication on Other Examples
66478545,Keras nomalise axis argument,"<p><code>x_train = tf.keras.utils.normalize(x_train,axis = 1)</code></p>
<p>Guys what's the axis argument here really means?? My image shape is (60000,28,28)</p>
",2021-03-04 15:54:15,"<p><code>axis</code> means the axis along which to normalize. In your case, for each example among 60,000 examples, with <code>axis = 1</code>, all the rows of each matrix 28x28 will be normalized.</p>
<p>Basically, your data has shape <code>(60000, 28, 28)</code>. This means <code>axis</code> can be 0, 1 or 2:</p>
<pre><code>axis=0 -&gt; 60,000 elements
axis=1 -&gt; 28 elements
axis=2 -&gt; 28 elements
</code></pre>
",14187980,29,https://stackoverflow.com/questions/66478545,Documentation Ambiguity
65982015,tf.keras.preprocessing.sequence.pad_sequences in JavaScript,"<p>How can we implement tf.keras.preprocessing.sequence.pad_sequences in TensorFlow.js?</p>
<pre><code>encoded_text = tokenizer.texts_to_sequences([input_text])[0]
pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')
</code></pre>
",2021-01-31 17:16:08,"<p>The <a href=""https://github.com/tensorflow/tfjs-models/tree/master/universal-sentence-encoder"" rel=""nofollow noreferrer"">universal sentence encoder</a> can be used to convert text into tensors</p>
<pre><code>require('@tensorflow/tfjs');
const use = require('@tensorflow-models/universal-sentence-encoder');

use.load().then(model =&gt; {
  // Embed an array of sentences.
  const sentences = [
    'Hello.',
    'How are you?'
  ];
  model.embed(sentences).then(embeddings =&gt; {
    // `embeddings` is a 2D tensor consisting of the 512-dimensional embeddings for each sentence.
    // So in this example `embeddings` has the shape [2, 512].
    embeddings.print(true /* verbose */);
  });
});
</code></pre>
<p><code>tf.pad</code> can later be used to padd the tensors</p>
",14226505,33,https://stackoverflow.com/questions/65982015,Documentation Replication on Other Examples
63851431,How to Augment the Training Set using the tf.keras.utils.Sequence API?,"<p>TensorFlow documentation have the following example that can illustrate how to create a batch generator to feed a training set in batches to a model when the training set is too large to fit in memory:</p>
<pre class=""lang-py prettyprint-override""><code>from skimage.io import imread
from skimage.transform import resize
import tensorflow as tf
import numpy as np
import math

# Here, `x_set` is list of path to the images
# and `y_set` are the associated classes.

class CIFAR10Sequence(tf.keras.utils.Sequence):

    def __init__(self, x_set, y_set, batch_size):
        self.x, self.y = x_set, y_set
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) *
        self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) *
        self.batch_size]

        return np.array([
            resize(imread(file_name), (200, 200))
               for file_name in batch_x]), np.array(batch_y)
</code></pre>
<p>My intention is to further increase the diversity of the training set by rotating each image 3x by 90. In each Epoch of the training process, the model would first be fed with the &quot;0 training set&quot; and next with the 90, 180 and 270 rotating sets, respectively.</p>
<p>How can I modify the previous piece of code to perform this operation inside the <code>CIFAR10Sequence()</code> data generator?</p>
<p>Please don't use <code>tf.keras.preprocessing.image.ImageDataGenerator()</code> so that the answer does not lose its generality for another type of similar problems that are of a different nature.</p>
<p>NB: The idea would be to create the new data &quot;in real time&quot; as the model is fed instead of creating (in advance) and storing on disk a new and augmented training set bigger than the original one to be used later (also in batches) during the training process of the model.</p>
<p>Thx in advance</p>
",2020-09-11 17:04:51,"<p>Use custom <code>Callback</code> and hook into <code>on_epoch_end</code>. After each epoch end change the angle of the data iterator object.</p>
<h3>Sample  (documented inline)</h3>
<pre><code>from skimage.io import imread
from skimage.transform import resize, rotate
import numpy as np

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.utils import Sequence 
from keras.models import Sequential
from keras.layers import Conv2D, Activation, Flatten, Dense

# Model architecture  (dummy)
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(15, 15, 4)))
model.add(Activation('relu'))
model.add(Flatten())
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# Data iterator 
class CIFAR10Sequence(Sequence):
    def __init__(self, filenames, labels, batch_size):
        self.filenames, self.labels = filenames, labels
        self.batch_size = batch_size
        self.angles = [0,90,180,270]
        self.current_angle_idx = 0

    # Method to loop throught the available angles
    def change_angle(self):
      self.current_angle_idx += 1
      if self.current_angle_idx &gt;= len(self.angles):
        self.current_angle_idx = 0
  
    def __len__(self):
        return int(np.ceil(len(self.filenames) / float(self.batch_size)))

    # read, resize and rotate the image and return a batch of images
    def __getitem__(self, idx):
        angle = self.angles[self.current_angle_idx]
        print (f&quot;Rotating Angle: {angle}&quot;)

        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]
        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]
        return np.array([
            rotate(resize(imread(filename), (15, 15)), angle)
               for filename in batch_x]), np.array(batch_y)

# Custom call back to hook into on epoch end
class CustomCallback(keras.callbacks.Callback):
    def __init__(self, sequence):
      self.sequence = sequence

    # after end of each epoch change the rotation for next epoch
    def on_epoch_end(self, epoch, logs=None):
      self.sequence.change_angle()               


# Create data reader
sequence = CIFAR10Sequence([&quot;f1.PNG&quot;]*10, [0, 1]*5, 8)
# fit the model and hook in the custom call back
model.fit(sequence, epochs=10, callbacks=[CustomCallback(sequence)])
</code></pre>
<p>Output:</p>
<pre><code>Rotating Angle: 0
Epoch 1/10
Rotating Angle: 0
Rotating Angle: 0
2/2 [==============================] - 2s 755ms/step - loss: 1.0153 - accuracy: 0.5000
Epoch 2/10
Rotating Angle: 90
Rotating Angle: 90
2/2 [==============================] - 0s 190ms/step - loss: 0.6975 - accuracy: 0.5000
Epoch 3/10
Rotating Angle: 180
Rotating Angle: 180
2/2 [==============================] - 2s 772ms/step - loss: 0.6931 - accuracy: 0.5000
Epoch 4/10
Rotating Angle: 270
Rotating Angle: 270
2/2 [==============================] - 0s 197ms/step - loss: 0.6931 - accuracy: 0.5000
Epoch 5/10
Rotating Angle: 0
Rotating Angle: 0
2/2 [==============================] - 0s 189ms/step - loss: 0.6931 - accuracy: 0.5000
Epoch 6/10
Rotating Angle: 90
Rotating Angle: 90
2/2 [==============================] - 2s 757ms/step - loss: 0.6932 - accuracy: 0.5000
Epoch 7/10
Rotating Angle: 180
Rotating Angle: 180
2/2 [==============================] - 2s 757ms/step - loss: 0.6931 - accuracy: 0.5000
Epoch 8/10
Rotating Angle: 270
Rotating Angle: 270
2/2 [==============================] - 2s 761ms/step - loss: 0.6932 - accuracy: 0.5000
Epoch 9/10
Rotating Angle: 0
Rotating Angle: 0
2/2 [==============================] - 1s 744ms/step - loss: 0.6932 - accuracy: 0.5000
Epoch 10/10
Rotating Angle: 90
Rotating Angle: 90
2/2 [==============================] - 0s 192ms/step - loss: 0.6931 - accuracy: 0.5000
&lt;tensorflow.python.keras.callbacks.History at 0x7fcbdf8bcdd8&gt;
</code></pre>
<pre><code></code></pre>
",14230555,7,https://stackoverflow.com/questions/63851431,Documentation Replication on Other Examples
68788593,Apply Tensorflow tf.keras.initializers.GlorotNormal(seed=1) to tf.Variable,"<p>How to apply the initializer to the tf.Variable function? Am I on the right track?</p>
<pre><code>def initialize_parameters():
                                
    initializer = tf.keras.initializers.GlorotNormal(seed=1)   
   
    W1 = tf.Variable(initializer(shape=([25, 12288]))
    b1 = tf.Variable(initializer(shape=([25, 1]))
    W2 = tf.Variable(initializer(shape=([12, 25]))
    b2 = tf.Variable(initializer(shape=([12, 1]))
    W3 = tf.Variable(initializer(shape=([6, 12]))
    b3 = tf.Variable(initializer(shape=([6, 1]))
  

    parameters = {&quot;W1&quot;: W1,
                  &quot;b1&quot;: b1,
                  &quot;W2&quot;: W2,
                  &quot;b2&quot;: b2,
                  &quot;W3&quot;: W3,
                  &quot;b3&quot;: b3}
    
    return parameters

</code></pre>
<p>I want the shapes to be as follow -</p>
<pre><code>
W1 shape: (25, 12288)
b1 shape: (25, 1)
W2 shape: (12, 25)
b2 shape: (12, 1)
W3 shape: (6, 12)
b3 shape: (6, 1)

</code></pre>
",2021-08-15 03:55:14,"<p>It should be <code>W1 = tf.Variable(initializer(shape=(25, 12288)))</code>. Notice the round bracket</p>
",13494561,53,https://stackoverflow.com/questions/68788593,Documentation Replicability
66623244,Output probability of prediction in tensorflow.js,"<p>I have a model.json generated from tensorflow via tensorflow.js coverter</p>
<p>In the original implementation of model in tensorflow in python, it is built like this:</p>
<pre><code>model = models.Sequential([
        base_model,
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
    ])
</code></pre>
<p>In tensorflow, the probability can be generated by score = tf.nn.softmax(predictions[0]), according to the tutorial on official website.</p>
<p>How do I get this probability in tensorflow.js?</p>
<p>I have copied the codes template as below:</p>
<pre><code>$(&quot;#predict-button&quot;).click(async function () {
    if (!modelLoaded) { alert(&quot;The model must be loaded first&quot;); return; }
    if (!imageLoaded) { alert(&quot;Please select an image first&quot;); return; }
    
    let image = $('#selected-image').get(0);
    
    // Pre-process the image
    console.log( &quot;Loading image...&quot; );
    let tensor = tf.browser.fromPixels(image, 3)
        .resizeNearestNeighbor([224, 224]) // change the image size
        .expandDims()
        .toFloat()
        // RGB -&gt; BGR
    let predictions = await model.predict(tensor).data();
    console.log(predictions);
    let top5 = Array.from(predictions)
        .map(function (p, i) { // this is Array.map
            return {
                probability: p,
                className: TARGET_CLASSES[i] // we are selecting the value from the obj
            };
        }).sort(function (a, b) {
            return b.probability - a.probability;
        }).slice(0, 2);
        console.log(top5);
    $(&quot;#prediction-list&quot;).empty();
    top5.forEach(function (p) {
        $(&quot;#prediction-list&quot;).append(`&lt;li&gt;${p.className}: ${p.probability.toFixed(6)}&lt;/li&gt;`);
        });
</code></pre>
<p>How should I modify the above code?</p>
<p>The output is just the same as the value of variable 'predictions':</p>
<pre><code>Float32Array(5)
0: -2.5525975227355957
1: 7.398464679718018
2: -3.252196788787842
3: 4.710395812988281
4: -4.636396408081055
buffer: (...)
byteLength: (...)
byteOffset: (...)
length: (...)
Symbol(Symbol.toStringTag): (...)
__proto__: TypedArray


0: {probability: 7.398464679718018, className: &quot;Sunflower&quot;}
1: {probability: 4.710395812988281, className: &quot;Rose&quot;}
length: 2
__proto__: Array(0)
</code></pre>
<p>Please help!!!
Thanks!</p>
",2021-03-14 10:12:32,"<p>In order to extract the probabilities from the logits of the model using a softmax function you can do the following:</p>
<p>This is the array of logits that are also the <code>predictions</code> you get from the model</p>
<pre><code>const logits = [-2.5525975227355957, 7.398464679718018, -3.252196788787842, 4.710395812988281, -4.636396408081055]
</code></pre>
<p>You can call <code>tf.softmax()</code> on the array of values</p>
<pre><code>const probabilities = tf.softmax(logits)
</code></pre>
<p>Result:</p>
<pre><code>[0.0000446, 0.9362511, 0.0000222, 0.0636765, 0.0000056]
</code></pre>
<p>Then if you wanted to get the index with the highest probability you can make use of <code>tf.argMax()</code>:</p>
<pre><code>const results = tf.argMax(probabilities).dataSync()[0]
</code></pre>
<p>Result:</p>
<pre><code>1
</code></pre>
<h3>Edit</h3>
<p>I am not too familiar with jQuery so this might not be correct. But here is how I would get the probabilities of the outputs in descending order:</p>
<pre><code>let probabilities = tf.softmax(predictions).dataSync();
$(&quot;#prediction-list&quot;).empty();
probabilities.forEach(function(p, i) {
  $(&quot;#prediction-list&quot;).append(
    `&lt;li&gt;${TARGET_CLASSES[i]}: ${p.toFixed(6)}&lt;/li&gt;`
  );
});
</code></pre>
",15365699,85,https://stackoverflow.com/questions/66623244,Documentation Replicability
64326029,Load tensorflow images and create patches,"<p>I am using <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer"">image_dataset_from_directory</a> to load a very large RGB imagery dataset from disk into a <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">Dataset</a>. For example,</p>
<pre><code>dataset = tf.keras.preprocessing.image_dataset_from_directory(
    &lt;directory&gt;,
    label_mode=None,
    seed=1,
    subset='training',
    validation_split=0.1)
</code></pre>
<p>The Dataset has, say, 100000 images grouped into batches of size 32 yielding a <code>tf.data.Dataset</code> with spec <code>(batch=32, width=256, height=256, channels=3)</code></p>
<p>I would like to extract patches from the images to create a new <code>tf.data.Dataset</code> with image spatial dimensions of, say, 64x64.</p>
<p>Therefore, I would like to create a new Dataset with 400000 patches still in batches of 32 with a <code>tf.data.Dataset</code> with spec <code>(batch=32, width=64, height=64, channels=3)</code></p>
<p>I've looked at the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window"" rel=""nofollow noreferrer"">window</a> method and the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/extract_patches"" rel=""nofollow noreferrer"">extract_patches</a> function but it's not clear from the documentation how to use them to create a new Dataset I need to start training on the patches. The <code>window</code> seems to be geared toward 1D tensors and the <code>extract_patches</code> seems to work with arrays and not with Datasets.</p>
<p>Any suggestions on how to accomplish this?</p>
<p>UPDATE:</p>
<p>Just to clarify my needs. I am trying to avoid manually creating the patches on disk. One, that would be untenable disk wise. Two, the patch size is not fixed. The experiments will be conducted over several patch sizes. So, I do not want to manually perform the patch creation either on disk or manually load the images in memory and perform the patching. I would prefer to have tensorflow handle the patch creation as part of the pipeline workflow to minimize disk and memory usage.</p>
",2020-10-12 22:18:54,"<p>What you're looking for is <a href=""https://www.tensorflow.org/api_docs/python/tf/image/extract_patches"" rel=""nofollow noreferrer""><code>tf.image.extract_patches</code></a>. Here's an example:</p>
<pre><code>import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import numpy as np

data = tfds.load('mnist', split='test', as_supervised=True)

get_patches = lambda x, y: (tf.reshape(
    tf.image.extract_patches(
        images=tf.expand_dims(x, 0),
        sizes=[1, 14, 14, 1],
        strides=[1, 14, 14, 1],
        rates=[1, 1, 1, 1],
        padding='VALID'), (4, 14, 14, 1)), y)

data = data.map(get_patches)

fig = plt.figure()
plt.subplots_adjust(wspace=.1, hspace=.2)
images, labels = next(iter(data))
for index, image in enumerate(images):
    ax = plt.subplot(2, 2, index + 1)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.imshow(image)
plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/qn1od.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qn1od.png"" alt=""enter image description here"" /></a></p>
",14438185,23,https://stackoverflow.com/questions/64326029,Documentation Replication on Other Examples
70880589,what does cardinality mean in relation to an image dataset?,"<p>After successfully creating a tensorflow image <code>Dataset</code> with:</p>
<p><code>dataset = tf.keras.utils.image_dataset_from_directory(...)</code></p>
<p>which returns</p>
<p><em>Found 21397 files belonging to 5 classes.
Using 17118 files for training.</em></p>
<p>There is the cardinality method:</p>
<p><code>dataset.cardinality()</code></p>
<p>which returns a tensor containing the single value</p>
<p><em>tf.Tensor(535, shape=(), dtype=int64)</em></p>
<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality"" rel=""nofollow noreferrer"">docs here</a> but I don't understand what 535 represents or why its different to the number of files?</p>
<p>I ask, because I would like to understand how cardinality plays into this equation:</p>
<p><code>steps_per_epoch = dataset.cardinality().numpy() // batch_size</code></p>
",2022-01-27 14:47:48,"<p>The cardinality, in your case, is simply the rounded number of batches:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import pathlib

dataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=123,
  image_size=(180, 180),
  batch_size=batch_size)

print(train_ds.cardinality())
</code></pre>
<pre><code>Found 3670 files belonging to 5 classes.
Using 2936 files for training.
tf.Tensor(92, shape=(), dtype=int64)
</code></pre>
<p>The equation is: <code>2936/32 = cardinality</code>, so it depends on your batch size.</p>
",14777655,73,https://stackoverflow.com/questions/70880589,Lack of Alternative Solutions/Documentation
66942311,"In Tensorflow, what does tf.GradientTape.gradients do when its ""target"" attribute is a multi-dimensional tensor?","<p>In my model, I'm using tf.keras.losses.MSE to calculate the <strong>mean squared error</strong> of my <strong>BATCH_SIZE x 256 x 256 x 3</strong> output and my <strong>BATCH_SIZE x 256 x 256 x 3</strong> input.</p>
<p>The <strong>output</strong> of this function appears to be <strong>(None,256,256)</strong>.</p>
<p>I then use tf.GradientTape.gradients, with the MSE <strong>output</strong> as the &quot;<strong>target</strong>&quot; attribute. In the documentation, it says that this attribute can be a tensor.</p>
<p>My understanding is that <strong>loss is a scalar number</strong> which is differentiated against each of the weights during backpropagation.</p>
<p>Therefore, my question is: <strong>What happens when a multi-dimensional tensor is passed into the gradients function? Is the sum of all elements in the tensor simple calculated?</strong></p>
<p>I ask this because my model is not training at the moment, with loss reading at 1.0 at every epoch. My assumption is that I am not calculating the gradients correctly, as all my gradients are reading as 0.0 for each weight.</p>
",2021-04-04 14:33:29,"<pre><code>import tensorflow as tf
x = tf.Variable([3.0, 2.0])
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
print(y)

Result: 
tf.Tensor([6. 4.], shape=(2,), dtype=float32)
tf.Tensor([9. 4.], shape=(2,), dtype=float32)
</code></pre>
<p>As described in the figure above, <code>tf.GradientTape.gradient</code> simply calculates the gradient dy/dx. In your case with multiple variables, tf seems to calculate the derivative of the corresponding tensor instead of automatically summing them.</p>
",15170857,79,https://stackoverflow.com/questions/66942311,Documentation Replicability
66659610,How does tf.keras.metrics.TopKCategoricalAccuracy differ from Precision@k?,"<p>Coming from recommender systems, precision@k is a popular metric.</p>
<blockquote>
<p>precision@k = number of relevant predictions in top k / k</p>
</blockquote>
<p>On the tensorflow docs for tf.keras.metrics.TopKCategoricalAccuracy it states</p>
<blockquote>
<p>Computes how often targets are in the top K predictions.</p>
</blockquote>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TopKCategoricalAccuracy"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TopKCategoricalAccuracy</a></p>
<p>Which seems to be exactly the same as precision@k. Am I missing something or are they equivalent and it just comes down to TF/recommender terminology?</p>
",2021-03-16 16:43:59,"<p>TopKCategoricalAccuracy and Precison at k , these two metrics are different from each other. let us see one example.</p>
<p>For instance in the recommendation usecase , we predict 5 movies [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;,&quot;D&quot;,&quot;F&quot;] for a user and the user viewed Movie 'A' and rejected the rest.</p>
<ol>
<li>Then precision at 1 = 1/1= 1</li>
<li>Then precision at 5 = 1/5(among 5 movies user select only one)</li>
<li>Top1CategoricalAccuracy(K=1)= 1 or 100%(Because in the prediction list the First movie 'A' was seen by the user)</li>
<li>Top5CategoricalAccuracy(K=5)= 1 or 100%(the right answer appears in your top five guesses)</li>
</ol>
",15409191,23,https://stackoverflow.com/questions/66659610,Documentation Replicability
71773122,cannot import name 'normalization' from 'tensorflow.python.keras.layers',"<p>I am trying to use tensorflow_rankings, but cannot import it due to the above error. I am using tensorflow 2.8.0 and tensorflow_rankings 0.5.0, which seem to be the latest stable builds. They are what get automatically installed from</p>
<pre><code>pip install tensorflow
pip install tensorflow_ranking
</code></pre>
<p>I am on Python 3.8.10, Windows 11.</p>
<p>The TF 2.8.0 docs show there is a Normalization layer in tf.keras.layers. The error seems to come from:</p>
<pre><code>from tensorflow.python.keras.layers import normalization as keras_norm
</code></pre>
<p>in</p>
<pre><code>from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
</code></pre>
<p>Any advice?</p>
",2022-04-06 20:16:27,"<p>Seems my installation of TF was corrupted. A full uninstall and reinstall fixed it.</p>
",15848470,548,https://stackoverflow.com/questions/71773122,Documentation Replicability
63600026,What are mixed layers in tf.keras.applications.InceptionV3?,"<p>I am currently trying to understand the architecture of Inseption v3 as implemented in <code>tf.keras.applications.InceptionV3</code>.</p>
<p>I am looking at the list of names in the model's layers:</p>
<pre class=""lang-py prettyprint-override""><code>print([layer.name for layer in model.layers])

#Outputs:
['input_1',
 'conv2d',
 'batch_normalization',
 'activation',
 'conv2d_1',
 'batch_normalization_1',
 'activation_1',
 'conv2d_2',
 ...
]

</code></pre>
<p>I understand how batch normalization, pooling and conv layers transform inputs, but deeper we have layers named <code>mixed1, mixed2, ...</code> and so on. I am trying to understand how they (mixed layers) are transforming their inputs.</p>
<p>So far, I couldn't find any information about them.
How does a mixed layer work? What does it do?</p>
",2020-08-26 14:37:30,"<p>Refer to <a href=""https://arxiv.org/abs/1512.00567"" rel=""nofollow noreferrer"">InceprtionV3 paper</a>.</p>
<p>You can see that the mixed layers are made of four parallel connections with single input and we get the output by concatenating all parallel outputs into one. Note that to contatenate all the outputs, all parallel feature maps have to have identical first two dimensions (number of feature maps can differ) and this is achieved by strides and pooling.</p>
<p><a href=""https://i.stack.imgur.com/H4pFo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H4pFo.png"" alt=""Inception layer"" /></a></p>
",14142345,908,https://stackoverflow.com/questions/63600026,Lack of Alternative Solutions/Documentation
68422887,Slow tensorflow code; can I batch evaluate and obtain multiple loss scores?,"<p>I recently switched from using the 'keras' package in Python to using 'tensorflow.keras', since this seems to be preferred now. The latest version of Keras was also giving me issues that seemed like I'd have to modify the internal Keras code to fix, whereas tf.keras works fine. However, upon making this switch, some of my code was slowed down by a factor of 30-40. I've identified the following calls to &quot;model.evaluate&quot; as a bottleneck, though I'm not sure why it's so much slower than before. The code is structured something like this:</p>
<pre><code># 'model' is a tensorflow.keras.models.Sequential

n_scores = 10000

inputs = np.zeros((n_scores, 8, 10), dtype=np.bool)
outputs = np.zeros((n_scores, 10), dtype=np.bool)

# [populate inputs and outputs]

scores = []
for i in range(n_scores):
    score = model.evaluate(inputs[i,:,:], outputs[i,:])
    scores.append(score)

return scores
</code></pre>
<p>I'm thinking the major bottleneck is that I'm making a bunch of SMALL calls to tensorflow, rather than one LARGE call. Using a GPU actually makes it even slower, presumably due to all the loading/unloading.</p>
<p>I'd like to just make a call like</p>
<pre><code>scores = model.evaluate(inputs, outputs)
</code></pre>
<p>but model.evaluate() seems to always output a single scalar, when I need the whole list of 10000 loss scores. I have been unable to find a solution in the documentation, is there a builtin way to do sort of a &quot;batch evaluate&quot; but get individual loss scores out for each sample?</p>
",2021-07-17 17:37:42,"<p>Per NONONONONO's comment, my solution was</p>
<pre><code># 'model' is a tensorflow.keras.models.Sequential

n_scores = 10000

inputs = np.zeros((n_scores, 8, 10), dtype=np.bool)
outputs = np.zeros((n_scores, 10), dtype=np.bool)

# [populate inputs and outputs]

predictions = model.predict(inputs)

from tensorflow.keras.losses import CategoricalCrossentropy
cce = CategoricalCrossentropy()

scores = []
for i in range(n_scores):
    score = cce(predictions[i], outputs[i])
    scores.append(score)

return scores
</code></pre>
",16470540,1,https://stackoverflow.com/questions/68422887,Documentation Replication on Other Examples
61998940,"Which type of parameter the Adam optimizer in GPflow is working on, constrained or unconstrained?","<p>In the document of GPflow like <a href=""https://gpflow.readthedocs.io/en/master/notebooks/advanced/gps_for_big_data.html"" rel=""nofollow noreferrer"">SVGP</a> and <a href=""https://gpflow.readthedocs.io/en/master/notebooks/advanced/natural_gradients.html"" rel=""nofollow noreferrer"">natural gradient</a>, the Adam optimizer in TensorFlow is used when it comes to training model parameters (lengthscale, variance, inducing inputs, etc) of the GP model using stochastic variational inference technique, while the natural gradient optimizer for variational parameters. A snippet looks as follows</p>

<pre class=""lang-py prettyprint-override""><code>def run_adam(model, iterations):
    """"""
    Utility function running the Adam optimizer

    :param model: GPflow model
    :param interations: number of iterations
    """"""
    # Create an Adam Optimizer action
    logf = []
    train_iter = iter(train_dataset.batch(minibatch_size))
    training_loss = model.training_loss_closure(train_iter, compile=True)
    optimizer = tf.optimizers.Adam()

    @tf.function
    def optimization_step():
        optimizer.minimize(training_loss, model.trainable_variables)

    for step in range(iterations):
        optimization_step()
        if step % 10 == 0:
            elbo = -training_loss().numpy()
            logf.append(elbo)
    return logf
</code></pre>

<p>As demonstrated, model.trainable_variables is passed to the Adam optimizer, which is inherited from tf.Module, and is composed of several parameters including lengthscale and variance. </p>

<p>What I am concerning is whether the Adam optimizer is working on unconstrained or constrained version of the parameters of the model. A snippet of test code runs as follows</p>

<pre class=""lang-py prettyprint-override""><code>import gpflow as gpf
import numpy as np

x = np.arange(10)[:, np.newaxis]
y = np.arange(10)[:, np.newaxis]
model = gpf.models.GPR((x, y), 
                       kernel = gpf.kernels.SquaredExponential(variance = 2, lengthscales = 3), 
                       noise_variance = 4)

model.kernel.parameters[0].unconstrained_variable is model.trainable_variables[0]
</code></pre>

<p>and returns</p>

<pre class=""lang-py prettyprint-override""><code>True
</code></pre>

<p>As far as I know, parameters of the gaussian process like lenghtscales and the variances of a kernel are nonegative, and they should be constrained when training. I am not an expert of the source code of GPflow or TensorFlow, but it seems that Adam is working on unconstrained parameters. Is this simply a misunderstanding of me, or anything else?</p>

<p>Thanks in advance for any help!</p>
",2020-05-25 08:52:09,"<p>You're right, and that's by design. A constrained variable in GPflow is represented by a <code>Parameter</code>. The <code>Parameter</code> wraps the <code>unconstrained_variable</code>. When you access <code>.trainable_variables</code> on your model, this will include the <code>unconstrained_variable</code> of the <code>Parameter</code>, and so when you pass these variables to the optimizer, the optimizer will train those rather than the <code>Parameter</code> itself.</p>

<p>But your model doesn't see the <code>unconstrained_value</code>, it sees the <code>Parameter</code> interface which is a <code>tf.Tensor</code>-like interface related to the wrapped <code>unconstrained_variable</code> via an optional transformation. This transformation maps the unconstrained value to a constrained value. As such, your model will only see the constrained value. It's not a problem that your constrained value must be positive, the transform will map negative values of the unconstrained values to positive values for the constrained value.</p>

<p>You can see the unconstrained and constrained values of the first <code>Parameter</code> for your kernel, as well as the transform that relates them, with</p>

<pre class=""lang-py prettyprint-override""><code>param = model.kernel.parameters[0]
param.value()  # this is what your model will see
param.unconstrained_variable  # this is what the optimizer will see
param.transform  # the above two are related via this
</code></pre>
",13560745,27,https://stackoverflow.com/questions/61998940,Documentation Ambiguity
64770581,What is the difference between tensorflow.keras.layers and keras.layers?,"<p>I have read a related question's answer about this on stackoverflow which explains the difference between <em>tensorflow.keras.layers</em> and <em>tensorflow.layers</em>, link:</p>
<p><a href=""https://stackoverflow.com/questions/51089334/what-is-the-difference-between-tf-keras-layers-versus-tf-layers"">What is the difference between tf.keras.layers versus tf.layers?</a></p>
<p>But, it doesn't answer the difference between <strong>tensorflow.keras.layers</strong> and <strong>keras.layers</strong>, whereas I noticed I can import them also.</p>
<p>So, what is the difference between them?</p>
",2020-11-10 14:13:42,"<p>Keras used to be able to support different backends (tensorflow, theano and CNTK). Since keras 2.3, there is no difference between <code>keras</code> and <code>tf.keras</code>.</p>
<p>An excerpt from the Readme of the <a href=""https://github.com/keras-team/keras"" rel=""nofollow noreferrer"">keras repository on github</a> :</p>
<blockquote>
<p>Multi-backend Keras and tf.keras</p>
<p>Multi-backend Keras has been discontinued. At this time, we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to tf.keras in TensorFlow 2.0.</p>
<p>Keras 2.2.5 was the last release of Keras implementing the 2.2.* API. It was the last release to only support TensorFlow 1 (as well as Theano and CNTK).</p>
<p><strong>The current release is Keras 2.4.0, which simply redirects to tf.keras.</strong></p>
</blockquote>
",13371696,1343,https://stackoverflow.com/questions/64770581,Documentation Ambiguity
68984841,How can I understand the kernel of tf.keras.layers.Dense for rank >2?,"<p>How can I understand the kernel of <code>tf.keras.layers.Dense</code> for rank &gt;2?</p>
<p>The official API doc states that:</p>
<blockquote>
<p>Note: If the input to the layer has a rank greater than 2, then Dense
computes the dot product between the inputs and the kernel along the
last axis of the inputs and axis 0 of the kernel (using tf.tensordot).
For example, if input has dimensions (batch_size, d0, d1), then we
create a kernel with shape (d1, units), and the kernel operates along
axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there
are batch_size * d0 such sub-tensors). The output in this case will
have shape (batch_size, d0, units).</p>
</blockquote>
<p>My understanding is that for a rank larger than 2 (for example rank 3) only <strong>one</strong> kernel is created and thus the same kernel is applied on all slices of the second dimension, like above.
That would consequently mean that the outputs for different indices of the second dimension are <strong>not independent</strong> of each other (especially during training).</p>
<p>Is my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?</p>
",2021-08-30 13:16:38,"<p>Yes, your understanding is correct.</p>
<p>To achieve what you want, you need to define a custom keras layer. Let's suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original <code>Dense</code> layer (link: <a href=""https://github.com/tensorflow/tensorflow/blob/22ffec3a9c44133cba2182d60678d49bb372f020/tensorflow/python/keras/layers/core.py#L1077"" rel=""nofollow noreferrer"">github</a>), except that</p>
<ol>
<li>In the <code>build</code> function, the shape of <code>self.kernel</code> is (d0, i0, units) instead. You can get the value of <code>d0</code> as well as <code>i0</code> from <code>input_shape</code>.</li>
<li>In the <code>call</code> function, to do the specified tensor multiplication between <code>inputs</code> and <code>self.kernel</code>, use <code>tf.einsum</code> with this equation: <code>tf.einsum('abc,bcg-&gt;abg', inputs, self.kernel)</code></li>
</ol>
",16787662,13,https://stackoverflow.com/questions/68984841,Documentation Replicability
76324368,Understanding tf.keras.layers.Dense(),"<p>I am trying to understand why there is a difference between calculating a dense layer operation directly and using the <code>keras</code> implementation.</p>
<p>Following the documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense</a>) <code>tf.keras.layers.Dense()</code> should implement the operation <code>output = activation(dot(input, kernel) + bias)</code> but <code>result</code> and <code>result1</code> below are not the same.</p>
<pre class=""lang-py prettyprint-override""><code>tf.random.set_seed(1)

bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32)
kernel = tf.Variable(tf.random.uniform(shape=(5,10)), dtype=tf.float32)
x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32))

result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x) + bias)
tf.print(result)

test = tf.keras.layers.Dense(units = 5, 
                            activation = 'relu',
                            use_bias = True, 
                            kernel_initializer = tf.keras.initializers.Constant(value=kernel), 
                            bias_initializer = tf.keras.initializers.Constant(value=bias), 
                            dtype=tf.float32)

result1 = test(tf.transpose(x))

print()
tf.print(result1)

</code></pre>
<p>output</p>
<pre class=""lang-py prettyprint-override""><code>
[[2.87080455]
 [3.25458574]
 [3.28776264]
 [3.14319134]
 [2.04760242]]

[[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]

</code></pre>
<p>Using <code>test.get_weights()</code> I can see that the kernel and bias (<code>b</code>) are getting set to the correct values. I am using TF version 2.12.0.</p>
",2023-05-24 14:00:25,"<p>After some experimentation I realized that the <code>kernel</code> for the dense layer needs to be of <code>shape=(10,5)</code> as apposed to <code>(5,10)</code> as in the code from the original question above. This is implicit because <code>units=5</code> so a vector of size <code>10</code> needs to be passed (hence why <code>input_shape=(10,)</code> is commented out as a reminder). Below is the corrected code:</p>
<pre class=""lang-py prettyprint-override""><code>tf.random.set_seed(1)

bias   = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32)
kernel = tf.Variable(tf.random.uniform(shape=(10,5)), dtype=tf.float32)
x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32))

result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x, transpose_a=True) + bias)
tf.print(result)

test = tf.keras.layers.Dense(units = 5, 
                            # input_shape=(10,),
                            activation = 'relu',
                            use_bias = True, 
                            kernel_initializer = tf.keras.initializers.Constant(value=kernel), 
                            bias_initializer = tf.keras.initializers.Constant(value=bias), 
                            dtype=tf.float32)

result1 = test(tf.transpose(x))

print()
tf.print(result1)

</code></pre>
<pre class=""lang-py prettyprint-override""><code>[[2.38769]
 [3.63470697]
 [2.62423944]
 [3.31286287]
 [2.91121125]]

[[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]
</code></pre>
<p>Ultimately, I am not entirely sure what was happening under the hood and why <code>keras</code> did not raise an error. I will check with the <code>tf.keras.layers.Dense()</code> implementation but any thoughts or suggestions by someone who knows the code already are highly appreciated!</p>
",18338104,5,https://stackoverflow.com/questions/76324368,Documentation Replication on Other Examples
74088086,Seed in tensorflow initializer (tf.keras.initializers) doesn't guarantees reproducible results,"<p>looking at tensorflow documentation (see, e.g., <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal</a>) a seed should guarantee that &quot;multiple initializers will produce the same sequence when constructed with the same seed value&quot;</p>
<p>The following easy experiment says otherwise</p>
<pre><code>import tensorflow as tf
initializer = tf.keras.initializers.GlorotNormal(seed=123)
values = initializer(shape=(2, 2))
print(values)

initializer1 = tf.keras.initializers.GlorotNormal(seed=123)
values1 = initializer1(shape=(2, 2))
print(values1)
</code></pre>
<p>Giving the output</p>
<pre><code>tf.Tensor(
[[-0.58071285 -0.06369764]
 [ 0.06184607 -1.2040431 ]], shape=(2, 2), dtype=float32)
tf.Tensor(
[[ 0.76186    -0.11021858]
 [-1.1184257  -1.430372  ]], shape=(2, 2), dtype=float32)
</code></pre>
<p>Interesting fact, if I run the python script multiple times I always get the same overall results. So the first seed somehow works, but when it is called a second time in the script it 'keeps advancing', although it should be fixed.</p>
<p>Any opinion about that? Do you think it is a bug? Do you think it is the intended behaviour (if yes could you explain me why)? It may be a problem of my TF installation? I have python 3.7.9 on Windows and Tensorflow version is 2.7.0</p>
<p>Of course, the same behaviour applies when inserting an initializer in a tf.keras.layer</p>
<pre><code>x = tf.constant(6, shape=(2,3))
dense = tf.keras.layers.Dense(units=3, kernel_initializer=tf.keras.initializers.GlorotNormal(seed=123))
dense1 = tf.keras.layers.Dense(units=3, kernel_initializer=tf.keras.initializers.GlorotNormal(seed=123))
print(dense(x), '\n', dense1(x))
</code></pre>
<p>giving</p>
<pre><code>tf.Tensor(
[[14.365635   3.3581433 -1.2498709]
 [14.365635   3.3581433 -1.2498709]], shape=(2, 3), dtype=float32)
 tf.Tensor(
[[10.644517  8.859441  5.136632]
 [10.644517  8.859441  5.136632]], shape=(2, 3), dtype=float32)
</code></pre>
<p>Thanks in advance for your time!</p>
",2022-10-16 14:53:17,"<p>If you go to the link you send you can read:</p>
<p><code>Note that a seeded initializer will not produce the same random values across multiple calls, but multiple initializers will produce the same sequence when constructed with the same seed value.</code></p>
<p>So yes is deterministic but not return the same value in a single build note that keras and tensorflow are keeping track of the calls you make if you want to do this in a single script you need to reset the backend for keras and is recommended use <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed"" rel=""nofollow noreferrer""><code>tf.keras.utils.set_random_seed</code></a> to set the seed, here an example how to do this</p>
<pre><code>import tensorflow as tf

seed = 123

tf.keras.utils.set_random_seed(
    seed
)
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))
print(values)
tf.keras.backend.clear_session()
tf.keras.utils.set_random_seed(
    seed
)

initializer1 = tf.keras.initializers.GlorotNormal()
values1 = initializer1(shape=(2, 2))
print(values1)
</code></pre>
<p>This will print :</p>
<pre><code>tf.Tensor(
[[-0.7219447  -1.4678022 ]
 [-0.35725543 -1.1963991 ]], shape=(2, 2), dtype=float32)
tf.Tensor(
[[-0.7219447  -1.4678022 ]
 [-0.35725543 -1.1963991 ]], shape=(2, 2), dtype=float32)
</code></pre>
",17788510,168,https://stackoverflow.com/questions/74088086,Documentation Replication on Other Examples
71019644,Equivalent tensorflow expression to numpy mask,"<p>I have a numpy array named PixelData of unknown shape, and I am using the following condition to filter values in the array greater than some value x using a mask:</p>
<pre><code>PixelData[PixelData&gt;=x] = PixelData[PixelData&gt;=x] - x
</code></pre>
<p>When I convert this numpy array to a tensor, I cannot perform the same masking operation. I have tried using tf.where as follows:</p>
<pre><code>PixelData = tf.where(PixelData&gt;=x, PixelData - x, PixelData)
</code></pre>
<p>In the official documentation, they always seem to define the mask dimensions in advance to equal the dimensions of the tensor being masked, but then they talk about the dimensions being broadcasted automatically, so I am a bit confused. Are these two functions equivalent? Are there any situations where they may produce different outputs?</p>
",2022-02-07 13:47:54,"<p>Not sure what <code>PixelData</code> looks like, but here is working example with both methods:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf

x = 2
np_pixel_data = np.array([[3, 4, 5, 1],
                       [6, 4, 2, 5]], dtype=np.float32)

np_pixel_data[np_pixel_data&gt;=x] = np_pixel_data[np_pixel_data&gt;=x] - x

tf_pixel_data = tf.constant([[3, 4, 5, 1],
                       [6, 4, 2, 5]], dtype=tf.float32)

tf_pixel_data = tf.where(tf.greater_equal(tf_pixel_data, x), tf_pixel_data - x, tf_pixel_data)

print(np_pixel_data)
print(tf_pixel_data)
</code></pre>
<pre><code>[[1. 2. 3. 1.]
 [4. 2. 0. 3.]]
tf.Tensor(
[[1. 2. 3. 1.]
 [4. 2. 0. 3.]], shape=(2, 4), dtype=float32)
</code></pre>
<p>You might have some minor rounding differences, but nothing significant.</p>
",17815854,45,https://stackoverflow.com/questions/71019644,Documentation Ambiguity
73213159,How to apply tf.data transformations to a DataFrame,"<p>I want to apply tf.data transformations to a panda  dataframe. According to the tensorflow docs <a href=""https://www.tensorflow.org/tutorials/load_data/pandas_dataframe"" rel=""nofollow noreferrer"">HERE</a> I can apply tf.data to a dataframe directly but the dtype of the dataframe should be uniform.</p>
<p>When I apply tf.data to my dataframe like below</p>
<pre><code>tf.data.Dataset.from_tensor_slices(df['reports'])
</code></pre>
<p>it generates this error</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
</code></pre>
<p>When I print <code>df['reports'].dtype</code> it is <code>dtype('O')</code> which seems to be not uniformed, if this is the case then how can I convert this dataframe to uniform <code>dtype</code>.</p>
",2022-08-02 20:02:51,"<p>Try using a ragged structure:</p>
<pre><code>import tensorflow as tf
import pandas as pd

df = pd.DataFrame(data={'reports': [[2.0, 3.0, 4.0], [2.0, 3.0], [2.0]]})

dataset = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(df['reports']))

for x in dataset:
  print(x)
</code></pre>
<pre><code>tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)
tf.Tensor([2. 3.], shape=(2,), dtype=float32)
tf.Tensor([2.], shape=(1,), dtype=float32)
</code></pre>
",19443650,168,https://stackoverflow.com/questions/73213159,Documentation Replicability
74029376,Tensorflow custom reduction function with axis support,"<p>I would like to get the value with the maximum absolute value in a tensor, with respect to an axis. Note that I don't want the maximum absolute value, I want the <em>value that has the maximum absolute value</em> (so I need to keep the sign).</p>
<p>Ideally, I would like something similar to <code>reduce_max</code> or <code>reduce_min</code>:</p>
<pre class=""lang-py prettyprint-override""><code>tensor = tf.constant(
  [
    [[ 1,  5, -3],
     [ 2, -3,  1],
     [ 3, -6,  2]],

    [[-2,  3, -5],
     [-1,  4,  2],
     [ 4, -1,  0]]
   ]
)
# tensor.shape = (2, 3, 3)

tensor.reduce_maxamplitude(tensor, axis=0)
# Tensor(
#  [[-2,  5, -5],
#   [ 2,  4,  2],
#   [ 4, -6,  2]]
# )
# shape: (3, 3)

tensor.reduce_maxamplitude(tensor, axis=1)
# Tensor(
#  [[3, -6, -3],
#   [4,  4, -5]]
# )
# shape: (2, 3)

tensor.reduce_maxamplitude(tensor, axis=2)
# Tensor(
#  [[5, -3, -6],
#   [-5,  4, 4]]
# )
# shape: (2, 3)
</code></pre>
<p>but I did not find anything useful in tensorflow documentation.</p>
<p>With a flat tensor, I know that I could use <code>tf.foldl</code> or <code>tf.foldr</code>:</p>
<pre class=""lang-py prettyprint-override""><code>flat = tf.reshape(tensor, -1)
tf.foldr(lambda a, x: x if tf.abs(x) &gt; tf.abs(a) else a, flat)
# -6
</code></pre>
<p>However, I don't know how to handle an axis parameter in the case of multidimensional tensors.</p>
",2022-10-11 13:57:31,"<p>It really depends on how many dimensions your tensor has, but for a 2D tensor you could just do:</p>
<pre><code>import tensorflow as tf

tensor = tf.constant(
  [[1,  5, -3],
   [2, -3,  1],
   [3, -6,  2]])

tf.gather(tensor, tf.argmax(tf.abs(tensor), axis=1), axis=1, batch_dims=1)
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 5, -3, -6], dtype=int32)&gt;
</code></pre>
<p>3D example:</p>
<pre><code>tensor = tf.constant(
  [
    [[ 1,  5, -3],
     [ 2, -3,  1],
     [ 3, -6,  2]],

    [[-2,  3, -5],
     [-1,  4,  2],
     [ 4, -1,  0]]
   ]
)

# axis = 0
argmax = tf.argmax(tf.abs(tensor), axis=0)
i, j = tf.meshgrid(
    tf.range(tensor.shape[1], dtype=tf.int64), 
    tf.range(tensor.shape[2], dtype=tf.int64),
                              indexing='ij')
tf.gather_nd(tensor, tf.stack([argmax, i, j], axis=-1))
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=
array([[-2,  5, -5],
       [ 2,  4,  2],
       [ 4, -6,  2]], dtype=int32)&gt;
</code></pre>
<pre><code># axis = 1
argmax = tf.argmax(tf.abs(tensor), axis=1)
i, j = tf.meshgrid(
    tf.range(tensor.shape[0], dtype=tf.int64), 
    tf.range(tensor.shape[2], dtype=tf.int64),
                              indexing='ij')
tf.gather_nd(tensor, tf.stack([i, argmax, j], axis=-1))
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[ 3, -6, -3],
       [ 4,  4, -5]], dtype=int32)&gt;
</code></pre>
<pre><code># axis = 2
i, j = tf.meshgrid(
    tf.range(tensor.shape[0], dtype=tf.int64), 
    tf.range(tensor.shape[1], dtype=tf.int64),
                              indexing='ij')
tf.gather_nd(tensor, tf.stack([i, j, argmax], axis=-1))
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[ 5, -3, -6],
       [-5,  4,  4]], dtype=int32)&gt;
</code></pre>
<p>For a 4D tensor just extend the <code>meshgrid</code>:</p>
<pre><code># axis=-1
i, j, k = tf.meshgrid(
    tf.range(tensor.shape[0], dtype=tf.int64), 
    tf.range(tensor.shape[1], dtype=tf.int64),
    tf.range(tensor.shape[2], dtype=tf.int64),
                              indexing='ij')
</code></pre>
<p>Quick function bundling everything by @leleogere</p>
<pre><code>def reduce_maxamplitude(tensor, axis):
    argmax = tf.argmax(tf.abs(tensor), axis=axis)
    mesh = tf.meshgrid(
        *[tf.range(tensor.shape[i], dtype=tf.int64) for i in range(tensor.shape.rank) if i != axis],
        indexing='ij'
    )
    return tf.gather_nd(tensor, tf.stack([*mesh[:axis], argmax, *mesh[axis:]], axis=-1))
</code></pre>
",18159603,955,https://stackoverflow.com/questions/74029376,Lack of Alternative Solutions/Documentation
