QuestionId,Title,Body,CleanedBody,QuestionURL,UserId,Issue Type
49868782,How to use tf.argmax,"<p>I want to test the function of tf.argmax(),but when I run the code , I encountered an error. Here is my code</p>
<pre><code>import tensorflow as tf
 
a=tf.argmax([1,0,0],1)
with tf.Session() as sess:
    print(sess.run(a))
</code></pre>
<p>My environment is python3 + tf1.3.</p>
<p>What's wrong with the code?</p>
","I want to test the function of tf.argmax(),but when I run the code , I encountered an error. Here is my code My environment is python3 + tf1.3. What's wrong with the code?",https://stackoverflow.com/questions/49868782,9639109,Documentation Replication on Other Examples
55298323,TensorFlow 2.0 returns unexpected output on dtype=int32 with GradientTape,"<p>The following code should output the gradient of y=x*x for x=2, i.e. the value of 4. However the code prints a value of None when using TensorFlow 2.0.0-alpha0. When the definition of x changes to use <code>tf.float32</code> instead of <code>tf.int32</code> as shown in the next snippet, the output changes to the correct value of 4. Is there any documentation that clarifies the requirement for the data type to be a floating point number for GradientTape to work correctly in this scenario?</p>

<pre class=""lang-py prettyprint-override""><code>print(tf.__version__)

x = tf.constant(2, dtype=tf.int32)

with tf.GradientTape() as tape:
  tape.watch(x)
  y = x ** 2
  print(tape.gradient(y, x))
</code></pre>

<p>outputs:</p>

<pre><code>2.0.0-alpha0
None
</code></pre>

<p>Notice the change to <code>tf.float32</code> in the next snippet:</p>

<pre class=""lang-py prettyprint-override""><code>print(tf.__version__)

x = tf.constant(2, dtype=tf.float32)

with tf.GradientTape() as tape:
  tape.watch(x)
  y = x ** 2
  print(tape.gradient(y, x))
</code></pre>

<p>outputs:</p>

<pre><code>2.0.0-alpha0
tf.Tensor(4.0, shape=(), dtype=float32)
</code></pre>
","The following code should output the gradient of y=x*x for x=2, i.e. the value of 4. However the code prints a value of None when using TensorFlow 2.0.0-alpha0. When the definition of x changes to use tf.float32 instead of tf.int32 as shown in the next snippet, the output changes to the correct value of 4. Is there any documentation that clarifies the requirement for the data type to be a floating point number for GradientTape to work correctly in this scenario? outputs: Notice the change to tf.float32 in the next snippet: outputs:",https://stackoverflow.com/questions/55298323,3809616,Requesting (Additional) Resources
49505986,How do i get the VALUES of trainable variables from a restored graph & checkpoint in tensorflow,"<p>I want to get the values of the variables from a trained model.  I have a check point file and I can restore graphs and checkpoints and do inference with them just fine.</p>

<p>However, I'm finding it extremely difficult to figure out how to get the trainable variable values (like the weight and bias values, not names...i want the VALUES) after I restore the checkpoint and graph.  I've read through Tensorflow documentation and there's lots of suggestions regarding ""with variable_scope"", ""reuse = True"", and ""tf.get_variable(""myvar"") within the scope...etc, but I get errors stating either the variable already exists or it hasn't been initialized.  tf.graphkeys only returns names...not values.</p>
","I want to get the values of the variables from a trained model. I have a check point file and I can restore graphs and checkpoints and do inference with them just fine. However, I'm finding it extremely difficult to figure out how to get the trainable variable values (like the weight and bias values, not names...i want the VALUES) after I restore the checkpoint and graph. I've read through Tensorflow documentation and there's lots of suggestions regarding ""with variable_scope"", ""reuse = True"", and ""tf.get_variable(""myvar"") within the scope...etc, but I get errors stating either the variable already exists or it hasn't been initialized. tf.graphkeys only returns names...not values.",https://stackoverflow.com/questions/49505986,3496060,Requesting (Additional) Resources
41156460,tensorflow doing gradients on sparse variable,"<p>I am trying to train a sparse variable in tensorflow, As far as I know current tensorflow doesn't allow for sparse variable. </p>

<p>I found two threads discussing similar issue: <a href=""https://stackoverflow.com/questions/37001686/using-sparsetensor-as-a-trainable-variable"">using-sparsetensor-as-a-trainable-variable</a> and <a href=""https://stackoverflow.com/questions/35803425/update-only-part-of-the-word-embedding-matrix-in-tensorflow"">update-only-part-of-the-word-embedding-matrix-in-tensorflow</a>. I am not quitely understand the answer, and it would be good if there is any example code</p>

<p>one way I have tried is:</p>

<pre><code># initialize the sparse variable sp_weights
# assuming w_s is the input sparse matrix contains indices information
dim=20
identity = tf.constant(np.identity(dim), dtype=tf.float32)
A=tf.sparse_tensor_dense_matmul(w_s, identity)  # convert w_s to dense
w_init = tf.random_normal([dim, dim], mean=0.0, stddev=0.1) 
w_tensor = tf.mul(A, w_init) # random initialize sparse tensor
vars['sp_weights'] = tf.Variable(w_tensor)

# doing some operations...
</code></pre>

<p>when compute the gradients, according to the <a href=""https://stackoverflow.com/questions/35803425/update-only-part-of-the-word-embedding-matrix-in-tensorflow"">second link</a> using <code>tf.IndexedSlices</code> </p>

<pre><code>grad = opt.compute_gradients(loss)
train_op = opt.apply_gradients(
    [tf.IndexedSlices(grad, indices)]) # indices is extracted from w_s
</code></pre>

<p>the above code of course don't work, and I am confused here. tf.IndexedSlices make the input to be IndexedSlices instance, how to use it to update the gradients given the indices? Also, many people mentioned using tf.scatter_add/sub/update. The official document doesn't contain any example code on how to use and where to use for gradient update. should I use tf.IndexedSlices or tf.scatter? it would be much helpful if there is any example code. Thank you!</p>
","I am trying to train a sparse variable in tensorflow, As far as I know current tensorflow doesn't allow for sparse variable. I found two threads discussing similar issue: using-sparsetensor-as-a-trainable-variable and update-only-part-of-the-word-embedding-matrix-in-tensorflow. I am not quitely understand the answer, and it would be good if there is any example code one way I have tried is: when compute the gradients, according to the second link using tf.IndexedSlices the above code of course don't work, and I am confused here. tf.IndexedSlices make the input to be IndexedSlices instance, how to use it to update the gradients given the indices? Also, many people mentioned using tf.scatter_add/sub/update. The official document doesn't contain any example code on how to use and where to use for gradient update. should I use tf.IndexedSlices or tf.scatter? it would be much helpful if there is any example code. Thank you!",https://stackoverflow.com/questions/41156460,6233298,Inadequate Examples
54282753,How to input 2d numpy array into Tensorflow? (also on how to get matrix input and output working with TF),"<p>I'm new to Tensorflow and I'm trying to understand how it processes data. Currently, this is what I want to have as my input. My full code is up on <a href=""https://github.com/stockfish8/PolySolver"" rel=""nofollow noreferrer"">github</a> should you want to download it.</p>

<pre><code>print (y_train[0])
&gt;&gt;&gt; [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 
1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 
1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 
1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 
1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 
0.0, 1.0, 0.0, 1.0, 0.0, 0.0]
# list of 80 elements

print (np.array(y_train))
&gt;&gt;&gt; [[0. 0. 1. ... 1. 0. 0.]
 [0. 1. 0. ... 0. 0. 0.]
 [1. 0. 1. ... 0. 0. 0.]
 ...
 [0. 0. 1. ... 1. 1. 0.]
 [1. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 1. 0. 1.]]

print (np.array(y_train).shape)
&gt;&gt;&gt; (11645, 80)

print (x_train[0])
&gt;&gt;&gt; [1.0, 4.0, 5.0, 2.0, 5.0, 3.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 4.0, 3.0, 
3.0, 4.0, 5.0, 4.0, 4.0, 5.0, 4.0, 3.0, 3.0, 4.0, 4.0, 5.0]

print (np.array(x_train)/5)
&gt;&gt;&gt; [[0.2 0.8 1.  ... 0.8 0.8 1. ]
[0.6 0.8 1.  ... 1.  1.  0.8]
[0.8 0.4 1.  ... 1.  0.6 1. ]
...
[1.  0.6 0.8 ... 0.4 0.8 0.6]
[1.  0.8 0.8 ... 0.4 0.6 1. ]
[0.6 0.8 0.8 ... 1.  0.8 0.6]]

print (np.array(x_train).shape)
&gt;&gt;&gt; (11645, 26)
</code></pre>

<p>So basically I have 11645 pieces of data in my dataset. For the input, I wish to have 26 inputs normalized from 0 to 1. For the output, I wish to have 80 binary outputs. I don't think TF can give binary outputs, so I probably will use a sigmoid activation function. </p>

<p>How do I get Tensorflow to understand that I have 11645 pieces of data I want to process and that the input shape should be 26x1 and the output 80x1? There are some pieces of Tensorflow and Keras that I don't understand how they fit together. For instance, if I want Tensorflow to understand that my input should be 1x26 and not some other input shape, should I use <code>x_train = tf.reshape(x_train, [-1,1*26])</code> and <code>y_train = tf.reshape(y_train, [-1,1*80])</code>? From the documentations it seems like it will shape x_train into a tensor of only 1 row and 26 columns, and I will have 11645 of those. But does that specify to Tensorflow that the input should only be 1x26 and it won't go off grabbing some other number (eg. 26x2). Or do I have to do something more explicit like this where I specify the input shape into the model? <code>model.add(tf.keras.layers.Dense(26, activation=keras.activations.relu, input_shape=(26,)))</code>? </p>

<p>Again, for my output, I want to have a 1x80 tensor that I can reshape and stuff. Do I have to specify to tensorflow explicitly? Or will something like <code>model.add(tf.keras.layers.Dense(80, activation=keras.activations.sigmoid))</code> be enough to tell Tensorflow that I want a 1x80 matrix, and (for eg, using the sigmoid function) that it should compare every piece of data in that predicted 1x80 with the 1x80 matrix I have in y_train to calculate the loss function?</p>

<p>Basically, I am confused as to how Tensorflow 'knows' what data to accept as an individual input and output. Is there a way to specify it or is it a step one can omit?</p>

<p>EDIT: Based on the answers, I have used the code:</p>

<pre><code>model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(26, input_dim=26,activation='relu'))
model.add(tf.keras.layers.Dense(80, activation='sigmoid'))
model.compile(optimizer='rmsprop',
      loss='binary_crossentropy',
      metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10, batch_size=32)
</code></pre>

<p>I'm getting the following matrix:</p>

<pre><code>[0.38176608 0.34900635 0.36545524 0.36806932 0.36692804 0.37398493
  0.36821148 0.35577637 0.38441166 0.3676901  0.41162464 0.40428266
  0.41464344 0.4040607  0.39316037 0.428753   0.3547327  0.35693064
  0.3422352  0.36919317 0.36431065 0.3515264  0.3889933  0.33974153
  0.37329385 0.35898593 0.3891792  0.42334762 0.40694237 0.41910493
  0.39983115 0.47813386 0.37625512 0.35567597 0.36811477 0.38242644
  0.36549032 0.35696995 0.37058106 0.3556903  0.37096408 0.34965912
  0.4247738  0.41512045 0.41622216 0.38645518 0.40850884 0.43454456
  0.3655926  0.34644917 0.36782715 0.34224963 0.35035127 0.3502
  0.3607877  0.38218996 0.37265536 0.3653391  0.41620222 0.41124558
  0.3916335  0.41291553 0.39959764 0.4649614  0.34603494 0.36731967
  0.34146535 0.34573284 0.33941117 0.35885242 0.3493014  0.35866526
  0.37188208 0.34971312 0.38165745 0.3962399  0.38913697 0.4078925
  0.38799426 0.4709055 ]
</code></pre>

<p>This is a far cry from the 0 and 1 matrix I want. What should I do to get closer to that? I've tried Googling my problem, but to no avail. Should I simply apply a threshold to this (eg. 0.4?) and convert it to a binary matrix that way? </p>
","I'm new to Tensorflow and I'm trying to understand how it processes data. Currently, this is what I want to have as my input. My full code is up on github should you want to download it. So basically I have 11645 pieces of data in my dataset. For the input, I wish to have 26 inputs normalized from 0 to 1. For the output, I wish to have 80 binary outputs. I don't think TF can give binary outputs, so I probably will use a sigmoid activation function. How do I get Tensorflow to understand that I have 11645 pieces of data I want to process and that the input shape should be 26x1 and the output 80x1? There are some pieces of Tensorflow and Keras that I don't understand how they fit together. For instance, if I want Tensorflow to understand that my input should be 1x26 and not some other input shape, should I use x_train = tf.reshape(x_train, [-1,1*26]) and y_train = tf.reshape(y_train, [-1,1*80])? From the documentations it seems like it will shape x_train into a tensor of only 1 row and 26 columns, and I will have 11645 of those. But does that specify to Tensorflow that the input should only be 1x26 and it won't go off grabbing some other number (eg. 26x2). Or do I have to do something more explicit like this where I specify the input shape into the model? model.add(tf.keras.layers.Dense(26, activation=keras.activations.relu, input_shape=(26,)))? Again, for my output, I want to have a 1x80 tensor that I can reshape and stuff. Do I have to specify to tensorflow explicitly? Or will something like model.add(tf.keras.layers.Dense(80, activation=keras.activations.sigmoid)) be enough to tell Tensorflow that I want a 1x80 matrix, and (for eg, using the sigmoid function) that it should compare every piece of data in that predicted 1x80 with the 1x80 matrix I have in y_train to calculate the loss function? Basically, I am confused as to how Tensorflow 'knows' what data to accept as an individual input and output. Is there a way to specify it or is it a step one can omit? EDIT: Based on the answers, I have used the code: I'm getting the following matrix: This is a far cry from the 0 and 1 matrix I want. What should I do to get closer to that? I've tried Googling my problem, but to no avail. Should I simply apply a threshold to this (eg. 0.4?) and convert it to a binary matrix that way?",https://stackoverflow.com/questions/54282753,9721336,Requesting (Additional) Resources
44141986,Clarification on Tensorflow tensor shapes and matmul,"<p>I need some clarification on how Tensorflow treats the shape of its tensors. This is taken from the <a href=""https://www.tensorflow.org/get_started/mnist/pros"" rel=""nofollow noreferrer"">MNIST example</a>:</p>

<p>I define a placeholder that will at some later point be fed with some of my training data:</p>

<p><code>x = tf.placeholder(tf.float32, shape=[None, 784])</code></p>

<p>During runtime I feed it in batches of 100, so its shape during runtime is <code>(100, 784)</code>. I also define weights and biases: </p>

<pre><code>W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))
</code></pre>

<p><code>W</code>is of shape <code>(784, 10)</code> and <code>b</code>is of shape <code>(10)</code>. Now I compute </p>

<pre><code>y = tf.matmul(x,W) + b
</code></pre>

<p>And this is where I am stuck. The matrix product of <code>x</code> and <code>W</code> is of shape <code>(None, 10)</code> or <code>(100, 10)</code> during runtime. However I can without error add vector <code>b</code> to it. This confuses me. How can this work? And is there some better documentation for this?</p>
","I need some clarification on how Tensorflow treats the shape of its tensors. This is taken from the MNIST example: I define a placeholder that will at some later point be fed with some of my training data: x = tf.placeholder(tf.float32, shape=[None, 784]) During runtime I feed it in batches of 100, so its shape during runtime is (100, 784). I also define weights and biases: Wis of shape (784, 10) and bis of shape (10). Now I compute And this is where I am stuck. The matrix product of x and W is of shape (None, 10) or (100, 10) during runtime. However I can without error add vector b to it. This confuses me. How can this work? And is there some better documentation for this?",https://stackoverflow.com/questions/44141986,578640,Requesting (Additional) Resources
38902433,TensorFlow strings: what they are and how to work with them,"<p>When I read file with <code>tf.read_file</code> I get something with type <code>tf.string</code>. Documentation says only that it is ""Variable length byte arrays. Each element of a Tensor is a byte array."" (<a href=""https://www.tensorflow.org/versions/r0.10/resources/dims_types.html"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.10/resources/dims_types.html</a>). I have no idea how to interpret this.</p>

<p>I can do nothing with this type. In usual python you can get elements by index like <code>my_string[:4]</code>, but when I run following code I get an error.</p>

<pre><code>import tensorflow as tf
import numpy as np

x = tf.constant(""This is string"")
y = x[:4]


init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
result = sess.run(y)
print result
</code></pre>

<p>It says </p>

<pre>  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 621, in assert_has_rank
    raise ValueError(""Shape %s must have rank %d"" % (self, rank))
ValueError: Shape () must have rank 1
</pre>

<p>Also I cannot convert my string to <code>tf.float32</code> tensor. It is <code>.flo</code> file and it has magic header ""PIEH"". This numpy code successfuly convert such header into number (see example here <a href=""https://stackoverflow.com/a/28016469/4744283"">https://stackoverflow.com/a/28016469/4744283</a>) but I can't do that with tensorflow. I tried <code>tf.string_to_number(string, out_type=tf.float32)</code> but it says </p>

<pre>tensorflow.python.framework.errors.InvalidArgumentError: StringToNumberOp could not correctly convert string: PIEH
</pre>

<p>So, what string is? What it's shape is? How can I at least get part of the string? I suppose that if I can get part of it I can just skip ""PIEH"" part.</p>

<p><strong>UPD</strong>: I forgot to say that <code>tf.slice(string, [0], [4])</code> also doesn't work with same error.</p>
","When I read file with tf.read_file I get something with type tf.string. Documentation says only that it is ""Variable length byte arrays. Each element of a Tensor is a byte array."" (https://www.tensorflow.org/versions/r0.10/resources/dims_types.html). I have no idea how to interpret this. I can do nothing with this type. In usual python you can get elements by index like my_string[:4], but when I run following code I get an error. It says Also I cannot convert my string to tf.float32 tensor. It is .flo file and it has magic header ""PIEH"". This numpy code successfuly convert such header into number (see example here https://stackoverflow.com/a/28016469/4744283) but I can't do that with tensorflow. I tried tf.string_to_number(string, out_type=tf.float32) but it says So, what string is? What it's shape is? How can I at least get part of the string? I suppose that if I can get part of it I can just skip ""PIEH"" part. UPD: I forgot to say that tf.slice(string, [0], [4]) also doesn't work with same error.",https://stackoverflow.com/questions/38902433,4744283,Documentation Ambiguity
36521908,How to Iteratively Create Tensorflow Graphs On The Fly Without Accumulating Memory?,"<p>What is the idiomatic way to construct a new neural network during each iteration of the training loop? It is an unusual thing to do, but I am working on an unusual project.</p>

<p>The tensorflow API documentation says ""A session may own resources, such as variables, queues, and readers. It is important to release these resources when they are no longer required. To do this, either invoke the close() method on the session, or use the session as a context manager."", so my first attempt involved declaring all my tensorflow variables within a session context manager, so that they can be freed at the next iteration of the training loop:</p>

<pre><code>with tf.device('/gpu:1'):
     while step * train_batchsize &lt; training_iters:
         with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False)) as sess:
             X_placeholder = tf.placeholder(tf.float32, [None, 227 * 227 * 3])

             y_placeholder = tf.placeholder(tf.float32, [None, nclasses])
             # mkoptimize is my function which constructs the neural network.
             weights,biases,optimizer,accuracy,cost,conv1,conv2,dense1 = mkoptimize(X_placeholder,y_placeholder,table,npyweights,npybiases,nclasses)
             feed = {X_placeholder : X.reshape((train_batchsize,227 * 227 * 3)), y_placeholder : gt, keep_prob: dropout}
             sess.run(tf.initialize_all_variables())
             sess.run(optimizer, feed_dict=feed)
             npyweights = {k : sess.run(v).flatten() for k,v in weights.items()}
             npybiases = {k : sess.run(v).flatten() for k,v in biases.items()}
             # manually modify npyweights and npybiases according to some ensemble methods, and do some other things of interest.
             feed_acc = {X_placeholder: xs.reshape((test_batchsize,227 * 227 * 3)), y_placeholder: gt, keep_prob: 1.}
             sess.run(tf.initialize_all_variables())
             acc = sess.run(accuracy,feed_dict=feed_acc)
</code></pre>

<p>The first iteration of the training loop executes correctly, but on the second iteration the call to <code>tf.initialize_all_variables()</code> leads to a memory error ""ValueError: GraphDef cannot be larger than 2GB."" Does exiting the session context not in fact free memory in the graph? Each iteration of the loop involves the same amount of data and the same number of parameters in the neural network, so I don't think it's my construction that leads to the memory error.</p>

<p>The documentation is not very detailed on what exactly <code>tf.reset_default_graph</code> does, but I've tried freeing memory by calling  at the end of the session, leading to the error:</p>

<pre><code>File ""/usr/local/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 706, in __exit__
   context_manager.__exit__(exec_type, exec_value, exec_tb)
   File ""/usr/local/anaconda3/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
   File ""/usr/local/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2978, in get_controller
    assert self.stack[-1] is default
IndexError: list index out of range
</code></pre>

<p>How can I iteratively create tensorflow graphs without accumulating memory?</p>
","What is the idiomatic way to construct a new neural network during each iteration of the training loop? It is an unusual thing to do, but I am working on an unusual project. The tensorflow API documentation says ""A session may own resources, such as variables, queues, and readers. It is important to release these resources when they are no longer required. To do this, either invoke the close() method on the session, or use the session as a context manager."", so my first attempt involved declaring all my tensorflow variables within a session context manager, so that they can be freed at the next iteration of the training loop: The first iteration of the training loop executes correctly, but on the second iteration the call to tf.initialize_all_variables() leads to a memory error ""ValueError: GraphDef cannot be larger than 2GB."" Does exiting the session context not in fact free memory in the graph? Each iteration of the loop involves the same amount of data and the same number of parameters in the neural network, so I don't think it's my construction that leads to the memory error. The documentation is not very detailed on what exactly tf.reset_default_graph does, but I've tried freeing memory by calling at the end of the session, leading to the error: How can I iteratively create tensorflow graphs without accumulating memory?",https://stackoverflow.com/questions/36521908,1483516,Documentation Completeness
65725030,How can I manage Queues in Tensorflow 2.0?,"<p>Well, I'm trying to understand the Threading and Queues.</p>
<p>I saw many documents on the web, but surprisingly there is not even a single example of this topic in tensorflow 2.0.</p>
<p>What I want my queues to do is to,</p>
<ol>
<li>Define an operation that generates examples.</li>
<li>Define a queue.</li>
<li>Define an enqueue_operation that enqueue examples in the queue made above using multiple threads.</li>
<li>Control this queue to dequeue batches.</li>
</ol>
<p>What I have in mind is,</p>
<pre><code>import tensorflow as tf
import threading

batch_size = 2
example = tf.random.normal([1, 2]) # Generate an example, shape = [1, 2]
queue = tf.queue.RandomShuffleQueue(capacity=10, min_after_dequeue=0, \
    dyptes=tf.float32, shapes=[1, 2])
enqueue_op = queue.enqueue(example)
# inputs = queue.dequeue(2) # Don't run this. This would stop your computer.
</code></pre>
<p>I have no idea what I'm doing. I also learned that how to manage multiple threads using <code>tf.train.Coordinator()</code> but I don't know where to use this..</p>
<p>While asking this, I have a suspicion that many APIs in the <code>tf.data.Dataset</code> replace all of these and multiple threads can be replaced with the <code>tf.data.experimental.AUTOTUNE</code>.</p>
<p>Sorry for all the mess here. I can't arrange this properly even during asking. <br/>
Any comments will be appreciated. Thanks in advance.</p>
","Well, I'm trying to understand the Threading and Queues. I saw many documents on the web, but surprisingly there is not even a single example of this topic in tensorflow 2.0. What I want my queues to do is to, What I have in mind is, I have no idea what I'm doing. I also learned that how to manage multiple threads using tf.train.Coordinator() but I don't know where to use this.. While asking this, I have a suspicion that many APIs in the tf.data.Dataset replace all of these and multiple threads can be replaced with the tf.data.experimental.AUTOTUNE. Sorry for all the mess here. I can't arrange this properly even during asking. Any comments will be appreciated. Thanks in advance.",https://stackoverflow.com/questions/65725030,7820717,Inadequate Examples
71315426,Using TFDS datasets with Keras Functional API,"<p>I'm trying to train a neural network made with the Keras Functional API with one of the default TFDS Datasets, but I keep getting dataset related errors.</p>
<p>The idea is doing a model for object detection, but for the first draft I was trying to do just plain image classification (img, label). The input would be (256x256x3) images. The input layer is as follows:</p>
<pre><code>img_inputs = keras.Input(shape=[256, 256, 3], name='image')
</code></pre>
<p>Then I'm trying to use the voc2007 dataset as available in TFDS (a very old and light version to make it faster)</p>
<pre><code>(train_ds, test_ds), ds_info = tfds.load(
'voc/2007',
split=['train', 'test'],
data_dir=&quot;/content/drive/My Drive&quot;,
with_info=True)
</code></pre>
<p>then preprocessing the data as follows:</p>
<pre><code>def resize_and_normalize_img(example):
  &quot;&quot;&quot;Normalizes images: `uint8` -&gt; `float32`.&quot;&quot;&quot;
  example['image'] = tf.image.resize(example['image'], [256, 256])
  example['image'] = tf.cast(example['image'], tf.float32) / 255.
  return example

def reduce_for_classification(example):
        for key in ['image/filename', 'labels_no_difficult', 'objects']:
            example.pop(key)
        return example

train_ds_class = train_ds.map(reduce_for_classification, num_parallel_calls=tf.data.AUTOTUNE)
train_ds_class = train_ds_class.map(resize_and_normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
train_ds_class = train_ds_class.cache()
train_ds_class = train_ds_class.shuffle(ds_info.splits['train'].num_examples)
train_ds_class = train_ds_class.batch(64)
train_ds_class = train_ds_class.prefetch(tf.data.AUTOTUNE)

test_ds_class = test_ds.map(reduce_for_classification, num_parallel_calls=tf.data.AUTOTUNE)
test_ds_class = test_ds_class.map(resize_and_normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
test_ds_class = test_ds_class.batch(64)
test_ds_class = test_ds_class.cache()
test_ds_class = test_ds_class.prefetch(tf.data.AUTOTUNE)
</code></pre>
<p>And then fitting the model like:</p>
<pre><code>epochs=8
history = model.fit(
  x=train_x, y =trian_y,
  validation_data=test_ds_clas,
  epochs=epochs
)
</code></pre>
<p>And after doing this is when I get an error saying that my model expects an input of shape [None, 256, 256, 3] but it's getting an input of shape [256, 256, 3].</p>
<p>I think it's an issue to do with the label. Before I got problems with the extra keys from the dictionary-like format of the data you get from tfds and tried to remove everything except the label, but now I'm still getting this and don't know how to go forward. I feel like after getting the dataset prepared with tfds it should be ready to be fed to a model, and after looking through the documentation, tutorials and stack overflow I haven't found the answer, I hope someone who comes across this can help.</p>
<p><strong>Update:</strong>
To give a bit more of information, this is the model I'm using:</p>
<p><strong>TLDR:</strong> Image input 256x256x3, a succession of convolutions and residual blocks, and an ending with average pooling, fully connected layer, and softmax that results in a (None, 1280) tensor. Using sparse categorical cross-entropy as loss and accuracy as metric.</p>
<pre><code>img_inputs = keras.Input(shape=[256, 256, 3], name='image')

# first convolution
conv_first = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same', name='first_conv')
x = conv_first(img_inputs)

# Second convolution
x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=2, padding='same', name='second_conv')(x)

# First residual block
res = tf.keras.layers.Conv2D(32, kernel_size=(1, 1), name='res_block1_conv1')(x)
res = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', name='res_block1_conv2')(res)
x = x + res

# Convolution after First residual block
x = tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same', name='first_post_res_conv')(x)

# Second residual Block
for i in range(2):
  shortcut = x
  res = tf.keras.layers.Conv2D(64, kernel_size=1, name=f'res_block2_conv1_loop{i}')(x)
  res = tf.keras.layers.Conv2D(128, kernel_size=3, padding='same', name=f'res_block2_conv2_loop{i}')(res)

  x = res + shortcut

# Convolution after Second residual block
x = tf.keras.layers.Conv2D(256, 3, strides=2, padding='same', name='second_post_res_conv')(x)

# Third residual Block
for i in range(8):
  shortcut = x
  res = tf.keras.layers.Conv2D(128, kernel_size=1, name=f'res_block3_conv1_loop{i}')(x)
  res = tf.keras.layers.Conv2D(256, kernel_size=3, padding='same', name=f'res_block3_conv2_loop{i}')(res)

  x = res + shortcut

# Convolution after Third residual block
x = tf.keras.layers.Conv2D(512, 3, strides=2, padding='same', name='third_post_res_conv')(x)

# Fourth residual Block
for i in range(8):
  shortcut = x
  res = tf.keras.layers.Conv2D(256, kernel_size=1, name=f'res_block4_conv1_loop{i}')(x)
  res = tf.keras.layers.Conv2D(512, kernel_size=3, padding='same', name=f'res_block4_conv2_loop{i}')(res)

  x = res + shortcut

# Convolution after Fourth residual block
x = tf.keras.layers.Conv2D(1024, 3, strides=2, padding='same', name='fourth_post_res_conv')(x)

# Fifth residual Block
for i in range(4):
  shortcut = x
  res = tf.keras.layers.Conv2D(512, kernel_size=1, name=f'res_block5_conv1_loop{i}')(x)
  res = tf.keras.layers.Conv2D(1024, kernel_size=3, padding='same', name=f'res_block5_conv2_loop{i}')(res)

  x = res + shortcut

# Global avg pooling
x = tf.keras.layers.GlobalAveragePooling2D(name='average_pooling')(x)

# Fully connected layer
x = tf.keras.layers.Dense(1280, name='fully_connected_layer')(x)

# Softmax
end_result = tf.keras.layers.Softmax(name='softmax')(x)

model = tf.keras.Model(inputs=img_inputs, outputs=end_result, name=&quot;darknet53&quot;)

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

</code></pre>
<p>After trying the solution proposed by AloneTogether I'm getting the following errors (I tried changing the axis in the tf.one_hot() function many times and same result):</p>
<pre><code>Node: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'
logits and labels must have the same first dimension, got logits shape [64,1280] and labels shape [1280]
     [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_20172]

</code></pre>
<p>Which seems to be related to the batching, but don't know exactly how to fix it.</p>
<p>The whole issue really seems related to the labels encoding, because when running that line without the tf.reduce_sum() function I get the same but with:</p>
<pre><code>First element had shape [2,20] and element 1 had shape [1,20].
</code></pre>
<p>And if I run the same without the one-hot encoding line, I get this error:</p>
<p>´´´
Node: 'IteratorGetNext'
Cannot batch tensors with different shapes in component 1. First element had shape [4] and element 1 had shape [1].
[[{{node IteratorGetNext}}]] [Op:__inference_train_function_18534]
´´´</p>
","I'm trying to train a neural network made with the Keras Functional API with one of the default TFDS Datasets, but I keep getting dataset related errors. The idea is doing a model for object detection, but for the first draft I was trying to do just plain image classification (img, label). The input would be (256x256x3) images. The input layer is as follows: Then I'm trying to use the voc2007 dataset as available in TFDS (a very old and light version to make it faster) then preprocessing the data as follows: And then fitting the model like: And after doing this is when I get an error saying that my model expects an input of shape [None, 256, 256, 3] but it's getting an input of shape [256, 256, 3]. I think it's an issue to do with the label. Before I got problems with the extra keys from the dictionary-like format of the data you get from tfds and tried to remove everything except the label, but now I'm still getting this and don't know how to go forward. I feel like after getting the dataset prepared with tfds it should be ready to be fed to a model, and after looking through the documentation, tutorials and stack overflow I haven't found the answer, I hope someone who comes across this can help. Update: To give a bit more of information, this is the model I'm using: TLDR: Image input 256x256x3, a succession of convolutions and residual blocks, and an ending with average pooling, fully connected layer, and softmax that results in a (None, 1280) tensor. Using sparse categorical cross-entropy as loss and accuracy as metric. After trying the solution proposed by AloneTogether I'm getting the following errors (I tried changing the axis in the tf.one_hot() function many times and same result): Which seems to be related to the batching, but don't know exactly how to fix it. The whole issue really seems related to the labels encoding, because when running that line without the tf.reduce_sum() function I get the same but with: And if I run the same without the one-hot encoding line, I get this error: ´´´ Node: 'IteratorGetNext' Cannot batch tensors with different shapes in component 1. First element had shape [4] and element 1 had shape [1]. [[{{node IteratorGetNext}}]] [Op:__inference_train_function_18534] ´´´",https://stackoverflow.com/questions/71315426,15934211,Lack of Alternative Solutions/Documentation
43736089,How to use tf.contrib.seq2seq.BahdanauAttention,"<p>I am trying to produce a simple code for a seq2seq model with attention in tf 1.1. I am not sure what is the parameter ""depth of query mechanism "". I am getting an error on creation of Attention Mechanisms saying that: </p>

<pre class=""lang-html prettyprint-override""><code>TypeError: int() argument must be a string, a bytes-like object or a number, not 'TensorShape'
</code></pre>



<p>Here is my code. Am I on a right track? I could not find any detailed documentation.</p>



<pre class=""lang-html prettyprint-override""><code>import tensorflow as tf
from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, BasicLSTMCell, DropoutWrapper, MultiRNNCell, EmbeddingWrapper, static_rnn 
import tensorflow.contrib.seq2seq as seq2seq
import attention_wrapper as wrapper


tf.reset_default_graph()
try:
    sess.close()
except:

    pass
sess = tf.InteractiveSession()


## Place holders

encode_input = [tf.placeholder(tf.int32, 
                                shape=(None,),
                                name = ""ei_%i"" %i)
                                for i in range(input_seq_length)]

labels = [tf.placeholder(tf.int32,
                                shape=(None,),
                                name = ""l_%i"" %i)
                                for i in range(output_seq_length)]

decode_input = [tf.zeros_like(encode_input[0], dtype=np.int32, name=""GO"")] + labels[:-1]



############ Encoder
lstm_cell = BasicLSTMCell(embedding_dim)
encoder_cell = EmbeddingWrapper(lstm_cell, embedding_classes=input_vocab_size, embedding_size=embedding_dim)
encoder_outputs, encoder_state = static_rnn(encoder_cell, encode_input, dtype=tf.float32) 

############ Decoder
# Attention Mechanisms. Bahdanau is additive style attention
attn_mech = tf.contrib.seq2seq.BahdanauAttention(
    num_units = input_seq_length, # depth of query mechanism
    memory = encoder_outputs, # hidden states to attend (output of RNN)
    normalize=False, # normalize energy term
    name='BahdanauAttention')

lstm_cell_decoder = BasicLSTMCell(embedding_dim)

# Attention Wrapper: adds the attention mechanism to the cell
attn_cell = wrapper.AttentionWrapper(
    cell = lstm_cell_decoder,# Instance of RNNCell
    attention_mechanism = attn_mech, # Instance of AttentionMechanism
    attention_size = embedding_dim, # Int, depth of attention (output) tensor
    attention_history=False, # whether to store history in final output
    name=""attention_wrapper"")


# Decoder setup
decoder = tf.contrib.seq2seq.BasicDecoder(
          cell = lstm_cell_decoder,
          helper = helper, # A Helper instance
          initial_state = encoder_state, # initial state of decoder
          output_layer = None) # instance of tf.layers.Layer, like Dense

# Perform dynamic decoding with decoder object
outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder)
</code></pre>


","I am trying to produce a simple code for a seq2seq model with attention in tf 1.1. I am not sure what is the parameter ""depth of query mechanism "". I am getting an error on creation of Attention Mechanisms saying that: Here is my code. Am I on a right track? I could not find any detailed documentation.",https://stackoverflow.com/questions/43736089,5579493,Inadequate Examples
43972949,TFlearn to categorical,"<p>I`m using DNN of tflearn, and I want to change my features and lables to be categorical and not numeric. </p>

<p>here is my net:</p>

<pre><code>x = tf.placeholder(dtype= tf.float32, shape=[None, 6], name='x')
# Build neural network
input_layer = tflearn.input_data(shape=[None, 6])
net = input_layer
net = tflearn.fully_connected(net, 128, activation='relu')
net = tflearn.fully_connected(net, 64, activation='relu')
net = tflearn.fully_connected(net, 16, activation='relu')
net = tflearn.fully_connected(net, 2, activation='sigmoid')
net = tflearn.regression(net, optimizer='adam', loss='mean_square', metric='R2')

w = tf.Variable(tf.truncated_normal([2, 2], stddev=0.1))
b = tf.Variable(tf.constant(1.0, shape=[2]))
y = tf.nn.softmax(tf.matmul(net, w) + b, name='y')

model = tflearn.DNN(net, tensorboard_verbose=3)
return model
</code></pre>

<p>I know about tflearn.data_utils.to_categorical but I dont know how to inject this method.
thanks</p>

<p><strong>EDIT:</strong>
I tried few things, like:</p>

<pre><code>train_goal = tflearn.data_utils.to_categorical(train_goal, nb_classes=2)
            test_goal = tflearn.data_utils.to_categorical(test_goal, nb_classes=2)
</code></pre>

<p>and also change the loss:</p>

<pre><code>net = tflearn.regression(net, optimizer='adadelta',  loss='categorical_crossentropy', metric= self.accuracy)
</code></pre>

<p>but I got loss more than 1:</p>

<pre><code>Training Step: 35  | total loss: 1.64734 | time: 1.322s
| AdaDelta | epoch: 001 | loss: 1.64734 - acc: 1.0000 | val_loss: 1.64313 - val_acc: 1.0000 -- iter: 2204/2204
--
Training Step: 70  | total loss: 1.61961 | time: 0.216s
| AdaDelta | epoch: 002 | loss: 1.61961 - acc: 1.0000 | val_loss: 0.00000 - val_acc: 0.0000 -- iter: 2204/2204
--
Training Step: 105  | total loss: 1.58511 | time: 1.188s
| AdaDelta | epoch: 003 | loss: 1.58511 - acc: 1.0000 | val_loss: 1.57300 - val_acc: 1.0000 -- iter: 2204/2204
</code></pre>

<p>where is the problem?</p>
","I`m using DNN of tflearn, and I want to change my features and lables to be categorical and not numeric. here is my net: I know about tflearn.data_utils.to_categorical but I dont know how to inject this method. thanks EDIT: I tried few things, like: and also change the loss: but I got loss more than 1: where is the problem?",https://stackoverflow.com/questions/43972949,6925731,Inadequate Examples
48514123,"Tensorflow: SavedModelBuilder, How to save model with best validation accuracy","<p>I have gone through tensorflow documentation but couldn't find the way of saving model with best validation accuracy, using SavedModelBuilder class. 
I am using tflearn for model building and below is the work around i have tried but it is taking lot of time, where i am running fit method for each epoch separately and saving model</p>

<pre><code>for i in range(epoch):
    model.fit(trainX, trainY, n_epoch=1, validation_set=(testX, testY), show_metric=True, batch_size=8)
    builder = tf.saved_model.builder.SavedModelBuilder('/tmp/serving/model/' + str(i))
    builder.add_meta_graph_and_variables(model.session,
                                     ['TRAINING'],
                                     signature_def_map={
                                         'predict': prediction_sig
                                     })
    builder.save()
</code></pre>

<p>Please suggest if there is a better approach.</p>
","I have gone through tensorflow documentation but couldn't find the way of saving model with best validation accuracy, using SavedModelBuilder class. I am using tflearn for model building and below is the work around i have tried but it is taking lot of time, where i am running fit method for each epoch separately and saving model Please suggest if there is a better approach.",https://stackoverflow.com/questions/48514123,2398191,Documentation Completeness
49285664,training a custom estimator in tensorflow,"<p>I am new to tensorflow and trying to train a custom CNN estimator with inputs being provided from <code>TFRecord</code> files. </p>

<p>The <code>Load_input()</code> function is supposed to look into <strong>DATA_DIR</strong> for <code>TFRecords</code> file and decode it through a call to <code>read_and_decode</code> function(which is supposed to do the actual decoding of the records), store the information into an instance of <strong>_image_object</strong> and return it.</p>

<p><code>cnn_model</code> is where I have defined the CNN architecture. And <code>generate_input_fn</code> is supposed to create the batches and feed it to the <code>estimator.train</code> while training.</p>

<p>I just have an abstract understanding of the codes, no idea of the internal mechanics which is the primary reason why I am not able to debug.</p>

<p>Here is my code :</p>

<pre><code>import tensorflow as tf 
import numpy as np 
import os 



DATA_DIR = ""./TFRecords/train""  #path to tfrecords directory
TRAINING_SET_SIZE = 3
BATCH_SIZE = 3
IMAGE_SIZE = 224


def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

# image object from protobuf
class _image_object:
    def __init__(self):
        self.image = tf.Variable([], dtype = tf.string)
        self.height = tf.Variable([], dtype = tf.int64)
        self.width = tf.Variable([], dtype = tf.int64)
        self.filename = tf.Variable([], dtype = tf.string)
        self.label = tf.Variable([], dtype = tf.int32)

def read_and_decode(filename_queue):
    # this module is responsible for extracting the features
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
    features = tf.parse_single_example(serialized_example, features = {
        ""image/encoded"": tf.FixedLenFeature([], tf.string),
        ""image/height"": tf.FixedLenFeature([], tf.int64),
        ""image/width"": tf.FixedLenFeature([], tf.int64),
        ""image/filename"": tf.FixedLenFeature([], tf.string),
        ""image/class/label"": tf.FixedLenFeature([], tf.int64),})
    image_encoded = features[""image/encoded""]
    image_raw = tf.image.decode_jpeg(image_encoded, channels=3)
    image_object = _image_object()
    image_object.image = tf.image.resize_image_with_crop_or_pad(image_raw, IMAGE_SIZE, IMAGE_SIZE)#resizes and crops
    image_object.height = features[""image/height""]
    image_object.width = features[""image/width""]
    image_object.filename = features[""image/filename""]
    image_object.label = tf.cast(features[""image/class/label""], tf.int64)
    return image_object

def Load_input():

    print 'Generating data from tfrecords...'
    filenames = [os.path.join(DATA_DIR, ""train-0000%d-of-00002.tfrecord"" % i) for i in xrange(0, 1)]

    for f in filenames:
        if not tf.gfile.Exists(f):
            raise ValueError(""Failed to find file: "" + f)
    filename_queue = tf.train.string_input_producer(filenames)
    print 'decoding queue contents ::{}'.format(filename_queue)
    image_object = read_and_decode(filename_queue)
    image = tf.image.per_image_standardization(image_object.image)
#    image = image_object.image
#    image = tf.image.adjust_gamma(tf.cast(image_object.image, tf.float32), gamma=1, gain=1) # Scale image to (0, 1)
    label = image_object.label
    filename = image_object.filename
    return image,label,filename


def cnn_model(features,labels,mode):

    print 'creating layers...'  
    #Input layer
    #inp = tf.reshape(features['x'],[-1,28,28,1])
    inp = tf.reshape(features,[-1,224,224,3])
    print 'input shape ::{}'.format(inp.shape)
    #convolutional layer #1
    conv1 = tf.layers.conv2d(inputs=inp,filters=32,kernel_size=[5,5],padding='same',activation=tf.nn.relu)
    print 'convolution-1 shape ::{}'.format(conv1.shape)

    #pooling Layer
    pool1=tf.layers.max_pooling2d(inputs=conv1,pool_size=[2,2],strides=2)
    print 'Pool-1 shape ::{}'.format(pool1.shape)
    #convolutional layer #2
    conv2 = tf.layers.conv2d(inputs=pool1,filters=64,kernel_size=[5,5],padding='same',activation=tf.nn.relu)
    print 'convolution-2 shape ::{}'.format(conv2.shape)
    #pooling layer
    pool2=tf.layers.max_pooling2d(inputs=conv2,pool_size=[2,2],strides=2)
    print 'Pool-2 shape ::{}'.format(pool2.shape)
    #dense layer
    pool2_flat = tf.reshape(pool2,[-1,56*56*64]) #dimension = [BATCH_SIZE,HEIGHT*WIDTH*CHANNELS of the last pooled layers]
    dense = tf.layers.dense(inputs=pool2_flat,units=1024,activation=tf.nn.relu) # units = number of neurons per layer
    dropout=tf.layers.dropout(inputs=dense,rate=0.4,training = (mode == tf.estimator.ModeKeys.TRAIN))

    #Logits Layer
    logits = tf.layers.dense(inputs=dropout,units=2) #has shape [batch_size, no_of_labels]
    predictions ={'classes':tf.argmax(input=logits,axis=1),'probabilities':tf.nn.softmax(logits,name='softmax_tensor')}
    print 'Logits shape ::{}'.format(logits.shape)
    print 'Labels shape ::{}'.format(labels.shape)

    #Calculate loss for TRAIN and EVAL mode
    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)

    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
    train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())
    print 'Layers created...'
    return tf.estimator.EstimatorSpec(mode=mode,loss=loss,train_op=train_op)



def generate_input_fn(image,label,batch_size=BATCH_SIZE):
   print(""Filling queue with images before starting to train. "" ""This will take a few minutes."")
   num_preprocess_threads = 1
   def _input_fn():
      image_placeholder=tf.placeholder(tf.float32,shape=[batch_size,224,224,3])
      label_placeholder=tf.placeholder(tf.int64,shape=[batch_size,1])
      image_batch, label_batch= tf.train.shuffle_batch(
            [image_placeholder, label_placeholder],
            batch_size = batch_size,
            num_threads = num_preprocess_threads,
            capacity = 8 * BATCH_SIZE,
            min_after_dequeue = 4 * BATCH_SIZE)
      return image_batch, label_batch 
   return _input_fn



def main(unused_argv):
    print 'program started...'
    image_data, label_data, filename = Load_input()
    print 'image_data::{} label_data::{}'.format(type(image_data),type(label_data))

    estimator = tf.estimator.Estimator(model_fn=cnn_model,model_dir='./')
    print 'Estimator ready...'
    tensors_to_log = {'probabilities':'softmax_tensor'}
    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log,every_n_iter=1)
    print 'Logs ready...'
    print 'Starting training...'
    estimator.train(input_fn=generate_input_fn(image=image_data, label=label_data),steps=2,hooks=[logging_hook])


if __name__=='__main__':
  tf.app.run()
  print 'Program ended...'
</code></pre>

<p>it gives me the following error :</p>

<blockquote>
  <p>ValueError: Dimension 0 in both shapes must be equal, but are 9 and 3. Shapes are [9,2] and [3,3]. for 'softmax_cross_entropy_with_logits_sg' (op: 'SoftmaxCross
  EntropyWithLogits') with input shapes: [9,2], [3,3].</p>
</blockquote>

<p>also the layers shapes are as follows :</p>

<pre><code>conv1 output shape :: (9, 224, 224, 32)
pool1 shape :: (9, 112, 112, 32)
conv2 shape ::(9, 112, 112, 64)
pool2 shape :: (9, 56, 56, 64)
Logits shape :: (9, 2)
Labels shape :: (3, 3)
</code></pre>

<p>I don't understand why is the <code>batch size</code> <strong>9</strong> even if I try to explicitly set it to <strong>3</strong> in the code.</p>

<p><strong>Note</strong> : If anyone has a better/easier solution please post it. The aim is to <strong>use tfrecords to train a custom CNN</strong></p>
","I am new to tensorflow and trying to train a custom CNN estimator with inputs being provided from TFRecord files. The Load_input() function is supposed to look into DATA_DIR for TFRecords file and decode it through a call to read_and_decode function(which is supposed to do the actual decoding of the records), store the information into an instance of _image_object and return it. cnn_model is where I have defined the CNN architecture. And generate_input_fn is supposed to create the batches and feed it to the estimator.train while training. I just have an abstract understanding of the codes, no idea of the internal mechanics which is the primary reason why I am not able to debug. Here is my code : it gives me the following error : also the layers shapes are as follows : I don't understand why is the batch size 9 even if I try to explicitly set it to 3 in the code. Note : If anyone has a better/easier solution please post it. The aim is to use tfrecords to train a custom CNN",https://stackoverflow.com/questions/49285664,7713497,Requesting (Additional) Resources
50017241,Converting grayscale to RGB in tfrecord,"<p>I have a dataset of grayscale images, and I'd like to use the sdd-mobilenet checkpoints for training my object detection.
What is the proper way to convert grayscale images to RGB that I can convert my dataset to tfrecord?
Here is the code that I use (notice that the commented parts didn't work for me)</p>

<pre><code>with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:
    encoded_jpg = fid.read()
# rgb_image = tf.image.grayscale_to_rgb(
#     tf.image.encode_jpeg(encoded_jpg),
#     name=None
# )
encoded_jpg_io = io.BytesIO(encoded_jpg)
encoded_jpg_io = tf.stack([encoded_jpg_io, encoded_jpg_io, encoded_jpg_io], axis=-1)
image = Image.open(encoded_jpg_io)
width, height = image.size

filename = group.filename.encode('utf8')
image_format = b'jpg'
xmins = []
xmaxs = []
ymins = []
ymaxs = []
classes_text = []
classes = []

for index, row in group.object.iterrows():
    xmins.append(row['xmin'] / width)
    xmaxs.append(row['xmax'] / width)
    ymins.append(row['ymin'] / height)
    ymaxs.append(row['ymax'] / height)
    classes_text.append(row['class'].encode('utf8'))
    classes.append(class_text_to_int(row['class']))

tf_example = tf.train.Example(features=tf.train.Features(feature={
    'image/height': dataset_util.int64_feature(height),
    'image/width': dataset_util.int64_feature(width),
    'image/filename': dataset_util.bytes_feature(filename),
    'image/source_id': dataset_util.bytes_feature(filename),
    # 'image/channels': dataset_util.int64_feature(),
    'image/encoded': dataset_util.bytes_feature(encoded_jpg),
    'image/format': dataset_util.bytes_feature(image_format),
    'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
    'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),
    'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),
    'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),
    'image/object/class/text': dataset_util.bytes_list_feature(classes_text),
    'image/object/class/label': dataset_util.int64_list_feature(classes),
}))
return tf_example
</code></pre>
","I have a dataset of grayscale images, and I'd like to use the sdd-mobilenet checkpoints for training my object detection. What is the proper way to convert grayscale images to RGB that I can convert my dataset to tfrecord? Here is the code that I use (notice that the commented parts didn't work for me)",https://stackoverflow.com/questions/50017241,8130289,Requesting (Additional) Resources
50220191,How to import(restore) Neural network model built by tflearn from files,"<p>I am referring to <a href=""https://sourcedexter.com/tensorflow-text-classification-python/"" rel=""nofollow noreferrer"">this tutorial</a> on text classification and built a custom training set for a text classification.</p>

<p>I am saving the model with below code.</p>

<pre><code># Define model and setup tensorboard
model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')
# Start training (apply gradient descent algorithm)
model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)
model.save('model.tflearn')
</code></pre>

<p>This generates below files.</p>

<pre><code>model.tflearn.data-00000-of-00001
model.tflearn.index
model.tflearn.meta
tflearn_logs folder
</code></pre>

<p>I want to use the model built in different iteration for testing purpose.</p>

<p>I tried ,</p>

<pre><code>with tf.Session() as sess:
    saver = tf.train.import_meta_graph('model.tflearn.meta')
    saver.restore(sess, tf.train.latest_checkpoint('./'))
</code></pre>

<p><em>but I get;</em></p>

<blockquote>
  <p><strong>KeyError: ""The name 'adam' refers to an Operation not in the graph.""</strong> error</p>
</blockquote>

<p>I know <a href=""http://tflearn.org/getting_started/#weights-persistence"" rel=""nofollow noreferrer"">from documentation</a> that <code>tflearn.DNN(network).load('file_name')</code> loads a model , but we need to create and pass the network instance, to build a network we again go through same code from scratch which takes time since it will do training which I want to avoid.</p>

<p>Code for building network</p>

<pre><code>net = tflearn.input_data(shape=[None, len(train_x[0])])
net = tflearn.fully_connected(net, 8)
net = tflearn.fully_connected(net, 8)
net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')
net = tflearn.regression(net)
</code></pre>

<p><code>tflearn.input_data</code> has shape input as mandatory , so we would again need training data to be fed again.So it causes rebuilding model.
I checked the documentation , could not find what I need (2-3 lines of code which would import build neural network model to save retraining time.</p>

<p>Please let me know if you guys know solution for this.</p>

<p><a href=""https://stackoverflow.com/questions/45595646/in-tensorflow-how-to-freeze-saved-model/46637498?noredirect=1#comment87414509_46637498"">Similar question</a> but its not duplicate </p>

<ul>
<li>OP was facing issue while building neural net during building tree , while I am facing issue importing build model.  </li>
<li>Tutorial mentioned in the answer does not have tflearn NN model import </li>
</ul>
","I am referring to this tutorial on text classification and built a custom training set for a text classification. I am saving the model with below code. This generates below files. I want to use the model built in different iteration for testing purpose. I tried , but I get; I know from documentation that tflearn.DNN(network).load('file_name') loads a model , but we need to create and pass the network instance, to build a network we again go through same code from scratch which takes time since it will do training which I want to avoid. Code for building network tflearn.input_data has shape input as mandatory , so we would again need training data to be fed again.So it causes rebuilding model. I checked the documentation , could not find what I need (2-3 lines of code which would import build neural network model to save retraining time. Please let me know if you guys know solution for this. Similar question but its not duplicate",https://stackoverflow.com/questions/50220191,7907591,Documentation Completeness
58044469,Gaussian Process Regression in Tensorflow 2.0 leads to no gradients?,"<p>The following code is basically from the documentation, slightly converted to run in tensorflow 2.0. The gradients are all None. I'm not sure if this is a bug or just something I am missing:</p>

<p>(corrected code)</p>

<pre><code>import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

tfd = tfp.distributions
psd_kernels = tfp.positive_semidefinite_kernels

tf.keras.backend.set_floatx('float64')

f = lambda x: np.sin(10*x[..., 0]) * np.exp(-x[..., 0]**2)

observation_index_points = np.random.uniform(-1., 1., 50)[..., np.newaxis]
observations = f(observation_index_points) + np.random.normal(0., .05, 50)


class Model(tf.keras.models.Model):
    def __init__(self):
        super().__init__()
        self.amplitude_ = tf.Variable(np.float64(0), trainable=True)
        self.amplitude = tf.exp(self.amplitude_, name='amplitude')
        self.length_scale_ = tf.Variable(np.float64(0), trainable=True)
        self.length_scale = tf.exp(self.length_scale_, name='length_scale')
        self.kernel = psd_kernels.ExponentiatedQuadratic(self.amplitude, self.length_scale)
        self.observation_noise_variance_ = tf.Variable(np.float64(-5), trainable=True)
        self.observation_noise_variance = tf.exp(self.observation_noise_variance_, name='observation_noise_variance')


    def gp(self, observation_index_points):
        return tfd.GaussianProcess(
            kernel=self.kernel,
            index_points=observation_index_points,
            observation_noise_variance=self.observation_noise_variance)

    def call(self, observation_index_points, observations, index_points):
        return tfd.GaussianProcessRegressionModel(
        kernel=self.kernel,
        index_points=index_points,
        observation_index_points=observation_index_points,
        observations=observations,
        observation_noise_variance=self.observation_noise_variance)

optimizer = tf.keras.optimizers.Adam(learning_rate=.05)

# We can construct the posterior at a new set of `index_points` using the same
# kernel (with the same parameters, which we'll optimize below).
index_points = np.linspace(-1., 1., 100)[..., np.newaxis]

model = Model()
gprm = model(observation_index_points, observations, index_points)
gp = model.gp(observation_index_points)
gp.log_prob(observations)
samples = gprm.sample(10)

trainable_variables = [model.amplitude_, model.length_scale_, model.observation_noise_variance_]
with tf.GradientTape() as tape:
    loss = -gp.log_prob(observations)
print(loss)
g = tape.gradient(loss, trainable_variables)
print(g)
</code></pre>

<p>UPDATE:</p>

<p>The following example now works. Am wondering if there is a better pattern for organizing this flow in tf 2.0? </p>

<pre><code> import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp
tfb = tfp.bijectors
tfd = tfp.distributions
psd_kernels = tfp.positive_semidefinite_kernels

m = 1000
n = 3
x = np.random.randn(m, n).astype(np.float32)
y = np.random.randn(m).astype(np.float32)
x_  = np.random.randn(100, n).astype(np.float32)


class GPRMatern(tf.keras.models.Model):
    def __init__(self, feature_ndims=1):
        super().__init__()
        self.kernel = psd_kernels.MaternFiveHalves()
        self.observation_noise_variance = tf.Variable(np.float32(.01), name='obs_noise_variance')

    def gprm(self, x_obs, y_obs, x):
        return tfd.GaussianProcessRegressionModel(
            kernel=self.kernel,
            index_points=x,
            observation_index_points=x_obs,
            observations=y_obs,
            observation_noise_variance=self.observation_noise_variance)

    def nll_for_train(self, x_obs, y_obs):
        gp = tfd.GaussianProcess(
            kernel=self.kernel,
            index_points=x_obs,
            observation_noise_variance=self.observation_noise_variance)
        return -tf.reduce_mean(gp.log_prob(y_obs))

class GPRExpQuad(tf.keras.models.Model):
    def __init__(self):
        super().__init__()
        self.amplitude = tf.Variable(np.float32(0.0), name='amplitude')
        self.length_scale = tf.Variable(np.float32(0.0), name='length_scale')
        self.observation_noise_variance = tf.Variable(np.float32(-5.0), name='obs_noise_variance')

    @property
    def kernel(self):
        return psd_kernels.ExponentiatedQuadratic(tf.exp(self.amplitude), tf.exp(self.length_scale))

    def nll_for_train(self, x_obs, y_obs):
        gp = tfd.GaussianProcess(
            kernel=self.kernel,
            index_points=x_obs,
            observation_noise_variance=tf.exp(self.observation_noise_variance))
        return -tf.reduce_mean(gp.log_prob(y_obs))

    def gprm(self, x_obs, y_obs, x):
        return tfd.GaussianProcessRegressionModel(
            kernel=self.kernel,
            index_points=x,
            observation_index_points=x_obs,
            observations=y_obs,
            observation_noise_variance=tf.exp(self.observation_noise_variance))

def test_model(model=GPRMatern):
    model = model()
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
    # model.fit(x, y, epochs=steps)
    for i in range(10):
        with tf.GradientTape() as tape:
            l = model.nll_for_train(x, y)
        g = tape.gradient(l, model.trainable_variables)
        optimizer.apply_gradients(zip(g, model.trainable_variables))
        print({x.name: x.numpy() for x in model.trainable_variables})

matern = GPRMatern()
expquad = GPRExpQuad()

test_matern = lambda : test_model(model=GPRMatern)
test_expquad = lambda : test_model(model=GPRExpQuad)
</code></pre>
","The following code is basically from the documentation, slightly converted to run in tensorflow 2.0. The gradients are all None. I'm not sure if this is a bug or just something I am missing: (corrected code) UPDATE: The following example now works. Am wondering if there is a better pattern for organizing this flow in tf 2.0?",https://stackoverflow.com/questions/58044469,287238,Documentation Replication on Other Examples
59796343,Transformer model not able to be saved,"<p>I'm trying to follow this tutrial <a href=""https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/transformer_chatbot.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/transformer_chatbot.ipynb</a>, However, when I tried to save the model in order to load it again without training I got an error mentioned here <a href=""https://stackoverflow.com/questions/58678836/notimplementederror-layers-with-arguments-in-init-must-override-get-conf"">NotImplementedError: Layers with arguments in `__init__` must override `get_config`</a>
I understood from the answer that I need to make the encoder and decoder as classes and customise it(instead of leaving it as functions like the colab tutrial) so I went back to tensor flow documentation of this model here: <a href=""https://www.tensorflow.org/tutorials/text/transformer#encoder_layer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/transformer#encoder_layer</a> and tried to edit in it. I made the encoder layer as:</p>

<pre><code>class EncoderLayer(tf.keras.layers.Layer):
  def __init__(self, d_model, num_heads,  rate=0.1,**kwargs,):
    #super(EncoderLayer, self).__init__()
    super().__init__(**kwargs)
    self.mha = MultiHeadAttention(d_model, num_heads)
    self.ffn = point_wise_feed_forward_network(d_model, dff)

    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

    self.dropout1 = tf.keras.layers.Dropout(rate)
    self.dropout2 = tf.keras.layers.Dropout(rate)
  def get_config(self):

        config = super().get_config().copy()
        config.update({
            #'vocab_size': self.vocab_size,
            #'num_layers': self.num_layers,
            #'units': self.units,
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'dropout': self.dropout,
        })
        return config

  def call(self, x, training, mask):

    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)
    attn_output = self.dropout1(attn_output, training=training)
    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)

    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)
    ffn_output = self.dropout2(ffn_output, training=training)
    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)

    return out2
</code></pre>

<p>and same for the decoder layer class. Then the same encoder in the documentation of tf</p>

<pre><code>class Encoder(tf.keras.layers.Layer):
  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
               maximum_position_encoding, rate=0.1):
    super(Encoder, self).__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
    self.pos_encoding = positional_encoding(maximum_position_encoding, 
                                            self.d_model)


    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) 
                       for _ in range(num_layers)]

    self.dropout = tf.keras.layers.Dropout(rate)

  def call(self, x, training, mask):

    seq_len = tf.shape(x)[1]

    # adding embedding and position encoding.
    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x += self.pos_encoding[:, :seq_len, :]

    x = self.dropout(x, training=training)

    for i in range(self.num_layers):
      x = self.enc_layers[i](x, training, mask)

    return x  # (batch_size, input_seq_len, d_model)
</code></pre>

<p>the function of the model as:</p>

<pre><code>def transformer(vocab_size,
                num_layers,
                units,
                d_model,
                num_heads,
                dropout,
                name=""transformer""):
  inputs = tf.keras.Input(shape=(None,), name=""inputs"")
  dec_inputs = tf.keras.Input(shape=(None,), name=""dec_inputs"")

  enc_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='enc_padding_mask')(inputs)
  # mask the future tokens for decoder inputs at the 1st attention block
  look_ahead_mask = tf.keras.layers.Lambda(
      create_look_ahead_mask,
      output_shape=(1, None, None),
      name='look_ahead_mask')(dec_inputs)
  # mask the encoder outputs for the 2nd attention block
  dec_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='dec_padding_mask')(inputs)

  enc_outputs = Encoder(
      num_layers=num_layers, d_model=d_model, num_heads=num_heads, 
                         input_vocab_size=vocab_size,


  )(inputs=[inputs, enc_padding_mask])

  dec_outputs = Decoder(
      num_layers=num_layers, d_model=d_model, num_heads=num_heads, 
                          target_vocab_size=vocab_size,


  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])

  outputs = tf.keras.layers.Dense(units=vocab_size, name=""outputs"")(dec_outputs)

  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)
</code></pre>

<p>and calling the model:</p>

<pre><code>#the model itself with its paramters:
# Hyper-parameters
NUM_LAYERS = 3
D_MODEL = 256
#D_MODEL=tf.cast(D_MODEL, tf.float32)

NUM_HEADS = 8
UNITS = 512
DROPOUT = 0.1
model = transformer(
    vocab_size=VOCAB_SIZE,
    num_layers=NUM_LAYERS,
    units=UNITS,
    d_model=D_MODEL,
    num_heads=NUM_HEADS,
    dropout=DROPOUT)
</code></pre>

<p>However, I got that error:
<code>TypeError: __init__() missing 2 required positional arguments: 'dff' and 'maximum_position_encoding'</code>
I am really confused and I don't understand what dff and maximum position encoding mean in the documentation and when I removed them from the encoder and decoder classes, I got anther error as positional_encoding function takes maximum position as input and also dff is passed as input inside the class. I am not so sure what I should do as I am not sure whether I am following the right steps or not</p>
","I'm trying to follow this tutrial https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/transformer_chatbot.ipynb, However, when I tried to save the model in order to load it again without training I got an error mentioned here NotImplementedError: Layers with arguments in `__init__` must override `get_config` I understood from the answer that I need to make the encoder and decoder as classes and customise it(instead of leaving it as functions like the colab tutrial) so I went back to tensor flow documentation of this model here: https://www.tensorflow.org/tutorials/text/transformer#encoder_layer and tried to edit in it. I made the encoder layer as: and same for the decoder layer class. Then the same encoder in the documentation of tf the function of the model as: and calling the model: However, I got that error: TypeError: __init__() missing 2 required positional arguments: 'dff' and 'maximum_position_encoding' I am really confused and I don't understand what dff and maximum position encoding mean in the documentation and when I removed them from the encoder and decoder classes, I got anther error as positional_encoding function takes maximum position as input and also dff is passed as input inside the class. I am not so sure what I should do as I am not sure whether I am following the right steps or not",https://stackoverflow.com/questions/59796343,10291435,Documentation Ambiguity
61665285,Problem with input to tf.keras.layers.GRU,"<p>I am trying to make a seq2seq model using <code>tfa.seq2seq.BaseDecoder</code> in TensorFlow 2.1. I have</p>

<pre><code>tf.keras.layers.GRU(64)(inputs, [states])
</code></pre>

<p>where inputs has shape <code>(batch_size, 1, embedding_dimension)</code> and comes from </p>

<pre><code>inputs = tf.keras.layers.Embedding(1000, 64, mask_zero=True)(tf.fill([batch_size, 1], value=1))
</code></pre>

<p>and <code>states</code> are the encoder hidden states for the batch.</p>

<p>I am implementing <code>tfa.seq2seq.BaseDecoder</code>'s <code>initialize</code>, <code>step</code> and some properties and the error is happening in <code>step</code> which contains the line that I have copied out here.</p>

<p>However, it gives me the following error message (some function names are changed to make explaining the question easier and are slightly different in the code).</p>

<pre><code>    Traceback (most recent call last):
  File ""/home/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2659, in _set_inputs
    outputs = self(inputs, **kwargs)
  File ""/home/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 773, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/home/.local/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 237, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:

    /home/lemmatizer_noattn.py:155 call  *
        output_layer, _, output_lens, _ = self.DecoderTraining((source_states, target_charseqs), True)
    /home/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:785 __call__
        str(e) + '\n""""""')

    TypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.
    Encountered error:
    """"""
    in converted code:

        /home/.local/lib/python3.7/site-packages/tensorflow_addons/seq2seq/decoder.py:162 call  *
            return dynamic_decode(
        /home/.local/lib/python3.7/site-packages/tensorflow_addons/seq2seq/decoder.py:405 body  *
            (next_outputs, decoder_state, next_inputs, decoder_finished) = decoder.step(
        /home/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py:2478 while_loop_v2
            return_same_structure=True)
        /home/lemmatizer_noattn.py:79 step  *
            outputs, [states] = self.lemmatizer.target_rnn_cell(inputs, [states])
        /home/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:539 __iter__
            self._disallow_iteration()
        /home/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:535 _disallow_iteration
            self._disallow_in_graph_mode(""iterating over `tf.Tensor`"")
        /home/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:515 _disallow_in_graph_mode
            "" this function with @tf.function."".format(task))

        OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.

    """"""
</code></pre>

<p>I didn't manage to figure out from the documentation where the error might be comming from nor did I find any advice on the internet. Any ideas on where the problem might be?</p>
","I am trying to make a seq2seq model using tfa.seq2seq.BaseDecoder in TensorFlow 2.1. I have where inputs has shape (batch_size, 1, embedding_dimension) and comes from and states are the encoder hidden states for the batch. I am implementing tfa.seq2seq.BaseDecoder's initialize, step and some properties and the error is happening in step which contains the line that I have copied out here. However, it gives me the following error message (some function names are changed to make explaining the question easier and are slightly different in the code). I didn't manage to figure out from the documentation where the error might be comming from nor did I find any advice on the internet. Any ideas on where the problem might be?",https://stackoverflow.com/questions/61665285,13045395,Requesting (Additional) Resources
64422727,What is tensorflow concrete function outputs correspond to structured_outputs?,"<p>I trained my customized ssd_mobilenet_v2 using <strong>TensorFlow2 Object Detection API</strong>. <br>
After training completed, I used exporter_main_v2.py to export a saved_model of my customized model.</p>
<p>If I load saved_model by TensorFlow2, it seem there are two kind of output format.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
saved_model = tf.saved_model.load(&quot;saved_model&quot;)
detect_fn = saved_model[&quot;serving_default&quot;]
print(detect_fn.outputs)
'''
[&lt;tf.Tensor 'Identity:0' shape=(1, 100) dtype=float32&gt;,
 &lt;tf.Tensor 'Identity_1:0' shape=(1, 100, 4) dtype=float32&gt;,
 &lt;tf.Tensor 'Identity_2:0' shape=(1, 100) dtype=float32&gt;,
 &lt;tf.Tensor 'Identity_3:0' shape=(1, 100, 7) dtype=float32&gt;,
 &lt;tf.Tensor 'Identity_4:0' shape=(1, 100) dtype=float32&gt;,
 &lt;tf.Tensor 'Identity_5:0' shape=(1,) dtype=float32&gt;,
 &lt;tf.Tensor 'Identity_6:0' shape=(1, 1917, 4) dtype=float32&gt;,
 &lt;tf.Tensor 'Identity_7:0' shape=(1, 1917, 7) dtype=float32&gt;]
'''
print(detect_fn.structured_outputs)
'''
{'detection_classes': TensorSpec(shape=(1, 100), dtype=tf.float32, name='detection_classes'),
 'detection_scores': TensorSpec(shape=(1, 100), dtype=tf.float32, name='detection_scores'),
 'detection_multiclass_scores': TensorSpec(shape=(1, 100, 7), dtype=tf.float32, name='detection_multiclass_scores'),
 'num_detections': TensorSpec(shape=(1,), dtype=tf.float32, name='num_detections'),
 'raw_detection_boxes': TensorSpec(shape=(1, 1917, 4), dtype=tf.float32, name='raw_detection_boxes'),
 'detection_boxes': TensorSpec(shape=(1, 100, 4), dtype=tf.float32, name='detection_boxes'),
 'detection_anchor_indices': TensorSpec(shape=(1, 100), dtype=tf.float32, name='detection_anchor_indices'),
 'raw_detection_scores': TensorSpec(shape=(1, 1917, 7), dtype=tf.float32, name='raw_detection_scores')}
'''
</code></pre>
<p>Then, I try to convert this saved_model to onnx format using tf2onnx. <br>
However, the outputs of onnxruntime was a list. <br>
By the shape of result in the list, I think that the sequence is same as detect_fn.outputs</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import onnxruntime as rt

sess = rt.InferenceSession(&quot;model.onnx&quot;)
input_name = sess.get_inputs()[0].name
pred_onx = sess.run(None, {input_name: np.zeros((1,300,300,3), dtype=np.uint8)})
print(pred_onx) # a list
print([i.shape for i in pred_onx])
'''
[(1, 100),
 (1, 100, 4),
 (1, 100),
 (1, 100, 7),
 (1, 100),
 (1,),
 (1, 1917, 4),
 (1, 1917, 7)]
'''
</code></pre>
<p>Because there is some shape of result which is same as others, so it become hard to recognized.<br>
Is there any document talk about this relationship that I can refer?</p>
","I trained my customized ssd_mobilenet_v2 using TensorFlow2 Object Detection API. After training completed, I used exporter_main_v2.py to export a saved_model of my customized model. If I load saved_model by TensorFlow2, it seem there are two kind of output format. Then, I try to convert this saved_model to onnx format using tf2onnx. However, the outputs of onnxruntime was a list. By the shape of result in the list, I think that the sequence is same as detect_fn.outputs Because there is some shape of result which is same as others, so it become hard to recognized. Is there any document talk about this relationship that I can refer?",https://stackoverflow.com/questions/64422727,14148018,Requesting (Additional) Resources
65264697,How to set signature in keras.models.save_ model,"<h1>My environment</h1>
<p>tf:2.3
system:ubuntu 18</p>
<h1>My question</h1>
<p>I updated from tf14 to tf2.3. The model I used is a model of keras type. After viewing the official document, adding signature failed</p>
<h1>My main code</h1>
<pre><code>model = VGG16(weights = weights_dir)
...
keras.models.save_model(model, model_dir_saved_model)
</code></pre>
<p>This function has the input of signature, but I don't know how to organize it</p>
<h1>Here's my try</h1>
<pre><code>
def saveKerasModelAsProtobuf(model, outputPath):
    inputs = {'image': utils.build_tensor_info(model.input)}
    outputs = {'scores': utils.build_tensor_info(model.output)}

    signature = tf.saved_model.signature_def_utils.build_signature_def(
        inputs, outputs, 'name')

    builder = tf.saved_model.builder.SavedModelBuilder(outputPath)
    builder.add_meta_graph_and_variables(
        sess=keras.backend.get_session(),
        tags=['serving_default'],
        signature_def_map={'serving_default': signature})
    builder.save()
</code></pre>
<p>So, what's the right way to keep it</p>
","tf:2.3 system:ubuntu 18 I updated from tf14 to tf2.3. The model I used is a model of keras type. After viewing the official document, adding signature failed This function has the input of signature, but I don't know how to organize it So, what's the right way to keep it",https://stackoverflow.com/questions/65264697,14812957,Documentation Replicability
66149002,How to convert .pb file to .tflite. in tensorflow 2.1.0,"<p>I have trained my model using <strong>SSD MobileNet V2 FPNLite 640x640</strong> and <strong>tensorflow 2.1.0</strong>. To create .pb file used file <em>export_tflite_graph_tf2.py</em>. Now , want to convert .pb file to .tflite.
I used following code code recommended by tensorflow community .But it is converting the model to 1kb tflite file.</p>
<p>I tried also using tf-nightly  but same converts into 1kb</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
</code></pre>
","I have trained my model using SSD MobileNet V2 FPNLite 640x640 and tensorflow 2.1.0. To create .pb file used file export_tflite_graph_tf2.py. Now , want to convert .pb file to .tflite. I used following code code recommended by tensorflow community .But it is converting the model to 1kb tflite file. I tried also using tf-nightly but same converts into 1kb",https://stackoverflow.com/questions/66149002,11311304,Inadequate Examples
70294847,Low accuracy using functional API + CNN and CIFAR10; incorrect initialization?,"<p>I'm new to using CNNs but I'm trying to make one using the functional API with the CIFAR10 dataset. The only thing is I'm getting very very low accuracy. I've looked over my textbook examples and documentation but can't figure out why it's so low when it should be starting way higher. This is my setup using DenseNet201 and tf version 2.7:</p>
<pre><code>#load in data 
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Normalize pixel values to be between 0 and 1
X_train, X_test = X_train / 255.0, X_test / 255.0

# one hot encode target values/labels
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

# have to preprocess before using functional API
X_testP = tf.keras.applications.densenet.preprocess_input(X_test)
X_trainP = tf.keras.applications.densenet.preprocess_input(X_train)

# data size we start with
inputs = tf.keras.Input(shape=(32,32,3))
# densenet expects 224x224 so use lambda layer
resized_images = tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (224, 224)))(inputs)

# initialize model
transfer = keras.applications.DenseNet201(include_top=False, weights='imagenet', pooling='max', input_tensor = resized_images,input_shape=(224,224,3), classes=1000)

# add your layers
x = transfer.output
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(256, activation='relu')(x)
x = tf.keras.layers.BatchNormalization() (x)
x = tf.keras.layers.Dense(200, activation='relu')(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
output = tf.keras.layers.Dense(10, activation='softmax')(x)

transfer_model = keras.Model(inputs=transfer.input, outputs=output)
transfer_model.trainable = False;

# here I try SGD but I also tried Adam to no better results
optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)

transfer_model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])

history_transfer = transfer_model.fit(X_trainP, y_train,epochs=20)
</code></pre>
<p>I feel like all the examples I've seen start way higher and that's even without additional layers. Am I misunderstanding something in the initialization?</p>
",I'm new to using CNNs but I'm trying to make one using the functional API with the CIFAR10 dataset. The only thing is I'm getting very very low accuracy. I've looked over my textbook examples and documentation but can't figure out why it's so low when it should be starting way higher. This is my setup using DenseNet201 and tf version 2.7: I feel like all the examples I've seen start way higher and that's even without additional layers. Am I misunderstanding something in the initialization?,https://stackoverflow.com/questions/70294847,12379067,Documentation Ambiguity
71520085,Tensorflow 2.8 GPU out_of_mem when using multiprocessing,"<p>I'm trying to convert .ogg files to tfrecords. I'm running the below code on my GPU using multiprocessing but my GPU RAM gets allocated 100% and the program crashes. Anyone have some input on using multiprocessing with tensorflow or any documentation to best practices? I haven't been able to find what I'm looking for.</p>
<pre><code>import argparse
import math
import os

import numpy as np
import pandas as pd
from multiprocessing import Pool, cpu_count
import tqdm
import tensorflow as tf
import tensorflow_io as tfio

_DEFAULT_META_CSV = 'train_metadata.csv'
_DEFAULT_OUTPUT_DIR = 'tfrecords'

_DEFAULT_DURATION = 4
_DEFAULT_SAMPLE_RATE = 50000

_DEFAULT_TEST_SIZE = 0.1
_DEFAULT_VAL_SIZE = 0.1

_DEFAULT_NUM_SHARDS_TRAIN = 16
_DEFAULT_NUM_SHARDS_TEST = 4
_DEFAULT_NUM_SHARDS_VAL = 4

_SEED = 42


def _float_feature(list_of_floats):
    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))

def _int_features(list_of_ints):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[list_of_ints]))

def _bytes_feature(value):
  &quot;&quot;&quot;Returns a bytes_list from a string / byte.&quot;&quot;&quot;
  if isinstance(value, type(tf.constant(0))):
    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _parallelize(func, data):

    processes = cpu_count() - 5
    with Pool(processes) as pool:
        list(tqdm.tqdm(pool.imap_unordered(func, data), total=len(data)))


class TFRecordsConverter:
    '''Convert audio to TFRecords'''
    def __init__(self, meta, output_dir, n_shards_train, n_shards_test,
                 n_shards_val, duration, sample_rate, test_size, val_size):        
        self.output_dir = output_dir
        self.duration = duration
        self.sample_rate = sample_rate
        
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        df = pd.read_csv(meta)
        # Shuffle the data
        self.df = df.sample(frac=1, random_state=_SEED)
        
        n_samples = len(df)
        self.n_test = math.ceil(n_samples * test_size)
        self.n_val = math.ceil(n_samples * val_size)
        self.n_train = n_samples - self.n_test - self.n_val
        
        if n_shards_train is None or n_shards_test is None or n_shards_val is None:
            self.n_shards_train = self._n_shards(self.n_train)
            self.n_shards_test = self._n_shards(self.n_test)
            self.n_shards_val = self._n_shards((self.n_val))
        else:
            self.n_shards_train = n_shards_train
            self.n_shards_test = n_shards_test
            self.n_shards_val = n_shards_val
            
        def __repr__(self):
            return ('{}.{}(output_dir={}, n_shards_train={}, n_shards_test={}, '
                    'n_shards_val={}, duration={}, sample_rate={}, n_train={}, '
                    'n_test={}, n_val={})').format(
                self.__class__.__module__,
                self.__class__.__name__,
                self.output_dir,
                self.n_shards_train,
                self.n_shards_test,
                self.n_shards_val,
                self.duration,
                self.sample_rate,
                self.n_train,
                self.n_test,
                self.n_val,
            )
    
    def _n_shards(self, n_samples):
        return math.ceil(n_samples /self._shard_size())
    
    def _shard_size(self):
        shard_max_bytes = 200 * 1024**2
        audio_bytes_per_second = self.sample_rate * 2
        audio_bytes_total = audio_bytes_per_second * self.duration
        shard_size = shard_max_bytes // audio_bytes_total
        return shard_size * self._COMPRESSION_SCALING_FACTOR
        
    def _get_shard_path(self, split, shard_id, shard_size):
        return os.path.join(self.output_dir, f'{split}-{shard_id}-{shard_size}.tfrecord')
    
    def _write_tfrecord_file(self, shard_data):
        shard_path, indices = shard_data
        with tf.io.TFRecordWriter(shard_path, options='ZLIB') as out:
            for index in indices:
                file_path = 'train_audio/' + self.df.filename.iloc[index]
                label = bytes(self.df.primary_label.iloc[index], 'utf-8')
            
                raw_audio = tf.io.read_file(file_path)
                audio = tfio.audio.decode_vorbis(raw_audio)
                
                example = tf.train.Example(features=tf.train.Features(feature={
                    'audio' :  _float_feature(audio.numpy().flatten().tolist()),
                    'label' : _bytes_feature(label)
                }))
                
                out.write(example.SerializeToString())
                
    def _split_data_into_shards(self):
        shards = []

        splits = ('train', 'test', 'validate')
        split_sizes = (self.n_train, self.n_test, self.n_val)
        split_n_shards = (self.n_shards_train, self.n_shards_test,
                          self.n_shards_val)

        offset = 0
        for split, size, n_shards in zip(splits, split_sizes, split_n_shards):
            print('Splitting {} set into TFRecord shards...'.format(split))
            shard_size = math.ceil(size / n_shards)
            cumulative_size = offset + size
            for shard_id in range(1, n_shards + 1):
                step_size = min(shard_size, cumulative_size - offset)
                shard_path = self._get_shard_path(split, shard_id, step_size)
                # Select a subset of indices to get only a subset of
                # audio-files/labels for the current shard.
                file_indices = np.arange(offset, offset + step_size)
                shards.append((shard_path, file_indices))
                offset += step_size

        return shards

    def convert(self):

        &quot;&quot;&quot;Convert to TFRecords.&quot;&quot;&quot;
        shard_splits = self._split_data_into_shards()
        _parallelize(self._write_tfrecord_file, shard_splits)

        print('Number of training examples: {}'.format(self.n_train))
        print('Number of testing examples: {}'.format(self.n_test))
        print('Number of validation examples: {}'.format(self.n_val))
        print('TFRecord files saved to {}'.format(self.output_dir))

def main():
    converter = TFRecordsConverter(_DEFAULT_META_CSV,
                                   _DEFAULT_OUTPUT_DIR,
                                   _DEFAULT_NUM_SHARDS_TRAIN,
                                   _DEFAULT_NUM_SHARDS_TEST,
                                   _DEFAULT_NUM_SHARDS_VAL,
                                   _DEFAULT_DURATION,
                                   _DEFAULT_SAMPLE_RATE,
                                   _DEFAULT_TEST_SIZE,
                                   _DEFAULT_VAL_SIZE)
    converter.convert()
        
if __name__ == '__main__':

    main()

</code></pre>
<p>Think i figured it out. I added the below code to the _write_tfrecord_file function:</p>
<pre><code>        if gpus:
          try:
            for gpu in gpus:
              tf.config.experimental.set_memory_growth(gpu, True)
          except RuntimeError as e:
            print(e) ```


It'd be great to see if anyone has any better or alternative solutions! 
</code></pre>
",I'm trying to convert .ogg files to tfrecords. I'm running the below code on my GPU using multiprocessing but my GPU RAM gets allocated 100% and the program crashes. Anyone have some input on using multiprocessing with tensorflow or any documentation to best practices? I haven't been able to find what I'm looking for. Think i figured it out. I added the below code to the _write_tfrecord_file function:,https://stackoverflow.com/questions/71520085,15891508,Requesting (Additional) Resources
71734880,Any example workflow from TensorFlow to OpenMV?,"<p>I have trained an image multi classification model based on MobileNet-V2(Only the Dense layer has been added), and have carried out full integer quantization(INT8), and then exported model.tflite file, using TF Class () to call this model.</p>
<p>Here is my code to quantify it:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np
import pathlib


def representative_dataset():
    for _ in range(100):
        data = np.random.rand(1, 96, 96, 3)  // random tensor for test
        yield [data.astype(np.float32)]


converter = tf.lite.TFLiteConverter.from_saved_model('saved_model/my_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
tflite_quant_model = converter.convert()

tflite_models_dir = pathlib.Path(&quot;/tmp/mnist_tflite_models/&quot;)
tflite_models_dir.mkdir(exist_ok=True, parents=True)

tflite_model_quant_file = tflite_models_dir/&quot;mnist_model_quant.tflite&quot;
tflite_model_quant_file.write_bytes(tflite_quant_model)
</code></pre>
<p>The accuracy of this model is quite good in the test while training. However, when tested on openmv, the same label is output for all objects (although the probability is slightly different).</p>
<p>I looked up some materials, one of them mentioned TF Classify() has offset and scale parameters, which is related to compressing RGB values to [- 1,0] or [0,1] during training, but this parameter is not available in the official API document.</p>
<pre class=""lang-py prettyprint-override""><code>for obj in tf.classify(self.net , img1, min_scale=1.0, scale_mul=0.5, x_overlap=0.0, y_overlap=0.0):
          print(&quot;**********\nTop 1 Detections at [x=%d,y=%d,w=%d,h=%d]&quot; % obj.rect())
          sorted_list = sorted(zip(self.labels, obj.output()), key = lambda x: x[1], reverse = True)
          for i in range(1):
          print(&quot;%s = %f&quot; % (sorted_list[i][0], sorted_list[i][1]))
          return sorted_list[i][0]
</code></pre>
<p>So are there any examples of workflow from tensorflow training model to deployment to openmv?</p>
","I have trained an image multi classification model based on MobileNet-V2(Only the Dense layer has been added), and have carried out full integer quantization(INT8), and then exported model.tflite file, using TF Class () to call this model. Here is my code to quantify it: The accuracy of this model is quite good in the test while training. However, when tested on openmv, the same label is output for all objects (although the probability is slightly different). I looked up some materials, one of them mentioned TF Classify() has offset and scale parameters, which is related to compressing RGB values to [- 1,0] or [0,1] during training, but this parameter is not available in the official API document. So are there any examples of workflow from tensorflow training model to deployment to openmv?",https://stackoverflow.com/questions/71734880,17221142,Lack of Alternative Solutions/Documentation
76103475,tensorflow dataset builder that runs download_and_prepare with multiprocessing,"<p>The TensorFlow Datasets <a href=""https://www.tensorflow.org/datasets/add_dataset"" rel=""nofollow noreferrer"">guide</a> on creating a dataset suggests subclassing the tfds.core.DatasetBuilder. Below is my subclass; it reads NetCDF files and extracts the relevant variables as examples.</p>
<p>How can I improve performance with parallel processing? When executing the <code>download_and_prepare</code> pethod, I'm only ever at 100% CPU and can't find any documentation on using multiple threads or cores. The fact that the class utilizes iteration in Python over a generator makes me afraid it's not possible. Is there an alternative approach?</p>
<pre><code>import tensorflow as tf
import tensorflow_datasets as tfds
import xarray as xr


class Builder(tfds.core.GeneratorBasedBuilder):

    VERSION = tfds.core.Version('0.1.0')

    def _info(self):
        keys = {'image': image_shape, 'label': label_shape}
        return self.dataset_info_from_configs(
            features=tfds.features.FeaturesDict({
                k: tfds.features.Tensor(shape=(v,), dtype=np.float32)
                for k, v in keys.items()
            }),
            supervised_keys=tuple(keys),
        )

    def _split_generators(self, *args):
        return {
            'train': self._generate_examples(path=DATA_DIR)
        }

    def _generate_examples(self, path):
        for item in path.iterdir():
            dataset = self.load_tensors(item)
            for j, jtem in enumerate(dataset.as_numpy_iterator()):
                yield f'{item.name}-{j}', jtem

    def load_tensors(self, path):
        vars = list(self.info.features)
        ds = xr.open_dataset(path)[vars]
        return tf.data.Dataset.from_tensor_slices({i: ds[i].values for i in vars})

</code></pre>
","The TensorFlow Datasets guide on creating a dataset suggests subclassing the tfds.core.DatasetBuilder. Below is my subclass; it reads NetCDF files and extracts the relevant variables as examples. How can I improve performance with parallel processing? When executing the download_and_prepare pethod, I'm only ever at 100% CPU and can't find any documentation on using multiple threads or cores. The fact that the class utilizes iteration in Python over a generator makes me afraid it's not possible. Is there an alternative approach?",https://stackoverflow.com/questions/76103475,687112,Documentation Completeness
76440119,Can't fold BatchNorm with Conv2D in Keras QAT basic example,"<p>I'm currently trying to use Keras' Quantization Aware Training, specifically because I need to do 8bit inference on a low-precision device. For this reason, I need to fold the batch norm onto the Convolution to avoid having the 32-bit moving mean and variance. The sample code I'm starting with is the following (tf1.15, tensorflow-model-optimization 0.6.0):</p>
<pre><code>    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),
        tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Activation('relu'),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1000)
    ])



    quantize_model = tfmot.quantization.keras.quantize_model

    # q_aware stands for for quantization aware.
    q_aware_model = quantize_model(model)

    # `quantize_model` requires a recompile.
    q_aware_model.compile(optimizer='adam',
                loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=smooth),
                metrics=['accuracy'])

    q_aware_model.summary()
</code></pre>
<p>The documentation states that 'Conv2D+BN+ReLU' should have the BatchNorm folded but that isn't the case in the .h5 file produced.</p>
","I'm currently trying to use Keras' Quantization Aware Training, specifically because I need to do 8bit inference on a low-precision device. For this reason, I need to fold the batch norm onto the Convolution to avoid having the 32-bit moving mean and variance. The sample code I'm starting with is the following (tf1.15, tensorflow-model-optimization 0.6.0): The documentation states that 'Conv2D+BN+ReLU' should have the BatchNorm folded but that isn't the case in the .h5 file produced.",https://stackoverflow.com/questions/76440119,21807405,Documentation Replication on Other Examples
33720645,Why is this TensorFlow implementation vastly less successful than Matlab's NN?,"<p>As a toy example I'm trying to fit a function <code>f(x) = 1/x</code> from 100 no-noise data points. The matlab default implementation is phenomenally successful with mean square difference ~10^-10, and interpolates perfectly.</p>

<p>I implement a neural network with one hidden layer of 10 sigmoid neurons. I'm a beginner at neural networks so be on your guard against dumb code.</p>

<pre><code>import tensorflow as tf
import numpy as np

def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)

#Can't make tensorflow consume ordinary lists unless they're parsed to ndarray
def toNd(lst):
    lgt = len(lst)
    x = np.zeros((1, lgt), dtype='float32')
    for i in range(0, lgt):
        x[0,i] = lst[i]
    return x

xBasic = np.linspace(0.2, 0.8, 101)
xTrain = toNd(xBasic)
yTrain = toNd(map(lambda x: 1/x, xBasic))

x = tf.placeholder(""float"", [1,None])
hiddenDim = 10

b = bias_variable([hiddenDim,1])
W = weight_variable([hiddenDim, 1])

b2 = bias_variable([1])
W2 = weight_variable([1, hiddenDim])

hidden = tf.nn.sigmoid(tf.matmul(W, x) + b)
y = tf.matmul(W2, hidden) + b2

# Minimize the squared errors.
loss = tf.reduce_mean(tf.square(y - yTrain))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

# For initializing the variables.
init = tf.initialize_all_variables()

# Launch the graph
sess = tf.Session()
sess.run(init)

for step in xrange(0, 4001):
    train.run({x: xTrain}, sess)
    if step % 500 == 0:
        print loss.eval({x: xTrain}, sess)
</code></pre>

<p>Mean square difference ends at ~2*10^-3, so about 7 orders of magnitude worse than matlab. Visualising with</p>

<pre><code>xTest = np.linspace(0.2, 0.8, 1001)
yTest = y.eval({x:toNd(xTest)}, sess)  
import matplotlib.pyplot as plt
plt.plot(xTest,yTest.transpose().tolist())
plt.plot(xTest,map(lambda x: 1/x, xTest))
plt.show()
</code></pre>

<p>we can see the fit is systematically imperfect:
<a href=""https://i.stack.imgur.com/Blxq9.png""><img src=""https://i.stack.imgur.com/Blxq9.png"" alt=""enter image description here""></a>
while the matlab one looks perfect to the naked eye with the differences uniformly &lt; 10^-5:
<a href=""https://i.stack.imgur.com/kC8aJ.jpg""><img src=""https://i.stack.imgur.com/kC8aJ.jpg"" alt=""enter image description here""></a>
I have tried to replicate with TensorFlow the diagram of the Matlab network:</p>

<p><a href=""https://i.stack.imgur.com/ORLXL.png""><img src=""https://i.stack.imgur.com/ORLXL.png"" alt=""enter image description here""></a></p>

<p>Incidentally, the diagram seems to imply a tanh rather than sigmoid activation function. I cannot find it anywhere in documentation to be sure. However, when I try to use a tanh neuron in TensorFlow the fitting quickly fails with <code>nan</code> for variables. I do not know why.</p>

<p>Matlab uses Levenberg–Marquardt training algorithm. Bayesian regularization is even more successful with mean squares at 10^-12 (we are probably in the area of vapours of float arithmetic).</p>

<p>Why is TensorFlow implementation so much worse, and what can I do to make it better?</p>
","As a toy example I'm trying to fit a function f(x) = 1/x from 100 no-noise data points. The matlab default implementation is phenomenally successful with mean square difference ~10^-10, and interpolates perfectly. I implement a neural network with one hidden layer of 10 sigmoid neurons. I'm a beginner at neural networks so be on your guard against dumb code. Mean square difference ends at ~2*10^-3, so about 7 orders of magnitude worse than matlab. Visualising with we can see the fit is systematically imperfect: while the matlab one looks perfect to the naked eye with the differences uniformly &lt; 10^-5: I have tried to replicate with TensorFlow the diagram of the Matlab network: Incidentally, the diagram seems to imply a tanh rather than sigmoid activation function. I cannot find it anywhere in documentation to be sure. However, when I try to use a tanh neuron in TensorFlow the fitting quickly fails with nan for variables. I do not know why. Matlab uses Levenberg–Marquardt training algorithm. Bayesian regularization is even more successful with mean squares at 10^-12 (we are probably in the area of vapours of float arithmetic). Why is TensorFlow implementation so much worse, and what can I do to make it better?",https://stackoverflow.com/questions/33720645,1715157,Lack of Alternative Solutions/Documentation
36339059,Exporter classification_signature,"<p>I'm trying to modify the <a href=""https://tensorflow.github.io/serving/serving_basic"" rel=""nofollow"">serving tutorial</a> to work with my model, which is basically the CIFAR example modified to work with a CSV file and JPEGs. I can't seem to find the documentation for the Exporter class, but here is what I have so far. It's in the train() function in the cifar10_train.py file:</p>

<pre><code>  # Save the model checkpoint periodically.
  if step % 10 == 0 or (step + 1) == FLAGS.max_steps:
    checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')
    saver.save(sess, checkpoint_path, global_step=step)

    export_dir = FLAGS.export_dir
    print 'Exporting trained model to ' + FLAGS.export_dir
    export_saver = tf.train.Saver(sharded=True)
    model_exporter = exporter.Exporter(export_saver)
    #
    # TODO: where to find x and y?
    #
    signature = exporter.classification_signature(input_tensor=x, scores_tensor=y)
    model_exporter.init(sess.graph.as_graph_def(),
                        default_graph_signature=signature)
    model_exporter.export(export_dir, tf.constant(FLAGS.export_version), sess)
</code></pre>

<p>Here is the code I use to train the model:</p>

<pre><code>  labels = numpy.fromfile(os.path.join(data_dir, 'labels.txt'), dtype=numpy.int32, count=-1, sep='\n')

  filenames_and_labels = []

  start_image_number = 1
  end_image_number = 8200

  for i in xrange(start_image_number, end_image_number):
    file_name = os.path.join(data_dir, 'image%d.jpg' % i)
    label = labels[i - 1]
    filenames_and_labels.append(file_name + "","" + str(label))


  print('Reading filenames for ' + str(len(filenames_and_labels)) + ' files (from ' + str(start_image_number) + ' to ' + str(end_image_number) + ')')

  for filename_and_label in filenames_and_labels:
    array = filename_and_label.split("","")
    f = array[0]
    # print(array)
    if not tf.gfile.Exists(f):
      raise ValueError('Failed to find file: ' + f)

  # Create a queue that produces the filenames to read.
  filename_and_label_queue = tf.train.string_input_producer(filenames_and_labels)

  filename_and_label_tensor = filename_and_label_queue.dequeue()
  filename, label = tf.decode_csv(filename_and_label_tensor, [[""""], [""""]], "","")
  file_contents = tf.read_file(filename)
  image = tf.image.decode_jpeg(file_contents)
</code></pre>

<p>Any ideas how I can set up Exporter correctly?</p>
","I'm trying to modify the serving tutorial to work with my model, which is basically the CIFAR example modified to work with a CSV file and JPEGs. I can't seem to find the documentation for the Exporter class, but here is what I have so far. It's in the train() function in the cifar10_train.py file: Here is the code I use to train the model: Any ideas how I can set up Exporter correctly?",https://stackoverflow.com/questions/36339059,563762,Lack of Alternative Solutions/Documentation
36585071,"Cannot feed value of shape (500,) for Tensor 'x_17:0', which has shape '(?, 500)'","<p>I'm just learning TensorFlow, so sorry if this is obvious. I've checked the documentation and experimented quite a bit and I just can't seem to get this to work.</p>

<pre><code>def train_network():
    OUT_DIMS = 1
    FIN_SIZE = 500
    x = tf.placeholder(tf.float32, [OUT_DIMS, FIN_SIZE], name=""x"")
    w = tf.Variable(tf.zeros([FIN_SIZE, OUT_DIMS]), name=""w"")
    b = tf.Variable(tf.zeros([OUT_DIMS]), name=""b"")
    y = tf.tanh(tf.matmul(x, w) + b)

    yhat = tf.placeholder(tf.float32, [None, OUT_DIMS])
    cross_entropy = -tf.reduce_sum(yhat*tf.log(y))

    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)

    # Launch the model
    init = tf.initialize_all_variables()
    sess = tf.Session()
    sess.run(init)

    for this_x, this_y in yield_financials():
        sess.run(train_step, feed_dict={x:    this_x,
                                        yhat: this_y})
        print(end=""."")
        sys.stdout.flush()
</code></pre>

<p>yield_financials() outputs an numpy array of 500 numbers and the number that I want it to guess. I've tried shuffling OUT_DIMS and FIN_SIZE around, I tried accumulating them into batches to more closely match what the tutorial looked like, I tried setting OUT_DIMS to 0, removing it entirely, and I tried replacing None with other numbers, but have not made any progress.</p>
","I'm just learning TensorFlow, so sorry if this is obvious. I've checked the documentation and experimented quite a bit and I just can't seem to get this to work. yield_financials() outputs an numpy array of 500 numbers and the number that I want it to guess. I've tried shuffling OUT_DIMS and FIN_SIZE around, I tried accumulating them into batches to more closely match what the tutorial looked like, I tried setting OUT_DIMS to 0, removing it entirely, and I tried replacing None with other numbers, but have not made any progress.",https://stackoverflow.com/questions/36585071,3471004,Inadequate Examples
37388604,How can I use intersphinx with Tensorflow and numpydoc?,"<p>The main question here is where (if) there is an <code>objects.inv</code> for TensorFlow, but an example how to actually use it would be nice.</p>

<p>For example, I currently have the following docstring:</p>

<pre><code>""""""
Load the weights of a model stored in saver.

Parameters
----------
checkpoint_dir : str
    The directory of checkpoints.
sess : tf.Session
    A Session to use to restore the parameters.
saver : tf.train.Saver
""""""
</code></pre>

<p>How do I use intersphinx to automatically link the object to the TensorFlow documentation?</p>
","The main question here is where (if) there is an objects.inv for TensorFlow, but an example how to actually use it would be nice. For example, I currently have the following docstring: How do I use intersphinx to automatically link the object to the TensorFlow documentation?",https://stackoverflow.com/questions/37388604,562769,Requesting (Additional) Resources
37980518,meaning of `grad` parameter in tensorflow gradient functions (python),"<p>What does the <code>grad</code> parameter in tensorflow gradient functions in python (like the example below from the docs) represent?</p>

<pre><code>@tf.RegisterGradient(""Sub"")
def _sub_grad(unused_op, grad):
  return grad, tf.neg(grad)
</code></pre>

<p>The docs say it represents ""the gradients with respect to each output of the op."" Which gradients? The gradients of Each output of the op, with respect to each output of the op?</p>

<p>The op here is x - y. Does that mean the <code>grad</code> parameter in this function refers to </p>

<p><a href=""https://i.stack.imgur.com/zl9ck.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zl9ck.png"" alt=""grad""></a></p>

<p>?</p>

<p>This would be consistent with the output of the function, which is</p>

<p><a href=""https://i.stack.imgur.com/gVwLR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gVwLR.png"" alt=""enter image description here""></a>,</p>

<p>but I wanted to make sure.</p>

<p>Thanks in advance for your clarification!</p>
","What does the grad parameter in tensorflow gradient functions in python (like the example below from the docs) represent? The docs say it represents ""the gradients with respect to each output of the op."" Which gradients? The gradients of Each output of the op, with respect to each output of the op? The op here is x - y. Does that mean the grad parameter in this function refers to ? This would be consistent with the output of the function, which is , but I wanted to make sure. Thanks in advance for your clarification!",https://stackoverflow.com/questions/37980518,1387992,Documentation Completeness
38190365,How does one initialize a variable with tf.get_variable and a numpy value in TensorFlow?,"<p>I wanted to initialize some of the variable on my network with numpy values. For the sake of the example consider:</p>

<pre><code>init=np.random.rand(1,2)
tf.get_variable('var_name',initializer=init)
</code></pre>

<p>when I do that I get an error:</p>

<pre><code>ValueError: Shape of a new variable (var_name) must be fully defined, but instead was &lt;unknown&gt;.
</code></pre>

<p>why is it that I am getting that error? </p>

<p>To try to fix it I tried doing:</p>

<pre><code>tf.get_variable('var_name',initializer=init, shape=[1,2])
</code></pre>

<p>which yielded a even weirder error:</p>

<pre><code>TypeError: 'numpy.ndarray' object is not callable
</code></pre>

<p>I tried reading <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#variable_scope"" rel=""noreferrer"">the docs and examples</a> but it didn't really help.</p>

<p>Is it not possible to initialize variables with numpy arrays with the get_variable method in TensorFlow?</p>
",I wanted to initialize some of the variable on my network with numpy values. For the sake of the example consider: when I do that I get an error: why is it that I am getting that error? To try to fix it I tried doing: which yielded a even weirder error: I tried reading the docs and examples but it didn't really help. Is it not possible to initialize variables with numpy arrays with the get_variable method in TensorFlow?,https://stackoverflow.com/questions/38190365,1601580,Requesting (Additional) Resources
38381887,How to read json files in Tensorflow?,"<p>I'm trying to write a function, that reads json files in tensorflow. The json files have the following structure: </p>

<pre><code>{
    ""bounding_box"": {
        ""y"": 98.5, 
        ""x"": 94.0, 
        ""height"": 197, 
        ""width"": 188
     }, 
    ""rotation"": {
        ""yaw"": -27.97019577026367,
        ""roll"": 2.206029415130615, 
        ""pitch"": 0.0}, 
        ""confidence"": 3.053506851196289, 
        ""landmarks"": {
            ""1"": {
                ""y"": 180.87722778320312, 
                ""x"": 124.47326660156205}, 
            ""0"": {
                ""y"": 178.60653686523438, 
                ""x"": 183.41931152343795}, 
            ""2"": {
                ""y"": 224.5936889648438, 
                ""x"": 141.62365722656205
}}}
</code></pre>

<p>I only need the bounding box information. There are a few examples on how to write read_and_decode-functions, and I'm trying to transform these examples into a function for json files, but there are still a lot of questions...: </p>

<pre><code>def read_and_decode(filename_queue):

  reader = tf.WhichKindOfReader() # ??? 
  _, serialized_example = reader.read(filename_queue)
  features = tf.parse_single_example( 
      serialized_example,

      features={

          'bounding_box':{ 

              'y': tf.VarLenFeature(&lt;whatstheproperdatatype&gt;) ???
              'x': 
              'height': 
              'width': 

          # I only need the bounding box... - do I need to write 
          # the format information for the other features...???

          }
      })

  y=tf.decode() # decoding necessary?
  x=
  height=
  width= 

  return x,y,height,width
</code></pre>

<p>I've done research on the internet for hours, but can't find anything really detailled on how to read json in tensorflow... </p>

<p>Maybe someone can give me a clue...</p>
","I'm trying to write a function, that reads json files in tensorflow. The json files have the following structure: I only need the bounding box information. There are a few examples on how to write read_and_decode-functions, and I'm trying to transform these examples into a function for json files, but there are still a lot of questions...: I've done research on the internet for hours, but can't find anything really detailled on how to read json in tensorflow... Maybe someone can give me a clue...",https://stackoverflow.com/questions/38381887,6539009,Lack of Alternative Solutions/Documentation
38678820,How do I create a single script file for when I do and don't want to collect TensorBoard statistics?,"<p>I want to have a single script, that either collects tensorboard data or not, depending on how I run it. I am aware that I can pass flags to tell my script how I want it to be run. I could even hard code it in the script and just manually change the script. </p>

<p>Either solution has a bigger problem. I find myself having to write an if statement everywhere on my script when I want the summary writer operations to be ran or not. For example I find that I would have to do something like:</p>

<pre><code>if tb_sys_arg = 'tensorboard':
    merged = tf.merge_all_summaries()
</code></pre>

<p>and then depending on the value of <code>tb_sys_arg</code> run the summaries or not, as in:</p>

<pre><code>if tb_sys_arg = 'tensorboard':
    merged = tf.merge_all_summaries()
else:
    train_writer = tf.train.SummaryWriter(tensorboard_data_dump_train, sess.graph)
</code></pre>

<p>this seems really silly to me. I'd rather not have to do that. Is this the right way to do this? I just don't want to collect statistics each time I run my main script but I also don't want to have two separate scripts either.</p>

<hr>

<p>As an anecdotical story, few months ago I started using TensorBoard and it seems I have been running my main file as follow:</p>

<pre><code>python main.py —logdir=/tmp/mdl_logs
</code></pre>

<p>so that it collects tensorboard data. But realized that I don't think I need that last flag to collect tensorboard data. Its been so long that now I forget if I actually need that. I've been reading the documentation and tutorials but it seems I don't need that last flag (its only needed to run the web app as in <code>tensorboard --logdir=path/to/log-directory</code>, right?) Have I been doing this wrong all this time?</p>
","I want to have a single script, that either collects tensorboard data or not, depending on how I run it. I am aware that I can pass flags to tell my script how I want it to be run. I could even hard code it in the script and just manually change the script. Either solution has a bigger problem. I find myself having to write an if statement everywhere on my script when I want the summary writer operations to be ran or not. For example I find that I would have to do something like: and then depending on the value of tb_sys_arg run the summaries or not, as in: this seems really silly to me. I'd rather not have to do that. Is this the right way to do this? I just don't want to collect statistics each time I run my main script but I also don't want to have two separate scripts either. As an anecdotical story, few months ago I started using TensorBoard and it seems I have been running my main file as follow: so that it collects tensorboard data. But realized that I don't think I need that last flag to collect tensorboard data. Its been so long that now I forget if I actually need that. I've been reading the documentation and tutorials but it seems I don't need that last flag (its only needed to run the web app as in tensorboard --logdir=path/to/log-directory, right?) Have I been doing this wrong all this time?",https://stackoverflow.com/questions/38678820,1601580,Documentation Ambiguity
38788912,Tensorflow 'features' format,"<p>I'm a total begginer with AI and tensorflow, so please forgive if this is a dumb question.
I've trained a tensorflow network using a script based on this tutorial:</p>

<p><a href=""https://www.tensorflow.org/versions/r0.10/tutorials/wide_and_deep/index.html"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.10/tutorials/wide_and_deep/index.html</a></p>

<p>I believe training was ok.
Now I whant to run this method to make a prediction for a single input:</p>

<pre><code>tf.contrib.learn.DNNClassifier.predict_proba(x=x)
</code></pre>

<p>But I cannot find any documentation on how to build the ""x"" parameter...
I tryed: </p>

<pre><code> x = {k: tf.SparseTensor(indices=[[0, 0]], values=[d_data[k]], shape=[1, 1]) for k in COLUMNS}
</code></pre>

<p>Where:
<strong>d_data</strong> is a dictionary containing about 150 key/value pairs.
<strong>COLUMNS</strong> is a list with all the keys needed. 
This same setup was used to train the network.</p>

<p>But got the error:  </p>

<pre><code>AttributeError: 'dict' object has no attribute 'dtype'
</code></pre>

<p>So... x should not be a 'dict'... but what should it be then?
Can anyone give me some directions?</p>

<p>Thanks a lot.</p>
","I'm a total begginer with AI and tensorflow, so please forgive if this is a dumb question. I've trained a tensorflow network using a script based on this tutorial: https://www.tensorflow.org/versions/r0.10/tutorials/wide_and_deep/index.html I believe training was ok. Now I whant to run this method to make a prediction for a single input: But I cannot find any documentation on how to build the ""x"" parameter... I tryed: Where: d_data is a dictionary containing about 150 key/value pairs. COLUMNS is a list with all the keys needed. This same setup was used to train the network. But got the error: So... x should not be a 'dict'... but what should it be then? Can anyone give me some directions? Thanks a lot.",https://stackoverflow.com/questions/38788912,6430078,Lack of Alternative Solutions/Documentation
39251552,"TensorFlow first attempt, bad results","<p>I can't solve my problem, help me please. It's my first attempt of neural networks, i tried to make nn which can check is number betwen (3:6) or not. I used several docs in internet and make some listing. But it has not working results. It's always ""not in (3:6)"". And I can't to understand what I'm doing wrong.</p>

<pre><code>#Is number between (3:6)
import tensorflow as tf
import numpy as np
import random

def is_num_between(num):
    right_border = 6
    left_border = 3
    if num &lt; right_border and num &gt; left_border:
        return 1
    return 0

def is_num_around(num):
    right_border = 6
    left_border = 3
    if num &lt;= left_border or num &gt;= right_border:
        return 1
    return 0

def init_weights(shape):
    return tf.Variable(tf.random_normal(shape, stddev=0.01))

def model(X, w_h, w_o):
    h = tf.nn.tanh(tf.matmul(X, w_h))
    return tf.nn.sigmoid(tf.matmul(h, w_o))

def included_or_not(i, prediction):
    return [str(i) + "" is in (3:6)"", str(i) + "" not in (3:6)""][prediction]

NUM_COUNT = 2
NUM_HIDDEN = 10
BATCH_SIZE = 10000

pre_trX = [np.random.random_sample() * 10 for i in range(100000)]
pre_trY1 = [is_num_between(i) for i in pre_trX]
pre_trY2 = [is_num_around(i) for i in pre_trX]

trX = np.array([np.array([pre_trX[i], 1]) for i in range(len(pre_trX))])
trY = np.array([np.array([pre_trY1[i], pre_trY2[i]]) for i in range(len(pre_trX))])


# print(type(trX))
# print(pre_trX)
# print(pre_trY1)
# print(pre_trY2)
# print(trX[0])
# exit()

X = tf.placeholder(""float"", [None, NUM_COUNT])
Y = tf.placeholder(""float"", [None, 2])

w_h = init_weights([NUM_COUNT, NUM_HIDDEN])
w_o = init_weights([NUM_HIDDEN, 2])

py_X = model(X, w_h, w_o)
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_X, Y))
train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

predict_op = tf.argmax(py_X, 1)


with tf.Session() as sess:
    tf.initialize_all_variables().run()

    for epoch in range(200):
        p = np.random.permutation(range(len(trX)))
        trX, trY = trX[p], trY[p]

        for start in range(0, len(trX), BATCH_SIZE):
            end = start + BATCH_SIZE
            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})

        print(epoch, np.mean(np.argmax(trY, axis=1) ==
                         sess.run(predict_op, feed_dict={X: trX, Y: trY})))


    # Tipo natrenirovana, nado ee potestit
    def check_nnetwork():
        numbers = [np.array([np.random.random_sample()*10, 1])]
        teX = np.array(numbers)
        teY = sess.run(predict_op, feed_dict={X: teX})
        output = np.vectorize(included_or_not)(""%.3f"" % numbers[0][0], teY)
        print(output)

    for i in range(40):
        check_nnetwork()
</code></pre>
","I can't solve my problem, help me please. It's my first attempt of neural networks, i tried to make nn which can check is number betwen (3:6) or not. I used several docs in internet and make some listing. But it has not working results. It's always ""not in (3:6)"". And I can't to understand what I'm doing wrong.",https://stackoverflow.com/questions/39251552,3178853,Requesting (Additional) Resources
39638468,What is the right way to make a barrier in distributed tensorflow?,"<p>During distributed training I want to sync after each epoch, do some calculations on chief worker and proceed or stop training depending on these calculations. I need a barrier to do so.</p>

<p>I don't see anything similar in documentation, so I implemented solution based on queues (similar to how gradients are stored and applied in distributed training):</p>

<pre><code>def build_barrier(tasks, task_index, barrier_name):
    queues = []
    for i, task in enumerate(tasks):
        with tf.device('%s/cpu:0' % task):
            with tf.name_scope(barrier_name):
                queues.append(
                    tf.FIFOQueue(
                        len(tasks),
                        (tf.float32),
                        shapes=(()),
                        name=str(i),
                        shared_name=str(i)))

    with tf.control_dependencies([queue.enqueue(1.) for queue in queues]):
        return queues[task_index].dequeue_many(len(tasks))
</code></pre>

<p>The idea is to create a queue per worker. For 'signal' I push a token in each queue and for 'join' I dequeue so many tokens from corresponding queue how many tasks I want to synchronize.</p>

<p>The question is: is it the right way to do or there is a better way?</p>
","During distributed training I want to sync after each epoch, do some calculations on chief worker and proceed or stop training depending on these calculations. I need a barrier to do so. I don't see anything similar in documentation, so I implemented solution based on queues (similar to how gradients are stored and applied in distributed training): The idea is to create a queue per worker. For 'signal' I push a token in each queue and for 'join' I dequeue so many tokens from corresponding queue how many tasks I want to synchronize. The question is: is it the right way to do or there is a better way?",https://stackoverflow.com/questions/39638468,2570037,Inadequate Examples
39770254,Fast softmax regression implementation in tensorflow,"<p>I am trying to implement the softmax regression model in tensorflow in order to make a benchmark with other mainstream deep-learning frameworks. The official documentation code is slow because of the <a href=""https://github.com/tensorflow/tensorflow/issues/2919"" rel=""nofollow"">feed_dict issue</a> in tensorflow. I am trying to serve the data as tensorflow constant but I don't know the most efficient way to do that. For now I just use the single batch as constant and trained through that batch. What are the efficient solutions of making minibatched solution of that code? Here is my code:</p>

<pre><code>from tensorflow.examples.tutorials.mnist import input_data

import tensorflow as tf
import numpy as np

mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)
batch_xs, batch_ys = mnist.train.next_batch(100)

x = tf.constant(batch_xs, name=""x"")
W = tf.Variable(0.1*tf.random_normal([784, 10]))
b = tf.Variable(tf.zeros([10]))
logits = tf.matmul(x, W) + b

batch_y = batch_ys.astype(np.float32)
y_ = tf.constant(batch_y, name=""y_"")

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_))
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
....
# Minitbatch is never updated during that for loop
for i in range(5500):
    sess.run(train_step)
</code></pre>
",I am trying to implement the softmax regression model in tensorflow in order to make a benchmark with other mainstream deep-learning frameworks. The official documentation code is slow because of the feed_dict issue in tensorflow. I am trying to serve the data as tensorflow constant but I don't know the most efficient way to do that. For now I just use the single batch as constant and trained through that batch. What are the efficient solutions of making minibatched solution of that code? Here is my code:,https://stackoverflow.com/questions/39770254,5251987,Requesting (Additional) Resources
40258943,"Using height, width information stored in a TFRecords file to set shape of a Tensor","<p>I have converted a directory of images and their labels into a TFRecords file, the feature maps include <code>image_raw</code>, <code>label</code>, <code>height</code>, <code>width</code> and <code>depth</code>. The function is as follows:</p>

<pre><code>def convert_to_tfrecords(data_samples, filename):
    def _int64_feature(value):
        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))
    def _bytes_feature(value):
        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
    writer = tf.python_io.TFRecordWriter(filename)
    for fname, lb in data_samples:
        im = cv2.imread(fname, cv2.IMREAD_UNCHANGED)
        image_raw = im.tostring()
        feats = tf.train.Features(
            feature =
            {
                'image_raw': _bytes_feature(image_raw),
                'label': _int64_feature(int(lb)),
                'height': _int64_feature(im.shape[0]),
                'width': _int64_feature(im.shape[1]),
                'depth': _int64_feature(im.shape[2])
            }
        )
        example = tf.train.Example(features=feats)
        writer.write(example.SerializeToString())
    writer.close()
</code></pre>

<p>Now, I would like to read this TFRecords file to feed a input pipeline. However, since <code>image_raw</code> has been flattened, we need to reshape it into the original <code>[height, width, depth]</code> size. So how can I get the values of <code>height</code>, <code>width</code> and <code>depth</code> from the TFRecords file? It seems the following code cannot work because <code>height</code> is a Tensor without values.</p>

<pre><code>def read_and_decode(filename_queue):
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
    feats = {
        'image_raw': tf.FixedLenFeature([], tf.string),
        'label': tf.FixedLenFeature([], tf.int64),
        'height': tf.FixedLenFeature([], tf.int64),
        'width': tf.FixedLenFeature([], tf.int64),
        'depth': tf.FixedLenFeature([], tf.int64)
    }
    features = tf.parse_single_example(serialized_example, features=feats)
    image = tf.decode_raw(features['image_raw'], tf.uint8)
    label = tf.cast(features['label'], tf.int32)
    height = tf.cast(features['height'], tf.int32)
    width = tf.cast(features['width'], tf.int32)
    depth = tf.cast(features['depth'], tf.int32)
    image = tf.reshape(image, [height, width, depth]) # &lt;== not work
    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5
    return image, label
</code></pre>

<p>When I read the Tensorflow's official documents, I found they usually pass into a known size, saying <code>[224,224,3]</code>. However, I don't like it, because this information has been stored into the TFRecords file, and manually passing into fixed size cannot ensure the size is consistent with the data stored in the file.</p>

<p>So any ideas?</p>
","I have converted a directory of images and their labels into a TFRecords file, the feature maps include image_raw, label, height, width and depth. The function is as follows: Now, I would like to read this TFRecords file to feed a input pipeline. However, since image_raw has been flattened, we need to reshape it into the original [height, width, depth] size. So how can I get the values of height, width and depth from the TFRecords file? It seems the following code cannot work because height is a Tensor without values. When I read the Tensorflow's official documents, I found they usually pass into a known size, saying [224,224,3]. However, I don't like it, because this information has been stored into the TFRecords file, and manually passing into fixed size cannot ensure the size is consistent with the data stored in the file. So any ideas?",https://stackoverflow.com/questions/40258943,1659534,Requesting (Additional) Resources
40435329,TensorFlow:What should be the first parameter for prediction using sess.run() in TensorFlow after loading a saved model.ckpt file?,"<p>I am new to TensorFlow and machine learning. I am trying to classify two objects a cup and a pendrive (jpeg images). I have trained and exported a model.ckpt successfully. Now I am trying to restore the saved model.ckpt for prediction of an image. Here is the script:</p>

<pre><code>with tf.Session() as sess:
   saver.restore(sess, ""./model.ckpt"")
   print ""...Model Loaded...""   
   x_ = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE , IMAGE_SIZE , IMAGE_CHANNELS])
   y_ = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES])
   keep_prob = tf.placeholder(tf.float32)

   init = tf.initialize_all_variables()

   sess.run(init)
   my_classification = sess.run(___________ , feed_dict={x_:image})
   print 'Neural Network predicted', my_classification[0], ""for your image""
</code></pre>

<p>In the above script what should I use as the first parameter in sess.run() ? I have read many stackoverflow and github posts but havent found a solution that works for my case. The TensorFlow Documentation is also not very clear.</p>

<p>Thank you in advance</p>
",I am new to TensorFlow and machine learning. I am trying to classify two objects a cup and a pendrive (jpeg images). I have trained and exported a model.ckpt successfully. Now I am trying to restore the saved model.ckpt for prediction of an image. Here is the script: In the above script what should I use as the first parameter in sess.run() ? I have read many stackoverflow and github posts but havent found a solution that works for my case. The TensorFlow Documentation is also not very clear. Thank you in advance,https://stackoverflow.com/questions/40435329,6438307,Documentation Ambiguity
40668712,Enqueue and increment variable in Tensor Flow,"<p>How can I make a Tensor Flow graph push an incrementing number to a queue?</p>

<p>I am just doing this for learning purposes, so I'd prefer if you kept it similar to what I'm doing (and correct what I'm doing wrong). This is my code:</p>

<pre><code>import tensorflow as tf

# create queue
queue = tf.RandomShuffleQueue(capacity=10, min_after_dequeue=1, dtypes=tf.float32)

# create variables, and ""add"" operation
push_var = tf.Variable(initial_value=1.0, trainable=False)
add = push_var.assign_add(1)

# enqueue operation
push = queue.enqueue(add)

# dequeue operation
pop = queue.dequeue()

sess = tf.InteractiveSession()

tf.initialize_all_variables().run()

# add var to stack
sess.run(push) # push_var = 2 after ran
sess.run(push) # push_var = 3 after ran
sess.run(push) # push_var = 4 after ran
sess.run(push) # push_var = 5 after ran
sess.run(push) # push_var = 6 after ran
sess.run(push) # push_var = 7 after ran
sess.run(push) # push_var = 8 after ran

# pop variable (random shuffle)
print sess.run(pop)
print sess.run(pop)

sess.close()
</code></pre>

<p>Output:</p>

<pre><code>8
8
</code></pre>

<p>I'm expecting it to be 2 random numbers between 2 and 8. Instead, it always is popping the current value of the variable.</p>

<p>Is this because instead of pushing the actual value of the variable I am instead pushing a pointer to the variable? Tensor Flow's <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#Variable"" rel=""nofollow noreferrer"">documentation</a> says <code>assign_add</code> returns </p>

<blockquote>
  <p>A Tensor that will hold the new value of this variable after the
  addition has completed.</p>
</blockquote>

<p>Again, I'm trying to learn about Tensor Flow. I'd appreciate any learning resources (besides the TensorFlow website) if you have any! Thanks.</p>

<p><strong>EDIT:</strong></p>

<p>Changing <code>push = queue.enqueue(add)</code> to <code>push = queue.enqueue(add + 0)</code> results in expected behavior. Could someone explain this?</p>
","How can I make a Tensor Flow graph push an incrementing number to a queue? I am just doing this for learning purposes, so I'd prefer if you kept it similar to what I'm doing (and correct what I'm doing wrong). This is my code: Output: I'm expecting it to be 2 random numbers between 2 and 8. Instead, it always is popping the current value of the variable. Is this because instead of pushing the actual value of the variable I am instead pushing a pointer to the variable? Tensor Flow's documentation says assign_add returns Again, I'm trying to learn about Tensor Flow. I'd appreciate any learning resources (besides the TensorFlow website) if you have any! Thanks. EDIT: Changing push = queue.enqueue(add) to push = queue.enqueue(add + 0) results in expected behavior. Could someone explain this?",https://stackoverflow.com/questions/40668712,877651,Documentation Replication on Other Examples
40819321,Understanding the conceptual basics of Distributed TensorFlow,"<p>Let me describe the cluster setup first :</p>

<ul>
<li>I have two nodes (each with 2 GPUs). I refer to them as Node A and Node B</li>
<li>Each node has its own SSD storage.</li>
<li><a href=""http://oar.imag.fr/docs/latest/user/commands/oarsub.html"" rel=""nofollow noreferrer"">OAR</a> is the cluster manager that is used.</li>
</ul>

<p>I have gone through the Distributed TensorFlow documentation but there are some functional basics I could not understand properly and hence this question.</p>

<p>Consider the following situation :</p>

<ul>
<li>I have copied around 600 GB of data on Node A.</li>
<li>I can use OAR to specifically ask for allocation of 4 GPUs across the two nodes.</li>
</ul>

<p>If I want to use Distributed TensorFlow to train a model :</p>

<ol>
<li>How do I specify network addresses to tf.train.ClusterSpec ? What are those network addresses ? In the documentation are names such as localhost:2222 the same names reserved for a particular node with the cluster manager ?</li>
<li>My data is copied to node A. During training will TensorFlow itself be responsible for sending this data as input to the GPU that is on node B ?</li>
<li>Will I need to manually create the TensorFlow Graph for each GPU on each node using tf.device() ?</li>
<li>If I also want to use some additional CPU nodes will I have to have their names beforehand and put them in the code ?</li>
</ol>
",Let me describe the cluster setup first : I have gone through the Distributed TensorFlow documentation but there are some functional basics I could not understand properly and hence this question. Consider the following situation : If I want to use Distributed TensorFlow to train a model :,https://stackoverflow.com/questions/40819321,6842947,Documentation Ambiguity
40859416,Confusion about rank and shape in TensorFlow,"<p>I am confused about rank and shape concept of TensorFlow. I have read the details from <a href=""https://www.tensorflow.org/versions/r0.12/resources/dims_types.html"" rel=""nofollow noreferrer"">here</a> and did run some code to clear my concept about them. But I am still confused and need help to understand.</p>

<pre><code>x = tf.placeholder(tf.float32, shape=[2, 12])
print(x.get_shape()) # ==&gt; (2, 12)
print(x[0, :].get_shape())  # ==&gt; (12,)
print(x[1, :].get_shape())  # ==&gt; (12,)
print(x[2, :].get_shape())  # ==&gt; (12,)
print(x[120, :].get_shape())  # ==&gt; (12,)
</code></pre>

<p>I thought <code>x</code> is like a 2d matrix where <code>2</code> is <code>number of rows</code> and <code>12</code> is <code>number of columns</code>. Then why I am getting shape for <code>x[120, :]</code> as <code>(12, )</code>? How even <code>x[120, :]</code> is possible with the given shape?</p>

<p>Besides, since I thought x is a 2D tensor, its rank is also 2 because dimension and rank is the same thing for tensors (according to my understanding). But when I run:</p>

<pre><code>print(x[0].get_shape())
</code></pre>

<p>I am getting this error:</p>

<pre><code>Shape (2, 12) must have rank 1
</code></pre>

<p>It means my understanding is wrong about rank and dimension. What I am missing about rank and dimensions? Is rank and dimension two different things? How the rank of <code>tensor x</code> in the above example is 1? How can I set the rank of a tensor? Can anyone explain in details with some comprehensive examples?</p>
","I am confused about rank and shape concept of TensorFlow. I have read the details from here and did run some code to clear my concept about them. But I am still confused and need help to understand. I thought x is like a 2d matrix where 2 is number of rows and 12 is number of columns. Then why I am getting shape for x[120, :] as (12, )? How even x[120, :] is possible with the given shape? Besides, since I thought x is a 2D tensor, its rank is also 2 because dimension and rank is the same thing for tensors (according to my understanding). But when I run: I am getting this error: It means my understanding is wrong about rank and dimension. What I am missing about rank and dimensions? Is rank and dimension two different things? How the rank of tensor x in the above example is 1? How can I set the rank of a tensor? Can anyone explain in details with some comprehensive examples?",https://stackoverflow.com/questions/40859416,5352399,Documentation Ambiguity
41164274,Creating a Tensor using a pre-populated pd dataframe?,"<p>I am trying to create a item-item based collaborative filtering recommendation engine. Because of the large volume of data flowing through, I have had to use TensorFlow. However I even after spending hours on the documentation and on the internet, I am unable to figure out how to create a tensor using a pre-populated ndarray. </p>

<p>I am trying to transpose a ndarray with user item actions to record the information in a tensor. I have debugged multiple errors including shape mismatches only to find a new error I can't find any information around. Below is the code. Any help/suggestions is highly appreciated.</p>

<pre><code>user_item_matrix = tf.Variable(np.zeros(shape = (num_usr,num_prd)),dtype=tf.int32)
init_op = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init_op)
for index,row in user_item_action.iterrows():
    update_row = usr_ht[row['user_id']]
    update_col = prd_ht[row['product_id']]
# Updating row slices of the tensor
update = [0]*num_prd
update[update_col] = row['action_score']
user_item_matrix = sess.run(tf.scatter_add(user_item_matrix,update_row,np.transpose(update)))
sess.close()
</code></pre>

<p>TypeError: Input 'ref' of 'ScatterAdd' Op requires l-value input</p>

<p>user_item_action.head():
        user_id     product_id        action_score
1354    76864       196823            10
2626    23364       234437            10
6422     8055       231014            10
9877    81965       200476            10
13334   88132       240015            10</p>
","I am trying to create a item-item based collaborative filtering recommendation engine. Because of the large volume of data flowing through, I have had to use TensorFlow. However I even after spending hours on the documentation and on the internet, I am unable to figure out how to create a tensor using a pre-populated ndarray. I am trying to transpose a ndarray with user item actions to record the information in a tensor. I have debugged multiple errors including shape mismatches only to find a new error I can't find any information around. Below is the code. Any help/suggestions is highly appreciated. TypeError: Input 'ref' of 'ScatterAdd' Op requires l-value input user_item_action.head(): user_id product_id action_score 1354 76864 196823 10 2626 23364 234437 10 6422 8055 231014 10 9877 81965 200476 10 13334 88132 240015 10",https://stackoverflow.com/questions/41164274,4425827,Inadequate Examples
41195121,Tensorflow.strided_slice missing argument 'strides'?,"<p>I am trying to run <code>cifar10_train.py</code> according to tutorials, but I got </p>

<pre><code>""cifar10_input.py"", line 87, in read_cifar10
tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)
TypeError: strided_slice() missing 1 required positional argument: 'strides'
</code></pre>

<p>The document says that <code>strides</code> is optional, and it did work properly on Ubuntu before.</p>

<p>My tensorflow version is 0.12.0rc1-cp35-cp35m-win_amd64. I have already installed the newest release.</p>

<p>May I have to pass this argument? I have no idea about it...</p>

<p>UPDATE: I replaced strided_slice with slice, and it works. According to issue#754, <code>strides</code> will be optional at 1.0 release. (maybe?)</p>
","I am trying to run cifar10_train.py according to tutorials, but I got The document says that strides is optional, and it did work properly on Ubuntu before. My tensorflow version is 0.12.0rc1-cp35-cp35m-win_amd64. I have already installed the newest release. May I have to pass this argument? I have no idea about it... UPDATE: I replaced strided_slice with slice, and it works. According to issue#754, strides will be optional at 1.0 release. (maybe?)",https://stackoverflow.com/questions/41195121,7244589,Documentation Completeness
41361766,tensorflow cifar10 tutorial fails,"<p>I have downloaded the CIFAR10 code from the link in the tutorial <a href=""https://www.tensorflow.org/tutorials/deep_cnn/"" rel=""nofollow noreferrer"">here</a> and am trying to run the tutorial. I run it with the command</p>

<pre><code>python cifar10_train.py
</code></pre>

<p>It starts ok and downloads the data file as expected. When it tries to open the input file it fails with the following trace:</p>

<pre><code>Traceback (most recent call last):
  File ""cifar10_train.py"", line 120, in &lt;module&gt;
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""cifar10_train.py"", line 116, in main
    train()
  File ""cifar10_train.py"", line 63, in train
    images, labels = cifar10.distorted_inputs()
  File ""/notebooks/Python Scripts/tensorflowModels/tutorials/image/cifar10/cifar10.py"", line 157, in distorted_inputs
    batch_size=FLAGS.batch_size)
  File ""/notebooks/Python Scripts/tensorflowModels/tutorials/image/cifar10/cifar10_input.py"", line 161, in distorted_inputs
    read_input = read_cifar10(filename_queue)
  File ""/notebooks/Python Scripts/tensorflowModels/tutorials/image/cifar10/cifar10_input.py"", line 87, in read_cifar10
    tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)
TypeError: strided_slice() takes at least 4 arguments (3 given)
</code></pre>

<p>Sure enough, when I investigate the code there is a call in cifar10_input.py to strided_slice() with only 3 arguments:</p>

<pre><code>tf.strided_slice(record_bytes, [0], [label_bytes])
</code></pre>

<p>Whereas the tensorflow documentation does indeed state that there must be at least 4 arguments.</p>

<p>What is going wrong? I have downloaded the latest tensorflow (0.12) and I'm running the master branch of the cifar code.</p>
","I have downloaded the CIFAR10 code from the link in the tutorial here and am trying to run the tutorial. I run it with the command It starts ok and downloads the data file as expected. When it tries to open the input file it fails with the following trace: Sure enough, when I investigate the code there is a call in cifar10_input.py to strided_slice() with only 3 arguments: Whereas the tensorflow documentation does indeed state that there must be at least 4 arguments. What is going wrong? I have downloaded the latest tensorflow (0.12) and I'm running the master branch of the cifar code.",https://stackoverflow.com/questions/41361766,2467383,Documentation Replicability
41515716,Tensorflow tf.contrib.learn.DNNClassifer estimation accuracy does not align to DNNClassifier prediction accuracy,"<p>In general, what alignment should we expect between the accuracy achieved during estimation, and the accuracy achieved during prediction? Simply put, if we achieved an 85% accuracy during estimation, should we expect a similar result during prediction? There are a few other posts discussion prediction accuracy issues, but none appear to ask this question directly.</p>

<p>To be more specific, I am using a DNNClassifier to learn against 1000 rows of feature data, each with 53 characteristics (i.e. columns). This is then tested against 100 samples. The model is configured to classify against two labels: 0 and 1, and achieves the prediction accuracy below.</p>

<p>INFO:tensorflow:Saving evaluation summary for step 45300:
accuracy: 0.846154
accuracy/baseline_label_mean: 0.197802
accuracy/threshold_0.500000_mean: 0.846154
auc: 0.743912
global_step: 45300
labels/actual_label_mean: 0.197802
labels/prediction_mean: 0.283492
loss: 0.398202
precision/positive_threshold_0.500000_mean: 1.0
recall/positive_threshold_0.500000_mean: 0.222222</p>

<p>However, when 15 examples are then used during the prediction step, and where 50% should be classified with a label of 1, they are generally all returned as 0s. Occasionally, a single example will be correctly classified as a 1.</p>

<p>For example:
Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</p>

<p>My DNNClassifier is configured as follows:</p>

<pre><code>m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,
                                       feature_columns=deep_columns,
                                       hidden_units=[32, 16],
                                       activation_fn=tf.nn.relu,
                                       n_classes=2,
                                       optimizer=tf.train.AdagradOptimizer(
                                       learning_rate=0.1))
</code></pre>

<p>and the predict function is called as below:</p>

<pre><code>y = m.predict(input_fn = lambda: input_fn(df_predict))
    predictions = list(itertools.islice(y, 15))
    print(""Predictions: {}"".format(str(predictions)))
    for i in predictions:
        print(i)
</code></pre>

<p>I have two questions.
(1) Am I correct to assume that there should be a direct correlation between estimation and prediction accuracy, and therefore that an 85% estimation accuracy should result in a similar prediction accuracy?</p>

<p>(2) Do the other results provided when training completes (i.e. ""labels/prediction_mean: 0.283492"") provide information that is relevant to the low level of prediction accuracy being achieved? I have not been able to find a definition of these result labels in the TensorFlow documentation, and my apologies if I have missed something.</p>

<p>Many thanks for any thoughts on this.</p>
","In general, what alignment should we expect between the accuracy achieved during estimation, and the accuracy achieved during prediction? Simply put, if we achieved an 85% accuracy during estimation, should we expect a similar result during prediction? There are a few other posts discussion prediction accuracy issues, but none appear to ask this question directly. To be more specific, I am using a DNNClassifier to learn against 1000 rows of feature data, each with 53 characteristics (i.e. columns). This is then tested against 100 samples. The model is configured to classify against two labels: 0 and 1, and achieves the prediction accuracy below. INFO:tensorflow:Saving evaluation summary for step 45300: accuracy: 0.846154 accuracy/baseline_label_mean: 0.197802 accuracy/threshold_0.500000_mean: 0.846154 auc: 0.743912 global_step: 45300 labels/actual_label_mean: 0.197802 labels/prediction_mean: 0.283492 loss: 0.398202 precision/positive_threshold_0.500000_mean: 1.0 recall/positive_threshold_0.500000_mean: 0.222222 However, when 15 examples are then used during the prediction step, and where 50% should be classified with a label of 1, they are generally all returned as 0s. Occasionally, a single example will be correctly classified as a 1. For example: Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] My DNNClassifier is configured as follows: and the predict function is called as below: I have two questions. (1) Am I correct to assume that there should be a direct correlation between estimation and prediction accuracy, and therefore that an 85% estimation accuracy should result in a similar prediction accuracy? (2) Do the other results provided when training completes (i.e. ""labels/prediction_mean: 0.283492"") provide information that is relevant to the low level of prediction accuracy being achieved? I have not been able to find a definition of these result labels in the TensorFlow documentation, and my apologies if I have missed something. Many thanks for any thoughts on this.",https://stackoverflow.com/questions/41515716,7382474,Documentation Completeness
41651034,input pipeline in tensorflow,"<p>While practicing on the official tensorflow mnist dataset tutorial for beginners, I'm trying to change the mnist data to my own images collected from search engines. </p>

<pre><code>strFilePaths,iLabels ,strSubFolderNames,iNumTotalDatasets = ScanForImage('Datasets')

tsFileNameQueue = tf.train.string_input_producer(strFilePaths)
tsReader = tf.WholeFileReader()
_,tsImage = tsReader.read(tsFileNameQueue)

tsImage = tf.image.decode_jpeg(tsImage, channels=3)
tsImage = tf.cast(tsImage,tf.float32)
tsLabels = tf.convert_to_tensor(iLabels, dtype=tf.float32)
tsImage = tf.reshape(tsImage, shape=[1,168*300*3])

matWeights = tf.Variable(tf.random_normal([168*300*3, 2]))
vBiases = tf.Variable(tf.zeros([2]))
vPredictions = tf.nn.softmax(tf.matmul(tsImage, matWeights) + vBiases)
fCrossEntropy = tf.reduce_mean(-tf.reduce_sum(tsLabels * tf.log(vPredictions), reduction_indices=[1]))
train_step = tf.train.GradientDescentOptimizer(0.1).minimize(fCrossEntropy)
init = tf.global_variables_initializer()

with tf.Session() as sess : 
    sess.run(init)
    for i in range (1000) : 
    tsTrainingSets = tf.train.batch([tsImage,tsLabels], batch_size=100)
    sess.run(train_step)
        if i % 20 == 0 : 
            correct_prediction = tf.equal(tf.argmax(vPredictions,1),tf.argmax(tsTrainingSets[1],1))
            accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
            print(sess.run(accuracy))
</code></pre>

<p>That strFilePaths is a standard python list containing all my image paths , iLabels is a list of lists representing labels. And I have only 2 classes in this case.</p>

<p>This program runs without error output but tensorflow just keeps on running and not giving me any output. I've read the ""reading files"" session in tensorflow website for like a thousand times but I still don't have a clue on whether I did things right or not.</p>

<p>Q1: What's wrong with this code?
Q2: Is there any complete example on how to read jpeg files into tensorflow and perform some training tasks on them?</p>
","While practicing on the official tensorflow mnist dataset tutorial for beginners, I'm trying to change the mnist data to my own images collected from search engines. That strFilePaths is a standard python list containing all my image paths , iLabels is a list of lists representing labels. And I have only 2 classes in this case. This program runs without error output but tensorflow just keeps on running and not giving me any output. I've read the ""reading files"" session in tensorflow website for like a thousand times but I still don't have a clue on whether I did things right or not. Q1: What's wrong with this code? Q2: Is there any complete example on how to read jpeg files into tensorflow and perform some training tasks on them?",https://stackoverflow.com/questions/41651034,3205742,Requesting (Additional) Resources
41796965,Tensorflow: How to use a trained model in a application?,"<p>I have trained a Tensorflow Model, and now I want to export the ""function"" to use it in my python program. Is that possible, and if yes, how? Any help would be nice, could not find much in the documentation. (I dont want to save a session!)</p>

<p>I have now stored the session as you suggested. I am loading it now like this:</p>

<pre><code>f = open('batches/batch_9.pkl', 'rb')
input = pickle.load(f)
f.close()
sess = tf.Session()

saver = tf.train.Saver()
saver.restore(sess, 'trained_network.ckpt')
y_pred = []

sess.run(y_pred, feed_dict={x: input})

print(y_pred)
</code></pre>

<p>However, I get the error ""no Variables to save"" when I try to initialize the saver. </p>

<p>What I want to do is this: I am writing a bot for a board game, and the input is the situation on the board formatted into a tensor. Now I want to return a tensor which gives me the best position to play next, i.e. a tensor with 0 everywhere and a 1 at one position.</p>
","I have trained a Tensorflow Model, and now I want to export the ""function"" to use it in my python program. Is that possible, and if yes, how? Any help would be nice, could not find much in the documentation. (I dont want to save a session!) I have now stored the session as you suggested. I am loading it now like this: However, I get the error ""no Variables to save"" when I try to initialize the saver. What I want to do is this: I am writing a bot for a board game, and the input is the situation on the board formatted into a tensor. Now I want to return a tensor which gives me the best position to play next, i.e. a tensor with 0 everywhere and a 1 at one position.",https://stackoverflow.com/questions/41796965,6480160,Lack of Alternative Solutions/Documentation
42164772,Tensorflow Estimator API: Summaries,"<p>I can't achieve to make summaries work with the Estimator API of Tensorflow.</p>

<p>The Estimator class is very useful for many reasons: I have already implemented my own classes which are really similar but I am trying to switch to this one.</p>

<p>Here is the code sample:</p>

<pre><code>import tensorflow as tf
import tensorflow.contrib.layers as layers
import tensorflow.contrib.learn as learn
import numpy as np

 # To reproduce the error: docker run --rm -w /algo -v $(pwd):/algo tensorflow/tensorflow bash -c ""python sample.py""

def model_fn(x, y, mode):
    logits = layers.fully_connected(x, 12, scope=""dense-1"")
    logits = layers.fully_connected(logits, 56, scope=""dense-2"")
    logits = layers.fully_connected(logits, 4, scope=""dense-3"")

    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y), name=""xentropy"")

    return {""predictions"":logits}, loss, tf.train.AdamOptimizer(0.001).minimize(loss)


def input_fun():
    """""" To be completed for a 4 classes classification problem """"""

    feature = tf.constant(np.random.rand(100,10))
    labels = tf.constant(np.random.random_integers(0,3, size=(100,)))

    return feature, labels

estimator = learn.Estimator(model_fn=model_fn, )

trainingConfig = tf.contrib.learn.RunConfig(save_checkpoints_secs=60)

estimator = learn.Estimator(model_fn=model_fn, model_dir=""./tmp"", config=trainingConfig)

# Works
estimator.fit(input_fn=input_fun, steps=2)

# The following code does not work

# Can't initialize saver

# saver = tf.train.Saver(max_to_keep=10) # Error: No variables to save

# The following fails because I am missing a saver... :(

hooks=[
        tf.train.LoggingTensorHook([""xentropy""], every_n_iter=100),
        tf.train.CheckpointSaverHook(""./tmp"", save_steps=1000, checkpoint_basename='model.ckpt'),
        tf.train.StepCounterHook(every_n_steps=100, output_dir=""./tmp""),
        tf.train.SummarySaverHook(save_steps=100, output_dir=""./tmp""),
]

estimator.fit(input_fn=input_fun, steps=2, monitors=hooks)
</code></pre>

<p>As you can see, I can create an Estimator and use it but I can achieve to add hooks to the fitting process.</p>

<p>The logging hooks works just fine but the others require both <strong>tensors</strong> and a <strong>saver</strong> which I can't provide.</p>

<p>The tensors are defined in the model function, thus I can't pass them to the <strong>SummaryHook</strong> and the <strong>Saver</strong> can't be initialized because there is no tensor to save...</p>

<p>Is there a solution to my problem? (I am guessing yes but there is a lack of documentation of this part in the tensorflow documentation)</p>

<ul>
<li>How can I initialized my <strong>saver</strong>? Or should I use other objects such as <em>Scaffold</em>?</li>
<li>How can I pass <strong>summaries</strong> to the <strong>SummaryHook</strong> since they are defined in my model function?</li>
</ul>

<p>Thanks in advance.</p>

<p><em>PS: I have seen the DNNClassifier API but I want to use the estimator API for Convolutional Nets and others. I need to create summaries for any estimator.</em></p>
","I can't achieve to make summaries work with the Estimator API of Tensorflow. The Estimator class is very useful for many reasons: I have already implemented my own classes which are really similar but I am trying to switch to this one. Here is the code sample: As you can see, I can create an Estimator and use it but I can achieve to add hooks to the fitting process. The logging hooks works just fine but the others require both tensors and a saver which I can't provide. The tensors are defined in the model function, thus I can't pass them to the SummaryHook and the Saver can't be initialized because there is no tensor to save... Is there a solution to my problem? (I am guessing yes but there is a lack of documentation of this part in the tensorflow documentation) Thanks in advance. PS: I have seen the DNNClassifier API but I want to use the estimator API for Convolutional Nets and others. I need to create summaries for any estimator.",https://stackoverflow.com/questions/42164772,5184894,Lack of Alternative Solutions/Documentation
42209854,The node 'Merge/MergeSummary' has inputs from different frames: what does it mean?,"<p>trying to merge all my summaries, I have an error saying that the inputs of <code>Merge/MergeSummary</code> comes from different frames. So, first of all: what is a <em>frame</em>? Could you please point me somewhere in the TF documentation about such stuff? -- of course, I googled a bit but could find almost nothing. How can I fix this issue? Below the code to reproduce the error. Thanks in advance.</p>

<pre><code>import numpy as np
import tensorflow as tf

tf.reset_default_graph()
tf.set_random_seed(23)

BATCH = 2
LENGTH = 4
SIZE = 5
ATT_SIZE = 3
NUM_QUERIES = 2

def linear(inputs, output_size, use_bias=True, activation_fn=None):
    """"""Linear projection.""""""

    input_shape = inputs.get_shape().as_list()
    input_size = input_shape[-1]
    output_shape = input_shape[:-1] + [output_size]
    if len(output_shape) &gt; 2:
        output_shape_tensor = tf.unstack(tf.shape(inputs))
        output_shape_tensor[-1] = output_size
        output_shape_tensor = tf.stack(output_shape_tensor)
        inputs = tf.reshape(inputs, [-1, input_size])

    kernel = tf.get_variable(""kernel"", [input_size, output_size])
    output = tf.matmul(inputs, kernel)
    if use_bias:
        output = output + tf.get_variable('bias', [output_size])

    if len(output_shape) &gt; 2:
        output = tf.reshape(output, output_shape_tensor)
        output.set_shape(output_shape)  # pylint: disable=I0011,E1101

    if activation_fn is not None:
        return activation_fn(output)
    return output


class Attention(object):
    """"""Attention mechanism implementation.""""""

    def __init__(self, attention_states, attention_size):
        """"""Initializes a new instance of the Attention class.""""""
        self._states = attention_states
        self._attention_size = attention_size
        self._batch = tf.shape(self._states)[0]
        self._length = tf.shape(self._states)[1]
        self._size = self._states.get_shape()[2].value
        self._features = None

    def _init_features(self):
        states = tf.reshape(
            self._states, [self._batch, self._length, 1, self._size])
        weights = tf.get_variable(
            ""kernel"", [1, 1, self._size, self._attention_size])
        self._features = tf.nn.conv2d(states, weights, [1, 1, 1, 1], ""SAME"")

    def get_weights(self, query, scope=None):
        """"""Reurns the attention weights for the given query.""""""
        with tf.variable_scope(scope or ""Attention""):
            if self._features is None:
                self._init_features()
            else:
                tf.get_variable_scope().reuse_variables()
            vect = tf.get_variable(""Vector"", [self._attention_size])
            with tf.variable_scope(""Query""):
                query_features = linear(query, self._attention_size, False)
                query_features = tf.reshape(
                    query_features, [-1, 1, 1, self._attention_size])

        activations = vect * tf.tanh(self._features + query_features)
        activations = tf.reduce_sum(activations, [2, 3])
        with tf.name_scope('summaries'):
            tf.summary.histogram('histogram', activations)
        return tf.nn.softmax(activations)

states = tf.placeholder(tf.float32, shape=[BATCH, None, SIZE])  # unknown length
queries = tf.placeholder(tf.float32, shape=[NUM_QUERIES, BATCH, ATT_SIZE])
attention = Attention(states, ATT_SIZE)
func = lambda x: attention.get_weights(x, ""Softmax"")
weights = tf.map_fn(func, queries)
for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):
    name = var.name.replace(':', '_')
    tf.summary.histogram(name, var)
summary_op = tf.summary.merge_all()

states_np = np.random.rand(BATCH, LENGTH, SIZE)
queries_np = np.random.rand(NUM_QUERIES, BATCH, ATT_SIZE)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    weights_np, summary_str = sess.run([weights, summary_op], {states: states_np, queries: queries_np})
    print weights_np
</code></pre>
","trying to merge all my summaries, I have an error saying that the inputs of Merge/MergeSummary comes from different frames. So, first of all: what is a frame? Could you please point me somewhere in the TF documentation about such stuff? -- of course, I googled a bit but could find almost nothing. How can I fix this issue? Below the code to reproduce the error. Thanks in advance.",https://stackoverflow.com/questions/42209854,1861627,Requesting (Additional) Resources
42535815,How to add more layers to Convolutional Neural Network text classification TensorFlow example?,"<p>According to the documentation, the model presented in <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification_character_cnn.py"" rel=""nofollow noreferrer"">this example</a> is similar to the following paper:
""<a href=""https://arxiv.org/abs/1509.01626"" rel=""nofollow noreferrer"">Character-level Convolutional Networks for Text Classification</a>""</p>

<p>I found that the original model (presented in the paper) contains 9 layers deep with 6 convolutional layers and 3 fully-connected layers, but the implemented example contains only two convolutional layers:</p>

<pre><code>with tf.variable_scope('CNN_Layer1'):
    # Apply Convolution filtering on input sequence.
    conv1 = tf.contrib.layers.convolution2d(
                 byte_list, N_FILTERS, FILTER_SHAPE1, padding='VALID')
    # Add a RELU for non linearity.
    conv1 = tf.nn.relu(conv1)
    # Max pooling across output of Convolution+Relu.
    pool1 = tf.nn.max_pool(
            conv1,
            ksize=[1, POOLING_WINDOW, 1, 1],
            strides=[1, POOLING_STRIDE, 1, 1],
            padding='SAME')
    # Transpose matrix so that n_filters from convolution becomes width.
    pool1 = tf.transpose(pool1, [0, 1, 3, 2])
with tf.variable_scope('CNN_Layer2'):
    # Second level of convolution filtering.
    conv2 = tf.contrib.layers.convolution2d(
                 pool1, N_FILTERS, FILTER_SHAPE2, padding='VALID')
    # Max across each filter to get useful features for classification.
    pool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1])
</code></pre>

<p>If anybody can help me to extend this model for more layers?</p>
","According to the documentation, the model presented in this example is similar to the following paper: ""Character-level Convolutional Networks for Text Classification"" I found that the original model (presented in the paper) contains 9 layers deep with 6 convolutional layers and 3 fully-connected layers, but the implemented example contains only two convolutional layers: If anybody can help me to extend this model for more layers?",https://stackoverflow.com/questions/42535815,3052875,Documentation Replication on Other Examples
42560998,Tensorflow classification labels datatype,"<p>I am using Tensorflow DNN model to do some <strong>classification</strong>.</p>

<p>I have a numerical (float32) data input but <strong>string type output</strong>.</p>

<pre><code>x = tf.placeholder(""float"", [None, n_input])
y = tf.placeholder(tf.string, [None, n_classes])
</code></pre>

<p>When I try to define the loss and optimizer as below:</p>

<pre><code>cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))
</code></pre>

<p>I encounter an error that </p>

<blockquote>
  <p>TypeError: sigmoid_cross_entropy_with_logits() got an unexpected
  keyword argument 'labels'</p>
</blockquote>

<p>I looked up the document from <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/classification"" rel=""nofollow noreferrer"">here</a>, it said that</p>

<blockquote>
  <p>logits and targets must have the same type and shape.</p>
</blockquote>

<p>Do I need to convert the class into a floating number(hashing string to number)?</p>

<pre><code>output_y = [[""apple"", ""apple"", ""orange"", ""banana""]]
encoded_y = [[1], [1], [2], [3]]
</code></pre>
","I am using Tensorflow DNN model to do some classification. I have a numerical (float32) data input but string type output. When I try to define the loss and optimizer as below: I encounter an error that I looked up the document from here, it said that Do I need to convert the class into a floating number(hashing string to number)?",https://stackoverflow.com/questions/42560998,6343552,Requesting (Additional) Resources
42598841,using validation monitor in tflearn.regression to create confusion matrix,"<p>So I have been trying to create a confusion metrics in my autoencoder</p>

<pre><code>from __future__ import division, print_function, absolute_import

import numpy as np
#import matplotlib.pyplot as plt
import tflearn
import tensorflow as tf
from random import randint
from tensorflow.contrib import metrics as ms 
# Data loading and preprocessing
import tflearn.datasets.mnist as mnist
Images, Lables, testImages, testLables = mnist.load_data(one_hot=True)


f = randint(0,20)

x = tf.placeholder(""float"",[None, 784])
y = tf.placeholder(""float"",[None, 10])
# Building the encoder
encoder = tflearn.input_data(shape=[None, 784])
encoder = tflearn.fully_connected(encoder, 256)
encoder = tflearn.fully_connected(encoder, 64)
encoder = tflearn.fully_connected(encoder, 10)

acc= tflearn.metrics.Accuracy()

# Regression, with mean square error
net = tflearn.regression(encoder, optimizer='adam', learning_rate=0.001,
                         loss='mean_square', metric=acc, shuffle_batches=True, validation_monitors = ?)


model = tflearn.DNN(net, tensorboard_verbose=0)

model.fit(Images, Lables, n_epoch=20, validation_set=(testImages, testLables),
          run_id=""auto_encoder"", batch_size=256,show_metric=True)

#Applying the above model on test Images and evaluating as well as prediction of the labels

evali= model.evaluate(testImages,testLables)
print(""Accuracy of the model is :"", evali)
lables = model.predict_label(testImages)
print(""The predicted labels are :"",lables[f])
prediction = model.predict(testImages)
print(""The predicted probabilities are :"", prediction[f])
</code></pre>

<p>I have gone through the documantation but they were not very useful to me.</p>

<p>How would I configure to get the confusion matrix?</p>

<pre><code>validation_monitors ={?}
</code></pre>
",So I have been trying to create a confusion metrics in my autoencoder I have gone through the documantation but they were not very useful to me. How would I configure to get the confusion matrix?,https://stackoverflow.com/questions/42598841,5540592,Inadequate Examples
42818842,Transitioning to Tensorflow 1.0.0,"<p>I am trying to translate <a href=""https://github.com/yenchenlin/pix2pix-tensorflow/blob/e0ef0323c5ccbd066fea117a012f5d7e748ee6ce/model.py"" rel=""nofollow noreferrer"">this</a> pix2pix GAN code to Tensorflow 1.0.0 using the following <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/tf_upgrade.py"" rel=""nofollow noreferrer"">script</a> as described in Tensorflow Documentation, but I keep getting the following error: </p>

<blockquote>
  <p>ValueError: Variable d_h0_conv/w/Adam/ does not exist, or was not
  created with tf.get_variable(). Did you mean to set reuse=None in
  VarScope?</p>
</blockquote>

<p>This is the Adam optimizer part: </p>

<pre><code> d_optim = tf.train.AdamOptimizer(args.lr, beta1=args.beta1) \
.minimize(self.d_loss, var_list=self.d_vars)
</code></pre>

<p>Where d_vars is:</p>

<pre><code>t_vars = tf.trainable_variables()

self.d_vars = [var for var in t_vars if 'd_' in var.name]
</code></pre>

<p>And the discriminator code: </p>

<pre><code>    def discriminator(self, image, y=None, reuse=False):
        # image is 256 x 256 x (input_c_dim + output_c_dim)
        if reuse:
            tf.get_variable_scope().reuse_variables()
        else:
            assert tf.get_variable_scope().reuse == False

        h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))
        # h0 is (128 x 128 x self.df_dim)
        h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))
        # h1 is (64 x 64 x self.df_dim*2)
        h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv')))
        # h2 is (32x 32 x self.df_dim*4)
        h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, d_h=1, d_w=1, name='d_h3_conv')))
        # h3 is (16 x 16 x self.df_dim*8)
        h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h3_lin')
  return tf.nn.sigmoid(h4), h4
</code></pre>
","I am trying to translate this pix2pix GAN code to Tensorflow 1.0.0 using the following script as described in Tensorflow Documentation, but I keep getting the following error: This is the Adam optimizer part: Where d_vars is: And the discriminator code:",https://stackoverflow.com/questions/42818842,7671750,Documentation Replicability
42838796,"Value error: Cannot feed value of shape (5, 15) for Tensor 'one_hot:0', which has shape '(5, 15, 2)'","<p>this is the code </p>

<pre><code>num_epochs = 100
total_series_length = 50000
truncated_backprop_length = 15
state_size = 4
num_classes = 2
echo_step = 3
batch_size = 5
num_batches = total_series_length//batch_size//truncated_backprop_length
</code></pre>

<p>genreating data{...}</p>

<pre><code>batchX_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])
batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])

#and one for the RNN state, 5,4 
init_state = tf.placeholder(tf.float32, [batch_size, state_size])

batchX_placeholder = tf.one_hot(batchX_placeholder, num_classes)
inputs_series = tf.unstack(batchX_placeholder, axis=1)

cell = tf.contrib.rnn.BasicRNNCell(state_size)
rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, inputs_series, initial_state=init_state)
</code></pre>

<p>some optimization code{....}
and then creating the graph</p>

<pre><code>#Step 3 Training the network
with tf.Session() as sess:
    #we stupidly have to do this everytime, it should just know
    #that we initialized these vars. v2 guys, v2..
    sess.run(tf.initialize_all_variables())
    #interactive mode
    plt.ion()
    #initialize the figure
    plt.figure()
    #show the graph
    plt.show()
    #to show the loss decrease
    loss_list = []

    for epoch_idx in range(num_epochs):
        #generate data at eveery epoch, batches run in epochs
        x,y = generateData()
        #initialize an empty hidden state
        _current_state = np.zeros((batch_size, state_size))

        print(""New data, epoch"", epoch_idx)
        #each batch
        for batch_idx in range(num_batches):
            #starting and ending point per batch
            #since weights reoccuer at every layer through time
            #These layers will not be unrolled to the beginning of time, 
            #that would be too computationally expensive, and are therefore truncated 
            #at a limited number of time-steps
            start_idx = batch_idx * truncated_backprop_length
            end_idx = start_idx + truncated_backprop_length

            batchX = x[:,start_idx:end_idx]
            batchY = y[:,start_idx:end_idx]

            #run the computation graph, give it the values
            #we calculated earlier
            _total_loss, _train_step, _final_state, _predictions_series = sess.run(
                [total_loss, train_step, final_state, predictions],
                feed_dict={
                    batchX_placeholder:batchX,
                    batchY_placeholder:batchY,
                    init_state:_current_state
                })

            loss_list.append(_total_loss)

            if batch_idx%100 == 0:
                print(""Step"",batch_idx, ""Loss"", _total_loss)
                plot(loss_list, _predictions_series, batchX, batchY)

plt.ioff()
plt.show()
</code></pre>

<p>and this the error:</p>

<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-7c3d1289d16b&gt; in &lt;module&gt;()
     40                     batchX_placeholder:batchX,
     41                     batchY_placeholder:batchY,
---&gt; 42                     init_state:_current_state
     43                 })
     44 

/home/pranshu_44/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    765     try:
    766       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 767                          run_metadata_ptr)
    768       if run_metadata:
    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/home/pranshu_44/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    942                 'Cannot feed value of shape %r for Tensor %r, '
    943                 'which has shape %r'
--&gt; 944                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
    945           if not self.graph.is_feedable(subfeed_t):
    946             raise ValueError('Tensor %s may not be fed.' % subfeed_t)

ValueError: Cannot feed value of shape (5, 15) for Tensor 'one_hot:0', which has shape '(5, 15, 2)'
</code></pre>

<p>i looked the docs but that does not helpful at all
if there is any other easy way that also will be helpful</p>
",this is the code genreating data{...} some optimization code{....} and then creating the graph and this the error: i looked the docs but that does not helpful at all if there is any other easy way that also will be helpful,https://stackoverflow.com/questions/42838796,7018732,Inadequate Examples
42939426,Tensorflow placeholder error,"<p>I have been playing around with tensorflow, I have managed to train the mode and serve it but when i try run the client to send data for classification i get this error</p>

<blockquote>
  <p>grpc.framework.interfaces.face.face.AbortionError:
  AbortionError(code=StatusCode.INVALID_ARGUMENT, details=""You must feed
  a value for placeholder tensor 'Placeholder_1' with dtype float<br>
  [[Node: Placeholder_1 = Placeholder_output_shapes=[[]],
  dtype=DT_FLOAT, shape=[],
  _device=""/job:localhost/replica:0/task:0/cpu:0""]]"")</p>
</blockquote>

<p>I do not quite understand this error, here are my placeholders</p>

<pre><code>X = tf.placeholder(tf.float32,[None,n_dim])
y = tf.placeholder(tf.float32,[None,n_classes])
</code></pre>

<p>And i used the builder as in the documentation, writing the prediction_signature as well as classification signatures.</p>

<p>If any may know why this is happening i would be extremely grateful </p>
","I have been playing around with tensorflow, I have managed to train the mode and serve it but when i try run the client to send data for classification i get this error I do not quite understand this error, here are my placeholders And i used the builder as in the documentation, writing the prediction_signature as well as classification signatures. If any may know why this is happening i would be extremely grateful",https://stackoverflow.com/questions/42939426,6664161,Documentation Replication on Other Examples
42953781,How to restore variables in Tensorflow r1.0,"<p>After upgrading Tensorflow to r1.0, the restore command does not seem to work.
For example, can anyone tell me what is wrong with the following?</p>

<pre><code>def foo():
    v1 = tf.Variable(1., name=""v1"")
    v2 = tf.Variable(2., name=""v2"")
    v3 = v1 + v2

    saver = tf.train.Saver()

    with tf.Session() as sess:
        tf.global_variables_initializer().run()

        saver.save(sess, ""temp"")

        # do something

        saver.restore(sess, ""temp"") 
</code></pre>

<p>From the last line, I got an error:</p>

<pre><code>tensorflow.python.framework.errors_impl.NotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for temp
     [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
</code></pre>

<p>Tensorflow documentation still holds the explanation of old versions for this matter.</p>
","After upgrading Tensorflow to r1.0, the restore command does not seem to work. For example, can anyone tell me what is wrong with the following? From the last line, I got an error: Tensorflow documentation still holds the explanation of old versions for this matter.",https://stackoverflow.com/questions/42953781,3755060,Lack of Alternative Solutions/Documentation
43315068,Tensorflow: has no attribute 'numpy_input_fn',"<p>I am using Eclipse's PyDev for tensorflow version:0.12.1<br>
I directly copy the sample code from tensorflow documentation,<br>
but a attribute is not found and it returned  </p>

<pre><code>input_fn = tf.contrib.learn.io.numpy_input_fn({""x"":x}, y, batch_size=4,
AttributeError: module 'tensorflow.contrib.learn.python.learn.learn_io' has no attribute 'numpy_input_fn'  
</code></pre>

<p>Tried to re-download pydev and tensorflow but none of them work  </p>

<p>The source code:  </p>

<pre><code>import tensorflow as tf
import numpy as np

features = [tf.contrib.layers.real_valued_column(""x"", dimension=1)]

estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)

x = np.array([1., 2., 3., 4.])
y = np.array([0., -1., -2., -3.])
input_fn = tf.contrib.learn.io.numpy_input_fn({""x"":x}, y, batch_size=4,num_epochs=1000)

estimator.fit(input_fn=input_fn, steps=1000)

estimator.evaluate(input_fn=input_fn)
</code></pre>
","I am using Eclipse's PyDev for tensorflow version:0.12.1 I directly copy the sample code from tensorflow documentation, but a attribute is not found and it returned Tried to re-download pydev and tensorflow but none of them work The source code:",https://stackoverflow.com/questions/43315068,3026820,Documentation Replicability
43634968,Parallelizing a tensorflow operation across multiple GPU's,"<p>In below code of a single hidden layer neural network I'm attempting to parallelize the gradient descent operation across two GPU's. I'm just attempting to thinking about this conceptually at the moment. There does not appear to be very much literature on how to perform this. Reading <a href=""https://stackoverflow.com/questions/41029037/training-multi-gpu-on-tensorflow-a-simpler-way"">Training Multi-GPU on Tensorflow: a simpler way?</a> does not provide a concrete answer. In below code I've added two functions <code>runOnGPU1()</code> &amp; <code>runOnGPU1()</code> which is a conceptual idea of how to split the training of the network across two GPU's. Can these two loops be split in order to share the computation across multiple GPU's ?</p>

<pre><code>import numpy as np
import tensorflow as tf

sess = tf.InteractiveSession()

# a batch of inputs of 2 value each
inputs = tf.placeholder(tf.float32, shape=[None, 2])

# a batch of output of 1 value each
desired_outputs = tf.placeholder(tf.float32, shape=[None, 1])

# [!] define the number of hidden units in the first layer
HIDDEN_UNITS = 4 

# connect 2 inputs to 3 hidden units
# [!] Initialize weights with random numbers, to make the network learn
weights_1 = tf.Variable(tf.truncated_normal([2, HIDDEN_UNITS]))

# [!] The biases are single values per hidden unit
biases_1 = tf.Variable(tf.zeros([HIDDEN_UNITS]))

# connect 2 inputs to every hidden unit. Add bias
layer_1_outputs = tf.nn.sigmoid(tf.matmul(inputs, weights_1) + biases_1)

# [!] The XOR problem is that the function is not linearly separable
# [!] A MLP (Multi layer perceptron) can learn to separe non linearly separable points ( you can
# think that it will learn hypercurves, not only hyperplanes)
# [!] Lets' add a new layer and change the layer 2 to output more than 1 value

# connect first hidden units to 2 hidden units in the second hidden layer
weights_2 = tf.Variable(tf.truncated_normal([HIDDEN_UNITS, 2]))
# [!] The same of above
biases_2 = tf.Variable(tf.zeros([2]))

# connect the hidden units to the second hidden layer
layer_2_outputs = tf.nn.sigmoid(
    tf.matmul(layer_1_outputs, weights_2) + biases_2)

# [!] create the new layer
weights_3 = tf.Variable(tf.truncated_normal([2, 1]))
biases_3 = tf.Variable(tf.zeros([1]))

logits = tf.nn.sigmoid(tf.matmul(layer_2_outputs, weights_3) + biases_3)

# [!] The error function chosen is good for a multiclass classification taks, not for a XOR.
error_function = 0.5 * tf.reduce_sum(tf.subtract(logits, desired_outputs) * tf.subtract(logits, desired_outputs))

train_step = tf.train.GradientDescentOptimizer(0.05).minimize(error_function)

sess.run(tf.global_variables_initializer())

training_inputs = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]

training_outputs = [[0.0], [1.0], [1.0], [0.0]]

def runOnGPU1() : 
    for i in range(5):
        _, loss = sess.run([train_step, error_function],
                           feed_dict={inputs: np.array(training_inputs),
                                      desired_outputs: np.array(training_outputs)})
        print(loss)

def runOnGPU2() : 
    for i in range(5):
        _, loss = sess.run([train_step, error_function],
                           feed_dict={inputs: np.array(training_inputs),
                                      desired_outputs: np.array(training_outputs)})
        print(loss)

runOnGPU1()
runOnGPU2()
</code></pre>
",In below code of a single hidden layer neural network I'm attempting to parallelize the gradient descent operation across two GPU's. I'm just attempting to thinking about this conceptually at the moment. There does not appear to be very much literature on how to perform this. Reading Training Multi-GPU on Tensorflow: a simpler way? does not provide a concrete answer. In below code I've added two functions runOnGPU1() &amp; runOnGPU1() which is a conceptual idea of how to split the training of the network across two GPU's. Can these two loops be split in order to share the computation across multiple GPU's ?,https://stackoverflow.com/questions/43634968,470184,Documentation Completeness
43649591,Serving Keras Models With Tensorflow Serving,"<p>Right now we are successfully able to serve models using Tensorflow Serving. We have used following method to export the model and host it with Tensorflow Serving. </p>

<pre><code>     ------------
      For exporting 
     ------------------
     from tensorflow.contrib.session_bundle import exporter

     K.set_learning_phase(0)
     export_path = ... # where to save the exported graph
     export_version = ... # version number (integer)

     saver = tf.train.Saver(sharded=True)
     model_exporter = exporter.Exporter(saver)
     signature = exporter.classification_signature(input_tensor=model.input,
                                          scores_tensor=model.output)
     model_exporter.init(sess.graph.as_graph_def(),
                default_graph_signature=signature)
     model_exporter.export(export_path, tf.constant(export_version), sess)

      --------------------------------------

      For hosting
      -----------------------------------------------

      bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=default --model_base_path=/serving/models
</code></pre>

<p>However our issue is - we want keras to be integrated with Tensorflow serving. We would like to serve the model through Tensorflow serving using Keras.
The reason we would like to have that is because - in our architecture we follow couple of different ways to train our model like deeplearning4j + Keras , 
Tensorflow + Keras, but for serving we would like to use only one servable engine that's Tensorflow Serving. We don't see any straight forward way to achieve that. Any comments ?</p>

<p>Thank you. </p>
","Right now we are successfully able to serve models using Tensorflow Serving. We have used following method to export the model and host it with Tensorflow Serving. However our issue is - we want keras to be integrated with Tensorflow serving. We would like to serve the model through Tensorflow serving using Keras. The reason we would like to have that is because - in our architecture we follow couple of different ways to train our model like deeplearning4j + Keras , Tensorflow + Keras, but for serving we would like to use only one servable engine that's Tensorflow Serving. We don't see any straight forward way to achieve that. Any comments ? Thank you.",https://stackoverflow.com/questions/43649591,7780215,Requesting (Additional) Resources
43662094,im2txt: Load input images from memory (instead of read from disk),"<p>I'm interested in modifying <a href=""https://github.com/tensorflow/models/tree/master/im2txt"" rel=""nofollow noreferrer"">the tensorflow implementation of Show and Tell</a>, in particular <a href=""https://github.com/tensorflow/models/tree/f653bd2340b15ce2a22669ba136b77b2751e462e/im2txt"" rel=""nofollow noreferrer"">this v0.12 snapshot</a>, in order to accept an image in numpy form instead of read it from disk.</p>

<p>Loading a filename using the upstream code results in a python string after </p>

<pre><code>with tf.gfile.GFile(filename, ""r"") as f:
    image = f.read()
</code></pre>

<p>in <code>run_inference.py</code> which is then turned into an ndarray of no shape. However, I can't replicate that.</p>

<p>I've tried the following:</p>

<h1>Loading the numpy array directly</h1>

<p>I wrote this function to load a pillow image from a filename, convert the image to a numpy array and feed it to the <code>beam_search</code> function in <code>run_inference.py</code></p>

<pre><code>def load_image(filename):
    from keras.preprocessing.image import img_to_array
    arr = img_to_array(PILImage.open(filename))
    return arr
...
captions = generator.beam_search(sess, image)
</code></pre>

<p>In this case, there is a size mismatch later, resulting in the following stack trace:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/pmelissi/repos/tensorflow-models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py"", line 107, in &lt;module&gt;
    tf.app.run()
  File ""/home/pmelissi/miniconda2/envs/im2txt/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""/home/pmelissi/repos/tensorflow-models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py"", line 97, in main
    captions = generator.beam_search(sess, image)
  File ""/home/pmelissi/repos/tensorflow-models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_utils/caption_generator.py"", line 142, in beam_search
    initial_state = self.model.feed_image(sess, encoded_image)
  File ""/home/pmelissi/repos/tensorflow-models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_wrapper.py"", line 41, in feed_image
    feed_dict={""image_feed:0"": encoded_image})
  File ""/home/pmelissi/miniconda2/envs/im2txt/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/home/pmelissi/miniconda2/envs/im2txt/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 943, in _run
    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (960, 640, 3) for Tensor u'image_feed:0', which has shape '()'

Process finished with exit code 1
</code></pre>

<p>Can I somehow trick numpy into thinking the array has no shape?</p>

<h1>Converting to tf.string</h1>

<p>Here I used the following function</p>

<pre><code>def encode_image(filename):
    g2 = tf.Graph()
    from keras.preprocessing.image import img_to_array
    with g2.as_default() as g:
        with g.name_scope(""g2"") as g2_scope:
            arr = img_to_array(PILImage.open(filename))
            image = tf.image.encode_jpeg(arr)
            return image
...
captions = generator.beam_search(sess, image)
</code></pre>

<p>This didn't work either:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/pmelissi/repos/tensorflow-models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py"", line 107, in &lt;module&gt;
    tf.app.run()
  File ""/home/pmelissi/miniconda2/envs/im2txt/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""/home/pmelissi/repos/tensorflow-models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/run_inference.py"", line 97, in main
    captions = generator.beam_search(sess, image)
  File ""/home/pmelissi/repos/tensorflow-models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_utils/caption_generator.py"", line 142, in beam_search
    initial_state = self.model.feed_image(sess, encoded_image)
  File ""/home/pmelissi/repos/tensorflow-models/im2txt/bazel-bin/im2txt/run_inference.runfiles/im2txt/im2txt/inference_wrapper.py"", line 41, in feed_image
    feed_dict={""image_feed:0"": encoded_image})
  File ""/home/pmelissi/miniconda2/envs/im2txt/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/home/pmelissi/miniconda2/envs/im2txt/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 924, in _run
    raise TypeError('The value of a feed cannot be a tf.Tensor object. '
TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays.
</code></pre>

<p>The last line in this stacktrace seems helpful, however there is no documentation as to what kind of structure is expected</p>

<pre><code>TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, or numpy ndarrays.
</code></pre>

<p>So, what should a valid input look like? The internals of preprocessing are not particularly clear to me.</p>

<p>Thanks for your time!</p>

<p>EDIT: <a href=""https://gist.github.com/PavlosMelissinos/9daa295d11af87848c3ea0778696eddd"" rel=""nofollow noreferrer"">Attached gist of the modified inference script for the big picture</a></p>

<p>EDIT 2: 
The path to sess.run goes like this:</p>

<p>1: run_inference.py</p>

<pre><code>captions = generator.beam_search(sess, image)
</code></pre>

<p>2: caption_generator.py</p>

<pre><code>def beam_search(self, sess, encoded_image):
    initial_state = self.model.feed_image(sess, encoded_image)
</code></pre>

<p>3: inference_wrapper.py</p>

<pre><code>def feed_image(self, sess, encoded_image):
    initial_state = sess.run(fetches=""lstm/initial_state:0"",
                         feed_dict={""image_feed:0"": encoded_image})
    return initial_state
</code></pre>

<p>EDIT 3: I forgot to mention that I'm restricted to TensorFlow v0.12 and therefore I'm using <a href=""https://github.com/tensorflow/models/tree/f653bd2340b15ce2a22669ba136b77b2751e462e/im2txt"" rel=""nofollow noreferrer"">this snapshot of the im2txt repo</a>.</p>
","I'm interested in modifying the tensorflow implementation of Show and Tell, in particular this v0.12 snapshot, in order to accept an image in numpy form instead of read it from disk. Loading a filename using the upstream code results in a python string after in run_inference.py which is then turned into an ndarray of no shape. However, I can't replicate that. I've tried the following: I wrote this function to load a pillow image from a filename, convert the image to a numpy array and feed it to the beam_search function in run_inference.py In this case, there is a size mismatch later, resulting in the following stack trace: Can I somehow trick numpy into thinking the array has no shape? Here I used the following function This didn't work either: The last line in this stacktrace seems helpful, however there is no documentation as to what kind of structure is expected So, what should a valid input look like? The internals of preprocessing are not particularly clear to me. Thanks for your time! EDIT: Attached gist of the modified inference script for the big picture EDIT 2: The path to sess.run goes like this: 1: run_inference.py 2: caption_generator.py 3: inference_wrapper.py EDIT 3: I forgot to mention that I'm restricted to TensorFlow v0.12 and therefore I'm using this snapshot of the im2txt repo.",https://stackoverflow.com/questions/43662094,7932035,Documentation Completeness
43834529,"Tensorflow - dimensions of data, placeholder","<p>I am a complete beginner in Tensorflow, and I apologize if my question is trivial, but I have looked into both the documentation and Google and I couldn't find the answer. (I also apologize for my english)</p>

<p>I'd like to do something like</p>

<pre><code>sess.run(train, {x:x_train, y:x_train}
</code></pre>

<p>where x_train is an array of size 3190 containing my input data (arrays of dimension 60*4)</p>

<p>My question is, should x be :</p>

<pre><code>x = tf.placeholder(tf.bool, [60,4])
</code></pre>

<p>or</p>

<pre><code>x = tf.placeholder(tf.bool, [None,60,4])
</code></pre>

<p>?</p>

<p>The first one gives the following error :</p>

<pre><code>ValueError: Cannot feed value of shape (3190, 60, 4) for Tensor u'Placeholder:0', which has shape '(60, 4)'
</code></pre>

<p>and if I use the second one, how can I reach x[i][j] with 0&lt;=i&lt;60 and 0&lt;=j&lt;4 if I want to compute for example</p>

<pre><code>tf.logical_and(x[i1][j1],x[i2][j2])
</code></pre>

<p>?</p>

<p>Thanking you in advance for your answer.</p>
","I am a complete beginner in Tensorflow, and I apologize if my question is trivial, but I have looked into both the documentation and Google and I couldn't find the answer. (I also apologize for my english) I'd like to do something like where x_train is an array of size 3190 containing my input data (arrays of dimension 60*4) My question is, should x be : or ? The first one gives the following error : and if I use the second one, how can I reach x[i][j] with 0&lt;=i&lt;60 and 0&lt;=j&lt;4 if I want to compute for example ? Thanking you in advance for your answer.",https://stackoverflow.com/questions/43834529,7976712,Inadequate Examples
43848414,How is memory managed during tensor transformations?,"<p>Even if the question involves TensorFlow, I will use normal math terminology to describe my question.</p>

<p>Let's say that</p>

<ul>
<li>I have a matrix <code>W</code> of dimension <code>n x k</code></li>
<li>An input vector <code>x</code> of size <code>1 x k</code></li>
<li>I need to compare <code>x</code> to each element in <code>W</code></li>
</ul>

<p>After reading some code examples, the way I do it now is as it follows (<code>n = self.nNodes</code>, <code>k=self.inputShape</code>):</p>

<pre><code>inputShape = (10,)

W = tf.Variable(tf.random_uniform( (nNodes, ) + inputShape, 0.0, 1.0, dtype=tf.float32), dtype=tf.float32, name='W' )

x = tf.placeholder(self.__datatype, inputShape, name='input')

x_M = tf.expand_dims(x, 0, name='x_m')

x_MM = tf.tile(x_M, (nNodes, 1), name='x_mm')

spatDiffs = tf.subtract(x_MM, self.W)
</code></pre>

<p>It seems to be - but I am not sure - that, after a while, TensorFlow has some difficulties in managing the memory (especially on the GPU) since the <code>expand_dims</code> and <code>tile</code> operations return <em>new</em> tensors.</p>

<p>Is there any way to allocate a tensor for <code>X_MM</code> (as I do for <code>W</code>) and <strong>copy</strong> the input value <code>x</code> into each element of <code>X_MM</code>. In this way the memory for <code>X_MM</code> would be allocated only once.</p>

<p>Is there an ""atomic"" instructions for copying a vector, line by line, into another (a sort of tiling without allocating new memory)? Should I use a TensorFlow iterator for obtaining this?</p>

<p>More in general, should I worry about memory management with TensorFlow? It seems to me it is an important topic, but cannot find any relevant info on the documentation and all of the examples I see use operators that allocate new memory.</p>
","Even if the question involves TensorFlow, I will use normal math terminology to describe my question. Let's say that After reading some code examples, the way I do it now is as it follows (n = self.nNodes, k=self.inputShape): It seems to be - but I am not sure - that, after a while, TensorFlow has some difficulties in managing the memory (especially on the GPU) since the expand_dims and tile operations return new tensors. Is there any way to allocate a tensor for X_MM (as I do for W) and copy the input value x into each element of X_MM. In this way the memory for X_MM would be allocated only once. Is there an ""atomic"" instructions for copying a vector, line by line, into another (a sort of tiling without allocating new memory)? Should I use a TensorFlow iterator for obtaining this? More in general, should I worry about memory management with TensorFlow? It seems to me it is an important topic, but cannot find any relevant info on the documentation and all of the examples I see use operators that allocate new memory.",https://stackoverflow.com/questions/43848414,774133,Lack of Alternative Solutions/Documentation
44000781,Tensorflow aggregation_method for optimizers,"<p>I could not find documentation regarding the aggregation method in tensorflow optimizer</p>

<p>I have the following line of code </p>

<pre><code>train_op = optimizer.minimize(loss, global_step=batch, aggregation_method = tf.AggregationMethod.EXPERIMENTAL_TREE)
</code></pre>

<p>However, this property can be changed to be </p>

<pre><code>tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N
</code></pre>

<p>Does anyone know what does it do? (I just know that when I used the default with an LSTM it did not have enough memory to run) </p>
","I could not find documentation regarding the aggregation method in tensorflow optimizer I have the following line of code However, this property can be changed to be Does anyone know what does it do? (I just know that when I used the default with an LSTM it did not have enough memory to run)",https://stackoverflow.com/questions/44000781,6828367,Documentation Completeness
44177817,Calling reshape on an LSTMStateTuple turns it into a tensor,"<p>I was using dynamic_rnn with an LSTMCell, which put out an LSTMStateTuple containing the inner state. Calling reshape on this object (by my mistake) results in a tensor without causing any error at graph creation. I didn't get any error at runtime when feeding input through the graph, either.</p>

<p>Code:</p>

<pre><code>cell = tf.contrib.rnn.LSTMCell(size, state_is_tuple=True, ...)
outputs, states = tf.nn.dynamic_rnn(cell, inputs, ...)
print(states) # state is an LSTMStateTuple
states = tf.reshape(states, [-1, size])
print(states) # state is a tensor of shape [?, size]
</code></pre>

<p>Is this a bug (I ask because it's not documented anywhere)? What is the reshaped tensor holding?</p>
","I was using dynamic_rnn with an LSTMCell, which put out an LSTMStateTuple containing the inner state. Calling reshape on this object (by my mistake) results in a tensor without causing any error at graph creation. I didn't get any error at runtime when feeding input through the graph, either. Code: Is this a bug (I ask because it's not documented anywhere)? What is the reshaped tensor holding?",https://stackoverflow.com/questions/44177817,4204963,Lack of Alternative Solutions/Documentation
44472358,Handling large image dataset in tensorflow,"<p>I have a dataset of over 1.5 million images and I have to classify them into 62 classes. I have created two numpy array features (path of png images)  and labels (int label). Now I want to load these images using opencv, but handing such large loaded input in RAM is inefficient. </p>

<p>So I also tried following using tensorflow input pipeline documentation:</p>

<pre><code>import tensorflow as tf

filename_queue = 
tf.train.string_input_producer(['batch1.csv','batch2.csv'])
reader = tf.TextLineReader(skip_header_lines=1)
key,value = reader.read(filename_queue)

record_defaults = [['1'],['1']]
paths, labels = tf.decode_csv(value, record_defaults=record_defaults)

features_path = tf.stack([paths])
labels = tf.stack([labels])

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    #Start all QueueRunners added into the graph
    threads = tf.train.start_queue_runners(coord=coord)

    for _ in range(1):
        # d_features, d_labels = sess.run([features_path, labels])
        # print len(d_features), len(d_labels)

        min_after_dequeue = 5
        batch_size = 32
        capacity = 30
        #capacity = min_after_dequeue + 3 * batch_size

        example_batch, label_batch = tf.train.shuffle_batch(
            [features_path, labels], batch_size=batch_size, 
            capacity=capacity,
            min_after_dequeue=min_after_dequeue
        )
        print sess.run([example_batch])
</code></pre>

<p>But this is getting stuck when I run it ( I tried printing the shape of tensor, which is coming as expected, but its not printing the batch of my features). </p>

<p>It will be really helpful if someone can guide me a better way to create batches and load images which can be later fed into the tensorflow model.</p>
","I have a dataset of over 1.5 million images and I have to classify them into 62 classes. I have created two numpy array features (path of png images) and labels (int label). Now I want to load these images using opencv, but handing such large loaded input in RAM is inefficient. So I also tried following using tensorflow input pipeline documentation: But this is getting stuck when I run it ( I tried printing the shape of tensor, which is coming as expected, but its not printing the batch of my features). It will be really helpful if someone can guide me a better way to create batches and load images which can be later fed into the tensorflow model.",https://stackoverflow.com/questions/44472358,3316461,Documentation Replicability
44482637,How to use Tensorflow queues in real example,"<p>I have a Tensorflow  DNN model in with I use <code>feed_dict</code> to feed the Input 
<code>Training and Test</code> data and the labels that belongs to them. To keep things simple  this the important part of the code: </p>

<pre><code>def feed_dict(train):
""""""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""
if train == True :
    xs,ys = next_Training_Batch()
    drop_out_value = 0.9
else:
    #Run a test
    xs,ys=  Testing_Data,Testing_Labels
    drop_out_value = 1
return {x:xs,y_:ys,keep_prob:drop_out_value}
for i in range(max_steps):
    if i%5 ==0: # Record summarie and Test-set accruracy
        summary, acc = sess.run([merged,accuracy], feed_dict=feed_dict(False))
        test_writer.add_summary(summary,i)
        #print('Accuracy at steps%s: %s '%(i,acc))
    else:# Record train set summaries and train
        if i%10==0:
            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
            run_metadata = tf.RunMetadata()
            summary, _ = sess.run([merged, train_steps],
                          feed_dict=feed_dict(True),
                          options=run_options,
                          run_metadata=run_metadata)
            train_writer.add_run_metadata(run_metadata, 'step%03d' % i)
            train_writer.add_summary(summary, i)
        else:
            summary,_ = sess.run([merged, train_steps], feed_dict=feed_dict(True))
            train_writer.add_summary(summary,i)
</code></pre>

<p>I've been reading about the TF queue are a way more efficient, I've have huge difficulties get them running, here is what I've done so far: </p>

<pre><code>    import tensorflow as tf

'''
# Both Training.csv and Test.csv  include the features values and the and labels as follows :
Feature0  Feature1  Feature2  Feature3  Feature4  Feature5   .......  ClassID(Labels)  onehot
0.200985  1.000000  0.064534  0.415348  0.005391  1.000000             1000             1
0.151232  1.000000  0.048849  0.312474  0.007160  1.000000             2001             2
0.061576  1.000000  0.026125  0.127097  0.017450  1.000000             1000             3
...............................................................................
Each file has &gt; 2500 rows
'''

fileNames = [""Training.csv"",""Test.csv""]
BATCH_SIZE = 20
number_OF_features  = 450

def batch_generator(fileNames):
    fileNames_queue = tf.train.string_input_producer(fileNames)
    reader = tf.TextLineReader(skip_header_lines=1)
    key , values = reader.read(fileNames_queue)
    record_defaults = [[1.0] for _ in range(number_OF_features)]
    content = tf.decode_csv(values,record_defaults = record_defaults)
    features = tf.stack(content[:-2])
    labels = content[-1]
    min_after_dequeue =10 * BATCH_SIZE
    capacity = 20*BATCH_SIZE
    # suffle the data
    data_batch, label_batch = tf.train.shuffle_batch([features, labels], batch_size=BATCH_SIZE,
    capacity =capacity , min_after_dequeue = min_after_dequeue)
    return data_batch , label_batch

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    for _ in range(100 ): # generating 100 batch
        sess.run(batch_generator(fileNames))
        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        # I just  don'T get how to proceed from this point
        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
        coord.request_stop()
        coord.join(threads)
</code></pre>

<p>My question is how can I ""feed""  data  in train and testing.  I've been reading TF docs but it didn't help.</p>

<p>Ref: The code I wrote is based on this <a href=""https://github.com/chiphuyen/tf-stanford-tutorials/tree/master/examples"" rel=""nofollow noreferrer"">great tutorials</a></p>
","I have a Tensorflow DNN model in with I use feed_dict to feed the Input Training and Test data and the labels that belongs to them. To keep things simple this the important part of the code: I've been reading about the TF queue are a way more efficient, I've have huge difficulties get them running, here is what I've done so far: My question is how can I ""feed"" data in train and testing. I've been reading TF docs but it didn't help. Ref: The code I wrote is based on this great tutorials",https://stackoverflow.com/questions/44482637,1563123,Documentation Completeness
44518790,"Are tensor names always prepended with ""import/"" when loaded from protobuf?","<p>When I load a (frozen) Tensorflow model from disk using:</p>

<pre><code>graph = tf.Graph()
  with graph.as_default():
        f = gfile.FastGFile(""frozen_graph.pb"", ""rb"")
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        tf.import_graph_def(graph_def)
</code></pre>

<p>It seems that all tensor names are prepended with  import/ .
 This is the code I use to print the names:</p>

<pre><code>with tf.Session(graph=graph) as sess:
        all_ops = sess.graph.get_operations()
        op_values =  [op.values() for op in all_ops]
        for values in op_values:
            for each in value:
                print each.name
</code></pre>

<p>Why? Is this some kind of default option that can be overriden? Or can I rely on this in my code? I could not find this documented anyhwere, can anybody point me to references regarding this?</p>
","When I load a (frozen) Tensorflow model from disk using: It seems that all tensor names are prepended with import/ . This is the code I use to print the names: Why? Is this some kind of default option that can be overriden? Or can I rely on this in my code? I could not find this documented anyhwere, can anybody point me to references regarding this?",https://stackoverflow.com/questions/44518790,7264906,Lack of Alternative Solutions/Documentation
44975585,Reading data in tensorflow,"<p>I am learning how to use tensorflow from the documentation. But, I cannot understand the below two functions. I've tried searching the documentation but, I could not get a clear answer.</p>

<pre><code>input_fn = tf.contrib.learn.io.numpy_input_fn({""x"":x_train}, y_train,
                                          batch_size=4,
                                          num_epochs=1000)
eval_input_fn = tf.contrib.learn.io.numpy_input_fn(
    {""x"":x_eval}, y_eval, batch_size=4, num_epochs=1000)
</code></pre>

<p>Also, it would be great if you could explain what the parameters in the functions are. Thanks in advance!</p>
","I am learning how to use tensorflow from the documentation. But, I cannot understand the below two functions. I've tried searching the documentation but, I could not get a clear answer. Also, it would be great if you could explain what the parameters in the functions are. Thanks in advance!",https://stackoverflow.com/questions/44975585,5459235,Documentation Ambiguity
45022315,Tensorflow ValueError: Too many vaues to unpack (expected 2),"<p>I have looked this up on Reddit, Stack Overflow, tech forums, documentation, GitHub issues etc etc and still can't solve this issue.</p>

<p>For reference, I am using <code>Python 3 TensorFlow</code> on Windows 10, 64 Bit.</p>

<p>I am trying to use my own dataset (300 pics of cats, 512x512, .png format) in <code>Tensorflow</code> to train it to know what a cat looks like. If this works I will train it with other animals and eventually objects.</p>

<p>I can't seem to figure out why I am getting the error <code>ValueError: too many values to unpack (expected 2)</code>. The error appears in the line <code>images,labal = create_batches(10)</code>, which points to my function <code>create_batches</code> (see below). I don't know what could be causing this as I am fairly new to <code>TensorFlow</code>. I am trying to make my own Neural Network based on the MNIST Dataset. Code below:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np
import os
import sys
import cv2


content = []
labels_list = []
with open(""data/cats/files.txt"") as ff:
    for line in ff:
        line = line.rstrip()
        content.append(line)

with open(""data/cats/labels.txt"") as fff:
    for linee in fff:
        linee = linee.rstrip()
        labels_list.append(linee)

def create_batches(batch_size):
    images = []
    for img in content:
        #f = open(img,'rb')
        #thedata = f.read().decode('utf8')
        thedata = cv2.imread(img)
        thedata = tf.contrib.layers.flatten(thedata)
        images.append(thedata)
    images = np.asarray(images)

    labels =tf.convert_to_tensor(labels_list,dtype=tf.string)

    print(content)
    #print(labels_list)

    while(True):
        for i in range(0,298,10):
            yield images[i:i+batch_size],labels_list[i:i+batch_size]


imgs = tf.placeholder(dtype=tf.float32,shape=[None,262144])
lbls = tf.placeholder(dtype=tf.float32,shape=[None,10])

W = tf.Variable(tf.zeros([262144,10]))
b = tf.Variable(tf.zeros([10]))

y_ = tf.nn.softmax(tf.matmul(imgs,W) + b)

cross_entropy = tf.reduce_mean(-tf.reduce_sum(lbls * tf.log(y_),reduction_indices=[1]))
train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)

sess = tf.InteractiveSession()
tf.global_variables_initializer().run()
for i in range(10000):
    images,labal = create_batches(10)
    sess.run(train_step, feed_dict={imgs:images, lbls: labal})

correct_prediction = tf.equal(tf.argmax(y_,1),tf.argmax(lbls,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

print(sess.run(accuracy, feed_dict={imgs:content, lbls:labels_list}))
</code></pre>

<p>And the Error:</p>

<pre><code>Traceback (most recent call last):
  File ""B:\Josh\Programming\Python\imgpredict\predict.py"", line 54, in &lt;module&gt;

    images,labal = create_batches(2)
ValueError: too many values to unpack (expected 2)
libpng warning: iCCP: known incorrect sRGB profile
libpng warning: iCCP: known incorrect sRGB profile
libpng warning: iCCP: known incorrect sRGB profile
libpng warning: iCCP: known incorrect sRGB profile
(A few hundred lines of this)
libpng warning: iCCP: known incorrect sRGB profile
libpng warning: iCCP: known incorrect sRGB profile
libpng warning: iCCP: known incorrect sRGB profile
</code></pre>

<p>My <a href=""https://github.com/supamonkey2000/jm-uofa"" rel=""nofollow noreferrer"" title=""GitHub"">GitHub link</a> link if anyone needs it. The project folder is the ""imgpredict"".</p>
","I have looked this up on Reddit, Stack Overflow, tech forums, documentation, GitHub issues etc etc and still can't solve this issue. For reference, I am using Python 3 TensorFlow on Windows 10, 64 Bit. I am trying to use my own dataset (300 pics of cats, 512x512, .png format) in Tensorflow to train it to know what a cat looks like. If this works I will train it with other animals and eventually objects. I can't seem to figure out why I am getting the error ValueError: too many values to unpack (expected 2). The error appears in the line images,labal = create_batches(10), which points to my function create_batches (see below). I don't know what could be causing this as I am fairly new to TensorFlow. I am trying to make my own Neural Network based on the MNIST Dataset. Code below: And the Error: My GitHub link link if anyone needs it. The project folder is the ""imgpredict"".",https://stackoverflow.com/questions/45022315,,Requesting (Additional) Resources
45097828,Tensorflow: load multiple images to process (Python),"<p>I am working on a Tensorflow project (<a href=""https://github.com/niektemme/tensorflow-mnist-predict"" rel=""nofollow noreferrer"">https://github.com/niektemme/tensorflow-mnist-predict</a>) and I am in front of a problem.
In the file named <code>predict_2.py</code> there is a block of code which load an image..</p>

<pre><code>def imageprepare(argv):
    """"""
    This function returns the pixel values.
    The input is a png file location.
    """"""

    im = Image.open(argv).convert('L')
    width = float(im.size[0])
    height = float(im.size[1])
    newImage = Image.new('L', (28, 28), (255)) #creates white canvas of 28x28 pixels

    if width &gt; height: #check which dimension is bigger
        #Width is bigger. Width becomes 20 pixels.
        nheight = int(round((20.0/width*height),0)) #resize height according to ratio width
        if (nheight == 0): #rare case but minimum is 1 pixel
            nheigth = 1  
        # resize and sharpen
        img = im.resize((20,nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)
        wtop = int(round(((28 - nheight)/2),0)) #caculate horizontal pozition
        newImage.paste(img, (4, wtop)) #paste resized image on white canvas
    else:
        #Height is bigger. Heigth becomes 20 pixels. 
        nwidth = int(round((20.0/height*width),0)) #resize width according to ratio height
        if (nwidth == 0): #rare case but minimum is 1 pixel
            nwidth = 1
         # resize and sharpen
        img = im.resize((nwidth,20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)
        wleft = int(round(((28 - nwidth)/2),0)) #caculate vertical pozition
        newImage.paste(img, (wleft, 4)) #paste resized image on white canvas

    #newImage.save(""sample.png"")

    tv = list(newImage.getdata()) #get pixel values

    #normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.
    tva = [ (255-x)*1.0/255.0 for x in tv] 
    return tva
</code></pre>

<p>Normally, I execute all the code (not just this one) in CMD so the program works.
What I would like to do now is to load multiple images (maybe from a different folder) and not just to pass the argv in the console which load only one image at the time.</p>

<p>What I previously thought was to create an array/list of images and then to put it instead of <code>im</code> variable. It doesn't work...</p>

<p>Can someone help ? Documentation on Tensorflow (and even the project itself) are very poor (atleast for me).</p>

<p>Thank you.</p>

<p>EDIT:</p>

<p>Here is full code</p>

<pre><code>""""""Predict a handwritten integer (MNIST expert).

Script requires
1) saved model (model2.ckpt file) in the same location as the script is run from.
(requried a model created in the MNIST expert tutorial)
2) one argument (png file location of a handwritten integer)

Documentation at:
http://niektemme.com/ @@to do
""""""

#import modules
import sys
import tensorflow as tf
from PIL import Image, ImageFilter
from PIL import Image as PImage
import os
from os import listdir
from datetime import datetime
from os import listdir

def predictint(imvalue):
    """"""
    This function returns the predicted integer.
    The input is the pixel values from the imageprepare() function.
    """"""

    # Define the model (same as when creating the model file)
    x = tf.placeholder(tf.float32, [None, 784])
    W = tf.Variable(tf.zeros([784, 10]))
    b = tf.Variable(tf.zeros([10]))

    def weight_variable(shape):
      initial = tf.truncated_normal(shape, stddev=0.1)
      return tf.Variable(initial)

    def bias_variable(shape):
      initial = tf.constant(0.1, shape=shape)
      return tf.Variable(initial)

    def conv2d(x, W):
      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

    def max_pool_2x2(x):
      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')   

    W_conv1 = weight_variable([5, 5, 1, 32])
    b_conv1 = bias_variable([32])

    x_image = tf.reshape(x, [-1,28,28,1])
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)

    W_conv2 = weight_variable([5, 5, 32, 64])
    b_conv2 = bias_variable([64])

    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    h_pool2 = max_pool_2x2(h_conv2)

    W_fc1 = weight_variable([7 * 7 * 64, 1024])
    b_fc1 = bias_variable([1024])

    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

    W_fc2 = weight_variable([1024, 10])
    b_fc2 = bias_variable([10])

    y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)

    init_op = tf.initialize_all_variables()
    saver = tf.train.Saver()

    """"""
    Load the model2.ckpt file
    file is stored in the same directory as this python script is started
    Use the model to predict the integer. Integer is returend as list.

    Based on the documentatoin at
    https://www.tensorflow.org/versions/master/how_tos/variables/index.html
    """"""
    with tf.Session() as sess:
        sess.run(init_op)
        saver.restore(sess, ""model2.ckpt"")
        #print (""Model restored."")

        prediction=tf.argmax(y_conv,1)

        return prediction.eval(feed_dict={x: [imvalue],keep_prob: 1.0}, session=sess)


def imageprepare(argv):
    """"""
    This function returns the pixel values.
    The input is a png file location.
    """"""

    im = Image.open(argv).convert('L')
    width = float(im.size[0])
    height = float(im.size[1])
    newImage = Image.new('L', (28, 28), (255)) #creates white canvas of 28x28 pixels

    if width &gt; height: #check which dimension is bigger
        #Width is bigger. Width becomes 20 pixels.
        nheight = int(round((20.0/width*height),0)) #resize height according to ratio width
        if (nheight == 0): #rare case but minimum is 1 pixel
            nheigth = 1  
        # resize and sharpen
        img = im.resize((20,nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)
        wtop = int(round(((28 - nheight)/2),0)) #caculate horizontal pozition
        newImage.paste(img, (4, wtop)) #paste resized image on white canvas
    else:
        #Height is bigger. Heigth becomes 20 pixels. 
        nwidth = int(round((20.0/height*width),0)) #resize width according to ratio height
        if (nwidth == 0): #rare case but minimum is 1 pixel
            nwidth = 1
         # resize and sharpen
        img = im.resize((nwidth,20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)
        wleft = int(round(((28 - nwidth)/2),0)) #caculate vertical pozition
        newImage.paste(img, (wleft, 4)) #paste resized image on white canvas

    #newImage.save(""sample.png"")

    tv = list(newImage.getdata()) #get pixel values

    #normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.
    tva = [ (255-x)*1.0/255.0 for x in tv] 
    return tva
    #print(tva)  


def main(argv):
    """"""
    Main function.
    """"""
    imvalue = imageprepare(argv)
    predint = predictint(imvalue)
    print (predint[0]) #first value in list

    timestr = datetime.now().strftime(""%Y%m%d-%H%M%S-%f"")
    file = open('B' + timestr + '.txt', 'w')
    file.write('Causale: ' + str(predint))
    file.close()

if __name__ == ""__main__"":
    main('C:\\Users\\Wkgrp\\Desktop\\numeri\\4.jpg')
</code></pre>
","I am working on a Tensorflow project (https://github.com/niektemme/tensorflow-mnist-predict) and I am in front of a problem. In the file named predict_2.py there is a block of code which load an image.. Normally, I execute all the code (not just this one) in CMD so the program works. What I would like to do now is to load multiple images (maybe from a different folder) and not just to pass the argv in the console which load only one image at the time. What I previously thought was to create an array/list of images and then to put it instead of im variable. It doesn't work... Can someone help ? Documentation on Tensorflow (and even the project itself) are very poor (atleast for me). Thank you. EDIT: Here is full code",https://stackoverflow.com/questions/45097828,8193135,Inadequate Examples
45204898,what TensorFlow hash_bucket_size matters,"<p>I am creating a DNNclassifier with sparse columns. The training data looks like this,</p>

<pre><code>samples        col1                         col2          price label
  eg1    [[0,1,0,0,0,2,0,1,0,3,...]    [[0,0,4,5,0,...]    5.2    0
  eg2    [0,0,...]                     [0,0,...]            0     1
  eg3    [0,0,...]]                    [0,0,...]            0     1
</code></pre>

<p>The following snippet can run successfully,</p>

<pre><code>import tensorflow as tf

sparse_feature_a = tf.contrib.layers.sparse_column_with_hash_bucket('col1', 3, dtype=tf.int32)
sparse_feature_b = tf.contrib.layers.sparse_column_with_hash_bucket('col2', 1000, dtype=tf.int32)

sparse_feature_a_emb = tf.contrib.layers.embedding_column(sparse_id_column=sparse_feature_a, dimension=2)
sparse_feature_b_emb = tf.contrib.layers.embedding_column(sparse_id_column=sparse_feature_b, dimension=2)
feature_c = tf.contrib.layers.real_valued_column('price')

estimator = tf.contrib.learn.DNNClassifier(
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb, feature_c],
    hidden_units=[5, 3],
    n_classes=2,
    model_dir='./tfTmp/tfTmp0')

# Input builders
def input_fn_train(): # returns x, y (where y represents label's class index).
    features = {'col1': tf.SparseTensor(indices=[[0, 1], [0, 5], [0, 7], [0, 9]],
                                  values=[1, 2, 1, 3],
                                  dense_shape=[3, int(250e6)]),
                'col2': tf.SparseTensor(indices=[[0, 2], [0, 3]],
                                    values=[4, 5],
                                    dense_shape=[3, int(100e6)]),
                        'price': tf.constant([5.2, 0, 0])}
    labels = tf.constant([0, 1, 1])
    return features, labels

estimator.fit(input_fn=input_fn_train, steps=100)
</code></pre>

<p>However, I have a question from this sentence,</p>

<pre><code>sparse_feature_a = tf.contrib.layers.sparse_column_with_hash_bucket('col1', 3, dtype=tf.int32)
</code></pre>

<p>where 3 means <strong>hash_bucket_size=3</strong>, but this sparse tensor includes 4 non-zero values,</p>

<pre><code>'col1': tf.SparseTensor(indices=[[0, 1], [0, 5], [0, 7], [0, 9]],
                              values=[1, 2, 1, 3],
                              dense_shape=[3, int(250e6)])
</code></pre>

<p>It seems <em>has_bucket_size</em> does nothing here. No matter how many non-zero values you have in your sparse tensor, you just need to set it with an integer > 1 and it works correctly.</p>

<p>I know my understanding may not be right. Could anyone explain how <strong>has_bucket_size</strong> works? Thanks a lot!</p>
","I am creating a DNNclassifier with sparse columns. The training data looks like this, The following snippet can run successfully, However, I have a question from this sentence, where 3 means hash_bucket_size=3, but this sparse tensor includes 4 non-zero values, It seems has_bucket_size does nothing here. No matter how many non-zero values you have in your sparse tensor, you just need to set it with an integer &gt; 1 and it works correctly. I know my understanding may not be right. Could anyone explain how has_bucket_size works? Thanks a lot!",https://stackoverflow.com/questions/45204898,5878281,Documentation Ambiguity
45288297,Meaning and dimensions of tf.contrib.learn.DNNClassifier's extracted weights and biases,"<p>I relatively new to tensorflow, but even with a lot of research I was unable to find a documentation of certain variable meanings.</p>

<p>For my current project, I want to train a DNN with the help of tensorflow, and afterwards I want to extract the weight and bias matrices from it to use it in another application OUTSIDE tensorflow. For the first try, I set up a simple network with a [4, 10, 2] structure, which predicts a binary outcome.</p>

<p>I used 3 real_valued_columns and a single sparse_column_with_keys (wrapped in an embedding_column) as features:</p>

<pre><code>def build_estimator(optimizer=None, activation_fn=tf.sigmoid):
    """"""Build an estimator""""""
    # Sparse base columns
    column_stay_point = tf.contrib.layers.sparse_column_with_keys(
        column_name='stay_point',
        keys=['no', 'yes'])

    # Continuous base columns
    column_heading = tf.contrib.layers.real_valued_column('heading')
    column_velocity = tf.contrib.layers.real_valued_column('velocity')
    column_acceleration = tf.contrib.layers.real_valued_column('acceleration')

    pedestrian_feature_columns = [column_heading, 
                                  column_velocity, 
                                  column_acceleration,
                                  tf.contrib.layers.embedding_column(
                                      column_stay_point, 
                                      dimension=8, 
                                      initializer=tf.truncated_normal_initializer)]

    # Create classifier
    estimator = tf.contrib.learn.DNNClassifier(
        hidden_units=[10],
        feature_columns=pedestrian_feature_columns,
        model_dir='./tmp/pedestrian_model',
        n_classes=2,
        optimizer=optimizer,
        activation_fn=activation_fn)

    return estimator
</code></pre>

<p>I called this function with default arguments and used estimator.fit(...) to train the DNN. Aside from some warnings concerning the deprecated 'scalar_summary' function, it ran successfully and produced reasonable results. I printed all variables of the model by using the following line:</p>

<pre><code>var = {k: estimator.get_variable_value(k) for k in estimator.get_variable_names())
</code></pre>

<p>I expected to get a weight matrices of size 10x4 and 2x10 as well as bias matrices of size 10x1 and 2x1. But I got the following:</p>

<pre><code>'dnn/binary_logistic_head/dnn/learning_rate': 0.05 (actual value, scalar)

'dnn/input_from_feature_columns/stay_point_embedding/weights': 2x8 array

'dnn/hiddenlayer_0/weights/hiddenlayer_0/weights/part_0/Adagrad': 11x10 array

'dnn/input_from_feature_columns/stay_point_embedding/weights/int_embedding/weights/part_0/Adagrad': 2x8 array

'dnn/hiddenlayer_0/weights': 11x10 array

'dnn/logits/biases': 1x1' array

'dnn/logits/weights/nn/dnn/logits/weights/part_0/Adagrad': 10x1 array

'dnn/logits/weights': 10x1 array

'dnn/logits/biases/dnn/dnn/logits/biases/part_0/Adagrad': 1x1 array

'global_step': 5800, (actual value, scalar)

'dnn/hiddenlayer_0/biases': 1x10 array

'dnn/hiddenlayer_0/biases//hiddenlayer_0/biases/part_0/Adagrad': 1x10 array
</code></pre>

<p>Is there any documentation what these cryptic names mean and why do the matrices have these weird dimensions? Also, why are there references to the Adagrad optimizer despite never specifying it?</p>

<p>Any help is highly appreciated!</p>
","I relatively new to tensorflow, but even with a lot of research I was unable to find a documentation of certain variable meanings. For my current project, I want to train a DNN with the help of tensorflow, and afterwards I want to extract the weight and bias matrices from it to use it in another application OUTSIDE tensorflow. For the first try, I set up a simple network with a [4, 10, 2] structure, which predicts a binary outcome. I used 3 real_valued_columns and a single sparse_column_with_keys (wrapped in an embedding_column) as features: I called this function with default arguments and used estimator.fit(...) to train the DNN. Aside from some warnings concerning the deprecated 'scalar_summary' function, it ran successfully and produced reasonable results. I printed all variables of the model by using the following line: I expected to get a weight matrices of size 10x4 and 2x10 as well as bias matrices of size 10x1 and 2x1. But I got the following: Is there any documentation what these cryptic names mean and why do the matrices have these weird dimensions? Also, why are there references to the Adagrad optimizer despite never specifying it? Any help is highly appreciated!",https://stackoverflow.com/questions/45288297,8334261,Lack of Alternative Solutions/Documentation
45308755,No scalar data in tensorboard,"<p>I have been reading tensorboard documentation about scalars and I have a problem with presenting it in tensorboard.</p>

<p>I have pip install tensorflow in windows 10</p>

<p>my code looks like this:</p>

<pre><code>import tensorflow as tf

a = tf.constant(7, name='test_variable')
tf.summary.scalar('variable', a)

with tf.Session() as sess:
    tf.summary.FileWriter('my_folder', graph=sess.graph)
    X = tf.global_variables_initializer()
    sess.run(X)
</code></pre>

<p>I see there is a file in <code>my_folder</code></p>

<p>in command prompt: <code>tensorboard --logdir=my_folder --port 6006</code></p>

<p>out:</p>

<pre><code>C:\Users\MM&gt;tensorboard --logdir=my_folder --port 6006
Starting TensorBoard b'54' at http://DESKTOP-9S2D9VF:6006
(Press CTRL+C to quit)
</code></pre>

<p>When I open browser i get:</p>

<pre><code>No scalar data was found. 
Probable causes: etc. etc.
</code></pre>
",I have been reading tensorboard documentation about scalars and I have a problem with presenting it in tensorboard. I have pip install tensorflow in windows 10 my code looks like this: I see there is a file in my_folder in command prompt: tensorboard --logdir=my_folder --port 6006 out: When I open browser i get:,https://stackoverflow.com/questions/45308755,8053410,Documentation Replicability
45316382,tensorflow: initialization of variables inside function,"<p>Newbee to tensorflow. I'm trying to write some simple net with following code:</p>

<pre><code>import tensorflow as tf 
import tensorflow.contrib as tfc
import tensorflow.contrib.layers as tfcl

def generator_deconv(z, kernel):
    with tf.variable_scope(""generator"", reuse=True):
        weights = tf.get_variable(""weights"")
        biases = tf.get_variable(""biases"")
        result = tf.matmul(z, weights)
        result = tf.add(result, biases)
        result = tf.reshape(result, tf.stack([tf.shape(result)[0],13,4,1]))
        result = tf.nn.conv2d_transpose(result, kernel, 
                output_shape=[tf.shape(result)[0],25,8,1], 
                strides=[1,2,2,1], 
                padding=""SAME"")
        result = tf.nn.conv2d_transpose(result, kernel, 
                output_shape=[tf.shape(result)[0],50,15,1], 
                strides=[1,2,2,1], 
                padding=""SAME"")
        result = tf.nn.conv2d_transpose(result, kernel, 
                output_shape=[tf.shape(result)[0],100,30,1], 
                strides=[1,2,2,1], 
                padding=""SAME"")    
        return result

kernel = tf.constant(1.0, shape=[4,4,1,1])
protype = tf.constant(1.0, shape=[3,4])
init = tf.global_variables_initializer()

config = tf.ConfigProto()
config.gpu_options.allocator_type = 'BFC'
config.gpu_options.allow_growth=True

with tf.variable_scope(""generator""):
    t1 = tf.get_variable(""weights"",shape=[4,52])
    t2 = tf.get_variable(""biases"", shape=[52])

test = generator_deconv(protype,kernel)

with tf.Session(config=config) as sess:
    sess.run(init)
    sess.run(tf.shape(t1))
    sess.run(tf.shape(t2))
    sess.run(tf.shape(test))
</code></pre>

<p>but got error: </p>

<blockquote>
  <p>tensorflow.python.framework.errors_impl.FailedPreconditionError:
  Attempting to use uninitialized value generator/weights</p>
</blockquote>

<p>for the <strong>last line</strong></p>

<pre><code>sess.run(tf.shape(test))
</code></pre>

<p>checked official api of tensorflow but still don't know what's wrong with the code.</p>

<p>================================UPDATE==========================</p>

<p>found 2 ways to fix it</p>

<p>1.if replace</p>

<pre><code>sess.run(init)
</code></pre>

<p>by</p>

<pre><code>sess.run(tf.global_variables_initializer())
</code></pre>

<p>then whole code works.</p>

<p>OR</p>

<p>2.run</p>

<pre><code>init = tf.global_variables_initializer()
with tf.Session(config=config) as sess:
    sess.run(init)
    sess.run(tf.shape(t1))
    sess.run(tf.shape(t2))
    sess.run(tf.shape(test))
</code></pre>

<p>again it also works.</p>

<p>BUT don't understand why</p>
",Newbee to tensorflow. I'm trying to write some simple net with following code: but got error: for the last line checked official api of tensorflow but still don't know what's wrong with the code. ================================UPDATE========================== found 2 ways to fix it 1.if replace by then whole code works. OR 2.run again it also works. BUT don't understand why,https://stackoverflow.com/questions/45316382,5155829,Requesting (Additional) Resources
45437572,Tensor Shape Error: Must be rank 2 but is rank 3,"<p>I am having difficulty searching for documentation, studies, or blogs that can help me in building text sequence (features) classifier. The text sequence that I have contains logs of network.</p>

<p>I am building a GRU model using TensorFlow, with an SVM as the classification function. I am having trouble with the tensor shapes. It says <code>ValueError: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [?,23,1], [512,2]</code>. <a href=""https://gist.github.com/AFAgarap/61b17a3f02a9e13eb0c7aad9406a3408"" rel=""nofollow noreferrer"">Here is a sample</a> of the data I am using for training my neural network.</p>

<p>The goal of my project is to use this GRU-SVM model for intrusion detection on <a href=""http://www.takakura.com/Kyoto_data/"" rel=""nofollow noreferrer"">Kyoto University's honeypot system intrusion detection dataset</a>. The dataset has 23 features, and a label (if there is an intrusion in the network or none).</p>

<pre><code>import data
import numpy as np
import os
import tensorflow as tf


BATCH_SIZE = 200
CELLSIZE = 512
NLAYERS = 3
SVMC = 1
learning_rate = 0.01

TRAIN_PATH = '/home/darth/GitHub Projects/gru_svm/dataset/train/6'

def main():
    examples, labels, keys = data.input_pipeline(path=TRAIN_PATH, batch_size=BATCH_SIZE, num_epochs=1)

    seqlen = examples.shape[1]

    x = tf.placeholder(shape=[None, seqlen, 1], dtype=tf.float32)
    y = tf.placeholder(shape=[None, 2], dtype=tf.float32)
    Hin = tf.placeholder(shape=[None, CELLSIZE*NLAYERS], dtype=tf.float32)

    # cell = tf.contrib.rnn.GRUCell(CELLSIZE)
    network = []
    for index in range(NLAYERS):
        network.append(tf.contrib.rnn.GRUCell(CELLSIZE))

    mcell = tf.contrib.rnn.MultiRNNCell(network, state_is_tuple=False)
    Hr, H = tf.nn.dynamic_rnn(mcell, x, initial_state=Hin, dtype=tf.float32)

    Hf = tf.transpose(Hr, [1, 0, 2])
    last = tf.gather(Hf, int(Hf.get_shape()[0]) - 1)

    weight = tf.Variable(tf.truncated_normal([CELLSIZE, 2], stddev=0.01), tf.float32)
    bias = tf.Variable(tf.constant(0.1, shape=[2]))
    logits = tf.matmul(last, weight) + bias

    regularization_loss = 0.5 * tf.reduce_sum(tf.square(weight))
    hinge_loss = tf.reduce_sum(tf.maximum(tf.zeros([BATCH_SIZE, 1]), 1 - y * logits))
    loss = regularization_loss + SVMC * hinge_loss

    train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)

    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())

    with tf.Session() as sess:
        sess.run(init_op)

        train_loss = 0

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)

        try:
            for index in range(100):
                for j in range(1000):
                    example_batch, label_batch, key_batch = sess.run([examples, labels, keys])
                    _, train_loss_ = sess.run([train_step, loss],
                        feed_dict = { x : example_batch,
                                        y : label_batch,
                                        Hin : np.zeros([BATCH_SIZE, CELLSIZE * NLAYERS])
                                    })
                    train_loss += train_loss_
                print('[{}] loss : {}'.format(index, (train_loss / 1000)))
                train_loss = 0
        except tf.errors.OutOfRangeError:
            print('EOF reached.')
        except KeyboardInterrupt:
            print('Interrupted by user at {}'.format(index))
        finally:
            coord.request_stop()
        coord.join(threads)

main()
</code></pre>

<p>Note: The reason why I built my <code>MultiRNNCell</code> as I did (snippet isolated below) is because I was having an error similar to this <a href=""https://stackoverflow.com/questions/44615147/valueerror-trying-to-share-variable-rnn-multi-rnn-cell-cell-0-basic-lstm-cell-k"">post</a>.</p>

<pre><code>network = []
for index in range(NLAYERS):
    network.append(tf.contrib.rnn.GRUCell(CELLSIZE))
</code></pre>

<p>Thank you in advance for your response!</p>

<p><strong>Update 08/01/2017</strong> 
The source was improved based on @jdehesa's sugestions:</p>

<pre><code>import data
import numpy as np
import os
import tensorflow as tf


BATCH_SIZE = 200
CELLSIZE = 512
NLAYERS = 3
SVMC = 1
learning_rate = 0.01

TRAIN_PATH = '/home/darth/GitHub Projects/gru_svm/dataset/train/6'

def main():
    examples, labels, keys = data.input_pipeline(path=TRAIN_PATH, batch_size=BATCH_SIZE, num_epochs=1)

    seqlen = examples.shape[1]

    x = tf.placeholder(shape=[None, seqlen, 1], dtype=tf.float32, name='x')
    y_input = tf.placeholder(shape=[None], dtype=tf.int32, name='y_input')
    y = tf.one_hot(y_input, 2, dtype=tf.float32, name='y')
    Hin = tf.placeholder(shape=[None, CELLSIZE*NLAYERS], dtype=tf.float32, name='Hin')

    network = []
    for index in range(NLAYERS):
        network.append(tf.contrib.rnn.GRUCell(CELLSIZE))

    mcell = tf.contrib.rnn.MultiRNNCell(network, state_is_tuple=False)
    Hr, H = tf.nn.dynamic_rnn(mcell, x, initial_state=Hin, dtype=tf.float32)

    Hf = tf.transpose(Hr, [1, 0, 2])
    last = tf.gather(Hf, int(Hf.get_shape()[0]) - 1)

    weight = tf.Variable(tf.truncated_normal([CELLSIZE, 2], stddev=0.01), tf.float32, name='weights')
    bias = tf.Variable(tf.constant(0.1, shape=[2]), name='bias')
    logits = tf.matmul(last, weight) + bias

    regularization_loss = 0.5 * tf.reduce_sum(tf.square(weight))
    hinge_loss = tf.reduce_sum(tf.maximum(tf.zeros([BATCH_SIZE, 1]), 1 - y * logits))
    loss = regularization_loss + SVMC * hinge_loss

    train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)

    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())

    with tf.Session() as sess:
        sess.run(init_op)

        train_loss = 0

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)

        try:
            for index in range(100):
                example_batch, label_batch, key_batch = sess.run([examples, labels, keys])
                _, train_loss_ = sess.run([train_step, loss],
                    feed_dict = { x : example_batch[..., np.newaxis],
                                    y_input : label_batch,
                                    Hin : np.zeros([BATCH_SIZE, CELLSIZE * NLAYERS])
                                })
                train_loss += train_loss_
                print('[{}] loss : {}'.format(index, (train_loss / 1000)))
                print('Weights : {}'.format(sess.run(weight)))
                print('Biases : {}'.format(sess.run(bias)))
                train_loss = 0
        except tf.errors.OutOfRangeError:
            print('EOF reached.')
        except KeyboardInterrupt:
            print('Interrupted by user at {}'.format(index))
        finally:
            coord.request_stop()
        coord.join(threads)

main()
</code></pre>

<p>My next move is to validate if the results I'm getting are correct.</p>
","I am having difficulty searching for documentation, studies, or blogs that can help me in building text sequence (features) classifier. The text sequence that I have contains logs of network. I am building a GRU model using TensorFlow, with an SVM as the classification function. I am having trouble with the tensor shapes. It says ValueError: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [?,23,1], [512,2]. Here is a sample of the data I am using for training my neural network. The goal of my project is to use this GRU-SVM model for intrusion detection on Kyoto University's honeypot system intrusion detection dataset. The dataset has 23 features, and a label (if there is an intrusion in the network or none). Note: The reason why I built my MultiRNNCell as I did (snippet isolated below) is because I was having an error similar to this post. Thank you in advance for your response! Update 08/01/2017 The source was improved based on @jdehesa's sugestions: My next move is to validate if the results I'm getting are correct.",https://stackoverflow.com/questions/45437572,6838049,Lack of Alternative Solutions/Documentation
45469356,Number of steps doesn't match when using tf.estimator.Estimator,"<p>I am figuring out the TensorFlow estimator framework. I finally have code for a model that trains. I am using a simple MNIST autoencoder for my tests. I have two questions. The first question is why the number of steps reported by training is different from the number of steps I specify in estimator train() method? The second one is how to use training hooks to do things like periodic evaluations, loss output every X steps etc? The docs seem to say to use training hooks, but I cannot seem to find any actual examples of how to use these.</p>

<p>Here is my code:</p>

<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import time
import shutil
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from IPython import display
from tensorflow.examples.tutorials.mnist import input_data

data = input_data.read_data_sets('.')
display.clear_output()

def _model_fn(features, labels, mode=None, params=None):
    # define inputs
    image = tf.feature_column.numeric_column('images', shape=(784, ))
    inputs = tf.feature_column.input_layer(features, [image, ])
    # encoder
    e1 = tf.layers.dense(inputs, 512, activation=tf.nn.relu)
    e2 = tf.layers.dense(e1, 256, activation=tf.nn.relu)
    # decoder
    d1 = tf.layers.dense(e2, 512, activation=tf.nn.relu)
    model = tf.layers.dense(d1, 784, activation=tf.nn.relu)
    # training ops
    loss = tf.losses.mean_squared_error(labels, model)
    train = tf.train.AdamOptimizer().minimize(loss, global_step=tf.train.get_global_step())
    if mode == tf.estimator.ModeKeys.TRAIN:
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train)

_train_input_fn = tf.estimator.inputs.numpy_input_fn({'images': data.train.images},
                                                     y=np.array(data.train.images),
                                                     batch_size=100,
                                                     shuffle=True)

shutil.rmtree(""logs"", ignore_errors=True)
tf.logging.set_verbosity(tf.logging.INFO)
estimator = tf.estimator.Estimator(_model_fn, 
                                   model_dir=""logs"", 
                                   config=tf.contrib.learn.RunConfig(save_checkpoints_steps=1000),
                                   params={})
estimator.train(_train_input_fn, steps=1000)
</code></pre>

<p>And here is the output I get (notice how training stops at 550 steps where the code explicitely calls for a 1000)</p>

<pre><code>INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x12b9fa630&gt;, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1
}
, '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_session_config': None, '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'logs'}
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Saving checkpoints for 1 into logs/model.ckpt.
INFO:tensorflow:loss = 0.102862, step = 1
INFO:tensorflow:global_step/sec: 41.8119
INFO:tensorflow:loss = 0.0191228, step = 101 (2.393 sec)
INFO:tensorflow:global_step/sec: 39.9923
INFO:tensorflow:loss = 0.0141014, step = 201 (2.500 sec)
INFO:tensorflow:global_step/sec: 40.9806
INFO:tensorflow:loss = 0.0116138, step = 301 (2.440 sec)
INFO:tensorflow:global_step/sec: 40.0043
INFO:tensorflow:loss = 0.00998991, step = 401 (2.500 sec)
INFO:tensorflow:global_step/sec: 39.2571
INFO:tensorflow:loss = 0.0124132, step = 501 (2.548 sec)
INFO:tensorflow:Saving checkpoints for 550 into logs/model.ckpt.
INFO:tensorflow:Loss for final step: 0.00940801.

&lt;tensorflow.python.estimator.estimator.Estimator at 0x12b9fa780&gt;
</code></pre>

<p><strong>Update #1</strong> I found the answer to the first question. The reason training stopped at step 550 was because numpy_input_fn() defaults to num_epochs=1. I am still looking for help with training hooks though.</p>
","I am figuring out the TensorFlow estimator framework. I finally have code for a model that trains. I am using a simple MNIST autoencoder for my tests. I have two questions. The first question is why the number of steps reported by training is different from the number of steps I specify in estimator train() method? The second one is how to use training hooks to do things like periodic evaluations, loss output every X steps etc? The docs seem to say to use training hooks, but I cannot seem to find any actual examples of how to use these. Here is my code: And here is the output I get (notice how training stops at 550 steps where the code explicitely calls for a 1000) Update #1 I found the answer to the first question. The reason training stopped at step 550 was because numpy_input_fn() defaults to num_epochs=1. I am still looking for help with training hooks though.",https://stackoverflow.com/questions/45469356,302268,Inadequate Examples
45679562,Output from fully connected network in tensorflow,"<pre><code>weights = {

# 5x5 conv, 1 input, 32 outputs

'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),
`# 5x5 conv, 32 inputs, 64 outputs`

'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])), 

# fully connected, 7*7*64 inputs, 1024 outputs
'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])), 
 # 1024 inputs, 10 outputs (class prediction)



'out': tf.Variable(tf.random_normal([1024, n_classes])) 


}
</code></pre>

<p>In this code,the output from fully connected layer is given as 1024 but I cannot understand from which calculation this '1024' is generated and I cannot find any satisfactory answer from tensorflow documentation.And How this ouput size affects the prediction result.
Thanks in advance.</p>
","In this code,the output from fully connected layer is given as 1024 but I cannot understand from which calculation this '1024' is generated and I cannot find any satisfactory answer from tensorflow documentation.And How this ouput size affects the prediction result. Thanks in advance.",https://stackoverflow.com/questions/45679562,7580858,Documentation Ambiguity
46080421,"Tensorflow, printing loss function causes error without feed_dictionary","<p>I am just reading Tensorflow documentation. In following code, I just changed last line. I pushed last line in iteration, to see what exactly is going on...</p>

<pre><code>import tensorflow as tf

# linear_model = W*x+B

W = tf.Variable(.3, dtype=tf.float32)
B = tf.Variable(-3., dtype=tf.float32)
x = tf.placeholder(dtype=tf.float32) #data_X
linear_model = W*x+B

y = tf.placeholder(dtype=tf.float32) #data_Y

loss = tf.reduce_sum(tf.square(linear_model-y))

optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

X_train = [1.0,2.0,3.0,4.0] #data_X
y_train = [0.0,-1.0,-2.0,-3.0] #data_y

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(1000):
        sess.run(train,{x:X_train, y:y_train})
        print(sess.run([W,B,loss], {x:X_train, y:y_train}))
</code></pre>

<p>Please check the very last line: <code>print(sess.run([W,B,loss], {x:X_train, y:y_train}))</code></p>

<p>Why do I need to include </p>

<pre><code>{x:X_train, y:y_train}
</code></pre>

<p>in order to print statement out? If you exclude this from the last line, you will get error. It makes no sense, because loss is already calculated in line before. Thanks</p>
","I am just reading Tensorflow documentation. In following code, I just changed last line. I pushed last line in iteration, to see what exactly is going on... Please check the very last line: print(sess.run([W,B,loss], {x:X_train, y:y_train})) Why do I need to include in order to print statement out? If you exclude this from the last line, you will get error. It makes no sense, because loss is already calculated in line before. Thanks",https://stackoverflow.com/questions/46080421,6709460,Documentation Ambiguity
46111888,Initializing variables with imported tensors from another graph,"<p>I am using tensorflow (version : v1.1.0-13-g8ddd727 1.1.0) in python3 (Python 3.4.3 (default, Nov 17 2016, 01:08:31) [GCC 4.8.4] on linux), it is installed from source and GPU-based.</p>

<p>I would like to know if it is possible to initialize variables with imported tensors from another session, as tensorflow documentations does not mention it and I have found it on stackoverflow.</p>

<pre><code>train_dir = './gan/train_logs'
    ckpt = tf.train.latest_checkpoint(train_dir)
    filename = ""."".join([ckpt, 'meta'])
    print(filename)
    saver = tf.train.import_meta_graph(filename)
    saver.restore(sess, ckpt)
    test = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')
</code></pre>

<p>Here the tensors are successfully imported and I want to use them to initialize same generator.</p>

<p>Thanks for your help!</p>
","I am using tensorflow (version : v1.1.0-13-g8ddd727 1.1.0) in python3 (Python 3.4.3 (default, Nov 17 2016, 01:08:31) [GCC 4.8.4] on linux), it is installed from source and GPU-based. I would like to know if it is possible to initialize variables with imported tensors from another session, as tensorflow documentations does not mention it and I have found it on stackoverflow. Here the tensors are successfully imported and I want to use them to initialize same generator. Thanks for your help!",https://stackoverflow.com/questions/46111888,8572742,Documentation Completeness
46646736,Applying custom learning rates to variables in Tensorflow,"<p>In Tensorflow, after I obtain my loss term, I give it to an optimizer and it adds the necessary differentiation and update terms to the computation graph:</p>

<pre><code>global_counter = tf.Variable(0, dtype=DATA_TYPE, trainable=False)
learning_rate = tf.train.exponential_decay(
    INITIAL_LR,  # Base learning rate.
    global_counter,  # Current index into the dataset.
    DECAY_STEP,  # Decay step.
    DECAY_RATE,  # Decay rate.
    staircase=True)
optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(network.finalLoss, global_step=global_counter)
feed_dict = {TRAIN_DATA_TENSOR: samples, TRAIN_LABEL_TENSOR: labels}
results = sess.run([optimizer], feed_dict=feed_dict)
</code></pre>

<p>I want a small modification to this process. I want to scale the <code>learning_rate</code> differently for my every distinct parameter in the network. For example, let <code>A</code> and <code>B</code> two different trainable parameters in the network and let <code>dL/dA</code> and <code>dL/dB</code> the partial derivatives of the parameters with respect to the loss. The momentum optimizer updates the variables as:</p>

<pre><code>   Ma &lt;- 0.9*Ma + learning_rate*dL/dA
   A &lt;- A - Ma

   Mb &lt;- 0.9*Mb + learning_rate*dL/dB
   B &lt;- B - Mb
</code></pre>

<p>I want to modify this as:</p>

<pre><code>   Ma &lt;- 0.9*Ma + ca*learning_rate*dL/dA
   A &lt;- A - Ma

   Mb &lt;- 0.9*Mb + cb*learning_rate*dL/dB
   B &lt;- B - Mb
</code></pre>

<p>Where <code>ca</code> and <code>cb</code> are special learning rate scales for different parameters. As far as I understand, Tensorflow has <code>compute_gradients</code> and <code>apply_gradients</code> methods we can call for such cases, but the documentation is not very clear about how to use them. Any help would be much appreciated. </p>
","In Tensorflow, after I obtain my loss term, I give it to an optimizer and it adds the necessary differentiation and update terms to the computation graph: I want a small modification to this process. I want to scale the learning_rate differently for my every distinct parameter in the network. For example, let A and B two different trainable parameters in the network and let dL/dA and dL/dB the partial derivatives of the parameters with respect to the loss. The momentum optimizer updates the variables as: I want to modify this as: Where ca and cb are special learning rate scales for different parameters. As far as I understand, Tensorflow has compute_gradients and apply_gradients methods we can call for such cases, but the documentation is not very clear about how to use them. Any help would be much appreciated.",https://stackoverflow.com/questions/46646736,1538049,Documentation Completeness
46648536,Tensorflow initialize certain scope only,"<p>He there,</p>

<p>I have a question regarding control over which variable scope is initialized, or at least, which variable scope is used during the run.</p>

<p>Take for example this easy piece of code</p>

<pre><code>import numpy as np
import tensorflow as tf

with tf.variable_scope('0') as scope:
    place_holder_batch_x = tf.Variable(np.random.rand(11,6), dtype=tf.float64)
    place_holder_batch_y = tf.Variable(np.random.rand(8,5), dtype=tf.float64)
    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(3)
    z = place_holder_batch_x*2

with tf.variable_scope('1') as scope:
    place_holder_batch_x = tf.Variable(np.random.rand(10,5), dtype=tf.float64)
    place_holder_batch_y = tf.Variable(np.random.rand(9,6), dtype=tf.float64)
    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(4)
    z = place_holder_batch_x*2

init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)
print(sess.run(z).shape)
</code></pre>

<p>If I would run this as is, I would get the shape of variable z as is defined in variable scope '1'.
But how can I specify which variable scope to use during the session? I couldn't find any answer on stackoverflow or in the documentation...</p>

<p>Of course I could just rename both z's to z1 and z2... but I want to stay on the situation where both scopes look a lot like each other and use the same names...</p>
","He there, I have a question regarding control over which variable scope is initialized, or at least, which variable scope is used during the run. Take for example this easy piece of code If I would run this as is, I would get the shape of variable z as is defined in variable scope '1'. But how can I specify which variable scope to use during the session? I couldn't find any answer on stackoverflow or in the documentation... Of course I could just rename both z's to z1 and z2... but I want to stay on the situation where both scopes look a lot like each other and use the same names...",https://stackoverflow.com/questions/46648536,6329284,Documentation Completeness
46820500,How to handle large amouts of data in tensorflow?,"<p>For my project I have large amounts of data, about 60GB spread into npy files, each holding about 1GB, each containing about 750k records and labels.</p>

<p>Each record is a 345 float32  and the labels are 5 float32.</p>

<p>I read the tensorflow dataset documentation and the queues / threads documentation as well but I can't figure out how to best handle the input for training and then how save the model and weights for future predicting.</p>

<p>My model is pretty straight forward, it looks like this:</p>

<pre><code>x = tf.placeholder(tf.float32, [None, 345], name='x')
y = tf.placeholder(tf.float32, [None, 5], name='y')
wi, bi = weight_and_bias(345, 2048)
hidden_fc = tf.nn.sigmoid(tf.matmul(x, wi) + bi)
wo, bo = weight_and_bias(2048, 5)
out_fc = tf.nn.sigmoid(tf.matmul(hidden_fc, wo) + bo)
loss = tf.reduce_mean(tf.squared_difference(y, out_fc))
train_op = tf.train.AdamOptimizer().minimize(loss)
</code></pre>

<p>The way I was training my neural net was reading the files one at a time in a random order then using a shuffled numpy array to index each file and manually creating each batch to feed the <code>train_op</code> using <code>feed_dict</code>. From everything I read this is very inefficient and I should somehow replace it with datasets or queue and threads but as I said the documentation was of no help.</p>

<p>So, what is the best way to handle large amounts of data in tensorflow?</p>

<p>Also, for reference, my data was saved to a numpy file in a 2 operation step:</p>

<pre><code>with open('datafile1.npy', 'wb') as fp:
    np.save(data, fp)
    np.save(labels, fp)
</code></pre>
","For my project I have large amounts of data, about 60GB spread into npy files, each holding about 1GB, each containing about 750k records and labels. Each record is a 345 float32 and the labels are 5 float32. I read the tensorflow dataset documentation and the queues / threads documentation as well but I can't figure out how to best handle the input for training and then how save the model and weights for future predicting. My model is pretty straight forward, it looks like this: The way I was training my neural net was reading the files one at a time in a random order then using a shuffled numpy array to index each file and manually creating each batch to feed the train_op using feed_dict. From everything I read this is very inefficient and I should somehow replace it with datasets or queue and threads but as I said the documentation was of no help. So, what is the best way to handle large amounts of data in tensorflow? Also, for reference, my data was saved to a numpy file in a 2 operation step:",https://stackoverflow.com/questions/46820500,686572,Inadequate Examples
47058158,restoring two Tensorflow models,"<p>I have trained two separate Tensorflow models and would like to use them both in one Jupyter notebook. I am following the following <a href=""https://stackoverflow.com/questions/41607144/loading-two-models-from-saver-in-the-same-tensorflow-session"">SO post</a>. However, I would like to avoid using <code>with</code> statement as it obscures my understanding of what is happening. Here is my code and error messages:</p>

<pre><code>meta_path_1 = r'.\NN state save\case_guessing-3.meta'
checkpoint_path_1 = r'.\NN state save'

meta_path_2 = r'.\NN state save\class_guessing-3.meta'
checkpoint_path_2 = r'.\NN state save'

new_all_saver_1 = tf.train.import_meta_graph(meta_path_1)
new_all_saver_2 = tf.train.import_meta_graph(meta_path_2)

graph_1 = tf.Graph()
graph_2 = tf.Graph()

sess_1 = tf.Session(graph = graph_1)
sess_2 = tf.Session(graph = graph_2)

new_all_saver_1.restore(sess_1, tf.train.latest_checkpoint(checkpoint_path_1))
new_all_saver_2.restore(sess_2, tf.train.latest_checkpoint(checkpoint_path_2))

predict_tensor_1= graph_1.get_tensor_by_name('predictions:0')
predict_tensor_2= graph_2.get_tensor_by_name('predictions:0')

x_1=graph_1.get_tensor_by_name('input_placeholder:0')
x_2=graph_2.get_tensor_by_name('input_placeholder:0')

print(sess_1.run(tf.shape(x_1)))
print(sess_2.run(tf.shape(x_2)))
</code></pre>

<p>Here is error message:</p>

<pre><code>INFO:tensorflow:Restoring parameters from .\TNC-Kaggle\Output\NN_1\NN state save\case_guessing-3
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-18-9f8dfdc2cc26&gt; in &lt;module&gt;()
     14 sess_2 = tf.Session(graph = graph_2)
     15 
---&gt; 16 new_all_saver_1.restore(sess_1, tf.train.latest_checkpoint(checkpoint_path_1))
     17 new_all_saver_2.restore(sess_2, tf.train.latest_checkpoint(checkpoint_path_2))
     18 

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\training\saver.py in restore(self, sess, save_path)
   1558     logging.info(""Restoring parameters from %s"", save_path)
   1559     sess.run(self.saver_def.restore_op_name,
-&gt; 1560              {self.saver_def.filename_tensor_name: save_path})
   1561 
   1562   @staticmethod

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~\AppData\Local\conda\conda\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1051       raise RuntimeError('Attempted to use a closed Session.')
   1052     if self.graph.version == 0:
-&gt; 1053       raise RuntimeError('The Session graph is empty.  Add operations to the '
   1054                          'graph before calling run().')
   1055 

RuntimeError: The Session graph is empty.  Add operations to the graph before calling run().
</code></pre>

<p>How can I fix it? I already re-read multiple times google docs on interaction between graph and session, but I am still unclear what is missing. Inserging <code>as_default()</code> as some places produced different errors (too many to reproduce here)</p>
","I have trained two separate Tensorflow models and would like to use them both in one Jupyter notebook. I am following the following SO post. However, I would like to avoid using with statement as it obscures my understanding of what is happening. Here is my code and error messages: Here is error message: How can I fix it? I already re-read multiple times google docs on interaction between graph and session, but I am still unclear what is missing. Inserging as_default() as some places produced different errors (too many to reproduce here)",https://stackoverflow.com/questions/47058158,1700890,Documentation Ambiguity
47117498,Does `tf.data.Dataset.repeat()` buffer the entire dataset in memory?,"<p>
Looking at this code example from the TF documentation:</p>

<pre class=""lang-py prettyprint-override""><code>filenames = [""/var/data/file1.tfrecord"", ""/var/data/file2.tfrecord""]
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(...)
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.batch(32)
dataset = dataset.repeat(num_epochs)
iterator = dataset.make_one_shot_iterator()
</code></pre>

<p>Does the <code>dataset.repeat(num_epochs)</code> require that the entire dataset be loaded into memory? Or is it re-initializing the dataset(s) that came before it when it receives an end-of-dataset exception?</p>

<p>The documentation is ambiguous about this point.</p>
",Looking at this code example from the TF documentation: Does the dataset.repeat(num_epochs) require that the entire dataset be loaded into memory? Or is it re-initializing the dataset(s) that came before it when it receives an end-of-dataset exception? The documentation is ambiguous about this point.,https://stackoverflow.com/questions/47117498,4790871,Documentation Ambiguity
47141359,How to calculate factorial in tensorflow?,"<p>I am new to tensorflow, I am trying to find a function that calculates the n!. 
I saw that one can use the gamma function, which was possible with theano, but did not work for tensorflow.</p>

<pre class=""lang-py prettyprint-override""><code>factorial = theano.tensor.gamma(v)
</code></pre>

<p>I am using a for loop to multiply number from n to 1, but I assume there is an easier and faster way. I saw functions related to gamma distribution, but couldn't figure out how to calculate the factorial. Would appreciate if one can point me to some documentation. </p>

<p>Here is the way I do it now</p>

<pre><code>import tensorflow as tf

factorial = tf.Variable(1, ""factorial"")
recursion = tf.Variable(1, ""recursion"")

# calculate factorial
mult = tf.multiply(recursion, factorial)
assign_fact = tf.assign(factorial, mult)

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init) 
    for i in range(2,10):
        counter = tf.assign(recursion, tf.constant(i))
        sess.run(counter)
        sess.run(assign_fact)

        print(i,""factorial is"", sess.run(factorial))

    sess.close()
</code></pre>

<p>Output is </p>

<pre><code>2 factorial is 2
3 factorial is 6
4 factorial is 24
5 factorial is 120
6 factorial is 720
7 factorial is 5040
8 factorial is 40320
9 factorial is 362880
</code></pre>
","I am new to tensorflow, I am trying to find a function that calculates the n!. I saw that one can use the gamma function, which was possible with theano, but did not work for tensorflow. I am using a for loop to multiply number from n to 1, but I assume there is an easier and faster way. I saw functions related to gamma distribution, but couldn't figure out how to calculate the factorial. Would appreciate if one can point me to some documentation. Here is the way I do it now Output is",https://stackoverflow.com/questions/47141359,1577800,Requesting (Additional) Resources
47399201,How to store a dictionary and map words to ints when using Tensorflow Serving?,"<p>I have trained an LSTM RNN classification model on Tensorflow. I was saving and restoring checkpoints to retrain and use the model for testing. Now I want to use Tensorflow serving so that I can use the model in production.</p>

<p>Initially, I would parse through a corpus to create my dictionary which is then used to map words in a string to integers. I would then store this dictionary in a pickle file which could be reloaded when restoring a checkpoint and retraining on a data set or just for using the model so that the mapping is consistent. How do I store this dictionary when saving the model using SavedModelBuilder?</p>

<p>My code for the neural network is as follows. The code for saving the model is towards the end (I am including an overview of the whole structure for context):</p>

<pre><code>...


# Read files and store them in variables
with open('./someReview.txt', 'r') as f:
    reviews = f.read()
with open('./someLabels.txt', 'r') as f:
    labels = f.read()

...

#Pre-processing functions
#Parse through dataset and create a vocabulary
vocab_to_int, reviews = RnnPreprocessing.map_vocab_to_int(reviews)
with open(pickle_path, 'wb') as handle:
    pickle.dump(vocab_to_int, handle, protocol=pickle.HIGHEST_PROTOCOL)

#More preprocessing functions
...


# Building the graph
lstm_size = 256
lstm_layers = 2
batch_size = 1000
learning_rate = 0.01            
n_words = len(vocab_to_int) + 1 

# Create the graph object
tf.reset_default_graph()
with tf.name_scope('inputs'):
    inputs_ = tf.placeholder(tf.int32, [None, None], name=""inputs"")
    labels_ = tf.placeholder(tf.int32, [None, None], name=""labels"")
    keep_prob = tf.placeholder(tf.float32, name=""keep_prob"")

#Create embedding layer LSTM cell, LSTM Layers

...

# Forward pass
with tf.name_scope(""RNN_forward""):
    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)


# Output. We are only interested in the latest output of the lstm cell
with tf.name_scope('predictions'):
    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)
    tf.summary.histogram('predictions', predictions)
#More functions for cost, accuracy, optimizer initialization

... 

# Training
epochs = 1
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    iteration = 1
    for e in range(epochs):
        state = sess.run(initial_state)

        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):
            feed = {inputs_: x,
                    labels_: y[:, None],
                    keep_prob: 0.5,
                    initial_state: state}
            summary, loss, state, _ = sess.run([merged, cost, final_state, optimizer], feed_dict=feed)

            train_writer.add_summary(summary, iteration)

            if iteration%1==0:
                print(""Epoch: {}/{}"".format(e, epochs),
                      ""Iteration: {}"".format(iteration),
                      ""Train loss: {:.3f}"".format(loss))

            if iteration%2==0:
                val_acc = []
                val_state = sess.run(cell.zero_state(batch_size, tf.float32))
                for x, y in get_batches(val_x, val_y, batch_size):
                    feed = {inputs_: x,
                            labels_: y[:, None],
                            keep_prob: 1,
                            initial_state: val_state}
                    summary, batch_acc, val_state = sess.run([merged, accuracy, final_state], feed_dict=feed)
                    val_acc.append(batch_acc)
                print(""Val acc: {:.3f}"".format(np.mean(val_acc)))
            iteration +=1
            test_writer.add_summary(summary, iteration)



    #Saving the model
    export_path = './SavedModel'
    print ('Exporting trained model to %s'%(export_path))

    builder = saved_model_builder.SavedModelBuilder(export_path)

    # Build the signature_def_map.    
    classification_inputs = utils.build_tensor_info(inputs_)
    classification_outputs_classes = utils.build_tensor_info(labels_)

    classification_signature = signature_def_utils.build_signature_def(
        inputs={signature_constants.CLASSIFY_INPUTS: classification_inputs},
        outputs={
          signature_constants.CLASSIFY_OUTPUT_CLASSES:
              classification_outputs_classes,
        },
      method_name=signature_constants.CLASSIFY_METHOD_NAME)


    legacy_init_op = tf.group(
        tf.tables_initializer(), name='legacy_init_op')
    #add the sigs to the servable
    builder.add_meta_graph_and_variables(
        sess, [tag_constants.SERVING],
        signature_def_map={
            signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
                classification_signature
        },
        legacy_init_op=legacy_init_op)
    print (""added meta graph and variables"")

    #save it!
    builder.save()
    print(""model saved"")
</code></pre>

<p>I am not entirely sure if this is the correct way to save a model such as this but this is the only implementation I have found in the documentation and online tutorials.</p>

<p>I haven't found any example or any explicit guide to saving the dictionary or how to use it when restoring a savedModel in the documentation.</p>

<p>When using checkpoints, I would just load the pickle file before running the session. How do I restore this savedModel so that I can use the same word to int mapping using the dictionary? Is there any specific way I should be saving the model or loading it?</p>

<p>I have also added inputs_ as the input for the input signature. This is a sequence of integeres 'after' the words have been mapped. I can't specify a string as input because I get an <code>AttributeError: 'str' object has no attribute 'dtype'</code> . In such cases, how exactly are words mapped to integers in models that are in production?</p>
","I have trained an LSTM RNN classification model on Tensorflow. I was saving and restoring checkpoints to retrain and use the model for testing. Now I want to use Tensorflow serving so that I can use the model in production. Initially, I would parse through a corpus to create my dictionary which is then used to map words in a string to integers. I would then store this dictionary in a pickle file which could be reloaded when restoring a checkpoint and retraining on a data set or just for using the model so that the mapping is consistent. How do I store this dictionary when saving the model using SavedModelBuilder? My code for the neural network is as follows. The code for saving the model is towards the end (I am including an overview of the whole structure for context): I am not entirely sure if this is the correct way to save a model such as this but this is the only implementation I have found in the documentation and online tutorials. I haven't found any example or any explicit guide to saving the dictionary or how to use it when restoring a savedModel in the documentation. When using checkpoints, I would just load the pickle file before running the session. How do I restore this savedModel so that I can use the same word to int mapping using the dictionary? Is there any specific way I should be saving the model or loading it? I have also added inputs_ as the input for the input signature. This is a sequence of integeres 'after' the words have been mapped. I can't specify a string as input because I get an AttributeError: 'str' object has no attribute 'dtype' . In such cases, how exactly are words mapped to integers in models that are in production?",https://stackoverflow.com/questions/47399201,6021490,Inadequate Examples
47798492,Using Datasets to consume Numpy arrays,"<p>I'm trying to use Numpy arrays within a graph, feeding in the data using a Dataset.</p>

<p>I've read through <a href=""https://www.tensorflow.org/programmers_guide/dataset"" rel=""nofollow noreferrer"">this</a>, but can't quite make sense of how I should feed placeholder arrays within a Dataset.</p>

<p>If we take a simple example, I start with:</p>

<pre><code>A = np.arange(4)
B = np.arange(10, 14)

a = tf.placeholder(tf.float32, [None])
b = tf.placeholder(tf.float32, [None])
c = tf.add(a, b)

with tf.Session() as sess:
    for i in range(10):
        x = sess.run(c, feed_dict={a: A, b:B})
        print(i, x)
</code></pre>

<p>Then I attempt to modify it to use a Dataset as follows:</p>

<pre><code>A = np.arange(4)
B = np.arange(10, 14)

a = tf.placeholder(tf.int32, A.shape)
b = tf.placeholder(tf.int32, B.shape)
c = tf.add(a, b)

dataset = tf.data.Dataset.from_tensors((a, b))

iterator = dataset.make_initializable_iterator()

with tf.Session() as sess3:
    sess3.run(tf.global_variables_initializer())
    sess3.run(iterator.initializer, feed_dict={a: A, b: B})

    for i in range(10):
        x = sess3.run(c)
        print(i, x)
</code></pre>

<p>If I run this I get 'InvalidArgumentError: You must feed a value for placeholder tensor ...'</p>

<p>The code until the for loop mimics the example <a href=""https://www.tensorflow.org/programmers_guide/datasets#consuming_numpy_arrays"" rel=""nofollow noreferrer"">here</a>, but I don't get how I can then employ the placeholders a &amp; b without supplying a feed_dict to every call to sess3.run(c) [which would be expensive]. I suspect I have to somehow use the iterator, but I don't understand how.</p>

<p><strong>Update</strong></p>

<p>It appears I oversimplified too much when picking the example. What I am really trying to do is use Datasets when training a neural network, or similar.</p>

<p>For a more sensible question, how would I go about using Datasets to feed placeholders in the below (though imagine X and Y_true are much longer...). The documentation takes me to the point where the loop starts and then I'm not sure.</p>

<pre><code>X = np.arange(8.).reshape(4, 2)
Y_true = np.array([0, 0, 1, 1])

x = tf.placeholder(tf.float32, [None, 2], name='x')
y_true = tf.placeholder(tf.float32, [None], name='y_true')

w = tf.Variable(np.random.randn(2, 1), name='w', dtype=tf.float32)

y = tf.squeeze(tf.matmul(x, w), name='y')

loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
                                labels=y_true, logits=y),
                                name='x_entropy')

# set optimiser
optimiser = tf.train.AdamOptimizer().minimize(loss)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for i in range(100):
        _, loss_out = sess.run([optimiser, loss], feed_dict={x: X, y_true:Y_true})
        print(i, loss_out)
</code></pre>

<p>Trying the following only gets me a InvalidArgumentError</p>

<pre><code>X = np.arange(8.).reshape(4, 2)
Y_true = np.array([0, 0, 1, 1])

x = tf.placeholder(tf.float32, [None, 2], name='x')
y_true = tf.placeholder(tf.float32, [None], name='y_true')

dataset = tf.data.Dataset.from_tensor_slices((x, y_true))
iterator = dataset.make_initializable_iterator()

w = tf.Variable(np.random.randn(2, 1), name='w', dtype=tf.float32)

y = tf.squeeze(tf.matmul(x, w), name='y')

loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
                                labels=y_true, logits=y),
                                name='x_entropy')

# set optimiser
optimiser = tf.train.AdamOptimizer().minimize(loss)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    sess.run(iterator.initializer, feed_dict={x: X,
                                              y_true: Y_true})

    for i in range(100):
        _, loss_out = sess.run([optimiser, loss])
        print(i, loss_out)
</code></pre>
","I'm trying to use Numpy arrays within a graph, feeding in the data using a Dataset. I've read through this, but can't quite make sense of how I should feed placeholder arrays within a Dataset. If we take a simple example, I start with: Then I attempt to modify it to use a Dataset as follows: If I run this I get 'InvalidArgumentError: You must feed a value for placeholder tensor ...' The code until the for loop mimics the example here, but I don't get how I can then employ the placeholders a &amp; b without supplying a feed_dict to every call to sess3.run(c) [which would be expensive]. I suspect I have to somehow use the iterator, but I don't understand how. Update It appears I oversimplified too much when picking the example. What I am really trying to do is use Datasets when training a neural network, or similar. For a more sensible question, how would I go about using Datasets to feed placeholders in the below (though imagine X and Y_true are much longer...). The documentation takes me to the point where the loop starts and then I'm not sure. Trying the following only gets me a InvalidArgumentError",https://stackoverflow.com/questions/47798492,9094969,Documentation Completeness
48038889,why embedding_lookup only used as encoder but no decoder in ptb_word_ln.py,"<p>I have a question about embedding_lookup while I looking the tensorflow's official sample code ptb_word_ln.py.
<a href=""https://i.stack.imgur.com/jIc00.png"" rel=""nofollow noreferrer"">the embedding_lookup node</a></p>

<p>I found it is only used as an input. the output doesn't use this. so the loss evaluation cannot be benefit from this embedding. so what is the benefit using embedding_lookup here? If I want to use this word-embedding in the optimizer, shouldn't I connect it with loss function explicitly?</p>

<p>the source code as following:</p>

<pre><code>self._input = input_

batch_size = input_.batch_size
num_steps = input_.num_steps
size = config.hidden_size
vocab_size = config.vocab_size

def lstm_cell():
  # With the latest TensorFlow source code (as of Mar 27, 2017),
  # the BasicLSTMCell will need a reuse parameter which is unfortunately not
  # defined in TensorFlow 1.0. To maintain backwards compatibility, we add
  # an argument check here:
  if 'reuse' in inspect.getargspec(
      tf.contrib.rnn.BasicLSTMCell.__init__).args:
    return tf.contrib.rnn.BasicLSTMCell(
        size, forget_bias=0.0, state_is_tuple=True,
        reuse=tf.get_variable_scope().reuse)
  else:
    return tf.contrib.rnn.BasicLSTMCell(
        size, forget_bias=0.0, state_is_tuple=True)
attn_cell = lstm_cell
if is_training and config.keep_prob &lt; 1:
  def attn_cell():
    return tf.contrib.rnn.DropoutWrapper(
        lstm_cell(), output_keep_prob=config.keep_prob)
cell = tf.contrib.rnn.MultiRNNCell(
    [attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)

self._initial_state = cell.zero_state(batch_size, data_type())

with tf.device(""/cpu:0""):
  embedding = tf.get_variable(
      ""embedding"", [vocab_size, size], dtype=data_type())
  inputs = tf.nn.embedding_lookup(embedding, input_.input_data)#only use embeddings here

if is_training and config.keep_prob &lt; 1:
  inputs = tf.nn.dropout(inputs, config.keep_prob)

outputs = []
state = self._initial_state
with tf.variable_scope(""RNN""):
  for time_step in range(num_steps):
    if time_step &gt; 0: tf.get_variable_scope().reuse_variables()
    (cell_output, state) = cell(inputs[:, time_step, :], state)
    outputs.append(cell_output)

output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])
softmax_w = tf.get_variable(
    ""softmax_w"", [size, vocab_size], dtype=data_type())
softmax_b = tf.get_variable(""softmax_b"", [vocab_size], dtype=data_type())
logits = tf.matmul(output, softmax_w) + softmax_b
loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
    [logits],
    [tf.reshape(input_.targets, [-1])],
    [tf.ones([batch_size * num_steps], dtype=data_type())])
self._cost = cost = tf.reduce_sum(loss) / batch_size
self._final_state = state

if not is_training:
  return

self._lr = tf.Variable(0.0, trainable=False)
tvars = tf.trainable_variables()
grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                  config.max_grad_norm)
optimizer = tf.train.GradientDescentOptimizer(self._lr)
self._train_op = optimizer.apply_gradients(
    zip(grads, tvars),
    global_step=tf.contrib.framework.get_or_create_global_step())

self._new_lr = tf.placeholder(
    tf.float32, shape=[], name=""new_learning_rate"")
self._lr_update = tf.assign(self._lr, self._new_lr)
</code></pre>
","I have a question about embedding_lookup while I looking the tensorflow's official sample code ptb_word_ln.py. the embedding_lookup node I found it is only used as an input. the output doesn't use this. so the loss evaluation cannot be benefit from this embedding. so what is the benefit using embedding_lookup here? If I want to use this word-embedding in the optimizer, shouldn't I connect it with loss function explicitly? the source code as following:",https://stackoverflow.com/questions/48038889,3134227,Documentation Ambiguity
48067854,Trouble understanding tensorflow shuffle_batch enqueue_many=False,"<p>I am reading the Tensorflow documentation and the code for the Cifar10 example.  This bit is currently racking my brain:</p>

<pre><code># Creates batches of 32 images and 32 labels.
image_batch, label_batch = tf.train.shuffle_batch(
  [single_image, single_label],
  batch_size=32,
  num_threads=4,
  capacity=50000,
  min_after_dequeue=10000)
</code></pre>

<p>We are passing in a single image, and somehow a batch of images results?? What is going on here?</p>
","I am reading the Tensorflow documentation and the code for the Cifar10 example. This bit is currently racking my brain: We are passing in a single image, and somehow a batch of images results?? What is going on here?",https://stackoverflow.com/questions/48067854,2886575,Documentation Ambiguity
48091693,tensorflow Dataset API diff between make_initializable_iterator and make_one_shot_iterator,"<p>I want to know the difference between <code>make_initializable_iterator</code> and <code>make_one_shot_iterator</code>.<br>
1. Tensorflow documentations said that <code>A ""one-shot"" iterator does not currently support re-initialization.</code> What exactly does that mean?<br>
2.  Are the following 2 snippets equivalent?<br>
Use <code>make_initializable_iterator</code>  </p>

<pre><code>iterator = data_ds.make_initializable_iterator()
data_iter = iterator.get_next()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for e in range(1, epoch+1):
    sess.run(iterator.initializer)
    while True:
        try:
            x_train, y_train = sess.run([data_iter])
            _, cost = sess.run([train_op, loss_op], feed_dict={X: x_train,
                                                               Y: y_train})
        except tf.errors.OutOfRangeError:   
            break
sess.close()
</code></pre>

<p>Use <code>make_one_shot_iterator</code>  </p>

<pre><code>iterator = data_ds.make_one_shot_iterator()
data_iter = iterator.get_next()
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for e in range(1, epoch+1):
    while True:
        try:
            x_train, y_train = sess.run([data_iter])
            _, cost = sess.run([train_op, loss_op], feed_dict={X: x_train,
                                                               Y: y_train})
        except tf.errors.OutOfRangeError:   
            break
sess.close()
</code></pre>
","I want to know the difference between make_initializable_iterator and make_one_shot_iterator. 1. Tensorflow documentations said that A ""one-shot"" iterator does not currently support re-initialization. What exactly does that mean? 2. Are the following 2 snippets equivalent? Use make_initializable_iterator Use make_one_shot_iterator",https://stackoverflow.com/questions/48091693,2641038,Documentation Ambiguity
48092772,Add operation to graph without with-as-clause,"<p>Since you are able to do a </p>

<pre><code>with tf.Session() as sess:
    #  Run stuff here with sess.run()
</code></pre>

<p>but also</p>

<pre><code>init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
sess.run(x)
</code></pre>

<p>I was wondering whether it is possible to do a similar thing with Graph creation, like:</p>

<pre><code>a_graph = tf.Graph()
x = tf.placeholder(dtype=tf.float32, name='test')
a_graph.add(x)
</code></pre>

<p>The conventional way to add a node/operation to a graph is of course...</p>

<pre><code>with a_graph.as_default():
    x = tf.placeholder(dtype=tf.float32, name='test')
</code></pre>

<p>I couldn't read anything about this in the docs.. and <code>dir(a_graph)</code> does not show me a simple <code>.add()</code> method. The only thing I could think of are add some operation to a collection... but I am not sure how to do that.</p>
","Since you are able to do a but also I was wondering whether it is possible to do a similar thing with Graph creation, like: The conventional way to add a node/operation to a graph is of course... I couldn't read anything about this in the docs.. and dir(a_graph) does not show me a simple .add() method. The only thing I could think of are add some operation to a collection... but I am not sure how to do that.",https://stackoverflow.com/questions/48092772,6329284,Inadequate Examples
48225315,Reading data into tensorflow and creating Dataset with TF-slim,"<p>I need to read in many 'images' from .txt files and want to generate a tensorflow dataset with them. Currently, I read in every single matrix with numpy.loadtxt and create an array of shape [N_matrices, height, width, N_channels], and a similar array with the label for every matrix. </p>

<p>I create a tensorflow dataset from these two arrays by using </p>

<pre><code>inputs = tf.convert_to_tensor(x_train, dtype=tf.float32)
labels = tf.convert_to_tensor(y_train, dtype=tf.float32)
dataset = tf.data.Dataset.from_tensor_slices( {""image"": inputs,""label"": labels})
</code></pre>

<p>I now want to make use of the following function to create batches from this dataset (as done <a href=""https://github.com/mnuke/tf-slim-mnist/blob/master/model.py"" rel=""nofollow noreferrer"">here</a>):</p>

<pre><code>def load_batch(dataset, batch_size=BATCH_SIZE, height=LENGTH_INPUT, width=LENGTH_INPUT):

    data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset)

    image, label = data_provider.get(['image', 'label'])

    images, labels = tf.train.batch(
    [image, label],
    batch_size=batch_size,
    allow_smaller_final_batch=True)

    return images, labels 
</code></pre>

<p>However, this gives me the following error:</p>

<blockquote>
<pre><code>data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset)
</code></pre>
  
  <p>File ""/home/.local/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py"", line 85, in <strong>init</strong>
      dataset.data_sources,</p>
  
  <p>AttributeError: 'TensorSliceDataset' object has no attribute 'data_sources'</p>
</blockquote>

<p>Why am I getting this error, and how can I fix it? I also suppose there are much better ways for handling input from txt files to tensorflow (or tensorflow-slim) but I've found very little information on this. How could I generate my Datasets in a better way?</p>
","I need to read in many 'images' from .txt files and want to generate a tensorflow dataset with them. Currently, I read in every single matrix with numpy.loadtxt and create an array of shape [N_matrices, height, width, N_channels], and a similar array with the label for every matrix. I create a tensorflow dataset from these two arrays by using I now want to make use of the following function to create batches from this dataset (as done here): However, this gives me the following error: Why am I getting this error, and how can I fix it? I also suppose there are much better ways for handling input from txt files to tensorflow (or tensorflow-slim) but I've found very little information on this. How could I generate my Datasets in a better way?",https://stackoverflow.com/questions/48225315,9208637,Requesting (Additional) Resources
48266862,Trouble Displaying an Image with Bounding Boxes,"<p>I have some code which is meant to simply draw a bounding box on an image, and save it out as a new image. However, when I run it I get a <code>TypeError: Cannot handle this data type</code> from pillow.</p>

<p>The code is</p>

<pre><code># Tests that the processed data file can be read correctly and has correct bounding boxes

import tensorflow as tf
import numpy as np
from PIL import Image

def read_processed_data(filename, num_show):
    """""" Reads in the processed data file and displays the
        given number of images, along with the bounding boxes.
    """"""
    with open(filename, 'r') as f:
        i = 0

        while i &lt; num_show:
            for line in f:
                filename = line.rstrip()
                next_line = f.readline()
                num_faces = int(next_line.rstrip())
                face_num = 0

                #while face_num &lt; num_faces:
                bb_line = f.readline().rstrip()
                y1, x1, y2, x2 = bb_line.split(',')

                box = [y1, x1, y2, x2]
                box = tf.cast(box, tf.float32)

                return box, filename

with tf.Session() as sess:
    bb, fn = read_processed_data(""processed.txt"", 1)
    image = tf.image.decode_image(tf.read_file(fn))
    img = image.eval()
    print(img.shape)

    img_show = np.asarray(img)
    Image.fromarray(img_show).save(""test_no_bb.jpg"") # Works

    bb_image = tf.image.draw_bounding_boxes(img, bb)
    print(bb_image.shape)
    bb_image = tf.cast(bb_image, tf.uint8)
    bb_img_jpeg = tf.image.encode_jpeg(bb_image)
    bb_image_np = np.asarray(bb_img_jpeg)
    Image.fromarray(bb_image_np).save(""test.jpg"") # Does not work
</code></pre>

<p><code>test_no_bb.jpg</code> gets created fine, but when I reach <code>Image.fromarray(bb_image_np).save(""test.jpg"")</code>, I get the aforementioned type error.</p>

<p>I have searched the web all over to no avail, and TensorFlow's documentation on this is lacking. The shape of the <code>bb_image</code> is correct, and the output of <code>bb</code> (the coordinates of the bounding box) is also correct so I am at a loss.</p>

<p>Any help is greatly appreciated.</p>
","I have some code which is meant to simply draw a bounding box on an image, and save it out as a new image. However, when I run it I get a TypeError: Cannot handle this data type from pillow. The code is test_no_bb.jpg gets created fine, but when I reach Image.fromarray(bb_image_np).save(""test.jpg""), I get the aforementioned type error. I have searched the web all over to no avail, and TensorFlow's documentation on this is lacking. The shape of the bb_image is correct, and the output of bb (the coordinates of the bounding box) is also correct so I am at a loss. Any help is greatly appreciated.",https://stackoverflow.com/questions/48266862,5280140,Documentation Completeness
48283090,Inverted Colours when Writing an Image using TensorFlow and PIL,"<p>I am trying to save images with bounding boxes displayed on them, just to test that my annotations file is working correctly. </p>

<p>Everything works out fine: the image is written to disk, the bounding box is there in the right place and so on. Except all the colours are inverted. So it looks like a negative of the original image.</p>

<p>Here is my code:</p>

<p>```</p>

<pre><code>import tensorflow as tf
import numpy as np
from PIL import Image

def read_processed_data(filename, num_show):
    """""" Reads in the processed data file and displays the
        given number of images, along with the bounding boxes.
    """"""
    with open(filename, 'r') as f:
        i = 0

        while i &lt; num_show:
            for line in f:
                filename = line.rstrip()
                next_line = f.readline()
                num_faces = int(next_line.rstrip())
                face_num = 0

                #while face_num &lt; num_faces:
                bb_line = f.readline().rstrip()
                y1, x1, y2, x2 = bb_line.split(',')
                y1 = float(y1)
                x1 = float(x1)
                y2 = float(y2)
                x2 = float(x2)

                box = [y1, x1, y2, x2]

                return box, filename


with tf.Session() as sess:
    bb, fn = read_processed_data(""processed.txt"", 1)
    image = tf.image.decode_image(tf.read_file(fn))

    image_as_float = tf.cast(image, dtype = tf.float32)
    image_4d = tf.expand_dims(image_as_float, 0)

    bb_2d = tf.expand_dims(bb, 0)
    bb_3d = tf.expand_dims(bb_2d, 0) # Box has to be 3d for the drawing to work
    bb_image = tf.image.draw_bounding_boxes(image_4d, bb_3d)
    bb_image_uint = tf.image.convert_image_dtype(bb_image, dtype = tf.uint8)
    bb_image_uint_3d = tf.reshape(bb_image_uint, [940, 650, 3]) # Reduce rank from 4 to 3
    data = bb_image_uint_3d.eval()

    base_fn = fn.split('.')[0]
    Image.fromarray(data).save(base_fn + ""_bb.jpg"")
</code></pre>

<p>```</p>

<p>I have searched the tensorflow documentation to no avail. I have also attempted <code>np.roll()</code> and the other suggestions from <a href=""https://stackoverflow.com/questions/4661557/pil-rotate-image-colors-bgr-rgb"">PIL rotate image colors (BGR -&gt; RGB)</a> again with no luck; those methods were able to change the colour, but not to the correct colours.</p>

<p><a href=""https://i.stack.imgur.com/N23ia.jpg"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/N23ia.jpg</a> shows the original image (without bounding box) at the top, and the resulting image (with the colour issue, as well as bounding box) below.</p>
","I am trying to save images with bounding boxes displayed on them, just to test that my annotations file is working correctly. Everything works out fine: the image is written to disk, the bounding box is there in the right place and so on. Except all the colours are inverted. So it looks like a negative of the original image. Here is my code: ``` ``` I have searched the tensorflow documentation to no avail. I have also attempted np.roll() and the other suggestions from PIL rotate image colors (BGR -&gt; RGB) again with no luck; those methods were able to change the colour, but not to the correct colours. https://i.stack.imgur.com/N23ia.jpg shows the original image (without bounding box) at the top, and the resulting image (with the colour issue, as well as bounding box) below.",https://stackoverflow.com/questions/48283090,5280140,Inadequate Examples
48335842,Tensorflow: Exogenous feature key raises KeyError on StructuralEnsembleRegressor predict call,"<p>I have a tensorflow implementation for timeseries forecasting. My data contains exogeneous features, I provide them in my train input and evaluate inputs. In the prediction step <code>predict_continuation_input_fn</code> raises KeyError for my exogenous feature column. Here is the simplified version of my code:</p>

<pre class=""lang-py prettyprint-override""><code>features = (ex_0, ex_1, ex_2)
reader = tf.contrib.timeseries.CSVReader(
  _DATA_FILE,
  column_names=(tf.contrib.timeseries.TrainEvalFeatures.TIMES, tf.contrib.timeseries.TrainEvalFeatures.VALUES) + features,
  column_dtypes=(tf.int64,tf.float32,tf.float32,tf.float32,tf.float32),
  skip_header_lines=1)

estimator = tf.contrib.timeseries.StructuralEnsembleRegressor(
  periodicities=[20],
  num_features=1,
  exogenous_feature_columns= [tf.contrib.layers.real_valued_column(column_name=f, dimension=1) for f in features])

train_input_fn=tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=4, window_size=100)
estimator.train(input_fn=train_input_fn, steps=20)

evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)
evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)

predict_input_fn = tf.contrib.timeseries.predict_continuation_input_fn(
      evaluation, steps=100)
(predictions,) = tuple(estimator.predict(input_fn=predict_input_fn))
</code></pre>

<p>At this point I get error <code>KeyError: 'ex_0'</code>. Error is obvious, since resulting <code>evaluation</code> variable does not contain my exogenous features. <code>predict_continuation_input_fn</code> has argument to get exogenous_features however I could not find any documentation on how to feed exogenous data from evaluation to that argument.</p>

<p>How should I provide those features to prediction? Is there a flaw in my implementation? Advises are very welcome.</p>
","I have a tensorflow implementation for timeseries forecasting. My data contains exogeneous features, I provide them in my train input and evaluate inputs. In the prediction step predict_continuation_input_fn raises KeyError for my exogenous feature column. Here is the simplified version of my code: At this point I get error KeyError: 'ex_0'. Error is obvious, since resulting evaluation variable does not contain my exogenous features. predict_continuation_input_fn has argument to get exogenous_features however I could not find any documentation on how to feed exogenous data from evaluation to that argument. How should I provide those features to prediction? Is there a flaw in my implementation? Advises are very welcome.",https://stackoverflow.com/questions/48335842,65071,Lack of Alternative Solutions/Documentation
48354243,"tensorflow estimator from_generator, how to set TensorShape?","<p>I am trying use a generator to feed data into estimator. The following is the code. However, when try to run, I got the following error:</p>

<p>Update2: I finally made it work.   So the correct tensorshape is 
([], [], [])</p>

<p>Update: I added tensorshape ([None], [None], [None]), then I changed ds.batch(10), to an assignment ds = ds.batch(10)  </p>

<p>but still got error.  </p>

<pre><code>Traceback (most recent call last):
  File ""xyz.py"", line 79, in &lt;module&gt;
    tf.app.run(main=main, argv=None)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""xyz.py"", line 67, in main
    model.train(input_fn=lambda: input_fn(100))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 783, in _train_model
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 521, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 892, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 967, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 952, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1024, in run
    run_metadata=run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 827, in run
    return self._sess.run(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: exceptions.ValueError: `generator` yielded an element of shape () where an element of shape (?,) was expected.
         [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64, DT_STRING, DT_FLOAT], token=""pyfunc_1""](arg0)]]
         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?], [?,?], [?,?]], output_types=[DT_INT64, DT_STRING, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
</code></pre>

<p>So my question, how to set the TensorShape? The from generator takes a third argument of TensorShape but I cannot find any example/doc on how to set it. Any help?  </p>

<p>Thanks,</p>

<pre><code>def gen(nn):
    ii = 0
    while ii &lt; nn:
        ii += 1
        yield ii, 't{0}'.format(ii), ii*2

def input_fn(n):
    ds = tf.data.Dataset.from_generator(lambda: gen(n), (tf.int64, tf.string, tf.float32), ([None], [None], [None])) 
    ds = ds.batch(10)
    x, y, z = ds.make_one_shot_iterator().get_next()
    return {'x': x, 'y': y}, tf.greater_equal(z, 10)

def build_columns():
    x = tf.feature_column.numeric_column('x')
    y = tf.feature_column.categorical_column_with_hash_bucket('y', hash_bucket_size=5)
    return [x, y]

def build_estimator():
    run_config = tf.estimator.RunConfig().replace(
            session_config=tf.ConfigProto(device_count={'GPU': 0}))
    return tf.estimator.LinearClassifier(model_dir=FLAGS.model_dir, feature_columns=build_columns(), config=run_config)

def main(unused):
  # Clean up the model directory if present
  shutil.rmtree(FLAGS.model_dir, ignore_errors=True)
  model = build_estimator()

  # Train and evaluate the model every `FLAGS.epochs_per_eval` epochs.
  for n in range(FLAGS.train_epochs // FLAGS.epochs_per_eval):
    model.train(input_fn=lambda: input_fn(100))
    results = model.evaluate(input_fn=lambda: input_fn(20))
</code></pre>
","I am trying use a generator to feed data into estimator. The following is the code. However, when try to run, I got the following error: Update2: I finally made it work. So the correct tensorshape is ([], [], []) Update: I added tensorshape ([None], [None], [None]), then I changed ds.batch(10), to an assignment ds = ds.batch(10) but still got error. So my question, how to set the TensorShape? The from generator takes a third argument of TensorShape but I cannot find any example/doc on how to set it. Any help? Thanks,",https://stackoverflow.com/questions/48354243,9243582,Inadequate Examples
48458509,Strange behavior of tensorflow when dividing by 0 in tf.metrics.mean_absolute_error,"<p>So I do not know if is a bug or the problem is in my code, but I am trying to understand what is happening. 
when I run the model and got to estimate the accuracy using Mean Relative Error. I know in my validation data I have 0s so I was expecting to get some error or some inf. However this is not the case.
This is my code:</p>

<pre><code>X_test_norm = preprocessing.scale(X_test)
predictions = sess.run(pred, feed_dict={x: X_test_norm})
prediction = tf.convert_to_tensor(predictions)
expectation = tf.cast(Y_test, tf.float32)

MANUAL_MRE = tf.reduce_mean(tf.abs((Y_test - tf.transpose(predictions)) / Y_test))
MAE_op, MAE = tf.metrics.mean_absolute_error(expectation, prediction)
MRE_op, MRE = tf.metrics.mean_relative_error(expectation, prediction, expectation)
tf.local_variables_initializer().run()

print(""MANUAL_MRE: %4f"" % sess.run(MANUAL_MRE))
print(""MRE: %4f"" % sess.run(MRE))
print(""MAE: %4f"" % sess.run(MAE))
</code></pre>

<p>This is the output:</p>

<pre><code>MANUAL_MRE:  inf

MRE: 1.603528

MAE: 76.489990
</code></pre>

<p>When I run it on a data that has values bigger than 0, my <code>MANUAL_MRE</code> and MRE values are the same like it should be. I checked the documentation of TF and the first case does not make sense. </p>

<p>Can someone tell me where I am wrong or I just found a bug/ new feature. </p>
","So I do not know if is a bug or the problem is in my code, but I am trying to understand what is happening. when I run the model and got to estimate the accuracy using Mean Relative Error. I know in my validation data I have 0s so I was expecting to get some error or some inf. However this is not the case. This is my code: This is the output: When I run it on a data that has values bigger than 0, my MANUAL_MRE and MRE values are the same like it should be. I checked the documentation of TF and the first case does not make sense. Can someone tell me where I am wrong or I just found a bug/ new feature.",https://stackoverflow.com/questions/48458509,4898951,Documentation Completeness
48531039,Create variable of weights from array,"<p>I have an array : [1, 4, -10, 3, 5]. I'm trying to create a <code>Variable</code> of weights using that array.</p>

<p>After doing training, I print the weight as: </p>

<pre><code>result = sess.run(w)
print(result)
</code></pre>

<p>the <code>result</code> is just the array in the format [1, 4, -10, 3, 5].</p>

<p>How I tried to create a <code>Variable</code> from the array:</p>

<pre><code> c = [1, 4, -10, 3, 5]
 for i in range(len(c)): 
     w = tf.Variable(c[i], name='weights')
</code></pre>

<p>Obviously, this weight result is wrong, and I've spent hours looking at documentation and SO posts. How do I create a <code>Variable</code> using that array?</p>
","I have an array : [1, 4, -10, 3, 5]. I'm trying to create a Variable of weights using that array. After doing training, I print the weight as: the result is just the array in the format [1, 4, -10, 3, 5]. How I tried to create a Variable from the array: Obviously, this weight result is wrong, and I've spent hours looking at documentation and SO posts. How do I create a Variable using that array?",https://stackoverflow.com/questions/48531039,7933838,Requesting (Additional) Resources
48767184,Tensorflow running session multiple times in a loop,"<p>I'm trying out a simple Tensorflow code to compute the product of two matrices multiple times. My code is as follows:</p>

<pre><code>import numpy as np
import tensorflow as tf

times = 10
alpha = 2
beta = 3

graph = tf.Graph()

with graph.as_default():
    A = tf.placeholder(tf.float32)
    B = tf.placeholder(tf.float32)
    C = tf.placeholder(tf.float32)
    alpha = tf.constant(2.0, shape=[1, 1])
    beta = tf.constant(3.0, shape=[1, 1])
    D = alpha*tf.matmul(A, B) + beta*C          

with tf.Session(graph=graph) as session:
    tf.initialize_all_variables().run()
    for time in xrange(1, 2):
        N = 10**time
        a = tf.constant(np.random.random((N, N)))
        b = tf.constant(np.random.random((N, N)))
        c = tf.constant(np.random.random((N, N)))

        for num in xrange(1, 3):
            print num
            session.run(D, feed_dict={A:a.eval(), B:b.eval(), C:c.eval()})      
            c = D
</code></pre>

<p>Upon running session.run() in the for loop:</p>

<pre><code>for num in xrange(1, 3):
    print num
    session.run(D, feed_dict={A:a.eval(), B:b.eval(), C:c.eval()})      
    c = D
</code></pre>

<p>I get the following error:</p>

<p><a href=""https://i.stack.imgur.com/Rh7N5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rh7N5.jpg"" alt=""Error on running session.run() in a loop""></a></p>

<p>I looked at the sample code for MNIST on the Tensorflow website but they run 'session.run()' in a similar manner in a for loop. I'm looking for any insight on why 'session.run()' in my code does not work inside a for loop.</p>

<p>Thank you.</p>
",I'm trying out a simple Tensorflow code to compute the product of two matrices multiple times. My code is as follows: Upon running session.run() in the for loop: I get the following error: I looked at the sample code for MNIST on the Tensorflow website but they run 'session.run()' in a similar manner in a for loop. I'm looking for any insight on why 'session.run()' in my code does not work inside a for loop. Thank you.,https://stackoverflow.com/questions/48767184,5279281,Requesting (Additional) Resources
48947083,Re-train pre-trained ResNet-50 model with tf slim for classification purposes,"<p>I would like to re-train a pre-trained ResNet-50 model with TensorFlow slim, and use it later for classifying purposes. </p>

<p>The ResNet-50 is designed to 1000 classes, but I would like just 10 classes (land cover types) as output.</p>

<p>First, I try to code it for only one image, what I can generalize later.
So this is my code:</p>

<pre><code>from tensorflow.contrib.slim.nets import resnet_v1
import tensorflow as tf
import tensorflow.contrib.slim as slim
import numpy as np

batch_size = 1
height, width, channels = 224, 224, 3
# Create graph
inputs = tf.placeholder(tf.float32, shape=[batch_size, height, width, channels])
with slim.arg_scope(resnet_v1.resnet_arg_scope()):
    logits, end_points = resnet_v1.resnet_v1_50(inputs, is_training=False)

saver = tf.train.Saver()    

with tf.Session() as sess:
    saver.restore(sess, 'd:/bitbucket/cnn-lcm/data/ckpt/resnet_v1_50.ckpt')
    representation_tensor = sess.graph.get_tensor_by_name('resnet_v1_50/pool5:0')
    #  list of files to read
    filename_queue = tf.train.string_input_producer(['d:/bitbucket/cnn-lcm/data/train/AnnualCrop/AnnualCrop_735.jpg']) 
    reader = tf.WholeFileReader()
    key, value = reader.read(filename_queue)
    img = tf.image.decode_jpeg(value, channels=3)    

    im = np.array(img)
    im = im.reshape(1,224,224,3)
    predict_values, logit_values = sess.run([end_points, logits], feed_dict= {inputs: im})
    print (np.max(predict_values), np.max(logit_values))
    print (np.argmax(predict_values), np.argmax(logit_values))

    #img = ...  #load image here with size [1, 224,224, 3]
    #features = sess.run(representation_tensor, {'Placeholder:0': img})
</code></pre>

<p>I am a bit confused about what comes next (I should open a graph, or I should load the structure of the network and load the weights, or load batches. There is a problem with the image shape as well. There are a lot of versatile documentations, which aren't easy to interpret :/</p>

<p>Any advice how to correct the code in order to fit my purposes?</p>

<p>The test image: AnnualCrop735</p>

<p><a href=""https://i.stack.imgur.com/RRd4u.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RRd4u.jpg"" alt=""AnnualCrop735""></a></p>
","I would like to re-train a pre-trained ResNet-50 model with TensorFlow slim, and use it later for classifying purposes. The ResNet-50 is designed to 1000 classes, but I would like just 10 classes (land cover types) as output. First, I try to code it for only one image, what I can generalize later. So this is my code: I am a bit confused about what comes next (I should open a graph, or I should load the structure of the network and load the weights, or load batches. There is a problem with the image shape as well. There are a lot of versatile documentations, which aren't easy to interpret :/ Any advice how to correct the code in order to fit my purposes? The test image: AnnualCrop735",https://stackoverflow.com/questions/48947083,4569591,Requesting (Additional) Resources
49017930,Errors implementing Sampled Softmax Tensorflow,"<p>I have been trying for a while to implement sampled softmax because I have half a million output classes.</p>

<p>I have tried to follow the official documentation exactly, but I always get an error. This is my code:</p>

<pre><code>def forward_propagation_sampled(X, parameters):

W1 = parameters['W1']
b1 = parameters['b1']
W2 = parameters['W2']
b2 = parameters['b2']
W3 = parameters['W3']
b3 = parameters['b3']


Z1 = tf.add(tf.matmul(W1, X), b1)
A1 = tf.nn.relu(Z1)
Z2 = tf.add(tf.matmul(W2,A1), b2)
A2 = tf.nn.relu(Z2)
Z3 = tf.add(tf.matmul(W3,A2), b3)


return Z3, W3, b3
</code></pre>

<p>This is the cost computation function:</p>

<pre><code>def compute_cost(Z3, W3, b3, Y, mode):
Z3.set_shape([1144,1])
if mode == ""train"":
    loss = tf.nn.sampled_softmax_loss(
    weights=tf.transpose(W3),
    biases=tf.Variable(b3),
    labels = tf.reshape(tf.argmax(Y, 1), [-1,1]), #Since Y is one hot encoded
    inputs=tf.Variable(initial_value=Z3,dtype=tf.float32, expected_shape=[1144,1]),
    num_sampled = 2000,
    num_classes = 1144,
    partition_strategy=""div""
    )

elif mode == ""eval"":
    logits = tf.matmul(inputs, tf.transpose(weights))
    logits = tf.nn.bias_add(logits, biases)
    labels_one_hot = tf.one_hot(labels, n_classes)
    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot,logits=logits)
cost = tf.reduce_mean(loss)
return cost
</code></pre>

<p>For the purpose of just testing this out, I am using 1144 output classes, which would otherwise scale to 500,000. There are 3144 training examples.</p>

<p>I get this error:</p>

<pre><code>Shape must be rank 1 but is rank 2 for 'sampled_softmax_loss/Slice_1' (op: 'Slice') with input shapes: [3144,1], [1], [1].
</code></pre>

<p>I am unable to debug this or make any sense out of it. Any help would be really appreciated.</p>
","I have been trying for a while to implement sampled softmax because I have half a million output classes. I have tried to follow the official documentation exactly, but I always get an error. This is my code: This is the cost computation function: For the purpose of just testing this out, I am using 1144 output classes, which would otherwise scale to 500,000. There are 3144 training examples. I get this error: I am unable to debug this or make any sense out of it. Any help would be really appreciated.",https://stackoverflow.com/questions/49017930,5054785,Documentation Replicability
49150587,"Distributed TensorFlow [Async, Between-Graph Replication]: which are the exactly interaction between workers and servers regarding Variables update","<p>I've read <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">Distributed TensorFlow Doc</a> and <a href=""https://stackoverflow.com/q/43147435/9099269"">this question on StackOverflow</a> but I still have some doubt about the dynamics behind the distributed training that can be done with TensorFlow and its Parameter Server Architecture.
This is a snipped of code from the Distributed TensorFlow Doc:</p>
<pre><code>if FLAGS.job_name == &quot;ps&quot;:
    server.join()
  elif FLAGS.job_name == &quot;worker&quot;:

    # Assigns ops to the local worker by default.
    with tf.device(tf.train.replica_device_setter(
        worker_device=&quot;/job:worker/task:%d&quot; % FLAGS.task_index,
        cluster=cluster)):

      # Build model...
      loss = ...
      global_step = tf.contrib.framework.get_or_create_global_step()

      train_op = tf.train.AdagradOptimizer(0.01).minimize(
          loss, global_step=global_step)
</code></pre>
<p>And here part of the answer of the StackOverflow question that I read:</p>
<blockquote>
<p>The worker reads all of the shared model parameters in parallel from
the PS task(s), and copies them to the worker task. These reads are
uncoordinated with any concurrent writes, and no locks are acquired:
in particular the worker may see partial updates from one or more
other workers (e.g. a subset of the updates from another worker may
have been applied, or a subset of the elements in a variable may have
been updated).</p>
<p>The worker computes gradients locally, based on a batch
of input data and the parameter values that it read in step 1.</p>
<p>The
worker sends the gradients for each variable to the appropriate PS
task, and applies the gradients to their respective variable, using an
update rule that is determined by the optimization algorithm (e.g.
SGD, SGD with Momentum, Adagrad, Adam, etc.). The update rules
typically use (approximately) commutative operations, so they may be
applied independently on the updates from each worker, and the state
of each variable will be a running aggregate of the sequence of
updates received.</p>
</blockquote>
<p>I have to reproduce this kind of parameter server architecture in another environment and I need to deeply understand how workers and PS tasks interact with each other inside the TensorFlow framework.
My question is, does the PS task do some kind of merging or updating operation after receiving the value from the workers or it just store the newest value ? Can be something reasonable just storing the newest value ? Looking at the code from the TensorFlow documentation I see that the PS task just do a join() and I wonder behind this method call which are the complete behaviour of the PS task.</p>
<p>One more question, what is the difference between compute a gradient and apply a gradient ?</p>
","I've read Distributed TensorFlow Doc and this question on StackOverflow but I still have some doubt about the dynamics behind the distributed training that can be done with TensorFlow and its Parameter Server Architecture. This is a snipped of code from the Distributed TensorFlow Doc: And here part of the answer of the StackOverflow question that I read: I have to reproduce this kind of parameter server architecture in another environment and I need to deeply understand how workers and PS tasks interact with each other inside the TensorFlow framework. My question is, does the PS task do some kind of merging or updating operation after receiving the value from the workers or it just store the newest value ? Can be something reasonable just storing the newest value ? Looking at the code from the TensorFlow documentation I see that the PS task just do a join() and I wonder behind this method call which are the complete behaviour of the PS task. One more question, what is the difference between compute a gradient and apply a gradient ?",https://stackoverflow.com/questions/49150587,9099269,Requesting (Additional) Resources
49178891,Filtering a Tensor using Tensorflow,"<p>I'm attempting to filter a matrix that represents a point cloud in tensorflow. It is an <code>n x 3</code> matrix.</p>

<p>I only want to keep rows with <code>z &gt; eps</code>. This corresponds to column index 2 of the matrix.</p>

<p>I have the following code:</p>

<pre>
<code>
import numpy as np
import tensorflow as tf

point_cloud = tf.placeholder(tf.float32, shape=[None,3])
eps = tf.placeholder(tf.float32)

mask = tf.greater(point_cloud[:,2], eps)
reduced_cloud = tf.boolean_mask(point_cloud, mask)


init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    _cloud = np.random.rand(5000,3)
    feed = {point_cloud:_cloud, eps:0.0025}
    _filtered = sess.run(reduced_cloud, feed_dict=feed)

</code>
</pre> 

<p>When I run the above code I get this:</p>

<pre>
<code>
ValueError: Number of mask dimensions must be specified, even if some dimensions are None.  E.g. shape=[None] is ok, but shape=None is not.
</code>
</pre>

<p>I don't understand the error message, having tried to specify shape in a number of places with no success, and the documentation seems to suggest the <code>boolean_mask</code> only works with <code>np.array</code>s. Is there any way to do this entirely on the tensorflow graph?  </p>
","I'm attempting to filter a matrix that represents a point cloud in tensorflow. It is an n x 3 matrix. I only want to keep rows with z &gt; eps. This corresponds to column index 2 of the matrix. I have the following code: When I run the above code I get this: I don't understand the error message, having tried to specify shape in a number of places with no success, and the documentation seems to suggest the boolean_mask only works with np.arrays. Is there any way to do this entirely on the tensorflow graph?",https://stackoverflow.com/questions/49178891,1747088,Documentation Ambiguity
49482390,"Tensorflow, replacing feed_dict with Dataset.from_generator","<p>I have an existing model which reads through a text file in a loop, the resulting input and output looks like this:</p>

<pre><code>    self.X = tf.placeholder('float32', shape=[None, None, max_word_length, ALPHABET_SIZE], name='X')
    self.Y = tf.placeholder('float32', shape=[None, 2], name='Y')
    ...
    _, c, a = sess.run([optimizer, cost, acc], feed_dict={self.X: batch_x, self.Y: batch_y})
</code></pre>

<p>But now i want to convert to using the Dataset.from_generator method, to get started i created a wrapper class around my text reader that implemted the generator function, this all works well, and returns the input data as expected:</p>

<pre><code>    dsr = DatasetReader(TRAIN_SET, BATCH_SIZE, max_word_length)
    ds = tf.data.Dataset.from_generator(dsr.generator, (tf.float32, tf.float32))
    ds = ds.prefetch(2)
    dsi = ds.make_one_shot_iterator()
    self.X, self.Y = dsi.get_next()
    _, c, a = sess.run([optimizer, cost, acc])
</code></pre>

<p>However i am getting an error</p>

<pre><code>InvalidArgumentError: You must feed a value for placeholder tensor 'X' with dtype float and shape [?,?,16,70]
</code></pre>

<p>And i assume this is because i have declared X/Y inputs as placeholders, the documentation states the values must be fed via feed_dict.</p>

<p>So i got a couple of questions:</p>

<ol>
<li><p>How do i convert from feed_dict and placeholders to using the from_generator correctly? I want to keep the naming of the tensors X and Y so i am able to feed them in by that name during inference</p></li>
<li><p>More generally, I dont see how the dataset and its iterator is linked to the session, is it linked purely by virtue of the iterator outputs being used as inputs to other operations in the graph?</p></li>
</ol>
","I have an existing model which reads through a text file in a loop, the resulting input and output looks like this: But now i want to convert to using the Dataset.from_generator method, to get started i created a wrapper class around my text reader that implemted the generator function, this all works well, and returns the input data as expected: However i am getting an error And i assume this is because i have declared X/Y inputs as placeholders, the documentation states the values must be fed via feed_dict. So i got a couple of questions:",https://stackoverflow.com/questions/49482390,1371314,Requesting (Additional) Resources
49560420,Deep Learning implementation in Tensorflow or Keras give drastic different results,"<p><em><strong>Context:</strong> I'm using a fully convolutional network to perform image segmentation. Typically, the input is an RGB image <code>shape = [512, 256]</code> and the target is a 2 channels binary mask defining the annotated regions (2nd channel is the opposite of the fist channel).</em></p>
<p><strong>Question:</strong> I have the same CNN implementation using Tensorflow and Keras. But the Tensorflow model doesn't start learning. Actually, the <code>loss</code> even grows with the number of epochs! What is wrong in this Tensorflow implementation that prevents it from learning?</p>
<p><strong>Setup:</strong> The dataset is split into 3 subsets: training (78%), testing (8%) and validation (14%) sets which are fed to the network by batches of 8 images. The graphs show the evolution of the <code>loss</code> for each subsets. The images show the <code>prediction</code> after 10 epoch for two different images.</p>
<hr />
<p><strong>Tensorflow</strong> implementation and results</p>
<pre><code>import tensorflow as tf

tf.reset_default_graph()
x = inputs = tf.placeholder(tf.float32, shape=[None, shape[1], shape[0], 3])
targets = tf.placeholder(tf.float32, shape=[None, shape[1], shape[0], 2])

for d in range(4):
    x = tf.layers.conv2d(x, filters=np.exp2(d+4), kernel_size=[3,3], strides=[1,1], padding=&quot;SAME&quot;, activation=tf.nn.relu)
    x = tf.layers.max_pooling2d(x, strides=[2,2], pool_size=[2,2], padding=&quot;SAME&quot;)
    
x = tf.layers.conv2d(x, filters=2, kernel_size=[1,1])
logits = tf.image.resize_images(x, [shape[1], shape[0]], align_corners=True)
prediction = tf.nn.softmax(logits)

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=logits))
optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

def run(mode, x_batch, y_batch):
    if mode == 'TRAIN':
        return sess.run([loss, optimizer], feed_dict={inputs: x_batch, targets: y_batch})
    else:
        return sess.run([loss, prediction], feed_dict={inputs: x_batch, targets: y_batch})
</code></pre>
<p><a href=""https://i.stack.imgur.com/cTbWl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cTbWl.png"" alt=""Tensorflow loss evolution"" /></a>
<a href=""https://i.stack.imgur.com/6Gbgm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Gbgm.png"" alt=""Tensorflow prediction after 10 epochs"" /></a></p>
<hr />
<p><strong>Keras</strong> implementation and reslults</p>
<pre><code>import keras as ke

ke.backend.clear_session()
x = inputs = ke.layers.Input(shape=[shape[1], shape[0], 3])

for d in range(4):
    x = ke.layers.Conv2D(int(np.exp2(d+4)), [3,3], padding=&quot;SAME&quot;, activation=&quot;relu&quot;)(x)
    x = ke.layers.MaxPool2D(padding=&quot;SAME&quot;)(x)

x = ke.layers.Conv2D(2, [1,1], padding=&quot;SAME&quot;)(x)
logits = ke.layers.Lambda(lambda x: ke.backend.tf.image.resize_images(x, [shape[1], shape[0]], align_corners=True))(x)
prediction = ke.layers.Activation('softmax')(logits)

model = ke.models.Model(inputs=inputs, outputs=prediction)
model.compile(optimizer=&quot;rmsprop&quot;, loss=&quot;categorical_crossentropy&quot;)

def run(mode, x_batch, y_batch):
    if mode == 'TRAIN':
        loss = model.train_on_batch(x=x_batch, y=y_batch)
        return loss, None
    else:
        loss = model.evaluate(x=x_batch, y=y_batch, batch_size=None, verbose=0)
        prediction = model.predict(x=x_batch, batch_size=None)
        return loss, prediction
</code></pre>
<p><a href=""https://i.stack.imgur.com/OXMFd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OXMFd.png"" alt=""Keras loss evolution"" /></a>
<a href=""https://i.stack.imgur.com/O0f8E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O0f8E.png"" alt=""Keras prediction after 10 epochs"" /></a></p>
<hr />
<p>There must be a difference between the two but my understanding of the documentation lead me nowhere. I would be really interested to know where the difference lies. Thanks in advance!</p>
","Context: I'm using a fully convolutional network to perform image segmentation. Typically, the input is an RGB image shape = [512, 256] and the target is a 2 channels binary mask defining the annotated regions (2nd channel is the opposite of the fist channel). Question: I have the same CNN implementation using Tensorflow and Keras. But the Tensorflow model doesn't start learning. Actually, the loss even grows with the number of epochs! What is wrong in this Tensorflow implementation that prevents it from learning? Setup: The dataset is split into 3 subsets: training (78%), testing (8%) and validation (14%) sets which are fed to the network by batches of 8 images. The graphs show the evolution of the loss for each subsets. The images show the prediction after 10 epoch for two different images. Tensorflow implementation and results Keras implementation and reslults There must be a difference between the two but my understanding of the documentation lead me nowhere. I would be really interested to know where the difference lies. Thanks in advance!",https://stackoverflow.com/questions/49560420,1782553,Documentation Ambiguity
49607071,use of updated weights in tensorflow,"<p>I have tried a sample example in tensorflow. My question is when I run y_pred with the same initial features x, does it use weights updated in the preceding for loop or it just uses the initialized weights. </p>

<pre><code>#linear regression for y = -(x-1)
x = tf.placeholder(dtype=tf.float32,shape=(None,1))
y_true = tf.placeholder(dtype=tf.float32,shape=(None,1))

linear_model = tf.layers.Dense(units=1)
y_pred = linear_model(x)

sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)

loss = tf.losses.mean_squared_error(labels = y_true,predictions=y_pred)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(loss)
for i in range(1000):
    _,loss_ = sess.run((train,loss),{x:[[0],[1],[2],[3]],y_true:[[1],[0],[-1],[-2]]})

print(sess.run(y_pred,{x:[[0],[1],[2],[3]]}))
</code></pre>

<p>My confusion arises from the documentation when it says that, to calculate the output of an operation it backtracks. So, to calculate y_pred, it backtracks and initialize the weights and calculate y_pred using x? or does it use already updated weights of the Dense layer?</p>

<p>The output of the above code is:</p>

<pre><code>[[ 0.9960759 ]
 [-0.00208616]
 [-1.0002482 ]
 [-1.9984105 ]]
</code></pre>
","I have tried a sample example in tensorflow. My question is when I run y_pred with the same initial features x, does it use weights updated in the preceding for loop or it just uses the initialized weights. My confusion arises from the documentation when it says that, to calculate the output of an operation it backtracks. So, to calculate y_pred, it backtracks and initialize the weights and calculate y_pred using x? or does it use already updated weights of the Dense layer? The output of the above code is:",https://stackoverflow.com/questions/49607071,4985049,Documentation Ambiguity
49775557,"How can I invoke a SageMaker model, trained with TensorFlow, using a csv file in the body of the call?","<p>I have deployed a TensorFlow model on AWS SageMaker, and I want to be able to invoke it using a csv file as the body of the call. The documentation says about creating a <code>serving_input_function</code> like the one below: </p>

<pre><code>def serving_input_fn(hyperparameters):
  # Logic to the following:
  # 1. Defines placeholders that TensorFlow serving will feed with inference requests
  # 2. Preprocess input data
  # 3. Returns a tf.estimator.export.ServingInputReceiver or tf.estimator.export.TensorServingInputReceiver,
  # which packages the placeholders and the resulting feature Tensors together.
</code></pre>

<p>In step 2, where it says preprocess input data, how do I get a handle on input data to process them?</p>
","I have deployed a TensorFlow model on AWS SageMaker, and I want to be able to invoke it using a csv file as the body of the call. The documentation says about creating a serving_input_function like the one below: In step 2, where it says preprocess input data, how do I get a handle on input data to process them?",https://stackoverflow.com/questions/49775557,4537553,Documentation Ambiguity
49785605,How does tensorflow connect the dimensions of linked convolutional layers?,"<p>The is a very basic tensorflow question, but I haven't yet seen a clear explanation in the docs.  Following the <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""nofollow noreferrer"">examples on the tensorflow site</a>, we basically have these two layers connected:</p>

<pre><code>conv1 = tf.layers.conv2d(
    inputs=input_layer,
    filters=32,
    kernel_size=[5, 5],
    padding=""same"",
    activation=tf.nn.relu)
</code></pre>

<p>The shape at this point will be <code>(28, 28, 32)</code>. </p>

<pre><code>conv2 = tf.layers.conv2d(
    inputs=conv1,
    filters=64,
    kernel_size=[5, 5],
    padding=""same"",
    activation=tf.nn.relu)
</code></pre>

<p>The shape at this point will be <code>(28, 28, 64)</code>.  How does tensorflow take the <code>(28, 28, 32)</code> and turn it into <code>(28, 28, 64)</code> using a 2d kernel.  Could you please explain or point me to the documentation?  How about when the output dimension of the second layer is smaller, say </p>

<pre><code>conv2 = tf.layers.conv2d(
    inputs=conv1,
    filters=8,
    kernel_size=[5, 5],
    padding=""same"",
    activation=tf.nn.relu)
</code></pre>

<p>How would tensorflow combine the 32 dimensions into 8?  </p>
","The is a very basic tensorflow question, but I haven't yet seen a clear explanation in the docs. Following the examples on the tensorflow site, we basically have these two layers connected: The shape at this point will be (28, 28, 32). The shape at this point will be (28, 28, 64). How does tensorflow take the (28, 28, 32) and turn it into (28, 28, 64) using a 2d kernel. Could you please explain or point me to the documentation? How about when the output dimension of the second layer is smaller, say How would tensorflow combine the 32 dimensions into 8?",https://stackoverflow.com/questions/49785605,1759909,Documentation Completeness
49865446,Understanding next step after saving TensorFlow model,"<p>I've a simple MNIST which I've successfully saved, being the code the next:</p>

<pre><code>from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
import tensorflow as tf

sess = tf.InteractiveSession()
tf_save_file = './mnist-to-save-saved'
x = tf.placeholder(tf.float32, shape=[None, 784])
y_ = tf.placeholder(tf.float32, shape=[None, 10])
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
saver = tf.train.Saver()

sess.run(tf.global_variables_initializer())

y = tf.matmul(x, W) + b
cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y))

train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
saver.save(sess, tf_save_file)

for _ in range(1000):
    batch = mnist.train.next_batch(100)
    train_step.run(feed_dict={x: batch[0], y_: batch[1]})

correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
saver.save(sess, tf_save_file, global_step=1000)

print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
</code></pre>

<p>Then, the next <strong>files</strong> are <strong>generated</strong>:</p>

<pre><code>checkpoint
mnist-to-save-saved-1000.data-00000-of-00001
mnist-to-save-saved-1000.index
mnist-to-save-saved-1000.meta
mnist-to-save-saved.data-00000-of-00001
mnist-to-save-saved.index
mnist-to-save-saved.meta
</code></pre>

<p>Now, in order to use it in production (and so, for example, pass it a number image), I want to be able to <strong>execute the trained model</strong> by <strong>passing</strong> it any <strong>number image to make the prediction</strong> (I mean, not deploying yet a server but making this prediction ""<strong>locally</strong>"", having in the same directory that ""fixed"" number image, so using the model would be like when you run an executable).</p>

<p>But, considering the (mid-low?) API level of my code, I'm confused about what would be the easiest correct next step (if restoring, using an Estimator, etc...), and how to do it.</p>

<p>Although I've read the official documentation, I insist that they seem to be many ways, but some are a bit complex and ""noisy"" for a simple model like this.</p>

<p><strong>Edit:</strong></p>

<p>I've edit and re-run the mnist file, whose code is the same as above except for those lines:</p>

<pre><code>...

x = tf.placeholder(tf.float32, shape=[None, 784], name='input')

...

correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1), name='result')

...
</code></pre>

<p>Then, I try to run this another .py code (in the same directory as the above code) in order to pass a local handwritten number image (""mnist-input-image.png"") located in the same directory:</p>

<pre><code>import tensorflow as tf
from PIL import Image
import numpy as np

image_test = Image.open(""mnist-input-image.png"")
image = np.array(image_test)

with tf.Session() as sess:
    saver = tf.train.import_meta_graph('/Users/username/.meta')
    new = saver.restore(sess, tf.train.latest_checkpoint('/Users/username/'))

    graph = tf.get_default_graph()
    input_x = graph.get_tensor_by_name(""input:0"")
    result = graph.get_tensor_by_name(""result:0"")

    feed_dict = {input_x: image}

    predictions = result.eval(feed_dict=feed_dict)
    print(predictions)
</code></pre>

<p>Now, if I correctly understand,  I've to pass the image as numpy array. Then, my questions are:</p>

<p>1) Which is the exact file reference of those lines (since I've no .meta folder in my User folder)?</p>

<pre><code>saver = tf.train.import_meta_graph('/Users/username/.meta')
new = saver.restore(sess, tf.train.latest_checkpoint('/Users/username/'))
</code></pre>

<p>I mean, to which exact files refer those lines (from my generated files list above)?</p>

<p>2) Translasted to my case, is correct this line to pass my numpy array into the feed dict?</p>

<pre><code>feed_dict = {input_x: image}
</code></pre>
","I've a simple MNIST which I've successfully saved, being the code the next: Then, the next files are generated: Now, in order to use it in production (and so, for example, pass it a number image), I want to be able to execute the trained model by passing it any number image to make the prediction (I mean, not deploying yet a server but making this prediction ""locally"", having in the same directory that ""fixed"" number image, so using the model would be like when you run an executable). But, considering the (mid-low?) API level of my code, I'm confused about what would be the easiest correct next step (if restoring, using an Estimator, etc...), and how to do it. Although I've read the official documentation, I insist that they seem to be many ways, but some are a bit complex and ""noisy"" for a simple model like this. Edit: I've edit and re-run the mnist file, whose code is the same as above except for those lines: Then, I try to run this another .py code (in the same directory as the above code) in order to pass a local handwritten number image (""mnist-input-image.png"") located in the same directory: Now, if I correctly understand, I've to pass the image as numpy array. Then, my questions are: 1) Which is the exact file reference of those lines (since I've no .meta folder in my User folder)? I mean, to which exact files refer those lines (from my generated files list above)? 2) Translasted to my case, is correct this line to pass my numpy array into the feed dict?",https://stackoverflow.com/questions/49865446,9499989,Lack of Alternative Solutions/Documentation
49944223,Tensorflow Hub Image Modules: Clarity on Preprocessing and Output values,"<p>Many thanks for support!
I currently use TF Slim - and TF Hub seems like a very useful addition for transfer learning. However the following things are not clear from the documentation:<BR><BR>
<strong>1. Is preprocessing done implicitly?</strong> Is this based on ""trainable=True/False"" parameter in constructor of module?</p>

<pre><code>module = hub.Module(""https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1"", trainable=True)
</code></pre>

<p>When I use Tf-slim I use the preprocess method:<BR></p>

<pre><code>inception_preprocessing.preprocess_image(image, img_height, img_width, is_training)
</code></pre>

<p><strong>2.How to get access to AuxLogits for an inception model?</strong> Seems to be missing:</p>

<pre><code>import tensorflow_hub as hub
import tensorflow as tf

img = tf.random_uniform([10,299,299,3])
module = hub.Module(""https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1"", trainable=True)
outputs = module(dict(images=img), signature=""image_feature_vector"", as_dict=True)
</code></pre>

<p>The output is</p>

<pre><code>dict_keys(['InceptionV3/Mixed_6b', 'InceptionV3/MaxPool_5a_3x3', 'InceptionV3/Mixed_6c', 'InceptionV3/Mixed_6d', 'InceptionV3/Mixed_6e', 'InceptionV3/Mixed_7a', 'InceptionV3/Mixed_7b', 'InceptionV3/Conv2d_2a_3x3', 'InceptionV3/Mixed_7c', 'InceptionV3/Conv2d_4a_3x3', 'InceptionV3/Conv2d_1a_3x3', 'InceptionV3/global_pool', 'InceptionV3/MaxPool_3a_3x3', 'InceptionV3/Conv2d_2b_3x3', 'InceptionV3/Conv2d_3b_1x1', 'default', 'InceptionV3/Mixed_5b', 'InceptionV3/Mixed_5c', 'InceptionV3/Mixed_5d', 'InceptionV3/Mixed_6a'])
</code></pre>
","Many thanks for support! I currently use TF Slim - and TF Hub seems like a very useful addition for transfer learning. However the following things are not clear from the documentation: 1. Is preprocessing done implicitly? Is this based on ""trainable=True/False"" parameter in constructor of module? When I use Tf-slim I use the preprocess method: 2.How to get access to AuxLogits for an inception model? Seems to be missing: The output is",https://stackoverflow.com/questions/49944223,6147954,Documentation Completeness
50066313,Tensorflow - 2D convolution with mutliple channels,"<p>I am defining my input and my kernels in this way</p>

<pre><code>import numpy as np
k = np.array([[
    [1, 0, 1],
    [2, 1, 0],
    [0, 0, 1]
],[
    [1, 0, 1],
    [2, 1, 0],
    [0, 0, 1]
]
], dtype=np.float32)
i = np.array([
    [4, 3, 1, 0],
    [2, 1, 0, 1],
    [1, 2, 4, 1],
    [3, 1, 0, 2]
], dtype=np.float32)
</code></pre>

<p>And convolve the two using</p>

<pre><code>import tensorflow as tf
kernel = tf.reshape(k, [3, 3, 1, 2], name='kernel')
image  = tf.reshape(i, [1, 4, 4, 1], name='image')
res = tf.squeeze(tf.nn.conv2d(image, kernel, [1, 1, 1, 1], ""VALID""))
with tf.Session() as sess:
   print sess.run(res)
</code></pre>

<p>Yielding a result of </p>

<pre><code>[[[11. 12.]
  [ 8.  6.]]

 [[11. 11.]
  [ 8.  8.]]]
</code></pre>

<p>What I want to do is to perform one convolution with one ""subfilter""</p>

<pre><code>[
[1, 0, 1],
[2, 1, 0],
[0, 0, 1]
]
</code></pre>

<p>over the input at the time. Doing it myself with pen and paper, I get</p>

<pre><code>[[[14.  6.]
  [ 6. 12.]]

 [[14.  6.]
  [ 6. 12.]]]
</code></pre>

<p>All other permutations of the ""reshape-parameters"" yield errors and I cannot find what I am doing wrong in the TF documentation. Does anyone know what I am doing wrong?</p>
","I am defining my input and my kernels in this way And convolve the two using Yielding a result of What I want to do is to perform one convolution with one ""subfilter"" over the input at the time. Doing it myself with pen and paper, I get All other permutations of the ""reshape-parameters"" yield errors and I cannot find what I am doing wrong in the TF documentation. Does anyone know what I am doing wrong?",https://stackoverflow.com/questions/50066313,4308982,Documentation Replication on Other Examples
50421555,Argmax function of Tensorflow does not print value when evaluated on a constant tensor,"<p>I'm new at Tensorflow. I am having a litte trouble at understanding its constants. I have this simple code mentioned below:</p>

<pre><code>import tensorflow as tf
vector = tf.constant([[1,2,3,4],[4,5,6,7],[8,9,1,2]],tf.int32,name=""vector"")
with tf.Session() as sess:
    v = sess.run(vector)
    argm = tf.argmax(v,1)
    print(argm)
</code></pre>

<p>I expect this to return something like <code>[4,7,8]</code>, as I understood from the documentation. Instead, I get this: </p>

<pre><code> Tensor(""ArgMax:0"", shape=(3,), dtype=int64). 
</code></pre>

<p>So, i don't know what am I doing wrong.</p>
","I'm new at Tensorflow. I am having a litte trouble at understanding its constants. I have this simple code mentioned below: I expect this to return something like [4,7,8], as I understood from the documentation. Instead, I get this: So, i don't know what am I doing wrong.",https://stackoverflow.com/questions/50421555,7070099,Documentation Replication on Other Examples
50600661,Tensorflow TFRecordDataset.map Error,"<p>I am making an input pipeline in tensorflow for a task I want to do. I have set up a TFRecord dataset which has been saved out to a file on disk. </p>

<p>I am trying to load in the dataset (to be batched and sent to the actual ML algorithm) using the following code:</p>

<pre><code>dataset = tf.data.TFRecordDataset(filename)

print(""Starting mapping..."")

dataset = dataset.map(map_func = read_single_record)
print(""Mapping complete"")

buffer = 500 # How large of a buffer will we sample from?
batch_size = 125
capacity = buffer + 2 * batch_size

print(""Shuffling dataset..."")
dataset = dataset.shuffle(buffer_size = buffer)
print(""Batching dataset..."")
dataset = dataset.batch(batch_size)
dataset = dataset.repeat()

print(""Creating iterator..."")
iterator = dataset.make_one_shot_iterator()
examples_batch, labels_batch = iterator.get_next()
</code></pre>

<p>However, I get an error on the dataset.map() line. The error I get looks like this: <code>TypeError: Expected int64, got &lt;tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x00000000085F74A8&gt; of type 'SparseTensor' instead.</code></p>

<p>The <code>read_single_record()</code> function looks like this:</p>

<pre><code>keys_to_features = {
                ""image/pixels"": tf.FixedLenFeature([], tf.string, default_value = """"),
                ""image/label/class"": tf.FixedLenFeature([], tf.int64, default_value = 0),
                ""image/label/numbb"": tf.FixedLenFeature([], tf.int64, default_value = 0),
                ""image/label/by"": tf.VarLenFeature(tf.float32),
                ""image/label/bx"": tf.VarLenFeature(tf.float32),
                ""image/label/bh"": tf.VarLenFeature(tf.float32),
                ""image/label/bw"": tf.VarLenFeature(tf.float32)
            }

features = tf.parse_single_example(record, keys_to_features)

image_pixels = tf.image.decode_image(features[""image/pixels""])
print(""Features: {0}"".format(features))

example = image_pixels  # May want to do some processing on this at some point

label = [features[""image/label/class""],
        features[""image/label/numbb""],
        features[""image/label/by""],
        features[""image/label/bx""],
        features[""image/label/bh""],
        features[""image/label/bw""]]

return example, label
</code></pre>

<p>I'm not sure where the issue lies. I got the idea for this code from the tensorflow API documentation, slightly modified for my purposes. I really have no idea where to start trying to fix this.</p>

<p>For reference, here is the code I have for generating the TFRecord file:</p>

<pre><code>def parse_annotations(in_file, img_filename, cell_width, cell_height):
    """""" Parses the annotations file to obtain the bounding boxes for a single image
    """"""
    y_mins = []
    x_mins = []
    heights = []
    widths = []
    grids_x = []
    grids_y = []
    classes = [0]

    num_faces = int(in_file.readline().rstrip())

    img_width, img_height = get_image_dims(img_filename)

    for i in range(num_faces):
        clss,  x, y, width, height = in_file.readline().rstrip().split(',')

        x = float(x)
        y = float(y)
        width = float(width)
        height = float(height)

        x = x - (width / 2.0)
        y = y - (height / 2.0)

        y_mins.append(y)
        x_mins.append(x)
        heights.append(height)
        widths.append(width)

        grid_x, grid_y = get_grid_loc(x, y, width, height, img_width, img_height, cell_width, cell_height)

    pixels = get_image_pixels(img_filename)

    example = tf.train.Example(features = tf.train.Features(feature = {
        ""image/pixels"": bytes_feature(pixels),
        ""image/label/class"": int_list_feature(classes),
        ""image/label/numbb"": int_list_feature([num_faces]),
        ""image/label/by"": float_list_feature(y_mins), 
        ""image/label/bx"": float_list_feature(x_mins), 
        ""image/label/bh"": float_list_feature(heights), 
        ""image/label/bw"": float_list_feature(widths)
    }))

    return example, num_faces

if len(sys.argv) &lt; 4:
    print(""Usage: python convert_to_tfrecord.py [path to processed annotations file] [path to training output file] [path to validation output file] [training fraction]"")

else:
    processed_fn = sys.argv[1]
    train_fn = sys.argv[2]
    valid_fn = sys.argv[3]
    train_frac = float(sys.argv[4])

    if(train_frac &gt; 1.0 or train_frac &lt; 0.0):
        print(""Training fraction (f) must be 0 &lt;= f &lt;= 1"")

    else:
        with tf.python_io.TFRecordWriter(train_fn) as writer:
            with tf.python_io.TFRecordWriter(valid_fn) as valid_writer:
                with open(processed_fn) as f:
                    for line in f:
                        ex, n_faces = parse_annotations(f, line.rstrip(), 30, 30)

                        randVal = rand.random()

                        if(randVal &lt; train_frac):
                            writer.write(ex.SerializeToString())

                        else:
                            valid_writer.write(ex.SerializeToString())
</code></pre>

<p>Note that I've removed some code that isn't to do with the actual serialisation/creation of the TFRecords file.</p>
","I am making an input pipeline in tensorflow for a task I want to do. I have set up a TFRecord dataset which has been saved out to a file on disk. I am trying to load in the dataset (to be batched and sent to the actual ML algorithm) using the following code: However, I get an error on the dataset.map() line. The error I get looks like this: TypeError: Expected int64, got &lt;tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x00000000085F74A8&gt; of type 'SparseTensor' instead. The read_single_record() function looks like this: I'm not sure where the issue lies. I got the idea for this code from the tensorflow API documentation, slightly modified for my purposes. I really have no idea where to start trying to fix this. For reference, here is the code I have for generating the TFRecord file: Note that I've removed some code that isn't to do with the actual serialisation/creation of the TFRecords file.",https://stackoverflow.com/questions/50600661,5280140,Documentation Replication on Other Examples
51016991,How to customize a RNN cell,"<p>I would like to implement a custom LSTM or GRU cell in TensorFlow (Python 3). For example, I want to scale the cell state signal from the cell at time step T before entering the cell at time step T+1. I've tried searching in TensorFlow documentation without a success.
Could you give me a hint?
Thank you.</p>

<p><strong>EDIT</strong><br>Having checked the answer given by <a href=""https://stackoverflow.com/questions/50262174/effect-of-setting-sequence-length-on-the-returned-state-in-dynamic-rnn/50289099#50289099"">@vijay m</a>, I create my model as follows:</p>

<pre><code>def dynamic_scale_RNN(x, timescale, seqlen, weights, biases, keep_prop):
    batch_size = tf.shape(x)[0]

    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)
    x = tf.unstack(x, max_seq_len, 1)
    timescale_unstack = tf.unstack(timescale, max_seq_len, 1)

    gru_cell = tf.contrib.rnn.GRUCell(n_hidden)

    #init_state has to be set to zero
    init_state = gru_cell.zero_state(batch_size, dtype=tf.float32)

    outputs = []
    # Create a loop of N LSTM cells, N = time_steps.
    for i in range(len(x)):
        output, state= tf.nn.static_rnn(gru_cell, [x[i]], dtype=tf.float32, initial_state= init_state)
        # copy the init_state with the new state
        mask = tf.tile(tf.expand_dims(timescale_unstack[i],axis=1),[1,state[0].get_shape()[-1]])
        init_state = tf.multiply(state,mask)
        # init_state = state
        outputs.append(output)

    # Transform the output to [batch_size, time_steps, vector_size]        
    outputs = tf.transpose(tf.squeeze(tf.stack(outputs)), [1, 0, 2])
</code></pre>

<p>In the code above, timescale is a tensor of shape <code>[batch_size, sequence_length, 1]</code> and I want to scale the cell state using this tensor. Even though the code can run, it returns <code>nan</code> for cost function.
If I uncomment the line <code>init_state = state</code>, it works, but it won't scale the cell state.</p>

<p>My question, for now, is that: Why I get <code>nan</code> values for cost function?</p>
","I would like to implement a custom LSTM or GRU cell in TensorFlow (Python 3). For example, I want to scale the cell state signal from the cell at time step T before entering the cell at time step T+1. I've tried searching in TensorFlow documentation without a success. Could you give me a hint? Thank you. EDITHaving checked the answer given by @vijay m, I create my model as follows: In the code above, timescale is a tensor of shape [batch_size, sequence_length, 1] and I want to scale the cell state using this tensor. Even though the code can run, it returns nan for cost function. If I uncomment the line init_state = state, it works, but it won't scale the cell state. My question, for now, is that: Why I get nan values for cost function?",https://stackoverflow.com/questions/51016991,2241766,Lack of Alternative Solutions/Documentation
51144993,Tensorflow: Is the learning rate you set in Adam and Adagrad just the initial learning rate?,"<p>I'm reading this blog</p>

<p><a href=""https://smist08.wordpress.com/2016/10/04/the-road-to-tensorflow-part-10-more-on-optimization/"" rel=""nofollow noreferrer"">https://smist08.wordpress.com/2016/10/04/the-road-to-tensorflow-part-10-more-on-optimization/</a></p>

<p>where it mentions all the tensorflow's learning rates</p>

<pre><code>optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)

optimizer = tf.train.AdadeltaOptimizer(starter_learning_rate).minimize(loss)

optimizer = tf.train.AdagradOptimizer(starter_learning_rate).minimize(loss)     # promising

optimizer = tf.train.AdamOptimizer(starter_learning_rate).minimize(loss)      # promising

optimizer = tf.train.MomentumOptimizer(starter_learning_rate, 0.001).minimize(loss) # diverges

optimizer = tf.train.FtrlOptimizer(starter_learning_rate).minimize(loss)    # promising

optimizer = tf.train.RMSPropOptimizer(starter_learning_rate).minimize(loss)   # promising
</code></pre>

<p>It says that the learning rate you input is only the starter learning rate. Does that mean that if you change the learning rate in the middle of training, that change will have no effect because it's not using the starter learning rate anymore?  </p>

<p>I tried looking at the API docs and it doesn't specify this. </p>
","I'm reading this blog https://smist08.wordpress.com/2016/10/04/the-road-to-tensorflow-part-10-more-on-optimization/ where it mentions all the tensorflow's learning rates It says that the learning rate you input is only the starter learning rate. Does that mean that if you change the learning rate in the middle of training, that change will have no effect because it's not using the starter learning rate anymore? I tried looking at the API docs and it doesn't specify this.",https://stackoverflow.com/questions/51144993,3259896,Documentation Completeness
51765061,Display realtime Training of model using tensorboard Python Tensorflow,"<p>I am unable to the documentation of the Tensorboard as there are many confusing stuffs for me.   </p>

<p>I want to know how I can display the complete training and validation movements of my model in tensorboard. I want to display values and how they are playing as input and what output is getting generated and the MSE error loss for the training and validation.    </p>

<p>Here is what tried till now. It only gives me the training loss result per iteration in an epoch.   </p>

<pre><code>seq_len = 9
## Basic Cell RNN in tensorflow
index_in_epoch = 0;
perm_array  = np.arange(x_train.shape[0])
np.random.shuffle(perm_array)
# function to get the next batch
def get_next_batch(batch_size):
    global index_in_epoch, x_train, perm_array   
    start = index_in_epoch
    index_in_epoch += batch_size

    if index_in_epoch &gt; x_train.shape[0]:
        np.random.shuffle(perm_array) # shuffle permutation array
        start = 0 # start next epoch
        index_in_epoch = batch_size

    end = index_in_epoch
    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]
# parameters
n_steps = seq_len-1 
n_inputs = x_train.shape[2]#4 

n_neurons = 400 
n_outputs = y_train.shape[1]#4
n_layers = 2
learning_rate = 0.000001

batch_size = 50
n_epochs = 100#200 
train_set_size = x_train.shape[0]
test_set_size = x_test.shape[0]

tf.reset_default_graph()

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_outputs])

# use Basic RNN Cell
layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.elu)
          for layer in range(n_layers)]                                                                 
multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) 
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
outputs = outputs[:,n_steps-1,:] # keep only last output of sequence                                     
loss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error 
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) 
training_op = optimizer.minimize(loss)
tf.summary.scalar('MyLoss', loss)
summ = tf.summary.merge_all()
</code></pre>

<p>Training starts like this:  </p>

<pre><code>saver = tf.train.Saver()
try:
    with tf.Session() as sess: 
        sess.run(tf.global_variables_initializer())        
        for iteration in range(int(n_epochs*train_set_size/batch_size)):
            x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch
            writer = tf.summary.FileWriter(""logs"", sess.graph)
            [_,s] = sess.run([training_op,summ], feed_dict={X: x_batch, y: y_batch}) 
            writer.add_summary(s, iteration)    
            if iteration % int(5*train_set_size/batch_size) == 0:
                mse_train = loss.eval(feed_dict={X: x_train, y: y_train}) 
                mse_valid = loss.eval(feed_dict={X: x_valid, y: y_valid}) 
                print('%.2f epochs: MSE train/valid = %.10f/%.10f'%(
                    iteration*batch_size/train_set_size, mse_train, mse_valid))
                save_path = saver.save(sess, ""modelsOHLC\\model""+str(iteration)+"".ckpt"")                
            writer.close()
except KeyboardInterrupt:
    print(""Keyboard stopped"")
</code></pre>

<p>Please anyone please guide me what I can do to see my training and validation lively. I just cannot understand whether my data is making any sense to the model or not. Hence, visualization can become a better option for me. Please help me.</p>
","I am unable to the documentation of the Tensorboard as there are many confusing stuffs for me. I want to know how I can display the complete training and validation movements of my model in tensorboard. I want to display values and how they are playing as input and what output is getting generated and the MSE error loss for the training and validation. Here is what tried till now. It only gives me the training loss result per iteration in an epoch. Training starts like this: Please anyone please guide me what I can do to see my training and validation lively. I just cannot understand whether my data is making any sense to the model or not. Hence, visualization can become a better option for me. Please help me.",https://stackoverflow.com/questions/51765061,4948889,Documentation Ambiguity
51777643,Tensorflow: Identifying the final state in MultiRNN,"<p>I am new to TF and I am trying to implement multiple GRU cells into the NN. However, I am unable to identify the final state of the MultiRNN cell.</p>

<p>For instance, when I use the following code:</p>

<pre><code>num_units = [128, 128]
tf.reset_default_graph()
x = tf.placeholder(tf.int32, [None, 134])
y = tf.placeholder(tf.int32, [None]) 
embedding_matrix = tf.Variable(tf.random_uniform([153, 128], -1.0, 1.0))
embeddings = tf.nn.embedding_lookup(embedding_matrix, x) 
cells = [tf.contrib.rnn.GRUCell(num_units=n) for n in num_units]
cell_type = tf.contrib.rnn.MultiRNNCell(cells=cells, state_is_tuple=True)
cell_type = tf.contrib.rnn.DropoutWrapper(cell=cell_type, output_keep_prob=0.75)
_, (encoding, _) = tf.nn.dynamic_rnn(cell_type, embeddings, dtype=tf.float32)
</code></pre>

<p>The output of the final line of code is:</p>

<pre><code>(&lt;tf.Tensor 'rnn/transpose_1:0' shape=(?, 134, 128) dtype=float32&gt;, (&lt;tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 128) dtype=float32&gt;, &lt;tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 128) dtype=float32&gt;))
</code></pre>

<p>I believe that the format is:</p>

<blockquote>
  <p><strong>Output Format</strong>: (a,[b, c])</p>
</blockquote>

<p>The documentation says that the output is in the format (output, state=[batch_size, cell.state_size]). However, I am unable to identify which of these is the final state of this memory cell. I think that it should be b.</p>

<p>Also, when I run the same code above with 4 GRU cells:</p>

<pre><code>num_units = [128, 128, 128, 128]
</code></pre>

<p>The output is even more confusing:</p>

<blockquote>
  <p><strong>Output Format</strong>: (a,[b, c, d, e])</p>
</blockquote>

<p>I am confused about which one of the above is the final memory state which I could then process further for loss calculation and making predictions.</p>
","I am new to TF and I am trying to implement multiple GRU cells into the NN. However, I am unable to identify the final state of the MultiRNN cell. For instance, when I use the following code: The output of the final line of code is: I believe that the format is: The documentation says that the output is in the format (output, state=[batch_size, cell.state_size]). However, I am unable to identify which of these is the final state of this memory cell. I think that it should be b. Also, when I run the same code above with 4 GRU cells: The output is even more confusing: I am confused about which one of the above is the final memory state which I could then process further for loss calculation and making predictions.",https://stackoverflow.com/questions/51777643,6657232,Documentation Replicability
51783265,in add_summary for value in summary.value: AttributeError: 'Tensor' object has no attribute 'value',"<p>This is a very basic tensorboard scalar log:</p>

<pre><code>import numpy as np
import tensorflow as tf
a = np.arange(10)
x = tf.convert_to_tensor(a, dtype=tf.float32)
x_summ = tf.summary.scalar(""X"", x)
writer = tf.summary.FileWriter('/tmp/logdir')
writer.add_summary(x_summ)
</code></pre>

<p>However, I get an error in add_summary for value in summary.value: </p>

<pre><code>AttributeError: 'Tensor' object has no attribute 'value'. 
</code></pre>

<p>Any solution for this?</p>

<p>TensorFlow documentation says ValueError is raised when the summary tensor has a wrong shape or type. When I print <code>x_summ</code> it shows:</p>

<pre><code>Tensor(""X:0"", shape=(), dtype=string)
</code></pre>

<p>I don't understand why is the shape <code>NULL</code> here.</p>
","This is a very basic tensorboard scalar log: However, I get an error in add_summary for value in summary.value: Any solution for this? TensorFlow documentation says ValueError is raised when the summary tensor has a wrong shape or type. When I print x_summ it shows: I don't understand why is the shape NULL here.",https://stackoverflow.com/questions/51783265,8713329,Requesting (Additional) Resources
52177257,Noob stuck Tensor flow adjusting the array shape,"<p>first thank you for any help that you can give. I am absolutely lost on the shape in tensensorflow. I have searched google, StackOverflow, discord, and youtube. I want to run a RNN on a CSV file. </p>

<pre><code>import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras as keras

data = pd.read_csv(""Big Data Chart Final no symbol.csv"")
data.shape 
(2941, 120)
</code></pre>

<h1>I believe this is a 2d dimension array? My batch size is 120?</h1>

<pre><code>inputs=tf.placeholder('float',[None,2],name=""input"")
targets = tf.placeholder('float',name='Target')
</code></pre>

<p>#So I have 120 input neurons? Since I have 120 columns?</p>

<h1>then my input data is 120? and I have 2941 sets of inputs,correct? so my batch shape should be 2941,120?</h1>

<p>I am getting lost even trying to explain it. AM I on the right track? I know there is a lot of info out there and I did read the documentation. Any help would be greatly appreciated.</p>
","first thank you for any help that you can give. I am absolutely lost on the shape in tensensorflow. I have searched google, StackOverflow, discord, and youtube. I want to run a RNN on a CSV file. #So I have 120 input neurons? Since I have 120 columns? I am getting lost even trying to explain it. AM I on the right track? I know there is a lot of info out there and I did read the documentation. Any help would be greatly appreciated.",https://stackoverflow.com/questions/52177257,10193731,Documentation Ambiguity
52311186,How model.fit works in Keras?,"<p><strong>My previous post or error is this <a href=""https://stackoverflow.com/questions/52261090/do-the-operations-defined-in-array-ops-in-tensorflow-have-gradient-defined"">one</a></strong>.
So, I found a different way of writing the function so it will be Tensorflow compatible. I tested it and it was working fine.
However when I want to integrate it into the keras ,I couldn't.
This is the solution for my previous post:</p>

<pre><code>graph = tf.Graph()
with graph.as_default():
i = tf.Variable(0)
error = tf.Variable(initial_value=0,dtype=tf.float64)
sol = tf.random_uniform(shape=[10, 36], dtype=tf.float64, 
maxval=1)
error_1 = tf.Variable(initial_value=0,dtype=tf.float64)
final_loss = tf.Variable(0)

def cond(i, sol, error):
    return tf.less(i, 9)
def body(i, sol,error):
    i = tf.add(i, 1)
    print('i',i)
    #sol = tf.add(sol, 1)
    original_reshaped_elem = original_dim* sol[i]
    original_reshaped_elem = tf.reshape(original_reshaped_elem, 
    [DIM,DIM])
    a = tf.reshape(original_reshaped_elem[:,DIM-1], [DIM,1])
    b = tf.reshape(original_reshaped_elem[:,1], [DIM,1])

    original_reshaped_elem = tf.concat 
    ([b,original_reshaped_elem], axis= 1)
    original_reshaped_elem = tf.concat 
    ([original_reshaped_elem,a], axis= 1)

    c= tf.reshape(original_reshaped_elem[DIM-1,:], [1,DIM+2])
    d= tf.reshape(original_reshaped_elem[1,:], [1,DIM+2])
    original_reshaped_elem = tf.concat 
    ([d,original_reshaped_elem],axis=0)
    reshaped_elem_extended = tf.concat 
    ([original_reshaped_elem,c],axis=0)
    print('reshaped shape', reshaped_elem_extended)


    error = 
tf.add(error,tf.norm(tf.norm(reshaped_elem_extended,ord=2,axis=0),ord=2,axis=0))
    error_1 = tf.divide(error, 36)
    return [i, sol, error_1]


with tf.Session(graph=graph) as session:
     tf.global_variables_initializer().run()

result = tf.while_loop(cond, body, [i, sol, error])
final_loss = tf.divide(result[2], 10)
print(final_loss.eval())
print(result[1].eval())
</code></pre>

<p>This is how I call it in my model:</p>

<pre><code>result = tf.while_loop(cond, body, [i, inputs, error])
final_loss = tf.divide(result[2], 10)
vae.add_loss(final_loss)
</code></pre>

<p>then I get again this error</p>

<pre><code>ValueError: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.
</code></pre>

<p>So, I want to know how model.fit works in keras ? Does it instantiate the graph ? I didn't find any clear documentation about how it works, so I can integrate my loss function accordingly.</p>
","My previous post or error is this one. So, I found a different way of writing the function so it will be Tensorflow compatible. I tested it and it was working fine. However when I want to integrate it into the keras ,I couldn't. This is the solution for my previous post: This is how I call it in my model: then I get again this error So, I want to know how model.fit works in keras ? Does it instantiate the graph ? I didn't find any clear documentation about how it works, so I can integrate my loss function accordingly.",https://stackoverflow.com/questions/52311186,5159740,Documentation Ambiguity
52323297,TypeError when using tf.keras.layers.Reshape,"<p>when building a model in Keras, I run into this error:</p>

<pre><code>TypeError: Expected int32, got 8.0 of type 'float' instead.
</code></pre>

<p>The error occurs when initially building the model (as opposed to during execution), more specifically on the last line of this snippet:</p>

<pre><code>    d_dense1 = Dense(
        ((IMAGE_SIZE/4)**2)*(n if vanilla_architecture else 3*n),
        input_shape = (h,),
        activation = ""relu"",
        name = name_prefix + ""dense1""
    )(d_in)
    d_reshape1 = tf.keras.layers.Reshape(
        (IMAGE_SIZE/4, IMAGE_SIZE/4, (n if vanilla_architecture else 3*n)),
        name = name_prefix + ""reshape1""
    )(d_dense1)
</code></pre>

<blockquote>
  <p>Side note: I am using tf.keras.layers.Dense, IMAGE_SIZE is an integer, vanilla_architecture is a boolean, and n is an integer</p>
</blockquote>

<p>Obviously the dense layer will pass along a tensor of floats because, well, it's a machine learning operation. The issue seems to be that Reshape requires a tensor of integers. I read the documentation but there is no information there. </p>

<p>Here are some things I've tried:</p>

<ul>
<li>using tf.reshape

<ul>
<li>Same issue</li>
</ul></li>
<li>using numpy reshape

<ul>
<li>Just plain doesn't work </li>
</ul></li>
<li>reading example code like like 54 of <a href=""https://github.com/keras-team/keras/blob/master/examples/mnist_acgan.py"" rel=""nofollow noreferrer"">this</a> 

<ul>
<li>they seem to be doing the same thing as me but theirs works</li>
</ul></li>
</ul>

<p>The weird part is that it works just fine when using eager execution. I don't want to have eager execution enabled though because I want to use tensorboard. </p>
","when building a model in Keras, I run into this error: The error occurs when initially building the model (as opposed to during execution), more specifically on the last line of this snippet: Obviously the dense layer will pass along a tensor of floats because, well, it's a machine learning operation. The issue seems to be that Reshape requires a tensor of integers. I read the documentation but there is no information there. Here are some things I've tried: The weird part is that it works just fine when using eager execution. I don't want to have eager execution enabled though because I want to use tensorboard.",https://stackoverflow.com/questions/52323297,4293998,Lack of Alternative Solutions/Documentation
52785827,Run 1-D Conv using tensorflow,"<p>I'm a beginner in TensorFlow. I want to train a 1-D conv model. 
I have one-rowed csv files for each row of my original data.</p>

<p>csv files look like this</p>

<pre><code>csv_file1: 1.1, 1.3, 1.5, 1.5, 1
csv_file2: 2.1, 2.3, 2.7, 2.9, 0
</code></pre>

<p>The last column(containing 1 &amp; 0) are the labels for the single-row csv files</p>

<p>Following the <a href=""https://stackoverflow.com/questions/45427637/numpy-to-tfrecords-is-there-a-more-simple-way-to-handle-batch-inputs-from-tfrec/45428167#45428167"">link</a> I wrote the following pieces of code. </p>

<p>I converted the csv files to TFRecord using the following code</p>

<pre><code>    with tf.python_io.TFRecordWriter(filename) as writer:
        features, label = df_values[:, 1:-1], df_values[:, -1:]
        example = tf.train.Example()
        example.features.feature[""features""].float_list.value.extend(features[0])
        example.features.feature[""label""].int64_list.value.append(label[0])
        writer.write(example.SerializeToString())
</code></pre>

<p>I want to now read the files and this is the code I'm using. </p>

<pre><code>def _parse_function(data_record):
    features = {
        'label': tf.FixedLenSequenceFeature([], tf.int64, allow_missing = True),
        'features': tf.FixedLenSequenceFeature([], tf.float32, allow_missing = True),
    }
    sample = tf.parse_single_example(data_record, features)
    return sample['features'], sample['label']

filenames = glob.glob(""*.tfrecords"")
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(_parse_function)  
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.batch(batch_len)
# Create a one-shot iterator
iterator = dataset.make_one_shot_iterator()

X,y = iterator.get_next()
</code></pre>

<p>From here the problem starts,
From the documentation I understand what session does but failing to put it into code.
Assuming I'll later figure out how to use</p>

<blockquote>
  <p>tf.seesion.run()</p>
</blockquote>

<p>I wrote the below code but don't know how to actually include it into my main script and further use it to train my model.</p>

<pre><code>x_train_batch, y_train_batch = tf.train.shuffle_batch(
tensors=[X_train, y_train],
batch_size=batch_size,
capacity=capacity,
min_after_dequeue=min_after_dequeue,
enqueue_many=True,
num_threads=8)

x_train_batch = tf.cast(x_train_batch, tf.float32)
x_train_batch = tf.reshape(x_train_batch, shape=(batch_size, 1,65281))

y_train_batch = tf.cast(y_train_batch, tf.int64)
y_train_batch = tf.one_hot(y_train_batch, num_classes)
</code></pre>

<p>Any help regarding how to proceed further will help. </p>

<p>PS: Assuming my data was loaded into an np.array
the dimension would be, (6571, 65281).
Since it's astronomical data, each star has 65781 points.</p>
","I'm a beginner in TensorFlow. I want to train a 1-D conv model. I have one-rowed csv files for each row of my original data. csv files look like this The last column(containing 1 &amp; 0) are the labels for the single-row csv files Following the link I wrote the following pieces of code. I converted the csv files to TFRecord using the following code I want to now read the files and this is the code I'm using. From here the problem starts, From the documentation I understand what session does but failing to put it into code. Assuming I'll later figure out how to use I wrote the below code but don't know how to actually include it into my main script and further use it to train my model. Any help regarding how to proceed further will help. PS: Assuming my data was loaded into an np.array the dimension would be, (6571, 65281). Since it's astronomical data, each star has 65781 points.",https://stackoverflow.com/questions/52785827,8115171,Documentation Ambiguity
52879126,How are tensors immutable in TensorFlow?,"<p>I read the following sentence in the TensorFlow documentation:</p>

<blockquote>
  <p>With the exception of tf.Variable, the value of a tensor is immutable,
  which means that in the context of a single execution tensors only
  have a single value. However, evaluating the same tensor twice can
  return different values; for example that tensor can be the result of
  reading data from disk, or generating a random number.</p>
</blockquote>

<p>Can someone elaborate a little bit on the ""immutable"" aspect of a Tensor?</p>

<ol>
<li>What is the ""scope of the immutability"" since evaluating a tensor twice could return different results? </li>
<li>What does it mean ""the context of a single execution""?</li>
</ol>
","I read the following sentence in the TensorFlow documentation: Can someone elaborate a little bit on the ""immutable"" aspect of a Tensor?",https://stackoverflow.com/questions/52879126,3926152,Documentation Ambiguity
52992821,Tensorflow graph fetch all consts in a scope,"<p>I create a graph and now I want to fetch their ops, how can I do this?</p>

<pre><code>g = tf.Graph()

with g.as_default():
    # Define inputs
    with tf.name_scope(""inputs""):
        a = tf.constant(2, tf.int32, name=""a"")
        b = tf.constant(3, tf.int32, name=""b"")

    # Ops
    with tf.name_scope(""ops""):
        c = tf.multiply(a, b, name=""c"")
        d = tf.add(a, b, name=""d"")
        e = tf.subtract(c, d, name=""e"")

sess = tf.InteractiveSession()

_c, _d, _e = ... &lt;-- (I need some code here!)
</code></pre>

<p>Can you show me document links about this?</p>
","I create a graph and now I want to fetch their ops, how can I do this? Can you show me document links about this?",https://stackoverflow.com/questions/52992821,6935676,Requesting (Additional) Resources
53144832,Tensorflow CNN 'tuple' object has no attribute 'initializer',"<p>Seems like I am messing up a step in preparing the dataset. Couldn't find a proper answer or look up the correct solution in documentation. I have pointed out the problem line with ###, bottom part.</p>

<pre><code>def parse_file(data_path):


    imagepaths = list()
    labels = list()
    # a working parser for os is here

    imagepaths = tf.constant(imagepaths, dtype=tf.string)
    labels = tf.constant(labels, dtype=tf.float32)

    return imagepaths, labels


def parse_image(imagepath, label):

    image_string = tf.read_file(imagepath)
    image_decoded = tf.image.decode_png(image_string, channels=3)
    # The image size is 425x425.
    image_resized = tf.image.resize_images(image_decoded, [img_size, img_size])
    image_normalized = image_resized * 1.0/255
    print(image_normalized)
    print(label)
    return image_normalized, label

dataset = tf.data.Dataset.from_tensor_slices((parsed_files))
dataset = dataset.map(parse_image)
dataset = dataset.batch(batch_size)

iterator = dataset.make_initializable_iterator()
iterator = iterator.get_next()

x = tf.placeholder(tf.float32, [None, img_size, img_size, channels])
y = tf.placeholder(tf.float32, [None, 1])
</code></pre>

<p>(Model goes here, irrelevant.)</p>

<pre><code>with tf.Session() as sess:

    ### AttributeError: 'tuple' object has no attribute 'initializer'
    sess.run(iterator.initializer)
    batch_x, batch_y = iterator.get_next()
    test1, test2 = sess.run([batch_x, batch_y])
    total_batch = int(total_input[0] / batch_size)
    # define the iterator for the network
    for epoch in range(epochs):
        avg_cost = 0
        for i in range(total_batch):
            batch_x, batch_y = sess.run(iterator)
            _, c = sess.run([optimiser, cross_entropy], feed_dict={x: batch_x, y: batch_y})
            avg_cost += c / total_batch

        test_acc = sess.run(accuracy,feed_dict={x: test_x, y: np.expand_dims(test_y, axis=-1)})
        print(""Epoch:"", (epoch + 1), ""cost ="", ""{:.3f}"".format(avg_cost), "" test accuracy: {:.3f}"".format(test_acc))
        summary = sess.run(merged, feed_dict={x: test_x, y: np.expand_dims(test_y, axis=-1)})

    print(""\nTraining complete!"")
    print(sess.run(accuracy, feed_dict={x: test_x, y: np.expand_dims(test_y, axis=-1)}))
</code></pre>
","Seems like I am messing up a step in preparing the dataset. Couldn't find a proper answer or look up the correct solution in documentation. I have pointed out the problem line with ###, bottom part. (Model goes here, irrelevant.)",https://stackoverflow.com/questions/53144832,10520111,Lack of Alternative Solutions/Documentation
53233633,2 Layer Neural Network Does not Converge,"<h1>Background</h1>

<p>I am a newbie to TensorFlow and I am trying to understand the basics of deep learning. I started from writing a two-layer neural network from scratch and it achieved 89% accuracy on MNIST dataset and now I am trying to implement the same network in TensorFlow and compare their performance. </p>

<h1>Problem</h1>

<p>I am not sure if I miss something basic in the code, but the following implementation seems to be unable to update weights and therefore could not output anything meaningful.</p>

<pre><code>num_hidden = 100
# x -&gt; (batch_size, 784)
x = tf.placeholder(tf.float32, [None, 784])

W1 = tf.Variable(tf.zeros((784, num_hidden)))
b1 = tf.Variable(tf.zeros((1, num_hidden)))
W2 = tf.Variable(tf.zeros((num_hidden, 10)))
b2 = tf.Variable(tf.zeros((1, 10)))
# z -&gt; (batch_size, num_hidden)
z = tf.nn.relu(tf.matmul(x, W1) + b1)
# y -&gt; (batch_size, 10)
y = tf.nn.softmax(tf.matmul(z, W2) + b2)

# y_ -&gt; (batch_size, 10)
y_ =  tf.placeholder(tf.float32, [None, 10])
# y_ * tf.log(y) -&gt; (batch_size, 10)
cross_entropy =  -tf.reduce_sum(y_ * tf.log(y+1e-10))
train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)
sess = tf.InteractiveSession()
tf.global_variables_initializer().run()
# tf.argmax(y, axis=1) returns the maximum index in each row
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
for epoch in range(1000):
    # batch_xs -&gt; (100, 784)
    # batch_ys -&gt; (100, 10), one-hot encoded
    batch_xs, batch_ys = mnist.train.next_batch(100)
    train_data = {x: batch_xs, y_: batch_ys}
    sess.run(train_step, feed_dict=train_data)
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
W1_e, b1_e, W2_e, b2_e = W1.eval(), b1.eval(), W2.eval(), b2.eval()
sess.close()
</code></pre>

<h1>What I Have Done</h1>

<p>I checked many the official docs and many other implementations, but I feel totally confused since they may use different versions and API varies greatly.</p>

<p>So could someone help me, thank you in advance.</p>
","I am a newbie to TensorFlow and I am trying to understand the basics of deep learning. I started from writing a two-layer neural network from scratch and it achieved 89% accuracy on MNIST dataset and now I am trying to implement the same network in TensorFlow and compare their performance. I am not sure if I miss something basic in the code, but the following implementation seems to be unable to update weights and therefore could not output anything meaningful. I checked many the official docs and many other implementations, but I feel totally confused since they may use different versions and API varies greatly. So could someone help me, thank you in advance.",https://stackoverflow.com/questions/53233633,7784797,Documentation Ambiguity
53470714,image classify and tensorflow serving,"<p>First of all sorry I am not prcise for this question but I am studying the tensorflow-serving and how to put in production my cnn. sincerely the documentation is quite confuse to me. I hope you can help to understand better the save model architecture. So please reply to me as teacher, i would like to know more about the whole flow.</p>

<p>I am developping a simple cnn to classify an image to 4 output.
I need tensorflow-serving to put it in production.
The image in input can be watherver size, the CNN should resize it first and predict.
Here the code</p>

<pre><code>import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot as plt
from scipy.misc import toimage
from keras.models import Sequential
from keras.layers import *
from keras.optimizers import *
from tensorflow.python.saved_model import builder as saved_model_builder
from tensorflow.python.saved_model import tag_constants, signature_constants, signature_def_utils_impl
import cv2



#train_path='Garage/train'
#train_datagen = ImageDataGenerator(rescale=1./255)
#train_batch = train_datagen.flow_from_directory(train_path, target_size=(64,64), class_mode='categorical', batch_size=10, color_mode='grayscale')


#validation_datagen = ImageDataGenerator(rescale=1./255)
#validation_batch = validation_datagen.flow_from_directory(
#        './Garage/validation',
#        target_size=(64, 64),
#        batch_size=3,
#        class_mode='categorical', color_mode='grayscale')

model = Sequential()
model.add(InputLayer(input_shape=[64,64,1]))
model.add(Conv2D(filters=32,kernel_size=5,strides=1,padding='same',activation='relu'))
model.add(MaxPool2D(pool_size=5,padding='same'))

model.add(Conv2D(filters=50,kernel_size=5,strides=1,padding='same',activation='relu'))
model.add(MaxPool2D(pool_size=5,padding='same'))

model.add(Conv2D(filters=80,kernel_size=5,strides=1,padding='same',activation='relu'))
model.add(MaxPool2D(pool_size=5,padding='same'))

model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512,activation='relu'))
model.add(Dropout(rate=0.5))
model.add(Dense(4,activation='softmax'))
optimizer=Adam(lr=1e-3)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
#model.fit_generator(
#        train_batch,
#        epochs=50,
#        steps_per_epoch=6,
#        validation_data=validation_batch,
#        validation_steps=5)

model.load_weights('model.h5')

#score = model.evaluate_generator(validation_batch,steps=3)
#print('Test loss:', score[0])
#print('Test accuracy:', score[1])

#model.save('model.h5')


from PIL import Image
import requests
from io import BytesIO

response = requests.get('http://192.168.3.21:7451/shot.jpg')
image_pil = Image.open(BytesIO(response.content))
image = np.asarray(image_pil)

img2 = cv2.resize(image,(64,64))
img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
img = np.reshape(img2,[1,64,64,1])

classes = model.predict_classes(img)

print(classes)

model_version=""1""

sess = tf.Session()

#setting values for the sake of saving the model in the proper format
x = model.input
y = model.output

prediction_signature = tf.saved_model.signature_def_utils.predict_signature_def({""inputs"":x}, {""prediction"":y})

valid_prediction_signature = tf.saved_model.signature_def_utils.is_valid_signature(prediction_signature)
if(valid_prediction_signature == False):
    raise ValueError(""Error: Prediction signature not valid!"")

builder = saved_model_builder.SavedModelBuilder('./'+model_version)
legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')

# Add the meta_graph and the variables to the builder
builder.add_meta_graph_and_variables(
      sess, [tag_constants.SERVING],
      signature_def_map={
           signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:prediction_signature, },
      legacy_init_op=legacy_init_op)

# save the graph
builder.save()
</code></pre>

<p>the code will take the picture from a cam <a href=""http://192.168.3.21:7451/shot.jpg"" rel=""nofollow noreferrer"">http://192.168.3.21:7451/shot.jpg</a>
and then it will predict it</p>

<p>When I compile the code it return a lot of errors when it try to save the model. can you please check it and tell me if the save model instructions are right?</p>

<p>I use x = model.input as input from the serving but I would like it take the picture as input from the server. 
I am quite confuse actually, sorry.
The scope is when I request by gRPC to predict the image the model can give me the prediction result 
Thanks</p>
","First of all sorry I am not prcise for this question but I am studying the tensorflow-serving and how to put in production my cnn. sincerely the documentation is quite confuse to me. I hope you can help to understand better the save model architecture. So please reply to me as teacher, i would like to know more about the whole flow. I am developping a simple cnn to classify an image to 4 output. I need tensorflow-serving to put it in production. The image in input can be watherver size, the CNN should resize it first and predict. Here the code the code will take the picture from a cam http://192.168.3.21:7451/shot.jpg and then it will predict it When I compile the code it return a lot of errors when it try to save the model. can you please check it and tell me if the save model instructions are right? I use x = model.input as input from the serving but I would like it take the picture as input from the server. I am quite confuse actually, sorry. The scope is when I request by gRPC to predict the image the model can give me the prediction result Thanks",https://stackoverflow.com/questions/53470714,10675469,Documentation Ambiguity
53816414,TensorFlow Operation and cannot be found in official API,"<p>recently I try to repeat and learn the code posted on GitHub by Nvidia--progressive_growing_of_gans. However, I find that there are several operations that I can not find reference based on official API as the following.</p>

<pre><code>feed_dict = {}
setter = tf.assign(var, tf.placeholder(var.dtype, var.shape, 'new_value'),name='setter')
feed_dict[setter.op.inputs[1]] = value
</code></pre>

<p>What does the setter.op.inputs mean?</p>

<pre><code>v = tf.cast(value_expr, tf.float32)
v.shape.ndims
</code></pre>

<p>What does the v.shape.ndims mean? </p>

<p>By the way, how can I get the reference for such class method? It seems that they are not included in official API.</p>

<p>Thank you, everybody!</p>
","recently I try to repeat and learn the code posted on GitHub by Nvidia--progressive_growing_of_gans. However, I find that there are several operations that I can not find reference based on official API as the following. What does the setter.op.inputs mean? What does the v.shape.ndims mean? By the way, how can I get the reference for such class method? It seems that they are not included in official API. Thank you, everybody!",https://stackoverflow.com/questions/53816414,9881203,Documentation Completeness
53963654,Error running Tensorflow Object Detection with custom dataset,"<p>I've been trying to use tensorflow's object detection api for a school project and I've managed to follow their instructions in the documentation, but I'm getting this error I can't find anywhere online.
This is the output in the console:</p>

<pre><code>WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.
WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.
WARNING:tensorflow:Estimator's model_fn (&lt;function create_model_fn.&lt;locals&gt;.model_fn at 0x7f118aa30e18&gt;) includes params argument, but params are not passed to Estimator.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /home/pipas/School/tensorflow-models/research/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
WARNING:tensorflow:From /home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From /home/pipas/School/tensorflow-models/research/object_detection/core/preprocessor.py:1218: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From /home/pipas/School/tensorflow-models/research/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.batch(..., drop_remainder=True)`.
2018-12-28 19:39:31.314235: E tensorflow/core/util/events_writer.cc:108] Write failed because file could not be opened.
2018-12-28 19:39:32.354316: E tensorflow/core/util/events_writer.cc:108] Write failed because file could not be opened.
2018-12-28 19:39:33.177681: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-12-28 19:39:33.246316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-28 19:39:33.246780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 3.59GiB
2018-12-28 19:39:33.246813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2018-12-28 19:39:33.798306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-12-28 19:39:33.798331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2018-12-28 19:39:33.798337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2018-12-28 19:39:33.798491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3310 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)
Traceback (most recent call last):
  File ""object_detection/model_main.py"", line 109, in &lt;module&gt;
    tf.app.run()
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""object_detection/model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
    return self.run_local()
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
    saving_listeners)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1468, in _train_with_estimator_spec
    log_step_count_steps=log_step_count_steps) as mon_sess:
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 504, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session
    return self._sess_creator.create_session()
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 807, in create_session
    hook.after_create_session(self.tf_sess, self.coord)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 559, in after_create_session
    ""graph.pbtxt"")
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/framework/graph_io.py"", line 71, in write_graph
    text_format.MessageToString(graph_def))
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 434, in atomic_write_string_to_file
    write_string_to_file(temp_pathname, contents)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 314, in write_string_to_file
    f.write(file_content)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 108, in write
    self._prewrite_check()
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 94, in _prewrite_check
    compat.as_bytes(self.__name), compat.as_bytes(self.__mode), status)
  File ""/home/pipas/.pyenv/versions/vcom/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.FailedPreconditionError: /home/pipas/School/tensorflow-models/research/porto-recognition/data/labels.pbtxt/graph.pbtxt.tmp13ac703bc82a469eaec2c658091efd80; Not a directory
</code></pre>

<p>This is what I'm running</p>

<pre><code>PIPELINE_CONFIG_PATH=/home/pipas/School/tensorflow-models/research/porto-recognition/models/ssd_mobilenet_v1_coco_2018_01_28/pipeline.config
MODEL_DIR=/home/pipas/School/tensorflow-models/research/porto-recognition/data/labels.pbtxt
NUM_TRAIN_STEPS=5000
SAMPLE_1_OF_N_EVAL_EXAMPLES=1
python object_detection/model_main.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --num_train_steps=${NUM_TRAIN_STEPS} \
    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \
    --alsolog
</code></pre>

<p>and here is the pipeline.config file</p>

<pre><code>model {
  ssd {
    num_classes: 5
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    feature_extractor {
      type: ""ssd_mobilenet_v1""
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.99999989895e-05
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.0299999993294
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.999700009823
          center: true
          scale: true
          epsilon: 0.0010000000475
          train: true
        }
      }
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.99999989895e-05
            }
          }
          initializer {
            truncated_normal_initializer {
              mean: 0.0
              stddev: 0.0299999993294
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.999700009823
            center: true
            scale: true
            epsilon: 0.0010000000475
            train: true
          }
        }
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.800000011921
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.20000000298
        max_scale: 0.949999988079
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.333299994469
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 0.300000011921
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.990000009537
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
  }
}
train_config {
  batch_size: 24
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  optimizer {
    rms_prop_optimizer {
      learning_rate {
        exponential_decay_learning_rate {
          initial_learning_rate: 0.00400000018999
          decay_steps: 800720
          decay_factor: 0.949999988079
        }
      }
      momentum_optimizer_value: 0.899999976158
      decay: 0.899999976158
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: ""/home/pipas/School/tensorflow-models/research/porto-recognition/models/ssd_mobilenet_v1_coco_2018_01_28/model.ckpt""
  from_detection_checkpoint: true
  num_steps: 200000
}
train_input_reader {
  label_map_path: ""/home/pipas/School/tensorflow-models/research/porto-recognition/data/labels.pbtxt""
  tf_record_input_reader {
    input_path: ""/home/pipas/School/tensorflow-models/research/porto-recognition/data/porto_train.record""
  }
}
eval_config {
  num_examples: 8000
  max_evals: 10
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: ""/home/pipas/School/tensorflow-models/research/porto-recognition/data/labels.pbtxt""
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: ""/home/pipas/School/tensorflow-models/research/porto-recognition/data/porto_val.record""
  }
}
</code></pre>
","I've been trying to use tensorflow's object detection api for a school project and I've managed to follow their instructions in the documentation, but I'm getting this error I can't find anywhere online. This is the output in the console: This is what I'm running and here is the pipeline.config file",https://stackoverflow.com/questions/53963654,,Documentation Replicability
53965422,Training loss and accuracy don't change - stuck around a value -- Tensorflow,"<p>My LSTM model below in tensorflow works without any error, but the values of the loss and accuracy seem to swarm around specific values, leading to a misleading conclusion after all the epochs. </p>

<p><strong>My thoughts were/are:</strong> </p>

<ul>
<li><p>It might be a problem of <strong>feeding the same batch each time</strong>. I think this cannot be the problem because I get the printing after each 10th epoch; so that I'm feeding different batches of the data. </p></li>
<li><p>It might be a cause of the <strong>low complexity of the LSTM network</strong>. However, I believe this cannot be the problem because the training phase should somehow improve over time because of the optimization.</p></li>
<li><p>As the loss and accuracy are around some value, I might be <strong>calculating them or printing them wrongly</strong>, leading to this values althought I don't get my error in the code. </p></li>
<li><p>I think that this issue is due to <strong>some typo or misunderstanding in my code</strong>, because in some other case, I would see the loss and accuracy either increase or decrease, even with not constant patterns but not this way. </p></li>
<li><p>Finally, if all my code is well arranged, I think that this values could be due to bad hyperparameters settings. <strong>My following steps</strong> could be to increase the complexity of the LSTM network, lowering the learning rate and increasing the batch size.</p></li>
</ul>

<p>As there is not a lot of documentation (and due to my little experience) about linking the dataset API, together with RNNs, different batches and different datasets, I may have done something wrongly.</p>

<p>Please, I would appreciate any review or knowdledge that could add some light to this.</p>

<p>Many thanks for your time. </p>

<p><strong>CODE:</strong></p>

<pre><code>'''All the data is numeric. The dataset is scaled using the StandardScaler from scikit-learn and in the following shapes:'''

#The 3D xt array has a shape of: (11, 69579, 74)
#The 3D xval array has a shape of: (11, 7732, 74)

#y shape is: (69579, 3)
#yval shape is: (7732, 3)

N_TIMESTEPS_X = xt.shape[0] ## The stack number
BATCH_SIZE = 256
#N_OBSERVATIONS = xt.shape[1]
N_FEATURES = xt.shape[2]
N_OUTPUTS = yt.shape[1]
N_NEURONS_LSTM = 128 ## Number of units in the LSTMCell 
N_EPOCHS = 600
LEARNING_RATE = 0.1

### Define the placeholders anda gather the data.
xt = xt.transpose([1,0,2])
xval = xval.transpose([1,0,2])

train_data = (xt, yt)
validation_data = (xval, yval)

## We define the placeholders as a trick so that we do not break into memory problems, associated with feeding the data directly.

'''As an alternative, you can define the Dataset in terms of tf.placeholder() tensors, and feed the NumPy arrays when you initialize an Iterator over the dataset.'''

batch_size = tf.placeholder(tf.int64)

x = tf.placeholder(tf.float32, shape=[None, N_TIMESTEPS_X, N_FEATURES], name='XPlaceholder')

y = tf.placeholder(tf.float32, shape=[None, N_OUTPUTS], name='YPlaceholder')

# Creating the two different dataset objects.

train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE).repeat()

val_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(BATCH_SIZE)

# Creating the Iterator type that permits to switch between datasets.

itr = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)
train_init_op = itr.make_initializer(train_dataset)
validation_init_op = itr.make_initializer(val_dataset)

next_features, next_labels = itr.get_next()

### Create the graph 

cellType = tf.nn.rnn_cell.LSTMCell(num_units=N_NEURONS_LSTM, name='LSTMCell')

inputs = tf.unstack(next_features, axis=1)

'''inputs: A length T list of inputs, each a Tensor of shape [batch_size, input_size]'''

RNNOutputs, _ = tf.nn.static_rnn(cell=cellType, inputs=inputs, dtype=tf.float32)

out_weights = tf.get_variable(""out_weights"", shape=[N_NEURONS_LSTM, N_OUTPUTS], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())
out_bias = tf.get_variable(""out_bias"", shape=[N_OUTPUTS], dtype=tf.float32, initializer=tf.zeros_initializer())

predictionsLayer = tf.matmul(RNNOutputs[-1], out_weights) + out_bias

### Define the cost function, that will be optimized by the optimizer. 

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictionsLayer, labels=next_labels, name='Softmax_plus_Cross_Entropy'))

optimizer_type = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, name='AdamOptimizer')

optimizer = optimizer_type.minimize(cost)

### Model evaluation 

correctPrediction = tf.equal(tf.argmax(predictionsLayer,1), tf.argmax(next_labels,1))

accuracy = tf.reduce_mean(tf.cast(correctPrediction,tf.float32))

N_BATCHES = train_data[0].shape[0] // BATCH_SIZE

## Saving variables so that we can restore them afterwards.

saver = tf.train.Saver()

save_dir = '/home/zmlaptop/Desktop/tfModels/{}_{}'.format(cellType.__class__.__name__, datetime.now().strftime(""%Y%m%d%H%M%S""))
os.mkdir(save_dir)

varDict = {'nTimeSteps':N_TIMESTEPS_X, 'BatchSize': BATCH_SIZE, 'nFeatures':N_FEATURES,
           'nNeuronsLSTM':N_NEURONS_LSTM, 'nEpochs':N_EPOCHS,
           'learningRate':LEARNING_RATE, 'optimizerType': optimizer_type.__class__.__name__}

varDicSavingTxt = save_dir + '/varDict.txt'
modelFilesDir = save_dir + '/modelFiles'
os.mkdir(modelFilesDir)

logDir = save_dir + '/TBoardLogs'
os.mkdir(logDir)

acc_summary = tf.summary.scalar('Accuracy', accuracy)
loss_summary = tf.summary.scalar('Cost_CrossEntropy', cost)
summary_merged = tf.summary.merge_all()

with open(varDicSavingTxt, 'w') as outfile:
    outfile.write(repr(varDict))

with tf.Session() as sess:

    tf.set_random_seed(2)
    sess.run(tf.global_variables_initializer())
    train_writer = tf.summary.FileWriter(logDir + '/train', sess.graph)
    validation_writer = tf.summary.FileWriter(logDir + '/validation')

    # initialise iterator with train data

    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: BATCH_SIZE})

    print('¡Training starts!')
    for epoch in range(N_EPOCHS):

        batchAccList = []
        tot_loss = 0

        for batch in range(N_BATCHES):

            optimizer_output, loss_value, summary, accBatch = sess.run([optimizer, cost, summary_merged, accuracy], feed_dict = {x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})
            tot_loss += loss_value
            batchAccList.append(accBatch)

            if batch % 10 == 0:

                train_writer.add_summary(summary, batch)

        epochAcc = tf.reduce_mean(batchAccList)
        epochAcc_num = sess.run(epochAcc, feed_dict = {x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})

        if epoch%10 == 0:

            print(""Epoch: {}, Loss: {:.4f}, Accuracy: {}"".format(epoch, tot_loss / N_BATCHES, epochAcc_num))

    # initialise iterator with validation data

    sess.run(validation_init_op, feed_dict = {x: validation_data[0], y: validation_data[1], batch_size:len(validation_data[0])})

    valLoss, valAcc = sess.run([cost, accuracy], feed_dict = {x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})
    print('Validation Loss: {:4f}, Validation Accuracy: {}'.format(valLoss, valAcc))

    summary_val = sess.run(summary_merged, feed_dict = {x: validation_data[0], y: validation_data[1], batch_size: len(validation_data[0])})

    validation_writer.add_summary(summary_val)

    saver.save(sess, modelFilesDir)
</code></pre>

<p><strong>This is the output in tensorboard for accuracy and loss (stopped at around 250 epochs):</strong></p>

<p><a href=""https://i.stack.imgur.com/dz6aw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dz6aw.png"" alt=""enter image description here""></a></p>
","My LSTM model below in tensorflow works without any error, but the values of the loss and accuracy seem to swarm around specific values, leading to a misleading conclusion after all the epochs. My thoughts were/are: As there is not a lot of documentation (and due to my little experience) about linking the dataset API, together with RNNs, different batches and different datasets, I may have done something wrongly. Please, I would appreciate any review or knowdledge that could add some light to this. Many thanks for your time. CODE: This is the output in tensorboard for accuracy and loss (stopped at around 250 epochs):",https://stackoverflow.com/questions/53965422,9273596,Lack of Alternative Solutions/Documentation
53998796,IPython Notebook Kernel Crashes when Assessing CNN Accuracy,"<p>I'm relatively new to TensorFlow. I have built a Logistic Regression classifier and a MultiLayer Perceptron in the past that have worked. Now that I have moved on to the Convolutional Neural Network, I am having some problems with testing accuracy. My code is below. The line I am having trouble with is only the <strong>very last line</strong> where I am attempting to print the test accuracy figure. The print 1, 2, 3 statements are intended to show this.</p>

<pre><code>### import libraries ###

import tensorflow as tf
import numpy as np
from tqdm import trange

### import mnist data ###

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot = True)

##### Begin Computational Graph #####

## initial variable values chosen for ease of use with ReLU ##

# input image vector and reshape to 28x28x1
# 28x28x1 is a single image
# the first dimension will be minibatch size
x = tf.placeholder(
    dtype = tf.float32,
    shape = [None, 784],
    name = ""x"")

xReshape = tf.reshape(x, [-1, 28, 28, 1])

# placeholder for data labels
y_ = tf.placeholder(
    dtype = tf.float32,
    shape = [None, 10],
    name = ""y_"")

### First Convolutional Layer ###

# define kernel for first convolution layer
# initial values are random small numbers
K1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32],
                                     stddev = 0.01))

# define bias for first convolution layer
# initial values of 0.1
b1 = tf.Variable(tf.ones([32]) / 10)

# perform convolution
C1 = tf.nn.conv2d(
    input = xReshape,
    filter = K1,
    strides = [1, 1, 1, 1],
    padding = ""SAME"") + b1

# use activation function
C1_act = tf.nn.relu(C1)

# 2x2 max pool
maxPool1 = tf.nn.max_pool(
    value = C1_act,
    ksize = [1,2,2,1],
    strides = [1,2,2,1],
    padding = ""SAME"")

### Second Convolutional Layer ###

# define kernel for first convolution layer
# initial values are random small numbers
K2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64],
                                     stddev = 0.01))

# define bias for first convolution layer
# initial values of 0.1
b2 = tf.Variable(tf.ones([64]) / 10)

# perform convolution
C2 = tf.nn.conv2d(
    input = maxPool1,
    filter = K2,
    strides = [1, 1, 1, 1],
    padding = ""SAME"") + b2

# use activation function
C2_act = tf.nn.relu(C2)

# 2x2 max pool
maxPool2 = tf.nn.max_pool(
    value = C2_act,
    ksize = [1,2,2,1],
    strides = [1,2,2,1],
    padding = ""SAME"")

### First Fully Connected Layer w/ 256 Hidden Units ###

# flatten maps into one vector
fVect = tf.reshape(maxPool2, [-1, 7 * 7 * 64])

W1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 256],
                                     stddev = 0.01))

fcBias1 = tf.Variable(tf.ones([256]) / 10)

prob_y1 = tf.nn.relu(tf.matmul(fVect, W1) + fcBias1)

### Final Fully Connected layer with 10 hidden Units ###

W2 = tf.Variable(tf.truncated_normal([256, 10],
                                     stddev = 0.01))

fcBias2 = tf.Variable(tf.ones([10]) / 10)

prob_y2 = tf.nn.softmax(logits = (tf.matmul(prob_y1, W2) + fcBias2))

### Loss Function and Optimizer ###

# define loss function
cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(prob_y2), axis = 1))

# set up gradient descent optimizer

train_step = tf.train.GradientDescentOptimizer(learning_rate =     0.05).minimize(cross_entropy_loss)

##### Train the Network #####

### start the session and initialize global variables ###

# Variable Initializer
init_op = tf.global_variables_initializer()

# Create a Session object, initialize all variables
sess = tf.Session()
sess.run(init_op)

for _ in trange(1000): 
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict = {x: batch_xs, y_: batch_ys})

### Test Prediction Accuracy ###

# test trained model
print(1)
correct_prediction = tf.equal(tf.argmax(prob_y2, axis = 1), tf.argmax(y_, axis = 1))
print(2)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(3)
print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict = {x: mnist.test.images, y_: mnist.test.labels})))

sess.close()
</code></pre>

<p>Apologies for the big code dump. I want make sure the issue is reproducible. The result of this code in my notebook is a pop-up window that says, ""The kernel appears to have died. It will restart automatically."" I'm hoping this is some small error in my syntax or something, but I've search all the functional documentation and forums and haven't identified my issue.</p>

<p>Any help is appreciated!</p>
","I'm relatively new to TensorFlow. I have built a Logistic Regression classifier and a MultiLayer Perceptron in the past that have worked. Now that I have moved on to the Convolutional Neural Network, I am having some problems with testing accuracy. My code is below. The line I am having trouble with is only the very last line where I am attempting to print the test accuracy figure. The print 1, 2, 3 statements are intended to show this. Apologies for the big code dump. I want make sure the issue is reproducible. The result of this code in my notebook is a pop-up window that says, ""The kernel appears to have died. It will restart automatically."" I'm hoping this is some small error in my syntax or something, but I've search all the functional documentation and forums and haven't identified my issue. Any help is appreciated!",https://stackoverflow.com/questions/53998796,4882698,Requesting (Additional) Resources
54041135,Multiclass U-Net segmentation in TensorFlow,"<p>I've seen variations of this question all over, but am still struggling to implement it correctly.  I have brain MRI images with ground-truth segmented masks with 4 classes (0- background, 1-tissue type1, 2-tissue type2, 3-inexplicably skipped, and 4-tissue type 4...BrATs dataset)</p>

<p><a href=""https://i.stack.imgur.com/IVMU5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IVMU5.png"" alt=""enter image description here""></a></p>

<p>I have a basic U-Net architecture implemented, but am having trouble extending it to non-binary classification.  Particularly, the loss function.</p>

<p>This is what I have implemented, but I'm obviously overlooking important details:</p>

<pre><code>[...]
output = tf.layers.conv2d_transpose(
conv18,
filters=5,
kernel_size=1,
strides=1,
padding='same',
data_format='channels_last',
activation=None,
use_bias=True,
kernel_initializer=None,
bias_initializer=tf.zeros_initializer(),
kernel_regularizer=tf.contrib.layers.l2_regularizer(reg),
bias_regularizer=None,
activity_regularizer=None,
kernel_constraint=None,
bias_constraint=None,
trainable=True,
name='output',
reuse=None
)
</code></pre>

<p>I thought 5 filters for the (0,1,2,3,4) possible mask values would be correct.  I then used the following loss function:</p>

<pre><code>loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
_sentinel=None,
labels=label,
logits=output,
name='cross_ent_loss'
)

return tf.reduce_mean(loss)
</code></pre>

<p>Where the logits would get passed the output from above, and the labels would be my stacked mask images [n_batch, x_dim, y_dim, 1]. Looking at the documentation, I know I am not passing labels the correct tensor.  </p>

<p>Am I even going about this correctly?  How do I implement the loss with multi-class labels contained within the 1 mask image?</p>
","I've seen variations of this question all over, but am still struggling to implement it correctly. I have brain MRI images with ground-truth segmented masks with 4 classes (0- background, 1-tissue type1, 2-tissue type2, 3-inexplicably skipped, and 4-tissue type 4...BrATs dataset) I have a basic U-Net architecture implemented, but am having trouble extending it to non-binary classification. Particularly, the loss function. This is what I have implemented, but I'm obviously overlooking important details: I thought 5 filters for the (0,1,2,3,4) possible mask values would be correct. I then used the following loss function: Where the logits would get passed the output from above, and the labels would be my stacked mask images [n_batch, x_dim, y_dim, 1]. Looking at the documentation, I know I am not passing labels the correct tensor. Am I even going about this correctly? How do I implement the loss with multi-class labels contained within the 1 mask image?",https://stackoverflow.com/questions/54041135,6749743,Requesting (Additional) Resources
54194053,"AttributeError: Layer has no inbound nodes, or AttributeError: The layer has never been called","<p>I need a way to get the shape of output tensor for any type of layer (i.e. Dense, Conv2D, etc) in TensorFlow. According to documentation, there is <code>output_shape</code> property which solves the problem. However every time I access it I get <code>AttributedError</code>.</p>

<p>Here is code sample showing the problem:</p>

<pre><code>import numpy as np
import tensorflow as tf


x = np.arange(0, 8, dtype=np.float32).reshape((1, 8))
x = tf.constant(value=x, dtype=tf.float32, verify_shape=True)

dense = tf.layers.Dense(units=2)

out = dense(x)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    res = sess.run(fetches=out)
    print(res)
    print(dense.output_shape)
</code></pre>

<p>The <code>print(dense.output_shape)</code> statement will produce error message:</p>

<pre><code>AttributeError: The layer has never been called and thus has no defined output shape.
</code></pre>

<p>or <code>print(dense.output)</code> will produce:</p>

<pre><code>AttributeError('Layer ' + self.name + ' has no inbound nodes.')
AttributeError: Layer dense_1 has no inbound nodes.
</code></pre>

<p>Is there any way to fix the error?</p>

<p><strong>P.S.:</strong>
I know that in example above I can get shape of output tensor via <code>out.get_shape()</code>. However I want to know why <code>output_shape</code> property doesn't work and how I can fix it?</p>
","I need a way to get the shape of output tensor for any type of layer (i.e. Dense, Conv2D, etc) in TensorFlow. According to documentation, there is output_shape property which solves the problem. However every time I access it I get AttributedError. Here is code sample showing the problem: The print(dense.output_shape) statement will produce error message: or print(dense.output) will produce: Is there any way to fix the error? P.S.: I know that in example above I can get shape of output tensor via out.get_shape(). However I want to know why output_shape property doesn't work and how I can fix it?",https://stackoverflow.com/questions/54194053,9565342,Documentation Replication on Other Examples
54521572,How to transform keras model to tpu model,"<p>I am trying to transform my Keras model in the Google cloud console into a TPU model. Unfortunatelly I am getting an error as shown below. My minimal example is the following:</p>

<pre><code>import keras
from keras.models import Sequential
from keras.layers import Dense, Activation
import tensorflow as tf
import os
model = Sequential()
model.add(Dense(32, input_dim=784))
model.add(Dense(32))
model.add(Activation('relu'))
model.compile(optimizer='rmsprop', loss='mse')
tpu_model = tf.contrib.tpu.keras_to_tpu_model(
    model,
    strategy=tf.contrib.tpu.TPUDistributionStrategy(
         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))
</code></pre>

<p>My output is:</p>

<pre><code>Using TensorFlow backend.
Traceback (most recent call last):
     File ""cloud_python4.py"", line 11, in &lt;module&gt;
     tpu_model = tf.contrib.tpu.keras_to_tpu_model(AttributeError: module 'tensorflow.contrib.tpu' has no attribute 'keras_to_tpu_model'
</code></pre>

<p>The keras_to_tpu_model method seems experimental as indicated on the tensorflow website. Has it recently been removed? If so, how can I proceed to make use of TPUs to estimate my Keras model? If the keras_to_tpu_model method would be still available, why can I not invoke it?</p>
","I am trying to transform my Keras model in the Google cloud console into a TPU model. Unfortunatelly I am getting an error as shown below. My minimal example is the following: My output is: The keras_to_tpu_model method seems experimental as indicated on the tensorflow website. Has it recently been removed? If so, how can I proceed to make use of TPUs to estimate my Keras model? If the keras_to_tpu_model method would be still available, why can I not invoke it?",https://stackoverflow.com/questions/54521572,6493472,Documentation Completeness
54657542,How to get subset of 10K MNIST images from Dataset class in tensorflow?,"<p>I found the following way to get mnist dataset in tensorflow:</p>

<pre><code>def get_input_fn(dataset_split, batch_size, capacity=10000, min_after_dequeue=3000):

  def _input_fn():
    images_batch, labels_batch = tf.train.shuffle_batch(
        tensors=[dataset_split.images, dataset_split.labels.astype(np.int32)],
        batch_size=batch_size,
        capacity=capacity,
        min_after_dequeue=min_after_dequeue,
        enqueue_many=True,
        num_threads=4)
    features_map = {'images': images_batch}
    return features_map, labels_batch

  return _input_fn

    data = tf.contrib.learn.datasets.mnist.load_mnist()

    train_input_fn = get_input_fn(data.train, batch_size=256)
    eval_input_fn = get_input_fn(data.validation, batch_size=5000)
</code></pre>

<p>data variable is Dataset object. 
This approach is quite unclear to me and I cannot figure out how to convert 60K dataset into 10K dataset.</p>

<p>When I do the following:</p>

<pre><code>data = tf.contrib.learn.datasets.mnist.load_mnist().take(10000)
</code></pre>

<p>I get error:</p>

<pre><code>AttributeError: 'Datasets' object has no attribute 'take'
</code></pre>

<p>But docs provide this method:
<a href=""https://i.stack.imgur.com/o6v4Q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o6v4Q.png"" alt=""enter image description here""></a></p>

<p>Thank you for help!</p>
",I found the following way to get mnist dataset in tensorflow: data variable is Dataset object. This approach is quite unclear to me and I cannot figure out how to convert 60K dataset into 10K dataset. When I do the following: I get error: But docs provide this method: Thank you for help!,https://stackoverflow.com/questions/54657542,3849781,Documentation Replicability
55246768,Python: Multiply two or more numeric_column in tensorflow,"<p>I am new to tensorflow and could not find any documentation on how to define operations on <code>numeric_column</code>s.</p>

<p>I have 3 columns in my data (say) <code>col1</code>, <code>col2</code>, <code>col3</code>. All have numeric values. I want to multiply these and create a new column (say) 'product`. I tried the following.</p>

<pre><code>c1 = tf.feature_column.numeric_column('col1')
c2 = tf.feature_column.numeric_column('col2')
c3 = tf.feature_column.numeric_column('col3')

p = tf.multiply(c1, c2)
p = tf.multiple(p, c3)
</code></pre>

<p>When I run the snippet (via a unittest), I get </p>

<pre><code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.feature_column.feature_column._NumericColumn'&gt; to Tensor.
</code></pre>

<p>How else can I get it done?</p>
","I am new to tensorflow and could not find any documentation on how to define operations on numeric_columns. I have 3 columns in my data (say) col1, col2, col3. All have numeric values. I want to multiply these and create a new column (say) 'product`. I tried the following. When I run the snippet (via a unittest), I get How else can I get it done?",https://stackoverflow.com/questions/55246768,1306819,Lack of Alternative Solutions/Documentation
55421290,TensorFlow 2.0 Keras: How to write image summaries for TensorBoard,"<p>I'm trying to setup an image recognition CNN with TensorFlow 2.0. To be able to analyze my image augmentation I'd like to see the images I feed into the network in tensorboard.</p>

<p>Unfortunately, I cannot figure out, how to do this with TensorFlow 2.0 and Keras. I also didn't really find documentation on this.</p>

<p>For simplicity, I'm showing the code of an MNIST example. How would I add the image summary here?</p>

<pre><code>import tensorflow as tf
(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()

def scale(image, label):
    return tf.cast(image, tf.float32) / 255.0, label

def augment(image, label):
    return image, label  # do nothing atm

dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
dataset = dataset.map(scale).map(augment).batch(32)

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(dataset, epochs=5, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='D:\\tmp\\test')])
</code></pre>
","I'm trying to setup an image recognition CNN with TensorFlow 2.0. To be able to analyze my image augmentation I'd like to see the images I feed into the network in tensorboard. Unfortunately, I cannot figure out, how to do this with TensorFlow 2.0 and Keras. I also didn't really find documentation on this. For simplicity, I'm showing the code of an MNIST example. How would I add the image summary here?",https://stackoverflow.com/questions/55421290,820833,Lack of Alternative Solutions/Documentation
55868173,Accepting base64-images as input for TensorFlow model,"<p>I am trying to export my TensorFlow image-classifying model such that it accepts base64 strings as input. </p>

<p>I have tried to implement the solution that is provided on <a href=""https://stackoverflow.com/a/47944205/11415897"">this question</a>, however I am getting the following error:</p>

<blockquote>
  <p>""InvalidArgumentError: Shape must be rank 0 but is rank 1 for
  'DecodeJpeg_1' (op: 'DecodeJpeg') with input shapes: [?].""</p>
</blockquote>

<p>The error appears as a result of the code on line 4. </p>

<pre><code>export_dir = '~/models/1'
builder = saved_model_builder.SavedModelBuilder(export_dir)
image = tf.placeholder(dtype=tf.string, shape=[None], name='source')
decoded = tf.image.decode_jpeg(image)
scores = build_model(decoded)
signature = predict_signature_def(inputs={'image_bytes': image},
                                 outputs={'output': scores})

with K.get_session() as sess:
    builder.add_meta_graph_and_variables(sess=sess,
                                         tags=[tag_constants.SERVING],
                                         signature_def_map={'predict': signature})
    builder.save()
sess.close()
</code></pre>

<p>Also,</p>

<p>I see that on line 5, ""scores"" provides the output of the model based on the <code>build_model</code> function. However, I can't find in the original question's answers or in the TensorFlow documentation where this function comes from.</p>
","I am trying to export my TensorFlow image-classifying model such that it accepts base64 strings as input. I have tried to implement the solution that is provided on this question, however I am getting the following error: The error appears as a result of the code on line 4. Also, I see that on line 5, ""scores"" provides the output of the model based on the build_model function. However, I can't find in the original question's answers or in the TensorFlow documentation where this function comes from.",https://stackoverflow.com/questions/55868173,11415897,Documentation Completeness
56672695,How to specify variables in tensorflow simple save,"<p>I am trying unsuccessfully to save my tensorflow model using the simple save method.</p>

<p>I have built a model using keras and trained it successfully, with an accuracy of 88%.  I am now trying to save this model so we can serve it, but the function I need, simple save, isn't clear about how to specify the variables that get passed in.</p>

<p>The the session and the export directory is clear enough, but the inputs and outputs are mysterious.  I believe that because I've used Keras, these variables are hidden by the abstraction of keras and the documentation from Tensorflow on simple save offers no explanation.</p>

<p>As a hailmary, I set Z equal to y just to put something in there, but obviously that is wrong.  Do I need to set up an output variable Z, and if so, what type is it?</p>

<p>Not sure if this is enough code to get to the bottom of this.  Even getting pointed at the right docs would be a big boost.</p>

<pre><code>import tensorflow as tf
session =  tf.keras.backend.get_session()
export_dir = ""/Users/somedir/""
z = np.array([])
tf.saved_model.simple_save(session,
            export_dir,
            inputs={""x"": X, ""y"": y},
            outputs={""z"": z})
</code></pre>

<p>X is my dataset -- an array of all independent variables.  Y is the outcome (dependent variable). I don't have another candidate for z, so I set it to an empty array.  </p>

<p>I get AttributeError: 'numpy.ndarray' object has no attribute 'get_shape'</p>
","I am trying unsuccessfully to save my tensorflow model using the simple save method. I have built a model using keras and trained it successfully, with an accuracy of 88%. I am now trying to save this model so we can serve it, but the function I need, simple save, isn't clear about how to specify the variables that get passed in. The the session and the export directory is clear enough, but the inputs and outputs are mysterious. I believe that because I've used Keras, these variables are hidden by the abstraction of keras and the documentation from Tensorflow on simple save offers no explanation. As a hailmary, I set Z equal to y just to put something in there, but obviously that is wrong. Do I need to set up an output variable Z, and if so, what type is it? Not sure if this is enough code to get to the bottom of this. Even getting pointed at the right docs would be a big boost. X is my dataset -- an array of all independent variables. Y is the outcome (dependent variable). I don't have another candidate for z, so I set it to an empty array. I get AttributeError: 'numpy.ndarray' object has no attribute 'get_shape'",https://stackoverflow.com/questions/56672695,914863,Documentation Completeness
57114360,"TypeError: float() argument must be a string or a number, not 'builtin_function_or_method'","<p>I am running a CNN that check for images but does not classify. In fact, the output layer is a dense layer that have as argument the size of the images in the labels in 1d.</p>

<p>As shown below in the code, I am using model.fit_generator() instead of model.fit and when it comes to start training the model the following error comes up:</p>

<p>TypeError: float() argument must be a string or a number, not 
  'builtin_function_or_method'</p>

<p>I am not really getting why this is happening. 
Here attached is the summary of the model:</p>

<hr>

<h1>Layer (type)                 Output Shape              Param #</h1>

<p>conv2d_4 (Conv2D)            (None, 26, 877, 32)       544       </p>

<hr>

<p>activation_5 (Activation)    (None, 26, 877, 32)       0         </p>

<hr>

<p>max_pooling2d_4 (MaxPooling2 (None, 13, 438, 32)       0         </p>

<hr>

<p>conv2d_5 (Conv2D)            (None, 12, 437, 16)       2064      </p>

<hr>

<p>activation_6 (Activation)    (None, 12, 437, 16)       0         </p>

<hr>

<p>max_pooling2d_5 (MaxPooling2 (None, 6, 218, 16)        0         </p>

<hr>

<p>conv2d_6 (Conv2D)            (None, 5, 217, 8)         520       </p>

<hr>

<p>activation_7 (Activation)    (None, 5, 217, 8)         0         </p>

<hr>

<p>max_pooling2d_6 (MaxPooling2 (None, 2, 108, 8)         0         </p>

<hr>

<p>activation_8 (Activation)    (None, 2, 108, 8)         0         </p>

<hr>

<p>flatten_2 (Flatten)          (None, 1728)              0         </p>

<hr>

<p>dropout_2 (Dropout)          (None, 1728)              0         </p>

<hr>

<p>dense_2 (Dense)              (None, 19316)             33397364  </p>

<p>=================================================================</p>

<p>Total params: 33,400,492
Trainable params: 33,400,492
Non-trainable params: 0</p>

<hr>

<p>Any suggestions ? 
Thanks a lot in advance!</p>

<p>I have already looked up many of the online forums/websites but I don't seem to find one that suits my case. </p>

<pre><code>def generator(data_arr, batch_size = 10):

num = len(data_arr) 

if num % batch_size != 0 : 
    num = int(num/batch_size)

# Loop forever so the generator never terminates
while True: 

    for offset in range(0, num, batch_size):

        batch_samples = (data_arr[offset:offset+batch_size])

        samples = []
        labels = []

        for batch_sample in batch_samples:

            samples.append(batch_sample[0])
            labels.append((np.array(batch_sample[1].flatten)).transpose())

        X_ = np.array(samples)
        Y_ = np.array(labels)

        X_ = X_[:, :, :, newaxis]

        print(X_.shape)
        print(Y_.shape)

        yield (X_, Y_)

# compile and train the model using the generator function
train_generator = generator(training_data, batch_size = 10)
validation_generator = generator(val_data, batch_size = 10)

run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)

model = Sequential()

model.add(Conv2D(32, (4, 4), strides=(2, 2), input_shape = (55, 1756, 
1)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2, 2)))

model.add(Conv2D(16, (2, 2)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2, 2)))

model.add(Conv2D(8, (2, 2)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size = (2, 2)))

model.add(Activation('softmax'))
model.add(Flatten())  # this converts our 3D feature maps to 1D feature 
vectors
model.add(Dropout(0.3))
model.add(Dense(19316))

model.compile(loss = 'sparse_categorical_crossentropy',
              optimizer = 'adam',
              metrics = ['accuracy'],
              options = run_opts)

model.summary()

batch_size = 20
nb_epoch = 6

model.fit_generator(train_generator, 
                    steps_per_epoch = len(training_data) ,
                    epochs = nb_epoch,
                    validation_data = validation_generator,
                    validation_steps = len(val_data))
</code></pre>
","I am running a CNN that check for images but does not classify. In fact, the output layer is a dense layer that have as argument the size of the images in the labels in 1d. As shown below in the code, I am using model.fit_generator() instead of model.fit and when it comes to start training the model the following error comes up: TypeError: float() argument must be a string or a number, not 'builtin_function_or_method' I am not really getting why this is happening. Here attached is the summary of the model: conv2d_4 (Conv2D) (None, 26, 877, 32) 544 activation_5 (Activation) (None, 26, 877, 32) 0 max_pooling2d_4 (MaxPooling2 (None, 13, 438, 32) 0 conv2d_5 (Conv2D) (None, 12, 437, 16) 2064 activation_6 (Activation) (None, 12, 437, 16) 0 max_pooling2d_5 (MaxPooling2 (None, 6, 218, 16) 0 conv2d_6 (Conv2D) (None, 5, 217, 8) 520 activation_7 (Activation) (None, 5, 217, 8) 0 max_pooling2d_6 (MaxPooling2 (None, 2, 108, 8) 0 activation_8 (Activation) (None, 2, 108, 8) 0 flatten_2 (Flatten) (None, 1728) 0 dropout_2 (Dropout) (None, 1728) 0 dense_2 (Dense) (None, 19316) 33397364 ================================================================= Total params: 33,400,492 Trainable params: 33,400,492 Non-trainable params: 0 Any suggestions ? Thanks a lot in advance! I have already looked up many of the online forums/websites but I don't seem to find one that suits my case.",https://stackoverflow.com/questions/57114360,11782908,Inadequate Examples
57133866,"How to fix:""Expected shapes TensorShape([Dimension(None), Dimension(785)]) got dataset with shapes TensorShape([Dimension(785)])""","<p>So I am trying to build Fashion-MNIST CNN classifier using tensorflow. and I am getting this dimensionality error.I am a newbie to tensorflow.</p>

<p>I read tensorflow documentation but i didnt get solution to my problem.
I am sharing the code which i find related to the error.
Following is the entire traceback </p>

<pre><code>    Traceback (most recent call last):

    File ""&lt;ipython-input-2-8a4292875dbe&gt;"", line 208, in &lt;module&gt;
        model.build()

    File ""&lt;ipython-input-2-8a4292875dbe&gt;"", line 133, in build
        self.getdata()

    File ""&lt;ipython-input-2-8a4292875dbe&gt;"", line 65, in getdata
        self.test_data_init = iterator.make_initializer(test_data)    
    # initializer for test_data

    File ""/home/aditya/anaconda3/envs/train/lib/python3.7/site- 
    packages/tensorflow/python/data/ops/iterator_ops.py"", line 369, 
    in make_initializer
       (self.output_shapes, dataset_output_shapes))

    TypeError: Expected output shapes compatible with 
    TensorShape([Dimension(None), Dimension(785)]) but got dataset 
    with output shapes TensorShape([Dimension(785)]).
</code></pre>

<p>my model.build()</p>

<pre><code>    def build(self):
    '''
    Build the computation graph
    '''
    self.getdata()
    self.inference()
    self.loss()
    self.optimize()
    self.eval()
    self.summary()
</code></pre>

<p>my getdata()</p>

<pre><code>    def getdata(self):
    with tf.name_scope('data'):
        train_data=tf.data.Dataset.from_tensor_slices(train)
        train_data = train_data.shuffle(10000) # if you want to shuffle your data
        train_data = train_data.batch(self.batch_size)

        test_data=tf.data.Dataset.from_tensor_slices(test)           
iterator=tf.data.Iterator.from_structure(train_data.output_types,train_data.output_shapes)
        self.train_data_init = iterator.make_initializer(train_data)  # initializer for train_data
        self.test_data_init = iterator.make_initializer(test_data)    # initializer for test_data
        img, self.label = iterator.get_next()
        self.img = tf.reshape(img, shape=[-1, 28, 28, 1])
        # reshape the image to make it work with tf.nn.conv2d
</code></pre>

<p>I was expecting to find a way to reshape but I couldnt do it.Any help will be appreciated</p>
",So I am trying to build Fashion-MNIST CNN classifier using tensorflow. and I am getting this dimensionality error.I am a newbie to tensorflow. I read tensorflow documentation but i didnt get solution to my problem. I am sharing the code which i find related to the error. Following is the entire traceback my model.build() my getdata() I was expecting to find a way to reshape but I couldnt do it.Any help will be appreciated,https://stackoverflow.com/questions/57133866,6873425,Inadequate Examples
57346191,Tensorflow pad sequence feature column,"<p>How to pad sequences in the feature column and also what is a <code>dimension</code> in the <code>feature_column</code>.</p>

<p>I am using <code>Tensorflow 2.0</code> and implementing an example of text summarization. Pretty new to machine learning, deep learning, and TensorFlow.</p>

<p>I came across <code>feature_column</code> and found them useful as I think they can be embedded in the processing pipeline of the model.</p>

<p>In a classic scenario where not using <code>feature_column</code>, I can pre-process the text, tokenize it, convert it into a sequence of numbers and then pad them to a <code>maxlen</code> of say 100 words. I am not able to get this done when using the <code>feature_column</code>.</p>

<p>Below is what I have written sofar. </p>

<pre class=""lang-py prettyprint-override""><code>
train_dataset = tf.data.experimental.make_csv_dataset(
    'assets/train_dataset.csv', label_name=LABEL, num_epochs=1, shuffle=True, shuffle_buffer_size=10000, batch_size=1, ignore_errors=True)

vocabulary = ds.get_vocabulary()

def text_demo(feature_column):
    feature_layer = tf.keras.experimental.SequenceFeatures(feature_column)
    article, _ = next(iter(train_dataset.take(1)))

    tokenizer = tf_text.WhitespaceTokenizer()

    tokenized = tokenizer.tokenize(article['Text'])

    sequence_input, sequence_length = feature_layer({'Text':tokenized.to_tensor()})

    print(sequence_input)

def categorical_column(feature_column):
    dense_column = tf.keras.layers.DenseFeatures(feature_column)

    article, _ = next(iter(train_dataset.take(1)))

    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(
      filters='')
    lang_tokenizer.fit_on_texts(article)

    tensor = lang_tokenizer.texts_to_sequences(article)

    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,
                                                         padding='post', maxlen=50)

    print(dense_column(tensor).numpy())


text_seq_vocab_list = tf.feature_column.sequence_categorical_column_with_vocabulary_list(key='Text', vocabulary_list=list(vocabulary))
text_embedding = tf.feature_column.embedding_column(text_seq_vocab_list, dimension=8)
text_demo(text_embedding)

numerical_voacb_list = tf.feature_column.categorical_column_with_vocabulary_list(key='Text', vocabulary_list=list(vocabulary))
embedding = tf.feature_column.embedding_column(numerical_voacb_list, dimension=8)
categorical_column(embedding)

</code></pre>

<p>I am also confused as to what to use here, <code>sequence_categorical_column_with_vocabulary_list</code> or <code>categorical_column_with_vocabulary_list</code>. In the documentation, <code>SequenceFeatures</code> is also not explained, all though I know it is an experimental feature.</p>

<p>I am also not able to understand what does <code>dimension</code> param do?</p>
","How to pad sequences in the feature column and also what is a dimension in the feature_column. I am using Tensorflow 2.0 and implementing an example of text summarization. Pretty new to machine learning, deep learning, and TensorFlow. I came across feature_column and found them useful as I think they can be embedded in the processing pipeline of the model. In a classic scenario where not using feature_column, I can pre-process the text, tokenize it, convert it into a sequence of numbers and then pad them to a maxlen of say 100 words. I am not able to get this done when using the feature_column. Below is what I have written sofar. I am also confused as to what to use here, sequence_categorical_column_with_vocabulary_list or categorical_column_with_vocabulary_list. In the documentation, SequenceFeatures is also not explained, all though I know it is an experimental feature. I am also not able to understand what does dimension param do?",https://stackoverflow.com/questions/57346191,1561188,Lack of Alternative Solutions/Documentation
57376829,"How can I filter and balance a Windowed Tensorflow dataset with a binary classification label, based on the label?","<p>I have an unbalanced tensorflow windowed dataset with labels (over 90% negative examples) which I am trying to balance by filtering. I am having trouble filtering as my windowed dataset with labels does not fall under the cases I have come across in my searches, or the tensorflow documentation.</p>

<p>I'm working on a model to predict a binary classification based on time series data. I start with a time series dataframe with a number of columns (price, volume, etc.) where each row is one minute.</p>

<p>Currently I am still stuck on filtering the different labels. My next step after filtering would be to get the size of both filtered datasets, find the smaller size (n), and then concatenate the smaller dataset with (n) elements from the bigger dataset, after shuffling the bigger dataset. This way I would have a balanced dataset with an equal number of 1 and 0 labels. If you have a better Idea I would be happy to hear it.</p>

<p><strong>EXPLAINING MY CODE:</strong>
DFrame is a pandas dataframe with columns such as price, volume, etc., and each row is a different minute, with the first row being the earliest/oldest time period. The last column of DFrame is the classifier 0 or 1.</p>

<p>I then create a tensorflow dataset from slices with the first input being all the DFrame columns except for the label which is in the last column, and the second input (the label) being the last column which is the classifier.</p>

<p>I then use the window function to create windows of size (hindsight) which is currently 512, meaning (If i'm not mistaken) it takes the previous 511 minutes as well as the current minute and uses this as a rolling window to associate with the label of the current minute. So my understanding is that x is then an array of 512 arrays, from the row of the current minute to the row of 511 minutes ago, and the y is the label of the current minute. So x is an array of 512 arrays (rows for each minute, from the dataframe), and y is just one integer, 1 or 0.</p>

<p>Ideally I would like to be able to apply the same balancing logic to a multiclass classification problem, where I essentially add additional labels for additional price movement ranges.</p>

<p>The error comes from the filter. The model seems to run without that, and even trains my keras model. as explained I would actually want to add more code after the filter once I get it working to balance the dataset but I need to filter it first.</p>

<pre><code>tensor= tf.data.Dataset.from_tensor_slices((tf.constant(DFrame[DFrame.columns.values[:-1]].values), tf.constant(DFrame[DFrame.columns.values[-1]].values)))

tensor = tensor.window(hindsight,1,1,True)

tensor = tensor.shuffle(1000)

tensor = tensor.filter(lambda x,y: tf.equal(y, 0))

tensor = tensor.flat_map(lambda x,y:tf.data.Dataset.zip((x.batch(hindsight), y.batch(1))))

tensor = tensor.batch(Batch_size).prefetch(1)



TypeError: Failed to convert object of type &lt;class 'tensorflow.python.data.ops.dataset_ops._VariantDataset'&gt; to Tensor. Contents: &lt;_VariantDataset shapes: (), types: tf.int64&gt;. Consider casting elements to a supported type.
</code></pre>
","I have an unbalanced tensorflow windowed dataset with labels (over 90% negative examples) which I am trying to balance by filtering. I am having trouble filtering as my windowed dataset with labels does not fall under the cases I have come across in my searches, or the tensorflow documentation. I'm working on a model to predict a binary classification based on time series data. I start with a time series dataframe with a number of columns (price, volume, etc.) where each row is one minute. Currently I am still stuck on filtering the different labels. My next step after filtering would be to get the size of both filtered datasets, find the smaller size (n), and then concatenate the smaller dataset with (n) elements from the bigger dataset, after shuffling the bigger dataset. This way I would have a balanced dataset with an equal number of 1 and 0 labels. If you have a better Idea I would be happy to hear it. EXPLAINING MY CODE: DFrame is a pandas dataframe with columns such as price, volume, etc., and each row is a different minute, with the first row being the earliest/oldest time period. The last column of DFrame is the classifier 0 or 1. I then create a tensorflow dataset from slices with the first input being all the DFrame columns except for the label which is in the last column, and the second input (the label) being the last column which is the classifier. I then use the window function to create windows of size (hindsight) which is currently 512, meaning (If i'm not mistaken) it takes the previous 511 minutes as well as the current minute and uses this as a rolling window to associate with the label of the current minute. So my understanding is that x is then an array of 512 arrays, from the row of the current minute to the row of 511 minutes ago, and the y is the label of the current minute. So x is an array of 512 arrays (rows for each minute, from the dataframe), and y is just one integer, 1 or 0. Ideally I would like to be able to apply the same balancing logic to a multiclass classification problem, where I essentially add additional labels for additional price movement ranges. The error comes from the filter. The model seems to run without that, and even trains my keras model. as explained I would actually want to add more code after the filter once I get it working to balance the dataset but I need to filter it first.",https://stackoverflow.com/questions/57376829,11743037,Requesting (Additional) Resources
57510115,cifar100 with MobiletNetV2,"<p>I am trying to train MobileNetV2 on CIFAR100 using keras.applications
Here is my code:</p>

<pre><code>(x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')

x_test = x_test.astype(""float32"")
x_train = x_train.astype(""float32"")

x_test /=255
x_train /=255

y_test = tf.keras.utils.to_categorical(y_test,100)
y_train = tf.keras.utils.to_categorical(y_train,100)
model = MobileNetV2(input_shape=(32,32,3),
                    alpha=1.0,
                    include_top=True,
                    weights=None,
                    classes=100)
epochs = 200
batch_size = 64
print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:
datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        zca_epsilon=1e-06,  # epsilon for ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        # randomly shift images horizontally (fraction of total width)
        width_shift_range=0.1,
        # randomly shift images vertically (fraction of total height)
        height_shift_range=0.1,
        shear_range=0.,  # set range for random shear
        zoom_range=0.,  # set range for random zoom
        channel_shift_range=0.,  # set range for random channel shifts
        # set mode for filling points outside the input boundaries
        fill_mode='nearest',
        cval=0.,  # value used for fill_mode = ""constant""
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False,  # randomly flip images
        # set rescaling factor (applied before any other transformation)
        rescale=None,
        # set function that will be applied on each input
        preprocessing_function=None,
        # image data format, either ""channels_first"" or ""channels_last""
        data_format=None,
        # fraction of images reserved for validation (strictly between 0 and 1)
        validation_split=0.0)
datagen.fit(x_train)

model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['acc'])
history = model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),
                        epochs=epochs,
                        validation_data=(x_test, y_test)
                             )
</code></pre>

<p>The issue is with the validation accuracy, after 200 epochs the acc is almost 40%. 
I tried to fine_tune the optimizer/loss params but still the same. My guess is the dim of the input is too small for the model as the default is 224*224, however according to the documentation you could use whatever you want!</p>

<p>Any advice? (I do not want to change the dim of cifar100 to 224*224 because of some assumptions related to this experiment)!</p>
","I am trying to train MobileNetV2 on CIFAR100 using keras.applications Here is my code: The issue is with the validation accuracy, after 200 epochs the acc is almost 40%. I tried to fine_tune the optimizer/loss params but still the same. My guess is the dim of the input is too small for the model as the default is 224*224, however according to the documentation you could use whatever you want! Any advice? (I do not want to change the dim of cifar100 to 224*224 because of some assumptions related to this experiment)!",https://stackoverflow.com/questions/57510115,458105,Requesting (Additional) Resources
57721804,"setting sample_weight_mode=""temporal"" doesn't seem to work","<p>I am working with LSTM in tensorflow 2.0 and I am trying to assign a weight to the training samples (I've already tried with class_weights dictionary but it complains that 3-d arrays are not supported for it. my array shape is (26000, 7, 1)).</p>

<p>As suggested in the documentation, I am setting sample_weight_mode to ""temporal"" in compile, but after that when I try to fit the model I still get the error below:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-100-1bb94008291c&gt; in &lt;module&gt;
      1 with tf.device(""/device:GPU:0""):
----&gt; 2     model.fit(X, y, epochs=500, batch_size=4096, verbose=1, validation_split=0.2, sample_weight=cw.reshape(26000,7,1))

C:\ProgramData\Anaconda3\envs\thesis-gpu\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    641         max_queue_size=max_queue_size,
    642         workers=workers,
--&gt; 643         use_multiprocessing=use_multiprocessing)
    644 
    645   def evaluate(self,

C:\ProgramData\Anaconda3\envs\thesis-gpu\lib\site-packages\tensorflow\python\keras\engine\training_arrays.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    630         steps=steps_per_epoch,
    631         validation_split=validation_split,
--&gt; 632         shuffle=shuffle)
    633 
    634     if validation_data:

C:\ProgramData\Anaconda3\envs\thesis-gpu\lib\site-packages\tensorflow\python\keras\engine\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2459           training_utils.standardize_weights(ref, sw, cw, mode)
   2460           for (ref, sw, cw, mode) in zip(y, sample_weights, class_weights,
-&gt; 2461                                          feed_sample_weight_modes)
   2462       ]
   2463       # Check that all arrays have the same length.

C:\ProgramData\Anaconda3\envs\thesis-gpu\lib\site-packages\tensorflow\python\keras\engine\training.py in &lt;listcomp&gt;(.0)
   2458       sample_weights = [
   2459           training_utils.standardize_weights(ref, sw, cw, mode)
-&gt; 2460           for (ref, sw, cw, mode) in zip(y, sample_weights, class_weights,
   2461                                          feed_sample_weight_modes)
   2462       ]

C:\ProgramData\Anaconda3\envs\thesis-gpu\lib\site-packages\tensorflow\python\keras\engine\training_utils.py in standardize_weights(y, sample_weight, class_weight, sample_weight_mode)
    839     if sample_weight is not None and len(sample_weight.shape) != 1:
    840       raise ValueError('Found a sample_weight array with shape ' +
--&gt; 841                        str(sample_weight.shape) + '. '
    842                        'In order to use timestep-wise sample weights, '
    843                        'you should specify '

ValueError: Found a sample_weight array with shape (26000, 7, 1). In order to use timestep-wise sample weights, you should specify sample_weight_mode=""temporal"" in compile(). If you just mean to use sample-wise weights, make sure your sample_weight array is 1D.
</code></pre>

<p>I have already tried changing the shape of the sample_weights passed in fit (also tried to flatten it), setting shuffle to False and removing the validation split without success.</p>

<p>Here's the code I am using</p>

<pre><code>import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import math

# Import TensorFlow
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop, Adam, SGD
from tensorflow.keras.layers import SimpleRNN, LSTM, Dense
from tensorflow.keras.models import Sequential

from sklearn.utils import class_weight

class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(y_train),
                                                 y_train)

cw = class_weight.compute_sample_weight({False:1, True:50},#'balanced',
                                                 #np.unique(y_train),
                                                 y.flatten())

model = Sequential()
model.add(LSTM(160, return_sequences=True, dropout=0.05))
model.add(LSTM(80, return_sequences=True, dropout=0.05, activation='relu'))
model.add(LSTM(40, return_sequences=True, dropout=0.05, activation='relu'))
model.add(LSTM(10, return_sequences=True, dropout=0.05))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', tf.keras.metrics.AUC()], sample_weight_mode=""temporal"")

with tf.device(""/device:GPU:0""):
    model.fit(X, y, epochs=500, batch_size=4096, verbose=1, validation_split=0.2, sample_weight=cw.reshape(26000,7,1))
</code></pre>
","I am working with LSTM in tensorflow 2.0 and I am trying to assign a weight to the training samples (I've already tried with class_weights dictionary but it complains that 3-d arrays are not supported for it. my array shape is (26000, 7, 1)). As suggested in the documentation, I am setting sample_weight_mode to ""temporal"" in compile, but after that when I try to fit the model I still get the error below: I have already tried changing the shape of the sample_weights passed in fit (also tried to flatten it), setting shuffle to False and removing the validation split without success. Here's the code I am using",https://stackoverflow.com/questions/57721804,9961790,Documentation Replication on Other Examples
58100071,What is the correct way to implement a 'useless loss' with Keras?,"<p>I have a Keras model that has two outputs:</p>

<ul>
<li><p><code>output</code> is the true output of the network on which the loss is going to be computed</p></li>
<li><p><code>additional</code> is used to make an external task during inference (no loss should be computed with this output)</p></li>
</ul>

<p>When I build the model, I write something like that:</p>

<pre><code>model = Model(inputs=inp, outputs=[output, additional])
</code></pre>

<p>Since my <code>Model</code> has two outputs, I need to provide two losses when compiling the model so I created a useless loss like this:</p>

<pre><code>class NoopLoss(object):

    def __call__(self, y_true, y_pred, **kwargs):
        return self.compute_loss(y_true, y_pred)

    def compute_loss(self, y_true, y_pred):
        return tf.math.square(0.0)

</code></pre>

<p>Which I integrate in the compile step like this:</p>

<pre><code>loss = UsefulLoss()  # the real loss I'm using
noop_loss = NoopLoss()

model.compile(loss=[loss, noop_loss], optimizer=optimizer, metrics=['binary_accuracy'])
</code></pre>

<p>It works, but I feel it is a bit hackish, is there a correct way to implement this behavior? I didn't find any official useless loss in the Keras documentation.</p>
","I have a Keras model that has two outputs: When I build the model, I write something like that: Since my Model has two outputs, I need to provide two losses when compiling the model so I created a useless loss like this: Which I integrate in the compile step like this: It works, but I feel it is a bit hackish, is there a correct way to implement this behavior? I didn't find any official useless loss in the Keras documentation.",https://stackoverflow.com/questions/58100071,2206908,Requesting (Additional) Resources
58254297,How to check gradient computation via unit test,"<p>I am trying to unit-test a custom layer. Writing the feed-forward test was pretty straight forward, but I have no idea how to implement the test for gradients.</p>

<p>I found out there is a function in the tensorflow test package called <code>compute_gradient</code> but I can't find any resource on how to use it. The documentation basically says it computes the gradients (jacobian matrix) which is what I want, but when I try to use it, I get <code>EagerTensor is not callable</code></p>

<p>This I the code that fails:</p>

<pre class=""lang-py prettyprint-override""><code>class LayerGradientTest(tf.test.TestCase):
    def test_gradient(self):
        with self.test_session():
            input_tensor = [...]
            expected_output = [...]
            expected_gradients = [...]
            test_layer = MyLayer()
            output_tensor = test_layer(tf.Variable(input_tensor))
            grad_computed = tf.test.compute_gradient(output_tensor, expected_output)
            self.assertAllEqual(grad_computed, expected_gradients)

</code></pre>

<p>I would expect the test to either pass or fail at the assertion but I get a
<code>TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable</code> from <code>compute_gradient</code></p>

<p><strong>Edit:</strong>
Of course gradients need a loss function, I'm an idiot... but still the output is of nonsense shape. I now use the following code:</p>

<pre class=""lang-py prettyprint-override""><code>function = tf.losses.mean_squared_error
grad_computed = tf.test.compute_gradient(function, [output_tensor, expected_output])
</code></pre>

<p>The input's shapes to my layer are (1, 2, 2, 3) and (1, 2, 2, 2) but the gradients are a zip object of 4 12x4 matrices but since I have no parameters in my layer I expected to get the error values at the input. Please correct me if I messed something up again. Just to clarify, my layer is just transforming data and therefore has no gradients on its own but must propagate them backwards correctly.</p>
","I am trying to unit-test a custom layer. Writing the feed-forward test was pretty straight forward, but I have no idea how to implement the test for gradients. I found out there is a function in the tensorflow test package called compute_gradient but I can't find any resource on how to use it. The documentation basically says it computes the gradients (jacobian matrix) which is what I want, but when I try to use it, I get EagerTensor is not callable This I the code that fails: I would expect the test to either pass or fail at the assertion but I get a TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable from compute_gradient Edit: Of course gradients need a loss function, I'm an idiot... but still the output is of nonsense shape. I now use the following code: The input's shapes to my layer are (1, 2, 2, 3) and (1, 2, 2, 2) but the gradients are a zip object of 4 12x4 matrices but since I have no parameters in my layer I expected to get the error values at the input. Please correct me if I messed something up again. Just to clarify, my layer is just transforming data and therefore has no gradients on its own but must propagate them backwards correctly.",https://stackoverflow.com/questions/58254297,12044023,Inadequate Examples
58259247,"object is not callable, when using tf.optimizers.Adam.minimize()","<p>I am new to tensorflow (2.0), so i wanted to ease with a simple linear regression. I have the following code but i dont know why it is wrong .</p>

<p>I have tried with the documentation but so far i have no answer.</p>

<pre class=""lang-py prettyprint-override""><code>x = np.random.normal(loc=10., scale = 0.1, size=170)
y = np.repeat(10.,170)
a_init = tf.random_normal_initializer()
a = tf.Variable(initial_value=a_init(shape = [1], dtype = 'float32'),trainable=True)
pred = tf.multiply(a,x)
loss = tf.nn.l2_loss(pred-y)
optim = tf.optimizers.Adam(lr = 0.002)
entreno = optim.minimize(loss, [a])
</code></pre>

<p>I get the following error,</p>

<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last)
&lt;ipython-input-45-e1a191781d0a&gt; in &lt;module&gt;
      2 loss = tf.nn.l2_loss(pred-y)
      3 optim = tf.optimizers.Adam(lr = 0.002)
----&gt; 4 entreno = optim.minimize(loss, [a])

TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable
</code></pre>

<p>If it helps i have a tensorflow 1 code:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow
import numpy as np
tf = tensorflow.compat.v1
x = np.random.normal(loc=1.,scale=0.1, size = 220)
y = np.repeat(14.37,220)
tf.disable_eager_execution()
x_d = tf.placeholder(shape = [1], dtype=tf.float32)
y_t = tf.placeholder(shape = [1], dtype = tf.float32)
A = tf.Variable(tf.random_normal(shape=[1]))
my_pred = tf.multiply(A,x_d)
loss = tf.square(my_pred-y_t)
optim = tf.train.GradientDescentOptimizer(learning_rate=0.02)
train_step = optim.minimize(loss)
init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
for _ in range(241):
    idx = np.random.choice(220)
    ranx = [x[idx]]
    rany = [y[idx]]
    session.run(train_step, feed_dict ={x_d : ranx, y_t : rany})
    if _%20 == 0:
        print(""A = {}, Loss : {}"".format(session.run(A), session.run(loss, feed_dict={x_d:ranx, y_t:rany})))

</code></pre>
","I am new to tensorflow (2.0), so i wanted to ease with a simple linear regression. I have the following code but i dont know why it is wrong . I have tried with the documentation but so far i have no answer. I get the following error, If it helps i have a tensorflow 1 code:",https://stackoverflow.com/questions/58259247,10942287,Documentation Replication on Other Examples
58527048,Accessing intermediate layers from a loaded saved_model in Tensorflow 2.0,"<p>When using SavedModels in Tensorflow 2.0, is it possible to access activations from intermediate layers? For example, with one of the models here: <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a>, I can run, for example,</p>

<pre><code>model = tf.saved_model.load('faster_rcnn_inception_v2_coco_2018_01_28/saved_model').signatures['serving_default']
outputs = model(input_tensor)
</code></pre>

<p>to get output predictions and bounding boxes. I would like to be able to access layers other than the outputs, but there doesn't seem to be any documentation for Tensorflow 2.0 on how to do this. The downloaded models also include checkpoint files, but there doesn't seem to be very good documentation for how to load those with Tensorflow 2.0 either...</p>
","When using SavedModels in Tensorflow 2.0, is it possible to access activations from intermediate layers? For example, with one of the models here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md, I can run, for example, to get output predictions and bounding boxes. I would like to be able to access layers other than the outputs, but there doesn't seem to be any documentation for Tensorflow 2.0 on how to do this. The downloaded models also include checkpoint files, but there doesn't seem to be very good documentation for how to load those with Tensorflow 2.0 either...",https://stackoverflow.com/questions/58527048,11357382,Lack of Alternative Solutions/Documentation
58571072,How to load local images in tensorflow?,"<p>I found from the tensorflow documentation that the code to load a dataset named ""flower_photos "" is</p>

<blockquote>
  <p>data_dir = tf.keras.utils.get_file(origin='<a href=""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"" rel=""nofollow noreferrer"">https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz</a>',
                                           fname='flower_photos', untar=True)</p>
</blockquote>

<p>I'm having some images in my local machine and want to load and use it for doing some neural network algorithms like CNN. How to load and preprocess a locally stored image in tensorflow?</p>
","I found from the tensorflow documentation that the code to load a dataset named ""flower_photos "" is I'm having some images in my local machine and want to load and use it for doing some neural network algorithms like CNN. How to load and preprocess a locally stored image in tensorflow?",https://stackoverflow.com/questions/58571072,6733836,Requesting (Additional) Resources
58644349,Can the bias of a dense layer be set to zero in Tensorflow?,"<p>I'm trying to implement a neural network in which I need the kernel multiplication with the input only. The dense layer in Tensorflow also adds bias which I am trying to set to zero. From the documentation the only variable that is available to play with is <code>bias_regularizer</code>. So I tried doing the following:</p>

<pre class=""lang-py prettyprint-override""><code>def make_zero(_):
    return np.zeros(21,)

out1 = tf.layers.dense(inputs=codeword, units=21, activation=None, bias_regularizer=make_zero)
</code></pre>

<p>But I still see the bias values are not zero. Is there any other method to achieve this?</p>
",I'm trying to implement a neural network in which I need the kernel multiplication with the input only. The dense layer in Tensorflow also adds bias which I am trying to set to zero. From the documentation the only variable that is available to play with is bias_regularizer. So I tried doing the following: But I still see the bias values are not zero. Is there any other method to achieve this?,https://stackoverflow.com/questions/58644349,6997665,Lack of Alternative Solutions/Documentation
58666537,Error while feeding tf.Dataset to fit(): KeyError: 'embedding_input',"<p>I'm using TensorFlow 2.0 Datasets to feed my model's fit function. Here is the code:</p>

<pre><code>def build_model(self):
    self.g_Model = Sequential()
    self.g_Model.add(Embedding(self.g_Max_features, output_dim=256))
    self.g_Model.add(LSTM(128))
    self.g_Model.add(Dropout(0.5))
    self.g_Model.add(Dense(1, activation='sigmoid'))
    self.g_Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

def train_model(self, filenames):
    lstm_feature_description = {
        'X': tf.io.FixedLenFeature(CONFIG.g_keras_lstm_max_document_length, tf.float32),
        'y': tf.io.FixedLenFeature((), tf.int64),
    }

    def _parse_lstm_function(example_proto):
        return tf.io.parse_single_example(serialized=example_proto, features=lstm_feature_description)

    self.build_model()

    # Start Preparing The Data
    raw_lstm_dataset = tf.data.TFRecordDataset(CONFIG.g_record_file_lstm)

    parsed_lstm_dataset = raw_lstm_dataset.map(_parse_lstm_function)
    parsed_lstm_dataset = parsed_lstm_dataset.shuffle(CONFIG.g_shuffle_s).batch(CONFIG.g_Batch_size)

    self.g_Model.fit(parsed_lstm_dataset, epochs=2)
</code></pre>

<p>But I receive the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""keras_lstm_v2.py"", line 79, in train_model
      1/Unknown - 0s 0s/step    self.g_Model.fit(parsed_lstm_dataset, epochs=2)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 324, in fit
    total_epochs=epochs)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 503, in _call
    self._initialize(args, kwds, add_initializers_to=initializer_map)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 408, in _initialize
    *args, **kwds))
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1848, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2150, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2041, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 915, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 358, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 66, in distributed_function
    model, input_iterator, mode)
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 118, in _prepare_feed_values
    inputs = [inputs[key] for key in model._feed_input_names]
  File ""venv_tf_new\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 118, in &lt;listcomp&gt;
    inputs = [inputs[key] for key in model._feed_input_names]
KeyError: 'embedding_input'
</code></pre>

<p>I've seen this <a href=""https://github.com/tensorflow/tensorflow/issues/19912"" rel=""nofollow noreferrer"">thread</a>, however it doesn't clarify things up for me. As far as I understood there is a problem with the loaded data, but according to documentation for Datasets it should work out of the box, so I couldn't figure out how to fix it.</p>

<p>Any help is appreciated. Thanks!</p>
","I'm using TensorFlow 2.0 Datasets to feed my model's fit function. Here is the code: But I receive the following error: I've seen this thread, however it doesn't clarify things up for me. As far as I understood there is a problem with the loaded data, but according to documentation for Datasets it should work out of the box, so I couldn't figure out how to fix it. Any help is appreciated. Thanks!",https://stackoverflow.com/questions/58666537,4541899,Documentation Ambiguity
58711366,Simple keras dense model freezes while fitting,"<p>I am learning NLP with Keras and I am going through a tutorial.  The code is the following:</p>

<pre><code>import tensorflow_datasets as tfds
imdb, info = tfds.load(""imdb_reviews"", with_info=True, as_supervised=True)


import numpy as np

train_data, test_data = imdb['train'], imdb['test']

training_sentences = []
training_labels = []

testing_sentences = []
testing_labels = []

# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()
for s,l in train_data:
  training_sentences.append(str(s.numpy()))
  training_labels.append(l.numpy())


training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)

vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type='post'
oov_tok = ""&lt;OOV&gt;""


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# CREATE AN INSTANCE OF THE Tokenizer.  WE DECIDE HOW MANY WORDS THE TOKENIZER WILL READ.

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)


# FIT THE TOKENIZER ON THE TEXTS. NOW THE TOKENIZER HAS SEEN THE WORDS IN THE TEXT.

tokenizer.fit_on_texts(training_sentences) # the training_sentences is a list of words.  Each word is considered a token.


# CREATE A DICTIONARY THAT INCLUDES ALL THE WORDS IN THE TEXT (UP TO THE MAXIMUM NUMBER DEFINED WHEN CREATING THE INSTANCE
# OF THE TOKENIZER)

word_index = tokenizer.word_index # the tokenizer creates a word_index with the words encountered in the text.  Each word
                                  # is assigned an integer which is the key while the word is the value.

# CONVERT THE SEQUENCES OF WORDS TO SEQUENCES OF INTEGERS

sequences = tokenizer.texts_to_sequences(training_sentences)  # the texts_to_sequences method converts the sequences of
                                            # words to sequences of integers using the key of each word in the dictionary

# PAD THE SEQUENCES OR TRUNCATE THEM ACCORDINGLY SO THAT ALL HAVE THE GIVEN max_length. NOW ALL SEQUENCES HAVE THE SAME LENGTH.

padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

# THE SAME FOR THE SEQUENCES WHICH WILL BE USED FOR TESTING

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)

# REVERSE THE DICTIONARY, MAKING KEYS THE WORDS AND VALUES THE INTEGERS WHICH REPRESENT THE WORDS

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])




# CREATE A FUNCTION THAT TURNS THE INTEGERS COMPRISING A SEQUENCE TO WORDS THUS DECODING THE SEQUENCE AND CONVERTING IT TO
# NATURAL LANGUAGE

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),

    # The Embedding layer gets as first argumnet the vocab_size which has been set to 10,000 and which was the value
    # passed to the Tokenizer.  On the other hand the vocabulary that was created using the training text was less
    # than vocab_size, it was 86539

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()


num_epochs = 10
model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))
</code></pre>

<p>Towards the end of the first epoch the model freezes and makes no further progress:</p>

<p><a href=""https://i.stack.imgur.com/2YM0N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2YM0N.png"" alt=""enter image description here""></a></p>

<p>When I dropped the last 1,000 sentences and repeated the process I got the same situation but now at an earlier point:</p>

<p><a href=""https://i.stack.imgur.com/wUJbO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wUJbO.png"" alt=""enter image description here""></a></p>

<p>I restarted my PC (Windows 10) but this did not solve the problem.</p>

<p>I then uninstalled tensorflow and reinstalled.  Then I run the following code found in the official documentation of tensorflow 2.0:</p>

<p><a href=""https://i.stack.imgur.com/pE6mL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pE6mL.png"" alt=""enter image description here""></a></p>

<p>But when I rerun the NLP code again the model froze while fitting the data:</p>

<pre><code>num_epochs = 10
model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))
Train on 24000 samples
Epoch 1/10
23968/24000 [============================&gt;.] - ETA: 6:09 - loss: 0.6878 - accuracy: 0.53 - ETA: 1:22 - loss: 0.6904 - accuracy: 0.56 - ETA: 50s - loss: 0.6927 - accuracy: 0.5069 - ETA: 37s - loss: 0.6932 - accuracy: 0.495 - ETA: 31s - loss: 0.6925 - accuracy: 0.492 - ETA: 26s - loss: 0.6923 - accuracy: 0.492 - ETA: 24s - loss: 0.6925 - accuracy: 0.490 - ETA: 21s - loss: 0.6925 - accuracy: 0.493 - ETA: 20s - loss: 0.6929 - accuracy: 0.490 - ETA: 19s - loss: 0.6931 - accuracy: 0.487 - ETA: 18s - loss: 0.6929 - accuracy: 0.490 - ETA: 17s - loss: 0.6929 - accuracy: 0.492 - ETA: 16s - loss: 0.6930 - accuracy: 0.489 - ETA: 15s - loss: 0.6927 - accuracy: 0.494 - ETA: 15s - loss: 0.6925 - accuracy: 0.498 - ETA: 14s - loss: 0.6925 - accuracy: 0.501 - ETA: 14s - loss: 0.6924 - accuracy: 0.504 - ETA: 14s - loss: 0.6925 - accuracy: 0.502 - ETA: 13s - loss: 0.6926 - accuracy: 0.503 - ETA: 13s - loss: 0.6925 - accuracy: 0.503 - ETA: 13s - loss: 0.6924 - accuracy: 0.506 - ETA: 12s - loss: 0.6926 - accuracy: 0.506 - ETA: 12s - loss: 0.6924 - accuracy: 0.508 - ETA: 12s - loss: 0.6924 - accuracy: 0.508 - ETA: 12s - loss: 0.6922 - accuracy: 0.508 - ETA: 12s - loss: 0.6920 - accuracy: 0.509 - ETA: 11s - loss: 0.6921 - accuracy: 0.509 - ETA: 11s - loss: 0.6917 - accuracy: 0.514 - ETA: 11s - loss: 0.6917 - accuracy: 0.513 - ETA: 11s - loss: 0.6918 - accuracy: 0.512 - ETA: 11s - loss: 0.6915 - accuracy: 0.515 - ETA: 11s - loss: 0.6911 - accuracy: 0.517 - ETA: 10s - loss: 0.6911 - accuracy: 0.517 - ETA: 10s - loss: 0.6911 - accuracy: 0.516 - ETA: 10s - loss: 0.6910 - accuracy: 0.517 - ETA: 10s - loss: 0.6909 - accuracy: 0.517 - ETA: 10s - loss: 0.6907 - accuracy: 0.516 - ETA: 10s - loss: 0.6902 - accuracy: 0.518 - ETA: 10s - loss: 0.6900 - accuracy: 0.518 - ETA: 9s - loss: 0.6896 - accuracy: 0.518 - ETA: 9s - loss: 0.6898 - accuracy: 0.51 - ETA: 9s - loss: 0.6893 - accuracy: 0.51 - ETA: 9s - loss: 0.6891 - accuracy: 0.52 - ETA: 9s - loss: 0.6887 - accuracy: 0.52 - ETA: 9s - loss: 0.6883 - accuracy: 0.52 - ETA: 9s - loss: 0.6880 - accuracy: 0.52 - ETA: 9s - loss: 0.6878 - accuracy: 0.52 - ETA: 9s - loss: 0.6874 - accuracy: 0.52 - ETA: 8s - loss: 0.6868 - accuracy: 0.52 - ETA: 8s - loss: 0.6863 - accuracy: 0.52 - ETA: 8s - loss: 0.6857 - accuracy: 0.52 - ETA: 8s - loss: 0.6853 - accuracy: 0.52 - ETA: 8s - loss: 0.6851 - accuracy: 0.52 - ETA: 8s - loss: 0.6845 - accuracy: 0.52 - ETA: 8s - loss: 0.6838 - accuracy: 0.53 - ETA: 8s - loss: 0.6829 - accuracy: 0.53 - ETA: 8s - loss: 0.6819 - accuracy: 0.53 - ETA: 8s - loss: 0.6803 - accuracy: 0.53 - ETA: 8s - loss: 0.6799 - accuracy: 0.53 - ETA: 8s - loss: 0.6791 - accuracy: 0.54 - ETA: 7s - loss: 0.6785 - accuracy: 0.54 - ETA: 7s - loss: 0.6779 - accuracy: 0.54 - ETA: 7s - loss: 0.6773 - accuracy: 0.54 - ETA: 7s - loss: 0.6765 - accuracy: 0.55 - ETA: 7s - loss: 0.6751 - accuracy: 0.55 - ETA: 7s - loss: 0.6745 - accuracy: 0.55 - ETA: 7s - loss: 0.6732 - accuracy: 0.55 - ETA: 7s - loss: 0.6722 - accuracy: 0.56 - ETA: 7s - loss: 0.6714 - accuracy: 0.56 - ETA: 7s - loss: 0.6699 - accuracy: 0.56 - ETA: 7s - loss: 0.6691 - accuracy: 0.56 - ETA: 7s - loss: 0.6680 - accuracy: 0.57 - ETA: 7s - loss: 0.6667 - accuracy: 0.57 - ETA: 6s - loss: 0.6657 - accuracy: 0.57 - ETA: 6s - loss: 0.6642 - accuracy: 0.57 - ETA: 6s - loss: 0.6632 - accuracy: 0.57 - ETA: 6s - loss: 0.6624 - accuracy: 0.58 - ETA: 6s - loss: 0.6614 - accuracy: 0.58 - ETA: 6s - loss: 0.6598 - accuracy: 0.58 - ETA: 6s - loss: 0.6591 - accuracy: 0.58 - ETA: 6s - loss: 0.6580 - accuracy: 0.59 - ETA: 6s - loss: 0.6573 - accuracy: 0.59 - ETA: 6s - loss: 0.6563 - accuracy: 0.59 - ETA: 6s - loss: 0.6554 - accuracy: 0.59 - ETA: 6s - loss: 0.6548 - accuracy: 0.59 - ETA: 6s - loss: 0.6538 - accuracy: 0.60 - ETA: 6s - loss: 0.6524 - accuracy: 0.60 - ETA: 6s - loss: 0.6521 - accuracy: 0.60 - ETA: 5s - loss: 0.6506 - accuracy: 0.60 - ETA: 5s - loss: 0.6497 - accuracy: 0.60 - ETA: 5s - loss: 0.6485 - accuracy: 0.61 - ETA: 5s - loss: 0.6472 - accuracy: 0.61 - ETA: 5s - loss: 0.6461 - accuracy: 0.61 - ETA: 5s - loss: 0.6451 - accuracy: 0.61 - ETA: 5s - loss: 0.6438 - accuracy: 0.61 - ETA: 5s - loss: 0.6428 - accuracy: 0.61 - ETA: 5s - loss: 0.6426 - accuracy: 0.62 - ETA: 5s - loss: 0.6416 - accuracy: 0.62 - ETA: 5s - loss: 0.6403 - accuracy: 0.62 - ETA: 5s - loss: 0.6393 - accuracy: 0.62 - ETA: 5s - loss: 0.6380 - accuracy: 0.62 - ETA: 5s - loss: 0.6368 - accuracy: 0.62 - ETA: 5s - loss: 0.6354 - accuracy: 0.63 - ETA: 4s - loss: 0.6345 - accuracy: 0.63 - ETA: 4s - loss: 0.6335 - accuracy: 0.63 - ETA: 4s - loss: 0.6323 - accuracy: 0.63 - ETA: 4s - loss: 0.6313 - accuracy: 0.63 - ETA: 4s - loss: 0.6300 - accuracy: 0.63 - ETA: 4s - loss: 0.6291 - accuracy: 0.64 - ETA: 4s - loss: 0.6281 - accuracy: 0.64 - ETA: 4s - loss: 0.6270 - accuracy: 0.64 - ETA: 4s - loss: 0.6260 - accuracy: 0.64 - ETA: 4s - loss: 0.6246 - accuracy: 0.64 - ETA: 4s - loss: 0.6238 - accuracy: 0.64 - ETA: 4s - loss: 0.6228 - accuracy: 0.64 - ETA: 4s - loss: 0.6216 - accuracy: 0.65 - ETA: 4s - loss: 0.6204 - accuracy: 0.65 - ETA: 4s - loss: 0.6199 - accuracy: 0.65 - ETA: 4s - loss: 0.6189 - accuracy: 0.65 - ETA: 3s - loss: 0.6176 - accuracy: 0.65 - ETA: 3s - loss: 0.6165 - accuracy: 0.65 - ETA: 3s - loss: 0.6150 - accuracy: 0.66 - ETA: 3s - loss: 0.6144 - accuracy: 0.66 - ETA: 3s - loss: 0.6132 - accuracy: 0.66 - ETA: 3s - loss: 0.6118 - accuracy: 0.66 - ETA: 3s - loss: 0.6108 - accuracy: 0.66 - ETA: 3s - loss: 0.6096 - accuracy: 0.66 - ETA: 3s - loss: 0.6088 - accuracy: 0.66 - ETA: 3s - loss: 0.6077 - accuracy: 0.66 - ETA: 3s - loss: 0.6064 - accuracy: 0.67 - ETA: 3s - loss: 0.6054 - accuracy: 0.67 - ETA: 3s - loss: 0.6046 - accuracy: 0.67 - ETA: 3s - loss: 0.6037 - accuracy: 0.67 - ETA: 3s - loss: 0.6028 - accuracy: 0.67 - ETA: 3s - loss: 0.6021 - accuracy: 0.67 - ETA: 3s - loss: 0.6012 - accuracy: 0.67 - ETA: 2s - loss: 0.6006 - accuracy: 0.67 - ETA: 2s - loss: 0.5997 - accuracy: 0.67 - ETA: 2s - loss: 0.5988 - accuracy: 0.67 - ETA: 2s - loss: 0.5974 - accuracy: 0.68 - ETA: 2s - loss: 0.5965 - accuracy: 0.68 - ETA: 2s - loss: 0.5960 - accuracy: 0.68 - ETA: 2s - loss: 0.5952 - accuracy: 0.68 - ETA: 2s - loss: 0.5941 - accuracy: 0.68 - ETA: 2s - loss: 0.5925 - accuracy: 0.68 - ETA: 2s - loss: 0.5918 - accuracy: 0.68 - ETA: 2s - loss: 0.5907 - accuracy: 0.68 - ETA: 2s - loss: 0.5900 - accuracy: 0.68 - ETA: 2s - loss: 0.5893 - accuracy: 0.69 - ETA: 2s - loss: 0.5888 - accuracy: 0.69 - ETA: 2s - loss: 0.5880 - accuracy: 0.69 - ETA: 2s - loss: 0.5873 - accuracy: 0.69 - ETA: 2s - loss: 0.5867 - accuracy: 0.69 - ETA: 1s - loss: 0.5854 - accuracy: 0.69 - ETA: 1s - loss: 0.5848 - accuracy: 0.69 - ETA: 1s - loss: 0.5844 - accuracy: 0.69 - ETA: 1s - loss: 0.5837 - accuracy: 0.69 - ETA: 1s - loss: 0.5829 - accuracy: 0.69 - ETA: 1s - loss: 0.5822 - accuracy: 0.69 - ETA: 1s - loss: 0.5817 - accuracy: 0.69 - ETA: 1s - loss: 0.5810 - accuracy: 0.70 - ETA: 1s - loss: 0.5803 - accuracy: 0.70 - ETA: 1s - loss: 0.5798 - accuracy: 0.70 - ETA: 1s - loss: 0.5789 - accuracy: 0.70 - ETA: 1s - loss: 0.5786 - accuracy: 0.70 - ETA: 1s - loss: 0.5782 - accuracy: 0.70 - ETA: 1s - loss: 0.5772 - accuracy: 0.70 - ETA: 1s - loss: 0.5767 - accuracy: 0.70 - ETA: 1s - loss: 0.5761 - accuracy: 0.70 - ETA: 1s - loss: 0.5756 - accuracy: 0.70 - ETA: 0s - loss: 0.5751 - accuracy: 0.70 - ETA: 0s - loss: 0.5747 - accuracy: 0.70 - ETA: 0s - loss: 0.5739 - accuracy: 0.70 - ETA: 0s - loss: 0.5733 - accuracy: 0.70 - ETA: 0s - loss: 0.5727 - accuracy: 0.71 - ETA: 0s - loss: 0.5720 - accuracy: 0.71 - ETA: 0s - loss: 0.5712 - accuracy: 0.71 - ETA: 0s - loss: 0.5705 - accuracy: 0.71 - ETA: 0s - loss: 0.5698 - accuracy: 0.71 - ETA: 0s - loss: 0.5692 - accuracy: 0.71 - ETA: 0s - loss: 0.5685 - accuracy: 0.71 - ETA: 0s - loss: 0.5680 - accuracy: 0.71 - ETA: 0s - loss: 0.5676 - accuracy: 0.71 - ETA: 0s - loss: 0.5670 - accuracy: 0.71 - ETA: 0s - loss: 0.5663 - accuracy: 0.71 - ETA: 0s - loss: 0.5657 - accuracy: 0.71 - ETA: 0s - loss: 0.5650 - accuracy: 0.71 - ETA: 0s - loss: 0.5643 - accuracy: 0.7185
</code></pre>
","I am learning NLP with Keras and I am going through a tutorial. The code is the following: Towards the end of the first epoch the model freezes and makes no further progress: When I dropped the last 1,000 sentences and repeated the process I got the same situation but now at an earlier point: I restarted my PC (Windows 10) but this did not solve the problem. I then uninstalled tensorflow and reinstalled. Then I run the following code found in the official documentation of tensorflow 2.0: But when I rerun the NLP code again the model froze while fitting the data:",https://stackoverflow.com/questions/58711366,8270077,Requesting (Additional) Resources
58871822,How to combine multiple output in tensorflow to create a metric?,"<p>I am working on a multi-task neural net which has two outputs, a segmentation ""mask"" and a classification ""label"":</p>

<pre><code>inputs = tf.keras.Input(shape=(352, 512, 3), name='image')
outputs = base_model(inputs)

mask = tf.keras.layers.Conv2D(4, (3,3), strides=1, padding='same', activation='sigmoid', name='mask')(outputs)

label = tf.keras.layers.GlobalAveragePooling2D()(outputs)
label = tf.keras.layers.Dense(4, activation='sigmoid', name='label')(label)
model = tf.keras.models.Model(inputs, [mask, label])
</code></pre>

<p>From this model I obtain one prediction ""mask"" and one prediction ""label"". The question is: I want to create a metric such that it only evaluates the dice coefficient of ""mask"" when the corresponding ""label"" > 0.5.</p>

<p>I have been searching this for quite a while and from tensorflow official site I can only find how to calculate metrics for each output, but no documentation on how to access all the predictions and combine them.</p>

<pre><code>metrics = {'mask': [dice_coef(threshold=0.5)],
        'label': [tf.keras.metrics.binary_accuracy]}
</code></pre>

<p>here each metric only accept two arguments (y_true, y_pred).</p>

<p>Update:</p>

<p>I have figured out a work around myself, instead of combining results in the metrics, I added one lambda layer in the network and added one more output</p>

<pre><code>inputs = tf.keras.Input(shape=(352, 512, 3), name='image')
outputs = base_model(inputs)

mask = tf.keras.layers.Conv2D(4, (3,3), strides=1, padding='same', activation='sigmoid', name='mask')(outputs)

label = tf.keras.layers.GlobalAveragePooling2D()(outputs)
label = tf.keras.layers.Dense(4, activation='sigmoid', name='label')(label)

label_exp = tf.keras.layers.Lambda(lambda label: tf.cast(tf.math.greater(label, 0.5), tf.float32))(label)
label_exp = tf.keras.layers.Reshape([1,1,4])(label_exp)
filtered = tf.keras.layers.Multiply(name='filtered')([mask, label_exp])

model = tf.keras.models.Model(inputs, [mask, filtered, label])
</code></pre>

<p>And the metrics now is set to be:</p>

<pre><code>metrics = {'mask': [dice_coef(threshold=0.5)],
            'filtered': [dice_coef(threshold=0.5)],
            'label': [tf.keras.metrics.binary_accuracy]}
</code></pre>

<p>This works but seems like will cost additional memory. Still looking for a neat solution...</p>
","I am working on a multi-task neural net which has two outputs, a segmentation ""mask"" and a classification ""label"": From this model I obtain one prediction ""mask"" and one prediction ""label"". The question is: I want to create a metric such that it only evaluates the dice coefficient of ""mask"" when the corresponding ""label"" &gt; 0.5. I have been searching this for quite a while and from tensorflow official site I can only find how to calculate metrics for each output, but no documentation on how to access all the predictions and combine them. here each metric only accept two arguments (y_true, y_pred). Update: I have figured out a work around myself, instead of combining results in the metrics, I added one lambda layer in the network and added one more output And the metrics now is set to be: This works but seems like will cost additional memory. Still looking for a neat solution...",https://stackoverflow.com/questions/58871822,12376581,Lack of Alternative Solutions/Documentation
58933545,Using Tensorflow 2.0 and eager execution without Keras,"<p>So this question might stem from a lack of knowledge about tensorflow. But I am trying to build a multilayer perceptron with <code>tensorflow 2.0</code>, but without <code>Keras</code>.</p>
<p>The reason being that it is a requirement for my machine learning course that we do not use keras. Why you might ask? I am not sure.</p>
<p>I already have implemented our model in <code>tensorflow 2.0</code> with Keras ease, and now I want to do the exact same thing without <code>keras</code>.</p>
<pre><code>model = Sequential()
model.add(Dense(64, activation='relu', input_dim=784))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(5, activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])

X_train = X[:7000]
y_train = tf.keras.utils.to_categorical(y[:7000], num_classes=5)
X_dev = X[7000:]
y_dev = tf.keras.utils.to_categorical(y[7000:], num_classes=5)

model.fit(X_train, y_train,
          epochs=100,
          batch_size=128)
score = model.evaluate(X_dev, y_dev, batch_size=128)
print(score)
</code></pre>
<p>Here is my problem. Whenever I look up the documentation on <code>Tensorflow 2.0</code>, then even the guides on custom training are using Keras.</p>
<p>As placeholders and sessions are a thing of the past in <code>tensorflow 2.0</code>, as I understand it, then I am a bit unsure of how to structure it.</p>
<p>I can make tensor objects. I have the impression that I need to use eager execution and use gradient tape. But I still am unsure of how to put these things together.</p>
<p>Now my question is. Where should I look to get a better understanding? Which direction has the greatest descent?</p>
<p>Please do tell me if I am doing this stack overflow post wrong. It is my first time here.</p>
","So this question might stem from a lack of knowledge about tensorflow. But I am trying to build a multilayer perceptron with tensorflow 2.0, but without Keras. The reason being that it is a requirement for my machine learning course that we do not use keras. Why you might ask? I am not sure. I already have implemented our model in tensorflow 2.0 with Keras ease, and now I want to do the exact same thing without keras. Here is my problem. Whenever I look up the documentation on Tensorflow 2.0, then even the guides on custom training are using Keras. As placeholders and sessions are a thing of the past in tensorflow 2.0, as I understand it, then I am a bit unsure of how to structure it. I can make tensor objects. I have the impression that I need to use eager execution and use gradient tape. But I still am unsure of how to put these things together. Now my question is. Where should I look to get a better understanding? Which direction has the greatest descent? Please do tell me if I am doing this stack overflow post wrong. It is my first time here.",https://stackoverflow.com/questions/58933545,12397330,Requesting (Additional) Resources
59406059,Tensorflow Keras GPU uses,"<p>I'm trying to make tensorflow and keras go on GPU in my code but I'm having problem that the uses are partially (the CPU is still used by both)</p>

<ul>
<li><strong>Force keras to GPU</strong>
to force keras to GPU I added this code before keras importation, I just force it to the first GPU, I didn't configure how to run on multiple GPU (when I tried 
<code>os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3""</code> an error occurred when running the project, I have 4 GPU) </li>
</ul>

<pre><code>import os
os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""  
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""

</code></pre>

<ul>
<li><strong>Force tensorflow to GPU</strong></li>
</ul>

<p>for tensorflow I installed tensorflow-gpu then i run this code as mentioned in the tensorflow official page
<a href=""https://www.tensorflow.org/guide/gpu"" rel=""nofollow noreferrer"">tensorflow official </a></p>

<pre><code>gpus = tf.config.experimental.list_physical_devices('GPU')

        if gpus:
            # Restrict TensorFlow to only use the first GPU
            try:
                tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
                logical_gpus = tf.config.experimental.list_logical_devices('GPU')
                logger.info(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPU"")
            except RuntimeError as e:
                # Visible devices must be set before GPUs have been initialized
                logger.error(""Error inn workig on GPU %s "" % e) 
</code></pre>

<p>the problem persist and the use of the GPU is not passing 15% and the cpu is at 40% of uses.</p>

<p><a href=""https://i.stack.imgur.com/sE2O1.png"" rel=""nofollow noreferrer"">tensorflow is runing  GPU  15%</a></p>

<p><a href=""https://i.stack.imgur.com/2W9o6.png"" rel=""nofollow noreferrer"">tensorflow is runing  CPU 40%</a></p>
",I'm trying to make tensorflow and keras go on GPU in my code but I'm having problem that the uses are partially (the CPU is still used by both) for tensorflow I installed tensorflow-gpu then i run this code as mentioned in the tensorflow official page tensorflow official the problem persist and the use of the GPU is not passing 15% and the cpu is at 40% of uses. tensorflow is runing GPU 15% tensorflow is runing CPU 40%,https://stackoverflow.com/questions/59406059,7479290,Documentation Replication on Other Examples
59438904,Applying callbacks in a custom training loop in Tensorflow 2.0,"<p>I'm writing a custom training loop using the code provided in the Tensorflow DCGAN implementation guide. I wanted to add callbacks in the training loop. In Keras I know we pass them as an argument to the 'fit' method, but can't find resources on how to use these callbacks in the custom training loop. I'm adding the code for the custom training loop from the Tensorflow documentation:</p>

<pre><code># Notice the use of `tf.function`
# This annotation causes the function to be ""compiled"".
@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator(noise, training=True)

      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images, training=True)

      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

def train(dataset, epochs):
  for epoch in range(epochs):
    start = time.time()

    for image_batch in dataset:
      train_step(image_batch)

    # Produce images for the GIF as we go
    display.clear_output(wait=True)
    generate_and_save_images(generator,
                             epoch + 1,
                             seed)

    # Save the model every 15 epochs
    if (epoch + 1) % 15 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)

    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))

  # Generate after the final epoch
  display.clear_output(wait=True)
  generate_and_save_images(generator,
                           epochs,
                           seed)
</code></pre>
","I'm writing a custom training loop using the code provided in the Tensorflow DCGAN implementation guide. I wanted to add callbacks in the training loop. In Keras I know we pass them as an argument to the 'fit' method, but can't find resources on how to use these callbacks in the custom training loop. I'm adding the code for the custom training loop from the Tensorflow documentation:",https://stackoverflow.com/questions/59438904,7694977,Documentation Replication on Other Examples
59549667,How to use Tensorflows GradientTape() to compute biases,"<p>I'm looking to implement GradientTape() on a custom NN architecture but I don't see an explanation anywhere on how to use this to compute biases. A similar question was answered <a href=""https://stackoverflow.com/questions/57814376/gradient-calculation-for-bias-term-using-gradienttape"">here</a>, but it was not answered fully.</p>

<p>As a simple example, I have the training step for my NN like so:</p>

<pre><code>self.W = ## Initialized earlier on
self.b = ## Initialized earlier on

@tf.function
    def train(self):
        with tf.GradientTape() as tape:
            pred = self.feedforward()
            loss = self.loss_evaluation()
        grad = tape.gradient(loss, self.W)
        grad = tape.gradient(loss, self.b) ## How do I do this?

        optimizer.apply_gradients(zip(grad, self.W))
        optimizer.apply_gradients(zip(grad, self.b)) ## How do I do this?
</code></pre>

<p>Put simply, I cannot evaluate the gradients with respect to the biases as nowhere in any documentation or tutorial do I see the bias term included. So, how do I go about implementing the bias term as a trainable variable in my code? I'm not looking to implement this with keras, so do not suggest I use <code>trainable_variables</code> as I want to do it from scratch.</p>
","I'm looking to implement GradientTape() on a custom NN architecture but I don't see an explanation anywhere on how to use this to compute biases. A similar question was answered here, but it was not answered fully. As a simple example, I have the training step for my NN like so: Put simply, I cannot evaluate the gradients with respect to the biases as nowhere in any documentation or tutorial do I see the bias term included. So, how do I go about implementing the bias term as a trainable variable in my code? I'm not looking to implement this with keras, so do not suggest I use trainable_variables as I want to do it from scratch.",https://stackoverflow.com/questions/59549667,11065415,Documentation Completeness
59697286,Creating SequenceExample(s) using TensorFlow Transform,"<p>With TensorFlow Transform, we can pre-process data using Apache Beam. One of the requirements when setting up such a pipeline is to define a <strong>DatasetMetadata</strong> object, which contains the schema that has the information needed to parse the data from its on-disk or in-memory format, into tensors.</p>

<p>In the official documentation, we are given an example of the form:</p>

<pre><code>raw_data_metadata = dataset_metadata.DatasetMetadata(
dataset_schema.from_feature_spec({
    's': tf.FixedLenFeature([], tf.string),
    'y': tf.FixedLenFeature([], tf.float32),
    'x': tf.FixedLenFeature([], tf.float32),
}))
</code></pre>

<p>This is all fine if your raw data is a dictionary of the form:</p>

<pre><code>{
    's': 'example string',
    'y': 32.0,
    'x': 35.0
}
</code></pre>

<p>However, I am somewhat lost when it comes to defining a schema for a <strong>SequenceExample</strong>.
More specifically, consider that my data has the following format:</p>

<pre><code>{
    # context features
    'length': 5,
    # sequence features
    'tokens': [
        {
            'raw': 'The',
            'ner-tag': 'O'
        },
        {
            'raw': 'European',
            'ner-tag': 'B-org'
        },
        {
            'raw': 'Union',
            'ner-tag': 'I-org'
        },
        {
            'raw': 'is',
            'ner-tag': 'O'
        },
        {
            'raw': 'nice',
            'ner-tag': 'O'
        }
        ...
    ]
}
</code></pre>

<p>Above I have a sentence with 2 sequences:</p>

<ul>
<li><strong>ner-tag</strong> sequence which is going to be used as a label for the model</li>
<li><strong>raw</strong> sequence which is going to be used as a feature for the model</li>
</ul>

<p>How can I create a TFT data schema for such examples? </p>

<p>The documentation is a bit absent for this one.
Any help much appreciated!</p>
","With TensorFlow Transform, we can pre-process data using Apache Beam. One of the requirements when setting up such a pipeline is to define a DatasetMetadata object, which contains the schema that has the information needed to parse the data from its on-disk or in-memory format, into tensors. In the official documentation, we are given an example of the form: This is all fine if your raw data is a dictionary of the form: However, I am somewhat lost when it comes to defining a schema for a SequenceExample. More specifically, consider that my data has the following format: Above I have a sentence with 2 sequences: How can I create a TFT data schema for such examples? The documentation is a bit absent for this one. Any help much appreciated!",https://stackoverflow.com/questions/59697286,5923976,Lack of Alternative Solutions/Documentation
59751851,I get an error when I load a tensorflow2.0 model,"<p>I am learning a simple model to perform a linear regression and then I save the model </p>

<pre><code>class NN(tf.keras.Model):
  def __init__(self):
    super(NN, self).__init__()
    L = 20
    self.W1 = tf.Variable(tf.random.truncated_normal([1, L], stddev=math.sqrt(3)))
    self.B1 = tf.Variable(tf.random.truncated_normal([1, L], stddev=1.0))
    self.W2 = tf.Variable(tf.random.truncated_normal([L, 1], stddev=math.sqrt(3/L)))
    self.B2 = tf.Variable(tf.zeros([1]))
  def call(self, inputs):
    Z1 = tf.matmul(inputs, self.W1) + self.B1
    Y1 = tf.nn.tanh(Z1)
    Y = tf.matmul(Y1, self.W2) + self.B2
    return Y

# The loss function to be optimized
def loss(model, X, Y_):
  error = model(X) - Y_
  return tf.reduce_mean(tf.square(error))

model = NN()
optimizer = tf.optimizers.Adam(learning_rate=0.001)
bsize = 20

# You can call this function in a loop to train the model, bsize samples at a time
def training_step(i):
  # read data
  x_batch, y_batch = func.next_batch(bsize)
  x_batch = np.reshape(x_batch, (bsize,1))
  y_batch = np.reshape(y_batch, (bsize,1))
  # compute training values
  loss_fn = lambda: loss(model, x_batch, y_batch)
  optimizer.minimize(loss_fn, [model.W1, model.B1, model.W2, model.B2])
  if i%5000 == 0:
    l = loss(model, x_batch, y_batch)
    print(str(i) + "": epoch: "" + str(func._epochs_completed) + "": loss: "" + str(l.numpy()))

for i in range(50001): 
  training_step(i)

# save the model
tf.saved_model.save(model, ""my_file"")
</code></pre>

<p>and then I am trying to load the model with the following lines following tensorflow documentation:</p>

<pre><code>model = tf.saved_model.load(""my_file"")
f = model.signatures[""serving_default""]
y = f(x)
</code></pre>

<p>However I get the following error message:</p>

<pre><code> f = model.signatures[""serving_default""]
File ""my_file/signature_serialization.py"", line 195, in __getitem__
    return self._signatures[key]
KeyError: 'serving_default'
</code></pre>

<p>What is wrong ? Why serving_default is not defined ?</p>
",I am learning a simple model to perform a linear regression and then I save the model and then I am trying to load the model with the following lines following tensorflow documentation: However I get the following error message: What is wrong ? Why serving_default is not defined ?,https://stackoverflow.com/questions/59751851,9427880,Documentation Replicability
59811773,How to use keras attention layer on top of LSTM/GRU?,"<p>I'd like to implement an encoder-decoder architecture based on a LSTM or GRU with an attention layer. I saw that Keras has a layer for that <code>tensorflow.keras.layers.Attention</code> and I'd like to use it (all other questions and resources seem to implement it themselves or use third party libraries). Also I'm not using the network for sequence to sequence translation but for binary classification, therefore the example provided in the documentation is a bit confusing to me.</p>

<p>I'm imagining a model like this.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

x = tf.keras.Input((100, 50))

# encoder
hidden_states = tf.keras.layers.GRU(32, return_sequences=True)(x)

# decoder + attention
? = tf.keras.layers.Attention()([?, ?])
z = tf.keras.layers.GRU(32)(?)

# classification
z = tf.keras.layers.Dense(1, activation='sigmoid')(z)

model = tf.keras.Model(inputs=x, outputs=z)
</code></pre>

<p>The decoder and attention part are of this network are unclear to me. I know that I need to create a context vector from the hidden states of the encoder and the decoders current hidden state.</p>

<p>How would I implement the decoder and attention part of this network?</p>
","I'd like to implement an encoder-decoder architecture based on a LSTM or GRU with an attention layer. I saw that Keras has a layer for that tensorflow.keras.layers.Attention and I'd like to use it (all other questions and resources seem to implement it themselves or use third party libraries). Also I'm not using the network for sequence to sequence translation but for binary classification, therefore the example provided in the documentation is a bit confusing to me. I'm imagining a model like this. The decoder and attention part are of this network are unclear to me. I know that I need to create a context vector from the hidden states of the encoder and the decoders current hidden state. How would I implement the decoder and attention part of this network?",https://stackoverflow.com/questions/59811773,3666302,Documentation Ambiguity
59968630,Tensorflow one custom metric for multioutput models,"<p>I can't find the info in the documentation so I am asking here.</p>

<p>I have a multioutput model with 3 different outputs:</p>

<pre class=""lang-py prettyprint-override""><code>model = tf.keras.Model(inputs=[input], outputs=[output1, output2, output3])
</code></pre>

<p>The predicted labels for validation are constructed from these 3 outputs to form only one, it's a post-processing step. The dataset used for training is a dataset of those 3 intermediary outputs, for validation I evaluate on a dataset of labels instead of the 3 kind of intermediary data.</p>

<p>I would like to evaluate my model using a custom metric that handle the post processing and comparaison with the ground truth.</p>

<p><strong>My question is</strong>, in the code of the custom metric, will <code>y_pred</code> be a list of the 3 outputs of the model?</p>

<pre class=""lang-py prettyprint-override""><code>class MyCustomMetric(tf.keras.metrics.Metric):

  def __init__(self, name='my_custom_metric', **kwargs):
    super(MyCustomMetric, self).__init__(name=name, **kwargs)

  def update_state(self, y_true, y_pred, sample_weight=None):
    # ? is y_pred a list [batch_output_1, batch_output_2, batch_output_3] ? 

  def result(self):
    pass 

# one single metric handling the 3 outputs?
model.compile(optimizer=tf.compat.v1.train.RMSPropOptimizer(0.01),
              loss=tf.keras.losses.categorical_crossentropy,
              metrics=[MyCustomMetric()])

</code></pre>
","I can't find the info in the documentation so I am asking here. I have a multioutput model with 3 different outputs: The predicted labels for validation are constructed from these 3 outputs to form only one, it's a post-processing step. The dataset used for training is a dataset of those 3 intermediary outputs, for validation I evaluate on a dataset of labels instead of the 3 kind of intermediary data. I would like to evaluate my model using a custom metric that handle the post processing and comparaison with the ground truth. My question is, in the code of the custom metric, will y_pred be a list of the 3 outputs of the model?",https://stackoverflow.com/questions/59968630,7483509,Lack of Alternative Solutions/Documentation
60269407,Simpler way to avoid the UserWarning: Converting sparse IndexedSlices,"<p>I have a rnn network structure that looks like following</p>

<pre><code>        cells = rnn.MultiRNNCell(
            [self._one_rnn_cell(l + 1) for l in range(self.layers)],
            state_is_tuple=True
        ) if self.layers &gt; 1 else self._one_rnn_cell(1)


        out, _ = tf.nn.dynamic_rnn(cells, self.inputs,
                                   dtype=tf.float32, scope=""DyRNN"")
        out = tf.transpose(out, [1, 0, 2])
        num_time_steps = int(out.get_shape()[0])
        last_state = tf.gather(out, num_time_steps - 1, name=""last_lstm_state"")
</code></pre>

<p>Here while I am running the code, I am getting the following warning.</p>

<blockquote>
  <p>UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""</p>
</blockquote>

<p>Now I understand why this error is coming. I tried out several ideas and the most common one is the following</p>

<p><a href=""https://stackoverflow.com/questions/45882401/how-to-deal-with-userwarning-converting-sparse-indexedslices-to-a-dense-tensor"">How to deal with UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape</a></p>

<p>But the problem is this requires too many variable like </p>

<p>max_length
time_steps
seq_length
n_dim
partitions</p>

<p>this make the code very unreadable. I wanted to know if there is a simpler way I can avoid the problem.</p>

<p>Also if the sequence length remain same across all the batches, can I assume <code>max_length == time_steps == seq_length</code> ?</p>

<p>Please help, documentation is very less.</p>
","I have a rnn network structure that looks like following Here while I am running the code, I am getting the following warning. Now I understand why this error is coming. I tried out several ideas and the most common one is the following How to deal with UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape But the problem is this requires too many variable like max_length time_steps seq_length n_dim partitions this make the code very unreadable. I wanted to know if there is a simpler way I can avoid the problem. Also if the sequence length remain same across all the batches, can I assume max_length == time_steps == seq_length ? Please help, documentation is very less.",https://stackoverflow.com/questions/60269407,4037927,Lack of Alternative Solutions/Documentation
60418931,How to create a custom pyfunc to make predictions using a model that requires an input shape with more than two dimensions using MLflow?,"<p>I'm new to TensorFlow and MLFlow and I have a similar problem to the one asked in <a href=""https://stackoverflow.com/questions/58917918/how-to-make-predictions-using-a-model-that-requires-an-input-shape-with-more-tha/60416642#60416642"">here</a>. I am implementing a TensorFlow model to predict timeseries values. With that aim, I used MLFlow's mlflow.tensorflow.autolog() to track and serve the models in my instance. Nevertheless, since my input shape has more than 2 dimensions, I was not able to use that method. </p>

<p>As previously <a href=""https://stackoverflow.com/a/60129726/8338272"">suggested</a>, I've tried to encode/decode the input data in prediction by using a custom pyfunc to do it. </p>

<p>In this way, I have a function <em>model_test.py</em> with a predict method that decode its input data that is:</p>

<pre class=""lang-py prettyprint-override""><code>import sys
import os
import json
import mlflow
import numpy as np
import pandas as pd
from mlflow.pyfunc import PythonModel
import tensorflow as tf
import base64



class ModelTest(PythonModel):

    def __init__(self, estimator=None,window_size = 64,batch_size = 256,shuffle_buffer_size = 100):
        # CODE TO CREATE THE EXPERIMENT
        self.window_size = window_size
        self.batch_size = batch_size
        self.shuffle_buffer_size = shuffle_buffer_size

    def windowed_dataset(self,series, window_size, batch_size, shuffle_buffer):
            series = tf.expand_dims(series, axis=-1)
            ds = tf.data.Dataset.from_tensor_slices(series)
            ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
            ds = ds.flat_map(lambda w: w.batch(window_size + 1))
            ds = ds.shuffle(shuffle_buffer)
            ds = ds.map(lambda w: (w[:-1], w[1:]))
            self.windowed_ds = ds.batch(batch_size).prefetch(1)
            return self

    def train(self, train_set, y = None, epochs = 500):

        model = tf.keras.models.Sequential([
                  tf.keras.layers.Conv1D(filters=60, kernel_size=5,
                                      strides=1, padding=""causal"",
                                      activation=""relu"",
                                      input_shape=[None, 1]),
                  tf.keras.layers.LSTM(60, return_sequences=True),
                  tf.keras.layers.Dense(10, activation=""relu""),
                  tf.keras.layers.Dense(1),
                  tf.keras.layers.Lambda(lambda x: x * 400)
                ])
        optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)
        model.compile(loss=tf.keras.losses.Huber(),optimizer=optimizer,metrics=[""mae""])
        model.fit(x=train_set, y=y,epochs=5)
        self.modelo = model

        return self

    def predict(self, series_encoded):
        # Decode the data that arrives to the method
        def decode_ts(x):
            return pd.Series(np.frombuffer(base64.b64decode(x)))
        series_decode = decode_ts(series_encoded)
        # Preprocess data
        series = np.expand_dims(series_decode, axis=1)
        ds = tf.data.Dataset.from_tensor_slices(series)
        # Replace the number by a variable window_size
        ds = ds.window(60, shift=1, drop_remainder=True)
        # Replace the number by a variable window_size
        ds = ds.flat_map(lambda w: w.batch(60))
        ds = ds.batch(32).prefetch(1)
        # Prediction
        forecast = self.modelo.predict(ds)

        return forecast
</code></pre>

<p>And a <em>run.py</em> file to train and save the model:</p>

<pre class=""lang-py prettyprint-override""><code>import os
import mlflow.pyfunc
import ModelTest as model_test
import sys
import json
import mlflow
import numpy as np
from pymongo import MongoClient
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import Preprocessing as pre

#@click(...) # define the click options according to MLproject file
def run():
    # Code to load time series data from MongoDB and preprocess it

    window_size = 64
    batch_size = 256
    shuffle_buffer_size = 100
    split_time = 400

    series = np.array(data_df['sensor_ts'])
    time = np.array(data_df['time'])
    time_train = time[:split_time]
    x_train = series[:split_time]
    time_valid = time[split_time:]
    x_valid = series[split_time:]


    modelo = modelo_tercero.ModelTest()
    modelo.windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

    with mlflow.start_run() as run:
        model = modelo.train(modelo.windowed_ds)
        model_path = os.path.join('models', run.info.run_id)

        # Save model
        mlflow.pyfunc.save_model(
            path=model_path,
            python_model= modelo.train(modelo.windowed_ds),
            code_path=['Modelthird.py'],
            conda_env={
                'channels': ['defaults', 'conda-forge'],
                'dependencies': [
                    'mlflow=1.6.0',
                    'numpy=1.18.1',
                    'tensorflow=2.1.0',
                    'pandas=0.25.3',
                    'python=3.7.6',
                    'cloudpickle==0.5.8'
                ],
                'name': 'mlflow-env'
            }
        )


if __name__ == ""__main__"":
    run()

</code></pre>

<p>When I execute the run.py I get the next errors when the model is going to be saved:</p>

<pre><code> Traceback (most recent call last):

File ""run.py"", line 116, in &lt;module&gt;
    run()
  File ""run.py"", line 110, in run
    'name': 'mlflow-env'
  File ""/opt/conda/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py"", line 596, in save_model
    code_paths=code_path, mlflow_model=mlflow_model)
  File ""/opt/conda/lib/python3.7/site-packages/mlflow/pyfunc/model.py"", line 141, in _save_model_with_class_artifacts_params
    cloudpickle.dump(python_model, out)
  File ""/opt/conda/lib/python3.7/site-packages/cloudpickle/cloudpickle.py"", line 1109, in dump
    CloudPickler(file, protocol=protocol).dump(obj)
  File ""/opt/conda/lib/python3.7/site-packages/cloudpickle/cloudpickle.py"", line 482, in dump
    return Pickler.dump(self, obj)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 437, in dump
    self.save(obj)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/opt/conda/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/opt/conda/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/opt/conda/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/opt/conda/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/opt/conda/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/opt/conda/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/opt/conda/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/opt/conda/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/opt/conda/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/opt/conda/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/opt/conda/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/opt/conda/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/opt/conda/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/opt/conda/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/opt/conda/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/opt/conda/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/opt/conda/lib/python3.7/pickle.py"", line 524, in save
    rv = reduce(self.proto)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 873, in __reduce__
    return convert_to_tensor, (self._numpy(),)
  File ""/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 910, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)
  File ""&lt;string&gt;"", line 3, in raise_from
</code></pre>

<p>I have looked at different documentation related to save and serialize TensorFlow models, but there is not so much documentation about TensorFlow models and custom pyfunc functions in MLFlow. Could anyone help me or give me any hint?</p>

<p>Thanks in advance!! :D</p>
","I'm new to TensorFlow and MLFlow and I have a similar problem to the one asked in here. I am implementing a TensorFlow model to predict timeseries values. With that aim, I used MLFlow's mlflow.tensorflow.autolog() to track and serve the models in my instance. Nevertheless, since my input shape has more than 2 dimensions, I was not able to use that method. As previously suggested, I've tried to encode/decode the input data in prediction by using a custom pyfunc to do it. In this way, I have a function model_test.py with a predict method that decode its input data that is: And a run.py file to train and save the model: When I execute the run.py I get the next errors when the model is going to be saved: I have looked at different documentation related to save and serialize TensorFlow models, but there is not so much documentation about TensorFlow models and custom pyfunc functions in MLFlow. Could anyone help me or give me any hint? Thanks in advance!! :D",https://stackoverflow.com/questions/60418931,8338272,Documentation Completeness
60463215,Can we use Microsoft TensorFlow as regular TensorFlow?,"<p>Most of the tutorials/guides I found on Microsoft ML are for sentiment analysis or image classification. For my project I need to use TensorFlow functions like it can be with Python. As TensorFlow nuget package is available I'm wondering if this can be used as original TensorFlow alternative. Asking reason of this question here is - I could not find a guide or documentation on this.</p>

<p>I can import TensorFlow after getting it from nuget package manager. But I can't access its regular methods or constants like it can be with Python.</p>

<p>The following code is valid:</p>

<pre><code>using System;
using Tensorflow;

namespace TF_Tests
{
    class Program
    {
        static void Main(string[] args)
        {
            var a = TensorInfo.Descriptor;

            Console.WriteLine(""Hello World! ""+a);
        }
    }
}
</code></pre>

<p>TensorFlow can be imported, but how can I use it in a regular way? For example: </p>

<pre><code>c = tf.strings.unicode_split(a, 'UTF-8').to_list() //how to access such method or atleast declare a constant?

print(c[0].__len__())

for i in c[0]:
    print(i)
</code></pre>
","Most of the tutorials/guides I found on Microsoft ML are for sentiment analysis or image classification. For my project I need to use TensorFlow functions like it can be with Python. As TensorFlow nuget package is available I'm wondering if this can be used as original TensorFlow alternative. Asking reason of this question here is - I could not find a guide or documentation on this. I can import TensorFlow after getting it from nuget package manager. But I can't access its regular methods or constants like it can be with Python. The following code is valid: TensorFlow can be imported, but how can I use it in a regular way? For example:",https://stackoverflow.com/questions/60463215,7291498,Lack of Alternative Solutions/Documentation
60607402,Style Transfer : Save&Restore checkpoint/model in tensorflow 1.15.0,"<p>i am a bit frustrated about saving and restoring models in tensorflow 1.15.0. I want to achieve it in a jupyter notebook / google colab notebook environment. The application is style-transfer of images.</p>

<p>I simply want to save the model and restore it in order to apply the style transfer for a larger number of images.</p>

<p>The tensorflow documentation is a bit confusing, (i did not find examples for this), so i never really know what the right syntax looks like.</p>

<p>I am at a point now where i want to achieve 1 thing:</p>

<ol>
<li>Restore the model
correctly.</li>
</ol>

<p>I will write the relevant lines now:</p>

<pre><code>model = get_model()  
opt = tf.train.AdamOptimizer(learning_rate=2.5,beta1=0.99, epsilon=1e-1)
saver = tf.train.Checkpoint(model=model, optimizer=opt)
saver.save('/content/sample_data/test/_____NEU____')
</code></pre>

<p>When i want to restore the model, i use the command:</p>

<pre><code>saver.restore('/content/sample_data/test/_____NEU____')
</code></pre>

<p>How can i fix this issue, and load my checkpoint files correctly? Thank you</p>

<hr>

<p>The google colab project is here:</p>

<p><a href=""https://colab.research.google.com/drive/12hTitoQ2-tH8pYEsfMDR5jtsg8a96PgC"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/12hTitoQ2-tH8pYEsfMDR5jtsg8a96PgC</a></p>

<hr>
","i am a bit frustrated about saving and restoring models in tensorflow 1.15.0. I want to achieve it in a jupyter notebook / google colab notebook environment. The application is style-transfer of images. I simply want to save the model and restore it in order to apply the style transfer for a larger number of images. The tensorflow documentation is a bit confusing, (i did not find examples for this), so i never really know what the right syntax looks like. I am at a point now where i want to achieve 1 thing: I will write the relevant lines now: When i want to restore the model, i use the command: How can i fix this issue, and load my checkpoint files correctly? Thank you The google colab project is here: https://colab.research.google.com/drive/12hTitoQ2-tH8pYEsfMDR5jtsg8a96PgC",https://stackoverflow.com/questions/60607402,8092502,Documentation Completeness
60701451,"In Tensorflow Classification, how are the labels ordered when using ""predict""?","<p>I'm using the MNIST handwritten numerals dataset to train a CNN.</p>

<p>After training the model, i use predict like this:</p>

<pre><code>predictions = cnn_model.predict(test_images)
predictions[0]
</code></pre>

<p>and i get output as:</p>

<pre><code>array([2.1273775e-06, 2.9292005e-05, 1.2424786e-06, 7.6307842e-05,
       7.4305902e-08, 7.2301691e-07, 2.5368356e-08, 9.9952960e-01,
       1.2401938e-06, 1.2787555e-06], dtype=float32)
</code></pre>

<p>In the output, there are 10 probabilities, one for each of numeral from 0 to 9. But how do i know which probability refers to which numeral ?</p>

<p>In this particular case, the probabilities are arranged sequentially for numerals 0 to 9. But why is that ? I didn't define that anywhere.</p>

<p>I tried going over documentation and example implementations found elsewhere on the internet, but no one seems to have addressed this particular behaviour.</p>

<p>Edit:</p>

<p>For context, I've defined my train/test data by:</p>

<pre><code>mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = (np.expand_dims(train_images, axis=-1)/255.).astype(np.float32)
train_labels = (train_labels).astype(np.int64)
test_images = (np.expand_dims(test_images, axis=-1)/255.).astype(np.float32)
test_labels = (test_labels).astype(np.int64)
</code></pre>

<p>And my model consists of a a few convulution and pooling layers, then a Flatten layer, then a Dense layer with 128 neurons and an output Dense layer with 10 neurons.</p>

<p>After that I simply fit my model and use predict like this:</p>

<pre><code>model.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)
predictions = cnn_model.predict(test_images)
</code></pre>

<p>I don't see where I've instructed my code to output first neuron as digit 0, second neuron as digit 1 etc
And if i wanted to change the the sequence in which the resulting digits are output, where do i do that ?
This is really confusing me a lot.</p>
","I'm using the MNIST handwritten numerals dataset to train a CNN. After training the model, i use predict like this: and i get output as: In the output, there are 10 probabilities, one for each of numeral from 0 to 9. But how do i know which probability refers to which numeral ? In this particular case, the probabilities are arranged sequentially for numerals 0 to 9. But why is that ? I didn't define that anywhere. I tried going over documentation and example implementations found elsewhere on the internet, but no one seems to have addressed this particular behaviour. Edit: For context, I've defined my train/test data by: And my model consists of a a few convulution and pooling layers, then a Flatten layer, then a Dense layer with 128 neurons and an output Dense layer with 10 neurons. After that I simply fit my model and use predict like this: I don't see where I've instructed my code to output first neuron as digit 0, second neuron as digit 1 etc And if i wanted to change the the sequence in which the resulting digits are output, where do i do that ? This is really confusing me a lot.",https://stackoverflow.com/questions/60701451,11501160,Documentation Ambiguity
60797147,Embedding layer output shape is 2D,"<p>I'm encountering some issue with the output shape of my embedding layer, as per the keras documentation, the embedding layer should have an output shape of 3D tensor, but my embedding layer is only outputting 2D tensor.</p>

<pre><code>class MyModel(Model):
  def __init__(self, vocab_size, embedding_matrix, max_length):
      super(MyModel, self).__init__()
      self.embedding_l1 = tf.keras.layers.Embedding(input_dim=vocab_size,
                                                    output_dim=max_length, 
                                                    input_length=max_length,
                                                    weights=[embedding_matrix], 
                                                    trainable=False)
      self.bidirectional_l1 = Bidirectional(
                             tf.compat.v1.keras.layers.CuDNNLSTM(32, 
                                                                 return_sequences=False))
      self.dense_l1 = Dense(units=256, activation='relu')
      self.dropout_l1 = Dropout(rate=2e-5)
      self.dense_l2 = Dense(units=1, activation='sigmoid')

  def call(self, x):
      embedding_out = self.embedding_l1(x)
      print(""SHAPE:"",embedding_out.shape)
      bid_out1 = self.bidirectional_l1(self.reshape_l1(embedding_out))
      dense_out1 = self.dense_l1(bid_out1)
      drop_out1 = self.dropout_l2(dense_out1)
      dense_out2 = self.dense_l2(drop_out2)
      return dense_out2
</code></pre>

<p>It outputs the shape of the embedding layer out as a 2D (300,300) tensor. which causes error on the bidirectional lstm:</p>

<p><code>ValueError: Input 0 of layer bidirectional is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [300, 300]</code></p>
","I'm encountering some issue with the output shape of my embedding layer, as per the keras documentation, the embedding layer should have an output shape of 3D tensor, but my embedding layer is only outputting 2D tensor. It outputs the shape of the embedding layer out as a 2D (300,300) tensor. which causes error on the bidirectional lstm: ValueError: Input 0 of layer bidirectional is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [300, 300]",https://stackoverflow.com/questions/60797147,11000852,Lack of Alternative Solutions/Documentation
60876340,How can I save a trained TensorFlow Federated model as a .h5 model?,"<p>I want to save a TensorFlow federated model which was trained with the FedAvg Algorithm as a Keras/.h5 model. I couldn't find the documents on this and would like to know how it may be done.
Also if possible, I'd like to have access to both the aggregated server model and the models of the clients.</p>

<p>The code I use to train the federated model is below:</p>

<pre><code>def model_fn():
    model = tf.keras.models.Sequential([
      tf.keras.layers.Input(shape=(segment_size,num_input_channels)),
      tf.keras.layers.Flatten(), 
      tf.keras.layers.Dense(units=400, activation='relu'),
      tf.keras.layers.Dropout(dropout_rate),
      tf.keras.layers.Dense(units=100, activation='relu'),
      tf.keras.layers.Dropout(dropout_rate),
      tf.keras.layers.Dense(activityCount, activation='softmax'),
    ])
    return tff.learning.from_keras_model(
      model,
      dummy_batch=batch,
      loss=tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])
trainer = tff.learning.build_federated_averaging_process(
    model_fn, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learningRate))

def evaluate(num_rounds=communicationRound):
  state = trainer.initialize()
  roundMetrics = []
  evaluation = tff.learning.build_federated_evaluation(model_fn)

  for round_num in range(num_rounds):
    t1 = time.time()
    state, metrics = trainer.next(state, train_data)
    t2 = time.time()
    test_metrics = evaluation(state.model, train_data)

    roundMetrics.append('round {:2d}, metrics={}, loss={}'.format(round_num, metrics.sparse_categorical_accuracy , metrics.loss))
    roundMetrics.append(""The test accuracy is "" + str(test_metrics.sparse_categorical_accuracy))
    roundMetrics.append('round time={}'.format(t2 - t1))
    print('round {:2d}, accuracy={}, loss={}'.format(round_num, metrics.sparse_categorical_accuracy , metrics.loss))
    print(""The test accuracy is "" + str(test_metrics.sparse_categorical_accuracy))
    print('round time={}'.format(t2 - t1))
  outF = open(filepath+'stats'+architectureType+'.txt', ""w"")
  for line in roundMetrics:
    outF.write(line)
    outF.write(""\n"")
  outF.close()
</code></pre>
","I want to save a TensorFlow federated model which was trained with the FedAvg Algorithm as a Keras/.h5 model. I couldn't find the documents on this and would like to know how it may be done. Also if possible, I'd like to have access to both the aggregated server model and the models of the clients. The code I use to train the federated model is below:",https://stackoverflow.com/questions/60876340,12992742,Lack of Alternative Solutions/Documentation
60974077,How to save Keras model as frozen graph?,"<p>I am working with Tensorflow 2.0 and want to store the following Keras model as frozen graph. </p>

<pre><code>import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(64, input_shape=[100]))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(2, activation='softmax'))
model.summary()
model.save('./models/')
</code></pre>

<p>I can't find any good examples how to do this in Tensorflow 2.0. I have found the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py"" rel=""noreferrer"">freeze_graph.py</a> file in the Tensorflow Github repository but find it hard to wrap my head around it.</p>

<p>I load the file mentioned above using:</p>

<pre><code>from tensorflow.python.tools.freeze_graph import freeze_graph
</code></pre>

<p>But what exactly do I have to provide to the <code>freeze_graph</code> function itself? Here I marked the arguments where I am not sure with a questionmark.</p>

<pre><code>freeze_graph(input_graph=?,
             input_saver='',
             input_binary=False,
             input_checkpoint=?,
             output_node_names=?,
             restore_op_name='',
             filename_tensor_name='',
             output_graph='./frozen_graph.pb',
             clear_devices=True,
             initializer_nodes='')
</code></pre>

<p>Can someone provide a simple example that shows how I can store the model above as a frozen graph using the <code>freeeze_graph</code> function?</p>
",I am working with Tensorflow 2.0 and want to store the following Keras model as frozen graph. I can't find any good examples how to do this in Tensorflow 2.0. I have found the freeze_graph.py file in the Tensorflow Github repository but find it hard to wrap my head around it. I load the file mentioned above using: But what exactly do I have to provide to the freeze_graph function itself? Here I marked the arguments where I am not sure with a questionmark. Can someone provide a simple example that shows how I can store the model above as a frozen graph using the freeeze_graph function?,https://stackoverflow.com/questions/60974077,3861775,Requesting (Additional) Resources
60996450,Keras ignores the input_shape provided to the first layer,"<p>I build a simple model like this, specifying an input shape of <code>(32, 32, 1)</code> to the first (and only) layer:</p>

<pre><code>import numpy as np
import tensorflow as tf

class Model(tf.keras.Model):
  def __init__(self):
    super().__init__()
    self.conv = tf.keras.layers.Conv2D(64, 3, input_shape=(32, 32, 1))
  def call(self, x):
    x = self.conv(x)
    return x

model = Model()
</code></pre>

<p>Now when I call</p>

<pre><code>print(model.summary())
</code></pre>

<p>I get the following error</p>

<pre class=""lang-none prettyprint-override""><code>This model has not yet been built. Build the model first by calling `build()` or calling `fit()`
with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.
</code></pre>

<p>despite the fact that <code>input_shape</code> is indeed specified.</p>

<p>The model also happily takes input with non-conforming shapes:</p>

<pre><code>x = np.zeros((1, 24, 24, 1), dtype=np.float32)
model(x)  # OK!
</code></pre>

<p>Does this mean that despite what the documentation says, <code>input_shape</code> specification to the first layer is simply ignored? How to enforce this value?</p>

<p><em>(My TF version is 2.1.0).</em></p>
","I build a simple model like this, specifying an input shape of (32, 32, 1) to the first (and only) layer: Now when I call I get the following error despite the fact that input_shape is indeed specified. The model also happily takes input with non-conforming shapes: Does this mean that despite what the documentation says, input_shape specification to the first layer is simply ignored? How to enforce this value? (My TF version is 2.1.0).",https://stackoverflow.com/questions/60996450,9973879,Documentation Completeness
61302405,"TensorFlow2 beginner, recalculating after assigning new value","<p>I'm new to TensorFlow (using TensorFlow2). <br/>
Trying to understand how to re-calculate a simple calculation, after re-assigning a value to variable. It sounds simple, but I'm having a hard time finding it in the new TF2 documentation. <br/> <br/>
Simple example: define a tensor which is a sum two variables (3+4). Then, if I re-assign one of the variables, I'd like to re-use this ""sum tensor"" - making it re-calculate (without having to create a new ""sum tensor""). Is there a way to achieve this please? thank!</p>

<pre><code>v1 = tf.Variable(3)
v2 = tf.Variable(4)
sum1=tf.add(v1,v2)
print(""Original sum 3+4:"",sum1)   # This hows 7 as expected
v1.assign(9)
print(""Sum after re-assign"",sum1) # Fails, shows the old 7
</code></pre>
","I'm new to TensorFlow (using TensorFlow2). Trying to understand how to re-calculate a simple calculation, after re-assigning a value to variable. It sounds simple, but I'm having a hard time finding it in the new TF2 documentation. Simple example: define a tensor which is a sum two variables (3+4). Then, if I re-assign one of the variables, I'd like to re-use this ""sum tensor"" - making it re-calculate (without having to create a new ""sum tensor""). Is there a way to achieve this please? thank!",https://stackoverflow.com/questions/61302405,13353258,Lack of Alternative Solutions/Documentation
61561253,AttributeError: module 'tensorflow' has no attribute 'RunOptions',"<p>I'm a beginner. 
I'm working with python - TensorFlow '2.2.0' on python IDLE. </p>

<pre><code>run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)
</code></pre>

<p>I got the following error while running the previous code.:</p>

<pre><code>AttributeError: module 'tensorflow' has no attribute 'RunOptions'"" 
</code></pre>

<p>however, according to example 18 from this <a href=""https://www.programcreek.com/python/example/90595/tensorflow.RunOptions"" rel=""nofollow noreferrer"">link</a> on the official page on Tensorflow, there's no error! </p>

<p>what's wrong in my case? How should I resolve this issue?</p>
","I'm a beginner. I'm working with python - TensorFlow '2.2.0' on python IDLE. I got the following error while running the previous code.: however, according to example 18 from this link on the official page on Tensorflow, there's no error! what's wrong in my case? How should I resolve this issue?",https://stackoverflow.com/questions/61561253,13454942,Requesting (Additional) Resources
61761477,What's the use for converter.build() in TensorRT?,"<p>The <a href=""https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html"" rel=""nofollow noreferrer"">official documentation on TensorRT</a> lists two ways to convert a TensorFlow SavedModel into a TensorRT SavedModel: the first is </p>

<pre class=""lang-py prettyprint-override""><code>from tensorflow.python.compiler.tensorrt import trt_convert as trt
converter = trt.TrtGraphConverterV2(input_saved_model_dir=input_saved_model_dir)
converter.convert()
converter.save(output_saved_model_dir)
</code></pre>

<p>and the second is </p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.python.compiler.tensorrt import trt_convert as trt

conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS
conversion_params = conversion_params._replace(
    max_workspace_size_bytes=(1&lt;&lt;32))
conversion_params = conversion_params._replace(precision_mode=""FP16"")
conversion_params = conversion_params._replace(
    maximum_cached_engiens=100)

converter = trt.TrtGraphConverterV2(
    input_saved_model_dir=input_saved_model_dir,
    conversion_params=conversion_params)
converter.convert()
def my_input_fn():
  for _ in range(num_runs):
    Inp1 = np.random.normal(size=(8, 16, 16, 3)).astype(np.float32)
    inp2 = np.random.normal(size=(8, 16, 16, 3)).astype(np.float32)
    yield inp1, inp2
converter.build(input_fn=my_input_fn)
converter.save(output_saved_model_dir)

saved_model_loaded = tf.saved_model.load(
    output_saved_model_dir, tags=[tag_constants.SERVING])
graph_func = saved_model_loaded.signatures[
    signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
frozen_func = convert_to_constants.convert_variables_to_constants_v2(
    graph_func)
output = frozen_func(input_data)[0].numpy()
</code></pre>

<p>Stripping out all of the boilerplate code for imports, inference etc the difference seems to lie in the call to <code>converter.build()</code>. The documentation explains this function as such:</p>

<p>""This method optimizes the converted function (returned by convert()) by building TensorRT engines. This is useful in case the user wants to perform the optimizations before runtime. The optimization is done by running inference on the converted function using the input data received from the argument input_fn. This argument is a generator function that yields input data as a list or tuple. ""</p>

<p>What does ""before runtime"" mean in this context? Will the ""optimizations"" be performed upon model loading, upon the first inference, or upon every single inference using the converted model? What <em>are</em> those optimizations, even? Isn't converting the model to TensorRT an optimization in itself?</p>

<p>I am asking because if I call <code>converter.build()</code> the conversion seems to fail in unpredictable ways after taking a LOT of time (more than two hours) to run without producing any sensible output, so I was wondering how much am I losing by not calling it and whether there is more comprehensive documentation on using TF2.x SavedModels with TensorRT. </p>

<p>Thanks in advance to whoever can answer!!</p>
","The official documentation on TensorRT lists two ways to convert a TensorFlow SavedModel into a TensorRT SavedModel: the first is and the second is Stripping out all of the boilerplate code for imports, inference etc the difference seems to lie in the call to converter.build(). The documentation explains this function as such: ""This method optimizes the converted function (returned by convert()) by building TensorRT engines. This is useful in case the user wants to perform the optimizations before runtime. The optimization is done by running inference on the converted function using the input data received from the argument input_fn. This argument is a generator function that yields input data as a list or tuple. "" What does ""before runtime"" mean in this context? Will the ""optimizations"" be performed upon model loading, upon the first inference, or upon every single inference using the converted model? What are those optimizations, even? Isn't converting the model to TensorRT an optimization in itself? I am asking because if I call converter.build() the conversion seems to fail in unpredictable ways after taking a LOT of time (more than two hours) to run without producing any sensible output, so I was wondering how much am I losing by not calling it and whether there is more comprehensive documentation on using TF2.x SavedModels with TensorRT. Thanks in advance to whoever can answer!!",https://stackoverflow.com/questions/61761477,5623016,Documentation Ambiguity
61767723,get_config missing while loading previously saved model without custom layers,"<p>I have a problem with loading the previously saved model.</p>

<p>This is my save:</p>

<pre><code>def build_rnn_lstm_model(tokenizer, layers):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, layers,input_length=843),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(layers, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01))),
        tf.keras.layers.Dense(layers, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),
        tf.keras.layers.Dense(layers/2, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    model.summary()
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',f1,precision, recall])
    print(""Layers: "", len(model.layers))
    return model

model_path = str(Path(__file__).parents[2]) + os.path.sep + 'model'
data_train_sequence, data_test_sequence, labels_train, labels_test, tokenizer = get_training_test_data_local()
model = build_rnn_lstm_model(tokenizer, 32)
model.fit(data_train_sequence, labels_train, epochs=num_epochs, validation_data=(data_test_sequence, labels_test))
model.save(model_path + os.path.sep + 'auditor_model', save_format='tf')
</code></pre>

<p>After this I can see that <code>auditor_model</code> is saved in <code>model</code> directory.</p>

<p>now I would like to load this model with:</p>

<pre><code>model = tf.keras.models.load_model(model_path + os.path.sep + 'auditor_model')
</code></pre>

<p>but I get:</p>

<blockquote>
  <p>ValueError: Unable to restore custom object of type _tf_keras_metric
  currently. Please make sure that the layer implements <code>get_config</code>and
  <code>from_config</code> when saving. In addition, please use the
  <code>custom_objects</code> arg when calling <code>load_model()</code>.</p>
</blockquote>

<p>I have read about <code>custom_objects</code> in <code>TensorFlow</code> docs but I don't understand how to implement it while I use no custom layers but the predefined ones.</p>

<p>Could anyone give me a hint how to make it work? I use TensorFlow 2.2 and Python3</p>
",I have a problem with loading the previously saved model. This is my save: After this I can see that auditor_model is saved in model directory. now I would like to load this model with: but I get: I have read about custom_objects in TensorFlow docs but I don't understand how to implement it while I use no custom layers but the predefined ones. Could anyone give me a hint how to make it work? I use TensorFlow 2.2 and Python3,https://stackoverflow.com/questions/61767723,1394504,Documentation Ambiguity
61882452,Tensorflow tf.layer.batch_normalization: how to use global moving stats to do normalization during training?,"<p>From the documentation, the arg training</p>

<blockquote>
  <p>Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). NOTE: make sure to set this parameter correctly, or else your training/inference will not work properly.</p>
</blockquote>

<p>It never mentioned that this arg also controls whether to update global moving stats. Even if the update ops is added through </p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, 'train_model') 
with tf.control_dependencies(update_ops):
    train_ops = ...
</code></pre>

<p>they will not be updated is <code>training=False</code>. I am curious if there is a way to have these moving stats keep updating during training, at the same time use them to perform normalization?</p>
","From the documentation, the arg training It never mentioned that this arg also controls whether to update global moving stats. Even if the update ops is added through they will not be updated is training=False. I am curious if there is a way to have these moving stats keep updating during training, at the same time use them to perform normalization?",https://stackoverflow.com/questions/61882452,11111055,Documentation Completeness
61994141,Loss and learning rate scaling strategies for Tensorflow distributed training when using TF Estimator,"<p>For those who don't want to read the whole story:</p>

<p><strong>TL; DR</strong>: When using <code>TF Estimator</code>, do we have to scale learning rate by the factor by which we increase batch size (I know this is the right way, I am not sure if TF handles this internally)? Similarly, do we have to scale per example loss by global batch size (batch_size_per_replica * number of replicas)?</p>

<p>Documentation on Tensorflow distributed learning is confusing. I need clarification on below points.</p>

<ol>
<li><p>It is now understood that if you increase the batch size by a factor of <code>k</code> then you need to increase the learning rate by <code>k</code> (see <a href=""https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf"" rel=""nofollow noreferrer"">this</a> and <a href=""https://arxiv.org/pdf/1711.00489.pdf"" rel=""nofollow noreferrer"">this</a> paper). However, Tensoflow official page on distributed learning makes no clarifying comment about this. They do mention <a href=""https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator#multiworkermirroredstrategy"" rel=""nofollow noreferrer"">here</a> that learning rate needs to be adjusted. Do they handle the learning rate scaling by themselves? To make matters more complicated, the behavior is different in Keras and tf.Estimator (see next point). Any suggestions on should I increase the LR by a factor of K or not <em>when I am using <code>tf.Estimator</code></em>?</p></li>
<li><p>It is widely accepted that the per example loss should be scaled by <code>global_batch_size = batch_size_per_replica * number of replicas</code>. Tensorflow mentions it <a href=""https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function"" rel=""nofollow noreferrer"">here</a> but then when illustrating how to achieve this with a tf.Estimator, they either forget or the scaling by <code>global_batch_size</code> is not required. See <a href=""https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator#define_the_model"" rel=""nofollow noreferrer"">here</a>, in the code snippet, loss is defined as follows.</p></li>
</ol>

<pre><code>loss = tf.reduce_sum(loss) * (1. / BATCH_SIZE)
</code></pre>

<p>and <code>BATCH_SIZE</code> to the best of my understanding is defined above as per replica batch size.</p>

<p>To complicate things further, the scaling is handled automatically if you are using Keras (for reasons I will never understand, it would have been better to keep everything consistent).</p>
","For those who don't want to read the whole story: TL; DR: When using TF Estimator, do we have to scale learning rate by the factor by which we increase batch size (I know this is the right way, I am not sure if TF handles this internally)? Similarly, do we have to scale per example loss by global batch size (batch_size_per_replica * number of replicas)? Documentation on Tensorflow distributed learning is confusing. I need clarification on below points. and BATCH_SIZE to the best of my understanding is defined above as per replica batch size. To complicate things further, the scaling is handled automatically if you are using Keras (for reasons I will never understand, it would have been better to keep everything consistent).",https://stackoverflow.com/questions/61994141,1586200,Documentation Ambiguity
62008061,TypeError when fitting keras model,"<p>I am a new to tensorflow, I am trying to build a simple neural network. But every time I get close, there are a list of errors stopping me. I followed tutorials and documentations and kept most of the code and changed only things I need to.</p>

<p>Here is my code:</p>

<pre><code>###
# Import
###

import tensorflow as tf
import pandas as pd
from tensorflow import keras

###
# Loading Data
###


# Training Data

# path of the training data
train_data_path = ""C:/Users/User/Desktop/Machine_Learning/Neural_Network/addition_train_data.csv""
train_data = pd.read_csv(train_data_path)  # loads the data using pandas

# Evalution Data

# path of the evalution data
eval_data_path = ""C:/Users/User/Desktop/Machine_Learning/Neural_Network/addition_eval_data.csv""
eval_data = pd.read_csv(eval_data_path)  # loads the data using pandas (again)


# Target Columns
train_target = train_data.pop(""Sum"")
eval_target = eval_data.pop(""Sum"")


###
# Creating the Model
###

model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(35, 2)))
model.add(keras.layers.Lambda(
    lambda x: tf.expand_dims(model.output, axis=-1)))
model.add(keras.layers.Dense(10, activation=""tanh""))
model.add(keras.layers.Dense(1, activation=""tanh""))

###
# Compiling the Model
###

model.compile(
    optimizer='adam',
    loss='mean_absolute_error',
    metrics=['accuracy']
)

###
# Training the Model
###

model.fit(
    eval_data, eval_target, epochs=10
)
</code></pre>

<p>Console Output:</p>

<pre><code>2020-05-25 10:53:35.491127: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not 
load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found                                                           
2020-05-25 10:53:35.493137: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart 
dlerror if you do not have a GPU set up on your machine.                                                                                
2020-05-25 10:53:37.162913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully 
opened dynamic library nvcuda.dll                                                                                                   
2020-05-25 10:53:37.194951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with 
properties:                                                                                                                         
pciBusID: 0000:01:00.0 name: GeForce RTX 2060 computeCapability: 7.5                                                                                                                                                                         
coreClock: 1.755GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s                                                                                                                                               
2020-05-25 10:53:37.200604: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not 
load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found                                                           
2020-05-25 10:53:37.206365: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully 
opened dynamic library cublas64_10.dll                                                                                              
2020-05-25 10:53:37.212086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully 
opened dynamic library cufft64_10.dll                                                                                               
2020-05-25 10:53:37.214531: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully 
opened dynamic library curand64_10.dll                                                                                              
2020-05-25 10:53:37.219340: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully 
opened dynamic library cusolver64_10.dll                                                                                            
2020-05-25 10:53:37.224932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully 
opened dynamic library cusparse64_10.dll                                                                                            
2020-05-25 10:53:37.233220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully 
opened dynamic library cudnn64_7.dll                                                                                                
2020-05-25 10:53:37.235711: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some 
GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would 
like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup 
the required libraries for your platform.                                                                                                                     
Skipping registering GPU devices...                                                                                                                                                                                                          
2020-05-25 10:53:37.241553: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports 
instructions that this TensorFlow binary was not compiled to use: AVX2                                                                    
2020-05-25 10:53:37.249295: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d9e0a5c5f0 
initialized for platform Host (this does not guarantee that XLA will be used). Devices:                                              
2020-05-25 10:53:37.252889: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device 
(0): Host, Default Version                                                                                                             
2020-05-25 10:53:37.255179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect 
StreamExecutor with strength 1 edge matrix:                                                                                         
2020-05-25 10:53:37.258151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]                                                                                                                                                         
Epoch 1/10                                                                                                                                                                                                                                   
WARNING:tensorflow:Model was constructed with shape (None, 35, 2) for input Tensor(""flatten_input:0"", 
shape=(None, 35, 2), dtype=float32), but it was called on an input with incompatible shape (None, 2).                                  
WARNING:tensorflow:Model was constructed with shape (None, 35, 2) for input Tensor(""flatten_input:0"", 
shape=(None, 35, 2), dtype=float32), but it was called on an input with incompatible shape (None, 2).                                  
Traceback (most recent call last):                                                                                                                                                                                                             
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 60, in 
quick_execute                                                                                                                                 
inputs, attrs, num_outputs)                                                                                                                                                                                                              
TypeError: An op outside of the function building code is being passed                                                                                                                                                                       
a ""Graph"" tensor. It is possible to have Graph tensors                                                                                                                                                                                       
leak out of the function building context by including a                                                                                                                                                                                     
tf.init_scope in your function building code.                                                                                                                                                                                                
For example, the following function will fail:                                                                                                                                                                                                 
@tf.function                                                                                                                                                                                                                                 
def has_init_scope():                                                                                                                                                                                                                          
my_constant = tf.constant(1.)                                                                                                                                                                                                                
with tf.init_scope():                                                                                                                                                                                                                          
added = my_constant * 2                                                                                                                                                                                                                
The graph tensor has name: dense_1/Identity:0                                                                                                                                                                                                                                                                                                                                                                                                                                             
During handling of the above exception, another exception occurred:                                                                                                                                                                                                                                                                                                                                                                                                                       
Traceback (most recent call last):                                                                                                                                                                                                             
File ""addition.py"", line 58, in &lt;module&gt;                                                                                                                                                                                                       
eval_data, eval_target, epochs=10                                                                                                                                                                                                          
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 66, in 
_method_wrapper                                                                                                                       
return method(self, *args, **kwargs)                                                                                                                                                                                                       
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 848, in 
fit                                                                                                                                  
tmp_logs = train_function(iterator)                                                                                                                                                                                                        
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 580, in 
__call__                                                                                                                                
result = self._call(*args, **kwds)                                                                                                                                                                                                         
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\eager\def_function.py"", line 644, in 
_call                                                                                                                                   
return self._stateless_fn(*args, **kwds)                                                                                                                                                                                                   
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 2420, in 
__call__                                                                                                                                   
return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access                                                                                                                                                     
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 1665, in 
_filtered_call                                                                                                                             
self.captured_inputs)                                                                                                                                                                                                                      
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 1746, in 
_call_flat                                                                                                                                 
ctx, args, cancellation_manager=cancellation_manager))                                                                                                                                                                                     
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\eager\function.py"", line 598, in call                                                                                                                                        
ctx=ctx)                                                                                                                                                                                                                                   
File ""C:\Users\User\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 74, in 
quick_execute                                                                                                                                 
""tensors, but found {}"".format(keras_symbolic_tensors))                                                                                                                                                                                  
tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras 
symbolic tensors, but found [&lt;tf.Tensor 'dense_1/Identity:0' shape=(None, 70, 1) dtype=float32&gt;]
</code></pre>

<p>Any help, tips, and advice is greatly appreciated.</p>
","I am a new to tensorflow, I am trying to build a simple neural network. But every time I get close, there are a list of errors stopping me. I followed tutorials and documentations and kept most of the code and changed only things I need to. Here is my code: Console Output: Any help, tips, and advice is greatly appreciated.",https://stackoverflow.com/questions/62008061,8620607,Documentation Replication on Other Examples
62309757,"tf.data.Dataset.from_generator - TypeError: If shallow structure is a sequence, input must also be a sequence","<p>I am trying to build a generator function that will take multiple inputs and outputs to pass to a model using a series of memory mapped numpy arrays (larger than available RAM). All preprocessing has already been performed and I just need to access these arrays in batches.</p>

<pre><code>def generator_function(input1, input2, input3, input4, input5, input6, label1, label2):

    def generator():
        for input1, input2, input3, input4, input5, input6, label1, label2 in zip(input1, input2, input3, input4, input5, input6, label1, label2):
            yield {""x1_train_data"": input1, 
                   ""x2_train_data"": input2, 
                   ""x3_train_data"": input3, 
                   ""x4_train_data"": input4,
                   ""x5_train_data"": input5,
                   ""x6_train_data"": input6}, {""x1_train_label"": label1, ""x2_train_label"": label2}

    dataset = tf.data.Dataset.from_generator(generator, 
                                             output_types=({""x1_train_data"": tf.float32, 
                                                            ""x2_train_data"": tf.float32,
                                                            ""x3_train_data"": tf.float32,
                                                            ""x4_train_data"": tf.float32,
                                                            ""x5_train_data"": tf.int64,
                                                            ""x6_train_data"": tf.float32},{""x1_train_label"": tf.int64, ""x2_train_label"": tf.int64}),
                                            output_shapes=tf.TensorShape([50,150,150, 150, 150,9, 1,5]))
    dataset = dataset.batch(2)
    return dataset
</code></pre>

<p>When I attempt to fit the model:</p>

<pre><code>model.fit(generator_function(np_array1, np_array2, np_array3, np_array4, mp_array5, np_array6, np_array7, np_array8), epochs=10)
</code></pre>

<p>I get the following error:</p>

<pre><code>TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: &lt;class 'tensorflow.python.framework.tensor_shape.TensorShape'&gt;.
</code></pre>

<p>I've tried referring the official documentation and other SO posts but haven't been able to find a solution that used both multiple inputs and multiple labels. </p>
",I am trying to build a generator function that will take multiple inputs and outputs to pass to a model using a series of memory mapped numpy arrays (larger than available RAM). All preprocessing has already been performed and I just need to access these arrays in batches. When I attempt to fit the model: I get the following error: I've tried referring the official documentation and other SO posts but haven't been able to find a solution that used both multiple inputs and multiple labels.,https://stackoverflow.com/questions/62309757,4613042,Lack of Alternative Solutions/Documentation
62704943,Tensorflow Custom Gradient in a Custom Layer,"<p>I am setting up a custom layer with a custom gradient. The inputs are a single 2-D tensor of shape (?, 2). The outputs are also a single 2-D tensor with shape (?, 2).</p>
<p>I am struggling with understanding how these objects behave. What I've gathered from the documentation is that for a given input, the gradient will have the same shape as the output and that I need to return a list of gradients for each input. I've been assuming that since my inputs look like (?, 2) and my outputs look like (?, 2), then the grad function should return a length-2 list: [input_1_grad, input_2_grad], where both list items are tensors with the shape of the output, (?, 2).</p>
<p>This is not working, which is why I'm hoping someone here could help.</p>
<p>Here is my error (appears to occur at compile time):</p>
<blockquote>
<p>ValueError: Num gradients 3 generated for op name:
&quot;custom_layer/IdentityN&quot; op: &quot;IdentityN&quot; input:
&quot;custom_layer_2/concat&quot; input: &quot;custom_layer_1/concat&quot; attr {   key:
&quot;T&quot;   value {
list {
type: DT_FLOAT
type: DT_FLOAT
}   } } attr {   key: &quot;_gradient_op_type&quot;   value {
s: &quot;CustomGradient-28729&quot;   } }  do not match num inputs 2</p>
</blockquote>
<p>The other wrinkle is that the input to the custom layer is itself also a custom layer (though without a custom gradient). I will provide the code for both layers, in case it's helpful.</p>
<p>Also, note that the network compiles and runs if I don't try to specify a custom gradient. But, since my functions need help differentiating themselves, I need to manually intervene, so having a working custom gradient is critical.</p>
<p><strong>First Custom Layer (no custom gradient):</strong></p>
<pre><code>class custom_layer_1(tensorflow.keras.layers.Layer):
    def __init__(self):
        super(custom_layer_1, self).__init__()
    
    def build(self, input_shape):
        self.term_1 = self.add_weight('term_1', trainable=True)
        self.term_2 = self.add_weight('term_2', trainable=True)
    
    def call(self, x):
        self.term_1 = formula in terms of x
        self.term_2 = another formula in terms of x
        
        return tf.concat([self.term_1, self.term_2], axis=1)
</code></pre>
<p><strong>Second Custom Layer (with the custom gradient):</strong></p>
<pre><code>class custom_layer_2(tensorflow.keras.layers.Layer):
    ### the inputs
    # x is the concatenation of term_1 and term_2
    def __init__(self):
        super(custom_layer_2, self).__init__()
    
    def build(self, input_shape):
        #self.weight_1 = self.add_weight('weight_1', trainable=True)
        #self.weight_2 = self.add_weight('weight_2', trainable=True)
    
    def call(self, x):
        return custom_function(x)
</code></pre>
<p><strong>The Custom Function:</strong></p>
<pre><code>@tf.custom_gradient
def custom_function(x):
    ### the inputs
    # x is a concatenation of term_1 and term_2
    
    weight_1 = function in terms of x
    weight_2 = another function in terms of x
    
    ### the gradient
    def grad(dy):
        # assuming dy has the output shape of (?, 2). could be wrong.
        d_weight_1 = K.reshape(dy[:, 0], shape=(K.shape(x)[0], 1))
        d_weight_1 = K.reshape(dy[:, 1], shape=(K.shape(x)[0], 1))
        
        term_1 = K.reshape(x[:, 0], shape=(K.shape(x)[0], 1))
        term_2 = K.reshape(x[:, 1], shape=(K.shape(x)[0], 1))
        
        d_weight_1_d_term_1 = tf.where(K.equal(term_1, K.zeros_like(term_1)), K.zeros_like(term_1), -term_2 / term_1) * d_weight_1
        d_weight_1_d_term_2 = tf.where(K.equal(term_1, K.zeros_like(term_1)), K.zeros_like(term_1), 1 / term_1) * d_weight_1
        
        d_weight_2_d_term_1 = tf.where(K.equal(term_2, K.zeros_like(term_2)), K.zeros_like(term_1), 2 * term_1 / term_2) * d_weight_2
        d_weight_2_d_term_2 = tf.where(K.equal(term_2, K.zeros_like(term_2)), K.zeros_like(term_1), -K.square(term_1 / term_2)) * d_weight_2
        
        return tf.concat([d_weight_1_d_term_1, d_weight_1_d_term_2], axis=1), tf.concat([d_weight_2_d_term_1, d_weight_2_d_term_2], axis=1)
  
  return tf.concat([weight_1, weight_2], axis=1), grad
</code></pre>
<p>Any help would be much appreciated!</p>
","I am setting up a custom layer with a custom gradient. The inputs are a single 2-D tensor of shape (?, 2). The outputs are also a single 2-D tensor with shape (?, 2). I am struggling with understanding how these objects behave. What I've gathered from the documentation is that for a given input, the gradient will have the same shape as the output and that I need to return a list of gradients for each input. I've been assuming that since my inputs look like (?, 2) and my outputs look like (?, 2), then the grad function should return a length-2 list: [input_1_grad, input_2_grad], where both list items are tensors with the shape of the output, (?, 2). This is not working, which is why I'm hoping someone here could help. Here is my error (appears to occur at compile time): The other wrinkle is that the input to the custom layer is itself also a custom layer (though without a custom gradient). I will provide the code for both layers, in case it's helpful. Also, note that the network compiles and runs if I don't try to specify a custom gradient. But, since my functions need help differentiating themselves, I need to manually intervene, so having a working custom gradient is critical. First Custom Layer (no custom gradient): Second Custom Layer (with the custom gradient): The Custom Function: Any help would be much appreciated!",https://stackoverflow.com/questions/62704943,13855733,Requesting (Additional) Resources
62912769,Min_delta on callbacks function of Keras model seems to not take effect,"<p>I am using the following callbacks function on a Keras model and I initialize the minimum delta to 0.002, so based on the documentation of Tensorflow/Keras any improvement in the validation loss function less than 0.002 won't be counted for an improvement. However, this seems to not get implemented in my case.</p>
<p>callback function:</p>
<pre class=""lang-py prettyprint-override""><code>def callback(folder_path, saved_model_name, patience_value, logdir, hparams):
    
    # Initialize parameters
    monitor_metric = 'val_loss'
    minimum_delta = 0.002
    patience_limit = patience_value
    verbose_value = 1
    mode_value = 'min'
    weights_fname = os.path.join(os.getcwd(), '{0}/{1}.h5'.format(folder_path, saved_model_name))
    
    # Initialize callbacks
    callbacks = [
        
        EarlyStopping(monitor=monitor_metric,
                      min_delta=minimum_delta,
                      patience=patience_limit,
                      verbose=verbose_value,
                      mode=mode_value,
                      restore_best_weights=True),

        ModelCheckpoint(filepath=weights_fname,
                        monitor=monitor_metric,
                        verbose=verbose_value,
                        save_best_only=True,
                        save_weights_only=True),
        
        tf.keras.callbacks.TensorBoard(logdir),
        
        hp.KerasCallback(logdir, hparams)
    ]
    
    return callbacks
</code></pre>
<p>The output per training epoch</p>
<p><a href=""https://i.stack.imgur.com/K4U1q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K4U1q.png"" alt=""enter image description here"" /></a></p>
<p>You can see that in two highlighted epochs the validation loss improved from</p>
<ul>
<li>0.02129 - 0.02015 = 0.00114 &lt; 0.002 (although it was counted as an improvement)</li>
<li>0.01880 - 0.01803 = 0.00077 &lt; 0.002 (also counted as an improvement in validation loss)</li>
</ul>
<p>What is going wrong?</p>
<p>Please also check my colab <a href=""https://colab.research.google.com/drive/1FqQjjB2xFFQCiAqZacvVsl5SU0kcJwGd?usp=sharing"" rel=""nofollow noreferrer"">notebook</a>, in order to check the whole training process</p>
","I am using the following callbacks function on a Keras model and I initialize the minimum delta to 0.002, so based on the documentation of Tensorflow/Keras any improvement in the validation loss function less than 0.002 won't be counted for an improvement. However, this seems to not get implemented in my case. callback function: The output per training epoch You can see that in two highlighted epochs the validation loss improved from What is going wrong? Please also check my colab notebook, in order to check the whole training process",https://stackoverflow.com/questions/62912769,10623444,Requesting (Additional) Resources
63068695,Getting error when trying to load data using keras.utils.get_file(),"<p>I am getting an error when trying to load a dataset using TensorFlow Keras. Here is the code:</p>
<pre><code>dataset_url = &quot;https://storage.googleapis.com/sample_org/sample_file.zip&quot;
data_dir = tf.keras.utils.get_file(origin=dataset_url, 
                                   fname='sample_file', 
                                   extract=True)
data_dir = pathlib.Path(data_dir)
</code></pre>
<p><strong>I have changed the URL to 'sample_file.zip' for security reasons.</strong></p>
<p>This is the error that I am getting:</p>
<pre><code>---------------------------------------------------------------------------
FileExistsError                           Traceback (most recent call last)
&lt;ipython-input-12-1cdc186b0389&gt; in &lt;module&gt;()
      2 data_dir = tf.keras.utils.get_file(origin=dataset_url, 
      3                                    fname='sample_file',
----&gt; 4                                    extract=True)
      5 data_dir = pathlib.Path(data_dir)

3 frames
/usr/lib/python3.6/zipfile.py in _extract_member(self, member, targetpath, pwd)
   1572         if member.is_dir():
   1573             if not os.path.isdir(targetpath):
-&gt; 1574                 os.mkdir(targetpath)
   1575             return targetpath
   1576 

FileExistsError: [Errno 17] File exists: '/root/.keras/datasets/sample_file'
</code></pre>
<p>What is causing this error? How can I fix it?</p>
<p>This is the first block of code run after imports so I don't know why 'FileExistsError' happens.</p>
<p>I have tried changing the name of the file.</p>
<p>I have checked TensorFlow documentation and it uses code like this:</p>
<pre><code>dataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;
data_dir = tf.keras.utils.get_file(origin=dataset_url, 
                                   fname='flower_photos', 
                                   untar=True)
data_dir = pathlib.Path(data_dir)
</code></pre>
<p>I have run this above code and it is working. But I cannot figure out why the same code is showing error for my data. Please advice.</p>
",I am getting an error when trying to load a dataset using TensorFlow Keras. Here is the code: I have changed the URL to 'sample_file.zip' for security reasons. This is the error that I am getting: What is causing this error? How can I fix it? This is the first block of code run after imports so I don't know why 'FileExistsError' happens. I have tried changing the name of the file. I have checked TensorFlow documentation and it uses code like this: I have run this above code and it is working. But I cannot figure out why the same code is showing error for my data. Please advice.,https://stackoverflow.com/questions/63068695,12424846,Documentation Replication on Other Examples
63175471,How to control if input features contribute exclusively to one neuron in subsequent layer of a Tensorflow neural network?,"<p>I'm trying to make the most basic of basic neural networks to get familiar with functional API in Tensorflow 2.x.</p>
<p>Basically what I'm trying to do is the following with my simplified iris dataset (i.e. setosa or not)</p>
<ol>
<li>Use the 4 features as input</li>
<li>Dense layer of 3</li>
<li>Sigmoid activation function</li>
<li>Dense layer of 2 (one for each class)</li>
<li>Softmax activation</li>
<li>Binary cross entropy / log-loss as my loss function</li>
</ol>
<p>However, I can't figure out how to control one key aspect of the model.  That is, <strong>how can I ensure that each feature from my input layer contributes to only one neuron in my subsequent dense layer?</strong>  Also, how can I allow a feature to contribute to more than one neuron?</p>
<p>This isn't clear to me from the documentation.</p>
<pre><code># Load data
from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris()
X, y = load_iris(return_X_y=True, as_frame=True)
X = X.astype(&quot;float32&quot;)
X.index = X.index.map(lambda i: &quot;iris_{}&quot;.format(i))
X.columns = X.columns.map(lambda j: j.split(&quot; (&quot;)[0].replace(&quot; &quot;,&quot;_&quot;))
y.index = X.index
y = y.map(lambda i:iris.target_names[i])
y_simplified = y.map(lambda i: {True:1, False:0}[i == &quot;setosa&quot;])
y_simplified = pd.get_dummies(y_simplified, columns=[&quot;setosa&quot;, &quot;not_setosa&quot;])

# Traing test split
from sklearn.model_selection import train_test_split
seed=0
X_train,X_test, y_train,y_test= train_test_split(X,y_simplified, test_size=0.3, random_state=seed)

# Simple neural network
import tensorflow as tf
tf.random.set_seed(seed)


# Input[4 features] -&gt; Dense layer of 3 neurons -&gt; Activation function -&gt; Dense layer of 2 (one per class) -&gt; Softmax
inputs = tf.keras.Input(shape=(4))
x = tf.keras.layers.Dense(3)(inputs)
x = tf.keras.layers.Activation(tf.nn.sigmoid)(x)
x = tf.keras.layers.Dense(2)(x)
outputs = tf.keras.layers.Activation(tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs, name=&quot;simple_binary_iris&quot;)
model.compile(loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;] )
model.summary()

history = model.fit(X_train, y_train, batch_size=64, epochs=10, validation_split=0.2)

test_scores = model.evaluate(X_test, y_test)
print(&quot;Test loss:&quot;, test_scores[0])
print(&quot;Test accuracy:&quot;, test_scores[1])
</code></pre>
<p>Results:</p>
<pre><code>Model: &quot;simple_binary_iris&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_44 (InputLayer)        [(None, 4)]               0         
_________________________________________________________________
dense_96 (Dense)             (None, 3)                 15        
_________________________________________________________________
activation_70 (Activation)   (None, 3)                 0         
_________________________________________________________________
dense_97 (Dense)             (None, 2)                 8         
_________________________________________________________________
activation_71 (Activation)   (None, 2)                 0         
=================================================================
Total params: 23
Trainable params: 23
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
2/2 [==============================] - 0s 40ms/step - loss: 0.6344 - accuracy: 0.6667 - val_loss: 0.6107 - val_accuracy: 0.7143
Epoch 2/10
2/2 [==============================] - 0s 6ms/step - loss: 0.6302 - accuracy: 0.6667 - val_loss: 0.6083 - val_accuracy: 0.7143
Epoch 3/10
2/2 [==============================] - 0s 7ms/step - loss: 0.6278 - accuracy: 0.6667 - val_loss: 0.6056 - val_accuracy: 0.7143
Epoch 4/10
2/2 [==============================] - 0s 7ms/step - loss: 0.6257 - accuracy: 0.6667 - val_loss: 0.6038 - val_accuracy: 0.7143
Epoch 5/10
2/2 [==============================] - 0s 7ms/step - loss: 0.6239 - accuracy: 0.6667 - val_loss: 0.6014 - val_accuracy: 0.7143
Epoch 6/10
2/2 [==============================] - 0s 7ms/step - loss: 0.6223 - accuracy: 0.6667 - val_loss: 0.6002 - val_accuracy: 0.7143
Epoch 7/10
2/2 [==============================] - 0s 7ms/step - loss: 0.6209 - accuracy: 0.6667 - val_loss: 0.5989 - val_accuracy: 0.7143
Epoch 8/10
2/2 [==============================] - 0s 7ms/step - loss: 0.6195 - accuracy: 0.6667 - val_loss: 0.5967 - val_accuracy: 0.7143
Epoch 9/10
2/2 [==============================] - 0s 7ms/step - loss: 0.6179 - accuracy: 0.6667 - val_loss: 0.5953 - val_accuracy: 0.7143
Epoch 10/10
2/2 [==============================] - 0s 7ms/step - loss: 0.6166 - accuracy: 0.6667 - val_loss: 0.5935 - val_accuracy: 0.7143
2/2 [==============================] - 0s 607us/step - loss: 0.6261 - accuracy: 0.6444
Test loss: 0.6261375546455383
Test accuracy: 0.644444465637207
</code></pre>
","I'm trying to make the most basic of basic neural networks to get familiar with functional API in Tensorflow 2.x. Basically what I'm trying to do is the following with my simplified iris dataset (i.e. setosa or not) However, I can't figure out how to control one key aspect of the model. That is, how can I ensure that each feature from my input layer contributes to only one neuron in my subsequent dense layer? Also, how can I allow a feature to contribute to more than one neuron? This isn't clear to me from the documentation. Results:",https://stackoverflow.com/questions/63175471,678572,Documentation Ambiguity
63253714,"What numbers go into DCGAN Generator models, in order to produce larger images","<p>I really have tried to do my due diligence here but I can't find a lot of documentation on why certain numbers are chosen. I'm also fairly hazy on how convolutions work in generators (have a better understanding in terms of classifiers) so that's not helping my case. I think my question should be pretty simple to address for some more experiences folks out there though.</p>
<p>Take Google's tutorial for example, the Generator class:</p>
<pre><code>def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    return model
</code></pre>
<p>Where is 7x7x256 coming from? I understand that 7x7 is a multiple of the eventual 28x28 size, so that makes sense somewhat, but what is the 256 all about? And then in the following layers, I notice a pattern but I'm not sure how to re-write it so it works for a wholly different image size. Any help or direction is appreciated.Thanks!</p>
<p>EDIT:
Thanks to the helpful input I changed my gen to:</p>
<pre><code>def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((8, 8, 256)))
    assert model.output_shape == (None, 8, 8, 256) # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 8, 8, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 16, 16, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 32, 32, 32)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 64, 64, 16)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(8, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 128, 128, 8)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 256, 256, 3)

    return model
</code></pre>
<p>and discriminator:</p>
<pre><code>def make_discriminator_model():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(8, (5, 5), strides=(2, 2), padding='same',
                                     input_shape=[IMAGE_DIM[0], IMAGE_DIM[1], IMAGE_DIM[2]]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))
    print(model.output_shape)

    model.add(layers.Conv2D(16, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))
    print(model.output_shape)

    model.add(layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))
    print(model.output_shape)

    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))
    print(model.output_shape)

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))
    print(model.output_shape)

    model.add(layers.Conv2D(256, (5, 5), strides=(1, 1), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))
    print(model.output_shape)

    model.add(layers.Flatten())
    model.add(layers.Dense(1))
    #16384 65536
    return model
</code></pre>
","I really have tried to do my due diligence here but I can't find a lot of documentation on why certain numbers are chosen. I'm also fairly hazy on how convolutions work in generators (have a better understanding in terms of classifiers) so that's not helping my case. I think my question should be pretty simple to address for some more experiences folks out there though. Take Google's tutorial for example, the Generator class: Where is 7x7x256 coming from? I understand that 7x7 is a multiple of the eventual 28x28 size, so that makes sense somewhat, but what is the 256 all about? And then in the following layers, I notice a pattern but I'm not sure how to re-write it so it works for a wholly different image size. Any help or direction is appreciated.Thanks! EDIT: Thanks to the helpful input I changed my gen to: and discriminator:",https://stackoverflow.com/questions/63253714,1135125,Lack of Alternative Solutions/Documentation
63574871,How to get the Keras history object when you abort training?,"<p>When I train with tensorflow 2.0 / Keras APIs, I usually do something like this</p>
<pre><code>model = tf.keras.Model(inputs, outputs)
history = model.fit(x, y, batch_size=64, epochs=10)
</code></pre>
<p>But sometimes things in life don't work out how I planned and I need to abort with ctrl-c or pressing stop in Jupyter notebook.
How can I still get the history object when I abort training early? I can't find any detailed documentation for how to get history.</p>
","When I train with tensorflow 2.0 / Keras APIs, I usually do something like this But sometimes things in life don't work out how I planned and I need to abort with ctrl-c or pressing stop in Jupyter notebook. How can I still get the history object when I abort training early? I can't find any detailed documentation for how to get history.",https://stackoverflow.com/questions/63574871,8202708,Documentation Completeness
63587813,Tensorflow - Conv2D batch_input_shape array shape error,"<p>I am trying to create a basic CNN in tensor flow using some custom dataset from 2D np arrays.
I cant seem to get the input data to line up with the input_shape or batch_input_shape parameter for the convolutional layer. I have tried every order of variables and the same as the documentation, but am unsure why it still produces an error.</p>
<p>Any help would be greatly appreciated!</p>
<pre><code>import os 
import pickle
import pandas as pd
import matplotlib as plt
import numpy as np

import tensorflow as tf

from tensorflow.keras import models, datasets, layers
</code></pre>
<pre><code>BATCH_SIZE = 4
TRAIN_SPLIT = 0.8
VAL_SPLIT = 0.1
TEST_SPLIT = 0.1
</code></pre>
<pre><code>with open((CWD+'/CLNY_X.npy'), mode='rb') as f:
    Xt = np.load(f, allow_pickle=True)
with open((CWD+'/CLNY_Y.npy'), mode='rb') as f:
    Y = np.load(f, allow_pickle=True)

X = Xt.reshape(Xt.shape + (1,))
DATASIZE = Y.shape[0]
print(&quot;Datasize: &quot;, DATASIZE)
</code></pre>
<pre><code>Datasize:  172
</code></pre>
<pre><code># test out with different period moving averages, so we take the
dataset = tf.data.Dataset.from_tensor_slices((X, Y))
</code></pre>
<pre><code>for feat, targ in dataset.take(1):
    print('NRows: {}, NCols: {}, Target: {}\nFeat: {}'.format(len(feat), len(feat[0]), targ, feat))
</code></pre>
<pre><code>NRows: 10000, NCols: 10, Target: 0.2587999999523163
Feat: [[[5.0292000e+01]
  [1.5998565e-01]
  [7.5094378e-01]
  ...
  [1.0000000e+00]
  [2.5231593e-05]
  [1.4535466e-01]]

 [[5.0492001e+01]
  [2.9965147e-01]
  [1.4065099e+00]
  ...
  [1.8729897e+00]
  [4.7258512e-05]
  [2.7224776e-01]]

 [[5.0692001e+01]
  [2.9965451e-01]
  [1.4065243e+00]
  ...
  [1.8730087e+00]
  [4.7258993e-05]
  [2.7225053e-01]]

 ...

 [[0.0000000e+00]
  [0.0000000e+00]
  [0.0000000e+00]
  ...
  [0.0000000e+00]
  [0.0000000e+00]
  [0.0000000e+00]]

 [[0.0000000e+00]
  [0.0000000e+00]
  [0.0000000e+00]
  ...
  [0.0000000e+00]
  [0.0000000e+00]
  [0.0000000e+00]]

 [[0.0000000e+00]
  [0.0000000e+00]
  [0.0000000e+00]
  ...
  [0.0000000e+00]
  [0.0000000e+00]
  [0.0000000e+00]]]
</code></pre>
<pre><code>train_size = int(DATASIZE*TRAIN_SPLIT)
val_size = int(DATASIZE*VAL_SPLIT)
test_size = int(DATASIZE*TEST_SPLIT)

dataset = dataset.shuffle(DATASIZE)
train_dataset = dataset.take(train_size).batch(BATCH_SIZE)
test_dataset = dataset.skip(train_size)
val_dataset = dataset.skip(test_size)
test_dataset = dataset.take(test_size)

CONVERTED_LENGTH = 10000
CONVERTED_WIDTH = 10
</code></pre>
<pre><code>model = models.Sequential()
#model.add(layers.Conv1D(32, kernel_size=(10), activation='relu', data_format='channels_last', batch_input_shape=(CONVERTED_LENGTH, CONVERTED_WIDTH, 1)))
model.add(layers.Conv2D(32, kernel_size=(2, 2), activation='relu', batch_input_shape=(CONVERTED_LENGTH, CONVERTED_WIDTH, BATCH_SIZE, 1)))
model.add(layers.Flatten())
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(1, activation='softmax'))
model.summary()
</code></pre>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (10000, 9, 3, 32)         160       
_________________________________________________________________
flatten (Flatten)            (10000, 864)              0         
_________________________________________________________________
dense (Dense)                (10000, 32)               27680     
_________________________________________________________________
dense_1 (Dense)              (10000, 1)                33        
=================================================================
Total params: 27,873
Trainable params: 27,873
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<pre><code>model.compile(optimizer='adam',
             loss=tf.keras.losses.MeanSquaredError(),
             metrics=['accuracy'])

history = model.fit(train_dataset, epochs=10, validation_data=(val_dataset)) # add the validation_data=(test_data, test_targets)
</code></pre>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-c0e1d31b7f23&gt; in &lt;module&gt;
      3              metrics=['accuracy'])
      4 
----&gt; 5 history = model.fit(train_dataset, epochs=10, validation_data=(val_dataset)) # add the validation_data=(test_data, test_targets)

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--&gt; 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    233           max_queue_size=max_queue_size,
    234           workers=workers,
--&gt; 235           use_multiprocessing=use_multiprocessing)
    236 
    237       total_samples = _get_total_number_of_samples(training_data_adapter)

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    591         max_queue_size=max_queue_size,
    592         workers=workers,
--&gt; 593         use_multiprocessing=use_multiprocessing)
    594     val_adapter = None
    595     if validation_data:

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in _process_inputs(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)
    704       max_queue_size=max_queue_size,
    705       workers=workers,
--&gt; 706       use_multiprocessing=use_multiprocessing)
    707 
    708   return adapter

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, standardize_function, **kwargs)
    700 
    701     if standardize_function is not None:
--&gt; 702       x = standardize_function(x)
    703 
    704     # Note that the dataset instance is immutable, its fine to reusing the user

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in standardize_function(dataset)
    682           return x, y
    683         return x, y, sample_weights
--&gt; 684       return dataset.map(map_fn, num_parallel_calls=dataset_ops.AUTOTUNE)
    685 
    686   if mode == ModeKeys.PREDICT:

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in map(self, map_func, num_parallel_calls)
   1589     else:
   1590       return ParallelMapDataset(
-&gt; 1591           self, map_func, num_parallel_calls, preserve_cardinality=True)
   1592 
   1593   def flat_map(self, map_func):

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
   3924         self._transformation_name(),
   3925         dataset=input_dataset,
-&gt; 3926         use_legacy_function=use_legacy_function)
   3927     self._num_parallel_calls = ops.convert_to_tensor(
   3928         num_parallel_calls, dtype=dtypes.int32, name=&quot;num_parallel_calls&quot;)

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
   3145       with tracking.resource_tracker_scope(resource_tracker):
   3146         # TODO(b/141462134): Switch to using garbage collection.
-&gt; 3147         self._function = wrapper_fn._get_concrete_function_internal()
   3148 
   3149         if add_to_graph:

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\eager\function.py in _get_concrete_function_internal(self, *args, **kwargs)
   2393     &quot;&quot;&quot;Bypasses error checking when getting a graph function.&quot;&quot;&quot;
   2394     graph_function = self._get_concrete_function_internal_garbage_collected(
-&gt; 2395         *args, **kwargs)
   2396     # We're returning this concrete function to someone, and they may keep a
   2397     # reference to the FuncGraph without keeping a reference to the

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\eager\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2387       args, kwargs = None, None
   2388     with self._lock:
-&gt; 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2390     return graph_function
   2391 

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\eager\function.py in _maybe_define_function(self, args, kwargs)
   2701 
   2702       self._function_cache.missed.add(call_context_key)
-&gt; 2703       graph_function = self._create_graph_function(args, kwargs)
   2704       self._function_cache.primary[cache_key] = graph_function
   2705       return graph_function, args, kwargs

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2591             arg_names=arg_names,
   2592             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2593             capture_by_value=self._capture_by_value),
   2594         self._function_attributes,
   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    976                                           converted_func)
    977 
--&gt; 978       func_outputs = python_func(*func_args, **func_kwargs)
    979 
    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in wrapper_fn(*args)
   3138           attributes=defun_kwargs)
   3139       def wrapper_fn(*args):  # pylint: disable=missing-docstring
-&gt; 3140         ret = _wrapper_helper(*args)
   3141         ret = structure.to_tensor_list(self._output_structure, ret)
   3142         return [ops.convert_to_tensor(t) for t in ret]

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in _wrapper_helper(*args)
   3080         nested_args = (nested_args,)
   3081 
-&gt; 3082       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
   3083       # If `func` returns a list of tensors, `nest.flatten()` and
   3084       # `ops.convert_to_tensor()` would conspire to attempt to stack

C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\autograph\impl\api.py in wrapper(*args, **kwargs)
    235       except Exception as e:  # pylint:disable=broad-except
    236         if hasattr(e, 'ag_error_metadata'):
--&gt; 237           raise e.ag_error_metadata.to_exception(e)
    238         else:
    239           raise

ValueError: in converted code:

    C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py:677 map_fn
        batch_size=None)
    C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py:2410 _standardize_tensors
        exception_prefix='input')
    C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_utils.py:582 standardize_input_data
        str(data_shape))

    ValueError: Error when checking input: expected conv2d_input to have shape (10, 4, 1) but got array with shape (10000, 10, 1)
</code></pre>
<p>It always says that either the input data is not in the expected format or that the ndims is wrong as it adds None to some of the values. I just can't make it run!!</p>
","I am trying to create a basic CNN in tensor flow using some custom dataset from 2D np arrays. I cant seem to get the input data to line up with the input_shape or batch_input_shape parameter for the convolutional layer. I have tried every order of variables and the same as the documentation, but am unsure why it still produces an error. Any help would be greatly appreciated! It always says that either the input data is not in the expected format or that the ndims is wrong as it adds None to some of the values. I just can't make it run!!",https://stackoverflow.com/questions/63587813,9807578,Documentation Replicability
63645132,Normalizing difference between x_train /= 255.0 and x_train = x_train/255.0,"<p>I have some simple code, which loads the mnist data and normalizes the images.</p>
<pre><code>    mnist = tf.keras.datasets.mnist

    (x_train, y_train),(x_test, y_test) = mnist.load_data()
    x_train = x_train/255.0
    x_test = x_test/255.0

</code></pre>
<p>The code above works, however, if I try to use the shorthand for division, I get an error:</p>
<pre><code>    mnist = tf.keras.datasets.mnist

    (x_train, y_train),(x_test, y_test) = mnist.load_data()
    x_train /= 255.0
    x_test /= 255.0
</code></pre>
<p>The error is as follows:
<code>TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'B') according to the casting rule ''same_kind''</code></p>
<p>By playing around, I found a fix to it, in that typecasting <code>x_train</code> to <code>float32</code>, would get rid of the error, but I only stumbled upon the fix by accident. I don't understand why the code below fixes the issue</p>
<pre><code>    mnist = tf.keras.datasets.mnist

    (x_train, y_train),(x_test, y_test) = mnist.load_data(path=path)
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255.0
    x_test /= 255.0
</code></pre>
<p>Could someone explain what's happening here? Why do the two versions behave differently? Why's an explicit case required in the second instance but not the first?
<br> I didn't have much luck finding this behaviour documented anywhere.</p>
<p>Edit: I'm not sure what additional 'debugging details' I'm required to provide, since I've basically provided the entire code, the results as well as the details which I did not understand. I've also received no comments explaining why the question was closed, and/or what additional information is expected here. I would like some constructive criticism so as to atleast be able to ask the question in a better manner, if the present form isn't satisfactory by itself.</p>
","I have some simple code, which loads the mnist data and normalizes the images. The code above works, however, if I try to use the shorthand for division, I get an error: The error is as follows: TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'B') according to the casting rule ''same_kind'' By playing around, I found a fix to it, in that typecasting x_train to float32, would get rid of the error, but I only stumbled upon the fix by accident. I don't understand why the code below fixes the issue Could someone explain what's happening here? Why do the two versions behave differently? Why's an explicit case required in the second instance but not the first? I didn't have much luck finding this behaviour documented anywhere. Edit: I'm not sure what additional 'debugging details' I'm required to provide, since I've basically provided the entire code, the results as well as the details which I did not understand. I've also received no comments explaining why the question was closed, and/or what additional information is expected here. I would like some constructive criticism so as to atleast be able to ask the question in a better manner, if the present form isn't satisfactory by itself.",https://stackoverflow.com/questions/63645132,12279039,Lack of Alternative Solutions/Documentation
63758810,Use my model to predict output in tensor flow 2 without using keras,"<p>I am new to TensorFlow and I have a very basic question. I have found several posts regarding this question for the previous TensorFlow versions but I could not use the answer for TensorFlow 2 which I am using. The examples I found in the documentation in the original site use Keras.</p>
<p>Now, about my question, say, I have built my own model using only TensorFlow without using Keras. I have finished training my model and now I want to use my trained model to predict output for some input I give.</p>
<p>I am starting out very simple in order to learn to use TensorFlow 2. I am stuck here and it would be of great help if someone provides me a solution. I have attached my snippet of code herewith.</p>
<pre><code># Defining input and output placeholders
x = tf.compat.v1.placeholder(tf.float32, shape=(128, 128, 1, 1)) # Placeholder for input
y = tf.compat.v1.placeholder(tf.float32, shape=(128, 128, 1, 1)) # Placeholder for ground truth 

inp = np.ones([128,128,1,1]).astype(np.float32) # Dummy input dataset I made
                                                # I will use it both for input and ground truth to train my model
# My neural network model
pred = my_model(x)

# Defining my optimizer, I am using gradient descent and l2 norm loss
l2_loss = tf.nn.l2_loss(tf.subtract(pred, y), name=None)
optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.000001

# Training the model
training_iters = 300
init = tf.compat.v1.global_variables_initializer()
with tf.compat.v1.Session() as sess:
  sess.run(init)
  summary_writer = tf.compat.v1.summary.FileWriter('./Output', sess.graph)
  for i in range(training_iters):
        dataset_dict = {x: inp, y: batch_y}
        opt = sess.run(optimizer.minimize(l2_loss), feed_dict = dataset_dict)
        loss = sess.run(l2_loss, feed_dict={x: batch_x, y: batch_y})
        print(loss)
  summary_writer.close()

# Use my trained model to predict output for some input I give
# ??
# ??
</code></pre>
","I am new to TensorFlow and I have a very basic question. I have found several posts regarding this question for the previous TensorFlow versions but I could not use the answer for TensorFlow 2 which I am using. The examples I found in the documentation in the original site use Keras. Now, about my question, say, I have built my own model using only TensorFlow without using Keras. I have finished training my model and now I want to use my trained model to predict output for some input I give. I am starting out very simple in order to learn to use TensorFlow 2. I am stuck here and it would be of great help if someone provides me a solution. I have attached my snippet of code herewith.",https://stackoverflow.com/questions/63758810,7341905,Documentation Replicability
64092664,"KerasClassifier fails to fit model, despite everything working fine otherwise","<p>I'm trying to use a <code>KerasClassifier</code> wrapper in order to make my workflow
scikit-friendly. However, when I try to use it with the following function, it
gives an error; training the model using native Keras model <code>fit()</code> works.
(this is Tensorflow 2.2.0, running in a conda environment)</p>
<pre class=""lang-py prettyprint-override""><code>def model_arch(n_features: int):
    i = tf.keras.layers.Input(shape=(n_features,))

    hidden_dense = tf.keras.layers.Dense(64)(i)
    hidden_dense = tf.keras.layers.BatchNormalization()(hidden_dense)
    hidden_dense = tf.keras.layers.Activation(tf.nn.tanh)(hidden_dense)

    o = tf.keras.layers.Dense(1)(hidden_dense)
    o = tf.keras.layers.BatchNormalization()(o)
    o = tf.keras.layers.Activation(&quot;sigmoid&quot;)(o)

    classifier = tf.keras.models.Model(inputs=i, outputs=o)

    opt = tf.keras.optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)
    classifier.compile(
        loss=&quot;binary_crossentropy&quot;,
        optimizer=opt,
        metrics=[&quot;accuracy&quot;],
    )
    
    return classifier
</code></pre>
<p>The following works:</p>
<pre class=""lang-py prettyprint-override""><code>X = np.random.random((100,3))
y = np.random.random((100,)) # 'y' is a binary vector in reality

clf = model_arch(3)
clf.fit(X, y, epochs=10)
</code></pre>
<p>However, when I try to use <code>KerasClassifier</code> wrapper, I get an error:</p>
<pre class=""lang-py prettyprint-override""><code>clf = KerasClassifier(model_arch(3), epochs=10)
clf.fit(X, y)

# ValueError: The first argument to `Layer.call` must always be passed.
</code></pre>
<p>Every example I have seen on the internet seems to do the same as I: define a
function that returns a compiled keras model, then pass it to the wrapper, and
fit it or use in a pipeline. The only difference I notice is that most (if not
all) examples use the <code>Sequential</code> API instead of the functional API, but afaik
that should not be a problem, right?</p>
<p>Tensorflow documentation doesn't seem to give any example of what kind of
function we should pass to the wrapper, but since every example uses one similar
to mine, I think that's correct.</p>
<p>Can anyone shed some light? Thanks.</p>
<p><strong>EDIT</strong> (after comments):</p>
<p>I import the KerasClassifier like this:</p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
</code></pre>
<p>Error log:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/adrian/miniconda3/envs/kaggle/lib/python3.8/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py&quot;, line 223, in fit
    return super(KerasClassifier, self).fit(x, y, **kwargs)
  File &quot;/home/adrian/miniconda3/envs/kaggle/lib/python3.8/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py&quot;, line 154, in fit
    self.model = self.build_fn(
  File &quot;/home/adrian/miniconda3/envs/kaggle/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 799, in __call__
    raise ValueError(
ValueError: The first argument to `Layer.call` must always be passed.
</code></pre>
","I'm trying to use a KerasClassifier wrapper in order to make my workflow scikit-friendly. However, when I try to use it with the following function, it gives an error; training the model using native Keras model fit() works. (this is Tensorflow 2.2.0, running in a conda environment) The following works: However, when I try to use KerasClassifier wrapper, I get an error: Every example I have seen on the internet seems to do the same as I: define a function that returns a compiled keras model, then pass it to the wrapper, and fit it or use in a pipeline. The only difference I notice is that most (if not all) examples use the Sequential API instead of the functional API, but afaik that should not be a problem, right? Tensorflow documentation doesn't seem to give any example of what kind of function we should pass to the wrapper, but since every example uses one similar to mine, I think that's correct. Can anyone shed some light? Thanks. EDIT (after comments): I import the KerasClassifier like this: Error log:",https://stackoverflow.com/questions/64092664,6560267,Inadequate Examples
64451250,How to load a Tensorflow model saved with make_image_classifier tool,"<p>I've made a custom image classifier model using a Tensorflow tool called make_image_classifier
<a href=""https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier"" rel=""nofollow noreferrer"">https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier</a></p>
<p>Now the model is exported into a .pb file and also 2 folders, assets and variables.</p>
<p>The question is how can I use this custom model to make predictions?
I've gone through all TF documentation and have tried many different things over these days but found no solution.</p>
<p>Someone wrote about it when he found no clear information, so he created a guide but it also doesn't work for me. In &quot;step 3&quot; its all the code required to load the module and classify an image using the custom model. The problem with this is I need to know the name of the input and output node, and I don't have them. I've tried to find them using Netron but it didn't work.
<a href=""https://heartbeat.fritz.ai/automl-vision-edge-exporting-and-loading-tensorflow-saved-models-with-python-f4e8ce1b943a"" rel=""nofollow noreferrer"">https://heartbeat.fritz.ai/automl-vision-edge-exporting-and-loading-tensorflow-saved-models-with-python-f4e8ce1b943a</a></p>
<pre><code>import tensorflow as tf
export_path = '/Users/aayusharora/Aftershoot/backend/loadmodel/models/'
with tf.Session(graph=tf.Graph()) as sess:
  tf.saved_model.loader.load(sess, ['serve'], export_path)
path = '/Users/aayusharora/Aftershoot/backend/sampleImage.jpg'
with open(path, &quot;rb&quot;) as img_file:
  y_pred = sess.run('tile:0', feed_dict={'normalised_input_image_tensor': [img_file.read()] })
print(y_pred)
</code></pre>
<p>Can someone please give me a clue about how to load a saved model and use it to make predictions?</p>
","I've made a custom image classifier model using a Tensorflow tool called make_image_classifier https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier Now the model is exported into a .pb file and also 2 folders, assets and variables. The question is how can I use this custom model to make predictions? I've gone through all TF documentation and have tried many different things over these days but found no solution. Someone wrote about it when he found no clear information, so he created a guide but it also doesn't work for me. In ""step 3"" its all the code required to load the module and classify an image using the custom model. The problem with this is I need to know the name of the input and output node, and I don't have them. I've tried to find them using Netron but it didn't work. https://heartbeat.fritz.ai/automl-vision-edge-exporting-and-loading-tensorflow-saved-models-with-python-f4e8ce1b943a Can someone please give me a clue about how to load a saved model and use it to make predictions?",https://stackoverflow.com/questions/64451250,14351825,Documentation Completeness
64908920,Can't use color_mode = 'grayscale' with grayscale JPEGs,"<p>I'm using tensorflow v 2.3.1</p>
<p>This is my code:</p>
<pre><code>train_batches = ImageDataGenerator(preprocessing_function = tf.keras.applications.vgg16.preprocess_input).flow_from_directory(
    directory = train_path, target_size=(224,224), classes = ['1', '2', '3'], color_mode = 'grayscale', batch_size = 20)
</code></pre>
<p>Errors out with :</p>
<pre><code>...    
    231   else:
    232     x[..., 0] -= mean[0]
--&gt; 233     x[..., 1] -= mean[1]
    234     x[..., 2] -= mean[2]
    235     if std is not None:
IndexError: index 1 is out of bounds for axis 2 with size 1
</code></pre>
<p>I thought, documentation says, grayscale is how jpeg will be preprocessed.</p>
<p>Any ideas about the cause?</p>
","I'm using tensorflow v 2.3.1 This is my code: Errors out with : I thought, documentation says, grayscale is how jpeg will be preprocessed. Any ideas about the cause?",https://stackoverflow.com/questions/64908920,14665728,Documentation Replicability
64941304,Unable to give Keras neural network multiple inputs,"<p>I am trying to get a data pipeline of text based data into a neural network with two heads. Made use of the official documentation that tells you to zip it into a dictionary of values, but it did not work.</p>
<pre><code>&lt;MapDataset shapes: (None, 32), types: tf.int64&gt;
&lt;MapDataset shapes: (None, 32), types: tf.int64&gt;
</code></pre>
<p>These are the shapes of the data that will go into each head. Have been converted into sequences of ints using the <code>VectorizeLayer()</code></p>
<p>This is the graph of the neural network</p>
<p><a href=""https://i.stack.imgur.com/6DHCH.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I am constructing the final dataset using</p>
<pre><code>final_dataset=tf.data.Dataset.from_tensors((
    {&quot;input_1&quot;:vectorized_sen1,&quot;input_2&quot;:vectorized_sen2},
    {&quot;output&quot;:label_db}
)).batch(64)
</code></pre>
<p>But this is the error it keeps throwing</p>
<pre><code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.data.ops.dataset_ops._NestedVariant'&gt; to Tensor. Contents: &lt;tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x7f9b073af780&gt;. Consider casting elements to a supported type.
</code></pre>
","I am trying to get a data pipeline of text based data into a neural network with two heads. Made use of the official documentation that tells you to zip it into a dictionary of values, but it did not work. These are the shapes of the data that will go into each head. Have been converted into sequences of ints using the VectorizeLayer() This is the graph of the neural network enter image description here I am constructing the final dataset using But this is the error it keeps throwing",https://stackoverflow.com/questions/64941304,14680310,Documentation Replicability
65006011,Size of output of a Conv1D layer in Keras,"<p>I am trying to understand the output of a 1D convolution layer applied on a number of batches (3 in this case) of 2D input shapes (6x6).</p>
<p>The output of the code below is <code>(4, 10, 32)</code>. This answer is quite straight-forward for the first 2 indices.</p>
<ul>
<li>(4) We plug in N examples, we get out N examples.</li>
<li>(8) When doing the convolution between a (10, 128) * (3, 1) and because the default padding is set to &quot;valid&quot;, two of the values in our input space won't be mapped to our final result, that's why we get 8.</li>
<li>I don't understand why the layer outputs 32 as the final index. What is the actual role of the <code>filters</code> argument in the Conv1D layer? It's not really intuitive for me how does the layer operates the final output as stated by the phrase down below.</li>
</ul>
<p><strong>By the documentation this should be the output shape</strong></p>
<blockquote>
<p>Output shape: 3+D tensor with shape: batch_shape + (new_steps, filters) steps value might have changed due to padding or strides.</p>
</blockquote>
<pre><code>input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
    32, 3, input_shape=input_shape[1:])(x)
print(y.shape) # (4, 10, 32)
</code></pre>
","I am trying to understand the output of a 1D convolution layer applied on a number of batches (3 in this case) of 2D input shapes (6x6). The output of the code below is (4, 10, 32). This answer is quite straight-forward for the first 2 indices. By the documentation this should be the output shape",https://stackoverflow.com/questions/65006011,7450491,Documentation Ambiguity
65013199,StopIteration error while trying to build data input for a model,"<pre><code>from __future__ import print_function

import tensorflow as tf
import os

#Dataset Parameters - CHANGE HERE
MODE = 'folder' # or 'file', if you choose a plain text file (see above).
DATASET_PATH = &quot;D:\\Downloads\\Work\\&quot; # the dataset file or root folder path.

# Image Parameters
N_CLASSES = 7 # CHANGE HERE, total number of classes
IMG_HEIGHT = 64 # CHANGE HERE, the image height to be resized to
IMG_WIDTH = 64 # CHANGE HERE, the image width to be resized to
CHANNELS = 3 # The 3 color channels, change to 1 if grayscale

# Reading the dataset
# 2 modes: 'file' or 'folder'
def read_images(dataset_path, mode, batch_size):
    imagepaths, labels = list(), list()
    if mode == 'file':
        # Read dataset file
        data = open(dataset_path, 'r').read().splitlines()
        for d in data:
            imagepaths.append(d.split(' ')[0])
            labels.append(int(d.split(' ')[1]))
    elif mode == 'folder':
        # An ID will be affected to each sub-folders by alphabetical order
        label = 0
        # List the directory
        #try:  # Python 2
        classes = next(os.walk(dataset_path))[1]
        #except Exception:  # Python 3
        #    classes = sorted(os.walk(dataset_path).__next__()[1])
        # List each sub-directory (the classes)
        for c in classes:
            c_dir = os.path.join(dataset_path, c)
            try:  # Python 2
                walk = os.walk(c_dir).next()
            except Exception:  # Python 3
                walk = os.walk(c_dir).__next__()
            # Add each image to the training set
            for sample in walk[2]:
                # Only keeps jpeg images
                if sample.endswith('.bmp'):
                    imagepaths.append(os.path.join(c_dir, sample))
                    labels.append(label)
            label += 1
    else:
        raise Exception(&quot;Unknown mode.&quot;)

    # Convert to Tensor
    imagepaths = tf.convert_to_tensor(imagepaths, dtype=tf.string)
    labels = tf.convert_to_tensor(labels, dtype=tf.int32)
    # Build a TF Queue, shuffle data
    image, label = tf.train.slice_input_producer([imagepaths, labels],
                                                 shuffle=True)

    # Read images from disk
    image = tf.read_file(image)
    image = tf.image.decode_jpeg(image, channels=CHANNELS)

    # Resize images to a common size
    image = tf.image.resize_images(image, [IMG_HEIGHT, IMG_WIDTH])

    # Normalize
    image = image * 1.0/127.5 - 1.0

    # Create batches
    X, Y = tf.train.batch([image, label], batch_size=batch_size,
                          capacity=batch_size * 8,
                          num_threads=4)

    return X, Y

# Parameters
learning_rate = 0.001
num_steps = 10000
batch_size = 32
display_step = 100

# Network Parameters
dropout = 0.75 # Dropout, probability to keep units

# Build the data input
X, Y = read_images(DATASET_PATH, MODE, batch_size)
</code></pre>
<p>Gives an error</p>
<pre><code>StopIteration                             Traceback (most recent call last)
&lt;ipython-input-27-510f945ab86c&gt; in &lt;module&gt;()
      9 
     10 # Build the data input
---&gt; 11 X, Y = read_images(DATASET_PATH, MODE, batch_size)

&lt;ipython-input-26-c715e653cf59&gt; in read_images(dataset_path, mode, batch_size)
     14         # List the directory
     15         #try:  # Python 2
---&gt; 16         classes = next(os.walk(dataset_path))[1]
     17         #except Exception:  # Python 3
     18         #    classes = sorted(os.walk(dataset_path).__next__()[1])

StopIteration: 
</code></pre>
<p>I saw the documentation for next() and found that you can no longer use at as .next but after correction, it still gives me StopIteration error
I checked the value of <em>classes</em> on my local Python and it gives me a list ['Class0', 'Class1', 'Class2', 'Class3', 'Class4', 'Class5', 'Class6']</p>
","Gives an error I saw the documentation for next() and found that you can no longer use at as .next but after correction, it still gives me StopIteration error I checked the value of classes on my local Python and it gives me a list ['Class0', 'Class1', 'Class2', 'Class3', 'Class4', 'Class5', 'Class6']",https://stackoverflow.com/questions/65013199,14709369,Documentation Completeness
65321954,Tensorflow custom preprocessing with tf.py_function losing shape,"<p>Im writing a model and doing the preprocessing part: I have a method which preprocesses my tensorflow dataset by calling:</p>
<p><code>ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)</code></p>
<p>I followed the tensorflow documentation and got this code for process_path:</p>
<pre><code>def process_path(filename):
  label = get_label(filename)

  image = tf.io.read_file(filename)
  image = tf.image.decode_jpeg(image, channels=3)
  image = tf.image.rgb_to_grayscale(image)
  image = tf.image.convert_image_dtype(image, tf.float32)
  image = tf.image.resize(image, [224, 224])
  
  return image, label
</code></pre>
<p>Then I want to add my own preprocessing, such as rotating the image so I created a rotate method wrapped with py_function as the documentation suggests:</p>
<pre><code>def rotate_image(image):
  return tfa.image.rotate(image, random.randrange(-5, 5)/1.0)

def tf_rotate_image(image, label):
  [image,] = tf.py_function(rotate_image, [image], [tf.float32])
  return image, label
</code></pre>
<p>However when I add this to my process_path the model seems to break and freezes... I added print statements with image.shape after each adjustment and it shows that after the rotate method the image shape becomes <code>&lt;unknown&gt;</code> so I believe this to be the error:</p>
<pre><code>def process_path(filename):
  label = get_label(filename)

  image = tf.io.read_file(filename)
  print(image.shape)
  image = tf.image.decode_jpeg(image, channels=3)
  print(image.shape)
  image = tf.image.rgb_to_grayscale(image)
  print(image.shape)
  image = tf.image.convert_image_dtype(image, tf.float32)
  print(image.shape)
  image = tf.image.resize(image, [224, 224])
  print(image.shape)

  image, label = tf_rotate_image(image, label)
  print(image.shape)
  
  return image, label
</code></pre>
<p>Output:</p>
<pre><code>()
(None, None, 3)
(None, None, 1)
(None, None, 1)
(224, 224, 1)
&lt;unknown&gt;
</code></pre>
<p>Any help is greatly appreciated.</p>
","Im writing a model and doing the preprocessing part: I have a method which preprocesses my tensorflow dataset by calling: ds = ds.map(process_path, num_parallel_calls=AUTOTUNE) I followed the tensorflow documentation and got this code for process_path: Then I want to add my own preprocessing, such as rotating the image so I created a rotate method wrapped with py_function as the documentation suggests: However when I add this to my process_path the model seems to break and freezes... I added print statements with image.shape after each adjustment and it shows that after the rotate method the image shape becomes &lt;unknown&gt; so I believe this to be the error: Output: Any help is greatly appreciated.",https://stackoverflow.com/questions/65321954,9209390,Documentation Replicability
65441499,Tensorflow: Too many dimensions,"<p>I'm trying to create a TensorFlow (2.0) variable like this:</p>
<pre><code>c_init = tf.zeros_initializer()
c = tf.Variable(initial_value=c_init(shape=shape, dtype=&quot;float32&quot;), trainable=True)
</code></pre>
<p>the shape variable is this:</p>
<pre><code>shape=(49, 52, 26, 49, 6, 3, 31, 11, 24, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
</code></pre>
<p>I'm getting this error message:</p>
<blockquote>
<p>InvalidArgumentError: Too many dimensions [Op:Fill] name: zeros/</p>
</blockquote>
<p>I did not know there is a limit in the number of dimensions. I did not see anything in TensorFlow documentation about it. Is there any way to get around this limitation?</p>
",I'm trying to create a TensorFlow (2.0) variable like this: the shape variable is this: I'm getting this error message: I did not know there is a limit in the number of dimensions. I did not see anything in TensorFlow documentation about it. Is there any way to get around this limitation?,https://stackoverflow.com/questions/65441499,67120,Documentation Completeness
65475110,Are Keras custom layer parameters non-trainable by default?,"<p>I built a simple custom layer in Keras and was surprised to find that the parameters were not set to trainable by default. I can get it to work by explicitly setting the trainable attribute. I can't explain why this is by looking at documentation or code. Is this how it is supposed to be or I am doing something wrong which is making the parameters non-trainable by default?
Code:</p>
<pre><code>import tensorflow as tf


class MyDense(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(MyDense, self).__init__(kwargs)
        self.dense = tf.keras.layers.Dense(2, tf.keras.activations.relu)

    def call(self, inputs, training=None):
        return self.dense(inputs)


inputs = tf.keras.Input(shape=10)
outputs = MyDense()(inputs)
model = tf.keras.Model(inputs=inputs, outputs=outputs, name='test')
model.compile(loss=tf.keras.losses.MeanSquaredError())
model.summary()
</code></pre>
<p>Output:</p>
<pre><code>Model: &quot;test&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 10)]              0         
_________________________________________________________________
my_dense (MyDense)           (None, 2)                 22        
=================================================================
Total params: 22
Trainable params: 0
Non-trainable params: 22
_________________________________________________________________
</code></pre>
<p>If I change the custom layer creation like this:</p>
<pre><code>outputs = MyDense(trainable=True)(inputs)
</code></pre>
<p>the output is what I expect (all parameters are trainable):</p>
<pre><code>=================================================================
Total params: 22
Trainable params: 22
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>then it works as expected and makes all the parameters trainable. I don't understand why that is needed though.</p>
",I built a simple custom layer in Keras and was surprised to find that the parameters were not set to trainable by default. I can get it to work by explicitly setting the trainable attribute. I can't explain why this is by looking at documentation or code. Is this how it is supposed to be or I am doing something wrong which is making the parameters non-trainable by default? Code: Output: If I change the custom layer creation like this: the output is what I expect (all parameters are trainable): then it works as expected and makes all the parameters trainable. I don't understand why that is needed though.,https://stackoverflow.com/questions/65475110,14890777,Documentation Ambiguity
65716925,Tensorflow dataset from numpy array,"<p>I have two numpy Arrays (X, Y) which I want to convert to a tensorflow dataset. <a href=""https://www.tensorflow.org/tutorials/load_data/numpy#load_numpy_arrays_with_tfdatadataset"" rel=""nofollow noreferrer"">According to the documentation</a> it should be possible to run</p>
<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices((X, Y))
model.fit(train_dataset)
</code></pre>
<p>When doing this however I get the error:
<code>ValueError: Shapes (15, 1) and (768, 15) are incompatible</code></p>
<p>This would make sense if the shapes of the numpy Arrays would be incompatible to the expected inputs/outputs.
But if I run it with the numpy arrays by using <code>model.fit(X,Y)</code> it runs without any problems, so the shapes seem to be okay.</p>
<p>In a next step I checked the output sizes:</p>
<pre><code>&gt;&gt;&gt; train_dataset.batch(4)
&lt;BatchDataset shapes: ((None, 768), (None, 15)), types: (tf.int64, tf.uint8)&gt;
</code></pre>
<p>The input layer for the neural network expect (None, None) and the output (None, 15). So this also seems to match.</p>
<p>My dataset is rather large, so it's difficult to share that, but here is a minimal reproducible example which shows the problem. It's the same error, and the fit with just the numpy arrays works.</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras import Model
import numpy as np

a = np.random.randint(10,size=(10,20,1))
b = np.random.rand(10,15)
train_dataset = tf.data.Dataset.from_tensor_slices((a,b))

inp = Input(shape=(None,), dtype=&quot;int32&quot;)
embedding = Embedding(12, 300, trainable=False, mask_zero=True)(inp)
gru = Bidirectional(GRU(128, recurrent_dropout=0.5))(embedding)
out = Dense(64, activation=tf.nn.relu)(gru)
out = Dropout(0.5)(out)
out = Dense(15, activation='sigmoid')(out)
m = Model(inputs=inp, outputs = out)
m.compile(&quot;adam&quot;, 'categorical_crossentropy')

m.fit(a,b)
m.fit(train_dataset)
</code></pre>
<p>Can someone point me into the right direction on how to solve this?</p>
<p>Tensorflow version is 2.3.1.</p>
","I have two numpy Arrays (X, Y) which I want to convert to a tensorflow dataset. According to the documentation it should be possible to run When doing this however I get the error: ValueError: Shapes (15, 1) and (768, 15) are incompatible This would make sense if the shapes of the numpy Arrays would be incompatible to the expected inputs/outputs. But if I run it with the numpy arrays by using model.fit(X,Y) it runs without any problems, so the shapes seem to be okay. In a next step I checked the output sizes: The input layer for the neural network expect (None, None) and the output (None, 15). So this also seems to match. My dataset is rather large, so it's difficult to share that, but here is a minimal reproducible example which shows the problem. It's the same error, and the fit with just the numpy arrays works. Can someone point me into the right direction on how to solve this? Tensorflow version is 2.3.1.",https://stackoverflow.com/questions/65716925,5632058,Documentation Replication on Other Examples
65922990,"Nan losses using ""Learning Rate Step Decay"" Scheduler with Adam Optimizer in Keras?","<p>I have this very deep model:</p>
<pre><code>def get_model2(mask_kind):

decay = 0.0

    inp_1 = keras.Input(shape=(64, 101, 1), name=&quot;RST_inputs&quot;)
    x = layers.Conv2D(256, kernel_size=(3, 3), kernel_regularizer=l2(1e-6), strides=(3, 3), padding=&quot;same&quot;)(inp_1)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Conv2D(128, kernel_size=(3, 3), kernel_regularizer=l2(1e-6), strides=(3, 3), padding=&quot;same&quot;)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Conv2D(64, kernel_size=(2, 2), kernel_regularizer=l2(1e-6), strides=(2, 2), padding=&quot;same&quot;)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Conv2D(32, kernel_size=(2, 2), kernel_regularizer=l2(1e-6), strides=(2, 2), padding=&quot;same&quot;)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Flatten()(x)
    x = layers.Dense(512)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Dense(256)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    out1 = layers.Dense(128, name=&quot;ls_weights&quot;)(x)

    if mask_kind == 1:  # APPLICA LA PRIMA MASCHERA
        binary_mask = layers.Lambda(mask_layer1, name=&quot;lambda_layer1&quot;, dtype='float64')(out1)
        print('shape', binary_mask.shape[0])
    elif mask_kind == 2:  # APPLICA LA SECONDA MASCHERA
        binary_mask = layers.Lambda(mask_layer2, name=&quot;lambda_layer2&quot;, dtype='float64')(out1)
    else:  # NON APPLICA NULLA
        binary_mask = out1

    x = layers.Dense(256)(binary_mask)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Dense(512)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Dense(192)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Reshape((2, 2, 48))(x)
    x = layers.Conv2DTranspose(32, kernel_size=(2, 2), strides=(2, 2), padding=&quot;same&quot;)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Conv2DTranspose(64, kernel_size=(3, 3), strides=(3, 3), padding=&quot;same&quot;)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Conv2DTranspose(128, kernel_size=(3, 3), strides=(3, 3), padding=&quot;same&quot;)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    x = layers.Conv2DTranspose(256, kernel_size=(3, 3), strides=(5, 5), padding=&quot;same&quot;)(x)
    x = layers.LeakyReLU(alpha=0.3)(x)
    soundfield_layer = layers.Conv2DTranspose(1, kernel_size=(1, 1), strides=(1, 1), padding='same')(x)
    # soundfield_layer = layers.Dense(40000, name=&quot;sf_vec&quot;)(x)

    if mask_kind == 1:
        model = keras.Model(inp_1, [binary_mask, soundfield_layer], name=&quot;2_out_model&quot;)
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1, decay=decay),  # in caso
                      # rimettere 0.001
                      loss=[&quot;mse&quot;, &quot;mse&quot;], loss_weights=[1, 1])
        # plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)
        model.summary()

    else:
        model = keras.Model(inp_1, [binary_mask, soundfield_layer], name=&quot;2_out_model&quot;)
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1, decay=decay),  # in caso
                      # rimettere 0.001
                      loss=[&quot;mse&quot;, &quot;mse&quot;], loss_weights=[0, 1])
        # plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)
        model.summary()

    return model
</code></pre>
<p>and I'm trying to use Learning rate Step Decay to see if I can improve my validation loss function during training. I'm defining the class for the scheduler as follows:</p>
<pre><code>class StepDecay:
    def __init__(self, initAlpha=0.1, factor=0.25, dropEvery=30):
        # store the base initial learning rate, drop factor, and
        # epochs to drop every
        self.initAlpha = initAlpha
        self.factor = factor
        self.dropEvery = dropEvery
    
    def __call__(self, epoch):
        # compute the learning rate for the current epoch
        exp = np.floor((1 + epoch) / self.dropEvery)
        alpha = self.initAlpha * (self.factor ** exp)
        # return the learning rate
        return float(alpha)
</code></pre>
<p>and then I run my training:</p>
<pre><code>schedule = StepDecay(initAlpha=1e-1, factor=0.25, dropEvery=30)
es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)
callbacks = [es, LearningRateScheduler(schedule)]

model = get_model2(mask_kind=1)

history = model.fit(X_train, [Y_train, Z_train], validation_data=(X_val, [Y_val, Z_val]), epochs=300,
                    batch_size=32,
                    callbacks=callbacks, verbose=1)

test_loss, _, _ = model.evaluate(X_test, [Y_test, Z_test], verbose=1)
print('Test: %.3f' % test_loss)
</code></pre>
<p>but when I train I get &quot;nan&quot; losses:</p>
<pre><code>25/25 [==============================] - 17s 684ms/step - loss: nan - lambda_layer1_loss: nan - conv2d_transpose_4_loss: nan - val_loss: nan - val_lambda_layer1_loss: nan etc....
</code></pre>
<p>and I don't understand why. The problem could be the decay rate which is a parameter present in the SGD optimizer but that from the documentation does not exists for Adam, but I get no error that so..any ideas?</p>
","I have this very deep model: and I'm trying to use Learning rate Step Decay to see if I can improve my validation loss function during training. I'm defining the class for the scheduler as follows: and then I run my training: but when I train I get ""nan"" losses: and I don't understand why. The problem could be the decay rate which is a parameter present in the SGD optimizer but that from the documentation does not exists for Adam, but I get no error that so..any ideas?",https://stackoverflow.com/questions/65922990,12863152,Lack of Alternative Solutions/Documentation
66015216,How can I save a Tensorflow Core model?,"<p>I'm a beginner in Tensorflow and I found this neural network for binary classification which is giving me decent results, I would like to know after I run the session how can I save the model? I already try from the official website but nothing is working.</p>
<pre><code>class AnnMLP():

def train(self,X_input,y_input,test,range_iteration,learning_rate): X = tf.compat.v1.placeholder(tf.float32, [None,27]) Y = tf.compat.v1.placeholder(tf.float32, [None,1])

# input
W1 = tf.Variable(tf.random.normal([27,60], seed=0), name='weight1')
b1 = tf.Variable(tf.random.normal([60], seed=0), name='bias1')
layer1 = tf.nn.sigmoid(tf.matmul(X,W1) + b1)
dropout_layer = keras.layers.Dropout(rate=0.4)
layer1=dropout_layer(layer1)

# hidden1
W2 = tf.Variable(tf.random.normal([60,60], seed=0), name='weight2')
b2 = tf.Variable(tf.random.normal([60], seed=0), name='bias2')
layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)

dropout_layer = keras.layers.Dropout(rate=0.4)
layer2=dropout_layer(layer2)

# hidden2
W3 = tf.Variable(tf.random.normal([60,90], seed=0), name='weight3')
b3 = tf.Variable(tf.random.normal([90], seed=0), name='bias3')
layer3 = tf.nn.sigmoid(tf.matmul(layer2,W3) + b3)
dropout_layer = keras.layers.Dropout(rate=0.4)
layer3=dropout_layer(layer3)

# output
W4 = tf.Variable(tf.random.normal([90,1], seed=0), name='weight4')
b4 = tf.Variable(tf.random.normal([1], seed=0), name='bias4')

logits = tf.matmul(layer3,W4) + b4
hypothesis = tf.nn.sigmoid(logits)


cost_i = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=Y)
cost = tf.reduce_mean(cost_i)
train =tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)
#train = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost) train = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(cost)

prediction = tf.cast(hypothesis &gt; 0.5, dtype=tf.float32)
correct_prediction = tf.equal(prediction, Y)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))
print(&quot;\n============Processing============&quot;)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for step in range(range_iteration):
        sess.run(train, feed_dict={X: X_input, Y: y_input})

        if step % 1000 == 0:

          loss, acc = sess.run([cost, accuracy], feed_dict={X: X_input, Y: y_input})
          print(&quot;Step: {:5}\tLoss: {:.3f}\tAcc: {:.2%}&quot;.format(step, loss, acc))



        train_acc = sess.run(accuracy, feed_dict={X: X_input, Y: y_input})
        if test == True:
          test_acc,test_predict,test_correct = sess.run([accuracy,prediction,correct_prediction], feed_dict={X: X_test, Y: y_test})

return test_predict

 
</code></pre>
","I'm a beginner in Tensorflow and I found this neural network for binary classification which is giving me decent results, I would like to know after I run the session how can I save the model? I already try from the official website but nothing is working.",https://stackoverflow.com/questions/66015216,14473036,Documentation Ambiguity
66320198,Keras fit with generator function always execute in the main thread,"<p>How can I make Keras Models <code>fit</code> method execute a generator in the main thread? From the docs, it looks like that setting workers=0 would execute the code in the main thread.</p>
<blockquote>
<p>workers   Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. <strong>If 0, will execute the generator on the main thread.</strong></p>
</blockquote>
<p>However when I do:</p>
<pre><code>import tensorflow as tf
import threading
model = tf.keras.Sequential([tf.keras.layers.Dense(1)])
model.compile(loss = &quot;mse&quot;, optimizer = &quot;adam&quot;)

def gen ():
  for i in range(100):
    print(threading.current_thread())
    yield (tf.random.normal(shape=(100,1)), tf.random.normal(shape = (100,)))

model.fit(gen(), epochs = 1, workers = 0, verbose = 0, steps_per_epoch = 3)
</code></pre>
<p>I get</p>
<pre><code>&lt;_MainThread(MainThread, started 140516450817920)&gt;
&lt;_DummyThread(Dummy-5, started daemon 140514709206784)&gt;
&lt;_DummyThread(Dummy-4, started daemon 140514717599488)&gt;
&lt;tensorflow.python.keras.callbacks.History at 0x7fcc1e8a8d68&gt;
</code></pre>
<p>Which I interpret as only the first step in the iterator has been executed in the main thread.</p>
<p>In my use case this is problematic because I need that the code inside the generator to always be executed in the main thread otherwise the program crashes.</p>
","How can I make Keras Models fit method execute a generator in the main thread? From the docs, it looks like that setting workers=0 would execute the code in the main thread. However when I do: I get Which I interpret as only the first step in the iterator has been executed in the main thread. In my use case this is problematic because I need that the code inside the generator to always be executed in the main thread otherwise the program crashes.",https://stackoverflow.com/questions/66320198,3297472,Lack of Alternative Solutions/Documentation
66477586,Loading Custom Data into TensorFlow with .get_file(),"<p>I'm relatively new to Tensor Flow and Stack overflow, so please be patient. My question is as follows: 'How do I load in a custom dataset spreadsheet into TensorFlow using the .get_file() method and pandas read method?' I  have searched the TensorFlow website, stack overflow, and other websites but they all seem to either use publically available data online or do some strange imports with different methods that I do not understand. Here is what I currently have:</p>
<pre><code>import tensorflow as tf
import pandas as pd


CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']
SPECIES = ['Setosa', 'Versicolor', 'Virginica']
# This is just some flower data online

train_path = tf.keras.utils.get_file(
    &quot;iris_training.csv&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv&quot;)
test_path = tf.keras.utils.get_file(
    &quot;iris_test.csv&quot;, &quot;https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv&quot;)
    # I have a spreadsheet on my machine with the exact same data. I want to use those files instead

train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)
# Here I am reading a csv file inputting the data, labels, and defining header. Should I use pd.read_excel instead because the files on my machine are excel files?

train_y = train.pop('Species')
test_y = test.pop('Species') # removes answers/thing to predict and test against
</code></pre>
<p>Thank you so much for reading!</p>
","I'm relatively new to Tensor Flow and Stack overflow, so please be patient. My question is as follows: 'How do I load in a custom dataset spreadsheet into TensorFlow using the .get_file() method and pandas read method?' I have searched the TensorFlow website, stack overflow, and other websites but they all seem to either use publically available data online or do some strange imports with different methods that I do not understand. Here is what I currently have: Thank you so much for reading!",https://stackoverflow.com/questions/66477586,15290446,Lack of Alternative Solutions/Documentation
66667080,Convert a KerasTensor object to a numpy array to visualize predictions in Callback,"<p>I am writing a custom on_train_end callback function for model.fit() method of tensorflow keras sequential model. The callback function is about plotting the predictions that the model makes, so it involves converting the inputs of the model to a numpy array and feeds it into model.predict(). I use self.model.inputs to access the inputs, which is a list of KerasTensor objects and the one at 0th index is what I want. I tried the following approach</p>
<pre><code>class my_visualizer(tf.keras.callbacks.Callback):

    def on_train_end(self, logs=None):

        x = tf.keras.backend.eval(self.model.inputs[0])
        y_predictions = self.model.predict(x)
        
</code></pre>
<p>but got the error</p>
<pre><code>AttributeError: 'KerasTensor' object has no attribute 'numpy'
</code></pre>
<p>So this method is for another type of tensor rather than KerasTensor. Other solutions I found work for tensorflow's Tensor object but not keras' KerasTensor object, and I did not find any mentioning of the ways to achieve the desired feature in keras documentation. Thanks for your help!</p>
","I am writing a custom on_train_end callback function for model.fit() method of tensorflow keras sequential model. The callback function is about plotting the predictions that the model makes, so it involves converting the inputs of the model to a numpy array and feeds it into model.predict(). I use self.model.inputs to access the inputs, which is a list of KerasTensor objects and the one at 0th index is what I want. I tried the following approach but got the error So this method is for another type of tensor rather than KerasTensor. Other solutions I found work for tensorflow's Tensor object but not keras' KerasTensor object, and I did not find any mentioning of the ways to achieve the desired feature in keras documentation. Thanks for your help!",https://stackoverflow.com/questions/66667080,12345152,Documentation Completeness
66778153,How exactly does tf.data.Dataset.interleave() differ from map() and flat_map()?,"<p>My current understanding is:</p>
<p><strong>Different map_func</strong>: Both  <code>interleave</code> and <code>flat_map</code> expect &quot;A function mapping a dataset element to a <strong>dataset</strong>&quot;. In contrast, <code>map</code> expects    &quot;A function mapping a dataset element to another <strong>dataset element</strong>&quot;.</p>
<p><strong>Arguments</strong>: Both <code>interleave</code> and <code>map</code> offer the argument num_parallel_calls, whereas <code>flat_map</code> does not. Moreover, interleave offers these magical arguments block_length and cycle_length. For cycle_length=1, the documentation states that  the outputs of interleave and flat_map are equal.</p>
<p>Last, I have seen <a href=""https://cs230.stanford.edu/blog/datapipeline/#building-an-image-data-pipeline"" rel=""noreferrer"">data loading pipelines without interleave</a> as well as <a href=""https://www.tensorflow.org/guide/data_performance"" rel=""noreferrer"">ones with interleave</a>. Any advice when to use <code>interleave</code> vs. <code>map</code> or <code>flat_map</code> would be greatly appreciated</p>
<hr />
<p>//EDIT: I do see the value of interleave, if we start out with different datasets, such as in <a href=""https://docs.w3cub.com/tensorflow%7Eguide/performance/datasets_performance"" rel=""noreferrer"">the code below</a></p>
<pre><code>  files = tf.data.Dataset.list_files(&quot;/path/to/dataset/train-*.tfrecord&quot;)
  dataset = files.interleave(tf.data.TFRecordDataset)
</code></pre>
<p>However, is there any benefit of using <code>interleave</code> over <code>map</code> in a scenario such as the one below?</p>
<pre><code>files = tf.data.Dataset.list_files(&quot;/path/to/dataset/train-*.png&quot;)
dataset = files.map(load_img, num_parallel_calls=tf.data.AUTOTUNE)
</code></pre>
","My current understanding is: Different map_func: Both interleave and flat_map expect ""A function mapping a dataset element to a dataset"". In contrast, map expects ""A function mapping a dataset element to another dataset element"". Arguments: Both interleave and map offer the argument num_parallel_calls, whereas flat_map does not. Moreover, interleave offers these magical arguments block_length and cycle_length. For cycle_length=1, the documentation states that the outputs of interleave and flat_map are equal. Last, I have seen data loading pipelines without interleave as well as ones with interleave. Any advice when to use interleave vs. map or flat_map would be greatly appreciated //EDIT: I do see the value of interleave, if we start out with different datasets, such as in the code below However, is there any benefit of using interleave over map in a scenario such as the one below?",https://stackoverflow.com/questions/66778153,2135504,Documentation Ambiguity
67275213,3d input for Dense Layer Keras,"<p>Is there any example of how Keras <code>Dense</code> layer handles <code>3D</code> input.</p>
<p>The documentation explains the following:</p>
<blockquote>
<p>If the input to the layer has a rank greater than 2, then Dense
computes the dot product between the inputs and the kernel along the
last axis of the inputs and axis 1 of the kernel (using tf.tensordot).</p>
</blockquote>
<p>But I could not understand the internal <code>matrix calculation</code></p>
<p>For example:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.layers import Dense
sample_3d_input = tf.constant(tf.random.normal(shape=(4,3,2)))
dense_layer  = Dense(5)
op = dense_layer(sample_3d_input)
</code></pre>
<p>based on the documentation for a <code>3D</code> input of shape <code>(m,d0,d1)</code>, the shape of <code>Layer's weight_matrix (or) kernel</code> will have the shape <code>(d1, units) which is (2,5)</code> in this case. But I don't understand how the op is calculated to have the shape <code>(m,d0, units)</code></p>
","Is there any example of how Keras Dense layer handles 3D input. The documentation explains the following: But I could not understand the internal matrix calculation For example: based on the documentation for a 3D input of shape (m,d0,d1), the shape of Layer's weight_matrix (or) kernel will have the shape (d1, units) which is (2,5) in this case. But I don't understand how the op is calculated to have the shape (m,d0, units)",https://stackoverflow.com/questions/67275213,5927701,Documentation Ambiguity
67497418,TensorFlow accuracy metrics,"<p>The following is a very simple <code>TensorFlow</code> 2 image classification model.
Note that the loss function is not the usual <code>SparseCategoricalCrossentropy</code>. Also, the last layer has only 1 output, so this is not the usual classification setting. The accuracy here does not have meaning, but I am just curious.</p>
<p>So this code does not work well as we expected, but still produces outputs with an accuracy of around 10%, which seems reasonable.</p>
<p>My question is how this accuracy is calculated? The prediction from this model is a continuous value and the y_true is an integer value. It is not impossible to have an x.0 for the prediction, then the accuracy is too high.</p>
<pre><code>import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=10)

===

Epoch 1/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.8237 - accuracy: 0.0922
Epoch 2/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.8266 - accuracy: 0.0931
Epoch 3/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.8335 - accuracy: 0.0921
Epoch 4/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.8109 - accuracy: 0.0931
Epoch 5/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.8210 - accuracy: 0.0926
Epoch 6/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.8067 - accuracy: 0.0921
Epoch 7/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.8028 - accuracy: 0.0925
Epoch 8/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.8070 - accuracy: 0.0929
Epoch 9/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.7879 - accuracy: 0.0925
Epoch 10/10
1875/1875 [==============================] - 3s 1ms/step - loss: 1.8055 - accuracy: 0.0914
&lt;tensorflow.python.keras.callbacks.History at 0x7f65db17df10&gt;
</code></pre>
<p>So, I have searched the <code>TensorFlow</code> API document to find the following example. And it makes sense.</p>
<pre><code>m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()

===

0.75
</code></pre>
<p>So I have tried the following code and get the 0.0 accuracy.</p>
<pre><code>m = tf.keras.metrics.Accuracy()
m.update_state(model.predict(x_train), y_train)
m.result().numpy()

===

0.0
</code></pre>
<p>Is there any explanation for this?</p>
","The following is a very simple TensorFlow 2 image classification model. Note that the loss function is not the usual SparseCategoricalCrossentropy. Also, the last layer has only 1 output, so this is not the usual classification setting. The accuracy here does not have meaning, but I am just curious. So this code does not work well as we expected, but still produces outputs with an accuracy of around 10%, which seems reasonable. My question is how this accuracy is calculated? The prediction from this model is a continuous value and the y_true is an integer value. It is not impossible to have an x.0 for the prediction, then the accuracy is too high. So, I have searched the TensorFlow API document to find the following example. And it makes sense. So I have tried the following code and get the 0.0 accuracy. Is there any explanation for this?",https://stackoverflow.com/questions/67497418,11381722,Requesting (Additional) Resources
67533336,Implementing Cosine similarity loss gives different answer than Tensorflow's,"<p>I was implementing cosine similarity loss with my custom python script but it gives me a very different answer than TensorFlow. First see <code>TensorFlow's</code> answer:-</p>
<pre><code>y_true = [[0., 1.], [1., 1.]]
y_pred = [[0., 1.], [0., 1.]]
loss = tf.keras.losses.CosineSimilarity()
print(loss(y_true, y_pred).numpy())
</code></pre>
<p>Output:</p>
<pre><code>&gt;&gt;&gt; -0.8535534
</code></pre>
<p>According to the TensorFlow documentation, the formula to compute the loss is this:-</p>
<p><a href=""https://i.stack.imgur.com/ofbYZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ofbYZ.png"" alt=""enter image description here"" /></a></p>
<p>I implemented the same with plain python as this:-</p>
<pre><code>def cosine_similarity(y_true, y_pred):
    loss = -np.sum(np.linalg.norm(y_true) * np.linalg.norm(y_pred))
    return loss

print(cosine_similarity(y_true, y_pred))
</code></pre>
<p>Output:</p>
<pre><code>&gt;&gt;&gt; -2.4494897427831783
</code></pre>
<p>I don't why I am getting <code>-2.45</code> and <code>TensorFlow</code> is outputting <code>-0.85</code>. Any solution so my answer can match with TensorFlow's?</p>
","I was implementing cosine similarity loss with my custom python script but it gives me a very different answer than TensorFlow. First see TensorFlow's answer:- Output: According to the TensorFlow documentation, the formula to compute the loss is this:- I implemented the same with plain python as this:- Output: I don't why I am getting -2.45 and TensorFlow is outputting -0.85. Any solution so my answer can match with TensorFlow's?",https://stackoverflow.com/questions/67533336,,Requesting (Additional) Resources
67801758,tf.Variable gets converted to normal tensor in loop,"<p>I have a custom layer in Keras in which I define some variables to which I assign some values in the call section.</p>
<pre><code>class Mine_layer(KL.Layer):
  def __init__(self,shape):
    self.block=tf.Variable(tf.constant(1,shape=shape))
  def call (self,indeces):
    self.block=self.block[indeces[0][0],indeces[0][1]].assign(1)
</code></pre>
<p>this works but if i try to utilize a for loop over all the indeces:</p>
<pre><code>for i in tf.range(0,limit=tf.shape(indeces)[0]):
   self.block=self.block[indeces[i][0],indeces[i][1]].assign(1)
</code></pre>
<p>this gives me an error saying that &quot;'Tensor' object has no attribute 'assign'&quot;.</p>
<p>Why does this happen? How can I solve it?</p>
<p>I tried looking at the documentation but I still don't get it.</p>
<p>Thanks in advance to anyone who may answer.</p>
","I have a custom layer in Keras in which I define some variables to which I assign some values in the call section. this works but if i try to utilize a for loop over all the indeces: this gives me an error saying that ""'Tensor' object has no attribute 'assign'"". Why does this happen? How can I solve it? I tried looking at the documentation but I still don't get it. Thanks in advance to anyone who may answer.",https://stackoverflow.com/questions/67801758,10067045,Documentation Ambiguity
67803574,Weights were not updated using Gradient Tape and apply_gradients(),"<p>I am building a DNN with a custom loss function and I am training this DNN using Gradient Tape in TensorFlow.keras<code>enter code here</code>. The code runs without any errors, however, as far as I can check the weights of the DNN, the weights were not being updated at all. I followed exactly what recommends from the TensorFlow website and search for the answers but still don't understand what is the reason. Here is my code:</p>
<pre><code>import numpy as np

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, LeakyReLU, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from tensorflow.keras import optimizers

# Generate a random train data
c0_train = np.array([30 * np.random.uniform() for i in range(10000)])

# Build a simple DNN
c0_input = Input(shape=(1,), name='c0')
hidden_1 = Dense(100)(c0_input)
activation_1 = LeakyReLU(alpha=0.1)(hidden_1)
hidden_2 = Dense(100)(activation_1)
activation_2 = LeakyReLU(alpha=0.1)(hidden_2)
hidden_3 = Dense(100)(activation_2)
activation_3 = LeakyReLU(alpha=0.1)(hidden_3)
x0_output = Dense(1, name='x0')(activation_3)

model = Model(inputs=c0_input, outputs=x0_output)

# Calculating the loss function 
def cal_loss(c0_input):
  x0_output = model(c0_input)
  loss = tf.reduce_mean(
      tf.multiply(c0_input, tf.square(tf.subtract(x0_output, c0_input))))
  return loss

# Compute the gradient calculation
@tf.function
def compute_loss_grads(c0_input):
  with tf.GradientTape() as tape:
    loss = cal_loss(c0_input)
  grads = tape.gradient(loss, model.trainable_variables)
  return loss, grads

# Optimizer
opt = optimizers.Adam(learning_rate=0.01)

# Start looping
for epoch in range(50):
  print('Epoch = ', epoch)
  # Compute the loss and gradients
  [loss, grads] = compute_loss_grads(tf.cast(c0_train, tf.float32))
  # Adjust the weights of the model
  opt.apply_gradients(zip(grads, model.trainable_variables))

</code></pre>
<p>I have checked the weights of the model using <code>model.get_weights()</code> and they look exactly the same before and after running the loop. So what is the problem here? And one more question, how can I print out the loss for every epoch?</p>
","I am building a DNN with a custom loss function and I am training this DNN using Gradient Tape in TensorFlow.kerasenter code here. The code runs without any errors, however, as far as I can check the weights of the DNN, the weights were not being updated at all. I followed exactly what recommends from the TensorFlow website and search for the answers but still don't understand what is the reason. Here is my code: I have checked the weights of the model using model.get_weights() and they look exactly the same before and after running the loop. So what is the problem here? And one more question, how can I print out the loss for every epoch?",https://stackoverflow.com/questions/67803574,16104905,Documentation Replicability
68651758,Printing outcome probabilities from a trained Keras Model,"<p>I am attempting to print the predicted probabilities of each class outcome from my trained model, when I present <strong>new</strong> raw data. This is a multi-class classification problem, with 8 outputs and 21 inputs.</p>
<p>I am able to print 1 outcome when I present new data, for example:</p>
<pre><code> &quot;Example 0 prediction: 1 (15.0%)&quot;
</code></pre>
<p>Instead, I would expect to see something similar to the below. Where the probabilities of each class (0, 1, 2, 3, 4, 6, Wide, Out) are shown:</p>
<pre><code>Example 0 prediction 0: (12.5%), prediction 1: (12.5%), prediction 2: (12.5%), prediction 3: (12.5%), prediction 4: (12.5%), prediction 6: (12.5%), prediction Wide: (12.5%), prediction Out: (12.5%)
</code></pre>
<p>Please note I have tried searching for similar issues including  <a href=""https://stackoverflow.com/questions/47599436/returning-probabilities-in-a-classification-prediction-in-keras"">here</a>, <a href=""https://stackoverflow.com/questions/50555434/keras-model-to-predict-probability-distribution"">here</a> and <a href=""https://stackoverflow.com/questions/48217119/keras-get-probability-per-each-class"">here</a> as well as consulted the TensorFlow documentation. However, these mainly discuss alterations to the model itself e.g. softmax activation on the final layer, categorical crossentropy as the loss function etc. so that probabilities are generated.</p>
<p>I have included the model architecture as well as the prediction code for full visibility.</p>
<p>Model:</p>
<pre><code>earlystopping = callbacks.EarlyStopping(monitor =&quot;val_loss&quot;, 
                                        mode =&quot;min&quot;, patience = 125, 
                                        restore_best_weights = True)
  
#define Keras
model = Sequential()
model.add(Dense(50, input_dim=21))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5,input_shape=(50,)))
model.add(Dense(50))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5,input_shape=(50,)))
model.add(Dense(8, activation='softmax'))

#compile the keras model
model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])   

model.fit(X, dummy_y, validation_split=0.25, epochs=1000, batch_size=100, verbose=1, callbacks=[earlystopping])

_, accuracy3 = model.evaluate(X, dummy_y, verbose=0)
print('Accuracy: %.2f' % (accuracy3*100))
</code></pre>
<p>Making predictions:</p>
<pre><code>class_names = ['0', '1', '2','3','4','6','Wide','Out']

predict_dataset = tf.convert_to_tensor([
  [1,5,1,0.459,0.322,0.041,0.002,0.103,0.032,0.041,14,0.404,0.284,0.052,0.008,0.128,0.044,0.037,0.043,54,0,],
    [1,18,5,0.512,0.286,0,0,0.083,0.024,0.095,13,0.24,0.44,0.08,0,0.08,0.08,0,0.08,173,3],
    [2,11,13,0.5,0.417,0,0,0.083,0,0.083,82,0.35,0.36,0.042,0.003,0.135,0.039,0.051,0.02,51,7]
])  

predictions = model(predict_dataset, training=False)

for i, logits in enumerate(predictions):
    class_idx = tf.argmax(logits).numpy()
    p = tf.nn.softmax(logits)[class_idx]
    name = class_names[class_idx]
    print(&quot;Example {} prediction: {} ({:4.1f}%)&quot;.format(i, name,100*p))
</code></pre>
<p>Output:</p>
<pre><code>Example 0 prediction: 1 (15.0%)
Example 1 prediction: 1 (16.0%)
Example 2 prediction: 0 (16.9%)
</code></pre>
<p>I have tried making changes to the for loop which makes use of TensorFlow's logits, but I am still unable to get it to print each outcome and associated probability.</p>
<p>Any guidance is much appreciated.</p>
","I am attempting to print the predicted probabilities of each class outcome from my trained model, when I present new raw data. This is a multi-class classification problem, with 8 outputs and 21 inputs. I am able to print 1 outcome when I present new data, for example: Instead, I would expect to see something similar to the below. Where the probabilities of each class (0, 1, 2, 3, 4, 6, Wide, Out) are shown: Please note I have tried searching for similar issues including here, here and here as well as consulted the TensorFlow documentation. However, these mainly discuss alterations to the model itself e.g. softmax activation on the final layer, categorical crossentropy as the loss function etc. so that probabilities are generated. I have included the model architecture as well as the prediction code for full visibility. Model: Making predictions: Output: I have tried making changes to the for loop which makes use of TensorFlow's logits, but I am still unable to get it to print each outcome and associated probability. Any guidance is much appreciated.",https://stackoverflow.com/questions/68651758,16306039,Lack of Alternative Solutions/Documentation
69693757,Loading checkpoints while training a Faster-RCNN model on a custom dataset,"<p>I'm trying to load checkpoints and populate model weights using The Faster-RCNN architecture (<code>Faster R-CNN ResNet50 V1 640x640</code> to be precise, from <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"" rel=""noreferrer"">here</a>. I'm trying to load the weights for this network similar to how it's done in the <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb"" rel=""noreferrer"">example notebook for RetinaNet</a>, where they do the following:</p>
<pre class=""lang-py prettyprint-override""><code>fake_box_predictor = tf.compat.v2.train.Checkpoint(
    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,
    _box_prediction_head=detection_model._box_predictor._box_prediction_head,
)

fake_model = tf.compat.v2.train.Checkpoint(
          _feature_extractor=detection_model._feature_extractor,
          _box_predictor=fake_box_predictor
)

ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)
ckpt.restore(checkpoint_path).expect_partial()
</code></pre>
<p>I'm trying to get a similar checkpoint loading mechanism going for the Faster-RCNN network I want to use, but the properties like <code>_base_tower_layers_for_heads</code>, <code>_box_prediction_head</code> only exist for the architecture used in the example, and not for anything else.</p>
<p>I also couldn't find documentation on which parts of the model to populate using <code>Checkpoint</code> for my particular use case. Would greatly appreciate any help on how to approach this!</p>
","I'm trying to load checkpoints and populate model weights using The Faster-RCNN architecture (Faster R-CNN ResNet50 V1 640x640 to be precise, from here. I'm trying to load the weights for this network similar to how it's done in the example notebook for RetinaNet, where they do the following: I'm trying to get a similar checkpoint loading mechanism going for the Faster-RCNN network I want to use, but the properties like _base_tower_layers_for_heads, _box_prediction_head only exist for the architecture used in the example, and not for anything else. I also couldn't find documentation on which parts of the model to populate using Checkpoint for my particular use case. Would greatly appreciate any help on how to approach this!",https://stackoverflow.com/questions/69693757,6274300,Lack of Alternative Solutions/Documentation
69709010,"Keras/Conv2D: Strange, I use padding=SAME, but the size is still reduced","<p>I set padding to SAME or same, but the output is still being reduced, what's wrong then?
😅, as I understand according to the official doc, the output size shall be the same to the input one, do I forget what is important?</p>
<pre><code>import tensorflow as tf

x = tf.keras.Input([120, 120, 3])

conv = tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3), strides=(2, 2), padding=&quot;SAME&quot;)(x)
conv = tf.keras.layers.Conv2D(filters=3, kernel_size=(3, 3), strides=(2, 2), padding=&quot;SAME&quot;)(conv)

model = tf.keras.Model(inputs=[x], outputs=[conv])

model.summary()

image_batch = tf.random.normal(shape=[10, 120, 120, 3])
y_pred = model(image_batch)
print(y_pred.shape)
</code></pre>
<p>Output</p>
<pre class=""lang-py prettyprint-override""><code>odel: &quot;model_7&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_8 (InputLayer)         [(None, 120, 120, 3)]     0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 60, 60, 3)         84        
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 30, 30, 3)         84        
=================================================================
Total params: 168
Trainable params: 168
Non-trainable params: 0
_________________________________________________________________
(10, 30, 30, 3)
</code></pre>
","I set padding to SAME or same, but the output is still being reduced, what's wrong then? 😅, as I understand according to the official doc, the output size shall be the same to the input one, do I forget what is important? Output",https://stackoverflow.com/questions/69709010,1835650,Documentation Replication on Other Examples
69932821,Keras preprocessing layer,"<p>I am trying to feed a neural network 50 features (All Yes/No values) to predict the probability of one Yes/No label. I am trying to do this with keras <code>CategoryEncoding</code>, but running into some issues.</p>
<p>The start of my code is below:</p>
<pre class=""lang-py prettyprint-override""><code>model = Sequential([
    tf.keras.Input(shape = (50,)),
    tf.keras.layers.CategoryEncoding(num_tokens=100, output_mode='one_hot'),
    tf.keras.layers.LayerNormalization(),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='softmax')
])
</code></pre>
<p>However, I get this error below:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: Exception encountered when calling layer &quot;category_encoding_12&quot; (type CategoryEncoding).

When output_mode is not `'int'`, maximum supported output rank is 2. Received output_mode one_hot and input shape (None, 50), which would result in output rank 3.

Call arguments received:
  • inputs=tf.Tensor(shape=(None, 50), dtype=float32)
  • count_weights=None
</code></pre>
<p>I am looking through the documentation, and I don't think I fully understand what a token is in its context here. Also, how would I preprocess my label here? I could use <code>pd.get_dummies</code>, but I don't know if tensorflow has anything that could do that automatically?</p>
","I am trying to feed a neural network 50 features (All Yes/No values) to predict the probability of one Yes/No label. I am trying to do this with keras CategoryEncoding, but running into some issues. The start of my code is below: However, I get this error below: I am looking through the documentation, and I don't think I fully understand what a token is in its context here. Also, how would I preprocess my label here? I could use pd.get_dummies, but I don't know if tensorflow has anything that could do that automatically?",https://stackoverflow.com/questions/69932821,16354723,Documentation Ambiguity
70163993,Replace tf.const with tf.variable in frozen graph for re-train frozen graph,"<p>I got trouble to re-train frozen graph Due to Const in Graph
Actually I checked out many reference codes
like <a href=""https://stackoverflow.com/questions/67702185/retrain-frozen-graph-in-tensorflow-2-x"">this</a> and <a href=""https://stackoverflow.com/questions/53085007/re-train-a-frozen-pb-model-in-tensorflow"">this</a>
Tried everything but no lock for me.</p>
<p>My environment below</p>
<pre><code>python 3.7.7
TensorFlow 2.1.0
</code></pre>
<p>My code below</p>
<pre><code>  graph_def = optimize_graph(graph)
  
  graph = tf.Graph()
  with tf.compat.v1.Session(graph=graph):
      tf.graph_util.import_graph_def(graph_def, name='')
  
  g = graph
  to_convert = [op.name for op in g.get_operations() if op.type == &quot;Const&quot;]
  const_var_name_pairs = []

  with g.as_default():
      for name in to_convert:
          tensor = g.get_tensor_by_name('{}:0'.format(name))
          with tf.compat.v1.Session() as sess:
              tensor_as_numpy_array = sess.run(tensor_cast(tensor, tensor.dtype))
          var_name = '{}_turned_var'.format(name)
          var = tf.Variable(name=var_name, dtype=tensor.dtype, shape=tensor.get_shape(),
                      initial_value=tensor_as_numpy_array
                      )
          var.assign(tensor_as_numpy_array)
          const_var_name_pairs.append((name, var_name))
    
  ge_graph = ge.Graph(g.as_graph_def())

  for const_name, var_name in const_var_name_pairs:
      const_op = ge_graph._node_name_to_node[const_name]
      var_reader_op = ge_graph._node_name_to_node[var_name]
      ge.swap_outputs(ge.sgv(const_op), ge.sgv(var_reader_op))
  
  detection_training_graph = ge_graph.to_tf_graph()
</code></pre>
<p>The problem is,</p>
<pre><code>/var/folders/2d/pqn3pzcj0qs_t6_wgtmd8p9r0000gn/T/ipykernel_50693/993434432.py in load_graph_model_and_signature_with_swap(model_dir, compat_mode)
     66       #var_reader_op = name_to_op[var_name + '/Read/ReadVariableOp']
     67       #print(var_reader_op)
---&gt; 68       ge.swap_outputs(ge.sgv(const_op), ge.sgv(var_reader_op))
     69 
     70 

ValueError: Dtypes &lt;dtype: 'float32'&gt; and &lt;dtype: 'resource'&gt; of tensors Const:0 and Const_turned_var:0 are not compatible.
</code></pre>
<p>When I replace variable by using below code</p>
<pre><code>  for const_name, var_name in const_var_name_pairs:
      const_op = ge_graph._node_name_to_node[const_name]
      var_reader_op = ge_graph._node_name_to_node[var_name + '/Read/ReadVariableOp']
      ge.swap_outputs(ge.sgv(const_op), ge.sgv(var_reader_op))
</code></pre>
<p>The model is modified well but, I could not inference by using this model due to initialize problem</p>
<pre><code>FailedPreconditionError:  Error while reading resource variable Const_99_turned_var_load_43352 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/Const_99_turned_var_load_43352/N10tensorflow3VarE does not exist.
     [[node Const_99_turned_var/Read/ReadVariableOp (defined at var/folders/2d/pqn3pzcj0qs_t6_wgtmd8p9r0000gn/T/ipykernel_50693/3656424118.py:3) ]] [Op:__inference_pruned_44901]
</code></pre>
<p>I'm struggling few weeks, I never know how TensorFlow Graph has been working inside. There is no exact document about it also. I read every document on Tensorflow page relevant. Please help me out from the dark</p>
","I got trouble to re-train frozen graph Due to Const in Graph Actually I checked out many reference codes like this and this Tried everything but no lock for me. My environment below My code below The problem is, When I replace variable by using below code The model is modified well but, I could not inference by using this model due to initialize problem I'm struggling few weeks, I never know how TensorFlow Graph has been working inside. There is no exact document about it also. I read every document on Tensorflow page relevant. Please help me out from the dark",https://stackoverflow.com/questions/70163993,4457567,Documentation Completeness
70365874,Validation_split with BatchDataset,"<p>I am trying to split my dataset into validation and training.
I was unable to call a validation subset in model.fit() as y data is not accepted for datasets, and the validation_split works only for tensors or numpy arrays. I checked the documentation for tensorflow, and there is no documentation of casting of BatchDataset to tensor, unless the neural network is altered itself, which I am unable to do as I am using the resnet architecture using keras.</p>
<p>The following errors showed up respectively:</p>
<pre><code> raise ValueError(&quot;`y` argument is not supported when using &quot;
ValueError: `y` argument is not supported when using dataset as input.

 raise ValueError(
ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [&lt;class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'&gt;]
</code></pre>
<p>Here is the code I am currently working on:</p>
<pre><code>test = tf.keras.preprocessing.image_dataset_from_directory(
  &quot;/Users/***/***/data-aug&quot;,
  labels=&quot;inferred&quot;,
  label_mode=&quot;categorical&quot;,
  color_mode=&quot;rgb&quot;,
  batch_size=32,
  image_size=(224, 224),
  shuffle=True,
  seed=123,
  interpolation=&quot;bilinear&quot;,
  follow_links=False,
  crop_to_aspect_ratio=True
)

H = model.fit(test,
              validation_split=0.2,
              epochs=200,
              shuffle=True,
              verbose=1,
              callbacks=[mc,es,pm])
</code></pre>
<p>Thank you for your time</p>
","I am trying to split my dataset into validation and training. I was unable to call a validation subset in model.fit() as y data is not accepted for datasets, and the validation_split works only for tensors or numpy arrays. I checked the documentation for tensorflow, and there is no documentation of casting of BatchDataset to tensor, unless the neural network is altered itself, which I am unable to do as I am using the resnet architecture using keras. The following errors showed up respectively: Here is the code I am currently working on: Thank you for your time",https://stackoverflow.com/questions/70365874,17280417,Lack of Alternative Solutions/Documentation
70495270,"ValueError: Dimensions must be equal, but are 100 and 19 with input shapes: [?,100], [?,100,19]","<p>I have an error in my code, and I've done read the documentation but it still error, How this error can be fixed?</p>
<p><strong>Code:</strong></p>
<pre><code>import tensorflow.keras.backend as K
import tensorflow_addons as tfa
from tensorflow_addons.layers import CRF
from keras_crf import CRFModel
def create_model(): #
  max_words=length_long_sentence
  MAX_SENTENCE_NUM=100
  embedding_size=100
  lstm_size=128
  learn_rate=0.01
  output_size=len(unique_tag_set)

  current_input=Input(shape=(MAX_SENTENCE_NUM,max_words,)) 
  emb_current = Embedding(vocab_size, embedding_size, weights= 
  [embedding_matrix],input_length=max_words, name='current_embed',trainable=False)(current_input)
  hidden_vectors=TimeDistributed(Bidirectional(LSTM(units=lstm_size, return_sequences=False))) 
  (emb_current ) 
  hidden_vectors=Bidirectional(LSTM(units=lstm_size, return_sequences=True))(hidden_vectors ) 
  
  base = tf.keras.Model(inputs=current_input, outputs=hidden_vectors)
  model = CRFModel(base, 19)
  opt = tf.keras.optimizers.Adam(learning_rate=learn_rate)
  model.compile(optimizer=opt, metrics=['acc'])
  print(model.summary())
  return model
model_2=create_model()
</code></pre>
<p>and here is the model summary:
<a href=""https://i.stack.imgur.com/umzIB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/umzIB.png"" alt=""enter image description here"" /></a></p>
<p>Here is the code to fit in training data:</p>
<pre><code>history_2=model_2.fit(x_train_split,y_train_split,
                    epochs=1,batch_size=16,
                    shuffle = False, verbose = 1,
                    validation_split=0.2,
                    sample_weight=sample_weights)
</code></pre>
<p>And I got this error:</p>
<p>ValueError: in user code:</p>
<pre><code>File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 878, in train_function  *
    return step_function(self, iterator)
File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 867, in step_function  **
    outputs = model.distribute_strategy.run(run_step, args=(data,))
File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;, line 860, in run_step  **
    outputs = model.train_step(data)
File &quot;/usr/local/lib/python3.7/dist-packages/keras_crf/crf_model.py&quot;, line 49, in train_step
    crf_loss = -tfa.text.crf_log_likelihood(potentials, y, sequence_length, kernel)[0]
File &quot;/usr/local/lib/python3.7/dist-packages/tensorflow_addons/text/crf.py&quot;, line 242, in crf_log_likelihood
    inputs, tag_indices, sequence_lengths, transition_params
File &quot;/usr/local/lib/python3.7/dist-packages/tensorflow_addons/text/crf.py&quot;, line 104, in crf_sequence_score
    return tf.cond(tf.equal(tf.shape(inputs)[1], 1), _single_seq_fn, _multi_seq_fn)
File &quot;/usr/local/lib/python3.7/dist-packages/tensorflow_addons/text/crf.py&quot;, line 97, in _multi_seq_fn
    unary_scores = crf_unary_score(tag_indices, sequence_lengths, inputs)
File &quot;/usr/local/lib/python3.7/dist-packages/tensorflow_addons/text/crf.py&quot;, line 277, in crf_unary_score
    flattened_tag_indices = tf.reshape(offsets + tag_indices, [-1])

ValueError: Dimensions must be equal, but are 100 and 19 for '{{node cond/add_1}} = AddV2[T=DT_INT32](cond/add, cond/add_1/Cast)' with input shapes: [?,100], [?,100,19].
</code></pre>
","I have an error in my code, and I've done read the documentation but it still error, How this error can be fixed? Code: and here is the model summary: Here is the code to fit in training data: And I got this error: ValueError: in user code:",https://stackoverflow.com/questions/70495270,3499140,Requesting (Additional) Resources
70776552,Incompatibility between input and final Dense Layer (Value Error),"<p>I'm following this <a href=""https://pub.towardsai.net/step-by-step-guide-in-creating-your-own-emotion-recognition-system-b8aba98134c8"" rel=""nofollow noreferrer"">tutorial</a> from Nabeel Ahmed to create your own emotion detector using Keras (I'm a noob) and I've found a strange behaviour that I'd like to understand. The input data is a bunch of <strong>48x48 images</strong>, each one with an integer value between 0 and 6 (each number stands for an emotion label), which represents the emotion present in the image.</p>
<pre><code>train_X.shape -&gt; (28709, 2304) // training-data, 28709 images of 48x48
train_Y.shape -&gt; (28709,) //The emotion present in each image as an integer, 1 = happiness, 2 = sadness, etc.
val_X.shape -&gt; (3589, 2304)
val_Y.shape -&gt; (3589, )
</code></pre>
<p>In order to feed the data into the model, <code>train_X</code> and <code>val_X</code> are reshaped (as the tutorial explains)</p>
<pre><code>train_X.shape -&gt; (28709, 48, 48, 1)
val_X.shape -&gt; (3589, 48, 48, 1)
</code></pre>
<p>The model, as it is in the tutorial, is this one:</p>
<pre><code>model = Sequential()
input_shape = (48,48,1)
#1st convolution layer
model.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))
model.add(Conv2D(64, (5, 5), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

#2nd convolution layer
model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))
model.add(Conv2D(128, (5, 5),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

#3rd convolution layer
model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))
model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(128))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.2))
################################################################
model.add(Dense(7)) # &lt;- problematic line
################################################################
model.add(Activation('softmax'))

my_optimiser = tf.keras.optimizers.Adam(
learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
name='Adam')

model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=my_optimiser)
</code></pre>
<p>However, when I try to use it, using the tutorial snippet, I get an error in the line of the <code>validation_data</code> like this</p>
<pre><code>history = model.fit(train_X,     
        train_Y, 
        batch_size=64, 
        epochs=80, 
        verbose=1, 
        validation_data=(val_X, val_Y),
        shuffle=True)

ValueError: Shapes (None, 1) and (None, 7) are incompatible
</code></pre>
<p>After reviewing the code and the documentation about the <code>fit</code> method, my only idea was to change the <code>7</code> in the last <code>Dense</code> layer of the model to 1, which mysteriously works. I'd like to know what is happening here if anyone could give me a hint.</p>
","I'm following this tutorial from Nabeel Ahmed to create your own emotion detector using Keras (I'm a noob) and I've found a strange behaviour that I'd like to understand. The input data is a bunch of 48x48 images, each one with an integer value between 0 and 6 (each number stands for an emotion label), which represents the emotion present in the image. In order to feed the data into the model, train_X and val_X are reshaped (as the tutorial explains) The model, as it is in the tutorial, is this one: However, when I try to use it, using the tutorial snippet, I get an error in the line of the validation_data like this After reviewing the code and the documentation about the fit method, my only idea was to change the 7 in the last Dense layer of the model to 1, which mysteriously works. I'd like to know what is happening here if anyone could give me a hint.",https://stackoverflow.com/questions/70776552,5457202,Documentation Ambiguity
70979815,How to crop an image using TensorFlow?,"<p>I'm trying to crop an image where I have a detected object. From TensorFlow's documentation there is a function.</p>
<pre><code>tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width)
</code></pre>
<p>I'm trying to work out how to get the given arguments but not sure what information to use. Here's the code I'm working with.</p>
<pre><code>img = cv2.imread(IMAGE_PATH)
image_np = np.array(img)

input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)
detections = detect_fn(input_tensor)

num_detections = int(detections.pop('num_detections'))
detections = {key: value[0, :num_detections].numpy()
              for key, value in detections.items()}


detections['num_detections'] = num_detections
# detection_classes should be ints.
detections['detection_classes'] = detections['detection_classes'].astype(np.int64)

label_id_offset = 1
image_np_with_detections = image_np.copy()

viz_utils.visualize_boxes_and_labels_on_image_array(
            image_np_with_detections,
            detections['detection_boxes'],
            detections['detection_classes']+label_id_offset,
            detections['detection_scores'],
            category_index,
            use_normalized_coordinates=True,
            max_boxes_to_draw=9,
            min_score_thresh=.5,
            agnostic_mode=False)

#print(detections['detection_boxes'])
plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))
plt.show()
</code></pre>
<p>With this being the result, I'm trying to crop out everything but the bounding box -
<a href=""https://i.stack.imgur.com/AXrAU.png"" rel=""nofollow noreferrer"">Identified object</a></p>
","I'm trying to crop an image where I have a detected object. From TensorFlow's documentation there is a function. I'm trying to work out how to get the given arguments but not sure what information to use. Here's the code I'm working with. With this being the result, I'm trying to crop out everything but the bounding box - Identified object",https://stackoverflow.com/questions/70979815,18114544,Requesting (Additional) Resources
71245237,tensorflow : @ symbol,"<p>I saw a code snippet in the tensorflow documentation and couldn't find any info about it
So, what is the role/purpose of the <code>@</code> symbol at the code below :</p>
<pre><code>x @ tf.transpose(x)
</code></pre>
","I saw a code snippet in the tensorflow documentation and couldn't find any info about it So, what is the role/purpose of the @ symbol at the code below :",https://stackoverflow.com/questions/71245237,14118985,Documentation Ambiguity
71543827,understanding tensorflow Recommending movies: retrieval / usage of : in python class /usage of : in python function,"<p>I was reading and trying to work with below documentation from tensorflow
<a href=""https://www.tensorflow.org/recommenders/examples/basic_retrieval?hl=sl"" rel=""nofollow noreferrer"">https://www.tensorflow.org/recommenders/examples/basic_retrieval?hl=sl</a></p>
<p>In this we have implementation of <code>MovielenseModel</code> class. Let me provide snippet of same code below</p>
<pre><code>class MovielensModel(tfrs.Model):

  def __init__(self, user_model, movie_model):
    super().__init__()
    self.movie_model: tf.keras.Model = movie_model
    self.user_model: tf.keras.Model = user_model
    self.task: tf.keras.layers.Layer = task

  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -&gt; tf.Tensor:
    # We pick out the user features and pass them into the user model.
    user_embeddings = self.user_model(features[&quot;user_id&quot;])
    # And pick out the movie features and pass them into the movie model,
    # getting embeddings back.
    positive_movie_embeddings = self.movie_model(features[&quot;movie_title&quot;])

    # The task computes the loss and the metrics.
    return self.task(user_embeddings, positive_movie_embeddings)
</code></pre>
<p>In this one usages are not clear and could not find much help in any online documentations</p>
<ol>
<li>Usage of <code>self.movie_model: tf.keras.Model = movie_model</code> . Looks like its first class object implementation of function but how does this work? When I simply tried <code>d:c=3</code>, just to replicate it worked fine <code>d</code> gets value 3 and <code>c</code> its saying as undefined.</li>
</ol>
",I was reading and trying to work with below documentation from tensorflow https://www.tensorflow.org/recommenders/examples/basic_retrieval?hl=sl In this we have implementation of MovielenseModel class. Let me provide snippet of same code below In this one usages are not clear and could not find much help in any online documentations,https://stackoverflow.com/questions/71543827,12271381,Documentation Ambiguity
71590479,GradientTape returning None when run in a loop,"<p>The following gradient descent is failing 'coz the gradients returned by <code>tape.gradient()</code> are none when the loop runs second time.</p>
<pre class=""lang-py prettyprint-override""><code>w = tf.Variable(tf.random.normal((3, 2)), name='w')
b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')
x = tf.constant([[1., 2., 3.]])


for i in range(10):
  print(&quot;iter {}&quot;.format(i))
  with tf.GradientTape() as tape:
    #forward prop
    y = x @ w + b  
    loss = tf.reduce_mean(y**2)
    print(&quot;loss is \n{}&quot;.format(loss))
    print(&quot;output- y is \n{}&quot;.format(y))
    #vars getting dropped after couple of iterations
    print(tape.watched_variables()) 
  
  #get the gradients to minimize the loss
  dl_dw, dl_db = tape.gradient(loss,[w,b]) 

  #descend the gradients
  w = w.assign_sub(0.001*dl_dw)
  b = b.assign_sub(0.001*dl_db)
</code></pre>
<pre><code>iter 0
loss is 
23.328645706176758
output- y is 
[[ 6.8125362  -0.49663293]]
(&lt;tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=
array([[-1.3461215 ,  0.43708783],
       [ 1.5931423 ,  0.31951016],
       [ 1.6574576 , -0.52424705]], dtype=float32)&gt;, &lt;tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;)
iter 1
loss is 
22.634033203125
output- y is 
[[ 6.7103477  -0.48918355]]
()

TypeError                                 Traceback (most recent call last)
c:\projects\pyspace\mltest\test.ipynb Cell 7' in &lt;cell line: 1&gt;()
     11 dl_dw, dl_db = tape.gradient(loss,[w,b]) 
     13 #descend the gradients
---&gt; 14 w = w.assign_sub(0.001*dl_dw)
     15 b = b.assign_sub(0.001*dl_db)

TypeError: unsupported operand type(s) for *: 'float' and 'NoneType'
</code></pre>
<p>I checked the documentation which explains the possibilities of the gradients becoming <code>None</code>, but none of them are helping.</p>
","The following gradient descent is failing 'coz the gradients returned by tape.gradient() are none when the loop runs second time. I checked the documentation which explains the possibilities of the gradients becoming None, but none of them are helping.",https://stackoverflow.com/questions/71590479,3151330,Inadequate Examples
71889649,(Conv1D) Tensorflow and Jax Resulting Different Outputs for The Same Input,"<p>I am trying to use conv1d functions to make a transposed convlotion repectively at jax and tensorflow. I read the documentation of both of jax and tensorflow for the con1d_transposed operation but they are resulting with different outputs for the same input.</p>
<p>I can not find out what the problem is. And I don't know which one produces the correct results. Help me please.</p>
<p>My Jax Implementation (Jax Code)</p>
<pre class=""lang-py prettyprint-override""><code>x = np.asarray([[[1, 2, 3, 4, -5], [1, 2, 3, 4, 5]]], dtype=np.float32).transpose((0, 2, 1))
filters = np.array([[[1, 0, -1], [-1,  0,  1]], 
                    [[1, 1,  1], [-1, -1, -1]]], 
                    dtype=np.float32).transpose((2, 1, 0))

kernel_rot = np.rot90(np.rot90(filters))

print(f&quot;x strides:  {x.strides}\nfilters strides: {kernel_rot.strides}\nx shape: {x.shape}\nfilters shape: {filters.shape}\nx: \n{x}\nfilters: \n{filters}\n&quot;)

dn1 = lax.conv_dimension_numbers(x.shape, filters.shape,('NWC', 'WIO', 'NWC'))
print(dn1)

res = lax.conv_general_dilated(x,kernel_rot,(1,),'SAME',(1,),(1,),dn1)     

res = np.asarray(res)
print(f&quot;result strides: {res.strides}\nresult shape: {res.shape}\nresult: \n{res}\n&quot;)
</code></pre>
<p>My TensorFlow Implementation (TensorFlow Code)</p>
<pre class=""lang-py prettyprint-override""><code>x = np.asarray([[[1, 2, 3, 4, -5], [1, 2, 3, 4, 5]]], dtype=np.float32).transpose((0, 2, 1))
filters = np.array([[[1, 0, -1], [-1,  0,  1]], 
                    [[1, 1,  1], [-1, -1, -1]]], 
                    dtype=np.float32).transpose((2, 1, 0))

print(f&quot;x strides:  {x.strides}\nfilters strides: {filters.strides}\nx shape: {x.shape}\nfilters shape: {filters.shape}\nx: \n{x}\nfilters: \n{filters}\n&quot;)
    
res = tf.nn.conv1d_transpose(x, filters, output_shape = x.shape, strides = (1, 1, 1), padding = 'SAME', data_format='NWC', dilations=1)

res = np.asarray(res)
print(f&quot;result strides: {res.strides}\nresult shape: {res.shape}\nresult: \n{res}\n&quot;)
</code></pre>
<p>Output from the Jax</p>
<pre><code>result strides: (40, 8, 4)
result shape: (1, 5, 2)
result: 
[[[ 0.  0.]
  [ 0.  0.]
  [ 0.  0.]
  [10. 10.]
  [ 0. 10.]]]
</code></pre>
<p>Output from the TensorFlow</p>
<pre><code>result strides: (40, 8, 4)
result shape: (1, 5, 2)
result: 
[[[  5.  -5.]
  [  8.  -8.]
  [ 11. -11.]
  [  4.  -4.]
  [  5.  -5.]]]
</code></pre>
",I am trying to use conv1d functions to make a transposed convlotion repectively at jax and tensorflow. I read the documentation of both of jax and tensorflow for the con1d_transposed operation but they are resulting with different outputs for the same input. I can not find out what the problem is. And I don't know which one produces the correct results. Help me please. My Jax Implementation (Jax Code) My TensorFlow Implementation (TensorFlow Code) Output from the Jax Output from the TensorFlow,https://stackoverflow.com/questions/71889649,17030468,Documentation Ambiguity
72311927,How to train mnist data with tensorflow ParameterServerStrategy distributed training?,"<p>I'm trying to train the mnist dataset using the ParameterServerStrategy. As a beginner, I find the documentations to be confusing specially when it comes to the section &quot;Clusters in the real world&quot;. This is the docs that I'm following:<a href=""https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/parameter_server_training.ipynb#scrollTo=zyby6M2Jqg6J&amp;uniqifier=1"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/parameter_server_training.ipynb#scrollTo=zyby6M2Jqg6J&amp;uniqifier=1</a> So far I have this:</p>
<pre class=""lang-py prettyprint-override""><code>
#this is mnist_setup.py

import os
import tensorflow as tf
import numpy as np

def mnist_dataset(batch_size):
  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
  # The `x` arrays are in uint8 and have values in the [0, 255] range.
  # You need to convert them to float32 with values in the [0, 1] range.
  x_train = x_train / np.float32(255)
  y_train = y_train.astype(np.int64)
  train_dataset = tf.data.Dataset.from_tensor_slices(
      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)
  return train_dataset

def build_and_compile_model():
  model = tf.keras.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
  ])
  model.compile(
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      optimizer=tf.keras.optimizers.SGD(),
      metrics=['accuracy'])
  return model
</code></pre>
<pre class=""lang-py prettyprint-override""><code>#this is main.py

import os
import json

import tensorflow as tf
import mnist_setup

per_worker_batch_size = 64

os.environ[&quot;TF_CONFIG&quot;] = json.dumps({
    &quot;cluster&quot;: {
        &quot;worker&quot;: ['ip_of_deeplearning_VM:port'], #worker1
        &quot;ps&quot;: ['ip_of_deeplearning_VM:port'], #worker2
        &quot;chief&quot;: ['ip_of_deeplearning_VM:port'] #masterz
    },
    &quot;task&quot;: {&quot;type&quot;: &quot;chief&quot;, &quot;index&quot;: 0}
})

cluster_spec = tf.train.ClusterSpec({
    'ps':['ip_of_deeplearning_VM:port'], #worker2
    'worker': ['ip_of_deeplearning_VM:port'] #worker1
})

cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(cluster_spec, task_type=&quot;ps&quot;,task_id=0)

tf_config = json.loads(os.environ['TF_CONFIG'])
num_workers = len(tf_config['cluster']['worker'])

strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)

global_batch_size = per_worker_batch_size * num_workers
multi_worker_dataset = mnist_setup.mnist_dataset(global_batch_size)

with strategy.scope():
  # Model building/compiling need to be within `strategy.scope()`.
    multi_worker_model = mnist_setup.build_and_compile_model()
    
print(&quot;chief gets called!&quot;)
result = multi_worker_model.fit(multi_worker_dataset, epochs=3)
</code></pre>
<p>I copy these files to the worker and ps VM, change the index and run &quot;main.py&quot; on all of them at the same time. I get the message saying that the server was started at ip_address but that's about it. Would anyone please show me what I need to do in order to get this working?</p>
","I'm trying to train the mnist dataset using the ParameterServerStrategy. As a beginner, I find the documentations to be confusing specially when it comes to the section ""Clusters in the real world"". This is the docs that I'm following:https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/parameter_server_training.ipynb#scrollTo=zyby6M2Jqg6J&amp;uniqifier=1 So far I have this: I copy these files to the worker and ps VM, change the index and run ""main.py"" on all of them at the same time. I get the message saying that the server was started at ip_address but that's about it. Would anyone please show me what I need to do in order to get this working?",https://stackoverflow.com/questions/72311927,11909464,Documentation Ambiguity
73359572,normalize trained data with tensorflow,"<p>this is the code from the TensorFlow website, but doesn't explain well,</p>
<pre><code>normalization_layer = tf.keras.layers.Rescaling(1./255)
train_data = train_data.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.
</code></pre>
<p>i know what is the goal of this code which is to normalize data and make it between 0 and 1 instead of 0 to 255, but I need to understand what does lambda means here.</p>
","this is the code from the TensorFlow website, but doesn't explain well, i know what is the goal of this code which is to normalize data and make it between 0 and 1 instead of 0 to 255, but I need to understand what does lambda means here.",https://stackoverflow.com/questions/73359572,4399890,Documentation Ambiguity
73569804,Dataset.batch doesn't work as expected with a zipped dataset,"<p>I have a dataset like this:</p>
<pre><code>a = tf.data.Dataset.range(1, 16)
b = tf.data.Dataset.range(16, 32)
zipped = tf.data.Dataset.zip((a, b))
list(zipped.as_numpy_iterator())

# output: 
[(0, 16),
 (1, 17),
 (2, 18),
 (3, 19),
 (4, 20),
 (5, 21),
 (6, 22),
 (7, 23),
 (8, 24),
 (9, 25),
 (10, 26),
 (11, 27),
 (12, 28),
 (13, 29),
 (14, 30),
 (15, 31)]
</code></pre>
<p>When I apply <code>batch(4)</code> to it, the expected result is an array of batches, where each batch contains four tuples:</p>
<pre><code>[[(0, 16), (1, 17), (2, 18), (3, 19)],
 [(4, 20), (5, 21), (6, 22), (7, 23)],
 [(9, 24), (10, 25), (10, 26), (11, 27)],
 [(12, 28), (13, 29), (14, 30), (15, 31)]]
</code></pre>
<p>But this is what I receive instead:</p>
<pre><code>batched = zipped.batch(4)
list(batched.as_numpy_iterator())

# Output:
[(array([0, 1, 2, 3]), array([16, 17, 18, 19])), 
 (array([4, 5, 6, 7]), array([20, 21, 22, 23])), 
 (array([ 8,  9, 10, 11]), array([24, 25, 26, 27])), 
 (array([12, 13, 14, 15]), array([28, 29, 30, 31]))]
</code></pre>
<p>I'm following this <a href=""https://www.youtube.com/watch?v=N_W4EYtsa10&amp;t=5591s"" rel=""nofollow noreferrer"">tutorial</a>, he does the same steps but gets the correct output somehow.</p>
<hr />
<p>Update: according to the documentation this is the intended behavior:</p>
<blockquote>
<p>The components of the resulting element will have an additional <strong>outer</strong> dimension, which will be batch_size</p>
</blockquote>
<p>But it doesn't make any sense. To my understanding, dataset is a list of pieces of data. It doesn't matter the shape of those pieces of data, when we are batching it we are combining the elements [whatever their shape is] into batches, therefore it should always insert the new dimention to the second position (<code>(length, a, b, c)</code> -&gt; <code>(length', batch_size, a, b, c)</code>).</p>
<p>So my questions are: I wonder what is the purpose of <code>batch()</code> being implemented this way? And what is the alternative that does what I described?</p>
","I have a dataset like this: When I apply batch(4) to it, the expected result is an array of batches, where each batch contains four tuples: But this is what I receive instead: I'm following this tutorial, he does the same steps but gets the correct output somehow. Update: according to the documentation this is the intended behavior: But it doesn't make any sense. To my understanding, dataset is a list of pieces of data. It doesn't matter the shape of those pieces of data, when we are batching it we are combining the elements [whatever their shape is] into batches, therefore it should always insert the new dimention to the second position ((length, a, b, c) -&gt; (length', batch_size, a, b, c)). So my questions are: I wonder what is the purpose of batch() being implemented this way? And what is the alternative that does what I described?",https://stackoverflow.com/questions/73569804,12694438,Requesting (Additional) Resources
73629508,Keras Model.fit() TypeError: __init__() missing 1 required positional argument: 'normalizer',"<p>I am attempting to build a sequential model in Keras and use it to do image classification. I am getting the following error in the Anaconda3 (Python 3.9) terminal and haven't been able to track down the cause:</p>
<pre><code>(tf) &gt;python image_classification.py 10

Building Sequential Model ...

Compiling ...

Fitting Sequential Model with (X_train, Y_train) ...

Epoch 1/10
Traceback (most recent call last):
  File &quot;C:\%\image_classification.py&quot;, line 121, in &lt;module&gt;
    high_level_neural_net(X_train, Y_train, X_test, Y_test,  M, Nnodes1, Nnodes2, inject_layer, af, MAKE_PLOTS)
  File &quot;C:\%\image_classification.py&quot;, line 67, in high_level_neural_net
    model.fit(x=X_train, y=Y_train, epochs=M)
  File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\%\AppData\Local\Temp\__autograph_generated_filev98m50s2.py&quot;, line 15, in tf__train_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
TypeError: in user code:

    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\training.py&quot;, line 1051, in train_function  *
        return step_function(self, iterator)
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\training.py&quot;, line 1040, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\training.py&quot;, line 1030, in run_step  **
        outputs = model.train_step(data)
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\training.py&quot;, line 894, in train_step
        return self.compute_metrics(x, y, y_pred, sample_weight)
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\training.py&quot;, line 987, in compute_metrics
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\compile_utils.py&quot;, line 480, in update_state
        self.build(y_pred, y_true)
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\compile_utils.py&quot;, line 393, in build
        self._metrics = tf.__internal__.nest.map_structure_up_to(
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\compile_utils.py&quot;, line 526, in _get_metric_objects
        return [self._get_metric_object(m, y_t, y_p) for m in metrics]
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\compile_utils.py&quot;, line 526, in &lt;listcomp&gt;
        return [self._get_metric_object(m, y_t, y_p) for m in metrics]
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\engine\compile_utils.py&quot;, line 545, in _get_metric_object
        metric_obj = metrics_mod.get(metric)
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\metrics\__init__.py&quot;, line 182, in get
        return deserialize(str(identifier))
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\metrics\__init__.py&quot;, line 138, in deserialize
        return deserialize_keras_object(
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\utils\generic_utils.py&quot;, line 718, in deserialize_keras_object
        return obj()
    File &quot;C:\%\Anaconda3\envs\tf\lib\site-packages\keras\dtensor\utils.py&quot;, line 141, in _wrap_function
        init_method(instance, *args, **kwargs)

    TypeError: __init__() missing 1 required positional argument: 'normalizer'

</code></pre>
<p>So far, I haven't seen the 'normalizer' argument mentioned in the Tensorflow/Keras documentation or on the stackoverflow forums. Here is my source code:</p>
<pre><code>import os
import sys
import math
import scipy as sp
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

def get_dataset():
    
    SHOW_IMAGE = False

    mnist = tf.keras.datasets.mnist     # 28x28 images of hand-written digits 0-9
    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()
    
    X_train = tf.keras.utils.normalize(X_train,axis=1)  # Normalize input datasets between 0 and 1, Helps the NN converge.
    X_test = tf.keras.utils.normalize(X_test,axis=1)

    if (SHOW_IMAGE):
        plt.imshow(X_train[0])
        plt.show()
        input()
        plt.imshow(Y_train[0])
        plt.show()
        input()

    return X_train, Y_train, X_test, Y_test

def high_level_neural_net(X_train, Y_train, X_test, Y_test, M, Nnodes1, Nnodes2, inject_layer, af, MAKE_PLOTS):
    
    class InjectInputCallback(tf.keras.callbacks.Callback):

        # Inject input data to layer between residual blocks; I'm unsure if this works yet.

        def __init__(self,train_dataset,layer,logs=None):
            self.first_trainds = train_dataset
            self.inject_layer = layer

        def on_layer_end(self,layer,logs=None):
            model.layers[self.inject_layer].output = model.layers[self.inject_layer].output + self.first_trainds
           
    print('\n')
    
    print('Building Sequential Model ...\n')
   
    model = Sequential()
    model.add(Flatten())
    model.add(Dense(Nnodes2, activation=tf.nn.relu))                                   # Residual block 1 (hidden layers) 
    model.add(Dense(Nnodes2, activation=tf.nn.relu))                                   # ...
    model.add(Dense(10, activation=tf.nn.relu))                                         # Output layer of residual block 1/Input layer of residual block 2
    model.add(Dense(Nnodes2, activation=tf.nn.relu))                                   # Residual block 2 (hidden layers)
    model.add(Dense(Nnodes2, activation=tf.nn.relu))                                   # ...
    model.add(Dense(10, activation=tf.nn.relu))                                 # Output layer of residual block 2/Output of neural net 

    print('Compiling ...\n')
    model.compile(optimizer=Adam(learning_rate=0.001),  # Uses Adam algorithm as loss function optimizer
                 loss='MeanSquaredError',                                  # sets the model to use MSE as loss function during training
                 metrics=['MeanRelativeError'])                            
    
    print('Fitting Sequential Model with (X_train, Y_train) ...\n') 
    
    #tf.print('X_train = ', X_train, 'with shape', tf.shape(X_train), '\n')
    #tf.print('Y_train = ', Y_train, 'with shape', tf.shape(Y_train), '\n')

    model.fit(x=X_train, y=Y_train, epochs=M)
    #callbacks=[InjectInputCallback(X_train,inject_layer)]

    if (MAKE_PLOTS):
        plt.plot(history.history['MeanSquareError'])
        plt.title('Model Training')
        plt.ylabel('Mean Squared Error')
        plt.xlabel('Epoch')
        plt.legend(['X_Train','Y_Train'], loc='upper right')
        plt.show()

    #print('Evaluating Sequential Model with Analytic Solution...')
    #model.evaluate()
    
    #print('Predicting')
    #model.predict()

    model.build(tf.shape(X_train)) 
    tf.print(model.summary())

if (__name__ == &quot;__main__&quot;):
 
    print(&quot;\n&quot;)

    MAKE_PLOTS = True               # Turns on plots during training and evaluation of neural net

    M = int(sys.argv[1])            # Number of training iterations
   
    Ninputs = 3             # Number of node inputs
    Nnodes1 = 10            # Number of nodes in layers
    Nnodes2 = 30            # Number of nodes in 2nd and 4th layers
    inject_layer = 3        # layer of neural net where we inject input data to output of layer

    X_train, Y_train, X_test, Y_test = get_dataset()
    high_level_neural_net(X_train, Y_train, X_test, Y_test,  M, Nnodes1, Nnodes2, inject_layer, af, MAKE_PLOTS)
</code></pre>
","I am attempting to build a sequential model in Keras and use it to do image classification. I am getting the following error in the Anaconda3 (Python 3.9) terminal and haven't been able to track down the cause: So far, I haven't seen the 'normalizer' argument mentioned in the Tensorflow/Keras documentation or on the stackoverflow forums. Here is my source code:",https://stackoverflow.com/questions/73629508,19936707,Documentation Completeness
73840190,Why is StringLookup from producing an extra label?,"<p>From TF documentation:
&quot;one_hot&quot;: Encodes each individual element in the input into an array <strong>the same size</strong> as the vocabulary.</p>
<pre><code>alphabet = set(&quot;abcdefghijklmnopqrstuvwxyz&quot;)
one_hot_encoder = tf.keras.layers.StringLookup(vocabulary=list(alphabet), output_mode='one_hot')
print(len(alphabet)) #26
print(one_hot_encoder(&quot;a&quot;).shape) #(27,)
</code></pre>
<p>As far as I understand it it should encode to a 26 shaped tensor. Why does it encode to a 27 shaped one? Should there be an extra label to represent &quot;no class&quot;?</p>
","From TF documentation: ""one_hot"": Encodes each individual element in the input into an array the same size as the vocabulary. As far as I understand it it should encode to a 26 shaped tensor. Why does it encode to a 27 shaped one? Should there be an extra label to represent ""no class""?",https://stackoverflow.com/questions/73840190,14742134,Documentation Completeness
73853604,how to include image input to a transformer model?,"<p>i am using this transformer architecture: <a href=""https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/IBM_Transformer%2BTimeEmbedding.ipynb"" rel=""nofollow noreferrer"">https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/IBM_Transformer%2BTimeEmbedding.ipynb</a></p>
<p>to make some binary clasification, i am adding some pictures as input, but i was wondering, how is the right way to do this?</p>
<p>my modified architecture is:</p>
<pre><code>'''Initialize time and transformer layers'''
    time_embedding = Time2Vector(seq_len)
    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)
    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)
    attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)
    '''Construct model'''
    liq_seq = Input(shape=(seq_len, XN_train.shape[2],))

    pic_seq = Input(name=&quot;input_images&quot;,shape=(500,700,3))
  
x_t = time_embedding(liq_seq)
x_liq= Concatenate(axis=-1)([liq_seq, x_t])
x_liq  = LSTM(
    units = 64, 
    return_sequences=False
    )(liq_seq)
x_liq=LayerNormalization()(x_liq)
x_liq = Dense(64)(x_liq)
x_liq=LayerNormalization()(x_liq)

x_pic = Conv2D(64, (10, 10), name=&quot;first_conv&quot;, activation='relu', input_shape=(500,700, 3))(pic_seq)
x_pic =MaxPooling2D((2, 2),name=&quot;first_pooling&quot;)(x_pic)
x_pic = Flatten(name=&quot;flatten&quot;)(x_pic)
x_pic =Dense(64, activation='tanh')(x_pic)
x_pic=LayerNormalization()(x_pic)


x_liq_pic = Concatenate(axis=1)([x_liq, x_pic])
x_liq_pic =Dense(seq_len*2, activation='tanh')(x_liq_pic)

x_liq_pic= Reshape((seq_len,2))(x_liq_pic)

#x_liq_pic = Concatenate(axis=-1)([x_liq_pic, x_t])  

x_liq_pic = attn_layer1((x_liq_pic, x_liq_pic, x_liq_pic))
x_liq_pic = attn_layer2((x_liq_pic, x_liq_pic, x_liq_pic))
x_liq_pic = attn_layer3((x_liq_pic, x_liq_pic, x_liq_pic))
x_liq_pic = GlobalAveragePooling1D(data_format='channels_first')(x_liq_pic)
x_liq_pic = Dropout(0.2)(x_liq_pic)
x_liq_pic = Dense(64, activation='tanh')(x_liq_pic)
x_liq_pic = Dropout(0.2)(x_liq_pic)
out = Dense(1, activation='softmax')(x_liq_pic)

model = Model(inputs=[pic_seq,liq_seq], outputs=out) 
</code></pre>
<p>here i am doing the concatenation of the time embedding beforethe first lstm(not sure is i should add this lstm layer and concatenate here) then i use the dense layer to make it have a common shape, then i put a convolutional 2d to start working whit the images, then it goes to the dense in order to make it have the desired shape</p>
<p>having this two outputs whit the same shape, i concatenate them and then pass it over a dense, then i reshape it, in order to do the time embedding concatenation again before sending all this mess up to the transformer's layers</p>
<p>here it is the the model's plot <a href=""https://i.stack.imgur.com/4rR5c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4rR5c.png"" alt=""enter image description here"" /></a></p>
<p>i really feel like im doing this wrong but i can't find too much documentation over this topic, also i am using a tensorflow dataset to feed the network</p>
<p>here i put the time2vec, attention, multihead and transformer classes (almost identical to the github code)</p>
<pre><code>class Time2Vector(Layer):
  def __init__(self, seq_len, **kwargs):
    super(Time2Vector, self).__init__()
    self.seq_len = seq_len

  def build(self, input_shape):
    '''Initialize weights and biases with shape (batch, seq_len)'''
    self.weights_linear = self.add_weight(name='weight_linear',
                                shape=(int(self.seq_len),),
                                initializer='uniform',
                                trainable=True)
    
    self.bias_linear = self.add_weight(name='bias_linear',
                                shape=(int(self.seq_len),),
                                initializer='uniform',
                                trainable=True)
    
    self.weights_periodic = self.add_weight(name='weight_periodic',
                                shape=(int(self.seq_len),),
                                initializer='uniform',
                                trainable=True)

    self.bias_periodic = self.add_weight(name='bias_periodic',
                                shape=(int(self.seq_len),),
                                initializer='uniform',
                                trainable=True)

  def call(self, x):
    '''Calculate linear and periodic time features'''
    x = tf.math.reduce_mean(x[:,:,:1], axis=-1) 
    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature
    time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)
    
    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)
    time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)
    return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)
   
  def get_config(self): # Needed for saving and loading model with custom layer
    config = super().get_config().copy()
    config.update({'seq_len': self.seq_len})
    return config


class SingleAttention(Layer):
  def __init__(self, d_k, d_v):
    super(SingleAttention, self).__init__()
    self.d_k = d_k
    self.d_v = d_v

  def build(self, input_shape):
    self.query = Dense(self.d_k, 
                       input_shape=input_shape, 
                       kernel_initializer='glorot_uniform', 
                       bias_initializer='glorot_uniform')
    
    self.key = Dense(self.d_k, 
                     input_shape=input_shape, 
                     kernel_initializer='glorot_uniform', 
                     bias_initializer='glorot_uniform')
    
    self.value = Dense(self.d_v, 
                       input_shape=input_shape, 
                       kernel_initializer='glorot_uniform', 
                       bias_initializer='glorot_uniform')

  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)
    q = self.query(inputs[0])
    k = self.key(inputs[1])

    attn_weights = tf.matmul(q, k, transpose_b=True)
    attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)
    attn_weights = tf.nn.softmax(attn_weights, axis=-1)
    
    v = self.value(inputs[2])
    attn_out = tf.matmul(attn_weights, v)
    return attn_out    

#############################################################################

class MultiAttention(Layer):
  def __init__(self, d_k, d_v, n_heads):
    super(MultiAttention, self).__init__()
    self.d_k = d_k
    self.d_v = d_v
    self.n_heads = n_heads
    self.attn_heads = list()

  def build(self, input_shape):
    for n in range(self.n_heads):
      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  
    
    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 
    self.linear = Dense(input_shape[0][-1], 
                        input_shape=input_shape, 
                        kernel_initializer='glorot_uniform', 
                        bias_initializer='glorot_uniform')

  def call(self, inputs):
    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]
    concat_attn = tf.concat(attn, axis=-1)
    multi_linear = self.linear(concat_attn)
    return multi_linear   

#############################################################################

class TransformerEncoder(Layer):
  def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):
    super(TransformerEncoder, self).__init__()
    self.d_k = d_k
    self.d_v = d_v
    self.n_heads = n_heads
    self.ff_dim = ff_dim
    self.attn_heads = list()
    self.dropout_rate = dropout

  def build(self, input_shape):
    self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)
    self.attn_dropout = Dropout(self.dropout_rate)
    self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)
    self.ff_LSTM= LSTM(units=self.ff_dim,input_shape=input_shape,return_sequences=True)
    self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='sigmoid')
    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 
    self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) 
    self.ff_dropout = Dropout(self.dropout_rate)
    self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    
  
  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)
    attn_layer = self.attn_multi(inputs)
    attn_layer = self.attn_dropout(attn_layer)
    attn_layer = self.attn_normalize(inputs[0] + attn_layer)
    ff_layer = self.ff_LSTM(attn_layer)
    ff_layer = self.ff_conv1D_1(ff_layer)
    ff_layer = self.ff_conv1D_2(ff_layer)
    ff_layer = self.ff_dropout(ff_layer)
    ff_layer = self.ff_normalize(inputs[0] + ff_layer)
    return ff_layer 

  def get_config(self): 
    config = super().get_config().copy()
    config.update({'d_k': self.d_k,
                   'd_v': self.d_v,
                   'n_heads': self.n_heads,
                   'ff_dim': self.ff_dim,
                   'attn_heads': self.attn_heads,
                   'dropout_rate': self.dropout_rate})
    return config      
</code></pre>
","i am using this transformer architecture: https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/IBM_Transformer%2BTimeEmbedding.ipynb to make some binary clasification, i am adding some pictures as input, but i was wondering, how is the right way to do this? my modified architecture is: here i am doing the concatenation of the time embedding beforethe first lstm(not sure is i should add this lstm layer and concatenate here) then i use the dense layer to make it have a common shape, then i put a convolutional 2d to start working whit the images, then it goes to the dense in order to make it have the desired shape having this two outputs whit the same shape, i concatenate them and then pass it over a dense, then i reshape it, in order to do the time embedding concatenation again before sending all this mess up to the transformer's layers here it is the the model's plot i really feel like im doing this wrong but i can't find too much documentation over this topic, also i am using a tensorflow dataset to feed the network here i put the time2vec, attention, multihead and transformer classes (almost identical to the github code)",https://stackoverflow.com/questions/73853604,11579387,Lack of Alternative Solutions/Documentation
74481219,How to get number of values in each row of a sparse tensor?,"<p>I have a Sparse Tensor as follows:</p>
<pre><code>st = tf.sparse.from_dense([[1, 0, 2, 5], [3, 0, 0, 4], [0, 0, 0, 0], [1, 1, 3, 0], [1, 2, 2, 2]])
print(st)
</code></pre>
<pre><code>SparseTensor(indices=tf.Tensor(
[[0 0]
 [0 2]
 [0 3]
 [1 0]
 [1 3]
 [3 0]
 [3 1]
 [3 2]
 [4 0]
 [4 1]
 [4 2]
 [4 3]], shape=(12, 2), dtype=int64), values=tf.Tensor([1 2 5 3 4 1 1 3 1 2 2 2], shape=(12,), dtype=int32), dense_shape=tf.Tensor([5 4], shape=(2,), dtype=int64))
</code></pre>
<p>I want to convert this sparse tensor to another 1D tensor of shape <code>(5, 1)</code> where the only column represents the number (or size) of values in each of the rows.</p>
<p>For example, for the above sparse tensor, desired 1D tensor would be <code>[3, 2, 0, 3, 4]</code>.</p>
<p>How can I do it?</p>
<p>I tried going through the TensorFlow API docs but couldn't find anything to try.</p>
","I have a Sparse Tensor as follows: I want to convert this sparse tensor to another 1D tensor of shape (5, 1) where the only column represents the number (or size) of values in each of the rows. For example, for the above sparse tensor, desired 1D tensor would be [3, 2, 0, 3, 4]. How can I do it? I tried going through the TensorFlow API docs but couldn't find anything to try.",https://stackoverflow.com/questions/74481219,3243499,Documentation Completeness
75020232,Get input shape with Keras custom layer,"<p>I am writing a custom layer using Keras that returns a tensors of zeros the first three times it is invoked and does nothing the other times. The code is the following</p>
<pre class=""lang-py prettyprint-override""><code>class MyLayer(tf.keras.layers.Layer):

    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
        self.__iteration = 0
        self.__returning_zeros = None

    def build(self, input_shape):
        self.__returning_zeros = tf.zeros(shape=input_shape, dtype=tf.float32)

    def call(self, inputs):
        self.__iteration += 1

        if self.__iteration &lt;= 3:
            return self.__returning_zeros
        else:
            return inputs
</code></pre>
<p>Unfortunately if I try to build a model using this layer like this</p>
<pre class=""lang-py prettyprint-override""><code>def build_model(input_shape, num_classes):
    input_layer = keras.Input(shape=input_shape, name='input')
    conv1 = layers.Conv2D(32, kernel_size=(3, 3), activation=&quot;relu&quot;, name='conv1')(input_layer)
    maxpool1 = layers.MaxPooling2D(pool_size=(2, 2), name='maxpool1')(conv1)
    conv2 = layers.Conv2D(64, kernel_size=(3, 3), activation=&quot;relu&quot;, name='conv2')(maxpool1)
    mylayer = MyLayer()(conv2)
    maxpool2 = layers.MaxPooling2D(pool_size=(2, 2), name='maxpool2')(mylayer)
    flatten = layers.Flatten(name='flatten')(maxpool2)
    dropout = layers.Dropout(0.5, name='dropout')(flatten)
    dense = layers.Dense(num_classes, activation=&quot;softmax&quot;, name='dense')(dropout)

    return keras.Model(inputs=(input_layer,), outputs=dense)
</code></pre>
<p>I get the following error message</p>
<pre><code>  File &quot;customlayerkeras.py&quot;, line 25, in build
    self.__returning_zeros = tf.zeros(shape=input_shape, dtype=tf.float32)
ValueError: Cannot convert a partially known TensorShape (None, 13, 13, 64) to a Tensor.
</code></pre>
<p>Where it seems that, despite using the build function as suggested in the documentation I am not able to retrieve the correct shape of the input.
How can I fix this problem?</p>
<p>EDIT:
I was complicating the problem without thinking, the best solution is to just multiply the inputs per zero like this</p>
<pre><code>def call(self, inputs):
        self.__iteration += 1

        if self.__iteration &lt;= 3:
            return inputs*0
        else:
            return inputs
</code></pre>
","I am writing a custom layer using Keras that returns a tensors of zeros the first three times it is invoked and does nothing the other times. The code is the following Unfortunately if I try to build a model using this layer like this I get the following error message Where it seems that, despite using the build function as suggested in the documentation I am not able to retrieve the correct shape of the input. How can I fix this problem? EDIT: I was complicating the problem without thinking, the best solution is to just multiply the inputs per zero like this",https://stackoverflow.com/questions/75020232,7286547,Documentation Replicability
75552310,How to use my pretrained LSTM saved model to make new classifications,"<p>I have a simple pretrained LSTM model builded with Keras and Tensorflow, I trained, compiled and fitted it, and make a test prediction with a simple sentence, and it works, then I saved my model using <code>model.save(sentanalysis.h5</code> and everything OK. Then, I loaded this model with <code>model.load_model()</code>, and it loads without error, but when I tried <code>model.predict()</code> I got an array with floats that doesn't shows anything related to the classes:</p>
<p>How can I use my pretrained model to make new classifications?
The dataset I use to train it is very simple, a csv with <code>text</code> and <code>sentiment</code> columns, nothing else.
Can you help me?
This is the code of the model:</p>
<pre><code>import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import nlp
import random
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences

dataset = nlp.load_dataset('csv', data_files={'train':'/content/drive/MyDrive/Proyect/BehaviorClassifier/tass2019_pe_train_final.csv',
                                              'test': '/content/drive/MyDrive/Proyect/BehaviorClassifier/tass2019_pe_test_final.csv',
                                              'validation': '/content/drive/MyDrive/Proyect/BehaviorClassifier/tass2019_pe_val_final.csv'})
train = dataset['train']
val = dataset['validation']
test = dataset['test']

def get_tweet(data):
    tweets = [x['Text'] for x in data]
    labels = [x['behavior'] for x in data]
    return tweets, labels

tweets, labels = get_tweet(train)

tokenizer = Tokenizer(num_words=10000, oov_token='&lt;UNK&gt;')
tokenizer.fit_on_texts(tweets)

maxlen = 140

def get_sequences(tokenizer, tweets):
    sequences = tokenizer.texts_to_sequences(tweets)
    padded = pad_sequences(sequences, truncating='post', padding='post', maxlen=maxlen)
    return padded

padded_train_seq = get_sequences(tokenizer, tweets)

classes = set(labels)
class_to_index = dict((c, i) for i, c in enumerate(classes))
index_to_class = dict((v, k) for k, v in class_to_index.items())
names_to_ids = lambda labels: np.array([class_to_index.get(x) for x in labels])
train_labels = names_to_ids(labels)

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(10000, 16, input_length=maxlen),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),
    tf.keras.layers.Dense(6, activation='softmax')
])
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

val_tweets, val_labels = get_tweet(val)
val_seq = get_sequences(tokenizer, val_tweets)
val_labels= names_to_ids(val_labels)
h = model.fit(
     padded_train_seq, train_labels,
     validation_data=(val_seq, val_labels),
     epochs=8#,
     #callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)]
)

test_tweets, test_labels=get_tweet(test)
test_seq = get_sequences(tokenizer, test_tweets)
test_labels=names_to_ids(test_labels)
model.evaluate(test_seq, test_labels)

# This code works when I loaded the previos code
sentence = 'I am very happy now'
sequence = tokenizer.texts_to_sequences([sentence])
paddedSequence = pad_sequences(sequence, truncating = 'post', padding='post', maxlen=maxlen)
p = model.predict(np.expand_dims(paddedSequence[0], axis=0))[0]
pred_class=index_to_class[np.argmax(p).astype('uint8')]
print('Sentence: ', sentence)
print('Sentiment: ', pred_class)
</code></pre>
<p>And this is how I save and load my model withouth loading previous code:</p>
<pre><code>model.save('/content/drive/MyDrive/Proyect/BehaviorClassifier/twitterBehaviorClassifier.h5')
model = keras.models.load_model('/content/drive/MyDrive/Proyect/BehaviorClassifier/twitterBehaviorClassifier.h5')

#### ISSUE HERE
new = [&quot;I am very happy&quot;]
tokenizer = Tokenizer(num_words=10000, oov_token='&lt;UNK&gt;')
tokenizer.fit_on_texts(new)
seq = tokenizer.texts_to_sequences(new)
padded = pad_sequences(seq, maxlen=140)
pred = model.predict(padded)
</code></pre>
<p>And I get this:</p>
<pre><code>1/1 [==============================] - 0s 29ms/step
[[7.0648360e-01 1.1568426e-01 1.7581969e-01 7.2872970e-04 4.2903548e-04
  8.5460022e-04]]
</code></pre>
<p>I've reading some doc, but nothing helped me.</p>
","I have a simple pretrained LSTM model builded with Keras and Tensorflow, I trained, compiled and fitted it, and make a test prediction with a simple sentence, and it works, then I saved my model using model.save(sentanalysis.h5 and everything OK. Then, I loaded this model with model.load_model(), and it loads without error, but when I tried model.predict() I got an array with floats that doesn't shows anything related to the classes: How can I use my pretrained model to make new classifications? The dataset I use to train it is very simple, a csv with text and sentiment columns, nothing else. Can you help me? This is the code of the model: And this is how I save and load my model withouth loading previous code: And I get this: I've reading some doc, but nothing helped me.",https://stackoverflow.com/questions/75552310,19156897,Inadequate Examples
75963463,reshaping logits to dim 1,"<p>I am currently working on a T5 model which I am trying to fine-tune. On the training step, I get my outputs, get my loss, and try to update the state of my metric. However, in order to update the state of the metric I am required to use a 1-D array.</p>
<p>From the documentation: &quot;Both
prediction and labels must be 1-D arrays of the same shape in order for this
function to work.&quot;</p>
<p>My question is, is there a recommended way to reshape my outputs.logits variable? it currently has a shape of (8, 2, 32128) and would like to bring it 8. I have tried the argmax function, and softmax but had no luck. Any ideas would be appreciated.</p>
<p>Code:</p>
<pre><code>model_name = 't5-base'
t5_model = TFT5ForConditionalGeneration.from_pretrained(model_name)
t5_tokenizer = T5Tokenizer.from_pretrained(model_name)

def train_step(self, inputs):
  input_ids = inputs['input_ids']
  attention_mask = inputs['attention_mask']
  labels = inputs['labels']
  labels_mask = inputs['labels_mask']
  with tf.GradientTape() as tape:
    outputs = self(input_ids=input_ids,
                       attention_mask=attention_mask,
                       labels=labels,
                       decoder_attention_mask=labels_mask,
                       training=True
                  )
    loss = self.compiled_loss(labels, outputs.logits, regularization_losses=self.losses)
    
  self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
  self.compiled_metrics.update_state(labels, outputs.logits) ## issue happens here
  return_metrics = {}
  for metric in self.metrics:
    result = metric.result()
    if isinstance(result, dict):
        return_metrics.update(result)
    else:
        return_metrics[metric.name] = result
  if &quot;loss&quot; in return_metrics and &quot;loss_loss&quot; in return_metrics:
    del return_metrics[&quot;loss_loss&quot;]
  return return_metrics
</code></pre>
","I am currently working on a T5 model which I am trying to fine-tune. On the training step, I get my outputs, get my loss, and try to update the state of my metric. However, in order to update the state of the metric I am required to use a 1-D array. From the documentation: ""Both prediction and labels must be 1-D arrays of the same shape in order for this function to work."" My question is, is there a recommended way to reshape my outputs.logits variable? it currently has a shape of (8, 2, 32128) and would like to bring it 8. I have tried the argmax function, and softmax but had no luck. Any ideas would be appreciated. Code:",https://stackoverflow.com/questions/75963463,1261440,Requesting (Additional) Resources
76262758,TF2.3 - More model outputs than targets,"<p>I am trying to write a model in which there are three outputs, the latter two of which are to be trained with respect to targets present in the dataset, the former should just be a non-trainable output.</p>
<p>First, defining a dataset:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Tuple

import numpy as np
import tensorflow as tf

def ds_fn(in_shape: Tuple[int],
          out_shape: Tuple[int],
          dtype: tf.DType = tf.float32) -&gt; tf.data.Dataset:
  # Generator function.
  def gen() -&gt; Tuple[Tuple[np.array], Tuple[np.array]]:
    for _ in range(1000):
      # Inputs.
      x0 = np.ones(in_shape, dtype=np.float32)
      x1 = 2 * x0

      # Outputs.
      y0 = np.ones(out_shape, dtype=np.float32)
      y1 = 2 * np.ones_like(y0)
      y2 = 3 * np.ones_like(y1)

      # Targets correspond to outputs 1 and 2 of the network. Output 0 has
      # no target.
      yield (x0, x1), (y1, y2)

  return tf.data.Dataset.from_generator(gen,
                                        output_types=((dtype,) * 2, # Input
                                                      (dtype,) * 2), # Output
                                        output_shapes=((in_shape,) * 2, # Input
                                                       (out_shape,) * 2)) # Output
</code></pre>
<p>In the above, the dataset has two targets, corresponding to the last two outputs of the model defined below.</p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow import keras

class ExampleModel(keras.Model):
  def __init__(self, out_dim: int):
    super().__init__()

    self.dense_a = keras.layers.Dense(out_dim)
    self.dense_b = keras.layers.Dense(out_dim)

  def call(self, inputs, training=False):
    a, b = inputs

    x0 = self.dense_a(a)
    x1 = self.dense_b(b)

    x = (x0 + x1) / 2

    return (x, # Output 0 - should not be trained.
            2 * x,
            3 * x)
</code></pre>
<p>From reading the Keras documentation, to handle this case where there are a greater number of model outputs than there are targets in the dataset (with the surplus outputs considered to be non-trainable), it appears that the <code>Model.compile</code> arg <code>loss_weights</code> should handle the matching between targets and losses. More concretely, given the following.</p>
<pre class=""lang-py prettyprint-override""><code>def model_fn(out_dim: int) -&gt; ExampleModel:

  m = ExampleModel(out_dim)

  losses = [
    None, # Output 0 - should not be trainable.
    'mse',
    'mse'
  ]

  loss_weights = [
    0, # Output 0 - should not be trainable.
    1,
    1
  ]

  m.compile(loss=losses,
            loss_weights=loss_weights,
            optimizer='sgd')

  return m
</code></pre>
<p>I would expect that Keras would disregard the first model output when computing the loss, given the <code>None</code> loss provided and the <code>0</code> loss weight, however I am seeing the following error.</p>
<pre><code>ValueError: The two structures don't have the same sequence length. Input structure has length 2, while shallow structure has length 3.
</code></pre>
<p>Which seems to indicate that this is not the case when run as follows.</p>
<pre class=""lang-py prettyprint-override""><code>if __name__ == &quot;__main__&quot;:
  bs = 16
  in_dim = 4
  out_dim = 8
  epochs = 10

  ds = ds_fn((bs, 1, in_dim), (bs, 1, out_dim))
  ds = ds.repeat(epochs)

  m = model_fn(out_dim)

  m.fit(ds,
        epochs=epochs,
        batch_size=bs)
</code></pre>
<p>If I provide an additional target in <code>data_fn</code> and <code>gen</code>, combined with a dummy loss (<code>lambda x, y: 0.0</code>. for example), then training commences. However, this will not scale to a non toy problem with potentially large outputs and targets (images, for example).</p>
<p>If I instead return a <code>dict</code> from the model <code>call</code> method and provide <code>dict</code> for <code>losses</code> and <code>loss_weights</code> (both with keys matching that returned from <code>call</code>), there is no change (I thought that the explicit output naming might allow Keras to match outputs, losses and targets).</p>
<p>Am I misunderstanding the intended purpose of lists as <code>losses</code> (in which <code>None</code> is allowed) and <code>loss_weights</code>?</p>
","I am trying to write a model in which there are three outputs, the latter two of which are to be trained with respect to targets present in the dataset, the former should just be a non-trainable output. First, defining a dataset: In the above, the dataset has two targets, corresponding to the last two outputs of the model defined below. From reading the Keras documentation, to handle this case where there are a greater number of model outputs than there are targets in the dataset (with the surplus outputs considered to be non-trainable), it appears that the Model.compile arg loss_weights should handle the matching between targets and losses. More concretely, given the following. I would expect that Keras would disregard the first model output when computing the loss, given the None loss provided and the 0 loss weight, however I am seeing the following error. Which seems to indicate that this is not the case when run as follows. If I provide an additional target in data_fn and gen, combined with a dummy loss (lambda x, y: 0.0. for example), then training commences. However, this will not scale to a non toy problem with potentially large outputs and targets (images, for example). If I instead return a dict from the model call method and provide dict for losses and loss_weights (both with keys matching that returned from call), there is no change (I thought that the explicit output naming might allow Keras to match outputs, losses and targets). Am I misunderstanding the intended purpose of lists as losses (in which None is allowed) and loss_weights?",https://stackoverflow.com/questions/76262758,588959,Documentation Replicability
76402835,ValueError: tf.function only supports singleton tf.Variables created on the first call,"<p>I have the following code:</p>
<pre><code>class Encoder(tf.keras.Model):
    def __init__(self, params):
        super(Encoder, self).__init__()
        self.params = params
        
        self.hidden_layer_one = tf.keras.layers.Conv2D(filters=self.params['e_filters'],
                                                       kernel_size=self.params['e_size'],
                                                       activation=self.params['activation'], strides=1, padding='same',
                                                       kernel_initializer=params['initializer'], use_bias=False)
        self.hidden_layer_two = tf.keras.layers.Conv2D(filters=self.params['num_endmembers'], kernel_size=1,
                                                       activation=self.params['activation'], strides=1, padding='same',
                                                       kernel_initializer=self.params['initializer'], use_bias=False)
        self.asc_layer = ASC(params=self.params, name='abundances')

    def call(self, input_patch):
        code = self.hidden_layer_one(input_patch)
        code = tf.keras.layers.BatchNormalization()(code)
        code = tf.keras.layers.SpatialDropout2D(0.2)(code)
        code = self.hidden_layer_two(code)
        code = tf.keras.layers.BatchNormalization()(code)
        code = tf.keras.layers.SpatialDropout2D(0.2)(code)
        code = self.asc_layer(code)
        return code

class Autoencoder(tf.keras.Model):
    def __init__(self, params):
        super(Autoencoder, self).__init__()
        self.encoder = Encoder(params)
        self.decoder = Decoder(params)
        self.params = params

    def call(self, patch):
        abunds = self.encoder(patch)
        abunds = tf.keras.layers.SpatialDropout2D(0.08)(abunds)
        output = self.decoder(abunds)
        return output

    
    def train(self, patches, callback):
        self.plotWhileTraining = callback
        self.fit(patches, patches, epochs=self.params['epochs'], batch_size=self.params['batch_size'],
                 callbacks=[self.plotWhileTraining], verbose=0)

</code></pre>
<p>I get the error in the title,</p>
<pre><code>ValueError: tf.function only supports singleton tf.Variables created on the first call. 
Make sure the tf.Variable is only created once or created outside tf.function. 
See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.
        
        
        Call arguments received by layer 'encoder_3' (type Encoder):
          • input_patch=tf.Tensor(shape=(15, 80, 80, 156), dtype=float32)
    
    
    Call arguments received by layer 'autoencoder_3' (type Autoencoder):
      • patch=tf.Tensor(shape=(15, 80, 80, 156), dtype=float32)
</code></pre>
<p>I read the documentation but I still don't know how to solve it.</p>
<p>I tried <code>model.build()</code> but I'm not sure where to call it.</p>
","I have the following code: I get the error in the title, I read the documentation but I still don't know how to solve it. I tried model.build() but I'm not sure where to call it.",https://stackoverflow.com/questions/76402835,14114819,Documentation Ambiguity
38800965,What does the column and rows for images in TensorBoard mean?,"<p>I was trying to use the tensorflow <code>tf. image_summary</code> but it wasn't clear to me how to use it. In the <a href=""https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/tensorboard/README.md"" rel=""nofollow noreferrer"">tensorboard readme</a> file they have the following sentence that confuses me:</p>

<blockquote>
  <p>The dashboard is set up so that each row corresponds to a different
  tag, and each column corresponds to a run.</p>
</blockquote>

<p>I don't understand the sentence and thus, I am having a hard time figuring out what the columns and rows mean for TensorBoard image visualization. What exactly is a ""tag"" and what exactly is a ""run""? How do I get multiple ""tags"" and multiple ""runs"" to display? Why would I want multiple ""tags"" and ""runs"" to display? </p>

<p>Does someone have a very simple but non-trivial example of how to use this?</p>

<p>Ideally, what I want to use is compare how my model performs with respect to PCA so in my head it would be nice to compare how the reconstructions compare to PCA reconstruction at each step. Not sure if this is a good idea but I also want to see what the activation images look like and how the templates look like.</p>

<hr>

<p>Curenttly I have a very simple script with the following lines:</p>

<pre><code>with tf.name_scope('input_reshape'):
    x_image = tf.to_float(x, name='ToFloat')
    image_shaped_input = tf.reshape(x_image, [-1, 28, 28, 1])
    tf.image_summary('input', image_shaped_input, 10)
</code></pre>

<p>currently I have managed to discover that the rows are of length 10 so i assume its showing me 10 images that have something to do with the current run/batch.</p>

<p><a href=""https://i.stack.imgur.com/1KUEP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1KUEP.png"" alt=""enter image description here""></a></p>

<p>however, if possible I'd like to see reconstruction, filters (currently I am doing fully connected to keep things simple but eventually it would be nice to see a conv net examples), activation units (with any number of units that I choose), etc. </p>
","I was trying to use the tensorflow tf. image_summary but it wasn't clear to me how to use it. In the tensorboard readme file they have the following sentence that confuses me: I don't understand the sentence and thus, I am having a hard time figuring out what the columns and rows mean for TensorBoard image visualization. What exactly is a ""tag"" and what exactly is a ""run""? How do I get multiple ""tags"" and multiple ""runs"" to display? Why would I want multiple ""tags"" and ""runs"" to display? Does someone have a very simple but non-trivial example of how to use this? Ideally, what I want to use is compare how my model performs with respect to PCA so in my head it would be nice to compare how the reconstructions compare to PCA reconstruction at each step. Not sure if this is a good idea but I also want to see what the activation images look like and how the templates look like. Curenttly I have a very simple script with the following lines: currently I have managed to discover that the rows are of length 10 so i assume its showing me 10 images that have something to do with the current run/batch. however, if possible I'd like to see reconstruction, filters (currently I am doing fully connected to keep things simple but eventually it would be nice to see a conv net examples), activation units (with any number of units that I choose), etc.",https://stackoverflow.com/questions/38800965,1601580,Inadequate Examples
38810424,How does one debug NaN values in TensorFlow?,"<p>I was running TensorFlow and I happen to have something yielding a NaN. I'd like to know what it is but I do not know how to do this. The main issue is that in a ""normal"" procedural program I would just write a print statement just before the operation is executed. The issue with TensorFlow is that I cannot do that because I first declare (or define) the graph, so adding print statements to the graph definition does not help. Are there any rules, advice, heuristics, anything to track down what might be causing the NaN?</p>

<hr>

<p>In this case I know more precisely what line to look at because I have the following:</p>

<pre><code>Delta_tilde = 2.0*tf.matmul(x,W) - tf.add(WW, XX) #note this quantity should always be positive because its pair-wise euclidian distance
Z = tf.sqrt(Delta_tilde)
Z = Transform(Z) # potentially some transform, currently I have it to return Z for debugging (the identity)
Z = tf.pow(Z, 2.0)
A = tf.exp(Z) 
</code></pre>

<p>when this line is present I have it that it returns NaN as declared by my summary writers. Why is this? Is there a way to at least explore what value Z has after its being square rooted?</p>

<hr>

<p>For the specific example I posted, I tried <code>tf.Print(0,Z)</code> but with no success it printed nothing. As in:</p>

<pre><code>Delta_tilde = 2.0*tf.matmul(x,W) - tf.add(WW, XX) #note this quantity should always be positive because its pair-wise euclidian distance
Z = tf.sqrt(Delta_tilde)
tf.Print(0,[Z]) # &lt;-------- TF PRINT STATMENT
Z = Transform(Z) # potentially some transform, currently I have it to return Z for debugging (the identity)
Z = tf.pow(Z, 2.0)
A = tf.exp(Z) 
</code></pre>

<p>I actually don't understand what <code>tf.Print</code> is suppose to do. Why does it need two arguments? If I want to print 1 tensor why would I need to pass 2? Seems bizarre to me.</p>

<hr>

<p>I was looking at the function <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/control_flow_ops.html#add_check_numerics_ops"" rel=""noreferrer"">tf.add_check_numerics_ops()</a> but it doesn't say how to use it (plus the docs seem to not be super helpful). Does anyone know how to use this?</p>

<hr>

<p>Since I've had comments addressing the data might be bad, I am using standard MNIST. However, I am computing a quantity that is positive (pair-wise eucledian distance) and then square rooting it. Thus, I wouldn't see how the data specifically would be an issue.</p>
","I was running TensorFlow and I happen to have something yielding a NaN. I'd like to know what it is but I do not know how to do this. The main issue is that in a ""normal"" procedural program I would just write a print statement just before the operation is executed. The issue with TensorFlow is that I cannot do that because I first declare (or define) the graph, so adding print statements to the graph definition does not help. Are there any rules, advice, heuristics, anything to track down what might be causing the NaN? In this case I know more precisely what line to look at because I have the following: when this line is present I have it that it returns NaN as declared by my summary writers. Why is this? Is there a way to at least explore what value Z has after its being square rooted? For the specific example I posted, I tried tf.Print(0,Z) but with no success it printed nothing. As in: I actually don't understand what tf.Print is suppose to do. Why does it need two arguments? If I want to print 1 tensor why would I need to pass 2? Seems bizarre to me. I was looking at the function tf.add_check_numerics_ops() but it doesn't say how to use it (plus the docs seem to not be super helpful). Does anyone know how to use this? Since I've had comments addressing the data might be bad, I am using standard MNIST. However, I am computing a quantity that is positive (pair-wise eucledian distance) and then square rooting it. Thus, I wouldn't see how the data specifically would be an issue.",https://stackoverflow.com/questions/38810424,1601580,Documentation Completeness
41604616,Save and load Tensorflow model,"<p>I want to save a Tensorflow (0.12.0) model, including graph and variable values, then later load and execute it. I have the read the docs and other posts on this but cannot get the basics to work. I am using the technique from <a href=""https://www.tensorflow.org/versions/r0.10/how_tos/meta_graph/"" rel=""nofollow noreferrer"">this page in the Tensorflow docs</a>. Code:</p>

<p>Save a simple model:</p>

<pre><code>myVar = tf.Variable(7.1)
tf.add_to_collection('modelVariables', myVar) # why?
init_op = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init_op)
    print sess.run(myVar)
    saver0 = tf.train.Saver()
    saver0.save(sess, './myModel.ckpt')
    saver0.export_meta_graph('./myModel.meta')
</code></pre>

<p>Later, load and execute the model:</p>

<pre><code>with tf.Session() as sess:
    saver1 = tf.train.import_meta_graph('./myModel.meta')
    saver1.restore(sess, './myModel.meta')
    print sess.run(myVar)
</code></pre>

<p><strong>Question 1:</strong>  The saving code seems to work but the loading code produces this error:</p>

<pre><code>W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open ./myModel.meta: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?
</code></pre>

<p>How to fix this?.</p>

<p><strong>Question 2:</strong> I included this line to follow the pattern in the TF docs...</p>

<pre><code>tf.add_to_collection('modelVariables', myVar)
</code></pre>

<p>... but why is that line necessary? Doesn't <code>expert_meta_graph</code>export the entire graph by default? If not then does one need to add every variable in the graph to the collection before saving? Or do we just add to the collection those variables that will be accessed after the restore?</p>

<p>---------------------- <em>Update January 12 2017</em> -----------------------------</p>

<p>Partial success based on Kashyap's suggestion below but a mystery still exists. The code below works <em>but</em> only if I include the lines containing <code>tf.add_to_collection</code> and <code>tf.get_collection</code>. Without those lines, 'load' mode throws an error in the last line:
<code>NameError: name 'myVar' is not defined</code>. My understanding was that by default <code>Saver.save</code> saves and restores all variables in the graph, so why is it necessary to specify the name of variables that will be used in the collection? I assume this has to do with mapping Tensorflow's variable names to Python names, but what are the rules of the game here? For which variables does this need to be done?</p>

<pre><code>mode = 'load' # or 'save'
if mode == 'save':
    myVar = tf.Variable(7.1)
    init_op = tf.global_variables_initializer()
    saver0 = tf.train.Saver()
    tf.add_to_collection('myVar', myVar) ### WHY NECESSARY?
    with tf.Session() as sess:
        sess.run(init_op)
        print sess.run(myVar)
        saver0.save(sess, './myModel')
if mode == 'load':
    with tf.Session() as sess:
        saver1 = tf.train.import_meta_graph('./myModel.meta')
        saver1.restore(sess, tf.train.latest_checkpoint('./'))
        myVar = tf.get_collection('myVar')[0]  ### WHY NECESSARY?
        print sess.run(myVar)
</code></pre>
","I want to save a Tensorflow (0.12.0) model, including graph and variable values, then later load and execute it. I have the read the docs and other posts on this but cannot get the basics to work. I am using the technique from this page in the Tensorflow docs. Code: Save a simple model: Later, load and execute the model: Question 1: The saving code seems to work but the loading code produces this error: How to fix this?. Question 2: I included this line to follow the pattern in the TF docs... ... but why is that line necessary? Doesn't expert_meta_graphexport the entire graph by default? If not then does one need to add every variable in the graph to the collection before saving? Or do we just add to the collection those variables that will be accessed after the restore? ---------------------- Update January 12 2017 ----------------------------- Partial success based on Kashyap's suggestion below but a mystery still exists. The code below works but only if I include the lines containing tf.add_to_collection and tf.get_collection. Without those lines, 'load' mode throws an error in the last line: NameError: name 'myVar' is not defined. My understanding was that by default Saver.save saves and restores all variables in the graph, so why is it necessary to specify the name of variables that will be used in the collection? I assume this has to do with mapping Tensorflow's variable names to Python names, but what are the rules of the game here? For which variables does this need to be done?",https://stackoverflow.com/questions/41604616,3444294,Documentation Replicability
33932901,What's the purpose of tf.app.flags in TensorFlow?,"<p>I am reading some example codes in Tensorflow, I found following code </p>

<pre><code>flags = tf.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')
flags.DEFINE_integer('max_steps', 2000, 'Number of steps to run trainer.')
flags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')
flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')
flags.DEFINE_integer('batch_size', 100, 'Batch size.  '
                 'Must divide evenly into the dataset sizes.')
flags.DEFINE_string('train_dir', 'data', 'Directory to put the training data.')
flags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '
                 'for unit testing.')
</code></pre>

<p>in <code>tensorflow/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py</code></p>

<p>But I can't find any docs about this usage of <code>tf.app.flags</code>. </p>

<p>And I found the implementation of this flags is in the 
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_flags.py""><code>tensorflow/tensorflow/python/platform/default/_flags.py</code></a></p>

<p>Obviously, this <code>tf.app.flags</code> is somehow used to configure a network, so why  is it not in the API docs? Can anyone explain what is going on here? </p>
","I am reading some example codes in Tensorflow, I found following code in tensorflow/tensorflow/g3doc/tutorials/mnist/fully_connected_feed.py But I can't find any docs about this usage of tf.app.flags. And I found the implementation of this flags is in the tensorflow/tensorflow/python/platform/default/_flags.py Obviously, this tf.app.flags is somehow used to configure a network, so why is it not in the API docs? Can anyone explain what is going on here?",https://stackoverflow.com/questions/33932901,5607347,Documentation Completeness
36612247,Is it possible to make tensorflow raise an error on invalid flag?,"<p>I can't find any documentation for tf.app.flags, but I see that the command line parser will happily accept invalid syntax, flags that have never been defined, etc. Is there a way to configure it to raise an error in these cases? I wasted a lot of time trying to figure out why decreasing my learning rate didn't help when the problem was just that I had typed ""-learning_rate"" instead of ""--learning_rate"".</p>
","I can't find any documentation for tf.app.flags, but I see that the command line parser will happily accept invalid syntax, flags that have never been defined, etc. Is there a way to configure it to raise an error in these cases? I wasted a lot of time trying to figure out why decreasing my learning rate didn't help when the problem was just that I had typed ""-learning_rate"" instead of ""--learning_rate"".",https://stackoverflow.com/questions/36612247,378469,Lack of Alternative Solutions/Documentation
56917051,Set flags within python,"<p>I would like to run the <a href=""https://github.com/tensorflow/models/tree/master/research/object_detection"" rel=""nofollow noreferrer"">TensorFlow Object Detection API</a> within a Jupyter Notebook and not from the terminal. In particular, I would like to start the training by calling <code>train.py</code>, which has a <code>main</code> function</p>

<pre class=""lang-py prettyprint-override""><code>def main(_):
   pass
</code></pre>

<p>that gets called when running the file with <code>python train.py &lt;additional flags&gt;</code> because of this part in the file:</p>

<pre class=""lang-py prettyprint-override""><code>if __name__ == '__main__':
    tf.app.run()
</code></pre>

<p>I found out that this invokes the main function to start running after it sets the global variables from the flags passed in the terminal:</p>

<pre class=""lang-py prettyprint-override""><code>flags = tf.app.flags
flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')
...
</code></pre>

<p>Is there a way to set the global variable <code>flags</code> without changing the script <code>train.py</code> and then calling the main function by importing the file. Unfortunately, I couldn't find any documentation on <code>tf.app.flags</code>.</p>
","I would like to run the TensorFlow Object Detection API within a Jupyter Notebook and not from the terminal. In particular, I would like to start the training by calling train.py, which has a main function that gets called when running the file with python train.py &lt;additional flags&gt; because of this part in the file: I found out that this invokes the main function to start running after it sets the global variables from the flags passed in the terminal: Is there a way to set the global variable flags without changing the script train.py and then calling the main function by importing the file. Unfortunately, I couldn't find any documentation on tf.app.flags.",https://stackoverflow.com/questions/56917051,1902610,Lack of Alternative Solutions/Documentation
38510339,the usage or API of tf.app.flags,"<p>When reading the <a href=""https://github.com/tensorflow/tensorflow/blob/e008ecf12607f691952a9bfe8971f406b30bc56e/tensorflow/models/image/cifar10/cifar10_train.py#L129"" rel=""nofollow"">cifar10 example</a>, I can see the following code segment, which is said to follow the google  commandline standard. But in specific, what does this code segment do? I did not find the API document to cover something like <code>tf.app.flags.DEFINE_string</code></p>

<pre><code>FLAGS = tf.app.flags.FLAGS

tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',
                       """"""Directory where to write event logs """"""
                       """"""and checkpoint."""""")
tf.app.flags.DEFINE_integer('max_steps', 1000000,
                        """"""Number of batches to run."""""")
tf.app.flags.DEFINE_boolean('log_device_placement', False,
                        """"""Whether to log device placement."""""")
</code></pre>
","When reading the cifar10 example, I can see the following code segment, which is said to follow the google commandline standard. But in specific, what does this code segment do? I did not find the API document to cover something like tf.app.flags.DEFINE_string",https://stackoverflow.com/questions/38510339,288609,Documentation Completeness
50371333,usage of tf.app.run() and argparse,"<p>I have understood what a parser does, but I do not get its use when it is mingled with <code>tf.app.run()</code> in the following code:  </p>

<pre><code>if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")

    parser.add_argument(""--ps_hosts"",
                        type=str,
                        default="""",
                        help=""Comma-seperated list of hostname:port pairs"")
    parser.add_argument(""--worker_hosts"",
                        type=str,
                        default="""",
                        help=""Comma-seperated list of hostname:port pairs"")
    parser.add_argument(""--job_name"",
                        type=str,
                        default="""",
                        help=""One of 'ps', 'worker'"")
    parser.add_argument(""--task_index"",
                        type=int,
                        default=0,
                        help=""Index of task within the job"")
    parser.add_argument(""--data_dir"",
                        type=str,
                    -   default=""/tmp/mnist_data"",
                        help=""Directory for storing input data"")
    parser.add_argument(""--log_dir"",
                        type=str,
                        default=""/tmp/train_logs"",
                        help=""Directory of train logs"")
    FLAGS, unparsed = parser.parse_known_args()
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
</code></pre>

<p>the main function in the program does not have any arguments as it is defined as <code>def main(_)</code>. So what is the <code>argv</code> argument in <code>tf.app.run()</code> supposed to mean or do?  </p>

<p>Thanks</p>
","I have understood what a parser does, but I do not get its use when it is mingled with tf.app.run() in the following code: the main function in the program does not have any arguments as it is defined as def main(_). So what is the argv argument in tf.app.run() supposed to mean or do? Thanks",https://stackoverflow.com/questions/50371333,7415247,Documentation Completeness
51266268,Exception thrown when running tf.app.run(),"<p>I am toying around with flags at the moment and came across some weird behavior when using <code>tf.app.run()</code>. The following code snippet should simply print the string given via the command line.</p>

<pre><code>import tensorflow as tf

# command line flags
tf.app.flags.DEFINE_string('mystring', 'Hello World!',
                           '''String to print to console.''')

FLAGS = tf.app.flags.FLAGS


def main():

    print(FLAGS.mystring)

if __name__ == '__main__':
    tf.app.run()
</code></pre>

<p>During execution, this error is thrown:</p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File """", line 1, in 
      runfile('/path/flags.py', wdir='/path')</p>
  
  <p>File
  ""/home/abc/anaconda3/envs/tensorflow/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py"",
  line 710, in runfile
      execfile(filename, namespace)</p>
  
  <p>File
  ""/home/abc/anaconda3/envs/tensorflow/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py"",
  line 101, in execfile
      exec(compile(f.read(), filename, 'exec'), namespace)</p>
  
  <p>File ""/path/flags.py"", line 19, in 
      tf.app.run()</p>
  
  <p>File
  ""/home/abc/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/platform/app.py"",
  line 126, in run
      _sys.exit(main(argv))</p>
  
  <p>TypeError: main() takes 0 positional arguments but 1 was given</p>
</blockquote>

<p>...which is strange because I do not give a single argument to main(). However, if I add an underscore <code>def main(_):</code>, it works without any errors.</p>

<p>I couldn't find a doc where this is use of the underscore is described. Does anybody know what happens here? Thank you!</p>
","I am toying around with flags at the moment and came across some weird behavior when using tf.app.run(). The following code snippet should simply print the string given via the command line. During execution, this error is thrown: ...which is strange because I do not give a single argument to main(). However, if I add an underscore def main(_):, it works without any errors. I couldn't find a doc where this is use of the underscore is described. Does anybody know what happens here? Thank you!",https://stackoverflow.com/questions/51266268,8334261,Documentation Completeness
56834934,How to replace tensorflow softmax with max for generating one hot vector at the output layer of Neural Network?,"<p>For a classification problem, softmax function is used in the last layer of the Neural Network.<br>
I want to replace the softmax layer with the max layer that generates one hot vector with one set to the index where maximum value occurred and set all other entries to zero.</p>

<p>I can do it with tf.argmax as suggested in <a href=""https://stackoverflow.com/questions/44724948/tensorflow-dense-vector-to-one-hot"">TensorFlow - dense vector to one-hot</a> and <a href=""https://stackoverflow.com/questions/46841116/tensorflow-convert-output-tensor-to-one-hot"">Tensorflow: Convert output tensor to one-hot</a>, but these are not a differentiable way of doing it and gradients cannot be calculated.</p>

<p>If not exact 0's and 1's can be obtained then values should be close enough.  </p>

<p>I was thinking to apply softmax multiple times but it is not recommended and I do not understand the reason behind it.  </p>

<p>Please suggest a differentiable solution.</p>
","For a classification problem, softmax function is used in the last layer of the Neural Network. I want to replace the softmax layer with the max layer that generates one hot vector with one set to the index where maximum value occurred and set all other entries to zero. I can do it with tf.argmax as suggested in TensorFlow - dense vector to one-hot and Tensorflow: Convert output tensor to one-hot, but these are not a differentiable way of doing it and gradients cannot be calculated. If not exact 0's and 1's can be obtained then values should be close enough. I was thinking to apply softmax multiple times but it is not recommended and I do not understand the reason behind it. Please suggest a differentiable solution.",https://stackoverflow.com/questions/56834934,7930290,Requesting (Additional) Resources
44191070,"Tensorflow, update the Variable to have arbitrary shape","<p>So, according to the <a href=""https://www.tensorflow.org/api_docs/python/tf/Variable"" rel=""nofollow noreferrer"">documentation</a>, we can use tf.assign with validate_shape=False to change the shape. It does change the shape of the content of the variable, but the shape you can get from get_shape() doesn't get updated. For example:</p>

<pre><code>&gt;&gt;&gt; a = tf.Variable([1, 1, 1, 1])
&gt;&gt;&gt; sess.run(tf.global_variables_initializer())
&gt;&gt;&gt; tf.assign(a, [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]], validate_shape=False).eval()
array([[1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1]], dtype=int32)
&gt;&gt;&gt; a.get_shape()
TensorShape([Dimension(4)])
</code></pre>

<p>It's pretty annoying that the later layers of the network base their shapes on the get_shape() value of this variable. So, even though the actual shape is correct, Tensorflow will complain the dimensions doesn't match. So any ideas on how to update the ""believed"" shape of each Variable?</p>
","So, according to the documentation, we can use tf.assign with validate_shape=False to change the shape. It does change the shape of the content of the variable, but the shape you can get from get_shape() doesn't get updated. For example: It's pretty annoying that the later layers of the network base their shapes on the get_shape() value of this variable. So, even though the actual shape is correct, Tensorflow will complain the dimensions doesn't match. So any ideas on how to update the ""believed"" shape of each Variable?",https://stackoverflow.com/questions/44191070,4434038,Requesting (Additional) Resources
44335033,Does 'tf.assign' return its argument?,"<p>The Tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/assign"" rel=""nofollow noreferrer"">documentation</a> says that <code>tf.assign(ref, ...)</code> returns <code>ref</code>, but it appears instead (not surprisingly) to return a <code>Tensor</code> (attached to the <code>assign</code> op):</p>

<pre><code>import tensorflow as tf
sess = tf.InteractiveSession()

Q = tf.Variable(tf.constant(range(1, 12)))
sess.run(tf.global_variables_initializer())
qop = tf.assign(Q, tf.zeros(Q.shape, tf.int32))#.eval()

print(Q.eval())
print(qop.eval())
print(Q.eval())
</code></pre>

<p>produces</p>

<pre><code>[ 1  2  3  4  5  6  7  8  9 10 11]
[0 0 0 0 0 0 0 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0]
</code></pre>

<p>demonstrating that the argument <code>Q</code> and what's returned <code>qop</code> behave differently (and that <code>Q</code> is unchanged until <code>qop</code> is executed).</p>

<p>Is the return value of <code>tf.assign</code> described correctly in the documentation?</p>
","The Tensorflow documentation says that tf.assign(ref, ...) returns ref, but it appears instead (not surprisingly) to return a Tensor (attached to the assign op): produces demonstrating that the argument Q and what's returned qop behave differently (and that Q is unchanged until qop is executed). Is the return value of tf.assign described correctly in the documentation?",https://stackoverflow.com/questions/44335033,656912,Documentation Ambiguity
45134654,Easily switching between feed_dict and queues for input to TensorFlow model,"<p>Right now I have a model configured to take its inputs with <code>feed_dict</code>. The code looks something like this:</p>

<pre><code># model.py
class MyModel(object):
  def __init__(self, hyperparams):
    self.build_model(hyperparams)

  def build_model(self, hps):
    self.input_data = tf.placeholder(dtype=tf.float32, shape=[hps.batch_size, hps.nfeats])
    self.labels = tf.placeholder(dtype=tf.float32, shape=[hps.batch_size])
    # Define hidden layers, loss, training step, etc.

# train.py
model = MyModel(hps)
for _ in range(100):
  x, y = some_python_function() # Read a batch from disk, preprocess
  sess.run(model.train_step, feed_dict={model.input_data: x, model.labels: y})
</code></pre>

<p>For performance reasons, I'd like to switch to using queues for training. But I'd like to maintain the ability to use <code>feed_dict</code>, e.g. for inference or testing.</p>

<p>Is there an elegant way to do this? What I'd like to do is, when using queues, 'swap out' the placeholder variables for the tensors returned by my queue's dequeue op. I thought that <code>tf.assign</code> would be the way to do this, i.e.:</p>

<pre><code>single_x, single_y = tf.parse_single_example(...)
x, y = tf.train.batch([single_x, single_y], batch_size)
model = MyModel(hps)
sess.run([tf.assign(model.input_data, x), tf.assign(model.labels, y)])
for _ in range(100):
  sess.run(model.train_step)
</code></pre>

<p>But this raises <code>AttributeError: 'Tensor' object has no attribute 'assign'</code>. The API docs for <a href=""https://www.tensorflow.org/api_docs/python/tf/assign"" rel=""nofollow noreferrer""><code>tf.assign</code></a> describe the first argument as: ""A mutable <code>Tensor</code>. Should be from a <code>Variable</code> node. May be uninitialized."" Does this mean my placeholders aren't mutable? Can I make them so? Or am I approaching this the wrong way?</p>

<p>Minimal runnable example <a href=""https://gist.github.com/colinmorris/8ca883c04831c755ee3bc745cab52761"" rel=""nofollow noreferrer"">here</a>.</p>
","Right now I have a model configured to take its inputs with feed_dict. The code looks something like this: For performance reasons, I'd like to switch to using queues for training. But I'd like to maintain the ability to use feed_dict, e.g. for inference or testing. Is there an elegant way to do this? What I'd like to do is, when using queues, 'swap out' the placeholder variables for the tensors returned by my queue's dequeue op. I thought that tf.assign would be the way to do this, i.e.: But this raises AttributeError: 'Tensor' object has no attribute 'assign'. The API docs for tf.assign describe the first argument as: ""A mutable Tensor. Should be from a Variable node. May be uninitialized."" Does this mean my placeholders aren't mutable? Can I make them so? Or am I approaching this the wrong way? Minimal runnable example here.",https://stackoverflow.com/questions/45134654,262271,Lack of Alternative Solutions/Documentation
49770934,Prevent a Variable from being converted into a Const when freezing a TensorFlow graph,"<p>I'm trying to use the freeze_graph.py tool to save a model, but have run into an issue.</p>

<p>There is a variable in my tensorflow graph which I either assign to using tf.assign, or feed before each inference. I need it to remain a variable because tf.assign requires a mutable tensor and you also can't feed to a const, but the freeze_graph script converts all variables into constants.</p>

<p>I've noticed that freeze_graph has whitelist and blacklist parameters, but I can't for the life of me find any documentation on what these are or how to use them. What can I do here?</p>

<p><em>edit:</em></p>

<p><code>single_c</code> and <code>single_h</code> are the variables I'd like to preserve:</p>

<pre><code>single_c = tf.Variable(tf.random_uniform([num_lstm_cells], 0, 1), trainable=True)
expanded_c = tf.reshape(single_c, [1, num_lstm_cells])
batched_c = tf.tile(expanded_c, tiling_shape, name='c')

single_h = tf.Variable(tf.random_uniform([num_lstm_cells], 0, 1), trainable=True)
expanded_h = tf.reshape(single_h, [1, num_lstm_cells])
batched_h = tf.tile(expanded_h, tiling_shape, name='h')

state = tf.contrib.rnn.LSTMStateTuple(batched_c, batched_h)
</code></pre>

<p>because I assign them using:</p>

<pre><code>restore_c = tf.assign(single_c, c_holder)
restore_h = tf.assign(single_h, h_holder)
</code></pre>

<p>and I feed <code>state</code> using</p>

<pre><code>_, er, new = sess.run([train_nn_step, error, new_state], feed_dict={batch_ph: batch_size, prob: training_dropout, state: new})
</code></pre>

<p>I can't assign or feed if <code>single_c</code> and <code>single_h</code> become constants</p>
","I'm trying to use the freeze_graph.py tool to save a model, but have run into an issue. There is a variable in my tensorflow graph which I either assign to using tf.assign, or feed before each inference. I need it to remain a variable because tf.assign requires a mutable tensor and you also can't feed to a const, but the freeze_graph script converts all variables into constants. I've noticed that freeze_graph has whitelist and blacklist parameters, but I can't for the life of me find any documentation on what these are or how to use them. What can I do here? edit: single_c and single_h are the variables I'd like to preserve: because I assign them using: and I feed state using I can't assign or feed if single_c and single_h become constants",https://stackoverflow.com/questions/49770934,6936275,Requesting (Additional) Resources
55688621,How to apply mask to a tensor and keep its original shape,"<p>I have two tensors: one containing data and the other mask of boolean values. I would like to set all values in data tensor to zero, if boolean values are False, while keeping the original shape of data tensor. 
So far I can achieve it only while mask is a numpy array. </p>

<p>Since <a href=""https://www.tensorflow.org/api_docs/python/tf/boolean_mask"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/boolean_mask</a> influences shape of the tensor, I cannot use it.</p>

<p>How to do that?</p>

<pre><code>import numpy as np
import tensorflow as tf
tf.enable_eager_execution()

# create dummy data
data_np = np.ones((4,2,3))
mask_np = np.array([[True, True],[False, True],[True, True],[False, False]])

# prepare tensors
data = tf.convert_to_tensor(data_np)
mask = tf.convert_to_tensor(mask_np)

# how to perform the same while avoiding numpy?
mask = np.expand_dims(mask, -1)
data *= mask
</code></pre>
","I have two tensors: one containing data and the other mask of boolean values. I would like to set all values in data tensor to zero, if boolean values are False, while keeping the original shape of data tensor. So far I can achieve it only while mask is a numpy array. Since https://www.tensorflow.org/api_docs/python/tf/boolean_mask influences shape of the tensor, I cannot use it. How to do that?",https://stackoverflow.com/questions/55688621,1435046,Requesting (Additional) Resources
44563648,How to effectively use tf.bucket_by_sequence_length in Tensorflow?,"<p>So I'm trying to use tf.bucket_by_sequence_length() from Tensorflow, but can not quite figure out how to make it work.</p>

<p>Basically, it should take sequences (of different lengths) as input and have buckets of sequences as output, but it does not seem to work this way.</p>

<p>From this discussion: 
<a href=""https://github.com/tensorflow/tensorflow/issues/5609"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/5609</a>
I have the impression that it needs a queue in order to feed this function, sequence by sequence. It's not clear though.</p>

<p>Function's documentation can be found here: <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.training/bucketing#bucket_by_sequence_length"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.training/bucketing#bucket_by_sequence_length</a></p>
","So I'm trying to use tf.bucket_by_sequence_length() from Tensorflow, but can not quite figure out how to make it work. Basically, it should take sequences (of different lengths) as input and have buckets of sequences as output, but it does not seem to work this way. From this discussion: https://github.com/tensorflow/tensorflow/issues/5609 I have the impression that it needs a queue in order to feed this function, sequence by sequence. It's not clear though. Function's documentation can be found here: https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.training/bucketing#bucket_by_sequence_length",https://stackoverflow.com/questions/44563648,8165188,Inadequate Examples
58525373,TensorFlow 2.0 clip_by_value change parameter,"<p>Hello I have an model that uses the <code>tf.clip_by_value</code> method: <a href=""http://tensorflow.biotecan.com/python/Python_1.8/tensorflow.google.cn/api_docs/python/tf/clip_by_value.html"" rel=""nofollow noreferrer"">Documentation tf.clip_by_value</a></p>

<p>After training I would like to change the clipping parameters. But this seems to be not possible:</p>

<pre><code>Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
tf_op_layer_clip_by_value_4/Min [(None, 1)]          0           x[0][0]                     
__________________________________________________________________________________________________
tf_op_layer_clip_by_value_5/Min [(None, 1)]          0           y[0][0]                     
__________________________________________________________________________________________________
tf_op_layer_clip_by_value_4 (Te [(None, 1)]          0           tf_op_layer_clip_by_value_4/Minim
__________________________________________________________________________________________________
tf_op_layer_clip_by_value_5 (Te [(None, 1)]          0           tf_op_layer_clip_by_value_5/Minim
</code></pre>

<p>Is there a way to access and change the parameter of the clipping layer within a model?</p>
",Hello I have an model that uses the tf.clip_by_value method: Documentation tf.clip_by_value After training I would like to change the clipping parameters. But this seems to be not possible: Is there a way to access and change the parameter of the clipping layer within a model?,https://stackoverflow.com/questions/58525373,12214381,Requesting (Additional) Resources
56413873,How BatchToSpaceND actually works?,"<p>I'm trying to figure out how <a href=""https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd"" rel=""nofollow noreferrer"">BatchToSpaceND</a> permutes the input matrix. One of the examples is the following:</p>

<blockquote>
  <p>(3) For the following input of shape [4, 2, 2, 1] and block_size of 2:</p>

<pre><code>x = [[[[1], [3]], [[9], [11]]],
     [[[2], [4]], [[10], [12]]],
     [[[5], [7]], [[13], [15]]],
     [[[6], [8]], [[14], [16]]]]
</code></pre>
  
  <p>The output tensor has shape [1, 4, 4, 1] and value:</p>

<pre><code>x = [[[1],   [2],  [3],  [4]],
     [[5],   [6],  [7],  [8]],
     [[9],  [10], [11],  [12]],
     [[13], [14], [15],  [16]]]
</code></pre>
</blockquote>

<p>Anyone know how the output tensor is derived? How come the first row is <code>[[1], [2], [3], [4]]</code> and not <code>[[1], [3], [9], [11]]</code> instead? I've also tried some code:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
sess = tf.InteractiveSession()

a = [[[[1], [3]], [[9], [11]]],
     [[[2], [4]], [[10], [12]]],
     [[[5], [7]], [[13], [15]]],
     [[[6], [8]], [[14], [16]]]]
b = [2, 2, 1, 2, 2, 1]
a = tf.reshape(a, b)

b = [1, 2, 2, 2, 2, 1]
a = tf.reshape(a, b)

b = [1, 4, 4, 1]
a = tf.reshape(a, b)

print(a.eval())

[[[[ 1]
   [ 3]
   [ 9]
   [11]]

  [[ 2]
   [ 4]
   [10]
   [12]]

  [[ 5]
   [ 7]
   [13]
   [15]]

  [[ 6]
   [ 8]
   [14]
   [16]]]]
</code></pre>

<p>which isn't quite the result in the doc.</p>
","I'm trying to figure out how BatchToSpaceND permutes the input matrix. One of the examples is the following: Anyone know how the output tensor is derived? How come the first row is [[1], [2], [3], [4]] and not [[1], [3], [9], [11]] instead? I've also tried some code: which isn't quite the result in the doc.",https://stackoverflow.com/questions/56413873,233798,Documentation Ambiguity
48495699,How to understand the effect of local_step in tf.ConditionalAccumulator(),"<p>I want to implement the function which conducts <code>backward()</code> after multiple <code>forward()</code> operations in order to increase the actual <code>batch_size</code> with limited GPU memory. So I came to <code>tf.ConditionalAccumulator</code>.</p>

<p>In the arguments of <a href=""https://www.tensorflow.org/api_docs/python/tf/ConditionalAccumulator#apply_grad"" rel=""nofollow noreferrer""><code>tf.ConditionalAccumulator().apply_grad()</code></a>, there is an argument <code>local_step</code> which I do not understand how to appoint. The document explains as follow:</p>

<blockquote>
  <p>Attempts to apply a gradient to the accumulator.</p>
  
  <p>The attempt is silently dropped if the gradient is stale, i.e., local_step is less than the accumulator's global time step.</p>
  
  <p>Args:</p>
  
  <p><code>grad</code>: The gradient tensor to be applied.</p>
  
  <p><code>local_step</code>: Time step at which the gradient was computed.</p>
  
  <p><code>name</code>: Optional name for the operation.</p>
</blockquote>

<p>I tried to search the implementation of <code>tf.CondionalAccumentor().apply_grad()</code>, but didn't find the member variable refers to <code>global_time_step</code>. In my understanding, there should be ten gradient slots, if we want to accumulate 10 times before one gradient update. The <code>global_time_step</code> is applied as an indicator to point out which slot should be used. if the <code>local_step</code> is less than the <code>global_time_step</code>, which means the corresponding slot has been used, so the gradient is stale and should be discarded.</p>

<p>In my implementation, I assign it with <code>global_step</code> variable, which is used to record the number of gradient update in training procedure, and it increases one in every <code>batch_size</code> iterations, thus it increases one after <code>batch_size</code> examples forward. I am not sure about the correctness of my implementation.</p>

<p>I hope someone can help to explain the mechanism of <code>tf.ConditionalAccumulator</code>. </p>
","I want to implement the function which conducts backward() after multiple forward() operations in order to increase the actual batch_size with limited GPU memory. So I came to tf.ConditionalAccumulator. In the arguments of tf.ConditionalAccumulator().apply_grad(), there is an argument local_step which I do not understand how to appoint. The document explains as follow: I tried to search the implementation of tf.CondionalAccumentor().apply_grad(), but didn't find the member variable refers to global_time_step. In my understanding, there should be ten gradient slots, if we want to accumulate 10 times before one gradient update. The global_time_step is applied as an indicator to point out which slot should be used. if the local_step is less than the global_time_step, which means the corresponding slot has been used, so the gradient is stale and should be discarded. In my implementation, I assign it with global_step variable, which is used to record the number of gradient update in training procedure, and it increases one in every batch_size iterations, thus it increases one after batch_size examples forward. I am not sure about the correctness of my implementation. I hope someone can help to explain the mechanism of tf.ConditionalAccumulator.",https://stackoverflow.com/questions/48495699,4202137,Documentation Ambiguity
53124755,How to prevent Tensorflow from allocating the totality of a GPU memory when using eager execution?,"<p>I have pretty much the same question that has already been answered <a href=""https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory"">here</a>, with a slight difference though:</p>

<p>I'm working on a server with a few GPUs that I share with my colleagues for training our deep learning models. The server should also run a small web application that samples from our models. The sampling script uses the relatively new <a href=""https://www.tensorflow.org/guide/eager"" rel=""nofollow noreferrer"">eager execution</a>. In theory it allows me to stop Tensorflow from allocating all the GPU memory by providing a configuration like this:</p>

<pre><code>config = tf.ConfigProto()
config.gpu_options.allow_growth = True
tf.enable_eager_execution(config=config)
</code></pre>

<p>In practice this does not work though. The documentation of the eager execution also states that not all the configuration options that work for sessions will work in eager execution (<a href=""https://www.tensorflow.org/api_docs/python/tf/enable_eager_execution#args"" rel=""nofollow noreferrer"">here</a>). But how can I limit the used memory then?</p>

<p>I know that I can limit the visible devices like this:</p>

<pre><code>os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""
os.environ[""CUDA_VISIBLE_DEVICES""] = ""1""
</code></pre>

<p>But I don't want to constantly block an entire GPU for a task that gets called very occasionally and actually needs way less memory.</p>
","I have pretty much the same question that has already been answered here, with a slight difference though: I'm working on a server with a few GPUs that I share with my colleagues for training our deep learning models. The server should also run a small web application that samples from our models. The sampling script uses the relatively new eager execution. In theory it allows me to stop Tensorflow from allocating all the GPU memory by providing a configuration like this: In practice this does not work though. The documentation of the eager execution also states that not all the configuration options that work for sessions will work in eager execution (here). But how can I limit the used memory then? I know that I can limit the visible devices like this: But I don't want to constantly block an entire GPU for a task that gets called very occasionally and actually needs way less memory.",https://stackoverflow.com/questions/53124755,4528518,Requesting (Additional) Resources
39657063,Is the collection in tensorflow.get_collection() cleared?,"<p>I'm learning about neural nets using Tensorflow through the Stanford course. I found this while implementing a RNN and did not quite understand why the losses are accumulated:</p>

<pre class=""lang-python prettyprint-override""><code># This adds a loss operation to the Graph for batch training
def add_loss_op(self, output):
    all_ones = [tf.ones([self.config.batch_size * self.config.num_steps])]
    cross_entropy = sequence_loss(
        [output], [tf.reshape(self.labels_placeholder, [-1])], all_ones, len(self.vocab))
    tf.add_to_collection('total_loss', cross_entropy)
    # Doesn't this increase in size every batch of training?
    loss = tf.add_n(tf.get_collection('total_loss')) 
    return loss
</code></pre>

<p>The documentation for <code>get_collection()</code>  <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html#Graph.get_collection"" rel=""nofollow"">here</a> doesn't mention anything about clearing the variables. Since this is run for every training step, are the losses incremented every epoch / minibatch of training and carried over? </p>

<p>I am still new to NNs so do correct any misunderstanding I have on this!</p>
","I'm learning about neural nets using Tensorflow through the Stanford course. I found this while implementing a RNN and did not quite understand why the losses are accumulated: The documentation for get_collection() here doesn't mention anything about clearing the variables. Since this is run for every training step, are the losses incremented every epoch / minibatch of training and carried over? I am still new to NNs so do correct any misunderstanding I have on this!",https://stackoverflow.com/questions/39657063,4207476,Documentation Completeness
47585864,What's the difference between get_collection and get_collection_ref?,"<p>I have checked the documentations of both methods but they look the same, except that get_collection can take an additional scope parameter. </p>



<pre class=""lang-python prettyprint-override""><code>In [11]: aaa = tf.get_collection_ref(tf.GraphKeys.UPDATE_OPS)
In [12]: aaaa = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
In [13]: aaa == aaaa
Out[13]: True
In [14]: aaa is aaaa
Out[14]: False
</code></pre>

<p>What's the difference between the two and when to use which one?</p>
","I have checked the documentations of both methods but they look the same, except that get_collection can take an additional scope parameter. What's the difference between the two and when to use which one?",https://stackoverflow.com/questions/47585864,3552975,Documentation Ambiguity
36227354,Saving tensorflow model after training is finished,"<p>I have finished running a big model in tensorflow python. But I have not saved it inside the session. Now that the training is over, I want to save the variables. I am doing the following: </p>

<pre><code>saver=tf.train.Saver()
with tf.Session(graph=graph) as sess:  
    save_path = saver.save(sess, ""86_model.ckpt"")
    print(""Model saved in file: %s"" % save_path)
</code></pre>

<p>This returns : ValueError: No variables to save. According to their website what is missing is initialize_all_variables(). The documentation says little about what exactly that does. The word ""initialize"" scares me, I do not want to reset all my trained values. Any way to save my model without re-running it? </p>
","I have finished running a big model in tensorflow python. But I have not saved it inside the session. Now that the training is over, I want to save the variables. I am doing the following: This returns : ValueError: No variables to save. According to their website what is missing is initialize_all_variables(). The documentation says little about what exactly that does. The word ""initialize"" scares me, I do not want to reset all my trained values. Any way to save my model without re-running it?",https://stackoverflow.com/questions/36227354,3761534,Documentation Ambiguity
56490000,Read values of tensor in tensorflow,"<p>I'm working with Python 3.6 in PyCharm. </p>

<p>In the file <code>site-packages/tensorflow/python/ops/nn_ops.py</code>, </p>

<p>I find after line 838</p>

<pre class=""lang-py prettyprint-override""><code>    with ops.name_scope(name, ""convolution"", [input, filter]) as name:
        input = ops.convert_to_tensor(input, name=""input"")  
        input_shape = input.get_shape()
        filter = ops.convert_to_tensor(filter, name=""filter"")  
        filter_shape = filter.get_shape()
        op = Convolution(
            input_shape,
            filter_shape,
            padding,
            strides=strides,
            dilation_rate=dilation_rate,
            name=name,
            data_format=data_format)
        return op(input,filter)
</code></pre>

<p>I want to know the values of input, filter and the returned tensor. </p>

<p>I tried, according to <a href=""https://www.tensorflow.org/api_docs/python/tf/InteractiveSession"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/InteractiveSession</a> to do</p>

<pre class=""lang-py prettyprint-override""><code>    with ops.name_scope(name, ""convolution"", [input, filter]) as name:
        input = ops.convert_to_tensor(input, name=""input"") 
        input_shape = input.get_shape()
        filter = ops.convert_to_tensor(filter, name=""filter"")  
        filter_shape = filter.get_shape()
        op = Convolution(
            input_shape,
            filter_shape,
            padding,
            strides=strides,
            dilation_rate=dilation_rate,
            name=name,
            data_format=data_format)
        temp = op(input,filter)
        import tensorflow as tf
        sess = tf.Session()
        with sess.as_default():
            assert tf.get_default_session() is sess
            test = filter.eval()
        return temp
</code></pre>

<p>Then, I got the error:</p>

<pre><code>    tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value conv2d_1/kernel
     [[{{node conv2d_1/kernel/read}}]]
</code></pre>

<p>What am I doing wrong?</p>
","I'm working with Python 3.6 in PyCharm. In the file site-packages/tensorflow/python/ops/nn_ops.py, I find after line 838 I want to know the values of input, filter and the returned tensor. I tried, according to https://www.tensorflow.org/api_docs/python/tf/InteractiveSession to do Then, I got the error: What am I doing wrong?",https://stackoverflow.com/questions/56490000,8516342,Documentation Replication on Other Examples
46920414,type mismatch using sparse_precision_at_k from tensorflow.metrics,"<p>I am working with a toy example to check how <code>tensorflow.metrics.sparse_precision_at_k</code> works</p>

<p>From the documentation:</p>

<blockquote>
  <p>labels: <code>int64</code> <code>Tensor</code> or <code>SparseTensor</code> with shape
      [D1, ... DN, num_labels] or [D1, ... DN], where the latter implies
      num_labels=1. N >= 1 and num_labels is the number of target classes for
      the associated prediction. Commonly, N=1 and <code>labels</code> has shape
      [batch_size, num_labels]. [D1, ... DN] must match <code>predictions</code>. Values
      should be in range [0, num_classes), where num_classes is the last
      dimension of <code>predictions</code>. Values outside this range are ignored.</p>
  
  <p>predictions: Float <code>Tensor</code> with shape [D1, ... DN, num_classes] where
      N >= 1. Commonly, N=1 and predictions has shape [batch size, num_classes].
      The final dimension contains the logit values for each class. [D1, ... DN]
      must match <code>labels</code>.</p>
  
  <p>k: Integer, k for @k metric.</p>
</blockquote>

<p>So I have written a following example accordingly: </p>

<pre><code>import tensorflow as tf
import numpy as np

pred = np.asarray([[.8,.1,.1,.1],[.2,.9,.9,.9]]).T
print(pred.shape)

segm = [0,1,1,1]
segm = np.asarray(segm, np.float32)
print(segm.shape)

segm_tf = tf.Variable(segm, np.int64)
pred_tf = tf.Variable(pred, np.float32)

print(""segm_tf"", segm_tf.shape)
print(""pred_tf"", pred_tf.shape)

prec,_ = tf.metrics.sparse_precision_at_k(segm_tf, pred_tf, 1, class_id=1)
sess = tf.InteractiveSession()
tf.variables_initializer([prec, segm_tf, pred_tf])
</code></pre>

<p>However, I am getting an error:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-7-c6243802dedc&gt; in &lt;module&gt;()
     25 print(""pred_tf"", pred_tf.shape)
     26 
---&gt; 27 prec,_ = tf.metrics.sparse_precision_at_k(segm_tf, pred_tf, 1, class_id=1)
     28 sess = tf.InteractiveSession()
     29 tf.variables_initializer([prec, segm_tf, pred_tf])

/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in sparse_precision_at_k(labels, predictions, k, class_id, weights, metrics_collections, updates_collections, name)
   2828         metrics_collections=metrics_collections,
   2829         updates_collections=updates_collections,
-&gt; 2830         name=scope)
   2831 
   2832 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in _sparse_precision_at_top_k(labels, predictions_idx, k, class_id, weights, metrics_collections, updates_collections, name)
   2726     tp, tp_update = _streaming_sparse_true_positive_at_k(
   2727         predictions_idx=top_k_idx, labels=labels, k=k, class_id=class_id,
-&gt; 2728         weights=weights)
   2729     fp, fp_update = _streaming_sparse_false_positive_at_k(
   2730         predictions_idx=top_k_idx, labels=labels, k=k, class_id=class_id,

/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in _streaming_sparse_true_positive_at_k(labels, predictions_idx, k, class_id, weights, name)
   1743     tp = _sparse_true_positive_at_k(
   1744         predictions_idx=predictions_idx, labels=labels, class_id=class_id,
-&gt; 1745         weights=weights)
   1746     batch_total_tp = math_ops.to_double(math_ops.reduce_sum(tp))
   1747 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in _sparse_true_positive_at_k(labels, predictions_idx, class_id, weights, name)
   1689       name, 'true_positives', (predictions_idx, labels, weights)):
   1690     labels, predictions_idx = _maybe_select_class_id(
-&gt; 1691         labels, predictions_idx, class_id)
   1692     tp = sets.set_size(sets.set_intersection(predictions_idx, labels))
   1693     tp = math_ops.to_double(tp)

/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in _maybe_select_class_id(labels, predictions_idx, selected_id)
   1651   if selected_id is None:
   1652     return labels, predictions_idx
-&gt; 1653   return (_select_class_id(labels, selected_id),
   1654           _select_class_id(predictions_idx, selected_id))
   1655 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py in _select_class_id(ids, selected_id)
   1627   filled_selected_id = array_ops.fill(
   1628       filled_selected_id_shape, math_ops.to_int64(selected_id))
-&gt; 1629   result = sets.set_intersection(filled_selected_id, ids)
   1630   return sparse_tensor.SparseTensor(
   1631       indices=result.indices, values=result.values, dense_shape=ids_shape)

/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/sets_impl.py in set_intersection(a, b, validate_indices)
    191     intersections.
    192   """"""
--&gt; 193   a, b, _ = _convert_to_tensors_or_sparse_tensors(a, b)
    194   return _set_operation(a, b, ""intersection"", validate_indices)
    195 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/sets_impl.py in _convert_to_tensors_or_sparse_tensors(a, b)
     82   b = sparse_tensor.convert_to_tensor_or_sparse_tensor(b, name=""b"")
     83   if b.dtype.base_dtype != a.dtype.base_dtype:
---&gt; 84     raise TypeError(""Types don't match, %s vs %s."" % (a.dtype, b.dtype))
     85   if (isinstance(a, sparse_tensor.SparseTensor) and
     86       not isinstance(b, sparse_tensor.SparseTensor)):

TypeError: Types don't match, &lt;dtype: 'int64'&gt; vs &lt;dtype: 'float32'&gt;.
</code></pre>
","I am working with a toy example to check how tensorflow.metrics.sparse_precision_at_k works From the documentation: So I have written a following example accordingly: However, I am getting an error:",https://stackoverflow.com/questions/46920414,1716733,Requesting (Additional) Resources
44464055,How the cell state size and cell output size is determined in BasicRNNCell?,"<p>Consider the following code:</p>

<pre><code>import tensorflow as tf
cell=tf.contrib.rnn.BasicRNNCell(num_units = rnn_size)
output, state = tf.nn.dynamic_rnn(cell, input, dtype=tf.float32) 
</code></pre>

<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">documentation of dynamic_rnn</a>, the <code>output</code> and <code>state</code> have shapes <code>[batch_size, max_time, cell.output_size]</code> and <code>[batch_size, cell.state_size]</code>, respectively. </p>

<p>The question: how the <code>cell.state_size</code> and <code>cell.output_size</code> is determined in <code>BasicRNNCell</code>? What is the relationship between <code>num_units = rnn_size</code> in the initilizer of BasicRNNCell and its <code>state_size</code> and <code>output_size</code>?</p>
","Consider the following code: According to the documentation of dynamic_rnn, the output and state have shapes [batch_size, max_time, cell.output_size] and [batch_size, cell.state_size], respectively. The question: how the cell.state_size and cell.output_size is determined in BasicRNNCell? What is the relationship between num_units = rnn_size in the initilizer of BasicRNNCell and its state_size and output_size?",https://stackoverflow.com/questions/44464055,5617507,Documentation Ambiguity
45523380,why the tf.nn.dynamic_rnn returns different result regarding sequence_length,"<p>while i studied RNN in the book called Hands-On Machine Learning with Scikit-Learn and TensorFlow, i encountered different result which must be the same.</p>
<hr />
<h1>first code</h1>
<pre><code>n_steps = 2
n_inputs = 3
n_neurons = 5

reset_graph()

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
basic_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)

seq_length = tf.placeholder(tf.int32, [None])
outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)


init = tf.global_variables_initializer()

X_batch = np.array([
        # step 0     step 1
        [[0, 1, 2], [9, 8, 7]], # instance 1
        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)
        [[6, 7, 8], [6, 5, 4]], # instance 3
        [[9, 0, 1], [3, 2, 1]], # instance 4
    ])

with tf.Session() as sess:
    init.run()
    outputs_val, states_val = sess.run(
        [outputs, states], feed_dict={X: X_batch})
</code></pre>
<h1>result</h1>
<pre><code>[[[ 0.10273618  0.03536123  0.14367972  0.1572928   0.23754682]
  [ 0.41665766  0.49650002  0.1549654   0.07568012  0.82703578]]

 [[ 0.35701871  0.20796996  0.13533755  0.21938165  0.64902753]
  [ 0.15402019  0.14915846  0.31022152  0.13305351  0.36220491]]

 [[ 0.48224443  0.24930702  0.07341093  0.18052572  0.72496963]
  [ 0.3561081   0.55856758  0.31825539  0.13380432  0.90042865]]

 [[ 0.02311822 -0.16510175  0.49798414 -0.06049323  0.23668778]
  [ 0.28713134  0.16252561  0.4774358   0.07630309  0.50222367]]]
</code></pre>
<hr />
<h1>second code</h1>
<pre><code>    n_steps = 2
    n_inputs = 3
    n_neurons = 5

    reset_graph()
    
    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
    basic_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)
    
    seq_length = tf.placeholder(tf.int32, [None])
    outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,
    sequence_length=seq_length)
    
    init = tf.global_variables_initializer()
    
    X_batch = np.array([
            # step 0     step 1
            [[0, 1, 2], [9, 8, 7]], # instance 1
            [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)
            [[6, 7, 8], [6, 5, 4]], # instance 3
            [[9, 0, 1], [3, 2, 1]], # instance 4
        ])
    seq_length_batch = np.array([2, 1, 2, 2])


    with tf.Session() as sess:
        init.run()
        outputs_val, states_val = sess.run(
            [outputs, states], feed_dict={X: X_batch,seq_length:seq_length_batch})
</code></pre>
<h1>result</h1>
<pre><code>    [[[-0.05313011 -0.03707792 -0.00771733  0.25379574  0.06289639]
      [ 0.34355608 -0.00646485 -0.00426668  0.35139424  0.02420545]]
    
     [[ 0.07838845 -0.04377012 -0.00758527  0.44615552  0.01038414]
      [ 0.          0.          0.          0.          0.        ]]
    
     [[ 0.16005152 -0.01226684 -0.0048396   0.48419252  0.00086438]
      [ 0.4990865  -0.03458051  0.01733598  0.35500884  0.02000519]]
    
     [[ 0.73743606 -0.00149451 -0.102979   -0.39292669  0.50247419]
      [ 0.6204772  -0.04163819 -0.4165332  -0.14101879  0.34553975]]]
</code></pre>
<hr />
<p>Tensorflow document says 'If sequence_length not provided, all batch entries are assumed to be full sequences; and time reversal is applied from time 0 to max_time for each sequence. '</p>
<p>So must the result be the same?</p>
","while i studied RNN in the book called Hands-On Machine Learning with Scikit-Learn and TensorFlow, i encountered different result which must be the same. Tensorflow document says 'If sequence_length not provided, all batch entries are assumed to be full sequences; and time reversal is applied from time 0 to max_time for each sequence. ' So must the result be the same?",https://stackoverflow.com/questions/45523380,4571281,Requesting (Additional) Resources
49012907,How to initialize intitial_state for LSTM in tf.nn.dynamic_rnn?,"<p>I am not sure how to pass a value for initial_state when the cell is a LSTMCell. I am using LSTMStateTuple as it is shown in the following piece of code:</p>

<pre><code>c_placeholder = tf.placeholder(tf.float32, [ None, config.state_dim], name='c_lstm')

h_placeholder = tf.placeholder(tf.float32, [ None, config.state_dim], name='h_lstm')

state_tuple = tf.nn.rnn_cell.LSTMStateTuple(c_placeholder, h_placeholder)

cell = tf.contrib.rnn.LSTMCell(num_units=config.state_dim, state_is_tuple=True, reuse=not is_training)  

rnn_outs, states = tf.nn.dynamic_rnn(cell=cell, inputs=x,sequence_length=seqlen, initial_state=state_tuple, dtype= tf.float32)
</code></pre>

<p>However, the execution returns this error:</p>

<pre><code>TypeError: 'Tensor' object is not iterable.
</code></pre>

<p>Here is the link of the documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">dynamic_rnn</a></p>
","I am not sure how to pass a value for initial_state when the cell is a LSTMCell. I am using LSTMStateTuple as it is shown in the following piece of code: However, the execution returns this error: Here is the link of the documentation for dynamic_rnn",https://stackoverflow.com/questions/49012907,8628566,Documentation Replication on Other Examples
49889153,How to retrieve intermediary state in TensorFlow RNN,"<p>I am running an RNN on a signal in fixed-size segments. The following code allows me to preserve the final state of the previous batch to initialize the initial state of the next batch. </p>

<pre><code>rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)
</code></pre>

<p>This works when the batches are non-overlapping. For example, my first batch processes samples 0:124 and <code>final_state</code> is the state after this processing. Then, the next batch processes samples 124:256, setting <code>init_state</code> to <code>final_state</code>. </p>

<p>My question is how to retrieve an intermediary state when the batches are overlapping. First, I process samples 0:124, then 10:134, 20:144, so the hop size is 10. I would like to retrieve not the <code>final_state</code> but the state after processing 10 samples. </p>

<p>Is it possible in TF to keep the intermediary state? The <a href=""https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn"" rel=""nofollow noreferrer"">documentation</a> shows that the return value consists only of the final state. </p>

<p>The image shows the issue I am facing due to state discontinuity. In my program, the RNN segment length is 215 and the hop length is 20.</p>

<p><a href=""https://i.stack.imgur.com/G9owu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G9owu.png"" alt=""Sample results""></a></p>

<p>Update: the easiest turned out to be what <a href=""https://stackoverflow.com/a/49890068/4008884"">David Parks</a> described:</p>

<pre><code>rnn_outputs_one, mid_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs_one, initial_state=rnn_tuple_state)
rnn_outputs_two, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs_two, initial_state=mid_state)
rnn_outputs = rnn_outputs_one + rnn_outputs_two
</code></pre>

<p>and </p>

<pre><code>prev_state = sess.run(mid_state)
</code></pre>

<p>Now, after just a few iterations, the results look much better. <a href=""https://i.stack.imgur.com/4EU6N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4EU6N.png"" alt=""enter image description here""></a></p>
","I am running an RNN on a signal in fixed-size segments. The following code allows me to preserve the final state of the previous batch to initialize the initial state of the next batch. This works when the batches are non-overlapping. For example, my first batch processes samples 0:124 and final_state is the state after this processing. Then, the next batch processes samples 124:256, setting init_state to final_state. My question is how to retrieve an intermediary state when the batches are overlapping. First, I process samples 0:124, then 10:134, 20:144, so the hop size is 10. I would like to retrieve not the final_state but the state after processing 10 samples. Is it possible in TF to keep the intermediary state? The documentation shows that the return value consists only of the final state. The image shows the issue I am facing due to state discontinuity. In my program, the RNN segment length is 215 and the hop length is 20. Update: the easiest turned out to be what David Parks described: and Now, after just a few iterations, the results look much better.",https://stackoverflow.com/questions/49889153,4008884,Requesting (Additional) Resources
52157213,Tensorflow tf.placeholder with shape = [],"<p>I am looking at a Tensorflow code that has learning rate input to the graph using placeholder with shape = [], as below:</p>

<pre><code>self.lr_placeholder = tf.placeholder(dtype=tf.float32, shape=[])
</code></pre>

<p>I looked at the official documentation page of Tensorflow (<a href=""https://www.tensorflow.org/api_docs/python/tf/placeholder"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/placeholder</a>) to understand what would shape=[] mean, but could not get an explanation for the shape set to empty list. If someone can explain what does this mean.</p>
","I am looking at a Tensorflow code that has learning rate input to the graph using placeholder with shape = [], as below: I looked at the official documentation page of Tensorflow (https://www.tensorflow.org/api_docs/python/tf/placeholder) to understand what would shape=[] mean, but could not get an explanation for the shape set to empty list. If someone can explain what does this mean.",https://stackoverflow.com/questions/52157213,7561372,Documentation Completeness
45521499,legacy_init_op in TensorFlow Serving,"<p>I've noticed every example on TensorFlow Serving uses <code>legacy_init_op</code> parameter in <code>SavedModelBuilder</code> but I have not found any clear explanations on what this is and why it is called <strong>legacy</strong>. Anyone knows the purpose of this argument? </p>

<p>Example:</p>

<pre><code>legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')

builder.add_meta_graph_and_variables(
      sess, [tf.saved_model.tag_constants.SERVING],
      signature_def_map={
          'predict_images':
              prediction_signature,
          tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
              classification_signature,
      },
      legacy_init_op=legacy_init_op)
</code></pre>
",I've noticed every example on TensorFlow Serving uses legacy_init_op parameter in SavedModelBuilder but I have not found any clear explanations on what this is and why it is called legacy. Anyone knows the purpose of this argument? Example:,https://stackoverflow.com/questions/45521499,539617,Documentation Completeness
49287202,How to periodically save tensorflow model using saved_model API?,"<p>So for various reasons (such as its language-independence) I want to use tensorflow's <a href=""https://www.tensorflow.org/programmers_guide/saved_model#apis_to_build_and_load_a_savedmodel"" rel=""nofollow noreferrer"">saved_model</a> API for saving/loading models.  I can save everything (and restore it successfully) with a call to <code>builder.add_meta_graph_and_variables()</code> at the end of training, but I don't see any way to save periodically.  Tensorflow docs on this are very sparse, and the template code they provide (<a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/builder/SavedModelBuilder"" rel=""nofollow noreferrer"">here</a>) doesn't help me:</p>

<pre><code>...
builder = tf.saved_model.builder.SavedModelBuilder(export_dir)

with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph_and_variables(sess,
                                  [""foo-tag""],
                                  signature_def_map=foo_signatures,
                                  assets_collection=foo_assets)
...

with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph([""bar-tag"", ""baz-tag""])
...

builder.save()
</code></pre>

<p>Calling <code>builder.save()</code> does not save the new variables into the model.  It just updates the model protobuf.  </p>

<p>What am I missing?  How do I save after e.g. the nth epoch using <code>saved_model</code>?</p>
","So for various reasons (such as its language-independence) I want to use tensorflow's saved_model API for saving/loading models. I can save everything (and restore it successfully) with a call to builder.add_meta_graph_and_variables() at the end of training, but I don't see any way to save periodically. Tensorflow docs on this are very sparse, and the template code they provide (here) doesn't help me: Calling builder.save() does not save the new variables into the model. It just updates the model protobuf. What am I missing? How do I save after e.g. the nth epoch using saved_model?",https://stackoverflow.com/questions/49287202,8721926,Inadequate Examples
38527096,Run a tensorflow with a list of fetches does not work,"<p>I'm playing around with Tensorflow and implemented a k means clustering algorithm. Everything works well, but if I want to run the session with a couple of fetches in a <code>list</code> I always get the error, that a <code>list</code> can not be converted to a <code>Tensor</code> or <code>Operation</code>.</p>

<p>The <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/client.html#Session"" rel=""nofollow"">documentation</a> explicitly says, that I can call <code>Session.run()</code> with a list. Am I doing anything wrong?</p>

<p>Here is the source code:</p>

<pre><code>import tensorflow as tf
import numpy as np

def tf_k_means(k, data, eps_=0.1):
    eps = tf.constant(eps_)

    cluster_means = tf.placeholder(tf.float32, [None, 2])
    tf_data = tf.placeholder(tf.float32, [None, 2], name='data')

    model = tf.initialize_all_variables()

    expanded_data = tf.expand_dims(tf_data, 0)
    expanded_means = tf.expand_dims(cluster_means, 1)
    distances = tf.reduce_sum(tf.square(tf.sub(expanded_means, expanded_data)), 2)
    mins = tf.to_int32(tf.argmin(distances, 0))

    clusters = tf.dynamic_partition(tf_data, mins, k)
    old_cluster_means = tf.identity(cluster_means)
    new_means = tf.concat(0, [tf.expand_dims(tf.reduce_mean(cluster, 0), 0) for cluster in clusters])

    clusters_moved = tf.reduce_sum(tf.square(tf.sub(old_cluster_means, new_means)), 1)
    converged = tf.reduce_all(tf.less(clusters_moved, eps))

    cms = data[np.random.randint(data.shape[0],size=k), :]

    with tf.Session() as sess:
        sess.run(model)
        conv = False
        while not conv:
            #####################################
            # THE FOLLOWING LINE DOES NOT WORK: #
            #####################################
            (cs, cms, conv) = sess.run([clusters, new_means, converged], 
                                        feed_dict={tf_data: data, cluster_means: cms})    

    return cs, cms
</code></pre>

<p>Here is the error message:</p>

<pre><code>TypeError: Fetch argument [&lt;tf.Tensor 'DynamicPartition_25:0' shape=(?, 2) dtype=float32&gt;, 
&lt;tf.Tensor 'DynamicPartition_25:1' shape=(?, 2) dtype=float32&gt;, 
&lt;tf.Tensor 'DynamicPartition_25:2' shape=(?, 2) dtype=float32&gt;, 
&lt;tf.Tensor 'DynamicPartition_25:3' shape=(?, 2) dtype=float32&gt;] of 
[&lt;tf.Tensor 'DynamicPartition_25:0' shape=(?, 2) dtype=float32&gt;, 
&lt;tf.Tensor 'DynamicPartition_25:1' shape=(?, 2) dtype=float32&gt;, 
&lt;tf.Tensor 'DynamicPartition_25:2' shape=(?, 2) dtype=float32&gt;, 
&lt;tf.Tensor 'DynamicPartition_25:3' shape=(?, 2) dtype=float32&gt;] has 
invalid type &lt;class 'list'&gt;, must be a string or Tensor. (Can not 
convert a list into a Tensor or Operation.)
</code></pre>
","I'm playing around with Tensorflow and implemented a k means clustering algorithm. Everything works well, but if I want to run the session with a couple of fetches in a list I always get the error, that a list can not be converted to a Tensor or Operation. The documentation explicitly says, that I can call Session.run() with a list. Am I doing anything wrong? Here is the source code: Here is the error message:",https://stackoverflow.com/questions/38527096,1264252,Documentation Replication on Other Examples
47193290,Servers and sessions,"<p>I'm learning about distributed TensorFlow applications, and I understand jobs, tasks, and servers. A server's target identifies its gRPC location, as in grpc://localhost:1234.</p>

<p>I don't understand what happens when you create a session with a server's target, as in the following code:</p>

<pre><code>with tf.Session(server.target) as sess:
    ...
</code></pre>

<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/Session"" rel=""nofollow noreferrer"">documentation</a> states that <code>server.target</code> identifies the session's execution engine. Another <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">page</a> says that the constructor creates a session on the server. This isn't clear to me. How exactly does a server's target affect the session's execution?</p>
","I'm learning about distributed TensorFlow applications, and I understand jobs, tasks, and servers. A server's target identifies its gRPC location, as in grpc://localhost:1234. I don't understand what happens when you create a session with a server's target, as in the following code: The documentation states that server.target identifies the session's execution engine. Another page says that the constructor creates a session on the server. This isn't clear to me. How exactly does a server's target affect the session's execution?",https://stackoverflow.com/questions/47193290,934904,Documentation Ambiguity
70894266,minimize method not taking argument,"<p>From the tensorflow doc i have read <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/AdamOptimizer"" rel=""nofollow noreferrer"">here</a>, I have tried to minimise the adam optimizer.</p>
<pre><code>optimizer = tf.compat.v1.train.AdamOptimizer
print(&quot;Using AdamOptimizer...&quot;)

train_step = optimizer.minimize(loss, global_step = global_step,var_list = [process_image])
</code></pre>
<p>But I receive this error below from the code. Even though I have passed through the 'loss' argument. I think it may be due to using Tensorflow 2?</p>
<p><a href=""https://i.stack.imgur.com/4GpQV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4GpQV.png"" alt=""enter image description here"" /></a></p>
","From the tensorflow doc i have read here, I have tried to minimise the adam optimizer. But I receive this error below from the code. Even though I have passed through the 'loss' argument. I think it may be due to using Tensorflow 2?",https://stackoverflow.com/questions/70894266,12164928,Documentation Replicability
37890394,Tensorflow GradientDescentOptimizer - how does it connect to tf.Variables?,"<p><a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/train.html#GradientDescentOptimizer"" rel=""nofollow"">Documentation</a></p>

<p>I'm just curious how you tell it to minimize which variables. For example in this linear regression code, TF does fine optimizing weights/bias without being told the names of the variables:</p>

<pre><code>y = W * x + b

cost_func = tf.nn.l2_loss(y_ - y)  # squared error

trainer = tf.train.GradientDescentOptimizer(0.01).minimize(cost_func)
</code></pre>

<p>How does tensorflow know I want it to update <code>W</code> and <code>b</code>? Does it just see that those are the only Variables in the session?</p>
","Documentation I'm just curious how you tell it to minimize which variables. For example in this linear regression code, TF does fine optimizing weights/bias without being told the names of the variables: How does tensorflow know I want it to update W and b? Does it just see that those are the only Variables in the session?",https://stackoverflow.com/questions/37890394,712997,Documentation Ambiguity
37921781,What does opt.apply_gradients() do in TensorFlow?,"<p>The documentation is not quite clear about this. I suppose the gradients one can obtain by <code>opt.compute_gradients(E, [v])</code> contain the <code>∂E/∂x = g(x)</code> for each element <code>x</code> of the tensor that <code>v</code> stores. Does <code>opt.apply_gradients(grads_and_vars)</code> essentially execute <code>x ← -η·g(x)</code>, where <code>η</code> is the learning rate? That would imply that if I want to add a positive additive change <code>p</code> to the variable, I would need to need to change <code>g(x) ← g(x) - (1/η)p</code>, e.g. like this:</p>

<pre class=""lang-py prettyprint-override""><code>opt = tf.train.GradientDescentOptimizer(learning_rate=l)
grads_and_vars = opt.compute_gradients(loss, var_list)

for l, gv in enumerate(grads_and_vars):
    grads_and_vars[l] = (gv[0] - (1/l) * p, gv[1])

train_op = opt.apply_gradients(grads_and_vars)
</code></pre>

<p>Is there a better way to do this?</p>
","The documentation is not quite clear about this. I suppose the gradients one can obtain by opt.compute_gradients(E, [v]) contain the ∂E/∂x = g(x) for each element x of the tensor that v stores. Does opt.apply_gradients(grads_and_vars) essentially execute x ← -η·g(x), where η is the learning rate? That would imply that if I want to add a positive additive change p to the variable, I would need to need to change g(x) ← g(x) - (1/η)p, e.g. like this: Is there a better way to do this?",https://stackoverflow.com/questions/37921781,852592,Documentation Completeness
47932738,TensorFlow: restoring model in a MonitoredSession,"<p>I have a model that contains multiple variables including a global step. I've been able to successfully use a MonitoredSession to save checkpoints and summaries every 100 steps. I was expecting the MonitoredSession to automatically restore all my variables when the session is run in multiple passes (based on <a href=""https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession"" rel=""nofollow noreferrer"">this</a> documentation), however this does not happen. If I take a look at the global step after running the training session again, I find that it starts back from zero. This is a simplified version of my code without the actual model. Let me know if more code is needed to solve this problem</p>

<pre><code>train_graph = tf.Graph()
with train_graph.as_default():
  # I create some datasets using the Dataset API
  # ...

  global_step = tf.train.create_global_step()

  # Create all the other variables and the model here
  # ...

  saver_hook = tf.train.CheckpointSaverHook(
      checkpoint_dir='checkpoint/',
      save_secs=None,
      save_steps=100,
      saver=tf.train.Saver(),
      checkpoint_basename='model.ckpt',
      scaffold=None)
  summary_hook = tf.train.SummarySaverHook(
      save_steps=100,
      save_secs=None,
      output_dir='summaries/',
      summary_writer=None,
      scaffold=None,
      summary_op=train_step_summary)
  num_steps_hook = tf.train.StopAtStepHook(num_steps=500) # Just for testing


  with tf.train.MonitoredSession(
      hooks=[saver_hook, summary_hook, num_steps_hook]) as sess:
    while not sess.should_stop():
      step = sess.run(global_step)
      if (step % 100 == 0):
        print(step)
      sess.run(optimizer)
</code></pre>

<p>When I run this code the first time, I get this output</p>

<pre><code>0
100
200
300
400
</code></pre>

<p>The checkpoint folder at this point has checkpoints for every hundredth step up to 500. If I run the program again I would expect to see the counter start at 500 and the increase up to 900, but instead I just get the same thing and all of my checkpoints get overwritten. Any ideas?</p>
","I have a model that contains multiple variables including a global step. I've been able to successfully use a MonitoredSession to save checkpoints and summaries every 100 steps. I was expecting the MonitoredSession to automatically restore all my variables when the session is run in multiple passes (based on this documentation), however this does not happen. If I take a look at the global step after running the training session again, I find that it starts back from zero. This is a simplified version of my code without the actual model. Let me know if more code is needed to solve this problem When I run this code the first time, I get this output The checkpoint folder at this point has checkpoints for every hundredth step up to 500. If I run the program again I would expect to see the counter start at 500 and the increase up to 900, but instead I just get the same thing and all of my checkpoints get overwritten. Any ideas?",https://stackoverflow.com/questions/47932738,6642285,Documentation Replication on Other Examples
48027341,Getting iterator handle in MonitoredTrainingSession,"<p>I want to try out <code>MonitoredTrainingSession</code>, but I also use several Dataset objects for train and validation sets. And to select the correct one, as the <a href=""https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator"" rel=""nofollow noreferrer"">manual</a> suggests I use string handles. But to pass the handle into a <code>feed_dict</code> while training, I need to evaluate it first. Like this:</p>

<pre><code>handle = sess.run(iterator.string_handle())
</code></pre>

<p>But when I do this in context of <code>MonitoredTrainingSession</code>, I get an error:</p>

<pre><code>RuntimeError: Graph is finalized and cannot be modified.
</code></pre>

<p>The way out, as I thought, was to create an <code>init_fn</code> for a <code>Scaffold</code> object which I pass into the session. But this didn't work out. If I try to run aforementioned code in the context <code>init_fn</code> is still get the same error.</p>

<p>As the documentation says about <code>init_fn</code>:</p>

<blockquote>
  <p>A callable to run after the init op to perform additional initializations.</p>
</blockquote>

<p>This makes me think that I'm radically wrong with the intended purpose of this callback, or Tensorflow misbehaves.</p>

<p>Could you help me to resolve this confusion.</p>

<p>My tensorflow version is <code>1.4.0</code>.</p>

<p><em>UPDATE</em></p>

<p>Adding a minimal example. The first block works, the second doesn't.</p>

<pre><code>import tensorflow as tf

dataset_a = tf.data.Dataset.range(10)
dataset_b = tf.data.Dataset.range(20, 25)

input_handle = tf.placeholder(tf.string, shape=())
input_iterator = tf.data.Iterator.from_string_handle(
    input_handle, dataset_a.output_types, dataset_a.output_shapes)

x = input_iterator.get_next()
plus_one = tf.add(x, 1)

with tf.Session() as sess:
    iterator = dataset_b.make_initializable_iterator()
    handle = sess.run(iterator.string_handle())
    sess.run(iterator.initializer)

    res = sess.run(plus_one, feed_dict={input_handle: handle})
    print(res)

with tf.train.MonitoredTrainingSession() as sess:
    iterator = dataset_a.make_initializable_iterator()
    handle = sess.run(iterator.string_handle())
    sess.run(iterator.initializer)

    res = sess.run(plus_one, feed_dict={input_handle: handle})
    print(res)
</code></pre>
","I want to try out MonitoredTrainingSession, but I also use several Dataset objects for train and validation sets. And to select the correct one, as the manual suggests I use string handles. But to pass the handle into a feed_dict while training, I need to evaluate it first. Like this: But when I do this in context of MonitoredTrainingSession, I get an error: The way out, as I thought, was to create an init_fn for a Scaffold object which I pass into the session. But this didn't work out. If I try to run aforementioned code in the context init_fn is still get the same error. As the documentation says about init_fn: This makes me think that I'm radically wrong with the intended purpose of this callback, or Tensorflow misbehaves. Could you help me to resolve this confusion. My tensorflow version is 1.4.0. UPDATE Adding a minimal example. The first block works, the second doesn't.",https://stackoverflow.com/questions/48027341,1201416,Requesting (Additional) Resources
42323640,Tensorflow: AttributeError: module 'tensorflow' has no attribute 'Supervisor',"<p>I'm trying to use the Supervisor class to create checkpoints that can be used to save/load partial trainings, as mentioned in the TensorFlow documentation - <a href=""https://www.tensorflow.org/programmers_guide/supervisor"" rel=""nofollow noreferrer"">https://www.tensorflow.org/programmers_guide/supervisor</a> . </p>

<p>But when I try to use it as mentioned in the docs - </p>

<pre><code>sv = tf.Supervisor(logdir=""/checkpoints"")` 
with sv.managed_session() as sess:
</code></pre>

<p>It throws the below error -</p>

<blockquote>
  <p>Tensorflow: AttributeError: module 'tensorflow' has no attribute 'Supervisor'</p>
</blockquote>

<p>Whats am I missing?</p>
","I'm trying to use the Supervisor class to create checkpoints that can be used to save/load partial trainings, as mentioned in the TensorFlow documentation - https://www.tensorflow.org/programmers_guide/supervisor . But when I try to use it as mentioned in the docs - It throws the below error - Whats am I missing?",https://stackoverflow.com/questions/42323640,4993842,Documentation Replicability
39068703,"Tensorflow: Using weights trained in one model inside another, different model","<p>I'm trying to train an LSTM in Tensorflow using minibatches, but after training is complete I would like to use the model by submitting one example at a time to it.   I can set up the graph within Tensorflow to train my LSTM network, but I can't use the trained result afterward in the way I want.</p>

<p>The setup code looks something like this:</p>

<pre><code>#Build the LSTM model.
cellRaw = rnn_cell.BasicLSTMCell(LAYER_SIZE)
cellRaw = rnn_cell.MultiRNNCell([cellRaw] * NUM_LAYERS)

cell = rnn_cell.DropoutWrapper(cellRaw, output_keep_prob = 0.25)

input_data  = tf.placeholder(dtype=tf.float32, shape=[SEQ_LENGTH, None, 3])
target_data = tf.placeholder(dtype=tf.float32, shape=[SEQ_LENGTH, None])
initial_state = cell.zero_state(batch_size=BATCH_SIZE, dtype=tf.float32)

with tf.variable_scope('rnnlm'):
    output_w = tf.get_variable(""output_w"", [LAYER_SIZE, 6])
    output_b = tf.get_variable(""output_b"", [6])

outputs, final_state = seq2seq.rnn_decoder(input_list, initial_state, cell, loop_function=None, scope='rnnlm')
output = tf.reshape(tf.concat(1, outputs), [-1, LAYER_SIZE])
output = tf.nn.xw_plus_b(output, output_w, output_b)
</code></pre>

<p>...Note the two placeholders, input_data and target_data.  I haven't bothered including the optimizer setup.  After training is complete and the training session closed, I would like to set up a new session that uses the trained LSTM network whose input is provided by a completely different placeholder, something like:</p>

<pre><code>with tf.Session() as sess:
with tf.variable_scope(""simulation"", reuse=None):
    cellSim = cellRaw
    input_data_sim  = tf.placeholder(dtype=tf.float32, shape=[1, 1, 3])
    initial_state_sim = cell.zero_state(batch_size=1, dtype=tf.float32)
    input_list_sim = tf.unpack(input_data_sim)

    outputsSim, final_state_sim = seq2seq.rnn_decoder(input_list_sim, initial_state_sim, cellSim, loop_function=None, scope='rnnlm')
    outputSim = tf.reshape(tf.concat(1, outputsSim), [-1, LAYER_SIZE])

    with tf.variable_scope('rnnlm'):
        output_w = tf.get_variable(""output_w"", [LAYER_SIZE, nOut])
        output_b = tf.get_variable(""output_b"", [nOut])

    outputSim = tf.nn.xw_plus_b(outputSim, output_w, output_b)
</code></pre>

<p>This second part returns the following error:</p>

<pre><code>tensorflow.python.framework.errors.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float
 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>...Presumably because the graph I'm using still has the old training placeholders attached to the trained LSTM nodes.  What's the right way to 'extract' the trained LSTM and put it into a new, different graph that has a different style of inputs?  The Varible scoping features that Tensorflow has seem to address something like this, but the examples <a href=""https://www.tensorflow.org/versions/r0.10/how_tos/variable_scope/index.html"" rel=""noreferrer"">in the documentation</a> all talk about using variable scope as a way of managing variable names so that the same piece of code will generate similar subgraphs within the same graph.  The 'reuse' feature seems to be close to what I want, but I don't find the Tensorflow documentation linked above to be clear at all on what it does.  The cells themselves cannot be given a name (in other words, </p>

<pre><code>cellRaw = rnn_cell.MultiRNNCell([cellRaw] * NUM_LAYERS, name=""multicell"")
</code></pre>

<p>is not valid), and while I can give a name to a seq2seq.rnn_decoder(), I presumably wouldn't be able to remove the rnn_cell.DropoutWrapper() if I used that node unchanged.  </p>

<p>Questions:</p>

<p>What is the proper way to move trained LSTM weights from one graph to another?</p>

<p>Is it correct to say that starting a new session ""releases resources"", but doesn't erase the graph built in memory?</p>

<p>It seems to me like the 'reuse' feature allows Tensorflow to search outside of the current variable scope for variables with the same name (existing in a different scope), and use them in the current scope.  Is this correct?  If it is, what happens to all of the graph edges from the non-current scope that link to that variable?  If it isn't, why does Tensorflow throw an error if you try to have the same variable name within two different scopes?  It seems perfectly reasonable to define two variables with identical names in two different scopes, e.g. conv1/sum1 and conv2/sum1.</p>

<p>In my code I'm working within a new scope but the graph won't run without data to be fed into a placeholder from the initial, default scope.   Is the default scope always 'in-scope' for some reason?  </p>

<p>If graph edges can span different scopes, and names in different scopes can't be shared unless they refer to the exact same node, then that would seem to defeat the purpose of having different scopes in the first place.  What am I misunderstanding here?</p>

<p>Thanks!</p>
","I'm trying to train an LSTM in Tensorflow using minibatches, but after training is complete I would like to use the model by submitting one example at a time to it. I can set up the graph within Tensorflow to train my LSTM network, but I can't use the trained result afterward in the way I want. The setup code looks something like this: ...Note the two placeholders, input_data and target_data. I haven't bothered including the optimizer setup. After training is complete and the training session closed, I would like to set up a new session that uses the trained LSTM network whose input is provided by a completely different placeholder, something like: This second part returns the following error: ...Presumably because the graph I'm using still has the old training placeholders attached to the trained LSTM nodes. What's the right way to 'extract' the trained LSTM and put it into a new, different graph that has a different style of inputs? The Varible scoping features that Tensorflow has seem to address something like this, but the examples in the documentation all talk about using variable scope as a way of managing variable names so that the same piece of code will generate similar subgraphs within the same graph. The 'reuse' feature seems to be close to what I want, but I don't find the Tensorflow documentation linked above to be clear at all on what it does. The cells themselves cannot be given a name (in other words, is not valid), and while I can give a name to a seq2seq.rnn_decoder(), I presumably wouldn't be able to remove the rnn_cell.DropoutWrapper() if I used that node unchanged. Questions: What is the proper way to move trained LSTM weights from one graph to another? Is it correct to say that starting a new session ""releases resources"", but doesn't erase the graph built in memory? It seems to me like the 'reuse' feature allows Tensorflow to search outside of the current variable scope for variables with the same name (existing in a different scope), and use them in the current scope. Is this correct? If it is, what happens to all of the graph edges from the non-current scope that link to that variable? If it isn't, why does Tensorflow throw an error if you try to have the same variable name within two different scopes? It seems perfectly reasonable to define two variables with identical names in two different scopes, e.g. conv1/sum1 and conv2/sum1. In my code I'm working within a new scope but the graph won't run without data to be fed into a placeholder from the initial, default scope. Is the default scope always 'in-scope' for some reason? If graph edges can span different scopes, and names in different scopes can't be shared unless they refer to the exact same node, then that would seem to defeat the purpose of having different scopes in the first place. What am I misunderstanding here? Thanks!",https://stackoverflow.com/questions/39068703,3280780,Lack of Alternative Solutions/Documentation
49114306,Avoiding duplicating graph in tensorflow (LSTM model),"<p>I have the following simplified code (actually, unrolled LSTM model):</p>

<pre><code>def func(a, b):
    with tf.variable_scope('name'):
        res = tf.add(a, b)
    print(res.name)
    return res

func(tf.constant(10), tf.constant(20))
</code></pre>

<p>Whenever I run the last line, it seems that it changes the graph. But I don't want the graph changes. Actually my code is different and is a neural network model but it is too huge, so I've added the above code. I want to call the <code>func</code> without changing the graph of model but it changes. I read about variable scope in <code>TensorFlow</code> but it seems that I've not understand it at all.</p>
","I have the following simplified code (actually, unrolled LSTM model): Whenever I run the last line, it seems that it changes the graph. But I don't want the graph changes. Actually my code is different and is a neural network model but it is too huge, so I've added the above code. I want to call the func without changing the graph of model but it changes. I read about variable scope in TensorFlow but it seems that I've not understand it at all.",https://stackoverflow.com/questions/49114306,9422652,Documentation Ambiguity
53122412,AOT compilation of an Estimator,"<p>I find it very challenging to find in the documentation any help on this important issue.. Indeed, after creating an estimator (canned or custom), one wants to <code>tf.compile</code> the resulting predictor, produce the <code>.so</code> and link it to one's project..</p>

<p>So I have my calib class in which I define a simple linear estimator</p>

<pre><code>self.model = tf.estimator.LinearRegressor(
        feature_columns=self.feature_columns,
        model_dir = self.model_dir)
</code></pre>

<p>After training, I want to 
1- get the trained model with optimal parameters (load it in my variable self.model)</p>

<p>2- extract the graph and freeze it</p>

<p>3- tf.compile that graph</p>

<p>I could not find any way to do parts 1- and 2-. Once I have them, part 3 is solved by using tf.compile</p>

<p>Can you please point me to a good way to it?</p>
","I find it very challenging to find in the documentation any help on this important issue.. Indeed, after creating an estimator (canned or custom), one wants to tf.compile the resulting predictor, produce the .so and link it to one's project.. So I have my calib class in which I define a simple linear estimator After training, I want to 1- get the trained model with optimal parameters (load it in my variable self.model) 2- extract the graph and freeze it 3- tf.compile that graph I could not find any way to do parts 1- and 2-. Once I have them, part 3 is solved by using tf.compile Can you please point me to a good way to it?",https://stackoverflow.com/questions/53122412,886724,Documentation Completeness
59725112,How to automatically assign free GPUs in TensorFlow,"<p>I have 4 Tesla K80 GPUs in my system. I would like to automatically allocate free GPUs based on an integer input in the code. I am aware of <code>tf.config.experimental.set_visible_devices()</code> to assign specific GPUs but currently do not know how to identify which of the GPUs are in-use (expect manually using <code>nvidia-smi</code>). I am currently changing the code below for every run.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    # Restrict TensorFlow to only use the first GPU
    try:
        tf.config.experimental.set_visible_devices(gpus[2:], 'GPU')
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPU"")
    except RuntimeError as e:
        # Visible devices must be set before GPUs have been initialized
        print(e)
</code></pre>

<p>The above code lets me set the GPUs I want to allocate (GPU 2,3 in above example) for the run. Is there anyway to obtain a list of free (unused) devices to automate the allocation process instead manually having to identify which of the devices should be set?
I am currently using TensorFlow version 1.15</p>
","I have 4 Tesla K80 GPUs in my system. I would like to automatically allocate free GPUs based on an integer input in the code. I am aware of tf.config.experimental.set_visible_devices() to assign specific GPUs but currently do not know how to identify which of the GPUs are in-use (expect manually using nvidia-smi). I am currently changing the code below for every run. The above code lets me set the GPUs I want to allocate (GPU 2,3 in above example) for the run. Is there anyway to obtain a list of free (unused) devices to automate the allocation process instead manually having to identify which of the devices should be set? I am currently using TensorFlow version 1.15",https://stackoverflow.com/questions/59725112,7537870,Requesting (Additional) Resources
47379766,Replacing a node in a frozen Tensorflow model,"<p>I have a <code>frozen inference graph</code> stored in a <code>.pb file</code>, which was obtained from a <code>trained Tensorflow model</code> by the <code>freeze_graph</code> function. </p>

<p><strong>Suppose, for simplicity,</strong> that I would like to change some of the <code>sigmoid activations</code> in the model to <code>tanh activations</code> (and let's not discuss whether this is a good idea). </p>

<p><strong>How can this be done with access only to the frozen graph in the .pb file, and without the possibility to retrain the model?</strong></p>

<p>I am aware of the <code>Graph Editor library in tf.contrib</code>, which should be able to do this kind of job, but I wasn't able to figure out a simple way to do this in the documentation.</p>
","I have a frozen inference graph stored in a .pb file, which was obtained from a trained Tensorflow model by the freeze_graph function. Suppose, for simplicity, that I would like to change some of the sigmoid activations in the model to tanh activations (and let's not discuss whether this is a good idea). How can this be done with access only to the frozen graph in the .pb file, and without the possibility to retrain the model? I am aware of the Graph Editor library in tf.contrib, which should be able to do this kind of job, but I wasn't able to figure out a simple way to do this in the documentation.",https://stackoverflow.com/questions/47379766,7869068,Inadequate Examples
42697341,"How to use softmax activation function at the output layer, but relus in the middle layers in TensorFlow?","<p>I have a neural net of 3 hidden layers (so I have 5 layers in total). I want to use <strong>Rectified Linear Units</strong> at each of the hidden layers, but at the outermost layer I want to apply <strong>Softmax</strong> on the logits. I want to use the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier"" rel=""nofollow noreferrer""><code>DNNClassifier</code></a>. I have read the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier"" rel=""nofollow noreferrer"">official documentation</a> of the TensorFlow where for setting value of the parameter <code>activation_fn</code> they say: </p>

<blockquote>
  <p>activation_fn: Activation function applied to each layer. If None, will use tf.nn.relu.</p>
</blockquote>

<p>I know I can always write my own model and use any arbitrary combination of the activation functions. But as the <code>DNNClassifier</code> is more concrete, I want to resort to that. So far I have:</p>

<pre><code>classifier = tf.contrib.learn.DNNClassifier(
  feature_columns=features_columns,
  hidden_units=[10,20,10],
  n_classes=3
  # , activation_fn:::: I want something like below
  # activation_fn = [relu,relu,relu,softmax]
)
</code></pre>
","I have a neural net of 3 hidden layers (so I have 5 layers in total). I want to use Rectified Linear Units at each of the hidden layers, but at the outermost layer I want to apply Softmax on the logits. I want to use the DNNClassifier. I have read the official documentation of the TensorFlow where for setting value of the parameter activation_fn they say: I know I can always write my own model and use any arbitrary combination of the activation functions. But as the DNNClassifier is more concrete, I want to resort to that. So far I have:",https://stackoverflow.com/questions/42697341,4933403,Requesting (Additional) Resources
44908968,Tensorflow multiple export strategies,"<p>I am training a model with the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"" rel=""nofollow noreferrer"">Experiment class</a> and although the documentation seems to suggest you can have more than one export strategy: </p>

<blockquote>
  <p>export_strategies: Iterable of ExportStrategys, or a single one, or None.</p>
</blockquote>

<p>When I include two I get an error while training with ml engine: </p>

<pre><code> AssertionError: Export directory already exists. Please specify a different export directory
</code></pre>

<p>When using the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/make_export_strategy"" rel=""nofollow noreferrer"">make_export_strategy function</a> there is no option to specify the export directory. </p>

<p>Am I approaching this in the wrong way? Ultimately I want people to be able to make prediction requests to the same model with CSV and JSON inputs.</p>

<pre><code> tf.contrib.learn.Experiment(
    estimator=estimator,
    train_input_fn=train_input,
    eval_input_fn=eval_input,
    eval_metrics=eval_metrics,
    train_steps=train_steps,
    eval_steps=eval_steps,
    eval_delay_secs=eval_delay_secs,
    min_eval_frequency=min_eval_frequency,        
    export_strategies=[
      saved_model_export_utils.make_export_strategy(
        serving_input_fn=csv_serving_input_fn,
        exports_to_keep=1),
      saved_model_export_utils.make_export_strategy(
        serving_input_fn=json_serving_input_fn,
        exports_to_keep=1)
    ]
 )
</code></pre>
",I am training a model with the Experiment class and although the documentation seems to suggest you can have more than one export strategy: When I include two I get an error while training with ml engine: When using the make_export_strategy function there is no option to specify the export directory. Am I approaching this in the wrong way? Ultimately I want people to be able to make prediction requests to the same model with CSV and JSON inputs.,https://stackoverflow.com/questions/44908968,6520820,Documentation Completeness
44313202,What are the 'from' and 'to' dimensions of transition_params in tf.contrib.crf.crf_log_likelihood?,"<p>On TensorFlow, I want to pass a transition_params matrix as argument to <code>tf.contrib.crf.crf_log_likelihood</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/crf/crf_log_likelihood"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/contrib/crf/crf_log_likelihood</a>), in order to initialize the transitions matrix of the CRF. Although, in the documentation, it is not clear which dimension of this matrix corresponds to the first tag of the transition and which dimension corresponds to the second.</p>

<p>So, let <code>T</code> be the transitions matrix, does <code>T[i,j]</code> represent the score of the transition from tag <code>i</code> to tag <code>j</code>, or is it the other way around?</p>
","On TensorFlow, I want to pass a transition_params matrix as argument to tf.contrib.crf.crf_log_likelihood (https://www.tensorflow.org/api_docs/python/tf/contrib/crf/crf_log_likelihood), in order to initialize the transitions matrix of the CRF. Although, in the documentation, it is not clear which dimension of this matrix corresponds to the first tag of the transition and which dimension corresponds to the second. So, let T be the transitions matrix, does T[i,j] represent the score of the transition from tag i to tag j, or is it the other way around?",https://stackoverflow.com/questions/44313202,1868775,Documentation Completeness
46444018,"Meaning of buffer_size in Dataset.map , Dataset.prefetch and Dataset.shuffle","<p>As per TensorFlow <a href=""https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset#prefetch"" rel=""noreferrer"">documentation</a> , the <code>prefetch</code> and <code>map</code> methods of <code>tf.contrib.data.Dataset</code> class, both have a parameter called <code>buffer_size</code>.</p>
<p>For <code>prefetch</code> method, the parameter is known as <code>buffer_size</code> and according to documentation :</p>
<blockquote>
<p>buffer_size: A tf.int64 scalar tf.Tensor, representing the maximum
number elements that will be buffered when prefetching.</p>
</blockquote>
<p>For the <code>map</code> method, the parameter is known as <code>output_buffer_size</code> and according to documentation :</p>
<blockquote>
<p>output_buffer_size: (Optional.) A tf.int64 scalar tf.Tensor,
representing the maximum number of processed elements that will be
buffered.</p>
</blockquote>
<p>Similarly for the <code>shuffle</code> method, the same quantity appears and according to documentation :</p>
<blockquote>
<p>buffer_size: A tf.int64 scalar tf.Tensor, representing the number of
elements from this dataset from which the new dataset will sample.</p>
</blockquote>
<p>What is the relation between these parameters ?</p>
<p>Suppose I create a<code>Dataset</code> object as follows :</p>
<pre><code> tr_data = TFRecordDataset(trainfilenames)
    tr_data = tr_data.map(providefortraining, output_buffer_size=10 * trainbatchsize, num_parallel_calls\
=5)
    tr_data = tr_data.shuffle(buffer_size= 100 * trainbatchsize)
    tr_data = tr_data.prefetch(buffer_size = 10 * trainbatchsize)
    tr_data = tr_data.batch(trainbatchsize)
</code></pre>
<p>What role is being played by the <code>buffer</code> parameters in the above snippet ?</p>
","As per TensorFlow documentation , the prefetch and map methods of tf.contrib.data.Dataset class, both have a parameter called buffer_size. For prefetch method, the parameter is known as buffer_size and according to documentation : For the map method, the parameter is known as output_buffer_size and according to documentation : Similarly for the shuffle method, the same quantity appears and according to documentation : What is the relation between these parameters ? Suppose I create aDataset object as follows : What role is being played by the buffer parameters in the above snippet ?",https://stackoverflow.com/questions/46444018,8530591,Documentation Ambiguity
49393659,tf.Data: what are stragglers in parallel interleaving?,"<p><a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/data/Dataset#interleave"" rel=""noreferrer""><code>interleave</code></a> is a <code>tf.Data.Dataset</code> method that can be used to interleave together elements from multiple datasets. <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/parallel_interleave"" rel=""noreferrer""><code>tf.contrib.data.parallel_interleave</code></a> provides a parallel version of the same functionality with the help of <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply"" rel=""noreferrer""><code>apply</code></a>.</p>

<p>I can see that reading from many datasets in parallel and having buffers for them as allowed by the parallel version will improve throughput. But the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/parallel_interleave"" rel=""noreferrer"">documentation</a> also has this to say about how <code>parallel_interleave</code> can increase data throughput:</p>

<blockquote>
  <p>Unlike tf.data.Dataset.interleave, it gets elements from cycle_length
  nested datasets in parallel, which increases the throughput,
  especially in the presence of stragglers.</p>
</blockquote>

<p>What exactly are stragglers, and why does <code>parallel_interleave</code> work especially well in terms of throughput in their presence?</p>
","interleave is a tf.Data.Dataset method that can be used to interleave together elements from multiple datasets. tf.contrib.data.parallel_interleave provides a parallel version of the same functionality with the help of apply. I can see that reading from many datasets in parallel and having buffers for them as allowed by the parallel version will improve throughput. But the documentation also has this to say about how parallel_interleave can increase data throughput: What exactly are stragglers, and why does parallel_interleave work especially well in terms of throughput in their presence?",https://stackoverflow.com/questions/49393659,5471520,Documentation Completeness
57009265,How transition from keras fit_generator() to the models input layer works exactly,"<p>I am working with image data and some scalar meta-data (like hair-color, eye-color, ...).
I am using a self-written generator to use the Keras <code>.fit_generator()</code> function.</p>

<p>The process looks like the following:</p>

<p>After applying some data augmentation I have the shape <code>((10,200,200,3),(10,),(10,),(10,),(10,))</code> of my dataset (For imagination: I extract images of shape <code>(200,200,3</code>) and stack together 10 of them -> <code>(10,200,200,3)</code>. Accordingly, I duplicate the metadata 10 times -> shapes <code>(10,)</code> for each )</p>

<p>Afterwards I use the tensorflow function <code>dataset = dataset.apply(tf.contrib.data.unbatch())</code> so that the shape of my dataset is <code>((200,200,3),(),(),(),())</code>. From here I now share the code with you: </p>

<p><strong>Edit (more Code):</strong></p>

<p>Following code are the last line of my generator-function which will be called from the <code>.fit_generator()</code> function in the <code>main()</code></p>

<pre><code>shape_dataset = tf.shape(dataset) # shape ((10,200,200,3),(10,),(10,),(10,),(10,)) like I mentioned above
dataset = dataset.apply(tf.contrib.data.unbatch()) # shape ((200,200,3),(),(),(),()) like I mentioned a bove 
dataset = dataset.shuffle(buffer_size = buffer_size)
dataset = dataset.batch(batch_size=batch_size) 
dataset = dataset.repeat()
iterator_all = dataset.make_one_shot_iterator()
next_all = iterator_all.get_next()

with tf.Session() as sess:
    while True:
        try:
            image, eye_color, hair_ color, labels = sess.run(next_all)
            yield [image, eye_color, hair_ color], labels

        except tf.errors.OutOfRangeError:
            print('Finished')
            break
</code></pre>

<p>This tensor will now be fed into my network via the keras <code>.fit_generator()</code> function.
The input layer looks like the following:</p>

<pre><code>input_image = Input(shape=(200, 200, 3))
input_eye_color = Input(shape=(1,), name='input_ec')
input_hair_color = Input(shape=(1,), name='input_hc')
</code></pre>

<p>Now I have some question:</p>

<ol>
<li><p>Where does the 10 from <code>((10,200,200,3),(10,),(10,),(10,),(10,))</code> go through the <code>tf.contrib.data.unbatch())</code> function? For me it feels like I am losing these 10 values and just get 1?</p></li>
<li><p>The <code>fit_generator()</code> function works batch-wise, but how? Stupid as i sounds I have the feeling my network gets data of shape <code>((200,200,3),(),(),(),())</code> for one iteration step. Obviously it gets data like  <code>((8,10,200,200,3),(8,10,),(8,10,),(8, 10,),(8, 10,))</code> as batch size is 8.</p></li>
</ol>

<p>Can someone explain this issue with the shapes to me?
And really I read a lot but I still do not get it. </p>

<p>Thanks for your help :-) </p>
","I am working with image data and some scalar meta-data (like hair-color, eye-color, ...). I am using a self-written generator to use the Keras .fit_generator() function. The process looks like the following: After applying some data augmentation I have the shape ((10,200,200,3),(10,),(10,),(10,),(10,)) of my dataset (For imagination: I extract images of shape (200,200,3) and stack together 10 of them -&gt; (10,200,200,3). Accordingly, I duplicate the metadata 10 times -&gt; shapes (10,) for each ) Afterwards I use the tensorflow function dataset = dataset.apply(tf.contrib.data.unbatch()) so that the shape of my dataset is ((200,200,3),(),(),(),()). From here I now share the code with you: Edit (more Code): Following code are the last line of my generator-function which will be called from the .fit_generator() function in the main() This tensor will now be fed into my network via the keras .fit_generator() function. The input layer looks like the following: Now I have some question: Can someone explain this issue with the shapes to me? And really I read a lot but I still do not get it. Thanks for your help :-)",https://stackoverflow.com/questions/57009265,11765047,Lack of Alternative Solutions/Documentation
52484694,TF Eager mode: Load a complete model from disk if possible,"<p><em>Update: ""AttributeError: 'AdamOptimizer' object has no attribute 'name'"" might be an alternative title for this question. If this can be solved then the whole thing might work right.</em></p>

<p>Have a new jupyter notebook that has NO model saving code.  The trained model was already saved using another notebook like this. The iris dataset was used here and the model is all trained up:</p>

<pre><code>#This might be the way:
#https://www.tensorflow.org/guide/eager#object_based_saving
#https://stackoverflow.com/questions/47852516/tensorflow-eager-mode-how-to-restore-a-model-from-a-checkpoint
savePrefix = ""/tmp/iris""
root = tfe.Checkpoint(optimizer=optimizer, model=model, optimizer_step=global_step)
restorePrefix = root.save(savePrefix)
# '/tmp/iris-1'
print(restorePrefix)
print(root)
print(""The model was saved to {}\nRestore the model using {}"".format(savePrefix, restorePrefix))
# Try loading this in a different notebook to prove it worked.

&gt;/tmp/iris-1
&gt;&lt;tensorflow.contrib.eager.python.checkpointable_utils.Checkpoint object at 0x7fb308799e80&gt;
&gt;The model was saved to /tmp/iris
&gt;Restore the model using /tmp/iris-1
</code></pre>

<p>I grabbed the output path from the above and then tried to load the model in a new notebook using tf.contrib.eager code but it fails:</p>

<pre><code>s = tfe.Saver([optimizer, model, global_step])
s.restore(file_prefix=""/tmp/iris-1"")

&gt;NameError: name 'optimizer' is not defined
</code></pre>

<p>So what is an actually WORKING use case code to load a previously developed model with the tf.contrib.eager api (not the session api) WHEN THE CODE IN THE MODEL-LOADING NOTEBOOK DOES NOT SAVE THE MODEL AND DOES NOT HAVE THE MODEL'S PARTS IN MEMORY ALREADY like optimizer, graph definition, and global_state?</p>

<p>The TF docs always choose to demo a pointless example of loading a model we already have in memory.  I can't tell if ""With the TF.contrib.eager API you have to have explicit model creation code and optimizer creation code and global step code right in your notebook because TFE cannot load this stuff from disk"" or ""its a new API and some features are missing"", or ""you have to also use session and graph coding api along with tf.contrib.eager api"" or ""just use Microsoft cntk which actually works with imperative code and they didnt forget critical parts of the API"". </p>

<p>Thanks if you know something. If speculating, please state.</p>

<p>I suspect it's some superset and subset combination of the following if it's going to work. (Scraped from SO posts on the subject.)</p>

<pre><code>tfe.restore_variables_on_create 
tf.train.import_meta_graph() 
new_saver.restore()
tf.train.latest_checkpoint()
tfe.save_network_checkpoint()
tfe.restore_network_checkpoint()
</code></pre>

<p>Update - I added code to first manually recreate the model, optimizer, etc into memory, and then ran the restore() code. Error - optimizer does not have a name. But, oddly, it actually does have a name attribute according to the documentation:</p>

<pre><code>#OK I'll just try to use code to create the model artifacts first. 
# I'd rather load it all from disk but maybe at least it might work.

optimizer = tf.train.AdamOptimizer() #ga
global_step = tf.train.get_or_create_global_step()  # what is a global_step?
model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required
  tf.keras.layers.Dense(10, activation=tf.nn.relu, kernel_initializer='glorot_uniform'),
  tf.keras.layers.Dense(3)
])
s = tfe.Saver([optimizer, model, global_step])
s.restore(file_prefix=""/tmp/iris-1"")
</code></pre>

<p>INFO:tensorflow:Restoring parameters from /tmp/iris-1</p>

<hr>

<p>AttributeError: 'AdamOptimizer' object has no attribute 'name'</p>

<p>The documentation says:</p>

<pre><code>__init__(
    learning_rate=0.001,
    beta1=0.9,
    beta2=0.999,
    epsilon=1e-08,
    use_locking=False,
    name='Adam'
)
</code></pre>

<p>It sure looks like Adam optimizer has a name!  That's funny. What's the problem then?</p>

<p>Maybe the TF error is really saying that an optimizer and a model and a global_state variable cannot be restored.  So then what would be restored from a checkpoint -- specifically what variables would go in the save and corresponding restore?  Thanks if you know anything.</p>
","Update: ""AttributeError: 'AdamOptimizer' object has no attribute 'name'"" might be an alternative title for this question. If this can be solved then the whole thing might work right. Have a new jupyter notebook that has NO model saving code. The trained model was already saved using another notebook like this. The iris dataset was used here and the model is all trained up: I grabbed the output path from the above and then tried to load the model in a new notebook using tf.contrib.eager code but it fails: So what is an actually WORKING use case code to load a previously developed model with the tf.contrib.eager api (not the session api) WHEN THE CODE IN THE MODEL-LOADING NOTEBOOK DOES NOT SAVE THE MODEL AND DOES NOT HAVE THE MODEL'S PARTS IN MEMORY ALREADY like optimizer, graph definition, and global_state? The TF docs always choose to demo a pointless example of loading a model we already have in memory. I can't tell if ""With the TF.contrib.eager API you have to have explicit model creation code and optimizer creation code and global step code right in your notebook because TFE cannot load this stuff from disk"" or ""its a new API and some features are missing"", or ""you have to also use session and graph coding api along with tf.contrib.eager api"" or ""just use Microsoft cntk which actually works with imperative code and they didnt forget critical parts of the API"". Thanks if you know something. If speculating, please state. I suspect it's some superset and subset combination of the following if it's going to work. (Scraped from SO posts on the subject.) Update - I added code to first manually recreate the model, optimizer, etc into memory, and then ran the restore() code. Error - optimizer does not have a name. But, oddly, it actually does have a name attribute according to the documentation: INFO:tensorflow:Restoring parameters from /tmp/iris-1 AttributeError: 'AdamOptimizer' object has no attribute 'name' The documentation says: It sure looks like Adam optimizer has a name! That's funny. What's the problem then? Maybe the TF error is really saying that an optimizer and a model and a global_state variable cannot be restored. So then what would be restored from a checkpoint -- specifically what variables would go in the save and corresponding restore? Thanks if you know anything.",https://stackoverflow.com/questions/52484694,39123,Documentation Completeness
49416931,What does the tensorflow.python.eager.tape do in the implementation of tf.contrib.eager.custom_gradient?,"<p>I am going through TensorFlow Eager Execution from <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/g3doc/guide.md"" rel=""nofollow noreferrer"">here</a> and find it difficult to understand the customizing gradients part. </p>

<pre><code>@tfe.custom_gradient
def logexp(x):
    e = tf.exp(x)
    def grad(dy):
        return dy * (1 - 1/(1 + e))
    return tf.log(1 + e), grad
</code></pre>

<p>First, it is difficult to make sense what does dy do in the gradient function.</p>

<p>When I read the implementation of tf.contrib.eager.custom_gradient.
I can't really make sense the working mechanism behind tape. Following is the code I borrow from the implementation of tf.contrib.eager.custom_gradient. Can anybody explain what does tape do here?</p>

<pre><code>from tensorflow.python.eager import tape
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import gen_array_ops
from tensorflow.python.util import nest
from tensorflow.python.framework import ops as tf_ops

def my_custom_gradient(f):
    def decorated(*args, **kwargs):
        for x in args:
            print('args {0}'.format(x))
        input_tensors = [tf_ops.convert_to_tensor(x) for x in args]

        with tape.stop_recording():
            result, grad_fn = f(*args, **kwargs)
            flat_result = nest.flatten(result)
            flat_result = [gen_array_ops.identity(x) for x in flat_result]

        def actual_grad_fn(*outputs):
            print(*outputs)
            return nest.flatten(grad_fn(*outputs))

       tape.record_operation(
            f.__name__, # the name of f, in this case logexp
            flat_result,
            input_tensors,
            actual_grad_fn) # backward_function
       flat_result = list(flat_result)
       return nest.pack_sequence_as(result, flat_result)
return decorated 
</code></pre>

<p>Even though I found the implementation of tape from <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/tape.py"" rel=""nofollow noreferrer"">here</a>. But I can't really get much out of it due the poor documentation.</p>
","I am going through TensorFlow Eager Execution from here and find it difficult to understand the customizing gradients part. First, it is difficult to make sense what does dy do in the gradient function. When I read the implementation of tf.contrib.eager.custom_gradient. I can't really make sense the working mechanism behind tape. Following is the code I borrow from the implementation of tf.contrib.eager.custom_gradient. Can anybody explain what does tape do here? Even though I found the implementation of tape from here. But I can't really get much out of it due the poor documentation.",https://stackoverflow.com/questions/49416931,3744927,Documentation Ambiguity
48814591,Too many values to unpack in TensorFlow KMean Class,"<p>I'm currently using the <code>KMeans</code> Class from <code>tensorflow.contrib.factorization</code> module. My input is (assuming all variables are defined):</p>

<pre><code>kmeans = KMeans(inputs=X, num_clusters=k, distance_metric='cosine', use_mini_batch=True)
</code></pre>

<p>I'm following the documentation at <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeans"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeans</a> to unpack the values like:</p>

<pre><code>(all_scores, cluster_idx, scores, cluster_centers_initialized, init_op, train_op) = kmeans.training_graph()
</code></pre>

<p>I get the error:</p>

<pre><code>----&gt; (all_scores, cluster_idx, scores, cluster_centers_initialized, init_op, train_op) = kmeans.training_graph()    
ValueError: too many values to unpack
</code></pre>

<p>I'm strongly guessing that the <strong>documentation in the link stated above isn't updated</strong> because the output of <code>kmeans.training_graph()</code> is :</p>

<pre><code>((&lt;tf.Tensor 'sub_14:0' shape=(?, ?) dtype=float32&gt;,),
 (&lt;tf.Tensor 'Squeeze_7:0' shape=&lt;unknown&gt; dtype=int64&gt;,),
 (&lt;tf.Tensor 'Squeeze_6:0' shape=&lt;unknown&gt; dtype=float32&gt;,),
 &lt;tf.Variable 'initialized_3:0' shape=() dtype=bool_ref&gt;,
 &lt;tf.Variable 'clusters_3:0' shape=&lt;unknown&gt; dtype=float32_ref&gt;,
 tf.Tensor 'cond_3/Merge:0' shape=() dtype=bool&gt;,
 &lt;tf.Operation 'group_deps_3' type=NoOp&gt;)
</code></pre>

<p>Please let me know what is the extra returned valued that I'm not aware of by reading the documentation.</p>
",I'm currently using the KMeans Class from tensorflow.contrib.factorization module. My input is (assuming all variables are defined): I'm following the documentation at https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeans to unpack the values like: I get the error: I'm strongly guessing that the documentation in the link stated above isn't updated because the output of kmeans.training_graph() is : Please let me know what is the extra returned valued that I'm not aware of by reading the documentation.,https://stackoverflow.com/questions/48814591,3749292,Documentation Replicability
55040014,How to weight clip in tensorflow?,"<p>I am coding a wgan in tensorflow on mnist dataset and it works well but I am finding it difficult to clip weights of discriminator model <code>[-0.01,0.01]</code> in tensorflow. In <a href=""https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan/wgan.py"" rel=""nofollow noreferrer"">keras</a> we can do weight clipping using.</p>

<pre><code>for l in self.discriminator.layers:
    weights = l.get_weights()
    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]
    l.set_weights(weights)
</code></pre>

<p>I have found a tensorflow doc for <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/gan/features/clip_discriminator_weights"" rel=""nofollow noreferrer"">weight clipping discrimantor</a> </p>

<pre><code>tf.contrib.gan.features.clip_discriminator_weights(
    optimizer,
    model,
    weight_clip
)
</code></pre>

<p>Other than this there is not much is given to how use this function.</p>

<pre><code>#my tf code
def generator(z):
    h=tf.nn.relu(layer_mlp(z,""g1"",[10,128]))
    prob=tf.nn.sigmoid(layer_mlp(h,""g2"",[128,784]))
    return prob


def discriminator(x):
    h=tf.nn.relu(layer_mlp(x,""d1"",[784,128]))
    logit=layer_mlp(h,""d2"",[128,1])
    prob=tf.nn.sigmoid(logit)
    return prob

G_sample=generator(z)
D_real= discriminator(x)
D_fake= discriminator(G_sample)


D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake)
G_loss = -tf.reduce_mean(D_fake)

for epoch in epochs:
    #training the model
</code></pre>
","I am coding a wgan in tensorflow on mnist dataset and it works well but I am finding it difficult to clip weights of discriminator model [-0.01,0.01] in tensorflow. In keras we can do weight clipping using. I have found a tensorflow doc for weight clipping discrimantor Other than this there is not much is given to how use this function.",https://stackoverflow.com/questions/55040014,996366,Inadequate Examples
53284674,Tensorflow serving with contrib operations,"<p>How can I serve model with <a href=""https://www.tensorflow.org/serving/"" rel=""nofollow noreferrer"">tensorflow-serving</a>, if there are tf.contrib operations. I use Tensorflow Serving via Docker (latest) (version of tf 1.11) and when I serve model there is the next message:</p>

<pre><code>“Failed to start server. Error: Unknown: 1 servable(s) did not become available: {{{name: slider_universal version: 1} due to error: Not found: Op type not registered ‘ImageProjectiveTransformV2’ in binary running on 254345a5d9f1. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) tf.contrib.resampler should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.}, }”
</code></pre>

<p>I also built with bazel but there was the same error</p>

<p>I use <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/image/transform"" rel=""nofollow noreferrer"">tf.contrib.image.transform</a> </p>

<p>If I delete this operation during exporting model it can be served by tensorflow serving</p>
","How can I serve model with tensorflow-serving, if there are tf.contrib operations. I use Tensorflow Serving via Docker (latest) (version of tf 1.11) and when I serve model there is the next message: I also built with bazel but there was the same error I use tf.contrib.image.transform If I delete this operation during exporting model it can be served by tensorflow serving",https://stackoverflow.com/questions/53284674,9702962,Requesting (Additional) Resources
41705377,What is the right initializer one should give to the tf.contrib.layers.convolution2d function in TensorFlow?,"<p>I was reading the documentation for <a href=""https://www.tensorflow.org/api_docs/python/contrib.layers/higher_level_ops_for_building_neural_network_layers_#convolution2d"" rel=""nofollow noreferrer"">making 2d convolutional layers</a> in tensorflow from the contrib section and was wondering what was the right or best way to initialize the weights when using the <a href=""https://www.tensorflow.org/api_docs/python/contrib.layers/higher_level_ops_for_building_neural_network_layers_#convolution2d"" rel=""nofollow noreferrer"">tf.contrib.layers.convolution2d</a> function. Unfortunately they don't really say explicitly nor provide an example, so it was unclear to me what is the intended way to use this. The function has a <code>weights_initializer</code> parameter which can be set. I have tried setting it to both:</p>

<ol>
<li>tf.contrib.layers.xavier_initializer</li>
<li>tf.contrib.layers.xavier_initializer_conv2d</li>
</ol>

<p>neither seem to return an error and the first one seems to train fine (as far as I can tell). However, it would be awesome to check if this is the right way of using this contrib layer (or maybe since it seems to be a contrib function, how does one check the ""official"" source code maybe to see their docs or test cases or maybe address the my question in their gitissues, if appropriate).</p>
","I was reading the documentation for making 2d convolutional layers in tensorflow from the contrib section and was wondering what was the right or best way to initialize the weights when using the tf.contrib.layers.convolution2d function. Unfortunately they don't really say explicitly nor provide an example, so it was unclear to me what is the intended way to use this. The function has a weights_initializer parameter which can be set. I have tried setting it to both: neither seem to return an error and the first one seems to train fine (as far as I can tell). However, it would be awesome to check if this is the right way of using this contrib layer (or maybe since it seems to be a contrib function, how does one check the ""official"" source code maybe to see their docs or test cases or maybe address the my question in their gitissues, if appropriate).",https://stackoverflow.com/questions/41705377,1601580,Inadequate Examples
45127850,Questions on the use of tf.contrib.layers.embedding_column in tensorflow,"<p>I have some questions on the use of the embedding columns implemented in tensorflow in tf.contrib.layers.embedding_column.</p>

<p>I'm training a binary classifier for a two player game (tennis, in this case). My features that I pass to the DNNClassifier as feature_columns look as follows:</p>

<pre><code>deep_columns = [
tf.contrib.layers.embedding_column(player1, dimension=9),
tf.contrib.layers.embedding_column(player2, dimension=9),
tf.contrib.layers.embedding_column(court, dimension=1),
tf.contrib.layers.embedding_column(surface, dimension=1),
p1Rank, p2Rank]
</code></pre>

<p>What I'm wondering about is this: am I now learning two different embeddings for the same set of players? And if so, is there a way to use one embedding layer for both players? Or is there nothing wrong with doing it the way I'm currently doing it?</p>

<p>A second question regarding the embedding_column: the docs mention this, as possible arguments for embedding_column:</p>

<pre><code>ckpt_to_load_from: (Optional). String representing checkpoint name/pattern to restore the column weights. Required if tensor_name_in_ckpt is not None.
tensor_name_in_ckpt: (Optional). Name of the Tensor in the provided checkpoint from which to restore the column weights. Required if ckpt_to_load_from is not None.
</code></pre>

<p>Does this imply that if none of these is provided, the embedding layers are initialized randomly again when restoring my model from a checkpoint?</p>

<p>And then one final question: The two embedding columns for <code>court</code> and <code>surface</code> have a dimension of 1, as they only have very few options. Is this a bad use of an embedding column? Or is it okay to use it like that?</p>

<p>Thanks in advance!</p>
","I have some questions on the use of the embedding columns implemented in tensorflow in tf.contrib.layers.embedding_column. I'm training a binary classifier for a two player game (tennis, in this case). My features that I pass to the DNNClassifier as feature_columns look as follows: What I'm wondering about is this: am I now learning two different embeddings for the same set of players? And if so, is there a way to use one embedding layer for both players? Or is there nothing wrong with doing it the way I'm currently doing it? A second question regarding the embedding_column: the docs mention this, as possible arguments for embedding_column: Does this imply that if none of these is provided, the embedding layers are initialized randomly again when restoring my model from a checkpoint? And then one final question: The two embedding columns for court and surface have a dimension of 1, as they only have very few options. Is this a bad use of an embedding column? Or is it okay to use it like that? Thanks in advance!",https://stackoverflow.com/questions/45127850,2903625,Requesting (Additional) Resources
53149059,Why do these two fc api act differently?,"<p>I was implementing the resnet and training on cifar-10 dataset. After ""global average pooling"" layer, I add 10-way fully connected layer. I used two tensorflow API to implement fc layer. However, the method 2 works to me, but the method 1 doesn't work. The method 1 is according to tensorflow official api: <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected"" rel=""nofollow noreferrer"">tf.contrib.layers.fully_connected</a>. The second method is according to <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/dense"" rel=""nofollow noreferrer"">tf.layers.dense</a>. I'm so confused why the first method doesn't work here. FYI, the output of global average pooling layer is [batch_size, 1, 1, 64]. Looking for some help! Thanks in advance!</p>

<p>Edit: Doesn't work means if I use the first method, all output logit of fc become zero, which doesn't make any sense to me. </p>

<pre><code># Method 1: 
def fc(x, n_outputs, scope):
  with tf.variable_scope(scope):
    # x = tf.layers.flatten(x)
    x = tf.contrib.layers.fully_connected(x, num_outputs=n_outputs, weights_initializer=weight_init, weights_regularizer=weight_reg, scope=scope)
    return x

# Method 2:
def fc(x, n_outputs, scope):
  with tf.variable_scope(scope):
     x = tf.layers.flatten(x)
     x = tf.layers.dense(x, units=n_outputs, kernel_initializer=weight_init, kernel_regularizer=weight_reg)
     return x
</code></pre>
","I was implementing the resnet and training on cifar-10 dataset. After ""global average pooling"" layer, I add 10-way fully connected layer. I used two tensorflow API to implement fc layer. However, the method 2 works to me, but the method 1 doesn't work. The method 1 is according to tensorflow official api: tf.contrib.layers.fully_connected. The second method is according to tf.layers.dense. I'm so confused why the first method doesn't work here. FYI, the output of global average pooling layer is [batch_size, 1, 1, 64]. Looking for some help! Thanks in advance! Edit: Doesn't work means if I use the first method, all output logit of fc become zero, which doesn't make any sense to me.",https://stackoverflow.com/questions/53149059,7212365,Documentation Ambiguity
49890477,Log accuracy metric while training a tf.estimator,"<p>What's the simplest way to print accuracy metrics along with the loss when training a pre-canned estimator?</p>

<p>Most tutorials and documentations seem to address the issue of when you're creating a custom estimator -- which seems overkill if the intention is to use one of the available ones.</p>

<p>tf.contrib.learn had a few (now deprecated) Monitor hooks. TF now suggests using the hook API, but it appears that it doesn't actually come with anything that can utilize the labels and predictions to generate an accuracy number. </p>
","What's the simplest way to print accuracy metrics along with the loss when training a pre-canned estimator? Most tutorials and documentations seem to address the issue of when you're creating a custom estimator -- which seems overkill if the intention is to use one of the available ones. tf.contrib.learn had a few (now deprecated) Monitor hooks. TF now suggests using the hook API, but it appears that it doesn't actually come with anything that can utilize the labels and predictions to generate an accuracy number.",https://stackoverflow.com/questions/49890477,98975,Lack of Alternative Solutions/Documentation
45155864,Tensorflow LinearRegressor not converging,"<p>I'm attempting to do a toy linear regression in Python with TensorFlow, using the pre-built estimator tf.contrib.learn.LinearRegressor instead of building my own estimator. 
The inputs I'm using are real-valued numbers between 0 and 1, and the outputs are just 3*inputs. TensorFlow seems to fit the data (no errors raised), but the outputs have no correlation to what they should be.</p>

<p>I'm not sure I'm getting the predictions done correctly- the documentation for the predict() function is pretty sparse.</p>

<p>Any ideas for how to improve the fitting?</p>

<pre><code>import numpy as np
import pandas as pd
import tensorflow as tf
import itertools
import matplotlib.pyplot as plt

#Defining data set
x = np.random.rand(200)
y = 3.0*x
data = pd.DataFrame({'X':x, 'Y':y})
training_data = data[50:]
test_data= data[:50]

COLUMNS = ['Y','X']
FEATURES = ['X']
LABELS = 'Y'

#Wrapper function for the inputs of LinearRegressor
def get_input_fn(data_set, num_epochs=None, shuffle=True):
  return tf.estimator.inputs.pandas_input_fn(
      x=pd.DataFrame(data_set[FEATURES]),
      y=pd.Series(data_set[LABELS]),
      num_epochs=num_epochs,
      shuffle=shuffle)


feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]
regressor = tf.contrib.learn.LinearRegressor(feature_columns=feature_cols)
regressor.fit(input_fn=get_input_fn(test_data), steps=100)

results = regressor.predict(input_fn=get_input_fn(test_data, 
num_epochs=1))
predictions = list(itertools.islice(results, 50))

#Visualizing the results
fig = plt.figure(figsize=[8,8])
ax = fig.add_subplot(111)
ax.scatter(test_data[LABELS], predictions)

ax.set_xlabel('Actual')
ax.set_ylabel('Predicted')
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/IGI8o.png"" rel=""nofollow noreferrer"">Scatter plot of results</a></p>
","I'm attempting to do a toy linear regression in Python with TensorFlow, using the pre-built estimator tf.contrib.learn.LinearRegressor instead of building my own estimator. The inputs I'm using are real-valued numbers between 0 and 1, and the outputs are just 3*inputs. TensorFlow seems to fit the data (no errors raised), but the outputs have no correlation to what they should be. I'm not sure I'm getting the predictions done correctly- the documentation for the predict() function is pretty sparse. Any ideas for how to improve the fitting? Scatter plot of results",https://stackoverflow.com/questions/45155864,1552018,Requesting (Additional) Resources
46821855,Got error when setting read_batch_size in tf.contrib.learn.read_batch_examples. default is ok,"<p>I modified code of the wide &amp; deep tutorial for reading large input from file using tf.contrib.learn.read_batch_examples. For speeding up the training process, I set the read_batch_size and got an error <strong>ValueError: All shapes must be fully defined: [TensorShape([]), TensorShape([Dimension(None)])]</strong>
My piece of code：</p>

<pre><code>def input_fn_pre(batch_size, filename):
  examples_op = tf.contrib.learn.read_batch_examples(
    filename,
    batch_size=5000,
    reader=tf.TextLineReader,
    num_epochs=5,
    num_threads=5,
    read_batch_size=2500,
    parse_fn=lambda x: tf.decode_csv(x, [tf.constant(['0'], dtype=tf.string)] * len(COLUMNS) * 2500, use_quote_delim=False))                                  
  examples_dict = {}

  for i, col in enumerate(COLUMNS):
    examples_dict[col] = examples_op[:, i]
  feature_cols = {k: tf.string_to_number(examples_dict[k], out_type=tf.float32) for k in CONTINUOUS_COLUMNS}
  feature_cols.update({k: dense_to_sparse(examples_dict[k]) for k in CATEGORICAL_COLUMNS})
  label = tf.string_to_number(examples_dict[LABEL_COLUMN], out_type=tf.int32)
  return feature_cols, label
</code></pre>

<p>while using the default parameter setting is ok:</p>

<pre><code>def input_fn_pre(batch_size, filename):
  examples_op = tf.contrib.learn.read_batch_examples(
    filename,
    batch_size=5000,
    reader=tf.TextLineReader,
    num_epochs=5,
    num_threads=5,
    parse_fn=lambda x: tf.decode_csv(x, [tf.constant(['0'], dtype=tf.string)] * len(COLUMNS), use_quote_delim=False))                                  
  examples_dict = {}

  for i, col in enumerate(COLUMNS):
    examples_dict[col] = examples_op[:, i]
  feature_cols = {k: tf.string_to_number(examples_dict[k], out_type=tf.float32) for k in CONTINUOUS_COLUMNS}
  feature_cols.update({k: dense_to_sparse(examples_dict[k]) for k in CATEGORICAL_COLUMNS})
  label = tf.string_to_number(examples_dict[LABEL_COLUMN], out_type=tf.int32)
  return feature_cols, label
</code></pre>

<p>There is not enough explanation in the tensorflow doc.</p>
","I modified code of the wide &amp; deep tutorial for reading large input from file using tf.contrib.learn.read_batch_examples. For speeding up the training process, I set the read_batch_size and got an error ValueError: All shapes must be fully defined: [TensorShape([]), TensorShape([Dimension(None)])] My piece of code： while using the default parameter setting is ok: There is not enough explanation in the tensorflow doc.",https://stackoverflow.com/questions/46821855,3004058,Documentation Completeness
41714318,Input parameters of tf.contrib.learn.read_batch_features,"<p>I am working through these tensorflow <a href=""https://github.com/dennybritz/chatbot-retrieval/tree/master/models%20&#39;codes&#39;"" rel=""nofollow noreferrer"">codes</a> which implement a LSTM in tensorflow. While going through the codes, I came across this function (in <a href=""https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_inputs.py"" rel=""nofollow noreferrer"">input_fn code</a> - line 38) <code>tf.contrib.learn.read_batch_features</code>. I looked up the documentation of <code>tf.contrib.learn.read_batch_features</code> <a href=""https://www.tensorflow.org/api_docs/python/contrib.learn/input_processing#read_batch_features"" rel=""nofollow noreferrer"">here</a>. This is what I got - </p>

<pre>
file_pattern: List of files or pattern of file paths containing Example records. 
batch_size: An int or scalar Tensor specifying the batch size to use.
features: A dict mapping feature keys to FixedLenFeature or VarLenFeature values.
randomize_input: Whether the input should be randomized.
num_epochs: Integer specifying the number of times to read through the dataset. If None, cycles through the dataset forever. NOTE - If specified, creates a variable that must be initialized, so call tf.local_variables_initializer() as shown in the tests.
queue_capacity: Capacity for input queue.
reader_num_threads: The number of threads to read examples.
name: Name of resulting op.
</pre>

<p>There are few input parameters that I am not able to understand and was hoping someone could help me with it.</p>

<ol>
<li><p>The <code>randomize_input</code> parameter. Does it mean it will shuffle the entire dataset?</p></li>
<li><p>For <code>num_epochs</code>, if I specify <code>None</code> does it mean that my <code>input_fn</code> will keep feeding to the <code>model_fn</code>. In that case the training wouldn't stop. This doesn't make sense to me. I guess I'm going wrong somewhere here.</p></li>
<li><p><code>queue_capacity</code> I am not sure what this means</p></li>
</ol>

<p>Would appreciate any help around these questions. Thanks in advance!</p>
","I am working through these tensorflow codes which implement a LSTM in tensorflow. While going through the codes, I came across this function (in input_fn code - line 38) tf.contrib.learn.read_batch_features. I looked up the documentation of tf.contrib.learn.read_batch_features here. This is what I got - There are few input parameters that I am not able to understand and was hoping someone could help me with it. Would appreciate any help around these questions. Thanks in advance!",https://stackoverflow.com/questions/41714318,2324298,Documentation Completeness
46338074,Tensorflow seq2seq,"<p>The api doc of tensorflow's tf.contrib.legacy_seq2seq.basic_rnn_seq2seq says:</p>

<pre><code>(outputs, states) = basic_rnn_seq2seq(
    encoder_inputs,
    decoder_inputs,
    cell,
    dtype=tf.float32,
    scope=None
)
</code></pre>

<p>where, <br>
encoder_inputs: A list of 2D Tensors [batch_size x input_size]. <br>
decoder_inputs: A list of 2D Tensors [batch_size x input_size].</p>

<p>Question 1: Why are the sequence sizes (input_size) same for both the encoder_inputs and decoder_inputs. In my case, there are seq - to - seq mappings for unequal lengthts. For instance, ""What are you doing"" (length 4) -> ""wie gehts (length 2)"". Is it necessary to pad and make the sequences of equal lengths while using this tensorflow module?</p>

<p>Question 2: If I use the </p>

<pre><code>cell = tf.nn.rnn_cell.LSTMCell(num_units = 128)
</code></pre>

<p>The outputs come out to be of the size (batch_size x 128 [num of hidden units in lstm]) which really confuses me. Shouldn't it be equal to (batch_size x output_size)? What am I missing here? I am really confused how the decoder works here.</p>
","The api doc of tensorflow's tf.contrib.legacy_seq2seq.basic_rnn_seq2seq says: where, encoder_inputs: A list of 2D Tensors [batch_size x input_size]. decoder_inputs: A list of 2D Tensors [batch_size x input_size]. Question 1: Why are the sequence sizes (input_size) same for both the encoder_inputs and decoder_inputs. In my case, there are seq - to - seq mappings for unequal lengthts. For instance, ""What are you doing"" (length 4) -&gt; ""wie gehts (length 2)"". Is it necessary to pad and make the sequences of equal lengths while using this tensorflow module? Question 2: If I use the The outputs come out to be of the size (batch_size x 128 [num of hidden units in lstm]) which really confuses me. Shouldn't it be equal to (batch_size x output_size)? What am I missing here? I am really confused how the decoder works here.",https://stackoverflow.com/questions/46338074,4341842,Documentation Ambiguity
43752099,Predictions for tf.contrib.metrics.streaming_auc distributed uniformly?,"<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/metrics/streaming_auc"" rel=""nofollow noreferrer"">TensorFlow documentation for tf.contrib.metrics.streaming_auc</a> mentions the following:</p>

<blockquote>
  <p>For best results, predictions should be distributed approximately
  uniformly in the range [0, 1] and not peaked around 0 or 1. The
  quality of the AUC approximation may be poor if this is not the case.</p>
</blockquote>

<p>I am a bit confused because I feel like a common paradigm is to have <code>predictions</code> be compared against a target 1-hot encoding:</p>

<pre><code>logits = tf.contrib.layers.fully_connected(
    last_fully_connected_layer,
    num_outputs=2,
    activation_fn=None)
loss = tf.losses.softmax_cross_entropy(target, logits)

# I believe this yields a 3-vector with all 0s but a 1 at 1 single position.
predictions = tf.argmax(logits, 1)
</code></pre>

<p>In those cases, the <code>predictions</code> tensor contains all 0s or 1s.</p>

<p>Should we avoid using <code>tf.contrib.metrics.streaming_auc</code> in those cases? I am not sure in what cases we would use <code>tf.contrib.metrics.streaming_auc</code> then.</p>
","The TensorFlow documentation for tf.contrib.metrics.streaming_auc mentions the following: I am a bit confused because I feel like a common paradigm is to have predictions be compared against a target 1-hot encoding: In those cases, the predictions tensor contains all 0s or 1s. Should we avoid using tf.contrib.metrics.streaming_auc in those cases? I am not sure in what cases we would use tf.contrib.metrics.streaming_auc then.",https://stackoverflow.com/questions/43752099,1276460,Documentation Ambiguity
43929037,What is the difference between LSTMBlockCell and BasicLSTMCell in Tensorflow contrib.rnn,"<p>I know that LSTMBlockCell is efficient to initialize at the begining of training. The official API guides of Tensorflow said that LSTMBlockCell add a forgot_bias. Can I just replace the BasicLSTMCell with LSTMBlockCell in my RNN models? And there are too many stuffs in tf.contrib.rnn, I feel that those APIs are really inconsistent.</p>
","I know that LSTMBlockCell is efficient to initialize at the begining of training. The official API guides of Tensorflow said that LSTMBlockCell add a forgot_bias. Can I just replace the BasicLSTMCell with LSTMBlockCell in my RNN models? And there are too many stuffs in tf.contrib.rnn, I feel that those APIs are really inconsistent.",https://stackoverflow.com/questions/43929037,8000679,Documentation Ambiguity
42470319,Output of Tensorflow LSTM-Cell,"<p>I've got a question on Tensorflow LSTM-Implementation. There are currently several implementations in TF, but I use:</p>
<pre class=""lang-py prettyprint-override""><code>cell = tf.contrib.rnn.BasicLSTMCell(n_units)
</code></pre>
<ul>
<li>where n_units is the amount of 'parallel' LSTM-Cells.</li>
</ul>
<p>Then to get my output I call:</p>
<pre class=""lang-py prettyprint-override""><code> rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, x,
                        initial_state=initial_state, time_major=False)
</code></pre>
<ul>
<li>where (as <code>time_major=False</code>) <code>x</code> is of shape <code>(batch_size, time_steps, input_length)</code></li>
<li>where <code>batch_size</code> is my batch_size</li>
<li>where <code>time_steps</code> is the amount of timesteps my RNN will go through</li>
<li>where <code>input_length</code> is the length of one of my input vectors (vector fed into the network on one specific timestep on one specific batch)</li>
</ul>
<p>I expect rnn_outputs to be of shape <code>(batch_size, time_steps, n_units, input_length)</code> as I have not specified another output size.
Documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer""><code>nn.dynamic_rnn</code></a> tells me that output is of shape <code>(batch_size, input_length, cell.output_size)</code>.
The documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell"" rel=""nofollow noreferrer""><code>tf.contrib.rnn.BasicLSTMCell</code></a> does have a property <code>output_size</code>, which is defaulted to n_units (the amount of LSTM-cells I use).</p>
<p>So does each LSTM-Cell only output a scalar for every given timestep? I would expect it to output a vector of the length of the input vector. This seems not to be the case from how I understand it right now, so I am confused. Can you tell me whether that's the case or how I could change it to output a vector of size of the input vector per single lstm-cell maybe?</p>
","I've got a question on Tensorflow LSTM-Implementation. There are currently several implementations in TF, but I use: Then to get my output I call: I expect rnn_outputs to be of shape (batch_size, time_steps, n_units, input_length) as I have not specified another output size. Documentation of nn.dynamic_rnn tells me that output is of shape (batch_size, input_length, cell.output_size). The documentation of tf.contrib.rnn.BasicLSTMCell does have a property output_size, which is defaulted to n_units (the amount of LSTM-cells I use). So does each LSTM-Cell only output a scalar for every given timestep? I would expect it to output a vector of the length of the input vector. This seems not to be the case from how I understand it right now, so I am confused. Can you tell me whether that's the case or how I could change it to output a vector of size of the input vector per single lstm-cell maybe?",https://stackoverflow.com/questions/42470319,6917400,Documentation Ambiguity
46455648,Tensorflow seq2seq Decoder problems?,"<p>I try to write a seq2seq decoder with the tensorflow tf.contrib.seq2seq package. 
I am wondering if my code is correct and if there is better way to rewrite it. The documentation is not easy to read. </p>

<p>Or my question can be: how can I easily debug this kind of code? How can I inspect some intermediate results in tensorflow?</p>

<pre><code>class Decoder:
    def __init__(self, embedding, hidden_size, num_layers=1, max_length=15):
        self.embedding = embedding
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.cell = tf.nn.rnn_cell.GRUCell(hidden_size)  
        self.linear = tf.Variable(tf.random_normal(shape=(self.hidden_size, cn_total_words))*0.1)


    def __call__(self, inputs, state, encoder_outputs, encoder_state, decoder_length, mode=""train""):


        with tf.variable_scope(""decoder"") as scope:

            inputs = tf.nn.embedding_lookup(self.embedding, inputs)
            encoder_state = tf.tile(tf.expand_dims(encoder_state, 1), (1, tf.shape(inputs)[1], 1))

            attention_mechanism = tf.contrib.seq2seq.LuongAttention(self.hidden_size, encoder_outputs)
            attn_cell = tf.contrib.seq2seq.AttentionWrapper(self.cell, attention_mechanism, self.hidden_size)
            if mode == ""train"":
                helper = tf.contrib.seq2seq.TrainingHelper(inputs=inputs, sequence_length=decoder_length)
            elif mode == ""infer"":
                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=self.embedding, 
                                                start_tokens=tf.tile([en_dict[""BOS""]], [tf.shape(inputs)[0]]), end_token=en_dict[""EOS""])

            decoder = tf.contrib.seq2seq.BasicDecoder(cell=attn_cell, helper=helper, 
                                                      initial_state=attn_cell.zero_state(tf.shape(inputs)[0], tf.float32))

            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder)
            outputs = tf.concat([tf.expand_dims(out, 1) for out in outputs], 1)

            outputs = tf.tensordot(outputs, self.linear, axes=[[2], [0]])
            return outputs, state
</code></pre>

<p>I got the following error when running the code</p>

<blockquote>
  <p>--------------------------------------------------------------------------- ValueError                                Traceback (most recent call
  last)
  ~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py
  in apply_op(self, op_type_name, name, **keywords)
      434                 preferred_dtype=default_dtype,
  --> 435                 as_ref=input_arg.is_ref)
      436             if input_arg.number_attr and len(</p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py
  in internal_convert_n_to_tensor(values, dtype, name, as_ref,
  preferred_dtype)
      736             as_ref=as_ref,
  --> 737             preferred_dtype=preferred_dtype))
      738   return ret</p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py
  in internal_convert_to_tensor(value, dtype, name, as_ref,
  preferred_dtype)
      675         if ret is None:
  --> 676           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
      677 </p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py
  in _TensorTensorConversionFunction(t, dtype, name, as_ref)
      548         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r""
  --> 549         % (dtype.name, t.dtype.name, str(t)))
      550   return t</p>
  
  <p>ValueError: Tensor conversion requested dtype float32 for Tensor with
  dtype int32: 'Tensor(""seq2seq-train/decoder/ExpandDims_2:0"", shape=(?,
  1, ?), dtype=int32)'</p>
  
  <p>During handling of the above exception, another exception occurred:</p>
  
  <p>TypeError                                 Traceback (most recent call
  last)  in ()
        4 emb_en = np.random.uniform(low=-0.1, high=0.1, size=(en_total_words, hidden_size))
        5 emb_cn = np.random.uniform(low=-0.1, high=0.1, size=(cn_total_words, hidden_size))
  ----> 6 model = Seq2Seq(hidden_size, num_layers, emb_en, emb_cn)
        7 sess = tf.Session()
        8 init = tf.global_variables_initializer()</p>
  
  <p> in <strong>init</strong>(self, hidden_size,
  num_layers, embed_words_en, embed_words_cn)
       81             encoder_outputs, encoder_state = self.encoder(self.encoder_inputs, self.encoder_length)
       82             decoder_length = tf.cast(tf.reduce_sum(self.decoder_mask, 1), tf.int32)
  ---> 83             decoder_outputs, decoder_state = self.decoder(self.decoder_inputs, encoder_state, encoder_outputs,
  encoder_state, decoder_length)
       84 
       85             # decoder_outputs.append(decoder_out)</p>
  
  <p> in <strong>call</strong>(self, inputs, state,
  encoder_outputs, encoder_state, decoder_length, mode)
       50 
       51             outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder)
  ---> 52             outputs = tf.concat([tf.expand_dims(out, 1) for out in outputs], 1)
       53 
       54             outputs = tf.tensordot(outputs, self.linear, axes=[[2], [0]])</p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py
  in concat(values, axis, name)    1064   return
  gen_array_ops._concat_v2(values=values,    1065<br>
  axis=axis,
  -> 1066                                   name=name)    1067     1068 </p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py
  in _concat_v2(values, axis, name)
      491   """"""
      492   result = _op_def_lib.apply_op(""ConcatV2"", values=values, axis=axis,
  --> 493                                 name=name)
      494   return result
      495 </p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py
  in apply_op(self, op_type_name, name, **keywords)
      461                                 (prefix, dtype.name))
      462               else:
  --> 463                 raise TypeError(""%s that don't all match."" % prefix)
      464             else:
      465               raise TypeError(""%s that are invalid."" % prefix)</p>
  
  <p>TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have
  types [float32, int32] that don't all match.</p>
</blockquote>
",I try to write a seq2seq decoder with the tensorflow tf.contrib.seq2seq package. I am wondering if my code is correct and if there is better way to rewrite it. The documentation is not easy to read. Or my question can be: how can I easily debug this kind of code? How can I inspect some intermediate results in tensorflow? I got the following error when running the code,https://stackoverflow.com/questions/46455648,5942350,Documentation Ambiguity
42608245,How to use tf.contrib.seq2seq.simple_decoder_fn_inference API,"<p>I did not understand the parameters that needed to be passed into the API call to   <code>tf.contrib.seq2seq.simple_decoder_fn_inference</code> in Tensorflow 1.0
for building the inference block for a Seq2Seq Attention mechanism RNN.</p>

<p>Can someone explain, in detail, what each parameter of this function call means and is supposed to do?</p>

<p>The link to the documentation is here :
<a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/attention_decoder_fn_inference"" rel=""nofollow noreferrer"">tf.contrib.seq2seq.attention_decoder_fn_inference()</a></p>
","I did not understand the parameters that needed to be passed into the API call to tf.contrib.seq2seq.simple_decoder_fn_inference in Tensorflow 1.0 for building the inference block for a Seq2Seq Attention mechanism RNN. Can someone explain, in detail, what each parameter of this function call means and is supposed to do? The link to the documentation is here : tf.contrib.seq2seq.attention_decoder_fn_inference()",https://stackoverflow.com/questions/42608245,7661539,Documentation Ambiguity
53000921,Tensorflow Sequence to Sequence CustomHelper,"<p>There are limited amount of documentation on Sequence to Sequence CustomHelper </p>

<p><code>helper = tf.contrib.seq2seq.CustomHelper(initialize_fn = initialize_fn,sample_fn = sample_fn, next_inputs_fn = next_inputs_fn)</code></p>

<p>in Tensorflow.</p>

<p>Would anyone explain the inputs of the Custom-helper, in terms of the input data</p>

<p><code>X = tf.placeholder(tf.float32, [batch_size x time_steps x features])</code></p>

<p>and encoder, </p>

<p><code>encoder_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)</code></p>

<p><code>initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)</code></p>

<p>and/or possibly </p>

<p><code>rnn_output, rnn_states = tf.nn.dynamic_rnn(encoder_cell, X, dtype=tf.float32)</code>
?</p>
","There are limited amount of documentation on Sequence to Sequence CustomHelper helper = tf.contrib.seq2seq.CustomHelper(initialize_fn = initialize_fn,sample_fn = sample_fn, next_inputs_fn = next_inputs_fn) in Tensorflow. Would anyone explain the inputs of the Custom-helper, in terms of the input data X = tf.placeholder(tf.float32, [batch_size x time_steps x features]) and encoder, encoder_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size) initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32) and/or possibly rnn_output, rnn_states = tf.nn.dynamic_rnn(encoder_cell, X, dtype=tf.float32) ?",https://stackoverflow.com/questions/53000921,7644642,Documentation Completeness
46359843,Parameters in tf.contrib.seq2seq.sequence_loss,"<p>I'm trying to use the tf.contrib.seq2seq.sequence_loss function in a RNN model to calculate the loss. 
According to the API document, this function requires at least three parameters: logits, targets and weights</p>

<pre><code>sequence_loss(
    logits,
    targets,
    weights,
    average_across_timesteps=True,
    average_across_batch=True,
    softmax_loss_function=None,
    name=None
)

logits: A Tensor of shape [batch_size, sequence_length, num_decoder_symbols] and dtype float. The logits correspond to the prediction across all classes at each timestep.
targets: A Tensor of shape [batch_size, sequence_length] and dtype int. The target represents the true class at each timestep. 
weights: A Tensor of shape [batch_size, sequence_length] and dtype float. weights constitutes the weighting of each prediction in the sequence. When using weights as masking, set all valid timesteps to 1 and all padded timesteps to 0, e.g. a mask returned by tf.sequence_mask.
average_across_timesteps: If set, sum the cost across the sequence dimension and divide the cost by the total label weight across timesteps.
average_across_batch: If set, sum the cost across the batch dimension and divide the returned cost by the batch size.
softmax_loss_function: Function (labels, logits) -&gt; loss-batch to be used instead of the standard softmax (the default if this is None). Note that to avoid confusion, it is required for the function to accept named arguments.
name: Optional name for this operation, defaults to ""sequence_loss"".
</code></pre>

<p>My understand is logits is my prediction after using Xw+b, so the shape of it should be [batch_size, sequence_length, output size]. Then target should be my label, but the shape required in is [batch_size, sequence_length]. I suppose my label should have the same shape as the logits. </p>

<p>So how to convert the 3d labels to 2d? Thanks  in  advance </p>
","I'm trying to use the tf.contrib.seq2seq.sequence_loss function in a RNN model to calculate the loss. According to the API document, this function requires at least three parameters: logits, targets and weights My understand is logits is my prediction after using Xw+b, so the shape of it should be [batch_size, sequence_length, output size]. Then target should be my label, but the shape required in is [batch_size, sequence_length]. I suppose my label should have the same shape as the logits. So how to convert the 3d labels to 2d? Thanks in advance",https://stackoverflow.com/questions/46359843,1779012,Requesting (Additional) Resources
40957286,AttributeError: module 'tensorflow.contrib.slim' has no attribute 'nets',"<p>I want to use the built in <code>resnet</code> in <code>tf-slim</code> for a quick experiment. I did according to the <code>README</code> in <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim"" rel=""nofollow noreferrer"">github</a>:</p>

<pre><code>import tensorflow as tf
import tensorflow.contrib.slim as slim

resnet = tf.contrib.slim.nets.resnet_v1

mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
x = tf.placeholder(""float"", shape=[None, 784])
y_ = tf.placeholder(""float"", shape=[None, 10])
pred = resnet.resnet_v1_50(x)

cross_entropy = -tf.reduce_sum(y_ * tf.log(pred))
</code></pre>

<p>But got such an error:<code>AttributeError: module 'tensorflow.contrib.slim' has no attribute 'nets'</code>. I have already installed the latest version of <code>tensorflow-0.12.0</code>.<br>
How can I fix this issue?</p>
",I want to use the built in resnet in tf-slim for a quick experiment. I did according to the README in github: But got such an error:AttributeError: module 'tensorflow.contrib.slim' has no attribute 'nets'. I have already installed the latest version of tensorflow-0.12.0. How can I fix this issue?,https://stackoverflow.com/questions/40957286,5785319,Documentation Replicability
46781847,How periodicaly evaluate the Performance of Models in TF-Slim?,"<p>I am trying to use <a href=""https://github.com/pudae/tensorflow-densenet"" rel=""nofollow noreferrer"">DensNet</a> for regression problem with TF-Slim. My data contains 60000 jpeg images with 37 float labels for each image. I divided my data into three different tfrecords files of a train set (60%), a validation set (20%) and a test set (20%). </p>

<p>I need to evaluate validation set during training loop and make a plot like <a href=""https://i.stack.imgur.com/HzLPq.jpg"" rel=""nofollow noreferrer"">image</a>. 
In TF-Slim documentation they just explain train loop and evaluation loop separately. I can just evaluate validation or test set after training loop finished. While as I said I need to evaluate during training.</p>

<p>I tried to use slim.evaluation.evaluation_loop function instead of slim.evaluation.evaluate_once. But it doesn't help.</p>

<pre><code>slim.evaluation.evaluation_loop(
    master=FLAGS.master,
    checkpoint_dir=checkpoint_path,
    logdir=FLAGS.eval_dir,
    num_evals=num_batches,
    eval_op=list(names_to_updates.values()) + print_ops,
    variables_to_restore=variables_to_restore,
    summary_op = tf.summary.merge(summary_ops),
    eval_interval_secs = eval_interval_secs )
</code></pre>

<p>I tried evaluation.evaluate_repeatedly as well.</p>

<pre><code>from tensorflow.contrib.training.python.training import evaluation

evaluation.evaluate_repeatedly(
    master=FLAGS.master,
    checkpoint_dir=checkpoint_path,
    eval_ops=list(names_to_updates.values()) + print_ops,
    eval_interval_secs = eval_interval_secs )
</code></pre>

<p>In both of these functions, they just read the latest available checkpoint from checkpoint_dir and apparently waiting for the next one, however when the new checkpoints are generated, they don't perform at all.</p>

<p>I use Python 2.7.13 and Tensorflow 1.3.0 on CPU.</p>

<p>Any help will be highly appreciated.</p>
","I am trying to use DensNet for regression problem with TF-Slim. My data contains 60000 jpeg images with 37 float labels for each image. I divided my data into three different tfrecords files of a train set (60%), a validation set (20%) and a test set (20%). I need to evaluate validation set during training loop and make a plot like image. In TF-Slim documentation they just explain train loop and evaluation loop separately. I can just evaluate validation or test set after training loop finished. While as I said I need to evaluate during training. I tried to use slim.evaluation.evaluation_loop function instead of slim.evaluation.evaluate_once. But it doesn't help. I tried evaluation.evaluate_repeatedly as well. In both of these functions, they just read the latest available checkpoint from checkpoint_dir and apparently waiting for the next one, however when the new checkpoints are generated, they don't perform at all. I use Python 2.7.13 and Tensorflow 1.3.0 on CPU. Any help will be highly appreciated.",https://stackoverflow.com/questions/46781847,8161718,Inadequate Examples
56092824,TF 2.0: Where can I find the upgrade of tf.contrib.training?,"<p>I want to use the HParams class from <strong>tf.contrib.training</strong> in tensorflow 2.0 version, but I can't find the replacement for this class neither in <em>tensorflow alpha</em> documentation nor in <em>tensorflow_addons</em></p>
","I want to use the HParams class from tf.contrib.training in tensorflow 2.0 version, but I can't find the replacement for this class neither in tensorflow alpha documentation nor in tensorflow_addons",https://stackoverflow.com/questions/56092824,6396977,Lack of Alternative Solutions/Documentation
47035862,How does tensorflow's tf.contrib.training.batch_sequences_with_states API work?,"<p>I am dealing with long sequential data which has to be passed to an RNN. To do truncated BPTT and batching, seems like there are two options: </p>

<ol>
<li>Create a batch by combining <em>respective</em> segments from different sequences. Preserve final state of each sequence in a batch and pass it on to next batch.</li>
<li>Consider each sequence as a mini-batch with segments from the sequence becoming members of the batch. Preserve the state of the last time step in one segment and pass it on to the first time step of the next segment.</li>
</ol>

<p>I came across <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/training/batch_sequences_with_states"" rel=""nofollow noreferrer""><code>tf.contrib.training.batch_sequences_with_states</code></a> which seems to be doing one of the two. The documentation is confusing to me and hence I want to be certain which way does it generate the batches. </p>

<p>My guess is it does it the first way.  That’s because, if the batching is being done the second way, then we cannot leverage the benefits of vectorization, since, to preserve the state between the last time step of one segement  to the first time step of the next segment, RNN should process one token at a time sequentially. </p>

<p>Question:</p>

<p>Which of these two batching strategies are implemented in <code>tf.contrib.training.batch_sequences_with_states</code>?</p>
","I am dealing with long sequential data which has to be passed to an RNN. To do truncated BPTT and batching, seems like there are two options: I came across tf.contrib.training.batch_sequences_with_states which seems to be doing one of the two. The documentation is confusing to me and hence I want to be certain which way does it generate the batches. My guess is it does it the first way. That’s because, if the batching is being done the second way, then we cannot leverage the benefits of vectorization, since, to preserve the state between the last time step of one segement to the first time step of the next segment, RNN should process one token at a time sequentially. Question: Which of these two batching strategies are implemented in tf.contrib.training.batch_sequences_with_states?",https://stackoverflow.com/questions/47035862,3001665,Documentation Ambiguity
47432870,Can tf.contrib.training.batch_sequences_with_states handle input sequences with variable lengths?,"<p>I was trying to use <code>tf.contrib.training.batch_sequences_with_states</code> to create padded batches of variable length input sequences in order to train an LSTM network.</p>
<p>While reading the documentation I stumbled upon contradicting statements, concerning the capabilities of this function.</p>
<p>Specifically the parameter <code>input_sequences</code> is confusing to me.</p>
<blockquote>
<p><code>input_sequence</code> is a dict with values that are tensors with time as first dimension. This time dimension must be the same across those tensors of an example. <strong>It can vary across examples.</strong></p>
<p><code>input_sequences</code>: A dict mapping string names to Tensor values. <strong>The values must all have matching first dimension</strong>, called value_length. They may vary from input to input</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/training/batch_sequences_with_states"" rel=""nofollow noreferrer"">Source</a></p>
</blockquote>
<p>How do I supply this function with multiple examples?</p>
<h2>My first attempt</h2>
<pre><code>import tensorflow as tf
import numpy as np

batch_size = 10
num_unroll = 4
num_enqueue_threads = 2
lstm_size = 8

seq1 = np.random.rand(8, 8, 2)
seq2 = np.random.rand(16, 8, 2)

sequences = {&quot;seq1&quot;: seq1, &quot;seq2&quot;: seq2}
context = {&quot;seq1&quot;: 0, &quot;seq2&quot;: 1}

initial_states = {&quot;c&quot;: tf.zeros((lstm_size,), dtype=tf.float32),
                  &quot;h&quot;: tf.zeros((lstm_size,), dtype=tf.float32)}

batch = tf.contrib.training.batch_sequences_with_states(
    input_key=&quot;key&quot;,
    input_sequences=sequences,
    input_context=context,
    input_length=None,
    initial_states=initial_states,
    num_unroll=num_unroll,
    batch_size=batch_size,
    num_threads=num_enqueue_threads,
    capacity=batch_size * num_enqueue_threads * 2,
    make_keys_unique=True
)

inputs = batch.sequences[&quot;seq1&quot;]

with tf.Session() as sess:
    tf.train.start_queue_runners()
    print(sess.run([inputs]))
</code></pre>
<p>Executing this code snippet leads to the following error message:</p>
<pre><code>ERROR:tensorflow:Exception in QueueRunner: assertion failed: 
[All sequence lengths must match, but received lengths: 8 
 All sequence lengths must match, but received lengths: 16]
</code></pre>
<p>An example showing how to correctly supply this function with multiple examples of varying input_sequence lengths would be really helpful!</p>
","I was trying to use tf.contrib.training.batch_sequences_with_states to create padded batches of variable length input sequences in order to train an LSTM network. While reading the documentation I stumbled upon contradicting statements, concerning the capabilities of this function. Specifically the parameter input_sequences is confusing to me. How do I supply this function with multiple examples? Executing this code snippet leads to the following error message: An example showing how to correctly supply this function with multiple examples of varying input_sequence lengths would be really helpful!",https://stackoverflow.com/questions/47432870,5970048,Documentation Ambiguity
49165112,How to use the old value and the new value of a Variable in Tensorflow?,"<p>I want to use the old value and the new value of a Variable. But I am
confused about when the <code>Assign Op</code> is applied.</p>
<p>Here is a simple example. The outputs of <code>output</code> and <code>output2</code> are different.</p>
<pre><code>v = tf.Variable(0)
with tf.Session() as sess:
    sess.run(v.initializer)

    new_v = v.assign(v + 10)
    output = v + 0 # `v` is evaluated before the assignment ?
    output2 = v  # `v` is evaluated after the assignment ?

    print(sess.run([ output, output2, new_v])) 
    print(sess.run(output))
</code></pre>
<p>The result is</p>
<blockquote>
<p>[0, 10, 10]</p>
<p>10</p>
</blockquote>
<p>Please advise me what is the proper way to use the old value and the new value of a Variable. Thanks.</p>
<hr />
<p>According to  chrisz's answer, I tried <code>tf.control_dependencies</code> to get the old value of <code>v</code>. But the result is not what I expected. I still need to add <code>0</code> to <code>v</code> to get the old value.</p>
<p>Here is the test code. And I add a <code>0</code> to get the same result as above. Otherwise, the result of <code>output_old</code> will be <code>10</code></p>
<pre><code>v = tf.Variable(0)
with tf.Session() as sess:
    sess.run(v.initializer)
    output_old = v + 0         # If I want the old value, this extra add is needed
    with tf.control_dependencies([output_old]):
        new_v = v.assign(v + 10)
        output_new = new_v

    print(sess.run([output_old, output_new, new_v])) 
    print(sess.run(output_old))
</code></pre>
","I want to use the old value and the new value of a Variable. But I am confused about when the Assign Op is applied. Here is a simple example. The outputs of output and output2 are different. The result is Please advise me what is the proper way to use the old value and the new value of a Variable. Thanks. According to chrisz's answer, I tried tf.control_dependencies to get the old value of v. But the result is not what I expected. I still need to add 0 to v to get the old value. Here is the test code. And I add a 0 to get the same result as above. Otherwise, the result of output_old will be 10",https://stackoverflow.com/questions/49165112,8904948,Requesting (Additional) Resources
51142984,how to calculate the weights for deconvolution layer based on the trained value weights of the corresponding convolution layer,"<p>Is this possible?
The correspoding layer for deconvolution layer is tf.conv2d_transpose(), but the document states it is just a transpose conv layer, not a real deconv.
So how can I calculate the weights for deconv layers?
Like in the following codes, how can I make y == x(by calculating W2 from W)? Is this possible? Or the only way is to <strong>train</strong> the deconv layer?</p>

<pre><code># [batch, height, width, depth]
x_image = tf.placeholder(tf.float32,shape=[3,2])
x = tf.reshape(x_image,[1,3,2,1])

#Filter: W  [kernel_height, kernel_width, output_depth, input_depth]
W_cpu = np.array([[-1,1]],dtype=np.float32)
W = tf.Variable(W_cpu)
W = tf.reshape(W, [1,2,1,1])

W_cpu2 = np.array([[-1,1]],dtype=np.float32)
W2 = tf.Variable(W_cpu2)
W2 = tf.reshape(W2, [1,2,1,1])

strides=[1, 1, 1, 1]
padding='VALID'

z = tf.nn.conv2d(x, W, strides=strides, padding=padding)

y = tf.nn.conv2d_transpose(z, W2, [1,3,2,1],strides, padding)

x_data = np.array([[1,-1],[2,2],[1,2]],dtype=np.float32)
with tf.Session() as sess:
    init = tf.initialize_all_variables()
    sess.run(init)

    x = (sess.run(x, feed_dict={x_image: x_data}))
    W = (sess.run(W, feed_dict={x_image: x_data}))
    z = (sess.run(z, feed_dict={x_image: x_data}))
    y = (sess.run(y, feed_dict={x_image: x_data}))

    print(""The shape of x:\t"", x.shape, "",\t and the x.reshape(3,2) is :"")
    print(x.reshape(3,2))
    print()

    print (""The shape of x:\t"", W.shape, "",\t and the W.reshape(1,2) is :"")
    print (W.reshape(1,2))
    print ("""")


    print (""The shape of z:\t"", W.shape, "",\t and the W.reshape(1,2) is :"")
    print (z.reshape(3))
    print ("""")

    print (""The shape of y:\t"", y.shape, "",\t and the y.reshape(3,3) is :"")
    print (y.reshape(3,2))
    print ("""")
</code></pre>

<p>But in <em>Visualizing and Understanding Convolutional Networks, Matthew D. Zeiler and Rob Fergus</em>, they proposed a way to do deconv with the""transposed
versions of the same filters*, does this mean the shape is transposed or it includes the weight?</p>
","Is this possible? The correspoding layer for deconvolution layer is tf.conv2d_transpose(), but the document states it is just a transpose conv layer, not a real deconv. So how can I calculate the weights for deconv layers? Like in the following codes, how can I make y == x(by calculating W2 from W)? Is this possible? Or the only way is to train the deconv layer? But in Visualizing and Understanding Convolutional Networks, Matthew D. Zeiler and Rob Fergus, they proposed a way to do deconv with the""transposed versions of the same filters*, does this mean the shape is transposed or it includes the weight?",https://stackoverflow.com/questions/51142984,10022303,Documentation Ambiguity
47007997,keras custom loss function Value error on convert to tensor,"<p>I'm trying to implement a custom loss function for keras using tensorflow backend. The idea i'm working with is to provide the ""Cost"" of a ""Signal"" as the y_true and handle the signals as the y_pred. This is basically a complicated classification situation.</p>

<p>As an example lets say I have 3 possible output signals and N samples then I would have a cost matrix that is Nx3 and my predictions would also be Nx3 (So as to not run into problems with y_true and y_pred needing to be the same size.</p>

<p>The way I am selecting my actual signal is by taking the max of the three output values and then the cost for this signal is the value in the cost matrix at the corresponding index. Total cost is the sum of these individual costs.</p>

<p>I have implemented this in tensorflow (I needed to use gather_nd so I couldn't use keras.backend but all the examples I have read online don't seem to care about this fact). The code for my implementation is:</p>

<pre><code>def maxSignalLossTF(costs, signals):

    signalChoices = tf.transpose(tf.stack((tf.to_int64(tf.range(tf.shape(signals)[0])), tf.argmax(signals, axis=1))))

    signalCosts = tf.gather_nd(costs, signalChoices)
    return tf.reduce_sum(signalCosts)
</code></pre>

<p>I have confirmed this function to be working as expected by comparing it to the numpy equivalent. I am passing this loss function to my model when compiling:</p>

<pre><code>model = Sequential()
model.add(Dense(250, input_shape=(trainX.shape[1],)))
model.add(Activation('relu'))
model.add(Dropout(0.1))

model.add(Dense(250))
model.add(Activation('relu'))

model.add(Dense(3))
model.add(Activation('linear'))
model.compile(optimizer='adam', loss=maxSignalLossTF)
</code></pre>

<p>All of this seems to work fine until I try to fit the model with:
    model.fit(TrainX, TrainY, epochs=100)
At which point the console spits out the following unhelpful message</p>

<pre><code>    Traceback (most recent call last):
  File ""tensortest.py"", line 331, in &lt;module&gt;
    model.fit(TrainX, TrainY, epochs=100)
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/keras/models.py"", line 845, in fit
    initial_epoch=initial_epoch)
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 1457, in fit
    self._make_train_function()
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/keras/engine/training.py"", line 1001, in _make_train_function
    self.total_loss)
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/keras/optimizers.py"", line 398, in get_updates
    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 856, in binary_op_wrapper
    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/myname/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 364, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
</code></pre>

<p>I tried to track down where this message was coming from in keras and tensorflow but while I found the functions it did not yield much insight for me as to what I am doing wrong. I have out of desperation also completely implemented to calculation manually using <code>tf.convert_to_tensor()</code> on my input X and Y which had no issues so I'm pretty stumped at this point what is going wrong.</p>

<p>I'm new to Keras and tensorflow and while I have looked at basically every example I could find its very likely I am missing something pretty simple. So any help is greatly appreciated. Thanks in advance</p>

<p>Also if there are pre-made solutions to this type of cost set up I would be happy to know about them. I came up with this idea after tinkering with a few different standard options without much success so I decided to try something that would give me more direct control over incentivizing the various output signals which better capture the underlying problem (Mis classification of some samples is not as important as misclassifying other samples)</p>
","I'm trying to implement a custom loss function for keras using tensorflow backend. The idea i'm working with is to provide the ""Cost"" of a ""Signal"" as the y_true and handle the signals as the y_pred. This is basically a complicated classification situation. As an example lets say I have 3 possible output signals and N samples then I would have a cost matrix that is Nx3 and my predictions would also be Nx3 (So as to not run into problems with y_true and y_pred needing to be the same size. The way I am selecting my actual signal is by taking the max of the three output values and then the cost for this signal is the value in the cost matrix at the corresponding index. Total cost is the sum of these individual costs. I have implemented this in tensorflow (I needed to use gather_nd so I couldn't use keras.backend but all the examples I have read online don't seem to care about this fact). The code for my implementation is: I have confirmed this function to be working as expected by comparing it to the numpy equivalent. I am passing this loss function to my model when compiling: All of this seems to work fine until I try to fit the model with: model.fit(TrainX, TrainY, epochs=100) At which point the console spits out the following unhelpful message I tried to track down where this message was coming from in keras and tensorflow but while I found the functions it did not yield much insight for me as to what I am doing wrong. I have out of desperation also completely implemented to calculation manually using tf.convert_to_tensor() on my input X and Y which had no issues so I'm pretty stumped at this point what is going wrong. I'm new to Keras and tensorflow and while I have looked at basically every example I could find its very likely I am missing something pretty simple. So any help is greatly appreciated. Thanks in advance Also if there are pre-made solutions to this type of cost set up I would be happy to know about them. I came up with this idea after tinkering with a few different standard options without much success so I decided to try something that would give me more direct control over incentivizing the various output signals which better capture the underlying problem (Mis classification of some samples is not as important as misclassifying other samples)",https://stackoverflow.com/questions/47007997,3199524,Documentation Ambiguity
48944611,How can I read binary file in Tensorflow,"<p>I'm trying to read binary file in <strong>tensorflow</strong>.
I want to ask, which method should I use, and how for reading binary file.
In tensorflow, they recommends use dataset in tf.data.
But I can't find simple example of using dataset, especially FixedLengthRecordDataset. I think I should use this method, but I don't know how to use. </p>

<pre><code>[hg file] := [file header] [image1] [image2] [image3] ...

[file header] := ""hg  "" (8 bytes)

[imageN] := [image header] [image data]

[image header] := [code(2 bytes)] [width (1 byte)] [height(1 byte)] [reserved(2 bytes)]

[image data] := 256 gray data (width * height bytes, row-major format)
</code></pre>

<p>This is the format of my binary file.</p>

<p>Please give me some advice for this work.</p>

<p>EDIT : All images have different size. So maybe I can't use FixedLengthRecordDatasest. I think I have to convert all images to same size of dataset</p>
","I'm trying to read binary file in tensorflow. I want to ask, which method should I use, and how for reading binary file. In tensorflow, they recommends use dataset in tf.data. But I can't find simple example of using dataset, especially FixedLengthRecordDataset. I think I should use this method, but I don't know how to use. This is the format of my binary file. Please give me some advice for this work. EDIT : All images have different size. So maybe I can't use FixedLengthRecordDatasest. I think I have to convert all images to same size of dataset",https://stackoverflow.com/questions/48944611,9400682,Inadequate Examples
73825623,Stream data using tf.data from cloud object store other than Google Storage possible?,"<p>I've found <a href=""https://www.tensorflow.org/datasets/gcs"" rel=""nofollow noreferrer"">this</a> nice article on how to directly stream data from Google Storage to tf.data. This is super handy if your compute tier has limited storage (like on KNative in my case) and network bandwidth is sufficient (and free of charge anyway).</p>
<blockquote>
<p>tfds.load(..., try_gcs=True)</p>
</blockquote>
<p>Unfortunately, my data resides in a non Google bucket and it isn't documented for other Cloud Object Store systems.</p>
<p>Does anybody know if it also works in non GS environments?</p>
","I've found this nice article on how to directly stream data from Google Storage to tf.data. This is super handy if your compute tier has limited storage (like on KNative in my case) and network bandwidth is sufficient (and free of charge anyway). Unfortunately, my data resides in a non Google bucket and it isn't documented for other Cloud Object Store systems. Does anybody know if it also works in non GS environments?",https://stackoverflow.com/questions/73825623,3656912,Requesting (Additional) Resources
44742878,Reading CSV files in Tensorflow 1.2.0,"<p>I am trying to read <a href=""https://github.com/chiphuyen/tf-stanford-tutorials/blob/master/data/heart.csv"" rel=""nofollow noreferrer"">heart.csv</a> file data in batches. Following the documentation from <a href=""https://www.tensorflow.org/versions/r0.12/how_tos/reading_data/#reading_from_files"" rel=""nofollow noreferrer"">tensorflow</a> website, I have the following code working to read row by row</p>

<pre><code>import tensorflow as tf
filename_queue = tf.train.string_input_producer([""heart.csv""])
reader = tf.TextLineReader(skip_header_lines=1)
_, csv_row = reader.read(filename_queue)

record_defaults = [[0], [0.0], [0.0], [0.0], [""""], [0], [0.0], [0.0], [0], [0]]
sbp, tobacco, ldl, adiposity, famhist, typea, obesity, alcohol, age, chd = tf.decode_csv(csv_row, record_defaults=record_defaults)
features = [sbp, tobacco, ldl, adiposity, famhist, typea, obesity, alcohol, age]

nof_examples = 10
with tf.Session() as sess:
    tf.global_variables_initializer().run()
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    while nof_examples &gt; 0:
        nof_examples -= 1
        try:
            data_features, data_chd = sess.run([features, chd])
#             data_features[4] = 1 if data_features[4] == 'Present' else 0
            print(data_features, data_chd)
        except tf.errors.OutOfRangeError:
            coord.request_stop()
            coord.join(threads)
            break
    coord.request_stop()
    coord.join(threads)
</code></pre>

<p>Output:</p>

<pre><code>([160, 12.0, 5.73, 23.110001, 'Present', 49, 25.299999, 97.199997, 52], 1)
([144, 0.0099999998, 4.4099998, 28.610001, 'Absent', 55, 28.870001, 2.0599999, 63], 1)
([118, 0.079999998, 3.48, 32.279999, 'Present', 52, 29.139999, 3.8099999, 46], 0)
([170, 7.5, 6.4099998, 38.029999, 'Present', 51, 31.99, 24.26, 58], 1)
([134, 13.6, 3.5, 27.780001, 'Present', 60, 25.99, 57.34, 49], 1)
([132, 6.1999998, 6.4699998, 36.209999, 'Present', 62, 30.77, 14.14, 45], 0)
([142, 4.0500002, 3.3800001, 16.200001, 'Absent', 59, 20.809999, 2.6199999, 38], 0)
([114, 4.0799999, 4.5900002, 14.6, 'Present', 62, 23.110001, 6.7199998, 58], 1)
([114, 0.0, 3.8299999, 19.4, 'Present', 49, 24.860001, 2.49, 29], 0)
([132, 0.0, 5.8000002, 30.959999, 'Present', 69, 30.110001, 0.0, 53], 1)
</code></pre>

<p>But when i try to read in batches as showed in the tensorflow documentation, i get</p>

<pre><code>TypeError: Cannot convert a list containing a tensor of dtype &lt;dtype:
float32'&gt; to &lt;dtype: 'int32'&gt; (Tensor is: &lt;tf.Tensor 'DecodeCSV_6:1'
shape=() dtype=float32&gt;)
</code></pre>

<p>Batch processing code</p>

<pre><code>import tensorflow as tf
batch_size = 1
def read_my_file_format(filename_queue):
    reader = tf.TextLineReader(skip_header_lines=1)
    _, csv_row = reader.read(filename_queue)
    record_defaults = [[0], [0.0], [0.0], [0.0], [""""], [0], [0.0], [0.0], [0], [0]]
    sbp, tobacco, ldl, adiposity, famhist, typea, obesity, alcohol, age, chd = tf.decode_csv(csv_row, record_defaults=record_defaults)
    feature = [sbp, tobacco, ldl, adiposity, famhist, typea, obesity, alcohol, age]
    label = [chd]
    return feature, label

def input_pipeline(filenames, batch_size, num_epochs=None):
    filename_queue = tf.train.string_input_producer(filenames, 
                                                    num_epochs=num_epochs, 
                                                    shuffle=True)
    feature, label = read_my_file_format(filename_queue)
    min_after_dequeue = 10000
    capacity = min_after_dequeue + 3 * batch_size
    feature_batch, label_batch = tf.train.shuffle_batch([feature, label], 
                                                        batch_size=batch_size, 
                                                        capacity=capacity,
                                                        min_after_dequeue=min_after_dequeue)
    return feature_batch, label_batch

features, labels = input_pipeline(['heart.csv'], batch_size)

with tf.Session() as sess:
    tf.global_variables_initializer().run()

    # start populating filename queue
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    try:
        while not coord.should_stop():
            feature_batch, label_batch = sess.run([features, labels])
            print(feature_batch)
    except tf.errors.OutOfRangeError:
        print('Done training, epoch reached')
    finally:
        coord.request_stop()
    coord.join(threads) 
</code></pre>

<p>Reading CSV files using tensorflow seems bit cumbersome but I am sure it has its importance in the library being a distributed system. I found it confusing and took more than 60 mins to read and get a grasp on how the reading feed pipeline worked for csv files. May be documentation should be better and more visuals are needed.</p>
","I am trying to read heart.csv file data in batches. Following the documentation from tensorflow website, I have the following code working to read row by row Output: But when i try to read in batches as showed in the tensorflow documentation, i get Batch processing code Reading CSV files using tensorflow seems bit cumbersome but I am sure it has its importance in the library being a distributed system. I found it confusing and took more than 60 mins to read and get a grasp on how the reading feed pipeline worked for csv files. May be documentation should be better and more visuals are needed.",https://stackoverflow.com/questions/44742878,471384,Documentation Ambiguity
45446236,Tensorflow slower when using multiple threads during preprocessing on CPU,"<p>I have a dataset that is generated on the fly on the CPU. Samples are computed in python by a function <code>make_sample</code> that is pretty complex and <em>cannot</em> be translated into tensorflow ops. Because sample generation is time consuming, I want to call the function from multiple threads to fill an input queue.</p>

<p>I started from the <a href=""https://www.tensorflow.org/programmers_guide/reading_data#batching"" rel=""nofollow noreferrer"">example given in the documentation</a> and arrived at the following toy example:</p>

<pre><code>import numpy as np
import tensorflow as tf
import time

def make_sample():
  # something that takes time and needs to be on CPU w/o tf ops
  p = 1
  for n in range(1000000):
    p = (p + np.random.random()) * np.random.random()
  return np.float32(p)

read_threads = 1

with tf.device('/cpu:0'):
  example_list = [tf.py_func(make_sample, [], [tf.float32]) for _ in range(read_threads)]
  for ex in example_list:
    ex[0].set_shape(())
  batch_size = 3
  capacity = 30
  batch = tf.train.batch_join(example_list, batch_size=batch_size, capacity=capacity)

with tf.Session().as_default() as sess:
  tf.global_variables_initializer().run()
  coord = tf.train.Coordinator()
  threads = tf.train.start_queue_runners(sess=sess, coord=coord)
  try:
    # dry run, left out of timing
    sess.run(batch)
    start_time = time.time()
    for it in range(5):
      print(sess.run(batch))
  finally:
    duration = time.time() - start_time
    print('duration: {0:4.2f}s'.format(duration))
    coord.request_stop()
  coord.join(threads)
</code></pre>

<p>What surprises me is that, when increasing <code>read_threads</code>, the CPU usage never goes above 50%. What's worse, the computation time plummets: on my computer,</p>

<ul>
<li><code>read_threads=1</code> → <code>duration: 12s</code></li>
<li><code>read_threads=2</code> → <code>duration: 46s</code></li>
<li><code>read_threads=4</code> → <code>duration: 68s</code></li>
<li><code>read_threads=8</code> → <code>duration: 112s</code></li>
</ul>

<p>Is there an explanation, and above all, a solution to get efficient multithreaded data generation with custom python function on tensorflow?</p>
","I have a dataset that is generated on the fly on the CPU. Samples are computed in python by a function make_sample that is pretty complex and cannot be translated into tensorflow ops. Because sample generation is time consuming, I want to call the function from multiple threads to fill an input queue. I started from the example given in the documentation and arrived at the following toy example: What surprises me is that, when increasing read_threads, the CPU usage never goes above 50%. What's worse, the computation time plummets: on my computer, Is there an explanation, and above all, a solution to get efficient multithreaded data generation with custom python function on tensorflow?",https://stackoverflow.com/questions/45446236,1735003,Documentation Replicability
53175991,How can I make predictions from a trained model inside a Tensorflow input pipeline?,"<p>I am trying to train a model for emotion recognition, which uses one of VGG's layer's output as an input.</p>

<p>I could manage what I want by running the prediction in a first step, saving the extracted features and then using them as input to my network, but I am looking for a way to do the whole process at once.</p>

<p>The second model uses a concatenated array of feature maps as input (I am working with video data), so I am not able to simply wire it to the output of VGG.</p>

<p>I tried to use a map operation as depicted in the <code>tf.data.dataset</code> API documentations this way :</p>

<pre><code>def trimmed_vgg16():
  vgg16 = tf.keras.applications.vgg16.VGG16(input_shape=(224,224,3))
  trimmed = tf.keras.models.Model(inputs=vgg16.get_input_at(0),
                                  outputs=vgg16.layers[-3].get_output_at(0))
  return trimmed

vgg16 = trimmed_vgg16()

def _extract_vgg_features(images, labels):
    pred = vgg16_model.predict(images, batch_size=batch_size, steps=1)
    return pred, labels

dataset = #load the dataset (image, label) as usual
dataset = dataset.map(_extract_vgg_features)
</code></pre>

<p>But I'm getting this error : <code>Tensor Tensor(""fc1/Relu:0"", shape=(?, 4096), dtype=float32) is not an element of this graph</code> which is pretty explicit. I'm stuck here, as I don't see a good way of inserting the trained model in the same graph and getting predictions ""on the fly"".</p>

<p>Is there a clean way of doing this or something similar ?</p>

<p>Edit: missed a line.<br>
Edit2: added details</p>
","I am trying to train a model for emotion recognition, which uses one of VGG's layer's output as an input. I could manage what I want by running the prediction in a first step, saving the extracted features and then using them as input to my network, but I am looking for a way to do the whole process at once. The second model uses a concatenated array of feature maps as input (I am working with video data), so I am not able to simply wire it to the output of VGG. I tried to use a map operation as depicted in the tf.data.dataset API documentations this way : But I'm getting this error : Tensor Tensor(""fc1/Relu:0"", shape=(?, 4096), dtype=float32) is not an element of this graph which is pretty explicit. I'm stuck here, as I don't see a good way of inserting the trained model in the same graph and getting predictions ""on the fly"". Is there a clean way of doing this or something similar ? Edit: missed a line. Edit2: added details",https://stackoverflow.com/questions/53175991,10563517,Requesting (Additional) Resources
66079584,tensorflow.dataset.shuffle = tensorflow.dataset.prefetch + then shuffle internally?,"<p>According to the documentation of tf.dataset.<a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"" rel=""nofollow noreferrer"">shuffle</a>, it will fill in a buffer with size <code>k</code> then shuffle inside of it. Tho I don't want the order of data to be changed, I want it to be buffered. Then I found there is tf.dataset.<a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch"" rel=""nofollow noreferrer"">prefetch</a>, which says &quot;This allows later elements to be prepared while the current element is being processed.&quot;</p>
<p>From the description I guess <code>prefetch</code> is what I want (i.e. pre-loading the data while the pervious data are being used in training), but while trying to look into the code of <code>tf.dataset.shuffle</code> to see if they actually call <code>tf.dataset.prefetch</code>, I got stuck in <a href=""https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/data/ops/dataset_ops.py#L3829-L3835"" rel=""nofollow noreferrer"">these</a> lines (paste them below), cannot find where is <code>shuffle_dataset_v3</code> defined.</p>
<pre><code>      variant_tensor = gen_dataset_ops.shuffle_dataset_v3(
          input_dataset._variant_tensor,  # pylint: disable=protected-access
          buffer_size=self._buffer_size,
          seed=self._seed,
          seed2=self._seed2,
          seed_generator=gen_dataset_ops.dummy_seed_generator(),
          reshuffle_each_iteration=self._reshuffle_each_iteration,
          **self._flat_structure)
</code></pre>
<p>My major question is whether <code>prefetch</code> is the replacement of <code>shuffle</code> in terms of buffering the data, and it would also be nice if someone can point me to where <code>shuffle_dataset_v3</code> was implemented?</p>
","According to the documentation of tf.dataset.shuffle, it will fill in a buffer with size k then shuffle inside of it. Tho I don't want the order of data to be changed, I want it to be buffered. Then I found there is tf.dataset.prefetch, which says ""This allows later elements to be prepared while the current element is being processed."" From the description I guess prefetch is what I want (i.e. pre-loading the data while the pervious data are being used in training), but while trying to look into the code of tf.dataset.shuffle to see if they actually call tf.dataset.prefetch, I got stuck in these lines (paste them below), cannot find where is shuffle_dataset_v3 defined. My major question is whether prefetch is the replacement of shuffle in terms of buffering the data, and it would also be nice if someone can point me to where shuffle_dataset_v3 was implemented?",https://stackoverflow.com/questions/66079584,13112529,Documentation Replication on Other Examples
48309631,TensorFlow - tf.data.Dataset reading large HDF5 files,"<p>I am setting up a TensorFlow pipeline for reading large HDF5 files as input for my deep learning models. Each HDF5 file contains 100 videos of variable size length stored as a collection of compressed JPG images (to make size on disk manageable). Using <code>tf.data.Dataset</code> and a map to <code>tf.py_func</code>, reading examples from the HDF5 file using custom Python logic is quite easy. For example:</p>

<pre><code>def read_examples_hdf5(filename, label):
    with h5py.File(filename, 'r') as hf:
        # read frames from HDF5 and decode them from JPG
    return frames, label

filenames = glob.glob(os.path.join(hdf5_data_path, ""*.h5""))
labels = [0]*len(filenames) # ... can we do this more elegantly?

dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))
dataset = dataset.map(
    lambda filename, label: tuple(tf.py_func(
        read_examples_hdf5, [filename, label], [tf.uint8, tf.int64]))
)

dataset = dataset.shuffle(1000 + 3 * BATCH_SIZE)
dataset = dataset.batch(BATCH_SIZE)
iterator = dataset.make_one_shot_iterator()
next_batch = iterator.get_next()
</code></pre>

<p>This example works, however the problem is that it seems like <code>tf.py_func</code> can only handle one example at a time. As my HDF5 container stores 100 examples, this limitation causes significant overhead as the files constantly need to be opened, read, closed and reopened. It would be much more efficient to read all the 100 video examples into the dataset object and then move on with the next HDF5 file (preferably in multiple threads, each thread dealing with it's own collection of HDF5 files).</p>

<p>So, what I would like is a number of threads running in the background, reading video frames from the HDF5 files, decode them from JPG and then feed them into the dataset object. Prior to the introduction of the <code>tf.data.Dataset</code> pipeline, this was quite easy using the <code>RandomShuffleQueue</code> and <code>enqueue_many</code> ops, but it seems like there is currently no elegant way of doing this (or the documentation is lacking). </p>

<p>Does anyone know what would be the best way of achieving my goal? I have also looked into (and implemented) the pipeline using <code>tfrecord</code> files, but taking a random sample of video frames stored in a <code>tfrecord</code> file seems quite impossible (see <a href=""https://stackoverflow.com/questions/48101576/tensorflow-read-video-frames-from-tfrecords-file"">here</a>). Additionally, I have looked at the <code>from_generator()</code> inputs for <code>tf.data.Dataset</code> but that is definitely not going to run in multiple threads it seems. Any suggestions are more than welcome.</p>
","I am setting up a TensorFlow pipeline for reading large HDF5 files as input for my deep learning models. Each HDF5 file contains 100 videos of variable size length stored as a collection of compressed JPG images (to make size on disk manageable). Using tf.data.Dataset and a map to tf.py_func, reading examples from the HDF5 file using custom Python logic is quite easy. For example: This example works, however the problem is that it seems like tf.py_func can only handle one example at a time. As my HDF5 container stores 100 examples, this limitation causes significant overhead as the files constantly need to be opened, read, closed and reopened. It would be much more efficient to read all the 100 video examples into the dataset object and then move on with the next HDF5 file (preferably in multiple threads, each thread dealing with it's own collection of HDF5 files). So, what I would like is a number of threads running in the background, reading video frames from the HDF5 files, decode them from JPG and then feed them into the dataset object. Prior to the introduction of the tf.data.Dataset pipeline, this was quite easy using the RandomShuffleQueue and enqueue_many ops, but it seems like there is currently no elegant way of doing this (or the documentation is lacking). Does anyone know what would be the best way of achieving my goal? I have also looked into (and implemented) the pipeline using tfrecord files, but taking a random sample of video frames stored in a tfrecord file seems quite impossible (see here). Additionally, I have looked at the from_generator() inputs for tf.data.Dataset but that is definitely not going to run in multiple threads it seems. Any suggestions are more than welcome.",https://stackoverflow.com/questions/48309631,3419427,Documentation Replication on Other Examples
73374580,optimize data input pipeline with keras datagenerator by using tf dataset,"<p>i want to train my autoencoder with ~100k hdf5 files. I wrote a datagenerator using keras.utils.Sequence. Everything works fine, but now im getting a data bottleneck. I watched some documentation on the tf datasets and how they perform much faster.</p>
<pre><code>class DataGenerator(keras.utils.Sequence):

def __init__(self,path,batch_size):
    self.path = path
    self.batch_size = batch_size
    self.ids = os.listdir(self.path)

def __len__(self):
    return int(np.floor(len(self.ids) / self.batch_size))

def __getitem__(self,index):
    epsilons,fields = list(), list()

    for id in self.ids[index*self.batch_size:(index+1)*self.batch_size]:

        hf = h5py.File(os.path.join(self.path, id), 'r')
        epsilons.append(np.array(hf.get('epsilon')))
        fields.append(np.array(hf.get('field')))
        hf.close()

    return np.asarray(epsilons), np.asarray(fields)
</code></pre>
<p>Normally I would use my generator like this:</p>
<pre><code>train = DataGenerator(args.p_train, args.bs)
m.fit(train, epochs=args.ep,callbacks = [tboard_callback])
</code></pre>
<p>Now I'm using the Dataset.from generator method:</p>
<pre><code>dataset = tf.data.Dataset.from_generator(lambda: train,(tf.float64,tf.float64))
dataset = dataset.prefetch(autotune)

m.fit(dataset, epochs=args.ep,callbacks = [tboard_callback])
</code></pre>
<p>Unfortunately my basic approach need 20s per epoch, the from_generator approach takes 31s.
Does anyone of you had similar problems on how to get your datagenerator much faster?</p>
<p>Thanks,
Lukas</p>
","i want to train my autoencoder with ~100k hdf5 files. I wrote a datagenerator using keras.utils.Sequence. Everything works fine, but now im getting a data bottleneck. I watched some documentation on the tf datasets and how they perform much faster. Normally I would use my generator like this: Now I'm using the Dataset.from generator method: Unfortunately my basic approach need 20s per epoch, the from_generator approach takes 31s. Does anyone of you had similar problems on how to get your datagenerator much faster? Thanks, Lukas",https://stackoverflow.com/questions/73374580,19264587,Requesting (Additional) Resources
46708822,returned size of tensorflow's dataset API is not constant,"<p>I am using tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset"" rel=""nofollow noreferrer"">dataset API</a>. And testing my code with simple case. Below shows the simple code I used. The problem is, when the dataset size is small, it seems the returned size from dataset API is not consistent. I'm sure there is a proper way to deal with it. But even though I read all the function in that page and tutorial, I could not find that. </p>

<pre><code>import numpy as np
import tensorflow as tf

data_source = tf.zeros([24, 200, 64, 64, 1]) #[number_of_video, steps, pixel_w, pixel_h, channel]
dataset = tf.contrib.data.Dataset.from_tensor_slices(data_source)
dataset = dataset.shuffle(buffer_size=100)
dataset = dataset.batch(16)
dataset = dataset.repeat()

iterator = tf.contrib.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)
next_element = iterator.get_next()
training_init_op = iterator.make_initializer(dataset)

with tf.Session() as sess:
    sess.run(training_init_op)
    next_elem = next_element.eval()
    print(np.shape(next_elem))
    next_elem = next_element.eval()
    print(np.shape(next_elem))
    next_elem = next_element.eval()
    print(np.shape(next_elem))
    next_elem = next_element.eval()
    print(np.shape(next_elem))
    next_elem = next_element.eval()
    print(np.shape(next_elem))
    next_elem = next_element.eval()
    print(np.shape(next_elem))
    next_elem = next_element.eval()
    print(np.shape(next_elem))
</code></pre>

<p>The dataset is grayscale video. There are totally 24 sequence of video and step size is all 200. Frame size is 64 by 64 and single channel. I set batch size as 16 and buffer size as 100. But the result of the code is,</p>

<pre><code>(16, 200, 64, 64, 1)
(8, 200, 64, 64, 1)
(16, 200, 64, 64, 1)
(8, 200, 64, 64, 1)
(16, 200, 64, 64, 1)
(8, 200, 64, 64, 1)
(16, 200, 64, 64, 1)
</code></pre>

<p>The returned size of video is either 16 or 8. I guess it is because the original data size is small, 24, when it reaches the end of data, the API just returns what is left. </p>

<p>But I don't understand. I also set buffer size as 100. That means the buffer should be filled in advance with small dataset. And from that buffer, the API should select next_element whose batch size if 16. </p>

<p>When I used queue-type API in tensorflow, I didn't have this problem. Whatever the size of original data is, anyway there is an moment when the iterator reaches the end of dataset. I wonder how this problem is solved by other people using this API. </p>
","I am using tensorflow's dataset API. And testing my code with simple case. Below shows the simple code I used. The problem is, when the dataset size is small, it seems the returned size from dataset API is not consistent. I'm sure there is a proper way to deal with it. But even though I read all the function in that page and tutorial, I could not find that. The dataset is grayscale video. There are totally 24 sequence of video and step size is all 200. Frame size is 64 by 64 and single channel. I set batch size as 16 and buffer size as 100. But the result of the code is, The returned size of video is either 16 or 8. I guess it is because the original data size is small, 24, when it reaches the end of data, the API just returns what is left. But I don't understand. I also set buffer size as 100. That means the buffer should be filled in advance with small dataset. And from that buffer, the API should select next_element whose batch size if 16. When I used queue-type API in tensorflow, I didn't have this problem. Whatever the size of original data is, anyway there is an moment when the iterator reaches the end of dataset. I wonder how this problem is solved by other people using this API.",https://stackoverflow.com/questions/46708822,5621202,Documentation Ambiguity
46870058,Calling TensorFlow's Dataset.from_generator method,"<p>The TensorFlow 1.4 documentation provides code that demonstrates the usage of <a href=""https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">Dataset.from_generator</a>. When I run the code, I get an InvalidArgumentError:<code>0-th value returned by pyfunc_0 is int32, but expects int64</code>.</p>

<p>I'm using Python 3.6.1. Here's the code:</p>

<pre><code>def gen():
    for i in itertools.count(1):
    yield (i, [1] * i)

ds = tf.data.Dataset.from_generator(gen, (tf.int64, tf.int64), 
    (tf.TensorShape([]), tf.TensorShape([None])))
value = ds.make_one_shot_iterator().get_next()

with tf.Session() as sess:
    sess.run(value)  # (1, array([1]))
    sess.run(value)  # (2, array([1, 1]))
</code></pre>

<p>Any ideas?</p>
","The TensorFlow 1.4 documentation provides code that demonstrates the usage of Dataset.from_generator. When I run the code, I get an InvalidArgumentError:0-th value returned by pyfunc_0 is int32, but expects int64. I'm using Python 3.6.1. Here's the code: Any ideas?",https://stackoverflow.com/questions/46870058,934904,Documentation Replication on Other Examples
46938530,Produce balanced mini batch with Dataset API,"<p>I've a question about the new dataset API (tensorflow 1.4rc1).
I've a unbalanced dataset wrt to labels <code>0</code> and <code>1</code>. My goal is to create balanced mini batches during the preprocessing.</p>

<p>Assume I've two filtered datasets:</p>

<pre><code>ds_pos = dataset.filter(lambda l, x, y, z: tf.reshape(tf.equal(l, 1), []))
ds_neg = dataset.filter(lambda l, x, y, z: tf.reshape(tf.equal(l, 0), [])).repeat()
</code></pre>

<p>Is there a way to combine these two datasets such that the resulting dataset looks like <code>ds = [0, 1, 0, 1, 0, 1]</code>:</p>

<p>Something like this:</p>

<pre><code>dataset = tf.data.Dataset.zip((ds_pos, ds_neg))
dataset = dataset.apply(...)
# dataset looks like [0, 1, 0, 1, 0, 1, ...]
dataset = dataset.batch(20)
</code></pre>

<p>My current approach is:</p>

<pre><code>def _concat(x, y):
   return tf.cond(tf.random_uniform(()) &gt; 0.5, lambda: x, lambda: y)
dataset = tf.data.Dataset.zip((ds_pos, ds_neg))
dataset = dataset.map(_concat)
</code></pre>

<p>But I've the feeling there is a more elegant way.</p>

<p>Thanks in advance!</p>
","I've a question about the new dataset API (tensorflow 1.4rc1). I've a unbalanced dataset wrt to labels 0 and 1. My goal is to create balanced mini batches during the preprocessing. Assume I've two filtered datasets: Is there a way to combine these two datasets such that the resulting dataset looks like ds = [0, 1, 0, 1, 0, 1]: Something like this: My current approach is: But I've the feeling there is a more elegant way. Thanks in advance!",https://stackoverflow.com/questions/46938530,863543,Requesting (Additional) Resources
49116343,Dataset API 'flat_map' method producing error for same code which works with 'map' method,"<p>I am trying to create a create a pipeline to read multiple CSV files using TensorFlow Dataset API and Pandas. However, using the <code>flat_map</code> method is producing errors. However, if I am using <code>map</code> method I am able to build the code and run it in session. This is the code I am using. I already opened <a href=""https://github.com/tensorflow/tensorflow/issues/17415"" rel=""nofollow noreferrer"">#17415</a> issue in TensorFlow Github repository. But apparently, it is not an error and they asked me to post here.</p>

<pre><code>folder_name = './data/power_data/'
file_names = os.listdir(folder_name)
def _get_data_for_dataset(file_name,rows=100):#
    print(file_name.decode())

    df_input=pd.read_csv(os.path.join(folder_name, file_name.decode()),
                         usecols =['Wind_MWh','Actual_Load_MWh'],nrows = rows)
    X_data = df_input.as_matrix()
    X_data.astype('float32', copy=False)

    return X_data
dataset = tf.data.Dataset.from_tensor_slices(file_names)
dataset = dataset.flat_map(lambda file_name: tf.py_func(_get_data_for_dataset, 
[file_name], tf.float64))
dataset= dataset.batch(2)
fiter = dataset.make_one_shot_iterator()
get_batch = iter.get_next()
</code></pre>

<p>I get the following error: <code>map_func must return a Dataset object</code>. The pipeline works without error when I use <code>map</code> but it doesn't give the output I want. For example, if Pandas is reading N rows from each of my CSV files I want the pipeline to concatenate data from B files and give me an array with shape (N*B, 2). Instead, it is giving me (B, N,2) where B is the Batch size. <code>map</code> is adding another axis instead of concatenating on the existing axis. From what I understood in the documentation <code>flat_map</code> is supposed to give a flatted output. In the documentation, both <code>map</code> and <code>flat_map</code> returns type Dataset. So how is my code working with map and not with flat_map?</p>

<p>It would also great if you could point me towards code where Dataset API has been used with Pandas module.</p>
","I am trying to create a create a pipeline to read multiple CSV files using TensorFlow Dataset API and Pandas. However, using the flat_map method is producing errors. However, if I am using map method I am able to build the code and run it in session. This is the code I am using. I already opened #17415 issue in TensorFlow Github repository. But apparently, it is not an error and they asked me to post here. I get the following error: map_func must return a Dataset object. The pipeline works without error when I use map but it doesn't give the output I want. For example, if Pandas is reading N rows from each of my CSV files I want the pipeline to concatenate data from B files and give me an array with shape (N*B, 2). Instead, it is giving me (B, N,2) where B is the Batch size. map is adding another axis instead of concatenating on the existing axis. From what I understood in the documentation flat_map is supposed to give a flatted output. In the documentation, both map and flat_map returns type Dataset. So how is my code working with map and not with flat_map? It would also great if you could point me towards code where Dataset API has been used with Pandas module.",https://stackoverflow.com/questions/49116343,7656080,Requesting (Additional) Resources
52636943,"What is a ""stateful object"" in tensorflow?","<p>In several parts of the documentation (e.g. <code>Dataset Iterators</code> <a href=""https://www.tensorflow.org/guide/datasets#consuming_values_from_an_iterator"" rel=""noreferrer"">here</a>) there are references to <code>Stateful Objects</code>. What exactly are they and what role do they play in the graph?</p>

<p>To clarify, in the Dataset documentation there's an example with the <code>one_shot_iterator</code> that works because it's stateless:  </p>

<pre><code>dataset = tf.data.Dataset.range(100)
iterator = dataset.make_one_shot_iterator()
</code></pre>

<p>what makes the iterator stateless?</p>
","In several parts of the documentation (e.g. Dataset Iterators here) there are references to Stateful Objects. What exactly are they and what role do they play in the graph? To clarify, in the Dataset documentation there's an example with the one_shot_iterator that works because it's stateless: what makes the iterator stateless?",https://stackoverflow.com/questions/52636943,1047543,Documentation Ambiguity
56992725,Is the shard operation on Tensorflow Datasets deterministic?,"<p>Tensorflow Datasets have a <code>shard</code> operation that creates a unique subset of a given Dataset.</p>

<p>We can use it to partition a Dataset, as follows:</p>

<pre><code>import tensorflow as tf
source_dataset = tf.data.Dataset.range(100)

number_of_partitions = 4
subset_one = source_dataset.shard(number_of_partitions, 0)
subset_two = source_dataset.shard(number_of_partitions, 1)
subset_three = source_dataset.shard(number_of_partitions, 2)
</code></pre>

<p>Is this partitioning deterministic?
i.e. the 3 subsets above would always be given the same elements?</p>

<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard"" rel=""nofollow noreferrer"">documentation</a> states the following about <code>shard</code>:</p>

<blockquote>
  <p>Creates a Dataset that includes only 1/num_shards of this dataset.</p>
  
  <p>This dataset operator is very useful when running distributed
  training, as it allows each worker to read a unique subset.</p>
</blockquote>
","Tensorflow Datasets have a shard operation that creates a unique subset of a given Dataset. We can use it to partition a Dataset, as follows: Is this partitioning deterministic? i.e. the 3 subsets above would always be given the same elements? The documentation states the following about shard:",https://stackoverflow.com/questions/56992725,11726832,Documentation Ambiguity
61131730,Tensorflow Datasets with string inputs do not preserve data type,"<p>All <strong>reproducible</strong> code below is run at Google Colab with TF 2.2.0-rc2.</p>

<p>Adapting the simple example from the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">documentation</a> for creating a dataset from a simple Python list:</p>

<pre><code>import numpy as np
import tensorflow as tf
tf.__version__
# '2.2.0-rc2'
np.version.version
# '1.18.2'

dataset1 = tf.data.Dataset.from_tensor_slices([1, 2, 3]) 
for element in dataset1: 
  print(element) 
  print(type(element.numpy()))
</code></pre>

<p>we get the result</p>

<pre><code>tf.Tensor(1, shape=(), dtype=int32)
&lt;class 'numpy.int32'&gt;
tf.Tensor(2, shape=(), dtype=int32)
&lt;class 'numpy.int32'&gt;
tf.Tensor(3, shape=(), dtype=int32)
&lt;class 'numpy.int32'&gt;
</code></pre>

<p>where all data types are <code>int32</code>, as expected.</p>

<p>But changing this simple example to feed a list of strings instead of integers:</p>

<pre><code>dataset2 = tf.data.Dataset.from_tensor_slices(['1', '2', '3']) 
for element in dataset2: 
  print(element) 
  print(type(element.numpy()))
</code></pre>

<p>gives the result</p>

<pre><code>tf.Tensor(b'1', shape=(), dtype=string)
&lt;class 'bytes'&gt;
tf.Tensor(b'2', shape=(), dtype=string)
&lt;class 'bytes'&gt;
tf.Tensor(b'3', shape=(), dtype=string)
&lt;class 'bytes'&gt;
</code></pre>

<p>where, surprisingly, and despite the tensors themselves being of <code>dtype=string</code>, their evaluations are of type <code>bytes</code>.</p>

<p>This behavior is not confined to the <code>.from_tensor_slices</code> method; here is the situation with <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files"" rel=""nofollow noreferrer""><code>.list_files</code></a> (the following snippet runs straightforward in a fresh Colab notebook):</p>

<pre><code>disc_data = tf.data.Dataset.list_files('sample_data/*.csv') # 4 csv files
for element in disc_data: 
  print(element) 
  print(type(element.numpy()))
</code></pre>

<p>the result being:</p>

<pre><code>tf.Tensor(b'sample_data/california_housing_test.csv', shape=(), dtype=string)
&lt;class 'bytes'&gt;
tf.Tensor(b'sample_data/mnist_train_small.csv', shape=(), dtype=string)
&lt;class 'bytes'&gt;
tf.Tensor(b'sample_data/california_housing_train.csv', shape=(), dtype=string)
&lt;class 'bytes'&gt;
tf.Tensor(b'sample_data/mnist_test.csv', shape=(), dtype=string)
&lt;class 'bytes'&gt;
</code></pre>

<p>where again, the file names in the evaluated tensors are returned as <code>bytes</code>, instead of <code>string</code>, despite that the tensors themselves are of <code>dtype=string</code>.</p>

<p>Similar behavior is observed also with the <code>.from_generator</code> method (not shown here).</p>

<p>A final demonstration: as shown in the <code>.as_numpy_iterator</code> method <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#as_numpy_iterator"" rel=""nofollow noreferrer"">documentation</a>, the following equality condition is evaluated as <code>True</code>:</p>

<pre><code>dataset3 = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]), 
                                               'b': [5, 6]}) 

list(dataset3.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5}, 
                                       {'a': (2, 4), 'b': 6}] 
# True
</code></pre>

<p>but if we change the elements of <code>b</code> to be strings, the equality condition is now surprisingly evaluated as <code>False</code>!</p>

<pre><code>dataset4 = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]), 
                                               'b': ['5', '6']})   # change elements of b to strings

list(dataset4.as_numpy_iterator()) == [{'a': (1, 3), 'b': '5'},   # here
                                       {'a': (2, 4), 'b': '6'}]   # also
# False
</code></pre>

<p>probably due to the different data types, since the values themselves are evidently identical.</p>

<hr>

<p>I didn't stumble upon this behavior by academic experimentation; I am trying to pass my data to TF Datasets using custom functions that read pairs of files from the disk of the form</p>

<pre><code>f = ['filename1', 'filename2']
</code></pre>

<p>which custom functions work perfectly well on their own, but mapped through TF Datasets give</p>

<pre><code>RuntimeError: not a string
</code></pre>

<p>which, after this digging, seems at least not unexplained, if the returned data types are indeed <code>bytes</code> and not <code>string</code>.</p>

<p>So, is this a bug (as it seems), or am I missing something here?</p>
","All reproducible code below is run at Google Colab with TF 2.2.0-rc2. Adapting the simple example from the documentation for creating a dataset from a simple Python list: we get the result where all data types are int32, as expected. But changing this simple example to feed a list of strings instead of integers: gives the result where, surprisingly, and despite the tensors themselves being of dtype=string, their evaluations are of type bytes. This behavior is not confined to the .from_tensor_slices method; here is the situation with .list_files (the following snippet runs straightforward in a fresh Colab notebook): the result being: where again, the file names in the evaluated tensors are returned as bytes, instead of string, despite that the tensors themselves are of dtype=string. Similar behavior is observed also with the .from_generator method (not shown here). A final demonstration: as shown in the .as_numpy_iterator method documentation, the following equality condition is evaluated as True: but if we change the elements of b to be strings, the equality condition is now surprisingly evaluated as False! probably due to the different data types, since the values themselves are evidently identical. I didn't stumble upon this behavior by academic experimentation; I am trying to pass my data to TF Datasets using custom functions that read pairs of files from the disk of the form which custom functions work perfectly well on their own, but mapped through TF Datasets give which, after this digging, seems at least not unexplained, if the returned data types are indeed bytes and not string. So, is this a bug (as it seems), or am I missing something here?",https://stackoverflow.com/questions/61131730,4685471,Documentation Replication on Other Examples
67676763,How to read an array (list of dictionaries) into Python Tensorflow?,"<p>As a result of dealing with a gigantic dataset that takes up too much memory, I need to tap into Tensorflow's generator functions (e.g. map, apply)</p>
<p>I have the following array that I'd like to load into Tensorflow:</p>
<pre><code>array = [{'field_one':'1','field_two':'2'},{'field_one':'3','field_two':'4'},{'field_one':'5','field_two':'6'}]
</code></pre>
<p>From reading the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=fr"" rel=""nofollow noreferrer"">documentation</a>, I've tried the following:</p>
<pre><code>import tensorflow as tf

array = [{'field_one':'1','field_two':'2'},{'field_one':'3','field_two':'4'},{'field_one':'5','field_two':'6'}]

dataset = tf.data.Dataset.from_tensor_slices(array)
</code></pre>
<p>However it returns the following error:</p>
<pre><code>ValueError: Attempt to convert a value with an unsupported type to a Tensor. 
</code></pre>
<p>I've also tried the following based on <a href=""https://www.kite.com/python/answers/how-to-convert-a-numpy-array-to-a-tensorflow-tensor-in-python"" rel=""nofollow noreferrer"">this documentation</a> that generates the same error:</p>
<pre><code>data_tensor = tf.convert_to_tensor(array)
</code></pre>
<p>I've also tried this as well, which generates a different error:</p>
<pre><code>tf.data.Dataset(array)
</code></pre>
<p>Error:</p>
<pre><code>TypeError: Can't instantiate abstract class DatasetV2 with abstract methods _inputs, element_spec
</code></pre>
","As a result of dealing with a gigantic dataset that takes up too much memory, I need to tap into Tensorflow's generator functions (e.g. map, apply) I have the following array that I'd like to load into Tensorflow: From reading the documentation, I've tried the following: However it returns the following error: I've also tried the following based on this documentation that generates the same error: I've also tried this as well, which generates a different error: Error:",https://stackoverflow.com/questions/67676763,2850808,Documentation Replicability
73691788,Tensorflow dataset not saved in multiple shards,"<p>I want to use the tensorflow dataset saving and loading functions but I am not sure to understand the sharding method.</p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#save"" rel=""noreferrer"">documentation</a> indicates :</p>
<blockquote>
<p>The saved dataset is saved in multiple file &quot;shards&quot;. By default, the dataset output is divided to shards in a round-robin fashion but custom sharding can be specified via the shard_func function.</p>
</blockquote>
<p>But when I save a dataset through the save function, it seems that only one huge shard is generated.</p>
<pre><code>import tempfile
import tensorflow as tf

path = os.path.join(tempfile.gettempdir(), &quot;saved_data&quot;)
dataset = tf.data.Dataset.range(10**8)

dataset.save(path)
</code></pre>
<p><a href=""https://i.stack.imgur.com/K2Qkh.png"" rel=""noreferrer"">generated dataset screenshot</a></p>
<p>Am I missing something ?</p>
<p>I use Tensorflow 2.10.0 and Python 3.9.7</p>
","I want to use the tensorflow dataset saving and loading functions but I am not sure to understand the sharding method. The documentation indicates : But when I save a dataset through the save function, it seems that only one huge shard is generated. generated dataset screenshot Am I missing something ? I use Tensorflow 2.10.0 and Python 3.9.7",https://stackoverflow.com/questions/73691788,5130199,Documentation Replication on Other Examples
75298969,OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed when using a dataset with tuples,"<p>I am trying to create my own transformer with tensorflow and of course I want to train it. For the purpuse I use dataset to handle my data. The data is created by a code snippet from the tensorflow dataset.from_tensor_slices() method documentation article. Nevertheless, tensorflow is giving me the following error when I call the fit() method:</p>
<blockquote>
<p>&quot;<strong>OperatorNotAllowedInGraphError: Iterating over a symbolic <code>tf.Tensor</code> is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.</strong>&quot;</p>
</blockquote>
<p>Here is the code that I am using:</p>
<pre><code>import numpy as np
import tensorflow as tf

batched_features = tf.constant([[[1, 3], [2, 3]],
                                [[2, 1], [1, 2]],
                                [[3, 3], [3, 2]]], shape=(3, 2, 2))
batched_labels = tf.constant([['A', 'A'],
                              ['B', 'B'],
                              ['A', 'B']], shape=(3, 2, 1))
dataset = tf.data.Dataset.from_tensor_slices((batched_features, batched_labels))
dataset = dataset.batch(1)
for element in dataset.as_numpy_iterator():
  print(element)

class MyTransformer(tf.keras.Model):
    def __init__(self):
        super().__init__()
        
    def call(self, inputs, training):
        print(type(inputs))
        feature, lable = inputs
        return feature

model = MyTransformer()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.FalseNegatives()])

model.fit(train_data, batch_size = 1, epochs = 1)
</code></pre>
<p>The code is reduced significantly just for the purpuse of reproducing the issue.</p>
<p>I've tried passing the data as dictionary instead of tuple and couple more things but nothing worked. It seems that I am missing something.</p>
","I am trying to create my own transformer with tensorflow and of course I want to train it. For the purpuse I use dataset to handle my data. The data is created by a code snippet from the tensorflow dataset.from_tensor_slices() method documentation article. Nevertheless, tensorflow is giving me the following error when I call the fit() method: Here is the code that I am using: The code is reduced significantly just for the purpuse of reproducing the issue. I've tried passing the data as dictionary instead of tuple and couple more things but nothing worked. It seems that I am missing something.",https://stackoverflow.com/questions/75298969,5757458,Documentation Replicability
54843448,"How to ""zip"" Tensorflow Dataset and train in Keras correctly?","<p>I have a <code>train_x.csv</code> and a <code>train_y.csv</code>, and I'd like to train a model using Dataset API and Keras interface. This what I'm trying to do:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
import tensorflow as tf

tf.enable_eager_execution()

N_FEATURES = 10
N_SAMPLES = 100
N_OUTPUTS = 2
BATCH_SIZE = 8
EPOCHS = 5

# prepare fake data
train_x = pd.DataFrame(np.random.rand(N_SAMPLES, N_FEATURES))
train_x.to_csv('train_x.csv', index=False)
train_y = pd.DataFrame(np.random.rand(N_SAMPLES, N_OUTPUTS))
train_y.to_csv('train_y.csv', index=False)

train_x = tf.data.experimental.CsvDataset('train_x.csv', [tf.float32] * N_FEATURES, header=True)
train_y = tf.data.experimental.CsvDataset('train_y.csv', [tf.float32] * N_OUTPUTS, header=True)
dataset = ...  # What to do here?

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(N_OUTPUTS, input_shape=(N_FEATURES,)),
    tf.keras.layers.Activation('linear'),
])
model.compile('sgd', 'mse')
model.fit(dataset, steps_per_epoch=N_SAMPLES/BATCH_SIZE, epochs=EPOCHS)
</code></pre>

<p>What's the right way to implement this <code>dataset</code>?</p>

<p>I tried <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip"" rel=""nofollow noreferrer""><code>Dataset.zip</code></a> API like <code>dataset = tf.data.Dataset.zip((train_x, train_y))</code> but it seems not working(code <a href=""https://pastebin.com/7nX8bxus"" rel=""nofollow noreferrer"">here</a> and error <a href=""https://pastebin.com/ikfZZ88E"" rel=""nofollow noreferrer"">here</a>). I also read <a href=""https://stackoverflow.com/a/46140332/2666624"">this</a> answer, it's working but I'd like a non-functional model declaration way.</p>
","I have a train_x.csv and a train_y.csv, and I'd like to train a model using Dataset API and Keras interface. This what I'm trying to do: What's the right way to implement this dataset? I tried Dataset.zip API like dataset = tf.data.Dataset.zip((train_x, train_y)) but it seems not working(code here and error here). I also read this answer, it's working but I'd like a non-functional model declaration way.",https://stackoverflow.com/questions/54843448,2666624,Requesting (Additional) Resources
52731624,"""A nested structure of tf.Tensor objects"" for Iterator.get_next() result type","<p>When I write Tensorflow code, I try to keep in mind the type of different things, e.g. tuple of two Tensors, or list of Tensors. This is important because when the types/shapes don't match, Tensorflow emits an error.</p>

<p>The text of the title of this question shows up a lot in the documentation especially when describing the result of some function, e.g. for <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Iterator#get_next"" rel=""nofollow noreferrer""><code>Iterator.get_next()</code></a>, but I find it too vague. It doesn't tell me exactly what to expect, a list of tuples? a tuple of tuples? What exactly is this 'nested structure'? Right now, the only way I can track this is to print the result after Session.run(). Is there a cleaner and more definitive way?</p>

<p>Also, it seems that the value of <code>Iterator.get_next()</code> is always a list of one element; I haven't been able to make it return a non-list, an empty list or a list with multiple elements. When does <code>Iterator.get_next()</code> return something that's not a list of one element? If never, then the wrapping of the content in a list seems superfluous -- why was <code>Iterator.get_next()</code> designed this way?</p>

<p>This is example code showing I mean:</p>

<pre><code>import numpy as np
import tensorflow as tf

ds = tf.data.Dataset.from_tensor_slices(np.array(range(0, 8)).reshape(4,2))
it = ds.make_one_shot_iterator()

with tf.Session() as sess:
    for i in range(0, 4):
        x = sess.run([it.get_next()])
        print(x)
</code></pre>

<p>Output:</p>

<pre><code>[array([0, 1])]
[array([2, 3])]
[array([4, 5])]
[array([6, 7])]
</code></pre>

<p>Why not just the following?</p>

<pre><code>array([0, 1])
array([2, 3])
array([4, 5])
array([6, 7])
</code></pre>
","When I write Tensorflow code, I try to keep in mind the type of different things, e.g. tuple of two Tensors, or list of Tensors. This is important because when the types/shapes don't match, Tensorflow emits an error. The text of the title of this question shows up a lot in the documentation especially when describing the result of some function, e.g. for Iterator.get_next(), but I find it too vague. It doesn't tell me exactly what to expect, a list of tuples? a tuple of tuples? What exactly is this 'nested structure'? Right now, the only way I can track this is to print the result after Session.run(). Is there a cleaner and more definitive way? Also, it seems that the value of Iterator.get_next() is always a list of one element; I haven't been able to make it return a non-list, an empty list or a list with multiple elements. When does Iterator.get_next() return something that's not a list of one element? If never, then the wrapping of the content in a list seems superfluous -- why was Iterator.get_next() designed this way? This is example code showing I mean: Output: Why not just the following?",https://stackoverflow.com/questions/52731624,679688,Documentation Completeness
58628787,What is the intuition behind the Iterator.get_next method?,"<p>The name of the method <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/data/Iterator#get_next"" rel=""nofollow noreferrer""><code>get_next()</code></a> is a little bit misleading. The documentation says</p>
<blockquote>
<p>Returns a nested structure of <code>tf.Tensor</code>s representing the next element.</p>
<p>In graph mode, you should typically call this method <strong>once</strong> and use its result as the input to another computation. A typical loop will then call <code>tf.Session.run</code> on the result of that computation. The loop will terminate when the <code>Iterator.get_next()</code> operation raises <code>tf.errors.OutOfRangeError</code>. The following skeleton shows how to use this method when building a training loop:</p>
</blockquote>
<pre><code>dataset = ...  # A `tf.data.Dataset` object.
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()

# Build a TensorFlow graph that does something with each element.
loss = model_function(next_element)
optimizer = ...  # A `tf.compat.v1.train.Optimizer` object.
train_op = optimizer.minimize(loss)

with tf.compat.v1.Session() as sess:
  try:
    while True:
      sess.run(train_op)
  except tf.errors.OutOfRangeError:
    pass
</code></pre>
<p>Python also has a function called <a href=""https://docs.python.org/3/library/functions.html#next"" rel=""nofollow noreferrer""><code>next</code></a>, which needs to be called every time we need the next element of the iterator. However, according to the documentation of <code>get_next()</code> quoted above, <code>get_next()</code> should be called only once and its result should be evaluated by calling the method <code>run</code> of the session, so this is a little bit unintuitive, because I was used to the Python's built-in function <code>next</code>. In <a href=""https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/bayesian_neural_network.py"" rel=""nofollow noreferrer"">this script</a>, <code>get_next()</code> is also called only and the result of the call is evaluated at every step of the computation.</p>
<p>What is the intuition behind <code>get_next()</code> and how is it different from <code>next()</code>? I think that the next element of the dataset (or feedable iterator), in the second example I linked above, is retrieved every time the result of the first call to <code>get_next()</code> is evaluated by calling the method <code>run</code>, but this is a little unintuitive. I don't get why we do not need to call <code>get_next</code> at every step of the computation (to get the next element of the feedable iterator), even after reading the note in the documentation</p>
<blockquote>
<p>NOTE: It is legitimate to call <code>Iterator.get_next()</code> multiple times, e.g. when you are distributing different elements to multiple devices in a single step. However, a common pitfall arises when users call <code>Iterator.get_next()</code> in each iteration of their training loop. <code>Iterator.get_next()</code> adds ops to the graph, and executing each op allocates resources (including threads); as a consequence, invoking it in every iteration of a training loop causes slowdown and eventual resource exhaustion. To guard against this outcome, we log a warning when the number of uses crosses a fixed threshold of suspiciousness.</p>
</blockquote>
<p>In general, it is not clear how the Iterator works.</p>
","The name of the method get_next() is a little bit misleading. The documentation says Python also has a function called next, which needs to be called every time we need the next element of the iterator. However, according to the documentation of get_next() quoted above, get_next() should be called only once and its result should be evaluated by calling the method run of the session, so this is a little bit unintuitive, because I was used to the Python's built-in function next. In this script, get_next() is also called only and the result of the call is evaluated at every step of the computation. What is the intuition behind get_next() and how is it different from next()? I think that the next element of the dataset (or feedable iterator), in the second example I linked above, is retrieved every time the result of the first call to get_next() is evaluated by calling the method run, but this is a little unintuitive. I don't get why we do not need to call get_next at every step of the computation (to get the next element of the feedable iterator), even after reading the note in the documentation In general, it is not clear how the Iterator works.",https://stackoverflow.com/questions/58628787,3924118,Documentation Ambiguity
37668485,Create an int list feature to save as tfrecord in tensorflow?,"<p>How can I create a tensorflow record from a list?</p>

<p>From the <a href=""https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/core/example/feature.proto#L9-10"" rel=""noreferrer"">documentation</a> here it seems possible. There's also this <a href=""https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/how_tos/reading_data/convert_to_records.py#L64-69"" rel=""noreferrer"">example</a> where they convert a numpy array into a byte array using the <code>.tostring()</code> from numpy. However when I try to pass in:</p>

<pre><code>labels = np.asarray([[1,2,3],[4,5,6]])
...
example = tf.train.Example(features=tf.train.Features(feature={
    'height': _int64_feature(rows),
    'width': _int64_feature(cols),
    'depth': _int64_feature(depth),
    'label': _int64_feature(labels[index]),
    'image_raw': _bytes_feature(image_raw)}))
writer.write(example.SerializeToString())
</code></pre>

<p>I get the error:</p>

<pre><code>TypeError: array([1, 2, 3]) has type type 'numpy.ndarray', but expected one of: (type 'int', type 'long')
</code></pre>

<p>Which doesn't help me to figure out how to store a list of integers into the tfrecord. I've tried looking through the docs.  </p>
",How can I create a tensorflow record from a list? From the documentation here it seems possible. There's also this example where they convert a numpy array into a byte array using the .tostring() from numpy. However when I try to pass in: I get the error: Which doesn't help me to figure out how to store a list of integers into the tfrecord. I've tried looking through the docs.,https://stackoverflow.com/questions/37668485,6416660,Documentation Replicability
58276774,How do you write a fixed len feature to tfrecord,"<p>I'm struggling with the basics of writing a tensorflow tfrecord file. I'm writing a simple example with an ndarray in python, but for some reason when I read it it's required to be variable-length and reads it as a SparseTensor.</p>

<p>Here's the example</p>

<pre><code>def serialize_tf_record(features, targets):
    record = {
        'shape': tf.train.Int64List(value=features.shape),
        'features': tf.train.FloatList(value=features.flatten()),
        'targets': tf.train.Int64List(value=targets),
    }

    return build_tf_example(record)

def deserialize_tf_record(record):
    tfrecord_format = {
        'shape': tf.io.VarLenFeature(tf.int64),
        'features': tf.io.VarLenFeature(tf.float32),
        'targets': tf.io.VarLenFeature(tf.int64),
    }

    features_tensor = tf.io.parse_single_example(record, tfrecord_format)
    return features_tensor
</code></pre>

<p>Can anybody explain to me why this writes a variable-length record? It is fixed in code, but I can't seem to write it in a way tensorflow knows its fixed. The tensorflow documentation is pretty horrific here. Can anybody clarify the API for me?</p>
","I'm struggling with the basics of writing a tensorflow tfrecord file. I'm writing a simple example with an ndarray in python, but for some reason when I read it it's required to be variable-length and reads it as a SparseTensor. Here's the example Can anybody explain to me why this writes a variable-length record? It is fixed in code, but I can't seem to write it in a way tensorflow knows its fixed. The tensorflow documentation is pretty horrific here. Can anybody clarify the API for me?",https://stackoverflow.com/questions/58276774,1196033,Documentation Completeness
48768206,How to use dataset.shard in tensorflow?,"<p>Recently I am looking into the dataset API in Tensorflow, and there is a method <code>dataset.shard()</code> which is for distributed computations.</p>

<p>This is what's stated in Tensorflow's documentation:</p>

<pre><code>Creates a Dataset that includes only 1/num_shards of this dataset.

d = tf.data.TFRecordDataset(FLAGS.input_file)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
d = d.repeat(FLAGS.num_epochs)
d = d.shuffle(FLAGS.shuffle_buffer_size)
d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)
</code></pre>

<p>This method is said to return a portion of the original dataset. If I have two workers, am I supposed to do:</p>

<pre><code>d_0 = d.shard(FLAGS.num_workers, worker_0)
d_1 = d.shard(FLAGS.num_workers, worker_1)
......
iterator_0 = d_0.make_initializable_iterator()
iterator_1 = d_1.make_initializable_iterator()

for worker_id in workers:
    with tf.device(worker_id):
        if worker_id == 0:
            data = iterator_0.get_next()
        else:
            data = iterator_1.get_next()
        ......
</code></pre>

<p>Because the documentation did not specify how to make subsequent calls, I am a bit confused here.</p>

<p>Thanks!</p>
","Recently I am looking into the dataset API in Tensorflow, and there is a method dataset.shard() which is for distributed computations. This is what's stated in Tensorflow's documentation: This method is said to return a portion of the original dataset. If I have two workers, am I supposed to do: Because the documentation did not specify how to make subsequent calls, I am a bit confused here. Thanks!",https://stackoverflow.com/questions/48768206,8736709,Documentation Completeness
63665112,Can anyone explain how the function of shuffle in tf.dataset work?,"<p>I can't find out how the function of shuffle in tf.dataset work.
I try to see output to guess what happen inside.</p>
<pre><code>dataset = tf.data.Dataset.range(6);
dataset = dataset.shuffle(buffer_size =1).batch(6)
for item in dataset:
  print(item)
</code></pre>
<p>Output:</p>
<pre><code>tf.Tensor([0 1 2 3 4 5], shape=(6,), dtype=int64)
</code></pre>
<p>===&gt; As you see, the shuffle doesn't work when buffer_size =1
But when I change buffer_size = 2 the list change the order but the first item only in 0 or 1 although I run 100 time again.</p>
<p>Anyone in group can explain the role of buffer_size. I read the document of tensorflow at
<a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle</a></p>
<p>In my thought, when set buffer_size = 1, as document said, maybe i can replace element in buffer_size. But the output I get make me confused.</p>
<p>can anyone run into the same problem ?</p>
","I can't find out how the function of shuffle in tf.dataset work. I try to see output to guess what happen inside. Output: ===&gt; As you see, the shuffle doesn't work when buffer_size =1 But when I change buffer_size = 2 the list change the order but the first item only in 0 or 1 although I run 100 time again. Anyone in group can explain the role of buffer_size. I read the document of tensorflow at https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle In my thought, when set buffer_size = 1, as document said, maybe i can replace element in buffer_size. But the output I get make me confused. can anyone run into the same problem ?",https://stackoverflow.com/questions/63665112,1755017,Documentation Ambiguity
50204609,Is there a way to partition a tf.Dataset with TensorFlow’s Dataset API?,"<p>I checked <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">the doc</a> but I could not find a method for it. I want to de cross validation, so I kind of need it.</p>

<p>Note that I'm not asking how to split a tensor, as I know that TensorFlow provides an API for that an has been answered in another question. I'm asking on how to partition a tf.Dataset (which is an abstraction).</p>
","I checked the doc but I could not find a method for it. I want to de cross validation, so I kind of need it. Note that I'm not asking how to split a tensor, as I know that TensorFlow provides an API for that an has been answered in another question. I'm asking on how to partition a tf.Dataset (which is an abstraction).",https://stackoverflow.com/questions/50204609,1120410,Inadequate Examples
55319623,Large dataset processing for Tensorflow Federated,"<p>What is the efficient way to prepare ImageNet (or other big datasets) for Tensorflow federated simulations? Particularly with applying custom map function on tf.Dataset object? I looked into the tutorials and docs but did not find anything helpful for this usecase. This tutorial (<a href=""https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2"" rel=""nofollow noreferrer"">https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2</a>) shows MNIST processing but this dataset is relatively small.</p>
",What is the efficient way to prepare ImageNet (or other big datasets) for Tensorflow federated simulations? Particularly with applying custom map function on tf.Dataset object? I looked into the tutorials and docs but did not find anything helpful for this usecase. This tutorial (https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2) shows MNIST processing but this dataset is relatively small.,https://stackoverflow.com/questions/55319623,10966395,Documentation Completeness
58672774,Training using tf.Dataset in TensorFlow 2.0,"<p>I'm having difficulty training my TensorFlow model using a <code>tf.Dataset</code> rather than, say, a <code>pd.DataFrame</code> (which works fine). </p>

<p>I have created a dummy example below that I would expect to work given what I have read online/on the <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">TensorFlow website</a>.</p>

<pre class=""lang-py prettyprint-override""><code>!pip install tensorflow==2.0.0 &gt; /dev/null

import numpy as np
import tensorflow as tf

features, target = np.random.rand(100, 30), np.random.randint(0, 2, 100)
dataset = tf.data.Dataset.from_tensor_slices((features, target))

model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation='relu', input_shape=(30,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Dense(30, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.fit(
    dataset, 
    epochs=10,
)
</code></pre>

<p>which returns the following error message</p>

<pre><code>...

ValueError: Error when checking input: expected dense_input to have shape (30,) but got array with shape (1,)
</code></pre>

<p>Is there anything obviously wrong in the above? Why is TensorFlow grabbing an input with shape <code>(1,)</code>?</p>
","I'm having difficulty training my TensorFlow model using a tf.Dataset rather than, say, a pd.DataFrame (which works fine). I have created a dummy example below that I would expect to work given what I have read online/on the TensorFlow website. which returns the following error message Is there anything obviously wrong in the above? Why is TensorFlow grabbing an input with shape (1,)?",https://stackoverflow.com/questions/58672774,11978086,Documentation Replication on Other Examples
72484718,"Tensorflow tf.dataset won't iterate with multiple inputs of different sizes ""Shapes of all inputs must match""","<p>I am trying to make a tensorflow model with two different inputs, one will have shape [9,10], the other will just have shape [8].</p>
<p>I am furthermore trying to use tf.dataset to iterate over my inputs.  However, whenever I try to do so it fails with the following error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes of all inputs must match: values[0].shape = [9,10] != values[1].shape = [8]
     [[{{node packed}}]] [Op:IteratorGetNext]
</code></pre>
<p>But surely it is possible to have differently sized inputs into different branches!  This is exactly the case in the example in tensorflow's functional API guide, however they do not use tf.dataset so I can't simply follow their example.</p>
<p>To give a little more specifics into the problem I am trying to solve and why I am using the tf.dataset api:</p>
<p>I am doing a time-series problem over multiple sites where my inputs are of two types: those that vary with time, and those that do not but do vary by site. For the time being, I'm just trying to estimate the next time step.</p>
<p>First, I get my dynamic covariates and targets in a sliding window using the timeseries_dataset_from_array util.</p>
<pre><code>    train_ds = tf.keras.preprocessing.timeseries_dataset_from_array(
        input_data, targets, sequence_length=window_size, batch_size=256)
</code></pre>
<p>This works perfectly and I can train models using this dataset as is.</p>
<p>However, I want to also use the static covariates from the specific site that the time series data is coming from.  The site id is included in the window input data in its own column, though it gets removed before training. Thus, what I am trying to do is grab the static covariates for the site and attach it as a separate input to my dataset.</p>
<pre><code>    train_ds = train_ds.map(lambda x, y: (tf.py_function(attach_static_covariates, [x, idindex, colnames], [tf.float64, tf.float64]), y))

    train_ds = train_ds.map(lambda x, y: ({'dynamic': x[0], 'static': x[1]}, y))
</code></pre>
<p>The code for the attach_static_covariates method is:</p>
<pre><code>def attach_static_covariates(x, idindex, colnames):
    id = x[0, idindex].numpy()
    static_cov = static_df.iloc[int(id)]
    #This just filters out the id column, now that it has served its purpose
    x = tf.gather(x, [i for i in range(len(colnames)) if i != idindex])

    return (x, static_cov)
</code></pre>
<p>I've confirmed that my code can run and train on multiple inputs provided by the above method provided they are the same size (e.g. if I return (x, x) I can run my model on two copies of the dynamic covariates inputted into two different branches of my model).  The problem is not due to a mismatch or a bad model definition because I get the same error from the following code:</p>
<pre><code>    for feature_batch, label_batch in train_ds.take(1):
        print(feature_batch)
</code></pre>
<p>I've looked everywhere on google and the tensorflow git and I can't find anyone else with this problem, and yet it surely MUST be possible to have differently shaped inputs using tf.dataset!  I can't imagine that such an incredibly common use case would be completely unsupported.  However, I can't find any examples online where someone has multiple inputs of different shapes and uses tf.dataset api.  Any help or links to such examples would be greatly appreciated.</p>
<p>Colab notebook to illustrate issue:
<a href=""https://colab.research.google.com/drive/1EnaJoUULl-fyAwlcG_5tcWfsRFVCOMtv#scrollTo=PHvsIOx6-Uux"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1EnaJoUULl-fyAwlcG_5tcWfsRFVCOMtv#scrollTo=PHvsIOx6-Uux</a></p>
","I am trying to make a tensorflow model with two different inputs, one will have shape [9,10], the other will just have shape [8]. I am furthermore trying to use tf.dataset to iterate over my inputs. However, whenever I try to do so it fails with the following error: But surely it is possible to have differently sized inputs into different branches! This is exactly the case in the example in tensorflow's functional API guide, however they do not use tf.dataset so I can't simply follow their example. To give a little more specifics into the problem I am trying to solve and why I am using the tf.dataset api: I am doing a time-series problem over multiple sites where my inputs are of two types: those that vary with time, and those that do not but do vary by site. For the time being, I'm just trying to estimate the next time step. First, I get my dynamic covariates and targets in a sliding window using the timeseries_dataset_from_array util. This works perfectly and I can train models using this dataset as is. However, I want to also use the static covariates from the specific site that the time series data is coming from. The site id is included in the window input data in its own column, though it gets removed before training. Thus, what I am trying to do is grab the static covariates for the site and attach it as a separate input to my dataset. The code for the attach_static_covariates method is: I've confirmed that my code can run and train on multiple inputs provided by the above method provided they are the same size (e.g. if I return (x, x) I can run my model on two copies of the dynamic covariates inputted into two different branches of my model). The problem is not due to a mismatch or a bad model definition because I get the same error from the following code: I've looked everywhere on google and the tensorflow git and I can't find anyone else with this problem, and yet it surely MUST be possible to have differently shaped inputs using tf.dataset! I can't imagine that such an incredibly common use case would be completely unsupported. However, I can't find any examples online where someone has multiple inputs of different shapes and uses tf.dataset api. Any help or links to such examples would be greatly appreciated. Colab notebook to illustrate issue: https://colab.research.google.com/drive/1EnaJoUULl-fyAwlcG_5tcWfsRFVCOMtv#scrollTo=PHvsIOx6-Uux",https://stackoverflow.com/questions/72484718,15913381,Lack of Alternative Solutions/Documentation
48552103,"Why does the TensorFlow tf.data.Dataset.shuffle function's reshuffle_each_iteration boolean argument default to None, as opposed to True?","<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"" rel=""nofollow noreferrer"">documentation for the tf.Dataset.data.shuffle function</a> states the following:</p>

<blockquote>
  <ul>
  <li><strong>reshuffle_each_iteration</strong>: (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to True.)</li>
  </ul>
</blockquote>

<p>However, the default value in the function is None, as mentioned on the same page and in <a href=""https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/data/ops/dataset_ops.py#L590"" rel=""nofollow noreferrer"">the actual code</a>:</p>

<pre><code>def shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None):
</code></pre>

<p>The function calls the ShuffleDataset class, whose <code>__init__</code> function also <a href=""https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/data/ops/dataset_ops.py#L1245"" rel=""nofollow noreferrer"">sets the same argument to None by default</a>, and <a href=""https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/data/ops/dataset_ops.py#L1292"" rel=""nofollow noreferrer"">uses the following logic</a> to set the default value of the argument to True:</p>

<pre><code>if reshuffle_each_iteration is None:
  self._reshuffle_each_iteration = True
else:
  self._reshuffle_each_iteration = reshuffle_each_iteration
</code></pre>

<p>Why isn't the argument just set to True by default in both the function and the class? This would make the above code block redundant and allow replacing it with only <code>self._reshuffle_each_iteration = reshuffle_each_iteration</code>.</p>
","The documentation for the tf.Dataset.data.shuffle function states the following: However, the default value in the function is None, as mentioned on the same page and in the actual code: The function calls the ShuffleDataset class, whose __init__ function also sets the same argument to None by default, and uses the following logic to set the default value of the argument to True: Why isn't the argument just set to True by default in both the function and the class? This would make the above code block redundant and allow replacing it with only self._reshuffle_each_iteration = reshuffle_each_iteration.",https://stackoverflow.com/questions/48552103,6457747,Documentation Ambiguity
50245039,record_defaults of tf.decode_csv in tensorflow,"<p>I used tf.decode_csv in tensorflow as decoder to parse training examples in a tab-delimited file into cnn models. For every training example, the features are 2 dimensions (100 columns, 2000 rows). After reading the document in tensorflow official site, I still have two questions. </p>

<ol>
<li>how to create record_defaults? The following is my code to do that, but I
am not sure if it is right.</li>
</ol>

<p>code</p>

<pre><code>filename_queue = tf.train.string_input_producer([file], num_epochs)

key, value = tf.TextLineReader().read(filename_queue)

record_defaults = [[1.0 for col in range(0, 100)] for row in range(0, 2000)]

content = tf.decode_csv(value, record_defaults = record_defaults, field_delim = '\t')

features = tf.pack(content[0:1999])
</code></pre>

<ol start=""2"">
<li>I am doing binary (0, 1) classification. Where do I put the labels for training examples? in the 2001th row? (For every training example, the first 2000 rows for features, and the 2001th row for label)</li>
</ol>

<p>Thanks for your time!</p>
","I used tf.decode_csv in tensorflow as decoder to parse training examples in a tab-delimited file into cnn models. For every training example, the features are 2 dimensions (100 columns, 2000 rows). After reading the document in tensorflow official site, I still have two questions. code Thanks for your time!",https://stackoverflow.com/questions/50245039,9378677,Documentation Completeness
59906819,What is the correct explanation for tf.dense?,"<p>I am using a tutorial to learn RNN. 
As shown in the image below, it has used tf.dense and given an explanation which I cannot understand. </p>

<p>(As a newbie to stackoverflow, I cannot insert images, hence the link)</p>

<p>The documentation at tensorflow.org also does not help much and it is so bad for a beginner like me that I've given a 1-star rating. It says the following (which does not make sense to me)</p>

<p><a href=""https://i.stack.imgur.com/cjFv5.png"" rel=""nofollow noreferrer"">The explanation at tensorflow.org</a></p>

<p>Can someone kindly explain it to me. Thank you</p>

<p><a href=""https://i.stack.imgur.com/tGIYM.png"" rel=""nofollow noreferrer"">Excerpt from the tutorial I am using to learn.</a></p>
","I am using a tutorial to learn RNN. As shown in the image below, it has used tf.dense and given an explanation which I cannot understand. (As a newbie to stackoverflow, I cannot insert images, hence the link) The documentation at tensorflow.org also does not help much and it is so bad for a beginner like me that I've given a 1-star rating. It says the following (which does not make sense to me) The explanation at tensorflow.org Can someone kindly explain it to me. Thank you Excerpt from the tutorial I am using to learn.",https://stackoverflow.com/questions/59906819,12483947,Documentation Completeness
46851689,Python-Tensorflow running on GPU instead of CPU,"<p>I have the sample tensorflow code below</p>

<pre><code>import tensorflow as tf
with tf.device('/cpu:0'):
  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
print(sess)
print(sess.run(c))
</code></pre>

<p>explicitly i have given tf.device('/cpu:0') but it is giving the following error :</p>

<pre><code>InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'MatMul_5': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.
     [[Node: MatMul_5 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/device:GPU:0""](a_5, b_5)]]
</code></pre>

<p>Tensorflow version : 1.3.0,
python version : 3.6.1 with Anaconda Distribution</p>
","I have the sample tensorflow code below explicitly i have given tf.device('/cpu:0') but it is giving the following error : Tensorflow version : 1.3.0, python version : 3.6.1 with Anaconda Distribution",https://stackoverflow.com/questions/46851689,6490241,Requesting (Additional) Resources
53845742,How to construct a multi-tower tensorflow graph from predefined models,"<p>I have a model that has been trained and stored to file in Tensorflow. It was a pretrained model that had some parameter tweaking done to produce the required results. Now I am looking to take that model and run it in parallel across multiple GPUs for inference.</p>

<p>I can only find resources showing how to produce multi-GPU towers when the graph is initially defined by using <code>tf.device</code> when defining the operations, but since I am reading in a GraphDef from file I don't have that option.</p>

<p>For reference, the model loading code (identical to what is on the tensorflow hub's <code>label_image.py</code>):</p>

<pre class=""lang-py prettyprint-override""><code>def load_graph(model_file):
  graph = tf.Graph()
  graph_def = tf.GraphDef()

  with open(model_file, ""rb"") as f:
    graph_def.ParseFromString(f.read())
  with graph.as_default():
    tf.import_graph_def(graph_def)

  return graph
</code></pre>
","I have a model that has been trained and stored to file in Tensorflow. It was a pretrained model that had some parameter tweaking done to produce the required results. Now I am looking to take that model and run it in parallel across multiple GPUs for inference. I can only find resources showing how to produce multi-GPU towers when the graph is initially defined by using tf.device when defining the operations, but since I am reading in a GraphDef from file I don't have that option. For reference, the model loading code (identical to what is on the tensorflow hub's label_image.py):",https://stackoverflow.com/questions/53845742,3773546,Requesting (Additional) Resources
38937984,"distributed tensorflow on localhosts failed by ""socket error, connection refused""","<p>I am experimenting distributed tensorflow using a slight modification of an <a href=""https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html"" rel=""nofollow noreferrer"">official example</a>.</p>

<p>My experiment code is (you can skip this for now and scroll down to the problem),</p>

<pre><code>import tensorflow as tf
import numpy as np

# Flags for defining the tf.train.ClusterSpec
tf.app.flags.DEFINE_string(""ps_hosts"", """",
                           ""Comma-separated list of hostname:port pairs"")
tf.app.flags.DEFINE_string(""worker_hosts"", """",
                           ""Comma-separated list of hostname:port pairs"")

# Flags for defining the tf.train.Server
tf.app.flags.DEFINE_string(""job_name"", """", ""One of 'ps', 'worker'"")
tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")

FLAGS = tf.app.flags.FLAGS


def main(_):
    ps_hosts = FLAGS.ps_hosts.split("","")
    worker_hosts = FLAGS.worker_hosts.split("","")

    # Create a cluster from the parameter server and worker hosts.
    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})

    # Create and start a server for the local task.
    server = tf.train.Server(cluster,
                             job_name=FLAGS.job_name,
                             task_index=FLAGS.task_index)

    if FLAGS.job_name == ""ps"":
        server.join()
    elif FLAGS.job_name == ""worker"":
        # Assigns ops to the local worker by default.
        with tf.device(tf.train.replica_device_setter(
                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,
                cluster=cluster)):

            # Build model...
            x = tf.placeholder(""float"", [10, 10], name=""x"")
            y = tf.placeholder(""float"", [10, 1], name=""y"")
            initial_w = np.zeros((10, 1))
            w = tf.Variable(initial_w, name=""w"", dtype=""float32"")
            loss = tf.pow(tf.add(y,-tf.matmul(x,w)),2,name=""loss"")
            global_step = tf.Variable(0)

            saver = tf.train.Saver()
            summary_op = tf.merge_all_summaries()
            init_op = tf.initialize_all_variables()

        # Create a ""supervisor"", which oversees the training process.
        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),
                                 logdir=""/tmp/train_logs"",
                                 init_op=init_op,
                                 summary_op=summary_op,
                                 saver=saver,
                                 global_step=global_step,
                                 save_model_secs=600)

        # The supervisor takes care of session initialization, restoring from
        # a checkpoint, and closing when done or an error occurs.
        with sv.managed_session(server.target) as sess:
            # Loop until the supervisor shuts down or 1000000 steps have completed.
            step = 0
            while not sv.should_stop() and step &lt; 1000000:
                # Run a training step asynchronously.
                # See `tf.train.SyncReplicasOptimizer` for additional details on how to
                # perform *synchronous* training.
                _, step = sess.run([loss, global_step])
                print(""job_name: %s; task_index: %s; step: %d"" % (FLAGS.job_name,FLAGS.task_index,step))

        # Ask for all the services to stop.
        sv.stop()


if __name__ == ""__main__"":
    tf.app.run()
</code></pre>

<p>Then I run the following commands as instructed by the official document (the script is named hello_distributed.py),</p>

<pre><code>sudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=ps --task_index=0

sudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=ps --task_index=1

sudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=worker --task_index=0

sudo python3 hello_distributed.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2777,localhost:2778 --job_name=worker --task_index=1
</code></pre>

<p>The first two lines for running ""ps"" are good. The last two lines get the following ""connection refused"" error. </p>

<p><a href=""https://i.stack.imgur.com/zkGPi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zkGPi.png"" alt=""enter image description here""></a></p>

<p>Thank you!</p>
","I am experimenting distributed tensorflow using a slight modification of an official example. My experiment code is (you can skip this for now and scroll down to the problem), Then I run the following commands as instructed by the official document (the script is named hello_distributed.py), The first two lines for running ""ps"" are good. The last two lines get the following ""connection refused"" error. Thank you!",https://stackoverflow.com/questions/38937984,4089301,Documentation Replication on Other Examples
71830863,MirroredStrategy causes IndexError: pop from empty list when using Keras Sequences as model input,"<p>While the <a href=""https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy"" rel=""nofollow noreferrer"">MirroredStrategy</a>'s <code>IndexError: pop from empty list</code> is now infamous and there are numerous possible causes for it, such as reported in the following questions:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/68309367/keras-multigpu-training-fails-with-error-message-indexerror-pop-from-empty-li"">MirroredStrategy <code>IndexError</code> caused by <code>K.clear_session()</code></a></li>
<li><a href=""https://github.com/keras-team/autokeras/issues/1101"" rel=""nofollow noreferrer"">MirroredStrategy <code>IndexError</code> within AutoKeras</a></li>
<li><a href=""https://github.com/tensorflow/tensorflow/issues/19744"" rel=""nofollow noreferrer"">MirroredStrategy <code>IndexError</code> when training from checkpoint</a></li>
</ul>
<p>And so forth, but none apply to my use case.</p>
<p>In my use case, I'm using <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"" rel=""nofollow noreferrer""><code>Keras Sequence</code></a> objects to generate the training inputs, as I'm working on large datasets (would not fit in RAM) with a single known positive class and unknown negatives.</p>
<p>Following tutorials such as the one available on the <a href=""https://keras.io/getting_started/faq/#how-can-i-train-a-keras-model-on-multiple-gpus-on-a-single-machine"" rel=""nofollow noreferrer"">Keras Documentation</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy"" rel=""nofollow noreferrer"">TensorFlow documentation</a> my code looks like the following:</p>
<pre class=""lang-py prettyprint-override""><code>
my_training_sequence = MySequenceObject()

if tf.config.list_physical_devices('GPU'):
    strategy = tf.distribute.MirroredStrategy(devices)
else:
    # Use the Default Strategy
    strategy = tf.distribute.get_strategy()

with strategy.scope():
    model = CreateMyKerasModel()
    # While in the TensorFlow documentation the compilation step
    # is shown OUTSIDE the scope, in the Keras one it happens
    # within the scope.
    # I  have found out that is NECESSARY to place it inside the scope
    # as the Keras Metrics need to be in the same strategy scope of the model
    # to work properly.
    model.compile(...)

# Then, OUSIDE from the score, run the fit
# which causes the IndexError
model.fit(my_training_sequence)
</code></pre>
<p>Any ideas on how to deal with this?</p>
","While the MirroredStrategy's IndexError: pop from empty list is now infamous and there are numerous possible causes for it, such as reported in the following questions: And so forth, but none apply to my use case. In my use case, I'm using Keras Sequence objects to generate the training inputs, as I'm working on large datasets (would not fit in RAM) with a single known positive class and unknown negatives. Following tutorials such as the one available on the Keras Documentation and TensorFlow documentation my code looks like the following: Any ideas on how to deal with this?",https://stackoverflow.com/questions/71830863,2550541,Documentation Replicability
46087294,using tf.nn.dynamic_rnn to make LSTM RNN of multiple hidden layers,"<p>I read the documentation of tf.dynamic.rnn in
<a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a></p>

<p>and used it to make a single layer rnn of multiple time-steps. I was wondering if I could use tf.dynamic.rnn to stack multiple hidden layers. Is it possible to do so? </p>
",I read the documentation of tf.dynamic.rnn in https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn and used it to make a single layer rnn of multiple time-steps. I was wondering if I could use tf.dynamic.rnn to stack multiple hidden layers. Is it possible to do so?,https://stackoverflow.com/questions/46087294,6147251,Lack of Alternative Solutions/Documentation
48431870,What does DNN mean in a TensorFlow Estimator.DNNClassifier?,"<p>I'm guessing that <code>DNN</code> in the sense used in <code>TensorFlow</code> means ""<a href=""http://www.abbreviations.com/DNN"" rel=""noreferrer"">deep neural network</a>"". But I find this deeply confusing since the notion of a ""deep"" neural network seems to be in wide use elsewhere to mean a network with typically several convolutional and/or associated layers (ReLU, pooling, dropout, etc).</p>

<p>In contrast,  the first instance many people will encounter this term (in the <a href=""https://www.tensorflow.org/get_started/estimator"" rel=""noreferrer"">tfEstimator Quickstart example code</a>) we find:</p>

<pre><code># Build 3 layer DNN with 10, 20, 10 units respectively.
  classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                                          hidden_units=[10, 20, 10],
                                          n_classes=3,
                                          model_dir=""/tmp/iris_model"")
</code></pre>

<p>This sounds suspiciously shallow, and even more suspiciously like an old-style multilayer perceptron (<a href=""https://en.wikipedia.org/wiki/Multilayer_perceptron"" rel=""noreferrer"">MLP</a>) network. However, there is no mention of <code>DNN</code> as an alternative term on that close-to-definitive source. So is a <code>DNN</code> in the TensorFlow <code>tf.estimator</code> context actually an <code>MLP</code>? Documentation on the <code>hidden_units</code> parameter suggests this is the case:</p>

<ul>
<li><strong>hidden_units:</strong> Iterable of number hidden units per layer. All layers are fully connected. Ex. [64, 32] means first layer has 64 nodes and second one has 32.</li>
</ul>

<p>That has MLP written all over it. Is this understanding correct? Is <code>DNN</code> therefore a misnomer, and if so should <code>DNNClassifier</code> ideally be deprecated in favour of <code>MLPClassifier</code>? Or does <code>DNN</code> stand for something other than <em>deep neural network</em>?</p>
","I'm guessing that DNN in the sense used in TensorFlow means ""deep neural network"". But I find this deeply confusing since the notion of a ""deep"" neural network seems to be in wide use elsewhere to mean a network with typically several convolutional and/or associated layers (ReLU, pooling, dropout, etc). In contrast, the first instance many people will encounter this term (in the tfEstimator Quickstart example code) we find: This sounds suspiciously shallow, and even more suspiciously like an old-style multilayer perceptron (MLP) network. However, there is no mention of DNN as an alternative term on that close-to-definitive source. So is a DNN in the TensorFlow tf.estimator context actually an MLP? Documentation on the hidden_units parameter suggests this is the case: That has MLP written all over it. Is this understanding correct? Is DNN therefore a misnomer, and if so should DNNClassifier ideally be deprecated in favour of MLPClassifier? Or does DNN stand for something other than deep neural network?",https://stackoverflow.com/questions/48431870,1099237,Documentation Ambiguity
52864435,Measuring GPU memory usage with TensorFlow profiler,"<p>Is there a way to properly measure GPU usage of tf.Estimator model using TensorFlow profiler? I've followed the documentation:</p>

<pre><code>g = tf.Graph()
sess = tf.Session(graph=g)
run_meta = tf.RunMetadata()

time_and_memory_args = tf.profiler.ProfileOptionBuilder.time_and_memory()

with g.as_default():
    data_shape = [BATCH_SIZE] + [3, 224, 224]
    in_plh = tf.placeholder(tf.float32, data_shape)

    model = some_model(in_plh, args=model_args, training=True)
    images = np.random.rand(BATCH_SIZE, 3, 224, 224)

    sess.run(tf.global_variables_initializer())
    sess.run(model, feed_dict={in_plh: images})

    time_and_memory = tf.profiler.profile(g, run_meta=run_meta, cmd='op',
                                          options=time_and_memory_args) 

    if time_and_memory is not None:
        print('Total requested bytes:', time_and_memory.total_requested_bytes)
</code></pre>

<p>But the printed result is always 0.</p>
",Is there a way to properly measure GPU usage of tf.Estimator model using TensorFlow profiler? I've followed the documentation: But the printed result is always 0.,https://stackoverflow.com/questions/52864435,5991102,Documentation Replicability
47856852,Estimator predict infinite loop,"<p>I don't understand how to make a single prediction using TensorFlow Estimator API - my code results in an endless loop that keeps predicting for the same input.</p>

<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#predict"" rel=""noreferrer"">documentation</a>, the prediction is supposed to stop when input_fn raises a StopIteration exception:</p>

<blockquote>
  <p>input_fn: Input function returning features which is a dictionary of
  string feature name to Tensor or SparseTensor. If it returns a tuple,
  first item is extracted as features. Prediction continues until
  input_fn raises an end-of-input exception (OutOfRangeError or
  StopIteration).</p>
</blockquote>

<p>Here's the relevant part in my code:</p>

<pre><code>classifier = tf.estimator.Estimator(model_fn=image_classifier, model_dir=output_dir,
                                    config=training_config, params=hparams)

def make_predict_input_fn(filename):
    queue = [ filename ]
    def _input_fn():
        if len(queue) == 0:
            raise StopIteration
        image = model.read_and_preprocess(queue.pop())
        return {'image': image}
    return _input_fn

predictions = classifier.predict(make_predict_input_fn('garden-rose-red-pink-56866.jpeg'))
for i, p in enumerate(predictions):
    print(""Prediction %s: %s"" % (i + 1, p[""class""]))
</code></pre>

<p>What am I missing?</p>
","I don't understand how to make a single prediction using TensorFlow Estimator API - my code results in an endless loop that keeps predicting for the same input. According to the documentation, the prediction is supposed to stop when input_fn raises a StopIteration exception: Here's the relevant part in my code: What am I missing?",https://stackoverflow.com/questions/47856852,2375105,Documentation Replicability
50164090,Tensorflow ServingInputReceiver input shape error in client,"<p>I'm currently working with tensorflow Estimator API and have problems with the confusing serving options that are available. My confusion comes from the very undetailed tensorflow documentation.</p>

<p>This is my goal:
Use tensorflow-serving prediction_service_pb2 by sending a serialized proto message as string to the ServingInputReceiver function of my exported Estimator model. I expect the ServingInputReceiver function to receive the serialized proto string on the ""input"" tensor which then will deserialize it to the features ""ink"" (=varlength float array) and ""shape"" (=fixedlength int64).</p>

<p><strong>This is my (implementation of google quickdraw model) estimator Input function:</strong></p>

<pre><code>def _parse_tfexample_fn(example_proto, mode):
    """"""Parse a single record which is expected to be a tensorflow.Example.""""""
    feature_to_type = {
        ""ink"": tf.VarLenFeature(dtype=tf.float32),
        ""shape"": tf.FixedLenFeature([2], dtype=tf.int64)
    }
    if mode != tf.estimator.ModeKeys.PREDICT:
        # The labels won't be available at inference time, so don't add them
        # to the list of feature_columns to be read.
        feature_to_type[""class_index""] = tf.FixedLenFeature([1], dtype=tf.int64)

    parsed_features = tf.parse_single_example(example_proto, feature_to_type)
    parsed_features[""ink""] = tf.sparse_tensor_to_dense(parsed_features[""ink""])

    if mode != tf.estimator.ModeKeys.PREDICT:
        labels = parsed_features[""class_index""]
        return parsed_features, labels
    else:
        return parsed_features  # In prediction, we have no labels
</code></pre>

<p><strong>This is my Serving Input Function:</strong></p>

<pre><code>def serving_input_receiver_fn():
""""""An input receiver that expects a serialized tf.Example.""""""
feature_to_type = {""ink"": tf.VarLenFeature(dtype=tf.float32), ""shape"": tf.FixedLenFeature([2], dtype=tf.int64)}

serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[None], name='input')

parsed_features = tf.parse_example(serialized_tf_example, feature_to_type)
parsed_features[""ink""] = tf.sparse_tensor_to_dense(parsed_features[""ink""])

return tf.estimator.export.ServingInputReceiver(parsed_features, serialized_tf_example)
</code></pre>

<p><strong>This is my client.py request:</strong></p>

<pre><code>features = {}
features[""ink""] = tf.train.Feature(float_list=tf.train.FloatList(value=np_ink.flatten()))
features[""shape""] = tf.train.Feature(int64_list=tf.train.Int64List(value=np_ink.shape))
f = tf.train.Features(feature=features)
data = tf.train.Example(features=f)
serialized=data.SerializeToString() # tensor to byte string
request.inputs['input'].ParseFromString(tf.contrib.util.make_tensor_proto(serialized, shape=[1], verify_shape=True))
</code></pre>

<p><strong>And this is the error I get after calling the Predict function in client.py</strong></p>

<pre><code>grpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=""input tensor alias not found in signature: ink. Inputs expected to be in the set {input}."")
</code></pre>

<p><strong>I tried the following Servingfunctions:</strong>
<strong>ServingInputReceiver</strong> and <strong>build_raw_serving_input_receiver_fn</strong> give me the same grpc error. When I use <strong>build_parsing_serving_input_receiver_fn</strong> it wont even export my model. I tried to wrap my head around the documentation but it is very undetailed and I don't understand when to use which serving input function.</p>
","I'm currently working with tensorflow Estimator API and have problems with the confusing serving options that are available. My confusion comes from the very undetailed tensorflow documentation. This is my goal: Use tensorflow-serving prediction_service_pb2 by sending a serialized proto message as string to the ServingInputReceiver function of my exported Estimator model. I expect the ServingInputReceiver function to receive the serialized proto string on the ""input"" tensor which then will deserialize it to the features ""ink"" (=varlength float array) and ""shape"" (=fixedlength int64). This is my (implementation of google quickdraw model) estimator Input function: This is my Serving Input Function: This is my client.py request: And this is the error I get after calling the Predict function in client.py I tried the following Servingfunctions: ServingInputReceiver and build_raw_serving_input_receiver_fn give me the same grpc error. When I use build_parsing_serving_input_receiver_fn it wont even export my model. I tried to wrap my head around the documentation but it is very undetailed and I don't understand when to use which serving input function.",https://stackoverflow.com/questions/50164090,8804834,Documentation Completeness
62490870,How to add random noise to the labels using DNNLinearCombinedClassifier for tf1.15?,"<p>Currently, I am working with the wide and deep classifier in tf1.15 (DNNLinearCombinedClassifier), and defined the following function to add random noise to the labels:</p>
<pre><code>def add_random_noise(labels):
    labels = tf.dtypes.cast(labels, tf.float32)
    rnd_noise = tf.random.uniform(tf.shape(labels))
    return tf.add(labels, tf.math.multiply_no_nan(0.05, rnd_noise))
</code></pre>
<p>This function is then applied to the dataset using <code>map</code> and applied only to the target column. However when I train my model, it generates the following output:</p>
<pre><code>(0) Invalid argument: assertion failed: [Labels must &lt;= n_classes - 1] [Condition x &lt;= y did not hold element-wise:] [x (head/labels:0) = ] [[0.00486446638][0.0133116841][0.0143840136]...] [y (head/assert_range/Const:0) = ] [1]
</code></pre>
<p>As far as I understand, <code>DNNLinearCombinedClassifier</code> is calculating the number of unique values in target and generated the error due to the difference between n_classes and the number of unique values. I reviewed the documentation of <code>DNNLinearCombinedEstimator</code>, but it uses MSE as loss, and I would definetely like to train my model using cross entropy as it is done in <code>DNNLinearCombinedClassifier</code>. I would like to ask you if there is a way to use wide and deep classifier with this <em>perturbed</em> label column and cross entropy as my training loss function.</p>
","Currently, I am working with the wide and deep classifier in tf1.15 (DNNLinearCombinedClassifier), and defined the following function to add random noise to the labels: This function is then applied to the dataset using map and applied only to the target column. However when I train my model, it generates the following output: As far as I understand, DNNLinearCombinedClassifier is calculating the number of unique values in target and generated the error due to the difference between n_classes and the number of unique values. I reviewed the documentation of DNNLinearCombinedEstimator, but it uses MSE as loss, and I would definetely like to train my model using cross entropy as it is done in DNNLinearCombinedClassifier. I would like to ask you if there is a way to use wide and deep classifier with this perturbed label column and cross entropy as my training loss function.",https://stackoverflow.com/questions/62490870,997333,Lack of Alternative Solutions/Documentation
51330841,How to save and restore a tf.estimator.Estimator model with export_savedmodel?,"<p>I started using Tensorflow recently and I try to get use to tf.estimator.Estimator objects. I would like to do something a priori quite natural: after having trained my classifier, i.e. an instance of tf.estimator.Estimator (with the <code>train</code> method), I would like to save it in a file (whatever the extension) and then reload it later to predict the labels for some new data. Since the official documentation recommends to use Estimator APIs, I guess something as important as that should be implemented and documented.</p>

<p>I saw on some other page that the method to do that is <code>export_savedmodel</code> (see <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Estimator#export_savedmodel"" rel=""nofollow noreferrer"">the official documentation</a>) but I simply don't understand the documentation. There is no explanation of how to use this method. What is the argument <code>serving_input_fn</code>? I never encountered it in the <a href=""https://www.tensorflow.org/guide/custom_estimators"" rel=""nofollow noreferrer"">Creating Custom Estimators</a> tutorial or in any of the tutorials that I read. By doing some googling, I discovered that around a year ago the estimators where defined using an other class (<code>tf.contrib.learn.Estimator</code>) and it looks like the tf.estimator.Estimator is reusing some of the previous APIs. But I don't find clear explanations in the documentation about it.</p>

<p>Could someone please give me a toy example? Or explain me how to define/find this <code>serving_input_fn</code>?</p>

<p>And then how would be load the trained classifier again?</p>

<p>Thank you for your help!</p>

<p><strong>Edit:</strong> I discovered that one doesn't necessarily need to use export_savemodel to save the model. It is actually done automatically. Then if we define later a new estimator having the same model_dir argument, it will also automatically restore the previous estimator, as explained <a href=""https://www.tensorflow.org/guide/checkpoints"" rel=""nofollow noreferrer"">here</a>.</p>
","I started using Tensorflow recently and I try to get use to tf.estimator.Estimator objects. I would like to do something a priori quite natural: after having trained my classifier, i.e. an instance of tf.estimator.Estimator (with the train method), I would like to save it in a file (whatever the extension) and then reload it later to predict the labels for some new data. Since the official documentation recommends to use Estimator APIs, I guess something as important as that should be implemented and documented. I saw on some other page that the method to do that is export_savedmodel (see the official documentation) but I simply don't understand the documentation. There is no explanation of how to use this method. What is the argument serving_input_fn? I never encountered it in the Creating Custom Estimators tutorial or in any of the tutorials that I read. By doing some googling, I discovered that around a year ago the estimators where defined using an other class (tf.contrib.learn.Estimator) and it looks like the tf.estimator.Estimator is reusing some of the previous APIs. But I don't find clear explanations in the documentation about it. Could someone please give me a toy example? Or explain me how to define/find this serving_input_fn? And then how would be load the trained classifier again? Thank you for your help! Edit: I discovered that one doesn't necessarily need to use export_savemodel to save the model. It is actually done automatically. Then if we define later a new estimator having the same model_dir argument, it will also automatically restore the previous estimator, as explained here.",https://stackoverflow.com/questions/51330841,8733572,Requesting (Additional) Resources
54060667,Continue training of a custom tf.Estimator with AdamOptimizer,"<p>I created a custom tf.Estimator whose weights I'm training using the tf.train.AdamOptimizer. When I continue training of an existing model, I observe a steep change in the metrics at the start of the continued training in Tensorboard. After a few steps, the metrics stabilise. The behaviour looks similar to the initial transients when training a model. The behaviour is the same if I continue training on the same Estimator instance, or if I recreate the estimator from a checkpoint. I suspect that the moving averages and/or the bias correction factor are reset when restarting the training. The model weights themselves seem to be properly restored, as the metrics do continue from where they settled before, only the effective learning rate seems to be too high.</p>

<p>Previous Stack-Overflow answers seem to suggest that these auxiliary learning parameters should be stored with the checkpoints together with the model weights. So what am I doing wrong here? How can I control restoring of these auxiliary variables? I would like to be able to continue training as if it had never been stopped. However, other people sometimes seem look for the opposite control, to completely reset the optimizer without resetting the model weights. An answer that shows how both effects can be achieved would probably most helpful.</p>

<p>Here is a sketch of my <code>model_fn</code>:</p>

<pre><code>def model_fn(features, labels, mode, params):
    inputs = features['inputs']
    logits = create_model(inputs, training=mode == tf.estimator.ModeKeys.TRAIN)

    if mode == tf.estimator.ModeKeys.PREDICT:
        ...

    if mode == tf.estimator.ModeKeys.TRAIN:
        outputs = labels['outputs']

        loss = tf.losses.softmax_cross_entropy(
            tf.one_hot(outputs,tf.shape(inputs)[-1]),
            logits,
#            reduction=tf.losses.Reduction.MEAN,
        )
        optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)

        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

        with tf.control_dependencies(update_ops):
            train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())

        accuracy = tf.metrics.accuracy(
            labels = outputs,
            predictions = tf.argmax(logits, axis=-1),
        )

        tf.summary.histogram('logits',logits)
        tf.summary.scalar('accuracy', accuracy[1])
        tf.summary.scalar('loss', loss)

        return tf.estimator.EstimatorSpec(
            mode=tf.estimator.ModeKeys.TRAIN,
            loss=loss,
            train_op=train_op)

    if mode == tf.estimator.ModeKeys.EVAL:
        ...

    raise ValueError(mode)
</code></pre>

<p>The training step is called as follows:</p>

<pre><code>cfg = tf.estimator.RunConfig(
    save_checkpoints_secs = 5*60,  # Save checkpoints every 1 minutes.
    keep_checkpoint_max = 10,       # Retain the 10 most recent checkpoints.
    save_summary_steps = 10,
    log_step_count_steps = 100,
)
estimator = tf.estimator.Estimator(
    model_fn = model_fn,
    params = dict(
        learning_rate = 1e-3,
    ),
    model_dir = model_dir,
    config=cfg,
)
# train for the first time
estimator.train(
    input_fn=train_input_fn,
)
# ... at some later time, train again
estimator.train(
    input_fn=train_input_fn,
)
</code></pre>

<p>EDIT:</p>

<p>The documentation of the <code>warm_start_from</code> argument of <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>tf.estimator.Estimator</code></a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings"" rel=""nofollow noreferrer""><code>tf.estimator.WarmStartSettings</code></a> are not entirely clear what exactly will happen in the default case, as I am using in the example above. However, the documentation of [<code>tf.train.warm_start</code>] (<a href=""https://www.tensorflow.org/api_docs/python/tf/train/warm_start"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/train/warm_start</a>) seems to suggest that in the default case, all <code>TRAINABLE_VARIABLES</code> will be warm-started, which</p>

<blockquote>
  <p>excludes variables such as accumulators and moving statistics from batch norm</p>
</blockquote>

<p>Indeed, I find Adam's accumulator variables in <code>VARIABLES</code>, but not in <code>TRAINABLE_VARIABLES</code>. These documentation pages also state how to change the list of warm-started variables, to either a list of <code>tf.Variable</code> instances, or a list of their names. However, one question remains: How do I create one of those lists in advance, given that with <code>tf.Estimator</code>, I have no graph to collect those variables/their names from?</p>

<p>EDIT2:</p>

<p>The source-code of <code>warm_start</code> highlights an undocumented feature: The list of variable names is in fact a list of regexes, to be matched against GLOBAL_VARIABLES. Thus, one may use </p>

<pre><code>    warm_start_from=tf.estimator.WarmStartSettings(
        ckpt_to_initialize_from=str(model_dir),
    #    vars_to_warm_start="".*"", # everything in TRAINABLE_VARIABLES - excluding optimiser params 
        vars_to_warm_start=["".*""], # everything in GLOBAL_VARIABLES - including optimiser params 
    ),
</code></pre>

<p>to load all variables. However, even with that, the spikes in the summary stats remain. With that, I'm completely at a loss now what is going on.</p>
","I created a custom tf.Estimator whose weights I'm training using the tf.train.AdamOptimizer. When I continue training of an existing model, I observe a steep change in the metrics at the start of the continued training in Tensorboard. After a few steps, the metrics stabilise. The behaviour looks similar to the initial transients when training a model. The behaviour is the same if I continue training on the same Estimator instance, or if I recreate the estimator from a checkpoint. I suspect that the moving averages and/or the bias correction factor are reset when restarting the training. The model weights themselves seem to be properly restored, as the metrics do continue from where they settled before, only the effective learning rate seems to be too high. Previous Stack-Overflow answers seem to suggest that these auxiliary learning parameters should be stored with the checkpoints together with the model weights. So what am I doing wrong here? How can I control restoring of these auxiliary variables? I would like to be able to continue training as if it had never been stopped. However, other people sometimes seem look for the opposite control, to completely reset the optimizer without resetting the model weights. An answer that shows how both effects can be achieved would probably most helpful. Here is a sketch of my model_fn: The training step is called as follows: EDIT: The documentation of the warm_start_from argument of tf.estimator.Estimator and tf.estimator.WarmStartSettings are not entirely clear what exactly will happen in the default case, as I am using in the example above. However, the documentation of [tf.train.warm_start] (https://www.tensorflow.org/api_docs/python/tf/train/warm_start) seems to suggest that in the default case, all TRAINABLE_VARIABLES will be warm-started, which Indeed, I find Adam's accumulator variables in VARIABLES, but not in TRAINABLE_VARIABLES. These documentation pages also state how to change the list of warm-started variables, to either a list of tf.Variable instances, or a list of their names. However, one question remains: How do I create one of those lists in advance, given that with tf.Estimator, I have no graph to collect those variables/their names from? EDIT2: The source-code of warm_start highlights an undocumented feature: The list of variable names is in fact a list of regexes, to be matched against GLOBAL_VARIABLES. Thus, one may use to load all variables. However, even with that, the spikes in the summary stats remain. With that, I'm completely at a loss now what is going on.",https://stackoverflow.com/questions/54060667,4112821,Documentation Completeness
56967648,Need help in writing the tf.estimator.export.ServingInputReceiver in Tensorflow 2.0.beta1,"<p>I am trying to serve my Keras model, but my model is trained on image of shape <code>(125,125,3)</code> and it expect an input as numpy array. But as I was learning the <code>tf.estimator.export.ServingInputReceiver</code> to pre-process my <code>b64_enoded</code> image to a numpy array and feed my model with it, I am stuck here.</p>

<p>All my model building and pre-processing is done in tensorflow 2.0. So I am  looking for a help to write the <code>tf.estimator.export.ServingInputReceiver</code> function for it.</p>

<p>Model Summary:</p>

<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_a (Conv2D)            (None, 125, 125, 32)      896       
_________________________________________________________________
Activation_a (Activation)    (None, 125, 125, 32)      0         
_________________________________________________________________
MaxPool_a (MaxPooling2D)     (None, 62, 62, 32)        0         
_________________________________________________________________
Conv2d_c (Conv2D)            (None, 62, 62, 64)        18496     
_________________________________________________________________
Activation_c (Activation)    (None, 62, 62, 64)        0         
_________________________________________________________________
MaxPool_b (MaxPooling2D)     (None, 31, 31, 64)        0         
_________________________________________________________________
Conv2d_d (Conv2D)            (None, 31, 31, 128)       73856     
_________________________________________________________________
Activation_d (Activation)    (None, 31, 31, 128)       0         
_________________________________________________________________
MaxPool_c (MaxPooling2D)     (None, 15, 15, 128)       0         
_________________________________________________________________
Flatten (Flatten)            (None, 28800)             0         
_________________________________________________________________
Dense_a (Dense)              (None, 512)               14746112  
_________________________________________________________________
dropout_a (Dropout)          (None, 512)               0         
_________________________________________________________________
Dense_b (Dense)              (None, 512)               262656    
_________________________________________________________________
dropout_b (Dropout)          (None, 512)               0         
_________________________________________________________________
output (Dense)               (None, 1)                 513       
=================================================================
Total params: 15,102,529
Trainable params: 15,102,529
Non-trainable params: 0
</code></pre>

<p>Here is my code :</p>

<pre class=""lang-py prettyprint-override""><code>
def build_serving_inputs(inputs):
    def decode_and_resize(image_str_tensor):
        image = tf.io.decode_base64(image_str_tensor)
        image = tf.image.decode_png(image, channels=3)
        image = tf.image.resize_with_crop_or_pad(image, 125, 125)
        image = image /255
        #image = tf.expand_dims(image, 0)
        #image = tf.cast(image, dtype=tf.float32)
        return image

    if not isinstance(inputs, np.ndarray):
        inputs = np.array([inputs])

    # Converting the B64 Image data to tensorflow tf.string
    inputs = tf.cast(inputs, tf.string)

    # Decoding the image and returning a numpy array
    decoded_images = tf.map_fn(decode_and_resize, inputs,
        dtype=tf.float32,back_prop=False)
    decoded_images = tf.identity(decoded_images, name=""output"")
    return tf.estimator.export.ServingInputReceiver()

estimator.export_savedmodel(""./model/my_model/"",
serving_input_receiver_fn=build_serving_inputs)

</code></pre>

<p>I want to export my model with the <code>ServingInput</code> Receiver as a pre-processing funtion to convert a <code>b64_encoded</code> image to numpy and feed it to the Model.</p>
","I am trying to serve my Keras model, but my model is trained on image of shape (125,125,3) and it expect an input as numpy array. But as I was learning the tf.estimator.export.ServingInputReceiver to pre-process my b64_enoded image to a numpy array and feed my model with it, I am stuck here. All my model building and pre-processing is done in tensorflow 2.0. So I am looking for a help to write the tf.estimator.export.ServingInputReceiver function for it. Model Summary: Here is my code : I want to export my model with the ServingInput Receiver as a pre-processing funtion to convert a b64_encoded image to numpy and feed it to the Model.",https://stackoverflow.com/questions/56967648,9887738,Requesting (Additional) Resources
53580335,Group by Regression in TensorFlow,"<p>I am very new to TensorFlow - so please bear with me if this is a trivial question.</p>

<p>I'm coding in Python+TensorFlow. I have a dataframe with the following structure - </p>

<p>Y | X_1 | X_2 | ... | X_p | Grp </p>

<p>where Y is the continuous response, X_1 through X_p are features, and Grp is a categorical value indicating group. I want to fit a separate linear regression of Y on (X_1,...,X_p)for each Grp and save the weights/coefficients. I do not want to use the out of the shelf tf.estimator.LinearRegressor. Instead I want to go the loss function-optimizer-session.run() route. </p>

<p>The relevant tutorial pages on internet talk about linear regression but not per group. I would appreciate any suggestions. I am thinking to do this - </p>

<p>For each g in Grps : 
 1. Call the optimizer by passing the data for Group g as the placeholders.
 2. Get the estimated weights (for Group g) and save them in a dataframe : Grp | weights</p>

<p>Another approach that sounds reasonable is to have separate graphs for each group and kick them all together using various ""sessions"".</p>

<p>Are these reasonable and feasible in TF? Which one is easier or are there better approaches? </p>

<p>Thank you,
Sai</p>
","I am very new to TensorFlow - so please bear with me if this is a trivial question. I'm coding in Python+TensorFlow. I have a dataframe with the following structure - Y | X_1 | X_2 | ... | X_p | Grp where Y is the continuous response, X_1 through X_p are features, and Grp is a categorical value indicating group. I want to fit a separate linear regression of Y on (X_1,...,X_p)for each Grp and save the weights/coefficients. I do not want to use the out of the shelf tf.estimator.LinearRegressor. Instead I want to go the loss function-optimizer-session.run() route. The relevant tutorial pages on internet talk about linear regression but not per group. I would appreciate any suggestions. I am thinking to do this - For each g in Grps : 1. Call the optimizer by passing the data for Group g as the placeholders. 2. Get the estimated weights (for Group g) and save them in a dataframe : Grp | weights Another approach that sounds reasonable is to have separate graphs for each group and kick them all together using various ""sessions"". Are these reasonable and feasible in TF? Which one is easier or are there better approaches? Thank you, Sai",https://stackoverflow.com/questions/53580335,4857940,Requesting (Additional) Resources
55345384,Load (or combine) several pretrained checkpoints with tf.estimator.WarmStartSettings,"<p>I want to use pretrained weights for 2 parts of my model. I have 2 checkpoints from different models, from which I can load only one into my main model with tf.estimator.WarmStart as I'm using the estimator architecture.</p>

<pre><code>tf.WarmStartSettings(ckpt_to_initialize_from=X)
</code></pre>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings"" rel=""nofollow noreferrer"">from the doc</a>: </p>

<blockquote>
  <p>Either the directory or a specific checkpoint can be provided (in the case of the former, the latest checkpoint will be used).</p>
</blockquote>

<p>I can't see how I can add an additional checkpoint. Maybe there is a way to load the weights from both checkpoint into one and load that one?</p>
","I want to use pretrained weights for 2 parts of my model. I have 2 checkpoints from different models, from which I can load only one into my main model with tf.estimator.WarmStart as I'm using the estimator architecture. from the doc: I can't see how I can add an additional checkpoint. Maybe there is a way to load the weights from both checkpoint into one and load that one?",https://stackoverflow.com/questions/55345384,2368505,Requesting (Additional) Resources
45959112,Get coefficients of a linear regression in Tensorflow,"<p>I've done a simple linear regression in Tensorflow. How can I know what are the coefficients of the regression? 
I've read the docs but I cannot find it anywhere! (<a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor</a>)</p>

<p><strong>EDIT</strong> Code example</p>

<pre><code>import numpy as np
import tensorflow as tf

# Declare list of features, we only have one real-valued feature
def model_fn(features, labels, mode):
  # Build a linear model and predict values
  W = tf.get_variable(""W"", [1], dtype=tf.float64)
  b = tf.get_variable(""b"", [1], dtype=tf.float64)
  y = W * features['x'] + b
  # Loss sub-graph
  loss = tf.reduce_sum(tf.square(y - labels))
  # Training sub-graph
  global_step = tf.train.get_global_step()
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = tf.group(optimizer.minimize(loss),
                   tf.assign_add(global_step, 1))
  # EstimatorSpec connects subgraphs we built to the
  # appropriate functionality.
  return tf.estimator.EstimatorSpec(
      mode=mode,
      predictions=y,
      loss=loss,
      train_op=train)

estimator = tf.estimator.Estimator(model_fn=model_fn)
# define our data sets
x_train = np.array([1., 2., 3., 4.])
y_train = np.array([0., -1., -2., -3.])
x_eval = np.array([2., 5., 8., 1.])
y_eval = np.array([-1.01, -4.1, -7, 0.])
input_fn = tf.estimator.inputs.numpy_input_fn(
    {""x"": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    {""x"": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)
eval_input_fn = tf.estimator.inputs.numpy_input_fn(
    {""x"": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)

# train
estimator.train(input_fn=input_fn, steps=1000)
# Here we evaluate how well our model did.
train_metrics = estimator.evaluate(input_fn=train_input_fn)
eval_metrics = estimator.evaluate(input_fn=eval_input_fn)
print(""train metrics: %r""% train_metrics)
print(""eval metrics: %r""% eval_metrics)
</code></pre>
",I've done a simple linear regression in Tensorflow. How can I know what are the coefficients of the regression? I've read the docs but I cannot find it anywhere! (https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor) EDIT Code example,https://stackoverflow.com/questions/45959112,3070571,Documentation Completeness
43284897,How can I multiply a vector and a matrix in tensorflow without reshaping?,"<p>This:</p>

<pre><code>import numpy as np
a = np.array([1, 2, 1])
w = np.array([[.5, .6], [.7, .8], [.7, .8]])

print(np.dot(a, w))
# [ 2.6  3. ] # plain nice old matrix multiplication n x (n, m) -&gt; m

import tensorflow as tf

a = tf.constant(a, dtype=tf.float64)
w = tf.constant(w)

with tf.Session() as sess:
    print(tf.matmul(a, w).eval())
</code></pre>

<p>results in:</p>

<pre class=""lang-none prettyprint-override""><code>C:\_\Python35\python.exe C:/Users/MrD/.PyCharm2017.1/config/scratches/scratch_31.py
[ 2.6  3. ]
# bunch of errors in windows...
Traceback (most recent call last):
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 671, in _call_cpp_shape_fn_impl
    input_tensors_as_shapes, status)
  File ""C:\_\Python35\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [3], [3,2].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/MrD/.PyCharm2017.1/config/scratches/scratch_31.py"", line 14, in &lt;module&gt;
    print(tf.matmul(a, w).eval())
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1765, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 1454, in _mat_mul
    transpose_b=transpose_b, name=name)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 2329, in create_op
    set_shapes_for_outputs(ret)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1717, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\framework\ops.py"", line 1667, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""C:\_\Python35\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 676, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Shape must be rank 2 but is rank 1 for 'MatMul' (op: 'MatMul') with input shapes: [3], [3,2].

Process finished with exit code 1
</code></pre>

<p>(not sure why the same exception is raised inside its handling)</p>

<p>The solution suggested in  <a href=""https://stackoverflow.com/a/34908326/281545"">Tensorflow exception with matmul</a> is reshaping the vector to a matrix but this leads to needlessly complicated code - is there still no other way to multiply a vector with a matrix?</p>

<p>Incidentally using <code>expand_dims</code> (as suggested in the link above) with default arguments raises a <code>ValueError</code> - that's not mentioned in the <a href=""https://www.tensorflow.org/api_docs/python/tf/expand_dims"" rel=""noreferrer"">docs</a> and defeats the purpose of having a default argument.</p>
",This: results in: (not sure why the same exception is raised inside its handling) The solution suggested in Tensorflow exception with matmul is reshaping the vector to a matrix but this leads to needlessly complicated code - is there still no other way to multiply a vector with a matrix? Incidentally using expand_dims (as suggested in the link above) with default arguments raises a ValueError - that's not mentioned in the docs and defeats the purpose of having a default argument.,https://stackoverflow.com/questions/43284897,281545,Documentation Completeness
40731433,Understanding tf.extract_image_patches for extracting patches from an image,"<p>I found the following method <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#extract_image_patches"" rel=""noreferrer"">tf.extract_image_patches</a> in tensorflow API, but I am not clear about its functionality. </p>

<p>Say the <code>batch_size = 1</code>, and an image is of size <code>225x225x3</code>, and we want to extract patches of size <code>32x32</code>. </p>

<p>How exactly does this function behave? Specifically, the documentation mentions the dimension of the output tensor to be <code>[batch, out_rows, out_cols, ksize_rows * ksize_cols * depth]</code> , but what <code>out_rows</code> and <code>out_cols</code> are is not mentioned.</p>

<p>Ideally, given an input image tensor of size <code>1x225x225x3</code> (where 1 is the batch size), I want to be able to get <code>Kx32x32x3</code> as output, where <code>K</code> is the total number of patches and <code>32x32x3</code> is the dimension of each patch. Is there something in tensorflow that already achieves this?</p>
","I found the following method tf.extract_image_patches in tensorflow API, but I am not clear about its functionality. Say the batch_size = 1, and an image is of size 225x225x3, and we want to extract patches of size 32x32. How exactly does this function behave? Specifically, the documentation mentions the dimension of the output tensor to be [batch, out_rows, out_cols, ksize_rows * ksize_cols * depth] , but what out_rows and out_cols are is not mentioned. Ideally, given an input image tensor of size 1x225x225x3 (where 1 is the batch size), I want to be able to get Kx32x32x3 as output, where K is the total number of patches and 32x32x3 is the dimension of each patch. Is there something in tensorflow that already achieves this?",https://stackoverflow.com/questions/40731433,1252766,Documentation Ambiguity
44217076,tf.extract_image_patches for 3D images,"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/extract_image_patches"" rel=""nofollow noreferrer"">The documentation of tf.extract_image_patches</a></p>

<p>It is only for 2D image, could it be expand to 3D images, which is useful for the implementation for SSIM loss function?</p>

<p>I cannot find the source code. There is a similar function <code>skimage.util.view_as_windows</code>, however, when I try to use this function with the tensorflow as backend in keras, there are errors. The transition from numpy array to tensor confused me a lot.</p>
","The documentation of tf.extract_image_patches It is only for 2D image, could it be expand to 3D images, which is useful for the implementation for SSIM loss function? I cannot find the source code. There is a similar function skimage.util.view_as_windows, however, when I try to use this function with the tensorflow as backend in keras, there are errors. The transition from numpy array to tensor confused me a lot.",https://stackoverflow.com/questions/44217076,7845074,Lack of Alternative Solutions/Documentation
67496007,how to create the feature column in tensorflow?,"<p>i am working on the feature_column construction following the tensorflow document, but i met some problem, can anyone help me? THX! this my code below:</p>
<p>#data is the dictionary of the train dataset</p>
<pre><code>def demo(feature_column):
    feature_layer = layers.DenseFeatures(feature_column)
    return(feature_layer(data).numpy())
age = feature_column.numeric_column('Age')
age_bucket = feature_column.bucketized_column(age, boundaries = [30, 40, 50, 60, 70, 80, 90])
age_fea = demo(age_bucket) #create age feature_column
gender = feature_column.categorical_column_with_vocabulary_list('Gender', ['F', 'M'])
gender_one_hot = feature_column.indicator_column(gender) #create gender feature_column
smoking = feature_column.categorical_column_with_vocabulary_list('Smoking', ['Y', 'N'])
smoking_one_hot = feature_column.indicator_column(smoking) #create smoking feature_column

#create the estimator
    model = tf.estimator.LinearClassifier( 
      n_classes = 2,
      model_dir = &quot;ongoing/train6 &quot;,
      feature_columns = [age_fea, gender_fea, icd9code_fea, smoking_fea]
    )
</code></pre>
<p>#train the estimator</p>
<pre><code>  model.train( 
      input_fn = get_input_fn(csv_data, 5000, 10, True),
      steps = 1000
    )
</code></pre>
<p>and got an error listed below:</p>
<pre><code>    (base) ous-MacBook-Pro:indiproject_copy congwang$ python try.py
2021-05-12 01:12:32.901336: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-05-12 01:12:32.901558: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From /Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/lazy_loader.py:63: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.

WARNING:tensorflow:From try.py:79: The name tf.estimator.inputs.pandas_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.pandas_input_fn instead.

WARNING:tensorflow:From /Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
WARNING:tensorflow:From /Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:60: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Traceback (most recent call last):
  File &quot;try.py&quot;, line 106, in &lt;module&gt;
    model.train(
  File &quot;/Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py&quot;, line 349, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File &quot;/Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py&quot;, line 1175, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File &quot;/Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py&quot;, line 1203, in _train_model_default
    estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,
  File &quot;/Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py&quot;, line 1163, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File &quot;/Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py&quot;, line 982, in _model_fn
    return _linear_model_fn(
  File &quot;/Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py&quot;, line 744, in _linear_model_fn
    logits = logit_fn(features=features)
  File &quot;/Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py&quot;, line 429, in linear_logit_fn
    linear_model = feature_column._LinearModel(  # pylint: disable=protected-access
  File &quot;/Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column.py&quot;, line 639, in __init__
    self._feature_columns = _normalize_feature_columns(
  File &quot;/Users/congwang/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column.py&quot;, line 2333, in _normalize_feature_columns
    raise ValueError('Items of feature_columns must be a _FeatureColumn. '
ValueError: Items of feature_columns must be a _FeatureColumn. Given (type &lt;class 'numpy.ndarray'&gt;): [[0. 0. 0. ... 0. 0. 1.]
 [0. 0. 1. ... 0. 0. 0.]
 [1. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 1. 0. 0.]
 [0. 1. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 1. 0. 0.]].
</code></pre>
<p>the list in the valueError is the age_fea, when i delete the age_fea from estimator construction, the valueError lists the the list of gender_fea. as the demo returns an numpy ndarray rather than tensor, i am confused about what to feed into the feature_columns when constructing the model.</p>
","i am working on the feature_column construction following the tensorflow document, but i met some problem, can anyone help me? THX! this my code below: #data is the dictionary of the train dataset #train the estimator and got an error listed below: the list in the valueError is the age_fea, when i delete the age_fea from estimator construction, the valueError lists the the list of gender_fea. as the demo returns an numpy ndarray rather than tensor, i am confused about what to feed into the feature_columns when constructing the model.",https://stackoverflow.com/questions/67496007,15525911,Requesting (Additional) Resources
52785019,Specify shape for categorical feature columns?,"<p>I know that I can use a <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity"" rel=""nofollow noreferrer""><code>categorical_column_with_identity</code></a> to turn a categorical feature into a series of one-hot features.</p>

<p>For instance, if my vocabulary is <code>[""ON"", ""OFF"", ""UNKNOWN""]</code>:<br>
<code>""OFF""</code> -> <code>[0, 1, 0]</code></p>

<pre><code>categorical_column = tf.feature_column.categorical_column_with_identity('column_name', num_buckets=3)
feature_column = tf.feature_column.indicator_column(categorical_column))
</code></pre>

<hr>

<p>However, I actually have an 1-dimensional array of categorical features. I would like to turn that into a 2-dimensional series of one-hot features:</p>

<p><code>[""OFF"", ""ON"", ""OFF"", ""UNKNOWN"", ""ON""]</code><br>
-><br>
<code>[[0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]]</code></p>

<p>Unlike every other feature column, it doesn't seem like there's a <code>shape</code> attribute on <code>categorical_column_with_identity</code> and I didn't find any help through Google or the docs.</p>

<p>Do I have to give up on <code>categorical_column_with_identity</code> and create the 2D array myself through a <code>numerical_column</code>?</p>
","I know that I can use a categorical_column_with_identity to turn a categorical feature into a series of one-hot features. For instance, if my vocabulary is [""ON"", ""OFF"", ""UNKNOWN""]: ""OFF"" -&gt; [0, 1, 0] However, I actually have an 1-dimensional array of categorical features. I would like to turn that into a 2-dimensional series of one-hot features: [""OFF"", ""ON"", ""OFF"", ""UNKNOWN"", ""ON""] -&gt; [[0, 1, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]] Unlike every other feature column, it doesn't seem like there's a shape attribute on categorical_column_with_identity and I didn't find any help through Google or the docs. Do I have to give up on categorical_column_with_identity and create the 2D array myself through a numerical_column?",https://stackoverflow.com/questions/52785019,2510391,Lack of Alternative Solutions/Documentation
56453002,Tensorflow handling arrays as feature_columns,"<p>I'm trying to build a classifier which takes an array of floats as an input.</p>

<p>Despite following steps <a href=""https://www.tensorflow.org/alpha/tutorials/estimators/linear"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column"" rel=""nofollow noreferrer"">here</a> to include an array as the input feature I keep getting an TypeError whereby the estimator doesn't recognise the shape of the input.</p>

<p>How do you include an array as a feature for an estimator? Can you simply pass in the numeric_column with an appropriate shape as expected in the docs?</p>

<p>Sample code here:</p>

<pre><code>import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import feature_column

z = [[1, 2], [3,4]]
df = pd.DataFrame(z)
df = df.apply(lambda x: np.array(x), axis=1)

feature_columns = []

for col in ['feature']:
    feature_columns.append(feature_column.numeric_column(col, shape=(2, )))

df = pd.DataFrame(df)
df.columns = ['feature']
df['target'] = 1
y_train = df.pop('target')

def make_input_fn(X, y, n_epochs=None, shuffle=True):
    def input_fn():
        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))
        if shuffle:
            dataset = dataset.shuffle(20)
        # For training, cycle thru dataset as many times as need (n_epochs=None).    
        dataset = dataset.repeat(n_epochs)  
        # In memory training doesn't use batching.
        dataset = dataset.batch(5)
        return dataset
    return input_fn

train_input_fn = make_input_fn(df, y_train)

linear_est = tf.estimator.LinearRegressor(feature_columns)

linear_est.train(train_input_fn, max_steps=100)
</code></pre>

<p>which gives a stack trace of </p>

<pre><code>Traceback (most recent call last):
  File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydevd_bundle/pydevd_exec2.py"", line 3, in Exec
    exec(exp, global_vars, local_vars)
  File ""&lt;string&gt;"", line 39, in &lt;module&gt;
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 359, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1139, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1166, in _train_model_default
    input_fn, ModeKeys.TRAIN))
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1003, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1094, in _call_input_fn
    return input_fn(**kwargs)
  File ""&lt;string&gt;"", line 23, in input_fn
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 279, in from_tensor_slices
    return TensorSliceDataset(tensors)
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2091, in __init__
    for i, t in enumerate(nest.flatten(tensors))
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2091, in &lt;listcomp&gt;
    for i, t in enumerate(nest.flatten(tensors))
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1050, in convert_to_tensor
    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1108, in convert_to_tensor_v2
    as_ref=False)
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1186, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 304, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 245, in constant
    allow_broadcast=True)
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py"", line 283, in _constant_impl
    allow_broadcast=allow_broadcast))
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py"", line 574, in make_tensor_proto
    append_fn(tensor_proto, proto_values)
  File ""tensorflow/python/framework/fast_tensor_util.pyx"", line 127, in tensorflow.python.framework.fast_tensor_util.AppendObjectArrayToTensorProto
  File ""/Users/nicholashilton/.virtualenvs/fantifi/lib/python3.7/site-packages/tensorflow/python/util/compat.py"", line 61, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got array([1, 2])
</code></pre>
",I'm trying to build a classifier which takes an array of floats as an input. Despite following steps here and here to include an array as the input feature I keep getting an TypeError whereby the estimator doesn't recognise the shape of the input. How do you include an array as a feature for an estimator? Can you simply pass in the numeric_column with an appropriate shape as expected in the docs? Sample code here: which gives a stack trace of,https://stackoverflow.com/questions/56453002,6801991,Documentation Replicability
55537333,How to use tensorflow sequence_numeric_column with an RNNClassifier?,"<p>I was looking throw the tensorflow contrib API and I wanted to use the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/estimator/RNNClassifier#__init__"" rel=""nofollow noreferrer"">RNNClassifier</a> available with Tensorflow 1.13. Contrary to non sequence estimators, this one needs sequence feature columns only. However I was not able to make it work on a toy dataset. I keep getting an error while using <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/feature_column/sequence_numeric_column"" rel=""nofollow noreferrer"">sequence_numeric_column</a>.</p>

<p>Here is the structure of my toy dataset:</p>

<pre><code>idSeq,kind,label,size
0,0,dwarf,117.6
0,0,dwarf,134.4
0,0,dwarf,119.0
0,1,human,168.0
0,1,human,145.25
0,2,elve,153.9
0,2,elve,218.49999999999997
0,2,elve,210.9
1,0,dwarf,166.6
1,0,dwarf,168.0
1,0,dwarf,131.6
1,1,human,150.5
1,1,human,208.25
1,1,human,210.0
1,2,elve,199.5
1,2,elve,161.5
1,2,elve,197.6
</code></pre>

<p>where idSeq allow us to see which rows belong to which sequence.
I want to predict the ""kind"" column thanks to the ""size"" column.</p>

<p>Below there is my code about make my RNN training on my dataset.</p>

<pre><code>import numpy as np
import pandas as pd
import tensorflow as tf


os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.logging.set_verbosity(tf.logging.INFO)

dataframe = pd.read_csv(""data_rnn.csv"")
dataframe_test = pd.read_csv(""data_rnn_test.csv"")


train_x = dataframe
train_y = dataframe.loc[:,([""kind""])]


size_feature_col = tf.contrib.feature_column.sequence_numeric_column('size ')


estimator = tf.contrib.estimator.RNNClassifier(
    sequence_feature_columns=[size_feature_col ],
    num_units=[32, 16],
    cell_type='lstm',
    model_dir=None,
    n_classes=3,
    optimizer='Adagrad'
)



def make_dataset(
    batch_size, 
    x, 
    y=None, 
    shuffle=False, 
    shuffle_buffer_size=1000,
    shuffle_seed=1):
    """"""
    An input function for training, evaluation or prediction.

    Parameters
    ----------------------
    batch_size: integer
        the size of the batch to use for the training of the neural network
    x: pandas dataframe 
        dataframe that contains the features of the samples to study
    y: pandas dataframe or array (Default: None)
        dataframe or array that contains the values to predict of the samples
        to study. If none, we want a dataset for evaluation or prediction.
    shuffle: boolean (Default: False)
        if True, we shuffle the elements of the dataset
    shuffle_buffer_size: integer (Default: 1000)
        if we shuffle the elements of the dataset, it is the size of the buffer
        used for it.
    shuffle_seed : integer
        the random seed for the shuffling

    Returns
    ---------------------
    dataset.make_one_shot_iterator().get_next(): Tensor
        a nested structure of tf.Tensors containing the next element of the 
        dataset to study
    """"""

    def input_fn():
        if y is not None:
            dataset = tf.data.Dataset.from_tensor_slices((dict(x), y))
        else:
            dataset = tf.data.Dataset.from_tensor_slices(dict(x))
        if shuffle:
            dataset = dataset.shuffle(
                buffer_size=shuffle_buffer_size,
                seed=shuffle_seed).batch(batch_size).repeat()
        else:
            dataset = dataset.batch(batch_size)
        return dataset.make_one_shot_iterator().get_next()

    return input_fn



batch_size = 50
random_seed = 1


input_fn_train = make_dataset(
            batch_size=batch_size, 
            x=train_x, 
            y=train_y, 
            shuffle=True, 
            shuffle_buffer_size=len(train_x),
            shuffle_seed=random_seed)

estimator.train(input_fn=input_fn_train, steps=5000)

</code></pre>

<p>But I only got the following error :</p>

<pre><code>INFO:tensorflow:Calling model_fn.
Traceback (most recent call last):
  File ""main.py"", line 125, in &lt;module&gt;
    estimator.train(input_fn=input_fn_train, steps=5000)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 358, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1124, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1154, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1112, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/contrib/estimator/python/estimator/rnn.py"", line 512, in _model_fn
    config=config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/contrib/estimator/python/estimator/rnn.py"", line 332, in _rnn_model_fn
    logits, sequence_length_mask = logit_fn(features=features, mode=mode)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_estimator/contrib/estimator/python/estimator/rnn.py"", line 226, in rnn_logit_fn
    features=features, feature_columns=sequence_feature_columns)
  File ""/root/.local/lib/python3.5/site-packages/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py"", line 120, in sequence_input_layer
    trainable=trainable)
  File ""/root/.local/lib/python3.5/site-packages/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py"", line 496, in _get_sequence_dense_tensor
    sp_tensor, default_value=self.default_value)
  File ""/root/.local/lib/python3.5/site-packages/tensorflow/python/ops/sparse_ops.py"", line 1432, in sparse_tensor_to_dense
    sp_input = _convert_to_sparse_tensor(sp_input)
  File ""/root/.local/lib/python3.5/site-packages/tensorflow/python/ops/sparse_ops.py"", line 68, in _convert_to_sparse_tensor
    raise TypeError(""Input must be a SparseTensor."")
TypeError: Input must be a SparseTensor.

</code></pre>

<p>So I don't understand what I've done wrong because on the documentation, it is written that we have to give a sequence column to the RNNEstimator. They do not say anything about giving sparse tensor.</p>

<p>Thanks in advance for your help and advices.</p>
","I was looking throw the tensorflow contrib API and I wanted to use the RNNClassifier available with Tensorflow 1.13. Contrary to non sequence estimators, this one needs sequence feature columns only. However I was not able to make it work on a toy dataset. I keep getting an error while using sequence_numeric_column. Here is the structure of my toy dataset: where idSeq allow us to see which rows belong to which sequence. I want to predict the ""kind"" column thanks to the ""size"" column. Below there is my code about make my RNN training on my dataset. But I only got the following error : So I don't understand what I've done wrong because on the documentation, it is written that we have to give a sequence column to the RNNEstimator. They do not say anything about giving sparse tensor. Thanks in advance for your help and advices.",https://stackoverflow.com/questions/55537333,5218377,Documentation Replication on Other Examples
63514487,TFLiteConverter Segmentation Fault when running integer quantization,"<p>I'm using tensorflow==1.15.3 and I'm hitting a segmentation fault attempting <a href=""https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only"" rel=""nofollow noreferrer"">int8 post-training quantization</a>. The documentation for the 1.15 version of the TFLiteConverter can be found <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I found <a href=""https://github.com/tensorflow/tensorflow/issues/29829"" rel=""nofollow noreferrer"">a similar issue on github</a>, but their solution to provide <code>--add_postprocessing_op=true</code> has not solved the segmentation fault.</p>
<p>I've debugged it using PDB and found exactly where it crashes. It never reaches my <code>representative_dataset</code> function. It faults when running <code>CreateWrapperCPPFromBuffer(model_content)</code>:</p>
<pre><code>&gt; .../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py(51)__init__()
-&gt; .CreateWrapperCPPFromBuffer(model_content))
(Pdb) s
Fatal Python error: Segmentation fault

Current thread 0x00007ff40ee9f740 (most recent call first):
  File &quot;.../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py&quot;, line 51 in __init__
  File &quot;.../python3.6/site-packages/tensorflow_core/lite/python/lite.py&quot;, line 236 in _calibrate_quantize_model
  File &quot;.../python3.6/site-packages/tensorflow_core/lite/python/lite.py&quot;, line 993 in convert
  File &quot;.../convert_model_to_tflite_int8.py&quot;, line 97 in &lt;module&gt;
  File &quot;&lt;string&gt;&quot;, line 1 in &lt;module&gt;
  File &quot;/usr/lib/python3.6/bdb.py&quot;, line 434 in run
  File &quot;/usr/lib/python3.6/pdb.py&quot;, line 1548 in _runscript
  File &quot;/usr/lib/python3.6/pdb.py&quot;, line 1667 in main
  File &quot;/usr/lib/python3.6/pdb.py&quot;, line 1694 in &lt;module&gt;
  File &quot;/usr/lib/python3.6/runpy.py&quot;, line 85 in _run_code
  File &quot;/usr/lib/python3.6/runpy.py&quot;, line 193 in _run_module_as_main
[1]    17668 segmentation fault (core dumped)  python -m pdb convert_model_to_tflite_int8.py  --add_postprocessing_op=true
</code></pre>
<p>Here is my conversion code:</p>
<pre class=""lang-py prettyprint-override""><code>converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file=pb_model_path,
  input_arrays=[&quot;device_0/input_node_name:1&quot;],
  output_arrays=[&quot;device_0/output_node_name&quot;],
  input_shapes={&quot;device_0/input_node_name:1&quot;: [100, 16384]}
)
converter.allow_custom_ops = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

def test():
  pdb.set_trace()
  print(' ! ! ! representative_dataset_gen ! ! ! ')
  zeros = np.zeros(shape=(1, 100, 16384), dtype='int8')
  ds = tf.data.Dataset.from_tensor_slices((zeros)).batch(1)
  for input_value in ds.take(1):
    yield [input_value]
converter.representative_dataset = test

pdb.set_trace()
tflite_model = converter.convert()

tflite_model_size = open(model_name, 'wb').write(tflite_model)
print('TFLite Model is %d bytes' % tflite_model_size)
</code></pre>
<p>FWIW my model conversion works for <code>tf.float16</code> (not using <code>representative_dataset</code> there, though).</p>
","I'm using tensorflow==1.15.3 and I'm hitting a segmentation fault attempting int8 post-training quantization. The documentation for the 1.15 version of the TFLiteConverter can be found here. I found a similar issue on github, but their solution to provide --add_postprocessing_op=true has not solved the segmentation fault. I've debugged it using PDB and found exactly where it crashes. It never reaches my representative_dataset function. It faults when running CreateWrapperCPPFromBuffer(model_content): Here is my conversion code: FWIW my model conversion works for tf.float16 (not using representative_dataset there, though).",https://stackoverflow.com/questions/63514487,3969602,Documentation Replicability
75135220,"How to define positional arguments: 'op', 'value_index', and 'dtype' in a tensor?","<p>when I run the below piece of code</p>
<pre><code>from tensorflow.python.ops.numpy_ops import np_config
np_config.enable_numpy_behavior()
import pandas as pd

df = pd.DataFrame(
    {'x':[1.,2.,3.,4.],
     'y':[1.59,4.24,2.38,0.53]}
)

data = tf.data.Dataset.from_tensor_slices(df.to_numpy())
data = data.flat_map(lambda x: x.reshape((2,1)))
</code></pre>
<p>I receive:
<em><strong>TypeError: <strong>init</strong>() missing 3 required positional arguments: 'op', 'value_index', and 'dtype'</strong></em> . I understand why this is happening as I didn't define values for 'op', 'value_index', and 'dtype' amd that tensorflow cant produce tensors.
Basically I want to use <strong>flat_map</strong> function to create tensors with <strong>shape = (1,)</strong> and <strong>dtype = tf.float64</strong> such that when I run the below code the printed tensors look like:</p>
<pre><code>for item in data:
    print(item)

tf.Tensor([1.], shape=(1,), dtype=float64)
tf.Tensor([2.], shape=(1,), dtype=float64)
tf.Tensor([3.], shape=(1,), dtype=float64)
tf.Tensor([4.], shape=(1,), dtype=float64)
tf.Tensor([1.59], shape=(1,), dtype=float64)
tf.Tensor([4.24], shape=(1,), dtype=float64)
tf.Tensor([2.38], shape=(1,), dtype=float64)
tf.Tensor([0.53], shape=(1,), dtype=float64)
</code></pre>
<p>How can I specify those values inside <strong>flat_map</strong> function or any other function which I can pass into <strong>flat_map</strong> function?</p>
<p>I checked here <a href=""https://www.tensorflow.org/api_docs/python/tf/Tensor"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/Tensor</a> but unfortunately I couldn't come up with a solution.</p>
<p>Thanks!</p>
","when I run the below piece of code I receive: TypeError: init() missing 3 required positional arguments: 'op', 'value_index', and 'dtype' . I understand why this is happening as I didn't define values for 'op', 'value_index', and 'dtype' amd that tensorflow cant produce tensors. Basically I want to use flat_map function to create tensors with shape = (1,) and dtype = tf.float64 such that when I run the below code the printed tensors look like: How can I specify those values inside flat_map function or any other function which I can pass into flat_map function? I checked here https://www.tensorflow.org/api_docs/python/tf/Tensor but unfortunately I couldn't come up with a solution. Thanks!",https://stackoverflow.com/questions/75135220,13841479,Requesting (Additional) Resources
53754540,Tensorflow Access CsvDataset values,"<p><strong>Eager execution</strong></p>

<p>I have been digging through the API for 2 days and I cant seem to find a way to use the data from a <code>CsvDataset</code> object.</p>

<p>I have the following sample from a dataset:</p>

<p><code>70,1,4,130,322,0,2,109,0,24,2,3,3,2
67,0,3,115,564,0,2,160,0,16,2,0,7,1
57,1,2,124,261,0,0,141,0,3,1,0,7,2
64,1,4,128,263,0,0,105,1,2,2,1,7,1
74,0,2,120,269,0,2,121,1,2,1,1,3,1
65,1,4,120,177,0,0,140,0,4,1,0,7,1
56,1,3,130,256,1,2,142,1,6,2,1,6,2
59,1,4,110,239,0,2,142,1,12,2,1,7,2
60,1,4,140,293,0,2,170,0,12,2,2,7,2
63,0,4,150,407,0,2,154,0,4,2,3,7,2</code></p>

<p>I read the csv as said in their high-level APIs video:</p>

<pre><code>tf.enable_eager_execution()
defaults = [tf.float64] * 14
dataset=tf.data.experimental.CsvDataset(path, defaults)
&gt;&gt;&gt; dataset
&gt;&gt;&gt; &lt;CsvDataset shapes: ((), (), (), (), (), (), (), (), (), (), (), (), (), ()), types: (tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64, tf.float64)&gt;
</code></pre>

<p>But from here on i cant acess any data like, getting the values of a column.</p>

<p>Converting the dataset to a list using: <code>list(dataset)</code> is not an option, as it takes a very long time with normal size csv's (~190k samples).</p>

<p>So, is there any way to get column or row values from this object? Or is there really no point in using TF to read data instead of using scikit/pandas?</p>

<p><strong>Edit 1:</strong>
Tried doing <code>col1 = dataset.map(lambda *row: row[0])</code> as said by @kvish, this return  a <code>&lt;MapDataset shapes: (), types: tf.float64&gt;</code> which is iterable. Problem is that having to loop over every column and then iterating over every <code>MapDataset</code> would make the complexity <code>O(n^2)</code>.</p>

<p>The idea output would be a list of tensors, each tensor containing all values from a column, similar to this:</p>

<pre><code>[&lt;tf.Tensor(shape=(10,), dtype=float64, 
numpy=array([70.0,67.0,57.0,64.0,74.0,65.0,56.0,59.0,60.0,63.0]))&gt;,
(...) x14]
</code></pre>
","Eager execution I have been digging through the API for 2 days and I cant seem to find a way to use the data from a CsvDataset object. I have the following sample from a dataset: 70,1,4,130,322,0,2,109,0,24,2,3,3,2 67,0,3,115,564,0,2,160,0,16,2,0,7,1 57,1,2,124,261,0,0,141,0,3,1,0,7,2 64,1,4,128,263,0,0,105,1,2,2,1,7,1 74,0,2,120,269,0,2,121,1,2,1,1,3,1 65,1,4,120,177,0,0,140,0,4,1,0,7,1 56,1,3,130,256,1,2,142,1,6,2,1,6,2 59,1,4,110,239,0,2,142,1,12,2,1,7,2 60,1,4,140,293,0,2,170,0,12,2,2,7,2 63,0,4,150,407,0,2,154,0,4,2,3,7,2 I read the csv as said in their high-level APIs video: But from here on i cant acess any data like, getting the values of a column. Converting the dataset to a list using: list(dataset) is not an option, as it takes a very long time with normal size csv's (~190k samples). So, is there any way to get column or row values from this object? Or is there really no point in using TF to read data instead of using scikit/pandas? Edit 1: Tried doing col1 = dataset.map(lambda *row: row[0]) as said by @kvish, this return a &lt;MapDataset shapes: (), types: tf.float64&gt; which is iterable. Problem is that having to loop over every column and then iterating over every MapDataset would make the complexity O(n^2). The idea output would be a list of tensors, each tensor containing all values from a column, similar to this:",https://stackoverflow.com/questions/53754540,3497258,Requesting (Additional) Resources
76187146,Gather elements from tensor with an inner batch dimension in tensorflow,"<p>I'll present my actual problem, and then a simpler version that is more easily reproducible.</p>
<h2>My actual problem</h2>
<p>I have a tensor that represents a batch of images from my training data. Its shape is:</p>
<pre class=""lang-py prettyprint-override""><code>[domains, batch_size, image_size, image_size, channels] = [4 x 4 x 64 x 64 x 4]
</code></pre>
<p>The 64x64x4 images are pixel art character sprites, and the <code>domains</code> dimension represent images of them in: back, left, front, and right poses. The reason that the <code>batch_size</code> dim is not the outer-most is that the images are read from a <code>tf.data.Dataset</code> that produces a 4-tuple for each sample (character in the back, left, front, and right poses). And when I do <code>next(iter(dataset))</code>, what I get is a tensor with the mentioned shape (<code>[domains, batch_size, image_size, image_size, channels]</code>).</p>
<p>At each training step, I need to randomly pick a target pose for each image in the batch, so I can ask a generator to translate them from their source into a target pose. I am using <code>tf.gather</code> for this, but it does not correctly select images the way I need. My code:</p>
<pre class=""lang-py prettyprint-override""><code>batch_size = 4
domains = 4
batch = next(iter(dataset))  # shape 4 x 4 x 64 x 64 x 4
# back, left, front, right = batch

target_indices = tf.random.uniform(shape=[batch_size], dtype=&quot;int32&quot;, maxval=domains)
target_images = tf.gather(batch, target_indices, axis=0)

print(&quot;Shape of target_images&quot;, tf.shape(target_images))
# 4 x 4 x 64 x 64 x 4
</code></pre>
<p>I need the resulting <code>target_images</code> to have a shape of <code>[4, 64, 64, 4]</code>, which would be 1 image for each character sprite in the randomly picked target pose. But as per the documentation of <code>tf.gather</code>, <em>&quot;the output shape has the same shape as the input, with the indexed-axis replaced by the shape of the indices.&quot;</em></p>
<p>One option is to use the <code>batch_dims=1</code> argument of <code>tf.gather</code>, doing:</p>
<pre><code>batch_size = 4
domains = 4
batch = next(iter(dataset))  # shape 4 x 4 x 64 x 64 x 4
# back, left, front, right = batch

target_indices = tf.random.uniform(shape=[batch_size], dtype=&quot;int32&quot;, maxval=domains)
target_images = tf.gather(batch, target_indices, axis=1, batch_dims=1)

print(&quot;Shape of target_images&quot;, tf.shape(target_images))
# 4 x 64 x 64 x 4
</code></pre>
<p>...that does yield a tensor with the correct shape, but not with the correctly gathered subtensors: in the simplified example I show next it is easier to see what is going on. The reason is that although the <code>batch_dims</code> argument exists, it seems to require the batch dimension(s) to be the outer-most one(s). Let' move on to a simpler problem.</p>
<h2>Simplification (reproducible)</h2>
<p>Suppose we have a tensor with a shape of:</p>
<pre class=""lang-py prettyprint-override""><code>[domains, batch_size, content] = [2, 3, 1]
</code></pre>
<p>I want to be able to gather one element (<code>content</code>, <code>axis=2</code>) for each element in the batch (<code>axis=1</code>) according to some random domain (<code>axis=0</code>). Let's try some code:</p>
<pre class=""lang-py prettyprint-override""><code>batch_size = 3
domains = 2
batch = tf.constant([
  [ [1],  [2],  [3]],
  [[10], [20], [30]]
])
target_indices = tf.constant([0, 1, 1])  # randomly picked indices to gather

# the batch has 3 elements and I want to get the content of domain 0 for the first element,
# and domain 1 for the second and third: [[1], [20], [30]], with shape (3, 1)
#
# attempts:
tf.gather(batch, target_indices, axis=0)
# shape=(3, 3, 1) ❌
# [[[ 1], [ 2], [ 3]],
#  [[10], [20], [30]],
#  [[10], [20], [30]]] 

tf.gather(batch, target_indices, axis=1)
# shape=(2, 3, 1) ❌
# [[[ 1], [ 2], [ 2]],
#  [[10], [20], [20]]] 

tf.gather(batch, target_indices, axis=1, batch_dims=1)
# InvalidArgumentError: params.shape[0]: 2 should be equal to indices.shape[0]: 3 [Op:GatherV2] ❌


# last attempt: permute the batch tensor so the batch dim becomes the outer-most
permuted_batch = tf.transpose(batch, [1, 0, 2])
print(&quot;Shape of permuted_batch&quot;, tf.shape(permuted_batch)) # (3, 2, 1)
tf.gather(permuted_batch, target_indices, axis=1, batch_dims=1)
# shape=(3, 1)
# [[1], [20], [30]] ✅
</code></pre>
<p>As we can see, permuting the tensor to put the batch dimension as the outer-most allows me to use <code>batch_dims=1, axis=1</code> for <code>tf.gather</code> and yields the correct result. However, I'm a bit afraid of the performance of <code>tf.transpose</code>.</p>
<p><strong>My question</strong>: is there some way I can gather the desired elements of a tensor that has a batch dimension as an inner dimension?</p>
","I'll present my actual problem, and then a simpler version that is more easily reproducible. I have a tensor that represents a batch of images from my training data. Its shape is: The 64x64x4 images are pixel art character sprites, and the domains dimension represent images of them in: back, left, front, and right poses. The reason that the batch_size dim is not the outer-most is that the images are read from a tf.data.Dataset that produces a 4-tuple for each sample (character in the back, left, front, and right poses). And when I do next(iter(dataset)), what I get is a tensor with the mentioned shape ([domains, batch_size, image_size, image_size, channels]). At each training step, I need to randomly pick a target pose for each image in the batch, so I can ask a generator to translate them from their source into a target pose. I am using tf.gather for this, but it does not correctly select images the way I need. My code: I need the resulting target_images to have a shape of [4, 64, 64, 4], which would be 1 image for each character sprite in the randomly picked target pose. But as per the documentation of tf.gather, ""the output shape has the same shape as the input, with the indexed-axis replaced by the shape of the indices."" One option is to use the batch_dims=1 argument of tf.gather, doing: ...that does yield a tensor with the correct shape, but not with the correctly gathered subtensors: in the simplified example I show next it is easier to see what is going on. The reason is that although the batch_dims argument exists, it seems to require the batch dimension(s) to be the outer-most one(s). Let' move on to a simpler problem. Suppose we have a tensor with a shape of: I want to be able to gather one element (content, axis=2) for each element in the batch (axis=1) according to some random domain (axis=0). Let's try some code: As we can see, permuting the tensor to put the batch dimension as the outer-most allows me to use batch_dims=1, axis=1 for tf.gather and yields the correct result. However, I'm a bit afraid of the performance of tf.transpose. My question: is there some way I can gather the desired elements of a tensor that has a batch dimension as an inner dimension?",https://stackoverflow.com/questions/76187146,1783793,Requesting (Additional) Resources
42608175,What does tf.gather_nd intuitively do?,"<p>Can you intuitively explain or give more examples about <code>tf.gather_nd</code> for indexing and slicing into high-dimensional tensors in Tensorflow? </p>

<p>I read the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather_nd"" rel=""noreferrer"">API</a>, but it is kept quite concise that I find myself hard to follow the function's concept.</p>
","Can you intuitively explain or give more examples about tf.gather_nd for indexing and slicing into high-dimensional tensors in Tensorflow? I read the API, but it is kept quite concise that I find myself hard to follow the function's concept.",https://stackoverflow.com/questions/42608175,5098762,Requesting (Additional) Resources
47774994,The difference between `sess.graph` and `tf.get_default_graph()`?,"<p><code>sess.graph</code> and <code>tf.get_default_graph()</code> gives the same results in tensorboard. It is not very clear to me what is the difference between them according to the manual. Could anybody help explain the difference? Could anyboby provide an example for which <code>sess.graph</code> and <code>tf.get_default_graph()</code> can not be used interchangeably?</p>

<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python
# vim: set noexpandtab tabstop=2 shiftwidth=2 softtabstop=-1 fileencoding=utf-8:

import tensorflow as tf
import sys

x = tf.Variable(1.0)
with tf.name_scope('ns_loss'):
    loss = tf.square(x)
with tf.name_scope('ns_adam'):
    train_op = tf.train.AdamOptimizer().minimize(loss)

init = tf.global_variables_initializer()

tf.summary.scalar('scalar_loss', loss)
merged_summary_op = tf.summary.merge_all()

with tf.Session() as sess:
    sess.run(init)
    summary_writer = tf.summary.FileWriter(logdir=sys.argv[1])
    summary_writer.add_graph(graph=tf.get_default_graph())
    #summary_writer.add_graph(graph=sess.graph)

    for i in xrange(100):
        sess.run(train_op)
        summary_writer.add_summary(
                summary = sess.run(merged_summary_op)
                , global_step = i
                )
</code></pre>
",sess.graph and tf.get_default_graph() gives the same results in tensorboard. It is not very clear to me what is the difference between them according to the manual. Could anybody help explain the difference? Could anyboby provide an example for which sess.graph and tf.get_default_graph() can not be used interchangeably?,https://stackoverflow.com/questions/47774994,1424739,Documentation Ambiguity
45074049,Tensorflow: How does tf.get_variable work?,"<p>I have read about <code>tf.get_variable</code> from this <a href=""https://stackoverflow.com/questions/37098546/difference-between-variable-and-get-variable-in-tensorflow"">question</a> and also a bit from the documentation available at the tensorflow website. However, I am still not clear and was unable to find an answer online.</p>
<p>How does <code>tf.get_variable</code> work? For example:</p>
<pre><code>var1 = tf.Variable(3.,dtype=float64)
var2 = tf.get_variable(&quot;var1&quot;,[],dtype=tf.float64)
</code></pre>
<p>Does it mean that <strong>var2</strong> is <strong>another</strong> variable with initialization similar to <strong>var1</strong>? Or is <strong>var2</strong> an alias for <strong>var1</strong> (I tried and it doesn't seem to)?</p>
<p>How are <strong>var1</strong> and <strong>var2</strong> related?</p>
<p>How is a variable constructed when the variable we are <em>getting</em> doesn't really exist?</p>
","I have read about tf.get_variable from this question and also a bit from the documentation available at the tensorflow website. However, I am still not clear and was unable to find an answer online. How does tf.get_variable work? For example: Does it mean that var2 is another variable with initialization similar to var1? Or is var2 an alias for var1 (I tried and it doesn't seem to)? How are var1 and var2 related? How is a variable constructed when the variable we are getting doesn't really exist?",https://stackoverflow.com/questions/45074049,6687875,Documentation Ambiguity
49223976,Default Initialization for Tensorflow LSTM states and weights?,"<p>I am using the LSTM cell in Tensorflow.</p>

<pre><code>lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_units)
</code></pre>

<p>I was wondering how the weights and states are initialized or rather what the default initializer is for LSTM cells (states and weights) in Tensorflow?</p>

<p>And is there an easy way to manually set an Initializer?</p>

<p>Note: For <code>tf.get_variable()</code> the glorot_uniform_initializer is used as far as I could find out from the <a href=""https://www.tensorflow.org/api_docs/python/tf/get_variable"" rel=""nofollow noreferrer"">documentation</a>.</p>
",I am using the LSTM cell in Tensorflow. I was wondering how the weights and states are initialized or rather what the default initializer is for LSTM cells (states and weights) in Tensorflow? And is there an easy way to manually set an Initializer? Note: For tf.get_variable() the glorot_uniform_initializer is used as far as I could find out from the documentation.,https://stackoverflow.com/questions/49223976,5763590,Documentation Completeness
54516938,Weights decay on evaluation step - Tensorflow,"<p>My weights are defined as </p>

<pre><code>weights = {
        'W_conv1': tf.get_variable('W_conv1', shape=[...], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01)),
        'W_conv2': tf.get_variable('W_conv2', shape=[...], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01)),
        'W_conv3': tf.get_variable('W_conv3', shape=[...], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01)),
        ...
}

# conv2d network
...
</code></pre>

<p>I want to use the weights decay so I add, for example, the argument</p>

<pre><code>regularizer=tf.contrib.layers.l1_regularizer(0.0005)
</code></pre>

<p>to the <code>tf.get_variable</code>. Now I'm wondering if during the evaluation phase this is still correct or maybe I have to set the regularizer factor to 0.</p>

<p>There is also another argument <code>trainable</code>. The documentation says <code>If True also add the variable to the graph collection GraphKeys.TRAINABLE_VARIABLES.</code> which is not clear to me. Should I use it?</p>

<p>Can someone explain to me if the weights decay effects in a sort of wrong way the evaluation step? How can I solve in that case?</p>
","My weights are defined as I want to use the weights decay so I add, for example, the argument to the tf.get_variable. Now I'm wondering if during the evaluation phase this is still correct or maybe I have to set the regularizer factor to 0. There is also another argument trainable. The documentation says If True also add the variable to the graph collection GraphKeys.TRAINABLE_VARIABLES. which is not clear to me. Should I use it? Can someone explain to me if the weights decay effects in a sort of wrong way the evaluation step? How can I solve in that case?",https://stackoverflow.com/questions/54516938,9540764,Documentation Ambiguity
57438371,tf.global_variable_initializer() with regard to session?,"<p>My understandings on Sessions in Tensorflow still seem to be flawed even after reading the <a href=""https://www.tensorflow.org/guide/graphs"" rel=""nofollow noreferrer"">official documentation</a> and <a href=""https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/"" rel=""nofollow noreferrer"">this</a> tutorial. </p>

<p>In particular, does <code>tf.global_variable_initializer()</code> initialize global variables with regard to a particular session, or for all the sessions in the program? Are there ways to ""uninitialize"" a variable in / during a session?</p>

<p>Can a <code>tf.variable</code> be used in multiple sessions? The answer seems to be yes (e.g. the following code), but then are there good cases where we want multiple sessions in a program, instead of a single one?</p>

<pre><code>#!/usr/bin/env python
import tensorflow as tf

def main():
    x = tf.constant(0.)
    with tf.Session() as sess:
        print(sess.run(x))
    with tf.Session() as sess:
        print(sess.run(x))

if __name__ == '__main__':
    main()
</code></pre>
","My understandings on Sessions in Tensorflow still seem to be flawed even after reading the official documentation and this tutorial. In particular, does tf.global_variable_initializer() initialize global variables with regard to a particular session, or for all the sessions in the program? Are there ways to ""uninitialize"" a variable in / during a session? Can a tf.variable be used in multiple sessions? The answer seems to be yes (e.g. the following code), but then are there good cases where we want multiple sessions in a program, instead of a single one?",https://stackoverflow.com/questions/57438371,3736306,Documentation Ambiguity
44433438,What is the purpose of tf.global_variables_initializer?,"<p>I would like to understand what <code>tf.global_variables_initializer</code> does in a bit more detail.
A <a href=""https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer"" rel=""noreferrer"">sparse description is given here</a>:</p>

<blockquote>
  <p>Returns an Op that initializes global variables.</p>
</blockquote>

<p>But that doesn't really help me. I know that the op is necessary to initialize the graph, but what does that actually mean? Is this the step where the graph is complied? </p>
","I would like to understand what tf.global_variables_initializer does in a bit more detail. A sparse description is given here: But that doesn't really help me. I know that the op is necessary to initialize the graph, but what does that actually mean? Is this the step where the graph is complied?",https://stackoverflow.com/questions/44433438,3747801,Documentation Ambiguity
56680233,TF gradient returning zero for simple problem,"<p>In TF 2.0, I traced back a bug and isolated to this simple problem:
tf.gradient is giving zero gradient, it should be infinity.</p>

<pre><code>bxe = tf.keras.losses.BinaryCrossentropy(); 
a=tf.Variable(1.0) 
with tf.GradientTape() as tape: 
   loss = bxe([0.], a) 
grads = tape.gradient(loss, a) # compute gradients 
print(loss, grads) 

tf.Tensor(15.333239, shape=(), dtype=float32) 
tf.Tensor(0.0, shape=(), dtype=float32)
</code></pre>

<p>As you can see, the loss should be infinite cause -log(1-1) in the binary cross entropy. However, I looked up documentation and I noticed that they clip the numbers before applying the log, that's why 15.333 show up instead of infinity. That's great, and help avoiding many troubles during training, <strong>BUT</strong>, what about the gradient? It is producing zero! 
According to my math it should be infinite, so, at least it should be producing something very high, rather than zero. As such, my training is stuck! Why is that happening? How did I even arrive at this point? and how come that this doesn't happen when I use Keras high level built classifier models?</p>
","In TF 2.0, I traced back a bug and isolated to this simple problem: tf.gradient is giving zero gradient, it should be infinity. As you can see, the loss should be infinite cause -log(1-1) in the binary cross entropy. However, I looked up documentation and I noticed that they clip the numbers before applying the log, that's why 15.333 show up instead of infinity. That's great, and help avoiding many troubles during training, BUT, what about the gradient? It is producing zero! According to my math it should be infinite, so, at least it should be producing something very high, rather than zero. As such, my training is stuck! Why is that happening? How did I even arrive at this point? and how come that this doesn't happen when I use Keras high level built classifier models?",https://stackoverflow.com/questions/56680233,10870968,Documentation Ambiguity
40304005,tf.gradients applied to pooling wrong result?,"<p>I have a problem in tensorflow with tf.gradients applied to pooling: </p>

<p>[edit]: I was able to reproduce my expectation by changing the equation to:</p>

<pre><code>gradpooltest, = tf.gradients((pooltest * pooltest)/2 , [x1])
</code></pre>

<p>Anyway, I am not sure why I have to do it this way and people answered below do not seem to understand my problem.</p>

<pre><code>input x1:
[[ 0.  0.  0.  0.  0.  0.]
 [ 0.  2.  2.  2.  0.  0.]
 [ 0. -2.  0.  0.  2.  1.]
 [ 0.  1.  0.  1.  2.  2.]
 [ 0.  1.  1.  2.  0.  1.]
 [ 0. -2.  2.  1. -1.  1.]]
pooling test forward:
[[ 2.  2.  0.]
 [ 1.  1.  2.]
 [ 1.  2.  1.]]
 tf.gradients pool test backward:
[[ 0.  0.  0.  0.  1.  0.]
 [ 0.  1.  1.  0.  0.  0.]
 [ 0.  0.  0.  0.  1.  0.]
 [ 0.  1.  0.  1.  0.  0.]
 [ 0.  1.  0.  1.  0.  1.]
 [ 0.  0.  0.  0.  0.  0.]]
but I expect actually this result by  tf.gradients pool test backward:
 0     0     0     0     0     0
 0     2     2     0     0     0
 0     0     0     0     2     0
 0     1     0     1     0     0
 0     1     0     0     0     1
 0     0     2     0     0     0
</code></pre>

<p>I don't understand the tf result for tf.gradients pool test backward. (Looks like tensorflow only returns the store matrix for the locations??). Any idea why tf does not return the actual upsampling result?</p>

<p>Here is my code:</p>

<pre><code>import numpy as np
import tensorflow as tf
sess = tf.Session()

#init input-----------------------------------------------------------
init1=np.array([ [0,0,0,0,0,0],
                 [0,2,2,2,0,0],
                 [0,-2,0,0,2,1],
                 [0,1,0,1,2,2],
                 [0,1,1,2,0,1],
                 [0,-2,2,1,-1,1] ],dtype=""float32"")           
init2 = init1.reshape(1,6,6,1)
x1 = tf.Variable(init2)               
#init weight-----------------------------------------------------------
init3 = np.array( [[[[3, 5], [2, -1]]]], dtype=""float32"")
init4 = init3.reshape(2,2,1,1)
w1 = tf.Variable(init4) 

#init model-----------------------------------------------------------
model = tf.initialize_all_variables()
sess.run(model)

#print values-----------------------------------------------------------
print('x1:')
#print sess.run(x6)
x1y = tf.reshape(x1, [6, 6])
print sess.run(x1y)

###################################
#ff: pooling
################################### 
#needs 4D volumes as inputs:
pooltest = tf.nn.max_pool(x1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
print('pooltest:')
#print sess.run(pooltest)
pooltesty = tf.reshape(pooltest, [3, 3])
print sess.run(pooltesty)

###################################
#bw: pooling 
################################### 
#needs 4D volumes as inputs:
gradpooltest, = tf.gradients(pooltest , [x1])
print('gradpooltest:')
#print sess.run(gradpooltest)
gradpooltesty = tf.reshape(gradpooltest, [6, 6])
print sess.run(gradpooltesty)

sess.close()
</code></pre>
","I have a problem in tensorflow with tf.gradients applied to pooling: [edit]: I was able to reproduce my expectation by changing the equation to: Anyway, I am not sure why I have to do it this way and people answered below do not seem to understand my problem. I don't understand the tf result for tf.gradients pool test backward. (Looks like tensorflow only returns the store matrix for the locations??). Any idea why tf does not return the actual upsampling result? Here is my code:",https://stackoverflow.com/questions/40304005,4132112,Documentation Ambiguity
58228821,Tensorflow gradientTape explanation,"<p>I am trying to understand an API from tensorflow tf.gradientTape</p>

<p>Below is the code I get from the official website:</p>

<pre><code>x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as g:
  g.watch(x)
  y = x * x
  z = y * y
dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)
dy_dx = g.gradient(y, x)  # 6.0
</code></pre>

<p>I wanted to know how did they get dz_dx as 108 and dy_dx as 6? </p>

<p>I also did another test like below:</p>

<pre><code>x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as g:
  g.watch(x)
  y = x * x * x
  z = y * y
dz_dx = g.gradient(z, x)  # 1458.0 
dy_dx = g.gradient(y, x)  # 6.0
</code></pre>

<p>this time the dz_dx becomes 1458 and I do not know why at all. Could any expert show me how the calculation being done? </p>
",I am trying to understand an API from tensorflow tf.gradientTape Below is the code I get from the official website: I wanted to know how did they get dz_dx as 108 and dy_dx as 6? I also did another test like below: this time the dz_dx becomes 1458 and I do not know why at all. Could any expert show me how the calculation being done?,https://stackoverflow.com/questions/58228821,11634815,Requesting (Additional) Resources
66283913,Error in computing gradients in keras(tensorflow backend),"<p>I am trying to compute gradients of one of CNN filters from VGG16 w.r.t an image input using tensorflow-gpu version 2.4.1 and Keras version 2.4.3 with the following code:</p>
<pre><code>from keras.applications import VGG16
from keras import backend as K
model = VGG16(weights = 'imagenet', 
             include_top = False)
layer_name = 'block3_conv1'
filter_index = 0
layer_output = model.get_layer(layer_name).output
loss = K.mean(layer_output[:, :, :, filter_index])

grads = K.gradients(loss, model.input)[0]


</code></pre>
<p>this results in the following error:</p>
<blockquote>
<p>RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.</p>
</blockquote>
<p>Also trying to use <code>tf.GradientTape</code> raised another error:</p>
<pre><code>with tf.GradientTape() as gtape:
    grads = gtape.gradient(loss, model.input)
</code></pre>
<blockquote>
<p>AttributeError: 'KerasTensor' object has no attribute '_id'</p>
</blockquote>
<p>trying to disable eager execution did not work either:</p>
<pre><code>tf.compat.v1.disable_eager_execution()
</code></pre>
<p>since it returns gradients as None.
I would appreciate any kind of information about any way to resolve this issue.
Thanks in advance.</p>
",I am trying to compute gradients of one of CNN filters from VGG16 w.r.t an image input using tensorflow-gpu version 2.4.1 and Keras version 2.4.3 with the following code: this results in the following error: Also trying to use tf.GradientTape raised another error: trying to disable eager execution did not work either: since it returns gradients as None. I would appreciate any kind of information about any way to resolve this issue. Thanks in advance.,https://stackoverflow.com/questions/66283913,12654107,Requesting (Additional) Resources
56588353,How to use Tensorflow BatchNormalization with GradientTape?,"<p>Suppose we have a simple Keras model that uses BatchNormalization:</p>

<pre><code>model = tf.keras.Sequential([
                     tf.keras.layers.InputLayer(input_shape=(1,)),
                     tf.keras.layers.BatchNormalization()
])
</code></pre>

<p>How to actually use it with GradientTape? The following doesn't seem to work as it doesn't update the moving averages?</p>

<pre><code># model training... we want the output values to be close to 150
for i in range(1000):
  x = np.random.randint(100, 110, 10).astype(np.float32)
  with tf.GradientTape() as tape:
    y = model(np.expand_dims(x, axis=1))
    loss = tf.reduce_mean(tf.square(y - 150))
  grads = tape.gradient(loss, model.variables)
  opt.apply_gradients(zip(grads, model.variables))
</code></pre>

<p>In particular, if you inspect the moving averages, they remain the same (inspect model.variables, averages are always 0 and 1). I know one can use .fit() and .predict(), but I would like to use the GradientTape and I'm not sure how to do this. Some version of the documentation suggests to update update_ops, but that doesn't seem to work in eager mode.</p>

<p>In particular, the following code will not output anything close to 150 after the above training.</p>

<pre><code>x = np.random.randint(200, 210, 100).astype(np.float32)
print(model(np.expand_dims(x, axis=1)))
</code></pre>
","Suppose we have a simple Keras model that uses BatchNormalization: How to actually use it with GradientTape? The following doesn't seem to work as it doesn't update the moving averages? In particular, if you inspect the moving averages, they remain the same (inspect model.variables, averages are always 0 and 1). I know one can use .fit() and .predict(), but I would like to use the GradientTape and I'm not sure how to do this. Some version of the documentation suggests to update update_ops, but that doesn't seem to work in eager mode. In particular, the following code will not output anything close to 150 after the above training.",https://stackoverflow.com/questions/56588353,1048214,Documentation Replication on Other Examples
60665006,Conceptual understanding of GradientTape.gradient,"<h2>Background</h2>

<p>In Tensorflow 2, there exists a class called <code>GradientTape</code> which is used to record operations on tensors, the result of which can then be differentiated and fed to some minimization algorithm.  For example, <a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">from the documentation</a> we have this example:</p>

<pre><code>x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x) # Will compute to 6.0
</code></pre>

<p>The <a href=""https://raw.githubusercontent.com/tensorflow/tensorflow/v2.1.0/tensorflow/python/eager/backprop.py"" rel=""nofollow noreferrer"">docstring</a> for the <code>gradient</code> method implies that the first argument can be not just a tensor, but a list of tensors:</p>

<pre><code> def gradient(self,
               target,
               sources,
               output_gradients=None,
               unconnected_gradients=UnconnectedGradients.NONE):
    """"""Computes the gradient using operations recorded in context of this tape.

    Args:
      target: a list or nested structure of Tensors or Variables to be
        differentiated.
      sources: a list or nested structure of Tensors or Variables. `target`
        will be differentiated against elements in `sources`.
      output_gradients: a list of gradients, one for each element of
        target. Defaults to None.
      unconnected_gradients: a value which can either hold 'none' or 'zero' and
        alters the value which will be returned if the target and sources are
        unconnected. The possible values and effects are detailed in
        'UnconnectedGradients' and it defaults to 'none'.

    Returns:
      a list or nested structure of Tensors (or IndexedSlices, or None),
      one for each element in `sources`. Returned structure is the same as
      the structure of `sources`.

    Raises:
      RuntimeError: if called inside the context of the tape, or if called more
       than once on a non-persistent tape.
      ValueError: if the target is a variable or if unconnected gradients is
       called with an unknown value.
    """"""
</code></pre>

<p>In the above example, it is easy to see that <code>y</code>, the <code>target</code>, is the function to be differentiated, and <code>x</code> is the dependent variable the ""gradient"" is taken with respect to.</p>

<p>From my limited experience, it appears that the <code>gradient</code> method returns a list of tensors, one per each element of <code>sources</code>, and each of these gradients is a tensor that is the same shape as the corresponding member of <code>sources</code>.</p>

<h2>Question</h2>

<p>The above description of the behavior of <code>gradients</code> makes sense if <code>target</code> contains a single 1x1 ""tensor"" to be differentiated, because mathematically a gradient vector should be the same dimension as the domain of the function. </p>

<p>However, if <code>target</code> is a list of tensors, the output of <code>gradients</code> is still the same shape. Why is this the case? If <code>target</code> is thought of as a list of functions, shouldn't the output resemble something like a Jacobian?  How am I to interpret this behavior conceptually?</p>
","In Tensorflow 2, there exists a class called GradientTape which is used to record operations on tensors, the result of which can then be differentiated and fed to some minimization algorithm. For example, from the documentation we have this example: The docstring for the gradient method implies that the first argument can be not just a tensor, but a list of tensors: In the above example, it is easy to see that y, the target, is the function to be differentiated, and x is the dependent variable the ""gradient"" is taken with respect to. From my limited experience, it appears that the gradient method returns a list of tensors, one per each element of sources, and each of these gradients is a tensor that is the same shape as the corresponding member of sources. The above description of the behavior of gradients makes sense if target contains a single 1x1 ""tensor"" to be differentiated, because mathematically a gradient vector should be the same dimension as the domain of the function. However, if target is a list of tensors, the output of gradients is still the same shape. Why is this the case? If target is thought of as a list of functions, shouldn't the output resemble something like a Jacobian? How am I to interpret this behavior conceptually?",https://stackoverflow.com/questions/60665006,6204891,Requesting (Additional) Resources
66171799,Tensorflow incompatible matrix size when using GradientTape,"<p>I am trying to run code that previously worked on tensorflow 2.2.0 on version 2.4.0-rc0 for apple silicon (using python 3.8), but it is now generating the following error regarding the matrix dimensions:</p>
<p><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: GetOutputShape: Matrix size-incompatible: In[0]: [256,4], In[1]: [4,400]</code></p>
<p>I am using nested gradient tapes to compute the gradient of my MLP model wrt the inputs (which form part of the loss), after which I compute the gradient of the loss wrt the trainable variables as below:</p>
<pre><code>    def get_grad_and_loss(self, x, y):
        with tf.GradientTape(persistent=True) as gl_tape:
            gl_tape.watch(x)

            with tf.GradientTape(persistent=True) as l_tape:
                l_tape.watch(x)
                y_pred = self.call(x)

            grad_mat = l_tape.gradient(y_pred, x)
            loss = tf.reduce_mean(tf.math.square(y_pred - y[:, tf.newaxis])) + tf.reduce_mean(tf.maximum(0, -1 * (grad_mat[:, 0])))

        g = gl_tape.gradient(loss, self.trainable_weights)

        return g, loss
</code></pre>
<p>In words I am computing the MSE and trying to force the sign of the gradient to be positive (as a soft constraint). I have read through the documentation on <a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">gradient tape</a> and as I understand it, setting <code>persistent=True</code> should allow me to recompute gradients freely. As a side note my code works fine if I omit the nested gradient tape and simply use the MSE metric, so I don't think the issue lies anywhere else in the code. Any pointers would be much appreciated, thanks in advance :)</p>
","I am trying to run code that previously worked on tensorflow 2.2.0 on version 2.4.0-rc0 for apple silicon (using python 3.8), but it is now generating the following error regarding the matrix dimensions: tensorflow.python.framework.errors_impl.InvalidArgumentError: GetOutputShape: Matrix size-incompatible: In[0]: [256,4], In[1]: [4,400] I am using nested gradient tapes to compute the gradient of my MLP model wrt the inputs (which form part of the loss), after which I compute the gradient of the loss wrt the trainable variables as below: In words I am computing the MSE and trying to force the sign of the gradient to be positive (as a soft constraint). I have read through the documentation on gradient tape and as I understand it, setting persistent=True should allow me to recompute gradients freely. As a side note my code works fine if I omit the nested gradient tape and simply use the MSE metric, so I don't think the issue lies anywhere else in the code. Any pointers would be much appreciated, thanks in advance :)",https://stackoverflow.com/questions/66171799,12860541,Requesting (Additional) Resources
51907487,How to use TensorFlow with Flask,"<p>I am building a multi threaded rest api with using flask, tensorflow and keras models. After getting <a href=""https://stackoverflow.com/questions/40785224/tensorflow-cannot-interpret-feed-dict-key-as-tensor"">this</a> error, I did some research and came up with the following solution:</p>

<pre><code>executor = ThreadPoolExecutor(10)

@app.route('/createLearningTask', methods=['POST'])
def createLearningTask():
    request_data = request.get_json(force=True)
    executor.submit(LearningTask().processData)
    return ('', 200) 
</code></pre>

<p>Basically, I submit each new post request to executor. And in each request, I build model with given parameters, producing model, predict and store result for another GET request.</p>

<pre><code>class LearningTask:

    resultDict = {} # access to this map is protected by locks, which I omitted in here

    def processData(self, **kwargs):
        graph = tf.Graph() # tf = tensorflow
        with graph.as_default():
            with tf.Session().as_default():
                model = Sequential() # keras model
                model.add(..)
                model.add(..)
                model.add(..)
                model.compile(..)
                model.fit(..)
                model.predict(..)
</code></pre>

<p>I omitted irrelevant parts of the code. It works good, after processing data and getting results, I save it to dictionary. I create new graph and new session for each post request.</p>

<p>After reading <a href=""https://github.com/keras-team/keras/issues/8538"" rel=""nofollow noreferrer"">this</a> and <a href=""https://github.com/keras-team/keras/issues/2397"" rel=""nofollow noreferrer"">this</a> github discussions, I produced my solution.</p>

<p><strong><em>My question is, is that solution safe and correct usage of tensorflow? In <a href=""https://www.tensorflow.org/api_docs/python/tf/Graph"" rel=""nofollow noreferrer"">documentation</a>, it says graph is not thread safe, but I did some load test and it can handle simultaneous requests with no problem.</em></strong></p>
","I am building a multi threaded rest api with using flask, tensorflow and keras models. After getting this error, I did some research and came up with the following solution: Basically, I submit each new post request to executor. And in each request, I build model with given parameters, producing model, predict and store result for another GET request. I omitted irrelevant parts of the code. It works good, after processing data and getting results, I save it to dictionary. I create new graph and new session for each post request. After reading this and this github discussions, I produced my solution. My question is, is that solution safe and correct usage of tensorflow? In documentation, it says graph is not thread safe, but I did some load test and it can handle simultaneous requests with no problem.",https://stackoverflow.com/questions/51907487,6002951,Requesting (Additional) Resources
56899105,How to use regularizer argument in tf.get_variable?,"<p>The syntax of the usage is clear:</p>

<pre><code>decay = tf.constant(0.001, dtype=tf.float32)
w = tf.get_variable(name='weight', shape=[512, 512],
                    regularizer=tf.contrib.layers.l2_regularizer(decay))
</code></pre>

<p>However, in the documentation only the following is stated:</p>

<blockquote>
  <p><code>regularizer</code>: A (Tensor -> Tensor or None) function; the result of applying it on a newly created variable will be added to the collection <code>tf.GraphKeys.REGULARIZATION_LOSSES</code> and can be used for regularization.</p>
</blockquote>

<p>The above does not imply that the regularization loss is automatically minimized. So do we need to manually get the variable from the collection  <code>tf.GraphKeys.REGULARIZATION_LOSSES</code> and add it to our main loss in order for it to be applied?</p>
","The syntax of the usage is clear: However, in the documentation only the following is stated: The above does not imply that the regularization loss is automatically minimized. So do we need to manually get the variable from the collection tf.GraphKeys.REGULARIZATION_LOSSES and add it to our main loss in order for it to be applied?",https://stackoverflow.com/questions/56899105,4958717,Documentation Completeness
54887445,Tensorflow tf.hessian returns only zeros,"<p>I have a trained keras model of which I need to compute both the gradients and hessian of the output respect to the input.
The input <code>X</code> is a 5000x3 numpy array and the output <code>y</code> is 5000x1.</p>

<p>The gradient computation works fine both using keras' gradients and tensorflow's gradients functions, and I get an array 5000x3 with the correct values in it, but the hessian using tf.hessian() returns only zeros.
This should not be the case as my model is approximating a highly nonlinear function, so that second derivatives are well expected to be nonzero.</p>

<p>The code is the following (I simplified some parameters for redeability):</p>

<pre><code>def get_derivatives_NN(X, y):

    # Define Keras model
    model = keras.Sequential()
    model.add(keras.layers.Dense(500, activation=tf.nn.relu, input_shape=(X.shape[1],)))
    model.add(keras.layers.Dense(300, activation=tf.nn.relu))
    model.add(keras.layers.Dense(100, activation=tf.nn.relu))
    model.add(keras.layers.Dense(y.shape[1]))

    # Compile and fit model
    optimz = keras.optimizers.Adam(optimizer_parameters)
    model.compile(optimizer=optimz, loss='mse', metrics=['mae'])
    model.fit(X, y, epochs = 200, validation_split=0)

    # Evaluate gradients in Keras
    grads = keras.backend.gradients(model.output, model.input)[0] # tensor
    get_gradients = keras.backend.function([model.input], [grads])
    evaluated_gradients = get_gradients([X]) # this is the evaluated gradient in Keras

    # Evaluate gradienst in tf
    session = keras.backend.get_session()
    session.run(tf.global_variables_initializer())
    evaluated_gradients_TF = session.run(tf.gradients(model.output, model.input), feed_dict={model.input: X})

    # Evaluate hessian in tf
    evaluated_hessian = session.run(tf.hessians(model.output, model.input), feed_dict={model.input: X})

    return evaluated_gradients, evaluated_gradients_TF, evaluated_hessian
</code></pre>

<p>The output is (truncating my copy-paste):</p>

<pre><code>GRADIENT KERAS:
[array([[-0.00286908,  0.06114262,  0.0178928 ],
       [-0.00717778,  0.05055936,  0.0415092 ],
       [-0.00725342,  0.0075229 ,  0.06268862],
       ..., dtype=float32)]


GRADIENT TF:
[array([[-0.00286908,  0.06114262,  0.0178928 ],
       [-0.00717778,  0.05055936,  0.0415092 ],
       [-0.00725342,  0.0075229 ,  0.06268862],
       ..., dtype=float32)]

HESSIAN TF:
[array([[[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         ...,
         [0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.]], ....... etcetera
</code></pre>

<p>There are two problems with this:</p>

<p>1) The size of the hessian doesn't really make sense to me. I expected a (5000, 3, 3) array, or a (5000,9) at most, while I get a (5000, 3, 5000, 3);</p>

<p>2) The values are all zeros, I have checked with <code>np.count_nonzero(evaluated_hessian)</code> which returns <code>0</code>.</p>

<p>I would understand if both the gradient and the hessian calculation failed, then it would be clear I have made something silly... but gradients works fine while hessians fails, and the docs seem to indicate they both obey to the same syntax call, which is what I have done here.
Any help as to why this is happening?</p>

<p>EDIT:
If I use the calculated gradient as input for another <code>get_derivative_NN</code> call I get the correct value for the second derivative out, so this proves that there is something strange going on with the <code>tf.hessians()</code> function.</p>
","I have a trained keras model of which I need to compute both the gradients and hessian of the output respect to the input. The input X is a 5000x3 numpy array and the output y is 5000x1. The gradient computation works fine both using keras' gradients and tensorflow's gradients functions, and I get an array 5000x3 with the correct values in it, but the hessian using tf.hessian() returns only zeros. This should not be the case as my model is approximating a highly nonlinear function, so that second derivatives are well expected to be nonzero. The code is the following (I simplified some parameters for redeability): The output is (truncating my copy-paste): There are two problems with this: 1) The size of the hessian doesn't really make sense to me. I expected a (5000, 3, 3) array, or a (5000,9) at most, while I get a (5000, 3, 5000, 3); 2) The values are all zeros, I have checked with np.count_nonzero(evaluated_hessian) which returns 0. I would understand if both the gradient and the hessian calculation failed, then it would be clear I have made something silly... but gradients works fine while hessians fails, and the docs seem to indicate they both obey to the same syntax call, which is what I have done here. Any help as to why this is happening? EDIT: If I use the calculated gradient as input for another get_derivative_NN call I get the correct value for the second derivative out, so this proves that there is something strange going on with the tf.hessians() function.",https://stackoverflow.com/questions/54887445,2924735,Documentation Replication on Other Examples
38543850,How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots)?,"<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md#image-dashboard"" rel=""noreferrer"">Image Dashboard</a> section of the Tensorboard ReadMe says:</p>

<blockquote>
  <p>Since the image dashboard supports arbitrary pngs, you can use this to embed custom visualizations (e.g. matplotlib scatterplots) into TensorBoard.</p>
</blockquote>

<p>I see how a pyplot image could be written to file, read back in as a tensor, and then used with tf.image_summary() to write it to TensorBoard, but this statement from the readme suggests there is a more direct way. Is there? If so, is there any further documentation and/or examples of how to do this efficiently?  </p>
","The Image Dashboard section of the Tensorboard ReadMe says: I see how a pyplot image could be written to file, read back in as a tensor, and then used with tf.image_summary() to write it to TensorBoard, but this statement from the readme suggests there is a more direct way. Is there? If so, is there any further documentation and/or examples of how to do this efficiently?",https://stackoverflow.com/questions/38543850,5587428,Requesting (Additional) Resources
55852943,How can i use tf.image.draw_bounding_boxes to draw bounding boxes on my original image to show where my object was detected?,"<p>Im new to Tensorflow and so far I've been able to build a classifier using data i got from Kaggle for a flower dataset and I have been able to train a CNN to identify a sunflower vs a daisy and plot the results with labels using the matplotlib.pyplot.figure() call.</p>

<p>Now I want to actually draw a bounding box on the original image itself to show where it detected the flower. I read about tf.image.draw_bounding_boxes but im a bit confused how to use it because technically the CNN has already drawn a bounding box over objects to be able to classify it. Is there a way to tap into that operation and draw abounding box the moment it frames an object in the source file?</p>

<p>Here is an example of what I want to do. I want to train my model on identifying sunflowers and then when I present a picture with sunflowers I want it to find where the sunflowers are and draw a bounding box around each sunflower.</p>

<p><img src=""https://i.stack.imgur.com/UJbxd.jpg"" alt=""sunflower""></p>

<p>and here is my code i'm using for this tutorial (assume the first three lines are just basic functions that create the labels and irrelevant for this question)</p>

<pre><code>training_images = train_data_with_label()
testing_images = test_data_with_label()
TTest = test_new_data()

# Assign images and labels
tr_img_data = np.array([i[0] for i in training_images]).reshape(-1, 64, 64, 1)
tr_lbl_data = np.array([i[1] for i in training_images])
tst_img_data = np.array([i[0] for i in testing_images]).reshape(-1, 64, 64, 1)
tst_lbl_data = np.array([i[1] for i in testing_images])

model = Sequential()

model.add(InputLayer(input_shape=[64, 64, 1]))

model.add(Conv2D(filters=16, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=5, padding='same'))

model.add(Conv2D(filters=32, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=5, padding='same'))

model.add(Conv2D(filters=50, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=5, padding='same'))

model.add(Conv2D(filters=80, kernel_size=5, strides=1, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=5, padding='same'))

model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(rate=0.5))
model.add(Dense(classes, activation='softmax'))
optimizer = Adam(lr=1e-4)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x=tr_img_data, y=tr_lbl_data, epochs=1000, batch_size=50)
model.summary()


# Save the model's weights
pth = 'S:/SavedWeights/Daisy_vs_Sunflower_vs_Tulip/weights.hdf5'
model.save_weights(pth, overwrite=True)
print(""Weights saved!!!"")


fig = plt.figure(figsize=(14, 14))

for cnt, data in enumerate(TTest[0:14]):

    y = fig.add_subplot(6, 5, cnt+1)
    img = data[0]
    data = img.reshape(1, 64, 64, 1)
    model_out = model.predict([data])

    if np.argmax(model_out) == 0:
        str_label = 'Daisy'
    elif np.argmax(model_out) == 1:
        str_label = 'Sunflower'
    else:
        str_label = 'Tulip'

    y.imshow(img, cmap='gray')
    plt.title(str_label)
    y.axes.get_xaxis().set_visible(False)
    y.axes.get_yaxis().set_visible(False)

plt.show()
</code></pre>

<p>What i would like to find is an example of how i can use this library on this kind of tutorial code and then take an arbitrary image and determine if there are sunflowers in that image and draw a box around them.</p>

<p>Thank you!</p>
",Im new to Tensorflow and so far I've been able to build a classifier using data i got from Kaggle for a flower dataset and I have been able to train a CNN to identify a sunflower vs a daisy and plot the results with labels using the matplotlib.pyplot.figure() call. Now I want to actually draw a bounding box on the original image itself to show where it detected the flower. I read about tf.image.draw_bounding_boxes but im a bit confused how to use it because technically the CNN has already drawn a bounding box over objects to be able to classify it. Is there a way to tap into that operation and draw abounding box the moment it frames an object in the source file? Here is an example of what I want to do. I want to train my model on identifying sunflowers and then when I present a picture with sunflowers I want it to find where the sunflowers are and draw a bounding box around each sunflower. and here is my code i'm using for this tutorial (assume the first three lines are just basic functions that create the labels and irrelevant for this question) What i would like to find is an example of how i can use this library on this kind of tutorial code and then take an arbitrary image and determine if there are sunflowers in that image and draw a box around them. Thank you!,https://stackoverflow.com/questions/55852943,3530309,Requesting (Additional) Resources
41216576,Tensorflow: transfer learning from vgg16 .tfmodel file,"<p>I'm trying to make a TF implementation of an image classifier (with py3.5 and Windows 10, TF 0.12), so I'm re-using existing models <a href=""https://www.tensorflow.org/versions/master/how_tos/image_retraining/#using_the_retrained_model"" rel=""nofollow noreferrer"">as described here</a> but without all the weird Bazel stuff. After fixing a py2-to-3 bug on <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py#L887"" rel=""nofollow noreferrer"">this line</a> (wrapping the <code>keys()</code> in <code>list()</code>), it ran nicely on my 10 folders of different categories. However, the performance is lacking; the training success rate is around 83% and the validation set is never above 60% at best. So I'd like to do some transfer learning from a vgg16 model (which is one I've used before in Caffe/ubuntu); <a href=""https://github.com/ry/tensorflow-vgg16"" rel=""nofollow noreferrer"">one I've found is here</a> ready to be downloaded. </p>

<p>My question now is, how do you load a .tfmodel file in Tensorflow? The script is expecting a tar.gz to be downloaded, fair enough. It apparently contains a file called <code>classify_image_graph_def.pb</code>, which is not a .tfmodel file. Looking in <a href=""https://github.com/ry/tensorflow-vgg16/blob/master/tf_forward.py"" rel=""nofollow noreferrer"">some example code</a> I see that it's pretty easy to load a .tfmodel file, so I've modified the <code>create_inception_graph</code> function to point straight at the <code>vgg16-20160129.tfmodel</code> file. Upon running this, I get this error:</p>

<pre><code>File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\importer.py"", line 450, in import_graph_def
    ret.append(name_to_op[operation_name].outputs[output_index])
KeyError: 'pool_3/_reshape'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""retrain.py"", line 995, in &lt;module&gt;
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\platform\app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""retrain.py"", line 713, in main
    create_inception_graph())
  File ""retrain.py"", line 235, in create_inception_graph
    RESIZED_INPUT_TENSOR_NAME]))
  File ""C:\Users\User\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\importer.py"", line 453, in import_graph_def
    'Requested return_element %r not found in graph_def.' % name)
ValueError: Requested return_element 'pool_3/_reshape:0' not found in graph_def.
</code></pre>

<p>And this is the loading code:</p>

<pre><code>def create_inception_graph():
  """"""""Creates a graph from saved GraphDef file and returns a Graph object.
  Returns:
    Graph holding the trained Inception network, and various tensors we'll be
    manipulating.
  """"""
  with tf.Session() as sess:
    #model_filename = os.path.join(
    #    FLAGS.model_dir, 'classify_image_graph_def.pb')
    model_filename = os.path.join(
        FLAGS.model_dir, 'vgg16-20160129.tfmodel')
    with gfile.FastGFile(model_filename, 'rb') as f:
      graph_def = tf.GraphDef()
      graph_def.ParseFromString(f.read())
      bottleneck_tensor, jpeg_data_tensor, resized_input_tensor = (
          tf.import_graph_def(graph_def, name='', return_elements=[
              BOTTLENECK_TENSOR_NAME, JPEG_DATA_TENSOR_NAME,
              RESIZED_INPUT_TENSOR_NAME]))
  return sess.graph, bottleneck_tensor, jpeg_data_tensor, resized_input_tensor
</code></pre>

<p>Something seems to be going awry in the <code>tf.import_graph_def</code> call but there's no documentation for that function, weirdly. Is what I'm trying even possible? There's a whole bunch of bottleneck tensor and jpeg data and resized input tensor names that I don't know what they're there for, which the example doesn't replicate. </p>
","I'm trying to make a TF implementation of an image classifier (with py3.5 and Windows 10, TF 0.12), so I'm re-using existing models as described here but without all the weird Bazel stuff. After fixing a py2-to-3 bug on this line (wrapping the keys() in list()), it ran nicely on my 10 folders of different categories. However, the performance is lacking; the training success rate is around 83% and the validation set is never above 60% at best. So I'd like to do some transfer learning from a vgg16 model (which is one I've used before in Caffe/ubuntu); one I've found is here ready to be downloaded. My question now is, how do you load a .tfmodel file in Tensorflow? The script is expecting a tar.gz to be downloaded, fair enough. It apparently contains a file called classify_image_graph_def.pb, which is not a .tfmodel file. Looking in some example code I see that it's pretty easy to load a .tfmodel file, so I've modified the create_inception_graph function to point straight at the vgg16-20160129.tfmodel file. Upon running this, I get this error: And this is the loading code: Something seems to be going awry in the tf.import_graph_def call but there's no documentation for that function, weirdly. Is what I'm trying even possible? There's a whole bunch of bottleneck tensor and jpeg data and resized input tensor names that I don't know what they're there for, which the example doesn't replicate.",https://stackoverflow.com/questions/41216576,3234562,Lack of Alternative Solutions/Documentation
37146272,How do I get TensorFlow's 'import_graph_def' to return Tensors,"<p>If I attempt to import a saved <a href=""https://www.tensorflow.org"" rel=""noreferrer"">TensorFlow</a> graph definition with</p>

<pre><code>import tensorflow as tf
from tensorflow.python.platform import gfile

with gfile.FastGFile(FLAGS.model_save_dir.format(log_id) + '/graph.pb', 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
x, y, y_ = tf.import_graph_def(graph_def, 
                               return_elements=['data/inputs',
                                                'output/network_activation',
                                                'data/correct_outputs'],
                               name='')
</code></pre>

<p>the returned values are not <code>Tensor</code>s as expected, but something else: instead, for example, of getting <code>x</code> as </p>

<pre><code>Tensor(""data/inputs:0"", shape=(?, 784), dtype=float32)
</code></pre>

<p>I get</p>

<pre><code>name: ""data/inputs_1""
op: ""Placeholder""
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""shape""
  value {
    shape {
    }
  }
}
</code></pre>

<p>That is, instead of getting the expected tensor <code>x</code> I get, <code>x.op</code>. This confuses me because the <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/framework.html#import_graph_def"" rel=""noreferrer"">documentation</a> seems to say I should get a <code>Tensor</code> (though there are a bunch of <em>or</em>s there that make it hard to understand).</p>

<p>How do I get <code>tf.import_graph_def</code> to return specific <code>Tensor</code>s that I can then use (e.g. in feeding the loaded model, or running analyses)?</p>
","If I attempt to import a saved TensorFlow graph definition with the returned values are not Tensors as expected, but something else: instead, for example, of getting x as I get That is, instead of getting the expected tensor x I get, x.op. This confuses me because the documentation seems to say I should get a Tensor (though there are a bunch of ors there that make it hard to understand). How do I get tf.import_graph_def to return specific Tensors that I can then use (e.g. in feeding the loaded model, or running analyses)?",https://stackoverflow.com/questions/37146272,656912,Documentation Replication on Other Examples
49692842,Tensorflow: Importing GraphDef with Placeholder,"<p>How do I replace a placeholder in a GraphDef loaded from a file to connect the imported graph to a dataset provider? </p>

<p>This script borrows heavily from the <a href=""https://github.com/tensorflow/models/blob/master/research/slim/eval_image_classifier.py"" rel=""nofollow noreferrer""><code>eval_image_classifier.py</code></a> script as part of the <code>slim</code> API. </p>

<p>First I open a graph</p>

<pre><code>with tf.Graph().as_default():
</code></pre>

<p>Then I set up a dataset provider and preprocessing function using the <code>slim</code> API</p>

<pre><code>  # Select the dataset
  # Create a dataset provider that loads data from the dataset
  # Select the preprocessing function
  ...
  image = image_preprocessing_fn(image, eval_image_size, eval_image_size)

  images, labels = tf.train.batch(
      [image, label],
      batch_size=batch_size,
      num_threads=num_preprocessing_threads,
      capacity=5 * batch_size)
</code></pre>

<p>Then I import a graph from a GraphDef from a file and load it into the current graph using <code>import_graph_def</code></p>

<pre><code>  quantized_graph_def = graph_pb2.GraphDef()
  with tf.gfile.FastGFile(path.join(cwd(), quantized_graph_filename), 'rb') as f:
    quantized_graph_def.ParseFromString(f.read())
  tf.import_graph_def(quantized_graph_def, input_map={'batch': images}, name='')
</code></pre>

<p>Then I set up the metrics and call <code>slim.evaluation.evaluate_once</code> to process the batches</p>

<pre><code>  # Define the metrics:
  names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({
      'Accuracy': slim.metrics.streaming_accuracy(predictions, labels),
      'Recall_5': slim.metrics.streaming_recall_at_k(
          logits, labels, 5),
  })

  ...

  slim.evaluation.evaluate_once(
      master=master,
      checkpoint_path=checkpoint_path,
      logdir=log_dir,
      num_evals=num_batches,
      eval_op=list(names_to_updates.values()))
</code></pre>

<p>When I run this I get the following error:</p>

<pre><code>Caused by op 'batch_1', defined at:
  File ""vanilla_vgg.py"", line 319, in &lt;module&gt;
    import_quantized_graph_with_imagenet()
  File ""vanilla_vgg.py"", line 251, in import_quantized_graph_with_imagenet
    tf.import_graph_def(quantized_graph_def, input_map={'batch': images}, name='')
  File ""/localtmp/mp3t/venv/doggett/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/localtmp/mp3t/venv/doggett/lib/python3.5/site-packages/tensorflow/python/framework/importer.py"", line 553, in import_graph_def
    op_def=op_def)
  File ""/localtmp/mp3t/venv/doggett/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3271, in create_op
    op_def=op_def)
  File ""/localtmp/mp3t/venv/doggett/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'batch_1' with dtype float and shape [100,224,224,3]
         [[Node: batch_1 = Placeholder[dtype=DT_FLOAT, shape=[100,224,224,3], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
</code></pre>

<p>The GraphDef that I am loading has a Placeholder op with the name <code>batch</code>, with the same shape and dtype as the tensor <code>images</code>. For reference, running <code>print(images)</code> returns:</p>

<pre><code>Tensor(""batch:0"", shape=(100, 224, 224, 3), dtype=float32)
</code></pre>

<p>Note that I have supplied the argument <code>input_map</code> to the <code>import_graph_def</code> function that should replace the <code>batch</code> placeholder with the tensor <code>images</code>. I have also tried using <code>batch:0</code> and <code>batch_1</code> as the key to the <code>input_map</code> but neither works.</p>

<p>According to the documentation for <code>tf.import_graph_def</code>:</p>

<blockquote>
  <p><code>input_map</code>: A dictionary mapping input names (as strings) in graph_def to Tensor objects. The values of the named input tensors in the imported graph will be re-mapped to the respective Tensor values.</p>
</blockquote>

<p>As I understand it, the <code>input_map</code> argument should connect the two graphs, but that doesn't seem to be working. See related article <a href=""https://blog.konpat.me/tf-connecting-two-graphs-together/"" rel=""nofollow noreferrer"">""Connecting Two Graphs Together using <code>import_graph_def</code>""</a>. I believe that I am doing the same thing as in the article.</p>

<p>Also, <code>evaluate_once</code> is a function that runs a batch of images in a single function call, so I cannot simply call <code>images.eval()</code> and pass the result to <code>evaluate_once</code> because it would only run the first batch. So the two graphs must be connected and able to be run with a single invocation.</p>
","How do I replace a placeholder in a GraphDef loaded from a file to connect the imported graph to a dataset provider? This script borrows heavily from the eval_image_classifier.py script as part of the slim API. First I open a graph Then I set up a dataset provider and preprocessing function using the slim API Then I import a graph from a GraphDef from a file and load it into the current graph using import_graph_def Then I set up the metrics and call slim.evaluation.evaluate_once to process the batches When I run this I get the following error: The GraphDef that I am loading has a Placeholder op with the name batch, with the same shape and dtype as the tensor images. For reference, running print(images) returns: Note that I have supplied the argument input_map to the import_graph_def function that should replace the batch placeholder with the tensor images. I have also tried using batch:0 and batch_1 as the key to the input_map but neither works. According to the documentation for tf.import_graph_def: As I understand it, the input_map argument should connect the two graphs, but that doesn't seem to be working. See related article ""Connecting Two Graphs Together using import_graph_def"". I believe that I am doing the same thing as in the article. Also, evaluate_once is a function that runs a batch of images in a single function call, so I cannot simply call images.eval() and pass the result to evaluate_once because it would only run the first batch. So the two graphs must be connected and able to be run with a single invocation.",https://stackoverflow.com/questions/49692842,129814,Requesting (Additional) Resources
41439254,What are the differences between tf.initialize_all_variables() and tf.global_variables_initializer(),"<p>On Tensorflow official website, it gives explantions of the <code>tf.initialize_all_variables()</code> and <code>tf.global_variables_initializer()</code> functions as follow  </p>

<blockquote>
  <h3>tf.initialize_all_variables():</h3>
  
  <p>Returns an op that initializes all variables.</p>
  
  <h3>tf.global_variables_initializer():</h3>
  
  <p>Adds an op to initialize all variables in the model</p>
</blockquote>

<p>It seems like both can be used to initialize all variables in graphs. Can we use these two functions exchangbly? If not, what would be the differences? </p>
","On Tensorflow official website, it gives explantions of the tf.initialize_all_variables() and tf.global_variables_initializer() functions as follow It seems like both can be used to initialize all variables in graphs. Can we use these two functions exchangbly? If not, what would be the differences?",https://stackoverflow.com/questions/41439254,6733064,Documentation Ambiguity
38400360,Working with tensorflow's shuffle_batch method,"<p>I'm experimenting with <a href=""https://www.tensorflow.org/"" rel=""nofollow"">tensorflow</a> and I'm trying to read from a <code>csv</code> file and print out a batch of its data via <code>shuffle_batch</code>. I've gone throw the <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#decode_csv"" rel=""nofollow"">decode_csv docs</a> and the <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#shuffle_batch"" rel=""nofollow"">shuffle_batch docs</a>, but I'm still unable to get it working.</p>

<p>Here's what I have:
import tensorflow as tf</p>

<pre><code>sess = tf.InteractiveSession()

filename_queue = tf.train.string_input_producer(
    [""./data/train.csv""], num_epochs=1, shuffle=True) # total record count in csv is 30K
reader = tf.TextLineReader()
key, value = reader.read(filename_queue)

record_defaults = [[""1""], [""2""]] # irrelevant for this discussion
input, outcome = tf.decode_csv(value, record_defaults=record_defaults)

min_after_dequeue = 1000
batch_size = 10
capacity = min_after_dequeue + 3 * batch_size

example_batch = tf.train.shuffle_batch([outcome], batch_size, capacity, min_after_dequeue)
coord = tf.train.Coordinator()
tf.train.start_queue_runners(sess, coord=coord)
example_batch.eval(session = sess)
</code></pre>

<p>Running this will generate this exception:</p>

<pre><code>OutOfRangeError: RandomShuffleQueue
    '_3_shuffle_batch_1/random_shuffle_queue' is closed 
    and has insufficient elements (requested 10, current size 0)
</code></pre>

<p>I'm not sure what the issue is. I have a feeling it's due to the session and the way I'm handling it; I'm probably not doing it properly. </p>
","I'm experimenting with tensorflow and I'm trying to read from a csv file and print out a batch of its data via shuffle_batch. I've gone throw the decode_csv docs and the shuffle_batch docs, but I'm still unable to get it working. Here's what I have: import tensorflow as tf Running this will generate this exception: I'm not sure what the issue is. I have a feeling it's due to the session and the way I'm handling it; I'm probably not doing it properly.",https://stackoverflow.com/questions/38400360,409865,Requesting (Additional) Resources
66685673,tf.io.decode_raw return tensor how to make it bytes or string,"<p>I'm struggling with this for a while. I searched stack and check tf2
doc a bunch of times. There is one solution indicated, but
I don't understand why my solution doesn't work.</p>
<p>In my case, I store a binary string (i.e., bytes) in tfrecords.
if I iterate over dataset via as_numpy_list or directly call numpy()
on each item, I can get back binary string.
while iterating the dataset, it does work.</p>
<p>I'm not sure what exactly map() passes to test_callback.
I see doesn't have a method nor property numpy, and the same about type
tf.io.decode_raw return. (it is Tensor, but it has no numpy as well)</p>
<p>Essentially I need to take a binary string, parse it via my
x = decoder.FromString(y) and then pass it my encoder
that will transform x binary string to tensor.</p>
<pre><code>def test_callback(example_proto):

    # I tried to figure out. can I use bytes?decode 
    # directly and what is the most optimal solution.

    parsed_features = tf.io.decode_raw(example_proto, out_type=tf.uint8)
    # tf.io.decoder returns tensor with N bytes.

    x = creator.FromString(parsed_features.numpy)
    encoded_seq = midi_encoder.encode(x)
    return encoded_seq

raw_dataset = tf.data.TFRecordDataset(filenames=[&quot;main.tfrecord&quot;])
raw_dataset = raw_dataset.map(test_callback)

</code></pre>
<p>Thank you, folks.</p>
","I'm struggling with this for a while. I searched stack and check tf2 doc a bunch of times. There is one solution indicated, but I don't understand why my solution doesn't work. In my case, I store a binary string (i.e., bytes) in tfrecords. if I iterate over dataset via as_numpy_list or directly call numpy() on each item, I can get back binary string. while iterating the dataset, it does work. I'm not sure what exactly map() passes to test_callback. I see doesn't have a method nor property numpy, and the same about type tf.io.decode_raw return. (it is Tensor, but it has no numpy as well) Essentially I need to take a binary string, parse it via my x = decoder.FromString(y) and then pass it my encoder that will transform x binary string to tensor. Thank you, folks.",https://stackoverflow.com/questions/66685673,9063497,Requesting (Additional) Resources
48077625,Why match_filenames_once function returns a local variable,"<p>I was trying to understand the mechanism of tensorflow for reading images using queues. I was using the code found <a href=""https://gist.github.com/eerwitt/518b0c9564e500b4b50f"" rel=""nofollow noreferrer"">here</a>, whom basic parts are:</p>

<pre><code>filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once('D:/Dataset/*.jpg'))
image_reader = tf.WholeFileReader()
image_name, image_file = image_reader.read(filename_queue)
image = tf.image.decode_jpeg(image_file)

with tf.Session() as sess:
    tf.global_variables_initializer().run()

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    image_tensor = sess.run([image])
    print(image_tensor)
</code></pre>

<p>which in reality does nothing special. I was getting an error:</p>

<blockquote>
  <p>OutOfRangeError (see above for traceback): FIFOQueue
  '_0_input_producer' is closed and has insufficient elements (requested
  1, current size 0)</p>
</blockquote>

<p>which lead to search for missing images, wrong folder, wrong glob pattern etc until I discovered that tensorflow basically meant this:
""You need to initialize local variables also""!</p>

<p>Besides the fact that the code seemed to work in the original <a href=""https://gist.github.com/eerwitt/518b0c9564e500b4b50f"" rel=""nofollow noreferrer"">gist</a> with just this substitution:</p>

<pre><code>tf.initialize_all_variables().run()
</code></pre>

<p>instead of</p>

<pre><code>tf.global_variables_initializer().run()
</code></pre>

<p>in my code it does not work. It produces the same error. I guess it has changed the implementation of <code>initialize_all_variables()</code> with tensorflow development (I am using 1.3.0), since in <a href=""https://stackoverflow.com/questions/40220201/tensorflow-tf-initialize-all-variables-vs-tf-initialize-local-variables"">here</a> it mentions that it initialize local variables also.</p>

<p>So, the final conclusion I came with was that I should initialize local variables also. And my code worked. The error message is awfully misleading (which did not help at all) but anyway to the main part I am a bit confused why am I getting a local variable by <code>match_filenames_once</code>. In <a href=""https://www.tensorflow.org/api_docs/python/tf/train/match_filenames_once"" rel=""nofollow noreferrer"">documentation</a> there is no reference about this (I am not sure it should though). </p>

<p>Am I always going to get local from this <code>match_filenames_once</code>? Can I control it somehow?</p>
","I was trying to understand the mechanism of tensorflow for reading images using queues. I was using the code found here, whom basic parts are: which in reality does nothing special. I was getting an error: which lead to search for missing images, wrong folder, wrong glob pattern etc until I discovered that tensorflow basically meant this: ""You need to initialize local variables also""! Besides the fact that the code seemed to work in the original gist with just this substitution: instead of in my code it does not work. It produces the same error. I guess it has changed the implementation of initialize_all_variables() with tensorflow development (I am using 1.3.0), since in here it mentions that it initialize local variables also. So, the final conclusion I came with was that I should initialize local variables also. And my code worked. The error message is awfully misleading (which did not help at all) but anyway to the main part I am a bit confused why am I getting a local variable by match_filenames_once. In documentation there is no reference about this (I am not sure it should though). Am I always going to get local from this match_filenames_once? Can I control it somehow?",https://stackoverflow.com/questions/48077625,3584765,Documentation Completeness
54728905,Docker Tensorflow-Serving Predictions too large,"<p>I'm trying to serve my model using Docker + tensorflow-serving.  However, due to restrictions with serving a model with an iterator (using<br>
<code>make_initializable_iterator()</code> ), I had to split up my model.  </p>

<p>I'm using grpc to interface with my model on docker.  The problem is that my predicted tensor is about 10MB and about 4.1MB serialized.  The error I'm getting is:</p>

<pre><code>""grpc_message"":""Received message larger than max (9830491 vs. 4194304)""
</code></pre>

<p>Is there a way to write out my predictions to disk instead of transmitting them in the grpc response?  The output file is a 32-channel tensor so I'm unable to decode it as a png before saving to disk using tf.io.write_file.</p>

<p>Thanks!</p>
","I'm trying to serve my model using Docker + tensorflow-serving. However, due to restrictions with serving a model with an iterator (using make_initializable_iterator() ), I had to split up my model. I'm using grpc to interface with my model on docker. The problem is that my predicted tensor is about 10MB and about 4.1MB serialized. The error I'm getting is: Is there a way to write out my predictions to disk instead of transmitting them in the grpc response? The output file is a 32-channel tensor so I'm unable to decode it as a png before saving to disk using tf.io.write_file. Thanks!",https://stackoverflow.com/questions/54728905,6693924,Requesting (Additional) Resources
56678776,Why do we have to import keras from tensorflow if we have already imported tensorflow in python?,"<p>I'm now learning tensorflow and keras and I see all tutorials have these two imports:</p>

<pre><code>import tensorflow as tf
from tensorflow import keras
</code></pre>

<p>Based on my understanding of python import, I thought the second line was an extra, since if we have already imported tensorflow in the first line, then we shall have imported every module in tensorflow. Just like if we have</p>

<pre><code>import math 
</code></pre>

<p>then we shouldd have math.log(), math.sqrt() available.</p>

<p>However, if I comment</p>

<pre><code>from tensorflow import keras
</code></pre>

<p>then this line of code</p>

<pre><code>model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
</code></pre>

<p>would return
NameErrorTraceback (most recent call last)</p>

<pre><code>&lt;ipython-input-3-740ba65f0ade&gt; in &lt;module&gt;()
----&gt; 1 model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])

NameError: name 'keras' is not defined
</code></pre>

<p>Why can't we directly use <strong>tf.keras</strong> if we only have <strong>import tensorflow as tf</strong>? What's special about this import compared to the import in <strong>import math</strong>?</p>

<p>Thanks</p>
","I'm now learning tensorflow and keras and I see all tutorials have these two imports: Based on my understanding of python import, I thought the second line was an extra, since if we have already imported tensorflow in the first line, then we shall have imported every module in tensorflow. Just like if we have then we shouldd have math.log(), math.sqrt() available. However, if I comment then this line of code would return NameErrorTraceback (most recent call last) Why can't we directly use tf.keras if we only have import tensorflow as tf? What's special about this import compared to the import in import math? Thanks",https://stackoverflow.com/questions/56678776,2850815,Documentation Ambiguity
73496717,"In neural networks, activation is applied by a function or layer?","<p>I am using the Functional API of the TensorFlow/Keras for building a CNN model. In this model, I am trying to apply a custom activation (with constraints) on the output layer.</p>
<p>After going through various resources (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/activations"" rel=""nofollow noreferrer"">1</a>, <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/activations"" rel=""nofollow noreferrer"">2</a>), I am confused about whether the activation needs to be applied by a simple python function or layer.</p>
<p>I tried implementing it by subclassing the Layer class as follows,</p>
<pre><code>class MapToBounds(layers.Layer):

    def __init__(self, lower_bound, upper_bound, **kwargs):
        super().__init__(**kwargs)
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound

    def call(self, inputs, *args, **kwargs):
        return tf.add(self.lower_bound, tf.multiply(tf.sigmoid(inputs), self.upper_bound))
    
</code></pre>
<p>and called it in the model as</p>
<pre><code>x = MapToBounds(lower_bound=-3.0, upper_bound=20.0)(x)
</code></pre>
<p>where <code>x</code> is the previous layer instance.</p>
<p>My questions are:</p>
<ol>
<li>Is it the right approach?</li>
<li>In this approach, do I have to set <code>training=False</code>?</li>
<li>Is there any simple way I can implement it with a python function instead of a layer?</li>
</ol>
","I am using the Functional API of the TensorFlow/Keras for building a CNN model. In this model, I am trying to apply a custom activation (with constraints) on the output layer. After going through various resources (1, 2), I am confused about whether the activation needs to be applied by a simple python function or layer. I tried implementing it by subclassing the Layer class as follows, and called it in the model as where x is the previous layer instance. My questions are:",https://stackoverflow.com/questions/73496717,11936209,Requesting (Additional) Resources
74818306,Issues with using Tensorflows preprocessing function for InceptionV3,"<pre class=""lang-py prettyprint-override""><code>#Here my args, they are pretty much the same for all three functions:
training_preprocessing_args = dict(
    labels='inferred',
    label_mode='int',
    class_names=classes,
    color_mode='rgb',
    image_size=hyper_parameter[&quot;image_size&quot;],
    shuffle=True,
    seed=seed,
    validation_split=None,
    subset=None,
    interpolation='bilinear',
    follow_links=False,
    crop_to_aspect_ratio=False
)



    logging.info(&quot;Training Data:&quot;)
    train_dataset:tf.data.Dataset =  tf.keras.utils.image_dataset_from_directory(directory=PATH_DATA_TRAINING, **training_preprocessing_args)

    logging.info(&quot;Testing Data:&quot;)
    test_dataset:tf.data.Dataset =  tf.keras.utils.image_dataset_from_directory(directory=PATH_DATA_TESTING, **testing_preprocessing_args)

    logging.info(&quot;Validation Data:&quot;)
    validation_dataset:tf.data.Dataset =  tf.keras.utils.image_dataset_from_directory(directory=PATH_DATA_VALIDATION, **validation_preprocessing_args)

    logging.info(&quot;Preprocessing:&quot;)
    train_dataset = tf.keras.applications.inception_v3.preprocess_input(tf.cast(train_dataset, tf.float32))
    validation_dataset = tf.keras.applications.inception_v3.preprocess_input(tf.cast(validation_dataset, tf.float32))
    test_dataset = tf.keras.applications.inception_v3.preprocess_input(tf.cast(test_dataset, tf.float32))
</code></pre>
<p>So this is my setup. At first I had it without the cast, then I would get a different error - since the documentation mentions the cast, I will discuss it like that.</p>
<p>Documentation does it as follows (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input</a>):</p>
<pre class=""lang-py prettyprint-override""><code>i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
</code></pre>
<p>which is kinda hard to translate into my real application.</p>
<p>I get the following Error:</p>
<pre><code>15-12-2022 23:21:15 INFO     Training Data:
Found 6988 files belonging to 10 classes.
2022-12-15 23:21:16.075523: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:tensorflow:Converted call: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D761FC0&gt;
    args: (&lt;tf.Tensor 'args_0:0' shape=() dtype=string&gt;,)
    kwargs: {}

15-12-2022 23:21:16 INFO     Converted call: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D761FC0&gt;
    args: (&lt;tf.Tensor 'args_0:0' shape=() dtype=string&gt;,)
    kwargs: {}

INFO:tensorflow:Allowlisted: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D761FC0&gt;: DoNotConvert rule for keras
15-12-2022 23:21:16 INFO     Allowlisted: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D761FC0&gt;: DoNotConvert rule for keras
15-12-2022 23:21:16 INFO     Testing Data:
Found 1699 files belonging to 10 classes.
INFO:tensorflow:Converted call: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D763490&gt;
    args: (&lt;tf.Tensor 'args_0:0' shape=() dtype=string&gt;,)
    kwargs: {}

15-12-2022 23:21:16 INFO     Converted call: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D763490&gt;
    args: (&lt;tf.Tensor 'args_0:0' shape=() dtype=string&gt;,)
    kwargs: {}

INFO:tensorflow:Allowlisted: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D763490&gt;: DoNotConvert rule for keras
15-12-2022 23:21:16 INFO     Allowlisted: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D763490&gt;: DoNotConvert rule for keras
15-12-2022 23:21:16 INFO     Validation Data:
Found 1700 files belonging to 10 classes.
INFO:tensorflow:Converted call: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D761BD0&gt;
    args: (&lt;tf.Tensor 'args_0:0' shape=() dtype=string&gt;,)
    kwargs: {}

15-12-2022 23:21:16 INFO     Converted call: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D761BD0&gt;
    args: (&lt;tf.Tensor 'args_0:0' shape=() dtype=string&gt;,)
    kwargs: {}

INFO:tensorflow:Allowlisted: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D761BD0&gt;: DoNotConvert rule for keras
15-12-2022 23:21:16 INFO     Allowlisted: &lt;function paths_and_labels_to_dataset.&lt;locals&gt;.&lt;lambda&gt; at 0x000002063D761BD0&gt;: DoNotConvert rule for keras
15-12-2022 23:21:16 INFO     Preprocessing:
Traceback (most recent call last):
  File &quot;_CORE\main.py&quot;, line 27, in &lt;module&gt;
    main()
  File &quot;_CORE\main.py&quot;, line 17, in main
    data:tuple = run_preprocessing()
  File &quot;_CORE\preprocessing\run.py&quot;, line 10, in run_preprocessing
    data = create_datasets()
  File &quot;_CORE\preprocessing\CreateDataset.py&quot;, line 23, in create_datasets
    train_dataset = tf.keras.applications.inception_v3.preprocess_input(train_dataset)#tf.cast(train_dataset, tf.float32))
  File &quot;_ENV\_ENV_1\lib\site-packages\keras\applications\inception_v3.py&quot;, line 448, in preprocess_input
    return imagenet_utils.preprocess_input(
  File &quot;_ENV\_ENV_1\lib\site-packages\keras\applications\imagenet_utils.py&quot;, line 123, in preprocess_input
    return _preprocess_symbolic_input(x, data_format=data_format, mode=mode)
  File &quot;_ENV\_ENV_1\lib\site-packages\keras\applications\imagenet_utils.py&quot;, line 271, in _preprocess_symbolic_input
    x /= 127.5
TypeError: unsupported operand type(s) for /=: 'BatchDataset' and 'float'

</code></pre>
<p>Now this is more or less useless, except for the last line. Anyone has a clue what I have done wrong? Do I need to resize anything or something like that? As far as I understood, the preprocessing function would do that for me.</p>
","So this is my setup. At first I had it without the cast, then I would get a different error - since the documentation mentions the cast, I will discuss it like that. Documentation does it as follows (https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input): which is kinda hard to translate into my real application. I get the following Error: Now this is more or less useless, except for the last line. Anyone has a clue what I have done wrong? Do I need to resize anything or something like that? As far as I understood, the preprocessing function would do that for me.",https://stackoverflow.com/questions/74818306,19409269,Documentation Replicability
60472754,Keras LearningRateScheduler callback on batches instead of epochs,"<p>I am using Tensorflow 2.x, The below is the custom learning rate scheduler which i have written</p>

<pre><code>def scheduler(epoch):
  if epoch == 1:
    return 3e-5
  else:
    return 3e-5 * (1/(1 + 0.01 * epoch ))
</code></pre>

<p>and i am calling it like this </p>

<pre><code>callback = tf.keras.callbacks.LearningRateScheduler(scheduler)

model.fit(inputs_train,tags_train,epochs=30,batch_size=32,validation_data=(inputs_val,tags_val),shuffle=False,callbacks=[callback])
</code></pre>

<p>But instead of calling it on epochs, i want to call it on each batch. I couldn't find anything below documentation regarding batches </p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler</a></p>

<p>Is it possible to call it on batches, if yes how to do that?</p>
","I am using Tensorflow 2.x, The below is the custom learning rate scheduler which i have written and i am calling it like this But instead of calling it on epochs, i want to call it on each batch. I couldn't find anything below documentation regarding batches https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler Is it possible to call it on batches, if yes how to do that?",https://stackoverflow.com/questions/60472754,6490241,Inadequate Examples
70368770,How to create checkpoint filenames with epoch or batch number when using ModelCheckpoint() with save_freq as interger?,"<p>I have tensorflow 2 v. 2.5.0 installed and am using jupyter notebooks with python 3.10.</p>
<p>I'm practicing using an argument, save_freq as an integer from an online course (they use tensorflow 2.0.0 where the following code runs fine but it does work in my more recent version).</p>
<p>here's the link to relevant documentation  without an example on using integer in save_freq.
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint</a></p>
<p>here is my code:</p>
<pre><code>    import tensorflow as tf
    from tensorflow.keras.callbacks import ModelCheckpoint
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
    
    # Use the CIFAR-10 dataset
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
    x_train = x_train / 255.0
    x_test = x_test / 255.0
    
    # using a smaller subset -- speeds things up
    x_train = x_train[:10000]
    y_train = y_train[:10000]
    x_test = x_test[:1000]
    y_test = y_test[:1000]
    
    # define a function that creates a new instance of a simple CNN.
    def create_model():
        model = Sequential([
            Conv2D(filters=16, input_shape=(32, 32, 3), kernel_size=(3, 3), 
                   activation='relu', name='conv_1'),
            Conv2D(filters=8, kernel_size=(3, 3), activation='relu', name='conv_2'),
            MaxPooling2D(pool_size=(4, 4), name='pool_1'),
            Flatten(name='flatten'),
            Dense(units=32, activation='relu', name='dense_1'),
            Dense(units=10, activation='softmax', name='dense_2')
        ])
        model.compile(optimizer='adam',
                      loss='sparse_categorical_crossentropy',
                      metrics=['accuracy'])
        return model
    
    
    # Create Tensorflow checkpoint object with epoch and batch details 
    
    checkpoint_5000_path = 'model_checkpoints_5000/cp_{epoch:02d}-{batch:04d}'
    checkpoint_5000 = ModelCheckpoint(filepath = checkpoint_5000_path,
                                     save_weights_only = True,
                                     save_freq = 5000,
                                     verbose = 1)
    
    
    # Create and fit model with checkpoint
    
    model = create_model()
    model.fit(x = x_train,
              y = y_train,
              epochs = 3,
              validation_data = (x_test, y_test),
              batch_size = 10,
              callbacks = [checkpoint_5000])
</code></pre>
<p>I want to create and save the checkpoint filenames including the epoch and batch number.
However, the files are not created and it writes 'File not found'. After I create manually the directory, model_checkpoints_5000, no files are added in.</p>
<p>(we can check the directory contents by running ' ! dir -a model_checkpoints_5000' (in windows), or  'ls -lh model_checkpoints_500' (in linux)).</p>
<p>I have also tried to change to 'model_checkpoints_5000/cp_{epoch:02d}', it still does not save the files with every epoch's number.</p>
<p>Then I have tried to follow the example from Checkpoint Callback options with save_freq, which saves files with me.
<a href=""https://www.tensorflow.org/tutorials/keras/save_and_load"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/keras/save_and_load</a></p>
<p>yet, it is still not saving any of my files.</p>
<pre><code>checkpoint_path = &quot;model_checkpoints_5000/cp-{epoch:02d}.ckpt&quot;
checkpoint_dir = os.path.dirname(checkpoint_path)

batch_size = 10

checkpoint_5000 = ModelCheckpoint(filepath = checkpoint_path,
                                 save_weights_only = True,
                                 save_freq = 500*batch_size,


model = create_model()

model.fit(x = x_train,
          y = y_train,
          epochs = 3,
          validation_data = (x_test, y_test),
          batch_size = batch_size,
          callbacks = [checkpoint_5000])                                verbose = 1)
</code></pre>
<p>any suggestions how to make it work? other than downgrading my tensorflow.</p>
","I have tensorflow 2 v. 2.5.0 installed and am using jupyter notebooks with python 3.10. I'm practicing using an argument, save_freq as an integer from an online course (they use tensorflow 2.0.0 where the following code runs fine but it does work in my more recent version). here's the link to relevant documentation without an example on using integer in save_freq. https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint here is my code: I want to create and save the checkpoint filenames including the epoch and batch number. However, the files are not created and it writes 'File not found'. After I create manually the directory, model_checkpoints_5000, no files are added in. (we can check the directory contents by running ' ! dir -a model_checkpoints_5000' (in windows), or 'ls -lh model_checkpoints_500' (in linux)). I have also tried to change to 'model_checkpoints_5000/cp_{epoch:02d}', it still does not save the files with every epoch's number. Then I have tried to follow the example from Checkpoint Callback options with save_freq, which saves files with me. https://www.tensorflow.org/tutorials/keras/save_and_load yet, it is still not saving any of my files. any suggestions how to make it work? other than downgrading my tensorflow.",https://stackoverflow.com/questions/70368770,11028689,Inadequate Examples
74297504,Understanding Keras Constraints,"<p>I have a question about tf.keras.constraints method.</p>
<pre><code>(1)

class WeightsSumOne(tf.keras.constraints.Constraint):
      def __call__(self, w):
          return tf.nn.softmax(w, axis=0)

output = layers.Dense(1, use_bias=False, 
                      kernel_constraint = WeightsSumOne())(input)
                                       


(2)

intermediate = layers.Dense(1, use_bias = False)
intermediate.set_weights(tf.nn.softmax(intermediate.get_weights(), axis=0))
</code></pre>
<p>Do (1) and (2) perform the same process?</p>
<p>The reason why I ask the question is that Keras Documentation said that</p>
<blockquote>
<p>They are per-variable projection functions applied to the target
variable <strong>after each gradient update</strong> (when using fit()).
(<a href=""https://keras.io/api/layers/constraints/"" rel=""nofollow noreferrer"">https://keras.io/api/layers/constraints/</a>)</p>
</blockquote>
<p>Unlike (1), I think that the constraint is applied before each gradient update in case of (2).</p>
<p>In my opinion, the gradients of weights of (1) and (2) are different, because the softmax is applied before the gradient calculation in the second case, but after the gradient calculation in the first case.</p>
<p>If I am wrong, I would appreciate it if you point out the wrong part.</p>
","I have a question about tf.keras.constraints method. Do (1) and (2) perform the same process? The reason why I ask the question is that Keras Documentation said that Unlike (1), I think that the constraint is applied before each gradient update in case of (2). In my opinion, the gradients of weights of (1) and (2) are different, because the softmax is applied before the gradient calculation in the second case, but after the gradient calculation in the first case. If I am wrong, I would appreciate it if you point out the wrong part.",https://stackoverflow.com/questions/74297504,7229276,Documentation Ambiguity
67754649,Mean of Tensorflow Keras's Glorot Normal Initializer is not zero,"<p>As per the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal"" rel=""nofollow noreferrer"">Glorot Normal</a>, <strong><code>mean</code></strong> of the <strong><code>Normal Distribution</code></strong> of the <strong><code>Initial Weights</code></strong> should be <strong><code>zero</code></strong>.</p>
<blockquote>
<p>Draws samples from a truncated normal distribution centered on 0</p>
</blockquote>
<p>But it doesn't seem to be <strong><code>zero</code></strong>, am I missing something?</p>
<p>Please find the code below:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np

print(tf.__version__)

initializer = tf.keras.initializers.GlorotNormal(seed = 1234)


model = Sequential([Dense(units = 3, input_shape = [1], kernel_initializer = initializer,
                         bias_initializer = initializer),
                   Dense(units = 1, kernel_initializer = initializer,
                         bias_initializer = initializer)])

batch_size = 1

x = np.array([-1.0, 0, 1, 2, 3, 4.0], dtype = 'float32')
y = np.array([-3, -1.0, 1, 3.0, 5.0, 7.0], dtype = 'float32')

x = np.reshape(x, (-1, 1))

# Prepare the training dataset.
train_dataset = tf.data.Dataset.from_tensor_slices((x, y))
train_dataset = train_dataset.shuffle(buffer_size=64).batch(batch_size)

epochs = 1
learning_rate=1e-3

# Instantiate an optimizer.
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)

for epoch in range(epochs):

    # Iterate over the batches of the dataset.
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        
        with tf.GradientTape() as tape:
            logits = model(x_batch_train, training=True)  # Logits for this minibatch

            # Compute the loss value for this minibatch.
            loss_value = tf.keras.losses.MSE(y_batch_train, logits)               
        
       
        Initial_Weights_1st_Hidden_Layer = model.trainable_weights[0]
       
        Mean_Weights_Hidden_Layer = tf.reduce_mean(Initial_Weights_1st_Hidden_Layer)
                          
        Initial_Weights_Output_Layer = model.trainable_weights[2]
        
        Mean_Weights_Output_Layer = tf.reduce_mean(Initial_Weights_Output_Layer)  
               
        Initial_Bias_1st_Hidden_Layer = model.trainable_weights[1]
        
        Mean_Bias_Hidden_Layer = tf.reduce_mean(Initial_Bias_1st_Hidden_Layer)      
        
        Initial_Bias_Output_Layer = model.trainable_weights[3]
        
        Mean_Bias_Output_Layer = tf.reduce_mean(Initial_Bias_Output_Layer)
        
        if epoch ==0 and step==0:
            
            print('\n Initial Weights of First-Hidden Layer = ', Initial_Weights_1st_Hidden_Layer)
            print('\n Mean of Weights of Hidden Layer = %s' %Mean_Weights_Hidden_Layer.numpy())
            
            print('\n Initial Weights of Second-Hidden/Output Layer = ', Initial_Weights_Output_Layer)
            print('\n Mean of Weights of Output Layer = %s' %Mean_Weights_Output_Layer.numpy())
                
            print('\n Initial Bias of First-Hidden Layer = ', Initial_Bias_1st_Hidden_Layer)
            print('\n Mean of Bias of Hidden Layer = %s' %Mean_Bias_Hidden_Layer.numpy())

            print('\n Initial Bias of Second-Hidden/Output Layer = ', Initial_Bias_Output_Layer)
            print('\n Mean of Bias of Output Layer = %s' %Mean_Bias_Output_Layer.numpy())
</code></pre>
","As per the documentation of Glorot Normal, mean of the Normal Distribution of the Initial Weights should be zero. But it doesn't seem to be zero, am I missing something? Please find the code below:",https://stackoverflow.com/questions/67754649,10016590,Documentation Replicability
64747663,How to apply Attention layer to LSTM model,"<p>I am doing a speech emotion recognition machine training.</p>
<p>I wish to apply an attention layer to the model. The instruction <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention"" rel=""nofollow noreferrer"">page</a> is hard to understand.</p>
<pre><code>def bi_duo_LSTM_model(X_train, y_train, X_test,y_test,num_classes,batch_size=68,units=128, learning_rate=0.005, epochs=20, dropout=0.2, recurrent_dropout=0.2):
    
    class myCallback(tf.keras.callbacks.Callback):

        def on_epoch_end(self, epoch, logs={}):
            if (logs.get('acc') &gt; 0.95):
                print(&quot;\nReached 99% accuracy so cancelling training!&quot;)
                self.model.stop_training = True

    callbacks = myCallback()

    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Masking(mask_value=0.0, input_shape=(X_train.shape[1], X_train.shape[2])))
    model.add(tf.keras.layers.Bidirectional(LSTM(units, dropout=dropout, recurrent_dropout=recurrent_dropout,return_sequences=True)))
    model.add(tf.keras.layers.Bidirectional(LSTM(units, dropout=dropout, recurrent_dropout=recurrent_dropout)))
    #     model.add(tf.keras.layers.Bidirectional(LSTM(32)))
    model.add(Dense(num_classes, activation='softmax'))

    adamopt = tf.keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-8)
    RMSopt = tf.keras.optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-6)
    SGDopt = tf.keras.optimizers.SGD(lr=learning_rate, momentum=0.9, decay=0.1, nesterov=False)

    model.compile(loss='binary_crossentropy',
                  optimizer=adamopt,
                  metrics=['accuracy'])

    history = model.fit(X_train, y_train,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=(X_test, y_test),
                        verbose=1,
                        callbacks=[callbacks])

    score, acc = model.evaluate(X_test, y_test,
                                batch_size=batch_size)

    yhat = model.predict(X_test)

    return history, yhat

</code></pre>
<p>How can I apply it to fit for my model?</p>
<p>And are <code>use_scale</code>, <code>causal</code> and <code>dropout</code> all the arguments?</p>
<p>If there is a <code>dropout</code> in <code>attention layer</code>, how do we deal with it since we have <code>dropout</code> in LSTM layer?</p>
","I am doing a speech emotion recognition machine training. I wish to apply an attention layer to the model. The instruction page is hard to understand. How can I apply it to fit for my model? And are use_scale, causal and dropout all the arguments? If there is a dropout in attention layer, how do we deal with it since we have dropout in LSTM layer?",https://stackoverflow.com/questions/64747663,14128879,Documentation Ambiguity
64203611,tf.keras.layers.BatchNormalization with trainable=False appears to not update its internal moving mean and variance,"<p>I am trying to find out, how exactly does BatchNormalization layer behave in TensorFlow. I came up with the following piece of code which to the best of my knowledge should be a perfectly valid keras model, however the mean and variance of BatchNormalization doesn't appear to be updated.</p>
<p>From docs <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</a></p>
<blockquote>
<p>in the case of the BatchNormalization layer, setting trainable = False on the layer means that the layer will be subsequently run in inference mode (meaning that it will use the moving mean and the moving variance to normalize the current batch, rather than using the mean and variance of the current batch).</p>
</blockquote>
<p>I expect the model to return a different value with each subsequent predict call.
What I see, however, are the exact same values returned 10 times.
Can anyone explain to me why does the BatchNormalization layer not update its internal values?</p>
<pre><code>import tensorflow as tf
import numpy as np

if __name__ == '__main__':

    np.random.seed(1)
    x = np.random.randn(3, 5) * 5 + 0.3

    bn = tf.keras.layers.BatchNormalization(trainable=False, epsilon=1e-9)
    z = input = tf.keras.layers.Input([5])
    z = bn(z)

    model = tf.keras.Model(inputs=input, outputs=z)

    for i in range(10):
        print(x)
        print(model.predict(x))
        print()
</code></pre>
<p>I use <strong>TensorFlow 2.1.0</strong></p>
","I am trying to find out, how exactly does BatchNormalization layer behave in TensorFlow. I came up with the following piece of code which to the best of my knowledge should be a perfectly valid keras model, however the mean and variance of BatchNormalization doesn't appear to be updated. From docs https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization I expect the model to return a different value with each subsequent predict call. What I see, however, are the exact same values returned 10 times. Can anyone explain to me why does the BatchNormalization layer not update its internal values? I use TensorFlow 2.1.0",https://stackoverflow.com/questions/64203611,3399825,Documentation Ambiguity
39564964,How to correctly create a batch normalization layer for a convolutional layer in TensorFlow?,"<p>I was looking at the <a href=""https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100"" rel=""nofollow"">official batch normalization layer</a> (BN) in TensorFlow however it didn't really explain how to use it for a convolutional layer. Does someone know how to do this? In particular its important that it applies and learns <strong>the same parameters per feature map</strong> (rather than per activation). In other order that it applies and learn BN per filter.</p>

<p>In a specific toy example say that I want to do conv2d with BN on MNIST (2D data essentially). Thus one could do:</p>

<pre><code>W_conv1 = weight_variable([5, 5, 1, 32]) # 5x5 filters with 32 filters
x_image = tf.reshape(x, [-1,28,28,1]) # MNIST image
conv = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='VALID') #[?,24,24,1]
z = conv # [?,24,24,32]
z = BN(z) # [?,24,24,32], essentially only 32 different scales and shift parameters to learn, per filer application
a = tf.nn.relu(z) # [?,24,24,32]
</code></pre>

<p>Where <code>z = BN(z)</code> applies the BN to each feature created by each individual filter. In pseudocode:</p>

<pre><code>x_patch = x[h:h+5,w:w+h,1] # patch to do convolution
z[h,w,f] = x_patch * W[:,:,f] = tf.matmul(x_patch, W[:,:,f]) # actual matrix multiplication for the convolution
</code></pre>

<p>we have a proper batch norm layer applied to it (in pseudocode omitting important details):</p>

<pre><code>z[h,w,f] = BN(z[h,w,f]) = scale[f] * (z[h,w,f]  - mu / sigma) + shift[f]
</code></pre>

<p>i.e. for each filter <code>f</code> we apply BN. </p>
",I was looking at the official batch normalization layer (BN) in TensorFlow however it didn't really explain how to use it for a convolutional layer. Does someone know how to do this? In particular its important that it applies and learns the same parameters per feature map (rather than per activation). In other order that it applies and learn BN per filter. In a specific toy example say that I want to do conv2d with BN on MNIST (2D data essentially). Thus one could do: Where z = BN(z) applies the BN to each feature created by each individual filter. In pseudocode: we have a proper batch norm layer applied to it (in pseudocode omitting important details): i.e. for each filter f we apply BN.,https://stackoverflow.com/questions/39564964,1601580,Documentation Completeness
50164572,BatchNormalization in Keras,"<p>How do I update moving mean and moving variance in keras BatchNormalization?</p>

<p>I found this in tensorflow documentation, but I don't know where to put <code>train_op</code> or how to work it with keras models:</p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
        with tf.control_dependencies(update_ops):
            train_op = optimizer.minimize( loss )
</code></pre>

<p>No posts I found say what to do with train_op and whether you can use it in <code>model.compile</code>.</p>
","How do I update moving mean and moving variance in keras BatchNormalization? I found this in tensorflow documentation, but I don't know where to put train_op or how to work it with keras models: No posts I found say what to do with train_op and whether you can use it in model.compile.",https://stackoverflow.com/questions/50164572,8748308,Documentation Ambiguity
62801832,Rewrite tf.contrib.layers.batch_norm in Tensorflow 2.0,"<p>Could somebody help me rewrite the following block of code in Tf2.0?<br />
I'm aware batch_norm is equivalent to keras.layers.BatchNormalization but the documentation doesn't give clear solution as to what 'decay' and 'epsilon' correspond to. Thanks!</p>
<pre><code>def batch_norm(opts, _input, is_train, reuse, scope, scale=True):
    &quot;&quot;&quot;Batch normalization based on tf.contrib.layers.

    &quot;&quot;&quot;
    return tf.contrib.layers.batch_norm(
        _input, center=True, scale=scale,
        epsilon=opts['batch_norm_eps'], decay=opts['batch_norm_decay'],
        is_training=is_train, reuse=True, updates_collections=None,
        scope=scope, fused=False)
</code></pre>
",Could somebody help me rewrite the following block of code in Tf2.0? I'm aware batch_norm is equivalent to keras.layers.BatchNormalization but the documentation doesn't give clear solution as to what 'decay' and 'epsilon' correspond to. Thanks!,https://stackoverflow.com/questions/62801832,13636839,Documentation Completeness
71114690,Tensorflow keras BatchNormalization for higher than 4-dimension Tensor (video input),"<p>I'm trying to implement S3D[<a href=""https://arxiv.org/pdf/1712.04851.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1712.04851.pdf</a>] for video classification and I encountered a problem with BatchNormalization.</p>
<p>Since the implementation that I'm dealing with is video classification, I need an additional temporal dimension for my input tensor. (i.e. [Batch, Time, Height, Width, Channel])</p>
<p>Here's my error situation.</p>
<pre><code>example = np.random.randint(0,255, (16,16,56,56,3)) 
example_tensor = tf.convert_to_tensor(example, dtype=tf.float32)

print(tf.keras.layers.BatchNormalization(axis=0)(example_tensor))
#print(tf.keras.layers.BatchNormalization(axis=1)(example_tensor)) # This gives error
print(tf.keras.layers.BatchNormalization(axis=2)(example_tensor))
print(tf.keras.layers.BatchNormalization(axis=3)(example_tensor))
#print(tf.keras.layers.BatchNormalization(axis=-1)(example_tensor)) # This gives error
</code></pre>
<p>And the error message is like this.</p>
<pre><code>InvalidArgumentError: Exception encountered when calling layer &quot;batch_normalization_56&quot; (type BatchNormalization).

input must be 4-dimensional[16,16,56,56,3] [Op:FusedBatchNormV3]

Call arguments received:
  • inputs=tf.Tensor(shape=(16, 16, 56, 56, 3), dtype=float32)
  • training=None
</code></pre>
<p>I read about the meaning of axis in BatchNormalization from this stackoverflow question <a href=""https://stackoverflow.com/questions/47538391/keras-batchnormalization-axis-clarification"">here</a> But I still don't understand why my BatchNormalization code gives an error depending on what axis I give as an argument.</p>
<p>Also, I've searched a lot of questions and read tensorflow BatchNormalization document. <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"" rel=""nofollow noreferrer"">[link]</a></p>
<p>I think this error message is telling me that it's expecting 4-dimensional input like we usually do for image processing([Batch, Height, Width, Channel])</p>
<p>Can anyone know what is happening here? and how to use BatchNormalization for a 5-dimension Tensor?</p>
","I'm trying to implement S3D[https://arxiv.org/pdf/1712.04851.pdf] for video classification and I encountered a problem with BatchNormalization. Since the implementation that I'm dealing with is video classification, I need an additional temporal dimension for my input tensor. (i.e. [Batch, Time, Height, Width, Channel]) Here's my error situation. And the error message is like this. I read about the meaning of axis in BatchNormalization from this stackoverflow question here But I still don't understand why my BatchNormalization code gives an error depending on what axis I give as an argument. Also, I've searched a lot of questions and read tensorflow BatchNormalization document. [link] I think this error message is telling me that it's expecting 4-dimensional input like we usually do for image processing([Batch, Height, Width, Channel]) Can anyone know what is happening here? and how to use BatchNormalization for a 5-dimension Tensor?",https://stackoverflow.com/questions/71114690,13102945,Requesting (Additional) Resources
50312519,"Dimensions must be equal, but are 1 and 2 for 'Conv2D' (op: 'Conv2D') with input shapes: [2,2,2,1], [1,1,2,1]","<p>I am trying to learn Conv2d by this code.</p>

<p>Based on conv2d doc:</p>

<p>shape of input = [batch, in_height, in_width, in_channels]</p>

<p>shape of filter = [filter_height, filter_width, in_channels, out_channels]</p>

<p>When I try to run it , it sends me the wrong message.</p>

<pre><code>#-*-coding:utf-8-*-

import tensorflow as tf

input_batch = tf.constant([
    [
        [[0.0], [1.0]],
        [[2.0], [3.0]]
    ],
    [
        [[2.0], [4.0]],
        [[6.0], [8.0]]
    ]
])

kernel = tf.constant([
    [
        [[1.0], [2.0]],
    ]
])

conv2d = tf.nn.conv2d(input_batch, kernel, strides=[1, 1, 1, 1],padding='SAME')

with tf.Session() as sess:
    sess.run(conv2d)
</code></pre>

<p>The wrong message:</p>

<pre><code>ValueError: Dimensions must be equal, but are 1 and 2 for 'Conv2D' (op: 'Conv2D') with input shapes: [2,2,2,1], [1,1,2,1].
</code></pre>
","I am trying to learn Conv2d by this code. Based on conv2d doc: shape of input = [batch, in_height, in_width, in_channels] shape of filter = [filter_height, filter_width, in_channels, out_channels] When I try to run it , it sends me the wrong message. The wrong message:",https://stackoverflow.com/questions/50312519,8874295,Documentation Replicability
73796471,using tf.keras.layers.Embedding for categorical variables in regression problem,"<p>Using the iris dataset as a hypothetical hello world example:</p>
<pre><code>import pandas as pd
from sklearn import datasets

iris = datasets.load_iris()
df = pd.DataFrame(iris['data'], columns = iris['feature_names'])
df['iris_class'] = pd.Series(iris['target'], name = 'target_values')
df['iris_class_name'] = df['iris_class'].replace([0,1,2], ['iris-' + species for species in iris['target_names'].tolist()])
df.columns = df.columns.str.replace(&quot;[() ]&quot;, &quot;&quot;)

print(df.head())
</code></pre>
<p>Let us say I want to use tf.keras.layers.Embedding instead of one-hot/dummy encoding as part of ANN for regression. e.g.:</p>
<p><strong>iris_class_name + sepalwidthcm + petallengthcm -&gt; sepallengthcm</strong></p>
<p>where sepallengthcm is the dependent variable. I came across <a href=""https://discuss.tensorflow.org/t/how-to-embed-categorical-features-which-are-strings-in-tensorflow/9381"" rel=""nofollow noreferrer"">this</a>:</p>
<pre><code>city_lookup = tf.keras.layers.StringLookup(vocabulary = city_vocabulary, mask_token = None);
city_embedding= tf.keras.Sequential([
    city_lookup,
    tf.keras.layers.Embedding(len(city_vocabulary) + 1, embedding_dimension)
  ], &quot;city_embedding&quot;)
  
city = features[&quot;city&quot;]
city_embedding_output = city_embedding(city)
</code></pre>
<p>but am not sure how to exactly use it in my use case. Any pointers very much welcome. Thanks!</p>
",Using the iris dataset as a hypothetical hello world example: Let us say I want to use tf.keras.layers.Embedding instead of one-hot/dummy encoding as part of ANN for regression. e.g.: iris_class_name + sepalwidthcm + petallengthcm -&gt; sepallengthcm where sepallengthcm is the dependent variable. I came across this: but am not sure how to exactly use it in my use case. Any pointers very much welcome. Thanks!,https://stackoverflow.com/questions/73796471,283538,Requesting (Additional) Resources
71919267,Trying to achieve same result with Pytorch and Tensorflow MultiheadAttention,"<p>I'm trying to recreate a transformer written in Pytorch and implement it in Tensorflow. The problem is that despite both the documentation for the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"" rel=""nofollow noreferrer"">Pytorch</a> version and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention"" rel=""nofollow noreferrer"">Tensorflow</a> version, they still come out pretty differently.
I wrote a little code snippet to show the issue:</p>
<pre><code>import torch
import tensorflow as tf
import numpy as np

class TransformerLayer(tf.Module):
    def __init__(self, d_model, nhead, dropout=0):
        super(TransformerLayer, self).__init__()
        self.self_attn = torch.nn.MultiheadAttention(d_model, nhead, dropout=dropout)

batch_size = 2
seq_length = 5
d_model = 10

src = np.random.uniform(size=(batch_size, seq_length, d_model))
srcTF = tf.convert_to_tensor(src)
srcPT = torch.Tensor(src.reshape((seq_length, batch_size, d_model)))

self_attnTF = tf.keras.layers.MultiHeadAttention(key_dim=10, num_heads=5, dropout=0)
transformer_encoder = TransformerLayer(d_model=10, nhead=5, dropout=0.0)

output, scores = self_attnTF(srcTF, srcTF, srcTF, return_attention_scores=True)
print(&quot;Tensorflow Attendtion outputs:&quot;, output)
print(&quot;Tensorflow (averaged) weights:&quot;, tf.math.reduce_mean(scores, 1))
print(&quot;Torch Attendtion outputs:&quot;, transformer_encoder.self_attn(srcPT,srcPT,srcPT)[0])
print(&quot;Torch attention output weights:&quot;, transformer_encoder.self_attn(srcPT,srcPT,srcPT)[1])
</code></pre>
<p>and the result is:</p>
<pre><code>Tensorflow Attendtion outputs: tf.Tensor(
[[[ 0.02602757 -0.14134401  0.00855263  0.4735083  -0.01851891
   -0.20382246 -0.18152176 -0.21076852  0.08623976 -0.33548725]
  [ 0.02607442 -0.1403394   0.00814065  0.47415024 -0.01882939
   -0.20353754 -0.18291879 -0.21234266  0.08595885 -0.33613583]
  [ 0.02524654 -0.14096384  0.00870436  0.47411725 -0.01800703
   -0.20486829 -0.18163288 -0.21082559  0.08571021 -0.3362339 ]
  [ 0.02518575 -0.14039244  0.0090138   0.47431853 -0.01775141
   -0.20391947 -0.18138805 -0.2118245   0.08432849 -0.33521986]
  [ 0.02556361 -0.14039293  0.00876258  0.4746476  -0.01891363
   -0.20398234 -0.18229616 -0.21147579  0.08555281 -0.33639923]]

 [[ 0.07844199 -0.1614371   0.01649148  0.5287745   0.05126739
   -0.13851154 -0.09829871 -0.1621251   0.01922669 -0.2428589 ]
  [ 0.07844222 -0.16024739  0.01805423  0.52941847  0.04975721
   -0.13537636 -0.09829231 -0.16129729  0.01979005 -0.24491176]
  [ 0.07800542 -0.160701    0.01677295  0.52902794  0.05082911
   -0.13843337 -0.09805533 -0.16165744  0.01928401 -0.24327613]
  [ 0.07815789 -0.1600025   0.01757433  0.5291927   0.05032986
   -0.1368022  -0.09849522 -0.16172451  0.01929555 -0.24438493]
  [ 0.0781548  -0.16028519  0.01764914  0.52846324  0.04941286
   -0.13746066 -0.09787872 -0.16141161  0.01994199 -0.2440269 ]]], shape=(2, 5, 10), dtype=float32)
Tensorflow (averaged) weights: tf.Tensor(
[[[0.199085   0.20275716 0.20086522 0.19873264 0.19856   ]
  [0.2015336  0.19960018 0.20218948 0.19891861 0.19775811]
  [0.19906266 0.20318432 0.20190334 0.19812575 0.19772394]
  [0.20074987 0.20104568 0.20269363 0.19744729 0.19806348]
  [0.19953248 0.20176074 0.20314851 0.19782843 0.19772986]]

 [[0.2010009  0.20053487 0.20004745 0.20092985 0.19748697]
  [0.20034568 0.20035927 0.19955876 0.20062163 0.19911464]
  [0.19967113 0.2006859  0.20012529 0.20047483 0.19904283]
  [0.20132652 0.19996871 0.20019794 0.20008174 0.19842513]
  [0.2006393  0.20000939 0.19938737 0.20054278 0.19942114]]], shape=(2, 5, 5), dtype=float32)
Torch Attendtion outputs: tensor([[[ 0.1097, -0.4467, -0.0719, -0.1779, -0.0766, -0.1247,  0.1557,
           0.0051, -0.3932, -0.1323],
         [ 0.1264, -0.3822,  0.0759, -0.0335, -0.1084, -0.1539,  0.1475,
          -0.0272, -0.4235, -0.1744]],

        [[ 0.1122, -0.4502, -0.0747, -0.1796, -0.0756, -0.1271,  0.1581,
           0.0049, -0.3964, -0.1340],
         [ 0.1274, -0.3823,  0.0754, -0.0356, -0.1091, -0.1547,  0.1477,
          -0.0272, -0.4252, -0.1752]],

        [[ 0.1089, -0.4427, -0.0728, -0.1746, -0.0756, -0.1202,  0.1501,
           0.0031, -0.3894, -0.1242],
         [ 0.1263, -0.3820,  0.0718, -0.0374, -0.1063, -0.1562,  0.1485,
          -0.0271, -0.4233, -0.1761]],

        [[ 0.1061, -0.4369, -0.0685, -0.1696, -0.0772, -0.1173,  0.1454,
           0.0012, -0.3860, -0.1201],
         [ 0.1265, -0.3820,  0.0762, -0.0325, -0.1082, -0.1560,  0.1501,
          -0.0271, -0.4249, -0.1779]],

        [[ 0.1043, -0.4402, -0.0705, -0.1719, -0.0791, -0.1205,  0.1508,
           0.0018, -0.3895, -0.1262],
         [ 0.1260, -0.3805,  0.0775, -0.0298, -0.1083, -0.1547,  0.1494,
          -0.0276, -0.4242, -0.1768]]], grad_fn=&lt;AddBackward0&gt;)
Torch attention output weights: tensor([[[0.2082, 0.2054, 0.1877, 0.1956, 0.2031],
         [0.2100, 0.2079, 0.1841, 0.1943, 0.2037],
         [0.2007, 0.1995, 0.1929, 0.1999, 0.2070],
         [0.1995, 0.1950, 0.1976, 0.2002, 0.2077],
         [0.1989, 0.1969, 0.1970, 0.2024, 0.2048]],

        [[0.2095, 0.1902, 0.1987, 0.2027, 0.1989],
         [0.2090, 0.1956, 0.1997, 0.2004, 0.1952],
         [0.2047, 0.1869, 0.2006, 0.2121, 0.1957],
         [0.2073, 0.1953, 0.1982, 0.2014, 0.1978],
         [0.2089, 0.2003, 0.1953, 0.1957, 0.1998]]], grad_fn=&lt;DivBackward0&gt;)
</code></pre>
<p>The output weights look similar but the base attention outputs are way off. Is there any way to make the Tensorflow model come out more like the Pytorch one? Any help would be greatly appreciated!</p>
","I'm trying to recreate a transformer written in Pytorch and implement it in Tensorflow. The problem is that despite both the documentation for the Pytorch version and Tensorflow version, they still come out pretty differently. I wrote a little code snippet to show the issue: and the result is: The output weights look similar but the base attention outputs are way off. Is there any way to make the Tensorflow model come out more like the Pytorch one? Any help would be greatly appreciated!",https://stackoverflow.com/questions/71919267,9404761,Documentation Replication on Other Examples
66878893,Tensorflow TextVectorization layer: How to define a custom standardize function?,"<p>I try to create a custom standardize function for the <a href=""https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization"" rel=""nofollow noreferrer"">TextVectorization layer in Tensorflow 2.1</a> but I seem to get something fundamentally wrong.</p>
<p>I have the following text data:</p>
<pre><code>import numpy as np

my_array = np.array([
    &quot;I am a sentence.&quot;,
    &quot;I am another sentence!&quot;
])
</code></pre>
<h2>My Goal</h2>
<p>I basically want to lower the text, remove punctuation and remove some words.
The default standardize-Function of the TextVectorization layer (<code>LOWER_AND_STRIP_PUNCTUATION</code>) lowers and removes punctuation, but afaik there is not way to remove whole words.</p>
<p>(<strong>If you know a way to do so, an alternative approach to mine as described below is of course also very much appreciated</strong>)</p>
<hr />
<h2>An example that is working</h2>
<p>First, find an example of a <strong>working</strong> custom standardiazion function <a href=""https://www.tensorflow.org/tutorials/text/word_embeddings"" rel=""nofollow noreferrer"">from the tensorflow documentation</a></p>
<pre><code>def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '&lt;br /&gt;', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation), '')
</code></pre>
<p>when I pass it to the <code>TextVectorization</code> and adapt in on <code>my_array</code>, it works just fine</p>
<pre><code>vectorize_layer_1 = TextVectorization(
    output_mode='int',
    standardize=custom_standardization,
    )

vectorize_layer_1.adapt(my_array)  # no error
</code></pre>
<hr />
<h2>The custom function that is not working</h2>
<p>However, my custom standardization keeps raising an error. Here is my code:</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.preprocessing.text import text_to_word_sequence

my_array = np.array([
    &quot;I am a sentence&quot;,
    &quot;I am another sentence&quot;
])

# these words should be removed
bad_words = [&quot;i&quot;, &quot;am&quot;]

def remove_words(tokens):
    return [word for word in tokens if word not in bad_words]

# this is the normalization function I want to apply
def my_custom_normalize(my_array):
    tokenized = [text_to_word_sequence(str(sentence)) for sentence in my_array]
    clean_texts = [&quot; &quot;.join(remove_words(tokenized_string))
                     for tokenized_string
                     in tokenized]
    clean_tensor = tf.convert_to_tensor(clean_texts)
    return clean_tensor
    
my_vectorize_layer = TextVectorization(
    output_mode='int',
    standardize=my_custom_normalize,
    )
</code></pre>
<p>However, once I try adapting, I keep running in an error:</p>
<pre><code>my_vectorize_layer.adapt(my_array)  # raises error
</code></pre>
<pre><code>InvalidArgumentError: Tried to squeeze dim index 1 for tensor with 1 dimensions. [Op:Squeeze]
</code></pre>
<p>And I really do not understand why. In the <a href=""https://keras.io/api/layers/preprocessing_layers/core_preprocessing_layers/text_vectorization/"" rel=""nofollow noreferrer"">documentation</a> it says:</p>
<blockquote>
<p>When using a custom callable for standardize, the data received by the callable will be exactly as passed to this layer. The callable should return a tensor of the same shape as the input</p>
</blockquote>
<p>I thought maybe thats what is causing the error.
but when I look at the shapes, everything seems correct:</p>
<pre><code>my_result = my_custom_normalize(my_array)
my_result.shape  # returns TensorShape([2])
working_result = custom_standardization(my_array)
working_result.shape # returns TensorShape([2])
</code></pre>
<p>I am really lost here. What am I doing wrong? am I not supposed to use list comprehensions?</p>
","I try to create a custom standardize function for the TextVectorization layer in Tensorflow 2.1 but I seem to get something fundamentally wrong. I have the following text data: I basically want to lower the text, remove punctuation and remove some words. The default standardize-Function of the TextVectorization layer (LOWER_AND_STRIP_PUNCTUATION) lowers and removes punctuation, but afaik there is not way to remove whole words. (If you know a way to do so, an alternative approach to mine as described below is of course also very much appreciated) First, find an example of a working custom standardiazion function from the tensorflow documentation when I pass it to the TextVectorization and adapt in on my_array, it works just fine However, my custom standardization keeps raising an error. Here is my code: However, once I try adapting, I keep running in an error: And I really do not understand why. In the documentation it says: I thought maybe thats what is causing the error. but when I look at the shapes, everything seems correct: I am really lost here. What am I doing wrong? am I not supposed to use list comprehensions?",https://stackoverflow.com/questions/66878893,10465165,Documentation Replication on Other Examples
67600983,How can I prevent TextVectorization in Tensorflow creating values for Unknown and blank strings?,"<p>I am looking to one hot encode string tensor as part of my dataset pipeline. It seems to me this can be achieved using <code>TextVectorization</code> to get an integer representation of the string tensor and then <code>one_hot</code> to convert to achieve the encoded 2d tensor.</p>
<p>When i use TextVectorization it seems to automatically try to map &quot;&quot; to 0 and strings out of vocabulary strings to 1. See below using Tensorflow 2.3:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers

possible_values = [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]
text_vectorization = layers.experimental.preprocessing.TextVectorization(output_sequence_length=1)
text_vectorization.set_vocabulary(possible_values)

print(text_vectorization.get_vocabulary())

['', '[UNK]', 'a', 'b', 'c']
</code></pre>
<p>I can see why it would be useful as it would handle the last 2 values in the below tensor without throwing an error and creating a feature for them in the process.</p>
<pre><code>test_tensor = tf.constant([&quot;b&quot;,&quot;b&quot;,&quot;c&quot;,&quot;b&quot;,&quot;a&quot;,&quot;potato&quot;,&quot;&quot;])

print(text_vectorization.call(test_tensor))

tf.Tensor(
[[3]
 [3]
 [4]
 [3]
 [2]
 [1]
 [0]], shape=(7, 1), dtype=int64)
</code></pre>
<p>In my application though I want to switch this behaviour off as I don't need it. <a href=""https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization"" rel=""nofollow noreferrer"">The documentation</a> doesn't seem to provide an option to disable it and for now I am just going to -2 from the output but that doesn't feel right.</p>
<p>Are there any cleaner, tensorflow native, solutions for generating integer representations of string tensors?</p>
","I am looking to one hot encode string tensor as part of my dataset pipeline. It seems to me this can be achieved using TextVectorization to get an integer representation of the string tensor and then one_hot to convert to achieve the encoded 2d tensor. When i use TextVectorization it seems to automatically try to map """" to 0 and strings out of vocabulary strings to 1. See below using Tensorflow 2.3: I can see why it would be useful as it would handle the last 2 values in the below tensor without throwing an error and creating a feature for them in the process. In my application though I want to switch this behaviour off as I don't need it. The documentation doesn't seem to provide an option to disable it and for now I am just going to -2 from the output but that doesn't feel right. Are there any cleaner, tensorflow native, solutions for generating integer representations of string tensors?",https://stackoverflow.com/questions/67600983,5799799,Documentation Completeness
68089330,TimeDistributed(Dense) vs Dense And TimeDistributed(Conv2D),"<p>I am trying to understand the how <code>TimeDitributed()</code> Layer works in keras!! I know that when we wrap <code>Conv2D</code> layer in <code>TimeDitributed()</code> it applies same Conv2D layer to all the time events of a video(or to different frames that are present in a video sequence). As mentioned here <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed</a>.</p>
<p>For the purpose of my project I am trying to build an LSTM model which is of the as follows:</p>
<pre><code>class Lstm_model_1(tf.keras.Model):

    def __init__(self, num_classes):
        super(Lstm_model_1, self).__init__()   
        self.Lstm1 = tf.keras.layers.LSTM(32,return_sequences=True)
        self.Lstm2 = tf.keras.layers.LSTM(32,return_sequences=True) 
        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')
        self.TimeDistributed=tf.keras.layers.TimeDistributed(self.classifier)

    def call(self, inputs):
        input_A=inputs
        x = self.Lstm1(input_A)
        x = self.Lstm2(x)
        output = self.TimeDistributed(x)
        
        return  output
lstm_1 = Lstm_model_1(3)
lstm_1.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy())
lstm_1.fit(X_train,Y_train, epochs=3,validation_data=(X_test,Y_test))
lstm_1.summary()
Model: &quot;lstm_model_1_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_9 (LSTM)                multiple                  55552     
_________________________________________________________________
lstm_10 (LSTM)               multiple                  8320      
_________________________________________________________________
dense_6 (Dense)              multiple                  99        
_________________________________________________________________
time_distributed (TimeDistri multiple                  99        
=================================================================
Total params: 63,971
Trainable params: 63,971
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>Here I am getting 99 parameters in the <code>TimeDistributed()</code> layer.</p>
<p>Now when I am not using <code>TimeDistributed()</code> Layer I am getting same number of parameters i.e 99.</p>
<p>I have read in the following posts that :-</p>
<blockquote>
<p>If return_sequences=True, then the Dense layer is used to apply at every timestep just like TimeDistributedDense.</p>
</blockquote>
<p>and</p>
<hr />
<blockquote>
<p>As a side note: this makes TimeDistributed(Dense(...)) and Dense(...) equivalent to each other.</p>
</blockquote>
<hr />
<blockquote>
<p>Another side note: be aware that this has the effect of shared weights.&quot;&quot;</p>
</blockquote>
<hr />
<ol>
<li><a href=""https://stackoverflow.com/questions/58560304/timedistributeddense-vs-dense-in-seq2seq"">TimeDistributed(Dense) vs Dense in seq2seq</a></li>
<li><a href=""https://stackoverflow.com/questions/52089601/keras-dense-layers-input-is-not-flattened/52092176#52092176"">Keras Dense layer&#39;s input is not flattened</a></li>
</ol>
<p>Now According to me it makes sense that the dense layer when applied on the LSTM <code>return_sequences=True</code> should have the same weights for the all the timestamps. But I have few questions that are mentioned below.</p>
<ol>
<li>Is the <code>TimeDitributed()</code> wrapped with <code>Dense()</code> is redundant and can we use <code>Dense()</code> directly?</li>
<li>If let's say I don't want to use shared weights corresponding to the sequence outputs then what should I do? I want my network to learn different set of weights corresponding to each of the output in case of <code>return_sequences=True</code></li>
<li>Why are we still wrapping our <code>Dense()</code> layer in the <code>TimeDitributed()</code> layer if both of them are sharing weights in the time squences? I have seen the usage of <code>TimeDitributed()</code> layer with <code>RepeatedVector()</code> layer here <a href=""https://datascience.stackexchange.com/questions/46491/what-is-the-job-of-repeatvector-and-timedistributed"">https://datascience.stackexchange.com/questions/46491/what-is-the-job-of-repeatvector-and-timedistributed</a></li>
<li>Is it only with the case of <code>Dense()</code> that <code>TimeDitribued()</code>  is redundant or is it also the same with <code>Conv2D</code> Layer?</li>
</ol>
",I am trying to understand the how TimeDitributed() Layer works in keras!! I know that when we wrap Conv2D layer in TimeDitributed() it applies same Conv2D layer to all the time events of a video(or to different frames that are present in a video sequence). As mentioned here https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed. For the purpose of my project I am trying to build an LSTM model which is of the as follows: Here I am getting 99 parameters in the TimeDistributed() layer. Now when I am not using TimeDistributed() Layer I am getting same number of parameters i.e 99. I have read in the following posts that :- and Now According to me it makes sense that the dense layer when applied on the LSTM return_sequences=True should have the same weights for the all the timestamps. But I have few questions that are mentioned below.,https://stackoverflow.com/questions/68089330,16156882,Documentation Replication on Other Examples
62270283,Tensorflow - Custom loss function with sample_weight,"<p>I'm trying to run a custom function that accepts sample_weights. I'm following this documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss</a>. </p>

<p>However, when I try to use the following cost function:</p>

<pre><code>class deltaE(Loss):
  def __call__(self, y_true, y_pred, sample_weight):
    errors = tf_get_deltaE2000(y_true * tf_Xtrain_labels_max, y_pred * tf_Xtrain_labels_max)
    errors *= sample_weight
    return tf.math.reduce_mean(errors, axis=-1)

loss_deltaE = deltaE()
</code></pre>

<p>I get this error on the <code>Model.fit</code> method.</p>

<pre><code>TypeError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:543 train_step  **
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:411 update_state
        metric_obj.update_state(y_t, y_p)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:90 decorated
        update_op = update_state_fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py:603 update_state
        matches = self._fn(y_true, y_pred, **self._fn_kwargs)

    TypeError: __call__() missing 1 required positional argument: 'sample_weight'
</code></pre>

<p>I'm using a generator that yields a tuple of length 3 just as required. I've checked that. That's working properly.</p>

<p>The cost function works fine too. When I use the code below, the model trains without problems.</p>

<pre><code>def loss_deltaE(y_true, y_pred):
  errors = tf_get_deltaE2000(y_true * tf_Xtrain_labels_max, y_pred * tf_Xtrain_labels_max)
  return tf.math.reduce_mean(errors, axis=-1)
</code></pre>

<p>If someone has any clue. I'd appreciate it. Thanks in advance!</p>
","I'm trying to run a custom function that accepts sample_weights. I'm following this documentation https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss. However, when I try to use the following cost function: I get this error on the Model.fit method. I'm using a generator that yields a tuple of length 3 just as required. I've checked that. That's working properly. The cost function works fine too. When I use the code below, the model trains without problems. If someone has any clue. I'd appreciate it. Thanks in advance!",https://stackoverflow.com/questions/62270283,6509883,Documentation Replicability
63656333,'Reduction' parameter in tf.keras.losses,"<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction"" rel=""noreferrer"">docs</a>, the <code>Reduction</code> parameter takes on 3 values - <code>SUM_OVER_BATCH_SIZE</code>, <code>SUM</code> and <code>NONE</code>.</p>
<pre><code>y_true = [[0., 2.], [0., 0.]]
y_pred = [[3., 1.], [2., 5.]]

mae = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)
mae(y_true, y_pred).numpy()
&gt; 5.5

mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
&gt; 2.75
</code></pre>
<p>What I could infer about the calculation after various trials, is this:-</p>
<ul>
<li><p>when <code>REDUCTION = SUM</code>,</p>
<p><code>Loss = Sum over all samples {(Sum of differences between y_pred and y_target vector of each sample / No of element in y_target of the sample )} = { (abs(3-0) + abs(1-2))/2 } + { (abs(2-0) + abs(5-0))/2 } = {4/2} + {7/2} = 5.5</code>.</p>
</li>
<li><p>when <code>REDUCTION = SUM_OVER_BATCH_SIZE</code>,</p>
<p><code>Loss = [Sum over all samples {(Sum of differences between y_pred and y_target vector of each sample / No of element in y_target of the sample )}] / Batch_size or No of Samples  = [ { (abs(3-0)} + abs(1-2))/2 } + { (abs(2-0) + abs(5-0))/2 } ]/2 = [ {4/2} + {7/2} ]/2 = [5.5]/2 = 2.75</code>.</p>
</li>
</ul>
<p>As a result, <code>SUM_OVER_BATCH_SIZE</code> is nothing but <code>SUM/batch_size</code>. Then, why is it called <code>SUM_OVER_BATCH_SIZE</code> when <code>SUM</code> actually adds up the losses over the entire batch, while <code>SUM_OVER_BATCH_SIZE</code> calculates the average loss of the batch.</p>
<p>Is my assumption regarding the workings of <code>SUM_OVER_BATCH_SIZE</code> and <code>SUM</code> at all correct?</p>
","According to the docs, the Reduction parameter takes on 3 values - SUM_OVER_BATCH_SIZE, SUM and NONE. What I could infer about the calculation after various trials, is this:- As a result, SUM_OVER_BATCH_SIZE is nothing but SUM/batch_size. Then, why is it called SUM_OVER_BATCH_SIZE when SUM actually adds up the losses over the entire batch, while SUM_OVER_BATCH_SIZE calculates the average loss of the batch. Is my assumption regarding the workings of SUM_OVER_BATCH_SIZE and SUM at all correct?",https://stackoverflow.com/questions/63656333,9895768,Documentation Ambiguity
57244733,Where does the documentation point to a list of values for the loss property of the compile function?,"<p>I'm following the <a href=""https://www.tensorflow.org/tutorials/keras/basic_classification"" rel=""nofollow noreferrer"">Tensorflow documentation for creating a simple neural network</a>.  One of the steps is</p>

<pre><code>model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])  
</code></pre>

<p>When I look at the documentation for the <code>loss</code> parameter, it says </p>

<blockquote>
  <p><strong>loss</strong>: String (name of objective function), objective function or
  tf.losses.Loss  instance. See tf.losses. If the model has multiple
  outputs, you can use a  different loss on each output by passing a
  dictionary or a list of losses. The  loss value that will be minimized
  by the model will then be the sum of all  individual losses.</p>
</blockquote>

<p>Based on this documentation of the <code>compile</code> function, how would I find a list of the strings and/or objective functions that I can pass for the <code>loss</code> parameter?  I found the <code>tr.keras.losses</code> that has the objective functions by Googling, but it seems like there should be a link or mention of that in the documentation for <code>Sequential.compile</code>.  Am I missing something?</p>
","I'm following the Tensorflow documentation for creating a simple neural network. One of the steps is When I look at the documentation for the loss parameter, it says Based on this documentation of the compile function, how would I find a list of the strings and/or objective functions that I can pass for the loss parameter? I found the tr.keras.losses that has the objective functions by Googling, but it seems like there should be a link or mention of that in the documentation for Sequential.compile. Am I missing something?",https://stackoverflow.com/questions/57244733,4820436,Documentation Replicability
63506296,Saving and Loading Tensorflow Model Results in Keras Error,"<p>For a project that I'm working on, I have created a simple model in TensorFlow that consists of a dense features layer followed by three dense layers.</p>
<pre class=""lang-py prettyprint-override""><code>def build_model(arguments):
    model = tf.keras.Sequential([
        tf.keras.layers.DenseFeatures(arguments),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(5, activation='sigmoid')
    ])
    
    return model
</code></pre>
<p>I am unable to go into more detail about the parameter <code>arguments</code>, but the above model function works perfectly fine and can train and save a <code>.h5</code> file perfectly fine using the code below.</p>
<pre class=""lang-py prettyprint-override""><code># Create a path for the saving location of the model
model_dir = log_dir + &quot;\model.h5&quot;

# Save the model
model.save(model_dir)
</code></pre>
<p>However, when I try to load the model back from the <code>.h5</code> file,</p>
<pre class=""lang-py prettyprint-override""><code>model = tf.keras.models.load_model(model_path)
</code></pre>
<p>I get the following error message.</p>
<pre class=""lang-py prettyprint-override""><code>  File &quot;sampleModel.py&quot;, line 342, in &lt;module&gt;
    model = tf.keras.models.load_model(model_path)
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\keras\saving\save.py&quot;, line 1
82, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, c
ompile)
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\keras\saving\hdf5_format.py&quot;,
 line 178, in load_model_from_hdf5
    custom_objects=custom_objects)
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\keras\saving\model_config.py&quot;
, line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\keras\layers\serialization.py
&quot;, line 175, in deserialize
    printable_module_name='layer')
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\keras\utils\generic_utils.py&quot;
, line 358, in deserialize_keras_object
    list(custom_objects.items())))
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\keras\engine\sequential.py&quot;,
line 487, in from_config
    custom_objects=custom_objects)
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\keras\layers\serialization.py
&quot;, line 175, in deserialize
    printable_module_name='layer')
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\keras\utils\generic_utils.py&quot;
, line 358, in deserialize_keras_object
    list(custom_objects.items())))
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\keras\feature_column\base_fea
ture_layer.py&quot;, line 141, in from_config
    config['feature_columns'], custom_objects=custom_objects)
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\feature_column\serialization.
py&quot;, line 186, in deserialize_feature_columns
    for c in configs
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\feature_column\serialization.
py&quot;, line 186, in &lt;listcomp&gt;
    for c in configs
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\feature_column\serialization.
py&quot;, line 138, in deserialize_feature_column
    columns_by_name=columns_by_name)
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\feature_column\feature_column
_v2.py&quot;, line 2622, in from_config
    config['normalizer_fn'], custom_objects=custom_objects)
  File &quot;C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\Python
\Python37\site-packages\tensorflow\python\feature_column\serialization.
py&quot;, line 273, in _deserialize_keras_object
    obj = module_objects.get(object_name)
AttributeError: 'NoneType' object has no attribute 'get'
</code></pre>
<p>Looking around, I suspect it has something to do with the <code>custom_objects</code> tag for the <code>load_model</code> function, but I am not 100% sure of how to implement it.</p>
<p>The only custom objects that I could be using are my loss, which I declare below, and the <code>accuracy</code> metric that I use.</p>
<p><code>loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</code></p>
","For a project that I'm working on, I have created a simple model in TensorFlow that consists of a dense features layer followed by three dense layers. I am unable to go into more detail about the parameter arguments, but the above model function works perfectly fine and can train and save a .h5 file perfectly fine using the code below. However, when I try to load the model back from the .h5 file, I get the following error message. Looking around, I suspect it has something to do with the custom_objects tag for the load_model function, but I am not 100% sure of how to implement it. The only custom objects that I could be using are my loss, which I declare below, and the accuracy metric that I use. loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)",https://stackoverflow.com/questions/63506296,13079914,Requesting (Additional) Resources
66420994,How to use tfa.metrics.F1Score with image_dataset_from_directory correctly?,"<p>Colab code is <a href=""https://colab.research.google.com/drive/1XhVpnjhpvtDq3kjZJ4_vjeAhoYQh9XsR?usp=sharing"" rel=""nofollow noreferrer"">here</a>:</p>
<p>I am following the docs <a href=""https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/F1Score"" rel=""nofollow noreferrer"">here</a> to get the result for multiclass prediction</p>
<p>When I train using</p>
<pre><code>#last layer
tf.keras.layers.Dense(2, activation='softmax')

model.compile(optimizer=&quot;adam&quot;,
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=[tf.keras.metrics.CategoricalAccuracy(),
                       tfa.metrics.F1Score(num_classes=2, average='macro')])
</code></pre>
<p>I get</p>
<pre><code>144/144 [==] - 8s 54ms/step - loss: 0.0613 - categorical_accuracy: 0.9789 - f1_score: 0.9788 - val_loss: 0.0826 - val_categorical_accuracy: 0.9725 - val_f1_score: 0.9722
</code></pre>
<p>When I do:</p>
<pre><code>model.evaluate(val_ds)
</code></pre>
<p>I get</p>
<pre><code>16/16 [==] - 0s 15ms/step - loss: 0.0826 - categorical_accuracy: 0.9725 - f1_score: 0.9722
[0.08255868405103683, 0.9725490212440491, 0.9722140431404114]
</code></pre>
<p>I would like to use the <code>metric.result</code> as in the official website. When I load the below code, I get <code>0.4875028</code> which is wrong. How can I get the correct <code>predicted_categories</code> and <code>true_categories</code>?</p>
<pre><code>metric = tfa.metrics.F1Score(num_classes=2, average='macro')

predicted_categories = model.predict(val_ds)
true_categories = tf.concat([y for x, y in val_ds], axis=0).numpy() 

metric.update_state(true_categories, predicted_categories)
result = metric.result()
print(result.numpy())

#0.4875028
</code></pre>
<p>Here is how I loaded my data</p>
<pre><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    main_folder,
    validation_split=0.1,
    subset=&quot;training&quot;,
    label_mode='categorical',
    seed=123,
    image_size=(dim, dim))

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    main_folder,
    validation_split=0.1,
    subset=&quot;validation&quot;,
    label_mode='categorical',
    seed=123,
    image_size=(dim, dim))
</code></pre>
","Colab code is here: I am following the docs here to get the result for multiclass prediction When I train using I get When I do: I get I would like to use the metric.result as in the official website. When I load the below code, I get 0.4875028 which is wrong. How can I get the correct predicted_categories and true_categories? Here is how I loaded my data",https://stackoverflow.com/questions/66420994,10868301,Documentation Replicability
71909901,Guiding tensorflow keras model training to achieve best Recall At Precision 0.95 for binary classification,"<p>I am hoping to get some help on the titular topic.
I have a database of medical data of patients with two similar pathologies, one severe and one much less so. I need flag most of the formers (≥95%) and leave out as many of the latter as possible.</p>
<p>Therefore, I want to create a binary classifier that reflects this. Looking around on the web (not an expert) I put together this piece of code, substituting the metric I found with RecallAtPrecision(0.95) in the middle part of the code. Below is an abridged version:</p>
<pre><code>import tensorflow as tf

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(10, input_dim=x_train.shape[1], activation='relu', kernel_initializer='he_normal'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=[tf.keras.metrics.RecallAtPrecision(0.95)])

history = model.fit(x_train, y_train, validation_split=0.33, batch_size=16, epochs=EPOCHS)
</code></pre>
<p>However, it simply doesn't work, as it throws the following error:</p>
<blockquote>
<p>AttributeError: module 'tensorflow_core.keras.metrics' has no attribute 'RecallAtPrecision'</p>
</blockquote>
<p>I am at a loss about why that happened, as I can clearly see it in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RecallAtPrecision"" rel=""nofollow noreferrer"">documentation</a>. The code works if I use Recall(), Precision() or most any other metrics. Looking around some more, I am beginning to think I am missing something fundamental.
Do any of you fine ladies and gentlemen have any pointers on how to solve this problem?</p>
","I am hoping to get some help on the titular topic. I have a database of medical data of patients with two similar pathologies, one severe and one much less so. I need flag most of the formers (≥95%) and leave out as many of the latter as possible. Therefore, I want to create a binary classifier that reflects this. Looking around on the web (not an expert) I put together this piece of code, substituting the metric I found with RecallAtPrecision(0.95) in the middle part of the code. Below is an abridged version: However, it simply doesn't work, as it throws the following error: I am at a loss about why that happened, as I can clearly see it in the documentation. The code works if I use Recall(), Precision() or most any other metrics. Looking around some more, I am beginning to think I am missing something fundamental. Do any of you fine ladies and gentlemen have any pointers on how to solve this problem?",https://stackoverflow.com/questions/71909901,18846088,Documentation Replication on Other Examples
63063260,Extracting features from EfficientNet Tensorflow,"<p>I have a CNN model trained using EfficientNetB6.
My task is to extract the features of this trained model by removing the last dense layer and then using those weights to train a boosting model.
i did this using Pytorch earlier and was able to extract the weights from the layers i was interested and predicted on my validation set and then boosted.</p>
<p>I am doing this now in tensorflow but currently stuck.
Below is my model structure and I have tried using the code on the website but did not had any luck.
<a href=""https://i.stack.imgur.com/eGdv4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eGdv4.png"" alt=""enter image description here"" /></a></p>
<p>I want to remove the last dense layer and predict on the validation set using the remaining layers.</p>
<p>I tried using :</p>
<p>layer_name = 'efficientnet-b6'
intermediate_layer_model = tf.keras.Model(inputs = model.input, outputs = model.get_layer(layer_name).output)</p>
<p>but i get an error &quot;
ValueError: Graph disconnected: cannot obtain value for tensor Tensor(&quot;input_1:0&quot;, shape=(None, 760, 760, 3), dtype=float32) at layer &quot;input_1&quot;. The following previous layers were accessed without issue: []&quot;</p>
<p>Any way to resolve this?</p>
","I have a CNN model trained using EfficientNetB6. My task is to extract the features of this trained model by removing the last dense layer and then using those weights to train a boosting model. i did this using Pytorch earlier and was able to extract the weights from the layers i was interested and predicted on my validation set and then boosted. I am doing this now in tensorflow but currently stuck. Below is my model structure and I have tried using the code on the website but did not had any luck. I want to remove the last dense layer and predict on the validation set using the remaining layers. I tried using : layer_name = 'efficientnet-b6' intermediate_layer_model = tf.keras.Model(inputs = model.input, outputs = model.get_layer(layer_name).output) but i get an error "" ValueError: Graph disconnected: cannot obtain value for tensor Tensor(""input_1:0"", shape=(None, 760, 760, 3), dtype=float32) at layer ""input_1"". The following previous layers were accessed without issue: []"" Any way to resolve this?",https://stackoverflow.com/questions/63063260,5197636,Requesting (Additional) Resources
66698823,How to modify output in keras for back propagation,"<p>I have a U-Net model written in tensorflow for a segmentation problem. I want to improve my segmentation with the same amount of training data and I was thinking of adding a level set method module to the output and then calculate the loss. Something like this <a href=""https://arxiv.org/pdf/1705.06260.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1705.06260.pdf</a></p>
<p>But I don't know how to modify the output of the last layer in tensorflow</p>
<pre class=""lang-py prettyprint-override""><code>def amodel(pretrained_weights=None,
           input_size=(512, 512, 1),
           act=&quot;relu&quot;):
    inputs = tf.keras.layers.Input(input_size)
    conv1 = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(inputs)
    model = tf.keras.Model(inputs=inputs, outputs=conv1)
    
    # model.compile(optimizer = Adam(lr = 1e-4), 
    loss = 'binary_crossentropy', metrics = ['accuracy'])
    model.compile(optimizer=tf.keras.optimizers.Adam(lr_scheduler),
                  loss=combo_loss(alpha=0, beta=0.4),
                  metrics=[dice_accuracy])

</code></pre>
<p>How do you apply a transformation to conv1 before forwarding to tf.keras.Model?</p>
<p>Thanks you</p>
",I have a U-Net model written in tensorflow for a segmentation problem. I want to improve my segmentation with the same amount of training data and I was thinking of adding a level set method module to the output and then calculate the loss. Something like this https://arxiv.org/pdf/1705.06260.pdf But I don't know how to modify the output of the last layer in tensorflow How do you apply a transformation to conv1 before forwarding to tf.keras.Model? Thanks you,https://stackoverflow.com/questions/66698823,,Requesting (Additional) Resources
64231624,Where does a TensorFlow model instance get `input` property from?,"<p>I am <strong>not</strong> talking about how to pass an input to a model.</p>
<p>If you make a model, e.g. from the docs:</p>
<pre class=""lang-py prettyprint-override""><code>model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])
</code></pre>
<p><code>model</code> actually has a few properties (or attributes) which are not listed in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#attributes_1"" rel=""nofollow noreferrer"">documentation</a>... These include <code>input</code>, <code>inputs</code>, <code>name</code>, and much more. I've listed them with <code>dir()</code>, but surely its documented <strong>somewhere</strong>.</p>
<p>I would like to know what exactly these are. It really seems like a silly question. Maybe there is a different document I cannot find?</p>
","I am not talking about how to pass an input to a model. If you make a model, e.g. from the docs: model actually has a few properties (or attributes) which are not listed in the documentation... These include input, inputs, name, and much more. I've listed them with dir(), but surely its documented somewhere. I would like to know what exactly these are. It really seems like a silly question. Maybe there is a different document I cannot find?",https://stackoverflow.com/questions/64231624,7365866,Requesting (Additional) Resources
68123049,Tensorflow Autoencoder ValueError: No gradients provided for any variable,"<p>I'm trying to create a autoencoder using tensorflow that analyses a dataset of cars for a university project. However the code outputs a error when starting to train that I can't seem to find the solution for.</p>
<p>First I tried reading the tensorflow documentation for the <code>fit</code> function but there was no reference to this error.
Next I tried to search for similar errors on StackOverflow but I couldn't find anything that was related.</p>
<pre class=""lang-py prettyprint-override""><code>import os
import pathlib

import cv2
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import backend as K
from tensorflow.keras.layers import (Activation, BatchNormalization, Conv2D,
                                     Conv2DTranspose, Dense, Flatten, Input,
                                     LeakyReLU, Reshape)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Configuration
HEIGHT = 28
WIDTH = 32
NUM_CHANNELS = 3
BATCH_SIZE = 32
LATENT_SPACE_DIM = 20
EPOCHS = 25

AUTOTUNE = tf.data.experimental.AUTOTUNE

# Download dataset
dataset_url = &quot;http://ai.stanford.edu/~jkrause/car196/car_ims.tgz&quot;
data_dir = tf.keras.utils.get_file(origin=dataset_url,
                                   fname='car_ims',
                                   untar=True)
data_dir = pathlib.Path(data_dir)

normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)

# Load dataset
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    labels=None,
    image_size=(WIDTH, HEIGHT),
    seed=123,
    validation_split=0.3,
    subset=&quot;training&quot;,
    smart_resize=True,
    batch_size=BATCH_SIZE)

dataset = dataset.map(normalization_layer)
dataset = dataset.cache()
dataset = dataset.prefetch(buffer_size=AUTOTUNE)

# Load testset
testset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    labels=None,
    image_size=(WIDTH, HEIGHT),
    seed=123,
    validation_split=0.3,
    subset=&quot;validation&quot;,
    smart_resize=True,
    batch_size=BATCH_SIZE)

testset = testset.map(normalization_layer)
testset = testset.cache()
testset = testset.prefetch(buffer_size=AUTOTUNE)

# Encoder
inputs = Input(shape =(WIDTH, HEIGHT, NUM_CHANNELS))

x = Conv2D(32, (3, 3), strides=2, padding=&quot;same&quot;)(inputs)
x = LeakyReLU(alpha=0.2)(x)
x = BatchNormalization()(x)

x = Conv2D(64, (3, 3), strides=2, padding=&quot;same&quot;)(x)
x = LeakyReLU(alpha=0.2)(x)
x = BatchNormalization()(x)

volumeSize = K.int_shape(x)
x = Flatten()(x)

# Latent space
latent = Dense(LATENT_SPACE_DIM, name=&quot;latent&quot;)(x)

#decoder
latentInputs = Input(shape=(LATENT_SPACE_DIM,))
y = Dense(np.prod(volumeSize[1:]))(latentInputs)
y = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(y)

y = Conv2DTranspose(64, (3, 3), strides=2, padding=&quot;same&quot;)(y)
y = LeakyReLU(alpha=0.2)(y)
y = BatchNormalization()(y)

y = Conv2DTranspose(32, (3, 3), strides=2, padding=&quot;same&quot;)(y)
y = LeakyReLU(alpha=0.2)(y)
y = BatchNormalization()(y)

y = Conv2DTranspose(NUM_CHANNELS, (3, 3), padding=&quot;same&quot;)(y)
outputs = Activation(&quot;sigmoid&quot;, name=&quot;decoded&quot;)(y)

encoder = Model(inputs, latent, name=&quot;encoder&quot;)
decoder = Model(latentInputs, outputs, name=&quot;decoder&quot;)
autoencoder = Model(inputs=inputs, outputs=decoder(encoder(inputs)))

encoder.summary()
decoder.summary()
autoencoder.summary()

# Prepare model
autoencoder.compile(loss=&quot;mse&quot;, optimizer=Adam(learning_rate=1e-3))

# train the convolutional autoencoder
history = autoencoder.fit(
    dataset,
    validation_data=testset,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE)
</code></pre>
<p>The portion of the output with the error:</p>
<pre><code>Epoch 1/25
Traceback (most recent call last):
  File &quot;/home/mightymime/repos/TA-2021/src/main.py&quot;, line 111, in &lt;module&gt;
    history = autoencoder.fit(
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 1183, in fit
    tmp_logs = self.train_function(iterator)
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py&quot;, line 889, in __call__
    result = self._call(*args, **kwds)
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py&quot;, line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py&quot;, line 763, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py&quot;, line 3050, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py&quot;, line 3444, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py&quot;, line 3279, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 999, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py&quot;, line 672, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File &quot;/home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 986, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *
        return step_function(self, iterator)
    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **
        outputs = model.train_step(data)
    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:799 train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:530 minimize
        return self.apply_gradients(grads_and_vars, name=name)
    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:630 apply_gradients
        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    /home/mightymime/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/utils.py:75 filter_empty_gradients
        raise ValueError(&quot;No gradients provided for any variable: %s.&quot; %

    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'latent/kernel:0', 'latent/bias:0', 'dense/kernel:0', 'dense/bias:0', 'conv2d_transpose/kernel:0', 'conv2d_transpose/bias:0', 'batch_normalization_2/gamma:0', 'batch_normalization_2/beta:0', 'conv2d_transpose_1/kernel:0', 'conv2d_transpose_1/bias:0', 'batch_normalization_3/gamma:0', 'batch_normalization_3/beta:0', 'conv2d_transpose_2/kernel:0', 'conv2d_transpose_2/bias:0'].
</code></pre>
<p>Can anyone help me debug this? Thanks in advance</p>
",I'm trying to create a autoencoder using tensorflow that analyses a dataset of cars for a university project. However the code outputs a error when starting to train that I can't seem to find the solution for. First I tried reading the tensorflow documentation for the fit function but there was no reference to this error. Next I tried to search for similar errors on StackOverflow but I couldn't find anything that was related. The portion of the output with the error: Can anyone help me debug this? Thanks in advance,https://stackoverflow.com/questions/68123049,10754901,Requesting (Additional) Resources
75364751,'Generator' object has no attribute 'shape' when trying to training model on data from generator,"<p>I have been trying to train a simpletf.keras.Sequential() model with the help of a custom generator. According to <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">this</a> I can pass a generator to model.fit() like so:</p>
<pre><code>model.fit(x=generator, epochs=5)
</code></pre>
<p>However I keep getting the following error.</p>
<pre><code>AttributeError                            Traceback (most recent call last)
Cell In[35], line 1
----&gt; 1 model.fit(x=generator, epochs=5)

File ~\venv\lib\site-packages\keras\utils\traceback_utils.py:70, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---&gt; 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~venv\lib\site-packages\keras\engine\data_adapter.py:885, in GeneratorDataAdapter.__init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)
    878     except NotImplementedError:
    879         # The above call may fail if the model is a container-like class
    880         # that does not implement its own forward pass (e.g. a GAN or
    881         # VAE where the forward pass is handled by subcomponents).  Such
    882         # a model does not need to be built.
    883         pass
--&gt; 885 self._first_batch_size = int(tf.nest.flatten(peek)[0].shape[0])
    887 def _get_tensor_spec(t):
    888     # TODO(b/226395276): Remove _with_tensor_ranks_only usage.
    889     return type_spec.type_spec_from_value(t)._with_tensor_ranks_only()

AttributeError: 'SequenceGenerator' object has no attribute 'shape'
</code></pre>
<p>The model I was trying to train:</p>
<pre><code># Create model
model = tf.keras.Sequential()
model.add(tf.keras.layers.LSTM(32,
                               input_shape=(gen_config[&quot;num_steps&quot;], 40,),
                               return_sequences=False,
                               stateful=False,
                               ))

model.add(tf.keras.layers.Dense(3, activation='softmax'))

#compile model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'],
              )
</code></pre>
<p>The generator I use to yield the inputs:</p>
<pre><code>class SequenceGenerator():

    def __iter__(self): 

        # set seed everytime that generator is restarted
        if self.seed:
            np.random.seed(self.seed)

        # set chunk_index, batch_index
        self.chunk_index = 0
        self.batch_index = 0

        # set chunk_indexer
        self._chunk_indexer_setup()  # self._chunk_indexer_reset()
        self.no_total_chunks = len(self._chunk_indexer)
        self._no_features = self.pipeline_function.no_features

        # Start iteration
        if len(self._chunk_indexer) &gt; 0:

            # Infinite iteration
            while True:

                # Fill pool of chunks if
                # (1) x_ts is not yet initialized
                # (2) self.x_ts is not enough to feed two batches
                if self.x_ts is None or len(self.x_ts) &lt; self.batch_size * 2:
                    chunk_pool = self.get_chunk_pool()

                    if len(chunk_pool) &gt; 0:
                        x, y, flag = [np.vstack(array) for array in zip(*chunk_pool)]
                        x_ts, y_ts = self._get_timeseries(x, y, flag)

                        # 1st iteration of generator
                        if self.x_ts is None:
                            self.x_ts, self.y_ts = x_ts, y_ts
                        # Iterations after 1st, i.e. observations exists: just append
                        else:
                            self.x_ts = np.concatenate((self.x_ts, x_ts))
                            self.y_ts = np.concatenate((self.y_ts, y_ts))

                # if x_ts contains at least one batch, yield batch
                # if generator is exhausted, yield empty batch
                if len(self.x_ts) &gt;= self.batch_size or self.exhausted:
                    yield self.x_ts[:self.batch_size], self.y_ts[:self.batch_size]
</code></pre>
<p>The generator outputs a tuple of length 2 (as specified in &quot;Unpacking behavior for iterator-like inputs&quot; in the documentation), consisting of two numpy arrays.
The shape of x is (32, 100, 40) and that of y is (32, 1).
I am cofused about what shape attribute model.fit() expects the generator to provide.
Any help is greatly appreciated.</p>
","I have been trying to train a simpletf.keras.Sequential() model with the help of a custom generator. According to this I can pass a generator to model.fit() like so: However I keep getting the following error. The model I was trying to train: The generator I use to yield the inputs: The generator outputs a tuple of length 2 (as specified in ""Unpacking behavior for iterator-like inputs"" in the documentation), consisting of two numpy arrays. The shape of x is (32, 100, 40) and that of y is (32, 1). I am cofused about what shape attribute model.fit() expects the generator to provide. Any help is greatly appreciated.",https://stackoverflow.com/questions/75364751,18073394,Documentation Completeness
61817055,Why BinaryCrossentropy as loss and metrics are not identical in classifier training using tf.keras (Tensorflow 2.0)?,"<p>I am using BinaryCrossentropy as both a loss and one of the metrics:</p>

<pre><code>model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), 
    loss=tf.keras.losses.BinaryCrossentropy(), 
    metrics=[tf.keras.metrics.BinaryCrossentropy(), tf.keras.metrics.AUC()])
</code></pre>

<p>Since they are the same thing, I think they should produce the same result. <strong>However they shows slightly different values on both training set and validation set respectively. Why is this? Shouldn't BinaryCrossentropy has the same value on the same data?</strong>
<a href=""https://i.stack.imgur.com/OojMH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OojMH.png"" alt=""enter image description here""></a></p>

<p>Is it possible that, the loss value is the loss on the final batch, and the metric value is calculated on all batches of the epoch (average?)?</p>

<p>I tried to find relevant information on <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile"" rel=""nofollow noreferrer"">tf.keras.Model.compile</a>, but I couldn't confirm this yet.</p>
","I am using BinaryCrossentropy as both a loss and one of the metrics: Since they are the same thing, I think they should produce the same result. However they shows slightly different values on both training set and validation set respectively. Why is this? Shouldn't BinaryCrossentropy has the same value on the same data? Is it possible that, the loss value is the loss on the final batch, and the metric value is calculated on all batches of the epoch (average?)? I tried to find relevant information on tf.keras.Model.compile, but I couldn't confirm this yet.",https://stackoverflow.com/questions/61817055,1516331,Documentation Ambiguity
53905146,How to make predictions in Keras using Tensorflow's Dataset API,"<p>I was trying to build model in Keras using Tensorflow's Dataset API. I successfully able to train model in keras. But for making a prediction for test data. it need to be in numpy array.</p>

<p><a href=""https://keras.io/models/model/"" rel=""nofollow noreferrer"">https://keras.io/models/model/</a></p>

<p>x needs to be numpy array. So i done something like this</p>

<pre><code>x = input_fn('test.tfrecords')
model = models.load_model(""model/model-40-0.35.hdf5"")

with tf.Session()) as sess:  
          x_out = np.asarray(sess.run(x))
pred = model.predict(x_out,batch_size=BATCH_SIZE, verbose=1)
print(pred)
</code></pre>

<p>it successfully made predictions, but i was thinking if there is any method where i able to insert tensor to a function for predictions. And not first converting it to numpy array. </p>

<p>I found this <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#predict"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#predict</a></p>

<p>But even whenever I input the tensor , this error comes</p>

<blockquote>
  <p>TypeError: predict() missing 1 required positional argument: 'x'</p>
</blockquote>
","I was trying to build model in Keras using Tensorflow's Dataset API. I successfully able to train model in keras. But for making a prediction for test data. it need to be in numpy array. https://keras.io/models/model/ x needs to be numpy array. So i done something like this it successfully made predictions, but i was thinking if there is any method where i able to insert tensor to a function for predictions. And not first converting it to numpy array. I found this https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#predict But even whenever I input the tensor , this error comes",https://stackoverflow.com/questions/53905146,9293927,Documentation Replication on Other Examples
42870727,Can one only implement gradient descent like optimizers with the code example from processing gradients in TensorFlow?,"<p>I was looking at the example code for processing gradients that TensorFlow has:</p>

<pre><code># Create an optimizer.
opt = GradientDescentOptimizer(learning_rate=0.1)

# Compute the gradients for a list of variables.
grads_and_vars = opt.compute_gradients(loss, &lt;list of variables&gt;)

# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you
# need to the 'gradient' part, for example cap them, etc.
capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]

# Ask the optimizer to apply the capped gradients.
opt.apply_gradients(capped_grads_and_vars)
</code></pre>

<p>however, I noticed that the <code>apply_gradients</code> function was derived from the <code>GradientDescentOptimizer</code>. Does that mean that using the example code from above, one can only implement gradient like descent rules (notice we could change the <code>opt = GradientDescentOptimizer</code> or <code>Adam</code> or any of the the other optimizers)? In particular, what does <code>apply_gradients</code> do? I definitively check the code in the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py"" rel=""nofollow noreferrer"">tf github page</a> but it was a bunch of python that had nothing to do with mathematical expressions, so it was hard to tell what that was doing and how it changed from optimizer to optimizer.</p>

<p>For example, if I wanted to implement my own custom optimizer that might use gradients (or might not e.g. just change the weights directly with some rule, maybe more biologically plausible rule), its not possible with the above example code?</p>

<hr>

<p>In particular I wanted to implement a gradient descent version that is artificially restricted in a compact domain. In particular I wanted to implement the following equation:</p>

<pre><code>w := (w - mu*grad + eps) mod B
</code></pre>

<p>in TensorFlow. I realized that the following is true:</p>

<pre><code>w := w mod B - mu*grad mod B + eps mod B
</code></pre>

<p>so I thought that I could just implement it by doing:</p>

<pre><code>def Process_grads(g,mu_noise,stddev_noise,B):
    return (g+tf.random_normal(tf.shape(g),mean=mu_noise,stddev=stddev_noise) ) % B
</code></pre>

<p>and then just having:</p>

<pre><code>processed_grads_and_vars = [(Process_grads(gv[0]), gv[1]) for gv in grads_and_vars]
# Ask the optimizer to apply the processed gradients.
opt.apply_gradients(processed_grads_and_vars)
</code></pre>

<p>however, I realized that that wasn't good enough because I don't actually have access to <code>w</code> so I can't implement:</p>

<pre><code>w mod B
</code></pre>

<p>at least not the way I tried. Is there a way to do this? i.e. to actually directly change the update rule? At least the way I tried?</p>

<p>I know its sort of a hacky update rule, but my point is more to change the update equation than actually caring to much about that update rule (so don't get hung up on it if its a bit weird).</p>

<hr>

<p>I came up with super hacky solution:</p>

<pre><code>def manual_update_GDL(arg,learning_rate,g,mu_noise,stddev_noise):
    with tf.variable_scope(arg.mdl_scope_name,reuse=True):
        W_var = tf.get_variable(name='W')
        eps = tf.random_normal(tf.shape(g),mean=mu_noise,stddev=stddev_noise)
        #
        W_new = tf.mod( W_var - learning_rate*g + eps , 20)
        sess.run( W_var.assign(W_new) )

def manual_GDL(arg,loss,learning_rate,mu_noise,stddev_noise,compact,B):
    # Compute the gradients for a list of variables.
    grads_and_vars = opt.compute_gradients(loss)
    # process gradients
    processed_grads_and_vars = [(manual_update_GDL(arg,learning_rate,g,mu_noise,stddev_noise), v) for g,v in grads_and_vars]
</code></pre>

<p>not sure if it works but something like that should work in general. The idea is to just write down the equation one wants to use (<strong>in TensorFlow</strong>) for the learning rate and then update the weights manually using a session.</p>

<p>Unfortunately, such a solution means we have to take care of the annealing (decaying learning rate manually which seems annoying). This solution probably has many other problems, feel free to point them out (and give solutions if you can).</p>

<hr>

<p>For this very simple problem I realized one can just do the normal optimizer update rule and then just take the mod of the weights and re-assign them to their value:</p>

<pre><code>sess.run(fetches=train_step)
if arg.compact:
    # apply w := ( w - mu*g + eps ) mod B
    W_val = W_var.eval()
    W_new = tf.mod(W_var,arg.B).eval()
    W_var.assign(W_new).eval()
</code></pre>

<p>but in this case its a coincidence that such a simple solution exists (unfortunately, bypasses the whole point of my question).</p>

<p>Actually, this solutions slows down the code a lot. For the moment is the best that I've got.</p>

<hr>

<p>As a reference, I have seen this question: <a href=""https://stackoverflow.com/questions/38431054/how-to-create-an-optimizer-in-tensorflow"">How to create an optimizer in Tensorflow</a> , but didn't find it responded directly to my question.</p>
","I was looking at the example code for processing gradients that TensorFlow has: however, I noticed that the apply_gradients function was derived from the GradientDescentOptimizer. Does that mean that using the example code from above, one can only implement gradient like descent rules (notice we could change the opt = GradientDescentOptimizer or Adam or any of the the other optimizers)? In particular, what does apply_gradients do? I definitively check the code in the tf github page but it was a bunch of python that had nothing to do with mathematical expressions, so it was hard to tell what that was doing and how it changed from optimizer to optimizer. For example, if I wanted to implement my own custom optimizer that might use gradients (or might not e.g. just change the weights directly with some rule, maybe more biologically plausible rule), its not possible with the above example code? In particular I wanted to implement a gradient descent version that is artificially restricted in a compact domain. In particular I wanted to implement the following equation: in TensorFlow. I realized that the following is true: so I thought that I could just implement it by doing: and then just having: however, I realized that that wasn't good enough because I don't actually have access to w so I can't implement: at least not the way I tried. Is there a way to do this? i.e. to actually directly change the update rule? At least the way I tried? I know its sort of a hacky update rule, but my point is more to change the update equation than actually caring to much about that update rule (so don't get hung up on it if its a bit weird). I came up with super hacky solution: not sure if it works but something like that should work in general. The idea is to just write down the equation one wants to use (in TensorFlow) for the learning rate and then update the weights manually using a session. Unfortunately, such a solution means we have to take care of the annealing (decaying learning rate manually which seems annoying). This solution probably has many other problems, feel free to point them out (and give solutions if you can). For this very simple problem I realized one can just do the normal optimizer update rule and then just take the mod of the weights and re-assign them to their value: but in this case its a coincidence that such a simple solution exists (unfortunately, bypasses the whole point of my question). Actually, this solutions slows down the code a lot. For the moment is the best that I've got. As a reference, I have seen this question: How to create an optimizer in Tensorflow , but didn't find it responded directly to my question.",https://stackoverflow.com/questions/42870727,1601580,Documentation Completeness
76324768,clipnorm vs clipvalue vs global_clipnorm in tf.keras.optimizers.Adam,"<p>I am looking at the definition of <code>clipvalue, clipnorm, global_clipnorm</code> arguments in <code>tf.keras.optimizers.Adam</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"" rel=""noreferrer"">here</a>. I have some questions related to that. The description of the arguments mentions following:</p>
<ul>
<li><code>clipnorm</code>:   Float. If set, the gradient of each weight is individually clipped so that its norm is no higher than this value.</li>
<li><code>clipvalue</code>:  Float. If set, the gradient of each weight is clipped to be no higher than this value.</li>
<li><code>global_clipnorm</code>:    Float. If set, the gradient of all weights is clipped so that their global norm is no higher than this value.</li>
</ul>
<p>Questions:</p>
<ul>
<li>As gradient of a weight (as the description says each weight) is 1-dimensional, clipnorm and clipvalue should be the same. Are there some scenarios in which clipnorm and clipvalue are different?</li>
<li>Seems global_clipnorm computes norm over all the weights in the model. From the available options, it seems we don’t have any argument in tf.keras.optimizers.Adam to do that clipnorm over the weights of a layer. Do we have that option with the api?</li>
</ul>
","I am looking at the definition of clipvalue, clipnorm, global_clipnorm arguments in tf.keras.optimizers.Adam here. I have some questions related to that. The description of the arguments mentions following: Questions:",https://stackoverflow.com/questions/76324768,7561372,Documentation Ambiguity
60598858,"tensorflow.keras.optimizers.Adam optimizer object not giving error when using ""minimize"" function","<p>I'm working on linear regression problem using 'keras' but when I try to use the function ""minimize"" for the ""Adam optimizer"" object,I get the following error.</p>

<blockquote>
  <p>AttributeError: 'Adam' object has no attribute 'minimize'</p>
</blockquote>

<p>This function can be found in the official documentation at :
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#minimize"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#minimize</a>
and the code can also be found at 
<a href=""https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L289-L318"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L289-L318</a> </p>

<p>my code is given below</p>

<pre><code>     # Initialize an adam optimizer
from  tensorflow.python.keras.optimizers import Adam

    opt = keras.optimizers.Adam(0.5)

    for j in range(100):
        opt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])

    # Apply minimize, pass the loss function, and supply the variables

        # Print every 10th value of the loss
        if j % 10 == 0:
            print(loss_function(intercept, slope).numpy())

    # Plot data and regression line
    plot_results(intercept, slope)
</code></pre>

<p>why  cant  I use the minimize function for the above Adam object?</p>

<p>Note : linear_regression and loss_function are two functions already defined as below.</p>

<pre><code>def linear_regression(intercept,slope,features=x):
    return feautres*slope + intercept
def loss_function(intercept,slope,features=x,target=price):
    predictions=linear_regression(intercept,slope)
    return tf.keras.losses.mse(target,predictions)
</code></pre>
","I'm working on linear regression problem using 'keras' but when I try to use the function ""minimize"" for the ""Adam optimizer"" object,I get the following error. This function can be found in the official documentation at : https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#minimize and the code can also be found at https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L289-L318 my code is given below why cant I use the minimize function for the above Adam object? Note : linear_regression and loss_function are two functions already defined as below.",https://stackoverflow.com/questions/60598858,5587971,Requesting (Additional) Resources
69536507,Understand relation between different tensorflow AdagradOptimizer APIs,"<p>I am new to tf, and during reading a model code, I noticed it used 1), but most document I can find are using 2) and 3). So what is the <code>tensorflow.python</code> library used for,seems it is not in official document? And what is the relation between 1 to 2,3?</p>
<ol>
<li>from tensorflow.python.training.adagrad import AdagradOptimizer</li>
<li>from tf.compat.v1.train import AdagradOptimizer</li>
<li>from tf.keras.optimizers import Adagrad</li>
</ol>
","I am new to tf, and during reading a model code, I noticed it used 1), but most document I can find are using 2) and 3). So what is the tensorflow.python library used for,seems it is not in official document? And what is the relation between 1 to 2,3?",https://stackoverflow.com/questions/69536507,1269298,Documentation Ambiguity
56141142,"How to pass ""step"" to ExponentialDecay in GradientTape","<p>I tried to use an optimizers.schedules.ExponentialDecay isntance as the learning_rate to Adm optimizer, but i don't know how to pass ""step"" to it when train the model in GradientTape.</p>

<p>I use tensorflow-gpu-2.0-alpha0 and python3.6.
And i read the doc <a href=""https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/optimizers/schedules/ExponentialDecay"" rel=""nofollow noreferrer"">https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/optimizers/schedules/ExponentialDecay</a> but with no idea how to tackle it.</p>

<pre><code>initial_learning_rate = 0.1
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100000,
    decay_rate=0.96)

optimizer = tf.optimizers.Adam(learning_rate = lr_schedule)

for epoch in range(self.Epoch):
    ...
    ...
    with GradientTape as tape:
        pred_label = model(images)
        loss = calc_loss(pred_label, ground_label)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

# I tried this but the result seem not right.
# I want to pass ""epoch"" as ""step"" to lr_schedule
</code></pre>
","I tried to use an optimizers.schedules.ExponentialDecay isntance as the learning_rate to Adm optimizer, but i don't know how to pass ""step"" to it when train the model in GradientTape. I use tensorflow-gpu-2.0-alpha0 and python3.6. And i read the doc https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/optimizers/schedules/ExponentialDecay but with no idea how to tackle it.",https://stackoverflow.com/questions/56141142,3167681,Lack of Alternative Solutions/Documentation
69595923,How to decrease the learning rate every 10 epochs by a factor of 0.9?,"<p>I want to set the learning rate at 10^-3 with a decay every 10 epochs by a factor of 0.9. I am using the Adam optimizer in Tensorflow Keras. I have found this code in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay"" rel=""nofollow noreferrer"">official documentation</a>:</p>
<pre class=""lang-py prettyprint-override""><code>initial_learning_rate = 0.1

lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=100000,
    decay_rate=0.96,
    staircase=True
)
</code></pre>
<p>I do not know what is this <code>decay_steps=100000</code>. Actually I want to decrease my learning rate after 10 epochs. How can I do it?</p>
",I want to set the learning rate at 10^-3 with a decay every 10 epochs by a factor of 0.9. I am using the Adam optimizer in Tensorflow Keras. I have found this code in the official documentation: I do not know what is this decay_steps=100000. Actually I want to decrease my learning rate after 10 epochs. How can I do it?,https://stackoverflow.com/questions/69595923,14808637,Documentation Replicability
60786257,How to view samples from ImageDataGenerator(),"<p>I am currently working through a tutorial which is using the cifar10 images. I have written some fully working code which has the line <code>model.fit(x_train, y_train)</code> where x_train as a numpy array of dimension 50000x32x32x3 and dtype ""uint8"". I.e. it contains 50000 32x32 pixel colour images. I can display a sample of these images with calls to imshow() - it all looks and works fine.</p>

<p>But now in the next part of the tutorial it suggests that the model will generalise better if we use ImageDataGenerator() to create multiple warped (rotated, zoomed, sheered etc) versions of our training images. I want to better understand ImageDataGenerator() by displaying some of the warped images that are produced in the process. Looking at <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"" rel=""nofollow noreferrer"">the documentation</a>, it gives the following example:</p>

<pre><code># here's a more ""manual"" example
for e in range(epochs):
    print('Epoch', e)
    batches = 0
    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):
        model.fit(x_batch, y_batch)
        batches += 1
        if batches &gt;= len(x_train) / 32:
            # we need to break the loop by hand because
            # the generator loops indefinitely
            break
</code></pre>

<p>My current code (without warping) trains the model with the line <code>model.fit(x_train, y_train)</code>, so looking at the line in the example <code>model.fit(x_batch, y_batch)</code> I assume that x_batch must be a collection of 32 different warped versions of the current x_train image. I tried to write some code so that I could actually display the 32 images like so:</p>

<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

def print_array_info(v):
    print(""{} is of type {} with shape {} and dtype {}"".format(v,
                                                           eval(""type({})"".format(v)),
                                                           eval(""{}.shape"".format(v)),
                                                           eval(""{}.dtype"".format(v))
                                                           ))

def show_samples(array_of_images):
    n = array_of_images.shape[0]
    total_rows = 1+int((n-1)/5)
    total_columns = 5
    fig = plt.figure()
    gridspec_array = fig.add_gridspec(total_rows, total_columns)

    for i, img in enumerate(array_of_images):
        row = int(i/5)
        col = i % 5
        ax = fig.add_subplot(gridspec_array[row, col])
        ax.imshow(img)

    plt.show()


cifar_data = tf.keras.datasets.cifar10
(x_train, y_train), (x_test, y_test) = cifar_data.load_data()

data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20)

print_array_info(""x_train"")

batches = 0
batch_size=32

for x_batch, y_batch in data_generator.flow(x_train, y_train, batch_size=batch_size):
    print_array_info(""x_batch"")
    batches += 1
    if batches &gt;= len(x_train) / 32:
        break
    show_samples(x_batch[:batch_size])
</code></pre>

<p>I thought that the first time through the loop I would be shown 32 different warped versions of the zeroth image in x_train. But when I run this it produces almost blank images - I say almost because one or to of them may contain a few garbage looking pixels. I expected x_batch to be of size 32x32x32x3, i.e. a collection of 32 colour images of size 32x32pixels and 3 colours which indeed appears true but the dtype was float32 which confuses me - I thought the warping process would not change the dtype.</p>

<p>Have I got a bug in my code or have I misunderstood the documentation?</p>
","I am currently working through a tutorial which is using the cifar10 images. I have written some fully working code which has the line model.fit(x_train, y_train) where x_train as a numpy array of dimension 50000x32x32x3 and dtype ""uint8"". I.e. it contains 50000 32x32 pixel colour images. I can display a sample of these images with calls to imshow() - it all looks and works fine. But now in the next part of the tutorial it suggests that the model will generalise better if we use ImageDataGenerator() to create multiple warped (rotated, zoomed, sheered etc) versions of our training images. I want to better understand ImageDataGenerator() by displaying some of the warped images that are produced in the process. Looking at the documentation, it gives the following example: My current code (without warping) trains the model with the line model.fit(x_train, y_train), so looking at the line in the example model.fit(x_batch, y_batch) I assume that x_batch must be a collection of 32 different warped versions of the current x_train image. I tried to write some code so that I could actually display the 32 images like so: I thought that the first time through the loop I would be shown 32 different warped versions of the zeroth image in x_train. But when I run this it produces almost blank images - I say almost because one or to of them may contain a few garbage looking pixels. I expected x_batch to be of size 32x32x32x3, i.e. a collection of 32 colour images of size 32x32pixels and 3 colours which indeed appears true but the dtype was float32 which confuses me - I thought the warping process would not change the dtype. Have I got a bug in my code or have I misunderstood the documentation?",https://stackoverflow.com/questions/60786257,169774,Documentation Ambiguity
58525480,Padded_batch with pre- or post-padding option,"<p>I have a dataset of variable-length sequences (a tensorflow TFRecord dataset) to feed an LSTM network and I want to try and compare pre- and post-padding in the batches, but current padded_batch function only pads at the sequences end. I know that we have <code>tf.keras.preprocessing.sequence.pad_sequences</code> function in API but I don't know how to apply this function to dataset batch processor. The padded_batch function in tensorflow does both padding and batching, and it will find the required paddding size per batch dynamically. How can I implement this myself? My code right now is like this, and I am reading multiple TFRecord files and interleave them to make my mixed dataset:</p>

<pre><code>featuresDict = {'data': tf.FixedLenFeature([], dtype=tf.string),
                'rows': tf.FixedLenFeature([], dtype=tf.int64),
                'label': tf.FixedLenFeature([], dtype=tf.int64)
               }

def parse_tfrecord(example):
    features = tf.parse_single_example(example, featuresDict)
    label = tf.one_hot(features['label'],N)
    rows = features['rows']
    data = tf.decode_raw(features['data'], tf.int64)
    data = tf.reshape(data, (rows,num_features)
    return data, label

def read_datasets(pattern, numFiles, numEpochs=None, batchSize=None):
    files = tf.data.Dataset.list_files(pattern)

    def _parse(x):
        x = tf.data.TFRecordDataset(x, compression_type='GZIP')
        return x

    dataset = files.interleave(_parse, cycle_length=numFiles, block_length=1).map(parse_tfrecord)
    padded_shapes = (tf.TensorShape([None, num_features]), tf.TensorShape([N,])))
    dataset = dataset.padded_batch(batchSize, padded_shapes)
    dataset = dataset.prefetch(buffer_size=batchSize)
    dataset = dataset.repeat(numEpochs)
    return dataset
</code></pre>
","I have a dataset of variable-length sequences (a tensorflow TFRecord dataset) to feed an LSTM network and I want to try and compare pre- and post-padding in the batches, but current padded_batch function only pads at the sequences end. I know that we have tf.keras.preprocessing.sequence.pad_sequences function in API but I don't know how to apply this function to dataset batch processor. The padded_batch function in tensorflow does both padding and batching, and it will find the required paddding size per batch dynamically. How can I implement this myself? My code right now is like this, and I am reading multiple TFRecord files and interleave them to make my mixed dataset:",https://stackoverflow.com/questions/58525480,6450489,Documentation Ambiguity
59657166,Convert frozen model(.pb) to savedmodel,"<p>Recently I tried to convert the model (tf1.x) to the saved_model, and followed the official <a href=""https://www.tensorflow.org/guide/migrate"" rel=""noreferrer"">migrate document</a>. However in my use case, most of model in my hand or tensorflow model zoo usually is pb file, and according to the <a href=""https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt"" rel=""noreferrer"">official document</a> says that </p>

<blockquote>
  <p>There is no straightforward way to upgrade a raw Graph.pb file to TensorFlow 2.0, but if you have a ""Frozen graph"" (a tf.Graph where the variables have been turned into constants), then it is possible to convert this to a concrete_function using v1.wrap_function:</p>
</blockquote>

<p>But I still do not understand how to converted to <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""noreferrer"">saved_model format</a>.</p>
","Recently I tried to convert the model (tf1.x) to the saved_model, and followed the official migrate document. However in my use case, most of model in my hand or tensorflow model zoo usually is pb file, and according to the official document says that But I still do not understand how to converted to saved_model format.",https://stackoverflow.com/questions/59657166,2469488,Documentation Ambiguity
76447111,where is the documentation of keras.engine.sequential.Sequential?,"<pre><code>import tensorflow as tf
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])
print(type(model))
</code></pre>
<p>I got &lt;class 'keras.engine.sequential.Sequential'&gt;.</p>
<p>I need documentation of keras.engine.sequential.Sequential, but can't locate it.</p>
","I got &lt;class 'keras.engine.sequential.Sequential'&gt;. I need documentation of keras.engine.sequential.Sequential, but can't locate it.",https://stackoverflow.com/questions/76447111,3646484,Documentation Completeness
56976078,How do i load images dataset using tf.keras.utils.get_file,"<p>I;m working with cifar-10 dataset and i need the dataset available publicly, so i pushed it to gitlab. i want to load this dataset in my code, after some digging i found an example where they used tf.keras.utils.get_file() which looked perfect but when i try to load my dataset i get a NotADirectoryError. but it loads just fine with the example i found online which is confusing, can someone please explain why it wouldn't work for my dataset?</p>
<p>here's the example i found that works, the is_dir() returns true</p>
<pre><code>import pathlib

data_root_orig = tf.keras.utils.get_file(
'flower_photos','https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',untar=True)
    data_root = pathlib.Path(data_root_orig)
    print(data_root.is_dir()
)

</code></pre>
<p>here's my dataset I'm trying to load. Initially throws train_data is not a directory, when i try again it seems to work but is_dir is false and i'm unable to get to the files in my dataset</p>
<pre><code>import pathlib
import tensorflow as tf
data_root_orig = tf.keras.utils.get_file('train',
                                         'https://gitlab.com/StephenAI/osato-file/raw/master/train.zip',
                                        untar=True, archive_format='zip')
data_root = pathlib.Path(data_root_orig)
print(data_root, type(data_root),data_root.is_dir())
</code></pre>
","I;m working with cifar-10 dataset and i need the dataset available publicly, so i pushed it to gitlab. i want to load this dataset in my code, after some digging i found an example where they used tf.keras.utils.get_file() which looked perfect but when i try to load my dataset i get a NotADirectoryError. but it loads just fine with the example i found online which is confusing, can someone please explain why it wouldn't work for my dataset? here's the example i found that works, the is_dir() returns true here's my dataset I'm trying to load. Initially throws train_data is not a directory, when i try again it seems to work but is_dir is false and i'm unable to get to the files in my dataset",https://stackoverflow.com/questions/56976078,3810448,Documentation Replication on Other Examples
75338588,Correct way to pass a set of images to a model for training,"<p>I'm trying to create a Keras model to train with a group of images, taken from a list of paths.
I know that the method <code>tf.keras.utils.image_dataset_from_directory</code> exists but it doesn't meet my needs because I want to learn the correct way to handle images and because I need to make a regression, not a classification.
Every approach I tried failed one way or another, mostly because the type of the x_train variable is wrong.</p>
<p>The most promising function I used to load a single image is:</p>
<pre><code>def encode_image(img_path):
  img = tf.keras.preprocessing.image.load_img(img_path)
  img_array = tf.keras.preprocessing.image.img_to_array(img)
  img_array = tf.expand_dims(img_array, 0)
  return img_array

x_train = df['filename'].apply(lambda i: encode_image(i))
</code></pre>
<p>This doesn't work because, when I call the .fit() method this way:</p>
<pre><code>history = model.fit(x_train, y_train, epochs=1)
</code></pre>
<p>I receive the following error:</p>
<pre><code>Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)
</code></pre>
<p>This makes me understand that I'm passing the data in a wrong format.
Can someone provide me a <strong>basic example</strong> of creating a (x_train, y_train) pair to feed a model for training using a set of images?
Thank you very much</p>
","I'm trying to create a Keras model to train with a group of images, taken from a list of paths. I know that the method tf.keras.utils.image_dataset_from_directory exists but it doesn't meet my needs because I want to learn the correct way to handle images and because I need to make a regression, not a classification. Every approach I tried failed one way or another, mostly because the type of the x_train variable is wrong. The most promising function I used to load a single image is: This doesn't work because, when I call the .fit() method this way: I receive the following error: This makes me understand that I'm passing the data in a wrong format. Can someone provide me a basic example of creating a (x_train, y_train) pair to feed a model for training using a set of images? Thank you very much",https://stackoverflow.com/questions/75338588,4795403,Requesting (Additional) Resources
75698781,Keras Migration issues ImageDataGenerator to image_dataset_from_directory,"<p>It appears that <code>ImageDataGenerator</code> has been deprecated <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"" rel=""nofollow noreferrer"">after TF 2.10</a>. Thus, I wanted to change my code to avoid using this class and use pre-processing layers to do the augmentation.</p>
<p>The recommendation on TF Documentation is to use <code>image_dataset_from_directory</code>. But I'm having problems with this class as now I get a lot of errors about the type of file.</p>
<p>I'm using the Kaggle Cats-vs-Dogs dataset to test this. Here is the code using the <code>ImageDataGenerator</code>:</p>
<pre><code>val_data_gen = ImageDataGenerator(rescale=1/255.0)
train_data_gen = ImageDataGenerator(rescale=1/255.0,
                                    rotation_range=15,
                                    width_shift_range=0.1,
                                    height_shift_range=0.1,
                                    shear_range=0.1,
                                    zoom_range=0.1,
                                    horizontal_flip=True,
                                    fill_mode='nearest')

train_generator = train_data_gen.flow_from_directory(
    directory=train_dir,
    batch_size=128,
    class_mode='binary',
    target_size=(150, 150))

val_generator = val_data_gen.flow_from_directory(
    directory=val_dir,
    batch_size=128,
    class_mode='binary',
    target_size=(150, 150))
</code></pre>
<p>For the approach using <code>image_dataset_from_directory</code>, I create the preprocessing layers like:</p>
<pre><code>augmentation = keras.Sequential(
[
    layers.Resizing(150, 150),
    layers.Rescaling(1./255),
    layers.RandomFlip(&quot;horizontal&quot;),
    layers.RandomRotation(0.05, fill_mode='nearest'),
    layers.RandomZoom(0.1),
    layers.RandomContrast(0.5),
])
</code></pre>
<p>I add them to my model using:</p>
<pre><code>model.add(layers.InputLayer(input_shape=(300, 300, 3)))
model.add(augmentation)
</code></pre>
<p>And then I create the datasets using:</p>
<pre><code>train_ds = tf.keras.utils.image_dataset_from_directory(
TRAIN_DIR,
image_size=(150, 150),
label_mode='binary',
shuffle=True,
seed=101,
batch_size=128
)
</code></pre>
<p>Same thing goes for the validation set. However with <code>image_dataset_from_directory</code> I get:</p>
<pre><code>Unknown image file format. One of JPEG, PNG, GIF, BMP required.
 [[{{node decode_image/DecodeImage}}]]
 [[IteratorGetNext]] [Op:__inference_train_function_6414]
</code></pre>
<p><code>ImageDataGenerator</code> works without any issue. Is this a bug?</p>
","It appears that ImageDataGenerator has been deprecated after TF 2.10. Thus, I wanted to change my code to avoid using this class and use pre-processing layers to do the augmentation. The recommendation on TF Documentation is to use image_dataset_from_directory. But I'm having problems with this class as now I get a lot of errors about the type of file. I'm using the Kaggle Cats-vs-Dogs dataset to test this. Here is the code using the ImageDataGenerator: For the approach using image_dataset_from_directory, I create the preprocessing layers like: I add them to my model using: And then I create the datasets using: Same thing goes for the validation set. However with image_dataset_from_directory I get: ImageDataGenerator works without any issue. Is this a bug?",https://stackoverflow.com/questions/75698781,7056614,Documentation Replication on Other Examples
42981493,Weights and biases in tf.layers module in TensorFlow 1.0,"<p>How do you access the weights and biases when using tf.layers module in TensorFlow 1.0? The advantage of tf.layers module is that you don't have to separately create the variables when making a fully connected layer or convolution layer. </p>

<p>I couldn't not find anything in the documentation regarding accessing them or adding them in summaries after they are created.</p>
",How do you access the weights and biases when using tf.layers module in TensorFlow 1.0? The advantage of tf.layers module is that you don't have to separately create the variables when making a fully connected layer or convolution layer. I couldn't not find anything in the documentation regarding accessing them or adding them in summaries after they are created.,https://stackoverflow.com/questions/42981493,7656080,Lack of Alternative Solutions/Documentation
51058476,why the parameter 'scale' of tf.layers.batch_normalization is disabled when next layer is relu?,"<p>In the tensorflow documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">tf.layers.batch_normalization</a>,it is said"" <strong>When the next layer is linear (also e.g. nn.relu), this(the parameter of 'scale' ) can be disabled since the scaling can be done by the next layer.</strong>"" ? It seems wrong because when the next layer is nn.relu, the linear coefficient is an invariant constant(1), and the value won't be sacled.</p>
","In the tensorflow documentation of tf.layers.batch_normalization,it is said"" When the next layer is linear (also e.g. nn.relu), this(the parameter of 'scale' ) can be disabled since the scaling can be done by the next layer."" ? It seems wrong because when the next layer is nn.relu, the linear coefficient is an invariant constant(1), and the value won't be sacled.",https://stackoverflow.com/questions/51058476,9155944,Documentation Ambiguity
48481873,how to set stride to zero when using tf.layers.conv2d,"<p>is there a way that I can turn off stride in tensor flow when using: tf.layers.conv2d()? According to the docs, the default is (1,1) but when I try to change this to (0,0) I get an error telling me that it has to be a positive number.</p>

<p>Thanks.</p>
","is there a way that I can turn off stride in tensor flow when using: tf.layers.conv2d()? According to the docs, the default is (1,1) but when I try to change this to (0,0) I get an error telling me that it has to be a positive number. Thanks.",https://stackoverflow.com/questions/48481873,9278090,Inadequate Examples
46221420,How to use TensorFlow Dataset API in combination with dense layers,"<p>I am trying out the Dataset API for my input pipeline shown in the <a href=""https://www.tensorflow.org/versions/r1.3/programmers_guide/datasets#using_high-level_apis"" rel=""nofollow noreferrer"">TensorFlow documentation</a> and use almost the same code:</p>

<pre><code>tr_data = Dataset.from_tensor_slices((train_images, train_labels))
tr_data = tr_data.map(input_parser, NUM_CORES, output_buffer_size=2000)
tr_data = tr_data.batch(BATCH_SIZE)
tr_data = tr_data.repeat(EPOCHS)

iterator = dataset.make_one_shot_iterator()
next_example, next_label = iterator.get_next()

# Script throws error here
loss = model_function(next_example, next_label)

with tf.Session(...) as sess:
    sess.run(tf.global_variables_initializer())

     while True:
        try:
            train_loss = sess.run(loss)
        except tf.errors.OutOfRangeError:
            print(""End of training dataset."")
            break
</code></pre>

<p>This should be faster since it avoids using the slow feed_dicts. But I can't make it work with my model, which is a simplified LeNet architecture. The <strong>problem</strong> is the <code>tf.layers.dense</code> in my <code>model_function()</code> which expects an known input shape (I guess because it has to know the number of weights beforehand). But <code>next_example</code> and <code>next_label</code> only get their shape by running them in the session. Before evaluating them their shape is just undefined <code>?</code></p>

<p>Declaring the <code>model_function()</code> throws this error:</p>

<blockquote>
  <p>ValueError: The last dimension of the inputs to <code>Dense</code> should be
  defined. Found <code>None</code>.</p>
</blockquote>

<p>Right now, I don't know if I am using this Dataset API in the intended way or if there is a workaround.</p>

<p>Thanks in advance!</p>

<p><strong>Edit 1:</strong> 
Below is my model and it throws the error at the first dense layer</p>

<pre><code>def conv_relu(input, kernel_shape):
    # Create variable named ""weights"".
    weights = tf.get_variable(""weights"", kernel_shape,
        initializer=tf.random_normal_initializer())
    # Create variable named ""biases"".
    biases = tf.get_variable(""biases"", kernel_shape[3],
        initializer=tf.constant_initializer(0.0))
    conv = tf.nn.conv2d(input, weights,
        strides=[1, 1, 1, 1], padding='VALID')
    return tf.nn.relu(conv + biases)

def fully(input, output_dim):
    assert len(input.get_shape())==2, 'Wrong input shape, need flattened tensor as input'
    input_dim = input.get_shape()[1]

    weight = tf.get_variable(""weight"", [input_dim, output_dim],
        initializer=tf.random_normal_initializer())
    bias = tf.get_variable('bias', [output_dim],
        initializer=tf.random_normal_initializer())

    fully = tf.nn.bias_add(tf.matmul(input, weight), bias)
    return fully


def simple_model(x):

    with tf.variable_scope('conv1'):
        conv1 = conv_relu(x, [3,3,1,10])
        conv1 = tf.nn.max_pool(conv1,[1,2,2,1],[1,2,2,1],'SAME')

    with tf.variable_scope('conv2'):
        conv2 = conv_relu(conv1, [3,3,10,10])
        conv2 = tf.nn.max_pool(conv2,[1,2,2,1],[1,2,2,1],'SAME')

    with tf.variable_scope('conv3'):
        conv3 = conv_relu(conv2, [3,3,10,10])
        conv3 = tf.nn.max_pool(conv3,[1,2,2,1],[1,2,2,1],'SAME')

    flat = tf.contrib.layers.flatten(conv3)
    with tf.variable_scope('fully1'):
        fully1 = tf.layers.dense(flat, 1000)
        fully1 = tf.nn.relu(fully1)

    with tf.variable_scope('fully2'):
        fully2 = tf.layers.dense(fully1, 100)
        fully2 = tf.nn.relu(fully2)

    with tf.variable_scope('output'):
        output = tf.layers.dense(fully2, 4)
        fully1 = tf.nn.relu(output)


    return output
</code></pre>

<p><strong>Edit 2:</strong>  </p>

<p>Here you see the print of the tensors. Notice that next_example does not have a shape</p>

<blockquote>
  <p>next_example: Tensor(""IteratorGetNext:0"", dtype=float32)<br>
  next_label: Tensor(""IteratorGetNext:1"", shape=(?, 4), dtype=float32)</p>
</blockquote>
","I am trying out the Dataset API for my input pipeline shown in the TensorFlow documentation and use almost the same code: This should be faster since it avoids using the slow feed_dicts. But I can't make it work with my model, which is a simplified LeNet architecture. The problem is the tf.layers.dense in my model_function() which expects an known input shape (I guess because it has to know the number of weights beforehand). But next_example and next_label only get their shape by running them in the session. Before evaluating them their shape is just undefined ? Declaring the model_function() throws this error: Right now, I don't know if I am using this Dataset API in the intended way or if there is a workaround. Thanks in advance! Edit 1: Below is my model and it throws the error at the first dense layer Edit 2: Here you see the print of the tensors. Notice that next_example does not have a shape",https://stackoverflow.com/questions/46221420,6661139,Documentation Replication on Other Examples
59262869,Kernel Constraint usage in Tensorflow v1.14,"<p>I am implementing a custom dense layer with weights of dimension <code>12x12</code> in which not all the neurons from one layer are connected to another layer. So I have defined a projection matrix like below:</p>

<pre class=""lang-py prettyprint-override""><code>projection_matrix = np.zeros((12, 12))
connections = [[2, 6, 9], [4, 7, 10], [0, 6, 9], [5, 8, 11], [1, 7, 10], [3, 8, 11], [0, 2, 9], [1, 4, 10],
               [3, 5, 11], [0, 2, 6], [1, 4, 7], [3, 5, 8]]
for i, connection in zip(range(projection_matrix.shape[0]), connections):
    for j in connection:
        projection_matrix[i, j] = 1
</code></pre>

<p>And then the idea is to multiply the weight matrix with this projection matrix:</p>

<pre class=""lang-py prettyprint-override""><code>new_weight_matrix = np.multiply(weight_matrix, projection_matrix)  # Might as well be tf.multiply
</code></pre>

<p>I was going through the documentation of <code>tf.layers.dense</code> from <a href=""https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/layers/dense"" rel=""nofollow noreferrer"">here</a>. There is a parameter called <code>kernel_constraint</code>, whose description reads:</p>

<blockquote>
  <p>An optional projection function to be applied to the bias after being updated by an Optimizer.</p>
</blockquote>

<p>My question is, does passing the <code>projection_matrix</code> to this parameter (<code>kernel_constraint</code>) achieve what I intend to achieve (connect only specific neurons defined by <code>projection_matrix</code>)?</p>
","I am implementing a custom dense layer with weights of dimension 12x12 in which not all the neurons from one layer are connected to another layer. So I have defined a projection matrix like below: And then the idea is to multiply the weight matrix with this projection matrix: I was going through the documentation of tf.layers.dense from here. There is a parameter called kernel_constraint, whose description reads: My question is, does passing the projection_matrix to this parameter (kernel_constraint) achieve what I intend to achieve (connect only specific neurons defined by projection_matrix)?",https://stackoverflow.com/questions/59262869,6997665,Documentation Ambiguity
53194918,Tensorflow tf.layers Dense Neural Net function vs. Class Interface,"<p>I am trying to implement a helper class to create a standard Feedforward Neural network in python.
</p>

<p>Since I want the class to be general, there is a method called addHiddenLayer() which should append layers to the Flow Graph.
</p>

<p>To add layers to the flow graph I went through the tf.layers module which provides two options <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/dense"" rel=""nofollow noreferrer"">tf.layers.dense</a> : A function which returns an object which can act as the input to the next layer.
</p>

<p>There is also <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/Dense"" rel=""nofollow noreferrer"">tf.layers.Dense</a> : A class which has almost identical attributes as the parameters of tf.layers.dense(), and implements essentially the same operation on the inputs.
</p>

<p>After going through the documentation for both, I fail to see any extra functionality added by using the class version. I think the function implementation should suffice for my use case the skeleton for which is given below.</p>

<pre><code>class myNeuralNet:
def __init__(self, dim_input_data, dim_output_data): 
    #Member variable for dimension of Input Data Vectors (Number of features...)
    self.dim_input_data = dim_input_data
    #Variable for dimension of output labels
    self.dim_output_data = dim_output_data
    #TF Placeholder for input data   
    self.x =  tf.placeholder(tf.float32, [None, 784])
    #TF Placeholder for labels 
    self.y_ = tf.placeholder(tf.float32, [None, 10])
    #Container to store all the layers of the network 
    #Containter to hold layers of NN
    self.layer_list = []

def addHiddenLayer(self, layer_dim, activation_fn=None, regularizer_fn=None):
    # Add a layer to the network of layer_dim
    # append the new layer to the container of layers 
    pass

def addFinalLayer(self, activation_fn=None, regularizer_fn=None):
    pass

def setup_training(self, learn_rate):
    # Define loss, you might want to store it as self.loss
    # Define the train step as self.train_step = ..., use an optimizer from tf.train and call minimize(self.loss)
    pass

def setup_metrics(self):
    # Use the predicted labels and compare them with the input labels(placeholder defined in __init__)
    # to calculate accuracy, and store it as self.accuracy
    pass

# add other arguments to this function as given below
def train(self, sess, max_epochs, batch_size, train_size, print_step = 100):                
    pass
</code></pre>

<p>Can someone give an example of a situation where the class version would be required? 
References:</p>

<p><a href=""https://stackoverflow.com/questions/50029121/how-to-use-tf-layers-classes-instead-of-functions"">Related question</a> on SO</p>

<p><a href=""https://gist.github.com/koaning/c26b2dd5c2bdeaf6d7479a68bd7023bb"" rel=""nofollow noreferrer"">Example</a> of function usage </p>
","I am trying to implement a helper class to create a standard Feedforward Neural network in python. Since I want the class to be general, there is a method called addHiddenLayer() which should append layers to the Flow Graph. To add layers to the flow graph I went through the tf.layers module which provides two options tf.layers.dense : A function which returns an object which can act as the input to the next layer. There is also tf.layers.Dense : A class which has almost identical attributes as the parameters of tf.layers.dense(), and implements essentially the same operation on the inputs. After going through the documentation for both, I fail to see any extra functionality added by using the class version. I think the function implementation should suffice for my use case the skeleton for which is given below. Can someone give an example of a situation where the class version would be required? References: Related question on SO Example of function usage",https://stackoverflow.com/questions/53194918,3642162,Requesting (Additional) Resources
59231031,Tensorflow: creating a diagonal matrix with input on the sub/superdiagonals,"<p>I have the following code:</p>

<pre><code>import tensorflow as tf

N = 10
X = tf.ones([N,], dtype=tf.float64)
D = tf.linalg.diag(X, k=1, num_rows=N+1, num_cols=N+1)

print(D)
</code></pre>

<p>which, based on the <a href=""https://www.tensorflow.org/api_docs/python/tf/linalg/diag"" rel=""nofollow noreferrer"">TF2 documentation</a>, I expect to return an 11x11 tensor with X inserted on the first superdiagonal (even without the optional <code>num_rows</code> and <code>num_cols</code> arguments). However, the result is</p>

<pre><code>tf.Tensor(
[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]], shape=(10, 10), dtype=float64)
</code></pre>

<p>Is there something obvious that I am missing?</p>
","I have the following code: which, based on the TF2 documentation, I expect to return an 11x11 tensor with X inserted on the first superdiagonal (even without the optional num_rows and num_cols arguments). However, the result is Is there something obvious that I am missing?",https://stackoverflow.com/questions/59231031,1799323,Documentation Replication on Other Examples
54162343,"tensorflow: object has no attribute 'matrix_inverse', how to verify it should be supported?","<p>I am running the following code on a server with a gpu, using tensorflow-gpu version=1.2:</p>

<blockquote>
  <p>matrix_b = tf.matrix_inverse(matrix_a)</p>
</blockquote>

<p>When running the same code on my laptop (no gpu) with tensorflow version=1.8 it works.</p>

<p>As I see in the <a href=""https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/matrix_inverse"" rel=""nofollow noreferrer"">documentation</a>, this is implemented for the tensorflow version I am using, so it should be supported. yet I get the following error:</p>

<blockquote>
  <p>AttributeError: 'module' object has no attribute 'matrix_inverse'</p>
</blockquote>

<p>so to my question - Can it be that the matrix_inverse() is not supported in tensorflow-gpu 1.2 but is supported in tensorflow 1.2?
and if so, where can I see the correct documentation?</p>
","I am running the following code on a server with a gpu, using tensorflow-gpu version=1.2: When running the same code on my laptop (no gpu) with tensorflow version=1.8 it works. As I see in the documentation, this is implemented for the tensorflow version I am using, so it should be supported. yet I get the following error: so to my question - Can it be that the matrix_inverse() is not supported in tensorflow-gpu 1.2 but is supported in tensorflow 1.2? and if so, where can I see the correct documentation?",https://stackoverflow.com/questions/54162343,9610896,Requesting (Additional) Resources
44231072,Possible tensorflow cholesky_solve inconsistency?,"<p>I am trying to solve a linear system of equations using <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/matrix_math_functions#cholesky_solve"" rel=""nofollow noreferrer"">tensorflow.cholesky_solve</a> and I'm getting some unexpected results.</p>

<p>I wrote a script to compare the output of a very simple linear system with simple matrix inversion a la <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/matrix_math_functions#matrix_inverse"" rel=""nofollow noreferrer"">tensorflow.matrix_inverse</a>, the non-cholesky based matrix equation solver <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/matrix_math_functions#matrix_solve"" rel=""nofollow noreferrer"">tensorflow.matrix_solve</a>, and <code>tensorflow.cholesky_solve</code>. </p>

<p>According to my understanding of the docs I've linked, these three cases should all yield a solution of the identity matrix divided by 2, but this is not the case for <code>tensorflow.cholesky_solve</code>. Perhaps I'm misunderstanding the docs? </p>

<pre><code>import tensorflow as tf

I = tf.eye(2, dtype=tf.float32)
X = 2 * tf.eye(2, dtype=tf.float32)
X_inv = tf.matrix_inverse(X)
X_solve = tf.matrix_solve(X, I)
X_chol_solve = tf.cholesky_solve(tf.cholesky(X), I)

with tf.Session() as sess:
    for x in [X_inv, X_solve, X_chol_solve]:
        print('{}:\n{}'.format(x.name, sess.run(x)))
        print
</code></pre>

<p>yielding output:</p>

<pre><code>MatrixInverse:0:
[[ 0.5  0. ]
 [ 0.   0.5]]

MatrixSolve:0:
[[ 0.5  0. ]
 [ 0.   0.5]]

cholesky_solve/MatrixTriangularSolve_1:0:
[[ 1.  0.]
 [ 0.  1.]]


Process finished with exit code 0
</code></pre>
","I am trying to solve a linear system of equations using tensorflow.cholesky_solve and I'm getting some unexpected results. I wrote a script to compare the output of a very simple linear system with simple matrix inversion a la tensorflow.matrix_inverse, the non-cholesky based matrix equation solver tensorflow.matrix_solve, and tensorflow.cholesky_solve. According to my understanding of the docs I've linked, these three cases should all yield a solution of the identity matrix divided by 2, but this is not the case for tensorflow.cholesky_solve. Perhaps I'm misunderstanding the docs? yielding output:",https://stackoverflow.com/questions/44231072,2467355,Documentation Ambiguity
48174988,Tensorflow: how to create a local variable?,"<p>I'm trying to understand how local and global variables are different in tensorflow and what's the right way to initialize the variables.</p>

<p>According to the doc, <code>tf.local_variables_initializer</code>:</p>

<blockquote>
  <p>Returns an Op that initializes all local variables.</p>
  
  <p>This is just a shortcut for variables_initializer(local_variables())</p>
</blockquote>

<p>So the essential part is <code>tf.local_variables</code>. <a href=""https://www.tensorflow.org/api_docs/python/tf/local_variables"" rel=""nofollow noreferrer"">The doc</a>:</p>

<blockquote>
  <p>Local variables - per process variables, usually not saved/restored to checkpoint and used for temporary or intermediate values. For example, they can be used as counters for metrics computation or number of epochs this machine has read data.</p>
</blockquote>

<p>It sounds logical, however, no matter how I tried, I couldn't make any variable local. </p>

<pre><code>features = 2
hidden = 3

with tf.variable_scope('start'):
  x = tf.placeholder(tf.float32, shape=[None, features], name='x')
  y = tf.placeholder(tf.float32, shape=[None], name='y')

with tf.variable_scope('linear'):
  W = tf.get_variable(name='W', shape=[features, hidden])
  b = tf.get_variable(name='b', shape=[hidden], initializer=tf.zeros_initializer)
  z = tf.matmul(x, W) + b

with tf.variable_scope('optimizer'):
  predict = tf.reduce_sum(z, axis=1)
  loss = tf.reduce_mean(tf.square(y - predict))
  optimizer = tf.train.AdamOptimizer(0.1).minimize(loss)

print(tf.local_variables())
</code></pre>

<p>The output is always an empty list. <strong>How</strong> and <strong>should</strong> I create local variables?</p>
","I'm trying to understand how local and global variables are different in tensorflow and what's the right way to initialize the variables. According to the doc, tf.local_variables_initializer: So the essential part is tf.local_variables. The doc: It sounds logical, however, no matter how I tried, I couldn't make any variable local. The output is always an empty list. How and should I create local variables?",https://stackoverflow.com/questions/48174988,9127536,Requesting (Additional) Resources
55063120,can anyone give a tiny example to explain the params of tf.random.categorical?,"<p>tensorflow's site gives this example</p>

<pre><code>tf.random.categorical(tf.log([[10., 10.]]), 5)
</code></pre>

<p>produces a tensor that ""has shape [1, 5], where each value is either 0 or 1 with equal probability""</p>

<p>I have already known, the basic <a href=""https://www.tensorflow.org/api_docs/python/tf/random/categorical"" rel=""noreferrer"">demo</a>, the meaning of <code>tf.log([[10., 10.]])</code>.</p>

<p>what I want to know is what does [batch_size, num_classes] do, can anyone give a tiny example to explain the params?</p>
","tensorflow's site gives this example produces a tensor that ""has shape [1, 5], where each value is either 0 or 1 with equal probability"" I have already known, the basic demo, the meaning of tf.log([[10., 10.]]). what I want to know is what does [batch_size, num_classes] do, can anyone give a tiny example to explain the params?",https://stackoverflow.com/questions/55063120,,Requesting (Additional) Resources
46219326,How to use tf.losses.log_loss in tensorflow?,"<p>What is the input of tf.losses.log_loss ?</p>

<pre><code>cross_entropy = tf.losses.log_loss(labels, predictions)
</code></pre>

<p>Anybody can show me some examples about it ?
Official guide has no examples .</p>
",What is the input of tf.losses.log_loss ? Anybody can show me some examples about it ? Official guide has no examples .,https://stackoverflow.com/questions/46219326,6407393,Inadequate Examples
48682703,How to set parameter weights in tf.losses.sigmoid_cross_entropy?,"<p>I'm now trying to use <code>tf.losses.sigmoid_cross_entropy</code> on an unbalanced dataset. However, I'm a little confused on the parameter weights. Here are the comments in the documentation:</p>

<blockquote>
  <p>weights: Optional Tensor whose rank is either 0, or the same rank as
  labels, and must be broadcastable to labels (i.e., all dimensions must
  be either 1, or the same as the corresponding losses dimension).</p>
</blockquote>

<p>I know in <code>tf.losses.softmax_cross_entropy</code> the parameter weights can be a rank 1 tensor with weight for each sample. Why must the weights in <code>tf.losses.sigmoid_cross_entropy</code> have the same rank as labels? </p>

<p>Can anybody answer me? Better with an example.</p>
","I'm now trying to use tf.losses.sigmoid_cross_entropy on an unbalanced dataset. However, I'm a little confused on the parameter weights. Here are the comments in the documentation: I know in tf.losses.softmax_cross_entropy the parameter weights can be a rank 1 tensor with weight for each sample. Why must the weights in tf.losses.sigmoid_cross_entropy have the same rank as labels? Can anybody answer me? Better with an example.",https://stackoverflow.com/questions/48682703,2066987,Documentation Ambiguity
56639621,how to use tensorflow tf.losses.softmax_cross_entropy?,"<p>I am doing some semantic segmentation problem and need to define loss function.</p>

<p>Does any one know how to use tensorflow ""tf.losses.softmax_cross_entropy""?</p>

<p>It is said in the documentation that the first input of the function is 
onehot_labels, so do we need to first transfer the pixel-wise class label into one hot encode format and input one hot encode into this function?</p>

<p>Or we can directly input the pixel class label like tf.losses.sigmoid_cross_entropy in this post <a href=""https://stackoverflow.com/questions/52046971/sigmoid-cross-entropy-loss-function-from-tensorflow-for-image-segmentation"">sigmoid_cross_entropy loss function from tensorflow for image segmentation</a>?</p>

<p>Thank you so much!</p>
","I am doing some semantic segmentation problem and need to define loss function. Does any one know how to use tensorflow ""tf.losses.softmax_cross_entropy""? It is said in the documentation that the first input of the function is onehot_labels, so do we need to first transfer the pixel-wise class label into one hot encode format and input one hot encode into this function? Or we can directly input the pixel class label like tf.losses.sigmoid_cross_entropy in this post sigmoid_cross_entropy loss function from tensorflow for image segmentation? Thank you so much!",https://stackoverflow.com/questions/56639621,9357193,Documentation Ambiguity
63869134,Converting TensorFlow tensor into Numpy array,"<h1>Problem Description</h1>
<p>I am trying to write a custom loss function in TensorFlow 2.3.0. To calculate the loss, I need the <code>y_pred</code> parameter to be converted to a numpy array. However, I can't find a way to convert it from <code>&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;</code> to numpy array, even though there seem to TensorFlow functions to do so.</p>
<h1>Code Example</h1>
<pre><code>def custom_loss(y_true, y_pred):
    print(type(y_pred))
    npa = y_pred.make_ndarray()
    ...
    

if __name__ == '__main__':
    ...
    model.compile(loss=custom_loss, optimizer=&quot;adam&quot;)
    model.fit(x=train_data, y=train_data, epochs=10)
</code></pre>
<p>gives the error message: <code>AttributeError: 'Tensor' object has no attribute 'make_ndarray</code>
after printing the type of the <code>y_pred</code> parameter: <code>&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;</code></p>
<h1>What I have tried so far</h1>
<p>Looking for a solution I found this seems to be a common issue and there a couple of suggestions, but they did not work for me so far:</p>
<p><strong>1. &quot; ... so just call .numpy() on the Tensor object.&quot;: <a href=""https://www.thetopsites.net/article/51302819.shtml"" rel=""noreferrer"">How can I convert a tensor into a numpy array in TensorFlow?</a></strong></p>
<p>so I tried:</p>
<pre><code>def custom_loss(y_true, y_pred):
    npa = y_pred.numpy()
    ...
</code></pre>
<p>giving me <code>AttributeError: 'Tensor' object has no attribute 'numpy'</code></p>
<p><strong>2. &quot;Use tensorflow.Tensor.eval() to convert a tensor to an array&quot;: <a href=""https://www.kite.com/python/answers/how-to-convert-a-tensorflow-tensor-to-a-numpy-array-in-python"" rel=""noreferrer"">How to convert a TensorFlow tensor to a NumPy array in Python</a></strong></p>
<p>so I tried:</p>
<pre><code>def custom_loss(y_true, y_pred):
    npa = y_pred.eval(session=tf.compat.v1.Session())
    ...
</code></pre>
<p>giving me one of the longest trace of error messages I ever have seen with the core being:</p>
<pre><code>InvalidArgumentError: 2 root error(s) found.
      (0) Invalid argument: You must feed a value for placeholder tensor 'functional_1/conv2d_2/BiasAdd/ReadVariableOp/resource' with dtype resource
         [[node functional_1/conv2d_2/BiasAdd/ReadVariableOp/resource (defined at main.py:303) ]]
         [[functional_1/cropping2d/strided_slice/_1]]
      (1) Invalid argument: You must feed a value for placeholder tensor 'functional_1/conv2d_2/BiasAdd/ReadVariableOp/resource' with dtype resource
         [[node functional_1/conv2d_2/BiasAdd/ReadVariableOp/resource (defined at main.py:303) ]]
</code></pre>
<p>also having to call TensorFlow Compatibility Functions from Version 1.x does not feel very future-proof, so I do not like this approach too much anyhow.</p>
<p><strong>3. Looking at the TensorFlow Docs there seemed to be the function I needed just waiting: <a href=""https://www.tensorflow.org/api_docs/python/tf/make_ndarray"" rel=""noreferrer"">tf.make_ndarray</a> Create a numpy ndarray from a tensor.</strong></p>
<p>so I tried:</p>
<pre><code>def custom_loss(y_true, y_pred):
    npa = tf.make_ndarray(y_pred)
    ...
</code></pre>
<p>giving me <code>AttributeError: 'Tensor' object has no attribute 'tensor_shape'</code></p>
<p>Looking at the example in the TF documentation they use this on a proto_tensor, so I tried converting to a proto first:</p>
<pre><code>def custom_loss(y_true, y_pred):
    proto_tensor = tf.make_tensor_proto(y_pred)
    npa = tf.make_ndarray(proto_tensor)
    ...
</code></pre>
<p>but already the <code>tf.make_tensor_proto(y_pred)</code> raises the error: <code>TypeError: Expected any non-tensor type, got a tensor instead.</code></p>
<p>Also trying to make a const tensor first gives the same error:</p>
<pre><code>def custom_loss(y_true, y_pred):
    a = tf.constant(y_pred)
    proto_tensor = tf.make_tensor_proto(a)
    npa = tf.make_ndarray(proto_tensor)
    ...
</code></pre>
<p>There are many more posts around this but it seems they are all coming back to these three basic ideas. Looking forward to your suggestions!</p>
","I am trying to write a custom loss function in TensorFlow 2.3.0. To calculate the loss, I need the y_pred parameter to be converted to a numpy array. However, I can't find a way to convert it from &lt;class 'tensorflow.python.framework.ops.Tensor'&gt; to numpy array, even though there seem to TensorFlow functions to do so. gives the error message: AttributeError: 'Tensor' object has no attribute 'make_ndarray after printing the type of the y_pred parameter: &lt;class 'tensorflow.python.framework.ops.Tensor'&gt; Looking for a solution I found this seems to be a common issue and there a couple of suggestions, but they did not work for me so far: 1. "" ... so just call .numpy() on the Tensor object."": How can I convert a tensor into a numpy array in TensorFlow? so I tried: giving me AttributeError: 'Tensor' object has no attribute 'numpy' 2. ""Use tensorflow.Tensor.eval() to convert a tensor to an array"": How to convert a TensorFlow tensor to a NumPy array in Python so I tried: giving me one of the longest trace of error messages I ever have seen with the core being: also having to call TensorFlow Compatibility Functions from Version 1.x does not feel very future-proof, so I do not like this approach too much anyhow. 3. Looking at the TensorFlow Docs there seemed to be the function I needed just waiting: tf.make_ndarray Create a numpy ndarray from a tensor. so I tried: giving me AttributeError: 'Tensor' object has no attribute 'tensor_shape' Looking at the example in the TF documentation they use this on a proto_tensor, so I tried converting to a proto first: but already the tf.make_tensor_proto(y_pred) raises the error: TypeError: Expected any non-tensor type, got a tensor instead. Also trying to make a const tensor first gives the same error: There are many more posts around this but it seems they are all coming back to these three basic ideas. Looking forward to your suggestions!",https://stackoverflow.com/questions/63869134,12207268,Inadequate Examples
48976538,Tensorflow AttributeError: 'module' object has no attribute 'manip',"<p>I try to roll a tensor and in the Tensorflow documentation i found a function called tf.manip.roll() but when i use it i get the error message:</p>

<p>AttributeError: 'module' object has no attribute 'manip'</p>

<p>Has someone an idea where this function has moved?</p>
",I try to roll a tensor and in the Tensorflow documentation i found a function called tf.manip.roll() but when i use it i get the error message: AttributeError: 'module' object has no attribute 'manip' Has someone an idea where this function has moved?,https://stackoverflow.com/questions/48976538,7487368,Documentation Replication on Other Examples
64181490,Having problems while doing multiclass classification with tensorflow,"<p><a href=""https://colab.research.google.com/drive/1EdCL6YXCAvKqpEzgX8zCqWv51Yum2PLO?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1EdCL6YXCAvKqpEzgX8zCqWv51Yum2PLO?usp=sharing</a></p>
<p>Hello,</p>
<p>Above, I'm trying to identify 5 different type of restorations on dental x-rays with tensorflow. i'm using the <a href=""https://www.tensorflow.org/tutorials/images/classification"" rel=""nofollow noreferrer"">official documentation</a> to follow the steps but now i'm kind of stucked and i need help. here are my questions:</p>
<p><code>1</code>-i have my data on my local disk. TF example on the link above downloads the data from a different repository. when i want to test my images, do i have any other way than to use the code below ?:</p>
<pre><code>import numpy as np
from keras.preprocessing import image

from google.colab import files
uploaded = files.upload()

# predicting images
for fn in uploaded.keys():
  path = fn
  img = image.load_img(path, target_size=(180, 180))
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images)
  print(fn)
  print(classes)
</code></pre>
<p>i'm asking this because the official documentation just shows the way to test images one-by-one, like this:</p>
<pre><code>img = keras.preprocessing.image.load_img(
sunflower_path, target_size=(img_height, img_width)
)
img_array = keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
&quot;This image most likely belongs to {} with a {:.2f} percent confidence.&quot;
.format(class_names[np.argmax(score)], 100 * np.max(score))
)
</code></pre>
<p><code>2</code>- i'm using &quot;image_dataset_from_directory&quot; method, so i don't have a separate validation directory. is that ok ? or should i use ImageDataGenerator ? For testing my data, i picked some data randomly from all 5 categories by hand and put them in my test folder which has 5 subfolders as i have that number of categories. is this what i am supposed to do for prediction, also separating the test data into different folders ? if yes, how can i load all these 5 folders simultaneously at test time ?</p>
<p><code>3</code>- i'm also supposed to create the confusion matrix. but i couldn't understand how i can apply <a href=""https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix"" rel=""nofollow noreferrer"">this</a> to my code ? some others say, use scikit-learn's <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"" rel=""nofollow noreferrer"">confusion matrix</a>, but this time i have to define y-true, y_pred values, which i cannot fit into this code. am i supposed to evaluate 5 different confusion matrices for 5 different predictions and how ?</p>
<p><code>4</code>-sometimes, i observe that the validation accuracy starts much higher than the training accuracy. is this unusual ? after 3-4 epochs, train accuracy cathces the validation accuracy and continues in a more balanced way. i thought this should not be happening. is everything alright ?</p>
<p><code>5</code>- final question, why the first epoch takes much much longer time than other epochs? in my setup, it's about 30-40 minutes to complete the first epoch, and then only about a minute or so to complete every other epoch. is there a way to fix it or does it always happen the same way ?</p>
<p>thanks.</p>
","https://colab.research.google.com/drive/1EdCL6YXCAvKqpEzgX8zCqWv51Yum2PLO?usp=sharing Hello, Above, I'm trying to identify 5 different type of restorations on dental x-rays with tensorflow. i'm using the official documentation to follow the steps but now i'm kind of stucked and i need help. here are my questions: 1-i have my data on my local disk. TF example on the link above downloads the data from a different repository. when i want to test my images, do i have any other way than to use the code below ?: i'm asking this because the official documentation just shows the way to test images one-by-one, like this: 2- i'm using ""image_dataset_from_directory"" method, so i don't have a separate validation directory. is that ok ? or should i use ImageDataGenerator ? For testing my data, i picked some data randomly from all 5 categories by hand and put them in my test folder which has 5 subfolders as i have that number of categories. is this what i am supposed to do for prediction, also separating the test data into different folders ? if yes, how can i load all these 5 folders simultaneously at test time ? 3- i'm also supposed to create the confusion matrix. but i couldn't understand how i can apply this to my code ? some others say, use scikit-learn's confusion matrix, but this time i have to define y-true, y_pred values, which i cannot fit into this code. am i supposed to evaluate 5 different confusion matrices for 5 different predictions and how ? 4-sometimes, i observe that the validation accuracy starts much higher than the training accuracy. is this unusual ? after 3-4 epochs, train accuracy cathces the validation accuracy and continues in a more balanced way. i thought this should not be happening. is everything alright ? 5- final question, why the first epoch takes much much longer time than other epochs? in my setup, it's about 30-40 minutes to complete the first epoch, and then only about a minute or so to complete every other epoch. is there a way to fix it or does it always happen the same way ? thanks.",https://stackoverflow.com/questions/64181490,6244386,Documentation Replicability
61872515,Tensorflow: Why does Modulo (i.e. tf.math.floormod) not support unsigned integers?,"<p>From the <a href=""https://www.tensorflow.org/api_docs/python/tf/math/floormod"" rel=""nofollow noreferrer"">Docs</a>:</p>

<blockquote>
  <p><em>x</em>:   A Tensor. Must be one of the following types: int32, int64,
  bfloat16, half, float32, float64.</p>
</blockquote>

<p>Example:</p>

<pre><code>x = tf.math.mod(tf.constant(4, tf.dtypes.uint64), tf.constant(2, tf.dtypes.uint64))

[...]

NotFoundError: Could not find valid device for node.
Node:{{node FloorMod}}
All kernels registered for op FloorMod :
  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
 [Op:FloorMod]
</code></pre>
",From the Docs: Example:,https://stackoverflow.com/questions/61872515,6920649,Lack of Alternative Solutions/Documentation
44064753,Multiply all elements of Tensor in Tensorflow,"<p>I have tensor with 3 elements which I want to multiply with each others.</p>

<p>My code currently looks like this:</p>

<pre><code>m1 = tf.multiply(y[0],y[1])
m2 = tf.multiply(m1,y[2])
</code></pre>

<p>Which imho is very unflexible, of course I could put a loop and iterate over the elements, but I was wondering if there is such functionallity already provded in tf ? I could not find anything in the docs</p>
","I have tensor with 3 elements which I want to multiply with each others. My code currently looks like this: Which imho is very unflexible, of course I could put a loop and iterate over the elements, but I was wondering if there is such functionallity already provded in tf ? I could not find anything in the docs",https://stackoverflow.com/questions/44064753,6786718,Requesting (Additional) Resources
44311820,Tensorflow tanh with quantized values,"<p>I am experimenting with the quantization of a neural network in Tensorflow 1.1.</p>

<p>According to the <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/tanh"" rel=""nofollow noreferrer"">documentation</a>, the <code>tanh</code> operation supports floating point inputs as well as fixed point inputs of type <code>qint32</code>. However, I can't get this to work:</p>

<pre><code>import tensorflow as tf

sess = tf.InteractiveSession()

x = tf.constant([1.,2.,3.], dtype=tf.float32)

from tensorflow.python.ops.gen_array_ops import quantize_v2
x_quant = quantize_v2(x, min_range=0., max_range=4., T=tf.qint32)

y_quant = tf.nn.tanh(x_quant[0])
</code></pre>

<p>The code yields an error message:</p>

<blockquote>
<pre><code>TypeError: Value passed to parameter 'x' has DataType qint32 not in list of 
allowed values: float16, float32, float64, complex64, complex128
</code></pre>
</blockquote>

<p>Is there a way out or is it just a bug in the docs?</p>
","I am experimenting with the quantization of a neural network in Tensorflow 1.1. According to the documentation, the tanh operation supports floating point inputs as well as fixed point inputs of type qint32. However, I can't get this to work: The code yields an error message: Is there a way out or is it just a bug in the docs?",https://stackoverflow.com/questions/44311820,1095888,Documentation Replicability
42140211,Tensorflow - eval() error: You must feed a value for placeholder tensor,"<p>I'm trying to use eval() to understand what is happening in each learning step.</p>

<p>However, if I use eval() on an tf.matmul operation, then I would get an error <code>You must feed a value for placeholder tensor</code>.</p>

<p>If I removed the eval(), then everything would work properly as expected. </p>

<pre><code>num_steps = 3001

with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()
    writer = tf.summary.FileWriter(""/home/ubuntu/tensorboard"", graph=tf.get_default_graph())
    for step in range(num_steps):
        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
        batch_data = train_dataset[offset:(offset + batch_size), :]
        batch_labels = train_labels[offset:(offset + batch_size), :]
        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
        _, l, predictions, summary = session.run([optimizer, loss, train_prediction, summary_op], feed_dict=feed_dict)
        writer.add_summary(summary, step)

        # If I removed this line, then it would work
        loss.eval()

batch_size = 128

graph = tf.Graph()
with graph.as_default():
    with tf.name_scope('tf_train_dataset'):
        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))
    with tf.name_scope('tf_train_labels'):
        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    with tf.name_scope('tf_valid_dataset'):
        tf_valid_dataset = tf.constant(valid_dataset)
    with tf.name_scope('tf_test_dataset'):
        tf_test_dataset = tf.constant(test_dataset)

    with tf.name_scope('weights'):
        weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))
    with tf.name_scope('biases'):
        biases = tf.Variable(tf.zeros([num_labels]))

    with tf.name_scope('logits'):
        logits = tf.matmul(tf_train_dataset, weights) + biases
    with tf.name_scope('loss'):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))
        tf.summary.scalar(""loss"", loss)

    with tf.name_scope('optimizer'):
        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

    with tf.name_scope(""train_prediction""):
        train_prediction = tf.nn.softmax(logits)
    with tf.name_scope(""valid_prediction""):
        valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)
    with tf.name_scope(""test_prediction""):
        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)

    with tf.name_scope(""correct_prediction""):
        correct_prediction = tf.equal(tf.argmax(tf_train_labels,1), tf.argmax(train_prediction,1))

    with tf.name_scope(""accuracy""):
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        tf.summary.scalar(""training_accuracy"", accuracy)

    summary_op = tf.summary.merge_all()
</code></pre>

<p>The exact error is:</p>

<pre><code>InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'tf_train_dataset/Placeholder' with dtype float and shape [128,784]
     [[Node: tf_train_dataset/Placeholder = Placeholder[dtype=DT_FLOAT, shape=[128,784], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>Does anyone have a better way to log the variables? I've tried tensor_summary, but it doesn't show it on the website.</p>

<p>Thanks all</p>
","I'm trying to use eval() to understand what is happening in each learning step. However, if I use eval() on an tf.matmul operation, then I would get an error You must feed a value for placeholder tensor. If I removed the eval(), then everything would work properly as expected. The exact error is: Does anyone have a better way to log the variables? I've tried tensor_summary, but it doesn't show it on the website. Thanks all",https://stackoverflow.com/questions/42140211,1157751,Requesting (Additional) Resources
42419837,Tensorflow tf.matmul example is incorrect?,"<p>I read the official document for <a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">tf.matmul</a> 
and I understand the first example.
It is a simple [2,3] x [3,2] operation:</p>

<pre><code>a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

c = tf.matmul(a, b) =&gt; [[58 64]
                    [139 154]]
</code></pre>

<p>However, the second example seems very strange :</p>

<pre><code>a = tf.constant(np.arange(1, 13, dtype=np.int32),
            shape=[2, 2, 3])

b = tf.constant(np.arange(13, 25, dtype=np.int32),
            shape=[2, 3, 2])

c = tf.matmul(a, b) =&gt; [[[ 94 100]
                     [229 244]],
                    [[508 532]
                     [697 730]]]
</code></pre>

<p>Why the matrix with shape [2,2,3] is allowed to multiply with [2,3,2] ?</p>
","I read the official document for tf.matmul and I understand the first example. It is a simple [2,3] x [3,2] operation: However, the second example seems very strange : Why the matrix with shape [2,2,3] is allowed to multiply with [2,3,2] ?",https://stackoverflow.com/questions/42419837,7611906,Documentation Replicability
44314992,Are valid `tf.matmul` arguments described correctly in the TensorFlow documentation?,"<p>Maybe I'm confused about what ""inner"" and ""outer"" tensor dimensions are, but the documentation for <code>tf.matmul</code> puzzles me:</p>

<blockquote>
  <p>The inputs must be matrices (or tensors of rank > 2, representing
  batches of matrices), with matching inner dimensions, possibly after
  transposition.</p>
</blockquote>

<p>Isn't it the case that R-rank arguments need to have matching (or no) R-2 outer dimensions, and that (as in normal matrix multiplication) the Rth, inner dimension of the first argument must match the R-1st dimension of the second. That is, in</p>

<pre><code>A = tf.constant(..., shape=[a, ..., z, p, x])
B = tf.constant(..., shape=[a', ..., z', x', q]) 
C = tf.matmul(A, B)
</code></pre>

<p>The outer dimensions <code>a, ..., z</code> must be identical to <code>a', ..., z'</code> (or not exist), and <code>x</code> and <code>x'</code> must match (while <code>p</code> and <code>q</code> can be anything).</p>

<p>Or put another way, shouldn't the docs say: </p>

<blockquote>
  <p>The inputs must, following any transpositions, be tensors of rank ≥ 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.</p>
</blockquote>
","Maybe I'm confused about what ""inner"" and ""outer"" tensor dimensions are, but the documentation for tf.matmul puzzles me: Isn't it the case that R-rank arguments need to have matching (or no) R-2 outer dimensions, and that (as in normal matrix multiplication) the Rth, inner dimension of the first argument must match the R-1st dimension of the second. That is, in The outer dimensions a, ..., z must be identical to a', ..., z' (or not exist), and x and x' must match (while p and q can be anything). Or put another way, shouldn't the docs say:",https://stackoverflow.com/questions/44314992,656912,Documentation Ambiguity
45496277,Having trouble understanding lstm use in tensorflow code sample,"<p>Why is the <code>pred</code> variable being calculated before any of the training iterations occur? I would expect that a <code>pred</code> would be generated (through the <code>RNN()</code> function) during <strong>each</strong> pass through of the data for every iteration? </p>

<p>There must be something I am missing. Is <code>pred</code> something like a function object? I have looked at the docs for <code>tf.matmul()</code> and that returns a tensor, not a function.</p>

<p>Full source: <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py"" rel=""nofollow noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py</a></p>

<p>Here is the code:</p>

<pre><code>def RNN(x, weights, biases):

    # Prepare data shape to match `rnn` function requirements
    # Current data input shape: (batch_size, n_steps, n_input)
    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)

    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)
    x = tf.unstack(x, n_steps, 1)

    # Define a lstm cell with tensorflow
    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)

    # Get lstm cell output
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)

    # Linear activation, using rnn inner loop last output
    return tf.matmul(outputs[-1], weights['out']) + biases['out']

pred = RNN(x, weights, biases)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Evaluate model
correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf.global_variables_initializer()
</code></pre>
","Why is the pred variable being calculated before any of the training iterations occur? I would expect that a pred would be generated (through the RNN() function) during each pass through of the data for every iteration? There must be something I am missing. Is pred something like a function object? I have looked at the docs for tf.matmul() and that returns a tensor, not a function. Full source: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py Here is the code:",https://stackoverflow.com/questions/45496277,8133943,Documentation Ambiguity
48483980,why explain logit as 'unscaled log probabililty' in sotfmax_cross_entropy_with_logits?,"<p>In the tensorflow documentation (<a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/softmax_cross_entropy_with_logits"" rel=""nofollow noreferrer"">softmax_cross_entropy_with_logits</a>), they said ""logits : unscaled log probablilty"". What is 'log probability'?
First, I understand that <code>'logits'</code> is an <code>'output before normalization'</code> or a <code>'score for class'</code>.</p>

<pre class=""lang-py prettyprint-override""><code>logits = tf.matmul(X,W) + b
hypothesis = tf.nn.softmax(logits)
</code></pre>

<p>If I got <code>[1.5, 2.4, 0,7]</code> by <code>tf.matmul(X,W) + b</code>, then <code>[1.5, 2.4, 0,7]</code> is <code>logits(score)</code> and this was unscaled. I can understand it up to this stage. But, I can't understand why <code>[1.5, 2.4, 0.7]</code> is <code>'log probability'</code>.</p>
","In the tensorflow documentation (softmax_cross_entropy_with_logits), they said ""logits : unscaled log probablilty"". What is 'log probability'? First, I understand that 'logits' is an 'output before normalization' or a 'score for class'. If I got [1.5, 2.4, 0,7] by tf.matmul(X,W) + b, then [1.5, 2.4, 0,7] is logits(score) and this was unscaled. I can understand it up to this stage. But, I can't understand why [1.5, 2.4, 0.7] is 'log probability'.",https://stackoverflow.com/questions/48483980,6333512,Documentation Ambiguity
58873635,What is tensorflow.matmul?,"<p>From the output of print, it is function. But according to the <a href=""https://www.tensorflow.org/api_docs/python/tf/Operation"" rel=""nofollow noreferrer"">official document</a>:</p>

<blockquote>
  <p>An Operation is a node in a TensorFlow Graph that takes zero or more
  Tensor objects as input, and produces zero or more Tensor objects as
  output. Objects of type Operation are created by calling a Python op
  constructor (such as tf.matmul) or tf.Graph.create_op.</p>
</blockquote>

<p>it is a constructor. So I think it is a class name. But, printing the return value of tf.matmul shows it is a tensor, not an ""Object of type Operation"". Is the class Tensor inherited from the class Operation? I tried to find the definition of tf.matmul in tensorflow source code but could not get it. </p>
","From the output of print, it is function. But according to the official document: it is a constructor. So I think it is a class name. But, printing the return value of tf.matmul shows it is a tensor, not an ""Object of type Operation"". Is the class Tensor inherited from the class Operation? I tried to find the definition of tf.matmul in tensorflow source code but could not get it.",https://stackoverflow.com/questions/58873635,10142726,Documentation Completeness
42956766,3D tensors with tensorflow tf.matmul,"<p>I want to do a multiplication with two 3-D tensors, as defined:  </p>

<pre><code>a = tf.random_uniform(shape = [5,3,3])   

b = tf.ones(shape = [5,3,1])   

c = tf.matmul(a,b) 
</code></pre>

<p>but I can't get the right answer as described in the tf.matmul function   </p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/matmul</a>  </p>
","I want to do a multiplication with two 3-D tensors, as defined: but I can't get the right answer as described in the tf.matmul function https://www.tensorflow.org/api_docs/python/tf/matmul",https://stackoverflow.com/questions/42956766,7752274,Documentation Replicability
55491752,Workaround for using tf.matmul with two non-constant inputs,"<p>We are currently trying to convert a transformer model to a tensorflow-lite graph but it seems that the problem is the self-attention mechanism. </p>

<p>We're not able to process the graph. Looking into tf-lite code we narrowed it down to the <code>tf.matmul</code> lite-version.</p>

<p>The <a href=""https://www.tensorflow.org/lite/guide/ops_compatibility"" rel=""nofollow noreferrer"">docs</a> state:</p>

<blockquote>
  <p><code>tf.matmul</code> - <em>as long as the second argument is constant and transposition is not used</em></p>
</blockquote>

<p>However, this is the case in self-attention:</p>

<p><a href=""https://i.stack.imgur.com/r7I3G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/r7I3G.png"" alt=""enter image description here""></a></p>

<p>(source: <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention is all you need</a>)</p>

<p>Is there a known workaround for such a situation?</p>
","We are currently trying to convert a transformer model to a tensorflow-lite graph but it seems that the problem is the self-attention mechanism. We're not able to process the graph. Looking into tf-lite code we narrowed it down to the tf.matmul lite-version. The docs state: However, this is the case in self-attention: (source: Attention is all you need) Is there a known workaround for such a situation?",https://stackoverflow.com/questions/55491752,826983,Requesting (Additional) Resources
45734487,tensorflow: Error multiplying a sparse matrix with a dense matrix using tf.matmul,"<p>In the following code, I want dense matrix <code>B</code> to left multiply a sparse matrix <code>A</code>, but I got errors.</p>

<pre><code>import tensorflow as tf
import numpy as np

A = tf.sparse_placeholder(tf.float32)
B = tf.placeholder(tf.float32, shape=(5,5))
C = tf.matmul(B,A,a_is_sparse=False,b_is_sparse=True)
sess = tf.InteractiveSession()
indices = np.array([[3, 2], [1, 2]], dtype=np.int64)
values = np.array([1.0, 2.0], dtype=np.float32)
shape = np.array([5,5], dtype=np.int64)
Sparse_A = tf.SparseTensorValue(indices, values, shape)
RandB = np.ones((5, 5))
print sess.run(C, feed_dict={A: Sparse_A, B: RandB})
</code></pre>

<p>The error message is as follows:</p>

<pre><code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensor'&gt; 
to Tensor. Contents: SparseTensor(indices=Tensor(""Placeholder_4:0"", shape=(?, ?), dtype=int64), values=Tensor(""Placeholder_3:0"", shape=(?,), dtype=float32), dense_shape=Tensor(""Placeholder_2:0"", shape=(?,), dtype=int64)). 
Consider casting elements to a supported type.
</code></pre>

<p>What's wrong with my code?</p>

<p>I'm doing this following the <a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">documentation</a> and it says we should use <code>a_is_sparse</code> to denote whether the first matrix is sparse, and similarly with <code>b_is_sparse</code>. Why is my code wrong?</p>

<p>As is suggested by vijay, I should use <code>C = tf.matmul(B,tf.sparse_tensor_to_dense(A),a_is_sparse=False,b_is_sparse=True)</code></p>

<p>I tried this but I met with another error saying:</p>

<pre><code>Caused by op u'SparseToDense', defined at:
  File ""a.py"", line 19, in &lt;module&gt;
    C = tf.matmul(B,tf.sparse_tensor_to_dense(A),a_is_sparse=False,b_is_sparse=True)
  File ""/home/fengchao.pfc/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py"", line 845, in sparse_tensor_to_dense
    name=name)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py"", line 710, in sparse_to_dense
    name=name)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_sparse_ops.py"", line 1094, in _sparse_to_dense
    validate_indices=validate_indices, name=name)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): indices[1] = [1,2] is out of order
[[Node: SparseToDense = SparseToDense[T=DT_FLOAT, Tindices=DT_INT64, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_Placeholder_4_0_2, _arg_Placeholder_2_0_0, _arg_Placeholder_3_0_1, SparseToDense/default_value)]]
</code></pre>

<p>Thank you all for helping me!</p>
","In the following code, I want dense matrix B to left multiply a sparse matrix A, but I got errors. The error message is as follows: What's wrong with my code? I'm doing this following the documentation and it says we should use a_is_sparse to denote whether the first matrix is sparse, and similarly with b_is_sparse. Why is my code wrong? As is suggested by vijay, I should use C = tf.matmul(B,tf.sparse_tensor_to_dense(A),a_is_sparse=False,b_is_sparse=True) I tried this but I met with another error saying: Thank you all for helping me!",https://stackoverflow.com/questions/45734487,5005808,Documentation Replicability
52246415,Dense vector on sparse matrix multiplication in tensorflow,"<p>What is proper way of dense vector on sparse matrix multiplication in tensorflow?</p>

<p>According to documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">tf.matmul</a> support sparse matrix multiplication, so do I need to use <a href=""https://www.tensorflow.org/api_docs/python/tf/sparse_matmul"" rel=""nofollow noreferrer"">tf.sparse_matmul</a>? (And also <a href=""https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_dense_matmul"" rel=""nofollow noreferrer"">tf.sparse_tensor_dense_matmul</a> exist, so in which cases each of them should be used?)</p>

<p>Also do I need to convert my sparse matrix to <a href=""https://www.tensorflow.org/api_docs/python/tf/SparseTensor"" rel=""nofollow noreferrer"">tf.SparseTensor</a> ? Also it not obvious what <a href=""https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor_or_sparse_tensor"" rel=""nofollow noreferrer"">tf.convert_to_tensor_or_sparse_tensor</a> is doing and how to convert dense numpy matrix or scipy sparse matrix for tensorflow suitable input.</p>

<p>Here what I have tried(timing are for CPU):</p>

<pre><code>import numpy as np
import tensorflow as tf

np.random.seed(2018)

# Parameters
n = 10*1000
m = 4*1000
p = 0.1

%%time

# Data preparation
dense_vector = np.random.rand(1,n).astype(np.float32)
print('dense_vector.shape', dense_vector.shape)
#print('dense_vector:')
#print(dense_vector)

dense_matrix = np.random.rand(n*m)
idx = np.random.choice(range(n*m), int((1.0-p)*n*m), replace=False)
dense_matrix[idx] = 0.0
dense_matrix = dense_matrix.reshape(n,m).astype(np.float32)
print('dense_matrix.shape', dense_matrix.shape)
#print('dense_matrix:')
#print(dense_matrix)

dense_vector.shape (1, 10000)
dense_matrix.shape (10000, 4000)
CPU times: user 9.8 s, sys: 2.38 s, total: 12.2 s
Wall time: 12.2 s

%%time

# Dense vector on dense matrix multiplication using numpy

res = dense_vector @ dense_matrix
print('res.shape', res.shape)
#print('res:')
#print(res)

%%time

# Dense vector on dense matrix multiplication using tensorflow tf.matmul V1

dense_vector_tf = tf.convert_to_tensor(dense_vector, np.float32)
dense_matrix_tf = tf.convert_to_tensor(dense_matrix, np.float32)
res_tf = tf.matmul(dense_vector_tf, dense_matrix_tf)

with tf.Session() as sess:
    res = sess.run(res_tf)
    print('res.shape', res.shape)
    #print('res:')
    #print(res)

res.shape (1, 4000)
CPU times: user 1.88 s, sys: 1.82 s, total: 3.7 s
Wall time: 3.54 s

%%time

# Dense vector on dense matrix multiplication using tensorflow tf.matmul V2

dense_vector_tf = tf.convert_to_tensor(dense_vector, np.float32)
dense_matrix_tf = tf.convert_to_tensor(dense_matrix, np.float32)
res_tf = tf.matmul(dense_vector_tf, dense_matrix_tf,
                   a_is_sparse=False,
                   b_is_sparse=True)

with tf.Session() as sess:
    res = sess.run(res_tf)
    print('res.shape', res.shape)
    #print('res:')
    #print(res)

res.shape (1, 4000)
CPU times: user 4.91 s, sys: 4.28 s, total: 9.19 s
Wall time: 9.07 s

%%time

# Dense vector on sparse matrix multiplication using tensorflow tf.sparse_matmul V1

dense_vector_tf = tf.convert_to_tensor(dense_vector, np.float32)
dense_matrix_tf = tf.convert_to_tensor(dense_matrix, np.float32)
res_tf = tf.sparse_matmul(dense_vector_tf, dense_matrix_tf,
                         a_is_sparse=False,
                         b_is_sparse=True)

with tf.Session() as sess:
    res = sess.run(res_tf)
    print('res.shape', res.shape)
    #print('res:')
    #print(res)

res.shape (1, 4000)
CPU times: user 4.82 s, sys: 4.18 s, total: 8.99 s
Wall time: 9 s

%%time

# Dense vector on sparse matrix multiplication using tensorflow tf.sparse_matmul V2

dense_vector_tf = tf.convert_to_tensor(dense_vector, np.float32)
dense_matrix_tf = tf.convert_to_tensor_or_sparse_tensor(dense_matrix, np.float32)
res_tf = tf.sparse_matmul(dense_vector_tf, dense_matrix_tf,
                         a_is_sparse=False,
                         b_is_sparse=True)

with tf.Session() as sess:
    res = sess.run(res_tf)
    print('res.shape', res.shape)
    #print('res:')
    #print(res)

res.shape (1, 4000)
CPU times: user 5.07 s, sys: 4.53 s, total: 9.6 s
Wall time: 9.61 s
</code></pre>

<p>Also I can't see any improvement using sparse matrices, what I'm doing wrong?</p>
","What is proper way of dense vector on sparse matrix multiplication in tensorflow? According to documentation tf.matmul support sparse matrix multiplication, so do I need to use tf.sparse_matmul? (And also tf.sparse_tensor_dense_matmul exist, so in which cases each of them should be used?) Also do I need to convert my sparse matrix to tf.SparseTensor ? Also it not obvious what tf.convert_to_tensor_or_sparse_tensor is doing and how to convert dense numpy matrix or scipy sparse matrix for tensorflow suitable input. Here what I have tried(timing are for CPU): Also I can't see any improvement using sparse matrices, what I'm doing wrong?",https://stackoverflow.com/questions/52246415,1179925,Documentation Completeness
47521759,tf.metrics.accuracy not working as intended,"<p>I have linear regression model that seems to be working fine, but I want to display the accuracy of the model. </p>

<p>First, I initialize the variables and placeholders...</p>

<pre><code>X_train, X_test, Y_train, Y_test = train_test_split(
    X_data, 
    Y_data, 
    test_size=0.2
)

n_rows = X_train.shape[0]

X = tf.placeholder(tf.float32, [None, 89])
Y = tf.placeholder(tf.float32, [None, 1])

W_shape = tf.TensorShape([89, 1])
b_shape = tf.TensorShape([1])

W = tf.Variable(tf.random_normal(W_shape))
b = tf.Variable(tf.random_normal(b_shape))

pred = tf.add(tf.matmul(X, W), b)

cost = tf.reduce_sum(tf.pow(pred-Y, 2)/(2*n_rows-1))

optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate).minimize(cost)
</code></pre>

<p><code>X_train</code> has shape <code>(6702, 89)</code> and <code>Y_train</code> has shape <code>(6702, 1)</code>. Next I run the session and I display the cost per epoch as well as the total MSE...</p>

<pre><code>init = tf.global_variables_initializer()

with tf.Session() as sess:

    sess.run(init)

    for epoch in range(FLAGS.training_epochs):

        avg_cost = 0

        for (x, y) in zip(X_train, Y_train):

            x = np.reshape(x, (1, 89))
            y = np.reshape(y, (1,1))
            sess.run(optimizer, feed_dict={X:x, Y:y})

        # display logs per epoch step
        if (epoch + 1) % FLAGS.display_step == 0:

            c = sess.run(
                cost, 
                feed_dict={X:X_train, Y:Y_train}
            )

            y_pred = sess.run(pred, feed_dict={X:X_test})
            test_error = r2_score(Y_test, y_pred)
            print(test_error)

            print(""Epoch:"", '%04d' % (epoch + 1), ""cost="", ""{:.9f}"".format(c))

    print(""Optimization Finished!"")

    pred_y = sess.run(pred, feed_dict={X:X_test})
    mse = tf.reduce_mean(tf.square(pred_y - Y_test))

    print(""MSE: %4f"" % sess.run(mse))
</code></pre>

<p>This all seems to work correctly. However, now I want to see the accuracy of my model, so I want to implement <code>tf.metrics.accuracy</code>. The documentation says it has 2 arguments, <code>labels</code> and <code>predictions</code>. I added the following next...</p>

<pre><code>accuracy, accuracy_op = tf.metrics.accuracy(labels=Y_test, predictions=pred)

init_local = tf.local_variables_initializer()

sess.run(init_local)

print(sess.run(accuracy))
</code></pre>

<p>Apparently I need to initialize local variales, however I think I am doing something wrong because the accuracy result that gets printed out is <code>0.0</code>. </p>

<p>I searched everywhere for a working example but I cannot get it to work for my model, what is the proper way to implement it?</p>
","I have linear regression model that seems to be working fine, but I want to display the accuracy of the model. First, I initialize the variables and placeholders... X_train has shape (6702, 89) and Y_train has shape (6702, 1). Next I run the session and I display the cost per epoch as well as the total MSE... This all seems to work correctly. However, now I want to see the accuracy of my model, so I want to implement tf.metrics.accuracy. The documentation says it has 2 arguments, labels and predictions. I added the following next... Apparently I need to initialize local variales, however I think I am doing something wrong because the accuracy result that gets printed out is 0.0. I searched everywhere for a working example but I cannot get it to work for my model, what is the proper way to implement it?",https://stackoverflow.com/questions/47521759,4333347,Requesting (Additional) Resources
47231777,how to use tf.metrics.__ with estimator model predict output,"<p>I try to follow the tensorflow API 1.4 document to achieve what I need in a learning process.</p>

<p>I am now at this stage, can produce a predict object for example:</p>

<pre><code>classifier = tf.estimator.DNNClassifier(feature_columns=feature_cols,hidden_units=[10, 20, 10], n_classes=3, model_dir=""/tmp/xlz_model"")

predict = classifier.predict(input_fn=input_pd_fn_prt (test_f),predict_keys=[""class_ids""])
label =tf.constant(test_l.values, tf.int64)
</code></pre>

<p>how can I use predict and label in <code>tf.metrics.auc</code> for example:</p>

<pre><code>out, opt = tf.metrics.auc(label, predict)
</code></pre>

<p>I have tried so many different options. there are no clear documentation how these tensorflow APIs can be should be used. </p>
","I try to follow the tensorflow API 1.4 document to achieve what I need in a learning process. I am now at this stage, can produce a predict object for example: how can I use predict and label in tf.metrics.auc for example: I have tried so many different options. there are no clear documentation how these tensorflow APIs can be should be used.",https://stackoverflow.com/questions/47231777,1203186,Documentation Replicability
49543158,How to use weights in tf.metrics.auc?,"<p>The docs for the <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/auc"" rel=""nofollow noreferrer"">tf.metrics.auc function in tensorflow</a> say</p>

<blockquote>
  <p>weights: Optional Tensor whose rank is either 0, or the same rank as labels, and must be broadcastable to labels (i.e., all dimensions must be either 1, or the same as the corresponding labels dimension).</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>If weights is None, weights default to 1. Use weights of 0 to mask values.</p>
</blockquote>

<p>Suppose I want to use the weights to measure two AUCs: one for men, one for women.</p>

<p>Can you give an example of how to do that?</p>

<p><strong>EDIT</strong>: And suppose I have enough classes that I don't want to divide the data into all the different classes, and enough data that I don't want to read it all into memory.  That is, I want to do it in a streaming fashion.</p>
","The docs for the tf.metrics.auc function in tensorflow say and Suppose I want to use the weights to measure two AUCs: one for men, one for women. Can you give an example of how to do that? EDIT: And suppose I have enough classes that I don't want to divide the data into all the different classes, and enough data that I don't want to read it all into memory. That is, I want to do it in a streaming fashion.",https://stackoverflow.com/questions/49543158,34935,Requesting (Additional) Resources
50433094,What do the return values of tf.metrics mean?,"<p>I've been trying to add some hopefully useful intermediate calculations used to derive my loss function to the <code>eval_metric_ops</code> dictionary for my evaluation <code>EstimatorSpec</code>. I have wrapped these in a call to <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/mean"" rel=""nofollow noreferrer""><code>tf.metrics.mean</code></a> as it seemed to fit my needs.</p>

<p>The return type of this function is a <code>tuple</code> of <code>(mean, update_op)</code>, where <code>mean</code> is ostensibly the current mean and <code>update_op</code> is an operation that computes the new mean and returns it.</p>

<p>However, when I try to evaluate it I see that the <code>value</code> and <code>update_op</code> fields seem to be different. The documentation doesn't provide an explanation for this as far as I can see.</p>

<p>For example, take the following snippet of code:</p>

<pre><code>    test_tensor = tensorflow.constant([[1, 2, 3], [4, 5, 6]])
    test_mean = tensorflow.metrics.mean(test_tensor)

    sess = tensorflow.Session()
    sess.run(tensorflow.global_variables_initializer())
    sess.run(tensorflow.local_variables_initializer())

    print sess.run(test_mean)
    print sess.run(test_mean)
    print sess.run(test_mean)
    print sess.run(test_mean)

    print sess.run(test_mean[0])
    print sess.run(test_mean)[1]
</code></pre>

<p>This returns the following:</p>

<pre><code>(0.0, 3.5)
(1.75, 3.5)
(2.3333333, 3.5)
(2.625, 3.5)
3.5
3.5
</code></pre>

<p>The second value of the tuple is obviously an overall average of the input value, but the left hand side values seem to be asymptoting towards <code>3.5</code>, while the taking the zero-index of test_mean and evaluating it results in <code>3.5</code> directly, as opposed to the value that I get by evaluating the whole operation and then taking the index.</p>

<p>What is happening here?</p>
","I've been trying to add some hopefully useful intermediate calculations used to derive my loss function to the eval_metric_ops dictionary for my evaluation EstimatorSpec. I have wrapped these in a call to tf.metrics.mean as it seemed to fit my needs. The return type of this function is a tuple of (mean, update_op), where mean is ostensibly the current mean and update_op is an operation that computes the new mean and returns it. However, when I try to evaluate it I see that the value and update_op fields seem to be different. The documentation doesn't provide an explanation for this as far as I can see. For example, take the following snippet of code: This returns the following: The second value of the tuple is obviously an overall average of the input value, but the left hand side values seem to be asymptoting towards 3.5, while the taking the zero-index of test_mean and evaluating it results in 3.5 directly, as opposed to the value that I get by evaluating the whole operation and then taking the index. What is happening here?",https://stackoverflow.com/questions/50433094,1613983,Documentation Completeness
49820105,Tensorflow Mean Absolute Error (MAE) for evaluation,"<p>Looking at tensorflow docs for MAE, I saw that <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/mean_absolute_error"" rel=""noreferrer"">tf.metrics.mean_absolute_error</a> will return:</p>

<ul>
<li><code>mean_absolute_error</code>: A Tensor representing the current mean, the value of total divided by count.</li>
<li><code>update_op</code>: An operation that increments the total and count variables appropriately and whose value matches mean_absolute_error.</li>
</ul>

<p>How to implement this for evaluation purpose? As stated <a href=""https://github.com/tensorflow/tensorflow/issues/12252#issuecomment-322276355"" rel=""noreferrer"">here</a>:</p>

<blockquote>
  <p>mean_absolute_error is intended for evaluation and so it doesn't have a gradient. mean_absolute_error also returns an update op (which are you ignoring in the code above) that must be used to update the mean, so the concept of a gradient for this function doesn't really make sense. The update op for tf.metrics.mean_absolute_error(pred, y) must be called before the mean can be obtained.</p>
</blockquote>

<p>I don't know how to deal with returned value from <code>mean_absolute_error</code> function. Can someone write a simple example with this function? Thanks a lot.</p>
","Looking at tensorflow docs for MAE, I saw that tf.metrics.mean_absolute_error will return: How to implement this for evaluation purpose? As stated here: I don't know how to deal with returned value from mean_absolute_error function. Can someone write a simple example with this function? Thanks a lot.",https://stackoverflow.com/questions/49820105,3280050,Requesting (Additional) Resources
56285229,How to properly use tf.metrics.mean_iou in Tensorflow to show confusion matrix on Tensorboard?,"<p>I found evaluation script in Tensorflow official implementation of DeeplabV3+ (<a href=""https://github.com/tensorflow/models/blob/master/research/deeplab/eval.py"" rel=""nofollow noreferrer"">eval.py</a>) uses <code>tf.metrics.mean_iou</code> to update mean IOU, and adds it to Tensorboard for record.</p>

<p><code>tf.metrics.mean_iou</code> actually returns 2 tensors, one is calculated mean IOU, the other is an opdate_op, and according to official doc (<a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/mean_iou"" rel=""nofollow noreferrer"">doc</a>), confusion matrix. It seems every time if you want to get calculated mean_iou, you have to call that update_op first.</p>

<p>I am trying to add this update_op into summary as a tensor, but it does not work. My question is how to add this confusion matrix into Tensorboard?</p>

<p>I saw some other threads on how to calculate confusion matrix and add it to Tensorboard, with extra operations. I just would like to know if one can do this without those extra operations.</p>

<p>Any help would be appreciated.</p>
","I found evaluation script in Tensorflow official implementation of DeeplabV3+ (eval.py) uses tf.metrics.mean_iou to update mean IOU, and adds it to Tensorboard for record. tf.metrics.mean_iou actually returns 2 tensors, one is calculated mean IOU, the other is an opdate_op, and according to official doc (doc), confusion matrix. It seems every time if you want to get calculated mean_iou, you have to call that update_op first. I am trying to add this update_op into summary as a tensor, but it does not work. My question is how to add this confusion matrix into Tensorboard? I saw some other threads on how to calculate confusion matrix and add it to Tensorboard, with extra operations. I just would like to know if one can do this without those extra operations. Any help would be appreciated.",https://stackoverflow.com/questions/56285229,7555390,Documentation Ambiguity
52451656,Custom metric across rows in TensorFlow,"<p>I'm trying to calculate metrics for my TensorFlow model across rows with a common key -- specifically precision at k for an information retrieval task -- and I'm finding this extremely nontrivial.  My data include a field that indicates the session ID of each row, and there are variable number of rows for each session ID (but small, under 100).  My task is to train across the rows as independent observations, so I don't want to group on the session ID and train per-session as it will bias the model.  The entire point of the model is to train and evaluate on individual items independently of context, but evaluate the quality of that evaluation within the context, by session IDs.</p>

<p>As a side note, part of the challenge I'm concerned about is data locality as I'm performing distributed training.  <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer"">However it seems that there may be a single evaluator?</a>  (""Evaluator is a special task that is not part of the training cluster."")</p>

<p>Once I realized that tf.metrics.precision_at_k calculates precision at k within class predictions for a given row / data point and not , I have considered writing a custom metric function to call from within the Estimator train_and_evaluate method that keeps an internal dict of session ID to tuples of labels and predictions, and transforms these into Tensors to feed to tf.metrics.precision_at_k.</p>

<p>Caveats:</p>

<ul>
<li>I don't know if I can store this dict, as I don't think I can put it in the computational graph.  Can / should I try to store its state in the metric method itself?  Will that even be retained after the graph is created, and will it be correctly accessed on subsequent calls to the method?</li>
<li>I don't know how or if I can group items with the same session ID onto the same executor -- even if a method like group_by_window or group_by_reducer on the Dataset works, how does this affect locality in a distributed context?</li>
<li>I could reduce my eval set size to fit into memory but I don't know how to force this to run on only one executor.</li>
</ul>

<p>I haven't had much luck finding any examples or more information online about anything like this, and the TF code and docs can be somewhat unhelpful, so I'd appreciate any advice!  Thanks!</p>
","I'm trying to calculate metrics for my TensorFlow model across rows with a common key -- specifically precision at k for an information retrieval task -- and I'm finding this extremely nontrivial. My data include a field that indicates the session ID of each row, and there are variable number of rows for each session ID (but small, under 100). My task is to train across the rows as independent observations, so I don't want to group on the session ID and train per-session as it will bias the model. The entire point of the model is to train and evaluate on individual items independently of context, but evaluate the quality of that evaluation within the context, by session IDs. As a side note, part of the challenge I'm concerned about is data locality as I'm performing distributed training. However it seems that there may be a single evaluator? (""Evaluator is a special task that is not part of the training cluster."") Once I realized that tf.metrics.precision_at_k calculates precision at k within class predictions for a given row / data point and not , I have considered writing a custom metric function to call from within the Estimator train_and_evaluate method that keeps an internal dict of session ID to tuples of labels and predictions, and transforms these into Tensors to feed to tf.metrics.precision_at_k. Caveats: I haven't had much luck finding any examples or more information online about anything like this, and the TF code and docs can be somewhat unhelpful, so I'd appreciate any advice! Thanks!",https://stackoverflow.com/questions/52451656,5026110,Lack of Alternative Solutions/Documentation
42364283,Tensorflow: calculate gradient for tf.multiply,"<p>I'm building a neural network that has the following two layers</p>

<pre><code>pseudo_inputs = tf.Variable(a_numpy_ndarray)
weights = tf.Variable(tf.truncated_normal(...))
</code></pre>

<p>I then want to multiply them using <code>tf.multiply</code> (which, unlike <code>tf.matmul</code> multiplies corresponding indices, i.e. c_ij = a_ij * b_ij)</p>

<pre><code>input = tf.multiply(pseudo_inputs, weights)
</code></pre>

<p>My goal is to learn <code>weights</code>. So I run</p>

<pre><code>train_step = tf.train.AdamOptimizer(learn_rate).minimize(loss, var_list=[weights])
</code></pre>

<p>But it doesn't work. The network doesn't change at all.</p>

<p>Looking at tensorboard, I could see that 'input' has no gradient, so I'm assuming that's the problem. Any ideas how to solve this?</p>

<p>From reading tensorflow docs it seems like I might have to write a gradient op for tf.multiply, but I find it hard to believe no one needed to do this before.</p>
","I'm building a neural network that has the following two layers I then want to multiply them using tf.multiply (which, unlike tf.matmul multiplies corresponding indices, i.e. c_ij = a_ij * b_ij) My goal is to learn weights. So I run But it doesn't work. The network doesn't change at all. Looking at tensorboard, I could see that 'input' has no gradient, so I'm assuming that's the problem. Any ideas how to solve this? From reading tensorflow docs it seems like I might have to write a gradient op for tf.multiply, but I find it hard to believe no one needed to do this before.",https://stackoverflow.com/questions/42364283,1660762,Requesting (Additional) Resources
48115096,Explicitly clear/reset a nested TensorFlow Graph scope,"<p>So, I'm using a bunch of functions from OpenAI <a href=""https://github.com/openai/baselines/tree/master/baselines/deepq"" rel=""nofollow noreferrer"">baselines</a> for Reinforcement Learning. In those functions, policy nets are initialised using statements like:</p>

<pre><code>with tf.variable_scope('deepq', reuse=True):
    ...
    return output
</code></pre>

<p>The problem is that the pointer to the output of those networks gets returned while still inside the scope, which means that when accessing those functions from another .py file I am still inside those scopes.</p>

<p>Basically I want to run a first function <code>train_policy(output_dir)</code> that trains the net and dumps the checkpoint to disk using <code>tf.Saver()</code>.
Next, I run a function <code>run_policy(output_dir)</code> that reinitializes the same tf Graph and loads it's pretrained values using the checkpoint dir.</p>

<p>Right now, when I try this, I get a ValueError:
<em><code>""Variable deepq/... already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?""</code></em> because at the point of running the second function, I'm still in the scope defined by the first.. I checked the <a href=""https://github.com/openai/baselines/blob/master/baselines/deepq/build_graph.py"" rel=""nofollow noreferrer"">code</a> from OpenAI baselines (very nested code, hard to see everything that's going on), and <strong>reuse is already set to True</strong>.</p>

<p>So I tried doing something like:</p>

<p><code>tf.get_default_session().close()</code> followed by:</p>

<p><code>tf.reset_default_graph()</code></p>

<p>after the first function call. (I don't need the session to remain active since I'm dumping everything to disk)</p>

<p>But this gives me errors because I'm still inside a nested graph scope and so I can't reset the default graph... (see eg <a href=""https://github.com/tensorflow/tensorflow/issues/11121"" rel=""nofollow noreferrer"">here</a>)</p>

<p>Alternatively I tried things like:</p>

<pre><code>tf.get_default_graph().as_graph_def().__exit__() 
</code></pre>

<p>or</p>

<pre><code>tf.name_scope('deepq').__exit__()
</code></pre>

<p>but the exit() function needs a whole bunch of args I don't know how to get... (and I can't find good <a href=""https://www.tensorflow.org/api_docs/python/tf/name_scope"" rel=""nofollow noreferrer"">documentation</a> on how to use this function).</p>

<p>My current solution is to run these functions in separate subprocesses in Python (and let the garbage collector do all the work), but this doensn't feel like a satisfactory solution..</p>

<p>Any ideas on how to deal with this? Ideally I'd need something like: <strong><code>tf.clear_all_graphs_and_sessions()</code></strong></p>
","So, I'm using a bunch of functions from OpenAI baselines for Reinforcement Learning. In those functions, policy nets are initialised using statements like: The problem is that the pointer to the output of those networks gets returned while still inside the scope, which means that when accessing those functions from another .py file I am still inside those scopes. Basically I want to run a first function train_policy(output_dir) that trains the net and dumps the checkpoint to disk using tf.Saver(). Next, I run a function run_policy(output_dir) that reinitializes the same tf Graph and loads it's pretrained values using the checkpoint dir. Right now, when I try this, I get a ValueError: ""Variable deepq/... already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?"" because at the point of running the second function, I'm still in the scope defined by the first.. I checked the code from OpenAI baselines (very nested code, hard to see everything that's going on), and reuse is already set to True. So I tried doing something like: tf.get_default_session().close() followed by: tf.reset_default_graph() after the first function call. (I don't need the session to remain active since I'm dumping everything to disk) But this gives me errors because I'm still inside a nested graph scope and so I can't reset the default graph... (see eg here) Alternatively I tried things like: or but the exit() function needs a whole bunch of args I don't know how to get... (and I can't find good documentation on how to use this function). My current solution is to run these functions in separate subprocesses in Python (and let the garbage collector do all the work), but this doensn't feel like a satisfactory solution.. Any ideas on how to deal with this? Ideally I'd need something like: tf.clear_all_graphs_and_sessions()",https://stackoverflow.com/questions/48115096,9088766,Requesting (Additional) Resources
47515377,TF namescope vs. variable_scope and how they impact variable sharing,"<p>I would like to understand how/why <code>name_scope</code> and <code>variable_scope</code> are both used in the code below. I have looked at the Tensorflow documentation for these methods and I have also looked at this very helpful SO post: <a href=""https://stackoverflow.com/questions/34215746/difference-between-variable-scope-and-name-scope-in-tensorflow"">Difference between variable_scope and name_scope in TensorFlow</a> but that has not cleared up my confusion.  </p>

<p>I am trying to reuse some code from <a href=""https://www.kansascityfed.org/en/publications/research/rwp/articles/2017/macroeconomic-indicator-forecasting-deep-neural-networks"" rel=""nofollow noreferrer"">this paper</a>, and in particular I'm looking at how they coded up an autoencoder. <code>input_param</code> is a tensor passed in to a function, and the code is inside the function.</p>

<pre><code>with tf.name_scope(""input"") as sc:
    frednet=input_param

mk_lstm = lambda x: [rnn.LSTMCell(12, use_peepholes=True,
                    initializer=layers.xavier_initializer()) for i in range(x)]
with tf.variable_scope(""lstm1"") as sc:

    #lstm=rnn.LSTMCell(5,use_peepholes=True,initializer=layers.xavier_initializer())
    lstm=rnn.MultiRNNCell(mk_lstm(2))
    frednet,st=tf.nn.dynamic_rnn(lstm,frednet,dtype=tf.float32)

with tf.variable_scope(""lstm2"") as scb:  
    #lstm=rnn.LSTMCell(12,use_peepholes=True,initializer=layers.xavier_initializer())
    lstm=rnn.MultiRNNCell(mk_lstm(2))
    frednet,st=tf.nn.dynamic_rnn(lstm,frednet,dtype=tf.float32,initial_state=st)

net=sm.flatten(frednet[:,:,:])
#     net=sm.dropout(net,keep_prob=.9)
net=sm.fully_connected(net,num_outputs=1,weights_initializer=layers.xavier_initializer(),
                           weights_regularizer=sm.l2_regularizer(1e-7))
</code></pre>

<p><strong>This code works fine BUT I would like to understand why modifications I made caused errors:</strong></p>

<p>(1) if I delete the <code>name_scope</code> portion of the code then my model doesn't really train and keeps printing out zeros.</p>

<p>(2) If I also delete the <code>variable_scope</code> portion of the code, then my code won't even run as tensorflow throws some name conflict error. </p>

<p><strong>Here are my questions:</strong></p>

<p>(1) What is going on with error (1) described above? How this name_scope help with the ""sharing"" of the <code>frednet</code> variable?</p>

<p>(2) Is there a reason they used first <code>sc</code> and then <code>scb</code> for the variable scope (first one matches the name scope but second doesn't). My understanding of Python is that these names are scoped only within the <code>with</code> clause so why don't all 3 just use the same?</p>

<p>(3) For error (2), is it correct that this is happening because <code>MultiRNNCell</code> has default names that it uses, and these are in conflict if I call that function multiple times? Is it always necessary to use <code>variable_scope</code> in the event of multiple uses of <code>MultiRNNCell</code>?</p>

<p>Thanks.</p>
","I would like to understand how/why name_scope and variable_scope are both used in the code below. I have looked at the Tensorflow documentation for these methods and I have also looked at this very helpful SO post: Difference between variable_scope and name_scope in TensorFlow but that has not cleared up my confusion. I am trying to reuse some code from this paper, and in particular I'm looking at how they coded up an autoencoder. input_param is a tensor passed in to a function, and the code is inside the function. This code works fine BUT I would like to understand why modifications I made caused errors: (1) if I delete the name_scope portion of the code then my model doesn't really train and keeps printing out zeros. (2) If I also delete the variable_scope portion of the code, then my code won't even run as tensorflow throws some name conflict error. Here are my questions: (1) What is going on with error (1) described above? How this name_scope help with the ""sharing"" of the frednet variable? (2) Is there a reason they used first sc and then scb for the variable scope (first one matches the name scope but second doesn't). My understanding of Python is that these names are scoped only within the with clause so why don't all 3 just use the same? (3) For error (2), is it correct that this is happening because MultiRNNCell has default names that it uses, and these are in conflict if I call that function multiple times? Is it always necessary to use variable_scope in the event of multiple uses of MultiRNNCell? Thanks.",https://stackoverflow.com/questions/47515377,8762321,Documentation Ambiguity
60104249,Missing modules and attributes for training in TensorFlow's Object Detection API,"<p>I'm currently attempting to train an object detection model. I'm following Gilbert Tanner's tutorial on YouTube. I am running TF version 1.9.0.</p>

<p>It seems as though I'm missing the necessary modules. When I run the following command:</p>

<pre><code>python model_main.py --logtostderr --model_dir=training/ --pipeline_config_path=traini
ng/faster_rcnn_inception_v2_pets.config
</code></pre>

<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""model_main.py"", line 26, in &lt;module&gt;
    from object_detection import model_lib
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\model_lib.py"", line 28, in &lt;module&gt;
    from object_detection import exporter as exporter_lib
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\exporter.py"", line 24, in &lt;module&gt;
    from object_detection.builders import model_builder
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\model_builder.py"", line 35, in &lt;module&gt;
    from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\models\faster_rcnn_inception_resnet_v2_feature_extractor
.py"", line 30, in &lt;module&gt;
    from nets import inception_resnet_v2
  File ""C:\Users\Admin\Desktop\ObjectDetection\models\research\object_detection\nets\inception_resnet_v2.py"", line 375, in &lt;module&gt;
    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS,
AttributeError: module 'tensorflow.compat' has no attribute 'v1'
</code></pre>

<p>For some reason, I've had to fix other problems with certain modules not being in the correct place (for instance, the nets module wasn't placed under the models/research/object_detection directory upon installation, it was instead placed under models/research/slim).</p>

<p>I'm not sure exactly how to fix this issue. I've tried bouncing between different 1.x versions of TensorFlow but each time I am met with similar errors, such as not having the 'v2' attribute. </p>

<p>I suspect I could be lacking a package that should be installed in my environment, but I'm not sure what it could be. I'm also unsure about why the necessary modules aren't properly installed. Here are all of the packages that are installed in my environment:</p>

<pre><code>Package Version Lastest Version
absl-py 0.9.0   0.8.1
astor   0.8.1   0.8.0
biwrap  0.1.6   
bleach  1.5.0   3.1.0
certifi 2019.11.28  2019.11.28
gast    0.3.3   0.3.2
grpcio  1.27.0  1.16.1
h5py    2.10.0  2.10.0
html5lib    0.9999999   1.0.1
keras-applications  1.0.8   1.0.8
keras-preprocessing 1.1.0   1.1.0
markdown    3.1.1   3.1.1
mock    3.0.5   3.0.5
numpy   1.18.1  1.18.1
object-detection    0.1 
pandas  1.0.0   1.0.0
pillow  7.0.0   7.0.0
pip 20.0.2  20.0.2
protobuf    3.11.3  3.11.2
pycocotools 2.0 
python  3.6.10  3.8.1
python-dateutil 2.8.1   2.8.1
pytz    2019.3  2019.3
setuptools  39.1.0  45.1.0
six 1.14.0  1.14.0
sqlite  3.31.1  3.31.1
tensorboard 1.9.0   2.0.0
tensorflow  1.9.0   2.0.0
tensorflow-estimator    1.13.0  2.0.0
tensorflow-plot 0.3.0   
tensorflow-tensorboard  1.5.1   
termcolor   1.1.0   1.1.0
vc  14.1    14.1
vs2015_runtime  14.16.27012 14.16.27012
werkzeug    0.16.1  0.16.1
wheel   0.34.2  0.34.2
wincertstore    0.2 0.2
</code></pre>

<p>Am I missing any necessary packages? Any help on this issue is appreciated. Please let me know if I have not included information that would be helpful.</p>

<p>EDIT:
Line 375 in C:\Users\Admin\Desktop\ObjectDetection\models\research\object_detection\nets\inception_resnet_v2.py is bolded below:</p>

<pre><code>def inception_resnet_v2_arg_scope(
    weight_decay=0.00004,
    batch_norm_decay=0.9997,
    batch_norm_epsilon=0.001,
    activation_fn=tf.nn.relu,
    **batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS**,
    batch_norm_scale=False):
</code></pre>

<p>Here is the link to the video I'm referring to. My problem is occurring when I run the command at 18:01.
<a href=""https://www.youtube.com/watch?v=HjiBbChYRDw"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=HjiBbChYRDw</a>
I realize the command I provided above is slightly different than the one shown in the video. However, in the written version of the tutorial, Gilbert Tanner has updated the command to the one I provided above.</p>

<p>Changing all references on tf.compat.v1.GraphKeys to tf.GraphKeys works, but more errors arise:</p>

<pre><code>AttributeError: module 'tensorflow.compat' has no attribute 'v2'
</code></pre>

<p>on this function signature:</p>

<pre><code>def global_pool(input_tensor, pool_op=tf.compat.v2.nn.avg_pool2d)
</code></pre>

<p>When I change it to this:</p>

<pre><code>def global_pool(input_tensor, pool_op=tf.nn.avg_pool2d)
</code></pre>

<p>I get this error:</p>

<pre><code>AttributeError: module 'tensorflow.nn' has no attribute 'avg_pool2d'
</code></pre>

<p>There is no documentation for avg_pool2d for TensorFlow 1.x and there is for TensorFlow 2.x, so I'm not sure why it's in this file if I have TensorFlow 1.9.</p>

<p>I notice tf.nn has attributes avg_pool and avg_pool3d, however, changing it to these causes a TypeError:</p>

<pre><code>Traceback (most recent call last):
  File ""model_main.py"", line 109, in &lt;module&gt;
    tf.app.run()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\training.py"", line 447, in train_and_evaluate
    return executor.run()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\training.py"", line 531, in run
    return self.run_local()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1119, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1129, in _train_model_default
    input_fn, model_fn_lib.ModeKeys.TRAIN))
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 985, in _get_features_and_labels_from_input_fn
    result = self._call_input_fn(input_fn, mode)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1074, in _call_input_fn
    return input_fn(**kwargs)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\inputs.py"", line 504, in _train_input_fn
    params=params)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\inputs.py"", line 607, in train_input
    batch_size=params['batch_size'] if params else train_config.batch_size)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\dataset_builder.py"", line 155, in build
    dataset = data_map_fn(process_fn, num_parallel_calls=num_parallel_calls)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 882, in map
    return ParallelMapDataset(self, map_func, num_parallel_calls)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1899, in __init__
    super(ParallelMapDataset, self).__init__(input_dataset, map_func)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1868, in __init__
    self._map_func.add_to_graph(ops.get_default_graph())
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\framework\function.py"", line 475, in add_to_graph
    self._create_definition_if_needed()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\framework\function.py"", line 331, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\framework\function.py"", line 340, in _create_definition_if_needed_impl
    self._capture_by_value, self._caller_device)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\framework\function.py"", line 804, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1833, in tf_map_func
    ret = map_func(nested_args)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\dataset_builder.py"", line 134, in process_fn
    processed_tensors = decoder.decode(value)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\data_decoders\tf_example_decoder.py"", line 388, in decod
e
    tensors = decoder.decode(serialized_example, items=keys)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\contrib\slim\python\slim\data\tfexample_decoder.py"", line 520, in decode
    outputs.append(handler.tensors_to_item(keys_to_tensors))
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\data_decoders\tf_example_decoder.py"", line 129, in tenso
rs_to_item
    item = self._handler.tensors_to_item(keys_to_tensors)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\data_decoders\tf_example_decoder.py"", line 98, in tensor
s_to_item
    return tf.maximum(self._name_to_id_table.lookup(unmapped_tensor),
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\ops\lookup_ops.py"", line 223, in lookup
    (self._key_dtype, keys.dtype))
TypeError: Signature mismatch. Keys must be dtype &lt;dtype: 'float32'&gt;, got &lt;dtype: 'string'&gt;.


</code></pre>

<p>Here is line 98 in tensors_to_item:</p>

<pre><code>    return tf.maximum(self._name_to_id_table.lookup(unmapped_tensor),
                      self._display_name_to_id_table.lookup(unmapped_tensor))
</code></pre>

<p>I'm not sure how to handle this issue and it seems like I shouldn't have changed the function signature. Is having to make this many changes to the modules normal?  </p>
","I'm currently attempting to train an object detection model. I'm following Gilbert Tanner's tutorial on YouTube. I am running TF version 1.9.0. It seems as though I'm missing the necessary modules. When I run the following command: I get the following error: For some reason, I've had to fix other problems with certain modules not being in the correct place (for instance, the nets module wasn't placed under the models/research/object_detection directory upon installation, it was instead placed under models/research/slim). I'm not sure exactly how to fix this issue. I've tried bouncing between different 1.x versions of TensorFlow but each time I am met with similar errors, such as not having the 'v2' attribute. I suspect I could be lacking a package that should be installed in my environment, but I'm not sure what it could be. I'm also unsure about why the necessary modules aren't properly installed. Here are all of the packages that are installed in my environment: Am I missing any necessary packages? Any help on this issue is appreciated. Please let me know if I have not included information that would be helpful. EDIT: Line 375 in C:\Users\Admin\Desktop\ObjectDetection\models\research\object_detection\nets\inception_resnet_v2.py is bolded below: Here is the link to the video I'm referring to. My problem is occurring when I run the command at 18:01. https://www.youtube.com/watch?v=HjiBbChYRDw I realize the command I provided above is slightly different than the one shown in the video. However, in the written version of the tutorial, Gilbert Tanner has updated the command to the one I provided above. Changing all references on tf.compat.v1.GraphKeys to tf.GraphKeys works, but more errors arise: on this function signature: When I change it to this: I get this error: There is no documentation for avg_pool2d for TensorFlow 1.x and there is for TensorFlow 2.x, so I'm not sure why it's in this file if I have TensorFlow 1.9. I notice tf.nn has attributes avg_pool and avg_pool3d, however, changing it to these causes a TypeError: Here is line 98 in tensors_to_item: I'm not sure how to handle this issue and it seems like I shouldn't have changed the function signature. Is having to make this many changes to the modules normal?",https://stackoverflow.com/questions/60104249,11477760,Lack of Alternative Solutions/Documentation
61087933,Inequivalent output from tf.nn.conv2d and keras.layers.Conv2D,"<p>I've been reading the <em>Hands-On Machine Learning</em> textbook (2nd edition) by Aurélien Géron (<a href=""https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/"" rel=""nofollow noreferrer"">textbook publisher webpage here</a>). I've gotten into the content that applies CNNs to images. In the section titled <em>Tensorflow Implementation</em> of Chapter 14, they manually create filters that get passed to <code>tf.nn.conv2d</code> and applied to an image to produce a set of feature maps. After these manual filter examples, the book says:</p>
<blockquote>
<p>in a real CNN you would normally define filters as trainable variables ... Instead of manually creating the variables, use the <code>keras.layers.Conv2D</code> layer.</p>
</blockquote>
<p>The above quote implies to me that given identical inputs (and equivalent initializations), we should be able to derive identical outputs from <code>tf.nn.conv2d</code> and <code>keras.layers.Conv2D</code>. To validate this idea, I looked up whether the two functions were equivalent. According to <a href=""https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d"">this previously answered SO post</a>, <strong>for convolution, the two functions are the same</strong>.</p>
<p>I set out to perform a simple test of their equivalence. I created a convolutional layer consisting of one feature map using a 7x7 filter (a.k.a: <em>convolutional kernel</em>) <strong>of all zeros</strong> that was implemented separately for <code>tf.nn.conv2d</code> and <code>keras.layers.Conv2D</code>. As expected, after summing all the pixel values in the difference of both images, this filter did cause the output images to have a value of zero for each pixel value. This difference of zero implies that the output images are identical.</p>
<p>I then decided to create the same 7x7 filter, but <strong>with all ones</strong> this time. Ideally, both functions should produce the same output, therefore the difference in the two output images should be zero. Unfortunately, when I check the difference in the output images (and sum the differences at each pixel), I get a nonzero sum value. Upon plotting the images and their difference, it is evident that they are not the same image (though they do look <em>very</em> similar at a glance).</p>
<p>After reading through the documentation for both functions, I believe that I am giving them equivalent inputs. <strong>What could I be doing/assuming incorrectly that is preventing both functions from producing identical outputs?</strong></p>
<p>I have attached my code and versionining information below for reference. The code uses the scikit-learn <code>china.jpg</code> sample image as input and <code>matplotlib.pyplot.imshow</code> to help in visualizing the output images and their difference.</p>
<blockquote>
<p>TF Version: 2.2.0-dev20200229</p>
<p>Keras Version: 2.3.1</p>
<p>Scikit-Learn Version: 0.22.1</p>
<p>Matplotlib Version: 3.1.3</p>
<p>Numpy Version: 1.18.1</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>from sklearn.datasets import load_sample_image
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
import numpy as np

# Get the feature map as a result of tf.nn.conv2d
def featureMap1(batch):
    
    # Extract the channels
    batch_size, height, width, channels = batch.shape

    # Make a (7,7,3,1) filter set (one set of a 7x7 filter per channel)
    # of just ones. 
    filters = np.ones(shape=(7, 7, channels, 1), dtype=np.float32)

    # Run the conv2d with stride of 1 (i.e: in.shape = out.shape)
    # Generate one feature map for this conv layer
    fmaps = tf.nn.conv2d(batch, filters,
                         strides=1, padding='SAME',
                         data_format='NHWC')
    
    # Return the feature map
    return fmaps

# Get the feature map as a result of keras.layers.Conv2D
def featureMap2(batch):

    # Create the input layer with the shape of the images
    inputLayer = keras.layers.Input(shape=batch.shape[1:])
    
    # Create the convLayer which should apply the filter of all ones
    convLayer = keras.layers.Conv2D(filters=1, kernel_size=7,
                                    strides=1, padding='SAME',
                                    kernel_initializer='ones',
                                    data_format='channels_last',
                                    activation='linear')

    # Create the ouput layer
    outputLayer = convLayer(inputLayer)

    # Set up the model
    model = keras.Model(inputs=inputLayer,
                        outputs=outputLayer)

    # Perform a prediction, no model fitting or compiling
    fmaps = model.predict(batch)

    return fmaps 

def main():

    # Get the image and scale the RGB values to [0, 1]
    china = load_sample_image('china.jpg') / 255

    # Build a batch of just one image
    batch = np.array([china])

    # Get the feature maps and extract
    # the images within them
    img1 = featureMap1(batch)[0, :, :, 0]
    img2 = featureMap2(batch)[0, :, :, 0]

    # Calculate the difference in the images
    # Ideally, this should be all zeros...
    diffImage = np.abs(img1 - img2)

    # Add up all the pixels in the diffImage,
    # we expect a value of 0 if the images are
    # identical
    print('Differences value: ', diffImage.sum())

    # Plot the images as a set of 4
    figsize = 10
    f, axarr = plt.subplots(2, 2, figsize=(figsize,figsize))

    axarr[0,0].set_title('Original Image')
    axarr[0,0].imshow(batch[0], cmap='gray')

    axarr[1,0].set_title('Conv2D through tf.nn.conv2d')
    axarr[1,0].imshow(img1, cmap='gray')
    
    axarr[1,1].set_title('Conv2D through keras.layers.Conv2D')
    axarr[1,1].imshow(img2, cmap='gray')

    axarr[0,1].set_title('Diff')
    axarr[0,1].imshow(diffImage, cmap='gray')
    
    plt.show()
    
    return


main()
</code></pre>
","I've been reading the Hands-On Machine Learning textbook (2nd edition) by Aurélien Géron (textbook publisher webpage here). I've gotten into the content that applies CNNs to images. In the section titled Tensorflow Implementation of Chapter 14, they manually create filters that get passed to tf.nn.conv2d and applied to an image to produce a set of feature maps. After these manual filter examples, the book says: The above quote implies to me that given identical inputs (and equivalent initializations), we should be able to derive identical outputs from tf.nn.conv2d and keras.layers.Conv2D. To validate this idea, I looked up whether the two functions were equivalent. According to this previously answered SO post, for convolution, the two functions are the same. I set out to perform a simple test of their equivalence. I created a convolutional layer consisting of one feature map using a 7x7 filter (a.k.a: convolutional kernel) of all zeros that was implemented separately for tf.nn.conv2d and keras.layers.Conv2D. As expected, after summing all the pixel values in the difference of both images, this filter did cause the output images to have a value of zero for each pixel value. This difference of zero implies that the output images are identical. I then decided to create the same 7x7 filter, but with all ones this time. Ideally, both functions should produce the same output, therefore the difference in the two output images should be zero. Unfortunately, when I check the difference in the output images (and sum the differences at each pixel), I get a nonzero sum value. Upon plotting the images and their difference, it is evident that they are not the same image (though they do look very similar at a glance). After reading through the documentation for both functions, I believe that I am giving them equivalent inputs. What could I be doing/assuming incorrectly that is preventing both functions from producing identical outputs? I have attached my code and versionining information below for reference. The code uses the scikit-learn china.jpg sample image as input and matplotlib.pyplot.imshow to help in visualizing the output images and their difference.",https://stackoverflow.com/questions/61087933,3217457,Documentation Ambiguity
44762631,How to use tf.nn.ctc_loss in cnn+ctc network,"<p>Recently, I try to use tensorflow to implement a cnn+ctc network base on the article <a href=""https://arxiv.org/pdf/1701.02720.pdf"" rel=""nofollow noreferrer""><em>Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks</em></a>.</p>

<p>I try to feed batch spectrogram data (shape:(10,120,155,3),batch_size is 10) into 10 convolution layer and 3 fully connected layer. So the output before connecting the ctc layer is 2d data(shape:(10,1024)). </p>

<p>Here is my problem: I want to use tf.nn.ctc_loss function in tensorflow library,but it generate the <em>ValueError: Dimension must be 2 but is 3 for 'transpose'(op:'Transpose') with input shapes:[?,1024],[3]</em>.</p>

<p>I guess the error is related to the dimension of my 2d input data. The discription of the ctc_loss function in tensorflow official site is require a 3d input with the shape (batch_size x max_time x num_classes).</p>

<p>So, what is the extra dimension of 'num_classes' ? what should I change the shape of my cnn+fc output data?</p>
","Recently, I try to use tensorflow to implement a cnn+ctc network base on the article Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks. I try to feed batch spectrogram data (shape:(10,120,155,3),batch_size is 10) into 10 convolution layer and 3 fully connected layer. So the output before connecting the ctc layer is 2d data(shape:(10,1024)). Here is my problem: I want to use tf.nn.ctc_loss function in tensorflow library,but it generate the ValueError: Dimension must be 2 but is 3 for 'transpose'(op:'Transpose') with input shapes:[?,1024],[3]. I guess the error is related to the dimension of my 2d input data. The discription of the ctc_loss function in tensorflow official site is require a 3d input with the shape (batch_size x max_time x num_classes). So, what is the extra dimension of 'num_classes' ? what should I change the shape of my cnn+fc output data?",https://stackoverflow.com/questions/44762631,3732470,Requesting (Additional) Resources
72128138,How tensorflow.nn.ctc_loss works in tensorflow 2 and how to use it for handwritting recognition?,"<p>I try to create a simple model for handwritting recognition with tensorflow 2.8 based on <a href=""https://keras.io/examples/vision/handwriting_recognition/#resizing-images-without-distortion"" rel=""nofollow noreferrer"">this keras example</a> but I have trouble understanding how the CTC loss <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss"" rel=""nofollow noreferrer"">tensorflow.nn.ctc_loss</a> works and how to use it in my code.</p>
<p>I use images representing words from the <a href=""https://fki.tic.heia-fr.ch/databases/iam-handwriting-database"" rel=""nofollow noreferrer"">IAM dataset</a> :</p>
<ul>
<li>Images are resized to shape (256, 64, 1)</li>
<li>Words are tokenized to sequences of shape 21 (max word length)</li>
</ul>
<pre class=""lang-py prettyprint-override""><code>IMG_WIDTH = 256
IMG_HEIGHT = 64

def preprocess_image(image_path, width=IMG_WIDTH, height=IMG_HEIGHT):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_png(image)
    image = tf.image.resize_with_pad(image, height, width, method=&quot;lanczos3&quot;)
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.transpose(image, perm=[1, 0, 2])
    image = tf.image.flip_left_right(image)

    return image

tokenizer = tf.keras.preprocessing.text.Tokenizer(
    filters=&quot;&quot;,
    lower=False,
    split=&quot;&quot;,
    char_level=True,
    oov_token=&quot;_&quot;
)

tokenizer.fit_on_texts(transcriptions)
print(&quot; &quot;.join(sorted(tokenizer.word_counts.keys())))
#  ! &quot; # &amp; ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z

MAX_LEN_WORD = max(len(wm.transcription) for wm in words_meta)

def pad_sequences(sequences):
    return tf.keras.preprocessing.sequence.pad_sequences(
        sequences, 
        maxlen=MAX_LEN_WORD, 
        padding=&quot;pre&quot;, 
        truncating=&quot;pre&quot;, 
        # value=tokenizer.word_index[pad_value]
    )

print(pad_sequences(tokenizer.texts_to_sequences([&quot;hello&quot;])))
# array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 4, 12, 13, 3]])
</code></pre>
<p>I tried to reuse the CTCLayer from the <a href=""https://keras.io/examples/vision/handwriting_recognition/#resizing-images-without-distortion"" rel=""nofollow noreferrer"">keras example</a> :</p>
<pre class=""lang-py prettyprint-override""><code>class CTCLayer(tf.keras.layers.Layer):

    def __init__(self, name=None):
        super(CTCLayer, self).__init__(name=name)

    def call(self, y_true, y_pred):
        batch_len = tf.cast(tf.shape(y_true)[0], dtype=&quot;int64&quot;)
        input_length = tf.cast(tf.shape(y_pred)[1], dtype=&quot;int64&quot;)
        label_length = tf.cast(tf.shape(y_true)[1], dtype=&quot;int64&quot;)

        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=&quot;int64&quot;)
        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=&quot;int64&quot;)
        loss = tf.nn.ctc_loss(y_true, y_pred, input_length, label_length)
        self.add_loss(loss)

        # At test time, just return the computed predictions.
        return y_pred
</code></pre>
<p>But when I try to construct a model with it I get an error :</p>
<pre class=""lang-py prettyprint-override""><code>input_img = tf.keras.Input(shape=(IMG_WIDTH, IMG_HEIGHT, 1), name=&quot;images&quot;)
labels = tf.keras.Input(shape=(MAX_LEN_WORD,), name=&quot;labels&quot;, dtype=tf.int32)

x = tf.keras.layers.Conv2D(64, 3, activation=&quot;relu&quot;, padding=&quot;same&quot;)(input_img)
x = tf.keras.layers.MaxPooling2D((2, 2))(x)

shape = x.get_shape()

x = tf.keras.layers.Reshape((shape[1], shape[2] * shape[3]))(x)

x = tf.keras.layers.Dense(64, activation=&quot;relu&quot;)(x)

x = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(128, return_sequences=True)
)(x)

x = tf.keras.layers.Dense(
    len(tokenizer.word_counts) + 2, activation=&quot;softmax&quot;
)(x)


output = CTCLayer(name=&quot;ctc_loss&quot;)(labels, x)

model = tf.keras.models.Model(
    inputs=[input_img, labels], outputs=output, name=&quot;handwriting_recognizer&quot;
)


model.compile(optimizer=&quot;adam&quot;)
model.summary()
</code></pre>
<p>The error :</p>
<pre><code>ValueError
Traceback (most recent call last) handwriting_reco.ipynb Cell 31' in &lt;cell line: 22&gt;()
     13 x = tf.keras.layers.Bidirectional(
     14     tf.keras.layers.LSTM(128, return_sequences=True)
     15 )(x)
     17 x = tf.keras.layers.Dense(
     18     len(tokenizer.word_counts) + 2, activation=&quot;softmax&quot;
     19 )(x)
---&gt; 22 output = CTCLayer(name=&quot;ctc_loss&quot;)(labels, x)
     24 model = tf.keras.models.Model(
     25     inputs=[input_img, labels], outputs=output, name=&quot;handwriting_recognizer&quot;
     26 )
     29 model.compile(optimizer=&quot;adam&quot;)

File C:\Apps\anaconda\envs\tf\lib\site-packages\keras\utils\traceback_utils.py:67, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File C:\Apps\anaconda\envs\tf\lib\site-packages\tensorflow\python\autograph\impl\api.py:692, in convert.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(*args, **kwargs)
    690 except Exception as e:  # pylint:disable=broad-except
    691   if hasattr(e, 'ag_error_metadata'):
--&gt; 692     raise e.ag_error_metadata.to_exception(e)
    693   else:
    694     raise

ValueError: Exception encountered when calling layer &quot;ctc_loss&quot; (type CTCLayer).

in user code:

    File &quot;ipykernel_25428\280978635.py&quot;, line 14, in call  *
        loss = self.loss_fn(y_true, y_pred, input_length, label_length)

    ValueError: Dimensions must be equal, but are 2 and 22 for '{{node ctc_loss/ctc_loss_dense/mul_1}} = Mul[T=DT_FLOAT](ctc_loss/ctc_loss_dense/ones, ctc_loss/ctc_loss_dense/one_hot_2)' with input shapes: [2,1,?], [22,?,1].


Call arguments received:
  • y_true=tf.Tensor(shape=(None, 21), dtype=int32)
  • y_pred=tf.Tensor(shape=(None, 128, 81), dtype=float32)
</code></pre>
<p>I also tried to use the <code>tf.nn.ctc_loss</code> function with simple tensors but I do not get how it works.</p>
<p>How could I use the CTC loss for my model in this use case ?</p>
<p>EDIT :</p>
<p>If I remove the <code>CTCLayer</code> this is what I obtain with <code>model.summary()</code> :</p>
<pre><code>Model: &quot;handwriting_recognizer&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 images (InputLayer)            [(None, 256, 64, 1)  0           []                               
                                ]                                                                 
                                                                                                  
 conv2d_1 (Conv2D)              (None, 256, 64, 64)  640         ['images[0][0]']                 
                                                                                                  
 max_pooling2d_1 (MaxPooling2D)  (None, 128, 32, 64)  0          ['conv2d_1[0][0]']               
                                                                                                  
 reshape_1 (Reshape)            (None, 128, 2048)    0           ['max_pooling2d_1[0][0]']        
                                                                                                  
 dense_2 (Dense)                (None, 128, 64)      131136      ['reshape_1[0][0]']              
                                                                                                  
 bidirectional_1 (Bidirectional  (None, 128, 256)    197632      ['dense_2[0][0]']                
 )                                                                                                
                                                                                                  
 labels (InputLayer)            [(None, 21)]         0           []                               
                                                                                                  
 dense_3 (Dense)                (None, 128, 81)      20817       ['bidirectional_1[0][0]']        
                                                                                                  
==================================================================================================
Total params: 350,225
Trainable params: 350,225
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre>
",I try to create a simple model for handwritting recognition with tensorflow 2.8 based on this keras example but I have trouble understanding how the CTC loss tensorflow.nn.ctc_loss works and how to use it in my code. I use images representing words from the IAM dataset : I tried to reuse the CTCLayer from the keras example : But when I try to construct a model with it I get an error : The error : I also tried to use the tf.nn.ctc_loss function with simple tensors but I do not get how it works. How could I use the CTC loss for my model in this use case ? EDIT : If I remove the CTCLayer this is what I obtain with model.summary() :,https://stackoverflow.com/questions/72128138,7685195,Documentation Ambiguity
44107208,In tf.nn.dropout what is the effect of keep_prob argument?,"<p>I am learning tf.nn.dropout command. Documentation says that with probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0. The scaling is so that the expected sum is unchanged. Can someone please explain that why we take 1/keep_prob. And if I set its value 0.1. Does it mean that I am keeping only 10 percent nodes?</p>
","I am learning tf.nn.dropout command. Documentation says that with probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0. The scaling is so that the expected sum is unchanged. Can someone please explain that why we take 1/keep_prob. And if I set its value 0.1. Does it mean that I am keeping only 10 percent nodes?",https://stackoverflow.com/questions/44107208,7997184,Documentation Ambiguity
43147428,MNIST Tensorflow example,"<pre><code>def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding='SAME')
</code></pre>

<p>This is the code from the <a href=""https://www.tensorflow.org/get_started/mnist/pros#convolution_and_pooling"" rel=""nofollow noreferrer"">Deep MNIST for experts</a> tutorial on Tensorflow website.</p>

<p>I have two questions:</p>

<p>1) The <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/max_pool"" rel=""nofollow noreferrer"">documentation</a> k-size is an integer list of length greater than 4 that refers to the size of the max-pool window. Shouldn't that be just [2,2] considering that it's a 2X2 window? I mean why is it [1, 2, 2, 1] instead of [2,2] ?</p>

<p>2) If we are taking a stride step on size one. Why do we need a vector of 4 values, wouldn't one value suffice?</p>

<pre><code>strides = [1]
</code></pre>

<p>3) If padding = 'SAME' why does the image size decrease by half? ( from 28 X 28 to 14 X 14  in the first convolutional process )</p>
","This is the code from the Deep MNIST for experts tutorial on Tensorflow website. I have two questions: 1) The documentation k-size is an integer list of length greater than 4 that refers to the size of the max-pool window. Shouldn't that be just [2,2] considering that it's a 2X2 window? I mean why is it [1, 2, 2, 1] instead of [2,2] ? 2) If we are taking a stride step on size one. Why do we need a vector of 4 values, wouldn't one value suffice? 3) If padding = 'SAME' why does the image size decrease by half? ( from 28 X 28 to 14 X 14 in the first convolutional process )",https://stackoverflow.com/questions/43147428,6203717,Documentation Ambiguity
46165406,tensorflow tf getting error cross entropy in built function,"<p>I have two functions as below, they come from Andrew Ng is deep learning course on coursera. The first function runs, but the second doesn't. logits and labels variables have the same shape as per the document <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits"" rel=""nofollow noreferrer"">requirements</a> I changed cost to <code>[0.0,0.0,1.0,1.0]</code>, but it didn't help :(</p>

<p>in case of the first function I am directly passing variables from the function call into the function </p>

<p>1)</p>

<pre><code>def one_hot_matrix(labels, C):
    """"""
    Creates a matrix where the i-th row corresponds to the ith class number and the jth column
                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) 
                     will be 1. 

    Arguments:
    labels -- vector containing the labels 
    C -- number of classes, the depth of the one hot dimension

    Returns: 
    one_hot -- one hot matrix
    """"""

    ### START CODE HERE ###

    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)
    #C = tf.constant(C, name = 'C')
    #C = tf.placeholder(tf.int32, name = 'C')
    #labels = tf.placeholder(tf.int32, name = 'labels')

    # Use tf.one_hot, be careful with the axis (approx. 1 line)
    one_hot_matrix = tf.one_hot(labels, C, axis=0)

    # Create the session (approx. 1 line)
    sess = tf.Session()

    # Run the session (approx. 1 line)
    #one_hot = sess.run(one_hot_matrix)
    one_hot = sess.run(one_hot_matrix)

    # Close the session (approx. 1 line). See method 1 above.
    sess.close()

    ### END CODE HERE ###

    return one_hot

labels = np.array([1,2,3,0,2,1])
one_hot = one_hot_matrix(labels, C = 4)
print (""one_hot = "" + str(one_hot))
</code></pre>

<p>2)</p>

<pre><code>    def cost(logits, labels):
    """"""
    Computes the cost using the sigmoid cross entropy
    
    Arguments:
    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)
    labels -- vector of labels y (1 or 0) 

    Note: What we've been calling ""z"" and ""y"" in this class are respectively called ""logits"" and ""labels"" 
    in the TensorFlow documentation. So logits will feed into z, and labels into y. 
    
    Returns:
    cost -- runs the session of the cost (formula (2))
    """"""

    ### START CODE HERE ### 

    # Create the placeholders for ""logits"" (z) and ""labels"" (y) (approx. 2 lines)
    z = tf.placeholder(tf.float32, name = 'z')
    y = tf.placeholder(tf.float32, name = 'y')



    # Use the loss function (approx. 1 line)
    #cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = z,  labels = y)
    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,  labels=labels)

    # Create a session (approx. 1 line). See method 1 above.
    sess = tf.Session()

    # Run the session (approx. 1 line).
    #cost = sess.run(cost, feed_dict = {z: logits, y:labels})
    cost = sess.run(cost)

    # Close the session (approx. 1 line). See method 1 above.
    sess.close()

    ### END CODE HERE ###

    return cost

logits = sigmoid(np.array([0.2,0.4,0.7,0.9]))
cost = cost(logits, np.array([0,0,1,1]))
print (""cost = "" + str(cost))
</code></pre>

<p>The error is </p>

<pre><code> ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-61-51f13e22d2ec&gt; in &lt;module&gt;()
      1 logits = sigmoid(np.array([0.2,0.4,0.7,0.9]))
----&gt; 2 cost = cost(logits, np.array([0,0,1,1]))
      3 print (""cost = "" + str(cost))

&lt;ipython-input-60-3febf014323d&gt; in cost(logits, labels)
     26     # Use the loss function (approx. 1 line)
     27     #cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = z,  labels = y)
---&gt; 28     cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,  labels=labels)
     29 
     30     # Create a session (approx. 1 line). See method 1 above.

/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py in sigmoid_cross_entropy_with_logits(_sentinel, labels, logits, name)
    169     relu_logits = array_ops.where(cond, logits, zeros)
    170     neg_abs_logits = array_ops.where(cond, -logits, logits)
--&gt; 171     return math_ops.add(relu_logits - logits * labels,
    172                         math_ops.log1p(math_ops.exp(neg_abs_logits)),
    173                         name=name)

/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    827       if not isinstance(y, sparse_tensor.SparseTensor):
    828         try:
--&gt; 829           y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
    830         except TypeError:
    831           # If the RHS is not a tensor, it might be a tensor aware object

/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)
    674       name=name,
    675       preferred_dtype=preferred_dtype,
--&gt; 676       as_ref=False)
    677 
    678 

/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    739 
    740         if ret is None:
--&gt; 741           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    742 
    743         if ret is NotImplemented:

/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    612     raise ValueError(
    613         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r""
--&gt; 614         % (dtype.name, t.dtype.name, str(t)))
    615   return t
    616 

ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int64: 'Tensor(""logistic_loss_4/labels:0"", shape=(4,), dtype=int64)'
</code></pre>
","I have two functions as below, they come from Andrew Ng is deep learning course on coursera. The first function runs, but the second doesn't. logits and labels variables have the same shape as per the document requirements I changed cost to [0.0,0.0,1.0,1.0], but it didn't help :( in case of the first function I am directly passing variables from the function call into the function 1) 2) The error is",https://stackoverflow.com/questions/46165406,2543622,Requesting (Additional) Resources
47120680,Why can't the output of the network go through a softmax when using softmax_cross_entropy_with_logits?,"<p>I want to use the tensorflow built-in cross-entropy function. However, in the documentation, I'm reading </p>

<blockquote>
  <p>Do not call this op with the output of softmax, as it will produce
  incorrect results.</p>
</blockquote>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits</a></p>

<p>Like it is done often, I am using the softmax activation in my last output layer, however: </p>

<pre><code>result = tf.layers.dense(input=dropout, classes_num, tf.nn.softmax)
</code></pre>

<p>Is it, therefore, incorrect to use this function, or is the documentation incorrect? I don't understand this, I would be thankful for a short explanation. 
(Which TensorFlow cost function would be correct to use for a softmax output layer then?)</p>
","I want to use the tensorflow built-in cross-entropy function. However, in the documentation, I'm reading https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits Like it is done often, I am using the softmax activation in my last output layer, however: Is it, therefore, incorrect to use this function, or is the documentation incorrect? I don't understand this, I would be thankful for a short explanation. (Which TensorFlow cost function would be correct to use for a softmax output layer then?)",https://stackoverflow.com/questions/47120680,5919010,Documentation Completeness
49930682,Getting InvalidArgumentError in softmax_cross_entropy_with_logits,"<p>I'm pretty new to tensorflow and trying to do some experiments with the Iris dataset. I created following model function (MWE):</p>

<pre><code>def model_fn(features, labels, mode):
    net = tf.feature_column.input_layer(features, [tf.feature_column.numeric_column(key=key) for key in FEATURE_NAMES])

    logits = tf.layers.dense(inputs=net, units=3)

    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)

    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
    train_op = optimizer.minimize(
        loss=loss,
        global_step=tf.train.get_global_step())

    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)
</code></pre>

<p>Unfortunately I get the following error:</p>

<pre><code>InvalidArgumentError: Input to reshape is a tensor with 256 values, but the requested shape has 1
 [[Node: Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](softmax_cross_entropy_with_logits_sg, Reshape/shape)]]
</code></pre>

<p>Seems to be some problem with the shapes of the tensors. However both logits and labels have an equal shape of (256, 3) - as it is required by the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits"" rel=""nofollow noreferrer"">documentation</a>. Also both tensors have type float32.</p>

<hr>

<p>Just for the sake of completeness, here is the input function for the estimator:</p>

<pre><code>import pandas as pd
import tensorflow as tf
import numpy as np

IRIS_DATA = ""data/iris.csv""

FEATURE_NAMES = [""sepal_length"", ""sepal_width"", ""petal_length"", ""petal_width""]
CLASS_NAME = [""class""]

COLUMNS = FEATURE_NAMES + CLASS_NAME

# read dataset
iris = pd.read_csv(IRIS_DATA, header=None, names=COLUMNS)

# encode classes
iris[""class""] = iris[""class""].astype('category').cat.codes

# train test split
np.random.seed(1)
msk = np.random.rand(len(iris)) &lt; 0.8
train = iris[msk]
test = iris[~msk]

def iris_input_fn(batch_size=256, mode=""TRAIN""):
    def prepare_input(data=None):

        #do mean normaization across all samples
        mu = np.mean(data)
        sigma = np.std(data)

        data = data - mu
        data = data / sigma
        is_nan = np.isnan(data)
        is_inf = np.isinf(data)
        if np.any(is_nan) or np.any(is_inf):
            print('data is not well-formed : is_nan {n}, is_inf: {i}'.format(n= np.any(is_nan), i=np.any(is_inf)))


        data = transform_data(data)
        return data

    def transform_data(data):
        data = data.astype(np.float32)
        return data


    def load_data():
        global train

        trn_all_data=train.iloc[:,:-1]
        trn_all_labels=train.iloc[:,-1]


        return (trn_all_data.astype(np.float32),
                                              trn_all_labels.astype(np.int32))

    data, labels = load_data()
    data = prepare_input(data)

    labels = tf.one_hot(labels, depth=3)

    labels = tf.cast(labels, tf.float32)
    dataset = tf.data.Dataset.from_tensor_slices((data.to_dict(orient=""list""), labels))

    dataset = dataset.shuffle(1000).repeat().batch(batch_size)

    return dataset.make_one_shot_iterator().get_next()
</code></pre>

<p>Dataset from <a href=""https://archive.ics.uci.edu/ml/datasets/iris"" rel=""nofollow noreferrer"">UCI repo</a></p>
","I'm pretty new to tensorflow and trying to do some experiments with the Iris dataset. I created following model function (MWE): Unfortunately I get the following error: Seems to be some problem with the shapes of the tensors. However both logits and labels have an equal shape of (256, 3) - as it is required by the documentation. Also both tensors have type float32. Just for the sake of completeness, here is the input function for the estimator: Dataset from UCI repo",https://stackoverflow.com/questions/49930682,8444976,Documentation Replicability
40350849,Rank mismatch: Rank of labels (received 2) should equal rank of logits minus 1 (received 2),"<p>I'm building DNN to predict if the object is present in the image or not. My network has two hidden layers and the last layer looks like this:</p>

<pre><code>  # Output layer
  W_fc2 = weight_variable([2048, 1])
  b_fc2 = bias_variable([1])

  y = tf.matmul(h_fc1, W_fc2) + b_fc2
</code></pre>

<p>Then I have placeholder for labels:</p>

<pre><code>y_ = tf.placeholder(tf.float32, [None, 1], 'Output')
</code></pre>

<p>I run training in batches (therefore first argument in Output layer shape is None). </p>

<p>I use the following loss function:</p>

<pre><code>cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(
    y[:, :1], y_[:, :1], name='xentropy')
loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')
predict_hand = tf.greater(y, 0.5)
correct_prediction = tf.equal(tf.to_float(predict_hand), y_)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
</code></pre>

<p>But in runtime I got the following error:</p>

<blockquote>
  <p>Rank mismatch: Rank of labels (received 2) should equal rank of logits
  minus 1 (received 2).</p>
</blockquote>

<p>I guess I should reshape labels layer, but not sure what it expects. I looked up in <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#sparse_softmax_cross_entropy_with_logits"" rel=""noreferrer"">documentation</a> and it says:</p>

<blockquote>
  <p>logits: Unscaled log probabilities of rank r and shape [d_0, d_1, ...,
  d_{r-2}, num_classes] and dtype float32 or float64. labels: Tensor of
  shape [d_0, d_1, ..., d_{r-2}] and dtype int32 or int64. Each entry in
  labels must be an index in [0, num_classes).</p>
</blockquote>

<p>If I have just single class, what my labels should look like (now it is just 0 or 1)? Any help appreciated</p>
","I'm building DNN to predict if the object is present in the image or not. My network has two hidden layers and the last layer looks like this: Then I have placeholder for labels: I run training in batches (therefore first argument in Output layer shape is None). I use the following loss function: But in runtime I got the following error: I guess I should reshape labels layer, but not sure what it expects. I looked up in documentation and it says: If I have just single class, what my labels should look like (now it is just 0 or 1)? Any help appreciated",https://stackoverflow.com/questions/40350849,76590,Requesting (Additional) Resources
54942875,"How could I get part of a tensor with indices coming from another tensor? (Keras, tf backend)","<p>I am working with RNN and attention mechanisms. </p>

<p>Different from soft attention (which weights time steps of RNN by attention tensor and sum up them), I want to select the most <code>k</code> important time steps and using other methods to combine them.</p>

<p>Thus, I have three tensors:</p>

<ul>
<li><code>RNNMatrix</code>,  with shape <code>(None, time_steps, cell_num)</code>, where <code>None</code> is the batch size</li>
<li><code>AttMatrix</code>,  with shape <code>(None, time_steps)</code></li>
<li><code>IdxMatrix</code>, with shape <code>(None, k)</code>, each row is the index of the most k important time steps, get from <code>AttMatrix</code></li>
</ul>

<p>Ideally, I can get <code>KRNNMatrix</code> shaped <code>(None, k, cell_num)</code> through <code>RNNMatrix</code> and <code>IdxMatrix</code>. </p>

<p>The straightforward method to get <code>IdxMatrix</code> is using <code>tf.nn.top_k</code>. My code below:</p>

<pre><code>selected_att_num = 10

def My_select_layer(ip, k_num):

    LSTM_out = ip[0]
    Attention = ip[1]

    idx = tf.nn.top_k(Attention, k=k_num, sorted = True).indices

    # gen row index
    row_idx = tf.range(tf.shape(Attention)[0])

    # repeat k times
    row_idx_repeat = tf.keras.backend.repeat(tf.reshape(row_idx,(-1,1)), k_num)

    # gen index for gather_nd
    row_idx_repeat_reshape = tf.reshape(row_idx_repeat, (-1,k_num))
    idx_stacked = tf.stack([row_idx_repeat_reshape, idx], axis=-1)

    return tf.gather_nd(LSTM_out, idx_stacked)


_input = Input(shape = (data_length, data_dim))
LSTM_out = LSTM(cells, return_sequences=True)(_input )

# compute att for each step
Attention = Activation('tanh')(LSTM_out)
Attention = Dense(1, use_bias=False)(Attention)
Attention = Flatten()(Attention)
Attention = Activation('softmax', name='attention')(Attention)

LSTM_out_gathered = Lambda(My_select_layer,arguments={'k_num':selected_att_num})([LSTM_out , Attention])
</code></pre>

<p>The model complies well and throws an error when I train it:</p>

<p>ValueError: An operation has <code>None</code> for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.</p>

<p>I'm pretty sure this error blame to my Lambda layer, most probably blame to the variable <code>indices</code> of <code>tf.nn.top_k</code>. </p>

<p>But I don't know how to fix it. </p>

<p>Or, is the operation <em>'select tensor by index'</em> theoretically nondifferentiable? If so, is there any workaround?<br>
In the worst case, could reinforcement learning hold this?</p>

<p>I also search a lot on the internet and find a very similar <a href=""https://stackoverflow.com/questions/46526869/keras-tensors-get-values-with-indices-coming-from-another-tensor/46527020#46527020"">question</a> asked by Daniel Möller. However, Daniel does not show more details about how he solves this problem. </p>
","I am working with RNN and attention mechanisms. Different from soft attention (which weights time steps of RNN by attention tensor and sum up them), I want to select the most k important time steps and using other methods to combine them. Thus, I have three tensors: Ideally, I can get KRNNMatrix shaped (None, k, cell_num) through RNNMatrix and IdxMatrix. The straightforward method to get IdxMatrix is using tf.nn.top_k. My code below: The model complies well and throws an error when I train it: ValueError: An operation has None for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval. I'm pretty sure this error blame to my Lambda layer, most probably blame to the variable indices of tf.nn.top_k. But I don't know how to fix it. Or, is the operation 'select tensor by index' theoretically nondifferentiable? If so, is there any workaround? In the worst case, could reinforcement learning hold this? I also search a lot on the internet and find a very similar question asked by Daniel Möller. However, Daniel does not show more details about how he solves this problem.",https://stackoverflow.com/questions/54942875,9997549,Requesting (Additional) Resources
43917456,Matrix norm in TensorFlow,"<p>I need to compute the Frobenius norm in order to achieve this formula using the TensorFlow framework:</p>

<p><a href=""https://i.stack.imgur.com/fgJ7o.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fgJ7o.gif"" alt=""Formula""></a></p>

<p>where <code>w</code> is a matrix with 50 rows and 100 columns.</p>

<p>I tried to write something, but I don't understand how to fill out the <code>axis</code> argument.</p>

<pre><code>tf.pow(
    tf.norm(x, ord='fro', axis=?), 2
)
</code></pre>

<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/norm"" rel=""noreferrer"">TensorFlow docs</a> I have to use a 2-tuple (or a 2-list) because it <em>determines the axies in tensor over which to compute a matrix norm</em>, but I simply need a plain Frobenius norm. In <code>SciPy</code>, for example, I can do it without specify any axis.</p>

<p>So, what should I use as <code>axis</code> to emulate the <code>SciPy</code> function?</p>
","I need to compute the Frobenius norm in order to achieve this formula using the TensorFlow framework: where w is a matrix with 50 rows and 100 columns. I tried to write something, but I don't understand how to fill out the axis argument. According to the TensorFlow docs I have to use a 2-tuple (or a 2-list) because it determines the axies in tensor over which to compute a matrix norm, but I simply need a plain Frobenius norm. In SciPy, for example, I can do it without specify any axis. So, what should I use as axis to emulate the SciPy function?",https://stackoverflow.com/questions/43917456,4735866,Documentation Ambiguity
58527671,Finding the input Tensors of a Tensorflow Operation,"<p>I am trying to generate some kind of textual representation for the TensorFlow Computational Graph. I know that Tensorboard can provide me with the visualization. However, I need some kind of representation (adjacency matrix or adjacency list) from where I can parse information associated with graphs.</p>

<p>So far, I have tried the following:</p>

<pre><code>import tensorflow as tf

a = tf.constant(1.3, name = const_a)
b = tf.constant(3.1, name = const_b)
c = tf.add(a,b, name = 'addition')
d = tf.multiply(c,a, name = 'multiplication')
e = tf.add(d,c, name = 'addition_1')

with tf.Session() as sess:
     print(sess.run([c,d,e]))
</code></pre>

<p>After this, I decided to keep the graph object in a separate variable and tried to parse information from there:</p>

<pre><code>graph = tf.get_default_graph()
</code></pre>

<p>I found out how to get the list of all operations from <a href=""https://www.tensorflow.org/api_docs/python/tf/Operation"" rel=""nofollow noreferrer"">this</a> documentation.</p>

<pre><code>for op in graph.get_operations():
     print(op.values())
</code></pre>

<p>This part actually provides me with the information of the nodes of the computation graph.</p>

<pre><code>(&lt;tf.Tensor 'const_a:0' shape=() dtype=float32&gt;,)
(&lt;tf.Tensor 'const_b:0' shape=() dtype=float32&gt;,)
(&lt;tf.Tensor 'addition:0' shape=() dtype=float32&gt;,)
(&lt;tf.Tensor 'multiplication:0' shape=() dtype=float32&gt;,)
(&lt;tf.Tensor 'addition_1:0' shape=() dtype=float32&gt;,)
</code></pre>

<p>However, I cannot seem to find any method that can provide me with information regarding the edges of the computation graph. I cannot find any method that can give me the input tensors associated with each operation. I would like to know that the operation named <code>addition_1</code> has input tensors produced by the operations <code>addition</code> and <code>multiplication;</code> or something that can be used to derive this information. From the <a href=""https://www.tensorflow.org/api_docs/python/tf/Operation"" rel=""nofollow noreferrer"">documentation</a>, it seems that the <code>Operation</code> object has a property named <code>inputs</code> which may be the thing I am looking for. Nonetheless, I don't see a method that can be called to return this property.</p>
","I am trying to generate some kind of textual representation for the TensorFlow Computational Graph. I know that Tensorboard can provide me with the visualization. However, I need some kind of representation (adjacency matrix or adjacency list) from where I can parse information associated with graphs. So far, I have tried the following: After this, I decided to keep the graph object in a separate variable and tried to parse information from there: I found out how to get the list of all operations from this documentation. This part actually provides me with the information of the nodes of the computation graph. However, I cannot seem to find any method that can provide me with information regarding the edges of the computation graph. I cannot find any method that can give me the input tensors associated with each operation. I would like to know that the operation named addition_1 has input tensors produced by the operations addition and multiplication; or something that can be used to derive this information. From the documentation, it seems that the Operation object has a property named inputs which may be the thing I am looking for. Nonetheless, I don't see a method that can be called to return this property.",https://stackoverflow.com/questions/58527671,6868602,Lack of Alternative Solutions/Documentation
41613767,tf.parse_example used in mnist export example,"<p>I am new to tensorflow and are reading mnist_export.py in tensorflow serving example.</p>

<p>There is something here I cannot understand:</p>

<pre><code>  sess = tf.InteractiveSession()
  serialized_tf_example = tf.placeholder(tf.string, name='tf_example')
  feature_configs = {
      'x': tf.FixedLenFeature(shape=[784], dtype=tf.float32),
  }
  tf_example = tf.parse_example(serialized_tf_example, feature_configs)
  x = tf.identity(tf_example['x'], name='x')  # use tf.identity() to assign name
</code></pre>

<p>Above, serialized_tf_example is a Tensor.</p>

<p>I have read the api document <a href=""https://www.tensorflow.org/api_docs/python/io_ops/converting#parse_example"" rel=""nofollow noreferrer"">tf.parse_example</a> but it seems that <code>serialized</code> is serialized <code>Example</code> protos like:</p>

<pre><code>serialized = [
  features
    { feature { key: ""ft"" value { float_list { value: [1.0, 2.0] } } } },
  features
    { feature []},
  features
    { feature { key: ""ft"" value { float_list { value: [3.0] } } }
]
</code></pre>

<p>So how to understand <code>tf_example = tf.parse_example(serialized_tf_example, feature_configs)</code> here as <code>serialized_tf_example</code> is a Tensor, not <code>Example</code> proto?</p>
","I am new to tensorflow and are reading mnist_export.py in tensorflow serving example. There is something here I cannot understand: Above, serialized_tf_example is a Tensor. I have read the api document tf.parse_example but it seems that serialized is serialized Example protos like: So how to understand tf_example = tf.parse_example(serialized_tf_example, feature_configs) here as serialized_tf_example is a Tensor, not Example proto?",https://stackoverflow.com/questions/41613767,5685754,Documentation Ambiguity
39112622,How do I set TensorFlow RNN state when state_is_tuple=True?,"<p>I have written an <a href=""https://github.com/wpm/tfrnnlm"" rel=""noreferrer"">RNN language model using TensorFlow</a>. The model is implemented as an <code>RNN</code> class. The graph structure is built in the constructor, while <code>RNN.train</code> and <code>RNN.test</code> methods run it.</p>

<p>I want to be able to reset the RNN state when I move to a new document in the training set, or when I want to run a validation set during training. I do this by managing the state inside the training loop, passing it into the graph via a feed dictionary.</p>

<p>In the constructor I define the the RNN like so</p>

<pre><code>    cell = tf.nn.rnn_cell.LSTMCell(hidden_units)
    rnn_layers = tf.nn.rnn_cell.MultiRNNCell([cell] * layers)
    self.reset_state = rnn_layers.zero_state(batch_size, dtype=tf.float32)
    self.state = tf.placeholder(tf.float32, self.reset_state.get_shape(), ""state"")
    self.outputs, self.next_state = tf.nn.dynamic_rnn(rnn_layers, self.embedded_input, time_major=True,
                                                  initial_state=self.state)
</code></pre>

<p>The training loop looks like this</p>

<pre><code> for document in document:
     state = session.run(self.reset_state)
     for x, y in document:
          _, state = session.run([self.train_step, self.next_state], 
                                 feed_dict={self.x:x, self.y:y, self.state:state})
</code></pre>

<p><code>x</code> and <code>y</code> are batches of training data in a document. The idea is that I pass the latest state along after each batch, except when I start a new document, when I zero out the state by running <code>self.reset_state</code>.</p>

<p>This all works.  Now I want to change my RNN to use the recommended <code>state_is_tuple=True</code>. However, I don't know how to pass the more complicated LSTM state object via a feed dictionary. Also I don't know what arguments to pass to the <code>self.state = tf.placeholder(...)</code> line in my constructor.</p>

<p>What is the correct strategy here? There still isn't much example code or documentation for <code>dynamic_rnn</code> available.</p>

<hr>

<p>TensorFlow issues <a href=""https://github.com/tensorflow/tensorflow/issues/2695"" rel=""noreferrer"">2695</a> and <a href=""https://github.com/tensorflow/tensorflow/issues/2838"" rel=""noreferrer"">2838</a> appear relevant.</p>

<p>A <a href=""http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/"" rel=""noreferrer"">blog post</a> on WILDML addresses these issues but doesn't directly spell out the answer.</p>

<p>See also <a href=""https://stackoverflow.com/questions/38241410/tensorflow-remember-lstm-state-for-next-batch-stateful-lstm"">TensorFlow: Remember LSTM state for next batch (stateful LSTM)</a>.</p>
","I have written an RNN language model using TensorFlow. The model is implemented as an RNN class. The graph structure is built in the constructor, while RNN.train and RNN.test methods run it. I want to be able to reset the RNN state when I move to a new document in the training set, or when I want to run a validation set during training. I do this by managing the state inside the training loop, passing it into the graph via a feed dictionary. In the constructor I define the the RNN like so The training loop looks like this x and y are batches of training data in a document. The idea is that I pass the latest state along after each batch, except when I start a new document, when I zero out the state by running self.reset_state. This all works. Now I want to change my RNN to use the recommended state_is_tuple=True. However, I don't know how to pass the more complicated LSTM state object via a feed dictionary. Also I don't know what arguments to pass to the self.state = tf.placeholder(...) line in my constructor. What is the correct strategy here? There still isn't much example code or documentation for dynamic_rnn available. TensorFlow issues 2695 and 2838 appear relevant. A blog post on WILDML addresses these issues but doesn't directly spell out the answer. See also TensorFlow: Remember LSTM state for next batch (stateful LSTM).",https://stackoverflow.com/questions/39112622,1120370,Requesting (Additional) Resources
48038883,Could we always use the list type as the function parameter when the tuple type parameter is expected?,"<p>I have been studying the Tensorflow API written in Python. I have two questions.</p>

<p><strong>1. Can we always use a list type as a function parameter when a tuple is expected?</strong></p>

<p>If we look at the official API definition about the <code>tf.placeholder</code> and its examples (<a href=""https://www.tensorflow.org/api_docs/python/tf/placeholder"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/placeholder</a>), we see that the second parameter of this function is the ""shape"". In the example code, we can see that a tuple is used to provide the shape information as a parameter as shown below.</p>

<pre><code>x = tf.placeholder(tf.float32, shape=(1024, 1024))
</code></pre>

<p>However, in the official tutorial page (<a href=""https://www.tensorflow.org/get_started/mnist/beginners"" rel=""nofollow noreferrer"">https://www.tensorflow.org/get_started/mnist/beginners</a>), the example uses the <strong>list</strong> as the shape rather than using the tuple as shown below.</p>

<pre><code>y_ = tf.placeholder(tf.float32, [None, 10])
</code></pre>

<p>I know that there are some differences between list and tuple such as being immutable vs mutable.</p>

<p>If the list supports all the functionality of a tuple, then could we always use the list instead of the tuple safely as a function parameter? And is it recommended?</p>

<p><strong>2. What's the meaning of [None, 10] in the above example code?</strong></p>

<p>In the above example code, [None, 10] is used. Are such expressions normally used? If so, then is ""None"" also considered as a kind of number?</p>
","I have been studying the Tensorflow API written in Python. I have two questions. 1. Can we always use a list type as a function parameter when a tuple is expected? If we look at the official API definition about the tf.placeholder and its examples (https://www.tensorflow.org/api_docs/python/tf/placeholder), we see that the second parameter of this function is the ""shape"". In the example code, we can see that a tuple is used to provide the shape information as a parameter as shown below. However, in the official tutorial page (https://www.tensorflow.org/get_started/mnist/beginners), the example uses the list as the shape rather than using the tuple as shown below. I know that there are some differences between list and tuple such as being immutable vs mutable. If the list supports all the functionality of a tuple, then could we always use the list instead of the tuple safely as a function parameter? And is it recommended? 2. What's the meaning of [None, 10] in the above example code? In the above example code, [None, 10] is used. Are such expressions normally used? If so, then is ""None"" also considered as a kind of number?",https://stackoverflow.com/questions/48038883,735008,Documentation Ambiguity
46647760,What is input and data parameters in tf.Print function?,"<p>There are <code>input</code> and <code>data</code> parameters in <a href=""https://www.tensorflow.org/api_docs/python/tf/Print"" rel=""nofollow noreferrer"">tf.Print function</a>. What do they mean? I don't understand this from documentaion. </p>

<p>Suppose I wish to print tensor <code>A</code>. Should I pass it as <code>input</code>, <code>data</code>, both or someother way?</p>
","There are input and data parameters in tf.Print function. What do they mean? I don't understand this from documentaion. Suppose I wish to print tensor A. Should I pass it as input, data, both or someother way?",https://stackoverflow.com/questions/46647760,258483,Documentation Completeness
49182351,How to print SparseTensor contents in TensorFlow?,"<p>For quick debugging purposes, I'm trying to print out the SparseTensor I've just initialized.</p>

<p>The built-in print function just says it's a SparseTensor object, and tf.Print() gives an error. The error statement does print the contents of the object, but not in a way that shows the actual entries (unless it's telling me it's empty, there's some :0s I don't know the significance of).</p>

<pre><code>rows = tf.Print(rows, [rows])

TypeError: Failed to convert object of type &lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensor'&gt; to Tensor. Contents: SparseTensor(indices=Tensor(""SparseTensor/indices:0"", shape=(6, 2), dtype=int64), values=Tensor(""SparseTensor/values:0"", shape=(6,), dtype=float32), dense_shape=Tensor(""SparseTensor/dense_shape:0"", shape=(2,), dtype=int64)). Consider casting elements to a supported type.
</code></pre>
","For quick debugging purposes, I'm trying to print out the SparseTensor I've just initialized. The built-in print function just says it's a SparseTensor object, and tf.Print() gives an error. The error statement does print the contents of the object, but not in a way that shows the actual entries (unless it's telling me it's empty, there's some :0s I don't know the significance of).",https://stackoverflow.com/questions/49182351,2430134,Requesting (Additional) Resources
50655213,Does input_ parameter matter to tf.Print?,"<p>I read the description for the parameter <code>input_</code> of <code>tf.Print</code> in <a href=""https://www.tensorflow.org/api_docs/python/tf/Print"" rel=""nofollow noreferrer"">this link</a>. I tried a couple of experiments and got the results makes me so confused.</p>

<p>I used this following code to experiment</p>

<pre><code>A = tf.constant([[1, 2, 3], [4, 5, 6]])
a1, a2 = tf.split(A, 2, axis=0)
p = tf.Print(A, [a1, a2])
with tf.Session() as sess:
    sess.run([p])
</code></pre>

<p>Output</p>

<p><code>[[1 2 3]][[4 5 6]]</code></p>

<p>I have replaced the line of code <code>p = tf.Print(A, [a1, a2])</code> with <code>p = tf.Print(a1, [a1, a2])</code> or <code>p = tf.Print(a2, [a1, a2])</code> and got exactly the same output: <code>[[1 2 3]][[4 5 6]]</code>. This makes me feel that ""<em>it does not matter what <code>input_</code> is, you can pass whatever you want</em>""</p>

<p>My questions are</p>

<ol>
<li>Does <code>input_</code> parameter matter to <code>tf.Print</code>?</li>
<li>If it does, could you guys suggest me an example?</li>
</ol>

<p>I found a similar question <a href=""https://stackoverflow.com/q/46647760/5657159"">here</a>, but IMO it does not cover the aspect what I wonder in this question.</p>
","I read the description for the parameter input_ of tf.Print in this link. I tried a couple of experiments and got the results makes me so confused. I used this following code to experiment Output [[1 2 3]][[4 5 6]] I have replaced the line of code p = tf.Print(A, [a1, a2]) with p = tf.Print(a1, [a1, a2]) or p = tf.Print(a2, [a1, a2]) and got exactly the same output: [[1 2 3]][[4 5 6]]. This makes me feel that ""it does not matter what input_ is, you can pass whatever you want"" My questions are I found a similar question here, but IMO it does not cover the aspect what I wonder in this question.",https://stackoverflow.com/questions/50655213,5657159,Documentation Replication on Other Examples
56146885,How can I inspect the contents of my tensor using TensorFlow’s eager execution?,"<p>I use TensorFlow 1.12 using eager execution, and I have the following (incomplete) function in which I want to inspect some intermediate tensor:</p>

<pre><code>def parse_example(example_proto, width, height, num_classes):
    features = {
        'image/encoded': tf.FixedLenFeature((), tf.string),
        'image/height': tf.FixedLenFeature((), tf.int64),
        'image/width': tf.FixedLenFeature((), tf.int64),
        'image/filename': tf.FixedLenFeature((), tf.string),
        'image/object/bbox/xmin': tf.VarLenFeature(tf.float32),
        'image/object/bbox/xmax': tf.VarLenFeature(tf.float32),
        'image/object/bbox/ymin': tf.VarLenFeature(tf.float32),
        'image/object/bbox/ymax': tf.VarLenFeature(tf.float32),
        'image/object/class/label': tf.VarLenFeature(tf.int64),
        'image/object/class/text': tf.VarLenFeature(tf.string),
        'image/object/mask': tf.VarLenFeature(tf.string),
        'image/depth': tf.FixedLenFeature((), tf.string)
    }

    parsed_example = tf.parse_single_example(example_proto, features)

    #print(tf.sparse_tensor_to_dense(parsed_example['image/object/mask'], default_value=0))

    # Decode image
    image = tf.image.decode_jpeg(parsed_example['image/encoded'])
    parsed_example['image/encoded'] = image

    # Depth + RGBD
    depth = utilities.decode_depth(parsed_example['image/depth'])
    parsed_example['image/depth'] = depth
    rgbd = tf.concat([tf.image.convert_image_dtype(image, tf.float32), depth], axis=2)
    rgbd = tf.reshape(rgbd, shape=tf.stack([height, width, 4]))
    parsed_example['image/rgbd'] = rgbd

    mask = tf.sparse.to_dense(parsed_example['image/object/mask'], default_value="""")
    mask = tf.map_fn(utilities.decode_png_mask, mask, dtype=tf.uint8)
    mask = tf.reshape(mask, shape=tf.stack([-1, height, width]), name='mask')
    print(mask)
    sys.exit()
</code></pre>

<p>However, <code>print(mask)</code> merely returns <code>Tensor(""mask:0"", shape=(?, 1000, 1200), dtype=uint8)</code>, while I would like to see the actual values. This should be possible, as demonstrated in <a href=""https://www.tensorflow.org/guide/eager"" rel=""nofollow noreferrer"">TensorFlow’s eager execution guide</a>. I also tried <code>tf.print(mask, output_stream=sys.stdout)</code>, but only a blank line is being printed. <code>mask.dtype</code> is <code>uint8</code>, so I guess it should contain integers, given that is has <em>a</em> shape. What I also find strange is that <code>mask.device</code> is the empty string. It should be stored on some device, right?</p>

<p>How can I print the contents of the <code>mask</code> tensor?</p>
","I use TensorFlow 1.12 using eager execution, and I have the following (incomplete) function in which I want to inspect some intermediate tensor: However, print(mask) merely returns Tensor(""mask:0"", shape=(?, 1000, 1200), dtype=uint8), while I would like to see the actual values. This should be possible, as demonstrated in TensorFlow’s eager execution guide. I also tried tf.print(mask, output_stream=sys.stdout), but only a blank line is being printed. mask.dtype is uint8, so I guess it should contain integers, given that is has a shape. What I also find strange is that mask.device is the empty string. It should be stored on some device, right? How can I print the contents of the mask tensor?",https://stackoverflow.com/questions/56146885,,Documentation Replication on Other Examples
70007364,tf.print gives TypeSpec TypeError,"<p>What does this error message mean?</p>
<pre><code>TypeError: Could not build a TypeSpec for name: &quot;tf.print/PrintV2&quot;
op: &quot;PrintV2&quot;
input: &quot;tf.print/StringFormat&quot;
attr {
  key: &quot;end&quot;
  value {
    s: &quot;\n&quot;
  }
}
attr {
  key: &quot;output_stream&quot;
  value {
    s: &quot;stdout&quot;
  }
}
 of unsupported type &lt;class 'google3.third_party.tensorflow.python.framework.ops.Operation'&gt;
</code></pre>
<p>I'm printing the shape of a tensor.  My code &quot;works&quot; without the print, so I'm sure it is this statement, and the tensor is valid.  I can print the shape of a tensor in a test colab. I'm clueless how to narrow this down and debug this.  My failure is in a big hairy program.</p>
<p>I can't find any information on the web about what might be causing this error.</p>
<p>What does it mean when I get a TypeSpec error from a tf.print?</p>
<p>-- Malcolm
(TF 2.7.0)</p>
","What does this error message mean? I'm printing the shape of a tensor. My code ""works"" without the print, so I'm sure it is this statement, and the tensor is valid. I can print the shape of a tensor in a test colab. I'm clueless how to narrow this down and debug this. My failure is in a big hairy program. I can't find any information on the web about what might be causing this error. What does it mean when I get a TypeSpec error from a tf.print? -- Malcolm (TF 2.7.0)",https://stackoverflow.com/questions/70007364,5662360,Lack of Alternative Solutions/Documentation
51668059,How to use PriorityQueue in TensorFlow?,"<p>I seem to have an issue with using <code>tf.PriorityQueue</code> in tensorflow. The documentation says that the shapes argument is not required for initialization. I cannot specify the shape of the tensor as it is dynamic and the shape is determined at runtime. </p>

<p>From the tensorflow documentation on <a href=""https://www.tensorflow.org/api_docs/python/tf/PriorityQueue"" rel=""nofollow noreferrer"">tf.PriorityQueue</a>:</p>

<pre><code>__init__(capacity,types,shapes=None,names=None,shared_name=None,name='priority_queue')
</code></pre>

<blockquote>
  <p>Args:</p>
  
  <p>capacity: An integer. The upper bound on the number of elements that may be stored in this queue.</p>
  
  <p>types: A list of DType objects. The length of types must equal the number of 
  tensors in each queue element, except the first priority element. The first tensor in each element is the priority, which must be type int64.</p>
  
  <p>shapes: (Optional.) A list of fully-defined TensorShape objects, with the same length as types, or None.</p>
  
  <p>names: (Optional.) A list of strings naming the components in the queue with the same length as dtypes, or None. If specified, the dequeue methods return a dictionary with the names as keys.</p>
  
  <p>shared_name: (Optional.) If non-empty, this queue will be shared under the given name across multiple sessions.</p>
  
  <p>name: Optional name for the queue operation.</p>
</blockquote>

<p>However, the following code produces a TypeError:</p>

<pre><code>def build_queue():
    with tf.name_scope(""Queue""):
        q = tf.PriorityQueue(capacity=2,types=tf.uint8,name=""iq"",shared_name=""queue"")
    return q

File ""C:\Users\devar\Documents\EngProj\SSPlayer\test\dist_cnn.py"", line 212, in create_model
    infer_q = build_infer_queue()
  File ""C:\Users\devar\Documents\EngProj\SSPlayer\test\dist_cnn.py"", line 143, in build_queue
    shared_name=""queue"")
  File ""C:\Users\devar\Envs\RL\lib\site-packages\tensorflow\python\ops\data_flow_ops.py"", line 903, in __init__
    name=name)
  File ""C:\Users\devar\Envs\RL\lib\site-packages\tensorflow\python\ops\gen_data_flow_ops.py"", line 3409, in priority_queue_v2

TypeError: Expected list for 'shapes' argument to 'priority_queue_v2' Op, not None.
</code></pre>

<p>Any ideas on what I am doing wrong?</p>
","I seem to have an issue with using tf.PriorityQueue in tensorflow. The documentation says that the shapes argument is not required for initialization. I cannot specify the shape of the tensor as it is dynamic and the shape is determined at runtime. From the tensorflow documentation on tf.PriorityQueue: However, the following code produces a TypeError: Any ideas on what I am doing wrong?",https://stackoverflow.com/questions/51668059,8538168,Documentation Replicability
38992445,Tensorflow: Py_func returns unknown shape,"<p>I have a simple question re the <code>tf.py_func</code> function.</p>

<p>I have an image tensor <code>my_img</code> of shape <code>(1,224,224,3)</code>. To test <code>py_func</code>, I feed the tensor to a python function <code>return_tf</code> that should give back the same tensor (after being converted to a numpy array as per docs). </p>

<p>Here's the code:</p>

<pre><code>def return_tf(x):
   return np.array(x)

test = tf.py_func(return_tf,[my_img],[tf.float32])
</code></pre>

<p>But when I checked the shape of the returned tensor called <code>test</code>, I get:</p>

<pre><code>tf.Tensor 'PyFunc:0' shape=unknown dtype=float32
</code></pre>

<p>I am also unable to run <code>eval()</code> on the tensor, since I get the error:</p>

<pre><code>AttributeError: 'list' object has no attribute 'eval'.
</code></pre>

<p>Anyone knows how could I fix the tensor shape of the tensor returned by <code>tf.py_func</code>?</p>
","I have a simple question re the tf.py_func function. I have an image tensor my_img of shape (1,224,224,3). To test py_func, I feed the tensor to a python function return_tf that should give back the same tensor (after being converted to a numpy array as per docs). Here's the code: But when I checked the shape of the returned tensor called test, I get: I am also unable to run eval() on the tensor, since I get the error: Anyone knows how could I fix the tensor shape of the tensor returned by tf.py_func?",https://stackoverflow.com/questions/38992445,6720221,Documentation Replicability
40418038,Tensorflow Custom Piecewise Cost Function,"<p>I need to define a custom cost function in tensorflow, which is piecewise in nature, something like this:</p>

<pre><code>def f1(a,b):
    pass #Will do Calculation on a and b
def f2(a,b):
    pass #Will do Calculation on a and b

def customCostFunction(calculated,target):
    if(target &gt; 0): #Trivial Criteria
       return f1(calculated,target)
    else:
       return f2(calculated,target)
</code></pre>

<p>I am aware that <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/script_ops.html"" rel=""nofollow noreferrer"">tf.py_func</a> exists, but I am unsure exactly how to use it in the above case. In essence, for each value in each of the tensors (predicted vs target), I need the predicted and the target values to be passed through a function defined by me, which will return slightly different results depending on the values passed through (I have two strictly different learning cases).</p>

<p>Then the training step would be defined like this:</p>

<pre><code>train = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE).minimize(tf.reduce_sum(customCostFunction(model,targets),0))
</code></pre>
","I need to define a custom cost function in tensorflow, which is piecewise in nature, something like this: I am aware that tf.py_func exists, but I am unsure exactly how to use it in the above case. In essence, for each value in each of the tensors (predicted vs target), I need the predicted and the target values to be passed through a function defined by me, which will return slightly different results depending on the values passed through (I have two strictly different learning cases). Then the training step would be defined like this:",https://stackoverflow.com/questions/40418038,1834057,Requesting (Additional) Resources
57838412,How do I create a layer from a function that does not accept Tensors/NumPy arrays as arguments?,"<p>I have two Python functions that take strings as inputs and return NumPy arrays. I am trying to use these functions to create Lambda layers that are then fed into another Keras model.</p>

<p>I can vectorize the function, and then create a TensorFlow operation via tf.py_func, like so (full code is further down below):</p>

<pre><code>def indices_tensor(tensor):
  return tf.py_func(np.vectorize(indices),[tensor],tf.float32)
def segments_tensor(tensor):
  return tf.py_func(np.vectorize(segments),[tensor],tf.float32)
</code></pre>

<pre><code>pretrained_path = 'uncased_L-12_H-768_A-12'
config_path = os.path.join(pretrained_path, 'bert_config.json')
checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')
vocab_path = os.path.join(pretrained_path, 'vocab.txt')
</code></pre>

<pre><code># TF_KERAS must be added to environment variables in order to use TPU
os.environ['TF_KERAS'] = '1'
import codecs
from keras_bert import load_trained_model_from_checkpoint
</code></pre>

<pre><code>token_dict = {}
with codecs.open(vocab_path, 'r', 'utf8') as reader:
    for line in reader:
        token = line.strip()
        token_dict[token] = len(token_dict)
</code></pre>

<pre><code>model = load_trained_model_from_checkpoint(config_path, checkpoint_path)
import numpy as np
from keras_bert import Tokenizer
tokenizer = Tokenizer(token_dict)
def tokenize(text):
  tokens = tokenizer.tokenize(text)
  indices, segments = tokenizer.encode(first=text, max_len=512)
  return indices,segments
</code></pre>

<pre><code>def indices(text):
  return tokenize(text)[0]
def segments(text):
  return tokenize(text)[1]
#@title Get indices and segments of a tensor of strings
def indices_tensor(tensor):
  return tf.py_func(np.vectorize(indices),[tensor],tf.float32)
def segments_tensor(tensor):
  return tf.py_func(np.vectorize(segments),[tensor],tf.float32)
</code></pre>

<pre><code>input_layer = Input(shape=(1,),dtype=tf.string)
indices_layer = Lambda(indices_tensor)(input_layer)
segments_layer = Lambda(segments_tensor)(input_layer)
``
**Logging**
print(type(indices_layer))
print(type(segments_layer))
</code></pre>

<p>I want my Lambda function calls to produce Lambda layers, and <a href=""https://keras.io/layers/core/"" rel=""nofollow noreferrer"">Keras's documentation</a> appears to suggest they should. Instead, per my indicated logging statements, they are creating Tensors.</p>

<p>What should I do to create working layers?</p>
","I have two Python functions that take strings as inputs and return NumPy arrays. I am trying to use these functions to create Lambda layers that are then fed into another Keras model. I can vectorize the function, and then create a TensorFlow operation via tf.py_func, like so (full code is further down below): I want my Lambda function calls to produce Lambda layers, and Keras's documentation appears to suggest they should. Instead, per my indicated logging statements, they are creating Tensors. What should I do to create working layers?",https://stackoverflow.com/questions/57838412,10083113,Requesting (Additional) Resources
54697451,Output TFRecord to Google Cloud Storage from Python,"<p>I know <code>tf.python_io.TFRecordWriter</code> has a concept of GCS, but it doesn't seem to have permissions to write to it.</p>

<p>If I do the following:</p>

<pre><code>output_path = 'gs://my-bucket-name/{}/{}.tfrecord'.format(object_name, record_name)
writer = tf.python_io.TFRecordWriter(output_path)
# write to writer
writer.close()
</code></pre>

<p>then I get 401s saying ""Anonymous caller does not have storage.objects.create access to my-bucket-name.""</p>

<p>However, on the same machine, if I do <code>gsutil rsync -d r gs://my-bucket-name bucket-backup</code>, it properly syncs it, so I've authenticated properly using gcloud.</p>

<p>How can I give <code>TFRecordWriter</code> permissions to write to GCS? I'm going to just use Google's GCP python API for now, but I'm sure there's a way to do this using TF alone.</p>
","I know tf.python_io.TFRecordWriter has a concept of GCS, but it doesn't seem to have permissions to write to it. If I do the following: then I get 401s saying ""Anonymous caller does not have storage.objects.create access to my-bucket-name."" However, on the same machine, if I do gsutil rsync -d r gs://my-bucket-name bucket-backup, it properly syncs it, so I've authenticated properly using gcloud. How can I give TFRecordWriter permissions to write to GCS? I'm going to just use Google's GCP python API for now, but I'm sure there's a way to do this using TF alone.",https://stackoverflow.com/questions/54697451,1129436,Requesting (Additional) Resources
55368272,"How do you index a RaggedTensor along the ragged dimension, in TensorFlow?","<p>I need to get values in a ragged tensor by indexing along the ragged dimension. Some indexing works (<code>[:, :x]</code>, <code>[:, -x:]</code> or <code>[:, x:y]</code>), but not direct indexing (<code>[:, x]</code>):</p>

<pre class=""lang-py prettyprint-override""><code>R = tf.RaggedTensor.from_tensor([[1, 2, 3], [4, 5, 6]])
print(R[:, :2]) # RaggedTensor([[1, 2], [4, 5]])
print(R[:, 1:2]) # RaggedTensor([[2], [5]])
print(R[:, 1])  # ValueError: Cannot index into an inner ragged dimension.
</code></pre>

<p>The <a href=""https://www.tensorflow.org/guide/ragged_tensors"" rel=""noreferrer"">documentation</a> explains why this fails:</p>

<blockquote>
  <p>RaggedTensors supports multidimensional indexing and slicing, with one
  restriction: indexing into a ragged dimension is not allowed. This
  case is problematic because the indicated value may exist in some rows
  but not others. In such cases, it's not obvious whether we should (1)
  raise an IndexError; (2) use a default value; or (3) skip that value
  and return a tensor with fewer rows than we started with. Following
  the guiding principles of Python (""In the face of ambiguity, refuse
  the temptation to guess"" ), we currently disallow this operation.</p>
</blockquote>

<p>This makes sense, but how do I actually implement options 1, 2 and 3? Must I convert the ragged array into a Python array of Tensors, and manually iterate over them? Is there a more efficient solution? One that would work 100% in a TensorFlow graph, without going through the Python interpreter?</p>
","I need to get values in a ragged tensor by indexing along the ragged dimension. Some indexing works ([:, :x], [:, -x:] or [:, x:y]), but not direct indexing ([:, x]): The documentation explains why this fails: This makes sense, but how do I actually implement options 1, 2 and 3? Must I convert the ragged array into a Python array of Tensors, and manually iterate over them? Is there a more efficient solution? One that would work 100% in a TensorFlow graph, without going through the Python interpreter?",https://stackoverflow.com/questions/55368272,38626,Inadequate Examples
50255035,Tensorflow dataset with partial shuffle,"<p>I am playing with TensorFlow's dataset API, and I am confused by the shuffle() method, according to the docs:</p>

<blockquote>
  <p>The Dataset.shuffle() transformation randomly shuffles the input dataset using a similar algorithm to tf.RandomShuffleQueue: it maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer.</p>
</blockquote>

<p>If I only 'partially' shuffle my dataset (e.g. buffer_size &lt;= no. of elements), I'd expect only the first <em>buffer_size</em> elements will be shuffled, however this is not the case, see example:</p>

<pre><code>dataset = tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6,7,8])
                         .shuffle(buffer_size=4, seed=42)
                         .batch(2)
iter = dataset.make_initializable_iterator() # create the iterator
el = iter.get_next()
with tf.Session() as sess:
    sess.run(iter.initializer) 
    print('batch:', sess.run(el))
</code></pre>

<p>output:</p>

<pre><code>batch: [2 5]
</code></pre>

<p>why is 5 here? as the buffer size is only 4? the first 2 elements should be within 1~4 right? what am I missing here?</p>

<p>Thanks</p>
","I am playing with TensorFlow's dataset API, and I am confused by the shuffle() method, according to the docs: If I only 'partially' shuffle my dataset (e.g. buffer_size &lt;= no. of elements), I'd expect only the first buffer_size elements will be shuffled, however this is not the case, see example: output: why is 5 here? as the buffer size is only 4? the first 2 elements should be within 1~4 right? what am I missing here? Thanks",https://stackoverflow.com/questions/50255035,1182671,Documentation Ambiguity
50997293,tf.reduce_mean prints strange results when reduction_indices is specified,"<p>tf.reduce_mean command is used to take mean along specified axis. But if don't specify axis and use reduction indices zero then it prints middle row. Why is it so? How this reduction indices effects output values? If I print sum.eval() command why it prints [[4,5,6]]?
I also go through the documentation but it doesn't give any explanation for reduction_indices.Please clear my concept about this command.</p>

<pre><code>a = tf.range(0,12)
b = tf.reshape(a,(4,3))
sum = tf.reduce_mean(b,reduction_indices=[0],keep_dims=True)   
sess = tf.InteractiveSession()
print (b.eval())
print (b.get_shape())
print (sum.eval())
print (sum.get_shape())
</code></pre>
","tf.reduce_mean command is used to take mean along specified axis. But if don't specify axis and use reduction indices zero then it prints middle row. Why is it so? How this reduction indices effects output values? If I print sum.eval() command why it prints [[4,5,6]]? I also go through the documentation but it doesn't give any explanation for reduction_indices.Please clear my concept about this command.",https://stackoverflow.com/questions/50997293,9854153,Documentation Completeness
69623404,tf optimizer compute_gradients error Dimension size,"<p>I am trying to utilize <a href=""https://tensorflow.google.cn/responsible_ai/privacy/api_docs/python/tf_privacy/v1/DPGradientDescentGaussianOptimizer#class-variables"" rel=""nofollow noreferrer""><code>DPGradientDescentGaussianOptimizer</code></a> to calculate compute_gradients.</p>
<p>By followed the <a href=""https://tensorflow.google.cn/responsible_ai/privacy/api_docs/python/tf_privacy/v1/DPGradientDescentGaussianOptimizer#class-variables"" rel=""nofollow noreferrer"">documents</a>, <code>sparse_softmax_cross_entropy_with_logits</code> is used then applied to <code>tf.reduce_mean()</code> to calculate the loss.</p>
<pre><code>    crx_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
                        labels=context[:, 1:], logits=output['logits'][:, :-1])
    print('crx_entropy_loss', crx_entropy_loss.shape, crx_entropy_loss)
    loss = tf.reduce_mean(crx_entropy_loss)
    print('reduce_mean_loss', loss.shape, loss)
</code></pre>
<p>which,</p>
<pre><code>crx_entropy_loss -&gt; 
shape: (1, ?) from Tensor(&quot;SparseSoftmaxCrossEntropyWithLogits/Reshape_2:0&quot;, shape=(1, ?), dtype=float32)

reduce_mean_loss -&gt; 
shape: () from Tensor(&quot;Mean:0&quot;, shape=(), dtype=float32)
</code></pre>
<p>But, when I tried to calculate optimizer gradients <code>compute_gradients</code>, it shows dimension error.</p>
<pre><code>opt = dp_optimizer.DPGradientDescentGaussianOptimizer(
            l2_norm_clip=l2_norm_clip,
            noise_multiplier=noise_multiplier,
            num_microbatches= batch_size, 
            unroll_microbatches=True,
            ledger=ledger,
            learning_rate=learning_rate )

opt_grads = opt.compute_gradients(loss, train_vars)
</code></pre>
<p>Error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/framework/ops.py&quot;, line 1607, in _create_c_op
c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl
.
InvalidArgumentError
:
Dimension size must be evenly divisible by 128 but is 1 for 'Reshape' (op: 'Reshape') with input shapes: [], [2] and with input tensors computed as partial shapes: input[1] = [128,?].

During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File &quot;GenerateTextMy.py&quot;, line 80, in &lt;module&gt;
main()
  File &quot;GenerateTextMy.py&quot;, line 50, in main
gpt2.finetune(sess, folder, batch_size = 128, multi_gpu = True, sample_every = 10000,
  File &quot;/home/gpt_2_simple_dp/gpt_2.py&quot;, line 253, in finetune
opt_grads = opt.compute_gradients(loss, train_vars)
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_privacy/privacy/optimizers/dp_optimizer.py&quot;, line 134, in compute_gradients
microbatches_losses = tf.reshape(loss, [self._num_microbatches, -1])
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/ops/array_ops.py&quot;, line 131, in reshape
result = gen_array_ops.reshape(tensor, shape, name)
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/ops/gen_array_ops.py&quot;, line 8114, in reshape
_, _, _op = _op_def_lib._apply_op_helper(
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/framework/op_def_library.py&quot;, line 792, in _apply_op_helper
op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/util/deprecation.py&quot;, line 513, in new_func
return func(*args, **kwargs)
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/framework/ops.py&quot;, line 3356, in create_op
return self._create_op_internal(op_type, inputs, dtypes, input_types, name,
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/framework/ops.py&quot;, line 3418, in _create_op_internal
ret = Operation(
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/framework/ops.py&quot;, line 1769, in __init__
self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,
  File &quot;/usr/local/lib/python3.8/dist-packages/tensorflow_core/python/framework/ops.py&quot;, line 1610, in _create_c_op
raise ValueError(str(e))
ValueError
:
Dimension size must be evenly divisible by 128 but is 1 for 'Reshape' (op: 'Reshape') with input shapes: [], [2] and with input tensors computed as partial shapes: input[1] = [128,?].
</code></pre>
<p>I did google and looked through StackOverflow with related topics, however, I still don't quite get the issue. My code is only able to execute when the batch size is 1.
May I ask for some advice or some help?</p>
<p>In addition, why my loss is in shape 0 or None, then it will show the error?</p>
<p>Version:
Tensorflow-1.15
Python 3.8.10</p>
<p>Thanks!</p>
","I am trying to utilize DPGradientDescentGaussianOptimizer to calculate compute_gradients. By followed the documents, sparse_softmax_cross_entropy_with_logits is used then applied to tf.reduce_mean() to calculate the loss. which, But, when I tried to calculate optimizer gradients compute_gradients, it shows dimension error. Error: I did google and looked through StackOverflow with related topics, however, I still don't quite get the issue. My code is only able to execute when the batch size is 1. May I ask for some advice or some help? In addition, why my loss is in shape 0 or None, then it will show the error? Version: Tensorflow-1.15 Python 3.8.10 Thanks!",https://stackoverflow.com/questions/69623404,5238467,Documentation Replication on Other Examples
34236252,What is the difference between np.mean and tf.reduce_mean?,"<p>In the <a href=""https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html"" rel=""noreferrer"">MNIST beginner tutorial</a>, there is the statement </p>

<pre><code>accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
</code></pre>

<p><code>tf.cast</code> basically changes the type of tensor the object is, but what is the difference between <a href=""https://www.tensorflow.org/api_docs/python/tf/reduce_mean"" rel=""noreferrer""><code>tf.reduce_mean</code></a> and <a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html"" rel=""noreferrer""><code>np.mean</code></a>? </p>

<p>Here is the doc on <a href=""https://www.tensorflow.org/api_docs/python/tf/reduce_mean"" rel=""noreferrer""><code>tf.reduce_mean</code></a>:</p>

<blockquote>
  <p><code>reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)</code></p>
  
  <p><code>input_tensor</code>: The tensor to reduce. Should have numeric type.</p>
  
  <p><code>reduction_indices</code>: The dimensions to reduce. If <code>None</code> (the defaut), reduces all dimensions.</p>

<pre><code># 'x' is [[1., 1. ]]
#         [2., 2.]]
tf.reduce_mean(x) ==&gt; 1.5
tf.reduce_mean(x, 0) ==&gt; [1.5, 1.5]
tf.reduce_mean(x, 1) ==&gt; [1.,  2.]
</code></pre>
</blockquote>

<p>For a 1D vector, it looks like <code>np.mean == tf.reduce_mean</code>, but I don't understand what's happening in <code>tf.reduce_mean(x, 1) ==&gt; [1.,  2.]</code>. <code>tf.reduce_mean(x, 0) ==&gt; [1.5, 1.5]</code> kind of makes sense, since mean of <code>[1, 2]</code> and <code>[1, 2]</code> is <code>[1.5, 1.5]</code>, but what's going on with <code>tf.reduce_mean(x, 1)</code>?</p>
","In the MNIST beginner tutorial, there is the statement tf.cast basically changes the type of tensor the object is, but what is the difference between tf.reduce_mean and np.mean? Here is the doc on tf.reduce_mean: For a 1D vector, it looks like np.mean == tf.reduce_mean, but I don't understand what's happening in tf.reduce_mean(x, 1) ==&gt; [1., 2.]. tf.reduce_mean(x, 0) ==&gt; [1.5, 1.5] kind of makes sense, since mean of [1, 2] and [1, 2] is [1.5, 1.5], but what's going on with tf.reduce_mean(x, 1)?",https://stackoverflow.com/questions/34236252,678572,Documentation Ambiguity
34141430,Tensorflow Tensor reshape and pad with zeros,"<p>Is there a way to reshape a tensor and pad any overflow with zeros? I know ndarray.reshape does this, but as I understand it, converting a Tensor to an ndarray would require flip-flopping between the GPU and CPU. </p>

<p>Tensorflow's reshape() documentation says the TensorShapes need to have the same number of elements, so perhaps the best way would be a pad() and then reshape()?</p>

<p>I'm trying to achieve:</p>

<pre><code>a = tf.Tensor([[1,2],[3,4]])
tf.reshape(a, [2,3])
a =&gt; [[1, 2, 3],
      [4, 0 ,0]]
</code></pre>
","Is there a way to reshape a tensor and pad any overflow with zeros? I know ndarray.reshape does this, but as I understand it, converting a Tensor to an ndarray would require flip-flopping between the GPU and CPU. Tensorflow's reshape() documentation says the TensorShapes need to have the same number of elements, so perhaps the best way would be a pad() and then reshape()? I'm trying to achieve:",https://stackoverflow.com/questions/34141430,4975126,Requesting (Additional) Resources
48309707,When to use tf.resource and tf.variant?,"<p>TensorFlow DType has tf.resource and tf.variant with pretty vague description. Can someone please explain to me what those two types are for? It'd be great if there are examples. Thanks a lot!</p>
",TensorFlow DType has tf.resource and tf.variant with pretty vague description. Can someone please explain to me what those two types are for? It'd be great if there are examples. Thanks a lot!,https://stackoverflow.com/questions/48309707,5029595,Requesting (Additional) Resources
60091461,What does seq_lengths mean in tensorflow.reverse arguments?,"<p>I'm new to the tensorflow world. With tensorflow.reverse_sequence we need to pass sequential_length but what are its exact requirements?</p>

<p>I played with the values provided in the <a href=""https://www.tensorflow.org/api_docs/python/tf/reverse_sequence"" rel=""nofollow noreferrer"">docs</a>. I cant grasp the concept. I'm curious about its properties and exact usage.</p>

<p>This is my code:</p>

<pre><code>import tensorflow as tf
from tensorflow import keras

#seq_lengths = [7, 2, 3, 5] 

seq_lengths = [0, 0, 0, 0]
input = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0],
         [1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]
output = tf.reverse_sequence(input, seq_lengths, seq_axis=1, batch_axis=0)
print(output)
</code></pre>
",I'm new to the tensorflow world. With tensorflow.reverse_sequence we need to pass sequential_length but what are its exact requirements? I played with the values provided in the docs. I cant grasp the concept. I'm curious about its properties and exact usage. This is my code:,https://stackoverflow.com/questions/60091461,7291498,Documentation Ambiguity
60595140,tensorflow: save model and load model,"<p>Currently trying to make this <a href=""https://github.com/SenticNet/conv-emotion/tree/master/ICON-end-to-end"" rel=""nofollow noreferrer"">repo</a> works. </p>

<p>I'm trying to save the trained model in the local machine so can be applied later. I read in tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/save"" rel=""nofollow noreferrer"">doc</a>, seems pretty intuitive to save the model, by calling <code>tf.save_model.save(object)</code>. But I'm not sure how to apply.</p>

<p>Original code is here: <a href=""https://github.com/SenticNet/conv-emotion/blob/master/ICON-end-to-end/model.py"" rel=""nofollow noreferrer"">model.py</a>
Following is my changes:</p>

<pre><code>import tensorflow as tf

class ICON(tf.Module): # make it a tensorflow modul

    def __init__(self, config, embeddingMatrix, session=None):

    def _build_inputs(self):

    def _build_vars(self):

    def _convolution(self, input_to_conv):

    def _inference(self):

    def batch_fit(self, queries, ownHistory, otherHistory, labels):

        feed_dict = {self._input_queries: queries, self._own_histories: ownHistory, self._other_histories: otherHistory,
                     self._labels: labels}
        loss, _ = self._sess.run([self.loss_op, self.train_op], feed_dict=feed_dict)
        return loss

    def predict(self, queries, ownHistory, otherHistory, ):

        feed_dict = {self._input_queries: queries, self._own_histories: ownHistory, self._other_histories: otherHistory}
        return self._sess.run(self.predict_op, feed_dict=feed_dict)

    def save(self): # attempt to save the model
        tf.saved_model.save(
            self, './output/model')
</code></pre>

<p>The code above produces ValueError as following:
<code>ValueError: Tensor(""ICON/CNN/embedding_matrix:0"", shape=(16832, 300), dtype=float32_ref) must be from the same graph as Tensor(""saver_filename:0"", shape=(), dtype=string).</code></p>
","Currently trying to make this repo works. I'm trying to save the trained model in the local machine so can be applied later. I read in tensorflow's doc, seems pretty intuitive to save the model, by calling tf.save_model.save(object). But I'm not sure how to apply. Original code is here: model.py Following is my changes: The code above produces ValueError as following: ValueError: Tensor(""ICON/CNN/embedding_matrix:0"", shape=(16832, 300), dtype=float32_ref) must be from the same graph as Tensor(""saver_filename:0"", shape=(), dtype=string).",https://stackoverflow.com/questions/60595140,10029273,Documentation Ambiguity
63656778,How to load a trained TF1 protobuf model into TF2?,"<p><em><strong>Update:</strong> This is a bug in tensorflow. Track progress <a href=""https://github.com/tensorflow/tensorflow/issues/42980"" rel=""noreferrer"">here</a>.</em></p>
<p>I have created and trained a model using stable-baselines, which uses Tensorflow 1.
Now I need to use this trained model in an environment where I only have access to Tensorflow 2 or PyTorch.
I figured I would go with Tensorflow 2 as the <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/load"" rel=""noreferrer"">documentation says</a> I should be able to load models created with Tensorflow 1.</p>
<p><em>I can load the pb file without a problem in Tensorflow 1</em>:</p>
<pre><code>global_session = tf.Session()

with global_session.as_default():
    model_loaded = tf.saved_model.load_v2('tensorflow_model')
    model_loaded = model_loaded.signatures['serving_default']

init = tf.global_variables_initializer()
global_session.run(init)
</code></pre>
<p><em>However in Tensorflow 2 I get the following error</em>:</p>
<pre><code>can_be_imported = tf.saved_model.contains_saved_model('tensorflow_model')
assert(can_be_imported)
model_loaded = tf.saved_model.load('tensorflow_model/')

ValueError: Node 'loss/gradients/model/batch_normalization_3/FusedBatchNormV3_1_grad/FusedBatchNormGradV3' has an _output_shapes attribute inconsistent with the GraphDef for output #3: Dimension 0 in both shapes must be equal, but are 0 and 64. Shapes are [0] and [64].
</code></pre>
<p><em>Model definition</em>:</p>
<pre><code>NUM_CHANNELS = 64

BN1 = BatchNormalization()
BN2 = BatchNormalization()
BN3 = BatchNormalization()
BN4 = BatchNormalization()
BN5 = BatchNormalization()
BN6 = BatchNormalization()
CONV1 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')
CONV2 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')
CONV3 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)
CONV4 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)
FC1 = Dense(128)
FC2 = Dense(64)
FC3 = Dense(7)

def modified_cnn(inputs, **kwargs):
    relu = tf.nn.relu
    log_softmax = tf.nn.log_softmax
    
    layer_1_out = relu(BN1(CONV1(inputs)))
    layer_2_out = relu(BN2(CONV2(layer_1_out)))
    layer_3_out = relu(BN3(CONV3(layer_2_out)))
    layer_4_out = relu(BN4(CONV4(layer_3_out)))
    
    flattened = tf.reshape(layer_4_out, [-1, NUM_CHANNELS * 3 * 2]) 
    
    layer_5_out = relu(BN5(FC1(flattened)))
    layer_6_out = relu(BN6(FC2(layer_5_out)))
    
    return log_softmax(FC3(layer_6_out))

class CustomCnnPolicy(CnnPolicy):
    def __init__(self, *args, **kwargs):
        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)

model = PPO2(CustomCnnPolicy, env, verbose=1)
</code></pre>
<p><em>Model saving in TF1:</em></p>
<pre><code>with model.graph.as_default():
    tf.saved_model.simple_save(model.sess, 'tensorflow_model', inputs={&quot;obs&quot;: model.act_model.obs_ph},
                                   outputs={&quot;action&quot;: model.act_model._policy_proba})
</code></pre>
<p>Fully reproducible code can be found in the following 2 google colab notebooks:
<a href=""https://colab.research.google.com/drive/1ftm9vgiJA8LGwLuUFtaG0aYCS9lUrzET?usp=sharing"" rel=""noreferrer"">Tensorflow 1 saving and loading</a>
<a href=""https://colab.research.google.com/drive/18IWxx-eppX2Sjoo2h1hGe0TzCHNiykF1?usp=sharing"" rel=""noreferrer"">Tensorflow 2 loading</a></p>
<p>Direct link to the saved model:
<a href=""https://drive.google.com/drive/folders/1KmfLDIlAqX80Ha1F7MmC314loI2ayiFY?usp=sharing"" rel=""noreferrer"">model</a></p>
","Update: This is a bug in tensorflow. Track progress here. I have created and trained a model using stable-baselines, which uses Tensorflow 1. Now I need to use this trained model in an environment where I only have access to Tensorflow 2 or PyTorch. I figured I would go with Tensorflow 2 as the documentation says I should be able to load models created with Tensorflow 1. I can load the pb file without a problem in Tensorflow 1: However in Tensorflow 2 I get the following error: Model definition: Model saving in TF1: Fully reproducible code can be found in the following 2 google colab notebooks: Tensorflow 1 saving and loading Tensorflow 2 loading Direct link to the saved model: model",https://stackoverflow.com/questions/63656778,1564252,Documentation Completeness
60106449,How to create a TensorFlow 2 SavedModel with more than 1 signatures?,"<p>I used <code>tf.saved_model.save</code> and <code>tf.saved_model.load</code> to save and load TF2 SavedModel. According to <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/save#used-in-the-notebooks"" rel=""nofollow noreferrer"">this link</a>, I created a signature and this signature is <code>serving_default</code>. Then I try to add a new function with signature decorator in class <code>Adder</code>. But after I loaded the model according to <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/load"" rel=""nofollow noreferrer"">this</a>, I find that the signatures disappear in the model, i.e., <code>print(adder1.signatures)</code> prints no signature names. I don't find any information about how to use multiple signatures while saving models. So can anyone provide me with some information? Thank you very much.  </p>

<p>Tensorflow <code>2.1.0</code>, on Google Colab. The code looks like this: </p>

<pre><code>import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import os
import pandas as pd

class Adder(tf.Module):

  @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32), tf.TensorSpec(shape=None, dtype=tf.float32)])# 
  def add(self, x, y):
    return x + y ** 2 + 1

  @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])
  def square(self, x):
    return x ** 2

to_export = Adder()
tf.saved_model.save(
    to_export, 
    '/tmp/adder'            
)

adder1 = tf.saved_model.load(""/tmp/adder"")
print(adder1.signatures)
adder1_sig = adder1.signatures[""serving_default""]
adder1_sig(x = tf.constant(1.), y = tf.constant(2.))


</code></pre>
","I used tf.saved_model.save and tf.saved_model.load to save and load TF2 SavedModel. According to this link, I created a signature and this signature is serving_default. Then I try to add a new function with signature decorator in class Adder. But after I loaded the model according to this, I find that the signatures disappear in the model, i.e., print(adder1.signatures) prints no signature names. I don't find any information about how to use multiple signatures while saving models. So can anyone provide me with some information? Thank you very much. Tensorflow 2.1.0, on Google Colab. The code looks like this:",https://stackoverflow.com/questions/60106449,12375578,Lack of Alternative Solutions/Documentation
43779129,Store a tf.Saver.save checkpoint in a variable (or in memory),"<p>I am using Tensorflow and storing the current ""best"" model on the hard drive for persistence, using <code>tf.Saver</code>:</p>

<pre><code>saver = tf.train.Saver(max_to_keep=1)

[...]

saver.save(
    sess,
    path_to_file,
    global_step=epoch
)
</code></pre>

<p>My network is rather small and very fast to run, a single epoch on the GPU runs in less than 10 seconds. However, saving the model to the hard drive takes between one to two minutes, taking up a lot time.</p>

<p>Is it possible to store the model in memory, to avoid taking up such a big chunk of the overall run time? If I somehow could store the ""best"" model in memory for a while, and dump it once I tell the model to, I could cut down the overall run time by a big factor.</p>

<p>I've looked at the <code>tf.Saver</code> documentation and implementation, and I can not see any way to achieve just what I want. Is there some other implementation or tool that can do what I want to?</p>
","I am using Tensorflow and storing the current ""best"" model on the hard drive for persistence, using tf.Saver: My network is rather small and very fast to run, a single epoch on the GPU runs in less than 10 seconds. However, saving the model to the hard drive takes between one to two minutes, taking up a lot time. Is it possible to store the model in memory, to avoid taking up such a big chunk of the overall run time? If I somehow could store the ""best"" model in memory for a while, and dump it once I tell the model to, I could cut down the overall run time by a big factor. I've looked at the tf.Saver documentation and implementation, and I can not see any way to achieve just what I want. Is there some other implementation or tool that can do what I want to?",https://stackoverflow.com/questions/43779129,921563,Requesting (Additional) Resources
38461214,how to increment matrix element in tensorflow using tf.scatter_add?,"<p>tf.scatter_add works nicely for 1d (shape <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#scatter_add"" rel=""nofollow"">1</a>) tensors:</p>

<pre><code>&gt; S = tf.Variable(tf.constant([1,2,3,4]))
&gt; sess.run(tf.initialize_all_variables())
&gt; sess.run(tf.scatter_add(S, [0], [10]))

array([11,  2,  3,  4], dtype=int32)

&gt; sess.run(tf.scatter_add(S, [0, 1], [10, 100]))

array([ 21, 102,   3,   4], dtype=int32)
</code></pre>

<p>But how can I increment, say [0,0] element of </p>

<pre><code>M = tf.Variable(tf.constant([[1,2], [3,4]]))
</code></pre>

<p>to make it [[2, 2], [3, 4]]
using tf.scatter_add?</p>

<p>the <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#scatter_add"" rel=""nofollow"">official documentation</a> is kind'a cryptic. And I tried different arg values, say</p>

<pre><code>&gt; sess.run(tf.scatter_add(M, [[0, 0]], [1]))
*** ValueError: Shapes (1,) and (1, 2, 2) are not compatible
</code></pre>

<p>and haven't succeeded.</p>

<p>Btw, in my case, M is quite large and is resized dynamically.
So adding zero-but-one equal to 1 element matrix to M is not the case.</p>
","tf.scatter_add works nicely for 1d (shape 1) tensors: But how can I increment, say [0,0] element of to make it [[2, 2], [3, 4]] using tf.scatter_add? the official documentation is kind'a cryptic. And I tried different arg values, say and haven't succeeded. Btw, in my case, M is quite large and is resized dynamically. So adding zero-but-one equal to 1 element matrix to M is not the case.",https://stackoverflow.com/questions/38461214,3301357,Documentation Replication on Other Examples
43697539,Eigenvalue problems in TensorFlow,"<p>I want to solve an eigenvalue problem using TensorFlow.
In particular, I have</p>

<pre><code>e, v = tf.self_adjoint_eig(laplacian, name=""eigendata"")
eigenmap = v[:,1:4]
</code></pre>

<p>so I don't want to compute all eigenvectors. </p>

<p>In Matlab, I would use <code>eigs(laplacian,4,'sm')</code></p>

<p>Looking at <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py</a>,
I see that <code>tf.self_adjoint_eig</code> calls <code>gen_linalg_ops._self_adjoint_eig_v2</code>.
However, I can't find <code>gen_linalg_ops</code> on Github or elsewhere.</p>

<p>Any advice on doing such linear algebra in TensorFlow, or is it best to go with other libraries in Python? </p>
","I want to solve an eigenvalue problem using TensorFlow. In particular, I have so I don't want to compute all eigenvectors. In Matlab, I would use eigs(laplacian,4,'sm') Looking at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py, I see that tf.self_adjoint_eig calls gen_linalg_ops._self_adjoint_eig_v2. However, I can't find gen_linalg_ops on Github or elsewhere. Any advice on doing such linear algebra in TensorFlow, or is it best to go with other libraries in Python?",https://stackoverflow.com/questions/43697539,3498821,Lack of Alternative Solutions/Documentation
44764887,How to restore trained LinearClassifier from tensorflow high level API and make predictions,"<p>I have trained a logistic regression model model using tensorflow's LinearClassifier() class, and set the model_dir parameter, which specifies the location where to save metagrahps of checkpoints during model training:</p>

<pre><code># Create temporary directory where metagraphs will evenually be saved
model_dir = tempfile.mkdtemp()

logistic_model = tf.contrib.learn.LinearClassifier(
    feature_columns=feature_columns, 
    n_classes=num_labels, model_dir=model_dir)
</code></pre>

<p>I've been reading about restoring models from metagraphs, but have found nothing about how to do so for models created using the high level api.  LinearClassifier() has a predict() function, but I can't find any documentation on how to run prediction using an instance of the model that has been restored via checkpoint metagraph. How would I go about doing this?  Once the model is restored, my understanding is that I am working with a tf.Sess object, which lacks all of the built in functionality of the LinearClassifier class, like this:</p>

<pre><code>with tf.Session() as sess:
  new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')
  new_saver.restore(sess, 'my-save-dir/my-model-10000')
  # Run prediction algorithm...
</code></pre>

<p>How do I run the same prediction algorithm used by the high-level api to make predictions on a restored model? Is there a better way to approach this?</p>

<p>Thanks for your input.</p>
","I have trained a logistic regression model model using tensorflow's LinearClassifier() class, and set the model_dir parameter, which specifies the location where to save metagrahps of checkpoints during model training: I've been reading about restoring models from metagraphs, but have found nothing about how to do so for models created using the high level api. LinearClassifier() has a predict() function, but I can't find any documentation on how to run prediction using an instance of the model that has been restored via checkpoint metagraph. How would I go about doing this? Once the model is restored, my understanding is that I am working with a tf.Sess object, which lacks all of the built in functionality of the LinearClassifier class, like this: How do I run the same prediction algorithm used by the high-level api to make predictions on a restored model? Is there a better way to approach this? Thanks for your input.",https://stackoverflow.com/questions/44764887,5540936,Lack of Alternative Solutions/Documentation
34706950,How many processes does TensorFlow open?,"<p>I am using torque to run some CNN-based learning using <code>tensorflow</code> library. (1 CPU per task)</p>

<p>When I run <code>top</code> on my server, I noticed that: <code>load average: 677.29, 668.59, 470.</code></p>

<p>I create a session like this: <code>sess = tf.Session()</code></p>

<p>So my question is there some place in documentation where I can read when and how many processes TensorFlow uses.</p>
","I am using torque to run some CNN-based learning using tensorflow library. (1 CPU per task) When I run top on my server, I noticed that: load average: 677.29, 668.59, 470. I create a session like this: sess = tf.Session() So my question is there some place in documentation where I can read when and how many processes TensorFlow uses.",https://stackoverflow.com/questions/34706950,1886138,Requesting (Additional) Resources
53488870,SageMaker fails when using Multi-GPU with keras.utils.multi_gpu_model,"<p>Running AWS SageMaker with a custom model, the TrainingJob fails with an <strong>Algorithm Error</strong> when using Keras plus a Tensorflow backend in multi-gpu configuration:</p>
<pre><code>from keras.utils import multi_gpu_model

parallel_model = multi_gpu_model(model, gpus=K)
parallel_model.compile(loss='categorical_crossentropy',
optimizer='rmsprop')
parallel_model.fit(x, y, epochs=20, batch_size=256)
</code></pre>
<p>This simple parallel model loading will fail. There is no further error or exception from CloudWatch logging. This configuration works properly on local machine with 2x NVIDIA GTX 1080, same Keras Tensorflow backend.</p>
<p>According to SageMaker documentation and <a href=""https://github.com/awslabs/keras-apache-mxnet/wiki/Multi-GPU-Model-Training-with-Keras-MXNet"" rel=""nofollow noreferrer"">tutorials</a> the <code>multi_gpu_model</code> utility will work ok when Keras backend is MXNet, but I did not find any mention when the backend is Tensorflow with the same multi gpu configuration.</p>
<p><strong>[UPDATE]</strong></p>
<p>I have updated the code with the suggested answer below, and I'm adding some logging before the TrainingJob hangs</p>
<p>This logging repeats twice</p>
<pre><code>2018-11-27 10:02:49.878414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3
2018-11-27 10:02:49.878462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-11-27 10:02:49.878471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 1 2 3
2018-11-27 10:02:49.878477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N Y Y Y
2018-11-27 10:02:49.878481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1: Y N Y Y
2018-11-27 10:02:49.878486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2: Y Y N Y
2018-11-27 10:02:49.878492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3: Y Y Y N
2018-11-27 10:02:49.879340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 14874 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)
2018-11-27 10:02:49.879486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 14874 MB memory) -&gt; physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)
2018-11-27 10:02:49.879694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:2 with 14874 MB memory) -&gt; physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)
2018-11-27 10:02:49.879872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:3 with 14874 MB memory) -&gt; physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
</code></pre>
<p>Before there is some logging info about each GPU, that repeats 4 times</p>
<pre><code>2018-11-27 10:02:46.447639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 3 with properties:
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:00:1e.0
totalMemory: 15.78GiB freeMemory: 15.37GiB
</code></pre>
<p>According to the logging all the 4 GPUs are visible and loaded in the Tensorflow Keras backend. After that no application logging follows, the TrainingJob status is <strong>inProgress</strong> for a while, after that it becomes <strong>Failed</strong> with the same <strong>Algorithm Error</strong>.</p>
<p>Looking at CloudWatch logging I can see some metrics at work. Specifically <code>GPU Memory Utilization</code>, <code>CPU Utilization</code> are ok, while <code>GPU utilization</code> is 0%.</p>
<p><a href=""https://i.stack.imgur.com/260hL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/260hL.png"" alt=""enter image description here"" /></a></p>
<p><strong>[UPDATE]</strong></p>
<p>Due to a <a href=""https://github.com/keras-team/keras/issues/8123#issuecomment-354857044"" rel=""nofollow noreferrer"">known</a> bug on Keras that is about saving a multi gpu model, I'm using this override of the <strong>multi_gpu_model</strong> utility in <strong>keras.utils</strong></p>
<pre><code>from keras.layers import Lambda, concatenate
from keras import Model
import tensorflow as tf
    
def multi_gpu_model(model, gpus):
    #source: https://github.com/keras-team/keras/issues/8123#issuecomment-354857044
  if isinstance(gpus, (list, tuple)):
    num_gpus = len(gpus)
    target_gpu_ids = gpus
  else:
    num_gpus = gpus
    target_gpu_ids = range(num_gpus)

  def get_slice(data, i, parts):
    shape = tf.shape(data)
    batch_size = shape[:1]
    input_shape = shape[1:]
    step = batch_size // parts
    if i == num_gpus - 1:
      size = batch_size - step * i
    else:
      size = step
    size = tf.concat([size, input_shape], axis=0)
    stride = tf.concat([step, input_shape * 0], axis=0)
    start = stride * i
    return tf.slice(data, start, size)

  all_outputs = []
  for i in range(len(model.outputs)):
    all_outputs.append([])

  # Place a copy of the model on each GPU,
  # each getting a slice of the inputs.
  for i, gpu_id in enumerate(target_gpu_ids):
    with tf.device('/gpu:%d' % gpu_id):
      with tf.name_scope('replica_%d' % gpu_id):
        inputs = []
        # Retrieve a slice of the input.
        for x in model.inputs:
          input_shape = tuple(x.get_shape().as_list())[1:]
          slice_i = Lambda(get_slice,
                           output_shape=input_shape,
                           arguments={'i': i,
                                      'parts': num_gpus})(x)
          inputs.append(slice_i)

        # Apply model on slice
        # (creating a model replica on the target device).
        outputs = model(inputs)
        if not isinstance(outputs, list):
          outputs = [outputs]

        # Save the outputs for merging back together later.
        for o in range(len(outputs)):
          all_outputs[o].append(outputs[o])

  # Merge outputs on CPU.
  with tf.device('/cpu:0'):
    merged = []
    for name, outputs in zip(model.output_names, all_outputs):
      merged.append(concatenate(outputs,
                                axis=0, name=name))
    return Model(model.inputs, merged)
</code></pre>
<p>This works ok on local <code>2x NVIDIA GTX 1080 / Intel Xeon / Ubuntu 16.04</code>. It will fails on SageMaker Training Job.</p>
<p>I have posted this issue on AWS Sagemaker forum in</p>
<ul>
<li><p><a href=""https://forums.aws.amazon.com/thread.jspa?messageID=879769"" rel=""nofollow noreferrer"">TrainingJob custom algorithm with Keras backend and multi GPU</a></p>
</li>
<li><p><a href=""https://forums.aws.amazon.com/thread.jspa?threadID=294095&amp;tstart=0"" rel=""nofollow noreferrer"">SageMaker Fails when using Multi-GPU with
keras.utils.multi_gpu_model</a></p>
</li>
</ul>
<p><strong>[UPDATE]</strong></p>
<p>I have slightly modified the <code>tf.session</code> code adding some initializers</p>
<pre><code>with tf.Session() as session:
    K.set_session(session)
    session.run(tf.global_variables_initializer())
    session.run(tf.tables_initializer())
</code></pre>
<p>and now at least I can see that one GPU (I assume device <code>gpu:0</code>) is used from the instance metrics. The multi-gpu does not work anyways.</p>
","Running AWS SageMaker with a custom model, the TrainingJob fails with an Algorithm Error when using Keras plus a Tensorflow backend in multi-gpu configuration: This simple parallel model loading will fail. There is no further error or exception from CloudWatch logging. This configuration works properly on local machine with 2x NVIDIA GTX 1080, same Keras Tensorflow backend. According to SageMaker documentation and tutorials the multi_gpu_model utility will work ok when Keras backend is MXNet, but I did not find any mention when the backend is Tensorflow with the same multi gpu configuration. [UPDATE] I have updated the code with the suggested answer below, and I'm adding some logging before the TrainingJob hangs This logging repeats twice Before there is some logging info about each GPU, that repeats 4 times According to the logging all the 4 GPUs are visible and loaded in the Tensorflow Keras backend. After that no application logging follows, the TrainingJob status is inProgress for a while, after that it becomes Failed with the same Algorithm Error. Looking at CloudWatch logging I can see some metrics at work. Specifically GPU Memory Utilization, CPU Utilization are ok, while GPU utilization is 0%. [UPDATE] Due to a known bug on Keras that is about saving a multi gpu model, I'm using this override of the multi_gpu_model utility in keras.utils This works ok on local 2x NVIDIA GTX 1080 / Intel Xeon / Ubuntu 16.04. It will fails on SageMaker Training Job. I have posted this issue on AWS Sagemaker forum in [UPDATE] I have slightly modified the tf.session code adding some initializers and now at least I can see that one GPU (I assume device gpu:0) is used from the instance metrics. The multi-gpu does not work anyways.",https://stackoverflow.com/questions/53488870,758836,Lack of Alternative Solutions/Documentation
57153123,Remove stdout output from tf.Session due to init in a loop,"<p>I've come to stack overflow because no one else seems to have asked this question before. When running my tensorflow model I use it inside a loop to do regression analysis. I use tf.Session inside the lop</p>

<p>loop:
    with tf.Session as sess:
          ...code rest of code</p>

<p>When I run the neural network it spits out all the tensorflow/core information in the loop and makes my actual output (for me to visualize) useless. I'd like to either re-direct the output or find a way to make it less verbose or completely silent.</p>

<p>I've gone through most of the documentation from tensorflow and the Session documentation doesn't reveal anything to me. Here is the current output to the terminal.</p>

<blockquote>
  <p>2019-07-22 16:09:46.813546: I >tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu >devices: 0
  2019-07-22 16:09:46.815548: I >tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect >StreamExecutor with strength 1 edge matrix:
  2019-07-22 16:09:46.817433: I >?>tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
  2019-07-22 16:09:46.818613: I >tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
  2019-07-22 16:09:46.819902: I >tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow >device (/job:localhost/replica:0/task:0/device:GPU:0 with 8698 MB memory) -> >physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, >compute capability: 7.5)</p>
</blockquote>
",I've come to stack overflow because no one else seems to have asked this question before. When running my tensorflow model I use it inside a loop to do regression analysis. I use tf.Session inside the lop loop: with tf.Session as sess: ...code rest of code When I run the neural network it spits out all the tensorflow/core information in the loop and makes my actual output (for me to visualize) useless. I'd like to either re-direct the output or find a way to make it less verbose or completely silent. I've gone through most of the documentation from tensorflow and the Session documentation doesn't reveal anything to me. Here is the current output to the terminal.,https://stackoverflow.com/questions/57153123,11821139,Documentation Completeness
44462550,Keras + Tensorflow : Debug NaNs,"<p>Here is a great question on how to find the first occurence of Nan in a tensorflow graph:</p>

<p><a href=""https://stackoverflow.com/questions/34046048/debugging-nans-in-the-backward-pass"">Debugging nans in the backward pass</a></p>

<p>The answer is quite helpful, here is the code from it:</p>

<pre><code>train_op = ...
check_op = tf.add_check_numerics_ops()

sess = tf.Session()
sess.run([train_op, check_op])  # Runs training and checks for NaNs
</code></pre>

<p>Apparently, running the training and the numerical check at the same time will result in an error report as soon as Nan is encountered for the first time.</p>

<p>How do I integrate this into Keras ?
In the documentation, I can't find anything that looks like this.</p>

<p>I checked the code, too.
The update step is executed here:
<a href=""https://github.com/fchollet/keras/blob/master/keras/engine/training.py"" rel=""noreferrer"">https://github.com/fchollet/keras/blob/master/keras/engine/training.py</a></p>

<p>There is a function called <code>_make_train_function</code> where an operation to compute the loss and apply updates is created. This is later called to train the network.</p>

<p>I could change the code like this (always assuming that we're running on a tf backend):</p>

<pre><code>check_op = tf.add_check_numerics_ops()

self.train_function = K.function(inputs, 
    [self.total_loss] + self.metrics_tensors + [check_op],
    updates=updates, name='train_function', **self._function_kwargs)
</code></pre>

<p>I'm currently trying to set this up properly and not sure whether the code above actually works.
Maybe there is an easier way ?</p>
","Here is a great question on how to find the first occurence of Nan in a tensorflow graph: Debugging nans in the backward pass The answer is quite helpful, here is the code from it: Apparently, running the training and the numerical check at the same time will result in an error report as soon as Nan is encountered for the first time. How do I integrate this into Keras ? In the documentation, I can't find anything that looks like this. I checked the code, too. The update step is executed here: https://github.com/fchollet/keras/blob/master/keras/engine/training.py There is a function called _make_train_function where an operation to compute the loss and apply updates is created. This is later called to train the network. I could change the code like this (always assuming that we're running on a tf backend): I'm currently trying to set this up properly and not sure whether the code above actually works. Maybe there is an easier way ?",https://stackoverflow.com/questions/44462550,497600,Lack of Alternative Solutions/Documentation
43126116,Range of size of tensor's dimension - tf.range,"<p>I'm trying to define an operation for a NN I'm implementing, but to do so I need to iterate over the dimension of a tensor. I have a small working example below.</p>

<pre><code>X = tf.placeholder(tf.float32, shape=[None, 10])
idx = [[i] for i in tf.range(X.get_shape()[0])]
</code></pre>

<p>This produces an error stating</p>

<pre><code>ValueError: Cannot convert an unknown Dimension to a Tensor: ?
</code></pre>

<p>When using the same code but using <code>tf.shape</code> instead, resulting in the code being</p>

<pre><code>X = tf.placeholder(tf.float32, shape=[None, 10])
idx = [[i] for i in tf.range(tf.shape(X)[0])]
</code></pre>

<p>Gives the following error</p>

<pre><code>TypeError: 'Tensor' object is not iterable.
</code></pre>

<p>The way that I'm implementing this NN, the <code>batch_size</code> isn't defined until the training function, which is at the end of the code. This is just where I'm building the graph itself, so the <code>batch_size</code> isn't known by this point, and it can't be fixed as the training <code>batch_size</code> and the test set batch_sizes are different. </p>

<p>What is the best way to fix this? This is the last thing keeping my code from running, as I got it to run with a fixed <code>batch_size</code>, though those results aren't useful. I've been pouring over the TensorFlow API Documentation and stack overflow for weeks to no avail.</p>

<p>I've also tried to feed in a placeholder into the range, so when I'm running the test/training set the code would be the following</p>

<pre><code>X = tf.placeholder(tf.float32, shape=[None, 10])
bs = tf.placeholder(tf.int32)

def My_Function(X):
    # Do some stuff to X
    idx = [[i] for i in tf.range(bs)]
    # return some tensor

A = tf.nn.relu(My_Function(X))
</code></pre>

<p>However, this gives the same error as above</p>

<pre><code>TypeError: 'Tensor' object is not iterable.
</code></pre>
","I'm trying to define an operation for a NN I'm implementing, but to do so I need to iterate over the dimension of a tensor. I have a small working example below. This produces an error stating When using the same code but using tf.shape instead, resulting in the code being Gives the following error The way that I'm implementing this NN, the batch_size isn't defined until the training function, which is at the end of the code. This is just where I'm building the graph itself, so the batch_size isn't known by this point, and it can't be fixed as the training batch_size and the test set batch_sizes are different. What is the best way to fix this? This is the last thing keeping my code from running, as I got it to run with a fixed batch_size, though those results aren't useful. I've been pouring over the TensorFlow API Documentation and stack overflow for weeks to no avail. I've also tried to feed in a placeholder into the range, so when I'm running the test/training set the code would be the following However, this gives the same error as above",https://stackoverflow.com/questions/43126116,1560300,Lack of Alternative Solutions/Documentation
47844264,tf.shape() returns a 2-d tensor instead of 1-d,"<p>In the tensorflow API <code>tf.shape</code>, it says </p>

<blockquote>
  <p>This operation returns a 1-D integer tensor representing the shape of input.</p>
</blockquote>

<p>However, when I call</p>

<pre><code>features = {
    'k_mask': tf.VarLenFeature(tf.int64),
    'features': tf.VarLenFeature(tf.int64),
    'labels': tf.FixedLenFeature([3], tf.int64),
    'k_ids': tf.VarLenFeature(tf.int64)
}
parsed_features = tf.parse_single_example(example_proto, features)
features_index = tf.sparse_tensor_to_dense(parsed_features['features'])
print(sess.run(tf.shape(features_index)))
</code></pre>

<p>I get the result of <code>[[59]]</code>, which is a 2-D integer tensor. The <code>feature_index</code> can be print as </p>

<pre><code>[[ 6217  5882 17223 17235  6008  3580 17233  6038 16340  6116  5458  5747
   5957  5755 17238  5745  6030  6078  5786  4373  5888 16284  3574  3569
   5811  6117  5748 17228  5810  5833  5823  5885  5986  6034  5756  6105
   5832  6199  6087  5744  6037  5933  6095  5785 16290  6124  3559  5787
   6111  3570  6109 17322  3840  5962  3566 16950  6006  3584  6011]]
</code></pre>

<p>I thought this is a normal [1, 59] tensor. I try the following code:</p>

<pre><code>v1 = tf.constant([[4,3,1,7]])
print(sess.run(v1)) # [[4 3 1 7]]
print(sess.run(tf.shape(v1))) # [1 4]
</code></pre>

<p>It looks as expected.</p>

<p>I want transform <code>feature_index</code> to shape of [59,1]. Would anyone knows why the return type is 2-d and how to convert the tensor?</p>
","In the tensorflow API tf.shape, it says However, when I call I get the result of [[59]], which is a 2-D integer tensor. The feature_index can be print as I thought this is a normal [1, 59] tensor. I try the following code: It looks as expected. I want transform feature_index to shape of [59,1]. Would anyone knows why the return type is 2-d and how to convert the tensor?",https://stackoverflow.com/questions/47844264,3528980,Documentation Replication on Other Examples
54271159,Variable size mismatch between x.shape and tf.shape(x)?,"<p>I am trying to understand tf code and for this I am printing out shapes of tensors. For the following code</p>

<pre><code>print(x.shape)
print(tf.shape(x))
</code></pre>

<p>I get output</p>

<pre><code>(?, 32, 32, 3)
Tensor(""input/Shape:0"", shape=(4,), dtype=int32)
</code></pre>

<p>It does not make a lot of sense. Based on what I found online tf.shape(x) can be used to dynamically get the size for the batch. But it gives rather wrong output - 4. I am not sure where this <code>(4,)</code> is coming from and how to get the right value for my tensor.</p>
","I am trying to understand tf code and for this I am printing out shapes of tensors. For the following code I get output It does not make a lot of sense. Based on what I found online tf.shape(x) can be used to dynamically get the size for the batch. But it gives rather wrong output - 4. I am not sure where this (4,) is coming from and how to get the right value for my tensor.",https://stackoverflow.com/questions/54271159,3849781,Documentation Ambiguity
41865218,Interleaving slim.dropout and slim.fully_connected in slim.stack?,"<p>In tf.slim, I'd like to create a stack of fully-connected layers with dropout.</p>

<p>To the example from documentation:
<code>slim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc')</code>, I'd like to add dropout. </p>

<p>Is it possible to use slim.stack or do I have to go back to the verbose approach?</p>

<pre><code>(pseudo-code) for every layer:
   slim.dropout(slim.fully_connected(...)
</code></pre>
","In tf.slim, I'd like to create a stack of fully-connected layers with dropout. To the example from documentation: slim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc'), I'd like to add dropout. Is it possible to use slim.stack or do I have to go back to the verbose approach?",https://stackoverflow.com/questions/41865218,314710,Requesting (Additional) Resources
48558181,variables_to_train flag in Tf-slim,"<p>I am fine-tuning my model from a pretrained model using TF-Slim. When I used the <code>create_train_op</code>, I found that it has a parameter that is <code>variables_to_train</code>. In some tutorial, it used the flag as follows:</p>

<pre><code>   all_trainable = [v for v in tf.trainable_variables()]
   trainable     = [v for v in all_trainable]
   train_op      = slim.learning.create_train_op(
        opt,
        global_step=global_step,
        variables_to_train=trainable,
        summarize_gradients=True)
</code></pre>

<p>But in the official TF-Slim, it does not use</p>

<pre><code>   all_trainable = [v for v in tf.trainable_variables()]
   trainable     = [v for v in all_trainable]
   train_op      = slim.learning.create_train_op(
        opt,
        global_step=global_step,            
        summarize_gradients=True)
</code></pre>

<p>So, what is different between with and without using <code>variables_to_train</code>?</p>
","I am fine-tuning my model from a pretrained model using TF-Slim. When I used the create_train_op, I found that it has a parameter that is variables_to_train. In some tutorial, it used the flag as follows: But in the official TF-Slim, it does not use So, what is different between with and without using variables_to_train?",https://stackoverflow.com/questions/48558181,2938494,Documentation Ambiguity
42201565,Using Tensorflow.slim to apply convolution2d_transpose,"<p>I am trying to apply 2 convolutional layers with the tf.slim.conv2d function, they basically reduce the size of my input image by half each time. Then I want to apply the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L906"" rel=""nofollow noreferrer"">convolution2d_transpose</a> to get my original image shape back. The problem is I don't exactly know how to use the transpose convolution function, and the documentation is not much help. </p>

<p>I am using a custom wrapper, but here is what I have so far:</p>

<pre><code>Input Batch [8, 161, 141] ----&gt; Conv2d [outputs = 32, 
kernel_size = [41,11], stride= [2,2]] 
which cuts the original image in half, and another such layer which cuts it again.
</code></pre>

<p>How can I apply the convolution_transpose function to reverse the effect of these two layers now ? </p>
","I am trying to apply 2 convolutional layers with the tf.slim.conv2d function, they basically reduce the size of my input image by half each time. Then I want to apply the convolution2d_transpose to get my original image shape back. The problem is I don't exactly know how to use the transpose convolution function, and the documentation is not much help. I am using a custom wrapper, but here is what I have so far: How can I apply the convolution_transpose function to reverse the effect of these two layers now ?",https://stackoverflow.com/questions/42201565,5016028,Lack of Alternative Solutions/Documentation
44264962,how tf.space_to_depth() works in tensorflow?,"<p>I am a pytorch user. I have got a pretrained model in tensorflow and I would like to transfer it into pytorch. In one part of model architecture, I mean in tensorflow-defined model, there is a function <strong>tf.space_to_depth</strong> which transfers an input size of (None, 38,38,64) to (None, 19,19, 256). (<a href=""https://www.tensorflow.org/api_docs/python/tf/space_to_depth"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/space_to_depth</a>) is the doc of this function. But I could not understand what this function actually do. Could you please provide some numpy codes to illustrate it for me?</p>

<p>Actually I would like to make an exact similar layer in pytorch.</p>

<p>Some codes in tensorflow reveals another secret:
Here is some codes:</p>

<pre><code>import numpy as np
import tensorflow as tf

norm = tf.random_normal([1, 2, 2, 1], mean=0, stddev=1)
trans = tf.space_to_depth(norm,2)

with tf.Session() as s:
    norm = s.run(norm)
    trans = s.run(trans)



print(""Norm"")
print(norm.shape)
for index,value in np.ndenumerate(norm):
    print(value)

print(""Trans"")
print(trans.shape)
for index,value in np.ndenumerate(trans):
    print(value)
</code></pre>

<p>And here is the output:</p>

<pre><code>Norm
(1, 2, 2, 1)
0.695261
0.455764
1.04699
-0.237587
Trans
(1, 1, 1, 4)
1.01139
0.898777
0.210135
2.36742
</code></pre>

<p>As you can see above, In Addition to data reshaping, the tensor values has changed!</p>
","I am a pytorch user. I have got a pretrained model in tensorflow and I would like to transfer it into pytorch. In one part of model architecture, I mean in tensorflow-defined model, there is a function tf.space_to_depth which transfers an input size of (None, 38,38,64) to (None, 19,19, 256). (https://www.tensorflow.org/api_docs/python/tf/space_to_depth) is the doc of this function. But I could not understand what this function actually do. Could you please provide some numpy codes to illustrate it for me? Actually I would like to make an exact similar layer in pytorch. Some codes in tensorflow reveals another secret: Here is some codes: And here is the output: As you can see above, In Addition to data reshaping, the tensor values has changed!",https://stackoverflow.com/questions/44264962,3475688,Documentation Ambiguity
67103766,bounding box format in tensorflow object detection api,"<p>thanks in advance.<br />
I try to use tensorflow object detection api with manual and web.<br />
But I confused about bounding box format in tensorflow object detection api.<br />
in tutorial, TODA(tensorflow object detection api) serve several pretrained model,
and its trained with coco dataset.</p>
<p>in coco dataset,<br />
bbox foramt is [xmin, ymin, width, height],<br />
there are many bbox format,
centerx, centery,  width, height, or xmin, ymin, xmax,ymax</p>
<p>which bbox format should I use for TODA??
(should I use coco format??)<br />
I cant find any info regarding this.</p>
<p>and x axis and y axis, this is also confused.
I understand X means width, Y means height.</p>
<p>bun TODA code,
I found this.<br />
def assert_or_prune_invalid_boxes(boxes):<br />
...<br />
ymin, xmin, ymax, xmax = tf.split(
boxes, num_or_size_splits=4, axis=1)</p>
<p>why x, y switching??<br />
TODA axis is different from others??</p>
<p>thanks.</p>
","thanks in advance. I try to use tensorflow object detection api with manual and web. But I confused about bounding box format in tensorflow object detection api. in tutorial, TODA(tensorflow object detection api) serve several pretrained model, and its trained with coco dataset. in coco dataset, bbox foramt is [xmin, ymin, width, height], there are many bbox format, centerx, centery, width, height, or xmin, ymin, xmax,ymax which bbox format should I use for TODA?? (should I use coco format??) I cant find any info regarding this. and x axis and y axis, this is also confused. I understand X means width, Y means height. bun TODA code, I found this. def assert_or_prune_invalid_boxes(boxes): ... ymin, xmin, ymax, xmax = tf.split( boxes, num_or_size_splits=4, axis=1) why x, y switching?? TODA axis is different from others?? thanks.",https://stackoverflow.com/questions/67103766,10748392,Documentation Completeness
33727935,How to use stop_gradient in Tensorflow,"<p>I'm wondering how to use <code>stop_gradient</code> in tensorflow, and the documentation is not clear to me.</p>
<p>I'm currently using <code>stop_gradient</code> to produce the gradient of the loss function w.r.t. the word embeddings in a CBOW word2vec model. I want to just get the value, and not do backpropagation (as I'm generating adversarial examples).</p>
<p>Currently, I'm using the code:</p>
<pre><code>lossGrad = gradients.gradients(loss, embed)[0]
real_grad = lossGrad.eval(feed_dict)
</code></pre>
<p><s>But when I run this, it does the backpropogation anyway!</s> What am I doing wrong, and just as importantly, how can I fix this?</p>
<p>CLARIFICATION: To clarify by &quot;backpropagation&quot; I mean &quot;calculating values and updating model parameters&quot;.</p>
<h3>UPDATE</h3>
<p>If I run the two lines above after the first training step, the I get a different loss after 100 training steps than when I don't run those two lines. I might be fundamentally misunderstanding something about Tensorflow.</p>
<p>I've tried setting using <code>set_random_seed</code> both in the beginning of the graph declaration and before each training step. The total loss is consistent between multiple runs, but not between including/excluding those two lines. So if it's not the RNG causing the disparity, and it's not unanticipated updating of the model parameters between training steps, do you have any idea what would cause this behavior?</p>
<h3>SOLUTION</h3>
<p>Welp, it's a bit late but here's how I solved it. I only wanted to optimize over some, but not all, variables. I thought that the way to prevent optimizing some variables would be to use <code>stop_grad</code> - but I never found a way to make that work. Maybe there is a way, but what worked for me was to adjust my <code>optimizer</code> to only optimize over a list of variables. So instead of:</p>
<pre><code>opt = tf.train.GradientDescentOptimizer(learning_rate=eta)
train_op = opt.minimize(loss)
</code></pre>
<p>I used:</p>
<pre><code>opt = tf.train.GradientDescentOptimizer(learning_rate=eta)
train_op = opt.minimize(loss, var_list=[variables to optimize over])
</code></pre>
<p>This prevented <code>opt</code> from updating the variables not in <code>var_list</code>. Hopefully it works for you, too!</p>
","I'm wondering how to use stop_gradient in tensorflow, and the documentation is not clear to me. I'm currently using stop_gradient to produce the gradient of the loss function w.r.t. the word embeddings in a CBOW word2vec model. I want to just get the value, and not do backpropagation (as I'm generating adversarial examples). Currently, I'm using the code: But when I run this, it does the backpropogation anyway! What am I doing wrong, and just as importantly, how can I fix this? CLARIFICATION: To clarify by ""backpropagation"" I mean ""calculating values and updating model parameters"". If I run the two lines above after the first training step, the I get a different loss after 100 training steps than when I don't run those two lines. I might be fundamentally misunderstanding something about Tensorflow. I've tried setting using set_random_seed both in the beginning of the graph declaration and before each training step. The total loss is consistent between multiple runs, but not between including/excluding those two lines. So if it's not the RNG causing the disparity, and it's not unanticipated updating of the model parameters between training steps, do you have any idea what would cause this behavior? Welp, it's a bit late but here's how I solved it. I only wanted to optimize over some, but not all, variables. I thought that the way to prevent optimizing some variables would be to use stop_grad - but I never found a way to make that work. Maybe there is a way, but what worked for me was to adjust my optimizer to only optimize over a list of variables. So instead of: I used: This prevented opt from updating the variables not in var_list. Hopefully it works for you, too!",https://stackoverflow.com/questions/33727935,5565980,Documentation Ambiguity
44770980,Tensorflow - Retrieve each character in a string tensor,"<p>I'm trying to retrieve the characters in a string tensor for character level prediction. The ground truths are words where each character has an id in dictionary. I have a tensor corresponding to the length of the string. </p>

<p>Now, I have to get each character in the string tensor. After checking the related posts, a simple retrieval can be as follows. Example string is ""This""</p>

<pre><code>a= tf.constant(""This"",shape=[1])
b=tf.string_split(a,delimiter="""").values  #Sparse tensor has the values array which stores characters
</code></pre>

<p>Now I want to make a string with spaces in between the letters ""This"" i.e "" T h i s "". I need spacing at the start and the end too. 
How do I do this?</p>

<p>I have tried to iterate through the characters like below</p>

<pre><code>for i in xrange(b.dense_shape[1]): # b.dense_shape[1] has the length of string
        x=b.values[i]
</code></pre>

<p>But the loop expects an integer rather than a tensor. </p>

<p>Any idea on how to do the above tasks? I couldn't find any documentation related to this (apart from the tf.string_split function). Any suggestions are welcome. Thanks</p>
","I'm trying to retrieve the characters in a string tensor for character level prediction. The ground truths are words where each character has an id in dictionary. I have a tensor corresponding to the length of the string. Now, I have to get each character in the string tensor. After checking the related posts, a simple retrieval can be as follows. Example string is ""This"" Now I want to make a string with spaces in between the letters ""This"" i.e "" T h i s "". I need spacing at the start and the end too. How do I do this? I have tried to iterate through the characters like below But the loop expects an integer rather than a tensor. Any idea on how to do the above tasks? I couldn't find any documentation related to this (apart from the tf.string_split function). Any suggestions are welcome. Thanks",https://stackoverflow.com/questions/44770980,5470522,Lack of Alternative Solutions/Documentation
37902705,How to manually create a tf.Summary(),"<p>I often want to log python variables --as opposed to tf tensors.</p>

<p>In the docs it says that ""you can pass a <code>tf.Summary</code> protocol buffer that you populate with your own data"" but there is no docs for <code>tf.Summary</code> and i could not figure out how to use it.</p>

<p>Anyone knows how to create a Scalar summary this way?</p>
","I often want to log python variables --as opposed to tf tensors. In the docs it says that ""you can pass a tf.Summary protocol buffer that you populate with your own data"" but there is no docs for tf.Summary and i could not figure out how to use it. Anyone knows how to create a Scalar summary this way?",https://stackoverflow.com/questions/37902705,6108836,Lack of Alternative Solutions/Documentation
42001566,TensorFlow: How to merge multiple 'collections'?,"<p>I have some <code>collections</code> that I would like to track with TensorBoard using a supervisor. In the Supervisor initializer I would like something to the effect</p>
<pre><code>summary_op = tf.summary.merge_all(['test', 'valid'])
</code></pre>
<p>But I get the error <code>TypeError: unhashable type: 'list'</code>, because the <code>key</code> must be a string (<a href=""https://www.tensorflow.org/api_docs/python/summary/generation_of_summaries_#merge_all"" rel=""nofollow noreferrer"">see documentation</a>).</p>
<br>
<hr />
<p><strong>Edit</strong>:</p>
<p>This doesn't work either:</p>
<pre><code>summary_op = [tf.summary.merge_all('train'), tf.summary.merge_all('valid')]
</code></pre>
","I have some collections that I would like to track with TensorBoard using a supervisor. In the Supervisor initializer I would like something to the effect But I get the error TypeError: unhashable type: 'list', because the key must be a string (see documentation). Edit: This doesn't work either:",https://stackoverflow.com/questions/42001566,3747801,Requesting (Additional) Resources
54812292,I try to print in TensorBoard an audio with tf.summary.audio any audio is shown,"<p>I'm using Python 3.6 and TensorFlow 1.8 in a Linux environment. I'm trying to print an audio in TensorBoard with the following code, and even is storing a file, no audio is printed.</p>

<pre><code>import tensorflow as tf
with tf.Session() as sess:
    writer = tf.summary.FileWriter('graphs', sess.graph)
    audio = tf.reshape(tf.linspace(0.0, 100.0, 4 * 10 * 2), (4, 10, 2))
    tf.summary.audio('k488', audio, 2)
    writer.close()
</code></pre>

<p>I have been looking examples or information, but there is no much about tf.summary.audio. This is a example that I found but can't make it work.</p>

<p>Thank you</p>
","I'm using Python 3.6 and TensorFlow 1.8 in a Linux environment. I'm trying to print an audio in TensorBoard with the following code, and even is storing a file, no audio is printed. I have been looking examples or information, but there is no much about tf.summary.audio. This is a example that I found but can't make it work. Thank you",https://stackoverflow.com/questions/54812292,11097109,Inadequate Examples
46735542,How to interpret histogram plot in tensorflow/tensorboard?,"<p>I compute <code>(n_samples,1)</code> rotation angles and pass them to histogram summary</p>

<pre><code>angles_histogram = tf.summary.histogram(""angles_histogram"", angles)
</code></pre>

<p>and pass it to writer with the following command</p>

<pre><code>angles_histogram_value = sess.run([self.angles_histogram])
rotations_writer.add_summary(angles_histogram_value, global_count - 1)
</code></pre>

<p>Angles are in radians and <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/histogram"" rel=""nofollow noreferrer"">in docs it is said</a> that all values are used to build histogram. Unfortunately, I see nothing similar to what I would expect</p>

<p><a href=""https://i.stack.imgur.com/6cdRt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6cdRt.png"" alt=""enter image description here""></a></p>

<p>How to interpet this plot?</p>
","I compute (n_samples,1) rotation angles and pass them to histogram summary and pass it to writer with the following command Angles are in radians and in docs it is said that all values are used to build histogram. Unfortunately, I see nothing similar to what I would expect How to interpet this plot?",https://stackoverflow.com/questions/46735542,258483,Inadequate Examples
56430357,Tensorflow: sess.run([x]) not working but sess.run([y]) works with the same feed_dict,"<p>I am learning Tensorboard, and I am following the code in <a href=""https://www.youtube.com/watch?v=eBbEDRsCmv4"" rel=""nofollow noreferrer"">this tutorial.</a></p>

<p>Below is my code:</p>

<pre><code>import tensorflow as tf
LOGDIR = ""/tmp/mnist_tutorial/""
mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=LOGDIR + ""data"", one_hot=True)

def conv_layer(input, size_in, size_out, name=""conv""):
    with tf.name_scope(name):
        w = tf.Variable(tf.zeros([5, 5, size_in, size_out]))
        b = tf.Variable(tf.zeros([size_out]))
        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=""SAME"")
        act = tf.nn.relu(conv + b)
        tf.summary.histogram(""weights"", w)
        tf.summary.histogram(""biases"", b)
        tf.summary.histogram(""activations"", act)
        return act


def fc_layer(input, size_in, size_out, name=""fc""):
    with tf.name_scope(name):
        w = tf.Variable(tf.zeros([size_in, size_out]))
        b = tf.Variable(tf.zeros([size_out]))
        act = tf.nn.relu(tf.matmul(input, w)+b)
        tf.summary.histogram(""weights"", w)
        tf.summary.histogram(""biases"", b)
        tf.summary.histogram(""activations"", act)
        return act

x = tf.placeholder(tf.float32, shape=[None, 784], name='x')
x_image = tf.reshape(x, [-1, 28, 28, 1])
tf.summary.image('input', x_image, 3)

y = tf.placeholder(tf.float32, shape=[None, 10], name='labels')

conv1 = conv_layer(x_image, 1, 32, name='conv1')
pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=""SAME"")

conv2 = conv_layer(pool1, 32, 64, name='conv2')
pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=""SAME"")
flattened = tf.reshape(pool2, [-1, 7*7*64])

fc1 = fc_layer(flattened, 7*7*64, 1024, name='fc1')
logits = fc_layer(fc1, 1024, 10, name='fc2')

with tf.name_scope('xent'):
    xent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))
    tf.summary.scalar('cross_entropy', xent)

with tf.name_scope('train'):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(xent)

with tf.name_scope('accruacy'):
    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar('accruacy', accuracy)

summ = tf.summary.merge_all()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    # writer =tf.summary.FileWriter(""tmp/mnist_demo/1"")
    # writer.add_graph(sess.graph)
    # writer.close()

    for i in range(20):
        batch = mnist.train.next_batch(100)

        # Occasionally report back the accruacy

        if i%2 == 0:
            [train_accruacy] = sess.run([accuracy], feed_dict={x:batch[0], y:batch[1]}) # works
#             [s, train_accruacy] = sess.run([summ, accuracy], feed_dict={x:batch[0], y:batch[1]}) #error!
            print(""step %d, training accruacy %g"" % (i, train_accruacy))

    sess.run(train_step, feed_dict={x:batch[0],y:batch[1]})
</code></pre>

<p>I run into an error when I use this line:</p>

<p><code>[s, train_accruacy] = sess.run([summ, accuracy], feed_dict={x:batch[0], y:batch[1]}) #error!</code></p>

<p>This is the error message I get:</p>

<p><code>You must feed a value for placeholder tensor 'x' with dtype float and shape [?,784]
     [[{{node x}} = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]</code></p>

<p>which I understand that the tensor which I fed in is not of the correct shape of (x, 784).</p>

<p>However, I don't get why <code>[train_accruacy] = sess.run([accuracy], feed_dict={x:batch[0], y:batch[1]}) # works</code>. After all, I'm feeding in the same thing into the same placeholder variables, which are accepting tensors of the same shape. </p>

<p>Unless I am completely mistaken, the first argument of <code>sess.run([argument], feed_dict=...)</code> describes the tensor to return. I don't see how that affects the shape of the data I'm feeding in.</p>

<p>Also: this model is supposed to have an error in it.</p>

<p>For those interested, the full code is <a href=""https://github.com/martinwicke/tf-dev-summit-tensorboard-tutorial/blob/master/mnist.py"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Could it also be that the return datatype is different? <code>tf.summary.merge_all()</code> returns a string tensor, but I doubt that's what causing the issue.</p>

<p>I can't seem to find any documentation of this problem online. Is this supposed to happen?</p>
","I am learning Tensorboard, and I am following the code in this tutorial. Below is my code: I run into an error when I use this line: [s, train_accruacy] = sess.run([summ, accuracy], feed_dict={x:batch[0], y:batch[1]}) #error! This is the error message I get: You must feed a value for placeholder tensor 'x' with dtype float and shape [?,784] [[{{node x}} = Placeholder[dtype=DT_FLOAT, shape=[?,784], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]] which I understand that the tensor which I fed in is not of the correct shape of (x, 784). However, I don't get why [train_accruacy] = sess.run([accuracy], feed_dict={x:batch[0], y:batch[1]}) # works. After all, I'm feeding in the same thing into the same placeholder variables, which are accepting tensors of the same shape. Unless I am completely mistaken, the first argument of sess.run([argument], feed_dict=...) describes the tensor to return. I don't see how that affects the shape of the data I'm feeding in. Also: this model is supposed to have an error in it. For those interested, the full code is here. Could it also be that the return datatype is different? tf.summary.merge_all() returns a string tensor, but I doubt that's what causing the issue. I can't seem to find any documentation of this problem online. Is this supposed to happen?",https://stackoverflow.com/questions/56430357,9721336,Lack of Alternative Solutions/Documentation
42012906,Create a custom Tensorflow histogram summary,"<p>There are a couple of SO answers on creating a custom scalar summary in TF (<a href=""https://stackoverflow.com/questions/37902705/how-to-manually-create-a-tf-summary"">here</a> and <a href=""https://stackoverflow.com/questions/37530228/how-do-i-add-an-arbitrary-value-to-a-tensorflow-summary"">here</a>), but I can't find anything on creating a custom <em>histogram</em> summary. The documentation seems to be very lacking for custom summaries. I have a numpy array of that I'd like to make a summary of - any ideas on how?</p>

<p>(tf.Summary.Value has a histo field that I tried using, but it requires a tensorflow::HistogramProto; there's no documentation on that class either, so I'm at a loss on how to make it. I've tried creating a minimal failing example below).</p>

<pre><code>import tensorflow as tf
import numpy as np
sess = tf.Session()
means_placeholder = tf.placeholder(tf.float32)
tf.summary.histogram('means', means_placeholder)
summaries = tf.summary.merge_all()
writer = tf.summary.FileWriter('./summaries')
means = np.random.random(10)    
writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag='means', histo=means)]))
</code></pre>
","There are a couple of SO answers on creating a custom scalar summary in TF (here and here), but I can't find anything on creating a custom histogram summary. The documentation seems to be very lacking for custom summaries. I have a numpy array of that I'd like to make a summary of - any ideas on how? (tf.Summary.Value has a histo field that I tried using, but it requires a tensorflow::HistogramProto; there's no documentation on that class either, so I'm at a loss on how to make it. I've tried creating a minimal failing example below).",https://stackoverflow.com/questions/42012906,4880003,Inadequate Examples
53367063,"tensorflow python expected dense_input to have 2 dimensions, but got array with shape (5, 28, 5)","<p>I am a complete newbie to tensorflow, trying to learn about it and solve a problem.  I tried a lot of tutorials but they all talked about the same classify image or mnist stuff, so I followed the documentation and tried to figure something out.  </p>

<p>The goal is to find a pattern to predict the result when the input is [[1000,10, 5, 3, 1744...etc.  There are only 5 cases when the value is 300 400, 500, 600, 700, with shape 28,5 and the result for each is 28,2 list.  The data is loaded from file and assigned to tf.tensor.  </p>

<p>Here's my code:</p>

<pre><code>model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(28, activation=tf.nn.relu, input_shape=(5,)))
model.add(tf.keras.layers.Dense(28, activation=tf.nn.relu, input_shape=(5,)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(28))

model.compile(optimizer='adam',
              loss='mean_squared_error',
              metrics=['accuracy'])

model.fit(newData, newResults, epochs=3, steps_per_epoch=5)
</code></pre>

<p>newData:</p>

<pre><code>[[[300, 10, 5, 3, 1744], [300, 10, 5, 5, 2848], [300, 10, 5, 4, 2418], [300, 10, 5, 2, 1152], [300, 10, 5, 3, 1126], [300, 10, 5, 3, 1897], [300, 10, 5, 3, 1089], [300, 10, 5, 2, 1581], [300, 10, 5, 4, 1793], [300, 10, 5, 3, 1525], [300, 10, 5, 2, 1529], [300, 10, 5, 3, 1052], [300, 10, 5, 2, 1556], [300, 10, 5, 3, 1569], [300, 10, 5, 5, 2873], [300, 10, 5, 4, 2269], [300, 10, 5, 3, 3003], [300, 10, 5, 3, 1310], [300, 10, 5, 3, 1464], [300, 10, 5, 3, 2807], [300, 10, 5, 2, 1262], [300, 10, 5, 3, 1734], [300, 10, 5, 2, 2709], [300, 10, 5, 3, 2234], [300, 10, 5, 3, 1961], [300, 10, 5, 2, 1594], [300, 10, 5, 2, 1836], [300, 10, 5, 2, 1345]], 
[[400, 10, 5, 3, 1744], [400, 10, 5, 5, 2848], [400, 10, 5, 4, 2418], [400, 10, 5, 2, 1152], [400, 10, 5, 3, 1126], [400, 10, 5, 3, 1897], [400, 10, 5, 3, 1089], [400, 10, 5, 2, 1581], [400, 10, 5, 4, 1793], [400, 10, 5, 3, 1525], [400, 10, 5, 2, 1529], [400, 10, 5, 3, 1052], [400, 10, 5, 2, 1556], [400, 10, 5, 3, 1569], [400, 10, 5, 5, 2873], [400, 10, 5, 4, 2269], [400, 10, 5, 3, 3003], [400, 10, 5, 3, 1310], [400, 10, 5, 3, 1464], [400, 10, 5, 3, 2807], [400, 10, 5, 2, 1262], [400, 10, 5, 3, 1734], [400, 10, 5, 2, 2709], [400, 10, 5, 3, 2234], [400, 10, 5, 3, 1961], [400, 10, 5, 2, 1594], [400, 10, 5, 2, 1836], [400, 10, 5, 2, 1345]], 
[[500, 10, 5, 3, 1744], [500, 10, 5, 5, 2848], [500, 10, 5, 4, 2418], [500, 10, 5, 2, 1152], [500, 10, 5, 3, 1126], [500, 10, 5, 3, 1897], [500, 10, 5, 3, 1089], [500, 10, 5, 2, 1581], [500, 10, 5, 4, 1793], [500, 10, 5, 3, 1525], [500, 10, 5, 2, 1529], [500, 10, 5, 3, 1052], [500, 10, 5, 2, 1556], [500, 10, 5, 3, 1569], [500, 10, 5, 5, 2873], [500, 10, 5, 4, 2269], [500, 10, 5, 3, 3003], [500, 10, 5, 3, 1310], [500, 10, 5, 3, 1464], [500, 10, 5, 3, 2807], [500, 10, 5, 2, 1262], [500, 10, 5, 3, 1734], [500, 10, 5, 2, 2709], [500, 10, 5, 3, 2234], [500, 10, 5, 3, 1961], [500, 10, 5, 2, 1594], [500, 10, 5, 2, 1836], [500, 10, 5, 2, 1345]], 
[[600, 10, 5, 3, 1744], [600, 10, 5, 5, 2848], [600, 10, 5, 4, 2418], [600, 10, 5, 2, 1152], [600, 10, 5, 3, 1126], [600, 10, 5, 3, 1897], [600, 10, 5, 3, 1089], [600, 10, 5, 2, 1581], [600, 10, 5, 4, 1793], [600, 10, 5, 3, 1525], [600, 10, 5, 2, 1529], [600, 10, 5, 3, 1052], [600, 10, 5, 2, 1556], [600, 10, 5, 3, 1569], [600, 10, 5, 5, 2873], [600, 10, 5, 4, 2269], [600, 10, 5, 3, 3003], [600, 10, 5, 3, 1310], [600, 10, 5, 3, 1464], [600, 10, 5, 3, 2807], [600, 10, 5, 2, 1262], [600, 10, 5, 3, 1734], [600, 10, 5, 2, 2709], [600, 10, 5, 3, 2234], [600, 10, 5, 3, 1961], [600, 10, 5, 2, 1594], [600, 10, 5, 2, 1836], [600, 10, 5, 2, 1345]], 
[[700, 10, 5, 3, 1744], [700, 10, 5, 5, 2848], [700, 10, 5, 4, 2418], [700, 10, 5, 2, 1152], [700, 10, 5, 3, 1126], [700, 10, 5, 3, 1897], [700, 10, 5, 3, 1089], [700, 10, 5, 2, 1581], [700, 10, 5, 4, 1793], [700, 10, 5, 3, 1525], [700, 10, 5, 2, 1529], [700, 10, 5, 3, 1052], [700, 10, 5, 2, 1556], [700, 10, 5, 3, 1569], [700, 10, 5, 5, 2873], [700, 10, 5, 4, 2269], [700, 10, 5, 3, 3003], [700, 10, 5, 3, 1310], [700, 10, 5, 3, 1464], [700, 10, 5, 3, 2807], [700, 10, 5, 2, 1262], [700, 10, 5, 3, 1734], [700, 10, 5, 2, 2709], [700, 10, 5, 3, 2234], [700, 10, 5, 3, 1961], [700, 10, 5, 2, 1594], [700, 10, 5, 2, 1836], [700, 10, 5, 2, 1345]]]
</code></pre>

<p>newResult:</p>

<pre><code>[[[29.0, 8.92], [52.0, 21.67], [41.0, 14.38], [7.0, 1.49], [26.0, 8.25], [18.0, 4.53], [24.0, 6.61], [21.0, 9.54], [17.0, 5.53], [27.0, 9.61], [11.0, 0.35], [22.0, 8.11], [7.0, 1.22], [36.0, 15.49], [57.0, 31.44], [43.0, 16.52], [34.0, 11.46], [15.0, 2.49], [20.0, 2.34], [16.0, 4.86], [10.0, 0.8], [8.0, 0.4], [1.0, 0.0], [30.0, 7.57], [24.0, 7.21], [5.0, 0.58], [14.0, 0.73], [4.0, 0.15]], 
[[45.0, 8.17], [100.0, 43.28], [54.0, 16.05], [10.0, 2.77], [37.0, 8.86], [27.0, 6.12], [33.0, 9.13], [34.0, 14.03], [20.0, 5.06], [45.0, 15.42], [21.0, 0.69], [26.0, 8.83], [11.0, 2.14], [44.0, 17.74], [73.0, 43.39], [43.0, 18.8], [46.0, 21.56], [29.0, 9.16], [21.0, 3.76], [20.0, 7.39], [16.0, 2.54], [1.0, 1.63], [1.0, 0.02], [28.0, 12.14], [30.0, 12.35], [7.0, 1.18], [19.0, 3.29], [4.0, 0.16]], 
[[59.0, 18.74], [100.0, 75.18], [69.0, 32.13], [11.0, 3.04], [49.0, 15.76], [30.0, 10.33], [45.0, 14.51], [43.0, 20.82], [37.0, 8.2], [69.0, 24.53], [1.0, 0.3], [38.0, 12.57], [1.0, 3.67], [65.0, 24.77], [91.0, 57.39], [53.0, 18.22], [47.0, 27.07], [34.0, 16.31], [25.0, 5.39], [31.0, 11.5], [23.0, 5.73], [19.0, 4.11], [2.0, 0.11], [35.0, 15.52], [41.0, 18.15], [7.0, 1.48], [25.0, 7.53], [3.0, 0.14]], 
[[80.0, 30.29], [100.0, 85.22], [94.0, 52.73], [11.0, 2.45], [72.0, 30.7], [46.0, 14.75], [70.0, 22.81], [50.0, 28.26], [40.0, 14.19], [60.0, 26.82], [14.0, 0.28], [45.0, 19.1], [16.0, 4.72], [82.0, 40.98], [100.0, 78.96], [66.0, 27.05], [67.0, 31.09], [34.0, 16.92], [23.0, 7.03], [48.0, 21.28], [27.0, 8.19], [21.0, 3.95], [2.0, 0.17], [43.0, 19.96], [55.0, 23.54], [8.0, 1.47], [28.0, 12.04], [4.0, 0.13]], 
[[95.0, 38.09], [100.0, 92.88], [99.0, 58.96], [13.0, 3.54], [96.0, 45.78], [33.0, 12.05], [87.0, 38.11], [62.0, 34.97], [48.0, 15.49], [84.0, 33.13], [10.0, 0.09], [63.0, 25.52], [16.0, 4.87], [100.0, 55.9], [100.0, 91.32], [90.0, 34.24], [96.0, 45.36], [37.0, 15.13], [27.0, 9.28], [49.0, 26.3], [30.0, 10.92], [22.0, 3.72], [3.0, 0.14], [67.0, 24.82], [73.0, 31.32], [8.0, 1.36], [31.0, 15.03], [4.0, 0.2]]]
</code></pre>

<p>Getting this error when I run it:</p>

<pre><code>  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1536, in fit
    validation_split=validation_split)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 992, in _standardize_user_data
    class_weight, batch_size)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1117, in _standardize_weights
    exception_prefix='input')
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 323, in standardize_input_data
    'with shape ' + str(data_shape))
ValueError: Error when checking input: expected dense_input to have 2 dimensions, but got array with shape (5, 28, 5)
</code></pre>

<p>I know my model definitely has something wrong with it, but I can't quite figure out what.  I have trouble finding information other than the afore mentioned examples.</p>
","I am a complete newbie to tensorflow, trying to learn about it and solve a problem. I tried a lot of tutorials but they all talked about the same classify image or mnist stuff, so I followed the documentation and tried to figure something out. The goal is to find a pattern to predict the result when the input is [[1000,10, 5, 3, 1744...etc. There are only 5 cases when the value is 300 400, 500, 600, 700, with shape 28,5 and the result for each is 28,2 list. The data is loaded from file and assigned to tf.tensor. Here's my code: newData: newResult: Getting this error when I run it: I know my model definitely has something wrong with it, but I can't quite figure out what. I have trouble finding information other than the afore mentioned examples.",https://stackoverflow.com/questions/53367063,10672298,Requesting (Additional) Resources
57206247,How to fix ‘RuntimeError: The Session graph is empty. Add operations to the graph before calling run().”,"<p>I just simply typed the code given in <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Tensor"" rel=""noreferrer"">tf.Tensor Tensorflow 2.0</a>, and here is my code:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
print(tf.__version__)
# Build a dataflow graph.
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)

# Construct a `Session` to execute the graph.
sess = tf.compat.v1.Session()

# Execute the graph and store the value that `e` represents in `result`.
result = sess.run(e)
</code></pre>

<p>But it raised an error:</p>

<pre><code>2.0.0-beta1
2019-07-25 17:06:35.972372: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/Users/yupng/Documents/Dissertation/kmnist/kminst_v1.0.py"", line 14, in &lt;module&gt;
    result = sess.run(e)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1098, in _run
    raise RuntimeError('The Session graph is empty.  Add operations to the '
RuntimeError: The Session graph is empty.  Add operations to the graph before calling run().

Process finished with exit code 1
</code></pre>

<p>What could I do to fix this error?</p>
","I just simply typed the code given in tf.Tensor Tensorflow 2.0, and here is my code: But it raised an error: What could I do to fix this error?",https://stackoverflow.com/questions/57206247,11837392,Documentation Replicability
70644285,Loading binary data with FixedLengthRecordDataset in TensorFlow,"<p>I'm trying to figure out how to load binary data file using <code>FixedLengthRecordDataset</code>:</p>
<pre><code>import tensorflow as tf
import struct
import numpy as np

RAW_N = 2 + 20*20 + 1

def convert_binary_to_float_array(register):
    return struct.unpack('f'*RAW_N, register.numpy())

raw_dataset = tf.data.FixedLengthRecordDataset(filenames=['mydata.bin'],record_bytes=RAW_N*4)
float_ds = raw_dataset.map(map_func=convert_binary_to_float_array)
</code></pre>
<p>This code throws:</p>
<pre><code>AttributeError: in user code:

tf-load-data.py:14 convert_binary_to_float_array  *
    return struct.unpack('f'*RAW_N, register.numpy())

AttributeError: 'Tensor' object has no attribute 'numpy'
</code></pre>
<p><code>numpy()</code> is available if I try to iterate over the dataset:</p>
<pre><code>raw_dataset = tf.data.FixedLengthRecordDataset(filenames=['mydata.bin'],record_bytes=RAW_N*4)

for register in raw_dataset:
    print(struct.unpack('f'*RAW_N, register.numpy()))
</code></pre>
<p>By reading the <a href=""https://www.tensorflow.org/api_docs/python/tf/Tensor"" rel=""nofollow noreferrer"">Tensor</a> type description, I realized that <code>numpy()</code> is available only during eager execution. Thus, I can deduce that during the <code>map()</code> call the elements are not provided as <code>EagerTensor</code>.</p>
<p>How to load this data into a dataset?</p>
<p>I'm using TensorFlow 2.4.1</p>
","I'm trying to figure out how to load binary data file using FixedLengthRecordDataset: This code throws: numpy() is available if I try to iterate over the dataset: By reading the Tensor type description, I realized that numpy() is available only during eager execution. Thus, I can deduce that during the map() call the elements are not provided as EagerTensor. How to load this data into a dataset? I'm using TensorFlow 2.4.1",https://stackoverflow.com/questions/70644285,3055724,Requesting (Additional) Resources
71951333,Prepare data input for tensorflow from numpy and scipy.sparse,"<p>How to prepare data for input into a tensorflow model (say a keras Sequential one) ?</p>
<p>I know how to prepare <code>x_train</code>, <code>y_train</code>, <code>x_test</code> and <code>y_test</code> using numpy and scipy (eventually pandas, <code>sklearn</code> style) where <code>train</code>/<code>test</code> datas are train and test datas for training a neural model, and <code>x</code>/<code>y</code> stand for a 2D sparse matrix and a 1D numpy array representing integer labels of the same size as the number of raws in the <code>x</code> data.</p>
<p>I'm struggling with the <a href=""https://www.tensorflow.org/guide/data#reading_input_data"" rel=""nofollow noreferrer"">Dataset documentation</a> without many insight so far ...</p>
<p>So far, I could only convert the scipy.sparse matrix into a <a href=""https://www.tensorflow.org/guide/sparse_tensor"" rel=""nofollow noreferrer"">tensorflow.SparseTensor</a> using something like</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf
from scipy import sparse as sp

x = sp.csr_matrix( ... )
x = tf.SparseTensor(indices=np.vstack([*x.nonzero()]).T, 
                    values=x.data, 
                    dense_shape=x.shape)
</code></pre>
<p>and I can convert the numpy array into a <a href=""https://www.tensorflow.org/api_docs/python/tf/Tensor"" rel=""nofollow noreferrer"">tensorflow.Tensor</a> using something like</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf

y = np.array( ... ) # 1D array of len == x.shape[0]
y = tf.constant(y)
</code></pre>
<ul>
<li>How to align the <code>x</code> and <code>y</code> into a single Dataset in order to construct the batch, buffers, ... and benefit from the Dataset utilities ?</li>
<li>Should I use either <code>zip</code>, <code>from_tensor_slices</code>, or any other method of the <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">tensorflow.data.Dataset</a> module ?</li>
</ul>
<p>Examples of <code>x</code> and <code>y</code> are</p>
<pre class=""lang-py prettyprint-override""><code>x = tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])
y = tf.constant(np.array(range(3)))
</code></pre>
","How to prepare data for input into a tensorflow model (say a keras Sequential one) ? I know how to prepare x_train, y_train, x_test and y_test using numpy and scipy (eventually pandas, sklearn style) where train/test datas are train and test datas for training a neural model, and x/y stand for a 2D sparse matrix and a 1D numpy array representing integer labels of the same size as the number of raws in the x data. I'm struggling with the Dataset documentation without many insight so far ... So far, I could only convert the scipy.sparse matrix into a tensorflow.SparseTensor using something like and I can convert the numpy array into a tensorflow.Tensor using something like Examples of x and y are",https://stackoverflow.com/questions/71951333,8844500,Requesting (Additional) Resources
65734836,"Numpy Equivalent to ""tf.tensor_scatter_nd_add"" method","<p>Question is in the title really, I am looking for a method in scipy/numpy/etc. (not TensorFlow) which encapsulates the behaviour described in the <a href=""https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add"" rel=""nofollow noreferrer"">tf.tensor_scatter_nd_add</a> but on Numpy arrays rather than tensors.</p>
<p>I have come across the <a href=""https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.ndimage.sum.html"" rel=""nofollow noreferrer"">scipy.ndimage.sum</a> method, but couldn't get this to reproduce the example I've given below.</p>
<p>Whichever method you think fits has to be able to reproduce the rank-3 example that is provided in the TF Documentation:</p>
<pre class=""lang-py prettyprint-override""><code>    indices = tf.constant([[0], [2]])
    updates = tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],
                            [7, 7, 7, 7], [8, 8, 8, 8]],
                           [[5, 5, 5, 5], [6, 6, 6, 6],
                            [7, 7, 7, 7], [8, 8, 8, 8]]])
    tensor = tf.ones([4, 4, 4],dtype=tf.int32)
    updated = tf.tensor_scatter_nd_add(tensor, indices, updates)
    print(updated)
</code></pre>
<p>Hopefully someone has solved a similar problem before and can help here - Thanks in advance!</p>
","Question is in the title really, I am looking for a method in scipy/numpy/etc. (not TensorFlow) which encapsulates the behaviour described in the tf.tensor_scatter_nd_add but on Numpy arrays rather than tensors. I have come across the scipy.ndimage.sum method, but couldn't get this to reproduce the example I've given below. Whichever method you think fits has to be able to reproduce the rank-3 example that is provided in the TF Documentation: Hopefully someone has solved a similar problem before and can help here - Thanks in advance!",https://stackoverflow.com/questions/65734836,7861160,Requesting (Additional) Resources
67652872,InvalidArgumentError: Inner dimensions of output shape must match inner dimensions of updates shape,"<p>I'm trying to implement an SPL loss in keras. All I need to do is pretty simple, I'll write it in numpy to explain what I need:</p>
<pre class=""lang-py prettyprint-override""><code>def spl_loss(y_true, y_pred, lmda):
    # compute any arbitrary loss function
    L = categorical_cross_entropy(y_true, y_pred)
    # set to zero those values with an error greater than lambda
    L[L&gt;lmda] = 0
    return L
</code></pre>
<p>I'm trying to implement it <a href=""https://medium.com/@Bloomore/how-to-write-a-custom-loss-function-with-additional-arguments-in-keras-5f193929f7a0"" rel=""nofollow noreferrer"">following this tutorial</a> but I'm having troubles with the step needed to set values to zero.</p>
<p>Currently I have the following code:</p>
<pre class=""lang-py prettyprint-override""><code>def spl_loss(lmda, loss_fn):
    def loss(y_true, y_pred):
         # compute an arbitrary loss function, L
        loss_value = loss_fn(y_true, y_pred) # tensor of shape (64,)
        # get the mask of L greater than lmda
        mask = tf.greater( loss_value, tf.constant( float(lmda) ) )    # tensor of shape (64,)
        # compute indexes for the mask
        indexes = tf.reshape(tf.where(mask), [-1])  # tensor of shape (n,); where n&lt;=64
        # set to zero values on indexes
        spl_loss_value = tf.tensor_scatter_nd_update(loss_value, indexes, tf.zeros_like(loss_value, dtype=loss_value.dtype) )  # this line gives the error
        
        return spl_loss_value
    return loss
</code></pre>
<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"" rel=""nofollow noreferrer"">docs</a>, <code>tensor_scatter_nd_update</code> operation should perform the assignment operation, but it fails with the following error:</p>
<pre><code>    spl_loss_value = tf.tensor_scatter_nd_update(loss_value, indexes, tf.zeros_like(loss_value, dtype=loss_value.dtype) )
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper  **
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5512 tensor_scatter_nd_update
        tensor=tensor, indices=indices, updates=updates, name=name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py:11236 tensor_scatter_update
        _ops.raise_from_not_ok_status(e, name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:6862 raise_from_not_ok_status
        six.raise_from(core._status_to_exception(e.code, message), None)
    &lt;string&gt;:3 raise_from
        

    InvalidArgumentError: Inner dimensions of output shape must match inner dimensions of updates shape. Output: [64] updates: [64] [Op:TensorScatterUpdate]
</code></pre>
<p>I'm running it in colab, <a href=""https://colab.research.google.com/drive/1JT2ub3p1kAQpLp5ESqOhsvna4b8M-MNp?usp=sharing"" rel=""nofollow noreferrer"">here</a> you can try it.</p>
<p>I tried several re-shapes, because I understand it is a matter of shapes expected vs obtained, but I don't find the way. What's going on here?</p>
<p>Thanks in advance</p>
","I'm trying to implement an SPL loss in keras. All I need to do is pretty simple, I'll write it in numpy to explain what I need: I'm trying to implement it following this tutorial but I'm having troubles with the step needed to set values to zero. Currently I have the following code: According to the docs, tensor_scatter_nd_update operation should perform the assignment operation, but it fails with the following error: I'm running it in colab, here you can try it. I tried several re-shapes, because I understand it is a matter of shapes expected vs obtained, but I don't find the way. What's going on here? Thanks in advance",https://stackoverflow.com/questions/67652872,4913143,Documentation Replicability
37822097,Using TensorArrays in the context of a while_loop to accumulate values,"<p>Below I have an implementation of a Tensorflow RNN Cell, designed to emulate Alex Graves' algorithm ACT in this paper: <a href=""http://arxiv.org/abs/1603.08983"" rel=""noreferrer"">http://arxiv.org/abs/1603.08983</a>.</p>

<p>At a single timestep in the sequence called via rnn.rnn(with a static sequence_length parameter, so the rnn is unrolled dynamically - I am using a fixed batch size of 20), we recursively call ACTStep, producing outputs of size(1,200) where the hidden dimension of the RNN cell is 200 and we have a batch size of 1. </p>

<p>Using the while loop in Tensorflow, we iterate until the accumulated halting probability is high enough. All of this works reasonably smoothly, but I am having problems accumulating states, probabilities and outputs within the while loop, which we need to do in order to create weighted combinations of these as the final cell output/state.</p>

<p>I have tried using a simple list, as below, but this fails when the graph is compiled as the outputs are not in the same frame(is it possible to use the ""switch"" function in control_flow_ops to forward the tensors to the point at which they are required, ie the add_n function just before we return the values?). I have also tried using the TensorArray structure, but I am finding this difficult to use as it seems to destroy shape information and replacing it manually hasn't worked. I also haven't been able to find much documentation on TensorArrays, presumably as they are, I imagine, mainly for internal TF use.</p>

<p>Any advice on how it might be possible to correctly accumulate the variables produced by ACTStep would be much appreciated.</p>

<pre><code>class ACTCell(RNNCell):
""""""An RNN cell implementing Graves' Adaptive Computation time algorithm""""""
def __init__(self, num_units, cell, epsilon, max_computation):

    self.one_minus_eps = tf.constant(1.0 - epsilon)
    self._num_units = num_units
    self.cell = cell
    self.N = tf.constant(max_computation)
@property
def input_size(self):
    return self._num_units
@property
def output_size(self):
    return self._num_units
@property
def state_size(self):
    return self._num_units

def __call__(self, inputs, state, scope=None):

    with vs.variable_scope(scope or type(self).__name__):

        # define within cell constants/ counters used to control while loop
        prob = tf.get_variable(""prob"", [], tf.float32,tf.constant_initializer(0.0))
        counter = tf.get_variable(""counter"", [],tf.float32,tf.constant_initializer(0.0))
        tf.assign(prob,0.0)
        tf.assign(counter, 0.0)

        # the predicate for stopping the while loop. Tensorflow demands that we have
        # all of the variables used in the while loop in the predicate.
        pred = lambda prob,counter,state,input,\
                      acc_state,acc_output,acc_probs:\
            tf.logical_and(tf.less(prob,self.one_minus_eps), tf.less(counter,self.N))

        acc_probs = []
        acc_outputs = []
        acc_states = []


        _,iterations,_,_,acc_states,acc_output,acc_probs = \
        control_flow_ops.while_loop(pred,
        self.ACTStep,
        [prob,counter,state,input,acc_states,acc_outputs,acc_probs])

    # TODO:fix last part of this, need to use the remainder.
    # TODO: find a way to accumulate the regulariser

    # here we take a weighted combination of the states and outputs 
    # to use as the actual output and state which is passed to the next timestep.

    next_state = tf.add_n([tf.mul(x,y) for x,y in zip(acc_probs,acc_states)])
    output = tf.add_n([tf.mul(x,y) for x,y in zip(acc_probs,acc_outputs)])


    return output, next_state

def ACTStep(self,prob,counter,state,input, acc_states,acc_outputs,acc_probs):

    output, new_state = rnn.rnn(self.cell, [input], state, scope=type(self.cell).__name__)

    prob_w = tf.get_variable(""prob_w"", [self.cell.input_size,1])
    prob_b = tf.get_variable(""prob_b"", [1])
    p = tf.nn.sigmoid(tf.matmul(prob_w,new_state) + prob_b)

    acc_states.append(new_state)
    acc_outputs.append(output)
    acc_probs.append(p)

    return [tf.add(prob,p),tf.add(counter,1.0),new_state, input,acc_states,acc_outputs,acc_probs]
</code></pre>
","Below I have an implementation of a Tensorflow RNN Cell, designed to emulate Alex Graves' algorithm ACT in this paper: http://arxiv.org/abs/1603.08983. At a single timestep in the sequence called via rnn.rnn(with a static sequence_length parameter, so the rnn is unrolled dynamically - I am using a fixed batch size of 20), we recursively call ACTStep, producing outputs of size(1,200) where the hidden dimension of the RNN cell is 200 and we have a batch size of 1. Using the while loop in Tensorflow, we iterate until the accumulated halting probability is high enough. All of this works reasonably smoothly, but I am having problems accumulating states, probabilities and outputs within the while loop, which we need to do in order to create weighted combinations of these as the final cell output/state. I have tried using a simple list, as below, but this fails when the graph is compiled as the outputs are not in the same frame(is it possible to use the ""switch"" function in control_flow_ops to forward the tensors to the point at which they are required, ie the add_n function just before we return the values?). I have also tried using the TensorArray structure, but I am finding this difficult to use as it seems to destroy shape information and replacing it manually hasn't worked. I also haven't been able to find much documentation on TensorArrays, presumably as they are, I imagine, mainly for internal TF use. Any advice on how it might be possible to correctly accumulate the variables produced by ACTStep would be much appreciated.",https://stackoverflow.com/questions/37822097,6466534,Documentation Completeness
59427969,Mathematical definition of tensordot operation on TensorFlow tensor,"<p>I'm trying to reverse engineer the behavior of tf.tensordot axes parameter, but having a hard time.</p>

<p>Given the following code:</p>

<pre><code>a = tf.constant([[1., 2.], [3., 4.], [4., 5.]])
b = tf.constant([1., 2.])
c = tf.constant([[1., 2.], [2., 3.], [3., 4.]])

print(f'Shape of c: {c.shape}')

ct = tf.transpose(c)

print(f'Shape of ct: {ct.shape}')

print('.................')

d = tf.tensordot(a, ct, axes=1)
print(f'Shape of d: {d.shape}')
print(d)

print('.................')


e = tf.tensordot(a, ct, axes=0)
print(f'Shape of e: {e.shape}')
print(e)


print('.................')


f = tf.tensordot(a, ct, axes=2)
print(f'Shape of f: {f.shape}')
print(e)
</code></pre>

<p>I understand how ""d"" is produced, but I don't understand how ""e"" and ""f"" are produced. The <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer"">TensorFlow Documentation</a> is not sufficient for me to understand.</p>
","I'm trying to reverse engineer the behavior of tf.tensordot axes parameter, but having a hard time. Given the following code: I understand how ""d"" is produced, but I don't understand how ""e"" and ""f"" are produced. The TensorFlow Documentation is not sufficient for me to understand.",https://stackoverflow.com/questions/59427969,3656912,Documentation Completeness
43504906,How do I read variable length 1D inputs in Tensorflow?,"<p>I'm trying to read variable length 1-D inputs into a Tensorflow CNN. </p>

<p>I have previously implemented reading fixed length inputs by first constructing a CSV file (where the first column is the label and the remaining columns are the input values - flattened spectrogram data all padded/truncated to the same length) using tf.TextLineReader(). </p>

<p>This time I have a directory full of files each one containing a line of data I want to use as input (flattened spectrogram data again but I do not want to force them to the same dimensions), and the line lengths are not fixed. I'm getting an error trying to use the previous approach of compiling a CSV first. I looked into the documentation of tf.TextLineReader() and it specifies that all CSV rows must be the same shape, so I am stuck! Any help would be much appreciated, thanks :) </p>
","I'm trying to read variable length 1-D inputs into a Tensorflow CNN. I have previously implemented reading fixed length inputs by first constructing a CSV file (where the first column is the label and the remaining columns are the input values - flattened spectrogram data all padded/truncated to the same length) using tf.TextLineReader(). This time I have a directory full of files each one containing a line of data I want to use as input (flattened spectrogram data again but I do not want to force them to the same dimensions), and the line lengths are not fixed. I'm getting an error trying to use the previous approach of compiling a CSV first. I looked into the documentation of tf.TextLineReader() and it specifies that all CSV rows must be the same shape, so I am stuck! Any help would be much appreciated, thanks :)",https://stackoverflow.com/questions/43504906,3296050,Requesting (Additional) Resources
44401495,Tensorflow program results in type conversion error,"<p>Tring to master Tensorflow, following documentation of TensorFlow.</p>

<p><strong>Below program results in 'Incompatible type conversion error'</strong></p>

<pre><code>import tensorflow as tf

W = tf.Variable([.3], tf.float32)
b = tf.Variable([-3], tf.float32)
x = tf.placeholder(tf.float32)
linear_model = 1.0
linear_model = W * x + b
#tf.to_float(linear_model, name='ToFloat')

# Global initialization is must
init = tf.global_variables_initializer()
sess.run(init)

print(sess.run(linear_model, {x:[1,2,3,4]}))
</code></pre>

<p><strong>Above program results in this error</strong></p>

<blockquote>
  <p>File ""v-prog3-variables.py"", line 7, in 
      linear_model = W * x + b
  .. .. ..
  ValueError: Incompatible type conversion requested to type 'float32'
  for variable of type 'int32_ref'</p>
</blockquote>

<p><strong>I tried to solve the problem by defining the 'linear_model' variable as float (linear_model = 1.0) or tf.to_float(linear_model = W * x + b)</strong></p>

<p>but nothing works</p>

<p>Im a TensorFlow newbie, please help me out.
Thanks in advance.</p>
","Tring to master Tensorflow, following documentation of TensorFlow. Below program results in 'Incompatible type conversion error' Above program results in this error I tried to solve the problem by defining the 'linear_model' variable as float (linear_model = 1.0) or tf.to_float(linear_model = W * x + b) but nothing works Im a TensorFlow newbie, please help me out. Thanks in advance.",https://stackoverflow.com/questions/44401495,557806,Documentation Replicability
55619070,GraphKeys.TRAINABLE_VARIABLES vs tf.trainable_variables(),"<p>Is <code>GraphKeys.TRAINABLE_VARIABLES</code> is the same as <code>tf.trainable_variables()</code> ?</p>

<p>Is <code>GraphKeys.TRAINABLE_VARIABLES</code> actually <code>tf.GraphKeys.TRAINABLE_VARIABLES</code>?</p>

<p>Looks like networks successfully trains with:</p>

<pre><code>optimizer = tf.train.AdamOptimizer(config.LEARNING_RATE)
with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
    self.train_op = optimizer.minimize(self.loss, var_list=tf.trainable_variables())
</code></pre>

<p>but not with </p>

<pre><code>optimizer = tf.train.AdamOptimizer(config.LEARNING_RATE)
with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
    self.train_op = optimizer.minimize(self.loss)
</code></pre>

<p>According to <a href=""https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer#minimize"" rel=""nofollow noreferrer"">documentation</a>:</p>

<pre><code>var_list: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.
</code></pre>

<p>Also as I can see in batch normalization example code <code>var_list</code> is omited:</p>

<pre class=""lang-py prettyprint-override""><code>  x_norm = tf.layers.batch_normalization(x, training=training)

  # ...

  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
  with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)
</code></pre>
",Is GraphKeys.TRAINABLE_VARIABLES is the same as tf.trainable_variables() ? Is GraphKeys.TRAINABLE_VARIABLES actually tf.GraphKeys.TRAINABLE_VARIABLES? Looks like networks successfully trains with: but not with According to documentation: Also as I can see in batch normalization example code var_list is omited:,https://stackoverflow.com/questions/55619070,1179925,Documentation Ambiguity
47965551,Use tf.train.ExponentialMovingAverage() to Train the model,"<p>I'm learning TensorFlow and trying to apply exponential moving average based gradient descent (instead vanilla gradient descent). Specifically I;m trying to use <a href=""https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"" rel=""nofollow noreferrer"">tf.train.ExponentialMovingAverage</a> but the document doesn't seem to provide guide for how to use it to build model that drives optimization.</p>

<p>Full code is available at - <a href=""https://github.com/vibhorj/tf/blob/master/so/ema.py"" rel=""nofollow noreferrer"">https://github.com/vibhorj/tf/blob/master/so/ema.py</a>
, but here's what i'm doing and the classifier isn't learning anything (after each epoch, w &amp; b still remain same .. no learning)</p>

<p><strong>STEP1</strong>: define weights &amp; biases</p>

<pre><code>with tf.variable_scope('scp1', reuse=tf.AUTO_REUSE):
w = tf.get_variable(name='weights', initializer = tf.ones(shape=[2,3],dtype=tf.float32))
b = tf.get_variable(name='bias', initializer = tf.ones(shape=[3],dtype=tf.float32))
</code></pre>

<p><strong>STEP2</strong>: define error / loss / optimizer</p>

<pre><code>X = tf.placeholder(tf.float32, shape=[None,2], name='X')
Y = tf.placeholder(tf.float32, shape=[None,3], name='Y')
Ylogits = tf.matmul(X,w) + b
error = -Y*tf.log(Ylogits)
loss = tf.reduce_mean(error, name = 'loss') 
opt = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)
</code></pre>

<p><strong>STEP3</strong>: Created ExponentialMovingAverage object, created training_op that (I expect) to update the moving averages after each training step. </p>

<pre><code>ema = tf.train.ExponentialMovingAverage(decay=0.50,name='EMA')
training_op = ema.apply([w, b])
</code></pre>

<p><strong>STEP4</strong>: Finaly run the iterations</p>

<pre><code>with tf.Session() as sess:
    sess.run(tf.variables_initializer(tf.global_variables()))
    for epoch in range(10): #increase epocs later
        _ = sess.run([training_op], feed_dict=feed_train)
        print(""\n POST:"")
        print(""    {}:\n {}"".format(ema.average_name(w),sess.run(ema.average(w))))
        print(""    {}:\n {}"".format(ema.average_name(b),sess.run(ema.average(b))))
</code></pre>

<p>When I run it, weights &amp; biases stay the same with each successive iterations!</p>

<p>I know there's something i'm missing (to update the parameters!) but unable to identify! The doc isn't much helpful either. Thanks a lot for any clue any guidance on how I can proceed further.</p>

<p>Thanks!</p>
","I'm learning TensorFlow and trying to apply exponential moving average based gradient descent (instead vanilla gradient descent). Specifically I;m trying to use tf.train.ExponentialMovingAverage but the document doesn't seem to provide guide for how to use it to build model that drives optimization. Full code is available at - https://github.com/vibhorj/tf/blob/master/so/ema.py , but here's what i'm doing and the classifier isn't learning anything (after each epoch, w &amp; b still remain same .. no learning) STEP1: define weights &amp; biases STEP2: define error / loss / optimizer STEP3: Created ExponentialMovingAverage object, created training_op that (I expect) to update the moving averages after each training step. STEP4: Finaly run the iterations When I run it, weights &amp; biases stay the same with each successive iterations! I know there's something i'm missing (to update the parameters!) but unable to identify! The doc isn't much helpful either. Thanks a lot for any clue any guidance on how I can proceed further. Thanks!",https://stackoverflow.com/questions/47965551,4736890,Documentation Completeness
67267305,How should Exponential Moving Average be used in custom TF2.4 training loop,"<p>I have a custom training loop that can be simplified as follow</p>
<pre><code>inputs = tf.keras.Input(dtype=tf.float32, shape=(None, None, 3))
model = tf.keras.Model({&quot;inputs&quot;: inputs}, {&quot;loss&quot;: f(inputs)})
optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)

for inputs in batches:
    with tf.GradientTape() as tape:
        results = model(inputs, training=True)
    grads = tape.gradient(results[&quot;loss&quot;], model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
</code></pre>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"" rel=""nofollow noreferrer"">TensorFlow documentation of ExponentialMovingAverage</a> is not clear on how it should be used in <a href=""https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"" rel=""nofollow noreferrer"">from-scratch training loop</a>. As anyone worked with this?</p>
<p>Additionally, how should the shadow variable be restored into the model if both are still in memory, and how can I check that that training variables were correctly updated?</p>
","I have a custom training loop that can be simplified as follow The TensorFlow documentation of ExponentialMovingAverage is not clear on how it should be used in from-scratch training loop. As anyone worked with this? Additionally, how should the shadow variable be restored into the model if both are still in memory, and how can I check that that training variables were correctly updated?",https://stackoverflow.com/questions/67267305,1782553,Documentation Completeness
51182162,set device on model trained on GPU and predict on CPU,"<p>I trained a a model on a GPU and saved it like this (export_path is my output directory)</p>

<pre><code>builder = tf.saved_model.builder.SavedModelBuilder(export_path)

tensor_info_x = tf.saved_model.utils.build_tensor_info(self.Xph)
tensor_info_y = tf.saved_model.utils.build_tensor_info(self.predprob)
tensor_info_it = tf.saved_model.utils.build_tensor_info(self.istraining)
tensor_info_do = tf.saved_model.utils.build_tensor_info(self.dropout)

prediction_signature = (
       tf.saved_model.signature_def_utils.build_signature_def(
              inputs={'myx': tensor_info_x, 'istraining': tensor_info_it, 'dropout': tensor_info_do},
              outputs={'ypred': tensor_info_y},
              method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))

builder.add_meta_graph_and_variables(
       net, [tf.saved_model.tag_constants.SERVING],
       signature_def_map={
           tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
           prediction_signature },)
builder.save()
</code></pre>

<p>Now I'm trying to load this and run predictions. It works fine if I am on a GPU, but w/o a GPU around I get:</p>

<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/layer_norm_basic_lstm_cell/dropout/add/Enter': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
</code></pre>

<p>Now I read about tf.train.import_meta_graph and the clear_device option, but I can't get this work. I'm loading my models like so:</p>

<pre><code>predict_fn = predictor.from_saved_model(modelname)
</code></pre>

<p>at which point is throw the error mentioned above. modelname is the full filename of the pb file. Is there a way to go through the nodes of the graph and manually set the device (or doing something similar)?
I'm using tensorflow 1.8.0 </p>

<p>I saw <a href=""https://stackoverflow.com/questions/40980035/can-a-model-trained-on-gpu-used-on-cpu-for-inference-and-vice-versa"">Can a model trained on gpu used on cpu for inference and vice versa?</a> which I don't think I'm duplicating.
The difference with that question is that I want to know what to do <strong>after training</strong> </p>
","I trained a a model on a GPU and saved it like this (export_path is my output directory) Now I'm trying to load this and run predictions. It works fine if I am on a GPU, but w/o a GPU around I get: Now I read about tf.train.import_meta_graph and the clear_device option, but I can't get this work. I'm loading my models like so: at which point is throw the error mentioned above. modelname is the full filename of the pb file. Is there a way to go through the nodes of the graph and manually set the device (or doing something similar)? I'm using tensorflow 1.8.0 I saw Can a model trained on gpu used on cpu for inference and vice versa? which I don't think I'm duplicating. The difference with that question is that I want to know what to do after training",https://stackoverflow.com/questions/51182162,2208967,Requesting (Additional) Resources
49716186,What's the difference between tf.train.ExponentialMovingAverage and tf.train.MomentumOptimizer?,"<p>I have seen the doc, tf.train.ExponentialMovingAverage implement this formula:</p>

<pre><code>shadow_variable = decay * shadow_variable + (1 - decay) * variable
</code></pre>

<p>I didn't find the formula of tf.train.MomentumOptimizer. But I think it may be:</p>

<pre><code>v = γ*v - learning_rate*dx
θ = θ - v
</code></pre>

<p>I think this two function have similar effect, so can they exchange each other?Or they have different application scenarios? Or I'm totally wrong? </p>

<p>And does the shadow_variable is equivalent to θ?</p>

<p>Thanks for any guidance.</p>
","I have seen the doc, tf.train.ExponentialMovingAverage implement this formula: I didn't find the formula of tf.train.MomentumOptimizer. But I think it may be: I think this two function have similar effect, so can they exchange each other?Or they have different application scenarios? Or I'm totally wrong? And does the shadow_variable is equivalent to θ? Thanks for any guidance.",https://stackoverflow.com/questions/49716186,9589731,Documentation Ambiguity
57193336,"What is the return of ""tf.train.slice_input_producer()""?","<p>I was interested in srgan, and I wanted to run the code with npy type input rather than png type input. However, since there is no function that accepts the npy format(such as ""decode_png()""), I was trying to fix the data_loader part. And I had difficulty in ""<strong>tf.train.slice_input_producer()</strong>"".</p>

<p>In tensorflow, the return of ""tf.train.slice_input_producer()"" is described as
""A list of tensors, one for each element of tensor_list. If the tensor in tensor_list has shape [N, a, b, .., z], then the corresponding output tensor will have shape [a, b, ..., z].""</p>

<p>But, I do not fully understand this explanation.
For example, if I have a list of 230 image tensors with a height of 100 and a width of 100([230,]), would I return one by one([1,])?</p>

<p>Then, ""<strong>read_file()</strong>"" returns  its entire contents for one image([100,100])?</p>

<pre class=""lang-py prettyprint-override""><code>with tf.variable_scope('load_image'):
    output = tf.train.slice_input_producer([image_list_LR_tensor, image_list_HR_tensor], shuffle=False, capacity=4096)

    # Reading and decode the images
    reader = tf.WholeFileReader(name='image_reader')
    image_LR = tf.read_file(output[0])
    image_HR = tf.read_file(output[1])

    input_image_LR = tf.image.decode_png(image_LR, channels=3)
    input_image_HR = tf.image.decode_png(image_HR, channels=3)
</code></pre>
","I was interested in srgan, and I wanted to run the code with npy type input rather than png type input. However, since there is no function that accepts the npy format(such as ""decode_png()""), I was trying to fix the data_loader part. And I had difficulty in ""tf.train.slice_input_producer()"". In tensorflow, the return of ""tf.train.slice_input_producer()"" is described as ""A list of tensors, one for each element of tensor_list. If the tensor in tensor_list has shape [N, a, b, .., z], then the corresponding output tensor will have shape [a, b, ..., z]."" But, I do not fully understand this explanation. For example, if I have a list of 230 image tensors with a height of 100 and a width of 100([230,]), would I return one by one([1,])? Then, ""read_file()"" returns its entire contents for one image([100,100])?",https://stackoverflow.com/questions/57193336,11833477,Documentation Ambiguity
41909915,tf.train.string_input_producer behavior in a loop,"<p>The following snippet has been taken from the TensorFlow 0.12 API documentation</p>

<pre class=""lang-py prettyprint-override""><code>def input_pipeline(filenames, batch_size, num_epochs=None):
  filename_queue = tf.train.string_input_producer(
      filenames, num_epochs=num_epochs, shuffle=True)
  example, label = read_my_file_format(filename_queue)
  # min_after_dequeue defines how big a buffer we will randomly sample
  #   from -- bigger means better shuffling but slower start up and more
  #   memory used.
  # capacity must be larger than min_after_dequeue and the amount larger
  #   determines the maximum we will prefetch.  Recommendation:
  #   min_after_dequeue + (num_threads + a small safety margin) * batch_size
  min_after_dequeue = 10000
  capacity = min_after_dequeue + 3 * batch_size
  example_batch, label_batch = tf.train.shuffle_batch(
      [example, label], batch_size=batch_size, capacity=capacity,
      min_after_dequeue=min_after_dequeue)
  return example_batch, label_batch
</code></pre>

<p>The question I have might be very basic for a regular TensorFlow user, but I am an absolute beginner. The question is the following :</p>

<ul>
<li><code>tf.train.string_input_producer</code> creates a queue for holding the filenames. As the <code>input_pipeline()</code> is called over and over again during training, how will it be ensured that everytime the same queue is used ? I guess, it is important since, if different calls to <code>input_pipeline()</code> result in a creation of a new queue, there does not seem to be a way to ensure that different images are picked everytime  and epoch counter and shuffling can be properly maintained.</li>
</ul>
","The following snippet has been taken from the TensorFlow 0.12 API documentation The question I have might be very basic for a regular TensorFlow user, but I am an absolute beginner. The question is the following :",https://stackoverflow.com/questions/41909915,6842947,Documentation Ambiguity
42453138,Tensorflow automatically deletes old model files,"<p>I use tf.train.Supervisor to start a session to train my model, and save model parameter every 1000 steps. But it seems that tensorflow would automatically delete old model files. Only 5 recent models were saved. Once model.ckpt-6000 is produced, the model.ckpt-1000 is deleted. But I can not find any documents on this operation.</p>
","I use tf.train.Supervisor to start a session to train my model, and save model parameter every 1000 steps. But it seems that tensorflow would automatically delete old model files. Only 5 recent models were saved. Once model.ckpt-6000 is produced, the model.ckpt-1000 is deleted. But I can not find any documents on this operation.",https://stackoverflow.com/questions/42453138,7620627,Documentation Completeness
45357661,Tensor returned by tf.tranpose() different when stored?,"<p>I am writing an application using TensorFlow and I'm using the tf.transpose() function. The API states that the function returns a transposed tensor, which is what you'd expect. However, I noticed the following phenomenon:</p>

<pre><code>&gt;&gt;&gt; tf.transpose([3, 5])
    &lt;tf.Tensor 'transpose:0' shape=(2,) dtype=int32&gt;
&gt;&gt;&gt; a = tf.transpose([3, 5])
&gt;&gt;&gt; a
   &lt;tf.Tensor 'transpose_1:0' shape=(2,) dtype=int32&gt;
&gt;&gt;&gt; a ==  tf.transpose([3, 5])
</code></pre>

<p>Does anyone know why this happens or how it should be used?</p>
","I am writing an application using TensorFlow and I'm using the tf.transpose() function. The API states that the function returns a transposed tensor, which is what you'd expect. However, I noticed the following phenomenon: Does anyone know why this happens or how it should be used?",https://stackoverflow.com/questions/45357661,4699762,Documentation Replication on Other Examples
38517533,How to permutate tranposition in tensorflow?,"<p>From the <a href=""https://www.tensorflow.org/api_docs/python/tf/transpose"" rel=""noreferrer"">docs</a>:</p>

<blockquote>
  <p>Transposes <code>a</code>. Permutes the dimensions according to perm.</p>
  
  <p>The returned tensor's dimension i will correspond to the input
  dimension <code>perm[i]</code>. If <code>perm</code> is not given, it is set to (n-1...0), where
  n is the rank of the input tensor. Hence by default, this operation
  performs a regular matrix transpose on 2-D input Tensors.</p>
</blockquote>

<p>But it's still a little unclear to me how should I be slicing the input tensor. E.g. from the docs too:</p>

<pre><code>tf.transpose(x, perm=[0, 2, 1]) ==&gt; [[[1  4]
                                      [2  5]
                                      [3  6]]

                                     [[7 10]
                                      [8 11]
                                      [9 12]]]
</code></pre>

<p><strong>Why is it that <code>perm=[0,2,1]</code> produces a 1x3x2 tensor?</strong></p>

<p>After some trial and error:</p>

<pre><code>twothreefour = np.array([ [[1,2,3,4], [5,6,7,8], [9,10,11,12]] , 
                        [[13,14,15,16], [17,18,19,20], [21,22,23,24]] ])
twothreefour
</code></pre>

<p>[out]:</p>

<pre><code>array([[[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]],

       [[13, 14, 15, 16],
        [17, 18, 19, 20],
        [21, 22, 23, 24]]])
</code></pre>

<p>And if I transpose it:</p>

<pre><code>fourthreetwo = tf.transpose(twothreefour) 
with tf.Session() as sess:
    init = tf.initialize_all_variables()
    sess.run(init)
    print (fourthreetwo.eval())
</code></pre>

<p>I get a 4x3x2 to a 2x3x4 and that sounds logical.</p>

<p>[out]:</p>

<pre><code>[[[ 1 13]
  [ 5 17]
  [ 9 21]]

 [[ 2 14]
  [ 6 18]
  [10 22]]

 [[ 3 15]
  [ 7 19]
  [11 23]]

 [[ 4 16]
  [ 8 20]
  [12 24]]]
</code></pre>

<p>But when I use the <code>perm</code> parameter the output, I'm not sure what I'm really getting:</p>

<pre><code>twofourthree = tf.transpose(twothreefour, perm=[0,2,1]) 
with tf.Session() as sess:
    init = tf.initialize_all_variables()
    sess.run(init)
    print (threetwofour.eval())
</code></pre>

<p>[out]:</p>

<pre><code>[[[ 1  5  9]
  [ 2  6 10]
  [ 3  7 11]
  [ 4  8 12]]

 [[13 17 21]
  [14 18 22]
  [15 19 23]
  [16 20 24]]]
</code></pre>

<p><strong>Why does <code>perm=[0,2,1]</code> returns a 2x4x3 matrix from a 2x3x4 ?</strong></p>

<p>Trying it again with <code>perm=[1,0,2]</code>:</p>

<pre><code>threetwofour = tf.transpose(twothreefour, perm=[1,0,2]) 
with tf.Session() as sess:
    init = tf.initialize_all_variables()
    sess.run(init)
    print (threetwofour.eval())
</code></pre>

<p>[out]:</p>

<pre><code>[[[ 1  2  3  4]
  [13 14 15 16]]

 [[ 5  6  7  8]
  [17 18 19 20]]

 [[ 9 10 11 12]
  [21 22 23 24]]]
</code></pre>

<p><strong>Why does <code>perm=[1,0,2]</code> return a 3x2x4 from a 2x3x4?</strong></p>

<p><strong>Does it mean that the <code>perm</code> parameter is taking my <code>np.shape</code> and transposing the tensor based on the elements based on my array shape?</strong></p>

<p>I.e. :</p>

<pre><code>_size = (2, 4, 3, 5)
randarray = np.random.randint(5, size=_size)

shape_idx = {i:_s for i, _s in enumerate(_size)}

randarray_t_func = tf.transpose(randarray, perm=[3,0,2,1]) 
with tf.Session() as sess:
    init = tf.initialize_all_variables()
    sess.run(init)
    tranposed_array = randarray_t_func.eval()
    print (tranposed_array.shape)

print (tuple(shape_idx[_s] for _s in [3,0,2,1]))
</code></pre>

<p>[out]:</p>

<pre><code>(5, 2, 3, 4)
(5, 2, 3, 4)
</code></pre>
","From the docs: But it's still a little unclear to me how should I be slicing the input tensor. E.g. from the docs too: Why is it that perm=[0,2,1] produces a 1x3x2 tensor? After some trial and error: [out]: And if I transpose it: I get a 4x3x2 to a 2x3x4 and that sounds logical. [out]: But when I use the perm parameter the output, I'm not sure what I'm really getting: [out]: Why does perm=[0,2,1] returns a 2x4x3 matrix from a 2x3x4 ? Trying it again with perm=[1,0,2]: [out]: Why does perm=[1,0,2] return a 3x2x4 from a 2x3x4? Does it mean that the perm parameter is taking my np.shape and transposing the tensor based on the elements based on my array shape? I.e. : [out]:",https://stackoverflow.com/questions/38517533,610569,Documentation Ambiguity
60593788,Does Tensorflow.unstack() Remove or Rearrange data?,"<p>I've been trying to understand how Tensorflow.Unstack() works. I've read the documentation a few times here: <a href=""https://www.tensorflow.org/api_docs/python/tf/unstack"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/unstack</a></p>

<p>According to the Tensorflow documentation ""the dimension unpacked along is gone"". It sounds like unstacking a tensor removes data from the original tensor. Is this true? Or does it only rearrange the data?</p>

<p>In my code example, in Y, it appears that it has removed the fourth row of X. What confuses me, is why does it leave the row on the side of matrix? Is the function actually removing the row or leaving it there? I'm not quite sure what to make of the output.</p>

<pre><code>import tensorflow as tf

X = tf.constant(np.array(range(24)).reshape(2, 3, 4))

Y = tf.unstack(X, axis=0)

with tf.Session() as sess:
    print(""X "", sess.run(X))
    print(""Y "", sess.run(Y))


#Ouput
X [[[ 0  1  2  3]
  [ 4  5  6  7]
  [ 8  9 10 11]]

 [[12 13 14 15]
  [16 17 18 19]
  [20 21 22 23]]]

Y [array([[ 0,  1,  2,  3],
          [ 4,  5,  6,  7],
          [ 8,  9, 10, 11]]), array([[12, 13, 14, 15],
          [16, 17, 18, 19],
          [20, 21, 22, 23]])]
</code></pre>
","I've been trying to understand how Tensorflow.Unstack() works. I've read the documentation a few times here: https://www.tensorflow.org/api_docs/python/tf/unstack According to the Tensorflow documentation ""the dimension unpacked along is gone"". It sounds like unstacking a tensor removes data from the original tensor. Is this true? Or does it only rearrange the data? In my code example, in Y, it appears that it has removed the fourth row of X. What confuses me, is why does it leave the row on the side of matrix? Is the function actually removing the row or leaving it there? I'm not quite sure what to make of the output.",https://stackoverflow.com/questions/60593788,5449789,Documentation Ambiguity
49020732,What is the difference between the trainable_weights and trainable_variables in the tensorflow basic lstm_cell?,"<p>While trying to copy the weights of a LSTM Cell in Tensorflow using the Basic LSTM Cell as documented <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell#class_basiclstmcell"" rel=""noreferrer"">here</a>, i stumbled upon both the trainable_weights and trainable_variables property. </p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/ops/rnn_cell_impl.py"" rel=""noreferrer"">Source code</a> has not really been informative for a noob like me sadly. A little bit of experimenting did yield the following information though: 
Both have the exact same layout, being a list of length two, where the first entry is a tf.Variable of shape: (2*num_units, 4*num_units), the second entry of the list is of shape (4*num_units,), where num_units is the num_units from initializing the BasicLSTMCell. 
The intuitive guess for me is now, that the first list item is a concatenation of the weights of the four internal layers of the lstm, the second item being a concatenation of the respective biases, fitting the expected sizes of these obviously. </p>

<p>Now the question is, whether there is actually any difference between these? I assume they might just be a result of inheriting these from the rnn_cell class? </p>
","While trying to copy the weights of a LSTM Cell in Tensorflow using the Basic LSTM Cell as documented here, i stumbled upon both the trainable_weights and trainable_variables property. Source code has not really been informative for a noob like me sadly. A little bit of experimenting did yield the following information though: Both have the exact same layout, being a list of length two, where the first entry is a tf.Variable of shape: (2*num_units, 4*num_units), the second entry of the list is of shape (4*num_units,), where num_units is the num_units from initializing the BasicLSTMCell. The intuitive guess for me is now, that the first list item is a concatenation of the weights of the four internal layers of the lstm, the second item being a concatenation of the respective biases, fitting the expected sizes of these obviously. Now the question is, whether there is actually any difference between these? I assume they might just be a result of inheriting these from the rnn_cell class?",https://stackoverflow.com/questions/49020732,6917400,Documentation Completeness
68353106,Why does the numpy dot product function returns an error when passed two TensorFlow Variable objects?,"<pre><code>import tensorflow as tf
import numpy as np

a = tf.Variable([[12, 13], [12, 13]])
b = tf.Variable([[12, 13], [12, 13]])

print(np.dot(a, b))
</code></pre>
<p>The above code returns the error:</p>
<pre><code>TypeError: __array__() takes 1 positional argument but 2 were given.
</code></pre>
<p>I understand that TensorFlow has a builtin method for matrix multiplication, but I was curious why the <code>np.dot</code> method does not work, specifically with tensorflow Variable objects (it seems to do fine with <code>tensorflow.constant</code> objects).</p>
<p>Other methods, such as <code>np.square</code>, <code>np.sqrt</code>, etc. all work with this, but it seems that only <code>np.dot</code> in particular does not.</p>
<p><strong>Edit</strong>: I was wondering <em>why</em> these objects in particular do not work when passed to the <code>np.dot</code> function. I realize that there are a variety of ways to find the dot product between two <code>tf.Variable</code> objects</p>
","The above code returns the error: I understand that TensorFlow has a builtin method for matrix multiplication, but I was curious why the np.dot method does not work, specifically with tensorflow Variable objects (it seems to do fine with tensorflow.constant objects). Other methods, such as np.square, np.sqrt, etc. all work with this, but it seems that only np.dot in particular does not. Edit: I was wondering why these objects in particular do not work when passed to the np.dot function. I realize that there are a variety of ways to find the dot product between two tf.Variable objects",https://stackoverflow.com/questions/68353106,15180247,Documentation Ambiguity
53611227,Changing the shape of a new assigned variable in Tensorflow,"<p>if you change a tf.Variable using <a href=""https://www.tensorflow.org/api_docs/python/tf/assign"" rel=""nofollow noreferrer"">tf.assign</a> with <code>validate_shape=False</code> <a href=""https://github.com/tensorflow/tensorflow/issues/10091"" rel=""nofollow noreferrer"">the shape is not updated</a>.</p>

<p>But if I use <a href=""https://www.tensorflow.org/api_docs/python/tf/Variable#set_shape"" rel=""nofollow noreferrer"">set_shape</a> to set the new (correct) shape I get a ValueError.</p>

<p>Here a quick example:</p>

<pre><code>import tensorflow as tf 

a = tf.Variable([3,3,3])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # [3 3 3]
    print(sess.run(a))

    sess.run(tf.assign(a, [4,4,4,4], validate_shape=False))
    # [4 4 4 4]
    print(sess.run(a))

# (3,)
print(a.get_shape())

# ValueError: Dimension 0 in both shapes must be equal, but are 3 and 4. Shapes are [3] and [4].
a.set_shape([4])
</code></pre>

<p>How do I change the shape of the Variable?</p>

<p>Note: I am aware that the code works if I use <code>a = tf.Variable([3,3,3], validate_shape=False)</code> but in my context I will not be able to initialize the variable myself.</p>
","if you change a tf.Variable using tf.assign with validate_shape=False the shape is not updated. But if I use set_shape to set the new (correct) shape I get a ValueError. Here a quick example: How do I change the shape of the Variable? Note: I am aware that the code works if I use a = tf.Variable([3,3,3], validate_shape=False) but in my context I will not be able to initialize the variable myself.",https://stackoverflow.com/questions/53611227,10743551,Requesting (Additional) Resources
59626966,Using tensors are dictionary keys in tensorflow,"<p>i have seen the answer <a href=""https://stackoverflow.com/questions/52879126/how-are-tensors-immutable-in-tensorflow"">here</a>. It is not what i am looking for.</p>

<p>I am running this on tensorflow2.0</p>

<p>I read the following sentence in the TensorFlow documentation:</p>

<blockquote>
  <p>With the exception of tf.Variable, the value of a tensor is immutable,
  which means that in the context of a single execution tensors only
  have a single value. However, evaluating the same tensor twice can
  return different values; for example that tensor can be the result of
  reading data from disk, or generating a random number.</p>
</blockquote>

<p>I tried using tensors as a dictionary key and i get the following error:</p>

<pre><code>Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key
</code></pre>

<ol>
<li>What does this error mean?</li>
<li>Are tf.Variables hashable as well? They also define a computation rather than being the computation, so why the distinction 'With the exception of tf.Variable, the value of a tensor is immutable' is there</li>
</ol>
",i have seen the answer here. It is not what i am looking for. I am running this on tensorflow2.0 I read the following sentence in the TensorFlow documentation: I tried using tensors as a dictionary key and i get the following error:,https://stackoverflow.com/questions/59626966,6546694,Documentation Ambiguity
54703473,TensorFlow why we still use tf.name_scope when we already have the function tf.variable_scope,"<p>I do not understand why we also need the function <code>tf.name_scope</code> when we already have <code>tf.variable_scope</code>. From the Tensorflow official API, I see that the <code>tf.variable_scope</code> is more powerful because it can have an effect on <code>tf.get_variable</code>. When we create layers and want to share variables, we always use <code>tf.variable_scope</code> and <code>tf.name_scope</code>. However, I try to learn something new from code released by Nvidia on GitHub. I found that it is frequent for coders to use <code>tf.name_scope</code>. Why do we still need this function? </p>
","I do not understand why we also need the function tf.name_scope when we already have tf.variable_scope. From the Tensorflow official API, I see that the tf.variable_scope is more powerful because it can have an effect on tf.get_variable. When we create layers and want to share variables, we always use tf.variable_scope and tf.name_scope. However, I try to learn something new from code released by Nvidia on GitHub. I found that it is frequent for coders to use tf.name_scope. Why do we still need this function?",https://stackoverflow.com/questions/54703473,9881203,Documentation Ambiguity
44268206,How to get the list of uninitialized variables from tf.report_uninitialized_variables,"<p>The documentation says it's a 1 d tensor, however, I have failed to figure out how to access the list.</p>

<p>I would prefer the actual variables rather than names as I would like to initialize them via <code>tf.variables_initializer()</code></p>
","The documentation says it's a 1 d tensor, however, I have failed to figure out how to access the list. I would prefer the actual variables rather than names as I would like to initialize them via tf.variables_initializer()",https://stackoverflow.com/questions/44268206,4261647,Documentation Ambiguity
49303136,tensorflow tfrecords for batch with variable length data in each example?,"<p>I am trying to use tfrecords to read batches that have a field that is a  variable length list in each example. The data might be</p>

<pre><code>example 1: [1,    2,  3] 
example 2: [10,  11]
example 3: [100,200,300,400]
</code></pre>

<p>I have been using </p>

<pre><code>tf.train.Feature(int64_list=tf.Int64List(value=x))
</code></pre>

<p>to store each of the above, that is I'll make 3 different tfrecords, where x is <code>[1,2,3]</code> then <code>[10,11]</code> then <code>[100,200,300,400]</code></p>

<p>Those three records got their <code>SerializeToString()</code> method called, and each appended to a file via the <code>TFRecordWriter</code></p>

<p>Reading back was tricky, I couldn't use <code>tf.FixedLenFeature</code>, so then I found <code>tf.VarLenFeature</code>. It seemed to be very nice, when I read the data batch in a batch size of 3, it looked like I was getting a  <code>tf.SparseVectorValue</code> back where column 0 of the indices was the example number in the batch, and column 1 was the value within the list, that is, it looked like I was getting (suppose the batch size is 3):</p>

<pre><code>indices=[[0,0],
         [0,1],
         [0,2],
         [1,0],
         [1,1],
         [2,0],
         [2,1],
         [2,2],
         [2,3]]
 values = [1,2,3,10,11,100,200,300,400]
</code></pre>

<p>but now that I'm working with more data, I don't think this is what I'm getting. </p>

<p>My question is, what does VarLenFeature return when you batch up variable length lists like this? Should it do what I explained? Then maybe I have a bug to find. </p>

<p>But if it does something different, then what should I do to read back a batch of data with variable length lists? I need to know the example number in the batch for each list, I could add another field to the tfrecord with the length of each of these lists.</p>

<p>-- EDIT --</p>

<p>I've done more testing, and I think it works like I think. I must have a problem in my bigger program. It would be nice if there was documentation saying exactly what <code>tf.VarLenFeature</code> is supposed to return for batched datasets, so I could be sure my above interpretation is correct.</p>

<p>Below is some test code I'm trying:</p>

<pre><code>import numpy as np
import tensorflow as tf
import random


def make_original_data(N):
    data = []
    for uid in range(N):
        varlen = random.randint(2, 20)
        varx = [random.randint(0,100) for kk in range(varlen)] 
        rec = {'uid': uid,
               'A':random.randint(0,100),
               'B':random.randint(0,100),
               'V':varx,
               'nV':varlen
            }
        data.append(rec)
    return data


def rec2tfrec_example(rec):
    def _int64_feat(value):
        arr_value = np.empty([1], dtype=np.int64)
        arr_value[0] = value
        return tf.train.Feature(int64_list=tf.train.Int64List(value=arr_value))

    def _int64list_feat(values):
        arr_values = np.empty([len(values)], dtype=np.int64)
        arr_values[:] = values[:]
        return tf.train.Feature(int64_list=tf.train.Int64List(value=arr_values))

    feat = {
        'uid': _int64_feat(rec['uid']),
        'A':   _int64_feat(rec['A']),
        'B':   _int64_feat(rec['B']),
        'nV':  _int64_feat(rec['nV']),
        'V':   _int64list_feat(rec['V'])
        }

    return tf.train.Example(features=tf.train.Features(feature=feat))


def parse_example(tfrec_serialized_string):
    feat = {
        'uid': tf.FixedLenFeature([], tf.int64),
        'A': tf.FixedLenFeature([], tf.int64),
        'B': tf.FixedLenFeature([], tf.int64),
        'nV': tf.FixedLenFeature([], tf.int64),
        'V': tf.VarLenFeature(tf.int64)
    }
    return tf.parse_example(tfrec_serialized_string, feat)


def to_serialized_tfrecs(data):
    serialized = []
    for rec in data:
        example = rec2tfrec_example(rec)
        serialized.append(example.SerializeToString())
    return serialized


def write_tfrecs_to_file(fname, recs):
        recwriter = tf.python_io.TFRecordWriter(fname)
        for rec in recs:
            recwriter.write(bytes(rec))
        recwriter.close()


def check_batch(data_batch, tfres):
    for ky in ['A', 'uid', 'B', 'nV']:
        orig_data = np.array([rec[ky] for rec in data_batch], dtype=np.int64)
        assert np.all(orig_data == tfres[ky]), ""batch_idx=%d ky=%s orig=%s tf=%s"" % \
            (batch_idx, ky, orig_data, tfres[ky])
    spTensorValue = tfres['V']
    tf_example_in_batch = spTensorValue.indices[:,0]
    tf_V = spTensorValue.values
    tf_sum_nV = np.sum(tfres['nV'])
    assert tf_sum_nV == len(tf_V), ""tf_sum_nV=%d != len(tf_V)=%d"" % (tf_sum_nV, len(tf_V))
    ex_example_in_batch = np.empty((tf_sum_nV,), np.int64)
    ex_V = np.empty((tf_sum_nV,), np.int64)
    idx = 0
    for example_in_batch, tf_num_this_example in enumerate(tfres['nV']):
        num_this_example = data_batch[example_in_batch]['nV']
        assert num_this_example == tf_num_this_example
        ex_example_in_batch[idx:idx+num_this_example] = example_in_batch
        ex_V[idx:idx+num_this_example] = np.array(data_batch[example_in_batch]['V'])
        idx += num_this_example
    assert np.all(ex_example_in_batch == tf_example_in_batch), ""example in batch wrong, expected=%s != tf=%s"" % (ex_example_in_batch, tf_example_in_batch)
    assert np.all(ex_V == tf_V), ""example in batch wrong, expected=%s != tf=%s"" % (ex_V, tf_V)


def check_tfrecs(sess, tfrec_output_filename, data, N, batch_size):
    dataset = tf.data.TFRecordDataset(tfrec_output_filename) \
                     .batch(batch_size) \
                     .map(parse_example, num_parallel_calls=2)
    tf_iter = dataset.make_initializable_iterator()
    get_next = tf_iter.get_next()

    sess.run(tf_iter.initializer)
    num_batches = N//batch_size
    nextIdx = 0
    for batch_idx in range(num_batches):
        data_batch = [data[idx] for idx in range(nextIdx, nextIdx + batch_size)]
        nextIdx += batch_size
        tfres = sess.run(get_next)
        check_batch(data_batch, tfres)


def main(N=1000, batch_size=5, tfrec_output_filename='tfrec_testing.tfrecords'):
    tf.reset_default_graph()
    data = make_original_data(N)
    tfrec_strings = to_serialized_tfrecs(data)
    write_tfrecs_to_file(tfrec_output_filename, tfrec_strings)
    with tf.Session() as sess:
        check_tfrecs(sess, tfrec_output_filename, data, N, batch_size)

if __name__ == '__main__':
    main()
</code></pre>
","I am trying to use tfrecords to read batches that have a field that is a variable length list in each example. The data might be I have been using to store each of the above, that is I'll make 3 different tfrecords, where x is [1,2,3] then [10,11] then [100,200,300,400] Those three records got their SerializeToString() method called, and each appended to a file via the TFRecordWriter Reading back was tricky, I couldn't use tf.FixedLenFeature, so then I found tf.VarLenFeature. It seemed to be very nice, when I read the data batch in a batch size of 3, it looked like I was getting a tf.SparseVectorValue back where column 0 of the indices was the example number in the batch, and column 1 was the value within the list, that is, it looked like I was getting (suppose the batch size is 3): but now that I'm working with more data, I don't think this is what I'm getting. My question is, what does VarLenFeature return when you batch up variable length lists like this? Should it do what I explained? Then maybe I have a bug to find. But if it does something different, then what should I do to read back a batch of data with variable length lists? I need to know the example number in the batch for each list, I could add another field to the tfrecord with the length of each of these lists. -- EDIT -- I've done more testing, and I think it works like I think. I must have a problem in my bigger program. It would be nice if there was documentation saying exactly what tf.VarLenFeature is supposed to return for batched datasets, so I could be sure my above interpretation is correct. Below is some test code I'm trying:",https://stackoverflow.com/questions/49303136,2280020,Lack of Alternative Solutions/Documentation
44569219,Tensorflow: can one prevent one branch of tf.where from executing?,"<p>I'm working on an encoder-decoder setup. I want to be able to run the encoder once and then perform multiple decoder runs. The solution I've come up with is to feed the decoder with a TF conditional node (using tf.where) which contains either the final hidden state of the encoder (in which case TF will run the encoder when I ask for the decoder output), or a placeholder with the stored results of the encoder (in which case in theory TF does not need to run the encoder).</p>

<p>Here is the relevant part of the code:</p>

<pre><code>encoder_state = tf.where(gen_math_ops.greater_equal(branching_points, 0), encoder_state,
                         rnn.static_rnn(encoder_cell, encoder_inputs, dtype=dtype)[1])
</code></pre>

<p>As I don't get a speedup from this method, I'm pretty sure it doesn't work and both branches of the tf.where are run by TF everytime, even when it only needs to read from the placeholder.</p>

<p>Is there any way to use tf.where such that it does not run the encoder? I've looked at the description of the method and I'm not sure whether both branches are always computed or not, I've seen contradictory information on this issue.</p>

<p>Thanks!</p>
","I'm working on an encoder-decoder setup. I want to be able to run the encoder once and then perform multiple decoder runs. The solution I've come up with is to feed the decoder with a TF conditional node (using tf.where) which contains either the final hidden state of the encoder (in which case TF will run the encoder when I ask for the decoder output), or a placeholder with the stored results of the encoder (in which case in theory TF does not need to run the encoder). Here is the relevant part of the code: As I don't get a speedup from this method, I'm pretty sure it doesn't work and both branches of the tf.where are run by TF everytime, even when it only needs to read from the placeholder. Is there any way to use tf.where such that it does not run the encoder? I've looked at the description of the method and I'm not sure whether both branches are always computed or not, I've seen contradictory information on this issue. Thanks!",https://stackoverflow.com/questions/44569219,8166504,Requesting (Additional) Resources
49286590,tf.while_loop only makes only one loop,"<p>After days of trying to apply a <code>tf.while_loop</code>, I still fail to understand how it works (or rather why it does not). The documentations and various questions here on StackOverflow haven't helped so far.</p>

<p>The main idea is to train the different columns of a tensor <code>trueY</code> separately using a <code>while_loop</code>. The problem is that when I trace this code, I see that the <code>while_loop</code> gets called only once. </p>

<p>I'd like to dynamically assign names to variables created in the <code>while_loop</code> so as to be able to access them outside the <code>while_loop</code> after they have been created (thus the ""gen_name"" function trying to dynamically generate names for the dense layers created in each loop), and make <code>tf.while_loop</code> run n times this way.</p>

<p>Here is a sample of my code with the issue (not the full code and modified to demonstrate this problem)</p>

<pre><code>...................
config['dim_y'] = 10
Xl = tf.placeholder( self.dtype, shape=(batchsize, config['dim_x']) )
Yl = tf.placeholder( self.dtype, shape=(batchsize, config['dim_y']) )
Gl = tf.placeholder( self.dtype, shape=(batchsize, config['dim_g']) )
costl, cost_m, self.cost_b = self.__cost( Xl, Yl, Gl, False )

def __eval_cost( self, A, X, Y, G, reuse ):
    AGXY = tf.concat( [A, G, X, Y], -1 )
        Z, mu_phi3, ls_phi3 = build_nn( AGXY, ....,  reuse )

    _cost = -tf.reduce_sum( ls_phi3, -1 )
    _cost += .5 * tf.reduce_sum( tf.pow( mu_phi3, 2 ), -1 )
    _cost += .5 * tf.reduce_sum( tf.exp( 2*ls_phi3 ), -1 )
    return _cost



def __cost( self, trueX, trueY, trueG, reuse ):
    ........
    columns = tf.unstack(trueY, axis=-1)
    AGX = tf.concat( [ AX, G ], -1 )
    pre_Y  = self.build_nn( AGX, ....., reuse )
    index_loop = (tf.constant(0), _cost, _cost_bl)

    def condition(index, _cost, _cost_supervised_bi_label):
        return tf.less(index, self.config['dim_y'])


    def bodylabeled(index, _cost, _cost_bl):
        def gen_name(var_name):
            # split eg 'cost/while/strided_slice_5:0' =&gt; '5'
            # split eg 'cost/while/strided_slice:0' =&gt; 'slice'
            iter = var_name.split('/')[-1].split(':')[0].split('_')[-1]
            if iter == ""slice"":
                return '0phi2y'
            else:
                return '{}phi2y'.format(int(iter) % self.config['dim_y'])

        y_i = tf.gather(columns, index)
        y = tf.expand_dims( tf.one_hot(tf.to_int32(y_i, name='ToInt32'), depth, dtype=self.dtype ), 0 )
        Y = tf.tile( y, [self.config['L'],1,1] )
        c = tf.constant(0, name='test')
        log_pred_Y = tf.layers.dense( pre_Y, 2, name=gen_name(iter[index].name), reuse=reuse )
        log_pred_Y = log_pred_Y - tf.reduce_logsumexp( log_pred_Y, -1, keep_dims=True )

        _cost += self.__eval_cost_given_axgy( A, X, Y, G, reuse=tf.AUTO_REUSE )
        _cost_bl += -tf.reduce_sum( tf.multiply( Y, log_pred_Y ), -1 )
        return tf.add(index, 1), _cost, _cost_supervised_bi_label

    _cost, _bl = tf.while_loop(condition, bodylabeled, index_loop, parallel_iterations=1, shape_invariants=(index_loop[0].get_shape(), tf.TensorShape([None, 100]), tf.TensorShape([None, 100])))[1:]

op = costl + cost_m + cost_b
with tf.Session(config=config) as sess:
    sess.run( tf.global_variables_initializer() )
    sess.run(tf.local_variables_initializer())
    for batchl in batches:
       sess.run( op,
                feed_dict={Xl:Xl[batchl,:],
                         Yl:Yl[batchl,:].toarray(),
                         Gl:Gl[batchl,:].toarray(),
                         is_training:True } )

for n in tf.get_default_graph().as_graph_def().node:
    print(n.name) 
</code></pre>
","After days of trying to apply a tf.while_loop, I still fail to understand how it works (or rather why it does not). The documentations and various questions here on StackOverflow haven't helped so far. The main idea is to train the different columns of a tensor trueY separately using a while_loop. The problem is that when I trace this code, I see that the while_loop gets called only once. I'd like to dynamically assign names to variables created in the while_loop so as to be able to access them outside the while_loop after they have been created (thus the ""gen_name"" function trying to dynamically generate names for the dense layers created in each loop), and make tf.while_loop run n times this way. Here is a sample of my code with the issue (not the full code and modified to demonstrate this problem)",https://stackoverflow.com/questions/49286590,1971741,Documentation Ambiguity
42909692,"Tensorflow error using while_loop: ""List of Tensors when single Tensor expected""","<p>I'm getting a TypeError(""List of Tensors when single Tensor expected"") when I run a Tensorflow while_loop. The error is from the third parameter, which should be a list of Tensors, according to the documentation. x, W, Win, Y, temp, and Wout are all previously declared as floats and arrays of floats. cond2 and test2 are functions I've written to be the condition and body. I use an almost identical call earlier in the program with no issues.</p>

<pre><code>t=0
t,x,W,Win,Y,temp,Wout = sess.run(tf.while_loop(cond2, test2,
                                 [t, tf.Variable(x), tf.constant(W),
                                  tf.constant(Win), tf.Variable(Y),
                                  tf.Variable(temp), tf.constant(Wout)],
                                 shape_invariants=[tf.TensorShape(None),
                                                   tf.TensorShape(None),
                                                   tf.TensorShape(None),
                                                   tf.TensorShape(None),
                                                   tf.TensorShape(None),
                                                   tf.TensorShape(None),
                                                   tf.TensorShape(None)]))
</code></pre>
","I'm getting a TypeError(""List of Tensors when single Tensor expected"") when I run a Tensorflow while_loop. The error is from the third parameter, which should be a list of Tensors, according to the documentation. x, W, Win, Y, temp, and Wout are all previously declared as floats and arrays of floats. cond2 and test2 are functions I've written to be the condition and body. I use an almost identical call earlier in the program with no issues.",https://stackoverflow.com/questions/42909692,7643546,Requesting (Additional) Resources
43457543,Building dynamic_rnn from scratch in tensorflow,"<p>I am coding rnn similar to dynamic_rnn provided by tensorflow. I tried to see the code on GitHub but cannot understand how they have implemented it. I want to build it from scratch so that I can customize rnn from within. How to do that?</p>

<p>Currently, my approach is thinking of a truncated time series as a tensor use tf.scan() and find the new hidden state for all time series. Then use tf.map_fn to find the output for the new stacked hidden variables. Finally, use tf.while_loop() to find the error for each tensor on the first dimension of stacked output and do back propagation with that loss.</p>

<p>My concern will the graph be dynamic after doing this. I mean let say first I unrolled for 5 times and then 4 times will the graph erase that one node rolled before?</p>

<p>Will this work?</p>

<p>Please guide.</p>

<p>Thank you,</p>
","I am coding rnn similar to dynamic_rnn provided by tensorflow. I tried to see the code on GitHub but cannot understand how they have implemented it. I want to build it from scratch so that I can customize rnn from within. How to do that? Currently, my approach is thinking of a truncated time series as a tensor use tf.scan() and find the new hidden state for all time series. Then use tf.map_fn to find the output for the new stacked hidden variables. Finally, use tf.while_loop() to find the error for each tensor on the first dimension of stacked output and do back propagation with that loss. My concern will the graph be dynamic after doing this. I mean let say first I unrolled for 5 times and then 4 times will the graph erase that one node rolled before? Will this work? Please guide. Thank you,",https://stackoverflow.com/questions/43457543,7492728,Requesting (Additional) Resources
47951994,Unable to print tf.zeros in tensorflow,"<pre><code>weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))
biases = tf.Variable(tf.zeros([num_labels]))`
</code></pre>

<p>This is a part of the code I encountered to minimize loss using Gradient Descent in tensorflow.
I understood what is going on and what <strong>tf.zeros</strong> is doing but when I tried to run the follwing code it showed an error::</p>

<pre><code> sess = tf.IntearctiveSession()
 tensor = tf.Variable(tf.zeros(shape=(10)))
 print(tensor.eval())
 sess.close()
</code></pre>

<p>An error occured in <code>print(tensor.eval())</code>.
Can someone point out where I understood the things wrong?</p>
",This is a part of the code I encountered to minimize loss using Gradient Descent in tensorflow. I understood what is going on and what tf.zeros is doing but when I tried to run the follwing code it showed an error:: An error occured in print(tensor.eval()). Can someone point out where I understood the things wrong?,https://stackoverflow.com/questions/47951994,8259521,Documentation Replicability
34589335,How does the distorted_inputs() function in the TensorFlow CIFAR-10 example tutorial get 128 images per batch?,"

<p>I was going through the CIFAR-10 example at <a href=""https://www.tensorflow.org/versions/master/tutorials/deep_cnn/index.html"" rel=""nofollow noreferrer"">TensorFlow getting started guide for CNN</a></p>

<p>Now in the train function in <strong>cifar10_train.py</strong> we get images as </p>

<pre class=""lang-py prettyprint-override""><code>images,labels = cifar10.distorted_inputs()
</code></pre>

<p>In the <code>distorted_inputs()</code> function we generate the filenames in a queue and then read a single record as </p>

<pre class=""lang-py prettyprint-override""><code> # Create a queue that produces the filenames to read.
 filename_queue = tf.train.string_input_producer(filenames)

 # Read examples from files in the filename queue.
 read_input = cifar10_input.read_cifar10(filename_queue)
 reshaped_image = tf.cast(read_input.uint8image, tf.float32)
</code></pre>

<p>When I add debugging code, the <code>read_input</code> variable contains only 1 record with an image and its height, width, and label name.</p>

<p>The example then applies some distortion to the read image/record and then passes it to the <code>_generate_image_and_label_batch()</code> function.</p>

<p>This function then returns a 4D Tensor of shape <code>[batch_size, 32, 32, 3]</code>  where <code>batch_size = 128</code>. </p>

<p>The above function utilizes the <code>tf.train.shuffle_batch()</code> function when returns the batch. </p>

<p>My question is where do the extra records come from in the <code>tf.train.shuffle_batch()</code> function? We are not passing it any filename or  reader object. </p>

<p>Can someone shed some light on how we go from 1 record to 128 records? I looked into the documentation but didn't understand.</p>
","I was going through the CIFAR-10 example at TensorFlow getting started guide for CNN Now in the train function in cifar10_train.py we get images as In the distorted_inputs() function we generate the filenames in a queue and then read a single record as When I add debugging code, the read_input variable contains only 1 record with an image and its height, width, and label name. The example then applies some distortion to the read image/record and then passes it to the _generate_image_and_label_batch() function. This function then returns a 4D Tensor of shape [batch_size, 32, 32, 3] where batch_size = 128. The above function utilizes the tf.train.shuffle_batch() function when returns the batch. My question is where do the extra records come from in the tf.train.shuffle_batch() function? We are not passing it any filename or reader object. Can someone shed some light on how we go from 1 record to 128 records? I looked into the documentation but didn't understand.",https://stackoverflow.com/questions/34589335,628096,Documentation Replication on Other Examples
34619177,What does tf.nn.conv2d do in tensorflow?,"<p>I was looking at the docs of tensorflow about <code>tf.nn.conv2d</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""noreferrer"">here</a>. But I can't understand what it does or what it is trying to achieve. It says on the docs,</p>
<blockquote>
<p>#1 : Flattens the filter to a 2-D matrix with shape</p>
<p><code>[filter_height * filter_width * in_channels, output_channels]</code>.</p>
</blockquote>
<p>Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below :</p>
<blockquote>
<p># 2: Extracts image patches from the the input tensor to form a virtual tensor of shape</p>
<p><code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>.</p>
<p># 3: For each patch, right-multiplies the filter matrix and the image patch vector.</p>
</blockquote>
<p>It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this.</p>
<p>I've tried coding a small portion and printing out the shape of the operation. Still, I can't understand.</p>
<p>I tried something like this:</p>
<pre class=""lang-python prettyprint-override""><code>op = tf.shape(tf.nn.conv2d(tf.random_normal([1,10,10,10]), 
              tf.random_normal([2,10,10,10]), 
              strides=[1, 2, 2, 1], padding='SAME'))

with tf.Session() as sess:
    result = sess.run(op)
    print(result)
</code></pre>
<p>I understand bits and pieces of convolutional neural networks. I studied them <a href=""http://cs231n.github.io/convolutional-networks/"" rel=""noreferrer"">here</a>. But the implementation on tensorflow is not what I expected. So it raised the question.</p>
<p><strong>EDIT</strong>:
So, I implemented a much simpler code. But I can't figure out what's going on. I mean how the results are like this. It would be extremely helpful if anyone could tell me what process yields this output.</p>
<pre class=""lang-python prettyprint-override""><code>input = tf.Variable(tf.random_normal([1,2,2,1]))
filter = tf.Variable(tf.random_normal([1,1,1,1]))

op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')
init = tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)

    print(&quot;input&quot;)
    print(input.eval())
    print(&quot;filter&quot;)
    print(filter.eval())
    print(&quot;result&quot;)
    result = sess.run(op)
    print(result)
</code></pre>
<p>output</p>
<pre><code>input
[[[[ 1.60314465]
   [-0.55022103]]

  [[ 0.00595062]
   [-0.69889867]]]]
filter
[[[[-0.59594476]]]]
result
[[[[-0.95538563]
   [ 0.32790133]]

  [[-0.00354624]
   [ 0.41650501]]]]
</code></pre>
","I was looking at the docs of tensorflow about tf.nn.conv2d here. But I can't understand what it does or what it is trying to achieve. It says on the docs, Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below : It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this. I've tried coding a small portion and printing out the shape of the operation. Still, I can't understand. I tried something like this: I understand bits and pieces of convolutional neural networks. I studied them here. But the implementation on tensorflow is not what I expected. So it raised the question. EDIT: So, I implemented a much simpler code. But I can't figure out what's going on. I mean how the results are like this. It would be extremely helpful if anyone could tell me what process yields this output. output",https://stackoverflow.com/questions/34619177,4341948,Documentation Replication on Other Examples
34642595,Tensorflow Strides Argument,"<p>I am trying to understand the <strong>strides</strong> argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. </p>

<p>The <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#max_pool"" rel=""noreferrer"">documentation</a> repeatedly says </p>

<blockquote>
  <p>strides: A list of ints that has length >= 4. The stride of the sliding window for each dimension of the input tensor.</p>
</blockquote>

<p>My questions are:</p>

<ol>
<li>What do each of the 4+ integers represent?</li>
<li>Why must they have strides[0] = strides[3] = 1 for convnets?</li>
<li>In <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3%20-%20Neural%20Networks/convolutional_network.ipynb"" rel=""noreferrer"">this example</a> we see <code>tf.reshape(_X,shape=[-1, 28, 28, 1])</code>. Why -1?</li>
</ol>

<p>Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.</p>
","I am trying to understand the strides argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. The documentation repeatedly says My questions are: Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.",https://stackoverflow.com/questions/34642595,3908247,Documentation Ambiguity
34931121,Can cond support TF ops with side effects?,"<p>The (source code) documentation for <code>tf.cond</code> is unclear on whether the functions to be performed when the predicate is evaluated can have side effects or not. I've done some tests but I'm getting conflicting results. For example the code below does not work:</p>

<pre><code>import tensorflow as tf
from tensorflow.python.ops import control_flow_ops

pred = tf.placeholder(tf.bool, [])
count = tf.Variable(0)
adder = count.assign_add(1)
subtractor = count.assign_sub(2)

my_op = control_flow_ops.cond(pred, lambda: adder, lambda: subtractor)

sess = tf.InteractiveSession()
tf.initialize_all_variables().run()

my_op.eval(feed_dict={pred: True})
count.eval() # returns -1

my_op.eval(feed_dict={pred: False})
count.eval() # returns -2
</code></pre>

<p>I.e. no matter what value the predicate evaluates to, both functions are getting run, and so the net result is a subtraction of 1. On the other hand, this code snippet does work, where the only difference is that I add new ops to the graph every time <code>my_op</code> is called:</p>

<pre><code>pred = tf.placeholder(tf.bool, [])
count = tf.Variable(0)

my_op = control_flow_ops.cond(pred, lambda:count.assign_add(1), lambda:count.assign_sub(2))

sess = tf.InteractiveSession()
tf.initialize_all_variables().run()

my_op.eval(feed_dict={pred: False})
count.eval() # returns -2

my_op.eval(feed_dict={pred: True})
count.eval() # returns -1
</code></pre>

<p>Not sure why creating new ops every time works while the other case doesn't, but I'd obviously rather not be adding nodes as the graph will eventually become too big.</p>
","The (source code) documentation for tf.cond is unclear on whether the functions to be performed when the predicate is evaluated can have side effects or not. I've done some tests but I'm getting conflicting results. For example the code below does not work: I.e. no matter what value the predicate evaluates to, both functions are getting run, and so the net result is a subtraction of 1. On the other hand, this code snippet does work, where the only difference is that I add new ops to the graph every time my_op is called: Not sure why creating new ops every time works while the other case doesn't, but I'd obviously rather not be adding nodes as the graph will eventually become too big.",https://stackoverflow.com/questions/34931121,2615676,Documentation Replicability
35689547,How to process single training file in parallel,"<p>I have a file <code>train.csv</code> that contains paths to images and their labels. ie:</p>

<pre><code>img1.jpg 3
img2.jpg 1
...
</code></pre>

<p>After going through the <a href=""https://www.tensorflow.org/versions/r0.7/how_tos/reading_data/index.html"" rel=""nofollow"">reading data tutorial</a> I came up with some code to go through each image, resize it and apply distortions:</p>

<pre><code>def apply_distortions(resized_image):
    # do a bunch of tf.image distortion...
    return float_image

def processing(filename):
    file_contents = tf.read_file(filename)
    image = tf.image.decode_jpeg(file_contents, channels=3)
    resized_image = tf.image.resize_images(image, 299, 299)
    distorted_image = apply_distortions(resized_image)
    return distorted_image

def parse_csv(filename_queue):
    line_reader = tf.TextLineReader()
    key, line = line_reader.read(filename_queue)
    filename, label = tf.decode_csv(line,     # line_batch or line (depending if you want to batch)
                               record_defaults=[tf.constant([],dtype=tf.string),
                                                tf.constant([],dtype=tf.int32)],
                               field_delim=' ')
    processed_image = processing(filename)
    return processed_image, label
</code></pre>

<p>The problem now is that I'm confused how to do these operations across the file in parallel. The documentation suggests either using <code>tf.train.batch_join</code> or <code>tf.train.batch</code> with num_threads=N.</p>

<p>I first tried following the example code using <code>tf.train.batch_join</code> but this seems to be intended for processing multiple files in parallel. In my case however I just have 1 file. </p>

<pre><code>filename_queue = tf.train.string_input_producer([""train.txt""], num_epochs=1, shuffle=True)    
example_list = [parse_csv(filename_queue) for _ in range(8)]
example_batch, label_batch = tf.train.batch_join(example_list, batch_size)
</code></pre>

<p>I also tried setting <code>tf.train.batch([example, label], batch_size, num_threads=8)</code> but its not clear to me if this is doing the right thing (although I can see more cpu cores in use)</p>

<pre><code>filename_queue = tf.train.string_input_producer([""train.txt""], num_epochs=1, shuffle=True)
example, label = parse_csv(filename_queue)
example_batch, label_batch = tf.train.batch([example, label], batch_size, num_threads=8)
</code></pre>

<p>Here is my code for executing the graph:</p>

<pre><code>sess.run(tf.initialize_all_variables())
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess,coord)
try:
    while not coord.should_stop():
        X, Y = sess.run([example_batch, label_batch])
        # Now run a training step
except tf.errors.OutOfRangeError:
    print('Done training -- epoch limit reached')
finally:
    # When done, ask the threads to stop.
    coord.request_stop()
coord.join(threads)
sess.close()
</code></pre>

<p>Whats the best way to process this file in parallel?</p>
","I have a file train.csv that contains paths to images and their labels. ie: After going through the reading data tutorial I came up with some code to go through each image, resize it and apply distortions: The problem now is that I'm confused how to do these operations across the file in parallel. The documentation suggests either using tf.train.batch_join or tf.train.batch with num_threads=N. I first tried following the example code using tf.train.batch_join but this seems to be intended for processing multiple files in parallel. In my case however I just have 1 file. I also tried setting tf.train.batch([example, label], batch_size, num_threads=8) but its not clear to me if this is doing the right thing (although I can see more cpu cores in use) Here is my code for executing the graph: Whats the best way to process this file in parallel?",https://stackoverflow.com/questions/35689547,2138200,Documentation Replication on Other Examples
36223157,Set weight and bias tensors of tensorflow conv2d operation,"<p>I have been given a trained neural network in torch and I need to rebuild it exactly in tensorflow. I believe I have correctly defined the network's architecture in tensorflow but I am having trouble transferring the weight and bias tensors. Using a third party package, I converted all the weight and bias tensors from the torch network to numpy arrays then wrote them to disk. I can load them back into my python program but I cannot figure out a way to assign them to the corresponding layers in my tensorflow network. </p>

<p>For instance, I have a convolution layer defined in tensorflow as</p>

<pre><code>kernel_1 = tf.Variable(tf.truncated_normal([11,11,3,64], stddev=0.1))
conv_kernel_1 = tf.nn.conv2d(input, kernel_1, [1,4,4,1], padding='SAME')
biases_1 = tf.Variable(tf.zeros[64])
bias_layer_1 = tf.nn_add(conv_kernel_1, biases_1)
</code></pre>

<p>According to the tensorflow documentation, the tf.nn.conv2d operation uses the shape defined in the kernel_1 variable to construct the weight tensor. However, I cannot figure out how to access that weight tensor to set it to the weight array I have loaded from file. </p>

<p><strong>Is it possible to explicitly set the weight tensor? And if so, how?</strong> </p>

<p>(The same question applies to bias tensor.)</p>
","I have been given a trained neural network in torch and I need to rebuild it exactly in tensorflow. I believe I have correctly defined the network's architecture in tensorflow but I am having trouble transferring the weight and bias tensors. Using a third party package, I converted all the weight and bias tensors from the torch network to numpy arrays then wrote them to disk. I can load them back into my python program but I cannot figure out a way to assign them to the corresponding layers in my tensorflow network. For instance, I have a convolution layer defined in tensorflow as According to the tensorflow documentation, the tf.nn.conv2d operation uses the shape defined in the kernel_1 variable to construct the weight tensor. However, I cannot figure out how to access that weight tensor to set it to the weight array I have loaded from file. Is it possible to explicitly set the weight tensor? And if so, how? (The same question applies to bias tensor.)",https://stackoverflow.com/questions/36223157,6114511,Lack of Alternative Solutions/Documentation
36570729,tf.IndexedSlicesValue when returned from tf.gradients(),"<p>I'm having the following problem, I have four embedding matrices and want to get the gradients of my loss function with respect to those matrices.</p>

<p>When I run the session to return the values for the gradients, two of those returned objects are of type tensorflow.python.framework.ops.IndexedSlicesValue, the other two are numpy arrays. Now for the numpy arrays, their shape corresponds to the shape of their corresponding embedding matrix, but I'm having problems with the IndexedSlicesValue objects. </p>

<p>If I call .values on one of those objects, I get an array whose shape does not match that of the gradient, the shape of the embedding matrix is [22,30], but calling .values on the IndexedSlicesValue object I get an array with shape [4200,30] ( The shape of my input tensor had dimensions of [30,20,7], the product of those dimensions equals 4200, not sure if this is relevant).
The IndexedSlicesValue object has an attribute called dense_shape, which is an array that holds the dimensions the gradient should have, i.e. array([22,30]) is value returned by .dense_shape.</p>

<p>I don't really understand the docs here: <a href=""https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#IndexedSlices"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#IndexedSlices</a></p>

<p>It says:</p>

<blockquote>
  <p>An IndexedSlices is typically used to represent a subset of a
  larger tensor dense of shape [LARGE0, D1, .. , DN] where LARGE0 >> D0.
  The values in indices are the indices in the first dimension of the
  slices that have been extracted from the larger tensor.</p>
</blockquote>

<p>So this array of shape (4200,30) is extracted from an array corresponding to an even larger, dense tensor?</p>

<p>What exactly is the gradient in this IndexedSlicesValue object and why does tensorflow automatically use this type for some gradients returned by tf.gradients()?</p>

<p>Here is my code:</p>

<pre><code>input_tensor = tf.placeholder(tf.int32, shape = [None, memory_size, max_sent_length], name = 'Input')
q_tensor = tf.placeholder(tf.int32, shape = [None,max_sent_length], name = 'Question')
a_tensor = tf.placeholder(tf.float32, shape = [None,V+1], name = 'Answer')
# Embedding matrices
A_prior = tf.get_variable(name = 'A', shape = [V+1,d], initializer = tf.random_normal_initializer(stddev = 0.1))
A = tf.concat(0,[tf.zeros(shape = tf.pack([1,tf.shape(A_prior)[1]])),tf.slice(A_prior,[1,0],[-1,-1])])
B = tf.get_variable(name = 'B', shape = [V+1,d], initializer = tf.random_normal_initializer(stddev = 0.1))
C = tf.get_variable(name = 'C', shape = [V+1,d], initializer = tf.random_normal_initializer(stddev = 0.1))
W = tf.get_variable(name = 'W', shape = [V+1,d], initializer= tf.random_normal_initializer(stddev = 0.1))
embeddings = tf.reduce_sum(tf.nn.embedding_lookup(A,input_tensor),2)
u = tf.reshape(tf.reduce_sum(tf.nn.embedding_lookup(B,q_tensor),1),[-1,1,d])
test = tf.transpose(embeddings, perm = [0,2,1])
test_batch_mul = tf.squeeze(tf.batch_matmul(u,test))
cond = tf.not_equal(test_batch_mul,0.0)
tt = tf.fill(tf.shape(test_batch_mul),-1000.0)
softmax_in = tf.select(cond, test_batch_mul, tt)
p_values = tf.nn.softmax(softmax_in)
c_values = tf.reduce_sum(tf.nn.embedding_lookup(C,input_tensor),2)
o = tf.squeeze(tf.batch_matmul(tf.expand_dims(p_values,1),c_values))
a_pred = tf.nn.softmax(tf.matmul(tf.squeeze(u)+o,tf.transpose(W)))
loss = tf.nn.softmax_cross_entropy_with_logits(a_pred, a_tensor, name = 'loss')
cost = tf.reduce_mean(loss)
global_step = tf.Variable(0,name = 'global_step', trainable= False)
#optimizer = tf.train.MomentumOptimizer(0.01,0.9)
vars_list = tf.trainable_variables()
grads = tf.gradients(cost, vars_list)
#train_op = optimizer.minimize( cost, global_step, vars_list, name = 'train_op')

sess = tf.Session()
init = tf.initialize_all_variables()
sess.run(init)
input_feed = {input_tensor : phrases, q_tensor : questions, a_tensor : answers}
grad_results = sess.run(grads, feed_dict = input_feed)
</code></pre>
","I'm having the following problem, I have four embedding matrices and want to get the gradients of my loss function with respect to those matrices. When I run the session to return the values for the gradients, two of those returned objects are of type tensorflow.python.framework.ops.IndexedSlicesValue, the other two are numpy arrays. Now for the numpy arrays, their shape corresponds to the shape of their corresponding embedding matrix, but I'm having problems with the IndexedSlicesValue objects. If I call .values on one of those objects, I get an array whose shape does not match that of the gradient, the shape of the embedding matrix is [22,30], but calling .values on the IndexedSlicesValue object I get an array with shape [4200,30] ( The shape of my input tensor had dimensions of [30,20,7], the product of those dimensions equals 4200, not sure if this is relevant). The IndexedSlicesValue object has an attribute called dense_shape, which is an array that holds the dimensions the gradient should have, i.e. array([22,30]) is value returned by .dense_shape. I don't really understand the docs here: https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#IndexedSlices It says: So this array of shape (4200,30) is extracted from an array corresponding to an even larger, dense tensor? What exactly is the gradient in this IndexedSlicesValue object and why does tensorflow automatically use this type for some gradients returned by tf.gradients()? Here is my code:",https://stackoverflow.com/questions/36570729,3042790,Documentation Replication on Other Examples
36631868,Tensorflow: Noise contrastive estimation language model,"<p>I want to change the loss function in the <a href=""https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow"">ptb_word_lm.py</a> example to <code>tf.nn.nce_loss</code>. Looking at the <code>tf.nn.nce_loss</code> implementation:</p>

<pre><code>def nce_loss(weights, biases, inputs, labels, num_sampled, num_classes,
         num_true=1,
         sampled_values=None,
         remove_accidental_hits=False,
         partition_strategy=""mod"",
         name=""nce_loss""):
</code></pre>

<p>I think </p>

<ul>
<li>the 3rd parameter (inputs) is the logits of language model,  </li>
<li>4th parameter (labels) is the next word (self._targets) of language
model, </li>
<li>num_classes is the vocab_size</li>
</ul>

<p>But I do not know what are the first two parameters, weights and biases. How could I adapt <code>tf.nn.nce_loss</code> to language model? Thanks.</p>

########UPDATES

<p>@Aaron:</p>

<p>Thanks, I have tried the following:</p>

<pre><code>loss = tf.reduce_mean(
        tf.nn.nce_loss(softmax_w, softmax_b, logits, tf.reshape(self._targets, [-1,1]),
                                     64, vocab_size))
</code></pre>

<p>According to the document at <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#nce_loss"" rel=""nofollow"">here</a>:</p>

<ul>
<li><p>weights: A Tensor of shape [num_classes, dim], or a list of Tensor
objects 
           whose concatenation along dimension 0 has shape [num_classes, dim]. The (possibly-partitioned) class embeddings.</p></li>
<li><p>biases: A Tensor of shape [num_classes]. The class biases.</p></li>
<li><p>inputs: A Tensor of shape [batch_size, dim]. The forward activations
of the input network.</p></li>
<li><p>labels: A Tensor of type int64 and shape [batch_size, num_true]. The
target classes.</p></li>
<li><p>num_sampled: An int. The number of classes to randomly sample per
batch.</p></li>
<li><p>num_classes: An int. The number of possible classes.</p></li>
</ul>

<p>So, </p>

<ul>
<li>weights is the softmax_w tensor, which has shape (hidden_size,
vocab_size)</li>
<li>biases is softmax_b, which has shape (vocab_size)</li>
<li>inputs is logits, which has shape (batch_size*num_steps, vocab_size)</li>
<li>labels is self._targets, which has shape (batch_size, num_steps),
thus, we need to reshape it, tf.reshape(self._targets, [-1,1])</li>
</ul>

<p>My PTBModel model looks like</p>

<pre><code>class PTBModel(object):
    def __init__(self, is_training, config):
        self.batch_size = batch_size = config.batch_size
        self.num_steps = num_steps = config.num_steps
        size = config.hidden_size
        vocab_size = config.vocab_size
        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])
        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])

        lstm_cell = rnn_cell.BasicLSTMCell(size, forget_bias=0.0)
        if is_training and config.keep_prob &lt; 1:
            lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)
        cell = rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)
        self._initial_state = cell.zero_state(batch_size, tf.float32)
        with tf.device(""/cpu:0""):
            embedding = tf.get_variable(""embedding"", [vocab_size, size])
            inputs = tf.nn.embedding_lookup(embedding, self._input_data)
        if is_training and config.keep_prob &lt; 1:
            inputs = tf.nn.dropout(inputs, config.keep_prob)

        outputs = []
        states = []
        state = self._initial_state
        with tf.variable_scope(""RNN""):
            for time_step in range(num_steps):
                if time_step &gt; 0: tf.get_variable_scope().reuse_variables()
                (cell_output, state) = cell(inputs[:, time_step, :], state)
                outputs.append(cell_output)
                states.append(state)
        output = tf.reshape(tf.concat(1, outputs), [-1, size])
        softmax_w = tf.get_variable(""softmax_w"", [size, vocab_size])
        softmax_b = tf.get_variable(""softmax_b"", [vocab_size])
        logits = tf.matmul(output, softmax_w) + softmax_b

        '''
        #minimize the average negative log probability using sequence_loss_by_example
        loss = seq2seq.sequence_loss_by_example([logits],
                                                [tf.reshape(self._targets, [-1])],
                                                [tf.ones([batch_size * num_steps])],
                                                vocab_size)

        loss = tf.reduce_mean(
            tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,
                                         num_sampled, vocabulary_size))
        weights: A Tensor of shape [num_classes, dim], or a list of Tensor objects 
            whose concatenation along dimension 0 has shape [num_classes, dim]. The (possibly-partitioned) class embeddings.
        biases: A Tensor of shape [num_classes]. The class biases.
        inputs: A Tensor of shape [batch_size, dim]. The forward activations of the input network.
        labels: A Tensor of type int64 and shape [batch_size, num_true]. The target classes.
        num_sampled: An int. The number of classes to randomly sample per batch.
        num_classes: An int. The number of possible classes.

        '''
        loss = tf.reduce_mean(
            tf.nn.nce_loss(softmax_w, softmax_b, logits, tf.reshape(self._targets, [-1,1]),
                                         64, vocab_size))


        self._cost = cost = tf.reduce_sum(loss) / batch_size
        self._final_state = states[-1]
        if not is_training:
            return
        self._lr = tf.Variable(0.0, trainable=False)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                          config.max_grad_norm)
        optimizer = tf.train.GradientDescentOptimizer(self.lr)
        self._train_op = optimizer.apply_gradients(zip(grads, tvars))
</code></pre>

<p>However, I got an error</p>

<pre><code>Epoch: 1 Learning rate: 1.000
W tensorflow/core/common_runtime/executor.cc:1102] 0x528c980 Compute status: Invalid argument: Index 9971 at offset 0 in Tindices is out of range
     [[Node: model/nce_loss/embedding_lookup = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](model/softmax_w/read, model/nce_loss/concat)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x528c980 Compute status: Invalid argument: Index 9971 at offset 0 in Tindices is out of range
     [[Node: model/nce_loss/embedding_lookup = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](model/softmax_w/read, model/nce_loss/concat)]]
     [[Node: _send_model/RNN/concat_19_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1438650956868917036, tensor_name=""model/RNN/concat_19:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](model/RNN/concat_19)]]
Traceback (most recent call last):
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 235, in &lt;module&gt;
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 225, in main
    verbose=True)
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 189, in run_epoch
    m.initial_state: state})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 315, in run
    return self._run(None, fetches, feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 511, in _run
    feed_dict_string)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 564, in _do_run
    target_list)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 586, in _do_call
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: Index 9971 at offset 0 in Tindices is out of range
     [[Node: model/nce_loss/embedding_lookup = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](model/softmax_w/read, model/nce_loss/concat)]]
Caused by op u'model/nce_loss/embedding_lookup', defined at:
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 235, in &lt;module&gt;
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 214, in main
    m = PTBModel(is_training=True, config=config)
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 122, in __init__
    64, vocab_size))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 798, in nce_loss
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 660, in _compute_sampled_logits
    weights, all_ids, partition_strategy=partition_strategy)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/embedding_ops.py"", line 86, in embedding_lookup
    validate_indices=validate_indices)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 447, in gather
    validate_indices=validate_indices, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2040, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1087, in __init__
    self._traceback = _extract_stack()
</code></pre>

<p>Did I miss anything here? Thanks again.</p>
","I want to change the loss function in the ptb_word_lm.py example to tf.nn.nce_loss. Looking at the tf.nn.nce_loss implementation: I think But I do not know what are the first two parameters, weights and biases. How could I adapt tf.nn.nce_loss to language model? Thanks. @Aaron: Thanks, I have tried the following: According to the document at here: So, My PTBModel model looks like However, I got an error Did I miss anything here? Thanks again.",https://stackoverflow.com/questions/36631868,200340,Documentation Replication on Other Examples
37044006,Tensorflow conditional throwing value error,"<p>I am trying to use conditionals with tensorflow and I am getting the error:</p>

<pre><code>ValueError: Shapes (1,) and () are not compatible
</code></pre>

<p>Below is the code I use that is throwing the error.
It is saying the error is in the conditional</p>

<pre><code>import tensorflow as tf
import numpy as np

X = tf.constant([1, 0])
Y = tf.constant([0, 1])
BOTH = tf.constant([1, 1])
WORKING = tf.constant(1)

def create_mult_func(tf, amount, list):
    def f1():
        return tf.scalar_mul(amount, list)
    return f1

def create_no_op_func(tensor):
    def f1():
        return tensor
    return f1

def stretch(tf, points, dim, amount):
    """"""points is a 2 by ??? tensor, dim is a 1 by 2 tensor, amount is tensor scalor""""""
    x_list, y_list = tf.split(0, 2, points)
    x_stretch, y_stretch = tf.split(1, 2, dim)
    is_stretch_X = tf.equal(x_stretch, WORKING, name=""is_stretch_x"")
    is_stretch_Y = tf.equal(y_stretch, WORKING, name=""is_stretch_Y"")
    x_list_stretched = tf.cond(is_stretch_X,
                               create_mult_func(tf, amount, x_list), create_no_op_func(x_list))
    y_list_stretched = tf.cond(is_stretch_Y,
                               create_mult_func(tf, amount, y_list), create_no_op_func(y_list))
    return tf.concat(1, [x_list_stretched, y_list_stretched])

example_points = np.array([[1, 1], [2, 2], [3, 3]], dtype=np.float32)
example_point_list = tf.placeholder(tf.float32)

result = stretch(tf, example_point_list, X, 1)
sess = tf.Session()

with tf.Session() as sess:
    result = sess.run(result, feed_dict={example_point_list: example_points})
    print(result)
</code></pre>

<p>Stack trace:</p>

<pre><code>  File ""/path/test2.py"", line 36, in &lt;module&gt;
    result = stretch(tf, example_point_list, X, 1)
  File ""/path/test2.py"", line 28, in stretch
    create_mult_func(tf, amount, x_list), create_no_op_func(x_list))
  File ""/path/tensorflow/python/ops/control_flow_ops.py"", line 1142, in cond
    p_2, p_1 = switch(pred, pred)
  File ""/path/tensorflow/python/ops/control_flow_ops.py"", line 203, in switch
    return gen_control_flow_ops._switch(data, pred, name=name)
  File ""/path/tensorflow/python/ops/gen_control_flow_ops.py"", line 297, in _switch
    return _op_def_lib.apply_op(""Switch"", data=data, pred=pred, name=name)
  File ""/path/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)
  File ""/path/tensorflow/python/framework/ops.py"", line 2156, in create_op
    set_shapes_for_outputs(ret)
  File ""/path/tensorflow/python/framework/ops.py"", line 1612, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/path/tensorflow/python/ops/control_flow_ops.py"", line 2032, in _SwitchShape
    unused_pred_shape = op.inputs[1].get_shape().merge_with(tensor_shape.scalar())
  File ""/path/tensorflow/python/framework/tensor_shape.py"", line 554, in merge_with
    (self, other))
ValueError: Shapes (1,) and () are not compatible
</code></pre>

<p>I have tried changing the WORKING to be an array instead of a scalar.</p>

<p>I believe that the problem is that <code>tf.equal</code> is returning an <code>int32</code> instead of the bool that it is supposed to return according to the documentation</p>
",I am trying to use conditionals with tensorflow and I am getting the error: Below is the code I use that is throwing the error. It is saying the error is in the conditional Stack trace: I have tried changing the WORKING to be an array instead of a scalar. I believe that the problem is that tf.equal is returning an int32 instead of the bool that it is supposed to return according to the documentation,https://stackoverflow.com/questions/37044006,2187510,Documentation Ambiguity
37376861,what does the tf.nn.lrn() method do?,"<p>Here is the code-snipped from the cifar10-tutorial. It's from the cifar10.py.</p>

<pre><code># conv1
with tf.variable_scope('conv1') as scope:
kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64],
                                     stddev=1e-4, wd=0.0)
conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')
biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))
bias = tf.nn.bias_add(conv, biases)
conv1 = tf.nn.relu(bias, name=scope.name)
_activation_summary(conv1)

# pool1
pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],
                     padding='SAME', name='pool1')
# norm1
norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,
                name='norm1')
</code></pre>

<p>What does the tf.nn.lrn-Method do? I can't find a definition in the API Documentation on <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/index.html"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.8/api_docs/python/index.html</a></p>
",Here is the code-snipped from the cifar10-tutorial. It's from the cifar10.py. What does the tf.nn.lrn-Method do? I can't find a definition in the API Documentation on https://www.tensorflow.org/versions/r0.8/api_docs/python/index.html,https://stackoverflow.com/questions/37376861,3019308,Lack of Alternative Solutions/Documentation
38111170,How is the input tensor for TensorFlow's tf.nn.dynamic_rnn operator structured?,"<p>I am trying to write a language model using word embeddings and recursive neural networks in TensorFlow 0.9.0 using the <code>tf.nn.dynamic_rnn</code> graph operation, but I don't understand how the <code>input</code> tensor is structured.</p>

<p>Let's say I have a corpus of <em>n</em> words. I embed each word in a vector of length <em>e</em>, and I want my RNN to unroll to <em>t</em> time steps. Assuming I use the default <code>time_major = False</code> parameter, what shape would my <code>input</code> tensor <code>[batch_size, max_time, input_size]</code> have?</p>

<p>Maybe a specific tiny example will make this question clearer. Say I have a corpus consisting of <em>n=8</em> words that looks like this.</p>

<pre><code>1, 2, 3, 3, 2, 1, 1, 2
</code></pre>

<p>Say I embed it in a vector of size <em>e=3</em> with the embeddings 1 -> [10, 10, 10], 2 -> [20, 20, 20], and 3 -> [30, 30, 30], what would my <code>input</code> tensor look like?</p>

<p>I've read the <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html#recurrent-neural-networks"" rel=""nofollow"">TensorFlow Recurrent Neural Network tutorial</a>, but that doesn't use <code>tf.nn.dynamic_rnn</code>. I've also read the documentation for <code>tf.nn.dynamic_rnn</code>, but find it confusing. In particular I'm not sure what ""max_time"" and ""input_size"" mean here.</p>

<p>Can anyone give the shape of the <code>input</code> tensor in terms of <em>n</em>, <em>t</em>, and <em>e</em>, and/or an example of what that tensor would look like initialized with data from the small corpus I describe?</p>

<p><em>TensorFlow 0.9.0, Python 3.5.1, OS X 10.11.5</em></p>
","I am trying to write a language model using word embeddings and recursive neural networks in TensorFlow 0.9.0 using the tf.nn.dynamic_rnn graph operation, but I don't understand how the input tensor is structured. Let's say I have a corpus of n words. I embed each word in a vector of length e, and I want my RNN to unroll to t time steps. Assuming I use the default time_major = False parameter, what shape would my input tensor [batch_size, max_time, input_size] have? Maybe a specific tiny example will make this question clearer. Say I have a corpus consisting of n=8 words that looks like this. Say I embed it in a vector of size e=3 with the embeddings 1 -&gt; [10, 10, 10], 2 -&gt; [20, 20, 20], and 3 -&gt; [30, 30, 30], what would my input tensor look like? I've read the TensorFlow Recurrent Neural Network tutorial, but that doesn't use tf.nn.dynamic_rnn. I've also read the documentation for tf.nn.dynamic_rnn, but find it confusing. In particular I'm not sure what ""max_time"" and ""input_size"" mean here. Can anyone give the shape of the input tensor in terms of n, t, and e, and/or an example of what that tensor would look like initialized with data from the small corpus I describe? TensorFlow 0.9.0, Python 3.5.1, OS X 10.11.5",https://stackoverflow.com/questions/38111170,1120370,Documentation Replicability
38114534,Basic 1d convolution in tensorflow,"<p>OK, I'd like to do a 1-dimensional convolution of time series data in Tensorflow. This is apparently supported using <code>tf.nn.conv2d</code>, according to <a href=""https://github.com/tensorflow/tensorflow/issues/2165"" rel=""noreferrer"">these</a> <a href=""https://github.com/tensorflow/tensorflow/issues/1136"" rel=""noreferrer"">tickets</a>, and <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#convolution"" rel=""noreferrer"">the manual</a>. the only requirement is to set <code>strides=[1,1,1,1]</code>. Sounds simple!</p>

<p>However, I cannot work out how to do this in even a very minimal test case. What am I doing wrong?</p>

<p>Let's set this up.</p>

<pre><code>import tensorflow as tf
import numpy as np
print(tf.__version__)
&gt;&gt;&gt; 0.9.0
</code></pre>

<p>OK, now generate a basic convolution test on two small arrays. I will make it easy by using a batch size of 1, and since time series are 1-dimensional, I will have an ""image height"" of 1. And since it's a univariate time series, clearly the number of ""channels"" is also 1, so this will be simple, right?</p>

<pre><code>g = tf.Graph()
with g.as_default():
    # data shape is ""[batch, in_height, in_width, in_channels]"",
    x = tf.Variable(np.array([0.0, 0.0, 0.0, 0.0, 1.0]).reshape(1,1,-1,1), name=""x"")
    # filter shape is ""[filter_height, filter_width, in_channels, out_channels]""
    phi = tf.Variable(np.array([0.0, 0.5, 1.0]).reshape(1,-1,1,1), name=""phi"")
    conv = tf.nn.conv2d(
        phi,
        x,
        strides=[1, 1, 1, 1],
        padding=""SAME"",
        name=""conv"")
</code></pre>

<p>BOOM. Error.</p>

<pre><code>ValueError: Dimensions 1 and 5 are not compatible
</code></pre>

<p>OK, For a start, I don't understand how this should happen with <em>any</em> dimension, since I've specified that I'm padding the arguments in the convolution OP. </p>

<p>but fine, maybe there are limits to that. I must have got the documentation confused and set up this convolution on the wrong axes of the tensor. I'll try all possible permutations:</p>

<pre><code>for i in range(4):
    for j in range(4):
        shape1 = [1,1,1,1]
        shape1[i] = -1
        shape2 = [1,1,1,1]
        shape2[j] = -1
        x_array = np.array([0.0, 0.0, 0.0, 0.0, 1.0]).reshape(*shape1)
        phi_array = np.array([0.0, 0.5, 1.0]).reshape(*shape2)
        try:
            g = tf.Graph()
            with g.as_default():
                x = tf.Variable(x_array, name=""x"")
                phi = tf.Variable(phi_array, name=""phi"")
                conv = tf.nn.conv2d(
                    x,
                    phi,
                    strides=[1, 1, 1, 1],
                    padding=""SAME"",
                    name=""conv"")
                init_op = tf.initialize_all_variables()
            sess = tf.Session(graph=g)
            sess.run(init_op)
            print(""SUCCEEDED!"", x_array.shape, phi_array.shape, conv.eval(session=sess))
            sess.close()
        except Exception as e:
            print(""FAILED!"", x_array.shape, phi_array.shape, type(e), e.args or e._message)
</code></pre>

<p>Result:</p>

<pre><code>FAILED! (5, 1, 1, 1) (3, 1, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (3, 1) Input: (1, 1)',)
FAILED! (5, 1, 1, 1) (1, 3, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (1, 3) Input: (1, 1)',)
FAILED! (5, 1, 1, 1) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 1 and 3 are not compatible',)
FAILED! (5, 1, 1, 1) (1, 1, 1, 3) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 5, 1, 1) (3, 1, 1, 1) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 5, 1, 1) (1, 3, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (1, 3) Input: (5, 1)',)
FAILED! (1, 5, 1, 1) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 1 and 3 are not compatible',)
FAILED! (1, 5, 1, 1) (1, 1, 1, 3) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 1, 5, 1) (3, 1, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (3, 1) Input: (1, 5)',)
FAILED! (1, 1, 5, 1) (1, 3, 1, 1) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 1, 5, 1) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 1 and 3 are not compatible',)
FAILED! (1, 1, 5, 1) (1, 1, 1, 3) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 1, 1, 5) (3, 1, 1, 1) &lt;class 'ValueError'&gt; ('Dimensions 5 and 1 are not compatible',)
FAILED! (1, 1, 1, 5) (1, 3, 1, 1) &lt;class 'ValueError'&gt; ('Dimensions 5 and 1 are not compatible',)
FAILED! (1, 1, 1, 5) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 5 and 3 are not compatible',)
FAILED! (1, 1, 1, 5) (1, 1, 1, 3) &lt;class 'ValueError'&gt; ('Dimensions 5 and 1 are not compatible',)
</code></pre>

<p>Hmm. OK, it looks like there are two problems now. Firstly, the <code>ValueError</code> is about applying the filter along the wrong axis, I guess, although there are two forms.</p>

<p>But then the axes along which I can apply the filter are confusing too - notice that it actually constructs the graph with input shape (5, 1, 1, 1)  and filter shape (1, 1, 1, 3). AFAICT from the documentation, this should be a filter that looks at on example from the batch, one ""pixel"" and one ""channel"" and outputs 3 ""channels"". Why does that one work, then, when others do not?</p>

<p>Anyway, sometimes it does not fail while constructing the graph.
Sometime it constructs the graph; then we get the <code>tensorflow.python.framework.errors.InvalidArgumentError</code>. From some <a href=""https://github.com/tensorflow/tensorflow/issues/524"" rel=""noreferrer"">confusing github tickets</a> I gather this is probably due to <del>the fact that I'm running on CPU instead of GPU, or vice versa</del> the fact that the convolution Op is only defined for 32 bit floats, not 64 bit floats. If anyone could throw some light on <em>which</em> axes I should be aligning <em>what</em> on, in order to convolve a time series with a kernel, I'd be very grateful.</p>
","OK, I'd like to do a 1-dimensional convolution of time series data in Tensorflow. This is apparently supported using tf.nn.conv2d, according to these tickets, and the manual. the only requirement is to set strides=[1,1,1,1]. Sounds simple! However, I cannot work out how to do this in even a very minimal test case. What am I doing wrong? Let's set this up. OK, now generate a basic convolution test on two small arrays. I will make it easy by using a batch size of 1, and since time series are 1-dimensional, I will have an ""image height"" of 1. And since it's a univariate time series, clearly the number of ""channels"" is also 1, so this will be simple, right? BOOM. Error. OK, For a start, I don't understand how this should happen with any dimension, since I've specified that I'm padding the arguments in the convolution OP. but fine, maybe there are limits to that. I must have got the documentation confused and set up this convolution on the wrong axes of the tensor. I'll try all possible permutations: Result: Hmm. OK, it looks like there are two problems now. Firstly, the ValueError is about applying the filter along the wrong axis, I guess, although there are two forms. But then the axes along which I can apply the filter are confusing too - notice that it actually constructs the graph with input shape (5, 1, 1, 1) and filter shape (1, 1, 1, 3). AFAICT from the documentation, this should be a filter that looks at on example from the batch, one ""pixel"" and one ""channel"" and outputs 3 ""channels"". Why does that one work, then, when others do not? Anyway, sometimes it does not fail while constructing the graph. Sometime it constructs the graph; then we get the tensorflow.python.framework.errors.InvalidArgumentError. From some confusing github tickets I gather this is probably due to the fact that I'm running on CPU instead of GPU, or vice versa the fact that the convolution Op is only defined for 32 bit floats, not 64 bit floats. If anyone could throw some light on which axes I should be aligning what on, in order to convolve a time series with a kernel, I'd be very grateful.",https://stackoverflow.com/questions/38114534,11730,Documentation Ambiguity
38641887,How to save a trained tensorflow model for later use for application?,"<p>I am a bit of a beginner with tensorflow so please excuse if this is a stupid question and the answer is obvious. </p>

<p>I have created a Tensorflow graph where starting with placeholders for X and y I have optimized some tensors which represent my model. Part of the graph is something where a vector of predictions can be calculated, e.g. for linear regression something like </p>

<pre class=""lang-py prettyprint-override""><code>y_model = tf.add(tf.mul(X,w),d)
y_vals = sess.run(y_model,feed_dict={....})
</code></pre>

<p>After training has been completed I have acceptable values for w and d and now I want to save my model for later. Then, in a different python session I want to restore the model so that I can again run</p>

<pre class=""lang-py prettyprint-override""><code>## Starting brand new python session
import tensorflow as tf
## somehow restor the graph and the values here: how????
## so that I can run this:
y_vals = sess.run(y_model,feed_dict={....})
</code></pre>

<p>for some different data and get back the y-values. </p>

<p>I want this to work in a way where the graph for calculating the y-values from the placeholders is also stored and restored - as long as the placeholders get fed the correct data, this should work transparently without the user (the one who applies the model) needing to know what the graph looks like). </p>

<p>As far as I understand tf.train.Saver().save(..) only saves the variables but I also want to save the graph. I think that tf.train.export_meta_graph could be relevant here but I do not understand how to use it correctly, the documentation is a bit cryptic to me and the examples do not even use export_meta_graph anywhere. </p>
","I am a bit of a beginner with tensorflow so please excuse if this is a stupid question and the answer is obvious. I have created a Tensorflow graph where starting with placeholders for X and y I have optimized some tensors which represent my model. Part of the graph is something where a vector of predictions can be calculated, e.g. for linear regression something like After training has been completed I have acceptable values for w and d and now I want to save my model for later. Then, in a different python session I want to restore the model so that I can again run for some different data and get back the y-values. I want this to work in a way where the graph for calculating the y-values from the placeholders is also stored and restored - as long as the placeholders get fed the correct data, this should work transparently without the user (the one who applies the model) needing to know what the graph looks like). As far as I understand tf.train.Saver().save(..) only saves the variables but I also want to save the graph. I think that tf.train.export_meta_graph could be relevant here but I do not understand how to use it correctly, the documentation is a bit cryptic to me and the examples do not even use export_meta_graph anywhere.",https://stackoverflow.com/questions/38641887,1382437,Inadequate Examples
38893526,What's the meaning of tf.nn.embedding_lookup_sparse in TensorFlow?,"<p>We spend a lot of time in reading the API document of <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#embedding_lookup_sparse"" rel=""nofollow"">tf.nn.embedding_lookup_sparse</a>. The meaning of <code>embedding_lookup_sparse</code> is confusing and it seems quite different from <code>embedding_lookup</code>.</p>

<p>Here's what I think and please correct me if I'm wrong. The example of wide and deep model uses <code>contrib.layers</code> APIs and call <code>embedding_lookup_sparse</code> for sparse feature colume. If it gets the SparseTensor(for example, country, which is sparse), it creates the embedding which is actually for one-host encoding. Then call <code>to_weights_sum</code> to return the result of <code>embedding_lookup_sparse</code> as <code>prediction</code> and the embedding as <code>variable</code>.</p>

<p>The the result of <code>embedding_lookup_sparse</code> add <code>bias</code> and become the <code>logits</code> for loss function and training operation. That means the <code>embedding_lookup_sparse</code> do something like <code>w * x</code>(part of <code>y = w * x + b</code>) for dense tensor.</p>

<p>Maybe for one-hot encoding or SparseTensor, the <code>weight</code> from <code>embedding_lookup_sparse</code> is actually the value of <code>w * x</code> because the look-up data is always <code>1</code> and no need to add other <code>0</code>s.</p>

<p>What I said is also confusing. Can anyone help to explain this in detail?</p>
","We spend a lot of time in reading the API document of tf.nn.embedding_lookup_sparse. The meaning of embedding_lookup_sparse is confusing and it seems quite different from embedding_lookup. Here's what I think and please correct me if I'm wrong. The example of wide and deep model uses contrib.layers APIs and call embedding_lookup_sparse for sparse feature colume. If it gets the SparseTensor(for example, country, which is sparse), it creates the embedding which is actually for one-host encoding. Then call to_weights_sum to return the result of embedding_lookup_sparse as prediction and the embedding as variable. The the result of embedding_lookup_sparse add bias and become the logits for loss function and training operation. That means the embedding_lookup_sparse do something like w * x(part of y = w * x + b) for dense tensor. Maybe for one-hot encoding or SparseTensor, the weight from embedding_lookup_sparse is actually the value of w * x because the look-up data is always 1 and no need to add other 0s. What I said is also confusing. Can anyone help to explain this in detail?",https://stackoverflow.com/questions/38893526,2502090,Documentation Replication on Other Examples
38962308,Unclear behavior for sampler in Tensorflow,"<p>For the samplers implemented in tensorflow, e.g. tf.nn.fixed_unigram_candidate_sampler. The behavior is not well-defined in the document. For instance, I would expect the labels specified in true_classes will be excluded from the sampling pool, and the sampling will be conducted for each batch. But according to my experiments, neither of above is true.</p>

<p>Consider the following code:</p>

<pre><code>import tensorflow as tf

labels_matrix = tf.reshape(tf.constant([1, 2, 3, 4], dtype=tf.int64), [-1, 1])

sampled_ids, _, _ = tf.nn.fixed_unigram_candidate_sampler(
true_classes = labels_matrix,
num_true = 1,
num_sampled = 1,
unique = True,
range_max = 5,
distortion = 0.0,
unigrams = range(5)
)

init = tf.initialize_all_variables()
with tf.Session() as sess:
sess.run(init)
print sess.run([sampled_ids])
</code></pre>

<p>The output can be 3, which actually belongs to the set of true classes. - Also, the output has the dimension [1], which basically means that the sampling is only conducted once, not for each batch.</p>

<p>Can someone help to clarify this?</p>
","For the samplers implemented in tensorflow, e.g. tf.nn.fixed_unigram_candidate_sampler. The behavior is not well-defined in the document. For instance, I would expect the labels specified in true_classes will be excluded from the sampling pool, and the sampling will be conducted for each batch. But according to my experiments, neither of above is true. Consider the following code: The output can be 3, which actually belongs to the set of true classes. - Also, the output has the dimension [1], which basically means that the sampling is only conducted once, not for each batch. Can someone help to clarify this?",https://stackoverflow.com/questions/38962308,4097290,Documentation Ambiguity
39133312,Why does setting an initialization value prevent placing a variable on a GPU in TensorFlow?,"<p>I get an exception when I try to run the following very simple TensorFlow code, although I virtually copied it from the documentation:</p>

<pre><code>import tensorflow as tf

with tf.device(""/gpu:0""):
  x = tf.Variable(0, name=""x"")

sess = tf.Session()
sess.run(x.initializer) # Bombs!
</code></pre>

<p>The exception is:</p>

<pre><code>tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to
node 'x': Could not satisfy explicit device specification '/device:GPU:0' because
no supported kernel for GPU devices is available.
</code></pre>

<p>If I change the variable's initial value to <code>tf.zeros([1])</code> instead, everything works fine:</p>

<pre><code>import tensorflow as tf

with tf.device(""/gpu:0""):
  x = tf.Variable(tf.zeros([1]), name=""x"")

sess = tf.Session()     
sess.run(x.initializer)  # Works fine
</code></pre>

<p>Any idea what's going on?</p>
","I get an exception when I try to run the following very simple TensorFlow code, although I virtually copied it from the documentation: The exception is: If I change the variable's initial value to tf.zeros([1]) instead, everything works fine: Any idea what's going on?",https://stackoverflow.com/questions/39133312,38626,Documentation Replicability
39210093,regarding the correct way to understand the result of tf.pad,"<p>When reading the document for <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#pad"" rel=""noreferrer"">tf.pad</a>, I feel quite confusing about the example given in the tutorial. For instance, padding is <code>[[1,1,],[2,2]]</code>, how does it cause the resulting tensor has the shape as shown in the figure. Besides, what's the mechanism to generate those padded values, e.g., the ones marked in red circle. It is not very clear how to connect the explanation with the example.</p>

<p><a href=""https://i.stack.imgur.com/7OHis.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7OHis.jpg"" alt=""enter image description here""></a></p>
","When reading the document for tf.pad, I feel quite confusing about the example given in the tutorial. For instance, padding is [[1,1,],[2,2]], how does it cause the resulting tensor has the shape as shown in the figure. Besides, what's the mechanism to generate those padded values, e.g., the ones marked in red circle. It is not very clear how to connect the explanation with the example.",https://stackoverflow.com/questions/39210093,785099,Lack of Alternative Solutions/Documentation
39211332,Custom initializer for get_variable,"<p>How can one specify a custom initializer as the third argument for <code>tf.get_variable()</code>? Specifically, I have a variable <code>y</code> which I want to initialize using another (already initialized) variable <code>x</code>. </p>

<p>This is easy to do using <code>tf.Variable()</code>, just say, <code>y = tf.Variable(x.initialized_value())</code>. But I couldn't find an analog in the documentation for <code>tf.get_variable()</code>.</p>
","How can one specify a custom initializer as the third argument for tf.get_variable()? Specifically, I have a variable y which I want to initialize using another (already initialized) variable x. This is easy to do using tf.Variable(), just say, y = tf.Variable(x.initialized_value()). But I couldn't find an analog in the documentation for tf.get_variable().",https://stackoverflow.com/questions/39211332,1994648,Lack of Alternative Solutions/Documentation
39450992,Tensorflow slim how to specify batch size during training,"<p>I'm trying to use slim interface to create and train a convolutional neural network, but I couldn't figure out how to specify the batch size for training.
During the training my net crashes because of ""Out of Memory"" on my graphic card.
So I think that should be a way to handle this condition...
Do I have to split the data and the labels in batches and then explicitly loop or the slim.learning.train is taking care of it?
In the code I paste train_data are all the data in my training set (numpy array)..and the model definition is not included here
I had a quick loop to the sources but no luck so far...</p>

<pre><code>g = tf.Graph()
    with g.as_default():
        # Set up the data loading:
        images = train_data
        labels = tf.contrib.layers.one_hot_encoding(labels=train_labels, num_classes=num_classes)
        # Define the model:
        predictions = model7_2(images, num_classes, is_training=True)

        # Specify the loss function:
        slim.losses.softmax_cross_entropy(predictions, labels)

        total_loss = slim.losses.get_total_loss()
        tf.scalar_summary('losses/total loss', total_loss)

        # Specify the optimization scheme:
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)
        train_tensor = slim.learning.create_train_op(total_loss, optimizer)

        slim.learning.train(train_tensor,
            train_log_dir,
            number_of_steps=1000,
            save_summaries_secs=300,
            save_interval_secs=600)
</code></pre>

<p>Any hints suggestions?</p>

<p>Edit:
I re-read the documentation...and I found this example</p>

<pre><code>image, label = MyPascalVocDataLoader(...)
images, labels = tf.train.batch([image, label], batch_size=32)
</code></pre>

<p>But It's not clear at all how to feed image and label to be passed to tf.train.batch... as MyPascalVocDataLoader function is not specified...
In my case my data set are loaded from a sqlite database and I have training data and labels as numpy array....still confused.
Of course I tried to pass my numpy arrays (converted to constant tensor) to the tf.train.batch like this</p>

<pre><code>    image = tf.constant(train_data)
    label = tf.contrib.layers.one_hot_encoding(labels=train_labels, num_classes=num_classes)
    images, labels = tf.train.batch([image, label], batch_size=32)
</code></pre>

<p>But seems not the right path to follow... it seems that the train.batch wants only one element from my data set...(how to pass this? it does not make sense to me to pass only train_data[0] and train_labels[0])</p>
","I'm trying to use slim interface to create and train a convolutional neural network, but I couldn't figure out how to specify the batch size for training. During the training my net crashes because of ""Out of Memory"" on my graphic card. So I think that should be a way to handle this condition... Do I have to split the data and the labels in batches and then explicitly loop or the slim.learning.train is taking care of it? In the code I paste train_data are all the data in my training set (numpy array)..and the model definition is not included here I had a quick loop to the sources but no luck so far... Any hints suggestions? Edit: I re-read the documentation...and I found this example But It's not clear at all how to feed image and label to be passed to tf.train.batch... as MyPascalVocDataLoader function is not specified... In my case my data set are loaded from a sqlite database and I have training data and labels as numpy array....still confused. Of course I tried to pass my numpy arrays (converted to constant tensor) to the tf.train.batch like this But seems not the right path to follow... it seems that the train.batch wants only one element from my data set...(how to pass this? it does not make sense to me to pass only train_data[0] and train_labels[0])",https://stackoverflow.com/questions/39450992,6695432,Documentation Ambiguity
39681026,Tensorflow: How to pass output from previous time-step as input to next timestep,"<p>It is a duplicate of this question <a href=""https://stackoverflow.com/questions/35145645/how-can-i-feed-last-output-yt-1-as-input-for-generating-yt-in-tensorflow-rnn#"">How can I feed last output y(t-1) as input for generating y(t) in tensorflow RNN?</a></p>

<p>I want to pass the output of RNN at time-step T as the input at time-step T+1. <code>input_RNN(T+1) = output_RNN(T)</code>
As per the documentation, the  tf.nn.rnn as well as tf.nn.dynamic_rnn functions explicitly take the complete input to all time-steps. </p>

<p>I checked the seq2seq example at <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py</a>
It uses a loop and calls the cell(input,state) function. The cell can be lstm or gru or any other rnn cell. I checked the documentation to find the data type and shape of the arguments to cell(), but I found only the contructor of the form cell(num_neurons). 
I would like to know the correct way of passing output to input. I don't want to use other libraries/wrappers like keras built over tensorflow. Any suggestions?</p>
","It is a duplicate of this question How can I feed last output y(t-1) as input for generating y(t) in tensorflow RNN? I want to pass the output of RNN at time-step T as the input at time-step T+1. input_RNN(T+1) = output_RNN(T) As per the documentation, the tf.nn.rnn as well as tf.nn.dynamic_rnn functions explicitly take the complete input to all time-steps. I checked the seq2seq example at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py It uses a loop and calls the cell(input,state) function. The cell can be lstm or gru or any other rnn cell. I checked the documentation to find the data type and shape of the arguments to cell(), but I found only the contructor of the form cell(num_neurons). I would like to know the correct way of passing output to input. I don't want to use other libraries/wrappers like keras built over tensorflow. Any suggestions?",https://stackoverflow.com/questions/39681026,2531306,Documentation Completeness
40195549,tf.rank function in Tensorflow,"<p>I ma trying to understand tf.rank function in tensorflow. From the documentation <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#rank"" rel=""nofollow"">here</a>, I understood that rank should return the number of distinct elements in the tensor. </p>

<p>Here x and weights are 2 distinct 2*2 tensors with 4 distinct elemnts in each of them. However, rank() function outputs are:</p>

<blockquote>
  <p>Tensor(""Rank:0"", shape=(), dtype=int32) Tensor(""Rank_1:0"", shape=(),
  dtype=int32)</p>
</blockquote>

<p>Also, for the tensor x, I used tf.constant() with dtype = float to convert ndarray into float32 tensor but the rank() still outputs as int32.</p>

<pre><code>g = tf.Graph()
with g.as_default():
    weights = tf.Variable(tf.truncated_normal([2,2]))
    x = np.asarray([[1 , 2], [3 , 4]])
    x = tf.constant(x, dtype = tf.float32)
    y = tf.matmul(weights, x)
    print (tf.rank(x), tf.rank(weights))


with tf.Session(graph = g) as s:
    tf.initialize_all_variables().run()
    print (s.run(weights), s.run(x))
    print (s.run(y))
</code></pre>

<p>How should I interpret the output.</p>
","I ma trying to understand tf.rank function in tensorflow. From the documentation here, I understood that rank should return the number of distinct elements in the tensor. Here x and weights are 2 distinct 2*2 tensors with 4 distinct elemnts in each of them. However, rank() function outputs are: Also, for the tensor x, I used tf.constant() with dtype = float to convert ndarray into float32 tensor but the rank() still outputs as int32. How should I interpret the output.",https://stackoverflow.com/questions/40195549,2289031,Documentation Replication on Other Examples
40394910,What do classes tf.train.Coordinator and class tf.train.QueueRunner do in tensorflow?,"<p>I understand that both classes deal with threads. According to the documentation, tf.train.Coordinator coordinates the termination of a set of threads and tf.train.QueueRunner holds a list of enqueue operations for a queue, each to be run in a thread. </p>

<p>However, what is their role in simple words? When are they necessary during the training?</p>
","I understand that both classes deal with threads. According to the documentation, tf.train.Coordinator coordinates the termination of a set of threads and tf.train.QueueRunner holds a list of enqueue operations for a queue, each to be run in a thread. However, what is their role in simple words? When are they necessary during the training?",https://stackoverflow.com/questions/40394910,1279459,Documentation Replicability
40451974,"Tensorflow, restore variables in a specific device","<p>Maybe my question is a bit naive, but I really didn't find anything in the tensorflow documentation.</p>

<p>I have a trained tensorflow model where the variables of it was placed in the GPU. Now I would like to restore this model and test it using the CPU.</p>

<p>If I do this via 'tf.train.Saver.restore` as in the example:
<code>
 saver = tf.train.import_meta_graph(""/tmp/graph.meta"")
 saver.restore(session, ""/tmp/model.ckp"")
</code></p>

<p>I have the following excpetion:</p>

<p><code>
InvalidArgumentError: Cannot assign a device to node 'b_fc8/b_fc8/Adam_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0
</code></p>

<p>How can I make restore these variables in the <code>CPU</code>?</p>

<p>Thanks</p>
","Maybe my question is a bit naive, but I really didn't find anything in the tensorflow documentation. I have a trained tensorflow model where the variables of it was placed in the GPU. Now I would like to restore this model and test it using the CPU. If I do this via 'tf.train.Saver.restore` as in the example: saver = tf.train.import_meta_graph(""/tmp/graph.meta"") saver.restore(session, ""/tmp/model.ckp"") I have the following excpetion: InvalidArgumentError: Cannot assign a device to node 'b_fc8/b_fc8/Adam_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0 How can I make restore these variables in the CPU? Thanks",https://stackoverflow.com/questions/40451974,5913101,Documentation Replication on Other Examples
40879504,How to apply Drop Out in Tensorflow to improve the accuracy of neural network?,"<p>Drop-Out is regularization techniques. And I want to apply it to notMNIST data to reduce over-fitting to finish my Udacity Deep Learning Course Assignment.I have read the <a href=""https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/index.html"" rel=""noreferrer"">docs of tensorflow</a> on how to call the <code>tf.nn.dropout</code>. And here is my code</p>

<pre class=""lang-py prettyprint-override""><code># before proceeding further.
from __future__ import print_function
import numpy as np  
import tensorflow as tf
from six.moves import cPickle as pickle


pickle_file = 'notMNIST.pickle'

with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_dataset = save['train_dataset']
    train_labels = save['train_labels']
    valid_dataset = save['valid_dataset']
    valid_labels = save['valid_labels']
    test_dataset = save['test_dataset']
    test_labels = save['test_labels']
    del save  # hint to help gc free up memory
    print('Training set', train_dataset.shape, train_labels.shape)
    print('Validation set', valid_dataset.shape, valid_labels.shape)
    print('Test set', test_dataset.shape, test_labels.shape)


image_size = 28
num_labels = 10

def reformat(dataset, labels):
    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels

    train_dataset, train_labels = reformat(train_dataset, train_labels)
    valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
    test_dataset, test_labels = reformat(test_dataset, test_labels)
    print('Training set', train_dataset.shape, train_labels.shape)
    print('Validation set', valid_dataset.shape, valid_labels.shape)
    print('Test set', test_dataset.shape, test_labels.shape)

    def accuracy(predictions, labels):
        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))  / predictions.shape[0])


# ReLU neuron
# param
training_epochs = 30
batch_size = 521
display_step = 1
n_input = 784 # img shape: 28*28
n_classes = 10 # MNIST total classes (0-9 digits)

# hyper-parameter
n_hidden_1 = 256 
learning_rate = 0.05
lambda_term = 0.01


graph = tf.Graph()
with graph.as_default():
    # init weights
    weights_hiden =  tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=np.sqrt(n_input)))
    weights_out = tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=np.sqrt(n_hidden_1)))

    biases_hidden = tf.Variable(tf.random_normal([n_hidden_1]))
    biases_out = tf.Variable(tf.random_normal([n_classes]))

    x = tf.placeholder(""float"", [None, n_input])
    y = tf.placeholder(""float"", [None, n_classes])

    def model(x, weights_hiden, weights_out, biases_hidden, biases_out):
        # hidden layer with RELU activation
        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights_hiden), biases_hidden))
        # apply DropOut to hidden layer
        keep_prob = tf.placeholder(tf.float32)  # DROP-OUT here
        drop_out = tf.nn.dropout(layer_1, keep_prob)  # DROP-OUT here
        # output layer with linear activation
        out_layer = tf.matmul(layer_1, weights_out) + biases_out
        return out_layer

    # Construct model
    pred = model(x, weights_hiden, weights_out, biases_hidden, biases_out)

    # Define loss and optimizer
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y) +
                          lambda_term * tf.nn.l2_loss(weights_hiden) + 
                          lambda_term * tf.nn.l2_loss(weights_out) +
                          lambda_term * tf.nn.l2_loss(biases_hidden) + 
                          lambda_term * tf.nn.l2_loss(biases_out))
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)


# run the graph
with tf.Session(graph=graph) as sess:
    tf.initialize_all_variables().run()
    print('Initialized')
    # Training cycle
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(train_dataset.shape[0]/batch_size)
        # Loop over all batches
        for i in range(total_batch):
            batch_x = train_dataset[(i*batch_size):((i*batch_size) + batch_size), :]
            batch_y = train_labels[(i*batch_size):((i*batch_size) + batch_size), :]
            # Run optimization op (backprop) and cost op (to get loss value)
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})
            # Compute average loss
            avg_cost += c / total_batch
        # Display logs per epoch step
        if epoch % display_step == 0:
            print(""Epoch:"", '%04d' % (epoch+1), ""cost="", ""{:.9f}"".format(avg_cost))
    print(""Optimization Finished!"")

    # Test model
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
    print(""Test data accuracy:"", accuracy.eval({x: test_dataset, y: test_labels}))
    print(""Valid data accuracy:"", accuracy.eval({x: valid_dataset, y: valid_labels}))
</code></pre>

<p>The <code>tf.nn.dropout</code> is called in function <code>model()</code>, but after I applied the DropOut technique to the neural network, the accuracy did seem any change, here is the result:</p>

<pre><code>Epoch: 0001 cost= 579980.086977807
Epoch: 0002 cost= 238859.802382506
Epoch: 0003 cost= 90672.733752856
Epoch: 0004 cost= 32649.040985028
Epoch: 0005 cost= 11325.878361874
Epoch: 0006 cost= 3866.805511076
Epoch: 0007 cost= 1357.785540469
Epoch: 0008 cost= 519.381747333
Epoch: 0009 cost= 225.359804119
Epoch: 0010 cost= 110.099476707
Epoch: 0011 cost= 55.212384386
Epoch: 0012 cost= 28.469241683
Epoch: 0013 cost= 14.511494627
Epoch: 0014 cost= 6.567228943
Epoch: 0015 cost= 3.186372240
Epoch: 0016 cost= 1.701917576
Epoch: 0017 cost= 1.041632473
Epoch: 0018 cost= 0.843376874
Epoch: 0019 cost= 0.786183911
Epoch: 0020 cost= 0.775412846
Epoch: 0021 cost= 0.782965020
Epoch: 0022 cost= 0.796788171
Epoch: 0023 cost= 0.814522117
Epoch: 0024 cost= 0.832090579
Epoch: 0025 cost= 0.849197715
Epoch: 0026 cost= 0.867473578
Epoch: 0027 cost= 0.889561496
Epoch: 0028 cost= 0.921837020
Epoch: 0029 cost= 16.655304543
Epoch: 0030 cost= 1.421570476
Optimization Finished!
Test data accuracy: 0.8775
Valid data accuracy: 0.8069
</code></pre>

<p>How can I apply DropOut by Tensorflow to improve the accuracy of the network? Thank you!</p>
","Drop-Out is regularization techniques. And I want to apply it to notMNIST data to reduce over-fitting to finish my Udacity Deep Learning Course Assignment.I have read the docs of tensorflow on how to call the tf.nn.dropout. And here is my code The tf.nn.dropout is called in function model(), but after I applied the DropOut technique to the neural network, the accuracy did seem any change, here is the result: How can I apply DropOut by Tensorflow to improve the accuracy of the network? Thank you!",https://stackoverflow.com/questions/40879504,5046896,Documentation Replication on Other Examples
41125183,Tensorflow: split_v with variable num_splits,"<p>I am wondering if the same holds for <code>tf.split_v()</code> as <code>tf.split()</code>.</p>

<p>According to the documentation <code>split_v</code> also accepts a Tensor as second argument.</p>

<p>However, when I try this code</p>

<pre><code>batch_size_ph = tf.placeholder(dtype = tf.int32, name='BatchSize')
seq_length_ph = tf.placeholder(dtype = tf.int32, name='SeqLength')
input_data = tf.placeholder(dtype=tf.float32, shape=[50, 25, 10])


inputs = tf.split_v(input_data,seq_length_ph, 1) #tf.ones(seq_length_ph, tf.int32)
#inputs = [tf.squeeze(input_, [1]) for input_ in inputs]

with tf.Session() as sess:
    [out] = sess.run([inputs],feed_dict = {batch_size_ph: 50,
                                           seq_length_ph: 25,
                                           input_data: np.random.rand(50,25,10)})

print out
print len(out)
print out[0].shape
</code></pre>

<p>The error is</p>

<blockquote>
  <p>NameError: global name 'value_shape' is not defined</p>
</blockquote>

<p>Is this possible or not?</p>
","I am wondering if the same holds for tf.split_v() as tf.split(). According to the documentation split_v also accepts a Tensor as second argument. However, when I try this code The error is Is this possible or not?",https://stackoverflow.com/questions/41125183,987397,Documentation Replicability
41283115,"Tensorflow, difference between tf.nn.softmax_cross_entropy_with_logits and tf.nn.sparse_softmax_cross_entropy_with_logits","<p>I have read the <a href=""https://www.tensorflow.org/api_docs/python/nn/classification"" rel=""nofollow noreferrer"">docs of both functions</a>, but as far as I know, for function <code>tf.nn.softmax_cross_entropy_with_logits(logits, labels, dim=-1, name=None)</code>, the result is the cross entropy loss, in which the dimensions of <code>logits</code> and <code>labels</code> are the same.</p>

<p>But, for function <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>, the dimensions of <code>logits</code> and <code>labels</code> are not the same?</p>

<p>Could you give a more detail example of <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>?</p>
","I have read the docs of both functions, but as far as I know, for function tf.nn.softmax_cross_entropy_with_logits(logits, labels, dim=-1, name=None), the result is the cross entropy loss, in which the dimensions of logits and labels are the same. But, for function tf.nn.sparse_softmax_cross_entropy_with_logits, the dimensions of logits and labels are not the same? Could you give a more detail example of tf.nn.sparse_softmax_cross_entropy_with_logits?",https://stackoverflow.com/questions/41283115,5046896,Requesting (Additional) Resources
41353079,Tensorflow image.central_crop (mis)behavior,"<p>In the Tensorflow documentation for <a href=""https://www.tensorflow.org/api_docs/python/image/cropping#central_crop"" rel=""nofollow noreferrer"">tf.image.central_crop</a> function:</p>

<pre><code>Remove the outer parts of an image but retain the central region of
the image along each dimension. If we specify central_fraction = 0.5,
this function returns the region marked with ""X"" in the below diagram.

 --------
|        |
|  XXXX  |
|  XXXX  |
|        |   where ""X"" is the central 50% of the image.
 --------
</code></pre>

<p>Consider the following code:</p>

<pre><code>In [2]: import tensorflow as tf
In [3]: image_raw = tf.placeholder(tf.string)
In [4]: image = tf.image.decode_jpeg(image_raw, channels=3)
In [5]: crop = tf.image.central_crop(image, central_fraction=0.5)
In [6]: init_op = tf.global_variables_initializer()
In [7]: sess = tf.Session()
In [8]: sess.run(init_op)
In [9]: image_np, crop_np = sess.run([image, crop],
   ...:     feed_dict={image_raw: open(""/tmp/test.jpg"", 'rb').read()})
In [10]: image_np.shape
Out[10]: (456, 450, 3)
</code></pre>

<p>Original image size is 456x450</p>

<pre><code>In [11]: crop_np.shape
Out[11]: (228, 226, 3)
</code></pre>

<p>Crop size is 228x226</p>

<p>Which gives area ratio of:</p>

<pre><code>In [12]: 228*226 / (456*450.)
Out[12]: 0.2511111111111111
</code></pre>

<p>Not 0.5 as I expected. Can someone help to clarify this?</p>
",In the Tensorflow documentation for tf.image.central_crop function: Consider the following code: Original image size is 456x450 Crop size is 228x226 Which gives area ratio of: Not 0.5 as I expected. Can someone help to clarify this?,https://stackoverflow.com/questions/41353079,6996238,Documentation Ambiguity
41437483,How does tf.nn.conv2d calculate its values?,"<p>I've looked at the <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn/convolution"" rel=""nofollow noreferrer"">documentation</a> for <code>tf.nn.conv2d</code> but it didn't really help much. So I tried to multiply the first 4 values of my input array that form a square <code>(0, 1, 2, 2.5)</code> with the first column of the <code>filter_weights</code> array <code>(0.19041163, -0.36705261, 0.69018674, 1.7655524)</code>. But regardless of how I multiply these values I'm not getting <code>1.13938534</code>, I don't know what I'm doing wrong. </p>

<p>Below I have the code that I used.</p>

<p>Given an array:</p>

<pre><code>x = np.array([
[0, 1, 0.5, 10],
[2, 2.5, 1, -8],
[4, 0, 5, 6],
[15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))

X = tf.constant(x)
</code></pre>

<p>And weights:</p>

<pre><code>filter_weights = tf.truncated_normal([2,2,1,3])
</code></pre>

<p>Which prints:</p>

<pre><code>[[[[ 0.19041163  0.59322315  0.27544078]]

  [[-0.36705261 -1.18738699  1.45393717]]]


 [[[ 0.69018674  1.08702338 -1.15911126]]

  [[ 1.76255524 -1.42660797 -0.18624328]]]]
</code></pre>

<p>How does:</p>

<pre><code>strides = [1, 2, 2, 1]
padding = ""SAME""
tf.nn.conv2d(input, F_W, strides, padding)
</code></pre>

<p>Give:</p>

<pre><code> [[[[  1.13938534  -5.06340981  -4.23629761]
   [-18.01530266  -4.67841816  16.20156288]]

  [[ -9.49576092  -4.87288237   6.77371216]
   [ -0.57718992 -12.70932484   0.89242649]]]]
</code></pre>
","I've looked at the documentation for tf.nn.conv2d but it didn't really help much. So I tried to multiply the first 4 values of my input array that form a square (0, 1, 2, 2.5) with the first column of the filter_weights array (0.19041163, -0.36705261, 0.69018674, 1.7655524). But regardless of how I multiply these values I'm not getting 1.13938534, I don't know what I'm doing wrong. Below I have the code that I used. Given an array: And weights: Which prints: How does: Give:",https://stackoverflow.com/questions/41437483,3529361,Lack of Alternative Solutions/Documentation
41467115,"Using word2vec pretrained vectors, how to generate ids of a sentence as input to tf.nn.embedding_lookup function in tensorflow?","<p>To extract the embedding representations of input data, the tensorflow documentation says we can use the following:</p>

<pre><code>embed = tf.nn.embedding_lookup(embeddings, input_data)
</code></pre>

<p>Accdg to the <a href=""https://www.tensorflow.org/api_docs/python/nn/embeddings#embedding_lookup"" rel=""nofollow noreferrer"">TF documentation</a>, the 2nd parameter of the function tf.nn.embedding_lookup is a tensor of ids:</p>

<blockquote>
  <p>ids: A Tensor with type int32 or int64 containing the ids to be looked up in params.</p>
</blockquote>

<p>My question is: Given a sentence, say, </p>

<blockquote>
  <p>""Welcome to the world""</p>
</blockquote>

<p>how can I represent and transform it into <code>ids</code>? In the code below, how can I transform my sentence into <code>input_data</code>. </p>

<pre><code>from gensim import models
embedding_path = ""../embeddings/GoogleNews-vectors-negative300.bin""
w = models.Word2Vec.load_word2vec_format(embedding_path, binary=True)
X = w.syn0
W = tf.Variable(tf.constant(0.0, shape=X.shape),trainable=False, name=""W"")
embedding_placeholder = tf.placeholder(tf.float32, X.shape)
embedding_init = W.assign(embedding_placeholder)
embed = tf.nn.embedding_lookup(embedding_init, input_data)
sess = tf.Session()
sess.run(embed, feed_dict={embedding_placeholder: X})
</code></pre>
","To extract the embedding representations of input data, the tensorflow documentation says we can use the following: Accdg to the TF documentation, the 2nd parameter of the function tf.nn.embedding_lookup is a tensor of ids: My question is: Given a sentence, say, how can I represent and transform it into ids? In the code below, how can I transform my sentence into input_data.",https://stackoverflow.com/questions/41467115,3009947,Documentation Replication on Other Examples
41602374,tf.zeros doesn't return a 1D tensor?,"<p>I'm trying to duplicate a tensor across a new axis, like this:</p>

<pre><code>original_tensor = tf.constant([1,2,3,4,5])
made_copies_tensor = tf.tile(original_tensor, 5)
final_result = tf.reshape([5,5])
</code></pre>

<p>However I'm getting this error:</p>

<pre><code>File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 650, in with_rank
raise ValueError(""Shape %s must have rank %d"" % (self, rank))
ValueError: Shape () must have rank 1
</code></pre>

<p>In the documentation it says the way I wrote tf.constant is supposed to have it return a 1D tensor but when I checked its shape with get_shape(), it has (5,) as its shape. I tried reshaping it but nothing changed.</p>

<p>Why am I getting this error? Thanks.</p>
","I'm trying to duplicate a tensor across a new axis, like this: However I'm getting this error: In the documentation it says the way I wrote tf.constant is supposed to have it return a 1D tensor but when I checked its shape with get_shape(), it has (5,) as its shape. I tried reshaping it but nothing changed. Why am I getting this error? Thanks.",https://stackoverflow.com/questions/41602374,3873000,Documentation Replicability
41673889,TensorFlow: does tf.train.batch automatically load the next batch when the batch has finished training?,"<p>For instance, after I have created my operations, fed the batch data through the operation and run the operation, does tf.train.batch automatically feed in another batch of data to the session?</p>

<p>I ask this because tf.train.batch has an attribute of <code>allow_smaller_final_batch</code> which makes it possible for the final batch to be loaded as a size lesser than the indicated batch size. Does this mean even without a loop, the next batch could be automatically fed? From the tutorial codes I am rather confused. When I load a single batch, I get literally a single batch size of shape [batch_size, height, width, num_channels], but the <a href=""https://www.tensorflow.org/api_docs/python/io_ops/input_pipeline#batch"" rel=""noreferrer"">documentation</a> says it <code>Creates batches of tensors in tensors.</code> Also, when I read the tutorial code in the <a href=""https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb"" rel=""noreferrer"">tf-slim walkthrough tutorial</a>, where there is a function called load_batch, there are only 3 tensors returned: <code>images, images_raw, labels</code>. Where are 'batches' of data as explained in the documentation?</p>

<p>Thank you for your help.</p>
","For instance, after I have created my operations, fed the batch data through the operation and run the operation, does tf.train.batch automatically feed in another batch of data to the session? I ask this because tf.train.batch has an attribute of allow_smaller_final_batch which makes it possible for the final batch to be loaded as a size lesser than the indicated batch size. Does this mean even without a loop, the next batch could be automatically fed? From the tutorial codes I am rather confused. When I load a single batch, I get literally a single batch size of shape [batch_size, height, width, num_channels], but the documentation says it Creates batches of tensors in tensors. Also, when I read the tutorial code in the tf-slim walkthrough tutorial, where there is a function called load_batch, there are only 3 tensors returned: images, images_raw, labels. Where are 'batches' of data as explained in the documentation? Thank you for your help.",https://stackoverflow.com/questions/41673889,5107084,Lack of Alternative Solutions/Documentation
41780655,What is the difference between tf.group and tf.control_dependencies?,"<p>Aside from <code>tf.control_dependencies</code> being a context manager (i.e. used with Python <code>with</code>), what's the difference between <code>tf.group</code> and <code>tf.control_dependencies</code>? </p>

<p>When should which be used? </p>

<p>Is it that <code>tf.group</code> doesn't have any particular order of operations? I'd assume <code>tf.group([op_1, op_2, op_3])</code> executes ops in the list's order, but maybe that's not the case? The docstring doesn't specify a behaviour.</p>
","Aside from tf.control_dependencies being a context manager (i.e. used with Python with), what's the difference between tf.group and tf.control_dependencies? When should which be used? Is it that tf.group doesn't have any particular order of operations? I'd assume tf.group([op_1, op_2, op_3]) executes ops in the list's order, but maybe that's not the case? The docstring doesn't specify a behaviour.",https://stackoverflow.com/questions/41780655,7287271,Documentation Replicability
41789133,What are c_state and m_state in Tensorflow LSTM?,"<p>Tensorflow r0.12's documentation for tf.nn.rnn_cell.LSTMCell describes this as the init:</p>

<pre><code>tf.nn.rnn_cell.LSTMCell.__call__(inputs, state, scope=None)
</code></pre>

<p>where <code>state</code> is as follows:</p>

<blockquote>
  <p>state: if state_is_tuple is False, this must be a state Tensor, 2-D, batch x state_size. If state_is_tuple is True, this must be a tuple of state Tensors, both 2-D, with column sizes c_state and m_state.</p>
</blockquote>

<p>What aare <code>c_state</code> and <code>m_state</code> and how do they fit into LSTMs? I cannot find reference to them anywhere in the documentation.</p>

<p><a href=""https://web.archive.org/web/20170223030652/https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods"" rel=""noreferrer"">Here is a link to that page in the documentation.</a></p>
",Tensorflow r0.12's documentation for tf.nn.rnn_cell.LSTMCell describes this as the init: where state is as follows: What aare c_state and m_state and how do they fit into LSTMs? I cannot find reference to them anywhere in the documentation. Here is a link to that page in the documentation.,https://stackoverflow.com/questions/41789133,5299052,Lack of Alternative Solutions/Documentation
41941940,TensorFlow: Understanding the `collections` argument in tf.summary.scalar,"<p>I am working with TensorBoard, specifically <code>tf.summary.scalar</code>. In the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/summary.md#tfsummaryscalarname-tensor-collectionsnone-scalar"" rel=""nofollow noreferrer"">documentation</a> it has an arugment <code>collections=None</code>, which is described as:</p>

<blockquote>
  <p><code>collections</code>: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to <code>[GraphKeys.SUMMARIES]</code>.</p>
</blockquote>

<p>I don't understand this description, and what <code>collections</code> is used for. Can someone please explain this to me, and perhaps point me towards a good example use-case?</p>
","I am working with TensorBoard, specifically tf.summary.scalar. In the documentation it has an arugment collections=None, which is described as: I don't understand this description, and what collections is used for. Can someone please explain this to me, and perhaps point me towards a good example use-case?",https://stackoverflow.com/questions/41941940,3747801,Documentation Ambiguity
42022950,Which seeds have to be set where to realize 100% reproducibility of training results in tensorflow?,"<p>In a general tensorflow setup like</p>

<pre><code>model = construct_model()
with tf.Session() as sess:
    train_model(sess)
</code></pre>

<p>Where <code>construct_model()</code> contains the model definition including random initialization of weights (<code>tf.truncated_normal</code>) and <code>train_model(sess)</code> executes the training of the model -</p>

<p>Which seeds do I have to set where to ensure 100% reproducibility between repeated runs of the code snippet above? <a href=""https://www.tensorflow.org/api_docs/python/tf/random/set_random_seed"" rel=""nofollow noreferrer"">The documentation</a> for <code>tf.random.set_random_seed</code> may be concise, but left me a bit confused. I tried:</p>

<pre><code>tf.set_random_seed(1234)
model = construct_model()
    with tf.Session() as sess:
        train_model(sess)
</code></pre>

<p>But got different results each time. </p>
","In a general tensorflow setup like Where construct_model() contains the model definition including random initialization of weights (tf.truncated_normal) and train_model(sess) executes the training of the model - Which seeds do I have to set where to ensure 100% reproducibility between repeated runs of the code snippet above? The documentation for tf.random.set_random_seed may be concise, but left me a bit confused. I tried: But got different results each time.",https://stackoverflow.com/questions/42022950,1934212,Documentation Replicability
42133661,Tensorflow - LSTM state reuse within batch,"<p>I am working on a Tensorflow NN which uses an LSTM to track a parameter (time series data regression problem). A batch of training data contains a batch_size of <em>consecutive</em> observations. I would like to use the LSTM state as input to the next sample. So, if I have a batch of data observations, I would like to feed the state of the first observation as input to the second observation and so on. Below I define the lstm state as a tensor of size = batch_size. I would like to reuse the state <em>within</em> a batch:</p>

<pre><code>state = tf.Variable(cell.zero_states(batch_size, tf.float32), trainable=False)
cell = tf.nn.rnn_cell.BasicLSTMCell(100)
output, curr_state = tf.nn.rnn(cell, data, initial_state=state) 
</code></pre>

<p>In the API there is a tf.nn.state_saving_rnn but the documentation is kinda vague. <strong>My question</strong>: How to reuse curr_state <em>within</em> a training batch.</p>
","I am working on a Tensorflow NN which uses an LSTM to track a parameter (time series data regression problem). A batch of training data contains a batch_size of consecutive observations. I would like to use the LSTM state as input to the next sample. So, if I have a batch of data observations, I would like to feed the state of the first observation as input to the second observation and so on. Below I define the lstm state as a tensor of size = batch_size. I would like to reuse the state within a batch: In the API there is a tf.nn.state_saving_rnn but the documentation is kinda vague. My question: How to reuse curr_state within a training batch.",https://stackoverflow.com/questions/42133661,2061800,Documentation Ambiguity
42333101,Predicting Next Word of LSTM Model from Tensorflow Example,"<p>My buddy and I are trying to utilize the trained model from the LSTM tensorflow example <a href=""https://www.tensorflow.org/tutorials/recurrent"" rel=""nofollow noreferrer"">here</a>. We've been able to train our model, save the model, and then import the model. We've just used tensorflow's Supervisor. It was in the tutorial, but you can read more about it <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard6/tf.train.Supervisor.md"" rel=""nofollow noreferrer"">here</a>. </p>

<p>It's weird because there's not a whole lot of clear documentation for this. I understand that tensorflow is an API that's going through a lot of changes and adaptations right now, but it's hard to find clear answers. For example, we want to use <code>tf.train.Saver()</code>, but we aren't sure if there is anything comparable to <code>tf.train.Supervisor()</code>'s <code>managed_session</code>. </p>

<p>More to the point, however, we <em>just want to use our model</em>. We want to be able to map a string using <code>tensorflow.models.rnn.ptb.reader</code>. We're not sure how to do this. We pass in a string, and we want to do a simple prediction in terms of like predicting the next word in a string. So, something similar to this:</p>

<pre><code>import tensorflow as tf
sess = tf.Session()
new_saver = tf.train.import_meta_graph('ptbmodel.meta')
new_saver.restore(sess, tf.train.latest_checkpoint('./')) # latest checkpoint
all_vars = tf.global_variables()
# just want to make sure all our variables are there!
for v in all_vars:
    v_ = sess.run(v)
    print(""This is {} with value: {}"".format(v.name, v_))


sent = raw_input(""Enter a string where you want to predict the next word: "")
split_sent = sent.split()
# THEN map these words into our LSTM model and pull off the most likely word that 
# is coming next
</code></pre>

<p>But again, my buddy and I are pretty new to this, so we're not sure about where to go. I know this is probably too broad of a question for stack, but we've been pouring over the documentation and haven't been able to make much progress. <strong>ANY</strong> help would be appreciated so much! </p>

<p>We've already found these other Stack links. Check them out <a href=""https://stackoverflow.com/questions/36286594/predicting-the-next-word-using-the-lstm-ptb-model-tensorflow-example"">here</a> and <a href=""https://stackoverflow.com/questions/33773661/predicting-next-word-using-the-language-model-tensorflow-example"">here</a>.</p>

<p>We are not sure how to associate the <code>logits</code> probability list with any meaningful words.</p>
","My buddy and I are trying to utilize the trained model from the LSTM tensorflow example here. We've been able to train our model, save the model, and then import the model. We've just used tensorflow's Supervisor. It was in the tutorial, but you can read more about it here. It's weird because there's not a whole lot of clear documentation for this. I understand that tensorflow is an API that's going through a lot of changes and adaptations right now, but it's hard to find clear answers. For example, we want to use tf.train.Saver(), but we aren't sure if there is anything comparable to tf.train.Supervisor()'s managed_session. More to the point, however, we just want to use our model. We want to be able to map a string using tensorflow.models.rnn.ptb.reader. We're not sure how to do this. We pass in a string, and we want to do a simple prediction in terms of like predicting the next word in a string. So, something similar to this: But again, my buddy and I are pretty new to this, so we're not sure about where to go. I know this is probably too broad of a question for stack, but we've been pouring over the documentation and haven't been able to make much progress. ANY help would be appreciated so much! We've already found these other Stack links. Check them out here and here. We are not sure how to associate the logits probability list with any meaningful words.",https://stackoverflow.com/questions/42333101,6347839,Documentation Replication on Other Examples
42334855,state output from tf.nn.dynamic_rnn operation,"<p>For this code snippet:</p>

<pre><code>rnn_cell = tf.contrib.rnn.BasicRNNCell(config.hidden_size, activation=tf.tanh)
initial_state = rnn_cell.zero_state(config.batch_size, tf.float32)
rnn_out, state = tf.nn.dynamic_rnn(rnn_cell, embed_out, initial_state=initial_state)
</code></pre>

<p>I was expected the last time index from rnn_out to be equal to state. Or, perhaps the tanh of the state. But this isn't what I am seeing - they don't match. In the context of this RNN recurrence relation, what value does state contain?</p>

<p>h(t) = tanh[b + W<em>h(t-1) + U</em>x(t)]</p>

<p>The answer here, implies the last time index of rnn_out and state should be equal (but they are not):</p>

<p><a href=""https://stackoverflow.com/questions/40384791/for-the-tf-nn-rnn-cell-basicrnn-whats-the-difference-between-the-state-and-outp"">for the tf.nn.rnn_cell.BasicRNN,what&#39;s the difference between the state and output</a></p>

<p>The TF documentation isn't clear to me on this point.</p>
","For this code snippet: I was expected the last time index from rnn_out to be equal to state. Or, perhaps the tanh of the state. But this isn't what I am seeing - they don't match. In the context of this RNN recurrence relation, what value does state contain? h(t) = tanh[b + Wh(t-1) + Ux(t)] The answer here, implies the last time index of rnn_out and state should be equal (but they are not): for the tf.nn.rnn_cell.BasicRNN,what's the difference between the state and output The TF documentation isn't clear to me on this point.",https://stackoverflow.com/questions/42334855,7590388,Documentation Ambiguity
42338981,It seems inconsistent the ways tensorflow allows me to specify variable length dimension,"<p>I'm a novice to tensorflow. I was practicing coding with this <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py"" rel=""nofollow noreferrer"">tutorial code</a>. Most of all the code made sense to me but at some points I got stuck.</p>

<pre><code>import tensorflow as tf
x = tf.placeholder(""float"", [None, n_steps, n_input])
x = tf.transpose(x, [1, 0, 2])
x = tf.reshape(x, [-1, n_input])
</code></pre>

<p>With <code>tf.placholder</code> function I had to specify variable length dimesion with <code>None</code>. But with <code>tf.reshape</code> I had to use <code>-1</code>, not <code>None</code>. In documentation for the two functions, both of the pertaining arguments have the name <code>shape</code>. So I am feeling lost here. Do they really have different meanings? Or is it just a small design mistake of the tensorflow developers?</p>
","I'm a novice to tensorflow. I was practicing coding with this tutorial code. Most of all the code made sense to me but at some points I got stuck. With tf.placholder function I had to specify variable length dimesion with None. But with tf.reshape I had to use -1, not None. In documentation for the two functions, both of the pertaining arguments have the name shape. So I am feeling lost here. Do they really have different meanings? Or is it just a small design mistake of the tensorflow developers?",https://stackoverflow.com/questions/42338981,,Documentation Ambiguity
42437115,"Tensorflow: Replacement for tf.nn.rnn_cell._linear(input, size, 0, scope)","<p>I am trying to get the SequenceGAN (<a href=""https://github.com/LantaoYu/SeqGAN"" rel=""noreferrer"">https://github.com/LantaoYu/SeqGAN</a>) from <a href=""https://arxiv.org/pdf/1609.05473.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1609.05473.pdf</a> to run.<br>
After fixing the obvious errors, like replacing <code>pack</code> with <code>stack</code>, it still doesn't run, since the highway-network part requires the <code>tf.nn.rnn_cell._linear</code> function:</p>

<pre><code># highway layer that borrowed from https://github.com/carpedm20/lstm-char-cnn-tensorflow
def highway(input_, size, layer_size=1, bias=-2, f=tf.nn.relu):
    """"""Highway Network (cf. http://arxiv.org/abs/1505.00387).

    t = sigmoid(Wy + b)
    z = t * g(Wy + b) + (1 - t) * y
    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.
    """"""
    output = input_
    for idx in range(layer_size):
        output = f(tf.nn.rnn_cell._linear(output, size, 0, scope='output_lin_%d' % idx)) #tf.contrib.layers.linear instad doesn't work either.
        transform_gate = tf.sigmoid(tf.nn.rnn_cell._linear(input_, size, 0, scope='transform_lin_%d' % idx) + bias)
        carry_gate = 1. - transform_gate

        output = transform_gate * output + carry_gate * input_

    return output
</code></pre>

<p>the <code>tf.nn.rnn_cell._linear</code> function doesn't appear to be there anymore in Tensorflow 1.0 or 0.12, and I have no clue what to replace it with. I can't find any new implementations of this, or any information on tensorflow's github or (unfortunately very sparse) documentation.</p>

<p>Does anybody know the new pendant of the function?
Thanks a lot in advance!</p>
","I am trying to get the SequenceGAN (https://github.com/LantaoYu/SeqGAN) from https://arxiv.org/pdf/1609.05473.pdf to run. After fixing the obvious errors, like replacing pack with stack, it still doesn't run, since the highway-network part requires the tf.nn.rnn_cell._linear function: the tf.nn.rnn_cell._linear function doesn't appear to be there anymore in Tensorflow 1.0 or 0.12, and I have no clue what to replace it with. I can't find any new implementations of this, or any information on tensorflow's github or (unfortunately very sparse) documentation. Does anybody know the new pendant of the function? Thanks a lot in advance!",https://stackoverflow.com/questions/42437115,5122790,Requesting (Additional) Resources
42675391,tf.nn.sigmoid_cross_entropy_with_logits companies about arguments from documentation,"<p>So I have the following model that I am wanting to test out an idea with. I am particularly interested in <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits"" rel=""nofollow noreferrer"">tf.nn.sigmoid_cross_entropy_with_logits()</a>  because my labels are not mutually exclusive. </p>

<pre><code>import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

x  = tf.placeholder(tf.float32, shape=[None, 784])
y_ = tf.placeholder(tf.float32, shape=[None, 10])

w1 = tf.get_variable(""w1"", shape=[784, 512], initializer=tf.contrib.layers.xavier_initializer())
b1 = tf.Variable(tf.zeros([512], dtype=tf.float32))
w2 = tf.Variable(tf.zeros([512, 10], dtype=tf.float32))
b2 = tf.Variable(tf.zeros([10], dtype=tf.float32))

h = tf.nn.relu(tf.matmul(x, w1) + b1)
y = tf.matmul(h, w2) + b2

cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y)
train_step = tf.train.AdamOptimizer().minimize(cross_entropy)

with tf.Session() as sess:

    sess.run(tf.initialize_all_variables())
    start = time.time()

    for i in range(20000):
        batch = mnist.train.next_batch(50)
        train_step.run(feed_dict={x: batch[0], y_: batch[1]})
</code></pre>

<p>However, I am getting the following error repeatedly, which seems to be contradicting the tensor flow documentation. </p>

<pre><code>Traceback (most recent call last):
File ""mnist_test.py"", line 19, in &lt;module&gt;
cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_, logits=y)
TypeError: sigmoid_cross_entropy_with_logits() got an unexpected keyword argument 'labels'
</code></pre>

<p>Please help!!</p>
","So I have the following model that I am wanting to test out an idea with. I am particularly interested in tf.nn.sigmoid_cross_entropy_with_logits() because my labels are not mutually exclusive. However, I am getting the following error repeatedly, which seems to be contradicting the tensor flow documentation. Please help!!",https://stackoverflow.com/questions/42675391,4500078,Documentation Replicability
42695305,Accessing row in a 2-D tensor,"<p>I have the following code of an incredibly simple neural network (this code is actually an adaptation for an easy question):</p>

<pre><code>import numpy as np
import tensorflow as tf

with tf.device(""cpu:0""):
    sess = tf.InteractiveSession()
    nNodes = 3
    inputDim = 1

    rowIdxs = np.zeros([nNodes, nNodes])
    colIdxs = np.zeros([nNodes, nNodes])
    for rowIdx in range(nNodes):
        for colIdx in range(nNodes):
            rowIdxs[rowIdx, colIdx] = rowIdx
            colIdxs[rowIdx, colIdx] = colIdx

    rowIdxs = np.reshape(rowIdxs, [-1])
    colIdxs = np.reshape(colIdxs, [-1])

    # build a matrix with nNodes x nNodes elements
    # with each row i containing the distance from node i to all the other nodes
    distances = np.zeros([nNodes, nNodes])
    for i in range(nNodes):
        for j in range(nNodes):
            distances[i, j] = ((rowIdxs[i] - rowIdxs[j]) ** 2 + (colIdxs[i] - colIdxs[j]) ** 2)
    print('distances=', distances)

    # tensorflow constant from distances matrix
    distances_ = tf.constant(distances, dtype=tf.float32)

    # w corresponds to a weight matrix in a neural network
    w = tf.random_uniform((nNodes, inputDim), 0.0, 1.0)

    # x corresponds to the input to the network
    x = tf.random_uniform((1, inputDim), 0.0, 1.0)

    xx = tf.tile(x, [nNodes,1])
    print('w', w.shape)
    print('x', x.shape)
    print('xx', xx.shape)

    # differences between weights and input vector
    diff = tf.reduce_sum(tf.abs(tf.subtract(xx, w)), 1)
    print('diff.shape', diff.shape)


    # index of the best matching unit
    bmu = tf.arg_min(diff, 0)       

    # Now I need to access the distances from BMU to the other nodes
    slice = tf.slice(distances_, [bmu, 0], [bmu, -1])

    sess.run(tf.global_variables_initializer())
    sess.run(slice)
    print('slice=', slice.eval())
    print('diff', diff.eval())
    print('bmu=', bmu.eval())
</code></pre>

<p>Basically, given an input <code>x</code>, compare it to the weights <code>w</code> and choose the node <code>BMU</code> with the minimum differences.</p>

<p>I have several problems with that code:</p>

<p><strong>1. sometimes it works without errors sometimes it raises an exception.</strong> </p>

<p><strong>When it DOES NOT work</strong>, the output is this:</p>

<pre><code>distances= 
 [[ 0.  1.  4.]
 [ 1.  0.  1.]
 [ 4.  1.  0.]]

w (3, 1)
x (1, 1)
xx (3, 1)
diff.shape (3,)

InvalidArgumentError (see above for traceback): Expected size[0] in [0, 1], but got 2
 [[Node: Slice = Slice[Index=DT_INT64, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const, Slice/begin, Slice/size)]]
</code></pre>

<p>The full stack follows:</p>

<pre><code>Traceback (most recent call last):
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1022, in _do_call
    return fn(*args)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""D:\python\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 1], but got 2
     [[Node: Slice = Slice[Index=DT_INT64, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const, Slice/begin, Slice/size)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/workspace/LiClipse Workspace/kerasPython/exercises/testIndexing.py"", line 44, in &lt;module&gt;
    sess.run(slice)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 767, in run
    run_metadata_ptr)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 1], but got 2
     [[Node: Slice = Slice[Index=DT_INT64, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const, Slice/begin, Slice/size)]]

Caused by op 'Slice', defined at:
  File ""D:/workspace/LiClipse Workspace/kerasPython/exercises/testIndexing.py"", line 39, in &lt;module&gt;
    slice = tf.slice(distances_, [bmu, 0], [bmu, -1])
  File ""D:\python\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 561, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3053, in _slice
    name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Expected size[0] in [0, 1], but got 2
     [[Node: Slice = Slice[Index=DT_INT64, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const, Slice/begin, Slice/size)]]
</code></pre>

<p><strong>When it works</strong></p>

<pre><code>w (3, 1)
x (1, 1)
xx (3, 1)
diff.shape (3,)
slice= [[ 1.  0.  1.]]
diff [ 0.29777944  0.08669317  0.09722018]
bmu= 0
</code></pre>

<p><code>bmu</code> is wrong, it should be <code>1</code>, but the slice is correct.</p>

<p>Sometimes I get this:</p>

<pre><code>w (3, 1)
x (1, 1)
xx (3, 1)
diff.shape (3,)
slice= []
diff [ 0.33319855  0.12426794  0.49753141]
bmu= 1
</code></pre>

<p><code>bmu</code> is 1, but slice is empty.</p>

<p><strong>2. When I switch to the GPU, I have an exception telling me I cannot use <code>bmu</code> for indexing.</strong>
Starting with <code>with tf.device(""gpu:0""):</code>, I get this:</p>

<pre><code>InvalidArgumentError (see above for traceback): Cannot assign a device to node 'Slice/size': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: Slice/size = Pack[N=2, T=DT_INT64, axis=0, _device=""/device:GPU:0""](ArgMin, Slice/size/1)]]
</code></pre>

<p>The full stack trace follows:</p>

<pre><code>Traceback (most recent call last):
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1022, in _do_call
    return fn(*args)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""D:\python\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'Slice/size': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: Slice/size = Pack[N=2, T=DT_INT64, axis=0, _device=""/device:GPU:0""](ArgMin, Slice/size/1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/workspace/LiClipse Workspace/kerasPython/exercises/testIndexing.py"", line 45, in &lt;module&gt;
    sess.run(slice)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 767, in run
    run_metadata_ptr)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'Slice/size': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: Slice/size = Pack[N=2, T=DT_INT64, axis=0, _device=""/device:GPU:0""](ArgMin, Slice/size/1)]]

Caused by op 'Slice/size', defined at:
  File ""D:/workspace/LiClipse Workspace/kerasPython/exercises/testIndexing.py"", line 40, in &lt;module&gt;
    slice = tf.slice(distances_, [bmu, 0], [bmu, -1])
  File ""D:\python\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 561, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3053, in _slice
    name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 491, in apply_op
    preferred_dtype=default_dtype)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""D:\python\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 923, in _autopacking_conversion_function
    return _autopacking_helper(v, inferred_dtype, name or ""packed"")
  File ""D:\python\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 886, in _autopacking_helper
    return gen_array_ops._pack(elems_as_tensors, name=scope)
  File ""D:\python\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 2041, in _pack
    result = _op_def_lib.apply_op(""Pack"", values=values, axis=axis, name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device to node 'Slice/size': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: Slice/size = Pack[N=2, T=DT_INT64, axis=0, _device=""/device:GPU:0""](ArgMin, Slice/size/1)]]
</code></pre>

<p>I cannot understand what's happening: I have an idea, but cannot find any reference in the documentation or anywhere else. May be I use the wrong keywords.</p>

<p>Is there anyone who can help me?</p>
","I have the following code of an incredibly simple neural network (this code is actually an adaptation for an easy question): Basically, given an input x, compare it to the weights w and choose the node BMU with the minimum differences. I have several problems with that code: 1. sometimes it works without errors sometimes it raises an exception. When it DOES NOT work, the output is this: The full stack follows: When it works bmu is wrong, it should be 1, but the slice is correct. Sometimes I get this: bmu is 1, but slice is empty. 2. When I switch to the GPU, I have an exception telling me I cannot use bmu for indexing. Starting with with tf.device(""gpu:0""):, I get this: The full stack trace follows: I cannot understand what's happening: I have an idea, but cannot find any reference in the documentation or anywhere else. May be I use the wrong keywords. Is there anyone who can help me?",https://stackoverflow.com/questions/42695305,774133,Lack of Alternative Solutions/Documentation
42754259,Sampled softmax loss over variable sequence batches?,"<p><strong>Background info</strong>: I'm working on sequence-to-sequence models, and right now my model accepts variable-length input tensors (not lists) with input shapes corresponding to [batch size, sequence length]. However, in my implementation, sequence length is <em>unspecified</em> (set to None) to allow for variable length inputs. Specifically, input sequence batches are padded only to the length of the longest sequence in that batch. This has sped up my training time considerably, so I'd prefer to keep it this way, as opposed to going back to bucketed models and/or padded all sequences in the training data to the same length. I'm using TensorFlow 1.0.0.</p>

<p><strong>Problem</strong>: I'm currently using the following to compute the loss (which runs just fine).</p>

<pre><code>loss = tf.losses.sparse_softmax_cross_entropy(
    weights=target_labels,  # shape: [batch size, None]
    logits=outputs[:, :-1, :], # shape: [batch size, None, vocab size]
    weights=target_weights[:, :-1]) # shape: [batch size, None]
</code></pre>

<p>where vocab size is typically about 40,000. I'd like to use a sampled softmax, but I've ran into an issue that's due to the unspecified nature of the input shape. According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer"">documentation for tf.nn.sampled_softmax_loss</a>, <strong>it requires the inputs to be fed separately for each timestep</strong>. However, I can't call, for example,  </p>

<pre><code>tf.unstack(target_labels, axis=1)
</code></pre>

<p>since the axis is unknown beforehand.Does anyone know how I might go about implementing this? One would assume that since both dynamic_rnn and <a href=""https://www.tensorflow.org/api_docs/python/tf/losses/sparse_softmax_cross_entropy"" rel=""nofollow noreferrer"">tf.losses.sparse_softmax_cross_entropy</a> seem to have no issue doing this, that a workaround could be implemented with the sampled softmax loss somehow. After digging around in the source code and even models repository, I've come up empty handed. Any help/suggestions would be greatly appreciated.</p>
","Background info: I'm working on sequence-to-sequence models, and right now my model accepts variable-length input tensors (not lists) with input shapes corresponding to [batch size, sequence length]. However, in my implementation, sequence length is unspecified (set to None) to allow for variable length inputs. Specifically, input sequence batches are padded only to the length of the longest sequence in that batch. This has sped up my training time considerably, so I'd prefer to keep it this way, as opposed to going back to bucketed models and/or padded all sequences in the training data to the same length. I'm using TensorFlow 1.0.0. Problem: I'm currently using the following to compute the loss (which runs just fine). where vocab size is typically about 40,000. I'd like to use a sampled softmax, but I've ran into an issue that's due to the unspecified nature of the input shape. According to the documentation for tf.nn.sampled_softmax_loss, it requires the inputs to be fed separately for each timestep. However, I can't call, for example, since the axis is unknown beforehand.Does anyone know how I might go about implementing this? One would assume that since both dynamic_rnn and tf.losses.sparse_softmax_cross_entropy seem to have no issue doing this, that a workaround could be implemented with the sampled softmax loss somehow. After digging around in the source code and even models repository, I've come up empty handed. Any help/suggestions would be greatly appreciated.",https://stackoverflow.com/questions/42754259,6706630,Documentation Replicability
42773379,tf.nn.relu vs. tf.contrib.layers.relu?,"<p>I see this ""tf.nn.relu"" documented here: <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/relu"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/relu</a></p>

<p>But then I also see usage of tf.contrib.layers.relu on this page in ""model_fn"":
<a href=""https://www.tensorflow.org/extend/estimators"" rel=""nofollow noreferrer"">https://www.tensorflow.org/extend/estimators</a></p>

<p>It seems like the latter isn't described like the first one in an API-like fashion, but only presented in use.</p>

<p>Why is this? Are the docs out of date? Why have two - is one old and no longer supported/going to be removed?</p>
","I see this ""tf.nn.relu"" documented here: https://www.tensorflow.org/api_docs/python/tf/nn/relu But then I also see usage of tf.contrib.layers.relu on this page in ""model_fn"": https://www.tensorflow.org/extend/estimators It seems like the latter isn't described like the first one in an API-like fashion, but only presented in use. Why is this? Are the docs out of date? Why have two - is one old and no longer supported/going to be removed?",https://stackoverflow.com/questions/42773379,384137,Documentation Ambiguity
42785026,tf.nn.conv2d vs tf.layers.conv2d,"<p>Is there any advantage in using <code>tf.nn.*</code> over <code>tf.layers.*</code>?</p>

<p>Most of the examples in the doc use <code>tf.nn.conv2d</code>, for instance, but it is not clear why they do so.</p>
","Is there any advantage in using tf.nn.* over tf.layers.*? Most of the examples in the doc use tf.nn.conv2d, for instance, but it is not clear why they do so.",https://stackoverflow.com/questions/42785026,326849,Lack of Alternative Solutions/Documentation
42933599,Slice a tensor in half in tensorflow,"<p>I have a tensor of shape <code>(32, 32, 32, 1)</code> and I want to slice it into two tensors, along the first dimension, containing the first and second halves like so</p>

<pre><code>half1  with shape = (16, 32, 32, 1)
half2  with shape = (16, 32, 32, 1)
</code></pre>

<p>I am trying to use tf.slice but I don't know how to use the begin and end indices, and the documentation is anything but clear. </p>
","I have a tensor of shape (32, 32, 32, 1) and I want to slice it into two tensors, along the first dimension, containing the first and second halves like so I am trying to use tf.slice but I don't know how to use the begin and end indices, and the documentation is anything but clear.",https://stackoverflow.com/questions/42933599,5016028,Documentation Ambiguity
43175272,check if tensorflow placeholder is filled,"<p>Suppose I have two placeholder quantities in tensorflow: placeholder_1 and placeholder_2. Essentially I would like the following computational functionality: ""if placeholder_1 is defined (ie is given a value in the feed_dict of sess.run()), compute X as f(placeholder_1), otherwise, compute X as g(placeholder_2)."" Think of X as being a hidden layer in a neural network that can optionally be computed in these two different ways. Eventually I would use X to produce an output, and I'd like to backpropagate error to the parameters of f or g depending on which placeholder I used. </p>

<p>One could accomplish this using the tf.where(condition, x, y) function if there was a way to make the condition ""placeholder_1 has a value"", but after looking through the tensorflow documentation on booleans and asserts I couldn't find anything that looked applicable.</p>

<p>Any ideas? I have a vague idea of how I could accomplish this basically by copying part of the network, sharing parameters and syncing the networks after updates, but I'm hoping for a cleaner way to do it.</p>
","Suppose I have two placeholder quantities in tensorflow: placeholder_1 and placeholder_2. Essentially I would like the following computational functionality: ""if placeholder_1 is defined (ie is given a value in the feed_dict of sess.run()), compute X as f(placeholder_1), otherwise, compute X as g(placeholder_2)."" Think of X as being a hidden layer in a neural network that can optionally be computed in these two different ways. Eventually I would use X to produce an output, and I'd like to backpropagate error to the parameters of f or g depending on which placeholder I used. One could accomplish this using the tf.where(condition, x, y) function if there was a way to make the condition ""placeholder_1 has a value"", but after looking through the tensorflow documentation on booleans and asserts I couldn't find anything that looked applicable. Any ideas? I have a vague idea of how I could accomplish this basically by copying part of the network, sharing parameters and syncing the networks after updates, but I'm hoping for a cleaner way to do it.",https://stackoverflow.com/questions/43175272,4938706,Lack of Alternative Solutions/Documentation
43367697,Batching and shuffling padded tf.train.SequenceExample,"<p>I have some training example of a sequence-to-sequence scenario which are stored as <code>tf.train.SequenceExample</code> in one (or more) file(s) written <code>TFRecordWriter</code>. I would like to read, decode them and feed shuffled batches of them into my network. I have been struggling with the documentation and some tutorials found here and there but I could not make anything out of such stuff. I am working on a self-contained example, here below. </p>

<pre><code>import random

import tensorflow as tf

from six.moves import xrange


MIN_LEN = 6
MAX_LEN = 12
NUM_EXAMPLES = 20
BATCH_SIZE = 3
PATH = 'ciaone.tfrecords'
MIN_AFTER_DEQUEUE = 10
NUM_THREADS = 2
SAFETY_MARGIN = 1
CAPACITY = MIN_AFTER_DEQUEUE + (NUM_THREADS + SAFETY_MARGIN) * BATCH_SIZE


def generate_example():
    # fake examples which are just useful to have a quick visualization.
    # The input is a sequence of random numbers.
    # The output is a sequence made of those numbers from the
    # input sequence which are greater or equal then the average.
    length = random.randint(MIN_LEN, MAX_LEN)
    input_ = [random.randint(0, 10) for _ in xrange(length)]
    avg = sum([1.0 * item for item in input_]) / len(input_)
    output = [item for item in input_ if item &gt;= avg]
    return input_, output


def encode(input_, output):
    length = len(input_)
    example = tf.train.SequenceExample(
        context=tf.train.Features(
            feature={
                'length': tf.train.Feature(
                    int64_list=tf.train.Int64List(value=[length]))
            }),
        feature_lists=tf.train.FeatureLists(
            feature_list={
                'input': tf.train.FeatureList(
                    feature=[
                        tf.train.Feature(
                            int64_list=tf.train.Int64List(value=[item]))
                        for item in input_]),
                'output': tf.train.FeatureList(
                    feature=[
                        tf.train.Feature(
                            int64_list=tf.train.Int64List(value=[item]))
                        for item in output])
            }
        )
    )
    return example


def decode(example):
    context_features = {
        'length': tf.FixedLenFeature([], tf.int64)
    }
    sequence_features = {
        'input': tf.FixedLenSequenceFeature([], tf.int64),
        'output': tf.FixedLenSequenceFeature([], tf.int64)
    }
    ctx, seq = tf.parse_single_sequence_example(
        example, context_features, sequence_features)
    input_ = seq['input']
    output = seq['output']
    return input_, output

if __name__ == '__main__':
    # STEP 1. -- generate a dataset.
    with tf.python_io.TFRecordWriter(PATH) as writer:
        for _ in xrange(NUM_EXAMPLES):
           record = encode(*generate_example())
           writer.write(record.SerializeToString())

    with tf.Session() as sess:
        queue = tf.train.string_input_producer([PATH])
        reader = tf.TFRecordReader()
        _, value = reader.read(queue)
        input_, output = decode(value)

        # HERE I AM STUCK!

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        sess.run(tf.local_variables_initializer())
        sess.run(tf.global_variables_initializer())
        try:
            while True:
                # do something...
        except tf.errors.OutOfRangeError, e:
            coord.request_stop(e)
        finally:
            coord.request_stop()
            coord.join(threads)
        coord.request_stop()
        coord.join(threads)
</code></pre>

<p>Can anyone suggest me how to proceed?
Thanks in advance!</p>

<p>P.S. as a side request: any pointer about resources to better understand the input pipeline APIs of TensorFlow is appreciated.</p>
","I have some training example of a sequence-to-sequence scenario which are stored as tf.train.SequenceExample in one (or more) file(s) written TFRecordWriter. I would like to read, decode them and feed shuffled batches of them into my network. I have been struggling with the documentation and some tutorials found here and there but I could not make anything out of such stuff. I am working on a self-contained example, here below. Can anyone suggest me how to proceed? Thanks in advance! P.S. as a side request: any pointer about resources to better understand the input pipeline APIs of TensorFlow is appreciated.",https://stackoverflow.com/questions/43367697,1861627,Requesting (Additional) Resources
43422949,CTC Loss InvalidArgumentError: sequence_length(b) <= time,"<p>I am running into this error while trying to use tf.nn.ctc_loss through keras (ctc_batch_cost):</p>

<blockquote>
  <p>InvalidArgumentError (see above for traceback): sequence_length(4) &lt;= 471</p>
</blockquote>

<p>According to the documentation for tf.nn.ctc_loss, Input requirements are:</p>

<blockquote>
  <p>sequence_length(b) &lt;= time for all b</p>
  
  <p>max(labels.indices(labels.indices[:, 1] == b, 2))   &lt;=
  sequence_length(b) for all b.</p>
</blockquote>

<p>I am having a hard time understanding what this means-- what is <code>b</code> and what is <code>sequence_length(b)</code>? </p>
","I am running into this error while trying to use tf.nn.ctc_loss through keras (ctc_batch_cost): According to the documentation for tf.nn.ctc_loss, Input requirements are: I am having a hard time understanding what this means-- what is b and what is sequence_length(b)?",https://stackoverflow.com/questions/43422949,740857,Documentation Ambiguity
43460838,tensorflow tfrecord storage for large datasets,"<p>I'm trying to understand the ""proper"" method of storage for large datasets for tensorflow ingestion. The documentation seems relatively clear that no matter what, tfrecord files are preferred. Large is a subjective measure, but the examples below are randomly generated regression datasets from sklearn.datasets.make_regression() of 10,000 rows and between 1 and 5,000 features, all float64.</p>

<p>I've experimented with two different methods of writing tfrecord files with dramatically different performance.</p>

<p>For numpy arrays, <code>X</code>, <code>y</code> (X.shape=(10000, n_features), y.shape=(10000,)</p>

<h2><code>tf.train.Example</code> with per-feature <code>tf.train.Features</code></h2>

<p>I construct a tf.train.Example in the way that tensorflow developers seem to prefer, at least judging by tensorflow example code at <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py</a>. </p>

<p>For each observation or row in <code>X</code>, I create a dictionary keyed with feature names (f_0, f_1, ...) whose values are <code>tf.train.Feature</code> objects with the feature's observation data as a single element of its float_list.</p>

<pre><code>def _feature_dict_from_row(row):
    """"""
    Take row of length n+1 from 2-D ndarray and convert it to a dictionary of
    float features:

      {
        'f_0': row[0],
        'f_1': row[1],
        ...
        'f_n': row[n]
      }
    """"""
    def _float64_feature(feature):
        return tf.train.Feature(float_list=tf.train.FloatList(value=[feature]))

    features = { ""f_{:d}"".format(i): _float64_feature(value) for i, value in enumerate(row) }
    return features

def write_regression_data_to_tfrecord(X, y, filename):
    with tf.python_io.TFRecordWriter('{:s}'.format(filename)) as tfwriter:
        for row_index in range(X.shape[0]):
            features = _feature_dict_from_row(X[row_index])
            features['label'] = y[row_index]

            example = tf.train.Example(features=tf.train.Features(feature=features))
            tfwriter.write(example.SerializeToString())
</code></pre>

<h2><code>tf.train.Example</code> with one large <code>tf.train.Feature</code> containing all features</h2>

<p>I construct a dictionary with one feature (really two counting the label) whose value is a <code>tf.train.Feature</code> with the entire feature row in as its float_list</p>

<pre><code>def write_regression_data_to_tfrecord(X, y, filename, store_by_rows=True):
    with tf.python_io.TFRecordWriter('{:s}'.format(filename)) as tfwriter:
        for row_index in range(X.shape[0]):
            features = { 'f_0': tf.train.Feature(float_list=tf.train.FloatList(value=X[row_index])) }
            features['label'] = y[row_index]

            example = tf.train.Example(features=tf.train.Features(feature=features))
            tfwriter.write(example.SerializeToString())
</code></pre>

<p>As the number of features in the dataset grows, the second option gets considerably faster than the first, as shown in the following graph. <em>Note the log scale</em></p>

<p><strong>10,000 rows:</strong></p>

<p><img src=""https://i.stack.imgur.com/p4w9M.png"" alt=""graph""></p>

<p>It makes intuitive sense to me that creating 5,000 <code>tf.train.Feature</code> objects is significantly slower than creating one object with a float_list of 5,000 elements, but it's not clear that this is the ""intended"" method for feeding large numbers of features into a tensorflow model.</p>

<p>Is there something inherently wrong with doing this the faster way?</p>
","I'm trying to understand the ""proper"" method of storage for large datasets for tensorflow ingestion. The documentation seems relatively clear that no matter what, tfrecord files are preferred. Large is a subjective measure, but the examples below are randomly generated regression datasets from sklearn.datasets.make_regression() of 10,000 rows and between 1 and 5,000 features, all float64. I've experimented with two different methods of writing tfrecord files with dramatically different performance. For numpy arrays, X, y (X.shape=(10000, n_features), y.shape=(10000,) I construct a tf.train.Example in the way that tensorflow developers seem to prefer, at least judging by tensorflow example code at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py. For each observation or row in X, I create a dictionary keyed with feature names (f_0, f_1, ...) whose values are tf.train.Feature objects with the feature's observation data as a single element of its float_list. I construct a dictionary with one feature (really two counting the label) whose value is a tf.train.Feature with the entire feature row in as its float_list As the number of features in the dataset grows, the second option gets considerably faster than the first, as shown in the following graph. Note the log scale 10,000 rows: It makes intuitive sense to me that creating 5,000 tf.train.Feature objects is significantly slower than creating one object with a float_list of 5,000 elements, but it's not clear that this is the ""intended"" method for feeding large numbers of features into a tensorflow model. Is there something inherently wrong with doing this the faster way?",https://stackoverflow.com/questions/43460838,306537,Documentation Replication on Other Examples
43792961,Understanding the while loop in Tensorflow,"<p>I am using the <a href=""https://www.tensorflow.org/api_docs/python/"" rel=""noreferrer"">Python API for Tensorflow</a>. I am trying to implement the <a href=""https://www.sfu.ca/~ssurjano/rosen.html"" rel=""noreferrer"">Rosenbrock function</a> given below without the use of a Python loop:</p>

<p><a href=""https://i.stack.imgur.com/9AdOH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/9AdOH.png"" alt=""Rosenbrock function""></a></p>

<p>My current implementation is as follows:</p>

<pre><code>def rosenbrock(data_tensor):
    columns = tf.unstack(data_tensor)

    summation = 0
    for i in range(1, len(columns) - 1):
        first_term = tf.square(tf.subtract(columns[i + 1], tf.square(columns[i])))
        second_term = tf.square(tf.subtract(columns[i], 1.0))
        summation += tf.add(tf.multiply(100.0, first_term), second_term)

    return summation
</code></pre>

<p>I have tried implementing the summation in a <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""noreferrer""><code>tf.while_loop()</code></a>; however, I found the API somewhat unintuitive when it comes to using an index integer that is meant to remain separate from the data. The example given in the <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""noreferrer"">documentation</a> uses the data as the index (or vice-versa):</p>

<pre><code>i = tf.constant(0)
c = lambda i: tf.less(i, 10)
b = lambda i: tf.add(i, 1)
r = tf.while_loop(c, b, [i])
</code></pre>
","I am using the Python API for Tensorflow. I am trying to implement the Rosenbrock function given below without the use of a Python loop: My current implementation is as follows: I have tried implementing the summation in a tf.while_loop(); however, I found the API somewhat unintuitive when it comes to using an index integer that is meant to remain separate from the data. The example given in the documentation uses the data as the index (or vice-versa):",https://stackoverflow.com/questions/43792961,1309401,Documentation Replication on Other Examples
43827792,How do I use strided_slice to select all the element in tensorflow?,"<p>I read the examples in document:</p>

<pre><code># 'input' is [[[1, 1, 1], [2, 2, 2]],
#             [[3, 3, 3], [4, 4, 4]],
#             [[5, 5, 5], [6, 6, 6]]]
tf.strided_slice(input, [1, 0, 0], [2, 1, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3]]]
tf.strided_slice(input, [1, 0, 0], [2, 2, 3], [1, 1, 1]) ==&gt; [[[3, 3, 3],
                                                               [4, 4, 4]]]
tf.strided_slice(input, [1, -1, 0], [2, -3, 3], [1, -1, 1]) ==&gt;[[[4, 4, 4],
                                                                 [3, 3, 3]]] 
</code></pre>

<p>It seems like that I can not simply use <code>input[:,:]</code> to select all the element, instead I have to use the syntax like <code>input[:-1, :-1]</code>. However in this way <code>input[:-1, :-1]</code> , I will miss the last row or last column. What should I do?</p>

<p>I take an example:</p>

<pre><code>ph = tf.placeholder(shape=[None, 3], dtype=tf.int32)
x = tf.strided_slice(ph, [0,0],[-1,-1],[1,1])
input_ = np.array([[1,2,3],
                  [3,4,5],
                  [7,8,9]])
sess = tf.InteractiveSession()
sess.run(x,feed_dict={ph:input_})
</code></pre>

<p>output:</p>

<pre><code>array([[1, 2],
       [3, 4]])
</code></pre>

<p>I read a lot of material and I found that I can use <code>tf.shape(ph)</code>,let see:</p>

<pre><code>ph = tf.placeholder(shape=[None, 3], dtype=tf.int32)
x = tf.strided_slice(ph, [0,0],tf.shape(ph),[1,1])
input_ = np.array([[1,2,3],
                  [3,4,5],
                  [7,8,9]])
sess = tf.InteractiveSession()
sess.run(x,feed_dict={ph:input_})
</code></pre>

<p>out:</p>

<pre><code>array([[1, 2, 3],
       [3, 4, 5],
       [7, 8, 9]])
</code></pre>

<p>However, if I want to get the result like this:</p>

<pre><code>[[1, 2],
 [3, 4],
 [7, 8]]
</code></pre>

<p>What can I do?</p>
","I read the examples in document: It seems like that I can not simply use input[:,:] to select all the element, instead I have to use the syntax like input[:-1, :-1]. However in this way input[:-1, :-1] , I will miss the last row or last column. What should I do? I take an example: output: I read a lot of material and I found that I can use tf.shape(ph),let see: out: However, if I want to get the result like this: What can I do?",https://stackoverflow.com/questions/43827792,6080827,Documentation Replicability
43885770,Clarification of tf.name_scope in TensorFlow documentation,"<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/name_scope"" rel=""nofollow noreferrer"">TensorFlow documentation</a> mentions the following for <code>tf.name_scope</code></p>

<pre><code>This context manager validates that the given values are from the same
graph, makes that graph the default graph, and pushes a name scope in 
that graph.
</code></pre>

<p>What is the meaning of <code>given values are from the same graph, makes that graph the default graph</code> ? </p>

<p><code>Same graph</code> refers to which graph ?</p>

<p>Also, what is the use of <code>values</code> parameter in <code>tf.name_scope</code> ? </p>
","The TensorFlow documentation mentions the following for tf.name_scope What is the meaning of given values are from the same graph, makes that graph the default graph ? Same graph refers to which graph ? Also, what is the use of values parameter in tf.name_scope ?",https://stackoverflow.com/questions/43885770,6842947,Documentation Ambiguity
43916019,Control dependencies and order of evaluation,"<p>Please consider the following code: </p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

with tf.device(""gpu:0""):
    sess = tf.InteractiveSession()
    idx = tf.constant(0)
    # 10 iterations
    while_condition = lambda i: tf.less(i, tf.constant(10))        
    acc = tf.Variable(0, dtype=tf.float64)
    # the body of the while adds 1 to acc in each iteration
    def body_accumulator(i):
        mainOp = tf.assign_add(acc, 1.0)
        return tf.tuple([tf.add(i, 1)], control_inputs=[mainOp])
    whileOp = tf.while_loop(while_condition, body_accumulator, [idx])

    # My idea: return acc after evaluating whileOp, whose code modifies acc
    def f(dummy):
        with tf.control_dependencies([whileOp]):
            # with return tf.identity(acc) it works
            return acc
    def g():
        return acc

    sess.run(tf.global_variables_initializer())
    print('""g: return acc .eval()"" - this is the only time where I would expect 0')
    print(g().eval())
    print('f(dummy)')
    print(f(1).eval())
    print('whileOp.eval()')
    print(whileOp.eval())
    print('acc value:')
    print(acc.eval())
    print('""g: return acc .eval()""')
    print(g().eval())
</code></pre>

<p>The output is:</p>

<pre><code>""g: return acc .eval()"" - this is the only time where I would expect 0
0.0
f(dummy)
0.0
whileOp.eval()
10
acc value:
10.0
""g: return acc .eval()""
10.0
</code></pre>

<p>My question is:</p>

<p>why does <code>f(1).eval()</code> return 0 even if there is a control dependency on the <code>whileOp</code> that modifies the returned variable <code>acc</code>?</p>

<p>After reading the documentation, I was expecting <code>whileOp</code> to be evaluated before returning acc. How should I write the function <code>f(.)</code> in order to force the evaluation of <code>whileOp</code>?</p>

<p>In <code>f(.)</code>, if I return <code>tf.identity(acc)</code> instead of <code>acc</code>, it works as I expect.</p>
","Please consider the following code: The output is: My question is: why does f(1).eval() return 0 even if there is a control dependency on the whileOp that modifies the returned variable acc? After reading the documentation, I was expecting whileOp to be evaluated before returning acc. How should I write the function f(.) in order to force the evaluation of whileOp? In f(.), if I return tf.identity(acc) instead of acc, it works as I expect.",https://stackoverflow.com/questions/43916019,774133,Inadequate Examples
44093698,How does Tensorflow Batch Normalization work?,"<p>I'm using tensorflow batch normalization in my deep neural network successfully. I'm doing it the following way:</p>

<pre class=""lang-py prettyprint-override""><code>if apply_bn:
    with tf.variable_scope('bn'):
        beta = tf.Variable(tf.constant(0.0, shape=[out_size]), name='beta', trainable=True)
        gamma = tf.Variable(tf.constant(1.0, shape=[out_size]), name='gamma', trainable=True)
        batch_mean, batch_var = tf.nn.moments(z, [0], name='moments')
        ema = tf.train.ExponentialMovingAverage(decay=0.5)

        def mean_var_with_update():
            ema_apply_op = ema.apply([batch_mean, batch_var])
            with tf.control_dependencies([ema_apply_op]):
                return tf.identity(batch_mean), tf.identity(batch_var)

        mean, var = tf.cond(self.phase_train,
                            mean_var_with_update,
                            lambda: (ema.average(batch_mean), ema.average(batch_var)))

        self.z_prebn.append(z)
        z = tf.nn.batch_normalization(z, mean, var, beta, gamma, 1e-3)
        self.z.append(z)

        self.bn.append((mean, var, beta, gamma))
</code></pre>

<p>And it works fine both for training and testing phases.
However I encounter problems when I try to use the computed neural network parameters in my another project, where I need to compute all the matrix multiplications and stuff by myself. The problem is that I can't reproduce the behavior of the <code>tf.nn.batch_normalization</code> function:</p>

<pre class=""lang-py prettyprint-override""><code>feed_dict = {
    self.tf_x: np.array([range(self.x_cnt)]) / 100, 
    self.keep_prob: 1,
    self.phase_train: False
}

for i in range(len(self.z)):
    # print 0 layer's 1 value of arrays
    print(self.sess.run([
        self.z_prebn[i][0][1], # before bn
        self.bn[i][0][1],      # mean
        self.bn[i][1][1],      # var
        self.bn[i][2][1],      # offset
        self.bn[i][3][1],      # scale
        self.z[i][0][1],       # after bn
    ], feed_dict=feed_dict))
    # prints
    # [-0.077417567, -0.089603029, 0.000436493, -0.016652612, 1.0055743, 0.30664611]
</code></pre>

<p>According to the formula on the page <a href=""https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/nn/batch_normalization"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/nn/batch_normalization</a>:</p>

<pre><code>bn = scale * (x - mean) / (sqrt(var) + 1e-3) + offset
</code></pre>

<p>But as we can see, </p>

<pre><code>1.0055743 * (-0.077417567 - -0.089603029)/(0.000436493^0.5 + 1e-3) + -0.016652612
= 0.543057
</code></pre>

<p>Which differs from the value <code>0.30664611</code>, computed by Tensorflow itself. 
So what am I doing wrong here and why I can't just calculate batch normalized value myself? </p>

<p>Thanks in advance!</p>
","I'm using tensorflow batch normalization in my deep neural network successfully. I'm doing it the following way: And it works fine both for training and testing phases. However I encounter problems when I try to use the computed neural network parameters in my another project, where I need to compute all the matrix multiplications and stuff by myself. The problem is that I can't reproduce the behavior of the tf.nn.batch_normalization function: According to the formula on the page https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/nn/batch_normalization: But as we can see, Which differs from the value 0.30664611, computed by Tensorflow itself. So what am I doing wrong here and why I can't just calculate batch normalized value myself? Thanks in advance!",https://stackoverflow.com/questions/44093698,2078632,Documentation Replication on Other Examples
44123088,How tf.nn.softmax_cross_entropy_with_logits can compute softmax cross entropy in tensorflow?,"<p>tf.nn.softmax_cross_entropy_with_logits, Documentation says that it computes softmax cross entropy between logits and labels what does it mean? Is it not applying cross entropy loss function formula on it? Why documentation says that it computes sofmax cross entropy?</p>
","tf.nn.softmax_cross_entropy_with_logits, Documentation says that it computes softmax cross entropy between logits and labels what does it mean? Is it not applying cross entropy loss function formula on it? Why documentation says that it computes sofmax cross entropy?",https://stackoverflow.com/questions/44123088,7997184,Documentation Ambiguity
44162432,Analysis of the output from tf.nn.dynamic_rnn tensorflow function,"<p>I am not able to understand the output from <code>tf.nn.dynamic_rnn</code> tensorflow function. The document just tells about the size of the output, but it doesn't tell what does each row/column means. From the documentation:</p>

<blockquote>
  <p><strong>outputs</strong>: The RNN output <code>Tensor</code>.</p>
  
  <p>If time_major == False (default), this will be a <code>Tensor</code> shaped:
      <code>[batch_size, max_time, cell.output_size]</code>.</p>
  
  <p>If time_major == True, this will be a <code>Tensor</code> shaped:
      <code>[max_time, batch_size, cell.output_size]</code>.</p>
  
  <p>Note, if <code>cell.output_size</code> is a (possibly nested) tuple of integers
  or <code>TensorShape</code> objects, then <code>outputs</code> will be a tuple having the<br>
  same structure as <code>cell.output_size</code>, containing Tensors having shapes
  corresponding to the shape data in <code>cell.output_size</code>.</p>
  
  <p><strong>state</strong>: The final state.  If <code>cell.state_size</code> is an int, this   will
  be shaped <code>[batch_size, cell.state_size]</code>.  If it is a<br>
  <code>TensorShape</code>, this will be shaped <code>[batch_size] + cell.state_size</code>.<br>
  If it is a (possibly nested) tuple of ints or <code>TensorShape</code>, this will
  be a tuple having the corresponding shapes.</p>
</blockquote>

<p>The <code>outputs</code> tensor is a 3-D matrix but what does each row/column represent?</p>
","I am not able to understand the output from tf.nn.dynamic_rnn tensorflow function. The document just tells about the size of the output, but it doesn't tell what does each row/column means. From the documentation: The outputs tensor is a 3-D matrix but what does each row/column represent?",https://stackoverflow.com/questions/44162432,333125,Documentation Completeness
44206534,Why is tf.transpose so important in a RNN?,"<p>I've been reading the docs to learn TensorFlow and have been struggling on when to use the following functions and their purpose.</p>

<pre><code>tf.split()
tf.reshape()
tf.transpose()
</code></pre>

<p>My guess so far is that:</p>

<p>tf.split() is used because inputs must be a sequence.</p>

<p>tf.reshape() is used to make the shapes compatible (Incorrect shapes tends to be a common problem / mistake for me). I used numpy for this before. I'll probably stick to tf.reshape() now. I am not sure if there is a difference between the two. </p>

<p>tf.transpose() swaps the rows and columns from my understanding. If I don't use tf.transpose() my loss doesn't go down. If the parameter values are incorrect the loss doesn't go down. So the purpose of me using tf.transpose() is so that my loss goes down and my predictions become more accurate. </p>

<p>This bothers me tremendously because I'm using tf.transpose() because I have to and have no understanding why it's such an important factor. I'm assuming if it's not used correctly the inputs and labels can be in the wrong position. Making it impossible for the model to learn. If this is true how can I go about using tf.transpose() so that I am not so reliant on figuring out the parameter values via trial and error?  </p>
",I've been reading the docs to learn TensorFlow and have been struggling on when to use the following functions and their purpose. My guess so far is that: tf.split() is used because inputs must be a sequence. tf.reshape() is used to make the shapes compatible (Incorrect shapes tends to be a common problem / mistake for me). I used numpy for this before. I'll probably stick to tf.reshape() now. I am not sure if there is a difference between the two. tf.transpose() swaps the rows and columns from my understanding. If I don't use tf.transpose() my loss doesn't go down. If the parameter values are incorrect the loss doesn't go down. So the purpose of me using tf.transpose() is so that my loss goes down and my predictions become more accurate. This bothers me tremendously because I'm using tf.transpose() because I have to and have no understanding why it's such an important factor. I'm assuming if it's not used correctly the inputs and labels can be in the wrong position. Making it impossible for the model to learn. If this is true how can I go about using tf.transpose() so that I am not so reliant on figuring out the parameter values via trial and error?,https://stackoverflow.com/questions/44206534,4005959,Documentation Replicability
44244763,TensorFlow tf.group ignoring dependencies?,"<p>Following on from an <a href=""https://stackoverflow.com/questions/44244275/tensorflow-fifoqueue-not-fifo"">earlier question</a>, it seems <code>tf.group</code> is indeed ignoring dependencies. Here's a simple stand-alone example (I have run it on Python 2.7 with TensorFlow 1.1):</p>

<pre><code>import tensorflow as tf
from tensorflow.python.ops import control_flow_ops

xs = [tf.constant(x) for x in range(10)]
xs = [tf.Print(x, [x]) for x in xs]
dependency = None
dxs = []

for x in xs:
    if dependency is None:
        dependency = x
    else:
        dependency = control_flow_ops.with_dependencies([dependency], x)

    dxs.append(dependency)

print_all_op = tf.group(*dxs)

with tf.Session() as session:
    session.run(print_all_op)
</code></pre>

<p>Expected output:</p>

<pre><code>2017-05-29 15:11:53.961221: I tensorflow/core/kernels/logging_ops.cc:79] [0]
2017-05-29 15:11:53.961236: I tensorflow/core/kernels/logging_ops.cc:79] [1]
2017-05-29 15:11:53.961255: I tensorflow/core/kernels/logging_ops.cc:79] [2]
2017-05-29 15:11:53.961237: I tensorflow/core/kernels/logging_ops.cc:79] [3]
2017-05-29 15:11:53.961262: I tensorflow/core/kernels/logging_ops.cc:79] [4]
2017-05-29 15:11:53.961263: I tensorflow/core/kernels/logging_ops.cc:79] [5]
2017-05-29 15:11:53.961268: I tensorflow/core/kernels/logging_ops.cc:79] [6]
2017-05-29 15:11:53.961272: I tensorflow/core/kernels/logging_ops.cc:79] [7]
2017-05-29 15:11:53.961274: I tensorflow/core/kernels/logging_ops.cc:79] [8]
2017-05-29 15:11:53.961221: I tensorflow/core/kernels/logging_ops.cc:79] [9]
</code></pre>

<p>Actual output (different each time the code is run):</p>

<pre><code>2017-05-29 15:16:26.279655: I tensorflow/core/kernels/logging_ops.cc:79] [0]
2017-05-29 15:16:26.279655: I tensorflow/core/kernels/logging_ops.cc:79] [9]
2017-05-29 15:16:26.279697: I tensorflow/core/kernels/logging_ops.cc:79] [3]
2017-05-29 15:16:26.279660: I tensorflow/core/kernels/logging_ops.cc:79] [1]
2017-05-29 15:16:26.279711: I tensorflow/core/kernels/logging_ops.cc:79] [8]
2017-05-29 15:16:26.279713: I tensorflow/core/kernels/logging_ops.cc:79] [4]
2017-05-29 15:16:26.279723: I tensorflow/core/kernels/logging_ops.cc:79] [5]
2017-05-29 15:16:26.279663: I tensorflow/core/kernels/logging_ops.cc:79] [2]
2017-05-29 15:16:26.279724: I tensorflow/core/kernels/logging_ops.cc:79] [7]
2017-05-29 15:16:26.279728: I tensorflow/core/kernels/logging_ops.cc:79] [6]
</code></pre>

<p>There's nothing in the <a href=""https://www.tensorflow.org/api_docs/python/tf/group"" rel=""nofollow noreferrer""><code>tf.group</code></a> documentation to indicate why dependencies are ignored.</p>

<p>Is there an alternative to <code>tf.group</code> that does consider dependencies?</p>

<p>Switching to use <code>tf.control_dependencies</code> instead of <code>tensorflow.python.ops.control_flow_ops.with_dependencies</code> doesn't help:</p>

<pre><code>import tensorflow as tf

xs = [tf.constant(x) for x in range(10)]
xs = [tf.Print(x, [x]) for x in xs]
dependency = None
dxs = []

for x in xs:
    if dependency is None:
        dependency = x
    else:
        with tf.control_dependencies([dependency]):
            dependency = x

    dxs.append(dependency)

print_all_op = tf.group(*dxs)

with tf.Session() as session:
    session.run(print_all_op)
</code></pre>
","Following on from an earlier question, it seems tf.group is indeed ignoring dependencies. Here's a simple stand-alone example (I have run it on Python 2.7 with TensorFlow 1.1): Expected output: Actual output (different each time the code is run): There's nothing in the tf.group documentation to indicate why dependencies are ignored. Is there an alternative to tf.group that does consider dependencies? Switching to use tf.control_dependencies instead of tensorflow.python.ops.control_flow_ops.with_dependencies doesn't help:",https://stackoverflow.com/questions/44244763,127480,Documentation Ambiguity
44357675,Documentation on how to use tf.estimator in TensorFlow,"<p>I understand that we can write custom models and encapsulate it using tf.estimator. But I just can't seem to find any documentation with an example.</p>

<p>I know that you have to define your model inside a 'model_fn' but what exactly should I return from this function. Also am I supposed to put the the loss and the training step within the 'model_fn' or just the network.  How should I modify the code give below to make it work with tf.estimator. Would really appreciate some help.</p>

<pre><code>def test_model(features,labels):
    X = tf.placeholder(tf.float32,shape=(None,1),name=""Data_Input"")
    #Output
    Y = tf.placeholder(tf.float32,shape=(None,1),name=""Target_Labels"")
    W =  tf.Variable(tf.random_normal([0],stddev=stddev0)) 
    b = tf.Variable(tf.random_normal([0],stddev=stddev0))

    Ypredict = W*X + b
    return Ypredict

 estimator = tf.estimator.Estimator(model_fn = test_model)
</code></pre>
",I understand that we can write custom models and encapsulate it using tf.estimator. But I just can't seem to find any documentation with an example. I know that you have to define your model inside a 'model_fn' but what exactly should I return from this function. Also am I supposed to put the the loss and the training step within the 'model_fn' or just the network. How should I modify the code give below to make it work with tf.estimator. Would really appreciate some help.,https://stackoverflow.com/questions/44357675,7656080,Documentation Replicability
44415901,tensorflow using tf.train.string_input_producer,"<p>I'm using tf.train.string_input_producer to read data from tfRecord file. I suppose it create a queue and pipeline and the data will automatically loaded and feed into my model. However, it stuck at the first batch, and show this exception:</p>

<blockquote>
  <p>FailedPreconditionError (see above for traceback): Attempting to use uninitialized value input_producer/limit_epochs/epochs</p>
</blockquote>

<p>my tfrecord was made by tf.train.SequenceExample, instead of tf.train.Example, which don't have clear documentation in the official guide.</p>

<p>here is code snapshot to reproduce my problem. (I believe my problem come from the queue initializing or sth. because it seems that the whole pipeline is hang up)</p>

<pre><code>from config.config import get_config

init = tf.global_variables_initializer()
config = get_config()

filename_queue = tf.train.string_input_producer(['data0.tfrecord,data1.tfrecord'], 5, capacity=16384)
reader = tf.TFRecordReader()

(keys, values) = reader.read_up_to(filename_queue, config.batch_size)

context_features = {
    ""seq_len"": tf.FixedLenFeature([1], dtype=tf.int64),
}
audio_features = {
    ""audio"": tf.FixedLenSequenceFeature([config.num_features], dtype=tf.float32),
    ""label"": tf.FixedLenSequenceFeature([config.num_classes], dtype=tf.float32)
}
audio_list = []
label_list = []
len_list = []

for i in range(config.batch_size):
    print(i)
    context, sequence = tf.parse_single_sequence_example(
        serialized=values[i],
        context_features=context_features,
        sequence_features=audio_features
    )
    audio = sequence['audio']
    label = sequence['label']
    # seq_len = context['seq_len'][0]
    seq_len = tf.shape(audio)[0]
    audio_list.append(audio)
    label_list.append(label)
    len_list.append(seq_len)

audio_tensor = tf.stack(audio_list)
label_tenor = tf.stack(label_list)
len_tensor = tf.stack(len_list)

with tf.Session() as sess:
    sess.run(init)

    threads = tf.train.start_queue_runners(sess=sess)
    for i in range(3):
        x, y, z = sess.run([audio_tensor, label_tenor, len_tensor])
        print(z)
</code></pre>
","I'm using tf.train.string_input_producer to read data from tfRecord file. I suppose it create a queue and pipeline and the data will automatically loaded and feed into my model. However, it stuck at the first batch, and show this exception: my tfrecord was made by tf.train.SequenceExample, instead of tf.train.Example, which don't have clear documentation in the official guide. here is code snapshot to reproduce my problem. (I believe my problem come from the queue initializing or sth. because it seems that the whole pipeline is hang up)",https://stackoverflow.com/questions/44415901,6861219,Lack of Alternative Solutions/Documentation
44478812,What kind of calculation does tf.nn.dynamic_rnn do with its input parameters?,"<p>What kind of calculation does <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer""><code>tf.nn.dynamic_rnn</code></a> perform? How does it use the parameters <code>cell</code> and <code>inputs</code> (to create the result)? </p>

<p>I have looked up in the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">documentation</a>, but I have not found an explanation.</p>
","What kind of calculation does tf.nn.dynamic_rnn perform? How does it use the parameters cell and inputs (to create the result)? I have looked up in the documentation, but I have not found an explanation.",https://stackoverflow.com/questions/44478812,6183280,Documentation Completeness
44526763,How to perform tf.image.per_image_standardization on a batch of images in tensorflow,"<p>I would like to know how to perform image whitening on a batch of images. </p>

<p>According to the documentation in <a href=""https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization</a>, it is said that <code>tf.image.per_image_standardization</code> takes as input a 3D tensor, that is an image, of shape: <code>[height, width, channels]</code>. </p>

<p>Is it a missing feature or there is a different method?</p>

<p>Any help is much appreciated. </p>
","I would like to know how to perform image whitening on a batch of images. According to the documentation in https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization, it is said that tf.image.per_image_standardization takes as input a 3D tensor, that is an image, of shape: [height, width, channels]. Is it a missing feature or there is a different method? Any help is much appreciated.",https://stackoverflow.com/questions/44526763,7886651,Documentation Replicability
44640357,Does Tensorflow's tf.while_loop automatically capture dependencies when executing in parallel?,"<p>I am interested in implementing a Recursive Neural Network in Tensorflow, like what has been done in <a href=""https://stackoverflow.com/questions/37054188/how-can-i-implement-a-recursive-neural-network-in-tensorflow"">How can I implement a recursive neural network in TensorFlow?</a>. </p>

<p>However, in his implementation, the <code>parallel_iterations</code> of the <code>tf.while_loop</code> statement was fixed to be 1. I fear that this might be too slow. Since the tree I am going to feed into tensorflow have parts that are not dependent on each other, I would hope that I could set <code>parallel_iterations</code> to a higher value. However, it is inevitable that there are some dependencies required in the tree I feed in as input to tensorflow, and I am afraid that setting it to higher value may break the dependency property. </p>

<p>So my question is, had Tensorflow's <code>tf.while_loop</code> automatically captured dependencies already, in order to only use paralleism on placed that are not dependent on each other?</p>

<p>The tensorflow documentation says the following:</p>

<blockquote>
  <p>For correct programs, while_loop should return the same result for any
  parallel_iterations > 0.</p>
</blockquote>

<p>But I am not sure what they mean by ""correct programs"".</p>
","I am interested in implementing a Recursive Neural Network in Tensorflow, like what has been done in How can I implement a recursive neural network in TensorFlow?. However, in his implementation, the parallel_iterations of the tf.while_loop statement was fixed to be 1. I fear that this might be too slow. Since the tree I am going to feed into tensorflow have parts that are not dependent on each other, I would hope that I could set parallel_iterations to a higher value. However, it is inevitable that there are some dependencies required in the tree I feed in as input to tensorflow, and I am afraid that setting it to higher value may break the dependency property. So my question is, had Tensorflow's tf.while_loop automatically captured dependencies already, in order to only use paralleism on placed that are not dependent on each other? The tensorflow documentation says the following: But I am not sure what they mean by ""correct programs"".",https://stackoverflow.com/questions/44640357,3608412,Documentation Ambiguity
44690363,How to use tf.train.ExponentialMovingAverage in Android/IOS,"<p>I use <code>freeze_graph</code> to export my model to a file named <code>""frozen.pb""</code>. But Found that the accuracy of predictions on <code>frozen.pb</code> is very bad.</p>

<p>I know the problem maybe <code>MovingAverage</code> not included in <code>frozen.pb</code>.</p>

<p>When I use <code>model.ckpt</code> files to restore model for evaluating, if I call <code>tf.train.ExponentialMovingAverage(0.999)</code> , then the accuracy is good as expected, else the accuracy is bad.</p>

<p><strong>So How To export a binary model which performance is the same as the one restored from checkpoint files?</strong>  I want to use <code>"".pb""</code> files in Android Devices.</p>

<p><a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/train/moving_averages"" rel=""nofollow noreferrer"">The official document</a> doesn't mention this.</p>

<p>Thanks!!</p>

<p>Freeze Command:</p>

<pre><code>~/bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=./graph.pbtxt \
  --input_checkpoint=./model.ckpt-100000 \
  --output_graph=frozen.pb \
  --output_node_names=output  \
  --restore_op_name=save/restore_all \
  --clear_devices
</code></pre>

<p>Evaluate Code:</p>

<pre><code>... ...
logits = carc19.inference(images)
top_k = tf.nn.top_k(logits, k=10)

# Precision: 97%
# Restore the moving average version of the learned variables for eval.
variable_averages = tf.train.ExponentialMovingAverage(carc19.MOVING_AVERAGE_DECAY)
variables_to_restore = variable_averages.variables_to_restore()
for k in variables_to_restore.keys():
  print (k,variables_to_restore[k])
saver = tf.train.Saver(variables_to_restore)

# Precision: 84%
#saver = tf.train.Saver()

#model_path = '/tmp/carc19_train/model.ckpt-9801'
with tf.Session() as sess:
  saver.restore(sess, model_path)
... ...
</code></pre>
","I use freeze_graph to export my model to a file named ""frozen.pb"". But Found that the accuracy of predictions on frozen.pb is very bad. I know the problem maybe MovingAverage not included in frozen.pb. When I use model.ckpt files to restore model for evaluating, if I call tf.train.ExponentialMovingAverage(0.999) , then the accuracy is good as expected, else the accuracy is bad. So How To export a binary model which performance is the same as the one restored from checkpoint files? I want to use "".pb"" files in Android Devices. The official document doesn't mention this. Thanks!! Freeze Command: Evaluate Code:",https://stackoverflow.com/questions/44690363,8058425,Lack of Alternative Solutions/Documentation
44753916,How to slice a part of tensor?,"<p>I want to slice [3.0 ,33.0].I have tried to access this slice by following code. I'm not so clear about tf.slice command. I'm not so clear about begin and size mentioned in documentaion about this command. Can someone please make it easy to understand.  </p>

<pre><code>batch = tf.constant([
  [#First image
    [[0.0,10.0],[1.0,11.0]],
    [[3.0,33.0],[4.0,44.0]]
  ],
  [#Second image
    [[5.0,55.0],[6.0,66.0]],
    [[7.0,77.0],[8.0,88.0]]
  ]
])
slice1 = tf.slice(batch,[0,0,0,0], [0,0,1,0]) 
sess = tf.InteractiveSEssion()
sess.run(tf.initialize_all_variables())
print slice1.eval()
</code></pre>
","I want to slice [3.0 ,33.0].I have tried to access this slice by following code. I'm not so clear about tf.slice command. I'm not so clear about begin and size mentioned in documentaion about this command. Can someone please make it easy to understand.",https://stackoverflow.com/questions/44753916,8214056,Documentation Ambiguity
44871420,TensorFlow dynamic_rnn input for regression,"<p>I'm stuck trying to convert an existing tensorflow sequence to sequence classifier to a regressor.</p>

<p>Currently I'm stuck in handling the input for <code>tf.nn.dynamic_rnn()</code>. According to the documentation and other answers, input should be in the shape of <code>(batch_size, sequence_length, input_size)</code>. However my input data has only two dimensions: <code>(sequence_length, batch_size)</code>.</p>

<p>The original solution uses <code>tf.nn.embedding_lookup()</code> as an intermediate step before feeding input to <code>dynamic_rnn()</code>. If I understand correctly, I believe I don't need this step since I'm working on a regression problem, not a classification problem.</p>

<p>Do I need the embedding_lookup step? If so, why? If not, how can I fit my <code>encoder_inputs</code> directly into <code>dynamic_rnn()</code>?</p>

<p>Below is a working minimalized example of the general idea:</p>

<pre><code>import numpy as np
import tensorflow as tf

tf.reset_default_graph()
sess = tf.InteractiveSession()

PAD = 0
EOS = 1
VOCAB_SIZE = 10 # Don't think I should need this for regression?
input_embedding_size = 20

encoder_hidden_units = 20
decoder_hidden_units = encoder_hidden_units

LENGTH_MIN = 3
LENGTH_MAX = 8
VOCAB_LOWER = 2
VOCAB_UPPER = VOCAB_SIZE
BATCH_SIZE = 10

def get_random_sequences():
    sequences = []
    for j in range(BATCH_SIZE):
        random_numbers = np.random.randint(3, 10, size=8)
        sequences.append(random_numbers)
    sequences = np.asarray(sequences).T
    return(sequences)

def next_feed():
    batch = get_random_sequences()

    encoder_inputs_ = batch
    eos = np.ones(BATCH_SIZE)
    decoder_targets_ = np.hstack((batch.T, np.atleast_2d(eos).T)).T
    decoder_inputs_ = np.hstack((np.atleast_2d(eos).T, batch.T)).T

    #print(encoder_inputs_)
    #print(decoder_inputs_)

    return {
        encoder_inputs: encoder_inputs_,
        decoder_inputs: decoder_inputs_,
        decoder_targets: decoder_targets_,
    }

### ""MAIN""

# Placeholders
encoder_inputs = tf.placeholder(shape=(LENGTH_MAX, BATCH_SIZE), dtype=tf.int32, name='encoder_inputs')
decoder_targets = tf.placeholder(shape=(LENGTH_MAX + 1, BATCH_SIZE), dtype=tf.int32, name='decoder_targets')
decoder_inputs = tf.placeholder(shape=(LENGTH_MAX + 1, BATCH_SIZE), dtype=tf.int32, name='decoder_inputs')

# Don't think I should need this for regression problems
embeddings = tf.Variable(tf.random_uniform([VOCAB_SIZE, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)

# Encoder RNN
encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_inputs_embedded, # Throws 'ValueError: Shape (8, 10) must have rank at least 3' if encoder_inputs is used
    dtype=tf.float32, time_major=True,
)

# Decoder RNN
decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)
decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(
    decoder_cell, decoder_inputs_embedded, 
    initial_state=encoder_final_state,
    dtype=tf.float32, time_major=True, scope=""plain_decoder"",
)
decoder_logits = tf.contrib.layers.linear(decoder_outputs, VOCAB_SIZE)
decoder_prediction = tf.argmax(decoder_logits, 2)

# Loss function
loss = tf.reduce_mean(tf.squared_difference(decoder_logits, tf.one_hot(decoder_targets, depth=VOCAB_SIZE, dtype=tf.float32)))
train_op = tf.train.AdamOptimizer().minimize(loss)


sess.run(tf.global_variables_initializer())

max_batches = 5000
batches_in_epoch = 500

print('Starting train')
try:
    for batch in range(max_batches):
        feed = next_feed()
        _, l = sess.run([train_op, loss], feed)

        if batch == 0 or batch % batches_in_epoch == 0:
            print('batch {}'.format(batch))
            print('  minibatch loss: {}'.format(sess.run(loss, feed)))
            predict_ = sess.run(decoder_prediction, feed)
            for i, (inp, pred) in enumerate(zip(feed[encoder_inputs].T, predict_.T)):
                print('  sample {}:'.format(i + 1))
                print('    input     &gt; {}'.format(inp))
                print('    predicted &gt; {}'.format(pred))
                if i &gt;= 2:
                    break
            print()
except KeyboardInterrupt:
    print('training interrupted')
</code></pre>

<p>I have read similar questions here on stackoverflow but find my self still puzzled as to how to solve this.</p>

<p>EDIT:
I think I should clarify that the code above works well, however the real desired output should mimic a noisy signal (text to speech for example) which is why I think I need continuous output values instead of words or letters.</p>
","I'm stuck trying to convert an existing tensorflow sequence to sequence classifier to a regressor. Currently I'm stuck in handling the input for tf.nn.dynamic_rnn(). According to the documentation and other answers, input should be in the shape of (batch_size, sequence_length, input_size). However my input data has only two dimensions: (sequence_length, batch_size). The original solution uses tf.nn.embedding_lookup() as an intermediate step before feeding input to dynamic_rnn(). If I understand correctly, I believe I don't need this step since I'm working on a regression problem, not a classification problem. Do I need the embedding_lookup step? If so, why? If not, how can I fit my encoder_inputs directly into dynamic_rnn()? Below is a working minimalized example of the general idea: I have read similar questions here on stackoverflow but find my self still puzzled as to how to solve this. EDIT: I think I should clarify that the code above works well, however the real desired output should mimic a noisy signal (text to speech for example) which is why I think I need continuous output values instead of words or letters.",https://stackoverflow.com/questions/44871420,1145023,Documentation Replication on Other Examples
44939540,How to get tensorflow to do a convolution on a 2 x 2 matrix with a 1 x 2 kernel?,"<p>I have the following matrix:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D&space;0&space;&amp;&space;1%5C%5C&space;2&space;&amp;&space;3&space;%5Cend%7Bbmatrix%7D"" title=""\begin{bmatrix} 0 &amp; 1\\ 2 &amp; 3 \end{bmatrix}"" /></p>

<p>and the following kernel:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D&space;1&space;&amp;&space;2&space;%5Cend%7Bbmatrix%7D"" title=""\begin{bmatrix} 1 &amp; 2 \end{bmatrix}"" /></p>

<p>If I do a convolution with no padding and slide by 1 row, I should get the following answer:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D&space;2&space;%5C%5C&space;8&space;%5Cend%7Bbmatrix%7D"" title=""\begin{bmatrix} 2 \\ 8 \end{bmatrix}"" /></p>

<p>Because:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?2&space;=&space;(0%5Ctimes1)&space;&plus;&space;(1%5Ctimes&space;2)"" title=""2 = (0\times1) + (1\times 2)"" /></p>

<p><img src=""https://latex.codecogs.com/gif.latex?8&space;=&space;(2%5Ctimes&space;1)&space;&plus;&space;(3&space;%5Ctimes&space;2)"" title=""8 = (2\times 1) + (3 \times 2)"" /></p>

<p>Based the documentation of  <code>tf.nn.conv2d</code>, I thought this code expresses what I just described above:</p>

<pre><code>import tensorflow as tf

input_batch = tf.constant([
    [
        [[.0], [1.0]],
        [[2.], [3.]]
    ]
])

kernel = tf.constant([
    [
        [[1.0, 2.0]]
    ]
])

conv2d = tf.nn.conv2d(input_batch, kernel, strides=[1, 1, 1, 1], padding='VALID')
sess = tf.Session()

print(sess.run(conv2d))
</code></pre>

<p>But it produces this output:</p>

<pre><code>[[[[ 0.  0.]
   [ 1.  2.]]

  [[ 2.  4.]
   [ 3.  6.]]]]
</code></pre>

<p>And I have no clue how that is computed. I've tried experimenting with different values for the strides padding parameter but still am not able to produce the result I expected.</p>
","I have the following matrix: and the following kernel: If I do a convolution with no padding and slide by 1 row, I should get the following answer: Because: Based the documentation of tf.nn.conv2d, I thought this code expresses what I just described above: But it produces this output: And I have no clue how that is computed. I've tried experimenting with different values for the strides padding parameter but still am not able to produce the result I expected.",https://stackoverflow.com/questions/44939540,3775778,Documentation Ambiguity
44946189,TypeError: __init__() got an unexpected keyword argument 'shape',"<p>I am new to Tensorflow and I met an error while trying to run some sample codes.</p>

<pre><code>import tensorflow as tf

g1 = tf.Graph()
with g1.as_default():
    v = tf.get_variable(""v"", initializer=tf.zeros_initializer(shape=[1]))
</code></pre>

<p>Running the code above gives the error:
TypeError: __init__() got an unexpected keyword argument 'shape'.</p>

<p>The comment below says that tf.zeros_initializer does not accept 'shape' argument according to the documentation. I tried</p>

<pre><code>v = tf.get_variable(""v"", initializer=tf.zeros_initializer())
</code></pre>

<p>and it says ValueError: Shape of a new variable (v) must be fully defined, but instead was .</p>

<p>So, what kind of argument/expression should I use to define the shape without causing a type error?</p>

<p>I cannot find how to solve it online. Please help. Thank you</p>
","I am new to Tensorflow and I met an error while trying to run some sample codes. Running the code above gives the error: TypeError: __init__() got an unexpected keyword argument 'shape'. The comment below says that tf.zeros_initializer does not accept 'shape' argument according to the documentation. I tried and it says ValueError: Shape of a new variable (v) must be fully defined, but instead was . So, what kind of argument/expression should I use to define the shape without causing a type error? I cannot find how to solve it online. Please help. Thank you",https://stackoverflow.com/questions/44946189,2504541,Documentation Replication on Other Examples
44963306,Cannot printout concatenated tensor by tf.concat() (tensorflow 1.2.1 - gpu / py36),"<p>Learning <strong><a href=""https://www.tensorflow.org/api_docs/python/"" rel=""nofollow noreferrer"">Tensorflow</a></strong> (Python bindings) since the last month. I've been reading the docs on <code>tf.concat()</code>, but cannot resolve the problem as shown below, so I'm asking for your help!</p>

<p>What I want to do is to see the contents of the concatenated tensor.
I tried <code>Tensor.eval()</code>.</p>

<pre><code>import tensorflow as tf 
import numpy as np 

a=np.zeros([3,3])
a_trail=np.ones([3,3])

with tf.Session() as sess:
    concatenated=tf.concat([a, a_trail], axis=0) 
    print(concatenated)
    print(type(concatenated)) 
    concatenated.eval() 
    sess.run(concatenated) 
    sess.run(tf.constant(concatenated)) 
</code></pre>

<p><strong>Output:</strong>   </p>

<pre><code>Tensor(""concat_2:0"", shape=(6, 3), dtype=float64)
&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;
(nothing prints)
(nothing shows up either meh =/)
Error: List of Tensors when single Tensor expected    
</code></pre>

<p><code>tf.concat()</code> supposed to return <code>Tensor</code> and looks like it does. But why aren't <code>T.eval()</code> and <code>sess.run()</code> not working?</p>
","Learning Tensorflow (Python bindings) since the last month. I've been reading the docs on tf.concat(), but cannot resolve the problem as shown below, so I'm asking for your help! What I want to do is to see the contents of the concatenated tensor. I tried Tensor.eval(). Output: tf.concat() supposed to return Tensor and looks like it does. But why aren't T.eval() and sess.run() not working?",https://stackoverflow.com/questions/44963306,8063281,Documentation Replication on Other Examples
45030619,Detecting out-of-bounds slicing with tf.slice like in numpy,"<p>In tensorflow, I'm trying to use tf.slice, but <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/array_ops/slicing_and_joining"" rel=""nofollow noreferrer"">as its documentation states</a>, it requires the slice to fit in the input array. For instance, if you try to slice the first 5 positions of the tensor [1,2,3,4] it will crash. I want to have the same functionality we get with python lists or numpy arrays where slicing gets you the intersection of the original array and the slice you asked for. For instance if you ask for positions 2 to 6 of [1,2,3,4] you'll get [2,3,4].</p>

<p>How can I do that in tensorflow?</p>

<p>Thanks!</p>
","In tensorflow, I'm trying to use tf.slice, but as its documentation states, it requires the slice to fit in the input array. For instance, if you try to slice the first 5 positions of the tensor [1,2,3,4] it will crash. I want to have the same functionality we get with python lists or numpy arrays where slicing gets you the intersection of the original array and the slice you asked for. For instance if you ask for positions 2 to 6 of [1,2,3,4] you'll get [2,3,4]. How can I do that in tensorflow? Thanks!",https://stackoverflow.com/questions/45030619,4189580,Documentation Replication on Other Examples
45090843,Does sequence_length help performance of dynamic_rnn?,"<p>In <a href=""https://github.com/tensorflow/nmt"" rel=""nofollow noreferrer"">Google's recent nmt tutorial</a>, they say this: </p>

<blockquote>
  <p>Note that sentences have different lengths to avoid wasting computation, we tell dynamic_rnn the exact source sentence lengths through source_seqence_length</p>
</blockquote>

<p>with this code: 
<code>encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_emb_inp,
    sequence_length=source_seqence_length, time_major=True)
</code></p>

<p>However, I was reading <a href=""https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/rnn.py"" rel=""nofollow noreferrer"">dynamic_rnn's documentation</a> and it says: </p>

<blockquote>
  <p>The parameter <code>sequence_length</code> is optional and is used to copy-through state
    and zero-out outputs when past a batch element's sequence length. So it's more
    for correctness than performance.</p>
</blockquote>

<p>I'm just wondering if sequence_length really helps performance of dynamic_rnn, e.g. they do some kind of dynamic bucketing? If they do, is there any place where I can read more about it? Thanks a lot.</p>
","In Google's recent nmt tutorial, they say this: with this code: encoder_outputs, encoder_state = tf.nn.dynamic_rnn( encoder_cell, encoder_emb_inp, sequence_length=source_seqence_length, time_major=True) However, I was reading dynamic_rnn's documentation and it says: I'm just wondering if sequence_length really helps performance of dynamic_rnn, e.g. they do some kind of dynamic bucketing? If they do, is there any place where I can read more about it? Thanks a lot.",https://stackoverflow.com/questions/45090843,5029595,Requesting (Additional) Resources
45151015,How does tf.gradients behave when passed a list of `ys` tensors?,"<p>How exactly does <code>tf.gradients</code> behave when passed a list of tensors as its first argument? Take this very small example:</p>

<pre><code>a = tf.constant(5)
b = tf.constant(7)
c = a + 2 * b
</code></pre>

<p>If I compute the gradients of a single tensor, <code>c</code>, with respect to <code>[a,b]</code>, I get the expected answer:</p>

<pre><code>grads = tf.gradients(c, [a, b])
with tf.Session() as sess:
    sess.run(grads) # returns (1, 2)
</code></pre>

<p>According to the Tensorflow documentation, if you pass in a <em>list</em> of tensors as your first argument <code>ys</code>, <code>tf.gradients</code> will sum the gradients over that list, returning <code>sum_over_ys(dy/dx)</code> for each <code>x</code> in your second argument. So I would expect:</p>

<pre><code>tf.gradients([a, b, c], [a, b])
</code></pre>

<p>to behave the same way as:</p>

<pre><code>tf.gradients(a + b + c, [a, b])
</code></pre>

<p>Am I reading the docs wrong? When I test this code, I get the expected result <code>[2, 3]</code> for the second expression (explicitly summing <code>a + b + c</code>), but <code>[2, 1]</code> for the first. Where is this <code>[2, 1]</code> coming from?</p>
","How exactly does tf.gradients behave when passed a list of tensors as its first argument? Take this very small example: If I compute the gradients of a single tensor, c, with respect to [a,b], I get the expected answer: According to the Tensorflow documentation, if you pass in a list of tensors as your first argument ys, tf.gradients will sum the gradients over that list, returning sum_over_ys(dy/dx) for each x in your second argument. So I would expect: to behave the same way as: Am I reading the docs wrong? When I test this code, I get the expected result [2, 3] for the second expression (explicitly summing a + b + c), but [2, 1] for the first. Where is this [2, 1] coming from?",https://stackoverflow.com/questions/45151015,1931098,Documentation Ambiguity
45229165,How can I serve the Faster RCNN with Resnet 101 model with tensorflow serving,"<p>I am trying to serve the Faster RCNN with Resnet 101 model with tensorflow serving.</p>

<p>I know I need to use tf.saved_model.builder.SavedModelBuilder to export the model definition as well as variables, then I need a script like inception_client.py provided by tensorflow_serving. </p>

<p>while I am going through the examples and documentation and experimenting, I think someone may have done the same thing. So plase help if you have done the same or know how to get it done. Thanks in advance.</p>
","I am trying to serve the Faster RCNN with Resnet 101 model with tensorflow serving. I know I need to use tf.saved_model.builder.SavedModelBuilder to export the model definition as well as variables, then I need a script like inception_client.py provided by tensorflow_serving. while I am going through the examples and documentation and experimenting, I think someone may have done the same thing. So plase help if you have done the same or know how to get it done. Thanks in advance.",https://stackoverflow.com/questions/45229165,5566610,Documentation Replication on Other Examples
45247909,Tensorflow - How to get the gradients of the output w.r.t the model parameters,"<p>I would like to know if it is possible to compute the gradients of the output of a model with respect to the model parameters.  In other words I would like to compute <code>dy / d theta</code>.</p>

<p>Here is a short example of what I mean:</p>

<pre><code>import keras
import tensorflow as tf

# Dummy input
test = np.random.rand(1, 32, 32, 1)

x = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))

model = keras.layers.Conv2D(16, 5, padding = 'same', activation='elu') (x)
model = keras.layers.Flatten() (model)
model = keras.layers.Dense(128, activation='relu') (model)
predictions = keras.layers.Dense(1) (model)

with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    y = sess.run(predictions, feed_dict={x: test})

    # Get gradients of y w.r.t model parameters.
    gradients = sess.run(tf.gradients(y, model_parameters))
</code></pre>

<p>I have looked at the documentation of <code>tf.gradients()</code> and it states</p>

<blockquote>
  <p><code>ys</code> and <code>xs</code> are each a <code>Tensor</code> or a list of tensors. <code>grad_ys</code> is a list of <code>Tensor</code>, holding the gradients received by the <code>ys</code>. The list must be the same length as <code>ys</code>.</p>
</blockquote>

<p>So I do understand that both args need to be a tensor. However, when I try </p>

<p><code>model_parameters = tf.trainable_variables()</code></p>

<p><code>model_parameters</code> is a list of elements of type <code>tensorflow.python.ops.variables.Variable</code></p>

<p>Is there a way to get the parameters of the model as a tensor to use for differentiation?</p>
","I would like to know if it is possible to compute the gradients of the output of a model with respect to the model parameters. In other words I would like to compute dy / d theta. Here is a short example of what I mean: I have looked at the documentation of tf.gradients() and it states So I do understand that both args need to be a tensor. However, when I try model_parameters = tf.trainable_variables() model_parameters is a list of elements of type tensorflow.python.ops.variables.Variable Is there a way to get the parameters of the model as a tensor to use for differentiation?",https://stackoverflow.com/questions/45247909,7476324,Documentation Replicability
45373740,Tensorflow ReLU normalizes strangely,"<p>in my opinion the rectified linear unit is supposed to execute the following function:</p>

<pre><code>relu(x) = max(x, 0)
</code></pre>

<p>However, this seems not to be the case with <code>tf.nn.relu</code>:</p>

<pre><code>import tensorflow as tf
import numpy as np
rand_large = np.random.randn(10, 3)*100
X = tf.placeholder(tf.float32, [10, 3])
sess = tf.Session()
sess.run(tf.nn.relu(X), feed_dict={X:rand_large})
</code></pre>

<p>The random matrix looks like this:</p>

<pre><code>&gt;&gt;&gt; rand_large
array([[  21.94064161,  -82.16632876,   16.25152777],
   [  55.54897693,  -93.15235155,  118.99166126],
   [ -13.36452239,   39.36508285,   65.42844521],
   [-193.34041145,  -97.08632376,   99.22162259],
   [  87.02924619,    2.04134891,  -27.29975745],
   [-181.11406687,   43.55952393,   42.29312993],
   [ -29.81242188,   93.5764354 , -165.62711447],
   [  17.78380711, -171.30536766, -197.20709038],
   [ 105.94903623,   34.07995616,   -7.27568839],
   [-100.59533697, -189.88957685,   -7.52421816]])
</code></pre>

<p>And the output from the relu function like this:</p>

<pre><code>&gt;&gt;&gt; sess.run(tf.nn.relu(X), feed_dict={X:rand_large})array([[ 1. ,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5]], dtype=float32)
</code></pre>

<p><strong>So, if I see it correctly, <code>tf.nn.relu</code> does some sort of normalization, right? If yes, why isn't it mentioned in the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/relu"" rel=""nofollow noreferrer"">docs</a>?</strong></p>

<p>Okay, I found out that the whole issue was related to my tensorflow installtion which seemed to be corrupt. On another machine, I did get the expected results.
Thank you for the help and helpful comments.</p>
","in my opinion the rectified linear unit is supposed to execute the following function: However, this seems not to be the case with tf.nn.relu: The random matrix looks like this: And the output from the relu function like this: So, if I see it correctly, tf.nn.relu does some sort of normalization, right? If yes, why isn't it mentioned in the docs? Okay, I found out that the whole issue was related to my tensorflow installtion which seemed to be corrupt. On another machine, I did get the expected results. Thank you for the help and helpful comments.",https://stackoverflow.com/questions/45373740,5631237,Lack of Alternative Solutions/Documentation
45401311,What are channels in tf.nn.conv2D?,"<p>I've looked through some <a href=""https://stackoverflow.com/questions/34619177/what-does-tf-nn-conv2d-do-in-tensorflow"">great explanations</a> on what different arguments of tf.nn.conv2D represent, but I still can't understand what exactly in_channels and out_channels represent.</p>

<p>Could someone please clarify this for me?</p>
","I've looked through some great explanations on what different arguments of tf.nn.conv2D represent, but I still can't understand what exactly in_channels and out_channels represent. Could someone please clarify this for me?",https://stackoverflow.com/questions/45401311,4588128,Documentation Replicability
45428557,Tensorflow: How to make return value of tf.unique same size as input,"<p>According to <a href=""https://www.tensorflow.org/api_docs/python/tf/unique"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/unique</a>, <code>tf.unique(x)</code> returns a tuple <code>(y, idx)</code>,  The <strong>shape of y is (?, )</strong> is not known during build time. Is there anyway I can pad <code>y</code> to match the input size <code>x</code>?.</p>

<p>For example, </p>

<pre><code># tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx = unique(x)
y ==&gt; [1, 2, 4, 7, 8]
idx ==&gt; [0, 0, 1, 2, 2, 2, 3, 4, 4]
</code></pre>

<p>I wanna make y = [1, 2, 4, 7, 8, 0, 0, 0, 0]</p>
","According to https://www.tensorflow.org/api_docs/python/tf/unique, tf.unique(x) returns a tuple (y, idx), The shape of y is (?, ) is not known during build time. Is there anyway I can pad y to match the input size x?. For example, I wanna make y = [1, 2, 4, 7, 8, 0, 0, 0, 0]",https://stackoverflow.com/questions/45428557,7765065,Inadequate Examples
45553038,How to compile custom ops in tensorflow without having to dynamically import them in python?,"<p>I checked through tensorflow documentation and they seem to only give information about compiling a custom op through a bazel rule:</p>

<pre><code>load(""//tensorflow:tensorflow.bzl"", ""tf_custom_op_library"")

tf_custom_op_library(
    name = ""zero_out.so"",
    srcs = [""zero_out.cc""],
)
</code></pre>

<p>Once bazel builds it, you get a zero_out.so file which you can import into python like below:</p>

<pre><code>import tensorflow as tf
zero_out_module = tf.load_op_library('./zero_out.so')
</code></pre>

<p>Is there anyway you can link custom_ops during the bazel build of tensorflow so that you don't need to manually import custom ops through tf.load_op_library?</p>
","I checked through tensorflow documentation and they seem to only give information about compiling a custom op through a bazel rule: Once bazel builds it, you get a zero_out.so file which you can import into python like below: Is there anyway you can link custom_ops during the bazel build of tensorflow so that you don't need to manually import custom ops through tf.load_op_library?",https://stackoverflow.com/questions/45553038,7922336,Documentation Completeness
45553280,TensorArray Initialization from another tensor,"<p>What is the right way to initialize a tensorarray from another tensor in tensorflow. </p>

<p>Suppose I have a tensor</p>

<pre><code>T1 

TensorArr = tf.TensorArray(tf.int32, 1, dynamic_size=True)
</code></pre>

<p>What is way to say that this tensorarray depends on T1?  Looking at the <a href=""https://www.tensorflow.org/api_docs/python/tf/TensorArray"" rel=""nofollow noreferrer"">documentation</a> I cant figure out how to initialize this. </p>

<p>Correct me if my understanding is wrong, T1 is a nested tensor and I want to loop over a dimension using tf.while_loop and hence I want to initialize the TensorArray with it. </p>
","What is the right way to initialize a tensorarray from another tensor in tensorflow. Suppose I have a tensor What is way to say that this tensorarray depends on T1? Looking at the documentation I cant figure out how to initialize this. Correct me if my understanding is wrong, T1 is a nested tensor and I want to loop over a dimension using tf.while_loop and hence I want to initialize the TensorArray with it.",https://stackoverflow.com/questions/45553280,4040998,Documentation Replication on Other Examples
45595419,Is it possible to have multiple conditions defined in tf.while_loop,"<p>Is it possible to define to multiple conditions for termination of a tf.while_loop in tensorflow? For example depending on the two tensor values achieving two specific values. eg. <code>i==2</code> and <code>j==3</code> ?</p>

<p>Also can I have several blocks of code in the body? In all the examples in the documentation, it seems that the body is more like a single statement returning a value or a tuple. I want to execute a set of several ""<strong>sequential</strong>"" statements in the body.</p>
","Is it possible to define to multiple conditions for termination of a tf.while_loop in tensorflow? For example depending on the two tensor values achieving two specific values. eg. i==2 and j==3 ? Also can I have several blocks of code in the body? In all the examples in the documentation, it seems that the body is more like a single statement returning a value or a tuple. I want to execute a set of several ""sequential"" statements in the body.",https://stackoverflow.com/questions/45595419,8348464,Inadequate Examples
45634450,What are the advantages of using tf.train.SequenceExample over tf.train.Example for variable length features?,"<p>Recently I read <a href=""http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features"" rel=""noreferrer"">this</a> guide on undocumented featuers in TensorFlow, as I needed to pass variable length sequences as input. However, I found the protocol for <code>tf.train.SequenceExample</code> relatively confusing (especially due to lack of documentation), and managed to build an input pipe using <code>tf.train.Example</code> just fine instead.</p>

<p>Are there any advantages to using <code>tf.train.SequenceExample</code>? Using the standard example protocol when there is a dedicated one for variable length sequences seems like a cheat, but does it bear any consequence?</p>
","Recently I read this guide on undocumented featuers in TensorFlow, as I needed to pass variable length sequences as input. However, I found the protocol for tf.train.SequenceExample relatively confusing (especially due to lack of documentation), and managed to build an input pipe using tf.train.Example just fine instead. Are there any advantages to using tf.train.SequenceExample? Using the standard example protocol when there is a dedicated one for variable length sequences seems like a cheat, but does it bear any consequence?",https://stackoverflow.com/questions/45634450,7000919,Documentation Replicability
45705070,how to load and use a saved model on tensorflow?,"<p>I have found 2 ways to save a model in Tensorflow: <code>tf.train.Saver()</code> and <code>SavedModelBuilder</code>. However, <strong>I can't find documentation on using the model</strong> after it being loaded  the second way.</p>

<p>Note: I want to use <code>SavedModelBuilder</code> way because I train the model in Python and will use it at serving time in another language (Go), and it seems that <code>SavedModelBuilder</code> is the only way in that case.</p>

<p>This works great with <code>tf.train.Saver()</code> (first way):</p>

<pre><code>model = tf.add(W * x, b, name=""finalnode"")

# save
saver = tf.train.Saver()
saver.save(sess, ""/tmp/model"")

# load
saver.restore(sess, ""/tmp/model"")

# IMPORTANT PART: REALLY USING THE MODEL AFTER LOADING IT
# I CAN'T FIND AN EQUIVALENT OF THIS PART IN THE OTHER WAY.

model = graph.get_tensor_by_name(""finalnode:0"")
sess.run(model, {x: [5, 6, 7]})
</code></pre>

<p><code>tf.saved_model.builder.SavedModelBuilder()</code> is defined in the <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/saved_model/"" rel=""noreferrer"">Readme</a>  but after loading the model with <code>tf.saved_model.loader.load(sess, [], export_dir)</code>), I can't find documentation on getting back at the nodes (see <code>""finalnode""</code> in the code above)</p>
","I have found 2 ways to save a model in Tensorflow: tf.train.Saver() and SavedModelBuilder. However, I can't find documentation on using the model after it being loaded the second way. Note: I want to use SavedModelBuilder way because I train the model in Python and will use it at serving time in another language (Go), and it seems that SavedModelBuilder is the only way in that case. This works great with tf.train.Saver() (first way): tf.saved_model.builder.SavedModelBuilder() is defined in the Readme but after loading the model with tf.saved_model.loader.load(sess, [], export_dir)), I can't find documentation on getting back at the nodes (see ""finalnode"" in the code above)",https://stackoverflow.com/questions/45705070,2210667,Lack of Alternative Solutions/Documentation
45774938,tensorflow: tf.split is given weird parameters,"<p>Here is code(<a href=""https://pythonprogramming.net/rnn-tensorflow-python-machine-learning-tutorial/"" rel=""nofollow noreferrer"">from here</a>):</p>

<pre><code>import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.ops import rnn, rnn_cell
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot = True)

hm_epochs = 3
n_classes = 10
batch_size = 128
chunk_size = 28
n_chunks = 28
rnn_size = 128


x = tf.placeholder('float', [None, n_chunks,chunk_size])
y = tf.placeholder('float')
def recurrent_neural_network(x):
    layer = {'weights':tf.Variable(tf.random_normal([rnn_size,n_classes])),
             'biases':tf.Variable(tf.random_normal([n_classes]))}

    x = tf.transpose(x, [1,0,2])
    x = tf.reshape(x, [-1, chunk_size])
    x = tf.split(0, n_chunks, x)

    lstm_cell = rnn_cell.BasicLSTMCell(rnn_size,state_is_tuple=True)
    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)

    output = tf.matmul(outputs[-1],layer['weights']) + layer['biases']

    return output
def train_neural_network(x):
    prediction = recurrent_neural_network(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )
    optimizer = tf.train.AdamOptimizer().minimize(cost)


    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())

        for epoch in range(hm_epochs):
            epoch_loss = 0
            for _ in range(int(mnist.train.num_examples/batch_size)):
                epoch_x, epoch_y = mnist.train.next_batch(batch_size)
                epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))

                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})
                epoch_loss += c

            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)

        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))

        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
        print('Accuracy:',accuracy.eval({x:mnist.test.images.reshape((-1, n_chunks, chunk_size)), y:mnist.test.labels}))

train_neural_network(x)
</code></pre>

<p>I have issue understanding <code>x = tf.split(0, n_chunks, x)</code> , more specificaly third parameter(<code>x</code>-input). By <a href=""https://www.tensorflow.org/api_docs/python/tf/split"" rel=""nofollow noreferrer"">documenation</a> this should be axis...but that can't be, right? Isn't <code>x</code> one dimensional?
I apologise if it's trivial, I'm beginner and can't sem to get it. Maybe it's just formality, but if it is I don't understand how it works...</p>
","Here is code(from here): I have issue understanding x = tf.split(0, n_chunks, x) , more specificaly third parameter(x-input). By documenation this should be axis...but that can't be, right? Isn't x one dimensional? I apologise if it's trivial, I'm beginner and can't sem to get it. Maybe it's just formality, but if it is I don't understand how it works...",https://stackoverflow.com/questions/45774938,8081595,Documentation Ambiguity
45784815,How best to implement a matrix mask operation in tensorflow?,"<p>I had a case where I needed to fill some holes (missing data) in an image processing application in tensorflow. The 'holes' are easy to locate as they are zeros and the good data is not zeros. I wanted to fill the holes with random data. This is quite easy to do using python numpy but doing it in tensorflow requires some work. I came up with a solution and wanted to see if there is a better or more efficient way to do the same thing. I understand that tensorflow does not yet support the more advanced numpy type indexing yet but there is a function tf.gather_nd() that seems promising for this. However, I could not tell from the documentation how to us it for what I wanted to do. I would appreciate answers that improve on what I did or especially if someone can show me how to do it using tf.gather_nd(). Also, tf.boolean_mask() does not work for what I am  trying to do because it does not allow you to use the output as an index.  In python what I am trying to do:</p>

<pre><code>a = np.ones((2,2))
a[0,0]=a[0,1] = 0
mask = a == 0
a[mask] = np.random.random_sample(a.shape)[mask]
print('new a = ', a)
</code></pre>

<p>What I ended up doing in Tensorflow to achieve same thing (skipping filling the array steps)</p>

<pre><code>zeros = tf.zeros(tf.shape(a))  
mask = tf.greater(a,zeros)
mask_n = tf.equal(a,zeros)
mask = tf.cast(mask,tf.float32)
mask_n = tf.cast(mask_n,tf.float32
r = tf.random_uniform(tf.shape(a),minval = 0.0,maxval=1.0,dtype=tf.float32)
r_add = tf.multiply(mask_n,r)
targets = tf.add(tf.multiply(mask,a),r_add)
</code></pre>
","I had a case where I needed to fill some holes (missing data) in an image processing application in tensorflow. The 'holes' are easy to locate as they are zeros and the good data is not zeros. I wanted to fill the holes with random data. This is quite easy to do using python numpy but doing it in tensorflow requires some work. I came up with a solution and wanted to see if there is a better or more efficient way to do the same thing. I understand that tensorflow does not yet support the more advanced numpy type indexing yet but there is a function tf.gather_nd() that seems promising for this. However, I could not tell from the documentation how to us it for what I wanted to do. I would appreciate answers that improve on what I did or especially if someone can show me how to do it using tf.gather_nd(). Also, tf.boolean_mask() does not work for what I am trying to do because it does not allow you to use the output as an index. In python what I am trying to do: What I ended up doing in Tensorflow to achieve same thing (skipping filling the array steps)",https://stackoverflow.com/questions/45784815,7447161,Inadequate Examples
45879776,TensorFlow how to make results reproducible for `tf.nn.sampled_softmax_loss`,"<p>I would like to get reproducible results for my tensorflow runs. The way I'm trying to make this happen is to set up the numpy and tensorflow seeds:</p>

<pre><code>import numpy as np
rnd_seed = 1
np.random.seed(rnd_seed)

import tensorflow as tf
tf.set_random_seed(rnd_seed)
</code></pre>

<p>As well as make sure that the weights of the neural network, that I initialized with <code>tf.truncated_normal</code> also use that seed: <code>tf.truncated_normal(..., seed=rnd_seed)</code></p>

<p>For reasons that are beyond the scope of this question, I'm using the sampled softmax loss function, <code>tf.nn.sampled_softmax_loss</code>, and unfortunately, I'm not able to control the stochasticity of this function with a random seed.</p>

<p>By a look at the TensorFlow documentation of this function (<a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss</a>), I can see that parameter <code>sampled_values</code> should be the only parameter that affects randomization, but I'm not able to understand how to actually use a seed.</p>

<p>[EDITED]
This is (part of) my script</p>

<pre><code>import numpy as np
# set a seed so that the results are consistent
rnd_seed = 1
np.random.seed(rnd_seed)

import tensorflow as tf
tf.set_random_seed(rnd_seed)

embeddings_ini = np.random.uniform(low=-1, high=1, size=(self.vocabulary_size, self.embedding_size))

with graph.as_default(), tf.device('/cpu:0'):

    train_dataset = tf.placeholder(tf.int32, shape=[None, None])
    train_labels = tf.placeholder(tf.int32, shape=[None, 1])
    valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)

    # Variables.
    initial_embeddings = tf.placeholder(tf.float32, shape=(self.vocabulary_size, self.embedding_size))
    embeddings = tf.Variable(initial_embeddings)

    softmax_weights = tf.Variable(
        tf.truncated_normal([self.vocabulary_size, self.embedding_size],
                            stddev=1.0 / math.sqrt(self.embedding_size), seed=rnd_seed))
    softmax_biases = tf.Variable(tf.zeros([self.vocabulary_size]))

    # Model.
    # Look up embeddings for inputs.
    if self.model == ""skipgrams"":
        # Skipgram model
        embed = tf.nn.embedding_lookup(embeddings, train_dataset)
    elif self.model == ""cbow"":
        # CBOW Model
        embeds = tf.nn.embedding_lookup(embeddings, train_dataset)
        embed = tf.reduce_mean(embeds, 1, keep_dims=False)

    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights,
                                                     biases=softmax_biases,
                                                     inputs=embed,
                                                     labels=train_labels,
                                                     num_sampled=self.num_sampled,
                                                     num_classes=self.vocabulary_size))
</code></pre>
","I would like to get reproducible results for my tensorflow runs. The way I'm trying to make this happen is to set up the numpy and tensorflow seeds: As well as make sure that the weights of the neural network, that I initialized with tf.truncated_normal also use that seed: tf.truncated_normal(..., seed=rnd_seed) For reasons that are beyond the scope of this question, I'm using the sampled softmax loss function, tf.nn.sampled_softmax_loss, and unfortunately, I'm not able to control the stochasticity of this function with a random seed. By a look at the TensorFlow documentation of this function (https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss), I can see that parameter sampled_values should be the only parameter that affects randomization, but I'm not able to understand how to actually use a seed. [EDITED] This is (part of) my script",https://stackoverflow.com/questions/45879776,863713,Documentation Replicability
45886201,Tensorflow: Can't use tf.case with input argument,"<p>I need to create a variable <code>epsilon_n</code> that changes definition (and value) based on the current <code>step</code>. Since I have more than two cases, it seems that I can't use <code>tf.cond</code> . I am trying to use <code>tf.case</code> as follows:</p>

<pre><code>import tensorflow as tf

####
EPSILON_DELTA_PHASE1 = 33e-4
EPSILON_DELTA_PHASE2 = 2.5
####
step = tf.placeholder(dtype=tf.float32, shape=None)


def fn1(step):
    return tf.constant([1.])

def fn2(step):
    return tf.constant([1.+step*EPSILON_DELTA_PHASE1])

def fn3(step):
    return tf.constant([1.+step*EPSILON_DELTA_PHASE2])

epsilon_n = tf.case(
        pred_fn_pairs=[
            (tf.less(step, 3e4), lambda step: fn1(step)),
            (tf.less(step, 6e4), lambda step: fn2(step)),
            (tf.less(step, 1e5), lambda step: fn3(step))],
            default=lambda: tf.constant([1e5]),
        exclusive=False)
</code></pre>

<p>However, I keep getting this error message: </p>

<pre><code>TypeError: &lt;lambda&gt;() missing 1 required positional argument: 'step'
</code></pre>

<p>I tried the following:</p>

<pre><code>epsilon_n = tf.case(
        pred_fn_pairs=[
            (tf.less(step, 3e4), fn1),
            (tf.less(step, 6e4), fn2),
            (tf.less(step, 1e5), fn3)],
            default=lambda: tf.constant([1e5]),
        exclusive=False)
</code></pre>

<p>Still I would the same error. The examples in Tensorflow documentation weigh in on cases where no input argument is passed to the callable functions. I couldn't find enough info about tf.case on the internet! Please any help?</p>
","I need to create a variable epsilon_n that changes definition (and value) based on the current step. Since I have more than two cases, it seems that I can't use tf.cond . I am trying to use tf.case as follows: However, I keep getting this error message: I tried the following: Still I would the same error. The examples in Tensorflow documentation weigh in on cases where no input argument is passed to the callable functions. I couldn't find enough info about tf.case on the internet! Please any help?",https://stackoverflow.com/questions/45886201,5650892,Lack of Alternative Solutions/Documentation
45955241,How do I create padded batches in Tensorflow for tf.train.SequenceExample data using the DataSet API?,"<p>For training an <strong>LSTM model</strong> in <strong>Tensorflow</strong>, I have structured my data into a <strong>tf.train.SequenceExample</strong> format and stored it into a <strong>TFRecord file</strong>. I would now like to use the new DataSet API to <strong>generate padded batches for training</strong>. In <a href=""https://www.tensorflow.org/programmers_guide/datasets"" rel=""noreferrer"">the documentation</a> there is an example for using padded_batch, but for my data I can't figure out what the value of <em>padded_shapes</em> should be.</p>

<p>For reading the TFrecord file into the batches I have written the following Python code:</p>

<pre><code>import math
import tensorflow as tf
import numpy as np
import struct
import sys
import array

if(len(sys.argv) != 2):
  print ""Usage: createbatches.py [RFRecord file]""
  sys.exit(0)


vectorSize = 40
inFile = sys.argv[1]

def parse_function_dataset(example_proto):
  sequence_features = {
      'inputs': tf.FixedLenSequenceFeature(shape=[vectorSize],
                                           dtype=tf.float32),
      'labels': tf.FixedLenSequenceFeature(shape=[],
                                           dtype=tf.int64)}

  _, sequence = tf.parse_single_sequence_example(example_proto, sequence_features=sequence_features)

  length = tf.shape(sequence['inputs'])[0]
  return sequence['inputs'], sequence['labels']

sess = tf.InteractiveSession()

filenames = tf.placeholder(tf.string, shape=[None])
dataset = tf.contrib.data.TFRecordDataset(filenames)
dataset = dataset.map(parse_function_dataset)
# dataset = dataset.batch(1)
dataset = dataset.padded_batch(4, padded_shapes=[None])
iterator = dataset.make_initializable_iterator()

batch = iterator.get_next()

# Initialize `iterator` with training data.
training_filenames = [inFile]
sess.run(iterator.initializer, feed_dict={filenames: training_filenames})

print(sess.run(batch))
</code></pre>

<p>The code works well if I use <code>dataset = dataset.batch(1)</code> (no padding needed in that case), but when I use the <code>padded_batch</code> variant, I get the following error:</p>

<blockquote>
  <p>TypeError: If shallow structure is a sequence, input must also be a
  sequence. Input has type: .</p>
</blockquote>

<p>Can you help me figuring out what I should pass for the <em>padded_shapes</em> parameter?</p>

<p>(I know there is lots of example code using threading and queues for this, but I'd rather use the new DataSet API for this project)</p>
","For training an LSTM model in Tensorflow, I have structured my data into a tf.train.SequenceExample format and stored it into a TFRecord file. I would now like to use the new DataSet API to generate padded batches for training. In the documentation there is an example for using padded_batch, but for my data I can't figure out what the value of padded_shapes should be. For reading the TFrecord file into the batches I have written the following Python code: The code works well if I use dataset = dataset.batch(1) (no padding needed in that case), but when I use the padded_batch variant, I get the following error: Can you help me figuring out what I should pass for the padded_shapes parameter? (I know there is lots of example code using threading and queues for this, but I'd rather use the new DataSet API for this project)",https://stackoverflow.com/questions/45955241,8536501,Documentation Replication on Other Examples
46139202,Tensorflow: TypeError with numpy_input_fn,"<p>I am coding a Convolutional Neural Network to classify images in TensorFlow but there is a problem:</p>

<p>When I try to feed my NumPy array of flattened  images (3 channels with RGB values from 0 to 255) to a tf.estimator.inputs.numpy_input_fn I get the following error:</p>

<pre><code>  TypeError: Failed to convert object of type &lt;class 'dict'&gt; to Tensor. 
  Contents: {'x': &lt;tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(8, 
  196608) dtype=uint8&gt;}. Consider casting elements to a supported type.
</code></pre>

<p>My numpy_imput_fn looks like this:</p>

<pre><code>train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={'x': train_x},
    y=train_y,
    batch_size=8,
    num_epochs=None,
    shuffle=True)
</code></pre>

<p>In the documentation for the function it is said that x should be a dict of NumPy array:</p>

<blockquote>
  <p>x: dict of numpy array object.</p>
</blockquote>
",I am coding a Convolutional Neural Network to classify images in TensorFlow but there is a problem: When I try to feed my NumPy array of flattened images (3 channels with RGB values from 0 to 255) to a tf.estimator.inputs.numpy_input_fn I get the following error: My numpy_imput_fn looks like this: In the documentation for the function it is said that x should be a dict of NumPy array:,https://stackoverflow.com/questions/46139202,5423940,Documentation Replicability
46298583,Tensorflow embeddings,"<p>I know what embeddings are and how they are trained. Precisely, while referring to the tensorflow's documentation, I came across two different articles. I wish to know what exactly is the difference between them.</p>

<p>link 1: <a href=""https://www.tensorflow.org/tutorials/word2vec#building_the_graph"" rel=""nofollow noreferrer"">Tensorflow | Vector Representations of words</a></p>

<p>In the first tutorial, they have explicitly trained embeddings on a specific dataset. There is a distinct session run to train those embeddings. I can then later on save the learnt embeddings as a numpy object and use the </p>

<p><code>tf.nn.embedding_lookup()</code> function while training an LSTM network.</p>

<p>link 2: <a href=""https://www.tensorflow.org/programmers_guide/embedding"" rel=""nofollow noreferrer"">Tensorflow | Embeddings</a></p>

<p>In this second article however, I couldn't understand what is happening. </p>

<pre><code>word_embeddings = tf.get_variable(“word_embeddings”,
[vocabulary_size, embedding_size])
embedded_word_ids = tf.gather(word_embeddings, word_ids)
</code></pre>

<p>This is given under the training embeddings sections. My doubt is: does the gather function train the embeddings automatically? I am not sure since this op ran very fast on my pc.</p>

<p>Generally: What is the right way to convert words into vectors (link1 or link2) in tensorflow for training a seq2seq model? Also, how to train the embeddings for a seq2seq dataset, since the data is in the form of separate sequences for my task unlike (a continuous sequence of words refer: link 1 dataset) </p>
","I know what embeddings are and how they are trained. Precisely, while referring to the tensorflow's documentation, I came across two different articles. I wish to know what exactly is the difference between them. link 1: Tensorflow | Vector Representations of words In the first tutorial, they have explicitly trained embeddings on a specific dataset. There is a distinct session run to train those embeddings. I can then later on save the learnt embeddings as a numpy object and use the tf.nn.embedding_lookup() function while training an LSTM network. link 2: Tensorflow | Embeddings In this second article however, I couldn't understand what is happening. This is given under the training embeddings sections. My doubt is: does the gather function train the embeddings automatically? I am not sure since this op ran very fast on my pc. Generally: What is the right way to convert words into vectors (link1 or link2) in tensorflow for training a seq2seq model? Also, how to train the embeddings for a seq2seq dataset, since the data is in the form of separate sequences for my task unlike (a continuous sequence of words refer: link 1 dataset)",https://stackoverflow.com/questions/46298583,4341842,Documentation Replication on Other Examples
46370159,Outputting batch/epoch training loss during `tf.train.MonitoredTrainingSession`,"<p>I would like to output my loss with <code>MonitoredTrainingSession</code> every epoch or batch.
Ideally I would love to get a flag that the epoch is ended or be able to provide a callback like in keras. I see that I can also do it by manually counting steps, but I want to use the tf functionality, which seems still poorly documented.</p>

<p>From what I could find in their documentation, one can use <code>tf.train.LoggingTensorHook</code> to print the tensors every <code>n</code> steps. </p>

<p>The problem however is that it prints with frequency different from what I request. When I run following with <code>every_n_iter=4</code> I get output every 2nd iteration:</p>

<pre><code>tf.reset_default_graph()
with g.as_default():
    loghook = tf.train.LoggingTensorHook([tf.reduce_mean(loss, name='m_loss')],
                                         every_n_iter=4,
                                         formatter=lambda x: ""LOSS\t%.4f"" % [tt for kk,tt in x.items() if kk.name.startswith('m_loss')][-1]
                                        )
    optimizer = get_optimizer(lr=lr, opt_name = opt_name)
    training_op = optimizer.minimize(loss)
    init_op = tf.global_variables_initializer()
    with tf.Session(graph=g) as sess:    
        sess.run(init_op)
    with tf.train.MonitoredTrainingSession(log_step_count_steps=1, hooks=[loghook]) as sess:
        losslist = []
        while not sess.should_stop():
            print('.')
            loss_ = sess.run(loss, feed_dict={K.learning_phase():1})
            sess.run(training_op)
            losslist.append(np.mean(loss_))
</code></pre>

<p>I am getting output like:</p>

<pre><code>.
INFO:tensorflow:LOSS    2.2416
.
.
INFO:tensorflow:LOSS    2.1547
.
.
INFO:tensorflow:LOSS    2.1186
.
.
</code></pre>

<p>etc. That is it outputs every 2nd step, not every 4th. </p>

<p>The documentation says: </p>

<pre><code>every_n_iter: `int`, print the values of `tensors` once every N local
      steps taken on the current worker.
</code></pre>

<p>I am running it on one local machine. Why one ""local step"" equals two loop python iterations? Why two and not five?</p>

<p>Looking at the Python source does not seem helping. Any Google folks aware of what it is doing?</p>
","I would like to output my loss with MonitoredTrainingSession every epoch or batch. Ideally I would love to get a flag that the epoch is ended or be able to provide a callback like in keras. I see that I can also do it by manually counting steps, but I want to use the tf functionality, which seems still poorly documented. From what I could find in their documentation, one can use tf.train.LoggingTensorHook to print the tensors every n steps. The problem however is that it prints with frequency different from what I request. When I run following with every_n_iter=4 I get output every 2nd iteration: I am getting output like: etc. That is it outputs every 2nd step, not every 4th. The documentation says: I am running it on one local machine. Why one ""local step"" equals two loop python iterations? Why two and not five? Looking at the Python source does not seem helping. Any Google folks aware of what it is doing?",https://stackoverflow.com/questions/46370159,1716733,Lack of Alternative Solutions/Documentation
46372554,When feeding a dictionary to a tensorflow function I get Why do I get TypeError: unhashable type: 'numpy.ndarray',"<p>I am working on a Tensor Flow Coursera Course and I dont understand why I am getting a type mismatch. </p>

<p>This is the function I am defining:</p>

<pre><code>def one_hot_matrix(labels, C):
    """"""
    Creates a matrix where the i-th row corresponds to the ith class number and the jth column
                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) 
                     will be 1. 

Arguments:
labels -- vector containing the labels 
C -- number of classes, the depth of the one hot dimension

Returns: 
one_hot -- one hot matrix
""""""

### START CODE HERE ###

# Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)
C = tf.constant(C, name=""C"")
#labels =tf.placeholder(labels, name=""labels"")

# Use tf.one_hot, be careful with the axis (approx. 1 line)
one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=0)

# Create the session (approx. 1 line)
sess = tf.Session()

# Run the session (approx. 1 line)
one_hot = sess.run(one_hot_matrix, feed_dict={labels:labels, C:C})

# Close the session (approx. 1 line). See method 1 above.
sess.close()

### END CODE HERE ###

return one_hot
</code></pre>

<p>And when running this: </p>

<pre><code>labels = np.array([1,2,3,0,2,1])
one_hot = one_hot_matrix(labels, C = 4)
print (""one_hot = "" + str(one_hot))
</code></pre>

<p>I get this type error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-113-2b9d0290645f&gt; in &lt;module&gt;()
      1 labels = np.array([1,2,3,0,2,1])
----&gt; 2 one_hot = one_hot_matrix(labels, C = 4)
      3 print (""one_hot = "" + str(one_hot))

&lt;ipython-input-112-f9f17c86d0ba&gt; in one_hot_matrix(labels, C)
     28 
     29     # Run the session (approx. 1 line)
---&gt; 30     one_hot = sess.run(one_hot_matrix, feed_dict={labels:labels, C:C})
     31 
     32     # Close the session (approx. 1 line). See method 1 above.

TypeError: unhashable type: 'numpy.ndarray'ter code here
</code></pre>

<p>I checked the Tensorflow documentation for tf.one_hot and there shouldn't be a problem with np.arrays.</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/one_hot"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/one_hot</a></p>
",I am working on a Tensor Flow Coursera Course and I dont understand why I am getting a type mismatch. This is the function I am defining: And when running this: I get this type error: I checked the Tensorflow documentation for tf.one_hot and there shouldn't be a problem with np.arrays. https://www.tensorflow.org/api_docs/python/tf/one_hot,https://stackoverflow.com/questions/46372554,5462568,Documentation Ambiguity
46381790,How does TensorFlow handle none shape?,"<p>I'm trying to implement a simple computational graph framework and test it with simple neural network, mainly by learning from TensorFlow. Now I would want to be clear how does TensorFlow handle none shape tensors.</p>

<p>In <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/multilayer_perceptron.py"" rel=""nofollow noreferrer"">this example</a>, <code>X</code> has shape <code>[None, n_input]</code>, <code>weights['h1']</code> has shape <code>[n_input, n_hidden_1]</code>, and <code>biases['b1']</code> has shape <code>[n_hidden_1]</code>. When it tries to do this: <code>layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])</code>, <code>tf.matmul(x, weights['h1'])</code> should have shape <code>[None, n_hidden_1]</code>, and how exactly does TensorFlow add it with <code>biases['b1']</code>? Based on the <a href=""https://www.tensorflow.org/api_docs/python/tf/add"" rel=""nofollow noreferrer"">documentation</a>, <code>tf.add</code> only works when the 2 operands have the same shape. If we run with a batch of size 10, <code>tf.matmul(x, weights['h1'])</code> will have shape <code>[10, n_hidden_1]</code>, and it shouldn't be able to be added with <code>biases['b1']</code>.</p>
","I'm trying to implement a simple computational graph framework and test it with simple neural network, mainly by learning from TensorFlow. Now I would want to be clear how does TensorFlow handle none shape tensors. In this example, X has shape [None, n_input], weights['h1'] has shape [n_input, n_hidden_1], and biases['b1'] has shape [n_hidden_1]. When it tries to do this: layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']), tf.matmul(x, weights['h1']) should have shape [None, n_hidden_1], and how exactly does TensorFlow add it with biases['b1']? Based on the documentation, tf.add only works when the 2 operands have the same shape. If we run with a batch of size 10, tf.matmul(x, weights['h1']) will have shape [10, n_hidden_1], and it shouldn't be able to be added with biases['b1'].",https://stackoverflow.com/questions/46381790,7420679,Documentation Replication on Other Examples
46418686,tf.nn.dynamic_rnn shape error in seq2seq,"<p>I am attempting to write my own basic seq2seq classifier. Im doing this by using <code>tf.nn.dynamic_rnn</code> and the code is shown below. However, there seems to be a problem with the shape of the tensor I'm sending to <code>tf.nn.dynamic_rnn</code>. The reason I'm doing this is because tensorflow's documentation when it comes to seq2seq is very much all over the place.</p>

<p>Running </p>

<pre><code>import numpy as np
source_batch = np.random.randint(x_letters, size=[batch_size, x_seq_length])
target_batch = np.random.randint(y_letters, size=[batch_size, y_seq_length+1])

sess.run(tf.global_variables_initializer())
loss = sess.run([loss],
            feed_dict = {inputs: source_batch, 
                         outputs: target_batch[:, :-1], 
                         targets: target_batch[:, 1:]})
</code></pre>

<p>gives me the error: <code>ValueError: Cannot feed value of shape (128, 10) for Tensor 'decoding/rnn/transpose:0', which has shape '(128, 10, 32)'</code>.</p>

<p><strong>The graph</strong> is shown below:</p>

<pre><code>import tensorflow as tf

x_seq_length = 29
y_seq_length = 10

x_letters = 60
y_letters = 13

epochs = 2
batch_size = 128
nodes = 32
embed_size = 10

####################
# Tensorflow Graph
####################
tf.reset_default_graph()
sess = tf.InteractiveSession()

inputs = tf.placeholder(tf.int32, (batch_size, x_seq_length), 'inputs')
outputs = tf.placeholder(tf.int32, (batch_size, y_seq_length), 'output')
targets = tf.placeholder(tf.int32, (batch_size, y_seq_length), 'targets')

input_embedding = tf.Variable(tf.random_uniform((x_letters, embed_size), -1, 1), name='enc_embedding')
output_embedding = tf.Variable(tf.random_uniform((y_letters, embed_size), -1, 1), name='dec_embedding')

date_input_embed = tf.nn.embedding_lookup(input_embedding, inputs)
date_output_embed = tf.nn.embedding_lookup(output_embedding, outputs)

with tf.variable_scope(""encoding"") as encoding_scope:
    lstm_enc = tf.contrib.rnn.BasicLSTMCell(nodes)
    _, last_state = tf.nn.dynamic_rnn(lstm_enc, dtype=tf.float32,inputs=date_input_embed)

with tf.variable_scope(""decoding"") as decoding_scope:
    lstm_dec = tf.contrib.rnn.BasicLSTMCell(nodes)
    outputs, _ = tf.nn.dynamic_rnn(lstm_dec, inputs=date_output_embed, initial_state=last_state)

logits = tf.contrib.layers.fully_connected(outputs, num_outputs=y_letters, activation_fn=None) 

with tf.name_scope(""optimization""):
    loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, y_seq_length]))
    optimizer = tf.train.AdamOptimizer().minimize(loss)
</code></pre>
","I am attempting to write my own basic seq2seq classifier. Im doing this by using tf.nn.dynamic_rnn and the code is shown below. However, there seems to be a problem with the shape of the tensor I'm sending to tf.nn.dynamic_rnn. The reason I'm doing this is because tensorflow's documentation when it comes to seq2seq is very much all over the place. Running gives me the error: ValueError: Cannot feed value of shape (128, 10) for Tensor 'decoding/rnn/transpose:0', which has shape '(128, 10, 32)'. The graph is shown below:",https://stackoverflow.com/questions/46418686,2530674,Documentation Replicability
46484373,Return a tf.Variable from an Estimator,"<p>I have an Tensorflow Estimator defined by a model function in the usual way. 
I want to determine which of my (zscore normalised) inputs are significant to the result, and which can be eliminated.   I have altered the model to introduce two changes: </p>

<p>(1) A new layer <code>weight_layer</code> which is randomly intialized and elementwise multiplied with <code>input_layer</code>. </p>

<pre><code>weight_layer   =  tf.Variable(tf.random_normal([1, inputs_n], 0.5, 1))
weighted_input = tf.multiply(weight_layer, input_layer)
first_hidden_layer =  tf.layers.dense(weighted_input,  
                                      int(inputs_n), 
                                      activation=tf.nn.relu, 
                                      name='dense1') 
</code></pre>

<p>(2) A penalty <code>sparsity</code> which is added to the loss function to penalize the loss by the sum of the weights in <code>weight_layer</code></p>

<pre><code> sparsity = tf.reduce_sum(weight_layer)    
 loss = tf.losses.mean_squared_error(labels, predictions) + (1000*sparsity)
</code></pre>

<p>The trouble comes at prediction time, when I try to return the values of <code>weight_layer</code>, as follows: </p>

<pre><code>if mode == tf.estimator.ModeKeys.PREDICT:
   return tf.estimator.EstimatorSpec( mode=mode, 
                                      predictions={
                                       ""predictions"": predictions, 
                                       ""sparsity"" : weight_layer})
</code></pre>

<p>I get the following error: </p>

<pre><code> TypeError: predictions[sparsity] must be Tensor, 
 given: &lt;tf.Variable 'Model/Variable:0' shape=(1, 275) dtype=float32_ref&gt;  
</code></pre>

<p>This seems odd, since although predictions[sparsity] is not a Tensor, it is a tf.Variable, and the tf.Variable documentation suggests I can treat a tf.Variable 'like a normal tf.Tensor'.  </p>

<p>How can I fix the above to return the weight_layer, or if I there is a more fundamental mistake, please recommend a way for me to determine which of my input variables are significant. </p>
","I have an Tensorflow Estimator defined by a model function in the usual way. I want to determine which of my (zscore normalised) inputs are significant to the result, and which can be eliminated. I have altered the model to introduce two changes: (1) A new layer weight_layer which is randomly intialized and elementwise multiplied with input_layer. (2) A penalty sparsity which is added to the loss function to penalize the loss by the sum of the weights in weight_layer The trouble comes at prediction time, when I try to return the values of weight_layer, as follows: I get the following error: This seems odd, since although predictions[sparsity] is not a Tensor, it is a tf.Variable, and the tf.Variable documentation suggests I can treat a tf.Variable 'like a normal tf.Tensor'. How can I fix the above to return the weight_layer, or if I there is a more fundamental mistake, please recommend a way for me to determine which of my input variables are significant.",https://stackoverflow.com/questions/46484373,1536634,Documentation Replication on Other Examples
46658607,where is tf.nn.l2_loss defined?,"<p>According to this documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss</a> it says</p>

<pre><code>Defined in tensorflow/python/ops/gen_nn_ops.py.
</code></pre>

<p>But when I go to tensorflow/python/ops/gen_nn_ops.py there is no l2_loss defined.</p>

<p>I'm trying to see what would be the difference between using <code>tf.nn.l2_loss(W)</code> or just using <code>tf.reduce_sum(tf.square(W))</code>.</p>
",According to this documentation https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss it says But when I go to tensorflow/python/ops/gen_nn_ops.py there is no l2_loss defined. I'm trying to see what would be the difference between using tf.nn.l2_loss(W) or just using tf.reduce_sum(tf.square(W)).,https://stackoverflow.com/questions/46658607,3907250,Documentation Ambiguity
46659101,Using `softmax_cross_entropy_with_logits()` with `seq2seq.sequence_loss()`,"<p>I have a working RNN using the default softmax loss function for <code>tf.contrib.seq2seq.sequence_loss()</code> (which I'm assuming is <code>tf.nn.softmax()</code>) but would instead like to use <code>tf.nn.softmax_cross_entropy_with_logits()</code>. According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss"" rel=""nofollow noreferrer"">seq2seq.sequence_loss</a> documentation, one may use <code>softmax_loss_function=</code> to override the default loss function:</p>

<blockquote>
  <p><strong>softmax_loss_function</strong>: Function (labels, logits) -> loss-batch to be
  used instead of the standard softmax (the default if this is None).
  Note that to avoid confusion, it is required for the function to
  accept named arguments.</p>
</blockquote>

<p>Here is my code that works:</p>

<pre><code>from tensorflow.python.layers.core import Dense

# Build the graph
train_graph = tf.Graph()
# Set the graph to default to ensure that it is ready for training
with train_graph.as_default():

    # Load the model inputs    
    input_data, targets, keep_prob, lr, target_sequence_length, max_target_sequence_length, source_sequence_length \
    = get_model_inputs()

    # Create the training and inference logits
    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, 
                                                                      targets, 
                                                                      lr, 
                                                                      target_sequence_length, 
                                                                      max_target_sequence_length, 
                                                                      source_sequence_length,
                                                                      len(source_letter_to_int),
                                                                      len(target_letter_to_int),
                                                                      encoding_embedding_size, 
                                                                      decoding_embedding_size, 
                                                                      rnn_size, 
                                                                      num_layers,
                                                                      keep_prob)    

    # Create tensors for the training logits and inference logits
    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')
    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')

    # Create the weights for sequence_loss
    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')

    with tf.name_scope(""optimization""):

        # Loss function
        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)

        # Optimizer
        optimizer = tf.train.AdamOptimizer(lr)

        # Gradient Clipping
        gradients = optimizer.compute_gradients(cost)
        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]
        train_op = optimizer.apply_gradients(capped_gradients)

        # Add variables to collection in order to load them up when retraining a saved graph
        tf.add_to_collection(""cost"", cost)
        tf.add_to_collection(""train_op"", train_op)
</code></pre>

<p>My attempt to change the loss function is as follows (I've only indicated the code that is different):</p>

<pre><code>with tf.name_scope(""optimization""):

    # One-hot encode targets and reshape to match logits, one row per batch_size per step
    y_one_hot = tf.one_hot(targets, len(target_letter_to_int))
    y_reshaped = tf.reshape(y_one_hot, [batch_size, len(target_letter_to_int), 30])

    # Loss function
    loss = tf.nn.softmax_cross_entropy_with_logits(logits=training_logits, labels=y_reshaped)
    loss = tf.reduce_mean(loss)
    cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks, softmax_loss_function=loss)
</code></pre>

<p>The line <code>cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks, softmax_loss_function=loss)</code> is now giving me ""<strong>TypeError</strong>: 'Tensor' object is not callable."" This is one of the most opaque errors I've seen Tensorflow produce and I haven't found much of anything in the way of explanation on the internet. Any help would be appreciated.</p>
","I have a working RNN using the default softmax loss function for tf.contrib.seq2seq.sequence_loss() (which I'm assuming is tf.nn.softmax()) but would instead like to use tf.nn.softmax_cross_entropy_with_logits(). According to the seq2seq.sequence_loss documentation, one may use softmax_loss_function= to override the default loss function: Here is my code that works: My attempt to change the loss function is as follows (I've only indicated the code that is different): The line cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks, softmax_loss_function=loss) is now giving me ""TypeError: 'Tensor' object is not callable."" This is one of the most opaque errors I've seen Tensorflow produce and I haven't found much of anything in the way of explanation on the internet. Any help would be appreciated.",https://stackoverflow.com/questions/46659101,852795,Documentation Replication on Other Examples
46752071,Feed a Tensor of SparseTensors to estimators,"<p>To get started with TF, I wanted to learn a predictor of match outcomes for a game. There are three features: the 5 heros on team 0, the 5 heroes on team 1, and the map. The winner is the label, 0 or 1.  I want to represent the teams and the maps as SparseTensors. Out of a possible 71 heroes, five will be selected. Likewise for maps, out of a possible 13, one will be selected.</p>

<pre><code>import tensorflow as tf
import packunpack as source
import tempfile
from collections import namedtuple

GameRecord = namedtuple('GameRecord', 'team_0 team_1 game_map winner')
def parse(line):
    parts = line.rstrip().split(""\t"")
    return GameRecord(
        game_map = parts[1], 
        team_0 = parts[2].split("",""), 
        team_1 = parts[3].split("",""), 
        winner = int(parts[4]))

def conjugate(record):
    return GameRecord(
        team_0 = record.team_1, 
        team_1 = record.team_0, 
        game_map = record.game_map, 
        winner = 0 if record.winner == 1 else 1)

def sparse_team(team):
    indices = list(map(lambda x: [x], map(source.encode_hero, team)))
    return tf.SparseTensor(indices=indices, values = [1] * len(indices), dense_shape=[len(source.heroes_array)])

def sparse_map(map_name):
    return tf.SparseTensor(indices=[[source.encode_hero(map_name)]], values = [1], dense_shape=[len(source.maps_array)])

def make_input_fn(filename, shuffle = True, add_conjugate_games = True):
    def _fn():
        records = []
        with open(filename, ""r"") as raw:
            i = 0
            for line in raw:
                record = parse(line)
                records.append(record)
                if add_conjugate_games:
                # since 0 and 1 are arbitrary team labels, learn and test the conjugate game whenever
                # learning the original inference
                    records.append(conjugate(record))

        print(""Making team 0"")
        team_0s = tf.constant(list(map(lambda r: sparse_team(r.team_0), records)))
        print(""Making team 1"")
        team_1s = tf.constant(list(map(lambda r: sparse_team(r.team_1), records)))
        print(""making maps"")
        maps = tf.constant(list(map(lambda r: sparse_map(r.game_map), records)))
        print(""Making winners"")
        winners = tf.constant(list(map(lambda r: tf.constant([r.winner]), records)))

        return {
                    ""team_0"": team_0s,
                    ""team_1"": team_1s,
                    ""game_map"": maps,
                }, winners
        #Please help me finish this function?

    return _fn

team_0 = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(""team_0"", source.heroes_array), len(source.heroes_array))
team_1 = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(""team_1"", source.heroes_array), len(source.heroes_array))
game_map = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(""game_map"", source.maps_array), len(source.maps_array))

model_dir = tempfile.mkdtemp()
m = tf.estimator.DNNClassifier(
    model_dir=model_dir,
    hidden_units = [1024, 512, 256], 
    feature_columns=[team_0, team_1, game_map])

def main():
    m.train(input_fn=make_input_fn(""tiny.txt""), steps = 100)

if __name__ == ""__main__"":
    main()
</code></pre>

<p>This fails on <code>team_0s = tf.constant(list(map(lambda r: sparse_team(r.team_0), records)))</code></p>

<p>It's very difficult to understand what tf wants me to return in my input_fn, because all of the examples I can find in the docs ultimately call out to a pandas or numpy helper function, and I'm not familiar with those frameworks. I thought that each dictionary value should be a Tensor containing all examples of a single feature. Each of my examples is a SparseTensor, and I want to simply embed them as their dense versions for the sake of the DNNClassifier.</p>

<p>I'm sure my mental model is horribly broken right now, and I appreciate any help setting it straight.</p>

<p>Error output:</p>

<pre><code>python3 estimator.py
Making team 0
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py"", line 468, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py"", line 468, in &lt;listcomp&gt;
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got &lt;tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fe8
b4d7aef0&gt;

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""estimator.py"", line 79, in &lt;module&gt;
    main()
  File ""estimator.py"", line 76, in main
    m.train(input_fn=make_input_fn(""tiny.txt""), steps = 100)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 709, in _train_model
    input_fn, model_fn_lib.ModeKeys.TRAIN)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 577, in _get_features_and_l
abels_from_input_fn
    result = self._call_input_fn(input_fn, mode)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 663, in _call_input_fn
    return input_fn(**kwargs)
  File ""estimator.py"", line 44, in _fn
    team_0s = tf.constant(list(map(lambda r: sparse_team(r.team_0), records)))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/constant_op.py"", line 208, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_util.py"", line 472, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type &lt;class 'list'&gt; to Tensor. Contents: [&lt;tensorflow.python.framework.sparse_tenso
r.SparseTensor object at 0x7fe8b4d7aef0&gt;, &lt;tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fe8b4d7af28
&gt;, &lt;tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fe8b4d7af60&gt;, &lt;tensorflow.python.framework.sparse_
tensor.SparseTensor object at 0x7fe8b4d7aeb8&gt; ... ]
</code></pre>
","To get started with TF, I wanted to learn a predictor of match outcomes for a game. There are three features: the 5 heros on team 0, the 5 heroes on team 1, and the map. The winner is the label, 0 or 1. I want to represent the teams and the maps as SparseTensors. Out of a possible 71 heroes, five will be selected. Likewise for maps, out of a possible 13, one will be selected. This fails on team_0s = tf.constant(list(map(lambda r: sparse_team(r.team_0), records))) It's very difficult to understand what tf wants me to return in my input_fn, because all of the examples I can find in the docs ultimately call out to a pandas or numpy helper function, and I'm not familiar with those frameworks. I thought that each dictionary value should be a Tensor containing all examples of a single feature. Each of my examples is a SparseTensor, and I want to simply embed them as their dense versions for the sake of the DNNClassifier. I'm sure my mental model is horribly broken right now, and I appreciate any help setting it straight. Error output:",https://stackoverflow.com/questions/46752071,86432,Inadequate Examples
46759271,Image pixel value normalized for tf.image.decode_jpeg and tf.train.shuffle_batch?,"<p>I am trying to use the tf.train.shuffle_batch function from tensorflow, then I need to first load the images using tf.image.decode_jpeg(or other similar functions to load png and jpg). But I just found out that the images are loaded as probability map, which means the max of the value of pixel is 1, and the min of the value of the pixel is 0. Below is my code updated from a github repo. I don't know why the values of pixels are normalized to [0,1], and I don't find related documentation on tensorflow. Could anyone help me? Thanks.  </p>

<pre><code>def load_examples(self, input_dir,  flip, scale_size, batch_size, min_queue_examples):
    input_paths = get_image_paths(input_dir)
    with tf.name_scope(""load_images""):
        path_queue = tf.train.string_input_producer(input_paths)
        reader = tf.WholeFileReader()
        paths, contents = reader.read(path_queue)
        # note this is important for truncated images
        raw_input = tf.image.decode_jpeg(contents,try_recover_truncated = True, acceptable_fraction=0.5)
        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)
        raw_input.set_shape([None, None, 3])

        # break apart image pair and move to range [-1, 1]
        width = tf.shape(raw_input)[1]  # [height, width, channels]
        a_images = preprocess(raw_input[:, :width // 2, :])
        b_images = raw_input[:, width // 2:, :]

    inputs, targets = [a_images, b_images]

    def transform(image):
        r = image

        r = tf.image.resize_images(r, [self.image_height, self.image_width], method=tf.image.ResizeMethod.AREA)
        return r
    def transform_gaze(image):
        r = image
        r = tf.image.resize_images(r, [self.gaze_height, self.gaze_width], method=tf.image.ResizeMethod.AREA)
        return r
    with tf.name_scope(""input_images""):
        input_images = transform(inputs)

    with tf.name_scope(""target_images""):
        target_images = transform(targets)
    total_image_count = len(input_paths)
    # target_images = tf.image.per_image_standardization(target_images)
    target_images = target_images[:,:,0]
    target_images = tf.expand_dims(target_images, 2)
    inputs_batch, targets_batch = tf.train.shuffle_batch([input_images, target_images],
                                         batch_size=batch_size,
                                         num_threads=1,
                                         capacity=min_queue_examples + 3 * batch_size,
                                         min_after_dequeue=min_queue_examples)
    # inputs_batch, targets_batch = tf.train.batch([input_images, target_images],batch_size=batch_size)
    return inputs_batch, targets_batch, total_image_count
</code></pre>
","I am trying to use the tf.train.shuffle_batch function from tensorflow, then I need to first load the images using tf.image.decode_jpeg(or other similar functions to load png and jpg). But I just found out that the images are loaded as probability map, which means the max of the value of pixel is 1, and the min of the value of the pixel is 0. Below is my code updated from a github repo. I don't know why the values of pixels are normalized to [0,1], and I don't find related documentation on tensorflow. Could anyone help me? Thanks.",https://stackoverflow.com/questions/46759271,3173450,Documentation Replication on Other Examples
46885191,tf.nn.conv2d_transpose output_shape dynamic batch_size,"<p>The documentation of tf.nn.conv2d_transpose says:</p>

<pre><code>tf.nn.conv2d_transpose(
    value,
    filter,
    output_shape,
    strides,
    padding='SAME',
    data_format='NHWC',
    name=None
)
</code></pre>

<p>The output_shape argument requires a 1D tensor specifying the shape of the tensor output by this op. Here, since my conv-net part has been built entirely on dynamic batch_length placeholders, I can't seem to device a workaround to the static <code>batch_size</code> requirement of the output_shape for this op. </p>

<p>There are many discussions around the web for this, however, I couldn't find any solid solution to this issue. Most of them are hacky ones with a <code>global_batch_size</code> variable defined. I wish to know the best possible solution to this problem. This trained model is going be shipped as a deployed service.</p>
","The documentation of tf.nn.conv2d_transpose says: The output_shape argument requires a 1D tensor specifying the shape of the tensor output by this op. Here, since my conv-net part has been built entirely on dynamic batch_length placeholders, I can't seem to device a workaround to the static batch_size requirement of the output_shape for this op. There are many discussions around the web for this, however, I couldn't find any solid solution to this issue. Most of them are hacky ones with a global_batch_size variable defined. I wish to know the best possible solution to this problem. This trained model is going be shipped as a deployed service.",https://stackoverflow.com/questions/46885191,4341842,Inadequate Examples
46976226,`tf.estimator.RunConfig` vs `tf.contrib.learn.RunConfig`,"<p>I am confused regarding whether I should be using <code>tf.estimator.RunConfig</code> or <code>tf.contrib.learn.RunConfig</code> to pass a <code>RunConfig</code> to an estimator. </p>

<p>using <code>tf.contrib.learn.RunConfig</code> is straightforward:</p>

<pre><code>rc = tf.contrib.learn.RunConfig(save_checkpoints_secs=1,
                                model_dir=model_dir)
</code></pre>

<p>But <code>tf.estimator.RunConfig</code> has some odd syntax:</p>

<pre><code>rc = tf.estimator.RunConfig()
rc = rc.replace(save_checkpoints_secs=1,
                model_dir=model_dir)
</code></pre>

<p>Is there any reason to prefer one <code>RunConfig</code> over the other? The documentation is not clear on this.</p>
",I am confused regarding whether I should be using tf.estimator.RunConfig or tf.contrib.learn.RunConfig to pass a RunConfig to an estimator. using tf.contrib.learn.RunConfig is straightforward: But tf.estimator.RunConfig has some odd syntax: Is there any reason to prefer one RunConfig over the other? The documentation is not clear on this.,https://stackoverflow.com/questions/46976226,592235,Lack of Alternative Solutions/Documentation
47119604,"In tensorflow, does tf.summary record average values over multiple steps?","<p>By default, <code>RunConfig.save_summary_steps</code> is 100 in <code>tf.estimator.Estimator</code>, so it saves summaries every 100 steps. At each time it saves a summary, does it just save the current summary value computed from the current <code>step/minibatch</code>? Or it saves the average summary values computed from the recent 100 <code>steps/minibatches</code>? I cannot find a clear description for this in the official documentation.</p>
","By default, RunConfig.save_summary_steps is 100 in tf.estimator.Estimator, so it saves summaries every 100 steps. At each time it saves a summary, does it just save the current summary value computed from the current step/minibatch? Or it saves the average summary values computed from the recent 100 steps/minibatches? I cannot find a clear description for this in the official documentation.",https://stackoverflow.com/questions/47119604,3295829,Lack of Alternative Solutions/Documentation
47205160,Tensorflow v1.4: Layer.input not supported in Eager mode,"<p>I understand that Eager mode is a new alpha feature on the nightly builds and that it is not perfect yet, but I do not know if there are any tf.keras workarounds for this problem.</p>

<p>The error <code>Layer.input not supported in Eager mode.</code> triggers on the block</p>

<pre><code>model = tf.keras.models.Sequential()
model.add(tf.layers.Dense(2, input_shape = (None, 1)))
model.add(tf.layers.Dense(units = 1))
model.compile(optimizer = ""sgd"", loss = ""mean_squared_error"")
</code></pre>

<p>I do not know anything about keras or the keras tensorflow API and I was wondering if there was a way to avoid <code>Layer.input</code> with keras techniques so as to stay within Eager mode. Following a tutorial in the tf.Eager docs I have confirmed that <code>model = tf.layers.Dense(1)</code> works but I don't know how to add another layer.</p>

<p>Any help is very much appreciated.</p>

<p><strong>EDIT</strong>
As of tensorflow v1.10, keras is supported in eager mode.</p>
","I understand that Eager mode is a new alpha feature on the nightly builds and that it is not perfect yet, but I do not know if there are any tf.keras workarounds for this problem. The error Layer.input not supported in Eager mode. triggers on the block I do not know anything about keras or the keras tensorflow API and I was wondering if there was a way to avoid Layer.input with keras techniques so as to stay within Eager mode. Following a tutorial in the tf.Eager docs I have confirmed that model = tf.layers.Dense(1) works but I don't know how to add another layer. Any help is very much appreciated. EDIT As of tensorflow v1.10, keras is supported in eager mode.",https://stackoverflow.com/questions/47205160,5496253,Documentation Replication on Other Examples
47319390,Why does this TensorFlow code behave differently when inside a test case?,"<p>I have a function (<code>foo</code> below) which is behaving differently when it's run directly vs when it is run inside a <code>tf.test.TestCase</code>.</p>

<p>The code is supposed to create a dataset with elems [1..5] and shuffle it. Then it repeats 3 times: create an iterator from the data and use that to print the 5 elements.</p>

<p>When run on its own it gives output where all the lists are shuffled e.g.:</p>

<pre><code>[4, 0, 3, 2, 1]
[0, 2, 1, 3, 4]
[2, 3, 4, 0, 1]
</code></pre>

<p>but when run inside a test case they are always the same, even between runs:</p>

<pre><code>[0, 4, 2, 3, 1]
[0, 4, 2, 3, 1]
[0, 4, 2, 3, 1]
</code></pre>

<p>I imagine it's something to do with how test cases handle random seeds but I can't see anything about that in the TensorFlow docs. Thanks for any help!</p>

<hr>

<h2>Code:</h2>

<pre><code>import tensorflow as tf

def foo():
    sess = tf.Session()
    dataset = tf.data.Dataset.range(5)
    dataset = dataset.shuffle(5, reshuffle_each_iteration=False)

    for _ in range(3):
        data_iter = dataset.make_one_shot_iterator()
        next_item = data_iter.get_next()
        with sess.as_default():
            data_new = [next_item.eval() for _ in range(5)]
        print(data_new)


class DatasetTest(tf.test.TestCase):
    def testDataset(self):
        foo()

if __name__ == '__main__':
    foo()
    tf.test.main()
</code></pre>

<p>I am running it with Python 3.6 and TensorFlow 1.4. No other modules should be needed.</p>
","I have a function (foo below) which is behaving differently when it's run directly vs when it is run inside a tf.test.TestCase. The code is supposed to create a dataset with elems [1..5] and shuffle it. Then it repeats 3 times: create an iterator from the data and use that to print the 5 elements. When run on its own it gives output where all the lists are shuffled e.g.: but when run inside a test case they are always the same, even between runs: I imagine it's something to do with how test cases handle random seeds but I can't see anything about that in the TensorFlow docs. Thanks for any help! I am running it with Python 3.6 and TensorFlow 1.4. No other modules should be needed.",https://stackoverflow.com/questions/47319390,1015528,Documentation Replication on Other Examples
47327256,Understanding Tensor Inputs & Transformations for use in an LSTM (dynamic RNN),"<p>I am building an LSTM style neural network in Tensorflow and am having some difficulty understanding exactly what input is needed and the subsequent transformations made by <em>tf.nn.dynamic_rnn</em> before it is passed to the <em>sparse_softmax_cross_entropy_with_logits</em> layer.</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a></p>

<h2>Understanding the input</h2>

<p>The input function is sending a feature tensor in the form </p>

<p><strong>[batch_size, max_time]</strong></p>

<p>However the manual states that input tensors must be in the form</p>

<p><strong>[batch_size, max_time, ...]</strong></p>

<p>I have therefore expanded the input with a 1d tensor to take the form </p>

<p><strong>[batch_size, max_time, 1]</strong></p>

<p>At this point the input does not break upon running, but I don't understand exactly what we have done here and suspect it may be causing the problems when calculating loss (see below).</p>

<h2>Understanding the Transformations</h2>

<p>This expanded tensor is then the 'features' tensor used in the code below</p>

<pre><code>LSTM_SIZE = 3
lstm_cell = rnn.BasicLSTMCell(LSTM_SIZE, forget_bias=1.0)
outputs, _ = tf.nn.dynamic_rnn(lstm_cell, features, dtype=tf.float64)

#slice to keep only the last cell of the RNN
outputs = outputs[-1]

#softmax layer

with tf.variable_scope('softmax'):
   W = tf.get_variable('W', [LSTM_SIZE, n_classes], dtype=tf.float64)
   b = tf.get_variable('b', [n_classes], initializer=tf.constant_initializer(0.0), dtype=tf.float64)

logits = tf.matmul(outputs, W) + b

loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))
</code></pre>

<p>This throws a value error at <em>loss</em></p>

<p>dimensions must be equal, but are <strong>[max_time, num_classes] and [batch_size]</strong></p>

<p>from <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/classification"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/classification</a> -</p>

<p><em>A common use case is to have logits of shape [batch_size, num_classes] and labels of shape [batch_size]. But higher dimensions are supported.</em></p>

<p>At some point in the process max_time and batch_size have been mixed up and I'm uncertain if its at input or during the LSTM. I'm grateful for any advice!</p>
","I am building an LSTM style neural network in Tensorflow and am having some difficulty understanding exactly what input is needed and the subsequent transformations made by tf.nn.dynamic_rnn before it is passed to the sparse_softmax_cross_entropy_with_logits layer. https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn The input function is sending a feature tensor in the form [batch_size, max_time] However the manual states that input tensors must be in the form [batch_size, max_time, ...] I have therefore expanded the input with a 1d tensor to take the form [batch_size, max_time, 1] At this point the input does not break upon running, but I don't understand exactly what we have done here and suspect it may be causing the problems when calculating loss (see below). This expanded tensor is then the 'features' tensor used in the code below This throws a value error at loss dimensions must be equal, but are [max_time, num_classes] and [batch_size] from https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/classification - A common use case is to have logits of shape [batch_size, num_classes] and labels of shape [batch_size]. But higher dimensions are supported. At some point in the process max_time and batch_size have been mixed up and I'm uncertain if its at input or during the LSTM. I'm grateful for any advice!",https://stackoverflow.com/questions/47327256,7822780,Lack of Alternative Solutions/Documentation
47380573,How to properly update variables in a while loop in TensorFlow?,"<p>Can someone please explain (or point me to the relevant place in the documentation that I've missed) how to properly update a <code>tf.Variable()</code> in a <code>tf.while_loop</code>? I am trying to update variables in the loop that will store some information until the next iteration of the loop using the <code>assign()</code> method. However, this isn't doing anything.</p>

<p>As the values of <code>mu_tf</code> and <code>sigma_tf</code> are being updated by the minimizer, while <code>step_mu</code> isn't, I am obviously doing something wrong, but I don't understand what it is. Specifically, I guess I should say that I know <a href=""https://stackoverflow.com/a/34220750/8931942""><code>assign()</code> does not do anything until it is executed when the graph is run</a>, so I know that I can do</p>

<p><code>sess.run(step_mu.assign(mu_tf))</code></p>

<p>and that will update <code>step_mu</code>, but I want to do this in the loop correctly. I don't understand how to add an <code>assign</code> operation to the body of the loop.</p>

<p>A simplified working example of what I'm doing follows here:</p>

<pre><code>import numpy as np
import tensorflow as tf

mu_true = 0.5
sigma_true = 1.5

n_events = 100000

# Placeholders
X = tf.placeholder(dtype=tf.float32)

# Variables
mu_tf = tf.Variable(initial_value=tf.random_normal(shape=[], mean=0., stddev=0.1,
                                                dtype=tf.float32),
                    dtype=tf.float32)
sigma_tf = tf.Variable(initial_value=tf.abs(tf.random_normal(shape=[], mean=1., stddev=0.1,
                                                dtype=tf.float32)),
                       dtype=tf.float32,
                       constraint=lambda x: tf.abs(x))

step_mu = tf.Variable(initial_value=-99999., dtype=tf.float32)   
step_loss = tf.Variable(initial_value=-99999., dtype=tf.float32)

# loss function
gaussian_dist = tf.distributions.Normal(loc=mu_tf, scale=sigma_tf)
log_prob = gaussian_dist.log_prob(value=X)
negative_log_likelihood = -1.0 * tf.reduce_sum(log_prob)

# optimizer
optimizer = tf.train.AdamOptimizer(learning_rate=0.1)

# sample data
x_sample = np.random.normal(loc=mu_true, scale=sigma_true, size=n_events)

# Construct the while loop.
def cond(step):
    return tf.less(step, 10)

def body(step):
    # gradient step
    train_op = optimizer.minimize(loss=negative_log_likelihood)

    # update step parameters
    with tf.control_dependencies([train_op]):
        step_mu.assign(mu_tf)

        return tf.add(step,1)

loop = tf.while_loop(cond, body, [tf.constant(0)])

# Execute the graph
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    step_loss = sess.run(fetches=negative_log_likelihood, feed_dict={X: x_sample})

    print('Before loop:\n')
    print('mu_tf: {}'.format(sess.run(mu_tf)))
    print('sigma_tf: {}'.format(sess.run(sigma_tf)))
    print('step_mu: {}'.format(sess.run(step_mu)))
    print('step_loss: {}\n'.format(step_loss))

    sess.run(fetches=loop, feed_dict={X: x_sample})

    print('After loop:\n')
    print('mu_tf: {}'.format(sess.run(mu_tf)))
    print('sigma_tf: {}'.format(sess.run(sigma_tf)))
    print('step_mu: {}'.format(sess.run(step_mu)))
    print('step_loss: {}'.format(step_loss))
</code></pre>
","Can someone please explain (or point me to the relevant place in the documentation that I've missed) how to properly update a tf.Variable() in a tf.while_loop? I am trying to update variables in the loop that will store some information until the next iteration of the loop using the assign() method. However, this isn't doing anything. As the values of mu_tf and sigma_tf are being updated by the minimizer, while step_mu isn't, I am obviously doing something wrong, but I don't understand what it is. Specifically, I guess I should say that I know assign() does not do anything until it is executed when the graph is run, so I know that I can do sess.run(step_mu.assign(mu_tf)) and that will update step_mu, but I want to do this in the loop correctly. I don't understand how to add an assign operation to the body of the loop. A simplified working example of what I'm doing follows here:",https://stackoverflow.com/questions/47380573,8931942,Documentation Replicability
47389988,How to control GPU memory size with tf.estimator,"<p>I'm trying to control the size of GPU memory allocated for one tensorflow estimator tf.estimator.Estimator. The purpose is to only allocate half to run other tensorflow net on the same GPU. I found for the contrib version but not for the official. Someone knows if it's possible?</p>
",I'm trying to control the size of GPU memory allocated for one tensorflow estimator tf.estimator.Estimator. The purpose is to only allocate half to run other tensorflow net on the same GPU. I found for the contrib version but not for the official. Someone knows if it's possible?,https://stackoverflow.com/questions/47389988,7007134,Documentation Replication on Other Examples
47568998,Tensorflow: Load data in multiple threads on cpu,"<p>I have a python class <code>SceneGenerator</code> which has multiple member functions for preprocessing and a generator function <code>generate_data()</code>. The basic structure is like this:</p>

<pre><code>class SceneGenerator(object):
    def __init__(self):
       # some inits

    def generate_data(self):
        """"""
        Generator. Yield data X and labels y after some preprocessing
        """"""
        while True:
            # opening files, selecting data
            X,y = self.preprocess(some_params, filenames, ...)            

            yield X, y
</code></pre>

<p>I used the class member function sceneGenerator.generate_data() in keras model.fit_generator() function to read the data from disk, preprocess it and yield it. In keras, this is done on multiple CPU threads, if the <code>workers</code> parameter of <code>model.fit_generator()</code> is set to something > 1.</p>

<p>I now want to use the same <code>SceneGenerator</code> class in tensorflow. My current approach is this:</p>

<pre><code>sceneGenerator = SceneGenerator(some_params)
for X, y in sceneGenerator.generate_data():

    feed_dict = {ops['data']: X,
                 ops['labels']: y,
                 ops['is_training_pl']: True
                 }
    summary, step, _, loss, prediction = sess.run([optimization_op, loss_op, pred_op],
                                                  feed_dict=feed_dict)
</code></pre>

<p>This, however, is slow and does not use multiple threads. I found the <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/data/Dataset"" rel=""noreferrer""><code>tf.data.Dataset</code></a> api with some <a href=""https://www.tensorflow.org/versions/master/programmers_guide/datasets"" rel=""noreferrer"">documentation</a>, but I fail to implement the methods.</p>

<p><strong>Edit:</strong> Notice that I do not work with images so that the image loading mechanisms with file paths etc. do not work here.
My <code>SceneGenerator</code> loads data from hdf5 files. But not complete datasets but - depending on the initialization parameters - only parts of a dataset. I would love to keep the generator function as it is and learn how this generator can be directly used as input for tensorflow and runs on multiple threads on the CPU. Rewriting the data from the hdf5 files to csv is not a good option because it duplicated lots of data.</p>

<p><strong>Edit 2:</strong>: I think something similar to this could help: <a href=""https://stackoverflow.com/questions/47086599/parallelising-tf-data-dataset-from-generator"">parallelising tf.data.Dataset.from_generator</a></p>
","I have a python class SceneGenerator which has multiple member functions for preprocessing and a generator function generate_data(). The basic structure is like this: I used the class member function sceneGenerator.generate_data() in keras model.fit_generator() function to read the data from disk, preprocess it and yield it. In keras, this is done on multiple CPU threads, if the workers parameter of model.fit_generator() is set to something &gt; 1. I now want to use the same SceneGenerator class in tensorflow. My current approach is this: This, however, is slow and does not use multiple threads. I found the tf.data.Dataset api with some documentation, but I fail to implement the methods. Edit: Notice that I do not work with images so that the image loading mechanisms with file paths etc. do not work here. My SceneGenerator loads data from hdf5 files. But not complete datasets but - depending on the initialization parameters - only parts of a dataset. I would love to keep the generator function as it is and learn how this generator can be directly used as input for tensorflow and runs on multiple threads on the CPU. Rewriting the data from the hdf5 files to csv is not a good option because it duplicated lots of data. Edit 2:: I think something similar to this could help: parallelising tf.data.Dataset.from_generator",https://stackoverflow.com/questions/47568998,3971621,Documentation Replication on Other Examples
47644412,TensorFlow Dataset API Parsing Error,"<p>I'm using the TensorFlow Dataset API to parse a CSV file and run a logistic regression. I'm following the example from the TF documentation <a href=""https://github.com/tensorflow/models/blob/master/official/wide_deep/wide_deep.py"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The following code snippet shows how I am setting up the model:</p>

<pre><code>def input_fn(path, num_epochs, batch_size):
    dataset = tf.data.TextLineDataset(path)
    dataset = dataset.map(parse_table, num_parallel_calls=12)
    dataset = dataset.repeat(num_epochs)
    dataset.batch(batch_size)

    iterator = dataset.make_one_shot_iterator()
    features, labels = iterator.get_next()
    return features, labels

def parse_table(value):
    cols = tf.decode_csv(value, record_defaults=TAB_COLUMN_DEFAULTS)
    indep_vars = dict(zip(CSV_COLS, cols))
    y = indep_vars.pop('y')
    return indep_vars, y

def build_indep_vars():
    continuous_vars = [
        tf.feature_column.numeric_column(x, shape=1) for x in CONT_COLS]
    categorical_vars = [
        tf.feature_column.categorical_column_with_hash_bucket(
            x, hash_bucket_size=100) for x in CAT_COLS]
    return categorical_vars + continuous_vars
</code></pre>

<p>When calling <code>lr.train(input_fn = lambda: input_fn(data_path, 1, 100))</code> (note: batch size is 100) I'm getting the error </p>

<pre><code>ValueError: Feature (key: V1) cannot have rank 0. Give: Tensor(""IteratorGetNext:0"", shape=(), dtype=float32, device=/device:CPU:0)
</code></pre>

<p>So I'm assuming this means one of the <code>tf.feature_column.numeric_column</code> calls is getting a scalar value which it doesn't like. However, I cannot figure out why this is the case. I've set <code>batch_size</code> to a positive integer and according to the documentation the shape of the NDarray resulting from <code>tf.feature_column.numeric_column</code> should be <code>1Xbatch_size</code> by default. Can anyone explain why TensorFlow is returning this error?</p>

<p>I'm sure this question has a simple answer that will make me feel stupid for not figuring it out, but after spending some time on this I'm still stumped.</p>
","I'm using the TensorFlow Dataset API to parse a CSV file and run a logistic regression. I'm following the example from the TF documentation here. The following code snippet shows how I am setting up the model: When calling lr.train(input_fn = lambda: input_fn(data_path, 1, 100)) (note: batch size is 100) I'm getting the error So I'm assuming this means one of the tf.feature_column.numeric_column calls is getting a scalar value which it doesn't like. However, I cannot figure out why this is the case. I've set batch_size to a positive integer and according to the documentation the shape of the NDarray resulting from tf.feature_column.numeric_column should be 1Xbatch_size by default. Can anyone explain why TensorFlow is returning this error? I'm sure this question has a simple answer that will make me feel stupid for not figuring it out, but after spending some time on this I'm still stumped.",https://stackoverflow.com/questions/47644412,8152457,Documentation Ambiguity
47665314,how can we get benefit from sharding the data to speed the training time?,"<p>My main issue is : I have 204 GB training tfrecords for 2 million images, and 28GB for validation tf.records files, of 302900 images. it takes 8 hour to train one epoch and this will take 33 day for training. I want to speed that by using multiple threads and shards but I am little bit confused about couple of things.</p>

<p>In <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">tf.data.Dataset API</a> there is shard function , So in the documentation they mentioned the following about shard function : </p>

<blockquote>
  <p>Creates a Dataset that includes only 1/num_shards of this dataset.</p>
  
  <p>This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.</p>
  
  <p>When reading a single input file, you can skip elements as follows:</p>
</blockquote>

<pre><code>d = tf.data.TFRecordDataset(FLAGS.input_file)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
d = d.repeat(FLAGS.num_epochs)
d = d.shuffle(FLAGS.shuffle_buffer_size)
d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)
</code></pre>

<blockquote>
  <p>Important caveats:</p>
  
  <p>Be sure to shard before you use any randomizing operator (such as shuffle).
  Generally it is best if the shard operator is used early in the dataset pipeline. >For example, when reading from a set of TFRecord files, shard before converting >the dataset to input samples. This avoids reading every file on every worker. The >following is an example of an efficient sharding strategy within a complete >pipeline:</p>
</blockquote>

<pre><code>d = Dataset.list_files(FLAGS.pattern)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
d = d.repeat(FLAGS.num_epochs)
d = d.shuffle(FLAGS.shuffle_buffer_size)
d = d.repeat()
d = d.interleave(tf.data.TFRecordDataset,
             cycle_length=FLAGS.num_readers, block_length=1)

d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)
</code></pre>

<p>So my question regarding the code above is when I try to makes d.shards of my data using shard function, if I set the number of shards (num_workers)to 10 , I will have 10 splits of my data , then should I set the num_reader in d.interleave function to 10 to guarantee that each reader take one split from the 10 split? </p>

<p>and how I can control which split the function interleave will take? because if I set the shard_index (worker_index) in shard function to 1 it will give me the first split. Can anyone give me an idea how can I perform this distributed training using the above functions? </p>

<p>then what about the num_parallel_call . should I set it to 10 as well? </p>

<p>knowing that I have single tf.records file for training and another one for validation , I don't split the tf.records files into multiple files.</p>
","My main issue is : I have 204 GB training tfrecords for 2 million images, and 28GB for validation tf.records files, of 302900 images. it takes 8 hour to train one epoch and this will take 33 day for training. I want to speed that by using multiple threads and shards but I am little bit confused about couple of things. In tf.data.Dataset API there is shard function , So in the documentation they mentioned the following about shard function : So my question regarding the code above is when I try to makes d.shards of my data using shard function, if I set the number of shards (num_workers)to 10 , I will have 10 splits of my data , then should I set the num_reader in d.interleave function to 10 to guarantee that each reader take one split from the 10 split? and how I can control which split the function interleave will take? because if I set the shard_index (worker_index) in shard function to 1 it will give me the first split. Can anyone give me an idea how can I perform this distributed training using the above functions? then what about the num_parallel_call . should I set it to 10 as well? knowing that I have single tf.records file for training and another one for validation , I don't split the tf.records files into multiple files.",https://stackoverflow.com/questions/47665314,8262057,Documentation Replication on Other Examples
47814401,How does tf.layers.batch_normalization calculate mean and variance during test time? (test data has machine-generated samples),"<p>I am trying to implement batch-normalization on my CNN that currently applies dropout. One problem is that I do not know how the mean and variance are calculated during test time.</p>

<p>On the documentation it says that if training=False is set then the normalization is done with moving statistics. What does this mean?</p>

<p>In addition, since my test data has lots of machine-generated samples I cannot use population mean and variance and just apply tf.nn.batch_normalization(). These samples are used to prevent hand labeling and are excluded when scoring my model</p>
","I am trying to implement batch-normalization on my CNN that currently applies dropout. One problem is that I do not know how the mean and variance are calculated during test time. On the documentation it says that if training=False is set then the normalization is done with moving statistics. What does this mean? In addition, since my test data has lots of machine-generated samples I cannot use population mean and variance and just apply tf.nn.batch_normalization(). These samples are used to prevent hand labeling and are excluded when scoring my model",https://stackoverflow.com/questions/47814401,8805495,Documentation Completeness
47898147,Tensorflow Module Import error: AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell',"<p>When attempting to pass my RNN call, I call tf.nn.rnn_cell and I receive the following error: </p>

<pre><code>AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'
</code></pre>

<p>Which is odd, because I'm sure I imported everything correctly: </p>

<pre><code>from __future__ import print_function, division
from tensorflow.contrib import rnn
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
</code></pre>

<p>But looking at the docs, things have moved around between tensorflow versions. </p>

<p>what would you all recommend to fix this?? </p>

<p>Line, I'm getting the error against: </p>

<pre><code>state_per_layer_list = tf.unstack(init_state, axis=0)
rnn_tuple_state = tuple(
    [tf.nn.rnn_cell.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])
     for idx in range(num_layers)]
)
</code></pre>

<p>Specifically: </p>

<pre><code>tf.nn.rnn_cell
</code></pre>

<p>I'm using anaconda 3 to manage all of this so, the dependancies should all be taken care of. I have already tried working around a damn rank/shape error with Tensor shapes which took ages to resolve. </p>

<p>Cheers in advance. </p>
","When attempting to pass my RNN call, I call tf.nn.rnn_cell and I receive the following error: Which is odd, because I'm sure I imported everything correctly: But looking at the docs, things have moved around between tensorflow versions. what would you all recommend to fix this?? Line, I'm getting the error against: Specifically: I'm using anaconda 3 to manage all of this so, the dependancies should all be taken care of. I have already tried working around a damn rank/shape error with Tensor shapes which took ages to resolve. Cheers in advance.",https://stackoverflow.com/questions/47898147,5552621,Documentation Replication on Other Examples
47984876,Tensorflow tf.map_fn parameters,"<p>I'm attempting to structure my parameters so that they will work properly with tf.map_fn() but most of the example documentation only discusses arrays or tensors of the same shape as function arguments.</p>

<p>Links include:</p>

<p><a href=""https://stackoverflow.com/questions/37086098/does-tensorflow-map-fn-support-taking-more-than-one-tensor"">Does tensorflow map_fn support taking more than one tensor?</a></p>

<p>My specific example is this:
I have some tensorflow function that expects [None, 2] and [x,y] as parameter tensor shapes.</p>

<p>Tensor A is of shape [batch_size, x*y, 2]</p>

<p>Tensor B is of shape [batch_size, x, y]</p>

<pre><code>lambdaData = (tensorA, tensorB)
lambdaFunc = lambda x: tensorflowFunc(x[0], x[1])
returnValues = tf.map_fn(lambdaFunc, lambdaData)
</code></pre>

<p>From the tensorflow documentation:</p>

<pre><code>If elems is a (possibly nested) list or tuple of tensors, then each of these 
tensors must have a matching first (unpack) dimension
</code></pre>

<p>Since tensorsA and B only match in dimension 0, I cannot stack or concatenate them; I have also tried creating lambdaData as:</p>

<ol>
<li>A list of two tensors</li>
<li>A tuple of two tensors</li>
<li>A list of tensor pairs</li>
</ol>

<p>All of the above result in varying dimension mismatch errors.  I would follow the recommended use as per documentation of placing all of the data into a single tensor, but because of dimension mismatching between tensorA and tensorB I am unable to.  Has anybody had any luck with tuples or lists of arguments for elems?</p>
","I'm attempting to structure my parameters so that they will work properly with tf.map_fn() but most of the example documentation only discusses arrays or tensors of the same shape as function arguments. Links include: Does tensorflow map_fn support taking more than one tensor? My specific example is this: I have some tensorflow function that expects [None, 2] and [x,y] as parameter tensor shapes. Tensor A is of shape [batch_size, x*y, 2] Tensor B is of shape [batch_size, x, y] From the tensorflow documentation: Since tensorsA and B only match in dimension 0, I cannot stack or concatenate them; I have also tried creating lambdaData as: All of the above result in varying dimension mismatch errors. I would follow the recommended use as per documentation of placing all of the data into a single tensor, but because of dimension mismatching between tensorA and tensorB I am unable to. Has anybody had any luck with tuples or lists of arguments for elems?",https://stackoverflow.com/questions/47984876,1519665,Lack of Alternative Solutions/Documentation
48033687,How to use tf.train.shuffle_batch to train NN?,"<p>I've trained my neural built with tensorflow network and got some overfit I'd like to reduce. I hoped learning the model on batches could help ad I tried to test this idea. I found tf.train.shuffle_batch() and fought this may do the thing. So I tried and it didn't work. Tensorflow's documentation doesn't help. I've found <a href=""https://stackoverflow.com/questions/45203872/how-tf-train-shuffle-batch-works"">one topic, but the example there</a> only prints arrays out. It was promising to use it to learn NN but in my case istead of getting data divided to n-element batches I got them multiplied n-times in additional dimension. </p>

<p>Here is the code sample:</p>

<pre><code>nnInput = tf.placeholder(tf.float32, [None, input_width], ""network_input"")
nnOutput = tf.placeholder(tf.float32, [None, output_width], ""expected_labels"")

batch_readings, batch_labels = tf.train.shuffle_batch(
    [
        tf.constant(train_readings), 
        tf.constant(train_labels)
    ],
    batch_size = 15,
    num_threads = 4,
    capacity = 500,
    min_after_dequeue = 250,
    allow_smaller_final_batch = True
)

sess.run(tf.global_variables_initializer())

for epoch in range(learning_steps):
    print(""epoch:"", epoch)
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    print(""Input data shapes:"", train_readings.shape, train_labels.shape)
    for batch in range(10):
        x, y = sess.run([batch_readings, batch_labels])
        print(""Batch shapes:"", x.shape, y.shape)
        sess.run(train, feed_dict = {nnInput : x, nnOutput : y})
    coord.request_stop()
    coord.join(threads)
</code></pre>

<p>and here is the output:</p>

<pre><code>epoch: 0 
Input data shapes: (165, 60) (165, 1) 
Batch shapes: (15, 165, 60) (15, 165, 1)
</code></pre>

<p>And the error list concludes with:</p>

<pre><code>ValueError: Cannot feed value of shape (15, 165, 60) for Tensor 'network_input_1:0', which has shape '(?, 60)'
</code></pre>

<p>The conlusion is not surprising when I fed the NN with 3D array but why do I get such a batch when I expect x:(15, 60) and y:(15, 1)? Why do I get x:(15, 165, 60) y:(15, 165, 1) and how to get useful batches?</p>

<p>I'm using tensorflow-gpu but hope this should work as well, right?</p>
","I've trained my neural built with tensorflow network and got some overfit I'd like to reduce. I hoped learning the model on batches could help ad I tried to test this idea. I found tf.train.shuffle_batch() and fought this may do the thing. So I tried and it didn't work. Tensorflow's documentation doesn't help. I've found one topic, but the example there only prints arrays out. It was promising to use it to learn NN but in my case istead of getting data divided to n-element batches I got them multiplied n-times in additional dimension. Here is the code sample: and here is the output: And the error list concludes with: The conlusion is not surprising when I fed the NN with 3D array but why do I get such a batch when I expect x:(15, 60) and y:(15, 1)? Why do I get x:(15, 165, 60) y:(15, 165, 1) and how to get useful batches? I'm using tensorflow-gpu but hope this should work as well, right?",https://stackoverflow.com/questions/48033687,8214796,Documentation Replication on Other Examples
48072635,Why and when does the tensor's shape information unspecific?,"<p>I found piece of code like this:</p>

<pre><code>y = tf.strided_slice(data, [0, i * num_steps + 1],
                     [batch_size, (i + 1) * num_steps + 1])
y.set_shape([batch_size, num_steps])
</code></pre>

<p>as <a href=""https://stackoverflow.com/questions/35451948/clarification-on-tf-tensor-set-shape"">Clarification on tf.Tensor.set_shape()</a> said, set_shape can make the shape information more specific. But why data's shape information here is not specific? When does the tensor's information unspecific?</p>
","I found piece of code like this: as Clarification on tf.Tensor.set_shape() said, set_shape can make the shape information more specific. But why data's shape information here is not specific? When does the tensor's information unspecific?",https://stackoverflow.com/questions/48072635,3134227,Documentation Replicability
48235239,The fit function from tf.contrib.learn.LinearRegressor asks to switch to tf.train.get_global_step,"<p>I am trying to get a <code>LinearRegressor</code> to work and I get an error for which there doesn't seem to be much documentation about.</p>

<p>When I do:</p>

<pre><code>regressor = tf.contrib.learn.LinearRegressor(feature_columns=linear_features)

regressor.fit(input_fn=training_input_fn, steps=10000)

regressor.evaluate(input_fn=eval_input_fn)
</code></pre>

<p>I get the error:</p>

<blockquote>
  <p>Instructions for updating: Please switch to tf.train.get_global_step</p>
</blockquote>

<p>I am not sure how to proceed.</p>

<p>I read from the docs:</p>

<blockquote>
  <p>SOME ARGUMENTS ARE DEPRECATED. They will be removed after 2016-12-01.
  Instructions for updating: Estimator is decoupled from Scikit Learn
  interface by moving into separate class SKCompat. Arguments x, y and
  batch_size are only available in the SKCompat class, Estimator will
  only accept input_fn. Example conversion: est = Estimator(...) -> est
  = SKCompat(Estimator(...))</p>
</blockquote>

<p>But I'm not sure to what I should change to, or how to switch to the global step.</p>

<p>I tried using <code>tf.estimator.LinearRegressor</code> mainly because I'm out of ideas, and did something like this:</p>

<pre><code>estimator = tf.estimator.LinearRegressor(feature_columns=linear_features)

estimator.train(input_fn=training_input_fn)
estimator.evaluate(input_fn=eval_input_fn)
estimator.predict(input_fn=eval_input_fn)
</code></pre>

<p>But got no output at all.</p>
","I am trying to get a LinearRegressor to work and I get an error for which there doesn't seem to be much documentation about. When I do: I get the error: I am not sure how to proceed. I read from the docs: But I'm not sure to what I should change to, or how to switch to the global step. I tried using tf.estimator.LinearRegressor mainly because I'm out of ideas, and did something like this: But got no output at all.",https://stackoverflow.com/questions/48235239,463065,Lack of Alternative Solutions/Documentation
48299597,How to efficiently shuffle a large tf.data.Dataset when using tf.estimator.train_and_evaluate?,"<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer""><code>tf.estimator.train_and_evaluate</code></a> documentation makes it clear that the input dataset must be properly shuffled for the training to see all examples:</p>

<blockquote>
  <p>Overfitting: In order to avoid overfitting, it is recommended to set up the training input_fn to shuffle the training data properly. It is also recommended to train the model a little longer, say multiple epochs, before performing evaluation, as the input pipeline starts from scratch for each training. It is particularly important for local training and evaluation.</p>
</blockquote>

<p>In my application, I would like to uniformly sample examples from the full <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code></a> with arbitrary evaluation frequency and <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"" rel=""nofollow noreferrer""><code>shuffle()</code></a>'s buffer size. Otherwise, the training can at most see the first:</p>

<pre><code>(steps_per_second * eval_delay * batch_size) + buffer_size
</code></pre>

<p>elements, effectively discarding the rest. Is there an efficient way to work around that without loading the complete dataset in the system memory?</p>

<p>I considered <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard"" rel=""nofollow noreferrer"">sharding</a> the dataset based on the buffer size, but if the evaluation does not occur frequently, it will iterate on the same shard multiple times (a <code>repeat()</code> closes the pipeline). Ideally, I would like to move to another shard after a complete iteration over the dataset, is that possible?</p>

<p>Thanks for any pointers!</p>
","The tf.estimator.train_and_evaluate documentation makes it clear that the input dataset must be properly shuffled for the training to see all examples: In my application, I would like to uniformly sample examples from the full tf.data.Dataset with arbitrary evaluation frequency and shuffle()'s buffer size. Otherwise, the training can at most see the first: elements, effectively discarding the rest. Is there an efficient way to work around that without loading the complete dataset in the system memory? I considered sharding the dataset based on the buffer size, but if the evaluation does not occur frequently, it will iterate on the same shard multiple times (a repeat() closes the pipeline). Ideally, I would like to move to another shard after a complete iteration over the dataset, is that possible? Thanks for any pointers!",https://stackoverflow.com/questions/48299597,2529808,Documentation Replicability
48396599,"Canonical Tensorflow ""for loop""","<p>What is the canonical way of running a Tensorflow ""for loop""? </p>

<p>Specifically, suppose we have some <code>body</code> function which does NOT depend on the loop iteration, but must be run <code>n</code> times. </p>

<p>One might think that a good method might be to run this inside of a <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/control_flow_operations#while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop</code></a> like this:</p>

<pre><code>def body(x):
    return ...

def while_body(i,x):
    return i+1, body(x) 

i, x = tf.while_loop(lambda i: tf.less(i, n), while_body, [tf.constant(0),x])
</code></pre>

<p>In fact, that is precisely what the highest rated answer in this question suggests:</p>

<p><a href=""https://stackoverflow.com/questions/35330117/how-can-i-run-a-loop-with-a-tensor-as-its-range-in-tensorflow"">How can I run a loop with a tensor as its range? (in tensorflow)</a></p>

<p>However, the <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/control_flow_operations#while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop</code> docs</a> specifically say </p>

<blockquote>
  <p>For correct programs, while_loop should return the same result for any parallel_iterations > 0.</p>
</blockquote>

<p>If you put a counter in the body, then it seems that that condition is violated. So it seems that there must be a different way of setting up a ""for loop"". </p>

<p>Furthermore, even if there is no explicit error, doing so seems like it will create a dependency between iterations meaning that I do not think they will run in parallel. </p>
","What is the canonical way of running a Tensorflow ""for loop""? Specifically, suppose we have some body function which does NOT depend on the loop iteration, but must be run n times. One might think that a good method might be to run this inside of a tf.while_loop like this: In fact, that is precisely what the highest rated answer in this question suggests: How can I run a loop with a tensor as its range? (in tensorflow) However, the tf.while_loop docs specifically say If you put a counter in the body, then it seems that that condition is violated. So it seems that there must be a different way of setting up a ""for loop"". Furthermore, even if there is no explicit error, doing so seems like it will create a dependency between iterations meaning that I do not think they will run in parallel.",https://stackoverflow.com/questions/48396599,3309610,Lack of Alternative Solutions/Documentation
48427269,What's the efficient way to feed elements from Iterator (from tf.data.Dataset) into TensorFlow model?,"<p>I'm using TensrFlow's new API for importing data via <code>tf.data.Dataset</code> and iterators. It is working fine, but I'm not sure if what I do is efficient. </p>

<p>What I'm doing at the moment is evaluating an iterator's <code>get_next()</code> method, which gives me a bunch of elements like the actual image, its label, filename, etc. I then feed the image into my model using the <code>feed_dict</code>. </p>

<p>I know that <code>feed_dict</code> is very slow, so am I losing benefits of <code>Dataset</code> and Iterators and having serialised dataset in <code>TFRecord</code>s by evaluating the entries and feeding them into the graph via <code>feed_dict</code>? I haven't found any examples in TF's documentation which shows how one's expected to use Iterator's <code>get_next()</code> to feed elements into the model. Is it better to unpack <code>get_next()</code> and use the result directly in my graph? </p>
","I'm using TensrFlow's new API for importing data via tf.data.Dataset and iterators. It is working fine, but I'm not sure if what I do is efficient. What I'm doing at the moment is evaluating an iterator's get_next() method, which gives me a bunch of elements like the actual image, its label, filename, etc. I then feed the image into my model using the feed_dict. I know that feed_dict is very slow, so am I losing benefits of Dataset and Iterators and having serialised dataset in TFRecords by evaluating the entries and feeding them into the graph via feed_dict? I haven't found any examples in TF's documentation which shows how one's expected to use Iterator's get_next() to feed elements into the model. Is it better to unpack get_next() and use the result directly in my graph?",https://stackoverflow.com/questions/48427269,298209,Inadequate Examples
48445751,Keras: Constrained dictionary search with CTC decode,"<p>I'm trying to constrain the CTC decoding to a specific (external) dictionary in Keras with a the tensorflow backend. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_decode"" rel=""nofollow noreferrer"">tensorflow documentation for Keras' ctc_decode</a>, it is written that when <code>greedy=False</code> a dictionary will be used. Here is the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder"" rel=""nofollow noreferrer"">documentation for  tf.nn.ctc_beam_search_decoder</a>, which will be called by this option as far as I understand.</p>

<p>Since there is no way to pass an external dictionary or language model (to constrain the search), I assume that with <code>greedy=False</code> it creates its own dictionary from the training data. Is this correct? Is there a way to constrain the search to a specific (external) dictionary?</p>
","I'm trying to constrain the CTC decoding to a specific (external) dictionary in Keras with a the tensorflow backend. In the tensorflow documentation for Keras' ctc_decode, it is written that when greedy=False a dictionary will be used. Here is the documentation for tf.nn.ctc_beam_search_decoder, which will be called by this option as far as I understand. Since there is no way to pass an external dictionary or language model (to constrain the search), I assume that with greedy=False it creates its own dictionary from the training data. Is this correct? Is there a way to constrain the search to a specific (external) dictionary?",https://stackoverflow.com/questions/48445751,1578793,Documentation Replication on Other Examples
48471926,In Tensorflow's Dataset API how do you map one element into multiple elements?,"<p>In the tensorflow <code>Dataset</code> pipeline I'd like to define a custom map function which takes a single input element (data sample) and returns multiple elements (data samples).</p>

<p>The code below is my attempt, along with the desired results. </p>

<p>I could not follow the documentation on <code>tf.data.Dataset().flat_map()</code> well enough to understand if it was applicable here or not.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

input = [10, 20, 30]

def my_map_func(i):
  return [[i, i+1, i+2]]       # Fyi [[i], [i+1], [i+2]] throws an exception

ds = tf.data.Dataset.from_tensor_slices(input)
ds = ds.map(map_func=lambda input: tf.py_func(
  func=my_map_func, inp=[input], Tout=[tf.int64]
))
element = ds.make_one_shot_iterator().get_next()

with tf.Session() as sess:
  for _ in range(9):
    print(sess.run(element))
</code></pre>

<p>Results:</p>

<pre><code>(array([10, 11, 12]),)
(array([20, 21, 22]),)
(array([30, 31, 32]),)
</code></pre>

<p>Desired results:</p>

<pre><code>(10)
(11)
(12)
(20)
(21)
(22)
(30)
(31)
(32)
</code></pre>
","In the tensorflow Dataset pipeline I'd like to define a custom map function which takes a single input element (data sample) and returns multiple elements (data samples). The code below is my attempt, along with the desired results. I could not follow the documentation on tf.data.Dataset().flat_map() well enough to understand if it was applicable here or not. Results: Desired results:",https://stackoverflow.com/questions/48471926,4790871,Documentation Replicability
48697799,Tensorflow feature column for variable list of values,"<p>From the TensorFlow docs it's clear how to use <code>tf.feature_column.categorical_column_with_vocabulary_list</code> to create a feature column which takes as input some string and outputs a one-hot vector. For example</p>

<pre><code>vocabulary_feature_column =
    tf.feature_column.categorical_column_with_vocabulary_list(
        key=""vocab_feature"",
        vocabulary_list=[""kitchenware"", ""electronics"", ""sports""])
</code></pre>

<p>Let's say <code>""kitchenware""</code> maps to <code>[1,0,0]</code> and <code>""electronics""</code> maps to <code>[0,1,0]</code>. My question is related to having a <strong>list of strings</strong> as a feature. For example, if the feature value was <code>[""kitchenware"",""electronics""]</code> then the desired output would be <code>[1,1,0]</code>. The input list length is not fixed but the output dimension is.</p>

<p>The use case is a straight bag-of-words type model (obviously with a much larger vocabulary list!).</p>

<p>What is the correct way to implement this?</p>
","From the TensorFlow docs it's clear how to use tf.feature_column.categorical_column_with_vocabulary_list to create a feature column which takes as input some string and outputs a one-hot vector. For example Let's say ""kitchenware"" maps to [1,0,0] and ""electronics"" maps to [0,1,0]. My question is related to having a list of strings as a feature. For example, if the feature value was [""kitchenware"",""electronics""] then the desired output would be [1,1,0]. The input list length is not fixed but the output dimension is. The use case is a straight bag-of-words type model (obviously with a much larger vocabulary list!). What is the correct way to implement this?",https://stackoverflow.com/questions/48697799,7978198,Documentation Replication on Other Examples
48736753,"How do you load ""any"" model from disk into a TensorFlow Estimator without having the model_fn source code?","<p>In Keras you can load a model that you had previously trained by using:</p>

<p>trained_keras_model = tf.keras.models.load_model(model_name)</p>

<p>Is there any equivalent method for doing this using TensorFlow estimator API? According to the documentation, I have to use: </p>

<p>trained_estimator = tf.estimator.Estimator (model_fn,model_dir)
I want to get the trained estimator using just the files in the model directory. To be more clear my idea was to load ""any"" model from disk without having the model_fn source code. Is it possible to do it this way? </p>

<p>This feature is implemented in Keras so I am at a loss to understand why Estimator API cannot do this. </p>
","In Keras you can load a model that you had previously trained by using: trained_keras_model = tf.keras.models.load_model(model_name) Is there any equivalent method for doing this using TensorFlow estimator API? According to the documentation, I have to use: trained_estimator = tf.estimator.Estimator (model_fn,model_dir) I want to get the trained estimator using just the files in the model directory. To be more clear my idea was to load ""any"" model from disk without having the model_fn source code. Is it possible to do it this way? This feature is implemented in Keras so I am at a loss to understand why Estimator API cannot do this.",https://stackoverflow.com/questions/48736753,7656080,Documentation Replicability
48815906,Implement early stopping in tf.estimator.DNNRegressor using the available training hooks,"<p>I am new to tensorflow and want to implement early stopping in <code>tf.estimator.DNNRegressor</code> with  available training hooks<a href=""https://www.tensorflow.org/api_guides/python/train#Training_Hooks"" rel=""noreferrer"">Training Hooks</a> for the MNIST dataset. The early stopping hook will stop training if the loss does not improve for some specified number of steps. Tensorflow documentaton only provides example for <a href=""https://www.tensorflow.org/tutorials/layers#set_up_a_logging_hook"" rel=""noreferrer"">Logging hooks</a>. Can someone write a code snippet for implementing it?</p>
",I am new to tensorflow and want to implement early stopping in tf.estimator.DNNRegressor with available training hooksTraining Hooks for the MNIST dataset. The early stopping hook will stop training if the loss does not improve for some specified number of steps. Tensorflow documentaton only provides example for Logging hooks. Can someone write a code snippet for implementing it?,https://stackoverflow.com/questions/48815906,6533039,Requesting (Additional) Resources
48914952,num_buckets as a parameter in a tensorflow feature column,"<p>Currently Tensorflow documentation define a categorical vocabulary column this way:</p>

<pre><code>vocabulary_feature_column =
tf.feature_column.categorical_column_with_vocabulary_list(
    key=""feature_name_from_input_fn"",
    vocabulary_list=[""kitchenware"", ""electronics"", ""sports""]) 
</code></pre>

<p>However this suppose that we input manually the vocabulary list.
In case of large dataset with many columns and many unique values I would like to automate the process this way:</p>

<pre><code>for k in categorical_feature_names:
    vocabulary_feature_column =
        tf.feature_column.categorical_column_with_vocabulary_list(
        key=""feature_name_from_input_fn"",
        vocabulary_list=list_of_unique_values_in_the_column) 
</code></pre>

<p>To do so I need to retrieve the parameter <code>list_of_unique_values_in_the_column</code>.
Is there anyway to do that with Tensorflow? </p>

<p>I know there is tf.unique that could return unique values in a tensor but I don't get how I could feed the column to it so it returns the right vocabulary list.</p>
",Currently Tensorflow documentation define a categorical vocabulary column this way: However this suppose that we input manually the vocabulary list. In case of large dataset with many columns and many unique values I would like to automate the process this way: To do so I need to retrieve the parameter list_of_unique_values_in_the_column. Is there anyway to do that with Tensorflow? I know there is tf.unique that could return unique values in a tensor but I don't get how I could feed the column to it so it returns the right vocabulary list.,https://stackoverflow.com/questions/48914952,2175173,Documentation Replication on Other Examples
49066695,How to use tf.nn.raw_rnn function in Tensorflow?,"<p>I am trying to implement LSTM based network where after hidden state computation we also apply linear + sigmoid transformation at each time step. I have found the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn"" rel=""nofollow noreferrer"">official documentation</a> and <a href=""https://hanxiao.github.io/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/"" rel=""nofollow noreferrer"">a nice article</a> that describe <code>tf.nn.raw_rnn</code> function suitable for this task however I struggle to understand why it does not work in my particular case.</p>

<h3>Input description</h3>

<p>So, let our input to LSTM be a minibatch of size <code>[num_steps x batch_size x size]</code>, concretely <code>[5, 32, 100]</code>. Let LSTM have 200 hidden units. Then the output of the LSTM is <code>[5, 32, 200]</code> tensor which we can later use for loss computation. I assume the input <code>[5, 32, 100]</code> tensor is first unstacked into an array of <code>[32, 100]</code> tensors and then stacked back if we use <code>tf.nn.dynamic_rnn</code> with <code>time_major=True</code> in Tensorflow:</p>

<pre><code>                                      tf.nn.dynamic_rnn(LSTM)
                   LSTM t=0    LSTM t=1   LSTM t=2   LSTM t=3   LSTM t=4  
[5, 32, 100] --&gt;   [[32, 100], [32, 100], [32, 100], [32, 100], [32, 100]] --&gt; [5, 32, 200]
</code></pre>

<h3>Hidden state model</h3>

<p>In addition after each LSTM cell I need to perform linear + sigmoid transformation to squash each <code>[32, 200]</code> tensor into <code>[32, 1]</code> for example. Our <code>tf.nn.dynamic_rnn</code> won't work for that since it only accepts cells. We need to use <code>tf.nn.raw_rnn</code> API. So, here is my try:</p>

<pre><code>def _get_raw_rnn_graph(self, inputs):
    time = tf.constant(0, dtype=tf.int32)
    _inputs_ta = tf.TensorArray(dtype=tf.float32, size=5)
    # our [5, 32, 100] tensor becomes [[32, 100], [32, 100], ...]
    _inputs_ta = _inputs_ta.unstack(inputs)  

    # create simple LSTM cell
    cell = tf.contrib.rnn.LSTMCell(config.hidden_size)

    # create loop_fn for raw_rnn
    def loop_fn(time, cell_output, cell_state, loop_state):
        emit_output = cell_output  # == None if time = 0

        if cell_output is None:  # time = 0
            next_cell_state = cell.zero_state(32, tf.float32)
            self._initial_state = next_cell_state
        else:
            next_cell_state = cell_state

        elements_finished = (time &gt;= 32)
        finished = tf.reduce_all(elements_finished)
        next_input = tf.cond(finished,
                             lambda: tf.zeros([32, config.input_size], dtype=tf.float32),                                                   
                             lambda: _inputs_ta.read(time))

        # apply linear + sig transform here
        next_input = self._linear_transform(next_input, activation=tf.sigmoid)

        next_loop_state = None
        return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)

    outputs_ta, final_state, _ = tf.nn.raw_rnn(cell, loop_fn)
    outputs = outputs_ta.stack()
    return outputs, final_state
</code></pre>

<p>This unfortunately does not work. The <code>loop_fn</code> iterates only two times instead of <code>num_steps</code> times as I expected and its output is <code>Tensor(""Train/Model/TensorArrayStack/TensorArrayGatherV3:0"", shape=(?, 32, 200), dtype=float32)</code> not <code>[5, 32, 1]</code> as we intended. What am I missing here?</p>
","I am trying to implement LSTM based network where after hidden state computation we also apply linear + sigmoid transformation at each time step. I have found the official documentation and a nice article that describe tf.nn.raw_rnn function suitable for this task however I struggle to understand why it does not work in my particular case. So, let our input to LSTM be a minibatch of size [num_steps x batch_size x size], concretely [5, 32, 100]. Let LSTM have 200 hidden units. Then the output of the LSTM is [5, 32, 200] tensor which we can later use for loss computation. I assume the input [5, 32, 100] tensor is first unstacked into an array of [32, 100] tensors and then stacked back if we use tf.nn.dynamic_rnn with time_major=True in Tensorflow: In addition after each LSTM cell I need to perform linear + sigmoid transformation to squash each [32, 200] tensor into [32, 1] for example. Our tf.nn.dynamic_rnn won't work for that since it only accepts cells. We need to use tf.nn.raw_rnn API. So, here is my try: This unfortunately does not work. The loop_fn iterates only two times instead of num_steps times as I expected and its output is Tensor(""Train/Model/TensorArrayStack/TensorArrayGatherV3:0"", shape=(?, 32, 200), dtype=float32) not [5, 32, 1] as we intended. What am I missing here?",https://stackoverflow.com/questions/49066695,1984680,Documentation Replication on Other Examples
49155119,Using TensorFlow ``grad_loss / grad_ys`` parameter to add gradients,"<p>I'm trying to use the <code>grad_loss</code> parameter in <code>optimizer.minimize(loss, grad_loss=)</code> to modify the network gradients with existing gradients.
I followed the comments here:
<a href=""https://stackoverflow.com/questions/42399401/use-of-grads-ys-parameter-in-tf-gradients-tensorflow"">Use of grads_ys parameter in tf.gradients - TensorFlow</a></p>

<p>and I would like to run a toy example, in which I recreate the default <code>1</code> values for <code>grad_ys</code>, as specified in the documentation.</p>

<p>Here's the relevant code segment:</p>

<pre><code>grads_and_vars = optimizer.compute_gradients(loss_op) 
vars_with_grad = [v for g, v in grads_and_vars if g is not None] 
grad_loss = [] 
for grad,var in grads_and_vars:
    grad_loss.append(tf.ones_like(grad))
train_op = optimizer.minimize(loss_op, grad_loss=grad_loss)
</code></pre>

<p>The first part extracts gradients using <code>compute_gradients</code>. The last line computes gradients of the loss function <code>loss_op</code> but attempts to use <code>1</code>-filled vectors for the grads. As far as I understand, this should behave similarly to funning <code>minimize</code> without the <code>grad_loss</code> parameter. </p>

<p>Unfortunately, this fails since it expects <code>grad_loss</code> to be a Tensor (and have a dtype) and not a list. Looking into <code>gradients_impl.py</code> I see that the function expected <code>grad_loss</code> to be of the same dimension as <code>loss</code> (which in this case is a scalar). </p>

<p>I would appreciate any assistance in this simple example - how do I add elements to the gradients this way? </p>

<p>EDIT: I guess the question boils down to the definition of <code>grad_loss</code>: ""A <code>Tensor</code> holding the gradient computed for <code>loss</code>."" How do I generate such a tensor from a set of gradients obtained by <code>compute_gradients</code>?</p>

<p>Thanks.</p>
","I'm trying to use the grad_loss parameter in optimizer.minimize(loss, grad_loss=) to modify the network gradients with existing gradients. I followed the comments here: Use of grads_ys parameter in tf.gradients - TensorFlow and I would like to run a toy example, in which I recreate the default 1 values for grad_ys, as specified in the documentation. Here's the relevant code segment: The first part extracts gradients using compute_gradients. The last line computes gradients of the loss function loss_op but attempts to use 1-filled vectors for the grads. As far as I understand, this should behave similarly to funning minimize without the grad_loss parameter. Unfortunately, this fails since it expects grad_loss to be a Tensor (and have a dtype) and not a list. Looking into gradients_impl.py I see that the function expected grad_loss to be of the same dimension as loss (which in this case is a scalar). I would appreciate any assistance in this simple example - how do I add elements to the gradients this way? EDIT: I guess the question boils down to the definition of grad_loss: ""A Tensor holding the gradient computed for loss."" How do I generate such a tensor from a set of gradients obtained by compute_gradients? Thanks.",https://stackoverflow.com/questions/49155119,635622,Documentation Replication on Other Examples
49201832,How to use TensorBoard and summary operations with the tf.layers module,"<p>I have followed the <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""nofollow noreferrer"">TensorFlow Layers tutorial</a> to create a CNN for MNIST digit classification using TensorFlow's tf.layers module. Now I'm trying to learn how to use TensorBoard from <a href=""https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard"" rel=""nofollow noreferrer"">TensorBoard: Visualizing Learning</a>. Perhaps this tutorial hasn't been updated recently, because it says its example code is a modification of that tutorial's and links to it, but the code is completely different: it manually defines a single-hidden-layer fully-connected network.</p>

<p>The TensorBoard tutorial shows how to use tf.summary to attach summaries to a layer by creating operations on the layer's weights tensor, which is directly accessible because we manually defined the layer, and attaching tf.summary objects to those operations. To do this if I'm using tf.layers and its tutorial code, I believe I'd have to:</p>

<ol>
<li>Modify the Layers tutorial's example code to use the non-functional interface (Conv2D instead of conv2d and Dense instead of dense) to create the layers</li>
<li>Use the layer objects' trainable_weights() functions to get the weight tensors and attach tf.summary objects to those</li>
</ol>

<p>Is that the best way to use TensorBoard with tf.layers, or is there a way that's more directly compatible with tf.layers and the functional interface? If so, is there an updated official TensorBoard tutorial? It would be nice if the documentation and tutorials were more unified. </p>
","I have followed the TensorFlow Layers tutorial to create a CNN for MNIST digit classification using TensorFlow's tf.layers module. Now I'm trying to learn how to use TensorBoard from TensorBoard: Visualizing Learning. Perhaps this tutorial hasn't been updated recently, because it says its example code is a modification of that tutorial's and links to it, but the code is completely different: it manually defines a single-hidden-layer fully-connected network. The TensorBoard tutorial shows how to use tf.summary to attach summaries to a layer by creating operations on the layer's weights tensor, which is directly accessible because we manually defined the layer, and attaching tf.summary objects to those operations. To do this if I'm using tf.layers and its tutorial code, I believe I'd have to: Is that the best way to use TensorBoard with tf.layers, or is there a way that's more directly compatible with tf.layers and the functional interface? If so, is there an updated official TensorBoard tutorial? It would be nice if the documentation and tutorials were more unified.",https://stackoverflow.com/questions/49201832,2328207,Requesting (Additional) Resources
49370940,One hot encoding characters,"<p>Is there a possibilty to one-hot encode characters of a text in Tensorflow or Keras?</p>

<ul>
<li><code>tf.one_hot</code> seem to take only integers.</li>
<li><code>tf.keras.preprocessing.text.one_hot</code> seems to one-hot encode sentences
to words, but not to characters.</li>
</ul>

<p>Beside that, <code>tf.keras.preprocessing.text.one_hot</code> works really strange, since the response does not really seem one-hot encoded, since the following code:</p>

<pre><code>text = ""ab bba bbd""
res = tf.keras.preprocessing.text.one_hot(text=text,n=3)
print(res)
</code></pre>

<p>Lead to this result:</p>

<pre><code>[1,2,2]
</code></pre>

<p>Every time I run this program, the output is a different 3d vector, sometimes it is <code>[1,1,1]</code> or <code>[2,1,1]</code>. The documentation says, that unicity is not guaranteed, but this seems really senseless to me.</p>
","Is there a possibilty to one-hot encode characters of a text in Tensorflow or Keras? Beside that, tf.keras.preprocessing.text.one_hot works really strange, since the response does not really seem one-hot encoded, since the following code: Lead to this result: Every time I run this program, the output is a different 3d vector, sometimes it is [1,1,1] or [2,1,1]. The documentation says, that unicity is not guaranteed, but this seems really senseless to me.",https://stackoverflow.com/questions/49370940,3921232,Documentation Ambiguity
49405794,Why tensor_summary doesn't work?,"<p>I use <code>tf.summary.tensor_summary</code> in my code, following this: <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/tensor_summary"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/summary/tensor_summary</a></p>

<p>But I didn't see anything new in tensorboard, and tensorboard also printed some warnings:</p>

<pre><code>W0321 12:50:47.244003 Reloader tf_logging.py:121] This summary with tag 'component_3_finalize/wealth_tensor' is oddly not associated with a plugin.
</code></pre>

<p>How to make this work? Do I need install some plugin? I didn't find any docs on this.</p>

<p>UPDATE:</p>

<p>So I here is how I create my summary:</p>

<pre><code>def finalize_graph(self, graph_building_context):
    tf.summary.scalar('loss', loss)

    # the wealth tensor is of shape [B], where B is the batch size at runtime
    tf.summary.scalar('wealth', tf.reduce_mean(wealth))
    # uhmm, this tensor_summary doesn't work yet
    tf.summary.tensor_summary('wealth_tensor', wealth)
</code></pre>

<p>Then I use a <code>MonitoredTrainingSession</code>, by default it will save the summary, and I can see my <code>loss</code> and <code>wealth</code> scalar summry, but not this <code>wealth_tensor</code> summary.</p>
","I use tf.summary.tensor_summary in my code, following this: https://www.tensorflow.org/api_docs/python/tf/summary/tensor_summary But I didn't see anything new in tensorboard, and tensorboard also printed some warnings: How to make this work? Do I need install some plugin? I didn't find any docs on this. UPDATE: So I here is how I create my summary: Then I use a MonitoredTrainingSession, by default it will save the summary, and I can see my loss and wealth scalar summry, but not this wealth_tensor summary.",https://stackoverflow.com/questions/49405794,2218586,Lack of Alternative Solutions/Documentation
49418325,"Use ""tf.contrib.factorization.KMeansClustering""","<p>Referring to this Link, <a href=""https://github.com/tensorflow/tensorflow/issues/17002"" rel=""nofollow noreferrer"">(the Link)</a>
I try to practice using tf.contrib.factorization.KMeansClustering for clustering. The simple codes as follow works okay:</p>

<pre><code>import numpy as np
import tensorflow as tf

# ---- Create Data Sample -----
k = 5
n = 100
variables = 5
points = np.random.uniform(0, 1000, [n, variables])

# ---- Clustering -----
input_fn=lambda: tf.train.limit_epochs(tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)
kmeans=tf.contrib.factorization.KMeansClustering(num_clusters=6)
kmeans.train(input_fn=input_fn)
centers = kmeans.cluster_centers()

# ---- Print out -----
cluster_indices = list(kmeans.predict_cluster_index(input_fn))
for i, point in enumerate(points):
  cluster_index = cluster_indices[i]
  print ('point:', point, 'is in cluster', cluster_index, 'centered at', centers[cluster_index])
</code></pre>

<p>My question is why would this ""input_fn"" code does the trick?
If I change the code to this, it will run into an infinite loop. Why??</p>

<pre><code>input_fn=lambda:tf.convert_to_tensor(points, dtype=tf.float32)
</code></pre>

<p>From the document <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering"" rel=""nofollow noreferrer"">(here)</a>, it seems that train() is expecting argument of input_fn, which is simply a A 'tf.data.Dataset' object , like Tensor(X). So, why do I have to do all these tricky things regarding lambda: tf.train.limit_epochs()?</p>

<p>Can anyone who is familiar with the fundamental of tensorflow estimators help to explain? Many Thanks!</p>
","Referring to this Link, (the Link) I try to practice using tf.contrib.factorization.KMeansClustering for clustering. The simple codes as follow works okay: My question is why would this ""input_fn"" code does the trick? If I change the code to this, it will run into an infinite loop. Why?? From the document (here), it seems that train() is expecting argument of input_fn, which is simply a A 'tf.data.Dataset' object , like Tensor(X). So, why do I have to do all these tricky things regarding lambda: tf.train.limit_epochs()? Can anyone who is familiar with the fundamental of tensorflow estimators help to explain? Many Thanks!",https://stackoverflow.com/questions/49418325,7270211,Inadequate Examples
49435335,Verify that keras GaussianNoise is enabled at train time when using inference with edward,"<p>I would like to check if noise is truly added and used during training of my neural network. I therefore build my NN with keras like this:</p>

<pre><code>from keras.layers import Input
from keras.layers.noise import GaussianNoise
inp = Input(tensor=self.X_ph)
noised_x = GaussianNoise(stddev=self.x_noise_std)(inp)
x = Dense(15, activation='elu')(noised_x)
x = Dense(15, activation='elu')(x)
self.estimator = x
...
# kernel weights, as output by the neural network
self.logits = logits = Dense(n_locs * self.n_scales, activation='softplus')(self.estimator)
self.weights = tf.nn.softmax(logits)

# mixture distributions
self.cat = cat = Categorical(logits=logits)
self.components = components = [MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc in locs_array for scale in scales_array]
self.mixtures = mixtures = Mixture(cat=cat, components=components, value=tf.zeros_like(self.y_ph))
</code></pre>

<p>Then I use edward to execute inference:</p>

<pre><code>self.inference = ed.MAP(data={self.mixtures: self.y_ph})
self.inference.initialize(var_list=tf.trainable_variables(), n_iter=self.n_training_epochs)
tf.global_variables_initializer().run()
</code></pre>

<p>According to the documentation, the closest I get to this is through ed.MAP's <code>run()</code> and <code>update()</code> functions. </p>

<p>Preferably, I would do something like this:</p>

<pre><code>noised_x = self.sess.run(self.X_ph, feed_dict={self.X_ph: X, self.y_ph: Y}) 
np.allclose(noised_x, X) --&gt; False
</code></pre>

<h2>How can I properly verify that noise is being used at train time and disabled at test time within ed.MAP?</h2>

<h3>Update 1</h3>

<p>Apparently the way I use GaussianNoise doesn't seem to add noise to my input since the following unittest fails:</p>

<pre><code>X, Y = self.get_samples(std=1.0)

model_no_noise = KernelMixtureNetwork(n_centers=5, x_noise_std=None, y_noise_std=None)
model_no_noise.fit(X,Y)
var_no_noise = model_no_noise.covariance(x_cond=np.array([[2]]))[0][0][0]

model_noise = KernelMixtureNetwork(n_centers=5, x_noise_std=20.0, y_noise_std=20.0)
model_noise.fit(X, Y)
var_noise = model_noise.covariance(x_cond=np.array([[2]]))[0][0][0]
self.assertGreaterEqual(var_noise - var_no_noise, 0.1)
</code></pre>

<p>I also made sure that during the <code>inference.update(...)</code> the assertion
<code>assert tf.keras.backend.learning_phase() == 1</code>
passes.</p>

<p>Where could have something gone wrong here?</p>
","I would like to check if noise is truly added and used during training of my neural network. I therefore build my NN with keras like this: Then I use edward to execute inference: According to the documentation, the closest I get to this is through ed.MAP's run() and update() functions. Preferably, I would do something like this: Apparently the way I use GaussianNoise doesn't seem to add noise to my input since the following unittest fails: I also made sure that during the inference.update(...) the assertion assert tf.keras.backend.learning_phase() == 1 passes. Where could have something gone wrong here?",https://stackoverflow.com/questions/49435335,7353970,Documentation Replication on Other Examples
49472402,Tensorflow tf.nn.softmax() function performs much better than hand-written softmax,"<p>I'm writing a simple logistic regression with tensorflow. I found out that when using tf.nn.softmax, the algorithm converges much quicker, and in the end the accuracy is higher.
If switched to my own implementation of softmax, the network converges slower, and the end accuracy is not as good.</p>
<p>Here's the code:</p>
<pre><code>SEED = 1025
W = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels], seed=SEED))
b = tf.Variable(tf.zeros([num_labels]))
logits = tf.matmul(train_dataset, W) + b

# My softmax:
y_ = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis=0)
# Tensorflow softmax: 
y_ = tf.nn.softmax(logits)

y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)
loss = -tf.reduce_mean(tf.reduce_sum(train_labels * tf.log(y_clipped), axis=1))
</code></pre>
<p>Using my softmax:</p>
<pre><code>Loss at step 0: 22.213934
Training accuracy: 12.7%
Validation accuracy: 13.2%
Loss at step 100: 12.777291
Training accuracy: 45.3%
Validation accuracy: 45.5%
Loss at step 200: 11.361242
Training accuracy: 48.2%
Validation accuracy: 47.4%
Loss at step 300: 10.658278
Training accuracy: 51.4%
Validation accuracy: 49.7%
Loss at step 400: 9.297832
Training accuracy: 59.2%
Validation accuracy: 56.8%
Loss at step 500: 8.902699
Training accuracy: 62.0%
Validation accuracy: 59.2%
Loss at step 600: 8.681184
Training accuracy: 64.2%
Validation accuracy: 61.0%
Loss at step 700: 8.529438
Training accuracy: 65.8%
Validation accuracy: 62.3%
Loss at step 800: 8.416442
Training accuracy: 66.8%
Validation accuracy: 63.3%
Test accuracy: 70.4%
</code></pre>
<p>Using tensorflow's softmax:</p>
<pre><code>Loss at step 0: 13.555875
Training accuracy: 12.7%
Validation accuracy: 14.5%
Loss at step 100: 2.194562
Training accuracy: 72.5%
Validation accuracy: 72.0%
Loss at step 200: 1.808641
Training accuracy: 75.5%
Validation accuracy: 74.5%
Loss at step 300: 1.593390
Training accuracy: 76.8%
Validation accuracy: 75.0%
Loss at step 400: 1.442661
Training accuracy: 77.7%
Validation accuracy: 75.2%
Loss at step 500: 1.327751
Training accuracy: 78.2%
Validation accuracy: 75.4%
Loss at step 600: 1.236314
Training accuracy: 78.5%
Validation accuracy: 75.6%
Loss at step 700: 1.161479
Training accuracy: 78.9%
Validation accuracy: 75.6%
Loss at step 800: 1.098717
Training accuracy: 79.4%
Validation accuracy: 75.8%
Test accuracy: 83.3%
</code></pre>
<p>From the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/softmax"" rel=""nofollow noreferrer"">documentation</a>, in theory tensorflow's softmax should be exact the same as I implemented, no?</p>
<blockquote>
<p>This function performs the equivalent of</p>
<p>softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)</p>
</blockquote>
<p><strong>EDIT:</strong> I added a seed when initializing from normal distribution, now I can reproduce the accuracy results.
When setting axis value in &quot;My softmax&quot; line, only axis=0 doesn't result in error. Setting axis=1 or axis=-1 both results in this error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 10 and 10000 for 'truediv' (op: 'RealDiv') with input shapes: [10000,10], [10000].
</code></pre>
","I'm writing a simple logistic regression with tensorflow. I found out that when using tf.nn.softmax, the algorithm converges much quicker, and in the end the accuracy is higher. If switched to my own implementation of softmax, the network converges slower, and the end accuracy is not as good. Here's the code: Using my softmax: Using tensorflow's softmax: From the documentation, in theory tensorflow's softmax should be exact the same as I implemented, no? EDIT: I added a seed when initializing from normal distribution, now I can reproduce the accuracy results. When setting axis value in ""My softmax"" line, only axis=0 doesn't result in error. Setting axis=1 or axis=-1 both results in this error:",https://stackoverflow.com/questions/49472402,778800,Documentation Replicability
49542954,What are tf.TensorArray objects?,"<p>I am not able to get an understanding of <a href=""https://www.tensorflow.org/api_docs/python/tf/TensorArray"" rel=""nofollow noreferrer"">tf.TensorArray</a> objects. What are they and where are they needed? I have some (highly likely faulty) understanding -
 they are used in while_loops especially to write the information to the loop_state. If somehow we end up increasing the number of tensors or their dimensions across iterations it throws back an error. hence normal way to pass the loop_state across iterations collecting information from each iteration, which would be passing a list, would throw back an error. So we create tf.TensorArray objects and write the information to them at each iteration and pass these tf.TensorArray objects across loops and for some reason that way it is able to pass through</p>

<p>I couldnt find any blog or documentation explaining the working of a tf.TensorArray objects and documentation doesn't help much either</p>

<p>So, if this is not the best place to be asking this question, kindly direct me to nearest help.</p>
","I am not able to get an understanding of tf.TensorArray objects. What are they and where are they needed? I have some (highly likely faulty) understanding - they are used in while_loops especially to write the information to the loop_state. If somehow we end up increasing the number of tensors or their dimensions across iterations it throws back an error. hence normal way to pass the loop_state across iterations collecting information from each iteration, which would be passing a list, would throw back an error. So we create tf.TensorArray objects and write the information to them at each iteration and pass these tf.TensorArray objects across loops and for some reason that way it is able to pass through I couldnt find any blog or documentation explaining the working of a tf.TensorArray objects and documentation doesn't help much either So, if this is not the best place to be asking this question, kindly direct me to nearest help.",https://stackoverflow.com/questions/49542954,6546694,Lack of Alternative Solutions/Documentation
49564318,Issue with fine-tuning inceptionv3 in slim tensorflow and tf record batches,"<p>I am trying to fine-tune inceptionv3 model using slim tensorflow library. 
I am unable to understand certain things while writing the code for it. I tried to read source code (no proper documentation) and figured out few things and I am able to fine-tune it and save the check point. Here are the steps I followed 
 1. I created a tf.record for my training data which is fine, now I am reading the data using the below code. </p>

<pre><code>import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import tensorflow.contrib.slim as slim
import matplotlib.pyplot as plt
import numpy as np

# get the data and labels here

data_path = '/home/sfarkya/nvidia_challenge/datasets/detrac/train1.tfrecords'

# Training setting
num_epochs = 100
initial_learning_rate = 0.0002
learning_rate_decay_factor = 0.7
num_epochs_before_decay = 5
num_classes = 5980

# load the checkpoint
model_path = '/home/sfarkya/nvidia_challenge/datasets/detrac/inception_v3.ckpt'

# log directory
log_dir = '/home/sfarkya/nvidia_challenge/datasets/detrac/fine_tuned_model'

with tf.Session() as sess:
    feature = {'train/image': tf.FixedLenFeature([], tf.string),
               'train/label': tf.FixedLenFeature([], tf.int64)}

    # Create a list of filenames and pass it to a queue
    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)

    # Define a reader and read the next record
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)

    # Decode the record read by the reader
    features = tf.parse_single_example(serialized_example, features=feature)

    # Convert the image data from string back to the numbers
    image = tf.decode_raw(features['train/image'], tf.float32)

    # Cast label data into int32
    label = tf.cast(features['train/label'], tf.int32)

    # Reshape image data into the original shape
    image = tf.reshape(image, [128, 128, 3])

    # Creates batches by randomly shuffling tensors
    images, labels = tf.train.shuffle_batch([image, label], batch_size=64, capacity=128, num_threads=2,
                                            min_after_dequeue=64)
</code></pre>

<p>Now I am finetuning the model using slim and this is the code. </p>

<pre><code>  init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    sess.run(init_op)

    # Create a coordinator and run all QueueRunner objects
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    # load model

    # load the inception model from the slim library - we are using inception v3
    #inputL = tf.placeholder(tf.float32, (64, 128, 128, 3))

    img, lbl = sess.run([images, labels])
    one_hot_labels = slim.one_hot_encoding(lbl, num_classes)

    with slim.arg_scope(slim.nets.inception.inception_v3_arg_scope()):
        logits, inceptionv3 = nets.inception.inception_v3(inputs=img, num_classes=5980, is_training=True,
                                                          dropout_keep_prob=.6)

    # Restore convolutional layers:

    variables_to_restore = slim.get_variables_to_restore(exclude=['InceptionV3/Logits', 'InceptionV3/AuxLogits'])
    init_fn = slim.assign_from_checkpoint_fn(model_path, variables_to_restore)

    # loss function
    loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits = logits)
    total_loss = tf.losses.get_total_loss()

    # train operation
    train_op = slim.learning.create_train_op(total_loss + loss, optimizer= tf.train.AdamOptimizer(learning_rate=1e-4))

    print('Im here')
    # Start training.
    slim.learning.train(train_op, log_dir, init_fn=init_fn, save_interval_secs=20, number_of_steps= 10)
</code></pre>

<p>Now I have few questions about the code, which I am quite unable to figure out. Once, the code reaches <strong>slim.learning.train</strong> I don't see anything printing however, it's training, I can see in the log. Now, 
1. How do I give the number of epochs to the code? Right now it's running step by step with each step has batch_size = 64.<br>
2. How do I make sure that in the code <strong>tf.train.shuffle_batch</strong> I am not repeating my images and I am training over the whole dataset? 
3. How can I print the loss values while it's training?</p>
","I am trying to fine-tune inceptionv3 model using slim tensorflow library. I am unable to understand certain things while writing the code for it. I tried to read source code (no proper documentation) and figured out few things and I am able to fine-tune it and save the check point. Here are the steps I followed 1. I created a tf.record for my training data which is fine, now I am reading the data using the below code. Now I am finetuning the model using slim and this is the code. Now I have few questions about the code, which I am quite unable to figure out. Once, the code reaches slim.learning.train I don't see anything printing however, it's training, I can see in the log. Now, 1. How do I give the number of epochs to the code? Right now it's running step by step with each step has batch_size = 64. 2. How do I make sure that in the code tf.train.shuffle_batch I am not repeating my images and I am training over the whole dataset? 3. How can I print the loss values while it's training?",https://stackoverflow.com/questions/49564318,7776604,Documentation Replication on Other Examples
49605330,Example of tf.feature_column.indicator_column,"<p>I’m reading tensorflow’s document about <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column"" rel=""nofollow noreferrer"">tf.feature_column.indicator_column</a>.</p>

<p>In this document, there is an example.</p>

<pre><code>name = indicator_column(categorical_column_with_vocabulary_list(
       'name', ['bob', 'george', 'wanda'])
columns = [name, ...]
features = tf.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)

dense_tensor == [[1, 0, 0]]  # If ""name"" bytes_list is [""bob""]
dense_tensor == [[1, 0, 1]]  # If ""name"" bytes_list is [""bob"", ""wanda""]
dense_tensor == [[2, 0, 0]]  # If ""name"" bytes_list is [""bob"", ""bob”]
</code></pre>

<p>My problem is the omitted(<code>...</code>) part of this code. I just want a complete, running, simple example. And I can’t find a kind example including tf.Example and so on.</p>

<p>Can anyone make this complete?</p>

<p>Thank you for advance.</p>
","I’m reading tensorflow’s document about tf.feature_column.indicator_column. In this document, there is an example. My problem is the omitted(...) part of this code. I just want a complete, running, simple example. And I can’t find a kind example including tf.Example and so on. Can anyone make this complete? Thank you for advance.",https://stackoverflow.com/questions/49605330,766330,Documentation Completeness
49633383,tensorflow tf.nn.rnn_cell.BasicLSTMCell,"<p>I'm trying to play with ""tf.nn.rnn_cell.BasicLSTMCell"" with TF on python.</p>

<p>Reading here ""<a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell</a>"", in the ""<strong>init</strong>"" method, I see the param ""num_units""
with description ""int, The number of units in the LSTM cell.""</p>

<p>But ... wait a moment ""number of UNITS""? Which type of units? The class is called ""...LSTMCell"" but from ""num_units"" it seem that we are speaking about a layer, not a single neuron.</p>

<p>I'm confused. Any help is appreciated.
TIA</p>
","I'm trying to play with ""tf.nn.rnn_cell.BasicLSTMCell"" with TF on python. Reading here ""https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell"", in the ""init"" method, I see the param ""num_units"" with description ""int, The number of units in the LSTM cell."" But ... wait a moment ""number of UNITS""? Which type of units? The class is called ""...LSTMCell"" but from ""num_units"" it seem that we are speaking about a layer, not a single neuron. I'm confused. Any help is appreciated. TIA",https://stackoverflow.com/questions/49633383,7835905,Documentation Replicability
49662470,Tensorflow global_step TypeError,"<p>I'm adding Tensorboard to an existing small Tensorflow project that I know works to practice working with Tensorboard but I get a type error that global_step must be a string or tensor, however I have assigned global_step to a <code>tf.Variable(0, name='global_step', trainable=False)</code> just like the documentation and every example I see online. Any idea of what I'm missing would be super appreciated. </p>



<p>---> 70             [summary_merge, tf.train.global_step(sess, global_step_tensor) ,update_model, weights], feed_dict)</p>

<p>TypeError: Fetch argument 0 has invalid type , must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)</p>

<pre class=""lang-python prettyprint-override""><code>import gym
import numpy as np
import random
import ipdb
import matplotlib.pyplot as plt
import tensorflow as tf
%matplotlib inline

tf.reset_default_graph()
env = gym.make('FrozenLake-v0')

global_step_tensor = tf.Variable(0, name='global_step', trainable=False)
saver = tf.train.Saver()

with tf.name_scope('policy-network'):
    observations = tf.placeholder(shape=[1, env.observation_space.n], dtype=tf.float32, name='input')
    weights = tf.Variable(tf.random_uniform([16,4], 0, 0.01))
    Qout = tf.matmul(observations, weights)
    predict = tf.argmax(Qout, 1)

    nextQ = tf.placeholder(shape=[1, 4], dtype=tf.float32)
    loss = tf.reduce_sum(tf.square(nextQ - Qout))
    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
    update_model = trainer.minimize(loss, global_step= global_step_tensor)

with tf.name_scope('summaries'):
    summary_merge = tf.summary.merge([
        tf.summary.histogram('loss', loss),
        tf.summary.histogram('weights', weights)
    ])


init = tf.global_variables_initializer()

gamma = .99
epsilon = 0.1
num_episodes = 2000

j_list = []
r_list = []

with tf.Session() as sess:
    # Create a writer for summary data
    writer = tf.summary.FileWriter('/tmp/display', sess.graph)
    sess.run(init)
    for i in range(num_episodes):
        s = env.reset()
        reward_all = 0
        done = False
        j = 0

        while j &lt; 99:
            j+= 1

            a, allQ = sess.run([predict, Qout], feed_dict={observations:np.identity(16)[s:s+1]})
            if np.random.rand(1) &lt; epsilon:
                a[0] = env.action_space.sample()

            s1, reward, done, _ = env.step(a[0])
            Q1 = sess.run(Qout, feed_dict={observations: np.identity(16)[s1:s1+1]}) 

            maxQ1 = np.max(Q1)
            targetQ = allQ

            targetQ[0, a[0]] = reward + gamma*maxQ1
            feed_dict = {observations: np.identity(16)[s:s+1], nextQ:targetQ}

            # Run prediction operation, evaluate summary_merge and assign to summaries
            summaries, global_step_tensor, _, W1 = sess.run(
            [summary_merge, tf.train.global_step(sess, global_step_tensor) ,update_model, weights], feed_dict)

            # Write summaries to writer
            writer.add_summary(summaries, global_step)

            reward_all += reward
            s = s1

            if done == True:
                epsilon = 1./((i/50) + 10)
                break

        if (i+1) % 1000 == 0:
            # Save the graph every 1000 episodes
            saver.save(sess, '/tmp/checkpoint', global_step=global_step)

        j_list.append(j)
        r_list.append(reward_all)

print(""Percent of succesful episodes: {} %"".format((sum(r_list)/num_episodes)))
</code></pre>
","I'm adding Tensorboard to an existing small Tensorflow project that I know works to practice working with Tensorboard but I get a type error that global_step must be a string or tensor, however I have assigned global_step to a tf.Variable(0, name='global_step', trainable=False) just like the documentation and every example I see online. Any idea of what I'm missing would be super appreciated. ---&gt; 70 [summary_merge, tf.train.global_step(sess, global_step_tensor) ,update_model, weights], feed_dict) TypeError: Fetch argument 0 has invalid type , must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)",https://stackoverflow.com/questions/49662470,5780994,Lack of Alternative Solutions/Documentation
49686860,Side effect in tf.while_loop,"<p>I am currently having a hard time trying to understand how tensorflow works, and I feel like the python interface is somehow obscure.</p>

<p>I recently tried to run a simple print statement inside a tf.while_loop, and there are many things that remains unclear to me:</p>

<pre><code>import tensorflow as tf

nb_iter = tf.constant(value=10)
#This solution does not work at all
#nb_iter = tf.get_variable('nb_iter', shape=(1), dtype=tf.int32, trainable=False)
i = tf.get_variable('i', shape=(), trainable=False,
                     initializer=tf.zeros_initializer(), dtype=nb_iter.dtype)

loop_condition = lambda i: tf.less(i, nb_iter)
def loop_body(i):
    tf.Print(i, [i], message='Another iteration')
    return [tf.add(i, 1)]

i = tf.while_loop(loop_condition, loop_body, [i])

initializer_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(initializer_op)
    res = sess.run(i)
    print('res is now {}'.format(res))
</code></pre>

<p>Notice that if I initialize nb_iter with</p>

<pre><code>nb_iter = tf.get_variable('nb_iter', shape=(1), dtype=tf.int32, trainable=False)
</code></pre>

<p>I got the following error:</p>

<blockquote>
  <p>ValueError: Shape must be rank 0 but is rank 1 for 'while/LoopCond'
  (op: 'LoopCond') with input shapes: [1].</p>
</blockquote>

<p>It get even worse when I try to use the 'i' index for indexing a tensor (example not shown here), I then get the following error</p>

<blockquote>
  <p>alueError: Operation 'while/strided_slice' has been marked as not
  fetchable.</p>
</blockquote>

<p>Can someone point me to a documentation that explains how tf.while_loop works when used with tf.Variables, and if it possible to use side_effects (like print) inside the loop, as well as indexing tensor with the loop variable ?</p>

<p>Thank you in advance for your help</p>
","I am currently having a hard time trying to understand how tensorflow works, and I feel like the python interface is somehow obscure. I recently tried to run a simple print statement inside a tf.while_loop, and there are many things that remains unclear to me: Notice that if I initialize nb_iter with I got the following error: It get even worse when I try to use the 'i' index for indexing a tensor (example not shown here), I then get the following error Can someone point me to a documentation that explains how tf.while_loop works when used with tf.Variables, and if it possible to use side_effects (like print) inside the loop, as well as indexing tensor with the loop variable ? Thank you in advance for your help",https://stackoverflow.com/questions/49686860,2697831,Requesting (Additional) Resources
49688134,TensorFlow session inside Keras custom loss function,"<p>After going through some Stack questions and the Keras documentation, I manage to write some code trying to evaluate the gradient of the output of a neural network w.r.t its inputs, the purpose being a simple exercise of approximating a bivariate function (<code>f(x,y) = x^2+y^2</code>)  using as loss the difference between analytical and automatic differentiation.</p>

<p>Combining answers from two questions (<a href=""https://stackoverflow.com/questions/46464549"">Keras custom loss function: Accessing current input pattern
</a> and <a href=""https://stackoverflow.com/questions/39561560"">Getting gradient of model output w.r.t weights using Keras
</a>), I came up with this:</p>

<pre><code>import tensorflow as tf
from keras import backend as K
from keras.models import Model
from keras.layers import Dense, Activation, Input

def custom_loss(input_tensor):

    outputTensor = model.output       
    listOfVariableTensors = model.input      
    gradients = K.gradients(outputTensor, listOfVariableTensors)

    sess = tf.InteractiveSession()
    sess.run(tf.initialize_all_variables())
    evaluated_gradients = sess.run(gradients,feed_dict={model.input:input_tensor})

    grad_pred = K.add(evaluated_gradients[0], evaluated_gradients[1])
    grad_true = k.add(K.scalar_mul(2, model.input[0][0]), K.scalar_mul(2, model.input[0][1])) 

    return K.square(K.subtract(grad_pred, grad_true))

input_tensor = Input(shape=(2,))
hidden = Dense(10, activation='relu')(input_tensor)
out = Dense(1, activation='sigmoid')(hidden)
model = Model(input_tensor, out)
model.compile(loss=custom_loss_wrapper(input_tensor), optimizer='adam')
</code></pre>

<p>Which yields the error: <code>TypeError: The value of a feed cannot be a tf.Tensor object.</code> because of <code>feed_dict={model.input:input_tensor}</code>. I understand the error, I just don't know how to fix it. </p>

<p>From what I gathered, I can't simply pass input data into the loss function, it must be a tensor. I realized Keras would 'understand' it when I call <code>input_tensor</code>. This all just leads me to think I'm doing things the wrong way, trying to evaluate the gradient like that. Would really appreciate some enlightenment.</p>
","After going through some Stack questions and the Keras documentation, I manage to write some code trying to evaluate the gradient of the output of a neural network w.r.t its inputs, the purpose being a simple exercise of approximating a bivariate function (f(x,y) = x^2+y^2) using as loss the difference between analytical and automatic differentiation. Combining answers from two questions (Keras custom loss function: Accessing current input pattern and Getting gradient of model output w.r.t weights using Keras ), I came up with this: Which yields the error: TypeError: The value of a feed cannot be a tf.Tensor object. because of feed_dict={model.input:input_tensor}. I understand the error, I just don't know how to fix it. From what I gathered, I can't simply pass input data into the loss function, it must be a tensor. I realized Keras would 'understand' it when I call input_tensor. This all just leads me to think I'm doing things the wrong way, trying to evaluate the gradient like that. Would really appreciate some enlightenment.",https://stackoverflow.com/questions/49688134,1058309,Documentation Replicability
49701918,tf.layers.batch_normalization parameters,"<p>I am not sure if it is only me who thinks that tensorflow documentation is a bit weak.</p>

<p>I was planing to use the tf.nn.batch_normalization function to implement batch normalization but later recognized the  tf.layers.batch_normalization function which seemingly should be the one to use for its simplicity. But the documentation is really poor if I may say it.</p>

<p>I am trying to understand how to <em>correctly</em> use it but with the information provided on the Web page is it really not easy. I am hoping that maybe some other people have experience and help me (and possibly many others) to understand it.. </p>

<p>Let me share the interface first:</p>

<pre><code>tf.layers.batch_normalization(
    inputs,
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer=tf.zeros_initializer(),
    gamma_initializer=tf.ones_initializer(),
    moving_mean_initializer=tf.zeros_initializer(),
    moving_variance_initializer=tf.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    training=False,
    trainable=True,
    name=None,
    reuse=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=None,
    virtual_batch_size=None,
    adjustment=None
)
</code></pre>

<p>Q1) beta values are initialized to zero and gamma values are initialized to 1. But it does not say why. When batch normalization used, I understand that the ordinary bias parameter of the neural network becomes obsolete and beta parameter in the batch normalization step kind of does the same thing. From that angle, setting beta to zero is understandable. But why are gamma values initialized to 1? Is that really the most efficient way?</p>

<p>Q2) I see a momentum parameter there as well. The documentation just says "" Momentum for the moving average."". I assume that this parameter is used when calculating the ""mean"" value for a certain mini batch in the corresponding hidden layer. With other words, the mean value used in batch normalization is NOT the mean of current mini batch, it is rather primarily the mean of the last 100 mini batches (since momentum = 0.99). But it is very unclear how this parameter affects the execution in testing, or if I am just validating my model on the dev set by calculating cost and accuracy. My <em>assumption</em> is that anytime I deal with test and dev sets, I set the parameter ""training"" to False so that momentum parameter becomes obsolete for that particular execution and the ""mean"" and ""variance"" values that were calculated during the training are used now instead of calculating new mean and variance values. It is how it should be if I am mistaken but I do not see anything in the documentation if it is the case. Could anyone confirm that my understanding correct? If not, I would really appreciate further explanation on this.</p>

<p>Q3) I am having difficulties to give a meaning to the trainable parameter. I assume beta and gamma params are meant here. Why would they not be trainable?</p>

<p>Q4) The ""reuse"" parameter. What is it really?</p>

<p>Q5) adjustment parameter. Another mistery..</p>

<p>Q5) A kind of summary question.. Here is my overall assumption that needs confirmation and feedback.. Important params here are:
- inputs
- axis
- momentum
- center
- scale
- training
And I assume that as long as the training=True when training, we are safe. And as long as training=False when validating dev set or test set or even when using the model in real life, we are safe too.</p>

<p>Any feedback will really be appreciated.</p>

<p>ADDENDUM:</p>

<p>Confusion continues. Help!</p>

<p>I am trying to use this function instead of implementing a batch normalizer manually. I have the following forward propagation function that loops through layers of the NN.</p>

<pre><code>def forward_propagation_with_relu(X, num_units_in_layers, parameters, 
                                  normalize_batch, training, mb_size=7):

    L = len(num_units_in_layers)

    A_temp = tf.transpose(X)

    for i in range (1, L):
        W = parameters.get(""W""+str(i))
        b = parameters.get(""b""+str(i))
        Z_temp = tf.add(tf.matmul(W, A_temp), b)

        if normalize_batch:
            if (i &lt; (L-1)):  
                with tf.variable_scope(""batch_norm_scope"", reuse=tf.AUTO_REUSE):
                    Z_temp = tf.layers.batch_normalization(Z_temp, axis=-1, 
                                                           training=training)

        A_temp = tf.nn.relu(Z_temp)

    return Z_temp   #This is the linear output of last layer
</code></pre>

<p>The tf.layers.batch_normalization(..) function wants to have static dimensions but I do not have it in my case.</p>

<p>Since I apply mini batches rather than training the entire train set each time before I run the optimizer, 1 dimension of the X appears to be unknown.</p>

<p>If I write:</p>

<pre><code>print(X.shape)
</code></pre>

<p>I get:</p>

<pre><code>(?, 5)
</code></pre>

<p>And when this is the case, when I run the whole program I get the following error below.</p>

<p>I saw in some other threads that some people say that they could solve the problem by using tf.reshape function. I try it.. Forward prop goes fine but later on it crashes in the Adam Optimizer..</p>

<p>Here is what I get when I run the code above (without using tf.reshape):</p>

<p>How do I solve this???</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-191-990fb7d7f7f6&gt; in &lt;module&gt;()
     24 parameters = nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs,
     25                       normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers,
---&gt; 26                       lambd, print_progress)
     27 
     28 print(parameters)

&lt;ipython-input-190-59594e979129&gt; in nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs, normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers, lambd, print_progress)
     34         # Forward propagation: Build the forward propagation in the tensorflow graph
     35         ZL = forward_propagation_with_relu(X_mini_batch, num_units_in_layers, 
---&gt; 36                                            parameters, normalize_batch, training)
     37 
     38     with tf.name_scope(""calc_cost""):

&lt;ipython-input-187-8012e2fb6236&gt; in forward_propagation_with_relu(X, num_units_in_layers, parameters, normalize_batch, training, mb_size)
     15                 with tf.variable_scope(""batch_norm_scope"", reuse=tf.AUTO_REUSE):
     16                     Z_temp = tf.layers.batch_normalization(Z_temp, axis=-1, 
---&gt; 17                                                            training=training)
     18 
     19         A_temp = tf.nn.relu(Z_temp)

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py in batch_normalization(inputs, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, training, trainable, name, reuse, renorm, renorm_clipping, renorm_momentum, fused, virtual_batch_size, adjustment)
    775       _reuse=reuse,
    776       _scope=name)
--&gt; 777   return layer.apply(inputs, training=training)
    778 
    779 

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py in apply(self, inputs, *args, **kwargs)
    805       Output tensor(s).
    806     """"""
--&gt; 807     return self.__call__(inputs, *args, **kwargs)
    808 
    809   def _add_inbound_node(self,

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    676           self._defer_regularizers = True
    677           with ops.init_scope():
--&gt; 678             self.build(input_shapes)
    679           # Create any regularizers added by `build`.
    680           self._maybe_create_variable_regularizers()

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py in build(self, input_shape)
    251       if axis_to_dim[x] is None:
    252         raise ValueError('Input has undefined `axis` dimension. Input shape: ',
--&gt; 253                          input_shape)
    254     self.input_spec = base.InputSpec(ndim=ndims, axes=axis_to_dim)
    255 

ValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(6), Dimension(None)]))
</code></pre>

<p>This is so hopeless.. </p>

<p>ADDENDUM(2)</p>

<p>I am adding more information:</p>

<p>The following simply means that there are 5 units in input layer, 6 units in each hidden layer, and 2 units in output layer.</p>

<pre><code>num_units_in_layers = [5,6,6,2] 
</code></pre>

<p>Here is the updated version of forward prop function with tf.reshape</p>

<pre><code>def forward_propagation_with_relu(X, num_units_in_layers, parameters, 
                                  normalize_batch, training, mb_size=7):

    L = len(num_units_in_layers)
    print(""X.shape before reshape: "", X.shape)             # ADDED LINE 1
    X = tf.reshape(X, [mb_size, num_units_in_layers[0]])   # ADDED LINE 2
    print(""X.shape after reshape: "", X.shape)              # ADDED LINE 3
    A_temp = tf.transpose(X)

    for i in range (1, L):
        W = parameters.get(""W""+str(i))
        b = parameters.get(""b""+str(i))
        Z_temp = tf.add(tf.matmul(W, A_temp), b)

        if normalize_batch:
            if (i &lt; (L-1)):  
                with tf.variable_scope(""batch_norm_scope"", reuse=tf.AUTO_REUSE):
                    Z_temp = tf.layers.batch_normalization(Z_temp, axis=-1, 
                                                           training=training)

        A_temp = tf.nn.relu(Z_temp)

    return Z_temp   #This is the linear output of last layer
</code></pre>

<p>When I do this, I can run the forward prop function. But it seems to be crashing in later execution. Here is the error that I get. (Note that I print out the shape of input X before and after reshaping in the forward prop function).</p>

<pre><code>X.shape before reshape:  (?, 5)
X.shape after reshape:  (7, 5)

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1349     try:
-&gt; 1350       return fn(*args)
   1351     except errors.OpError as e:

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1328                                    feed_dict, fetch_list, target_list,
-&gt; 1329                                    status, run_metadata)
   1330 

~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    515             compat.as_text(c_api.TF_Message(self.status.status)),
--&gt; 516             c_api.TF_GetCode(self.status.status))
    517     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: Incompatible shapes: [7] vs. [2]
     [[Node: forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub = Sub[T=DT_FLOAT, _class=[""loc:@batch_norm_scope/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward_prop/batch_norm_scope/batch_normalization/cond_2/Switch_1:1, forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1:1)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-222-990fb7d7f7f6&gt; in &lt;module&gt;()
     24 parameters = nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs,
     25                       normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers,
---&gt; 26                       lambd, print_progress)
     27 
     28 print(parameters)

&lt;ipython-input-221-59594e979129&gt; in nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs, normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers, lambd, print_progress)
     88                                                                         cost_mini_batch,
     89                                                                         accuracy_mini_batch],
---&gt; 90                                                                         feed_dict={training: True})
     91                       nr_of_minibatches += 1
     92                       sum_minibatch_costs += minibatch_cost

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1126     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1127       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1128                              feed_dict_tensor, options, run_metadata)
   1129     else:
   1130       results = []

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1342     if handle is None:
   1343       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-&gt; 1344                            options, run_metadata)
   1345     else:
   1346       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1361         except KeyError:
   1362           pass
-&gt; 1363       raise type(e)(node_def, op, message)
   1364 
   1365   def _extend_graph(self):

InvalidArgumentError: Incompatible shapes: [7] vs. [2]
     [[Node: forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub = Sub[T=DT_FLOAT, _class=[""loc:@batch_norm_scope/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward_prop/batch_norm_scope/batch_normalization/cond_2/Switch_1:1, forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1:1)]]

Caused by op 'forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub', defined at:
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py"", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py"", line 478, in start
    self.io_loop.start()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2850, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-222-990fb7d7f7f6&gt;"", line 26, in &lt;module&gt;
    lambd, print_progress)
  File ""&lt;ipython-input-221-59594e979129&gt;"", line 36, in nn_model
    parameters, normalize_batch, training)
  File ""&lt;ipython-input-218-62e4c6126c2c&gt;"", line 19, in forward_propagation_with_relu
    training=training)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 777, in batch_normalization
    return layer.apply(inputs, training=training)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 807, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 697, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 602, in call
    lambda: self.moving_mean)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/utils.py"", line 211, in smart_cond
    return control_flow_ops.cond(pred, true_fn=fn1, false_fn=fn2, name=name)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1985, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1839, in BuildCondBranch
    original_result = fn()
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 601, in &lt;lambda&gt;
    lambda: _do_update(self.moving_mean, new_mean),
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 597, in _do_update
    var, value, self.momentum, zero_debias=False)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/training/moving_averages.py"", line 87, in assign_moving_average
    update_delta = (variable - value) * decay
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 778, in _run_op
    return getattr(ops.Tensor, operator)(a._AsTensor(), *args)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 934, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4819, in _sub
    ""Sub"", x=x, y=y, name=name)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3267, in create_op
    op_def=op_def)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Incompatible shapes: [7] vs. [2]
     [[Node: forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub = Sub[T=DT_FLOAT, _class=[""loc:@batch_norm_scope/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward_prop/batch_norm_scope/batch_normalization/cond_2/Switch_1:1, forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1:1)]]
</code></pre>

<p>Regarding the question why the shape of X is not static.. I don't know...
HEre is how I setup the dataset.</p>

<pre><code>with tf.name_scope(""next_train_batch""):
    filenames = tf.placeholder(tf.string, shape=[None])
    dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv))
    dataset = dataset.shuffle(buffer_size=1000)
    dataset = dataset.batch(minibatch_size)
    iterator = dataset.make_initializable_iterator()
    X_mini_batch, Y_mini_batch = iterator.get_next()
</code></pre>

<p>I have 2 csv files that include the train data.</p>

<pre><code>train_path1 = ""train1.csv""
train_path2 = ""train2.csv""
train_input_paths = [train_path1, train_path2]
</code></pre>

<p>And I use the initializable iterator as following:</p>

<pre><code>sess.run(iterator.initializer, 
         feed_dict={filenames: train_input_paths})
</code></pre>

<p>During the training, I keep getting mini batches from the train set. Everything works fine when I disable batch normalization. If I enable batch norm, it requires static shape of the input X (mini batch). I reshape it but this time it crashes later in the execution as seen above. </p>

<p>ADDENDUM(3)</p>

<p>I guess I figured out where it crashes. It probably crashes when I run the optimizer after calculating the cost.</p>

<p>First the sequence of commands:
First forward prop, then compute cost, then run optimizer. First 2 seems to be working but not the optimizer.</p>

<p>HEre is how I define the optimizer:</p>

<pre><code>with tf.name_scope(""train""):
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):        
        # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.
        optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_mini_batch)
</code></pre>

<p>I have the update_ops there to be able to update the moving averages. If I interpret it right, it is just crashing when it tries to update moving averages. I might be misinterpreting the error msg as well.. </p>

<p>ADDENDUM(4)</p>

<p>I tried to normalize based on the known dimension and it worked! But that's not the dimension I would like to normalize, which is now confusing. Let me elaborate:</p>

<p>nr of units in input layer: 5
nr of units in layer 1 (first hidden layer): 6
so weight1 is (6, 5) matrix
Assume that mini batch size is 7.
Shape of A[0] (or X_mini_batch) in my case is: (7, 5), where 7 is the # training samples in mini batch, and 5 is the # units in input layer.</p>

<p>When calculating Z[1]...
Z[1] = weight1 * A[0].transpose
... then shape of Z[1] is (6, 7) matrix, where each column gives 6 features for each train sample.</p>

<p>The question is then which column do we want to normalize in Z[1]? What makes sense to me is that you normalize each feature from all given train samples. This means that I need to normalize each row bcz I have different feature values for different train examples in each row. And since Z[1] has the shape (6, 7), if I set axis=0, it should refer to normalization in each row. And 7 is the unknown number in my case so it doesn't hurt. Based on this logic, it works! But I am totally puzzled if axis=0 really refers to each row here... Let me show another example about this axis issue, which has bothered me for a long time now..</p>

<p>The <em>irrelevant from this topic</em> code example:</p>

<pre><code>cc = tf.constant([[1.,2.,3.], 
                  [4.,5.,6.]])

with tf.Session() as sess:
    print(sess.run(tf.reduce_mean(cc, axis=0)))
    print(sess.run(tf.reduce_mean(cc, axis=1)))  
</code></pre>

<p>This gives the following output:</p>

<pre><code>[2.5 3.5 4.5]
[2. 5.]
</code></pre>

<p>When I set axis to 0, it is giving the average of each column. And if axis=1, it is giving the average of each row.</p>

<p>(Note that cc.shape gives (2,3))</p>

<p>Now the million dollar question: In a 2 dimensional matrix, is axis 0 or 1 when I want to address each row?</p>

<p>ADDENDUM(5)
I guess I get it now correctly. Let me summarize my axis understanding here. Hopefully I am getting it right now...</p>

<p>Here is the Z[1] matrix representation with the shape (6,7):</p>

<p>t_ex :   train example
f:       feature</p>

<pre><code>t_ex1   t_ex2   t_ex3   t_ex4   t_ex5   t_ex6   t_ex7
  f1      f1      f1      f1      f1      f1      f1
  f2      f2      f2      f2      f2      f2      f2
  f3      f3      f3      f3      f3      f3      f3
  f4      f4      f4      f4      f4      f4      f4
  f5      f5      f5      f5      f5      f5      f5
  f6      f6      f6      f6      f6      f6      f6
</code></pre>

<p>In this mini batch above, there are 7 train examples and each train ex has 6 features (since there are 6 units in layer 1). When we say ""tf.layers.batch_normalization(..,axis=0)"", we mean that the normalization has to be done per row for each feature to eliminate the high variance between - say - f1 values in the first row.</p>

<p>With other words, we do NOT normalize f1,f2,f3,f4,f5,f6 with each other. We normalize f1:s with each other, and f2:s with each other, and so on..</p>
","I am not sure if it is only me who thinks that tensorflow documentation is a bit weak. I was planing to use the tf.nn.batch_normalization function to implement batch normalization but later recognized the tf.layers.batch_normalization function which seemingly should be the one to use for its simplicity. But the documentation is really poor if I may say it. I am trying to understand how to correctly use it but with the information provided on the Web page is it really not easy. I am hoping that maybe some other people have experience and help me (and possibly many others) to understand it.. Let me share the interface first: Q1) beta values are initialized to zero and gamma values are initialized to 1. But it does not say why. When batch normalization used, I understand that the ordinary bias parameter of the neural network becomes obsolete and beta parameter in the batch normalization step kind of does the same thing. From that angle, setting beta to zero is understandable. But why are gamma values initialized to 1? Is that really the most efficient way? Q2) I see a momentum parameter there as well. The documentation just says "" Momentum for the moving average."". I assume that this parameter is used when calculating the ""mean"" value for a certain mini batch in the corresponding hidden layer. With other words, the mean value used in batch normalization is NOT the mean of current mini batch, it is rather primarily the mean of the last 100 mini batches (since momentum = 0.99). But it is very unclear how this parameter affects the execution in testing, or if I am just validating my model on the dev set by calculating cost and accuracy. My assumption is that anytime I deal with test and dev sets, I set the parameter ""training"" to False so that momentum parameter becomes obsolete for that particular execution and the ""mean"" and ""variance"" values that were calculated during the training are used now instead of calculating new mean and variance values. It is how it should be if I am mistaken but I do not see anything in the documentation if it is the case. Could anyone confirm that my understanding correct? If not, I would really appreciate further explanation on this. Q3) I am having difficulties to give a meaning to the trainable parameter. I assume beta and gamma params are meant here. Why would they not be trainable? Q4) The ""reuse"" parameter. What is it really? Q5) adjustment parameter. Another mistery.. Q5) A kind of summary question.. Here is my overall assumption that needs confirmation and feedback.. Important params here are: - inputs - axis - momentum - center - scale - training And I assume that as long as the training=True when training, we are safe. And as long as training=False when validating dev set or test set or even when using the model in real life, we are safe too. Any feedback will really be appreciated. ADDENDUM: Confusion continues. Help! I am trying to use this function instead of implementing a batch normalizer manually. I have the following forward propagation function that loops through layers of the NN. The tf.layers.batch_normalization(..) function wants to have static dimensions but I do not have it in my case. Since I apply mini batches rather than training the entire train set each time before I run the optimizer, 1 dimension of the X appears to be unknown. If I write: I get: And when this is the case, when I run the whole program I get the following error below. I saw in some other threads that some people say that they could solve the problem by using tf.reshape function. I try it.. Forward prop goes fine but later on it crashes in the Adam Optimizer.. Here is what I get when I run the code above (without using tf.reshape): How do I solve this??? This is so hopeless.. ADDENDUM(2) I am adding more information: The following simply means that there are 5 units in input layer, 6 units in each hidden layer, and 2 units in output layer. Here is the updated version of forward prop function with tf.reshape When I do this, I can run the forward prop function. But it seems to be crashing in later execution. Here is the error that I get. (Note that I print out the shape of input X before and after reshaping in the forward prop function). Regarding the question why the shape of X is not static.. I don't know... HEre is how I setup the dataset. I have 2 csv files that include the train data. And I use the initializable iterator as following: During the training, I keep getting mini batches from the train set. Everything works fine when I disable batch normalization. If I enable batch norm, it requires static shape of the input X (mini batch). I reshape it but this time it crashes later in the execution as seen above. ADDENDUM(3) I guess I figured out where it crashes. It probably crashes when I run the optimizer after calculating the cost. First the sequence of commands: First forward prop, then compute cost, then run optimizer. First 2 seems to be working but not the optimizer. HEre is how I define the optimizer: I have the update_ops there to be able to update the moving averages. If I interpret it right, it is just crashing when it tries to update moving averages. I might be misinterpreting the error msg as well.. ADDENDUM(4) I tried to normalize based on the known dimension and it worked! But that's not the dimension I would like to normalize, which is now confusing. Let me elaborate: nr of units in input layer: 5 nr of units in layer 1 (first hidden layer): 6 so weight1 is (6, 5) matrix Assume that mini batch size is 7. Shape of A[0] (or X_mini_batch) in my case is: (7, 5), where 7 is the # training samples in mini batch, and 5 is the # units in input layer. When calculating Z[1]... Z[1] = weight1 * A[0].transpose ... then shape of Z[1] is (6, 7) matrix, where each column gives 6 features for each train sample. The question is then which column do we want to normalize in Z[1]? What makes sense to me is that you normalize each feature from all given train samples. This means that I need to normalize each row bcz I have different feature values for different train examples in each row. And since Z[1] has the shape (6, 7), if I set axis=0, it should refer to normalization in each row. And 7 is the unknown number in my case so it doesn't hurt. Based on this logic, it works! But I am totally puzzled if axis=0 really refers to each row here... Let me show another example about this axis issue, which has bothered me for a long time now.. The irrelevant from this topic code example: This gives the following output: When I set axis to 0, it is giving the average of each column. And if axis=1, it is giving the average of each row. (Note that cc.shape gives (2,3)) Now the million dollar question: In a 2 dimensional matrix, is axis 0 or 1 when I want to address each row? ADDENDUM(5) I guess I get it now correctly. Let me summarize my axis understanding here. Hopefully I am getting it right now... Here is the Z[1] matrix representation with the shape (6,7): t_ex : train example f: feature In this mini batch above, there are 7 train examples and each train ex has 6 features (since there are 6 units in layer 1). When we say ""tf.layers.batch_normalization(..,axis=0)"", we mean that the normalization has to be done per row for each feature to eliminate the high variance between - say - f1 values in the first row. With other words, we do NOT normalize f1,f2,f3,f4,f5,f6 with each other. We normalize f1:s with each other, and f2:s with each other, and so on..",https://stackoverflow.com/questions/49701918,9328846,Lack of Alternative Solutions/Documentation
49768997,Writing TFRecords in batches,"<p>All documentations I found regarding TFRecords are generating <code>tf.train.Example()</code>s one by one, and writing them using</p>

<pre><code>writer = tf.python_io.TFRecordWrite(path)
ex = generate_example(features)  # Returns tf.train.Example() instance
writer.write(ex.SerializeToString())
</code></pre>

<p>Since I'm dealing with very big data, I know that I'll pay a high overhead price for writing examples separately</p>

<p>Is there any way to write multiple <code>tf.train.Example()</code> to a TFRecord at once?</p>
","All documentations I found regarding TFRecords are generating tf.train.Example()s one by one, and writing them using Since I'm dealing with very big data, I know that I'll pay a high overhead price for writing examples separately Is there any way to write multiple tf.train.Example() to a TFRecord at once?",https://stackoverflow.com/questions/49768997,5368083,Documentation Replication on Other Examples
49899526,Tensorflow input pipeline where multiple rows correspond to a single observation?,"<p>So I've just started using Tensorflow, and I'm struggling to properly understand input pipelines. </p>

<p>The problem I'm working on is sequence classification.
I'm trying to read in a CSV file with shape (100000, 4). First 3 columns are features, 4th column is the label. BUT - the data represents sequences of length 10 i.e. rows 1-10 are sequence 1, rows 11-20 are sequence 2 etc. This also means each label is repeated 10 times.</p>

<p>So at some point in this input pipeline, I'll need to reshape my feature tensor like tf.reshape(features, [batch_size_, rows_per_ob, input_dim]). 
And only take every 10th row of my label tensor like label[::rows_per_ob]</p>

<p>Another thing I should point out is that my actual dataset is in the billions of rows so I have to think about performance.</p>

<p>I've put together the below code from documentation and other posts on here, but I don't think I fully understand this because I'm seeing the following error:</p>

<blockquote>
  <p>INFO:tensorflow:Error reported to Coordinator: , Attempting to use uninitialized value input_producer_2/limit_epochs/epochs</p>
</blockquote>

<p>There seems to be an out of range error.</p>

<p>I also can't figure out what to do with these batches once I get them working. Initially, I thought I would reshape them then just feed them into ""feed_dict"", but then I read that this is really bad, and I should be using a tf.data.Dataset object. But I'm not sure how to feed these batches into a Dataset. I'm also not entirely sure when would be the optimal time in this process to reshape my data?</p>

<p>And a final point of confusion - when you use an Iterator with a Dataset object, I see that we use the get_next() method. Does this mean that each element in the Dataset represent a full batch of data? And does this then mean that if we want to change the batch size, we need rebuild the entire Dataset object?</p>

<p>I'm really struggling to fit all the pieces together. If anyone has any pointers for me, it would be very much appreciated! Thanks!</p>

<pre><code># import
import tensorflow as tf

# constants
filename = ""tensorflow_test_data.csv""
num_rows = 100000
rows_per_ob = 10
batch_size_ = 5
num_epochs_ = 2
num_batches = int(num_rows * num_epochs_ / batch_size_ / rows_per_ob)

# read csv line
def read_from_csv(filename_queue):
    reader = tf.TextLineReader(skip_header_lines=1)
    _, value = reader.read(filename_queue)
    record_defaults = [[0.0], [0.0], [0.0], [0.0]]
    a, b, c, d = tf.decode_csv(value, record_defaults=record_defaults)
    features = tf.stack([a, b, c])
    return features, d

def input_pipeline(filename=filename, batch_size=batch_size_, num_epochs=num_epochs_):
    filename_queue = tf.train.string_input_producer([filename],
                                                    num_epochs=num_epochs,
                                                    shuffle=False)
    x, y = read_from_csv(filename_queue)
    x_batch, y_batch = tf.train.batch([x, y],
                                      batch_size = batch_size * rows_per_ob,
                                      num_threads=1,
                                      capacity=10000)
    return x_batch, y_batch

###
x, y = input_pipeline(filename, batch_size=batch_size_,
                      num_epochs = num_epochs_)

# I imagine using lists is wrong here - this was more just for me to
# see the output
x_list = []
y_list = []
with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    for _ in range(num_batches):
        x_batch, y_batch = sess.run([x, y])
        x_list.append(x_batch)
        y_list.append(y_batch)
    coord.request_stop()
    coord.join(threads)
</code></pre>
","So I've just started using Tensorflow, and I'm struggling to properly understand input pipelines. The problem I'm working on is sequence classification. I'm trying to read in a CSV file with shape (100000, 4). First 3 columns are features, 4th column is the label. BUT - the data represents sequences of length 10 i.e. rows 1-10 are sequence 1, rows 11-20 are sequence 2 etc. This also means each label is repeated 10 times. So at some point in this input pipeline, I'll need to reshape my feature tensor like tf.reshape(features, [batch_size_, rows_per_ob, input_dim]). And only take every 10th row of my label tensor like label[::rows_per_ob] Another thing I should point out is that my actual dataset is in the billions of rows so I have to think about performance. I've put together the below code from documentation and other posts on here, but I don't think I fully understand this because I'm seeing the following error: There seems to be an out of range error. I also can't figure out what to do with these batches once I get them working. Initially, I thought I would reshape them then just feed them into ""feed_dict"", but then I read that this is really bad, and I should be using a tf.data.Dataset object. But I'm not sure how to feed these batches into a Dataset. I'm also not entirely sure when would be the optimal time in this process to reshape my data? And a final point of confusion - when you use an Iterator with a Dataset object, I see that we use the get_next() method. Does this mean that each element in the Dataset represent a full batch of data? And does this then mean that if we want to change the batch size, we need rebuild the entire Dataset object? I'm really struggling to fit all the pieces together. If anyone has any pointers for me, it would be very much appreciated! Thanks!",https://stackoverflow.com/questions/49899526,5323535,Documentation Replication on Other Examples
49959130,how to insert two or more label lists in the tf.estimator.inputs.numpy_input_fn?,"<p>I am using the <code>tf.estimator.inputs.numpy_input_fn</code> to feed data in my model and train it in a similar way with the <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""nofollow noreferrer"">MNIST example</a>. The only difference is that I need to insert two numpy lists of labels instead of one. I tried passing them in a dictionary like this:</p>

<pre><code>train_input_fn = tf.estimator.inputs.numpy_input_fn(
            x={""x"": training_images},
            y={""labels1"": training_labels1, ""labels2"": training_labels2},
            batch_size=BATCH_SIZE,
            num_epochs=None,
            shuffle=True)

my_cnn_model.train(input_fn=train_input_fn,steps=NUM_TRAINING_STEPS)
</code></pre>

<p>Then when I try to retrieve them in the model like so:</p>

<pre><code>def build_cnn_model(features, labels, mode):
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>AttributeError: 'dict' object has no attribute 'shape'</p>
</blockquote>

<p>I also tried to change the name of the variable ""labels"" to be ""targets"" according to <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn"" rel=""nofollow noreferrer"">tensorflow inputs.numpy_input_fn documentation</a>:</p>

<pre><code>def build_cnn_model(features, targets, mode):
</code></pre>

<p>and I get this error:</p>

<blockquote>
  <p><code>ValueError: model_fn (&lt;function build_cnn_model at 0x7f88df9c9d08&gt;)
  has following not expected args: ['targets']</code></p>
</blockquote>

<p>If you have any solution or suggestion to my problem, please let me know.</p>

<p>Thanks a lot in advance.</p>

<p>Antonios</p>
","I am using the tf.estimator.inputs.numpy_input_fn to feed data in my model and train it in a similar way with the MNIST example. The only difference is that I need to insert two numpy lists of labels instead of one. I tried passing them in a dictionary like this: Then when I try to retrieve them in the model like so: I get the following error: I also tried to change the name of the variable ""labels"" to be ""targets"" according to tensorflow inputs.numpy_input_fn documentation: and I get this error: If you have any solution or suggestion to my problem, please let me know. Thanks a lot in advance. Antonios",https://stackoverflow.com/questions/49959130,7184238,Documentation Ambiguity
49987839,How to handle None in tf.clip_by_global_norm?,"<p>I have read in answers to <a href=""https://stackoverflow.com/questions/36498127/how-to-apply-gradient-clipping-in-tensorflow"">this question here</a> that tf.clip_by_global_norm() handles None values by simply ignoring them (comment by danijar in comments to the answer by @danijar) but when i try to apply it i seem to be doing something wrong as it throws </p>

<p>ValueError: None values not supported.</p>

<pre><code>tf.reset_default_graph()
z = tf.get_variable(name = 'z', shape = [1])
b = tf.get_variable('b', [1])
c = b*b - 2*b + 1
optimizer = tf.train.AdamOptimizer(0.1)
gradients, variables = zip(*optimizer.compute_gradients(c))
gradients = tf.clip_by_global_norm(gradients, 2.5)
train_op = optimizer.apply_gradients(zip(gradients, variables))
</code></pre>

<p>Can somebody please tell me what am i doing wrong or if tf.clip_by_global_norm() does not handle None gradients and i have to take care of them manually</p>

<p>The official documentation seems to agree with @danijar's comments. see <a href=""https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/clip_by_global_norm"" rel=""nofollow noreferrer"">here</a></p>

<blockquote>
  <p>Any of the entries of t_list that are of type None are ignored.</p>
</blockquote>
",I have read in answers to this question here that tf.clip_by_global_norm() handles None values by simply ignoring them (comment by danijar in comments to the answer by @danijar) but when i try to apply it i seem to be doing something wrong as it throws ValueError: None values not supported. Can somebody please tell me what am i doing wrong or if tf.clip_by_global_norm() does not handle None gradients and i have to take care of them manually The official documentation seems to agree with @danijar's comments. see here,https://stackoverflow.com/questions/49987839,6546694,Documentation Replicability
49997294,Moving away from tf.contrib.learn: distributed training with dedicated evaluator process,"<p>In TF 1.8's upcoming release, <code>tf.contrib.learn.*</code> will be deprecated.
The <code>tf.contrib.learn.Experiment</code> class recommends switching to <code>tf.estimator.train_and_evaluate</code> instead, so I'm trying to port my code to that framework.</p>

<p>What I want to do is set up distributed training on two machines' GPUs, plus a third CPU-only process that does continuous evaluation on a small validation set.</p>

<p>Following the examples in <a href=""https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer"">the documentation of <code>train_and_evaluate</code></a> and the <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">Distributed Tensorflow</a> guide, I managed to set up the training half of my desired architecture, but I can't find a way to set up an estimator.</p>

<p>So far, what I have looks as follows:</p>

<pre><code>def input_fn(mode, num_classes, batch_size):  
  # [...] build input pipeline
  return {'input': images}, labels

def model_fn(features, labels, num_classes, mode):
  # [...] build model
  return tf.estimator.EstimatorSpec(
    mode=mode,
    predictions=predictions,
    loss=total_loss,
    train_op=train_op,
    eval_metric_ops=metrics,
    export_outputs=export_outputs)

def distributed_main_v2(unused_argv):
  """"""Expects `unused_argv` to be a list ['&lt;task_type&gt;', '&lt;task_id&gt;']""""""  
  import json
  # Set up environment variables according to the parameters passed to the process
  TF_CONFIG = {
    'cluster': {
        ""ps"": [
            ""host1:2222"",
        ],
        ""chief"": [
            ""host1:2223"",
            ],
        ""worker"": [
            ""host2:2224""
            ]
    },
    'environment': 'cluster',    
    'task': {
        'type': unused_argv[1].strip(),
        'index': unused_argv[2].strip() if len(unused_argv) &gt; 2 else 0
        }
  }
  os.environ['TF_CONFIG'] = json.dumps(TF_CONFIG)
  if unused_argv[1].strip() not in ['worker', 'chief']:
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # leave the GPU to the worker process

  # create the estimator
  # define warm start configuration
  regex = '^(?!.*final_layer*|.*aux_logits*)'
  ws_settings = tf.estimator.WarmStartSettings('checkpoint_path', regex)

  gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.95) # fix for cuDNN fatal memory error with tf.contrib.learn.Experiment (TODO: still necessary?)
  sess_conf = tf.ConfigProto(gpu_options=gpu_opts)
  run_conf = tf.estimator.RunConfig(session_config=sess_conf)

  # Create the Estimator
  estimator = tf.estimator.Estimator(
    model_fn=lambda features, labels, mode: model_fn(features, labels, NUM_CLASSES, mode),
    model_dir=model_dir,
    config=run_conf,
    warm_start_from=ws_settings)

  # Set up input functions for training and evaluation
  train_input_fn = lambda : input_fn(tf.estimator.ModeKeys.TRAIN, NUM_CLASSES, batch_size)
  eval_input_fn = lambda : input_fn(tf.estimator.ModeKeys.EVAL, NUM_CLASSES, batch_size)

  train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=steps)
  eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)

  # start distributed training
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

if __name__ == '__main__':
  # set up globals and parse known arguments
  distributed_main_v2(unused_argv)
</code></pre>

<p>This code works, although my understanding of it is still pretty limited. I get what the PS and workers do, but from the specification of <code>chief</code> I understand this should be the ""master"" worker that also logs summaries and saves checkpoints. What I'm missing now is the periodic evaluation... and I'm at a loss. From the <code>train_and_evaluate</code> codebase I see there's some ""evaluator"" support but I don't understand how to set it up properly.</p>
","In TF 1.8's upcoming release, tf.contrib.learn.* will be deprecated. The tf.contrib.learn.Experiment class recommends switching to tf.estimator.train_and_evaluate instead, so I'm trying to port my code to that framework. What I want to do is set up distributed training on two machines' GPUs, plus a third CPU-only process that does continuous evaluation on a small validation set. Following the examples in the documentation of train_and_evaluate and the Distributed Tensorflow guide, I managed to set up the training half of my desired architecture, but I can't find a way to set up an estimator. So far, what I have looks as follows: This code works, although my understanding of it is still pretty limited. I get what the PS and workers do, but from the specification of chief I understand this should be the ""master"" worker that also logs summaries and saves checkpoints. What I'm missing now is the periodic evaluation... and I'm at a loss. From the train_and_evaluate codebase I see there's some ""evaluator"" support but I don't understand how to set it up properly.",https://stackoverflow.com/questions/49997294,3214872,Documentation Replication on Other Examples
50029121,How to use tf.layers classes instead of functions,"<p>It seems that tf.Layer modules come in two flavours: functions and classes. I normally use the functions directly (e.g, tf.layers.dense) but I'd like to know how to use classes directly (tf.layers.<strong>D</strong>ense). I've started experimenting with the new eager execution mode in tensorflow and I think using classes are going to be useful there as well but I haven't seen good examples in the documentation. Is there any part of TF documentation that shows how these are used? </p>

<p>I guess it would make sense to use them in a class where these layers are instantiated in the <code>__init__</code> and then they're linked in the <code>__call__</code> method when the inputs and dimensions are known?</p>

<p>Are these tf.layer classes related to <code>tf.keras.Model</code>? Is there an equivalent wrapper class for using <code>tf.layers</code>?</p>

<p><strong>Update:</strong> for eager execution there's <code>tfe.Network</code> that must be inherited. There's an example <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/neural_network_eager_api.py"" rel=""noreferrer"">here</a></p>
","It seems that tf.Layer modules come in two flavours: functions and classes. I normally use the functions directly (e.g, tf.layers.dense) but I'd like to know how to use classes directly (tf.layers.Dense). I've started experimenting with the new eager execution mode in tensorflow and I think using classes are going to be useful there as well but I haven't seen good examples in the documentation. Is there any part of TF documentation that shows how these are used? I guess it would make sense to use them in a class where these layers are instantiated in the __init__ and then they're linked in the __call__ method when the inputs and dimensions are known? Are these tf.layer classes related to tf.keras.Model? Is there an equivalent wrapper class for using tf.layers? Update: for eager execution there's tfe.Network that must be inherited. There's an example here",https://stackoverflow.com/questions/50029121,298209,Lack of Alternative Solutions/Documentation
50054453,Tensorflow shape inference static RNN compiler error,"<p>I am working on OCR software optimized for phone camera images. </p>

<p>Currently, each 300 x 1000 x 3 (RGB) image is reformatted as a 900 x 1000 numpy array. I have plans for a more complex model architecture, but for now I just want to get a baseline working. I want to get started by training a static RNN on the data that I've generated.</p>

<p>Formally, I am feeding in n_t at each timestep t for T timesteps, where n_t is a 900-vector and T = 1000 (similar to reading the whole image left to right). Here is the Tensorflow code in which I create batches for training:</p>

<pre><code>sequence_dataset = tf.data.Dataset.from_generator(example_generator, (tf.int32, 
tf.int32))
sequence_dataset = sequence_dataset.batch(experiment_params['batch_size'])
iterator = sequence_dataset.make_initializable_iterator() 
x_batch, y_batch = iterator.get_next()
</code></pre>

<p>The tf.nn.static_bidirectional_rnn documentation claims that the input must be a ""length T list of inputs, each a tensor of shape [batch_size, input_size], or a nested tuple of such elements."" So, I go through the following steps in order to get the data into the correct format.</p>

<pre><code># Dimensions go from [batch, n , t] -&gt; [t, batch, n]
x_batch = tf.transpose(x_batch, [2, 0, 1])

# Unpack such that x_batch is a length T list with element dims [batch_size, n]
x_batch = tf.unstack(x_batch, experiment_params['example_t'], 0)
</code></pre>

<p>Without altering the batch any further, I make the following call:</p>

<pre><code>output, _, _ = tf.nn.static_rnn(lstm_fw_cell, x_batch, dtype=tf.int32)
</code></pre>

<p>Note that I do not explicitly tell Tensorflow the dimensions of the matrices (this could be the problem). They all have the same dimensionality, yet I am getting the following bug:</p>

<pre><code>ValueError: Input size (dimension 0 of inputs) must be accessible via shape 
inference, but saw value None.
</code></pre>

<p>At which point in my stack should I be declaring the dimensions of my input? Because I am using a Dataset and hoping to get its batches directly to the RNN, I am not sure that the ""placeholder -> feed_dict"" route makes sense. If that in fact is the method that makes the most sense, let me know what that looks like (I definitely do not know). Otherwise, let me know if you have any other insights to the problem. Thanks!</p>
","I am working on OCR software optimized for phone camera images. Currently, each 300 x 1000 x 3 (RGB) image is reformatted as a 900 x 1000 numpy array. I have plans for a more complex model architecture, but for now I just want to get a baseline working. I want to get started by training a static RNN on the data that I've generated. Formally, I am feeding in n_t at each timestep t for T timesteps, where n_t is a 900-vector and T = 1000 (similar to reading the whole image left to right). Here is the Tensorflow code in which I create batches for training: The tf.nn.static_bidirectional_rnn documentation claims that the input must be a ""length T list of inputs, each a tensor of shape [batch_size, input_size], or a nested tuple of such elements."" So, I go through the following steps in order to get the data into the correct format. Without altering the batch any further, I make the following call: Note that I do not explicitly tell Tensorflow the dimensions of the matrices (this could be the problem). They all have the same dimensionality, yet I am getting the following bug: At which point in my stack should I be declaring the dimensions of my input? Because I am using a Dataset and hoping to get its batches directly to the RNN, I am not sure that the ""placeholder -&gt; feed_dict"" route makes sense. If that in fact is the method that makes the most sense, let me know what that looks like (I definitely do not know). Otherwise, let me know if you have any other insights to the problem. Thanks!",https://stackoverflow.com/questions/50054453,9707928,Documentation Replication on Other Examples
50078749,Tensorflow-hub Text-Module Preprocessing,"<p>I'm playing around with the new Modules which are available on the tensorflow-hub (which I really like - thanks for that).</p>

<p>Whats unclear to me, is the preprocessing which should take place when feeding a sentence. The module <a href=""https://www.tensorflow.org/hub/modules/google/nnlm-en-dim128/1"" rel=""nofollow noreferrer"">documentation</a> says, that in the preprocessing step the inputj sentences gets splitted at the spaces.</p>

<p>However, when I run the following program, I only get a single vector:</p>

<pre><code>with tf.device(""/cpu:0""):
  embed = hub.Module(""https://tfhub.dev/google/nnlm-en-dim128/1"")

global_step1 = tf.train.get_or_create_global_step()
with tf.device(""/cpu:0""):
  embeddings = embed({""default"": [""Cat sat on mat""]})

with tf.train.MonitoredTrainingSession(is_chief=True) as sess:
  message_embeddings_cat = sess.run(embeddings)
  print(message_embeddings_cat.shape) # (result: (1, 128))
</code></pre>

<p>How do I get the embeddings for each word, and what does the single vector represents? A <em>fixed-dimensional representation</em> of the sentence, the <em>Unknown-Word</em> embedding or something else?</p>

<p>Thanks in advance!</p>

<p>Edit: It seems the result is a combined embedding created with <a href=""https://github.com/tensorflow/hub/blob/master/examples/text_embeddings/export.py#L130"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup_sparse</code></a>. (Thanks for the confirmation @svsgoogle)</p>
","I'm playing around with the new Modules which are available on the tensorflow-hub (which I really like - thanks for that). Whats unclear to me, is the preprocessing which should take place when feeding a sentence. The module documentation says, that in the preprocessing step the inputj sentences gets splitted at the spaces. However, when I run the following program, I only get a single vector: How do I get the embeddings for each word, and what does the single vector represents? A fixed-dimensional representation of the sentence, the Unknown-Word embedding or something else? Thanks in advance! Edit: It seems the result is a combined embedding created with tf.nn.embedding_lookup_sparse. (Thanks for the confirmation @svsgoogle)",https://stackoverflow.com/questions/50078749,863543,Documentation Replication on Other Examples
50149953,Using tf.keras within Tensorflow,"<p>What is the correct way of using the <code>tf.keras</code> API. Can <code>tf.layers.*</code> be directly replaced with <code>tf.keras.layers</code>(Similarly activations or loss functions)? Is it necessary to import <code>tf.keras.backend</code> and do <code>set_learning_phase</code>? This doesnt seem to be explained on the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras"" rel=""nofollow noreferrer"">official TF docs</a> but is mentioned in this <a href=""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"" rel=""nofollow noreferrer"">relatively old blog post</a>. </p>
",What is the correct way of using the tf.keras API. Can tf.layers.* be directly replaced with tf.keras.layers(Similarly activations or loss functions)? Is it necessary to import tf.keras.backend and do set_learning_phase? This doesnt seem to be explained on the official TF docs but is mentioned in this relatively old blog post.,https://stackoverflow.com/questions/50149953,3656081,Lack of Alternative Solutions/Documentation
50203668,Using tf.custom_gradient in tensorflow r1.8,"<h3>System information</h3>

<ul>
<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Y</li>
<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>
<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>
<li><strong>TensorFlow version (use command below)</strong>: r1.8</li>
<li><strong>Python version</strong>: 2.7.14</li>
<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4</li>
<li><strong>CUDA/cuDNN version</strong>: 8.0/7.0</li>
<li><strong>GPU model and memory</strong>: GTX1080, 8G</li>
<li><strong>Bazel version</strong>: N/A</li>
<li><strong>Exact command to reproduce</strong>: python test_script.py</li>
</ul>

<h3>Describe the problem</h3>

<p>Hello, I'm trying to make a custom_gradient op using the function of <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">tf.custom_gradient</a>. I made my test code based on the API explanation online. However, it seems there is a problem in the custom_gradient function. Thanks!</p>

<h3>Source code / logs</h3>

<pre><code>import tensorflow as tf
import numpy as np

@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.log(1 + e), grad

x = tf.constant(100.)
f = tf.custom_gradient(log1pexp)

y, dy = f(x)

sess = tf.Session()
print (y.eval(session=sess), y.eval(session=sess).shape)

File ""/home/local/home/research/DL/unit_tests/tf_test_custom_grad.py"", line 14, in &lt;module&gt;
    y, dy = f(x)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated
    return _graph_mode_decorator(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 132, in _graph_mode_decorator
    result, grad_fn = f(*args)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 439, in __iter__
""Tensor objects are not iterable when eager execution is not ""
TypeError: Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn.
</code></pre>
","Hello, I'm trying to make a custom_gradient op using the function of tf.custom_gradient. I made my test code based on the API explanation online. However, it seems there is a problem in the custom_gradient function. Thanks!",https://stackoverflow.com/questions/50203668,9749663,Documentation Ambiguity
50210594,the function of 'bounding_boxes' and 'min_object_covered' in tf.image.sample_distorted_bounding_box?,"<p>How parameters 'bounding_boxes' and 'min_object_covered' control the generation of a single randomly distorted bounding box for an image in tf.image.sample_distorted_bounding_box? </p>

<p>I have read the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/sample_distorted_bounding_box"" rel=""nofollow noreferrer"">function</a> in tensorflow api, but I still can not understand the proplem. Maybe I need a intuitive example.</p>
","How parameters 'bounding_boxes' and 'min_object_covered' control the generation of a single randomly distorted bounding box for an image in tf.image.sample_distorted_bounding_box? I have read the function in tensorflow api, but I still can not understand the proplem. Maybe I need a intuitive example.",https://stackoverflow.com/questions/50210594,8307005,Requesting (Additional) Resources
50222149,How to scan through tensor not at dimension 0?,"<p>The tensorflow document states that tf.scan scans on the list of tensors unpacked from elems on dimension 0.
The simplest version of scan repeatedly applies the callable fn to a sequence of elements from first to last. The elements are made of the tensors unpacked from elems on dimension 0.</p>

<p>My question is:
How to scan on the list of tensors on other dimension instead of dimension 0?
For example, 
I have a tensor, ref, defined as below.</p>

<pre><code>&gt;&gt;&gt; ref = tf.Variable(tf.ones([2,3,3],tf.int32))
....
&gt;&gt;&gt; print(ref.eval())
[[[1 1 1]
  [1 1 1]
  [1 1 1]]

 [[1 1 1]
  [1 1 1]
  [1 1 1]]]
</code></pre>

<p>I want to scan through the ref[1,0], ref[1,1], ref[1,2] and apply a function to each of the, ,say add 1.
That is to say, I want ref be after the operation</p>

<pre><code>&gt;&gt;&gt; print(ref.eval())
[[[1 1 1]
  [1 1 1]
  [1 1 1]]

 [[2 2 2]
  [2 2 2]
  [2 2 2]]]
</code></pre>

<p>Can I use tf.scan to do that? If yes, how?
If not, any how to do in other way?</p>

<p>Thanks.</p>
","The tensorflow document states that tf.scan scans on the list of tensors unpacked from elems on dimension 0. The simplest version of scan repeatedly applies the callable fn to a sequence of elements from first to last. The elements are made of the tensors unpacked from elems on dimension 0. My question is: How to scan on the list of tensors on other dimension instead of dimension 0? For example, I have a tensor, ref, defined as below. I want to scan through the ref[1,0], ref[1,1], ref[1,2] and apply a function to each of the, ,say add 1. That is to say, I want ref be after the operation Can I use tf.scan to do that? If yes, how? If not, any how to do in other way? Thanks.",https://stackoverflow.com/questions/50222149,4987560,Documentation Replication on Other Examples
50226274,how to explain the output of tf.rank in tensorflow,"<p>I am new in tensorflow and have a question about tf.rank method.</p>

<p>In the doc <a href=""https://www.tensorflow.org/api_docs/python/tf/rank"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/rank</a> there is a simple example about the tf.rank:</p>

<pre><code># shape of tensor 't' is [2, 2, 3]
t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
tf.rank(t)  # 3
</code></pre>

<p>But when I run the code below:</p>

<pre><code>t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
print(tf.rank(t))  # 3
</code></pre>

<p>I get output like:</p>

<pre><code>Tensor(""Rank:0"", shape=(), dtype=int32)
</code></pre>

<p>Why can I get the output of ""3""?</p>
","I am new in tensorflow and have a question about tf.rank method. In the doc https://www.tensorflow.org/api_docs/python/tf/rank there is a simple example about the tf.rank: But when I run the code below: I get output like: Why can I get the output of ""3""?",https://stackoverflow.com/questions/50226274,4710918,Documentation Replication on Other Examples
50243230,Unable to understand tf.nn.raw_rnn,"<p>In the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn"" rel=""nofollow noreferrer"">official documentation</a> of <code>tf.nn.raw_rnn</code> we have emit structure as the third output of <code>loop_fn</code> when the <code>loop_fn</code> is run for the first time.</p>

<p>Later on the emit_structure is used to copy <code>tf.zeros_like(emit_structure)</code> to the minibatch entries that are finished by <code>emit = tf.where(finished, tf.zeros_like(emit_structure), emit)</code>.</p>

<p>my lack of understanding or lousy documentation on google's part is: emit structure is <code>None</code> so <code>tf.where(finished, tf.zeros_like(emit_structure), emit)</code> is going to throw a ValueError as <code>tf.zeros_like(None)</code> does so. Can somebody please fill in what I am missing here?</p>
","In the official documentation of tf.nn.raw_rnn we have emit structure as the third output of loop_fn when the loop_fn is run for the first time. Later on the emit_structure is used to copy tf.zeros_like(emit_structure) to the minibatch entries that are finished by emit = tf.where(finished, tf.zeros_like(emit_structure), emit). my lack of understanding or lousy documentation on google's part is: emit structure is None so tf.where(finished, tf.zeros_like(emit_structure), emit) is going to throw a ValueError as tf.zeros_like(None) does so. Can somebody please fill in what I am missing here?",https://stackoverflow.com/questions/50243230,6546694,Documentation Replication on Other Examples
50246535,Tensorflow estimator input function: defining each feature or not?,"<p>With <code>x</code> is a 120 x 4 feature matrix of Iris data (4 features) and <code>y</code> is a label, I can make an input function for <code>tf.estimator</code> like below </p>

<pre><code>def input_function(x, y):
    dict_x = {
        ""sepal_length"" : x[:,0],
        ""sepal_width"" :  x[:,1],
        ""petal_length"" : x[:,2],
        ""petal_width"" :  x[:,3]
    }

    dataset = tf.data.Dataset.from_tensor_slices((
        dict_x, y
    ))

    return dataset
</code></pre>

<p>then define the feature column like below:</p>

<pre><code>feature_columns = [
    tf.feature_column.numeric_column(key=""sepal_length""),
    tf.feature_column.numeric_column(key=""sepal_width""),
    tf.feature_column.numeric_column(key=""petal_length""),
    tf.feature_column.numeric_column(key=""petal_width"")
]
</code></pre>

<p>But, I found in the internet (I forget the source, still searching) that I also can define the input function like below. The difference with previous method is all four features now defined with only one key, <code>""x""</code>.</p>

<pre><code>def input_function(x, y):
    dict_x = {
        ""x"" : x,
    }

    dataset = tf.data.Dataset.from_tensor_slices((
        dict_x, y
    ))

    return dataset
</code></pre>

<p>then define the feature column like below:</p>

<pre><code>feature_columns = [
    tf.feature_column.numeric_column(key=""x"",shape=4),
]
</code></pre>

<p>I've run both method and both give almost same result. <strong>My question</strong>: I can't find any documentation that explain the difference between both method, because at a glance <code>dict_x</code> have different shape. Are they still treated equally at input layer on neural networks?</p>

<p>I'm new using <code>tf.estimator</code>, Thank You</p>

<p>My estimator code if needed:</p>

<pre><code>classifier = tf.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[10],
    n_classes=3,
    optimizer=tf.train.GradientDescentOptimizer(0.001),
    activation_fn=tf.nn.relu
)

# Train the model
classifier.train(
    input_fn=lambda:input_function(xtrain, ytrain, True)
)
</code></pre>
","With x is a 120 x 4 feature matrix of Iris data (4 features) and y is a label, I can make an input function for tf.estimator like below then define the feature column like below: But, I found in the internet (I forget the source, still searching) that I also can define the input function like below. The difference with previous method is all four features now defined with only one key, ""x"". then define the feature column like below: I've run both method and both give almost same result. My question: I can't find any documentation that explain the difference between both method, because at a glance dict_x have different shape. Are they still treated equally at input layer on neural networks? I'm new using tf.estimator, Thank You My estimator code if needed:",https://stackoverflow.com/questions/50246535,2147347,Lack of Alternative Solutions/Documentation
50383462,how to randomly initialize weights in tensorflow?,"<p>in tensorflow, I learned from the tutorial that one would initialize the variables with something like
<code>
sess.run(tf.global_variables_initializer())
</code></p>

<p>however I found that every time I run this with the same input dataset, the loss value starts with the same value.</p>

<p>I presume this is due to the fact that the initialization is always setting up the variables with the same values. (probably zero)</p>

<p>I wish to randomize the values of weights. I've tried searching for this but  tensorflow docs doesn't give a clear answer if the initialization is done with zero values by default or random values.</p>

<p>How can I specify the initializaing to setup random values?</p>

<hr>

<p><strong>update</strong></p>

<p>my network is first a bunch of CNNs and pooling layers like below:
```
conv1 = tf.layers.conv2d(inputs=input_layer, filters=32, kernel_size=[3,3], padding=""same"", activation=tf.nn.relu, name=""conv_chad_1"")</p>

<pre><code>    pool1 = tf.layers.max_pooling2d(inputs=conv1,pool_size=[2,2],strides=2)

    conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3,3], padding=""same"", activation=tf.nn.relu, name=""conv_chad_2"")

    pool2 = tf.layers.max_pooling2d(inputs=conv2,pool_size=[2,2],strides=2, name=""pool_chad_2"")
</code></pre>

<p>```</p>

<p>AFAIK, the weights are defined inside these predefined layers. How do I specify these layers to initialize their weight variables randomly??</p>
","in tensorflow, I learned from the tutorial that one would initialize the variables with something like sess.run(tf.global_variables_initializer()) however I found that every time I run this with the same input dataset, the loss value starts with the same value. I presume this is due to the fact that the initialization is always setting up the variables with the same values. (probably zero) I wish to randomize the values of weights. I've tried searching for this but tensorflow docs doesn't give a clear answer if the initialization is done with zero values by default or random values. How can I specify the initializaing to setup random values? update my network is first a bunch of CNNs and pooling layers like below: ``` conv1 = tf.layers.conv2d(inputs=input_layer, filters=32, kernel_size=[3,3], padding=""same"", activation=tf.nn.relu, name=""conv_chad_1"") ``` AFAIK, the weights are defined inside these predefined layers. How do I specify these layers to initialize their weight variables randomly??",https://stackoverflow.com/questions/50383462,3044831,Documentation Replication on Other Examples
50442156,Loading a model from tensorflow SavedModel onto mutliple GPUs,"<p>Let's say someone hands me a TF SavedModel and I would like to replicate this model on the 4 GPUs I have on my machine so I can run inference in parallel on batches of data. Are there any good examples of how to do this? </p>

<p>I can load a saved model in this way:</p>

<pre><code>def load_model(self, saved_model_dirpath):
    '''Loads a model from a saved model directory - this should 
       contain a .pb file and a variables directory'''

    signature_key = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
    input_key = 'input'
    output_key = 'output'

    meta_graph_def = tf.saved_model.loader.load(self.sess, [tf.saved_model.tag_constants.SERVING],
                                                saved_model_dirpath)
    signature = meta_graph_def.signature_def

    input_tensor_name = signature[signature_key].inputs[input_key].name
    output_tensor_name = signature[signature_key].outputs[output_key].name

    self.input_tensor = self.sess.graph.get_tensor_by_name(input_tensor_name)
    self.output_tensor = self.sess.graph.get_tensor_by_name(output_tensor_name)
</code></pre>

<p>..but this would require that I have a handle to the session. For models that I have written myself, I would have access to the inference function and I could just call it and wrap it using <code>with tf.device()</code>, but in this case, I'm not sure how to extract the inference function out of a Saved Model. Should I load 4 separate sessions or is there a better way? Couldn't find much documentation on this, but apologies in advance if I missed something. Thanks!</p>
","Let's say someone hands me a TF SavedModel and I would like to replicate this model on the 4 GPUs I have on my machine so I can run inference in parallel on batches of data. Are there any good examples of how to do this? I can load a saved model in this way: ..but this would require that I have a handle to the session. For models that I have written myself, I would have access to the inference function and I could just call it and wrap it using with tf.device(), but in this case, I'm not sure how to extract the inference function out of a Saved Model. Should I load 4 separate sessions or is there a better way? Couldn't find much documentation on this, but apologies in advance if I missed something. Thanks!",https://stackoverflow.com/questions/50442156,3953896,Inadequate Examples
50454095,tf.gradients - dimensions of output,"<p>Here is my code: </p>

<pre><code>import tensorflow as tf
tf.reset_default_graph()
x = tf.placeholder(tf.float32, [None, 3],name='x')
W_1 = tf.get_variable('W_1', [3,3], dtype = tf.float32, initializer=tf.constant_initializer(1.0))
layer_out = tf.matmul(x, W_1, name = 'layer_out')
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run([tf.gradients(layer_out, [x])], feed_dict = {x: np.array([[1,7,5]])} )
</code></pre>

<p>it returns:</p>

<pre><code>[[array([[3., 3., 3.]], dtype=float32)]]
</code></pre>

<p>I am expecting to get 3 by 3 matrix or as per <code>tf.gradients</code> docs list of dim 3 with 3 elements for each list entry.</p>

<p>What I am missing?</p>

<p><strong>UPDATE:</strong></p>

<p>I see in docs <a href=""https://www.tensorflow.org/api_docs/python/tf/gradients"" rel=""nofollow noreferrer"">tf.gradients</a></p>

<pre><code>A list of sum(dy/dx) for each x in xs
</code></pre>

<p>but why sum and how do I get all entries of Jacobian?</p>
",Here is my code: it returns: I am expecting to get 3 by 3 matrix or as per tf.gradients docs list of dim 3 with 3 elements for each list entry. What I am missing? UPDATE: I see in docs tf.gradients but why sum and how do I get all entries of Jacobian?,https://stackoverflow.com/questions/50454095,1700890,Lack of Alternative Solutions/Documentation
50457247,Tensorflow finding matching strings in tensor,"<p>I'm trying to find variables that end in <code>train_step</code> from <code>tf.report_uninitialized_variables(),</code> but you can't iterate over tensors without <code>eager execution.</code> I get that you need to use <code>tf.map_fn,</code> but I do not understand it well enough.</p>

<p>This is what I have:</p>

<pre><code>variables = []
for s, t in zip(tf.report_uninitialized_variables().eval(session=sess), 
                tf.report_uninitialized_variables()):
    if 'train_step' in s:
        variables.append(t)
train_step_init = tf.variables_initializer(variables, name='train_step_init')
</code></pre>
","I'm trying to find variables that end in train_step from tf.report_uninitialized_variables(), but you can't iterate over tensors without eager execution. I get that you need to use tf.map_fn, but I do not understand it well enough. This is what I have:",https://stackoverflow.com/questions/50457247,3938049,Documentation Replicability
50500579,what is the difference between tf.nn.max_pool() and tf.layers.max_pooling2d(),"<p>I am a beginner of tensorflow,i have met two methods about create max_pool layer these days. one is ""tf.nn.max_pool()"" and the other is ""tf.layers.max_pooling2d()"".i want to learn about its difference,and when to use them suitably.based on this ,i have read its official document and searched in google,it is not any help at all.i have searched it in stackoverflow , there is a similar answer(<a href=""https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d"">tf.nn.conv2d vs tf.layers.conv2d</a>）but it didn't solve my problem.does any one can help me? thanks in advance.</p>
","I am a beginner of tensorflow,i have met two methods about create max_pool layer these days. one is ""tf.nn.max_pool()"" and the other is ""tf.layers.max_pooling2d()"".i want to learn about its difference,and when to use them suitably.based on this ,i have read its official document and searched in google,it is not any help at all.i have searched it in stackoverflow , there is a similar answer(tf.nn.conv2d vs tf.layers.conv2d）but it didn't solve my problem.does any one can help me? thanks in advance.",https://stackoverflow.com/questions/50500579,7955072,Inadequate Examples
50560013,"Tensorflow, multi-label confusion matrix","<p>I am trying to figure out how to the generate a confusion matrix for a multi-label classification task using neural networks. I previously managed to calculate the accuracy using the function &quot;intersection&quot;, since for that I did not care about any ordering.</p>
<pre><code>intersection = tf.sets.set_intersection(predictions, labels)
</code></pre>
<p>However, in order to calculate the confusion matrix, I do care about the indexing order of the predictions/labels. And since the labels have always the same value (<code>1,1</code> or <code>0.5,0.5</code>) there is no possible sorting according to higher/lower value.</p>
<p>I wonder:</p>
<p><strong>1) Is it possible to calculate a confusion matrix for the multi-label classification task?</strong></p>
<p><strong>2) How would that be implemented ?</strong></p>
<p><strong>3) How can you handle the case of failure in predicting both labels? Since it is not possible to know which confusion belongs to which prediction.</strong></p>
<p><strong>4) What is the logic behind the sorting of the function tf.nn.top_k()</strong></p>
<p>Below I show an example of the code that I was trying to use.</p>
<pre><code>import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

Z = np.array([[7.0, 3.0, 5.0, 1.0, 0.0, 6.0],[2.0, 3.0, 4.0, 1.0, 3.25, 2.2], [2.0 , 5.0, 1.0, 7.0, 0.0, 8.0]])
Y = np.array([[0.5, 0, 0, 0.0, 0, 0.5],[0, 0.0, 0.5, 0, 0.5, 0], [0,0,0,0.5,0,0.5]])

_, predicted_softmax = tf.nn.top_k(tf.nn.softmax(Z), k = 2, sorted = False)
_ , labels = tf.nn.top_k(Y, k = 2, sorted = False)

with tf.Session() as sess:
    # reshape to (6,1) because there is 2 correct values per sample(2*3)
    print(predicted_softmax.eval().reshape(6,1))
    print(labels.eval().reshape(6,1))
    predicted = predicted_softmax.eval().reshape(6,1)
    labels_idx = labels.eval().reshape(6,1)

class_labels = np.arange(6)
cnf_matrix_train = confusion_matrix(labels_idx, predicted, labels = class_labels)

print(cnf_matrix_train)
</code></pre>
<p>I don't really get why the output of predicted_softmax is:</p>
<pre><code>[[5] [0] [4] [2] [3] [5]] , 
</code></pre>
<p>I was expecting [5] [3] for the last two terms. There is no any logic to this output. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/top_k"" rel=""nofollow noreferrer"">documentation</a> they don't specify anything about the ordering in the case that <code>sorted = False</code> thought, but I was expecting some consistent behavior.</p>
<p>Thanks for any help!</p>
","I am trying to figure out how to the generate a confusion matrix for a multi-label classification task using neural networks. I previously managed to calculate the accuracy using the function ""intersection"", since for that I did not care about any ordering. However, in order to calculate the confusion matrix, I do care about the indexing order of the predictions/labels. And since the labels have always the same value (1,1 or 0.5,0.5) there is no possible sorting according to higher/lower value. I wonder: 1) Is it possible to calculate a confusion matrix for the multi-label classification task? 2) How would that be implemented ? 3) How can you handle the case of failure in predicting both labels? Since it is not possible to know which confusion belongs to which prediction. 4) What is the logic behind the sorting of the function tf.nn.top_k() Below I show an example of the code that I was trying to use. I don't really get why the output of predicted_softmax is: I was expecting [5] [3] for the last two terms. There is no any logic to this output. In the documentation they don't specify anything about the ordering in the case that sorted = False thought, but I was expecting some consistent behavior. Thanks for any help!",https://stackoverflow.com/questions/50560013,9527947,Documentation Completeness
50606178,TensorFlow tf.data.Dataset and bucketing,"<p>For an LSTM network, I've seen great improvements with bucketing.</p>

<p>I've come across the <a href=""https://www.tensorflow.org/api_guides/python/contrib.training#Bucketing"" rel=""noreferrer"">bucketing section in the TensorFlow docs</a> which (tf.contrib).</p>

<p>Though in my network, I am using the <code>tf.data.Dataset</code> API, specifically I'm working with TFRecords, so my input pipeline looks something like this</p>

<pre><code>dataset = tf.data.TFRecordDataset(TFRECORDS_PATH)
dataset = dataset.map(_parse_function)
dataset = dataset.map(_scale_function)
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.padded_batch(batch_size, padded_shapes={.....})
</code></pre>

<p>How can I incorporate the bucketing method into a the <code>tf.data.Dataset</code> pipeline?</p>

<p>If it matters, in every record in the TFRecords file I have the sequence length saved as an integer.</p>
","For an LSTM network, I've seen great improvements with bucketing. I've come across the bucketing section in the TensorFlow docs which (tf.contrib). Though in my network, I am using the tf.data.Dataset API, specifically I'm working with TFRecords, so my input pipeline looks something like this How can I incorporate the bucketing method into a the tf.data.Dataset pipeline? If it matters, in every record in the TFRecords file I have the sequence length saved as an integer.",https://stackoverflow.com/questions/50606178,5368083,Documentation Replicability
50724495,Train and validate using tensorflow estimator,"<p>I have created a shallow NN using tf.estimator API. I would like to something similar to the hyperparameter search explained in here <a href=""https://www.youtube.com/watch?time_continue=948&amp;v=eBbEDRsCmv4"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?time_continue=948&amp;v=eBbEDRsCmv4</a> at TensorFlow Dev Summit.</p>

<p>I could not find any updated documentation about how can you do this. I have the following code (I will try to simplify as much as possible):</p>

<pre><code># Define nn architecture
def neural_net(features):
    input_layer = tf.cast(features['x'], tf.float32)
    hidden_layer = nn_layer(input_layer, 10, 'hidden_layer', act=tf.nn.relu)
    out_layer = nn_layer(hidden_layer, 2, 'out_layer', act=tf.nn.relu)
    return out_layer

# Define model function
def model_fn(features, labels, mode):
    # Build the neural network
    logits = neural_net(features&lt;9


    with tf.name_scope('loss'):
    # Define loss and optimizer
        loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)

    # Configure the Training
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
        train_op = optimizer.minimize(
        loss=loss,
        global_step=tf.train.get_global_step())

        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)


nn_classifier = tf.estimator.Estimator(model_fn=model_fn)


train_input_fn = tf.estimator.inputs.numpy_input_fn(
            x={""x"": train_data},
            y=train_labels,
            batch_size=100,
            num_epochs=None,
            shuffle=True)

nn_classifier.train(
        input_fn=train_input_fn,
        steps=20000
        )
</code></pre>

<p>Executing this code I can obtain the summary for the loss and observe it in Tensorboard. But imagine I want to obtain different curves. Let's say that I want to see how the loss evolves with the number of samples, so I would train two models with different sample size. Or two models with a different architecture... whatever.</p>

<p>How can I get these two curves in Tensorboard?</p>
","I have created a shallow NN using tf.estimator API. I would like to something similar to the hyperparameter search explained in here https://www.youtube.com/watch?time_continue=948&amp;v=eBbEDRsCmv4 at TensorFlow Dev Summit. I could not find any updated documentation about how can you do this. I have the following code (I will try to simplify as much as possible): Executing this code I can obtain the summary for the loss and observe it in Tensorboard. But imagine I want to obtain different curves. Let's say that I want to see how the loss evolves with the number of samples, so I would train two models with different sample size. Or two models with a different architecture... whatever. How can I get these two curves in Tensorboard?",https://stackoverflow.com/questions/50724495,8380638,Documentation Replication on Other Examples
50820781,quesion about the axis of tf.stack(),"<p>I read the doc of <code>tf.stack()</code> on <a href=""https://www.tensorflow.org/api_docs/python/tf/stack"" rel=""nofollow noreferrer"">tensorflow stack </a>. There is an example on the page:</p>

<pre><code>&gt;&gt;&gt; x = tf.constant([1, 4])
&gt;&gt;&gt; y = tf.constant([2, 5])
&gt;&gt;&gt; z = tf.constant([3, 6])
&gt;&gt;&gt; sess=tf.Session()
&gt;&gt;&gt; sess.run(tf.stack([x, y, z]))
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)
&gt;&gt;&gt; sess.run(tf.stack([x, y, z], axis=1))
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)
</code></pre>

<p>what I don't understand is the second example where <code>axis=1</code>.</p>

<p>From the result it seems it converts the three inputs rows to columns first </p>

<p>and then put them toghter along the <code>axis=1</code>, but </p>

<p>I think the result should be </p>

<pre><code>array([[1,4, 2, 5, 3, 6 ]] dtype=int32 )
</code></pre>

<p>can anyone help explain this?</p>

<p>Thanks!</p>
","I read the doc of tf.stack() on tensorflow stack . There is an example on the page: what I don't understand is the second example where axis=1. From the result it seems it converts the three inputs rows to columns first and then put them toghter along the axis=1, but I think the result should be can anyone help explain this? Thanks!",https://stackoverflow.com/questions/50820781,1347796,Inadequate Examples
50825446,Restoring a trained generator network in DCGAN,"<p>I have a question regarding the saving and storing models in tensorflow. I know how to save a model with tf.train.Saver() and load it later through meta file. My problem is this:</p>

<p>I have trained a variant of DCGAN (Deep Convolution GAN), now I want to use only generator network for other tasks. Unfortunately, I do not know how to get entire generator network such that if I feed it with a new vector z, it generates an output based on the trained parameters. All the example I found in the stackoverflow, or tensorflow documentation, just mention very simple operations with two numbers. This is not I want. I want to understand if you have trained a giant network, say with 50 layers, how to load it and feed it with new input and get the output without going into the different parameters and layers in the trained network. I want to load it as a blackbox.</p>
","I have a question regarding the saving and storing models in tensorflow. I know how to save a model with tf.train.Saver() and load it later through meta file. My problem is this: I have trained a variant of DCGAN (Deep Convolution GAN), now I want to use only generator network for other tasks. Unfortunately, I do not know how to get entire generator network such that if I feed it with a new vector z, it generates an output based on the trained parameters. All the example I found in the stackoverflow, or tensorflow documentation, just mention very simple operations with two numbers. This is not I want. I want to understand if you have trained a giant network, say with 50 layers, how to load it and feed it with new input and get the output without going into the different parameters and layers in the trained network. I want to load it as a blackbox.",https://stackoverflow.com/questions/50825446,9932068,Documentation Replication on Other Examples
50840759,"Incorrect name returned in Tensorflow causes ""Tensor which does not exist"" error while invoking get_tensor_by_name","<p>As per the <a href=""https://www.tensorflow.org/programmers_guide/graphs#naming_operations"" rel=""nofollow noreferrer"" title=""description"">documentation</a> TensorFlow would append ""_1"", ""_2"", and so on to the name in tf.Graph namespace, in order to make it unique.  Here I define two convolutional operations.  It is expected that the first one will be named as ""conv2d"" and second one ""conv2d_1"".  But when I try to obtain the name of the second convolution it returns ""conv2d_2"".  I causes error when I try to invoke get_tensor_by_name. Here is the code:</p>

<pre><code>import numpy as np
import tensorflow as tf
import os

x = tf.constant(np.random.randn(1,2,2,1), dtype=tf.float32)
kernel_size = (1,1)
no_of_out = 20
strides = (1,1)
conv_out1 = tf.layers.conv2d(x, 10, (1,1), (1,1))
conv_out2 = tf.layers.conv2d(x, 10, (1,1), (1,1))

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print conv_out1.name # conv2d/BiasAdd:0 .  This value is correct
    print conv_out2.name # conv2d_2/BiasAdd:0 .  This value is incorrect.  It should be conv2d_1/BiasAdd:0
    conv_weights1 = tf.get_default_graph().get_tensor_by_name(os.path.split(conv_out1.name)[0] + '/kernel:0')
    conv_weights2 = tf.get_default_graph().get_tensor_by_name('conv2d_1/kernel:0')  
    conv_weights2 = tf.get_default_graph().get_tensor_by_name(os.path.split(conv_out2.name)[0] + '/kernel:0')
</code></pre>

<p>I could not understand why conv_out2.name returns ""conv2d_2"" instead of ""conv2d_1""</p>
","As per the documentation TensorFlow would append ""_1"", ""_2"", and so on to the name in tf.Graph namespace, in order to make it unique. Here I define two convolutional operations. It is expected that the first one will be named as ""conv2d"" and second one ""conv2d_1"". But when I try to obtain the name of the second convolution it returns ""conv2d_2"". I causes error when I try to invoke get_tensor_by_name. Here is the code: I could not understand why conv_out2.name returns ""conv2d_2"" instead of ""conv2d_1""",https://stackoverflow.com/questions/50840759,8284911,Documentation Ambiguity
50967885,"tf.gradients, how can I understand `grad_ys` and use it?","<p>In <code>tf.gradients</code>,  there is a keyword argument <code>grad_ys</code></p>

<blockquote>
  <p><code>grad_ys</code> is a list of tensors of the same length as <code>ys</code> that holds the initial gradients for each <code>y</code> in <code>ys</code>. When <code>grad_ys</code> is None, we fill in a tensor of ‘1’s of the shape of <code>y</code> for each <code>y</code> in <code>ys</code>. A user can provide their own initial <code>grad_ys</code> to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).</p>
</blockquote>

<p>Why is <code>grads_ys</code> needed here? The docs here is implicit. Could you please give some specific purpose and code?</p>

<p>And my example code for <code>tf.gradients</code> is</p>

<pre class=""lang-py prettyprint-override""><code>In [1]: import numpy as np

In [2]: import tensorflow as tf

In [3]: sess = tf.InteractiveSession()

In [4]: X = tf.placeholder(""float"", shape=[2, 1])

In [5]: Y = tf.placeholder(""float"", shape=[2, 1])

In [6]: W = tf.Variable(np.random.randn(), name='weight')

In [7]: b = tf.Variable(np.random.randn(), name='bias')

In [8]: pred = tf.add(tf.multiply(X, W), b)

In [9]: cost = 0.5 * tf.reduce_sum(tf.pow(pred-Y, 2))

In [10]: grads = tf.gradients(cost, [W, b])

In [11]: sess.run(tf.global_variables_initializer())

In [15]: W_, b_, pred_, cost_, grads_ = sess.run([W, b, pred, cost, grads], 
                                    feed_dict={X: [[2.0], [3.]], Y: [[3.0], [2.]]})
</code></pre>
","In tf.gradients, there is a keyword argument grad_ys Why is grads_ys needed here? The docs here is implicit. Could you please give some specific purpose and code? And my example code for tf.gradients is",https://stackoverflow.com/questions/50967885,5046896,Documentation Replicability
51069173,What exactly qualifies as a 'Tensor' in TensorFlow?,"<p>I am new to TensorFlow and just went through the eager execution tutorial and came across the tf.decode_csv function. Not knowing about it, I read the documentation.  <a href=""https://www.tensorflow.org/api_docs/python/tf/decode_csv"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/decode_csv</a></p>

<p>I don't really understand it. </p>

<p>The documentation says 'records: A Tensor of type string.' 
<strong>So, my question is: What qualifies as a 'Tensor'?</strong> </p>

<p>I tried the following code:</p>

<pre><code>dec_res = tf.decode_csv('0.1,0.2,0.3', [[0.0], [0.0], [0.0]])
print(dec_res, type(dec_res))



l = [[1,2,3],[4,5,6],[7,8,9]]
r = tf.reshape(l, [9,-1])
print(l, type(l))
print(r, type(r))
</code></pre>

<p>So the list <code>dec_res</code> contains tf.tensor objects. That seems reasonable to me. But is an ordinary string also a 'Tensor' according to the documentation?</p>

<p>Then I tried something else with the <code>tf.reshape</code> function. In the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/reshape"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/reshape</a> it says that 'tensor: A Tensor.' So, <code>l</code> is supposed to be a tensor. But it is not of type <code>tf.tensor</code> but simply a python <code>list</code>. This is confusing.</p>

<p>Then the documentation says </p>

<blockquote>
  <p>Returns:</p>
  
  <p>A Tensor. Has the same type as tensor.</p>
</blockquote>

<p>But the type of <code>l</code> is <code>list</code> where the type of <code>r</code> is <code>tensorflow.python.framework.ops.Tensor</code>. So the types are not the same. </p>

<p>Then I thought that TensorFlow is very generous with things being a tensor. So I tried:</p>

<pre><code>class car(object):
def __init__(self, color):
    self.color = color


red_car = car('red')
#test_reshape = tf.reshape(red_car, [1, -1])
print(red_car.color) # to check, that red_car exists.
</code></pre>

<p>Now, the line in comments results in an error. </p>

<p>So, can anyone help me to find out, what qualifies as a 'Tensor'?</p>

<p>P.S.: I tried to read the source code of <code>tf.reshape</code> as given in the documentation </p>

<blockquote>
  <p>Defined in tensorflow/python/ops/gen_array_ops.py.</p>
</blockquote>

<p>But this file does not exist in the Github repo. Does anyone know how to read it?</p>
","I am new to TensorFlow and just went through the eager execution tutorial and came across the tf.decode_csv function. Not knowing about it, I read the documentation. https://www.tensorflow.org/api_docs/python/tf/decode_csv I don't really understand it. The documentation says 'records: A Tensor of type string.' So, my question is: What qualifies as a 'Tensor'? I tried the following code: So the list dec_res contains tf.tensor objects. That seems reasonable to me. But is an ordinary string also a 'Tensor' according to the documentation? Then I tried something else with the tf.reshape function. In the documentation https://www.tensorflow.org/api_docs/python/tf/reshape it says that 'tensor: A Tensor.' So, l is supposed to be a tensor. But it is not of type tf.tensor but simply a python list. This is confusing. Then the documentation says But the type of l is list where the type of r is tensorflow.python.framework.ops.Tensor. So the types are not the same. Then I thought that TensorFlow is very generous with things being a tensor. So I tried: Now, the line in comments results in an error. So, can anyone help me to find out, what qualifies as a 'Tensor'? P.S.: I tried to read the source code of tf.reshape as given in the documentation But this file does not exist in the Github repo. Does anyone know how to read it?",https://stackoverflow.com/questions/51069173,6810233,Requesting (Additional) Resources
51077930,tf.image.resize_bilinear()-when align_corners=False,"<p>I am using Tensorflow 1.4.0</p>

<p>The Tensorflow tf.image.resize_bilinear() has an argument called 'align_corners' and I am confused with the behavior when we set it to be False. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/resize_bilinear"" rel=""noreferrer"">official document</a>, it says:</p>

<blockquote>
  <p>align_corners: An optional bool. Defaults to False. If true, the centers of the 4 corner pixels of the input and output tensors are aligned, preserving the values at the corner pixels. Defaults to false.</p>
</blockquote>

<p>When I use tf.image.resize_bilinear() with align_corners=True in the following program:</p>

<pre><code>import tensorflow as tf
sess = tf.Session()
x = tf.Variable(tf.Variable([[[[1],[2]],[[3],[4]]]]))
pooling_output_size = [4, 4]
pool_output = tf.image.resize_bilinear(x, pooling_output_size,align_corners=True)
sess.run(tf.global_variables_initializer())
print pool_output.eval(session=sess)
</code></pre>

<p>it outputs</p>

<pre><code>[[[[1.       ]
   [1.3333334]
   [1.6666667]
   [2.       ]]

  [[1.6666667]
   [2.       ]
   [2.3333335]
   [2.6666667]]

  [[2.3333335]
   [2.6666665]
   [3.       ]
   [3.3333335]]

  [[3.       ]
   [3.3333333]
   [3.6666667]
   [4.       ]]]]
</code></pre>

<p>which corners are correctly aligned.</p>

<p>However when I set the align_corners=False, I got the following weird outputs</p>

<pre><code>[[[[1. ]
   [1.5]
   [2. ]
   [2. ]]

  [[2. ]
   [2.5]
   [3. ]
   [3. ]]

  [[3. ]
   [3.5]
   [4. ]
   [4. ]]

  [[3. ]
   [3.5]
   [4. ]
   [4. ]]]]
</code></pre>

<p>Is there anyone who understand why Tensorflow will use this weird implementation? I didn't find any explanation anywhere.</p>

<p>Actually PyTorch's bilinear upsampling has the align_corner argument too, when you set it to True, it works well. But if you set it to False, it performs a differnet behaviour to Tensorflow's. I am totally confused with their implementations now (maybe just use align_corners=True will be fine).</p>
","I am using Tensorflow 1.4.0 The Tensorflow tf.image.resize_bilinear() has an argument called 'align_corners' and I am confused with the behavior when we set it to be False. In the official document, it says: When I use tf.image.resize_bilinear() with align_corners=True in the following program: it outputs which corners are correctly aligned. However when I set the align_corners=False, I got the following weird outputs Is there anyone who understand why Tensorflow will use this weird implementation? I didn't find any explanation anywhere. Actually PyTorch's bilinear upsampling has the align_corner argument too, when you set it to True, it works well. But if you set it to False, it performs a differnet behaviour to Tensorflow's. I am totally confused with their implementations now (maybe just use align_corners=True will be fine).",https://stackoverflow.com/questions/51077930,9809485,Documentation Ambiguity
51248442,Behavior of the parameter 'throttle_secs' in tf.estimator.EvalSpec for use in tf.estimator.train_and_evaluate,"<p>I am using tensorflow's train_and_eval function as in the <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer"">example</a>. Therefore i create an instance of tf.estimator.EvalSpec, according to </p>

<pre><code>eval_spec = tf.estimator.EvalSpec(input_fn=...,throttle_secs=60).
</code></pre>

<p>According to its <a href=""http://eval_spec%20=%20tf.estimator.EvalSpec(input_fn=lambda:%20tf_data_utils.monuseg_input_func(mdl_info,train=False,normalize=True),throttle_secs=60*5)"" rel=""nofollow noreferrer"">documentation</a> the explanation of the parameter throttle_secs states that </p>

<p>""Of course, evaluation does not occur if no new checkpoints are available, hence, this is the minimum.""</p>

<p>However, i observe a different behavior. If there is no new checkpoint and evaluation should be triggered according to the passed parameter a new checkpoint is created and evaluation is performed. </p>

<p>Is this a bug or am i missing something here?</p>
","I am using tensorflow's train_and_eval function as in the example. Therefore i create an instance of tf.estimator.EvalSpec, according to According to its documentation the explanation of the parameter throttle_secs states that ""Of course, evaluation does not occur if no new checkpoints are available, hence, this is the minimum."" However, i observe a different behavior. If there is no new checkpoint and evaluation should be triggered according to the passed parameter a new checkpoint is created and evaluation is performed. Is this a bug or am i missing something here?",https://stackoverflow.com/questions/51248442,6456025,Documentation Replicability
51278422,Interpreting the FLOPs profile result of tensorflow,"<p>I want to profile the FLOPs of a very simple neural network model, which is used to classify the MNIST dataset, and the batch size is 128. As I followed the official tutorials, I got the result of the following model, but I cannot understand some parts of the output.</p>

<pre><code>w1 = tf.Variable(tf.random_uniform([784, 15]), name='w1')
w2 = tf.Variable(tf.random_uniform([15, 10]), name='w2')
b1 = tf.Variable(tf.zeros([15, ]), name='b1')
b2 = tf.Variable(tf.zeros([10, ]), name='b2')

hidden_layer = tf.add(tf.matmul(images_iter, w1), b1)
logits = tf.add(tf.matmul(hidden_layer, w2), b2)

loss_op = tf.reduce_sum(\
    tf.nn.softmax_cross_entropy_with_logits(logits=logits, 
                                            labels=labels_iter))
opetimizer = tf.train.AdamOptimizer(learning_rate=0.01)
train_op = opetimizer.minimize(loss_op)
</code></pre>

<p>The <code>images_iter</code> and the <code>labels_iter</code> are the iterators of tf.data, which are similar to the placeholder. </p>

<pre><code>tf.profiler.profile(
    tf.get_default_graph(),
    options=tf.profiler.ProfileOptionBuilder.float_operation())
</code></pre>

<p>I used this code, which equals to <code>scope -min_float_ops 1 -select float_ops -account_displayed_op_only</code> in tfprof comments line tool, to profile the FLOPs and got the below result.</p>

<pre><code>Profile:
node name | # float_ops
_TFProfRoot (--/23.83k flops)
  random_uniform (11.76k/23.52k flops)
    random_uniform/mul (11.76k/11.76k flops)
    random_uniform/sub (1/1 flops)
  random_uniform_1 (150/301 flops)
    random_uniform_1/mul (150/150 flops)
    random_uniform_1/sub (1/1 flops)
  Adam/mul (1/1 flops)
  Adam/mul_1 (1/1 flops)
  softmax_cross_entropy_with_logits_sg/Sub (1/1 flops)
  softmax_cross_entropy_with_logits_sg/Sub_1 (1/1 flops)
  softmax_cross_entropy_with_logits_sg/Sub_2 (1/1 flops)
</code></pre>

<p>My questions are   </p>

<ol>
<li>What do the numbers in the parentheses mean? For example, <code>random_uniform_1 (150/301 flops)</code>, what are 150 and 301?</li>
<li>Why is the first number in the parentheses of _TFProfRoot ""--""?</li>
<li>Why are the flops of Adam/mul and softmax_cross_entropy_with_logits_sg/Sub 1?</li>
</ol>

<p>I know it is discouraging to read a question so long, but a desperate boy who cannot find relating information from the official document needs your guys to help.</p>
","I want to profile the FLOPs of a very simple neural network model, which is used to classify the MNIST dataset, and the batch size is 128. As I followed the official tutorials, I got the result of the following model, but I cannot understand some parts of the output. The images_iter and the labels_iter are the iterators of tf.data, which are similar to the placeholder. I used this code, which equals to scope -min_float_ops 1 -select float_ops -account_displayed_op_only in tfprof comments line tool, to profile the FLOPs and got the below result. My questions are I know it is discouraging to read a question so long, but a desperate boy who cannot find relating information from the official document needs your guys to help.",https://stackoverflow.com/questions/51278422,6156468,Lack of Alternative Solutions/Documentation
51396366,TensorFlow with keras: Where is the ReLU layer in tf version 1.8?,"<p>Update: Found it: The class is <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation#__init__"" rel=""nofollow noreferrer"">tf.keras.layers.Activation</a>; needs to be called with argument activation='relu'....</p>

<hr>

<p>Trying to access tf.keras.layers.ReLU gives the error:</p>

<blockquote>
  <p>AttributeError: module
  'tensorflow.tools.api.generator.api.keras.layers' has no attribute
  'ReLU'.</p>
</blockquote>

<p>In the docs, version <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/keras/layers/ReLU"" rel=""nofollow noreferrer"">master</a> has such a layer. Version 1.8 (and 1.9) only seems to have leaky relu, PReLU, and other derivatives. </p>

<p>Right now I'm using <a href=""https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/keras/layers/ThresholdedReLU"" rel=""nofollow noreferrer"">ThresholdedReLU</a> with theta of 0.0, I hope this results in a standard ReLU. But there must be a simple 'ReLU' layer as well?</p>

<p>Where can I find keras' ReLU layer in tensorflow 1.8?
I want a keras layer class, i.e., not <a href=""https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/keras/backend/relu"" rel=""nofollow noreferrer"">tf.keras.backend.relu</a>.</p>

<p>It feels as if I'm overlooking something completely obvious. I haven't used keras before, so, sorry if this is a super stupid question.</p>
","Update: Found it: The class is tf.keras.layers.Activation; needs to be called with argument activation='relu'.... Trying to access tf.keras.layers.ReLU gives the error: In the docs, version master has such a layer. Version 1.8 (and 1.9) only seems to have leaky relu, PReLU, and other derivatives. Right now I'm using ThresholdedReLU with theta of 0.0, I hope this results in a standard ReLU. But there must be a simple 'ReLU' layer as well? Where can I find keras' ReLU layer in tensorflow 1.8? I want a keras layer class, i.e., not tf.keras.backend.relu. It feels as if I'm overlooking something completely obvious. I haven't used keras before, so, sorry if this is a super stupid question.",https://stackoverflow.com/questions/51396366,4726173,Documentation Replication on Other Examples
51507788,What are inputs and outputs in tf.saved_model.simple_save?,"<p>In tf.saved_model.simple_save, there are 4 params:</p>

<ul>
<li>session, the session</li>
<li>export_dir, the dir where the model will be saved</li>
<li>inputs, <strong>what it this?</strong></li>
<li>outputs, <strong>what is this?</strong></li>
</ul>

<p>I've been reading how to <a href=""https://www.tensorflow.org/guide/saved_model#simple_save"" rel=""noreferrer"">simple_save</a> but I haven't been able to figure out what to put in these two parameters (inputs and outputs). I know the model must have input values so that it can be either trained or predict. So I don't know what these two parameters should contain and wether they should map variables inside the model or what...</p>

<p>The docs aren't that great so any explaining would be much appreciated.</p>
","In tf.saved_model.simple_save, there are 4 params: I've been reading how to simple_save but I haven't been able to figure out what to put in these two parameters (inputs and outputs). I know the model must have input values so that it can be either trained or predict. So I don't know what these two parameters should contain and wether they should map variables inside the model or what... The docs aren't that great so any explaining would be much appreciated.",https://stackoverflow.com/questions/51507788,1071459,Requesting (Additional) Resources
51586693,"Tensor has shape [?, 0] -- how to reshape to [?,]","<p>When <code>src</code> has shape <code>[?]</code>, <code>tf.gather(src, tf.where(src != 0))</code> returns a tensor with shape <code>[?, 0]</code>. I'm not sure how a dimension can have size 0, and I'm especially unsure how to change the tensor back. I didn't find anything in the documentation to explain this, either.</p>

<p>I tried to <code>tf.transpose(tensor)[0]</code>, but the first dimension of the transposed tensor has size 0 and cannot be accessed! What's wrong?</p>
","When src has shape [?], tf.gather(src, tf.where(src != 0)) returns a tensor with shape [?, 0]. I'm not sure how a dimension can have size 0, and I'm especially unsure how to change the tensor back. I didn't find anything in the documentation to explain this, either. I tried to tf.transpose(tensor)[0], but the first dimension of the transposed tensor has size 0 and cannot be accessed! What's wrong?",https://stackoverflow.com/questions/51586693,6772171,Documentation Replication on Other Examples
51612489,tensorflow tf.edit_distance explanation required?,"<p>How does tensorflow <code>tf.edit_distance</code> function works?
How it compares string stored in two different sparse matrix equivalent of 2d or 3d dense matrix. </p>

<p>Example given on tensorflow web page <a href=""https://www.tensorflow.org/api_docs/python/tf/edit_distance"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/edit_distance</a> is not so obvious. Please provide explanation using some other examples. </p>

<p>Also this example is not clear.</p>

<pre><code>#'hypothesis' is a tensor of shape [2, 1] with variable-length values:
#(0,0) = [""a""] and (1,0) = [""b""]

hypothesis = tf.SparseTensor([[0, 0, 0],[1, 0, 0]],[""a"", ""b""],(2, 1, 1))

#'truth' is a tensor of shape `[2, 2]` with variable-length values:
#(0,0) = [], (0,1) = [""a""], (1,0) = [""b"", ""c""],(1,1) = [""a""]

truth = tf.SparseTensor([[0, 1, 0],[1, 0, 0],[1, 0, 1],[1, 1, 0]],[""a"", ""b"", 
""c"", ""a""],(2, 2, 2))

normalize = True

#'output' is a tensor of shape [2, 2] with edit distances normalized by 
#'truth' lengths.

output ==&gt; [[inf, 1.0],[0.5, 1.0]],

(0,0): no truth, (0,1): no hypothesis, (1,0): addition, (1,1): no hypothesis
</code></pre>

<p>How output is of dimension [2,2]?</p>

<p>What normalization is doing here?</p>
","How does tensorflow tf.edit_distance function works? How it compares string stored in two different sparse matrix equivalent of 2d or 3d dense matrix. Example given on tensorflow web page https://www.tensorflow.org/api_docs/python/tf/edit_distance is not so obvious. Please provide explanation using some other examples. Also this example is not clear. How output is of dimension [2,2]? What normalization is doing here?",https://stackoverflow.com/questions/51612489,7930290,Lack of Alternative Solutions/Documentation
51625529,How to use tf.data's initializable iterator and reinitializable interator and feed data to estimator api?,"<p>All the official google tutorials use the one shot iterator for all the estimator api implementation, i couldnt find any documentation on how to use tf.data's initializable iterator and reinitializable interator instead of one shot iterator.</p>

<p>Can someone kindly show me how to switch between train_data and test_data using tf.data's initializable iterator and reinitializable interator. We need to run a session to use feed dict and switch the dataset in the initializable iterator, its a low level api and its confusing how to use it part of estimator api architecture</p>

<p>PS : I did find that google mentions 
""Note: Currently, one-shot iterators are the only type that is easily usable with an Estimator.""</p>

<p>But is there any work around within the community? or should we just stick with one shot iterator for some good reason</p>
","All the official google tutorials use the one shot iterator for all the estimator api implementation, i couldnt find any documentation on how to use tf.data's initializable iterator and reinitializable interator instead of one shot iterator. Can someone kindly show me how to switch between train_data and test_data using tf.data's initializable iterator and reinitializable interator. We need to run a session to use feed dict and switch the dataset in the initializable iterator, its a low level api and its confusing how to use it part of estimator api architecture PS : I did find that google mentions ""Note: Currently, one-shot iterators are the only type that is easily usable with an Estimator."" But is there any work around within the community? or should we just stick with one shot iterator for some good reason",https://stackoverflow.com/questions/51625529,10163720,Lack of Alternative Solutions/Documentation
51687832,Probability Distribution for tf.nn.softmax_cross_entropy_with_logits_v2,"<p>I am trying to understand the Tensorflow documentation better for tf.nn.softmax_cross_entropy_with_logits_v2().</p>

<p>In the documentation, it states:
While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.</p>

<p>Does this mean that, for my labels, I shouldn't be simply using one-hot encoding, but should also account for the number of instances of each label? For example, if I have 2 classes, and there are 90 examples for class ""A"" and only 10 examples for class ""B"", should my label for a class A be [0.9, 0.1], instead of just [1, 0]?</p>

<p>I hope this makes sense. Thanks!</p>
","I am trying to understand the Tensorflow documentation better for tf.nn.softmax_cross_entropy_with_logits_v2(). In the documentation, it states: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect. Does this mean that, for my labels, I shouldn't be simply using one-hot encoding, but should also account for the number of instances of each label? For example, if I have 2 classes, and there are 90 examples for class ""A"" and only 10 examples for class ""B"", should my label for a class A be [0.9, 0.1], instead of just [1, 0]? I hope this makes sense. Thanks!",https://stackoverflow.com/questions/51687832,10012553,Documentation Replication on Other Examples
51691199,How does tf.create_partitioned_variables work?,"<p>I am trying to figure out how to use <a href=""https://www.tensorflow.org/api_docs/python/tf/create_partitioned_variables"" rel=""nofollow noreferrer"">tf.create_partitioned_variables</a>
I am reading the documentation but I am having a hard time understanding.</p>

<p>Could anyone explain how it works and give some examples of its usage? </p>

<p>From what I understand I can use it to get a list of slices from a variable.
I just dont understand how I get the slices</p>

<p>ex:
how would i get a list of <code>[[1.],[3.]]</code> from <code>tf.Variable(np.array([[1.0],[3.0]]), dtype=tf.float32)</code></p>

<p>or list of </p>

<pre><code>[[[1 0] [3 0]], [[0 5] [0 7]]]
</code></pre>

<p>from</p>

<pre><code>[[[1 0]
  [3 0]]

 [[0 5]
  [0 7]]]
</code></pre>
","I am trying to figure out how to use tf.create_partitioned_variables I am reading the documentation but I am having a hard time understanding. Could anyone explain how it works and give some examples of its usage? From what I understand I can use it to get a list of slices from a variable. I just dont understand how I get the slices ex: how would i get a list of [[1.],[3.]] from tf.Variable(np.array([[1.0],[3.0]]), dtype=tf.float32) or list of from",https://stackoverflow.com/questions/51691199,774972,Inadequate Examples
51706848,How does tf.reshape() work internally ?,"<p>I'm trying to understand how tf.reshape works. Let's have an example:</p>

<pre><code>embeddings = tf.placeholder(tf.float32, shape=[N0,N1])
M_2D = tf.placeholder(tf.float32, shape=[N0,None])
M_3D = tf.reshape(M_2D, [-1,N0,1])
weighted_embeddings = tf.multiply(embeddings, M_3D)
</code></pre>

<p>Here I have a 2D tensor M_2D whose columns represent coefficients for the N0 embeddings of dimension N1. I want to create a 3D tensor where each column of M_2D is placed in the first dimension of M_3D, and columns are keep in the same order. My final goal is to create a 3D tensor of 2D embeddings, each weighted by the columns of M_2D. </p>

<p>How can I be sure that reshape actually place each column in the new dimension of M_3D. Is it possible that it places the rows instead ? Is there somewhere in tensorflow documentation a clear explanation on the internal working process of tf.reshape, particularly when -1 is provided ?    </p>
","I'm trying to understand how tf.reshape works. Let's have an example: Here I have a 2D tensor M_2D whose columns represent coefficients for the N0 embeddings of dimension N1. I want to create a 3D tensor where each column of M_2D is placed in the first dimension of M_3D, and columns are keep in the same order. My final goal is to create a 3D tensor of 2D embeddings, each weighted by the columns of M_2D. How can I be sure that reshape actually place each column in the new dimension of M_3D. Is it possible that it places the rows instead ? Is there somewhere in tensorflow documentation a clear explanation on the internal working process of tf.reshape, particularly when -1 is provided ?",https://stackoverflow.com/questions/51706848,6084245,Documentation Completeness
51776390,how to use tensorboard debugger with datalab which uses tf.estimator on google cloud platform,"<p>When I start tensorboard via datalab it uses the google syntax which is described <a href=""https://googledatalab.github.io/pydatalab/google.datalab.ml.html"" rel=""nofollow noreferrer"">here</a>.  This document only mentions start, stop and list.  However, there is a debugger pane which I can not use.</p>

<p><a href=""https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/debugger"" rel=""nofollow noreferrer"">This</a> document describes how to use tensorboard debugger with a tf.estimator but it uses a different syntax.</p>

<p>Is there someway to blend the two so the debugger is usable with datalab?</p>
","When I start tensorboard via datalab it uses the google syntax which is described here. This document only mentions start, stop and list. However, there is a debugger pane which I can not use. This document describes how to use tensorboard debugger with a tf.estimator but it uses a different syntax. Is there someway to blend the two so the debugger is usable with datalab?",https://stackoverflow.com/questions/51776390,1008596,Documentation Replication on Other Examples
51824310,Difference between Keras and tensorflow implementation of LSTM with dropout,"<p>I was reviewing the documentation for the LSTM cell in tensorflow and Keras. In particular, I want to apply dropout as well. Here is what I have in Keras and would like to apply the same LSTM cell in tensorflow:</p>

<pre><code>cell = LSTM(num_units_2, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)(net)
</code></pre>

<p>Therefore, I know that I need to use <code>tf.nn.rnn_cell.LSTMCell</code> in tensorflow with <code>num_units = num_units_2</code>. Second, I need a <code>DropoutWrapper</code> as:</p>

<pre><code>cell = tf.nn.rnn_cell.DropoutWrapper(cell)
</code></pre>

<p>Now, I want to apply <code>dropout</code> and <code>recurrent_dropout</code> similar to the Keras code. Therefore, I found that tensorflow's implementation of dropout will apply a different dropout mask at every time step unless <code>variational_recurrent</code> is set to True (Yet I'm not sure how variational_recurrent works in details). </p>

<p>Additionally, I'm not sure if the LSTM in Keras apply different Mask at each time step as well. </p>

<p>Second, I was confused about the difference between the <code>output_keep_prob</code> and the <code>state_keep_prob</code> as both mention: </p>

<p><em>output_keep_prob</em>: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added...</p>

<p>Any help is much appreciated!!</p>
","I was reviewing the documentation for the LSTM cell in tensorflow and Keras. In particular, I want to apply dropout as well. Here is what I have in Keras and would like to apply the same LSTM cell in tensorflow: Therefore, I know that I need to use tf.nn.rnn_cell.LSTMCell in tensorflow with num_units = num_units_2. Second, I need a DropoutWrapper as: Now, I want to apply dropout and recurrent_dropout similar to the Keras code. Therefore, I found that tensorflow's implementation of dropout will apply a different dropout mask at every time step unless variational_recurrent is set to True (Yet I'm not sure how variational_recurrent works in details). Additionally, I'm not sure if the LSTM in Keras apply different Mask at each time step as well. Second, I was confused about the difference between the output_keep_prob and the state_keep_prob as both mention: output_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added... Any help is much appreciated!!",https://stackoverflow.com/questions/51824310,7886651,Documentation Replication on Other Examples
51856041,Input dimension for tf.nn.in_top_k,"<p>I am following the TF documentation with respect to in_top_k: <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k</a> where it states that <code>targets</code> should be a vector of <code>batch size</code></p>

<p>Nevertheless, I'm continuously prompted with the following error:</p>

<blockquote>
  <p>InvalidArgumentError (see above for traceback): targets must be
  1-dimensional
           [[Node: in_top_k/InTopKV2 = InTopKV2[T=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Cas t_2/_4305, sub/_4307, in_top_k/InTopKV2/k)]]</p>
</blockquote>

<p>In my case, my <code>predictions</code> and <code>targets</code> inputs have the following shapes:</p>

<ul>
<li>predictions: <code>Tensor(""Cast_2:0"", shape=(128, 1000), dtype=float32, device=/device:GPU:0)</code></li>
<li>targets: <code>Tensor(""sub:0"", shape=(128,), dtype=int32, device=/device:GPU:0)</code></li>
</ul>

<p>From my understanding, there is still something not ok with my <code>targets</code> label, and despite using different combinations of tf.reshape or tf.squeeze I cannot seem to find where is the error. Is there any way to work around this issue?</p>
","I am following the TF documentation with respect to in_top_k: https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k where it states that targets should be a vector of batch size Nevertheless, I'm continuously prompted with the following error: In my case, my predictions and targets inputs have the following shapes: From my understanding, there is still something not ok with my targets label, and despite using different combinations of tf.reshape or tf.squeeze I cannot seem to find where is the error. Is there any way to work around this issue?",https://stackoverflow.com/questions/51856041,10228489,Documentation Ambiguity
51858970,"tf.gradients() sums over ys, does it?","<p><a href=""https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients"" rel=""noreferrer"">https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients</a></p>

<p>In the documentation for tf.gradients(ys, xs) it states that </p>

<blockquote>
  <p>Constructs symbolic derivatives of sum of ys w.r.t. x in xs</p>
</blockquote>

<p>I am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example:</p>

<pre><code>x_dims = 3
batch_size = 4

x = tf.placeholder(tf.float32, (None, x_dims))

y = 2*(x**2)

grads = tf.gradients(y,x)

sess = tf.Session()

x_val = np.random.randint(0, 10, (batch_size, x_dims))
y_val, grads_val = sess.run([y, grads], {x:x_val})

print('x = \n', x_val)
print('y = \n', y_val)
print('dy/dx = \n', grads_val[0])
</code></pre>

<p>This gives the following output:</p>

<pre><code>x = 
 [[5 3 7]
 [2 2 5]
 [7 5 0]
 [3 7 6]]
y = 
 [[50. 18. 98.]
 [ 8.  8. 50.]
 [98. 50.  0.]
 [18. 98. 72.]]
dy/dx = 
 [[20. 12. 28.]
 [ 8.  8. 20.]
 [28. 20.  0.]
 [12. 28. 24.]]
</code></pre>

<p>This is the output I would expect, simply the derivative dy/dx for every element in the batch. I don't see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: <a href=""https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html"" rel=""noreferrer"">https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html</a>). Why is this necessary?</p>

<p>I am using Tensorflow 1.6 and Python 3.</p>
","https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients In the documentation for tf.gradients(ys, xs) it states that I am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example: This gives the following output: This is the output I would expect, simply the derivative dy/dx for every element in the batch. I don't see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html). Why is this necessary? I am using Tensorflow 1.6 and Python 3.",https://stackoverflow.com/questions/51858970,8820311,Documentation Ambiguity
51859776,lambda layer function definition without tf.keras.backend (Python Keras Package),"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda"" rel=""nofollow noreferrer"">tf.keras.layers.Lambda</a> documentation explains how a function can be defined in a lambda layer. That document provides the following function as an example,</p>

<pre><code>def antirectifier(x):

    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)

    pos = K.relu(x)
    neg = K.relu(-x)

    return K.concatenate([pos, neg], axis=1)

model.add(Lambda(antirectifier))
</code></pre>

<p>But according to that, <code>tf.keras.backend</code> must be used to conduct operations on the input Tensor object.</p>

<p>Is there any way we can use default python packages and user-defined function to define the steps of a lambda function.</p>

<p>If it's possible, please be kind enough to provide some examples.</p>
","tf.keras.layers.Lambda documentation explains how a function can be defined in a lambda layer. That document provides the following function as an example, But according to that, tf.keras.backend must be used to conduct operations on the input Tensor object. Is there any way we can use default python packages and user-defined function to define the steps of a lambda function. If it's possible, please be kind enough to provide some examples.",https://stackoverflow.com/questions/51859776,261433,Inadequate Examples
51883196,Tensorflow: tf.reverse_sequence - seq_dim and batch_dim,"<p>I am trying to learn Tensorflow and was looking at <code>tf.reverse_sequence</code>. It has two parameters <code>seq_dim</code> and <code>batch_dim</code>. From the documentation given <a href=""https://www.tensorflow.org/api_docs/python/tf/reverse_sequence"" rel=""nofollow noreferrer"">here</a> I understand that setting <code>batch_dim = 0</code> means we go through the the rows from top and setting <code>seq_dim = 1</code> means we go through columns from left to right but what do these numbers mean? I can't understand from the documentation when I should set <code>batch_dim = 1</code> or <code>2</code>. I tried <code>reverse_sequence</code> on <code>x = [[1,2,3], [4,5,6], [7,8,9]]</code> and got <code>[[3,2,1], [6,5,4], [9,8,7]]</code> with <code>batch_dim = 0</code> and <code>seq_dim = 1</code>. But I always get errors when I change <code>seq_dim</code> and <code>batch_dim</code> values from <code>1</code> and <code>0</code> respectively. Could someone explain the meaning of these values?</p>
","I am trying to learn Tensorflow and was looking at tf.reverse_sequence. It has two parameters seq_dim and batch_dim. From the documentation given here I understand that setting batch_dim = 0 means we go through the the rows from top and setting seq_dim = 1 means we go through columns from left to right but what do these numbers mean? I can't understand from the documentation when I should set batch_dim = 1 or 2. I tried reverse_sequence on x = [[1,2,3], [4,5,6], [7,8,9]] and got [[3,2,1], [6,5,4], [9,8,7]] with batch_dim = 0 and seq_dim = 1. But I always get errors when I change seq_dim and batch_dim values from 1 and 0 respectively. Could someone explain the meaning of these values?",https://stackoverflow.com/questions/51883196,7460955,Lack of Alternative Solutions/Documentation
51971050,Graph optimizations on a tensorflow serveable created using tf.Estimator,"<p><strong>Context</strong>:</p>

<p>I have a simple classifier based on <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier"" rel=""noreferrer"">tf.estimator.DNNClassifier</a> that takes text and output probabilities over an intent tags.  I am able to train an export the model to a serveable as well as serve the serveable using <a href=""https://www.tensorflow.org/serving/setup"" rel=""noreferrer"">tensorflow serving</a>.  The problem is this servable is too big (around 1GB) and so I wanted to try some <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#shrinking-file-size"" rel=""noreferrer"">tensorflow graph transforms</a> to try to reduce the size of the files being served.  </p>

<p><strong>Problem</strong>:</p>

<p>I understand how to take the <code>saved_model.pb</code> and use <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py"" rel=""noreferrer"">freeze_model.py</a> to create a new <code>.pb</code> file that can be used to call transforms on. The result of these transforms (a <code>.pb</code> file as well) is not a servable and cannot be used with tensorflow serving.</p>

<p>How can a developer go from:</p>

<pre><code>saved model -&gt; graph transforms -&gt; back to a servable
</code></pre>

<p>There's <a href=""https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models"" rel=""noreferrer"">documentation</a> that suggests that this is certainly possible, but its not at all intuitive from the docs as to how to do this.</p>

<p><strong>What I've Tried</strong>:</p>

<pre><code>import tensorflow as tf

from tensorflow.saved_model import simple_save
from tensorflow.saved_model import signature_constants
from tensorflow.saved_model import tag_constants
from tensorflow.tools.graph_transforms import TransformGraph


with tf.Session(graph=tf.Graph()) as sess_meta:
    meta_graph_def = tf.saved_model.loader.load(
        sess_meta,
        [tag_constants.SERVING],
        ""/model/path"")

    graph_def = meta_graph_def.graph_def

    other_graph_def = TransformGraph(
        graph_def,
        [""Placeholder""],
        [""dnn/head/predictions/probabilities""],
        [""quantize_weights""])


    with tf.Graph().as_default():
        graph = tf.get_default_graph()
        tf.import_graph_def(other_graph_def)
        in_tensor = graph.get_tensor_by_name(
            ""import/Placeholder:0"")
        out_tensor = graph.get_tensor_by_name(
            ""import/dnn/head/predictions/probabilities:0"")

        inputs = {""inputs"": in_tensor}
        outputs = {""outputs"": out_tensor}

        simple_save(sess_meta, ""./new"", inputs, outputs)
</code></pre>

<p>My idea was to load the servable, extract the graph_def from the meta_graph_def, transform the graph_def and then try to recreate the servable.  This seems to be the incorrect approach.</p>

<p>Is there a way to successfully perform transforms (to reduce file size at inference) on a graph from an exported servable, and then recreate a servable with the transformed graph?</p>

<p>Thanks.</p>

<p><strong>Update (2018-08-28)</strong>:</p>

<p>Found <a href=""https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/meta_graph_transform/meta_graph_transform.py"" rel=""noreferrer"">contrib.meta_graph_transform()</a> which looks promising.</p>

<p><strong>Update (2018-12-03)</strong>:</p>

<p>A related <a href=""https://github.com/tensorflow/serving/issues/1078#issuecomment-443906077"" rel=""noreferrer"">github issue</a> I opened that seems to be resolved in a detailed blog post which is listed at the end of the ticket.</p>
","Context: I have a simple classifier based on tf.estimator.DNNClassifier that takes text and output probabilities over an intent tags. I am able to train an export the model to a serveable as well as serve the serveable using tensorflow serving. The problem is this servable is too big (around 1GB) and so I wanted to try some tensorflow graph transforms to try to reduce the size of the files being served. Problem: I understand how to take the saved_model.pb and use freeze_model.py to create a new .pb file that can be used to call transforms on. The result of these transforms (a .pb file as well) is not a servable and cannot be used with tensorflow serving. How can a developer go from: There's documentation that suggests that this is certainly possible, but its not at all intuitive from the docs as to how to do this. What I've Tried: My idea was to load the servable, extract the graph_def from the meta_graph_def, transform the graph_def and then try to recreate the servable. This seems to be the incorrect approach. Is there a way to successfully perform transforms (to reduce file size at inference) on a graph from an exported servable, and then recreate a servable with the transformed graph? Thanks. Update (2018-08-28): Found contrib.meta_graph_transform() which looks promising. Update (2018-12-03): A related github issue I opened that seems to be resolved in a detailed blog post which is listed at the end of the ticket.",https://stackoverflow.com/questions/51971050,3222797,Lack of Alternative Solutions/Documentation
52046902,cudnnLSTM won't restore into a cudnnCompatibleLSTM,"<p>I'm trying to train an elementary network on a GPU machine (AWS p3x2, Volta) with TF 1.9 / 1.10. Not Keras -- TF only.</p>

<p>Based on the [rather limited] documentation my aim is to train with cudnnLSTM cell, save a checkpoint, and then restore for inference on a CPU. Per that aim, I thought that cudnnCompatibleLSTM is the way to go as it is supposed to suck in the weights from the GPU-specific LSTM implementation.</p>

<p>I get the following error, no matter what I try:</p>

<pre><code>NotFoundError (see above for traceback): Key caseTesting/testbed/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias not found in checkpoint   [[Node: caseTesting/testbed/save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT],
_device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_ caseTesting/testbed/save/Const_0_0, caseTesting/testbed/save/RestoreV2/tensor_names, caseTesting/testbed/save/RestoreV2/shape_and_slices)]]
</code></pre>

<p>Another related issue is that cudnnCompatibleLSTM and cudnnLSTM are not the same mathematically. I get different results for initialized cells. [initialized by some tf.constant() as initializer, no save/restore]. Seems that cudnnLSTM does depend on the random seed [dropout is zero], which means that there are some unique tensor/tensor initialization going on, separating it from cudnnCompatibleLSTM.</p>

<p>Does anybody have a clue?</p>
","I'm trying to train an elementary network on a GPU machine (AWS p3x2, Volta) with TF 1.9 / 1.10. Not Keras -- TF only. Based on the [rather limited] documentation my aim is to train with cudnnLSTM cell, save a checkpoint, and then restore for inference on a CPU. Per that aim, I thought that cudnnCompatibleLSTM is the way to go as it is supposed to suck in the weights from the GPU-specific LSTM implementation. I get the following error, no matter what I try: Another related issue is that cudnnCompatibleLSTM and cudnnLSTM are not the same mathematically. I get different results for initialized cells. [initialized by some tf.constant() as initializer, no save/restore]. Seems that cudnnLSTM does depend on the random seed [dropout is zero], which means that there are some unique tensor/tensor initialization going on, separating it from cudnnCompatibleLSTM. Does anybody have a clue?",https://stackoverflow.com/questions/52046902,6281444,Documentation Replicability
52073782,Computations (such as tf.greater and tf.cond) on random value tensors not working as expected,"<p>I am a tensorflow beginner. According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/greater"" rel=""nofollow noreferrer"">documentation</a>, <strong>tf.greater returns the truth value of (x>y) element-wise</strong></p>

<p>My code is as below:</p>

<pre><code>x = tf.random_uniform([])  # Empty array as shape creates a scalar.
y = tf.random_uniform([])
print('x: '+str(x.eval()))
print('y: ' +str(y.eval()))
out = tf.cond(tf.greater(x, y), lambda: x + y, lambda: x - y)
print(sess.run(tf.greater(x, y)))
print(sess.run(out))
</code></pre>

<p>The output I got is:</p>

<pre><code>x: 0.79379404
y: 0.30891895
False
0.3438499
</code></pre>

<p>x is bigger than y so it should return True and x+y should be 1.10271299
why is my expected output different than the actual output?</p>
","I am a tensorflow beginner. According to the documentation, tf.greater returns the truth value of (x&gt;y) element-wise My code is as below: The output I got is: x is bigger than y so it should return True and x+y should be 1.10271299 why is my expected output different than the actual output?",https://stackoverflow.com/questions/52073782,8496110,Documentation Replication on Other Examples
52234780,Reading from .tfrecord files using tf.data.Dataset,"<p>I want to read the dataset generated by <a href=""https://github.com/tensorflow/models/blob/master/research/slim/datasets/download_and_convert_cifar10.py"" rel=""nofollow noreferrer"">this code</a> with the <code>tf.data.Dataset</code> api. The repo shows it was written like this:</p>

<pre><code>def image_to_tfexample(image_data, image_format, height, width, class_id):
  return tf.train.Example(features=tf.train.Features(feature={
      'image/encoded': bytes_feature(image_data),
      'image/format': bytes_feature(image_format),
      'image/class/label': int64_feature(class_id),
      'image/height': int64_feature(height),
      'image/width': int64_feature(width),
  }))
</code></pre>

<p>with <code>(encoded byte-string, b'png', 32, 32, label)</code> as parameters.</p>

<p>So, to read the .tfrecord file, the data format would have to be:</p>

<pre><code>example_fmt = {
    'image/encoded': tf.FixedLenFeature((), tf.string, """"),
    'image/format': tf.FixedLenFeature((), tf.string, """"),
    'image/class/label': tf.FixedLenFeature((), tf.int64, -1),
    'image/height': tf.FixedLenFeature((), tf.int64, -1),
    'image/width': tf.FixedLenFeature((), tf.int64, -1)
}
parsed = tf.parse_single_example(example, example_fmt)
image = tf.decode_raw(parsed['image/encoded'], out_type=tf.uint8)
</code></pre>

<p>But it doesn't work. The dataset is empty after reading and generating an iterator with it raises <code>OutOfRangeError: End of sequence</code>.</p>

<p>A short python script for reproduction can be found <a href=""https://pastebin.com/zEG4GKm9"" rel=""nofollow noreferrer"">here</a>. I'm struggling to find exact documentation or examples for this problem.</p>
","I want to read the dataset generated by this code with the tf.data.Dataset api. The repo shows it was written like this: with (encoded byte-string, b'png', 32, 32, label) as parameters. So, to read the .tfrecord file, the data format would have to be: But it doesn't work. The dataset is empty after reading and generating an iterator with it raises OutOfRangeError: End of sequence. A short python script for reproduction can be found here. I'm struggling to find exact documentation or examples for this problem.",https://stackoverflow.com/questions/52234780,4443082,Documentation Replication on Other Examples
52254253,How does tf.layers.dense() interact with inputs of higher dim?,"<p>In tensorflow layers.dense(inputs, units, activation) implements a Multi-Layer Perceptron layer with arbitrary activation function. </p>

<p>Output = activation(matmul(input, weights) + bias)</p>

<p>Typically input has shape=[batch_size, input_size] and might look like this: (units = 128 and activation = tf.nn.relu are chosen arbitrarily)</p>

<pre><code>inputx = tf.placeholder(float, shape=[batch_size, input_size])
dense_layer = tf.layers.dense(inputx, 128, tf.nn.relu)
</code></pre>

<p>I have not found any documentation on what would happen, if i fed higher dimensional input, e.g. because one might have time_steps resulting in a tensor of shape=[time_step, batch_size, input_size]. What one would want here is that the layer is applied to each single input_vector for each timestep for each element of the batch. To put it a bit differently, the internal matmul of layers.dense() should simply use broadcasting in numpy style. Is the behaviour i expect here what actually happens? I.e. is: </p>

<pre><code>inputx = tf.placeholder(float, shape=[time_step, batch_size, input_size])
dense_layer = tf.layers.dense(inputx, 128, tf.nn.relu)
</code></pre>

<p>applying the dense layer to each input of size input_size for each time_step for each element in batch_size? This should then result in a tensor(in dense_layer above) of shape=[time_step, batch_size, 128]
I'm asking, as e.g. tf.matmul does not support broadcasting in the numpy style, so i'm not sure, how tensorflow handles these cases.</p>

<p>Edit: <a href=""https://stackoverflow.com/questions/46697389/reshape-3d-tensor-before-dense-layer"">This post is related, but does not finally answer my question</a></p>
","In tensorflow layers.dense(inputs, units, activation) implements a Multi-Layer Perceptron layer with arbitrary activation function. Output = activation(matmul(input, weights) + bias) Typically input has shape=[batch_size, input_size] and might look like this: (units = 128 and activation = tf.nn.relu are chosen arbitrarily) I have not found any documentation on what would happen, if i fed higher dimensional input, e.g. because one might have time_steps resulting in a tensor of shape=[time_step, batch_size, input_size]. What one would want here is that the layer is applied to each single input_vector for each timestep for each element of the batch. To put it a bit differently, the internal matmul of layers.dense() should simply use broadcasting in numpy style. Is the behaviour i expect here what actually happens? I.e. is: applying the dense layer to each input of size input_size for each time_step for each element in batch_size? This should then result in a tensor(in dense_layer above) of shape=[time_step, batch_size, 128] I'm asking, as e.g. tf.matmul does not support broadcasting in the numpy style, so i'm not sure, how tensorflow handles these cases. Edit: This post is related, but does not finally answer my question",https://stackoverflow.com/questions/52254253,6917400,Lack of Alternative Solutions/Documentation
52319765,Swap a TensorFlow Dataset input pipeline with a placeholder after training,"<p>I'm working with the new <code>tf.data.Dataset</code> API and I can't seem to figure out how to perform inference. Ultimately, I want to convert my model to a TensorRT graph and run it on the TX2, and all of the <a href=""https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/workflows/tf_to_tensorrt.html#Importing-the-UFF-Model-into-TensorRT-and-Building-an-Engine"" rel=""noreferrer"">examples I have found</a> assume you have a <code>tf.placeholder</code> for the input. Here is pseudocode for how I am training. The [...] is just meant to be a placeholder since I didn't actually run the code. Let's not debate the model, as it is just suppose to give an example:</p>

<pre><code>import tensorflow as tf

# Setup iterator
datain = tf.data.FixedLengthRecordDataset(datafiles, record_bytes1)
labels = tf.data.FixedLengthRecordDataset(labelfiles, record_bytes2)
dataset = tf.data.Dataset.zip((datain, labels))
dataset = dataset.prefetch(batch_size)
dataset = dataset.repeat(n_epoch)
iterator = dataset.make_initializable_iterator()

sess = tf.Session()
sess.run(iterator.initializer)
[batch_x, batch_y] = iterator.get_next()

# Define model function (let's not debate model except as relevant to question)
def model_fn(xin):
    x0 = tf.transpose(tf.reshape(xin, [...], name='input'))
    w = tf.Variable(tf.truncated_normal([...], stddev=0.1))
    x1 = tf.nn.conv2d(x0, w, strides=[...], padding='VALID')
    b = tf.Variable(tf.constant(0.0, shape=[...]))
    x2 = tf.nn.bias_add(x1, b)
    x3 = tf.nn.relu(x2, name='output')
    return x3

# Setup training environment
model = model_fn(batch_x)
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=batch_y))
optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)

# Train Model
while True:
    try:
        sess.run(optimizer)
    except tf.errors.OutOfRangeError:
        break

# Save model
saver = tf.train.Saver(name='saver')
saver.save(sess, 'temp/path')
</code></pre>

<p>My question is how do I get this into TensorRT without having the input be a <code>tf.placeholder</code>? All of the example I can find use a <code>tf.placeholder</code> as the input. <a href=""https://stackoverflow.com/questions/50061416/freezing-graph-in-tensorflow-when-using-tf-image-dataset/"">This example</a> suggests that I can replace the iterator with a placeholder using the <code>SavedModel</code> class, but I cannot seem to find any documentation on how to accomplish that.</p>

<p>Thanks!</p>

<p><strong>EDIT: Here is my solution thanks to the help below</strong></p>

<pre><code>from tensorflow.python.tools import optimize_for_inference_lib
import uff

# You can feed data to the IteratorGetNext node using feed_dict
input_node_name = 'iterator_scope_name/IteratorGetNext'
output_node_name = 'model_scope_name/output'

# Run inference on the trained model:
graph = tf.get_default_graph()
batch_x = graph.get_tensor_by_name(input_node_name + ':0')
networkout = graph.get_tensor_by_name(output_node_name + ':0')
testdata, testlabel = custom_data_reader_fn(data_folder)
# This will evaluate the model
label = sess.run(networkout, feed_dict={batch_x: testdata})

# Freeze model and create a UFF file:
graph_def = graph.as_graph_def() # Convert the graph to a serialized pb
frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess,
    graph_def, [output_node_name])
opt_graph_def = optimize_for_inference_lib.optimize_for_inference(
    frozen_graph_def, [input_node_name], [output_node_name],
    tf.float32.as_datatype_enum)
uff.from_tensorflow(opt_graph_def, [output_node_name], quiet=False,
    output_filename='opt_model.uff')
</code></pre>

<p>that will write out a UFF file that TensorRT can utilize. The biggest issues that I encountered was:</p>

<ol>
<li>I didn't realize that the <code>optimize_for_inference_lib.optimize_for_inference</code> operation replaced the <code>iterator</code> with a <code>tf.placeholder</code></li>
<li>I did not know what node to feed data to for evaluation: you can feed data to the <code>IteratorGetNext</code> node</li>
</ol>
","I'm working with the new tf.data.Dataset API and I can't seem to figure out how to perform inference. Ultimately, I want to convert my model to a TensorRT graph and run it on the TX2, and all of the examples I have found assume you have a tf.placeholder for the input. Here is pseudocode for how I am training. The [...] is just meant to be a placeholder since I didn't actually run the code. Let's not debate the model, as it is just suppose to give an example: My question is how do I get this into TensorRT without having the input be a tf.placeholder? All of the example I can find use a tf.placeholder as the input. This example suggests that I can replace the iterator with a placeholder using the SavedModel class, but I cannot seem to find any documentation on how to accomplish that. Thanks! EDIT: Here is my solution thanks to the help below that will write out a UFF file that TensorRT can utilize. The biggest issues that I encountered was:",https://stackoverflow.com/questions/52319765,8759276,Lack of Alternative Solutions/Documentation
52473088,How tf.Variable maintains state of the graph?,"<p>I am trying to learn tensorflow. I am really confused with the usage of tf.Variable . I know that in machine learning we have to randomly assign weights to the filter. But this can be done with tf.truncated_normal function. Then what is the role of tf.Variable here? Documentation states that tf.Variable maintains the state of graph. What does it mean? If I omit tf.Variable result is same. So what is the role of tf.Variable? Can someone please help me to understand this?  </p>

<pre><code>`def weight_variable(shape):
    initial = tf.truncated_normal(shape, mean=0, stddev=0.1)
    return tf.Variable(initial)
#function call
filter = weight_variable([1,2,2,1])`
</code></pre>
",I am trying to learn tensorflow. I am really confused with the usage of tf.Variable . I know that in machine learning we have to randomly assign weights to the filter. But this can be done with tf.truncated_normal function. Then what is the role of tf.Variable here? Documentation states that tf.Variable maintains the state of graph. What does it mean? If I omit tf.Variable result is same. So what is the role of tf.Variable? Can someone please help me to understand this?,https://stackoverflow.com/questions/52473088,10333833,Lack of Alternative Solutions/Documentation
52533156,Weight Initialization Tensorflow tf.estimator,"<p>Is there a way to adjust the weight initialization in the pre-built tf.estimator?
I would like to use the method after Xavier (<code>tf.contrib.layers.xavier_initializer</code>) or from He. Which method is used by default? I couldn't figure it out from the documentation. </p>

<p>I use the DNNRegressor.</p>
",Is there a way to adjust the weight initialization in the pre-built tf.estimator? I would like to use the method after Xavier (tf.contrib.layers.xavier_initializer) or from He. Which method is used by default? I couldn't figure it out from the documentation. I use the DNNRegressor.,https://stackoverflow.com/questions/52533156,10203191,Documentation Ambiguity
52572275,tensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)?,"<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/manip/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd documentation</a> and run the example code for 1D and 3D tensors... and now I'm trying to do it for a 2D tensor.  I want to 'interleave' the columns of two tensors.  For 1D tensors, one can do this via</p>

<pre><code>'''
We want to interleave elements of 1D tensors arr1 and arr2, where
arr1 = [10, 11, 12]
arr2 = [1, 2, 3, 4, 5, 6]
such that
desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12]
'''

import tensorflow as tf

with tf.Session() as sess:

    updates1 = tf.constant([1,2,3,4,5,6])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)

    updates2 = tf.constant([10,11,12])
    indices2 = tf.constant([[2], [5], [8]])
    scatter2 = tf.scatter_nd(indices2, updates2, shape)

    result = scatter1 + scatter2

    print(sess.run(result))
</code></pre>

<p>(aside: is there a <em>better</em> way to do this?  I'm all ears.)</p>

<p>This gives the output</p>

<p><code>[ 1  2 10  3  4 11  5  6 12]</code></p>

<p>Yay! that worked!</p>

<p>Now lets' try to extend this to 2D.</p>

<pre><code>    '''
    We want to interleave the *columns* (not rows; rows would be easy!) of

    arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]
    arr2 = [[10 11 12], [10 11 12], [10 11 12]]
    such that
    desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]]
    '''

    updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([3, 9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)
</code></pre>

<p>This gives the error
<code>ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1
dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but
are 6 and 3. Shapes are [6] and [3]. for 'ScatterNd_2' (op: 'ScatterNd') with
input shapes: [6,1], [3,6], [2].</code></p>

<p>Seems like my <code>indices</code> is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean
I need to <em>explicitly</em> specify every single pair of indices for every element in <code>updates1</code>?
Or is there some kind of 'wildcard' specification I can use for the rows? (Note <code>indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]])</code> gives syntax errors, as it probably should.)</p>

<p>Would it be easier to just do a transpose, interleave the rows, then transpose back?
Because I tried that...</p>

<pre><code>scatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))
print(sess.run(tf.transpose(scatter1)))
</code></pre>

<p>...and got a <em>much</em> longer error message, that I don't feel like posting unless someone requests it. </p>

<p>PS- I searched to make sure this isn't a duplicate -- I find it hard to imagine that someone else hasn't asked this before -- but turned up nothing. </p>
","I've read the tf.scatter_nd documentation and run the example code for 1D and 3D tensors... and now I'm trying to do it for a 2D tensor. I want to 'interleave' the columns of two tensors. For 1D tensors, one can do this via (aside: is there a better way to do this? I'm all ears.) This gives the output [ 1 2 10 3 4 11 5 6 12] Yay! that worked! Now lets' try to extend this to 2D. This gives the error ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1 dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but are 6 and 3. Shapes are [6] and [3]. for 'ScatterNd_2' (op: 'ScatterNd') with input shapes: [6,1], [3,6], [2]. Seems like my indices is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean I need to explicitly specify every single pair of indices for every element in updates1? Or is there some kind of 'wildcard' specification I can use for the rows? (Note indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]]) gives syntax errors, as it probably should.) Would it be easier to just do a transpose, interleave the rows, then transpose back? Because I tried that... ...and got a much longer error message, that I don't feel like posting unless someone requests it. PS- I searched to make sure this isn't a duplicate -- I find it hard to imagine that someone else hasn't asked this before -- but turned up nothing.",https://stackoverflow.com/questions/52572275,4259243,Documentation Replication on Other Examples
52597523,How to load_weights to a Keras model from a Tensorflow checkpoint,"<p>I have some python code to train a network using Tensorflow's TFRecords and Dataset APIs. I have built the network using tf.Keras.layers, this being arguably the easiest and fastest way. The handy function model_to_estimator()</p>

<pre><code>modelTF = tf.keras.estimator.model_to_estimator(
    keras_model=model,
    custom_objects=None,
    config=run_config,
    model_dir=checkPointDirectory
)
</code></pre>

<p>converts a Keras model to an estimator, which allows us to take advantage of the Dataset API nicely, and automatically save checkpoints to checkPointDirectory during training, and upon training completion. The estimator API presents some invaluable features, such as automatically distributing the workload over multiple GPUs, with, e.g.</p>

<pre><code>distribution = tf.contrib.distribute.MirroredStrategy()
run_config = tf.estimator.RunConfig(train_distribute=distribution)
</code></pre>

<p>Now for big models and lots of data, it is often useful to execute predictions after training using some form of saved model. It seems that as of Tensorflow 1.10 (see <a href=""https://github.com/tensorflow/tensorflow/issues/19295"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/19295</a>), a tf.keras.model object supports load_weights() from a Tensorflow checkpoint. This is mentioned briefly in the Tensorflow docs, but not the Keras docs, and I can't find anyone showing an example of this. After defining the model layers again in some new .py, I have tried </p>

<pre><code>checkPointPath = os.path.join('.', 'tfCheckPoints', 'keras_model.ckpt.index')
model.load_weights(filepath=checkPointPath, by_name=False)
</code></pre>

<p>but this gives a NotImplementedError:</p>

<pre><code>Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.

2018-10-01 14:24:49.912087:
Traceback (most recent call last):
  File ""C:/Users/User/PycharmProjects/python/mercury.classifier reductions/V3.2/wikiTestv3.2/modelEvaluation3.2.py"", line 141, in &lt;module&gt;
    model.load_weights(filepath=checkPointPath, by_name=False)
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 1526, in load_weights
    checkpointable_utils.streaming_restore(status=status, session=session)
  File ""C:\Users\User\Anaconda3\lib\site-packages\tensorflow\python\training\checkpointable\util.py"", line 880, in streaming_restore
    ""Streaming restore not supported from name-based checkpoints. File a ""
NotImplementedError: Streaming restore not supported from name-based checkpoints. File a feature request if this limitation bothers you.
</code></pre>

<p>I would like to do as suggested by the Warning and use the 'object-based saver' instead, but I haven't found a way to do this via a RunConfig passed to estimator.train(). </p>

<p>So is there a better way to get the saved weights back into an estimator for use in prediction? The github thread seems to suggest that this is already implemented (though based on the error, probably in a different way than I am attempting above). Has anyone successfully used load_weights() on a TF checkpoint? I haven't been able to find any tutorials/examples on how this can be done, so any help is appreciated. </p>
","I have some python code to train a network using Tensorflow's TFRecords and Dataset APIs. I have built the network using tf.Keras.layers, this being arguably the easiest and fastest way. The handy function model_to_estimator() converts a Keras model to an estimator, which allows us to take advantage of the Dataset API nicely, and automatically save checkpoints to checkPointDirectory during training, and upon training completion. The estimator API presents some invaluable features, such as automatically distributing the workload over multiple GPUs, with, e.g. Now for big models and lots of data, it is often useful to execute predictions after training using some form of saved model. It seems that as of Tensorflow 1.10 (see https://github.com/tensorflow/tensorflow/issues/19295), a tf.keras.model object supports load_weights() from a Tensorflow checkpoint. This is mentioned briefly in the Tensorflow docs, but not the Keras docs, and I can't find anyone showing an example of this. After defining the model layers again in some new .py, I have tried but this gives a NotImplementedError: I would like to do as suggested by the Warning and use the 'object-based saver' instead, but I haven't found a way to do this via a RunConfig passed to estimator.train(). So is there a better way to get the saved weights back into an estimator for use in prediction? The github thread seems to suggest that this is already implemented (though based on the error, probably in a different way than I am attempting above). Has anyone successfully used load_weights() on a TF checkpoint? I haven't been able to find any tutorials/examples on how this can be done, so any help is appreciated.",https://stackoverflow.com/questions/52597523,9731282,Inadequate Examples
52711895,How to run define Tensorflow graph were all variables are in float16 instead instead of float32,"<p>By default, the variables Tensorflow is in float32. To save memory, I'm trying to run in float16. In my graph, every place where I could define the datatype as float16, I did. However, I get an error when I run the code </p>

<p>Here's my code below. </p>

<pre><code>import math
import numpy as np
import tensorflow as tf

vocabulary_size = 10
batch_size = 64 
embedding_size = 100 
num_inputs =4
num_sampled = 128 

graph = tf.Graph()

with graph.as_default(): #took out "" , tf.device('/cpu:0')""


    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, num_inputs ])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])

    embeddings = tf.get_variable( 'embeddings', dtype=tf.float16,
        initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, dtype=tf.float16) )

    softmax_weights = tf.get_variable( 'softmax_weights', dtype=tf.float16,
        initializer= tf.truncated_normal([vocabulary_size, embedding_size],
                             stddev=1.0 / math.sqrt(embedding_size), dtype=tf.float16 ) )

    softmax_biases = tf.get_variable('softmax_biases', dtype=tf.float16,
        initializer= tf.zeros([vocabulary_size], dtype=tf.float16),  trainable=False )

    embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is

    embed_reshaped = tf.reshape( embed, [batch_size*num_inputs, embedding_size] )

    segments= np.arange(batch_size).repeat(num_inputs)

    averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)

    sam_sof_los = tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)

    loss = tf.reduce_mean( sam_sof_los )

    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) 

    saver = tf.train.Saver()
</code></pre>

<p>And this is this is the error message</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    509                 as_ref=input_arg.is_ref,
--&gt; 510                 preferred_dtype=default_dtype)
    511           except TypeError as err:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1143     if ret is None:
-&gt; 1144       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1145 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    980         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
--&gt; 981         (dtype.name, t.dtype.name, str(t)))
    982   return t

ValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: 'Tensor(""sampled_softmax_loss/Log:0"", shape=(64, 1), dtype=float32)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-2-12d508b9e5d7&gt; in &lt;module&gt;()
     46 
     47     sam_sof_los = tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
---&gt; 48                                    labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)
     49 
     50     loss = tf.reduce_mean( sam_sof_los )

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py in sampled_softmax_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name, seed)
   1347       partition_strategy=partition_strategy,
   1348       name=name,
-&gt; 1349       seed=seed)
   1350   labels = array_ops.stop_gradient(labels, name=""labels_stop_gradient"")
   1351   sampled_losses = nn_ops.softmax_cross_entropy_with_logits_v2(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py in _compute_sampled_logits(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name, seed)
   1126     if subtract_log_q:
   1127       # Subtract log of Q(l), prior probability that l appears in sampled.
-&gt; 1128       true_logits -= math_ops.log(true_expected_count)
   1129       sampled_logits -= math_ops.log(sampled_expected_count)
   1130 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    860     with ops.name_scope(None, op_name, [x, y]) as name:
    861       if isinstance(x, ops.Tensor) and isinstance(y, ops.Tensor):
--&gt; 862         return func(x, y, name=name)
    863       elif not isinstance(y, sparse_tensor.SparseTensor):
    864         try:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in sub(x, y, name)
   8316   if _ctx is None or not _ctx._eager_context.is_eager:
   8317     _, _, _op = _op_def_lib._apply_op_helper(
-&gt; 8318         ""Sub"", x=x, y=y, name=name)
   8319     _result = _op.outputs[:]
   8320     _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    544                   ""%s type %s of argument '%s'."" %
    545                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
--&gt; 546                    inferred_from[input_arg.type_attr]))
    547 
    548           types = [values.dtype]

TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type float16 of argument 'x'.
</code></pre>

<p>The error comes from line <code>tf.nn.sampled_softmax_loss</code>.</p>

<p>At first I thought perhaps tf.segment_mean may cast the output as a float32, so I tried casting averaged_embeds to float16 but I still get the same error. </p>

<p>From the documentation, there doesn't seem to be a way to define any data types in sampled_softmax_loss</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss</a></p>
","By default, the variables Tensorflow is in float32. To save memory, I'm trying to run in float16. In my graph, every place where I could define the datatype as float16, I did. However, I get an error when I run the code Here's my code below. And this is this is the error message The error comes from line tf.nn.sampled_softmax_loss. At first I thought perhaps tf.segment_mean may cast the output as a float32, so I tried casting averaged_embeds to float16 but I still get the same error. From the documentation, there doesn't seem to be a way to define any data types in sampled_softmax_loss https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss",https://stackoverflow.com/questions/52711895,3259896,Lack of Alternative Solutions/Documentation
52731151,Tesnorflow: How to provide your own `sampled_values` for tf.nn.sampled_softmax_loss?,"<p>In tf.nn.sampled_softmax_loss, one of the optional inputs is to put your own samples values. I would like to provide my own samples values so that I can use float16 (half precision) variables. If <code>sampled_values</code> is left blank, Tensorflow will use <code>log_uniform_candidate_sampler</code> to get values, which can only return float32. </p>

<p>Here are all the inputs. </p>

<pre><code>tf.nn.sampled_softmax_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=True,
    partition_strategy='mod',
    name='sampled_softmax_loss',
    seed=None
)
</code></pre>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss</a></p>

<p>This is the information they give for the sampled_values arg :</p>

<blockquote>
  <p>sampled_values: a tuple of (sampled_candidates, true_expected_count,
  sampled_expected_count) returned by a *_candidate_sampler function.
  (if None, we default to log_uniform_candidate_sampler)</p>
</blockquote>

<p>I'm trying to figure out how to provide this tuple. What exactly are the <code>sampled_candidates</code>, <code>true_expected_count</code>, <code>sampled_expected_count</code> ? </p>

<p>I know that it's sampling the weights and corresponding biases, so do I put them together in it's own tuple for <code>sampled_candidates</code> ? Also, am I putting the int for the place of the weight in the matrix, or am I putting the whole embedding itself?</p>

<p>I've also looked at Tensorflow's math supplimental on negative sampling but I couldn't find any information for my issue <a href=""https://www.tensorflow.org/extras/candidate_sampling.pdf"" rel=""noreferrer"">https://www.tensorflow.org/extras/candidate_sampling.pdf</a></p>

<p>In my search, I found this very similar question on a google forum</p>

<p><a href=""https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/6IDJ-XAIb9M"" rel=""noreferrer"">https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/6IDJ-XAIb9M</a></p>

<p>The Answer given is </p>

<blockquote>
  <p><code>sampled_values</code> is the tuple returned by our *candidate_sampler
  classes. These classes implement methods that sample contrastive
  labels (not observed, but used during training) according to some
  distribution Q for use in approximate training methods like
  noise-contrastive estimation (NCE) and Sampled Softmax. An example is
  the log_uniform_candidate_sampler, which samples labels according to
  the log-uniform distribution. </p>
  
  <p>You almost never need to provide these yourself. You would simply pass
  in the result of a call to a *candidate_sampler function in the tf.nn
  module (where * can be ""uniform"", ""log_uniform"", ""zipfian_binned"",
  etc), e.g.</p>
  
  <p>sampled_values = tf.nn.zipfian_binned_candidate_sampler(...)</p>
  
  <p>If you just want to get it to work, just leave it to None, and it
  would default to the log_uniform_candidate_sampler (often a good
  choice).</p>
  
  <p>If you are interested in the math behind this, see this document:
  <a href=""https://www.tensorflow.org/versions/r0.8/extras/candidate_sampling.pdf"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.8/extras/candidate_sampling.pdf</a>.</p>
  
  <p>But to answer your question: For each batch of observed labels L, and
  a candidate sampling distribution Q, the tuple consists of:</p>
  
  <ul>
  <li>the tensor with the actual sampled contrastive labels N, </li>
  <li>the tensor with the log-expected-values of the observed labels L under Q, i.e. log Q(L), and </li>
  <li>the tensor with the log-expected values of the contrastive labels under Q, i.e. log Q(N).</li>
  </ul>
  
  <p>The latter are required for the math to go through (see above
  document). So sampled_values contains(with a hopefully clear abuse of
  notation): </p>
  
  <p>sampled_values = (N, log Q(L), log Q(N)).</p>
</blockquote>

<p>However, I still don't know how to input a value. I'm not sure what the datatypes should be, and if N is the int place in the embedding matrix, or the embedding itself. Also, I'm guessing N should be a list of values itself, the size of the number of negative labels we have to sample. </p>

<p>I was wondering if I could get a example with some values. For example, for a negative sampling of 3, do I do something like this? </p>

<p>sampled_values = ([4,29, 12], [1, 1, 1], [0, 0, 0])</p>

<p>Also, the documentation says that the tuple should be "" returned by a *_candidate_sampler function""</p>

<p>Does that mean I need to provide a function that returns the tuple, instead of the tuple itself?</p>
","In tf.nn.sampled_softmax_loss, one of the optional inputs is to put your own samples values. I would like to provide my own samples values so that I can use float16 (half precision) variables. If sampled_values is left blank, Tensorflow will use log_uniform_candidate_sampler to get values, which can only return float32. Here are all the inputs. https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss This is the information they give for the sampled_values arg : I'm trying to figure out how to provide this tuple. What exactly are the sampled_candidates, true_expected_count, sampled_expected_count ? I know that it's sampling the weights and corresponding biases, so do I put them together in it's own tuple for sampled_candidates ? Also, am I putting the int for the place of the weight in the matrix, or am I putting the whole embedding itself? I've also looked at Tensorflow's math supplimental on negative sampling but I couldn't find any information for my issue https://www.tensorflow.org/extras/candidate_sampling.pdf In my search, I found this very similar question on a google forum https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/6IDJ-XAIb9M The Answer given is However, I still don't know how to input a value. I'm not sure what the datatypes should be, and if N is the int place in the embedding matrix, or the embedding itself. Also, I'm guessing N should be a list of values itself, the size of the number of negative labels we have to sample. I was wondering if I could get a example with some values. For example, for a negative sampling of 3, do I do something like this? sampled_values = ([4,29, 12], [1, 1, 1], [0, 0, 0]) Also, the documentation says that the tuple should be "" returned by a *_candidate_sampler function"" Does that mean I need to provide a function that returns the tuple, instead of the tuple itself?",https://stackoverflow.com/questions/52731151,3259896,Documentation Replication on Other Examples
52802359,Problem with tf.SparseTensor and tf.while_loop,"<p>I face a problem when I try to change the shape of <code>tf.SparseTensor</code> inside a <code>tf.while_loop</code>. Let's say I have this sparse tensor:</p>

<pre><code>indices = np.array([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5],
               [1, 0], [1, 1], [1, 3], [1, 4], [1, 5],
               [2, 1], [2, 2], [2, 3], [2, 4],
               [3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [3, 5],
               [4, 0], [4, 2], [4, 3], [4, 4], [4, 5]], dtype=np.int64)

values = np.array([7, 6, 7, 4, 5, 4,
              6, 7, 4, 3, 4,
              3, 3, 1, 1,
              1, 2, 2, 3, 3, 4,
              1, 1, 2, 3, 3], dtype=np.float64)

dense_shape = np.array([5, 6], dtype=np.int64)

tRatings = tf.SparseTensor(indices, values, dense_shape)
</code></pre>

<p>So, I want to take a slice from the first 3 rows. I know for that purpose I can use <code>tf.sparse_slice</code> but this is an example. In my real code, I gather multiple rows from the sparse Tensor which they are not serial. The code I wrote is this:</p>

<pre><code>subTensor = tf.sparse_slice(tRatings, [0, 0], [1, 6])

i = tf.constant(1)
def condition(i, sub):
    return tf.less(i, 3)

def body(i, sub):
    tempUser = tf.sparse_slice(tRatings, [i, 0], [1, 6])
    sub = tf.sparse_concat(axis = 0, sp_inputs = [sub, tempUser])
    return [tf.add(i, 1), sub]

subTensor = tf.while_loop(condition1, body1, [i, subTensor], shape_invariants=[i.get_shape(), tf.TensorShape([2])])[1] 
</code></pre>

<p>which does't work for some reason when I run it. I get this:</p>

<pre><code>ValueError: Dimensions 1 and 2 are not compatible
</code></pre>

<p>According to <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/while_loop</a> it says that:</p>

<p>The shape_invariants argument allows the caller to specify a less specific shape invariant for each loop variable, which is needed if the shape varies between iterations. The tf.Tensor.set_shape function may also be used in the body function to indicate that the output loop variable has a particular shape. The shape invariant for SparseTensor and IndexedSlices are treated specially as follows:</p>

<p>a) If a loop variable is a SparseTensor, the shape invariant must be TensorShape([r]) where r is the rank of the dense tensor represented by the sparse tensor. It means the shapes of the three tensors of the SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here is the shape of the SparseTensor.dense_shape property. It must be the shape of a vector.</p>

<p>What am I missing here?</p>
","I face a problem when I try to change the shape of tf.SparseTensor inside a tf.while_loop. Let's say I have this sparse tensor: So, I want to take a slice from the first 3 rows. I know for that purpose I can use tf.sparse_slice but this is an example. In my real code, I gather multiple rows from the sparse Tensor which they are not serial. The code I wrote is this: which does't work for some reason when I run it. I get this: According to https://www.tensorflow.org/api_docs/python/tf/while_loop it says that: The shape_invariants argument allows the caller to specify a less specific shape invariant for each loop variable, which is needed if the shape varies between iterations. The tf.Tensor.set_shape function may also be used in the body function to indicate that the output loop variable has a particular shape. The shape invariant for SparseTensor and IndexedSlices are treated specially as follows: a) If a loop variable is a SparseTensor, the shape invariant must be TensorShape([r]) where r is the rank of the dense tensor represented by the sparse tensor. It means the shapes of the three tensors of the SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here is the shape of the SparseTensor.dense_shape property. It must be the shape of a vector. What am I missing here?",https://stackoverflow.com/questions/52802359,9854132,Documentation Replication on Other Examples
52814880,Neuron freezing in Tensorflow,"<p>I need to implement <strong>neurons freezing</strong> in CNN for a deep learning research,
I tried to find any function in the Tensorflow docs, but I didn't find anything.
How can I freeze specific neuron when I implemented the layers with tf.nn.conv2d?</p>
","I need to implement neurons freezing in CNN for a deep learning research, I tried to find any function in the Tensorflow docs, but I didn't find anything. How can I freeze specific neuron when I implemented the layers with tf.nn.conv2d?",https://stackoverflow.com/questions/52814880,6194504,Documentation Replicability
52878311,How to extract rows and columns from a 3D array in Tensorflow,"<p>I wanted to do the following indexing operation on a TensorFlow tensor.
What should be the equivalent operations in TensorFlow to get <code>b</code> and <code>c</code> as output? Although <code>tf.gather_nd</code> documentation has several examples but I could not generate equivalent <code>indices</code> tensor to get these results.</p>
<pre><code>import tensorflow as tf
import numpy as np

a=np.arange(18).reshape((2,3,3))

idx=[2,0,1] #it can be any validing re-ordering index list

#These are the two numpy operations that I want to do in Tensorflow
b=a[:,idx,:]
c=a[:,:,idx] 

# TensorFlow operations

aT=tf.constant(a)
idxT=tf.constant(idx)

# what should be these two indices  
idx1T=tf.reshape(idxT, (3,1)) 
idx2T=tf.reshape(idxT, (1,1,3))

bT=tf.gather_nd(aT, idx1T ) #does not work
cT=tf.gather_nd(aT, idx2T)  #does not work

with tf.Session() as sess:
    b1,c1=sess.run([bT,cT])

print(np.allclose(b,b1))
print(np.allclose(c,c1))
</code></pre>
<p>I am not restricted to <code>tf.gather_nd</code> Any other suggestion to achieve the same operations on GPU will be helpful.</p>
<h1>Edit: I have updated the question for a typo:</h1>
<p>old statement: <code>c=a[:,idx]</code>,</p>
<p>New statement: <code>c=a[:,:,idx]</code>
What I wanted to achieve was re-ordering of columns as well.</p>
","I wanted to do the following indexing operation on a TensorFlow tensor. What should be the equivalent operations in TensorFlow to get b and c as output? Although tf.gather_nd documentation has several examples but I could not generate equivalent indices tensor to get these results. I am not restricted to tf.gather_nd Any other suggestion to achieve the same operations on GPU will be helpful. old statement: c=a[:,idx], New statement: c=a[:,:,idx] What I wanted to achieve was re-ordering of columns as well.",https://stackoverflow.com/questions/52878311,4082304,Inadequate Examples
52956268,tf.data.Dataset: Map fails to split string,"<p>I have a <code>tf.data.Dataset</code> that I've created like this:</p>

<pre><code>dataset = tf.data.Dataset.from_tensor_slices(({""reviews"": x_train}, y_train))
</code></pre>

<p>I want to split just the reviews (strings) on whitespace.  When I do this:</p>

<pre><code>dataset = dataset.map(lambda string: tf.string_split([string]))
</code></pre>

<p>Python complains, telling me:</p>

<pre><code>TypeError: &lt;lambda&gt;() takes exactly 1 argument (2 given)
</code></pre>

<p>I've looked at the docs and it's not obvious why Python thinks I've given two arguments...any ideas?</p>

<p>Thanks!</p>
","I have a tf.data.Dataset that I've created like this: I want to split just the reviews (strings) on whitespace. When I do this: Python complains, telling me: I've looked at the docs and it's not obvious why Python thinks I've given two arguments...any ideas? Thanks!",https://stackoverflow.com/questions/52956268,1316501,Documentation Replicability
52976606,Global step not incrementing with batch norm and custom estimator,"<p>I have a customer estimator that has several layers that look like the following in the model function:</p>

<pre><code>natural_layer = tf.layers.dense(inputs = natural_layer, 
                                units = units, 
                                activation = None,
                                use_bias = False,
                                kernel_regularizer = params['regularizer'],
                                name = 'pre_batch_norm_layer_' + str(i + 1))

natural_layer = tf.layers.batch_normalization(natural_layer,
                                              axis = 1,
                                              center = True,
                                              scale = True,
                                              training = (mode == tf.estimator.ModeKeys.TRAIN),
                                              name = 'batch_norm_layer_' + str(i + 1))

natural_layer = params['natural_layer_activation'](natural_layer, name = 'activation_layer_' + str(i + 1))
</code></pre>

<p>Because I'm using batch norm, the training op is set up like this:</p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    optimizer = tf.contrib.opt.MultitaskOptimizerWrapper(params['optimization_algorithm'](params['training_rate']))
    train_op = optimizer.minimize(loss, global_step = tf.train.get_global_step())
</code></pre>

<p>Where the optimizer is usually tf.train.AdamOptimizer.</p>

<p>However, when I go to train the estimator the global step never increments (so training will run forever), and I get this:</p>

<p>WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.</p>

<p>I am passing tf.train.get_global_step() to minimize, so I'm not sure why it never gets updated. My hunch is that it has something to do with the batch normalization because when I remove that or replace it with dropout, everything works fine (even when keeping the update ops lines that are required for batch normalization per the documentation).</p>

<p>Anyone know what is going on? Happy to post more code if helpful. </p>
","I have a customer estimator that has several layers that look like the following in the model function: Because I'm using batch norm, the training op is set up like this: Where the optimizer is usually tf.train.AdamOptimizer. However, when I go to train the estimator the global step never increments (so training will run forever), and I get this: WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize. I am passing tf.train.get_global_step() to minimize, so I'm not sure why it never gets updated. My hunch is that it has something to do with the batch normalization because when I remove that or replace it with dropout, everything works fine (even when keeping the update ops lines that are required for batch normalization per the documentation). Anyone know what is going on? Happy to post more code if helpful.",https://stackoverflow.com/questions/52976606,9226290,Documentation Replication on Other Examples
53032922,TensorFlow while loop with condition dependent on body,"<p>I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop()</code></a>.</p>

<p>My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by</p>

<pre><code>import numpy as np
import tensorflow as tf
IMAGE_SHAPE = [960, 720]
CROP_SHAPE = [320, 240]
max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
</code></pre>

<p>and the condition is</p>

<pre><code>cond = tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)
</code></pre>

<p>Going over the documentation and examples of <code>tf.while_loop(cond, body, loop_vars, ...)</code>, what I understand is that both <code>cond</code> and <code>body</code> should take the same arguments given in <code>loop_vars</code>.
I don't see how I can have <code>cond</code> depend on <code>img_crop</code> which would be calculated inside <code>body</code>, and isn't provided in <code>loop_vars</code>.</p>

<p>I could equivalently compute <code>cond</code> using <code>crop_begin_index</code> without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem.</p>

<p>Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use <code>tf.while_loop()</code>?</p>
","I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with tf.while_loop(). My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by and the condition is Going over the documentation and examples of tf.while_loop(cond, body, loop_vars, ...), what I understand is that both cond and body should take the same arguments given in loop_vars. I don't see how I can have cond depend on img_crop which would be calculated inside body, and isn't provided in loop_vars. I could equivalently compute cond using crop_begin_index without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem. Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use tf.while_loop()?",https://stackoverflow.com/questions/53032922,2260153,Documentation Replication on Other Examples
53079436,tensorflow Tf.cond giving unexpected output,"<p>I seem to be having a misunderstanding on how <code>tf.cond</code> works. In the tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer"">documentation</a>, it gives the following example:</p>

<pre><code>z = tf.multiply(a, b)
result = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))
</code></pre>

<p>The result of the example, if <code>x&lt;y</code> is <code>True</code> is <code>tf.add(x,z)</code> else <code>tf.square(y)</code></p>

<p>Following this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation.</p>

<p>in my example, <code>deterministic_action = 4</code>, <code>random_action = 11</code>, <code>chose_random=False</code>. The <code>stochastic_action</code> should be <code>4</code>, instead it is <code>1</code>.
Where did the value 1 come from?</p>

<pre><code>#!/usr/bin/env python3

import tensorflow as tf
import numpy as np

with tf.Graph().as_default():
    with tf.device('/cpu:0'):
        stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"")
        eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0))
        with tf.variable_scope('test_cond') as sc:
            deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4
            random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11
            chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) &lt; eps # False because eps = 0
            stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1
            #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action)


    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init, feed_dict={stochastic_ph: True})
    print (""s_ph = "", stochastic_ph)
    d_action = sess.run(deterministic_action)
    print (""det_action= "", d_action)
    r_action = sess.run(random_action)
    print (""rand_action= "", r_action)
    e = sess.run(eps)
    c_action = sess.run(chose_random)
    print (""chose_rand= "", c_action)
    s_action = sess.run(stochastic_action)
    print (""s_action= "", s_action)
    #output = sess.run(output_action)
</code></pre>

<p>here is the output:</p>

<pre><code>python random_vec.py
2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  1
</code></pre>
","I seem to be having a misunderstanding on how tf.cond works. In the tensorflow documentation, it gives the following example: The result of the example, if x&lt;y is True is tf.add(x,z) else tf.square(y) Following this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation. in my example, deterministic_action = 4, random_action = 11, chose_random=False. The stochastic_action should be 4, instead it is 1. Where did the value 1 come from? here is the output:",https://stackoverflow.com/questions/53079436,1059860,Documentation Replication on Other Examples
53167302,Does tf.keras.layers.Conv2D as first layer in model truly need input_shape?,"<p>According to <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D"" rel=""nofollow noreferrer"">the official document</a> on <code>tf.keras.layers.Conv2D</code>,</p>

<blockquote>
  <p>When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the sample axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=""channels_last"".</p>
</blockquote>

<p>but actually without input_shape it does work in both graph execution and eager execution environment.</p>

<p>In graph execution, </p>

<pre><code>import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Flatten, Dense

class CNN(tf.keras.Model):

    def __init__(self):
        super(CNN, self).__init__()
        self.conv = Conv2D(1, 3, padding='same', data_format='channels_first')
        self.flatten = Flatten()
        self.dense = Dense(1)

    def call(self, inputs):
        x = self.conv(inputs)
        x = self.flatten(x)
        return self.dense(x)


cnn = CNN()
inputs = tf.random_uniform([2, 3, 16, 16])
outputs = cnn(inputs)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    outputs = sess.run(outputs)
    print(outputs)
</code></pre>

<p>works without any error and in eager execution,</p>

<pre><code>import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Flatten, Dense
tf.enable_eager_execution()

class CNN(tf.keras.Model):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv = Conv2D(1, 3, padding='same', data_format='channels_first')
        self.flatten = Flatten()
        self.dense = Dense(1)

    def call(self, inputs):
        x = self.conv(inputs)
        x = self.flatten(x)
        return self.dense(x)


cnn = CNN()
inputs = tf.random_uniform([2, 3, 16, 16])
outputs = cnn(inputs)
print(outputs)
</code></pre>

<p>also does.</p>

<p>Q1: Does <code>tf.keras.layers.Conv2D</code> as the first layer in a model truly need to specifying <code>input_shape</code>?</p>

<p>Q2: If not, when is it needed and why is it mentioned so in the official document?</p>

<p>UPDATE1:
<a href=""https://www.tensorflow.org/tutorials/eager/custom_layers#layers_common_sets_of_useful_operations"" rel=""nofollow noreferrer"">Tutorial on tf.keras</a> says </p>

<blockquote>
  <p>The number of input dimensions is often unnecessary, as it can be inferred
  the first time the layer is used, but it can be provided if you want to 
  specify it manually, which is useful in some complex models.</p>
</blockquote>

<p>UPDATE2:
<code>git blame</code> of docstring in TensorFlow source revealed that this document is copied from Keras API (which is not TensorFlow keras API).</p>
","According to the official document on tf.keras.layers.Conv2D, but actually without input_shape it does work in both graph execution and eager execution environment. In graph execution, works without any error and in eager execution, also does. Q1: Does tf.keras.layers.Conv2D as the first layer in a model truly need to specifying input_shape? Q2: If not, when is it needed and why is it mentioned so in the official document? UPDATE1: Tutorial on tf.keras says UPDATE2: git blame of docstring in TensorFlow source revealed that this document is copied from Keras API (which is not TensorFlow keras API).",https://stackoverflow.com/questions/53167302,8384504,Documentation Replication on Other Examples
53206900,Sound way of managing multiple sessions and graphs,"<p>I'd like to manage multiple Keras models in multiple sessions. My application is constructed such that models can be live at the same time, in addition to creating, saving and loading them.</p>

<p><strong>What is the proper way of managing this situation?</strong></p>

<p>Currently one model is represented by an instance of a wrapper class. This is used in the training, saving, loading and prediction. One <code>tf.Graph</code> and <code>tf.Session</code> is created per instance, and they are used in every function requiring the actual model.</p>

<pre><code>class NN:
    def __init__(self):
        self.graph = tf.Graph()
        self.session = tf.Session(graph=self.graph)

    def predict(self, x):
        with self.graph.as_default():
            with self.session.as_default():
                return self.model.predict(x)
</code></pre>

<p>Similar functions using the <code>with</code> statements are created for compiling the network, fitting, saving (weights to .h5 and model to JSON) and loading. So whenever the model is needed, the graph and session are brought to context.</p>

<p>This resulted in a strange error (<a href=""https://stackoverflow.com/questions/53002518/poor-exit-code-when-managing-multiple-sessions"">Q</a> for further context), and I was left wondering, what is the standard way of dealing with this. I tried to release all possible resources before creating or loading a model, but it hasn't helped. This function is just a compilation of all possible routines scraped off the internet, and is purely guesswork.</p>

<pre><code>def _new_session(self):
    if self.session is not None:
        self.session.close()
    k.clear_session()
    gc.collect()
    self.graph = tf.Graph()
    self.session = tf.Session(graph=self.graph)
</code></pre>

<p>I've not found good documentation of a similar situation. So I'd very much appreciate any real insight into this.</p>

<hr>

<p>I might need to delete the old question, as it's quite all over the place. At the time of asking I had no idea what was going on. But it's there for now.</p>

<hr>

<p>Some specific questions have arisen.</p>

<ul>
<li>Loading and making predictions on a model works, compiling and fitting doesn't, although just compiling does. Do the two contexts differ in any way? Is the loaded model exactly the same?</li>
<li>At which points should a new context be created when manipulating the models? (e.g. at load, compilation, fitting, probably not with every prediction)</li>
<li>Which actions are necessary to take when releasing the resources of previous contexts? Either when a network is disposed of or when creating a new context.</li>
<li>Why exactly is the context switch needed for multiple models?</li>
<li>What are the roles of graph vs. session, given that different things are executed on the graph and session?</li>
</ul>

<h3>Updates</h3>

<ul>
<li>Compiling, fitting and saving one network works without any context trickery. Doing the same for another model in the same context works too (or at least does not produce an error).</li>
<li>In addition to above, <strong>loading the saved model and predicting works too</strong>, right after the training and for both models! Now I'm not sure whether the prediction is made correctly, but again, no error. This only begs the question I posed above even more: <em>why are the different contexts needed?</em></li>
</ul>

<hr>

<p>The underlying issue with the error has been finally (and somewhat embarassingly) <a href=""https://stackoverflow.com/questions/53002518/poor-exit-code-when-managing-multiple-sessions"">resolved</a> by updating all packages.</p>
","I'd like to manage multiple Keras models in multiple sessions. My application is constructed such that models can be live at the same time, in addition to creating, saving and loading them. What is the proper way of managing this situation? Currently one model is represented by an instance of a wrapper class. This is used in the training, saving, loading and prediction. One tf.Graph and tf.Session is created per instance, and they are used in every function requiring the actual model. Similar functions using the with statements are created for compiling the network, fitting, saving (weights to .h5 and model to JSON) and loading. So whenever the model is needed, the graph and session are brought to context. This resulted in a strange error (Q for further context), and I was left wondering, what is the standard way of dealing with this. I tried to release all possible resources before creating or loading a model, but it hasn't helped. This function is just a compilation of all possible routines scraped off the internet, and is purely guesswork. I've not found good documentation of a similar situation. So I'd very much appreciate any real insight into this. I might need to delete the old question, as it's quite all over the place. At the time of asking I had no idea what was going on. But it's there for now. Some specific questions have arisen. The underlying issue with the error has been finally (and somewhat embarassingly) resolved by updating all packages.",https://stackoverflow.com/questions/53206900,7089239,Lack of Alternative Solutions/Documentation
53272508,inception v3 using tf.data?,"<p>I'm using a bit of code that is derived from inception v3 as distributed by the Google folks, but it's now complaining that the queue runners used to read the data are deprecated (tf.train.string_input_producer in image_processing.py, and similar).  Apparently I'm supposed to switch to tf.data for this kind of stuff.</p>

<p>Unfortunately, the documentation on tf.data isn't doing much to relieve my concern that I've got too much data to fit in memory, especially given that I want to batch it in a reusable way, etc. I'm confident that the tf.data stuff <em>can</em> do this; I just don't know <em>how</em> to do it. Can anyone point me to a full example of code that uses tf.data to deal with batches of data that won't all fit in memory?  Ideally, it would simply be an updated version of the inception-v3 code, but I'd be happy to try and work with anything.  Thanks!</p>
","I'm using a bit of code that is derived from inception v3 as distributed by the Google folks, but it's now complaining that the queue runners used to read the data are deprecated (tf.train.string_input_producer in image_processing.py, and similar). Apparently I'm supposed to switch to tf.data for this kind of stuff. Unfortunately, the documentation on tf.data isn't doing much to relieve my concern that I've got too much data to fit in memory, especially given that I want to batch it in a reusable way, etc. I'm confident that the tf.data stuff can do this; I just don't know how to do it. Can anyone point me to a full example of code that uses tf.data to deal with batches of data that won't all fit in memory? Ideally, it would simply be an updated version of the inception-v3 code, but I'd be happy to try and work with anything. Thanks!",https://stackoverflow.com/questions/53272508,7274120,Lack of Alternative Solutions/Documentation
53307954,TensorFlow Custom Estimator predict throwing value error,"<p>Note: this question has an accompanying, documented <a href=""https://colab.research.google.com/drive/1GKAqEo7qSr6kMAgxPrOrudA5eLndQ6Ub"" rel=""nofollow noreferrer"">Colab</a> notebook.</p>
<p>TensorFlow's documentation can, at times, leave a lot to be desired. Some of the older docs for lower level apis seem to have been expunged, and most newer documents point towards using higher level apis such as TensorFlow's subset of <code>keras</code> or <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>estimators</code></a>. This would not be so problematic if the higher level apis did not so often rely closely on their lower levels. Case in point, <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>estimators</code></a> (especially the <code>input_fn</code> when using TensorFlow Records).</p>
<p>Over the following Stack Overflow posts:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/52035692/tensorflow-v1-10-store-images-as-byte-strings-or-per-channel"">Tensorflow v1.10: store images as byte strings or per channel?</a></li>
<li><a href=""https://stackoverflow.com/questions/52064866/tensorflow-1-10-tfrecorddataset-recovering-tfrecords"">Tensorflow 1.10 TFRecordDataset - recovering TFRecords</a></li>
<li><a href=""https://stackoverflow.com/questions/52874647/tensorflow-v1-10-why-is-an-input-serving-receiver-function-needed-when-checkpoi"">Tensorflow v1.10+ why is an input serving receiver function needed when checkpoints are made without it?</a></li>
<li><a href=""https://stackoverflow.com/questions/52641737/tensorflow-1-10-custom-estimator-early-stopping-with-train-and-evaluate"">TensorFlow 1.10+ custom estimator early stopping with train_and_evaluate</a></li>
<li><a href=""https://stackoverflow.com/questions/53226898/tensorflow-custom-estimator-stuck-when-calling-evaluate-after-training"">TensorFlow custom estimator stuck when calling evaluate after training</a></li>
</ul>
<p>and with the gracious assistance of the TensorFlow / StackOverflow community, we have moved closer to doing what the TensorFlow <a href=""https://www.tensorflow.org/guide/custom_estimators"" rel=""nofollow noreferrer"">&quot;Creating Custom Estimators&quot; guide</a> has not, demonstrating how to make an estimator one might actually use in practice (rather than toy example) e.g. one which:</p>
<ul>
<li>has a validation set for early stopping if performance worsen,</li>
<li>reads from TF Records because many datasets are larger than the TensorFlow recommend 1Gb for in memory, and</li>
<li>that saves its best version whilst training</li>
</ul>
<p>While I still have many questions regarding this (from the best way to encode data into a TF Record, to what exactly the <code>serving_input_fn</code> expects), there is one question that stands out more prominently than the rest:</p>
<p>How to predict with the custom estimator we just made?</p>
<p>Under the documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#predict"" rel=""nofollow noreferrer"">predict</a>, it states:</p>
<blockquote>
<p><code>input_fn</code>: A function that constructs the features. Prediction continues until <code>input_fn</code> raises an end-of-input exception (<code>tf.errors.OutOfRangeError</code> or <code>StopIteration</code>). See Premade Estimators for more information. The function should construct and return one of the following:</p>
<ul>
<li>A tf.data.Dataset object: Outputs of Dataset object must have same constraints as below.</li>
<li>features: A tf.Tensor or a dictionary of string feature name to Tensor. features are consumed by model_fn. They should satisfy the expectation of model_fn from inputs.</li>
<li>A tuple, in which case the first item is extracted as features.</li>
</ul>
</blockquote>
<p>(perhaps) Most likely, if one is using <code>estimator.predict</code>, they are using data in memory such as a dense tensor (because a held out test set would likely go through <code>evaluate</code>).</p>
<p>So I, in the accompanying <a href=""https://colab.research.google.com/drive/1GKAqEo7qSr6kMAgxPrOrudA5eLndQ6Ub"" rel=""nofollow noreferrer"">Colab</a>, create a single dense example, wrap it up in a <code>tf.data.Dataset</code>, and call <code>predict</code> to get a <code>ValueError</code>.</p>
<p>I would greatly appreciate it if someone could explain to me how I can:</p>
<ol>
<li>load my saved estimator</li>
<li>given a dense, in memory example, predict the output with the estimator</li>
</ol>
","Note: this question has an accompanying, documented Colab notebook. TensorFlow's documentation can, at times, leave a lot to be desired. Some of the older docs for lower level apis seem to have been expunged, and most newer documents point towards using higher level apis such as TensorFlow's subset of keras or estimators. This would not be so problematic if the higher level apis did not so often rely closely on their lower levels. Case in point, estimators (especially the input_fn when using TensorFlow Records). Over the following Stack Overflow posts: and with the gracious assistance of the TensorFlow / StackOverflow community, we have moved closer to doing what the TensorFlow ""Creating Custom Estimators"" guide has not, demonstrating how to make an estimator one might actually use in practice (rather than toy example) e.g. one which: While I still have many questions regarding this (from the best way to encode data into a TF Record, to what exactly the serving_input_fn expects), there is one question that stands out more prominently than the rest: How to predict with the custom estimator we just made? Under the documentation for predict, it states: (perhaps) Most likely, if one is using estimator.predict, they are using data in memory such as a dense tensor (because a held out test set would likely go through evaluate). So I, in the accompanying Colab, create a single dense example, wrap it up in a tf.data.Dataset, and call predict to get a ValueError. I would greatly appreciate it if someone could explain to me how I can:",https://stackoverflow.com/questions/53307954,5623899,Requesting (Additional) Resources
53466500,Can't save and load a trained CNN model for binary image classification,"<p>I have built a Binary Image classifier using Convolutional Neural Networks using TensorFlow.It is running fine, however, each time it takes too long to train from scratch. So, I want to save the trained model and load it next time. I can't seem to understand how to implement <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">these</a> guides in my program as shown in the TensorFlow documentation. 
Here's the full code: </p>

<pre><code># Python program to create
# Image Classifier using CNN

# Importing the required libraries
import cv2
import os
import numpy as np
from random import shuffle
from tqdm import tqdm
from keras.models import Sequential
'''Setting up the env'''

TRAIN_DIR = 'D:\\Project\\Final_Project\\chest_xray\\train\\'
TEST_DIR = 'D:\\Project\\Final_Project\\chest_xray\\test0\\'
check_point = 'D:\\Project\\Final_Project\\chest_xray\\chkpt\\'
IMG_SIZE = 80
LR = 1e-4

'''Setting up the model which will help with tensorflow models'''
MODEL_NAME = 'NormalVsAbnormalXRays-{}-{}.model'.format(LR, '6conv-basic')

'''Labelling the dataset'''


def label_img(img):
    word_label = img.split('.')[-3]
    # DIY One hot encoder
    if word_label == 'Nor':
        return [1, 0]
    elif word_label == 'Pne':
        return [0, 1]
    else :
        return[0, 0]

'''Creating the training data'''


def create_train_data():
    # Creating an empty list where we should the store the training data
    # after a little preprocessing of the data
    training_data = []

    # tqdm is only used for interactive loading
    # loading the training data
    for img in tqdm(os.listdir(TRAIN_DIR)):
        # labeling the images
        label = label_img(img)

        path = os.path.join(TRAIN_DIR, img)

        # loading the image from the path and then converting them into
        # greyscale for easier covnet prob
        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)

        # resizing the image for processing them in the covnet
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))

        # final step-forming the training data list with numpy array of the images
        training_data.append([np.array(img), np.array(label)])

        # shuffling of the training data to preserve the random state of our data
    shuffle(training_data)

    # saving our trained data for further uses if required
    np.save('train_data.npy', training_data)
    return training_data


'''Processing the given test data'''


# Almost same as processing the traning data but
# we dont have to label it.
def process_test_data():
    testing_data = []
    for img in tqdm(os.listdir(TEST_DIR)):
        path = os.path.join(TEST_DIR, img)
        img_num = img.split('.')[0]
        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        testing_data.append([np.array(img), img_num])

    shuffle(testing_data)
    np.save('test_data.npy', testing_data)
    return testing_data


'''Running the training and the testing in the dataset for our model'''
#train_data = create_train_data()
#test_data = process_test_data()

train_data = np.load('train_data.npy')
test_data = np.load('test_data.npy')
'''Creating the neural network using tensorflow'''
# Importing the required libraries
import tflearn
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.estimator import regression

import tensorflow as tf
model = Sequential()
tf.reset_default_graph()


saver = tf.train.import_meta_graph('D:\\Project\\Final_Project\\chest_xray\\check_point-78.meta')
convnet = input_data(shape=[None,IMG_SIZE, IMG_SIZE, 1], name='input')

convnet = conv_2d(convnet, 32, 4, activation='relu')
convnet = max_pool_2d(convnet, 2)


convnet = conv_2d(convnet, 64, 4, activation='relu')
convnet = max_pool_2d(convnet, 2)


convnet = conv_2d(convnet, 128, 4, activation='relu')
convnet = max_pool_2d(convnet, 2)

convnet = conv_2d(convnet, 64, 4, activation='relu')
convnet = max_pool_2d(convnet, 2)

convnet = conv_2d(convnet, 32, 4, activation='relu')
convnet = max_pool_2d(convnet, 2)

convnet = fully_connected(convnet, 1024, activation='relu')
convnet = dropout(convnet, 0.3)

convnet = fully_connected(convnet, 2, activation='softmax')
convnet = regression(convnet, optimizer='adam', learning_rate=LR,
                     loss='categorical_crossentropy', name='targets')

model = tflearn.DNN(convnet, tensorboard_dir='log', checkpoint_path='check_point',best_checkpoint_path= 'check_point',max_checkpoints= 5)

# Splitting the testing data and training data
train = train_data
test = train_data

'''Setting up the features and lables'''
# X-Features &amp; Y-Labels

X = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)
Y = [i[1] for i in train]
test_x = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)
test_y = [i[1] for i in test]

'''Fitting the data into our model'''
# epoch = 40 taken
model.fit({'input': X}, {'targets': Y}, n_epoch=1,
          validation_set=0.05,
          snapshot_step=500, show_metric=True, run_id=MODEL_NAME)
model.save(MODEL_NAME)

'''Testing the data'''
import matplotlib.pyplot as plt

# if you need to create the data:
# test_data = process_test_data()
# if you already have some saved:
test_data = np.load('test_data.npy')

fig = plt.figure(figsize=(80,80))

for num, data in enumerate(test_data[:1]):


    img_num = data[1]
    img_data = data[0]

    y = fig.add_subplot(1, 1, num + 1)
    orig = img_data
    data = img_data.reshape(IMG_SIZE, IMG_SIZE, 1)

    # model_out = model.predict([data])[0]
    model_out = model.predict([data])[0]

    if np.argmax(model_out) == 1:
        str_label = 'Abnormal'
    else:
        str_label = 'Normal'

    y.imshow(orig, cmap='gray')
    plt.title(str_label,fontsize=20)
    y.axes.get_xaxis().set_visible(False)
    y.axes.get_yaxis().set_visible(False)
 plt.show()
</code></pre>

<p>I have tried to use saver = <code>tf.train.import_meta_graph('D:\\Project\\Final_Project\\chest_xray\\check_point-78.meta')</code> to import the graph but I get this error</p>

<pre><code>Traceback (most recent call last):
  File ""D:/Project/Final_Project/chest_xray/Final_CNN.py"", line 104, in &lt;module&gt;
    saver = tf.train.import_meta_graph('D:\\Project\\Final_Project\\chest_xray\\check_point-78.meta')
  File ""C:\Users\waqar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 1674, in import_meta_graph
    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]
  File ""C:\Users\waqar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\training\saver.py"", line 1696, in _import_meta_graph_with_return_elements
    **kwargs))
  File ""C:\Users\waqar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\meta_graph.py"", line 852, in import_scoped_meta_graph_with_return_elements
    ops.prepend_name_scope(value, scope_to_prepend_to_names))
  File ""C:\Users\waqar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3490, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""C:\Users\waqar\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3550, in _as_graph_element_locked
    ""graph."" % repr(name))
KeyError: ""The name 'Adam' refers to an Operation not in the graph.""
</code></pre>

<p>Process finished with exit code 1</p>
","I have built a Binary Image classifier using Convolutional Neural Networks using TensorFlow.It is running fine, however, each time it takes too long to train from scratch. So, I want to save the trained model and load it next time. I can't seem to understand how to implement these guides in my program as shown in the TensorFlow documentation. Here's the full code: I have tried to use saver = tf.train.import_meta_graph('D:\\Project\\Final_Project\\chest_xray\\check_point-78.meta') to import the graph but I get this error Process finished with exit code 1",https://stackoverflow.com/questions/53466500,8023082,Documentation Replicability
53569622,Difference between tf.train.Checkpoint and tf.train.Saver,"<p>I found there are different ways to save/restore models and variables in <code>Tensorflow</code>. These ways including:</p>

<ul>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/simple_save"" rel=""nofollow noreferrer"">tf.saved_model.simple_save</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint"" rel=""nofollow noreferrer"">tf.train.Checkpoint</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/train/Saver"" rel=""nofollow noreferrer"">tf.train.Saver</a></li>
</ul>

<p>In tensorflow's documentations, I found some differences between them:</p>

<ol>
<li><code>tf.saved_model</code> is a thin wrapper around <code>tf.train.Saver</code></li>
<li><code>tf.train.Checkpoint</code> support eager execution but <code>tf.train.Saver</code> <strong>not</strong>.</li>
<li><code>tf.train.Checkpoint</code> not creating <code>.meta</code> file but still can load graph structure (here is a big question! how it can do that?)</li>
</ol>

<p>How <code>tf.train.Checkpoint</code> can load graph without <code>.meta</code> file? or more generally What is the difference between <code>tf.train.Saver</code> and <code>tf.train.Checkpoint</code>?</p>
","I found there are different ways to save/restore models and variables in Tensorflow. These ways including: In tensorflow's documentations, I found some differences between them: How tf.train.Checkpoint can load graph without .meta file? or more generally What is the difference between tf.train.Saver and tf.train.Checkpoint?",https://stackoverflow.com/questions/53569622,1462770,Documentation Ambiguity
53572533,What is the second argument of TensorFlow's tf.data.filter() that I find no documentation of?,"<p>I recently had a <code>TypeError</code> when using</p>

<pre><code>def lie_filter(line):
    return tf.equal(line['lie_id'], 2)
</code></pre>

<p>in</p>

<pre><code>dataset = (
    tf.data
    .TextLineDataset('shots.csv')
    .skip(1)
    .map(decode_line)
    .filter(lie_filter)
    .cache())
</code></pre>

<p>The exact error was <code>TypeError: lie_filter() takes 1 positional argument but 2 were given</code>.</p>

<p>Simply changing the function signature to <code>lie_filter(line, x)</code> made the error go away and the filtering appears to work as intended. However, it left me wondering what is this mysterious second argument.</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter"" rel=""nofollow noreferrer"">TensorFlow manual for tf.data.filter()</a> only specifies one argument. There are also numerous examples by TensorFlow where filtering is done as per my attempt above. Take a look at, e.g., <a href=""https://github.com/tensorflow/tensorflow/blob/6d14dcba7225d205f2e7834551f42385802aa2cf/tensorflow/examples/get_started/regression/imports85.py#L116"" rel=""nofollow noreferrer"">imports85.py</a>.</p>

<p>Printing the <code>x</code> inside <code>lie_filter</code> yields <code>Tensor(""arg12:0"", shape=(), dtype=float32)</code>.</p>

<p>What is the second argument and where can I find documentation about it?</p>

<p>Thank you!</p>
","I recently had a TypeError when using in The exact error was TypeError: lie_filter() takes 1 positional argument but 2 were given. Simply changing the function signature to lie_filter(line, x) made the error go away and the filtering appears to work as intended. However, it left me wondering what is this mysterious second argument. TensorFlow manual for tf.data.filter() only specifies one argument. There are also numerous examples by TensorFlow where filtering is done as per my attempt above. Take a look at, e.g., imports85.py. Printing the x inside lie_filter yields Tensor(""arg12:0"", shape=(), dtype=float32). What is the second argument and where can I find documentation about it? Thank you!",https://stackoverflow.com/questions/53572533,7676920,Documentation Completeness
53583456,What problem does a reinitializable iterator solve?,"<p>From the <a href=""https://www.tensorflow.org/guide/datasets#creating_an_iterator"" rel=""nofollow noreferrer"">tf.data documentation</a>:</p>

<blockquote>
  <p>A reinitializable iterator can be initialized from multiple different
  Dataset objects. For example, you might have a training input pipeline
  that uses random perturbations to the input images to improve
  generalization, and a validation input pipeline that evaluates
  predictions on unmodified data. These pipelines will typically use
  different Dataset objects that have the same structure (i.e. the same
  types and compatible shapes for each component).</p>
</blockquote>

<p>the following example was given:</p>

<pre><code># Define training and validation datasets with the same structure.
training_dataset = tf.data.Dataset.range(100).map(
    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))
validation_dataset = tf.data.Dataset.range(50)

# A reinitializable iterator is defined by its structure. We could use the
# `output_types` and `output_shapes` properties of either `training_dataset`
# or `validation_dataset` here, because they are compatible.
iterator = tf.data.Iterator.from_structure(training_dataset.output_types,
                                           training_dataset.output_shapes)
next_element = iterator.get_next()

training_init_op = iterator.make_initializer(training_dataset)
validation_init_op = iterator.make_initializer(validation_dataset)

# Run 20 epochs in which the training dataset is traversed, followed by the
# validation dataset.
for _ in range(20):
  # Initialize an iterator over the training dataset.
  sess.run(training_init_op)
  for _ in range(100):
    sess.run(next_element)

  # Initialize an iterator over the validation dataset.
  sess.run(validation_init_op)
  for _ in range(50):
    sess.run(next_element)
</code></pre>

<p>It is unclear what the benefit of this complexity is.<br>
Why not simply create 2 different iterators?</p>
",From the tf.data documentation: the following example was given: It is unclear what the benefit of this complexity is. Why not simply create 2 different iterators?,https://stackoverflow.com/questions/53583456,2341218,Documentation Ambiguity
53612973,TensorFlow Sigmoid Cross Entropy with Logits for 1D data,"<h1>Context</h1>

<p>Suppose we have some 1D data (e.g. time series), where all series have fixed length <em>l</em>:</p>

<pre><code>        # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11] index
example = [ 0,  1,  1,  0, 23, 22, 20, 14,  9,  2,  0,  0] # l = 12
</code></pre>

<p>and we want to perform semantic segmentation, with <em>n</em> classes:</p>

<pre><code>          # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]    index            
labeled = [
            [ 0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0], # class 1
            [ 0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  0], # class 2
            [ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  0,  0], # class 3
           #[                     ...                      ],
            [ 1,  1,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1], # class n
 ]
</code></pre>

<p>then the output for a single example has shape <code>[n, l]</code> (i.e. the <code>data_format</code> is not <code>""channels_last""</code>) and the batched output has shape <code>[b, n, l]</code>, where <code>b</code> is the number of examples in the batch.</p>

<p>These classes are independent, so it is my understanding that the use <em>sigmoid</em> cross entropy is applicable here as the loss rather than softmax cross entropy.</p>

<hr>

<h1>Question</h1>

<p>I have a few small related questions in regards to the expected format for and use of <code>tf.nn.sigmoid_cross_entropy_with_logits</code>:</p>

<ol>
<li><p>since the network outputs a tensor in the same shape as the batched labels, should I train the network under the assumption that it outputs logits, or take the keras approach (see keras's <code>binary_crossentropy</code>) and assume it outputs probabilities?</p></li>
<li><p>given the 1d segmentation problem, should I call <code>tf.nn.sigmoid_cross_entropy_with_logits</code> on:</p>

<ul>
<li><code>data_format='channels_first'</code> (as shown above), or</li>
<li><code>data_format='channels_last'</code>  (example.T)</li>
</ul>

<p>if I want the labels to be assigned individually per channel?</p></li>
<li><p>should the loss operation passed to the optimizer be:</p>

<ul>
<li><code>tf.nn.sigmoid_cross_entropy_with_logits(labels, logits)</code>, </li>
<li><code>tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels, logits))</code>, or</li>
<li><code>tf.losses.sigmoid_cross_entropy</code>?</li>
</ul></li>
</ol>

<hr>

<h1>Code</h1>

<p>This <a href=""https://colab.research.google.com/drive/12EiVRy8Tdt4UyvSuuBr3wcXfCmXPVFku"" rel=""nofollow noreferrer"">Colab</a>, highlights my confusion and demonstrates that the <code>data_format</code> does in fact matter..., but the documentation does not explicitly state which is expected.</p>

<h2>Dummy data</h2>

<pre><code>c = 5  # number of channels (label classes)
p = 10 # number of positions ('pixels')


# data_format = 'channels_first', shape = [classes, pixels]
# 'logits' for 2 examples
pred_1 = np.array([[random.random() for v in range(p)]for n in range(c)]).astype(float)
pred_2 = np.array([[random.random() for v in range(p)]for n in range(c)]).astype(float)

# 'ground truth' for the above 2 examples
targ_1 = np.array([[0 if random.random() &lt; 0.8 else 1 for v in range(p)]for n in range(c)]).astype(float)
targ_2 = np.array([[0 if random.random() &lt; 0.8 else 1 for v in range(p)]for n in range(c)]).astype(float)

# batched form of the above examples
preds = np.array([pred_1, pred_2])
targs = np.array([targ_1, targ_2])


# data_format = 'channels_last', shape = [pixels, classes]
t_pred_1 = pred_1.T
t_pred_2 = pred_2.T
t_targ_1 = targ_1.T
t_targ_2 = targ_2.T

t_preds = np.array([t_pred_1, t_pred_2])
t_targs = np.array([t_targ_1, t_targ_2])
</code></pre>

<h2>losses</h2>

<h3>tf.nn</h3>

<pre><code># calculate individual losses for 'channels_first'
loss_1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=targ_1, logits=pred_1)
loss_2 = tf.nn.sigmoid_cross_entropy_with_logits(labels=targ_2, logits=pred_2)
# calculate batch loss for 'channels_first'
b_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targs, logits=preds)

# calculate individual losses for 'channels_last'
t_loss_1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=t_targ_1, logits=t_pred_1)
t_loss_2 = tf.nn.sigmoid_cross_entropy_with_logits(labels=t_targ_2, logits=t_pred_2)
# calculate batch loss for 'channels_last'
t_b_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=t_targs, logits=t_preds)
# get actual tensors
with tf.Session() as sess:
  # loss for 'channels_first'
  l1   = sess.run(loss_1)
  l2   = sess.run(loss_2)
  # batch loss for 'channels_first'
  bl   = sess.run(b_loss)

  # loss for 'channels_last'
  t_l1 = sess.run(t_loss_1)
  t_l2 = sess.run(t_loss_2)

  # batch loss for 'channels_last'
  t_bl = sess.run(t_b_loss)
</code></pre>

<h3>tf.reduced_mean(tf.nn)</h3>

<pre><code># calculate individual losses for 'channels_first'
rm_loss_1 = tf.reduce_mean(loss_1)
rm_loss_2 = tf.reduce_mean(loss_2)
# calculate batch loss for 'channels_first'
rm_b_loss = tf.reduce_mean(b_loss)

# calculate individual losses for 'channels_last'
rm_t_loss_1 = tf.reduce_mean(t_loss_1)
rm_t_loss_2 = tf.reduce_mean(t_loss_2)
# calculate batch loss for 'channels_last'
rm_t_b_loss = tf.reduce_mean(t_b_loss)
# get actual tensors
with tf.Session() as sess:
  # loss for 'channels_first'
  rm_l1   = sess.run(rm_loss_1)
  rm_l2   = sess.run(rm_loss_2)
  # batch loss for 'channels_first'
  rm_bl   = sess.run(rm_b_loss)

  # loss for 'channels_last'
  rm_t_l1 = sess.run(rm_t_loss_1)
  rm_t_l2 = sess.run(rm_t_loss_2)

  # batch loss for 'channels_last'
  rm_t_bl = sess.run(rm_t_b_loss)
</code></pre>

<h3>tf.losses</h3>

<pre><code># calculate individual losses for 'channels_first'
tf_loss_1 = tf.losses.sigmoid_cross_entropy(multi_class_labels=targ_1, logits=pred_1)
tf_loss_2 = tf.losses.sigmoid_cross_entropy(multi_class_labels=targ_2, logits=pred_2)
# calculate batch loss for 'channels_first'
tf_b_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=targs, logits=preds)

# calculate individual losses for 'channels_last'
tf_t_loss_1 = tf.losses.sigmoid_cross_entropy(multi_class_labels=t_targ_1, logits=t_pred_1)
tf_t_loss_2 = tf.losses.sigmoid_cross_entropy(multi_class_labels=t_targ_2, logits=t_pred_2)
# calculate batch loss for 'channels_last'
tf_t_b_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=t_targs, logits=t_preds)
# get actual tensors
with tf.Session() as sess:
  # loss for 'channels_first'
  tf_l1   = sess.run(tf_loss_1)
  tf_l2   = sess.run(tf_loss_2)
  # batch loss for 'channels_first'
  tf_bl   = sess.run(tf_b_loss)

  # loss for 'channels_last'
  tf_t_l1 = sess.run(tf_t_loss_1)
  tf_t_l2 = sess.run(tf_t_loss_2)

  # batch loss for 'channels_last'
  tf_t_bl = sess.run(tf_t_b_loss)
</code></pre>

<h2>Test equivalency</h2>

<h3>data_format equivalency</h3>

<pre><code># loss _should_(?) be the same for 'channels_first' and 'channels_last' data_format
# test example_1
e1 = (l1 == t_l1.T).all()
# test example 2
e2 = (l2 == t_l2.T).all()

# loss calculated for each example and then batched together should be the same 
# as the loss calculated on the batched examples
ea = (np.array([l1, l2]) == bl).all()
t_ea = (np.array([t_l1, t_l2]) == t_bl).all()

# loss calculated on the batched examples for 'channels_first' should be the same
# as loss calculated on the batched examples for 'channels_last'
eb = (bl == np.transpose(t_bl, (0, 2, 1))).all()


e1, e2, ea, t_ea, eb
# (True, False, False, False, True) &lt;- changes every time, so True is happenstance
</code></pre>

<h3>equivalency between tf.reduce_mean and tf.losses</h3>

<pre><code>l_e1 = tf_l1 == rm_l1
l_e2 = tf_l2 == rm_l2
l_eb = tf_bl == rm_bl

l_t_e1 = tf_t_l1 == rm_t_l1
l_t_e2 = tf_t_l2 == rm_t_l2
l_t_eb = tf_t_bl == rm_t_bl

l_e1, l_e2, l_eb, l_t_e1, l_t_e2, l_t_eb
# (False, False, False, False, False, False)
</code></pre>
","Suppose we have some 1D data (e.g. time series), where all series have fixed length l: and we want to perform semantic segmentation, with n classes: then the output for a single example has shape [n, l] (i.e. the data_format is not ""channels_last"") and the batched output has shape [b, n, l], where b is the number of examples in the batch. These classes are independent, so it is my understanding that the use sigmoid cross entropy is applicable here as the loss rather than softmax cross entropy. I have a few small related questions in regards to the expected format for and use of tf.nn.sigmoid_cross_entropy_with_logits: This Colab, highlights my confusion and demonstrates that the data_format does in fact matter..., but the documentation does not explicitly state which is expected.",https://stackoverflow.com/questions/53612973,5623899,Lack of Alternative Solutions/Documentation
53915078,"What are b, y, x and c which get flattened and returned along with the max-pooled features in tf.nn.max_pool_with_argmax?","<p>I went through the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax"" rel=""nofollow noreferrer"">tf.nn.max_pool_with_argmax</a> where it is written</p>

<blockquote>
  <p>Performs max pooling on the input and outputs both max values and indices.</p>
  
  <p>The indices in argmax are flattened, so that a maximum value at
  position [b, y, x, c] becomes flattened index ((b * height + y) *
  width + x) * channels + c.</p>
  
  <p>The indices returned are always in [0, height) x [0, width) before
  flattening, even if padding is involved and the mathematically correct
  answer is outside (either negative or too large). This is a bug, but
  fixing it is difficult to do in a safe backwards compatible way,
  especially due to flattening.</p>
</blockquote>

<p>The variables b, y, x and c haven't been explicitly defined hence I was having issues implementing this method. Can someone please provide the same.</p>
","I went through the documentation of tf.nn.max_pool_with_argmax where it is written The variables b, y, x and c haven't been explicitly defined hence I was having issues implementing this method. Can someone please provide the same.",https://stackoverflow.com/questions/53915078,7184172,Documentation Replicability
53919290,tensorflow sparse categorical cross entropy with logits,"<p>I am a novice programmer trying to follow <a href=""https://www.tensorflow.org/tutorials/sequences/text_generation"" rel=""nofollow noreferrer"">this</a> guide.
However, I ran across an issue. The guide says to define the loss function as:</p>

<pre><code>def loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)
</code></pre>

<p>This gives me the following error:</p>

<blockquote>
  <p>sparse_categorical_crossentropy() got an unexpected keyword argument
  'from_logits'</p>
</blockquote>

<p>which I take to mean that <code>from_logits</code> is an argument not specified in the function, which is supported by the documentation, which that <code>tf.keras.losses.sparse_categorical_crossentropy()</code> has only two possible inputs. </p>

<p>Is there a way to specify that logits are being used or is that even necesarry?</p>
","I am a novice programmer trying to follow this guide. However, I ran across an issue. The guide says to define the loss function as: This gives me the following error: which I take to mean that from_logits is an argument not specified in the function, which is supported by the documentation, which that tf.keras.losses.sparse_categorical_crossentropy() has only two possible inputs. Is there a way to specify that logits are being used or is that even necesarry?",https://stackoverflow.com/questions/53919290,9185745,Documentation Replication on Other Examples
53922040,How does tf.keras.layers.Conv2DTranspose behave with stride and padding?,"<p>While a convolution layer in TensorFlow has a complete description <a href=""https://www.tensorflow.org/api_guides/python/nn#Convolution"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_guides/python/nn#Convolution</a>, transposed convolution does not have one.</p>

<p>Although tf.keras.layers.Conv2DTranspose has a reference to <a href=""https://arxiv.org/pdf/1603.07285.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1603.07285.pdf</a>, it is not complete.</p>

<p>Is there any documentation that describes how tf.keras.layers.Conv2DTranspose behaves?</p>
","While a convolution layer in TensorFlow has a complete description https://www.tensorflow.org/api_guides/python/nn#Convolution, transposed convolution does not have one. Although tf.keras.layers.Conv2DTranspose has a reference to https://arxiv.org/pdf/1603.07285.pdf, it is not complete. Is there any documentation that describes how tf.keras.layers.Conv2DTranspose behaves?",https://stackoverflow.com/questions/53922040,8384504,Documentation Completeness
53924692,Why can tf.random.truncated_normal get a shape that is not a vector even though it says it only receives shape of a vector?,"<p>I am working with TensorFlow in Python. </p>

<p>I read through the documentation of 
<a href=""https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal"" rel=""nofollow noreferrer"">tf.random.truncated_normal</a>
that the input 'shape' gets 1-D tensor or python array, i.e. a vector (according to <a href=""https://www.tensorflow.org/guide/tensors"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/tensors</a>). </p>

<p>However, with the example I'm using, 'shape' is a 4-D tensor. Or is it considered a vector? Perhaps I have problem with the definition of vectors and tensors? </p>

<pre><code>def weight_variable(shape, name = 'noname'):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial, name = name)

W_conv1 = weight_variable([5, 5, 3, 32], 'W_conv1')
</code></pre>
","I am working with TensorFlow in Python. I read through the documentation of tf.random.truncated_normal that the input 'shape' gets 1-D tensor or python array, i.e. a vector (according to https://www.tensorflow.org/guide/tensors). However, with the example I'm using, 'shape' is a 4-D tensor. Or is it considered a vector? Perhaps I have problem with the definition of vectors and tensors?",https://stackoverflow.com/questions/53924692,10832672,Documentation Replicability
54183967,Using tf.map_fn with multiple GPUs,"<p>I'm trying to extend my single-GPU TensorFlow code to multi-GPU. I have to work on 3 degrees of freedom and unfortunately I need to use tf.map_fn to parallelize over the 3rd one. I tried to use device placement as shown in the official documentation, but it looks like it is impossible to do it with <code>tf.map_fn</code>. Is there a way to run <code>tf.map_fn</code> on multiple GPUs?</p>

<p>Here the error output:</p>

<pre><code>InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'map_1/TensorArray_1': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/device:GPU:1'
Colocation Debug Info:
Colocation group had the following types and devices: 
TensorArrayGatherV3: GPU CPU 
Range: GPU CPU 
TensorArrayWriteV3: GPU CPU 
TensorArraySizeV3: GPU CPU 
MatMul: GPU CPU 
Enter: GPU CPU 
TensorArrayV3: GPU CPU 
Const: GPU CPU 

Colocation members and user-requested devices:
  map_1/TensorArrayStack/range/delta (Const) 
  map_1/TensorArrayStack/range/start (Const) 
  map_1/TensorArray_1 (TensorArrayV3) 
  map_1/while/TensorArrayWrite/TensorArrayWriteV3/Enter (Enter) /device:GPU:1
  map_1/TensorArrayStack/TensorArraySizeV3 (TensorArraySizeV3) 
  map_1/TensorArrayStack/range (Range) 
  map_1/TensorArrayStack/TensorArrayGatherV3 (TensorArrayGatherV3) 
  map_1/while/MatMul (MatMul) /device:GPU:1
  map_1/while/TensorArrayWrite/TensorArrayWriteV3 (TensorArrayWriteV3) /device:GPU:1

         [[Node: map_1/TensorArray_1 = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=&lt;unknown&gt;, identical_element_shapes=true, tensor_array_name=""""](map_1/TensorArray_1/size)]]
</code></pre>

<p>Here a simple code example to reproduce it:</p>

<pre><code>import tensorflow as tf
import numpy

rc = 1000

sess = tf.Session()

for deviceName in ['/cpu:0', '/device:GPU:0', '/device:GPU:1']:
        with tf.device(deviceName):
                matrices = tf.random_uniform([rc,rc,4],minval = 0, maxval = 1, dtype = tf.float32)

                def mult(i):
                        product = tf.matmul(matrices[:,:,i],matrices[:,:,i+1])
                        return product

                mul = tf.zeros([rc,rc,3], dtype = tf.float32)
                mul = tf.map_fn(mult, numpy.array([0,1,2]), dtype = tf.float32, parallel_iterations = 10)

m = sess.run(mul)


</code></pre>
","I'm trying to extend my single-GPU TensorFlow code to multi-GPU. I have to work on 3 degrees of freedom and unfortunately I need to use tf.map_fn to parallelize over the 3rd one. I tried to use device placement as shown in the official documentation, but it looks like it is impossible to do it with tf.map_fn. Is there a way to run tf.map_fn on multiple GPUs? Here the error output: Here a simple code example to reproduce it:",https://stackoverflow.com/questions/54183967,6044435,Documentation Replicability
54524992,Tensorflow serving trained model saved with saved_model,"<p>I find tf.saved_model documentation not clear, is there any valuable resources how to read trained model within other session?</p>
","I find tf.saved_model documentation not clear, is there any valuable resources how to read trained model within other session?",https://stackoverflow.com/questions/54524992,9914396,Documentation Replication on Other Examples
54686895,Tensorflow dilation behave differently than morphological dilation,"<p>As the following piece of code shows, the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d"" rel=""noreferrer"">tensorflow <code>tf.nn.dilation2D</code> function</a> doesn't behave as a <a href=""https://homepages.inf.ed.ac.uk/rbf/HIPR2/dilate.htm"" rel=""noreferrer"">conventional dilation operator</a>. </p>

<pre><code>import tensorflow as tf
tf.InteractiveSession()
A = [[0, 0, 0, 0, 0, 0, 0],
     [0, 0, 0, 0, 1, 0, 0],
     [0, 0, 0, 1, 1, 1, 0],
     [0, 0, 0, 0, 1, 0, 0],
     [0, 0, 0, 0, 0, 0, 0],
     [0, 0, 0, 0, 0, 0, 0]]
kernel = tf.ones((3,3,1))
input4D = tf.cast(tf.expand_dims(tf.expand_dims(A, -1), 0), tf.float32)
output4D = tf.nn.dilation2d(input4D, filter=kernel, strides=(1,1,1,1), rates=(1,1,1,1), padding=""SAME"")
print(tf.cast(output4D[0,:,:,0], tf.int32).eval())
</code></pre>

<p>Returns the following tensor:</p>

<pre><code>array([[1, 1, 1, 2, 2, 2, 1],
       [1, 1, 2, 2, 2, 2, 2],
       [1, 1, 2, 2, 2, 2, 2],
       [1, 1, 2, 2, 2, 2, 2],
       [1, 1, 1, 2, 2, 2, 1],
       [1, 1, 1, 1, 1, 1, 1]], dtype=int32)
</code></pre>

<p>I don't understand neither <strong>why</strong> it behaves like that, neither <strong>how</strong> I should use <code>tf.nn.dilation2d</code> to retrieve the expected output:</p>

<pre><code>array([[0, 0, 0, 1, 1, 1, 0],
       [0, 0, 1, 1, 1, 1, 1],
       [0, 0, 1, 1, 1, 1, 1],
       [0, 0, 1, 1, 1, 1, 1],
       [0, 0, 0, 1, 1, 1, 0],
       [0, 0, 0, 0, 0, 0, 0]], dtype=int32)
</code></pre>

<p>Can someone enlighten the succinct documentation of tensorflow and give an explanation of what the the <code>tf.nn.dilation2D</code> function does ?</p>
","As the following piece of code shows, the tensorflow tf.nn.dilation2D function doesn't behave as a conventional dilation operator. Returns the following tensor: I don't understand neither why it behaves like that, neither how I should use tf.nn.dilation2d to retrieve the expected output: Can someone enlighten the succinct documentation of tensorflow and give an explanation of what the the tf.nn.dilation2D function does ?",https://stackoverflow.com/questions/54686895,1782553,Documentation Ambiguity
54897832,Feeding large numpy arrays into TensorFlow estimators via tf.data.Dataset,"<p>TensorFlow's <a href=""https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code> documentation on consuming numpy arrays</a> states that in order to use numpy arrays in combination with the <code>Dataset</code> API, the arrays have to be small enough (&lt;2 GB in total) to be used as tensors, or they can be fed into the dataset via placeholders.</p>

<p>However, if you use <code>Dataset</code> in conjunction with estimators (where placeholders are not available), the documentation does not provide a solution on working with large arrays without placeholders. </p>

<p>Are there other options for passing placeholder values into estimators that can be used or is the solution to provide the data in <code>tfrecord</code> or <code>csv</code> format?</p>
","TensorFlow's tf.data.Dataset documentation on consuming numpy arrays states that in order to use numpy arrays in combination with the Dataset API, the arrays have to be small enough (&lt;2 GB in total) to be used as tensors, or they can be fed into the dataset via placeholders. However, if you use Dataset in conjunction with estimators (where placeholders are not available), the documentation does not provide a solution on working with large arrays without placeholders. Are there other options for passing placeholder values into estimators that can be used or is the solution to provide the data in tfrecord or csv format?",https://stackoverflow.com/questions/54897832,4443082,Inadequate Examples
54934603,"tensorflow documentation says ""WARNING: Avoid writing code which relies on the value of a Variable..."" what does it mean?","<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/Variable"" rel=""nofollow noreferrer"">tf.Variable documentation</a> 
contains the following warning:</p>

<blockquote>
  <p>WARNING: tf.Variable objects by default have a non-intuitive memory model. A Variable is represented internally as a mutable Tensor which can non-deterministically alias other Tensors in a graph. The set of operations which consume a Variable and can lead to aliasing is undetermined and can change across TensorFlow versions. Avoid writing code which relies on the value of a Variable either changing or not changing as other operations happen. For example, using Variable objects or simple functions thereof as predicates in a tf.cond is dangerous and error-prone:</p>
</blockquote>

<pre><code>v = tf.Variable(True)
tf.cond(v, lambda: v.assign(False), my_false_fn)  # Note: this is broken.
</code></pre>

<p>I don't quite understand what time means and why the example above is broken. What does it mean that one cannot rely on the value of a Variable? Is it possible to have an example where the code above works not as expected?</p>
",The tf.Variable documentation contains the following warning: I don't quite understand what time means and why the example above is broken. What does it mean that one cannot rely on the value of a Variable? Is it possible to have an example where the code above works not as expected?,https://stackoverflow.com/questions/54934603,1754568,Requesting (Additional) Resources
54945641,"keras, model still setting expected input shape from training despite input_shape(None, ...)","<p>I have a simple CNN model written in the tf.keras framework, which I wish to use with variable input size.</p>

<p>According to <a href=""https://github.com/keras-team/keras/issues/1920#issuecomment-193883690"" rel=""nofollow noreferrer"">this</a> ""documentation"" I can use variable input size by setting <code>input_shape=(None, None, n_channels)</code>, and I have used a <code>GlobalMaxPooling2D</code> layer before my dense layer to standardize the input to the dense layer.</p>

<p>Yet when I train the model with one size of image and try to predict on a different size I get the error:</p>

<pre><code>  File ""multilabel_384.py"", line 180, in main
probabilities = model.predict(test_data)
File ""/usr/local/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1471, in predict
x, check_steps=True, steps_name='steps', steps=steps)
File ""/usr/local/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 868, in _standardize_user_data
exception_prefix='input')
File ""/usr/local/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 191, in standardize_input_data
' but got array with shape ' + str(data_shape))
ValueError: Error when checking input: expected sequential_input to have shape (16, 24, 1) but got array with shape (32, 48, 1)
</code></pre>

<p>This is the code used to define my model:</p>

<pre><code>from tensorflow.keras import layers
import tensorflow as tf

def make_model(num_classes=8):
    # type (int) -&gt; tf.keras.model
    """"""implementation of SimpleNet in keras""""""
    model = tf.keras.Sequential()
    # conv layers
    model.add(layers.ZeroPadding2D(2))
    model.add(layers.Conv2D(input_shape=(None, None, 1),
                            filters=32, kernel_size=5, activation=""relu""))
    model.add(layers.BatchNormalization())
    model.add(layers.ZeroPadding2D(2))
    model.add(layers.Conv2D(filters=64,  kernel_size=5, activation=""relu""))
    model.add(layers.Conv2D(filters=128, kernel_size=3, activation=""relu""))
    model.add(layers.Conv2D(filters=256, kernel_size=3, activation=""relu""))
    model.add(layers.Conv2D(filters=128, kernel_size=3, activation=""relu""))
    model.add(layers.GlobalMaxPooling2D())
    # dense layers
    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation=""relu""))
    model.add(layers.Dropout(0.25))
    model.add(layers.Dense(256, activation=""relu""))
    model.add(layers.Dropout(0.25))
    # use sigmoid for multiclass problems
    model.add(layers.Dense(num_classes, activation=""sigmoid""))
    return model
</code></pre>

<p>So in essence my question is why is keras <em>still</em> defining an expected input shape, and is there any way to disable this implicit <code>standardize_input_data</code> that's going on?</p>
","I have a simple CNN model written in the tf.keras framework, which I wish to use with variable input size. According to this ""documentation"" I can use variable input size by setting input_shape=(None, None, n_channels), and I have used a GlobalMaxPooling2D layer before my dense layer to standardize the input to the dense layer. Yet when I train the model with one size of image and try to predict on a different size I get the error: This is the code used to define my model: So in essence my question is why is keras still defining an expected input shape, and is there any way to disable this implicit standardize_input_data that's going on?",https://stackoverflow.com/questions/54945641,5445177,Documentation Replicability
54966581,tf.boolean_mask not accepting the axis argument,"<p>Here is my code:  </p>

<pre><code> 44     scores = tf.boolean_mask(box_class_scores,filtering_mask,axis=-1)
 45     boxes = tf.boolean_mask(boxes,filtering_mask,axis=-1)
 46     classes = tf.boolean_mask(box_classes,filtering_mask,axis=-1)
</code></pre>

<p>Error, I'm getting:</p>

<blockquote>
  <p>TypeError: boolean_mask() got an unexpected keyword argument 'axis'</p>
</blockquote>

<p>The <code>tf.boolean_mask()</code> is not accepting the axis argument but is a valid argument as can be seen in the documentation: <a href=""https://www.tensorflow.org/api_docs/python/tf/boolean_mask"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/boolean_mask</a></p>

<p><a href=""https://i.stack.imgur.com/bne6B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bne6B.png"" alt=""enter image description here""></a></p>
","Here is my code: Error, I'm getting: The tf.boolean_mask() is not accepting the axis argument but is a valid argument as can be seen in the documentation: https://www.tensorflow.org/api_docs/python/tf/boolean_mask",https://stackoverflow.com/questions/54966581,9902786,Documentation Ambiguity
54985037,How to convert TF Tensor holding value into Tensor holding categorical values,"<p>I'm paring TFRecords which provide me a label as numerical value. But I need to convert this value into categorical vector while I'm reading proto records. How can I do that. Here is code snippet for reading the proto records:</p>

<pre><code> def parse(example_proto):
     features={'label':: tf.FixedLenFeature([], tf.int64), ...}
     parsed_features = tf.parse_single_example(example_proto, features)
     label = tf.cast(parsed_features['label'], tf.int32)
     # at this point label is a Tensor which holds numerical value
     # but I need to return a Tensor which holds categorical vector
     # for instance, if my label is 1 and I have two classes
     # I need to return a vector [1,0] which represents categorical values
</code></pre>

<p>I know that there is <code>tf.keras.utils.to_categorical</code> function but it does not take a Tensor as an input.</p>
",I'm paring TFRecords which provide me a label as numerical value. But I need to convert this value into categorical vector while I'm reading proto records. How can I do that. Here is code snippet for reading the proto records: I know that there is tf.keras.utils.to_categorical function but it does not take a Tensor as an input.,https://stackoverflow.com/questions/54985037,779856,Documentation Replication on Other Examples
54989442,"RNN in Tensorflow vs Keras, depreciation of tf.nn.dynamic_rnn()","<p>My question is: Are the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""noreferrer""><code>tf.nn.dynamic_rnn</code></a> and <code>keras.layers.RNN(cell)</code> truly identical as stated in docs? </p>

<p>I am planning on building an RNN, however, it seems that <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""noreferrer""><code>tf.nn.dynamic_rnn</code></a> is depricated in favour of Keras.</p>

<p>In particular, it states that:</p>

<blockquote>
  <p>Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future
  version. Instructions for updating: Please use keras.layers.RNN(cell),
  which is equivalent to this API</p>
</blockquote>

<p>But I don't see how the APIs are equivalent, in the case of variable sequence lengths!</p>

<p>In raw TF, we can specify a tensor of shape <code>(batch_size, seq_lengths)</code>. This way, if our sequence is <code>[0, 1, 2, 3, 4]</code> and the longest sequence in the batch is of size 10, we can pad it with 0s and <code>[0, 1, 2, 3, 4, 0, 0, 0, 0, 0]</code>, we can say <code>seq_length=5</code> to process <code>[0, 1, 2, 3, 4]</code>.</p>

<p>However, in Keras, this is not how it works! What we can do, is specify the <code>mask_zero=True</code> in previous Layers, e.g. the Embedding Layer. This will also mask the 1st zero!</p>

<p>I can go around it by adding ones to the whole vector, but then thats extra preprocessing that I need to do after processing using <code>tft.compute_vocabulary()</code>, which maps vocabulary words to 0 indexed vector.</p>
","My question is: Are the tf.nn.dynamic_rnn and keras.layers.RNN(cell) truly identical as stated in docs? I am planning on building an RNN, however, it seems that tf.nn.dynamic_rnn is depricated in favour of Keras. In particular, it states that: But I don't see how the APIs are equivalent, in the case of variable sequence lengths! In raw TF, we can specify a tensor of shape (batch_size, seq_lengths). This way, if our sequence is [0, 1, 2, 3, 4] and the longest sequence in the batch is of size 10, we can pad it with 0s and [0, 1, 2, 3, 4, 0, 0, 0, 0, 0], we can say seq_length=5 to process [0, 1, 2, 3, 4]. However, in Keras, this is not how it works! What we can do, is specify the mask_zero=True in previous Layers, e.g. the Embedding Layer. This will also mask the 1st zero! I can go around it by adding ones to the whole vector, but then thats extra preprocessing that I need to do after processing using tft.compute_vocabulary(), which maps vocabulary words to 0 indexed vector.",https://stackoverflow.com/questions/54989442,4729764,Documentation Replication on Other Examples
55005915,How to resize image to put into tf.train.Example,"<p>I have an image (JPEG or PNG) as a byte buffer (read from the internet), and this is the way I was putting it in a <code>tf.train.Example</code> before:</p>

<pre><code>record = tf.train.Example(features=tf.train.Features(feature={
    'image/encoded': dataset_util.bytes_feature(image_bytes)
    # there are more features but they're not relevant
}))
</code></pre>

<p>However, for my usecase, the images are too big, so I'd like to resize them either before I put them in the <code>tf.train.Example</code> or just after (whichever is easiest).</p>

<p>Here's what I'm trying:</p>

<pre><code># predeclared
# - image_bytes
# - image_format
# - height
# - width

# resize image
if image_format == b'jpeg':
    image = tf.image.decode_jpeg(image_bytes, None, tf.float32)
elif image_format == b'png':
    image = tf.image.decode_png(image_bytes, None, tf.float32)

image = tf.image.resize_images(image, (int(height), int(width)))

image = tf.image.convert_image_dtype(image, tf.uint8)
record = tf.train.Example(features=tf.train.Features(feature={
    'image/encoded': dataset_util.bytes_feature(tf.image.encode_jpeg(image))
    # there are more features but they're not relevant
}))
</code></pre>

<p>I suspect this is valid right up until I actually try to put it in the <code>tf.train.Example</code>, at which point it tells me <code>TypeError: &lt;tf.Tensor 'EncodeJpeg:0' shape=() dtype=string&gt; has type Tensor, but expected one of: bytes</code>. I've tried figuring out how to get the <code>Tensor</code> into a <code>BytesList</code> or something like it, but I haven't been able to find any documentation for this. I suspect there may be a better way to approach the entire process however.</p>

<p>How can I do this the right way?</p>
","I have an image (JPEG or PNG) as a byte buffer (read from the internet), and this is the way I was putting it in a tf.train.Example before: However, for my usecase, the images are too big, so I'd like to resize them either before I put them in the tf.train.Example or just after (whichever is easiest). Here's what I'm trying: I suspect this is valid right up until I actually try to put it in the tf.train.Example, at which point it tells me TypeError: &lt;tf.Tensor 'EncodeJpeg:0' shape=() dtype=string&gt; has type Tensor, but expected one of: bytes. I've tried figuring out how to get the Tensor into a BytesList or something like it, but I haven't been able to find any documentation for this. I suspect there may be a better way to approach the entire process however. How can I do this the right way?",https://stackoverflow.com/questions/55005915,1129436,Lack of Alternative Solutions/Documentation
55044905,"Tensorflow low level api, batch normalization problem","<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">tf.layers.batch_normalization</a> documentation says it will be removed in a future version, and should be replaced by <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"" rel=""nofollow noreferrer"">tf.keras.layers.BatchNormalization</a>, but i cannot find a way to replace the functionality using tensorflow low level api.</p>

<pre><code>import tensorflow as tf
bn = tf.layers.batch_normalization(tf.constant([0.0]), training=True)
print(tf.get_collection(tf.GraphKeys.UPDATE_OPS))
</code></pre>

<p>which outputs:</p>

<pre><code>[&lt;tf.Operation 'batch_normalization/AssignMovingAvg' type=AssignSub&gt;,
&lt;tf.Operation 'batch_normalization/AssignMovingAvg_1' type=AssignSub&gt;]
</code></pre>

<p>If we instead use keras as suggested in the documentation</p>

<pre><code>bn = tf.keras.layers.BatchNormalization(axis=-1)(tf.constant([0.0]), training=True)
</code></pre>

<p>we get an empty output:</p>

<pre><code>[]
</code></pre>

<p>Since UPDATE_OPS is empty, the model is unable to update the batch normalization moving_avg_mean and moving_avg_variance during training using keras (resulting in a much higer test error). Any suggestion how to solve this is greatly appreciated! </p>

<p>The example above is taken from an older post of how to use <a href=""https://stackoverflow.com/questions/48874558/tensorflow-tf-layers-batch-normalization-doesnt-add-update-ops-to-tf-graphke"">tf.layers.batch_normalization</a></p>
","The tf.layers.batch_normalization documentation says it will be removed in a future version, and should be replaced by tf.keras.layers.BatchNormalization, but i cannot find a way to replace the functionality using tensorflow low level api. which outputs: If we instead use keras as suggested in the documentation we get an empty output: Since UPDATE_OPS is empty, the model is unable to update the batch normalization moving_avg_mean and moving_avg_variance during training using keras (resulting in a much higer test error). Any suggestion how to solve this is greatly appreciated! The example above is taken from an older post of how to use tf.layers.batch_normalization",https://stackoverflow.com/questions/55044905,10455493,Documentation Replication on Other Examples
55094952,Understanding Tensorflow control dependencies,"<p>I am trying to gain a stronger grasp of TensorFlow. I came across the concept of control dependencies. I understand that the order of ops as specified by us is not really relevant to Tensorflow during execution. In order to optimise the speed of execution TensorFlow decides its own order of calculating nodes. 
But we can customise order of execution by using tf.control_dependencies.
I am not able to understand the use cases of the function. Can anyone direct me to some resource(other than the documentation) or explain the working of this function?
An example:</p>

<pre><code>tf.reset_default_graph()
x = tf.Variable(5)
y=tf.Variable(3)
assign = tf.assign(x,x+y)
z = x+assign
with tf.Session() as sess:
   sess.run(tf.global_variables_initializer())
   with tf.control_dependencies([assign]):
        z_out = sess.run(z)

print(z_out)
</code></pre>

<p>The output of the code is 8. So I infer that since z=x+y,the assign node has not been evaluated(right?). But doesn't this mean that the result of tensorflow may be erroneous? This means we need to create new nodes during every operation to force TensorFlow to calculate all the nodes leading up to the result. But in say training a neural network with 10000 steps if each step creates a new set of 1000 weights/parameters won't the space complexity explode?</p>
","I am trying to gain a stronger grasp of TensorFlow. I came across the concept of control dependencies. I understand that the order of ops as specified by us is not really relevant to Tensorflow during execution. In order to optimise the speed of execution TensorFlow decides its own order of calculating nodes. But we can customise order of execution by using tf.control_dependencies. I am not able to understand the use cases of the function. Can anyone direct me to some resource(other than the documentation) or explain the working of this function? An example: The output of the code is 8. So I infer that since z=x+y,the assign node has not been evaluated(right?). But doesn't this mean that the result of tensorflow may be erroneous? This means we need to create new nodes during every operation to force TensorFlow to calculate all the nodes leading up to the result. But in say training a neural network with 10000 steps if each step creates a new set of 1000 weights/parameters won't the space complexity explode?",https://stackoverflow.com/questions/55094952,10726895,Inadequate Examples
55141486,Unable to see keras model graph in Tensorboard when using TensorFlow 2.0 Alpha,"<p>I am trying custom training on TensorFlow 2.0 alpha and at the same time I am trying to add some metrics and my training graph to TensorBoard. Consider the following contrived example</p>

<pre><code>import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model


def create_model():
    inp = Input((32, ))
    net = Dense(16, activation=""relu"")(inp)
    net = Dense(8, activation=""relu"")(net)
    net = Dense(2, activation=None)(net)
    return Model(inp, net)


@tf.function
def grad(model, loss, x, y):
    with tf.GradientTape() as tape:
        y_ = model(x)
        loss_value = loss(y_true=y, y_pred=y_)
    return loss_value, tape.gradient(loss_value, model.trainable_variables)


@tf.function
def train_step(model, loss, optimizer, features, labels):
    loss_value, grads = grad(model, loss, features, labels)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss_value


def train():
    tf.summary.trace_on(graph=True, profiler=True)

    with tf.summary.create_file_writer(""model"").as_default():
        model = create_model()

        loss = tf.keras.losses.MeanSquaredError()
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)

        for i in range(10):
            tf.summary.experimental.set_step(i)

            features = tf.random.normal((16, 32))
            labels = tf.random.normal((16, 2))
            loss_value = train_step(model, loss, optimizer, features, labels)
            print(loss_value)

        tf.summary.trace_export(""model"", profiler_outdir=""model"")


if __name__ == ""__main__"":
    train()
</code></pre>

<p>This, does not show the model graph properly, on doing</p>

<pre><code>tensorboard --logdir model
</code></pre>

<p>In the graphs tab I am seeing <a href=""https://i.stack.imgur.com/TRbqc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TRbqc.png"" alt=""tensorboard""></a></p>

<p>I am getting the graph when I am training through model.fit or estimator. For example, here is the graphs section when I use <code>model_to_estimator</code> to convert a model</p>

<p><a href=""https://i.stack.imgur.com/h0rBQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/h0rBQ.png"" alt=""model_to_estiamtor""></a></p>

<p><a href=""https://www.tensorflow.org/alpha/tutorials/eager/custom_training_walkthrough"" rel=""noreferrer"">The guide article</a> does not track metrics through tensorboard, and I did not find any sections on the new workflow for custom adding and tracking of metrics in TensorBoard on alpha (<a href=""https://www.tensorflow.org/alpha"" rel=""noreferrer"">https://www.tensorflow.org/alpha</a>). My contrived implementation is based on the API documentation of tf.summary (<a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary"" rel=""noreferrer"">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary</a>) </p>
","I am trying custom training on TensorFlow 2.0 alpha and at the same time I am trying to add some metrics and my training graph to TensorBoard. Consider the following contrived example This, does not show the model graph properly, on doing In the graphs tab I am seeing I am getting the graph when I am training through model.fit or estimator. For example, here is the graphs section when I use model_to_estimator to convert a model The guide article does not track metrics through tensorboard, and I did not find any sections on the new workflow for custom adding and tracking of metrics in TensorBoard on alpha (https://www.tensorflow.org/alpha). My contrived implementation is based on the API documentation of tf.summary (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary)",https://stackoverflow.com/questions/55141486,3673043,Documentation Replication on Other Examples
55168906,Tensorflow - tf.nn.weighted_cross_entropy_with_logits - logits and targets must have the same shape,"<p>I've just started using tensorflow for a project I'm working on. The program aims to be a binary classifier with input being 12 features. The output is either normal patient or patient with a disease. The prevalence of the disease is quite low and so my dataset is very imbalanced, with 502 examples of normal controls and only 38 diseased patients. For this reason, I'm trying to use <code>tf.nn.weighted_cross_entropy_with_logits</code> as my cost function.</p>

<p>The code is based on the iris custom estimator from the official tensorflow documentation, and works with <code>tf.losses.sparse_softmax_cross_entropy</code> as the cost function. However, when I change to <code>weighted_cross_entropy_with_logits</code>, I get a shape error and I'm not sure how to fix this.</p>

<pre><code>ValueError: logits and targets must have the same shape ((?, 2) vs (?,))
</code></pre>

<p>I have searched and similar problems have been solved by just reshaping the labels - I have tried to do this unsuccessfully (and don't understand why <code>tf.losses.sparse_softmax_cross_entropy</code> works fine and the weighted version does not). </p>

<p>My full code is here
<a href=""https://gist.github.com/revacious/83142573700c17b8d26a4a1b84b0dff7"" rel=""nofollow noreferrer"">https://gist.github.com/revacious/83142573700c17b8d26a4a1b84b0dff7</a></p>

<p>Thanks!</p>
","I've just started using tensorflow for a project I'm working on. The program aims to be a binary classifier with input being 12 features. The output is either normal patient or patient with a disease. The prevalence of the disease is quite low and so my dataset is very imbalanced, with 502 examples of normal controls and only 38 diseased patients. For this reason, I'm trying to use tf.nn.weighted_cross_entropy_with_logits as my cost function. The code is based on the iris custom estimator from the official tensorflow documentation, and works with tf.losses.sparse_softmax_cross_entropy as the cost function. However, when I change to weighted_cross_entropy_with_logits, I get a shape error and I'm not sure how to fix this. I have searched and similar problems have been solved by just reshaping the labels - I have tried to do this unsuccessfully (and don't understand why tf.losses.sparse_softmax_cross_entropy works fine and the weighted version does not). My full code is here https://gist.github.com/revacious/83142573700c17b8d26a4a1b84b0dff7 Thanks!",https://stackoverflow.com/questions/55168906,11204499,Documentation Replication on Other Examples
55176818,How to support masking in custom tf.keras.layers.Layer,"<p>I'm implementing a custom <code>tf.keras.layers.Layer</code> that needs to support masking.</p>

<p>Consider the following scenario</p>

<pre class=""lang-py prettyprint-override""><code>embedded = tf.keras.layer.Embedding(input_dim=vocab_size + 1, 
                                    output_dim=n_dims, 
                                    mask_zero=True)
x = MyCustomKerasLayers(embedded)
</code></pre>

<p>Now per the documentation</p>

<blockquote>
  <p><code>mask_zero</code>: Whether or not the input value 0 is a special ""padding"" value that should be masked out. This is useful when using recurrent layers which may take variable length input. <strong>If this is True then all subsequent layers in the model need to support masking or an exception will be raised</strong>. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).</p>
</blockquote>

<p>I wonder, what does that mean? Looking through <a href=""https://www.tensorflow.org/tutorials/eager/custom_layers"" rel=""noreferrer"">TensorFlow's custom layers guide</a> and the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer"" rel=""noreferrer"">tf.keras.layer.Layer</a> documentation it is not clear what should be done to support masking</p>

<ol>
<li><p>How do I support masking?</p></li>
<li><p>How do I access the mask from the past layer? </p></li>
<li><p>Assuming input of <code>(batch, time, channels)</code> or `(batch, time) would the masks look different? What will be their shapes?</p></li>
<li><p>How do I pass it on to the next layer? </p></li>
</ol>
","I'm implementing a custom tf.keras.layers.Layer that needs to support masking. Consider the following scenario Now per the documentation I wonder, what does that mean? Looking through TensorFlow's custom layers guide and the tf.keras.layer.Layer documentation it is not clear what should be done to support masking",https://stackoverflow.com/questions/55176818,5368083,Lack of Alternative Solutions/Documentation
55300544,TF 2.0 @tf.function example,"<p>In the tensorflow documentation at the <a href=""https://www.tensorflow.org/alpha/guide/autograph"" rel=""nofollow noreferrer"">autograph</a> section we have the following code snippet</p>

<pre class=""lang-py prettyprint-override""><code>@tf.function
def train(model, optimizer):
  train_ds = mnist_dataset()
  step = 0
  loss = 0.0
  accuracy = 0.0
  for x, y in train_ds:
    step += 1
    loss = train_one_step(model, optimizer, x, y)
    if tf.equal(step % 10, 0):
      tf.print('Step', step, ': loss', loss, '; accuracy', compute_accuracy.result())
  return step, loss, accuracy

step, loss, accuracy = train(model, optimizer)
print('Final step', step, ': loss', loss, '; accuracy', compute_accuracy.result())
</code></pre>

<p>I have a small question concerning the <code>step</code> variable, it's an integer and not a tensor, autograph supports built-in python type such as integer. Therefore the <code>tf.equal(step%10,0)</code> could be changed to simply <code>step%10 == 0</code> right ? </p>
","In the tensorflow documentation at the autograph section we have the following code snippet I have a small question concerning the step variable, it's an integer and not a tensor, autograph supports built-in python type such as integer. Therefore the tf.equal(step%10,0) could be changed to simply step%10 == 0 right ?",https://stackoverflow.com/questions/55300544,8593338,Documentation Replication on Other Examples
55347304,Error when applying sample/class weights to fit generator,"<p>I am using a tf.keras.Model fit_generator (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit_generator"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit_generator</a>)  to feed batches of data to a model. According to TensorFlow Documentation, the fit generator should be able to accept size 2 (inputs, targets) or 3 (inputs, targets, sample_weights) tuple. We have the size 2 working, but we have unbalanced classes, so I have determined sample weights. When the fit generator returns a size 3 tuple, I get the error:
”tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got [batch_size]""</p>

<p>I am using tensorflow 1.12</p>

<p>Loss Function is tf.losses.softmax_cross_entropy</p>
","I am using a tf.keras.Model fit_generator (https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit_generator) to feed batches of data to a model. According to TensorFlow Documentation, the fit generator should be able to accept size 2 (inputs, targets) or 3 (inputs, targets, sample_weights) tuple. We have the size 2 working, but we have unbalanced classes, so I have determined sample weights. When the fit generator returns a size 3 tuple, I get the error: ”tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got [batch_size]"" I am using tensorflow 1.12 Loss Function is tf.losses.softmax_cross_entropy",https://stackoverflow.com/questions/55347304,11257466,Documentation Replication on Other Examples
55363728,How to feed .h5 files in tf.data pipeline in tensorflow model,"<p>I'm trying to optimize the input pipeline for .h5 data with tf.data. But I encountered a <code>TypeError: expected str, bytes or os.PathLike object, not Tensor</code>. I did a research but can't find anything about converting a tensor of string to string.</p>

<p>This simplified code is executable and return the same error:</p>

<pre><code>batch_size = 1000
conv_size = 3
nb_conv = 32
learning_rate = 0.0001

# define parser function
def parse_function(fname):
    with h5py.File(fname, 'r') as f: #Error comes from here
        X = f['X'].reshape(batch_size, patch_size, patch_size, 1)
        y = f['y'].reshape(batch_size, patch_size, patch_size, 1)
        return X, y

# create a list of files path
flist = []
for dirpath, _, fnames in os.walk('./proc/'):
    for fname in fnames:
        if fname.startswith('{}_{}'.format(patch_size, batch_size)) and fname.endswith('h5'):
            flist.append(fname)

# prefetch data
dataset = tf.data.Dataset.from_tensor_slices((flist))
dataset = dataset.shuffle(len(flist))
dataset = dataset.map(parse_function, num_parallel_calls=4)
dataset = dataset.batch(1)
dataset = dataset.prefetch(3)

# simplest model that I think of
X_ph = tf.placeholder(tf.float32, shape=None)
y_ph = tf.placeholder(tf.float32, shape=None)
W = tf.get_variable('w', shape=[conv_size, conv_size, 1, 1], initializer=tf.contrib.layers.xavier_initializer())
loss = tf.reduce_mean(tf.losses.mean_squared_error(tf.nn.softmax(labels=y_ph, predictions=tf.matmul(X_ph, W))))
train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

# start session
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(train_op, feed_dict={X_ph: dataset[0], y_ph: dataset[1]}))
</code></pre>

<p>Apparently the <code>fname</code> is a tensor of string but the positional argument waits for only a string. I can't find any documentation on this. And the answer of <a href=""https://stackoverflow.com/questions/45246764/how-to-convert-a-string-tensor-to-a-python-string-in-tensorflow"">another post</a> doesn't solve this problem. In my case, I work only with h5 where one h5 store one batch.</p>

<hr>

<p><strong>Update Solution:</strong>
Thanks to the comment of @kvish, the part of loading .h5 file is solved.
The code is upgraded with a simple conv layer, the placeholders have been taken. <strong>Each .h5 is one batch.</strong> I want to prefetch in parallele multiple batches(h5py doesn't support multithread reading so I write batches into multiple files). One can <strong>copy-paste-and-launch</strong>:</p>

<pre><code>import h5py
import threading
import numpy as np
import tensorflow as tf

# generate some img data
for i in range(5):
    with h5py.File('./test_{}.h5'.format(i), 'w') as f:
        f.create_dataset('X', shape=(1000, 100, 100), dtype='float32', data=np.random.rand(10**7).reshape(1000, 100, 100))
        f.create_dataset('y', shape=(1000, 100, 100), dtype='float32', data=np.random.rand(10**7).reshape(1000, 100, 100))
        print(threading.get_ident())

# params
num_cores = 3
shuffle_size = 1
batch_size = 1

# read .h5 file
def parse_file(f):
    print(f.decode('utf-8'))
    with h5py.File(f.decode(""utf-8""), 'r') as fi:
        X = fi['X'][:].reshape(1000, 100, 100, 1)
        y = fi['y'][:].reshape(1000, 100, 100, 1)
        print(threading.get_ident())  # to see the thread id
        return X, y

# py_func wrapper
def parse_file_tf(filename):
    return tf.py_func(parse_file, [filename], [tf.float32, tf.float32])

# tf.data input pipeline
files = tf.data.Dataset.list_files('./test_*.h5')
dataset = files.map(parse_file_tf, num_parallel_calls=num_core)
dataset = dataset.batch(batch_size).shuffle(shuffle_size).prefetch(3)
it = dataset.make_initializable_iterator()
iter_init_op = it.initializer
X_it, y_it = it.get_next()

# simplest model that I can think of 
with tf.name_scope(""Conv1""):
    W = tf.get_variable(""W"", shape=[3, 3, 1, 1],
                         initializer=tf.contrib.layers.xavier_initializer())
    b = tf.get_variable(""b"", shape=[1], initializer=tf.contrib.layers.xavier_initializer())
    layer1 = tf.nn.conv2d(X_it, W, strides=[1, 1, 1, 1], padding='SAME') + b
    out = tf.nn.relu(layer1)

loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_it, predictions=out))
train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)

# session
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(iter_init_op)
sess.run([train_op])
sess.close()
</code></pre>

<hr>

<p>Somehow there will be another cudnn issue which isn't related to this post.</p>

<p>tensorflow-cpu v1.12: work fine</p>

<p>tensorflow-gpu v1.12: <strong>runtime</strong> issue happens</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"",
  line 1334, in _do_call
      return fn(*args)   File ""/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"",
  line 1319, in _run_fn
      options, feed_dict, fetch_list, target_list, run_metadata)   File ""/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"",
  line 1407, in _call_tf_sessionrun
      run_metadata) tensorflow.python.framework.errors_impl.NotFoundError: No algorithm
  worked!    [[{{node Conv1/Conv2D}} = Conv2D[T=DT_FLOAT,
  data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"",
  strides=[1, 1, 1, 1], use_cudnn_on_gpu=true,
  _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/Conv1/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer,
  W/read)]]      [[{{node
  mean_squared_error/num_present/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_2/_37}}
  = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"",
  send_device=""/job:localhost/replica:0/task:0/device:GPU:0"",
  send_device_incarnation=1, tensor_name=""edge_63_me...t/Switch_2"",
  tensor_type=DT_INT32,
  _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]
  tensorflow-cpu v1.12: works fine!</p>
</blockquote>
","I'm trying to optimize the input pipeline for .h5 data with tf.data. But I encountered a TypeError: expected str, bytes or os.PathLike object, not Tensor. I did a research but can't find anything about converting a tensor of string to string. This simplified code is executable and return the same error: Apparently the fname is a tensor of string but the positional argument waits for only a string. I can't find any documentation on this. And the answer of another post doesn't solve this problem. In my case, I work only with h5 where one h5 store one batch. Update Solution: Thanks to the comment of @kvish, the part of loading .h5 file is solved. The code is upgraded with a simple conv layer, the placeholders have been taken. Each .h5 is one batch. I want to prefetch in parallele multiple batches(h5py doesn't support multithread reading so I write batches into multiple files). One can copy-paste-and-launch: Somehow there will be another cudnn issue which isn't related to this post. tensorflow-cpu v1.12: work fine tensorflow-gpu v1.12: runtime issue happens",https://stackoverflow.com/questions/55363728,9217178,Lack of Alternative Solutions/Documentation
55379830,How do I resize image with unknown size in Tensorflow(tf.shape(input) method doesn't work),"<p>According to <a href=""https://stackoverflow.com/questions/52410154/resizing-tensorflow-images-with-unknown-size"">this post</a>, one can use tf.shape() to resize image with unknown size like placeholder. But the method doesn't seem to work for me. I have some simple code that looks like:</p>

<pre><code>import tensorflow as tf
import numpy as np

def speed_tune(x, lower_bound=0.8, upper_bound=2.0):
    speed_rate = np.random.uniform(lower_bound, upper_bound)
    newshape = tf.shape(x)[1:] # get the tensor shape except for rank 0(None)
    newshape *= speed_rate # randomly stretch or compress the signal 
    return tf.resize(x, newshape)

sess = tf.InteractiveSession()
x = tf.placeholder(tf.int16, (None, 1000)) # x is a 1D audio signal
y = speed_tune(x)
data = np.random.randint(10, size=1000)
output = sess.run(y, feed_dict={x:data})
</code></pre>

<p>Basically, my code does the following: Given an input 1D data x, the program tries to stretch or compress the sequence by some random factor and return the tuned sequence. Since I didn't find any Tensorflow function that directly performs this operation, I use tf.resize by treating the data as 1xD image where D is the length of the signal. But I got an error:</p>

<pre><code>Traceback (most recent call last):
  File ""d:\SVNRepo\Python_codes\scratch.py"", line 33, in &lt;module&gt;
    y = speed_tune(x)
  File ""d:\SVNRepo\Python_codes\scratch.py"", line 28, in speed_tune
    newshape *= speed_rate # randomly stretch or compress the signal 
TypeError: unsupported operand type(s) for *=: 'Tensor' and 'float'
</code></pre>

<p>So it seems like <code>tf.shape(x)</code> returns a Tensor rather than integer values that specify the shape of the tensor(verified by <a href=""https://www.tensorflow.org/api_docs/python/tf/shape"" rel=""nofollow noreferrer"">Tensorflow document</a>). How can I solve this?</p>
","According to this post, one can use tf.shape() to resize image with unknown size like placeholder. But the method doesn't seem to work for me. I have some simple code that looks like: Basically, my code does the following: Given an input 1D data x, the program tries to stretch or compress the sequence by some random factor and return the tuned sequence. Since I didn't find any Tensorflow function that directly performs this operation, I use tf.resize by treating the data as 1xD image where D is the length of the signal. But I got an error: So it seems like tf.shape(x) returns a Tensor rather than integer values that specify the shape of the tensor(verified by Tensorflow document). How can I solve this?",https://stackoverflow.com/questions/55379830,8577452,Documentation Replication on Other Examples
55422537,Testing TF serving model fails with bytes as strings and strings as bytes confusion,"<p>I'm having a problem serving my text classification model on <code>Tensorflow 1.12</code>. I'm using <code>tf.estimator.inputs.pandas_input_fn</code> to read in my data, and <code>tf.estimator.DNNClassifier</code> to train/evaluate. I'd then like to serve my model.
(Apologies in advance, it's tough to provide a full working example here, but it's very much like the example TF provides at <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier</a>  )</p>

<p>I'm currently saving my model with ...</p>

<pre class=""lang-py prettyprint-override""><code>...
estimator.export_savedmodel(""./TEST_SERVING/"", self.serving_input_receiver_fn, strip_default_attrs=True)
...
def serving_input_receiver_fn(self):
      """"""An input receiver that expects a serialized tf.Example.""""""

      # feature spec dictionary  determines our input parameters for the model
      feature_spec = {
          'Headline': tf.VarLenFeature(dtype=tf.string),
          'Description': tf.VarLenFeature(dtype=tf.string)
      }

      # the inputs will be initially fed as strings with data serialized by
      # Google ProtoBuffers
      serialized_tf_example = tf.placeholder(
          dtype=tf.string, shape=None, name='input_example_tensor')
      receiver_tensors = {'examples': serialized_tf_example}

      # deserialize input
      features = tf.parse_example(serialized_tf_example, feature_spec)
      return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)


</code></pre>

<p>This actually fails to run with the error:</p>

<pre class=""lang-sh prettyprint-override""><code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensor'&gt; to Tensor. Contents: SparseTensor(indices=Tensor(""ParseExample/ParseExample:0"", shape=(?, 2), 
dtype=int64), values=Tensor(""ParseExample/ParseExample:2"", shape=(?,), dtype=string), dense_shape=Tensor(""ParseExample/ParseExample:4"", shape=(2,), dtype=int64)). Consider casting elements to a supported type.

</code></pre>

<p>I tried to save a second way doing:</p>

<pre class=""lang-py prettyprint-override""><code>def serving_input_receiver_fn(self):
  """"""Build the serving inputs.""""""
  INPUT_COLUMNS = [""Headline"",""Description""]
  inputs = {}
  for feat in INPUT_COLUMNS:
    inputs[feat] = tf.placeholder(shape=[None], dtype=tf.string, name=feat)
  return tf.estimator.export.ServingInputReceiver(inputs, inputs)
</code></pre>

<p>This actually works, until I try testing it with the <code>saved_model_cli</code>.
Some output for <code>saved_model_cli show --all --dir TEST_SERVING/1553879255/</code>:</p>

<pre class=""lang-sh prettyprint-override""><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['predict']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['Description'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Description:0
    inputs['Headline'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Headline:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['class_ids'] tensor_info:
        dtype: DT_INT64
        shape: (-1, 1)
        name: dnn/head/predictions/ExpandDims:0
    outputs['classes'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: dnn/head/predictions/str_classes:0
    outputs['logits'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/logits/BiasAdd:0
    outputs['probabilities'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/head/predictions/probabilities:0
  Method name is: tensorflow/serving/predict

</code></pre>

<p>But now I can't seem to test it.</p>

<pre class=""lang-sh prettyprint-override""><code>&gt;&gt;&gt; saved_model_cli run --dir TEST_SERVING/1553879255/ --tag_set serve --signature_def predict --input_examples 'inputs=[{""Description"":[""What is going on""],""Headline"":[""Help me""]}]'
Traceback (most recent call last):
 ...
  File ""/Users/Josh/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 489, in _create_example_string
    feature_list)
TypeError: 'What is going on' has type str, but expected one of: bytes

</code></pre>

<p>Ok, lets turn it into a bytes object by changing to <code>b[""What is going on""]</code> and <code>b[""Help me""]</code>...</p>

<pre class=""lang-sh prettyprint-override""><code>ValueError: Type &lt;class 'bytes'&gt; for value b'What is going on' is not supported for tf.train.Feature.
</code></pre>

<p>Any ideas/thoughts??
Thanks!</p>
","I'm having a problem serving my text classification model on Tensorflow 1.12. I'm using tf.estimator.inputs.pandas_input_fn to read in my data, and tf.estimator.DNNClassifier to train/evaluate. I'd then like to serve my model. (Apologies in advance, it's tough to provide a full working example here, but it's very much like the example TF provides at https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier ) I'm currently saving my model with ... This actually fails to run with the error: I tried to save a second way doing: This actually works, until I try testing it with the saved_model_cli. Some output for saved_model_cli show --all --dir TEST_SERVING/1553879255/: But now I can't seem to test it. Ok, lets turn it into a bytes object by changing to b[""What is going on""] and b[""Help me""]... Any ideas/thoughts?? Thanks!",https://stackoverflow.com/questions/55422537,5088987,Documentation Replication on Other Examples
55425811,Implementing Intersection over Union Loss Using Tensorflow,"<p>This may be more of a Tensorflow gradient question. I have been attempting to implement Intersection over Union (IoU) as losses and have been running into some problems. To the point, here is the snippet of my code that computes the IoU:</p>

<pre><code>def get_iou(masks, predictions):
    ious = []
    for i in range(batch_size):
        mask = masks[i]
        pred = predictions[i]
        masks_sum = tf.reduce_sum(mask)
        predictions_sum = tf.reduce_mean(pred)
        intersection = tf.reduce_sum(tf.multiply(mask, pred))
        union = masks_sum + predictions_sum - intersection
        iou = intersection / union
        ious.append(iou)
    return ious

iou = get_iou(masks, predictions)
mean_iou_loss = -tf.log(tf.reduce_sum(iou))
train_op = tf.train.AdamOptimizer(0.001).minimize(mean_iou_loss)
</code></pre>

<p>It works as predicted. However, the issue that I am having is the losses do not decrease. The model does train, though the results are less than ideal so I am wondering if I am implementing it correctly. Do I have to compute the gradients myself? I can compute the gradients for this IoU loss derived by <a href=""https://arxiv.org/pdf/1608.01471.pdf"" rel=""noreferrer"">this paper</a> using <code>tf.gradients()</code>, though I am not sure how to incorporate that with the <code>tf.train.AdamOptimizer()</code>. Reading the documentation, I feel like <code>compute_gradients</code> and <code>apply_gradients</code> are the commands that I need to use, but I can't find any examples on how to use them. My understanding is that the Tensorflow graph should be able to come up with the gradient itself via chain rule. So is a custom gradient even necessary in this problem? If the custom gradient is not necessary then I may just have an ill-posed problem and need to adjust some hyperparameters.</p>

<p><strong>Note:</strong> I have tried Tensorflow's implementation of the IoU, <code>tf.metrics.mean_iou()</code>, but it spits out <code>inf</code> every time so I have abandoned that.</p>
","This may be more of a Tensorflow gradient question. I have been attempting to implement Intersection over Union (IoU) as losses and have been running into some problems. To the point, here is the snippet of my code that computes the IoU: It works as predicted. However, the issue that I am having is the losses do not decrease. The model does train, though the results are less than ideal so I am wondering if I am implementing it correctly. Do I have to compute the gradients myself? I can compute the gradients for this IoU loss derived by this paper using tf.gradients(), though I am not sure how to incorporate that with the tf.train.AdamOptimizer(). Reading the documentation, I feel like compute_gradients and apply_gradients are the commands that I need to use, but I can't find any examples on how to use them. My understanding is that the Tensorflow graph should be able to come up with the gradient itself via chain rule. So is a custom gradient even necessary in this problem? If the custom gradient is not necessary then I may just have an ill-posed problem and need to adjust some hyperparameters. Note: I have tried Tensorflow's implementation of the IoU, tf.metrics.mean_iou(), but it spits out inf every time so I have abandoned that.",https://stackoverflow.com/questions/55425811,9754177,Documentation Replication on Other Examples
55560676,How to use tf.while_loop with eager execution?,"<p>In the documentation, the body of a tf.while_loop needs to be a python callable.</p>

<pre><code>i = tf.constant(0)
b = lambda i: tf.add(i,1)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>works but</p>

<pre><code>def b(i):
    tf.add(i,1)

i = tf.constant(0)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>throws a ValueError: Attempt to convert a value (None) with an unsupported type() to a Tensor</p>

<p>In 2.0, eager execution is default, I wonder what's the problem?!</p>
","In the documentation, the body of a tf.while_loop needs to be a python callable. works but throws a ValueError: Attempt to convert a value (None) with an unsupported type() to a Tensor In 2.0, eager execution is default, I wonder what's the problem?!",https://stackoverflow.com/questions/55560676,7779411,Documentation Replicability
55573670,Unexpected output for tf.nn.sparse_softmax_cross_entropy_with_logits,"<p>The TensorFlow documentation for <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> explicitly declares that I should not apply softmax to the inputs of this op:</p>

<blockquote>
  <p>This op expects unscaled logits, since it performs a softmax on logits
  internally for efficiency. Do not call this op with the output of
  softmax, as it will produce incorrect results.</p>
</blockquote>

<p>However if I use cross entropy without softmax it gives me unexpected results. According to <a href=""https://cs231n.github.io/neural-networks-3/#sanitycheck"" rel=""nofollow noreferrer"">CS231n course</a> the expected loss value is around 2.3 for CIFAR-10:</p>

<blockquote>
  <p>For example, for CIFAR-10 with a Softmax classifier we would expect
  the initial loss to be 2.302, because we expect a diffuse probability
  of 0.1 for each class (since there are 10 classes), and Softmax loss
  is the negative log probability of the correct class so: -ln(0.1) =
  2.302.</p>
</blockquote>

<p>However without softmax I get much bigger values, for example 108.91984.</p>

<p>What exactly am I doing wrong with <code>sparse_softmax_cross_entropy_with_logits</code>? The TF code is shown below.</p>

<pre><code>import tensorflow as tf
import numpy as np
from tensorflow.python import keras


(_, _), (x_test, y_test) = keras.datasets.cifar10.load_data()
x_test = np.reshape(x_test, [-1, 32, 32, 3])

y_test = np.reshape(y_test, (10000,))
y_test = y_test.astype(np.int32)

x = tf.placeholder(dtype=tf.float32, shape=(None, 32, 32, 3))
y = tf.placeholder(dtype=tf.int32, shape=(None,))

layer = tf.layers.Conv2D(filters=16, kernel_size=3)(x)
layer = tf.nn.relu(layer)
layer = tf.layers.Flatten()(layer)
layer = tf.layers.Dense(units=1000)(layer)
layer = tf.nn.relu(layer)
logits = tf.layers.Dense(units=10)(layer)

# If this line is uncommented I get expected value around 2.3
# logits = tf.nn.softmax(logits)

loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,
                                                      logits=logits)
loss = tf.reduce_mean(loss, name='cross_entropy')

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    res = sess.run(loss, feed_dict={x: x_test[0:256], y: y_test[0:256]})
    print(""loss: "", res)
    # Expected output is value close to 2.3
    # Real outputs are 108.91984, 72.82324, etc.

</code></pre>
","The TensorFlow documentation for tf.nn.sparse_softmax_cross_entropy_with_logits explicitly declares that I should not apply softmax to the inputs of this op: However if I use cross entropy without softmax it gives me unexpected results. According to CS231n course the expected loss value is around 2.3 for CIFAR-10: However without softmax I get much bigger values, for example 108.91984. What exactly am I doing wrong with sparse_softmax_cross_entropy_with_logits? The TF code is shown below.",https://stackoverflow.com/questions/55573670,9565342,Documentation Replication on Other Examples
55703097,Training while loop in Tensorflow,"<p>I've attempted converting a Python-side training loop to Tensorflow to (hypothetically) make the code run faster - not having to pass control over to cpu constantly. However, I can't manage using <code>tf.while_loop</code>.</p>

<p>Here's the code that works:</p>

<pre><code>import numpy as np
import tensorflow as tf

from tqdm import tqdm
from sklearn.datasets import load_iris
from sklearn.preprocessing import RobustScaler

x, y = load_iris(True)
x = RobustScaler().fit_transform(x)

shape = (10, 10)
max_epochs = 1000


graph = tf.Graph()
sess = tf.Session(graph=graph)

x = x.astype(np.float64)


# Construct graph
with graph.as_default():
    weights = tf.get_variable(
        'weights', shape, initializer=tf.constant_initializer, dtype=tf.float64
    )
    curr_epoch = tf.placeholder(dtype=tf.int64, shape=())

    with tf.name_scope('data'):
        data = tf.data.Dataset.from_tensor_slices(x)
        data = data.shuffle(buffer_size=10000)
        data = data.repeat(max_epochs)
        data = data.batch(1)
        data = data.make_one_shot_iterator().get_next()

    with tf.name_scope('update'):
        update_op = make_update_op(weights)

    init = tf.global_variables_initializer()


sess.run(init)

for i in tqdm(range(max_epochs)):
    for _ in range(x.shape[0]):
        sess.run(update_op, feed_dict={
            curr_epoch: i
        })

np_weights = sess.run(weights)
print(np_weights) # Correctly prints an array of 150's.
</code></pre>

<p>Now, if I create an update function to pass <code>tf.while_loop</code>, an error is thrown.</p>

<pre><code>def make_update_op(w):
    return w.assign(
        w + 0.001
    )

# In the code above:
update_op = tf.while_loop(lambda _: True, make_update_op, (weights,), maximum_iterations=x.shape[0])

# No inner loop:
for i in tqdm(range(max_epochs)):
    sess.run(update_op, feed_dict={
        curr_epoch: i
    })
</code></pre>

<blockquote>
  <p>Line 22, in make_update_op
      <code>return w.assign(</code>
  AttributeError: 'Tensor' object has no attribute 'assign'</p>
</blockquote>

<p>I don't quite understand what is happening even after reading the documentation. <code>weights</code> is a <code>Variable</code> after all. What could be done to correctly make the training loop?</p>
","I've attempted converting a Python-side training loop to Tensorflow to (hypothetically) make the code run faster - not having to pass control over to cpu constantly. However, I can't manage using tf.while_loop. Here's the code that works: Now, if I create an update function to pass tf.while_loop, an error is thrown. I don't quite understand what is happening even after reading the documentation. weights is a Variable after all. What could be done to correctly make the training loop?",https://stackoverflow.com/questions/55703097,7089239,Documentation Replication on Other Examples
55711355,How to restore dangling tf.py_func within the tf.data.Dataset() with tf.saved_model API?,"<p>After doing a research for restoring the <code>tf.py_func()</code> when using saved_model API in vain, I couldn't find other information than documented in <a href=""https://www.tensorflow.org/api_docs/python/tf/py_func"" rel=""nofollow noreferrer"">tensorflow</a>:</p>

<blockquote>
  <p>The operation must run in the same address space as the Python program that calls <code>tf.py_func()</code>. If you are using distributed TensorFlow, you must run a <code>tf.train.Server</code> in the same process as the program that calls <code>tf.py_func()</code> and you must pin the created operation to a device in that server (e.g. using with <code>tf.device()</code>:)</p>
</blockquote>

<p>Two save/load snippets help to illustrate the situation. </p>

<p><strong>Save part:</strong></p>

<pre><code>def wrapper(x, y):
    with tf.name_scope('wrapper'):
        return tf.py_func(Copy, [x, y], [tf.float32, tf.float32])

def Copy(x, y):
    return x, y

x_ph = tf.placeholder(tf.float32, [None], 'x_ph')
y_ph = tf.placeholder(tf.float32, [None], 'y_ph')

with tf.name_scope('input'):
    ds = tf.data.Dataset.from_tensor_slices((x_ph, y_ph))
    ds = ds.map(wrapper)
    ds = ds.batch(1)
    it = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)
    it_init_op = it.make_initializer(ds, name='it_init_op')

x_it, y_it = it.get_next()

# Simple operation
with tf.name_scope('add'):
    res = tf.add(x_it, y_it)

with tf.Session() as sess:
    sess.run([tf.global_variables_initializer(), it_init_op], feed_dict={y_ph: [10] * 10, x_ph: [i for i in range(10)]})
    sess.run([res])
    tf.saved_model.simple_save(sess, './dummy/test', {'x_ph': x_ph, 'y_ph': y_ph}, {'res': res})
</code></pre>

<p><strong>Load part:</strong></p>

<pre><code>graph = tf.Graph()
graph.as_default()
with tf.Session(graph=graph) as sess:
    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], './dummy/test')

    res = graph.get_tensor_by_name('add/Add:0')
    it_init_op = graph.get_operation_by_name('input/it_init_op')
    x_ph = graph.get_tensor_by_name('x_ph:0')
    y_ph = graph.get_tensor_by_name('y_ph:0')
    sess.run([it_init_op], feed_dict={x_ph: [5] * 5, y_ph: [i for i in range(5)]})

    for _ in range(5):
        sess.run([res])
</code></pre>

<p><strong>Error:</strong></p>

<blockquote>
  <p>ValueError: callback pyfunc_0 is not found</p>
</blockquote>

<p>It's well known that the function wrapped by the <code>tf.py_func()</code> isn't saved with the model. Does anybody has a solution to restore this by using the small hint given by the tf doc applying <code>tf.train.Server</code></p>
","After doing a research for restoring the tf.py_func() when using saved_model API in vain, I couldn't find other information than documented in tensorflow: Two save/load snippets help to illustrate the situation. Save part: Load part: Error: It's well known that the function wrapped by the tf.py_func() isn't saved with the model. Does anybody has a solution to restore this by using the small hint given by the tf doc applying tf.train.Server",https://stackoverflow.com/questions/55711355,9217178,Lack of Alternative Solutions/Documentation
55718702,How to correctly train with tf.keras.layers.BatchNormalization: Is there still a tf.GraphKeys.UPDATE_OPS dependency?,"<p>My goal is how to correctly train with batch normalizations layers in TensorFlow (TensorFlow version 1.13.1 for Python in Graph Mode) using the recommended tf.keras.layers.BatchNormalization class (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</a>).</p>

<p>An older recommended approach was to use tf.layers.batch_normalization.  The documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization</a>) indicates that it is currently deprecating instead in favor of tf.keras.layers.BatchNormalization.  </p>

<p>While using the older class, the documentation indicates we must explicitly add dependency on the mean and variance update operations, which would otherwise be dangling nodes outside from any dependencies in training operations:</p>

<pre><code>update_ops_including_from_batch_norms  =  tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
   my_optimizer = tf.super_cool_optimizer(loss)
</code></pre>

<p>My question:  Is this explicit dependence on UPDATE_OPS still needed when training batch norms in TF 1.13 with tf.keras.layers.BatchNormalization?  I don't see this mentioned in the documentation, however, I would be much more comfortable if someone knew for sure (and even better if can point to official documentation or code) that these operation dependencies are implicitly taken care of.</p>
","My goal is how to correctly train with batch normalizations layers in TensorFlow (TensorFlow version 1.13.1 for Python in Graph Mode) using the recommended tf.keras.layers.BatchNormalization class (https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization). An older recommended approach was to use tf.layers.batch_normalization. The documentation (https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) indicates that it is currently deprecating instead in favor of tf.keras.layers.BatchNormalization. While using the older class, the documentation indicates we must explicitly add dependency on the mean and variance update operations, which would otherwise be dangling nodes outside from any dependencies in training operations: My question: Is this explicit dependence on UPDATE_OPS still needed when training batch norms in TF 1.13 with tf.keras.layers.BatchNormalization? I don't see this mentioned in the documentation, however, I would be much more comfortable if someone knew for sure (and even better if can point to official documentation or code) that these operation dependencies are implicitly taken care of.",https://stackoverflow.com/questions/55718702,6367958,Lack of Alternative Solutions/Documentation
55778682,fix/freeze individual kernel weights in a convolutional operation,"<p>I'm trying out a workaround for fixing individual kernel weights in a convolutional operation in TensorFlow using Python 3.7. I do it by creating </p>

<ol>
<li>a trainable variable, </li>
<li>an identical non-trainable variable and </li>
<li>a ""mask"" tensor consisting of <strong>1</strong>s and <strong>0s</strong> with the same shape as the created variables in step 1 and 2 above.</li>
</ol>

<p>A <strong>1</strong> in the ""mask"" tensor indicates that I want to fix/freeze that specific weight during training, i.e. not update it in the backward pass.</p>

<p>Now, this workaround works perfectly fine when applied to a fully connected layer but fails when applied to a convolutional layer and I can't figure out why or how to make it work.</p>

<p>Something seems to be happening in the <strong>tf.nn.conv2d()</strong> function call (see code example below) and according to the documentation this is what they do:</p>

<blockquote>
  <p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code><br>
    and a filter / kernel tensor of shape<br>
   <code>[filter_height, filter_width, in_channels, out_channels]</code>, this op<br>
    performs the following:<br>
    1. Flattens the filter to a 2-D matrix with shape<br>
       <code>[filter_height * filter_width * in_channels, output_channels]</code>.<br>
    2. Extracts image patches from the input tensor to form a <em>virtual</em><br>
       tensor of shape <code>[batch, out_height, out_width,&lt;br&gt;
       filter_height * filter_width * in_channels]</code>.<br>
    3. For each patch, right-multiplies the filter matrix and the image patch<br>
       vector.</p>
</blockquote>

<p>But since I use <strong>weights_frozen</strong> which is a tensor and depends on the trainable variable, non-trainable variable and <strong>mask_weights</strong> it should get zero-valued gradients on the positions where I have a 1 in the <strong>mask_weights</strong> tensor.</p>

<pre class=""lang-py prettyprint-override""><code>def conv(input_, layer_name...):

    weights = tf.get_variable(shape=[filter_height, filter_width, in_channels, out_channels], dtype=tf.float32, initializer=tf.glorot_uniform_initializer(), trainable=True)

    weights_fixed = tf.Variable(tf.identity(weights), trainable=False)

    mask_weights = tf.placeholder(tf.float32, weights.shape)


    weights_frozen = tf.add(tf.multiply(mask_weights, weights_fixed), tf.multiply((1 - mask_weights), weights))


    out_conv = tf.nn.conv2d(input=input_, filter=weights_frozen, strides=strides_, padding='SAME')
    out_add = tf.nn.bias_add(value=out_conv, bias=biases_frozen)

    out = tf.nn.relu(features=out_add)

    return out
</code></pre>

<p>As mentioned, I expect to get zero-valued gradients on the positions where I have a <strong>1</strong> in the <strong>mask_weights</strong> tensor, but instead they are non-zero and therefore those weights are being trained, which is not the behavior I'm trying to achieve.</p>
","I'm trying out a workaround for fixing individual kernel weights in a convolutional operation in TensorFlow using Python 3.7. I do it by creating A 1 in the ""mask"" tensor indicates that I want to fix/freeze that specific weight during training, i.e. not update it in the backward pass. Now, this workaround works perfectly fine when applied to a fully connected layer but fails when applied to a convolutional layer and I can't figure out why or how to make it work. Something seems to be happening in the tf.nn.conv2d() function call (see code example below) and according to the documentation this is what they do: But since I use weights_frozen which is a tensor and depends on the trainable variable, non-trainable variable and mask_weights it should get zero-valued gradients on the positions where I have a 1 in the mask_weights tensor. As mentioned, I expect to get zero-valued gradients on the positions where I have a 1 in the mask_weights tensor, but instead they are non-zero and therefore those weights are being trained, which is not the behavior I'm trying to achieve.",https://stackoverflow.com/questions/55778682,9496160,Documentation Ambiguity
55788007,Unexpected results when using tfrecords loaded using tf.data.Dataset.list_files() with shuffle argument,"<p>I'm hoping to get clarification on how the <code>shuffle</code> argument in <code>tf.data.Dataset.list_files()</code> works. The documentation states that when <code>shuffle=True</code>, the filenames will be shuffled randomly. I've made model predictions using a tfrecords dataset that has been loaded using <code>tf.data.Dataset.list_files()</code>, and I would've expected the accuracy metric to be the same no matter the order of the files (i.e. whether shuffle is True or False), but am seeing otherwise. </p>

<p>Is this expected behavior or is there something wrong with my code or intepretation? I have reproducible example code below.</p>

<p>Oddly, as long as <code>tf.random.set_random_seed()</code> is set initially (and it seems it doesn't even matter what seed value is set), then the predictions results are the same no matter whether shuffle is True or False in <code>list_files()</code>.</p>

<p>tensorflow==1.13.1, keras==2.2.4</p>

<p>Thanks for any clarifications!</p>

<p>Edit: re-thinking it through and wondering if <code>Y = [y[0] for _ in range(steps) for y in sess.run(Y)]</code> is a separate and independent call?</p>

<pre><code># Fit and Save a Dummy Model
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import np_utils
from sklearn import datasets, metrics

seed = 7
np.random.seed(seed)
tf.random.set_random_seed(seed)

dataset = datasets.load_iris()

X = dataset.data
Y = dataset.target
dummy_Y = np_utils.to_categorical(Y)

# 150 rows
print(len(X))

model = Sequential()
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, dummy_Y, epochs=10, batch_size=10,  verbose=2)
model.save('./iris/iris_model')

predictions = model.predict(X)
predictions = np.argmax(predictions, axis=1)

# returns accuracy = 0.3466666666666667
print(metrics.accuracy_score(y_true=Y, y_pred=predictions))
</code></pre>

<p>Split dataset into multiple tfrecords files so we can reload it with list_files() later:</p>

<pre><code>numrows = 15
for i, j in enumerate(range(0, len(X), numrows)):
    with tf.python_io.TFRecordWriter('./iris/iris{}.tfrecord'.format(i)) as writer:
        for x, y in zip(X[j:j+numrows, ], Y[j:j+numrows, ]):
            features = tf.train.Features(feature=
                {'X': tf.train.Feature(float_list=tf.train.FloatList(value=x)), 
                'Y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y]))
                })
            example = tf.train.Example(features=features)
            writer.write(example.SerializeToString())
</code></pre>

<p>At this point, I exit (ipython) and restart again:</p>

<pre><code>import numpy as np
import tensorflow as tf
from keras.models import load_model
from sklearn import metrics

model = load_model('./iris/iris_model')

batch_size = 10
steps = int(150/batch_size)
file_pattern = './iris/iris*.tfrecord'

feature_description = {
    'X': tf.FixedLenFeature([4], tf.float32),
    'Y': tf.FixedLenFeature([1], tf.int64)
}

def _parse_function(example_proto):
    return tf.parse_single_example(example_proto, feature_description)

def load_data(filenames, batch_size):
    raw_dataset = tf.data.TFRecordDataset(filenames)
    dataset = raw_dataset.map(_parse_function)
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.prefetch(2)
    iterator = dataset.make_one_shot_iterator()
    record = iterator.get_next()
    return record['X'], record['Y']

def get_predictions_accuracy(filenames):
    X, Y = load_data(filenames=filenames, batch_size=batch_size)

    predictions = model.predict([X], steps=steps)
    predictions = np.argmax(predictions, axis=1)
    print(len(predictions))

    with tf.Session() as sess:
        Y = [y[0] for _ in range(steps) for y in sess.run(Y)]

    print(metrics.accuracy_score(y_true=Y, y_pred=predictions))
</code></pre>

<pre><code># No shuffle results:
# Returns expected accuracy = 0.3466666666666667
filenames_noshuffle = tf.data.Dataset.list_files(file_pattern=file_pattern, shuffle=False)
get_predictions_accuracy(filenames_noshuffle)
</code></pre>

<pre><code># Shuffle results, no seed value set:
# Returns UNEXPECTED accuracy (non-deterministic value)
filenames_shuffle_noseed = tf.data.Dataset.list_files(file_pattern=file_pattern, shuffle=True)
get_predictions_accuracy(filenames_shuffle_noseed)
</code></pre>

<pre><code># Shuffle results, seed value set:
# Returns expected accuracy = 0.3466666666666667
# It seems like it doesn't even matter what seed value you set, as long as you you set it
seed = 1000
tf.random.set_random_seed(seed)
filenames_shuffle_seed = tf.data.Dataset.list_files(file_pattern=file_pattern, shuffle=True)
get_predictions_accuracy(filenames_shuffle_seed)
</code></pre>
","I'm hoping to get clarification on how the shuffle argument in tf.data.Dataset.list_files() works. The documentation states that when shuffle=True, the filenames will be shuffled randomly. I've made model predictions using a tfrecords dataset that has been loaded using tf.data.Dataset.list_files(), and I would've expected the accuracy metric to be the same no matter the order of the files (i.e. whether shuffle is True or False), but am seeing otherwise. Is this expected behavior or is there something wrong with my code or intepretation? I have reproducible example code below. Oddly, as long as tf.random.set_random_seed() is set initially (and it seems it doesn't even matter what seed value is set), then the predictions results are the same no matter whether shuffle is True or False in list_files(). tensorflow==1.13.1, keras==2.2.4 Thanks for any clarifications! Edit: re-thinking it through and wondering if Y = [y[0] for _ in range(steps) for y in sess.run(Y)] is a separate and independent call? Split dataset into multiple tfrecords files so we can reload it with list_files() later: At this point, I exit (ipython) and restart again:",https://stackoverflow.com/questions/55788007,6921786,Documentation Replication on Other Examples
55868686,how to use eager execution to save and restore in TensorFlow？,"<p>We always use <code>tf.train.Saver()</code> to save and restore weights, like in <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">this</a> example. </p>

<p>But how to use eager execution to save? how to change the following example? </p>

<p>Another question, is it a good idea to use eager? </p>

<p>I found <code>tf.contrib.eager.Saver</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/eager/Saver"" rel=""nofollow noreferrer"">here</a>, but it says, </p>

<blockquote>
  <p>""Saver's name-based checkpointing strategy is fragile"". </p>
</blockquote>

<p>What does it mean?</p>

<pre><code># Create some variables.
v1 = tf.get_variable(""v1"", shape=[3], initializer = tf.zeros_initializer)
v2 = tf.get_variable(""v2"", shape=[5], initializer = tf.zeros_initializer)

inc_v1 = v1.assign(v1+1)
dec_v2 = v2.assign(v2-1)

# Add an op to initialize the variables.
init_op = tf.global_variables_initializer()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, initialize the variables, do some work, and save the
# variables to disk.
with tf.Session() as sess:
  sess.run(init_op)
  # Do some work with the model.
  inc_v1.op.run()
  dec_v2.op.run()
  # Save the variables to disk.
  save_path = saver.save(sess, ""/tmp/model.ckpt"")
  print(""Model saved in path: %s"" % save_path)
</code></pre>
","We always use tf.train.Saver() to save and restore weights, like in this example. But how to use eager execution to save? how to change the following example? Another question, is it a good idea to use eager? I found tf.contrib.eager.Saver here, but it says, What does it mean?",https://stackoverflow.com/questions/55868686,7241796,Documentation Replication on Other Examples
55904359,TypeError computing gradients with GradientTape.gradient,"<p>Hello,  </p>

<p>I'm currently trying to compute gradients in <strong>Tensorflow 1.13.1</strong> and using the <code>GradientTape</code> class as explained in the <a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">official documentation</a> , but I am getting a <code>TypeError: Fetch argument None has invalid type &lt;class 'NoneType'&gt;</code>.<br>
Below, I will include two simple cases where I get this error, using only out-of-the-box Tensorflow function, the first one being the simpler minimal working example, and the second one that I actually need to solve/get a work-around. For completeness, I am using <strong>Python 3.6.8</strong>.</p>

<h2>Simpler one</h2>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

tf.reset_default_graph()
x = tf.constant([1., 2., 3.])
with tf.GradientTape(persistent=True) as gg:
    gg.watch(x)
    f1 = tf.map_fn(lambda a: a**2, x)
    f2 = x*x

# Computes gradients
d_fx1 = gg.gradient(f1, x)     #Line that causes the error
d_fx2 = gg.gradient(f2, x)     #No error
del gg #delete persistent GradientTape

with tf.Session() as sess:
    d1, d2 = sess.run((d_fx1, d_fx2))
print(d1, d2)
</code></pre>

<p>In this code, <code>f1</code> and <code>f2</code> are computed in different ways, but give the same array. However, when trying to compute the gradients associated with them, the first line one gives the following error, whereas the second line works flawlessly. I report below the stack trace of the error</p>

<pre class=""lang-py prettyprint-override""><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-1-9c59a2cf2d9b&gt; in &lt;module&gt;()
     15 
     16 with tf.Session() as sess:
---&gt; 17     d1, d2 = sess.run((d_fx1, d_fx2))
     18 print(d1, d2)

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1135     # Create a fetch handler to take care of the structure of fetches.
   1136     fetch_handler = _FetchHandler(
-&gt; 1137         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
   1138 
   1139     # Run request and get response.

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in __init__(self, graph, fetches, feeds, feed_handles)
    469     """"""
    470     with graph.as_default():
--&gt; 471       self._fetch_mapper = _FetchMapper.for_fetch(fetches)
    472     self._fetches = []
    473     self._targets = []

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in for_fetch(fetch)
    259     elif isinstance(fetch, (list, tuple)):
    260       # NOTE(touts): This is also the code path for namedtuples.
--&gt; 261       return _ListFetchMapper(fetch)
    262     elif isinstance(fetch, collections.Mapping):
    263       return _DictFetchMapper(fetch)

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in __init__(self, fetches)
    368     """"""
    369     self._fetch_type = type(fetches)
--&gt; 370     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
    371     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    372 

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in &lt;listcomp&gt;(.0)
    368     """"""
    369     self._fetch_type = type(fetches)
--&gt; 370     self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
    371     self._unique_fetches, self._value_indices = _uniquify_fetches(self._mappers)
    372 

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in for_fetch(fetch)
    256     if fetch is None:
    257       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,
--&gt; 258                                                                  type(fetch)))
    259     elif isinstance(fetch, (list, tuple)):
    260       # NOTE(touts): This is also the code path for namedtuples.

TypeError: Fetch argument None has invalid type &lt;class 'NoneType'&gt;
</code></pre>

<p>Please note that I also tried computing only one gradient at a time, i.e with <code>persistent=False</code>, and got the same results.</p>

<h2>Actual need</h2>

<p>Below, I will include also the minimal working example to reproduce the same error I got, but trying to resolve the problem I am actually working on.</p>

<p>In this code, I'm using a <code>RNN</code> to compute an output w.r.t some inputs, and I need to compute the <code>jacobian</code> of this output w.r.t the inputs. </p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.keras.layers import RNN, GRUCell

# Define size of variable. TODO: adapt to data
inp_dim = 2
num_units = 50
batch_size = 100
timesteps = 10

# Reset the graph, so as to avoid errors
tf.reset_default_graph()

# Building the model
inputs = tf.ones(shape=(timesteps, batch_size, inp_dim))

# Follow gradient computations
with tf.GradientTape() as g:
    g.watch(inputs)
    cells = [GRUCell(num_units), GRUCell(num_units)]
    rnn = RNN(cells, time_major=True, return_sequences=True)
    f = rnn(inputs)
d_fx = g.batch_jacobian(f, inputs)

# Run graph
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    grads = sess.run(d_fx)
grads.shape
</code></pre>

<p>Regarding the stack trace, I get the same error but with less lines (there are one <code>for_fetch</code>, <code>&lt;listcomp&gt;</code> and <code>__init</code> less in this stack trace). For completeness, I still include it below</p>

<pre class=""lang-py prettyprint-override""><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-5-bb2ce4eebe87&gt; in &lt;module&gt;()
     25 with tf.Session() as sess:
     26     sess.run(tf.global_variables_initializer())
---&gt; 27     grads = sess.run(d_fx)
     28 grads.shape

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1135     # Create a fetch handler to take care of the structure of fetches.
   1136     fetch_handler = _FetchHandler(
-&gt; 1137         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
   1138 
   1139     # Run request and get response.

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in __init__(self, graph, fetches, feeds, feed_handles)
    469     """"""
    470     with graph.as_default():
--&gt; 471       self._fetch_mapper = _FetchMapper.for_fetch(fetches)
    472     self._fetches = []
    473     self._targets = []

C:\HOMEWARE\Miniconda3-Windows-x86_64\envs\rdwsenv\lib\site-packages\tensorflow\python\client\session.py in for_fetch(fetch)
    256     if fetch is None:
    257       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,
--&gt; 258                                                                  type(fetch)))
    259     elif isinstance(fetch, (list, tuple)):
    260       # NOTE(touts): This is also the code path for namedtuples.

TypeError: Fetch argument None has invalid type &lt;class 'NoneType'&gt;
</code></pre>

<p>I feel like there is a bug with some Tensorflow function that gets me the error, however I am not sure. At the end, what interest me is getting a <code>tensor</code> containing the <strong>jacobian</strong> of the output of my network w.r.t to the inputs. How can I achieve that using other tools, or correcting my code ?</p>

<p><strong>EDIT</strong>: Ok, so I took into account the comments by danyfang, and tried to look into the issue raised on Github he quoted about <code>tf.gradients</code> returning <code>None</code> instead of <code>0</code> due to some implementation design in low-level Tensorflow.</p>

<p>Therefore, I tried to create a simple case where I am sure that gradient are different from <code>0</code>, by computing <code>tf.matmul(tf.transpose(x), x)</code>. I am posting below a MWE.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

tf.reset_default_graph()
x = tf.constant([[1., 2., 3.]])
with tf.GradientTape(persistent=True) as gg:
    gg.watch(x)
    y = tf.matmul(x, tf.transpose(x))
    f1 = tf.map_fn(lambda a: a, y)

# Computes gradients
d_fx1 = gg.gradient(f1, x)
d_yx = gg.gradient(y, x)
del gg #delete persistent GradientTape

with tf.Session() as sess:
    #d1 = sess.run(d_fx1) # Same error None type
    d2 = sess.run(d_yx) #Works flawlessly. returns array([[2., 4., 6.]], dtype=float32)
d2
</code></pre>

<p>This shows (at least in my opinion) that the error arises not because of the behavior reported by this <a href=""https://github.com/tensorflow/tensorflow/issues/3972"" rel=""nofollow noreferrer"">issue</a>, but another thing due to lower level implementation.</p>
","Hello, I'm currently trying to compute gradients in Tensorflow 1.13.1 and using the GradientTape class as explained in the official documentation , but I am getting a TypeError: Fetch argument None has invalid type &lt;class 'NoneType'&gt;. Below, I will include two simple cases where I get this error, using only out-of-the-box Tensorflow function, the first one being the simpler minimal working example, and the second one that I actually need to solve/get a work-around. For completeness, I am using Python 3.6.8. In this code, f1 and f2 are computed in different ways, but give the same array. However, when trying to compute the gradients associated with them, the first line one gives the following error, whereas the second line works flawlessly. I report below the stack trace of the error Please note that I also tried computing only one gradient at a time, i.e with persistent=False, and got the same results. Below, I will include also the minimal working example to reproduce the same error I got, but trying to resolve the problem I am actually working on. In this code, I'm using a RNN to compute an output w.r.t some inputs, and I need to compute the jacobian of this output w.r.t the inputs. Regarding the stack trace, I get the same error but with less lines (there are one for_fetch, &lt;listcomp&gt; and __init less in this stack trace). For completeness, I still include it below I feel like there is a bug with some Tensorflow function that gets me the error, however I am not sure. At the end, what interest me is getting a tensor containing the jacobian of the output of my network w.r.t to the inputs. How can I achieve that using other tools, or correcting my code ? EDIT: Ok, so I took into account the comments by danyfang, and tried to look into the issue raised on Github he quoted about tf.gradients returning None instead of 0 due to some implementation design in low-level Tensorflow. Therefore, I tried to create a simple case where I am sure that gradient are different from 0, by computing tf.matmul(tf.transpose(x), x). I am posting below a MWE. This shows (at least in my opinion) that the error arises not because of the behavior reported by this issue, but another thing due to lower level implementation.",https://stackoverflow.com/questions/55904359,9092979,Documentation Replication on Other Examples
55909188,How can I apply a TensorFlow 2D Convolution (tf.nn.conv2d) to a single (non-batch) 2D image?,"<p>I would like to use the function <code>tf.nn.conv2d()</code> on a <strong>single</strong> image example, but the TensorFlow documentation seems to only mention applying this transformation to a <strong>batch</strong> of images. </p>

<p>The docs mention that the input image must be of shape <code>[batch, in_height, in_width, in_channels]</code> and the kernel must be of shape <code>[filter_height, filter_width, in_channels, out_channels]</code>. However, what is the most straightforward way to achieve 2D convolution with input shape <code>[in_height, in_width, in_channels]</code>?</p>

<p>Here is an example of the current approach, where <code>img</code> has shape (height, width, channels):</p>

<pre><code>img = tf.random_uniform((10,10,3))  # a single image
img = tf.nn.conv2d([img], kernel)[0] # creating a batch of 1, then indexing the single example
</code></pre>

<p>I am reshaping the input as follows:</p>

<p><code>[in_height, in_width, in_channels]-&gt;[1, in_height, in_width, in_channels]-&gt;[in_height, in_width, in_channels]</code> </p>

<p>This feels like an unnecessary and costly operation when I am only interested in transforming one example.</p>

<p>Is there a simple/standard way to do this that doesn't involve reshaping?</p>
","I would like to use the function tf.nn.conv2d() on a single image example, but the TensorFlow documentation seems to only mention applying this transformation to a batch of images. The docs mention that the input image must be of shape [batch, in_height, in_width, in_channels] and the kernel must be of shape [filter_height, filter_width, in_channels, out_channels]. However, what is the most straightforward way to achieve 2D convolution with input shape [in_height, in_width, in_channels]? Here is an example of the current approach, where img has shape (height, width, channels): I am reshaping the input as follows: [in_height, in_width, in_channels]-&gt;[1, in_height, in_width, in_channels]-&gt;[in_height, in_width, in_channels] This feels like an unnecessary and costly operation when I am only interested in transforming one example. Is there a simple/standard way to do this that doesn't involve reshaping?",https://stackoverflow.com/questions/55909188,9672143,Documentation Replication on Other Examples
55916743,How to get gradients with respect to input and change input (rather than trainable vars) to minimize loss in TF 2?,"<p>I want to use a trained model to change the input so it minimizes the loss (rather than changing the trainable variables) a la Deep Dreaming in Tensorflow 2.0 but I am not having success.</p>

<p>Say I have a basic NN as the one in the docs</p>

<pre><code>class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
    self.d2 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)

model = MyModel()

</code></pre>

<p>Which I train using a simple tf.GradientTape function</p>

<pre><code>@tf.function
def train_step(image, label):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
</code></pre>

<p>What's the idiomatic way to create a function that will instead calculate and apply the gradients to the input - images.</p>

<p>I assumed it will be as simple as</p>

<pre><code>def train_step(image, label):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  gradients = tape.gradient(loss, image)
  optimizer.apply_gradients(zip(gradients, image))
</code></pre>

<p>However, that doesn't work.</p>
","I want to use a trained model to change the input so it minimizes the loss (rather than changing the trainable variables) a la Deep Dreaming in Tensorflow 2.0 but I am not having success. Say I have a basic NN as the one in the docs Which I train using a simple tf.GradientTape function What's the idiomatic way to create a function that will instead calculate and apply the gradients to the input - images. I assumed it will be as simple as However, that doesn't work.",https://stackoverflow.com/questions/55916743,1540616,Documentation Replication on Other Examples
55936016,TensorFlow 2.0 clip_by_value with dynamic bounds,"<p>It is not clear from the TensorFlow documentation whether I can have dynamic range constraints imposed on a <code>tf.Variable</code> via <code>tf.clip_by_value</code>. From my testing it doesn't seem to work, but I would like to be sure, and if it isn't then I also would like to know how to achieve this (there are certain parts of my parameter space that cause NaNs in my loss function, and these constraints can only be described in terms of combinations of parameters).</p>

<p>Here is my test scenario:</p>

<pre><code>import tensorflow as tf
import tensorflow_probability as tfp
from tensorflow_probability import distributions as tfd
import numpy as np

N = 2
Ntrials = 100
x = [tf.Variable(np.zeros(Ntrials,dtype='float32'), name='x{0}'.format(i)) for i in range(N)]
eta = [tf.Variable(np.zeros(Ntrials,dtype='float32'),
  constraint=lambda z: tf.clip_by_value(z, -x[i] + 0.0001, np.infty), name='eta{0}'.format(i))
   for i in range(N)]
# i.e. x[i] + eta[i] should always be positive

cov = [[1.0, 0.0],[0.0,1.0]]
mvn0 = tfd.MultivariateNormalFullCovariance([1,1],cov)

# Get samples
samples0 = mvn0.sample(Ntrials) # Ntrials)

print(""samples.shape:"", samples0.shape)

# Get pdf values for samples
etax = None
def get_loss():
   global etax
   etax = [xi + etai for xi,etai in zip(x,eta)]
   # Need to stack eta variables for broadcasting in MultivariateNormalFullCovariance
   all_etax = tf.stack(etax, axis=1, name='all_eta')
   #print(""all_eta.shape:"", all_eta.shape)
   mvnfree = tfd.MultivariateNormalFullCovariance(all_etax,cov)
   p = mvnfree.log_prob(samples0)
   #print(""p:"",p)
   return -2*tf.math.reduce_sum(p,axis=0)

# Minimise
tol = 0.01 * N
delta_loss = 1e9
prev_loss = 1e9
i = 0
print(""tol:"", tol)
opt = tf.optimizers.SGD(0.1)
while delta_loss &gt; tol:
    opt.minimize(get_loss,var_list=eta+x)
    last_loss = get_loss()  
    delta_loss = np.abs(prev_loss -last_loss)
    print(""i:"", i, "" delta_loss:"", delta_loss)
    tf.print(""  x:"", x)
    tf.print(""  eta:"", eta)
    tf.print(""  etax:"", etax)
    i+=1
    prev_loss = last_loss

print(""Finished!"")
</code></pre>

<p>Output:</p>

<pre><code>samples.shape: (100, 2)
tol: 0.02
i: 0  delta_loss: 999999400.0
  x: [[0.186258137 0.269369602 0.520818532 ... 0.354108274 0.137650698 0.25976041], [-0.149198815 0.34362933 0.128372893 ... 0.391129225 0.239100441 0.264151126]]
  eta: [[0.186258137 0.269369602 0.520818532 ... 0.354108274 0.137650698 0.25976041], [0.0001 0.34362933 0.128372893 ... 0.391129225 0.239100441 0.264151126]]
  etax: [[0.372516274 0.538739204 1.04163706 ... 0.708216548 0.275301397 0.519520819], [-0.149098814 0.687258661 0.256745785 ... 0.782258451 0.478200883 0.528302252]]
i: 1  delta_loss: 111.583984
  x: [[0.298013031 0.430991352 0.83330965 ... 0.566573262 0.220241114 0.415616632], [-0.268577874 0.549806893 0.205396622 ... 0.625806808 0.3825607 0.422641814]]
  eta: [[0.298013031 0.430991352 0.83330965 ... 0.566573262 0.220241114 0.415616632], [0.149298817 0.549806893 0.205396622 ... 0.625806808 0.3825607 0.422641814]]
  etax: [[0.596026063 0.861982703 1.6666193 ... 1.13314652 0.440482229 0.831233263], [-0.119279057 1.09961379 0.410793245 ... 1.25161362 0.7651214 0.845283628]]
</code></pre>

<p>The constraint on <code>eta</code> should enforce that <code>x + eta</code> is positive, but already we see negative values appearing in the first two loops.</p>

<p>Or is this perhaps an issue of the order of updating of variables? For example in the minimise loop I guess it is important that the <code>x</code> variables get updated first so that the constraint on the <code>eta</code> variables gets calculated correctly? I guess there is no guarantee of that, and I need to tell TensorFlow to do this? </p>

<p>Edit: I attempted to manually clip the variables in the optimisation loop, like so:</p>

<pre><code>while delta_loss &gt; tol:
    opt.minimize(get_loss,var_list=eta+x)
    # Manually clip variables
    clip_op = [eta[i].assign(tf.clip_by_value(eta[i], -x[i] + 0.0001, np.infty)) for i in range(N)]
    ...
</code></pre>

<p>and it seems to kind of work in this case, but in other test cases it seems to make the minimiser go haywire. I guess because it fights with the minimiser about what values the variables should have. So I'm not sure that this is a good solution.</p>

<p>Edit 2: Well for my specific case I realised that I could solve my problem with math, i.e. by a change of variables I can make the problem not require any explicit constraints. Which is probably the best thing to do if it is possible. But it won't always be, so I am still curious how to do it with constraints.</p>
","It is not clear from the TensorFlow documentation whether I can have dynamic range constraints imposed on a tf.Variable via tf.clip_by_value. From my testing it doesn't seem to work, but I would like to be sure, and if it isn't then I also would like to know how to achieve this (there are certain parts of my parameter space that cause NaNs in my loss function, and these constraints can only be described in terms of combinations of parameters). Here is my test scenario: Output: The constraint on eta should enforce that x + eta is positive, but already we see negative values appearing in the first two loops. Or is this perhaps an issue of the order of updating of variables? For example in the minimise loop I guess it is important that the x variables get updated first so that the constraint on the eta variables gets calculated correctly? I guess there is no guarantee of that, and I need to tell TensorFlow to do this? Edit: I attempted to manually clip the variables in the optimisation loop, like so: and it seems to kind of work in this case, but in other test cases it seems to make the minimiser go haywire. I guess because it fights with the minimiser about what values the variables should have. So I'm not sure that this is a good solution. Edit 2: Well for my specific case I realised that I could solve my problem with math, i.e. by a change of variables I can make the problem not require any explicit constraints. Which is probably the best thing to do if it is possible. But it won't always be, so I am still curious how to do it with constraints.",https://stackoverflow.com/questions/55936016,1447953,Documentation Replication on Other Examples
55986982,What is the way to use Tensor flow 2.0 object in open cv2 python and why is it so circuitous?,"<p>I load an image using tensor flow api (2.0) like so : </p>

<pre><code>def load(image_file):
  image = tf.io.read_file(image_file)
  image = tf.image.decode_jpeg(image)
</code></pre>

<p>Now that I have this object, I want to show this image, I can simply use matplotlib.pyplot, and this works. </p>

<pre><code>plt.figure()
plt.imshow(re/255.0)
plt.show()
</code></pre>

<p>However attempting this with OpenCV2 is problematic from the start, most of the examples are from 1.0 with .eval() session based suggestion for numpy conversion. One way would be to first convert tensor flow object to numpy, here is the function to do that from API documentation :</p>

<pre><code>TensorFlow
API r2.0
TensorFlow Core 2.0a
Python
tf.make_ndarray
Create a numpy ndarray from a tensor.
</code></pre>

<p>I dont understand why this does not works and I get a number of errors while all I want is to do something simple and then use some open cv2 functions like remap, resize etc.:</p>

<blockquote>
  <p>File
  ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"",
  line 426, in <strong>call</strong>
      self._initialize(args, kwds, add_initializers_to=initializer_map)   File
  ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"",
  line 370, in _initialize
      *args, **kwds))   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"",
  line 1313, in _get_concrete_function_internal_garbage_collected
      graph_function, _, _ = self._maybe_define_function(args, kwargs)   File
  ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"",
  line 1580, in _maybe_define_function
      graph_function = self._create_graph_function(args, kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"",
  line 1512, in _create_graph_function
      capture_by_value=self._capture_by_value),   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\framework\func_graph.py"",
  line 694, in func_graph_from_py_func
      func_outputs = python_func(*func_args, **func_kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"",
  line 317, in wrapped_fn
      return weak_wrapped_fn().<strong>wrapped</strong>(*args, **kwds)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\framework\func_graph.py"",
  line 686, in wrapper
      ), args, kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\autograph\impl\api.py"",
  line 392, in converted_call
      result = converted_f(*effective_args, **kwargs)   File ""C:\Users\syeda\AppData\Local\Temp\tmpnahp3og4.py"", line 32, in
  tf__random_deform
      im2 = ag__.converted_call('make_ndarray', tf, ag__.ConversionOptions(recursive=True, verbose=0,
  strip_decorators=(tf.function, defun_9, ag__.convert,
  ag__.do_not_convert, ag__.converted_call), force_conversion=False,
  optional_features=(), internal_convert_user_code=True), (real_image,),
  {})   File
  ""C:\Python\Python37\lib\site-packages\tensorflow\python\autograph\impl\api.py"",
  line 267, in converted_call
      return _call_unconverted(f, args, kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\autograph\impl\api.py"",
  line 188, in _call_unconverted
      return f(*args, **kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\framework\tensor_util.py"",
  line 596, in MakeNdarray
      shape = [d.size for d in tensor.tensor_shape.dim] AttributeError: 'Tensor' object has no attribute 'tensor_shape'</p>
</blockquote>

<p><strong>Update 5/5/2018 :</strong> After searching more I found out that this has something to do with Tensorflow graph execution. 
I have a function </p>

<pre><code>def load_image_train(image_file):
  input_image, real_image = load(image_file)
 print(type(real_image))
  print(real_image.shape)
  some_image = Open CV operations like filtering, jitter etc performed on real_image
return some_image
</code></pre>

<p>This works nicely when called eagerly with .numpy() attribute, however when called like following code and when you try to inspect what real_image is and its type returns</p>

<blockquote>
  <p>class 'tensorflow.python.framework.ops.Tensor'   (None, None, None)</p>
</blockquote>

<p>Please advice.</p>

<pre><code># Input pipeline
train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')
train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.map(load_image_train,
                               num_parallel_calls=tf.data.experimental.AUTOTUNE)
train_dataset = train_dataset.batch(1)
</code></pre>

<p><strong>Update 5/5/2018 :</strong> I decided to do a preprocessing of the data so I don't have to worry about the using any opencv functionality during the load time of the data. However during training time I still want to do some openCV operations. Now as per the suggestion of @giser_yugang I tried using py_function, I wrap opencv operations in py_function and call that function in a wrapper tf.function. This wrapper tf.function I call in train step. However the output I get from this wrapper function is like so : </p>

<pre><code>class 'tensorflow.python.framework.ops.Tensor'
unknown
</code></pre>

<p>Then if I try to consume this tensor in the next train step operation I get a </p>

<pre><code>incompatible with the layer: its rank is undefined, but the layer requires a defined rank.
</code></pre>

<p>If I don't use this py_function wrapper in my train step and directly try the numpy operations using opencv I get another error </p>

<pre><code>AttributeError: 'Tensor' object has no attribute 'numpy'
</code></pre>

<p>I guess both ways you cant win !</p>
","I load an image using tensor flow api (2.0) like so : Now that I have this object, I want to show this image, I can simply use matplotlib.pyplot, and this works. However attempting this with OpenCV2 is problematic from the start, most of the examples are from 1.0 with .eval() session based suggestion for numpy conversion. One way would be to first convert tensor flow object to numpy, here is the function to do that from API documentation : I dont understand why this does not works and I get a number of errors while all I want is to do something simple and then use some open cv2 functions like remap, resize etc.: Update 5/5/2018 : After searching more I found out that this has something to do with Tensorflow graph execution. I have a function This works nicely when called eagerly with .numpy() attribute, however when called like following code and when you try to inspect what real_image is and its type returns Please advice. Update 5/5/2018 : I decided to do a preprocessing of the data so I don't have to worry about the using any opencv functionality during the load time of the data. However during training time I still want to do some openCV operations. Now as per the suggestion of @giser_yugang I tried using py_function, I wrap opencv operations in py_function and call that function in a wrapper tf.function. This wrapper tf.function I call in train step. However the output I get from this wrapper function is like so : Then if I try to consume this tensor in the next train step operation I get a If I don't use this py_function wrapper in my train step and directly try the numpy operations using opencv I get another error I guess both ways you cant win !",https://stackoverflow.com/questions/55986982,4930939,Documentation Ambiguity
56047272,Explicit vs implicit type definition in TensorFlow,"<p>I'm just beginning to learn TensorFlow. Quoting from the <a href=""https://www.tensorflow.org/guide/low_level_intro#graph"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Let's build a simple computational graph. The most basic operation is a constant. The Python function that builds the operation takes a tensor value as input. The resulting operation takes no inputs. When run, it outputs the value that was passed to the constructor. We can create two floating point constants a and b as follows:</p>
</blockquote>

<pre><code>a = tf.constant(3.0, dtype=tf.float32)
b = tf.constant(4.0) # also tf.float32 implicitly
total = a + b
print(a)
print(b)
print(total)
</code></pre>

<p>The second constant is implicitly typed as a float32. Is that based on the explicit typing of the first constant? And does that imply that the first <code>dtype</code> is required? <a href=""https://www.tensorflow.org/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">tf.constant documentation</a> would imply that it does not:</p>

<blockquote>
  <p>If the argument dtype is not specified, then the type is inferred from the type of <code>value</code>.</p>
</blockquote>

<p>But then it would be unnecessary to explicitly type the 3.0 constant above.</p>

<p>I'm just looking for some clarification on this, since, like I said, I'm just starting out.</p>
","I'm just beginning to learn TensorFlow. Quoting from the documentation: The second constant is implicitly typed as a float32. Is that based on the explicit typing of the first constant? And does that imply that the first dtype is required? tf.constant documentation would imply that it does not: But then it would be unnecessary to explicitly type the 3.0 constant above. I'm just looking for some clarification on this, since, like I said, I'm just starting out.",https://stackoverflow.com/questions/56047272,7287543,Documentation Replication on Other Examples
56166885,How to check evaluation auc after every epoch when using tf.estimator.EstimatorSpec?,"<p>I defined my model using tf.estimator.EstimatorSpec. I know it has train, evaluation and prediction modes. But I want to check some metric scores such as auc after every epoch. Does this API support it like keras?</p>
","I defined my model using tf.estimator.EstimatorSpec. I know it has train, evaluation and prediction modes. But I want to check some metric scores such as auc after every epoch. Does this API support it like keras?",https://stackoverflow.com/questions/56166885,7185861,Inadequate Examples
56212366,TensorFlow tf.data processing dev set after each epoch,"<pre class=""lang-py prettyprint-override""><code>batch_size = 2
x_dim = 2
m = 5
m_dev = 4
epochs = 2

# Toy data
X_train = np.random.randn(m, x_dim)
Y_train = np.random.randint(0, 5, size=m).reshape(-1, 1)
X_dev = np.random.randn(m_dev, x_dim)
Y_dev = np.random.randint(0, 5, size=m_dev).reshape(-1, 1)

X = tf.placeholder(X_train.dtype, shape=[None, x_dim], name='X')
Y = tf.placeholder(Y_train.dtype, shape=[None, 1], name='Y')

# Create two separate datasets
train_dataset = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size)
dev_dataset = tf.data.Dataset.from_tensor_slices((X, Y)).batch(X_dev.shape[0])

# Create a generic Iterator
iterator = tf.data.Iterator.from_structure(train_dataset.output_types,
                                           train_dataset.output_shapes)

# Create two init ops
train_init_op = iterator.make_initializer(train_dataset)
dev_init_op = iterator.make_initializer(dev_dataset)

next_data = iterator.get_next()

with tf.Session() as sess:
    for epoch in range(epochs):
        # Training data
        sess.run(train_init_op, feed_dict={X: X_train, Y: Y_train})
        while True:
            try:
                X_batch, Y_batch = sess.run(next_data)
                # process training data
            except tf.errors.OutOfRangeError:
                break

        # Epoch done: process the dev data
        sess.run(dev_init_op, feed_dict={X: X_dev, Y: Y_dev})
        X_dev_all, Y_dev_all = sess.run(next_data)
</code></pre>

<p>I am using <code>tf.data</code> with reinitializable iterator to handle training and dev set data. For each epoch, I initialize the training data set. <a href=""https://www.tensorflow.org/guide/datasets#creating_an_iterator"" rel=""nofollow noreferrer"">The official documentation</a> has similar structure. I think this is not efficient especially if the training set is large. </p>

<p>Some of the resources I found online has <code>sess.run(train_init_op, feed_dict={X: X_train, Y: Y_train})</code> before the for loop to avoid this issue. But then we can't process the dev set after each epoch; we can only process it after we are done iterating over <code>epochs</code> epochs.</p>

<p>Is there a way to efficiently process the dev set after each epoch?</p>
","I am using tf.data with reinitializable iterator to handle training and dev set data. For each epoch, I initialize the training data set. The official documentation has similar structure. I think this is not efficient especially if the training set is large. Some of the resources I found online has sess.run(train_init_op, feed_dict={X: X_train, Y: Y_train}) before the for loop to avoid this issue. But then we can't process the dev set after each epoch; we can only process it after we are done iterating over epochs epochs. Is there a way to efficiently process the dev set after each epoch?",https://stackoverflow.com/questions/56212366,6217326,Documentation Replication on Other Examples
56229730,Trouble with zero-padding inputs for Steered Convolution Layer,"<p>I'm using Tensorflow's new graphics library to apply a steered convolution to a series of meshes.  In many cases, you will have a series of meshes that are not the same size and you must zero-pad the smaller ones.  According to the documentation, the ""sizes"" argument of the graph_conv.feature_steered_convolution_layer function takes in an int tensor consisting of the number of non-padded elements of each mesh.  For some reason, when this argument is set something other than ""None"", I get a warning telling me that the sparse array used in the ""neighbors"" argument is being converted to a dense matrix.  This causes my program to run absurdly slowly.  </p>

<p>The issue seems to be tied to the way that it calculates gradients.  If the optimizer is commented out, the error does not come up.  </p>

<p>I read about a similar problem (link below) where the solution to the problem was to use tf.dynamic_partition rather than tf.gather.  However, the tf.gather functions, in this case are located within the graph_convolution library.  I attempted to make some edits in my copy of the library, but to no avail.</p>

<p><a href=""https://stackoverflow.com/questions/45882401/how-to-deal-with-userwarning-converting-sparse-indexedslices-to-a-dense-tensor/45917500#45917500"">How to deal with UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape</a></p>

<pre class=""lang-py prettyprint-override""><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from absl.testing import parameterized
import numpy as np
import tensorflow as tf

from tensorflow_graphics.nn.layer import graph_convolution as graph_conv

#Number of meshes
N = 2
#Number of spatial dimensions
d = 2

#################################
#Data consists of the vertices of two meshes.  The first mesh has 5 vertices and the second has 4.
    #Shape of data is (numberOfMeshes,maxNumberofVertices,numberofSpatialDimensions)

#An array containing the actual size of each non-padded mesh
sz = np.array([5,4],dtype=np.int64)
#The maximum number of vertices in a mesh
datav = 5

#Input placeholder for input data (vertices)
V0 = tf.placeholder(dtype=tf.float64,name=""V0"",shape=(N,datav,d)) 
#Input Placeholder for labels for classification (For now, I'm just using throw-away data as my labels)
L = tf.placeholder(shape=(N,5,1),dtype=tf.float64)
SZ = tf.placeholder(shape=(N),dtype=tf.int64)
#Input placeholder for the sparse array representing the adjacency matrix shape:(numberOfMeshes,datav,datav)
    #The warning is not raised if ""SZ"" is changed to ""None
adj_sp = tf.sparse_placeholder(shape=(SZ.shape[0],datav,datav),dtype=tf.float64,name='SPP')



#The steered graph convolution that is included in Tensorflow's new graphics package
output = graph_conv.feature_steered_convolution_layer(data=V0,neighbors=adj_sp,sizes=SZ,translation_invariant=False,num_weight_matrices=1,num_output_channels=1)

loss = tf.losses.softmax_cross_entropy(L,output, weights=1.0)
optimizer = tf.train.AdamOptimizer(learning_rate=.001).minimize(loss) #Warning not raised if this is commented out
</code></pre>

<p>When the above code is run, I get the following warning:</p>

<pre><code>C:\Python37\lib\site-packages\tensorflow\python\ops\gradients_impl.py:110: 
UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown 
shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
</code></pre>

<p>I am starting to think that this might have more to do with the library itself than with this piece of code.  I have referenced this posed in GitHub in case it requires updates (or additional documentation) to the library.
<a href=""https://github.com/tensorflow/graphics/issues/13"" rel=""nofollow noreferrer"">https://github.com/tensorflow/graphics/issues/13</a></p>
","I'm using Tensorflow's new graphics library to apply a steered convolution to a series of meshes. In many cases, you will have a series of meshes that are not the same size and you must zero-pad the smaller ones. According to the documentation, the ""sizes"" argument of the graph_conv.feature_steered_convolution_layer function takes in an int tensor consisting of the number of non-padded elements of each mesh. For some reason, when this argument is set something other than ""None"", I get a warning telling me that the sparse array used in the ""neighbors"" argument is being converted to a dense matrix. This causes my program to run absurdly slowly. The issue seems to be tied to the way that it calculates gradients. If the optimizer is commented out, the error does not come up. I read about a similar problem (link below) where the solution to the problem was to use tf.dynamic_partition rather than tf.gather. However, the tf.gather functions, in this case are located within the graph_convolution library. I attempted to make some edits in my copy of the library, but to no avail. How to deal with UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape When the above code is run, I get the following warning: I am starting to think that this might have more to do with the library itself than with this piece of code. I have referenced this posed in GitHub in case it requires updates (or additional documentation) to the library. https://github.com/tensorflow/graphics/issues/13",https://stackoverflow.com/questions/56229730,11506976,Requesting (Additional) Resources
56231695,When should tf.losses.add_loss() be used in TensorFlow?,"<p>I cannot find an answer to this question in the TensorFlow documentation. I once read that one should add losses from <code>tf.nn</code> functions but it isn't necessary for functions from <code>tf.losses</code>. Therefore:</p>

<p>When should I use <code>tf.losses.add_loss()</code>?</p>

<p>Example:</p>

<pre><code>loss = tf.reduce_mean(tf.nn.sparse_softmax_corss_entropy_with_logits
                       (labels=ground_truth, logits=predictions))

tf.losses.add_loss(loss) &lt;-- when is this required?
</code></pre>

<p>Thank yoou.</p>
",I cannot find an answer to this question in the TensorFlow documentation. I once read that one should add losses from tf.nn functions but it isn't necessary for functions from tf.losses. Therefore: When should I use tf.losses.add_loss()? Example: Thank yoou.,https://stackoverflow.com/questions/56231695,7353970,Lack of Alternative Solutions/Documentation
56284927,tf.keras equivalent code block written in tf.contrib.slim,"<p>I'm trying to re-implement a research paper code in tf.keras, in init block it was written as:</p>

<pre><code>with slim.arg_scope([slim.conv2d,separable_conv],activation_fn=tf.nn.relu6, normalizer_fn=slim.batch_norm):
    with slim.arg_scope([slim.batch_norm], is_training=is_training, activation_fn=None):
        with tf.variable_scope(name):
            net = slim.conv2d(inputs, num_outputs=depth, kernel_size=3, stride=2, scope=""conv"") #padding same
</code></pre>

<p>I didn't find a equivalent in tf.keras.layer.Conv2D arguments for normalizer_fn=slim.batch_norm. How to achieve this in keras?</p>

<p>I tried:</p>

<pre><code>model.add(Conv2D(""some arguments"") #0
model.add(BatchNormalization())
</code></pre>

<p>Is this a valid equivalent to the above tf.contrib.slim code. With limited documentation of tf.contrib.slim, I'm really confused.</p>
","I'm trying to re-implement a research paper code in tf.keras, in init block it was written as: I didn't find a equivalent in tf.keras.layer.Conv2D arguments for normalizer_fn=slim.batch_norm. How to achieve this in keras? I tried: Is this a valid equivalent to the above tf.contrib.slim code. With limited documentation of tf.contrib.slim, I'm really confused.",https://stackoverflow.com/questions/56284927,10544741,Documentation Replication on Other Examples
56286350,tf.keras.metrics.SpecificityAtSensitivity num_thresholds interpretation,"<p>I'm trying to get my head around <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/metrics/SensitivityAtSpecificity"" rel=""nofollow noreferrer"">tf.keras.metrics.SensitivityAtSpecificity</a>. I'm fine with the concept of sensity and specificity in isolation, but I'm unsure how the two are related in this single metric.</p>

<p>More specifically, I'm unsure how to interpret the <code>num_thresholds</code> argument. The example in documentation has <code>num_thresholds=1</code>. Setting <code>num_thresholds</code> greater than 1 with the same input data seems to always return a metric value of 1.0.</p>

<pre class=""lang-py prettyprint-override""><code>def print_metric_value(num_thresholds):
    # other values based on docs example
    m = tf.keras.metrics.SensitivityAtSpecificity(
        0.4, num_thresholds=num_thresholds)
    m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
    print('Result with num_thresholds = %d: %.1f' %
          (num_thresholds, m.result().numpy()))

print_metric_value(1)    # 0.5 - same as docs
print_metric_value(2)    # 1.0
print_metric_value(200)  # 1.0
</code></pre>
","I'm trying to get my head around tf.keras.metrics.SensitivityAtSpecificity. I'm fine with the concept of sensity and specificity in isolation, but I'm unsure how the two are related in this single metric. More specifically, I'm unsure how to interpret the num_thresholds argument. The example in documentation has num_thresholds=1. Setting num_thresholds greater than 1 with the same input data seems to always return a metric value of 1.0.",https://stackoverflow.com/questions/56286350,3098092,Documentation Replication on Other Examples
56344827,"in TF2, how do you save models/weights when not using the tf.keras API?","<p>In the documentation it seems they focus on how to save and restore tf.keras.models, but i was wondering how do you save and restore models trained customly through some basic iteration loop?</p>

<p>Now that there isnt a graph or a session, how do we save structure defined in a tf function that is customly built without using layer abstractions?</p>
","In the documentation it seems they focus on how to save and restore tf.keras.models, but i was wondering how do you save and restore models trained customly through some basic iteration loop? Now that there isnt a graph or a session, how do we save structure defined in a tf function that is customly built without using layer abstractions?",https://stackoverflow.com/questions/56344827,10038330,Lack of Alternative Solutions/Documentation
56386901,Example for tf. group_by_reducer?,"<p>Can someone show me an example of tf.data.experimental.group_by_reducer? I find the documentation tricky and couldn't understand fully.</p>

<p>How can I use it for calculating average?</p>
",Can someone show me an example of tf.data.experimental.group_by_reducer? I find the documentation tricky and couldn't understand fully. How can I use it for calculating average?,https://stackoverflow.com/questions/56386901,7418127,Documentation Replicability
56436701,tf.data.Dataset.window example from the documentation fails,"<p>I'm trying to use an example from the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window"" rel=""nofollow noreferrer"">TF documentation</a> for <code>tf.data.Dataset.window</code> and the example from the documentation is failing.</p>

<p>Code derived from the documentation:</p>

<pre><code>import tensorflow as tf

ds = tf.data.Dataset.range(7).window(2)
next_element = ds.make_one_shot_iterator().get_next()

with tf.Session() as sess:
    print(sess.run(next_element))
</code></pre>

<p>Produces this error (trace removed):</p>

<pre><code>TypeError: Can not convert a _VariantDataset into a Tensor or Operation.
During handling of the above exception, another exception occurred:
TypeError: Fetch argument &lt;_VariantDataset shapes: (), types: tf.int64&gt; has invalid type &lt;class 'tensorflow.python.data.ops.dataset_ops._VariantDataset'&gt;, must be a string or Tensor. (Can not convert a _VariantDataset into a Tensor or Operation.)
</code></pre>

<p>So <code>iterator.get_next()</code> is returning a <code>VariantDataset</code> rather than the usual tensor.</p>

<p><strong>TF Version: 1.13.1</strong></p>
",I'm trying to use an example from the TF documentation for tf.data.Dataset.window and the example from the documentation is failing. Code derived from the documentation: Produces this error (trace removed): So iterator.get_next() is returning a VariantDataset rather than the usual tensor. TF Version: 1.13.1,https://stackoverflow.com/questions/56436701,4790871,Documentation Replication on Other Examples
56458133,Tensorflow error when adding writing summaries 'Tensor' object has no attribute 'value',"<p>this is my code:</p>

<pre><code>import tensorflow as tf
import tensorboardcolab as tb
tbc = tb.TensorBoardColab()
writer=  tbc.get_writer()
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot = True)
n_nodes_hl1 = 500
n_nodes_hl2 = 500
n_nodes_hl3 = 500

n_classes = 10
batch_size = 100

tf.reset_default_graph()
x = tf.placeholder('float', [None, 784], name='input_data')
y = tf.placeholder('float', [None, 10], name='labels')
def neural_network_model(data):
  with tf.name_scope('feedforward'):
    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),
                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}

    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),
                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}

    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),
                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}

    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),
                    'biases':tf.Variable(tf.random_normal([n_classes])),}


    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])
    l1 = tf.nn.relu(l1)

    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])
    l2 = tf.nn.relu(l2)

    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])
    l3 = tf.nn.relu(l3)

    tf.summary.histogram(""l1"", l1)
    tf.summary.histogram(""l2"", l2)
    tf.summary.histogram(""l3"", l3)

    output = tf.add(tf.matmul(l3,output_layer['weights']),output_layer['biases'])
    return output

def train_neural_network(x):
    prediction = neural_network_model(x)
    with tf.name_scope('cost'):
      cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )
    optimizer = tf.train.AdamOptimizer().minimize(cost)
    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))

    hm_epochs = 10
    with tf.Session() as sess:
        sess.run(tf.initialize_all_variables())
        writer.add_graph(sess.graph)
        for epoch in range(hm_epochs):
            epoch_loss = 0
            for t in range(int(mnist.train.num_examples/batch_size)):
                epoch_x, epoch_y = mnist.train.next_batch(batch_size)
                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})
                epoch_loss += c
                if t%50 == 0:
                  val_acc = accuracy.eval({x:mnist.test.images, y:mnist.test.labels})
                  print('Accuracy:',val_acc)


            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)


#             acc_summ = tf.summary.scalar('val_acc', val_acc)
#             cost_summ = tf.summary.scalar('epoch_loss',epoch_loss)
#             merged = tf.summary.merge([cost_summ, acc_summ])
#             writer.add_summary(merged)

train_neural_network(x)
</code></pre>

<p>It runs fine when the commented parts are commented out. However, I don't get why when<code>writer.add_summary()</code> is uncommented out I get the following error:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-204-e776a1e8e4f1&gt; in &lt;module&gt;()
----&gt; 1 train_neural_network(x)

1 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py in add_summary(self, summary, global_step)
    125     # to save space - we just store the metadata on the first value with a
    126     # specific tag.
--&gt; 127     for value in summary.value:
    128       if not value.metadata:
    129         continue

AttributeError: 'Tensor' object has no attribute 'value'
</code></pre>

<p>It doesn't make sense to me. The docs <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter#close"" rel=""nofollow noreferrer"">for the FileWriter class</a> clearly states that it takes in a list of String tensors, and the docs for the <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/scalar"" rel=""nofollow noreferrer"">tf.summary.scalar</a> says that it returns a tensor of type string with summary protocol, so it seems like it should work.</p>

<p>Here's what I've tried, by people who seemed to have the same issue as me, but nothing worked:
<a href=""https://stackoverflow.com/questions/47030644/tensorboard-error-tensor-object-has-no-attribute-value"">Tensorboard expects a string for summary</a>, adding <code>eval()</code> didn't work.</p>

<p><a href=""https://stackoverflow.com/questions/51783265/in-add-summary-for-value-in-summary-value-attributeerror-tensor-object-has-n?rq=1"">The value error is raised because you have to evaluate the summary node within a session.</a> I am already running it in session.</p>

<p><a href=""https://stackoverflow.com/questions/35413618/tensorflow-placeholder-error-when-using-tf-merge-all-summaries"">Merges all summaries in the default graph</a>. Initially, I was using merging all summaries, but now I switched to <code>tf.summary.merge</code>.</p>

<p><a href=""https://github.com/keras-team/keras/issues/7362#issuecomment-341637550"" rel=""nofollow noreferrer"">Coming from improperly connected tensors</a>. I am using <code>tf.matmul</code> and <code>tf.add</code> here instead of regular operands.</p>

<p>Another thing I tried was putting tensor objects into the second argument of <code>tf.summary.scalar</code>, something like replacing the commented part of the code above like this:</p>

<pre><code>    cost_summ = tf.summary.scalar('epoch_loss',c)
    merged = tf.summary.merge([cost_summ])
    writer.add_summary(merged)
</code></pre>

<p>But event that didn't seem to work.</p>

<p>Why doesn't my above code work, while <a href=""https://stackoverflow.com/questions/49951902/in-tensorflow-is-it-possible-to-append-some-summaries-to-already-merged-summary"">this code here</a> works? I don't see a difference.</p>

<pre><code>import tensorflow as tf

a = tf.summary.scalar('a', tf.constant(0))
b = tf.summary.scalar('b', tf.constant(1))
c = tf.summary.scalar('c', tf.constant(2))
d = tf.summary.scalar('d', tf.constant(3))

ab = tf.summary.merge([a, b])
cd = tf.summary.merge([c, d])
abcd = tf.summary.merge([ab, cd])

with tf.Session() as sess:
    writer = tf.summary.FileWriter('.', sess.graph)
    summary = sess.run(abcd)
    writer.add_summary(summary)
</code></pre>

<p>Also, I am using Google Colab, which is why I'm importing tensorboardcolab. </p>
","this is my code: It runs fine when the commented parts are commented out. However, I don't get why whenwriter.add_summary() is uncommented out I get the following error: It doesn't make sense to me. The docs for the FileWriter class clearly states that it takes in a list of String tensors, and the docs for the tf.summary.scalar says that it returns a tensor of type string with summary protocol, so it seems like it should work. Here's what I've tried, by people who seemed to have the same issue as me, but nothing worked: Tensorboard expects a string for summary, adding eval() didn't work. The value error is raised because you have to evaluate the summary node within a session. I am already running it in session. Merges all summaries in the default graph. Initially, I was using merging all summaries, but now I switched to tf.summary.merge. Coming from improperly connected tensors. I am using tf.matmul and tf.add here instead of regular operands. Another thing I tried was putting tensor objects into the second argument of tf.summary.scalar, something like replacing the commented part of the code above like this: But event that didn't seem to work. Why doesn't my above code work, while this code here works? I don't see a difference. Also, I am using Google Colab, which is why I'm importing tensorboardcolab.",https://stackoverflow.com/questions/56458133,9721336,Documentation Ambiguity
56491633,What is the difference between tf.scatter_add and tf.scatter_nd when indices is a matrix?,"<p>Both <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_add"" rel=""nofollow noreferrer"">tf.scatter_add</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd</a> allow <code>indices</code> to be a matrix. It is clear from the documentation of tf.scatter_nd that the last dimension of <code>indices</code> contains values that are used to index a tensor of shape <code>shape</code>. The other dimensions of <code>indices</code> define the number of elements/slices to be scattered. Suppose <code>updates</code> has a rank <code>N</code>. First <code>k</code> dimensions of <code>indices</code> (except the last dimension) should match with first <code>k</code> dimensions of <code>updates</code>.  The last <code>(N-k)</code> dimensions of <code>updates</code> should match with the last <code>(N-k)</code> dimensions of <code>shape</code>.</p>

<p>This implies that <code>tf.scatter_nd</code> can be used to perform an <code>N</code>-dimensional scatter. However, <code>tf.scatter_add</code> also takes matrices as <code>indices</code>. But, its not clear which dimensions of <code>indices</code> correspond to the number of scatters to be performed and how do these dimensions align with <code>updates</code>. Can someone provide a clear explanation possibly with examples?</p>
","Both tf.scatter_add and tf.scatter_nd allow indices to be a matrix. It is clear from the documentation of tf.scatter_nd that the last dimension of indices contains values that are used to index a tensor of shape shape. The other dimensions of indices define the number of elements/slices to be scattered. Suppose updates has a rank N. First k dimensions of indices (except the last dimension) should match with first k dimensions of updates. The last (N-k) dimensions of updates should match with the last (N-k) dimensions of shape. This implies that tf.scatter_nd can be used to perform an N-dimensional scatter. However, tf.scatter_add also takes matrices as indices. But, its not clear which dimensions of indices correspond to the number of scatters to be performed and how do these dimensions align with updates. Can someone provide a clear explanation possibly with examples?",https://stackoverflow.com/questions/56491633,1313405,Inadequate Examples
56552397,Custom metric: Using scikit learn's AucRoc Calculator with tf.keras,"<p>I'm training a multilabel classifier using tf.keras and horovod that has 14 classes. AucRoc is used as the metric to evaluate the performance of the classifier. I want to be able to use scikit learn's AucRoc calculator as mentioned here: <a href=""https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras"">How to compute Receiving Operating Characteristic (ROC) and AUC in keras?</a>. If I feed the tensors as is for the following function:</p>

<pre><code>def sci_auc_roc(y_true, y_pred):
    return tf.py_func(roc_auc_score(y_true, y_pred), tf.double)
</code></pre>

<p>I get an error that looks like this: </p>

<pre><code>/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.
  warnings.warn('The output shape of `ResNet50(include_top=False)` '
Traceback (most recent call last):
  File ""official_resnet_tf_1.12.0_auc.py"", line 531, in &lt;module&gt;
    main()
  File ""official_resnet_tf_1.12.0_auc.py"", line 420, in main
    model = chexnet_model(FLAGS)
  File ""official_resnet_tf_1.12.0_auc.py"", line 375, in chexnet_model
    metrics=[tf_auc_roc,sci_auc_roc])
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 648, in compile
    sample_weights=self.sample_weights)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 313, in _handle_metrics
    output, output_mask))
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 270, in _handle_per_output_metrics
    y_true, y_pred, weights=weights, mask=mask)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 598, in weighted
    score_array = fn(y_true, y_pred)
  File ""official_resnet_tf_1.12.0_auc.py"", line 327, in sci_auc_roc
    return tf.py_func(roc_auc_score(y_true, y_pred), tf.double)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/sklearn/metrics/ranking.py"", line 349, in roc_auc_score
    y_type = type_of_target(y_true)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/sklearn/utils/multiclass.py"", line 243, in type_of_target
    'got %r' % y)
ValueError: Expected array-like (array or non-string sequence), got &lt;tf.Tensor 'dense_target:0' shape=(?, ?) dtype=float32&gt;

</code></pre>

<p>I'm trying to convert tf tensors into a numpy array and then feed them to the roc_auc_score method like so: </p>

<pre><code>def sci_auc_roc(y_true, y_pred):
    with tf.Session() as sess:
        y_true, y_pred = sess.run([y_true, y_pred])
    return tf.py_func(roc_auc_score(y_true, y_pred), tf.double)
</code></pre>

<p>I get the following error:</p>

<pre><code> warnings.warn('The output shape of `ResNet50(include_top=False)` '
Traceback (most recent call last):
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,256,256,3]
         [[{{node input_1}} = Placeholder[dtype=DT_FLOAT, shape=[?,256,256,3], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
         [[{{node dense_target/_5}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2237_dense_target"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""official_resnet_tf_1.12.0_auc.py"", line 531, in &lt;module&gt;
    main()
  File ""official_resnet_tf_1.12.0_auc.py"", line 420, in main
    model = chexnet_model(FLAGS)
  File ""official_resnet_tf_1.12.0_auc.py"", line 375, in chexnet_model
    metrics=[tf_auc_roc,sci_auc_roc])
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 648, in compile
    sample_weights=self.sample_weights)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 313, in _handle_metrics
    output, output_mask))
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 270, in _handle_per_output_metrics
    y_true, y_pred, weights=weights, mask=mask)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 598, in weighted
    score_array = fn(y_true, y_pred)
  File ""official_resnet_tf_1.12.0_auc.py"", line 324, in sci_auc_roc
    y_true, y_pred = sess.run([y_true, y_pred])
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,256,256,3]
         [[node input_1 (defined at /mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/keras_applications/resnet50.py:214)  = Placeholder[dtype=DT_FLOAT, shape=[?,256,256,3], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
         [[{{node dense_target/_5}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2237_dense_target"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op 'input_1', defined at:
  File ""official_resnet_tf_1.12.0_auc.py"", line 531, in &lt;module&gt;
    main()
  File ""official_resnet_tf_1.12.0_auc.py"", line 420, in main
    model = chexnet_model(FLAGS)
  File ""official_resnet_tf_1.12.0_auc.py"", line 339, in chexnet_model
    input_shape=(FLAGS.image_size, FLAGS.image_size, 3))
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/applications/__init__.py"", line 70, in wrapper
    return base_fun(*args, **kwargs)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/applications/resnet50.py"", line 32, in ResNet50
    return resnet50.ResNet50(*args, **kwargs)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/keras_applications/resnet50.py"", line 214, in ResNet50
    img_input = layers.Input(shape=input_shape)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_layer.py"", line 229, in Input
    input_tensor=tensor)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_layer.py"", line 112, in __init__
    name=self.name)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1747, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5206, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,256,256,3]
         [[node input_1 (defined at /mnt/lustrefs/rakvee/miniconda3/envs/docker_pip2/lib/python3.6/site-packages/keras_applications/resnet50.py:214)  = Placeholder[dtype=DT_FLOAT, shape=[?,256,256,3], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
         [[{{node dense_target/_5}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_2237_dense_target"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[52342,1],0]
  Exit code:    1
--------------------------------------------------------------------------
</code></pre>

<p>I've also tried tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/auc"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/metrics/auc</a> like so: </p>

<pre><code>def tf_auc_roc(y_true, y_pred):
    auc = tf.metrics.auc(y_true, y_pred)[1]
    K.get_session().run(tf.local_variables_initializer())
    return auc
</code></pre>

<p>It works just fine. However, it gives me a single number for aucroc. I wonder what that number represents, is it an average aucroc value for all the 14 classes? or max aucscores of all the classes? or how does it get to a single number?</p>

<pre><code>1216/1216 [==============================] - 413s 340ms/step - loss: 0.1513 - tf_auc_roc: 0.7944 - val_loss: 0.2212 - val_tf_auc_roc: 0.8074
Epoch 2/15
 582/1216 [=============&gt;................] - ETA: 3:16 - loss: 0.1459 - tf_auc_roc: 0.8053

</code></pre>

<p>1) How do I fix the error with roc_auc_score? </p>

<p>2) What does that single number represent?</p>
","I'm training a multilabel classifier using tf.keras and horovod that has 14 classes. AucRoc is used as the metric to evaluate the performance of the classifier. I want to be able to use scikit learn's AucRoc calculator as mentioned here: How to compute Receiving Operating Characteristic (ROC) and AUC in keras?. If I feed the tensors as is for the following function: I get an error that looks like this: I'm trying to convert tf tensors into a numpy array and then feed them to the roc_auc_score method like so: I get the following error: I've also tried tensorflow's https://www.tensorflow.org/api_docs/python/tf/metrics/auc like so: It works just fine. However, it gives me a single number for aucroc. I wonder what that number represents, is it an average aucroc value for all the 14 classes? or max aucscores of all the classes? or how does it get to a single number? 1) How do I fix the error with roc_auc_score? 2) What does that single number represent?",https://stackoverflow.com/questions/56552397,10287380,Lack of Alternative Solutions/Documentation
56553579,How to export Estimator's best model?,"<p>I am training a simple CNN based on a Custom Estimator with TF Records.
I am trying to export the best model in terms of validation loss during the <code>train_and_evaluate</code> phase. </p>

<p>According to the documentation of the <code>tf.estimator.BestExporter</code>, I should feed a function that returns a <code>ServingInputReceiver</code> but after doing so, the <code>train_and_evaluate</code> phase crashes with a <code>NotFoundError: model/m01/eval; No such file or directory</code>.</p>

<p>Seems like if the BestExporter does not permit saving the evaluation results as it would do without the exporter. I tried with different <code>ServingInputReceiver</code> but I keep getting the same error.</p>

<p>As defined <a href=""https://www.tensorflow.org/guide/saved_model#using_savedmodel_with_estimators"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>feature_spec = {
        'shape': tf.VarLenFeature(tf.int64),
        'image_raw': tf.FixedLenFeature((), tf.string),
        'label_raw': tf.FixedLenFeature((43), tf.int64)
    }

def serving_input_receiver_fn():
  serialized_tf_example = tf.placeholder(dtype=tf.string,
                                         shape=[120, 120, 3],
                                         name='input_example_tensor')
  receiver_tensors = {'image': serialized_tf_example}
  features = tf.parse_example(serialized_tf_example, feature_spec)
  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)
</code></pre>

<p>and <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/BestExporter#__init__"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>def serving_input_receiver_fn():
    feature_spec = {
            'image': tf.FixedLenFeature((), tf.string)
        }
    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)
</code></pre>

<p>Here are my exporter and training procedure:</p>

<pre><code>exporter = tf.estimator.BestExporter(
    name=""best_exporter"",
    serving_input_receiver_fn=serving_input_receiver_fn,
    exports_to_keep=5)

train_spec = tf.estimator.TrainSpec(
    input_fn=lambda: imgs_input_fn(train_path, True, epochs, batch_size))

eval_spec = tf.estimator.EvalSpec(
    input_fn=lambda: imgs_input_fn(eval_path, perform_shuffle=False, batch_size=1),
    exporters=exporter)

tf.estimator.train_and_evaluate(ben_classifier, train_spec, eval_spec)
</code></pre>

<p><a href=""https://gist.github.com/hichameyessou/f2710391066f6ed5786693892ac93dbe"" rel=""nofollow noreferrer"">This is a gist</a> with the output.
What's the correct way to define a <code>ServingInputReceiver</code> for the <code>BestExporter</code>?</p>
","I am training a simple CNN based on a Custom Estimator with TF Records. I am trying to export the best model in terms of validation loss during the train_and_evaluate phase. According to the documentation of the tf.estimator.BestExporter, I should feed a function that returns a ServingInputReceiver but after doing so, the train_and_evaluate phase crashes with a NotFoundError: model/m01/eval; No such file or directory. Seems like if the BestExporter does not permit saving the evaluation results as it would do without the exporter. I tried with different ServingInputReceiver but I keep getting the same error. As defined here: and here Here are my exporter and training procedure: This is a gist with the output. What's the correct way to define a ServingInputReceiver for the BestExporter?",https://stackoverflow.com/questions/56553579,3674176,Documentation Ambiguity
56606757,Tensorflow: output of multi-step decay function returns a TypeError,"<p>We are trying to write a multi-step decay function in Tensorflow using tf.train.piecewise_constant() as suggested <a href=""https://stackoverflow.com/a/47174243/5079359"">here</a>. Tensorflow documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/train/piecewise_constant_decay"" rel=""nofollow noreferrer"">here</a> states that:</p>

<p>""When eager execution is enabled, this function returns a function which in turn returns the decayed learning rate Tensor""</p>

<p>However, when we tried running the code, it returned a  TypeError. 
It returns the same error even when lr() is used.</p>

<pre><code>import tensorflow as tf
tf.enable_eager_execution()
import numpy as np

def conv3x3(out_planes, data_format ='channels_last',  stride=1, padding='same', dilation=1, name = None,use_bias = False):
    """"""3x3 convolution with padding""""""
    return  tf.keras.layers.Conv2D(filters = out_planes, kernel_size = 3,data_format= data_format,
                                   strides=(stride, stride), padding='same', use_bias=use_bias,
                                   dilation_rate = (dilation,dilation) , kernel_initializer=tf.initializers.he_normal(),name = name)


def conv1x1(out_planes,data_format ='channels_last', padding = 'same', stride=1):
    """"""1x1 convolution""""""
    return tf.keras.layers.Conv2D(filters = out_planes, kernel_size = 1, strides=(stride, stride),data_format= data_format,
                                  padding=padding, use_bias=False, kernel_initializer=tf.initializers.he_normal())

class BasicBlock(tf.keras.Model):
    expansion = 1

    def __init__(self, planes=1, stride=1, data_format= 'channels_last', downsample=None,  dilation=(1, 1), residual=True, key=None, stage = None):
        super(BasicBlock, self).__init__()
        self.data_format = data_format
        bn_axis = 1 if self.data_format == 'channels_first' else 3
        self.conv1 = conv3x3(out_planes= planes, stride = stride, padding='same' ,
                             data_format = self.data_format, dilation=dilation[0], name = '{}_{}_conv0'.format(key,stage))

        self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis, name = '{}_{}_BN0'.format(key,stage))

        self.conv2 = conv3x3(out_planes =planes, padding='same',
                             data_format = self.data_format, dilation=dilation[0],name = '{}_{}_conv1'.format(key,stage))

        self.bn2 = tf.keras.layers.BatchNormalization(axis=bn_axis,name = '{}_{}_BN1'.format(key,stage))

        self.downsample = downsample
        self.relu = tf.keras.layers.ReLU(name = '{}_{}_Relu'.format(key,stage))
        self.stride = stride
        self.residual = residual

    def get_config(self):
        base_config = {}
        base_config['conv1'] = self.conv1.get_config()
        base_config['bn1'] = self.bn1.get_config()
        base_config['conv2'] = self.conv2.get_config()
        base_config['bn2'] = self.bn2.get_config()
        if self.downsample is not None:
            base_config['downsample'] = self.downsample.get_config()
        return base_config


    def call(self, inputs, training=None):
        residual = inputs
        out = self.conv1(inputs)
        out = self.bn1(out,training = training)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(inputs)
        if self.residual:
            out += residual
        out = self.relu(out)
        return out


class Bottleneck(tf.keras.Model):
    expansion = 4

    def __init__(self, planes, stride=1, data_format = 'channels_last',downsample=None,dilation=(1, 1)):
        super(Bottleneck, self).__init__()

        bn_axis = 1 if data_format == 'channels_first' else 3
        self.conv1 = conv1x1(planes, data_format = data_format)
        self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis)
        self.relu = tf.keras.layers.ReLU()
        self.conv2 = conv3x3(planes, stride, padding= 'same', bias=False,  data_format = data_format, dilation=dilation[1])
        self.bn2 = tf.keras.layers.BatchNormalization(axis=bn_axis)
        self.conv3 =conv1x1( planes * 4, data_format = data_format, )
        self.bn3 =  tf.keras.layers.BatchNormalization(axis=bn_axis) # nn.BatchNorm2d(planes * self.expansion)
        self.downsample = downsample
        self.stride = stride

    def get_config(self):
        base_config = {}
        base_config['conv1'] = self.conv1.get_config()
        base_config['bn1'] = self.bn1.get_config()
        base_config['conv2'] = self.conv2.get_config()
        base_config['bn2'] = self.bn2.get_config()
        base_config['conv3'] = self.conv3.get_config()
        base_config['bn3'] = self.bn3.get_config()
        if self.downsample is not None:
            base_config['downsample'] = self.downsample.get_config()
        return base_config



    def call(self, inputs, training=None):
        identity = inputs
        out = self.conv1(inputs)
        out = self.bn1(out,training = training)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out,training = training)
        out = tf.nn.relu(out)
        out = self.conv3(out)
        out = self.bn3(out,training = training)
        if self.downsample is not None:
            identity = self.downsample(inputs)
        out += identity
        out = self.relu(out)
        return out

class pooling (tf.keras.Model):
    def __init__(self, pool_size, stride = None, data_format='channels_last'):
        super(pooling, self).__init__()
        self.pool_size = pool_size
        self.data_format = data_format
        if stride is None:
            self.stride =self.pool_size
        else:
            self.stride = stride


    def call(self, inputs):
        return tf.layers.average_pooling2d(inputs, strides =self.stride, pool_size = self.pool_size, data_format = self.data_format)


class DRN(tf.keras.Model):
    def __init__(self, block, layers, data_format='channels_last', num_classes=7,channels=(16, 32, 64, 128, 256, 512, 512, 512),
                 out_map=False, out_middle=False, pool_size=28, arch='D'):
        super(DRN, self).__init__()
        self.inplanes = channels[0]
        self.out_map = out_map
        self.out_dim = channels[-1]
        self.out_middle = out_middle
        self.arch = arch
        self.poolsize = pool_size
        self.data_format = data_format
        self.bn_axis = 1 if data_format == 'channels_first' else 3

        self.conv0 = tf.keras.layers.Conv2D(filters=channels[0], kernel_size=7, strides=1,  padding='same',
                                               use_bias=False, data_format = self.data_format, kernel_initializer=tf.initializers.he_normal(), name ='L0_conv0' )
        self.bn0 = tf.keras.layers.BatchNormalization(axis=self.bn_axis,name ='L0_BN0')
        self.relu0 = tf.keras.layers.ReLU(name ='L0_Relu0')


        if arch == 'C':
            self.layer1 = self._make_layer(block = BasicBlock, planes = channels[0], blocks = layers[0], stride=1, data_format = self.data_format, key='CL1')
            self.layer2 = self._make_layer(block = BasicBlock, planes =  channels[1], blocks = layers[1], stride=2, data_format = self.data_format, key='CL2')
        elif arch == 'D':
            self.layer1 = self._make_conv_layers(channels = channels[0],convs = layers[0], stride=1, data_format = self.data_format, key='DL1')
            self.layer2 = self._make_conv_layers(channels = channels[1],convs = layers[1], stride=2, data_format = self.data_format, key='DL2')


        self.layer3 = self._make_layer(block = block, planes = channels[2], blocks = layers[2], stride=2, data_format = self.data_format, key='L3')
        self.layer4 = self._make_layer(block = block, planes = channels[3], blocks = layers[3], stride=2, data_format = self.data_format, key='L4')
        self.layer5 = self._make_layer(block = block, planes = channels[4], blocks = layers[4], dilation=2, new_level=False, data_format = self.data_format, key='L5')
        self.layer6 = None if layers[5] == 0 else self._make_layer(block, channels[5], layers[5], dilation=4, new_level=False, data_format = self.data_format, key='L6')

        if arch == 'C':
            self.layer7 = None if layers[6] == 0 else self._make_layer(BasicBlock, channels[6], layers[6], dilation=2, new_level=False, residual=False, data_format = self.data_format, key='CL7')
            self.layer8 = None if layers[7] == 0 else self._make_layer(BasicBlock, channels[7], layers[7], dilation=1, new_level=False, residual=False, data_format = self.data_format, key='CL8')
        elif arch == 'D':
            self.layer7 = None if layers[6] == 0 else self._make_conv_layers(channels[6], layers[6], dilation=2, data_format = self.data_format, key='DL7')
            self.layer8 = None if layers[7] == 0 else self._make_conv_layers(channels[7], layers[7], dilation=1, data_format = self.data_format, key='DL8')

        if num_classes &gt; 0:
            self.avgpool = tf.keras.layers.GlobalAveragePooling2D(data_format = self.data_format)
            self.fc = tf.keras.layers.Dense(units=num_classes)


    def _make_layer(self, block, planes, blocks, stride=1,dilation=1, new_level=True, data_format = 'channels_last', residual=True, key=None):
        assert dilation == 1 or dilation % 2 == 0
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = tf.keras.Sequential([conv1x1(out_planes = planes * block.expansion,stride = stride, data_format = data_format),
                      tf.keras.layers.BatchNormalization(axis=self.bn_axis)], name = 'downsample')

#
        layers = []
        layers.append(block(planes= planes, stride =  stride, downsample = downsample, dilation=(1, 1) if dilation == 1 else (
                dilation // 2 if new_level else dilation, dilation), data_format=data_format, residual=residual, key = key, stage = '0'))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(planes, residual=residual,dilation=(dilation, dilation), data_format=data_format, key = key, stage = i))
        return tf.keras.Sequential(layers, name = key)


    def _make_conv_layers(self, channels, convs, stride=1, dilation=1 ,data_format = 'channels_last', key = None):
        modules = []
        for i in range(convs):
            modules.extend([
                conv3x3(out_planes= channels, stride=stride if i == 0 else 1,
                          padding= 'same' , use_bias=False, dilation=dilation,  data_format = data_format,name ='{}_{}_Conv'.format(key,i)),
                tf.keras.layers.BatchNormalization(axis=self.bn_axis,name ='{}_{}_BN'.format(key,i)),
                tf.keras.layers.ReLU(name ='{}_{}_Relu'.format(key,i))])
            self.inplanes = channels
        return tf.keras.Sequential(modules,name=key)


    def call(self, x, training=None):
        x = self.conv0(x)
        x = self.bn0(x,training = training)
        x = self.relu0(x)
        x = self.layer1(x,training = training)
        x = self.layer2(x,training = training)
        x = self.layer3(x,training = training)
        x = self.layer4(x,training = training)
        x = self.layer5(x,training = training)

        if self.layer6 is not None:
            x = self.layer6(x,training = training)

        if self.layer7 is not None:
            x = self.layer7(x)
        if self.layer8 is not None:
            x = self.layer8(x)
        if self.out_map:
            x = self.fc(x)
        else:
            x = self.avgpool(x)
            x = self.fc(x)
        return x

def loss(logits, labels):
  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))

def make_scheduler(policy, init_lr, n_step_epoch, global_step):
    total_steps= n_step_epoch * 10 #10 epochs
    milestones = policy.split('_')
    milestones.pop(0)
    milestones = list(map(lambda x: int(x), milestones))
    boundaries = np.multiply(milestones,n_step_epoch)
    values = [init_lr] + [init_lr/(0.1**-i) for i in  range(1,len(milestones)+1)]
    learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)
    return learning_rate


def train(model, optimizer, step_counter ):
  """"""Trains model on `dataset` using `optimizer`.""""""

  for (batch, i) in enumerate(range(10)):
      print('Training Loop {}'.format(i))
      images = tf.random.uniform((4, 224, 224,3))
      labels = tf.constant(np.random.randint(4, size=4))
      with tf.contrib.summary.record_summaries_every_n_global_steps(10, global_step=step_counter):
          with tf.GradientTape() as tape:
            logits = model(images, training=True)
            loss_value = loss(logits, labels)
          grads = tape.gradient(loss_value, model.variables)
          optimizer.apply_gradients(zip(grads, model.variables), global_step=step_counter)


def test(model):
  """"""Perform an evaluation of `model` on the examples from `dataset`.""""""
  for  i in (range(10)):
    images = tf.random.uniform((4, 225, 225,3))
    logits = model(images, training=False)
    print(logits)

def main():
    model =  DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='C',num_classes = 4)
    device = '/gpu:0'
    step_counter = tf.train.get_or_create_global_step()
    lr = make_scheduler(policy='multistep_2_5',init_lr=0.1,n_step_epoch = 10,global_step= step_counter)
    optimizer = tf.train.MomentumOptimizer(lr,momentum=0.5)

    with tf.device(device):
        for _ in range(10):
           train(model, optimizer,step_counter)
           print(optimizer._lr_t)
           test(model)

if __name__ == '__main__':
  main()

</code></pre>

<blockquote>
  <p>File """", line 1, in 
      runfile('/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py', wdir='/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug')</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"", line 709, in runfile
      execfile(filename, namespace)</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"", line 108, in execfile
      exec(compile(f.read(), filename, 'exec'), namespace)</p>
  
  <p>File ""/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py"", line 311, in 
      main()</p>
  
  <p>File ""/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py"", line 305, in main
      train(model, optimizer,step_counter)</p>
  
  <p>File ""/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py"", line 284, in train
      optimizer.apply_gradients(zip(grads, model.variables), global_step=step_counter)</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py"", line 598, in apply_gradients
      self._prepare()</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/momentum.py"", line 87, in _prepare
      learning_rate = learning_rate()</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/learning_rate_decay_v2.py"", line 171, in decayed_lr
      boundaries = ops.convert_n_to_tensor(boundaries)</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1273, in convert_n_to_tensor
      as_ref=False)</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in internal_convert_n_to_tensor
      raise TypeError(""values must be a list."")</p>
  
  <p>TypeError: values must be a list.</p>
</blockquote>

<p>The code works as expected when we provide a constant learning rate. Is there something that we are missing?</p>
","We are trying to write a multi-step decay function in Tensorflow using tf.train.piecewise_constant() as suggested here. Tensorflow documentation here states that: ""When eager execution is enabled, this function returns a function which in turn returns the decayed learning rate Tensor"" However, when we tried running the code, it returned a TypeError. It returns the same error even when lr() is used. The code works as expected when we provide a constant learning rate. Is there something that we are missing?",https://stackoverflow.com/questions/56606757,5079359,Documentation Ambiguity
56635027,Feeding array (shape with rank 1) to TensorFlow tf.case,"<p>Following this example from the <code>tf.case</code> documentation:</p>

<pre><code>def f1(): return tf.constant(17)
def f2(): return tf.constant(23)
def f3(): return tf.constant(-1)
r = tf.case({tf.less(x, y): f1, tf.greater(x, z): f2},
            default=f3, exclusive=True)
</code></pre>

<p>I want to do the same, but allow to use a feed_dict as input, illustrated by this snipped:</p>

<pre><code>x = tf.placeholder(tf.float32, shape=[None])
y = tf.placeholder(tf.float32, shape=[None])
z = tf.placeholder(tf.float32, shape=[None])
def f1(): return tf.constant(17)
def f2(): return tf.constant(23)
def f3(): return tf.constant(-1)
r = tf.case({tf.less(x, y): f1, tf.greater(x, z): f2},
            default=f3, exclusive=True)
print(sess.run(r, feed_dict={x: [0, 1, 2, 3], y: [1, 1, 1, 1], z: [2, 2, 2, 2]}))
# result should be [17, -1, -1, 23]
</code></pre>

<p>So, basically I want to feed three <code>int</code>-arrays of equal length and receive an array of <code>int</code>-values containing either 17, 23, or -1. Unfortunately, there code above gives and error:</p>

<blockquote>
  <p>ValueError: Shape must be rank 0 but is rank 1 for 'case/cond/Switch' (op: 'Switch') with input shapes: [?], [?].</p>
</blockquote>

<p>I understand, that <code>tf.case</code> requires boolean scalar tensor input values but is there any way to achieve what I want? I also tried <code>tf.cond</code> without success.</p>
","Following this example from the tf.case documentation: I want to do the same, but allow to use a feed_dict as input, illustrated by this snipped: So, basically I want to feed three int-arrays of equal length and receive an array of int-values containing either 17, 23, or -1. Unfortunately, there code above gives and error: I understand, that tf.case requires boolean scalar tensor input values but is there any way to achieve what I want? I also tried tf.cond without success.",https://stackoverflow.com/questions/56635027,11545782,Documentation Replicability
56693863,Why does model.losses return regularization losses?,"<p>I have met a snippet of code of tensorflow 2.0, which is used for calculating the loss. The total loss is composed of two parts: 1) regularization loss, 2) prediction loss. My question is why <code>model.losses</code> is regularization loss? <code>model</code> here is an instance of <code>tf.keras.Model</code>. I'm kind of confused by the tensorflow official API documentation. <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#losses"" rel=""noreferrer"">tf.keras.Model</a>, it says</p>
<blockquote>
<p>Losses which are associated with this Layer.</p>
<p>Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a <code>tf.GradientTape</code> will propagate gradients back to the corresponding variables.</p>
</blockquote>
<p>Why could we get regularization loss via accessing <code>losses</code> property? Also, what is eager safe? If <code>losses</code> property is returning regularization loss, why is it named <code>losses</code> instead of <code>regularization_loss</code>?</p>
<pre class=""lang-py prettyprint-override""><code>with tf.GradientTape() as tape:
  outputs = model(images, training=True)
  regularization_loss = tf.reduce_sum(model.losses)
  pred_loss = ...
  total_loss = pred_loss + regularization_loss
</code></pre>
","I have met a snippet of code of tensorflow 2.0, which is used for calculating the loss. The total loss is composed of two parts: 1) regularization loss, 2) prediction loss. My question is why model.losses is regularization loss? model here is an instance of tf.keras.Model. I'm kind of confused by the tensorflow official API documentation. tf.keras.Model, it says Why could we get regularization loss via accessing losses property? Also, what is eager safe? If losses property is returning regularization loss, why is it named losses instead of regularization_loss?",https://stackoverflow.com/questions/56693863,7212365,Documentation Ambiguity
56754293,TensorflowServing on a trained native Keras model with a preprocessing function for the input,"<p>My final goal is to use the What-If-Tool on tensorboard. In order to do that, I need to serve my Keras model on TensorflowServing, and the data in a TFRecordFile. So the data has to be transformed into tf.Examples.
The tool is supposed to grab the network to run inference on the data.
however, the network cannot handle tf.Examples as an input. So the served model needs to have a preprocessing function.</p>

<p>According to the tensorflow documentation, one way is to create a tensorflow Estimator, and to use ""serving_input_receiver_fn"" to preprocess the data. 
This would have been perfect except for the case that I can't make an already trained native Keras model into an Estimator. It seems that the only way it to create it from a tf.keras model (and not a native keras model like I have), and to train it directly with the estimator.</p>

<p>Another way would be to use the tf.saved_model.simple_save function, and then use TensorflowServing, but I did not find a way to preprocess the tf.Examples to make a correct input for the network.</p>

<p>Since this is not working, I have no clue on how to resolve this.</p>

<p><strong>Edit:</strong> I tried to transform my native keras into a tf.keras model. My model is really big, so I build this function: </p>

<pre><code>def create_tf_keras_model_from_native_keras(native_model):
    list_layers = []
    for i, layer in enumerate(native_model.layers):
        type_layer = str(layer).split('.')[2]
        second_type_layer = str(layer).split('.')[3].split(' ')[0]
        if type_layer == 'input_layer':
            new_layer = tf.keras.layers.InputLayer(**layer.get_config())
        elif type_layer == 'convolutional':
            new_layer = tf.keras.layers.Conv2D(**layer.get_config())
        elif type_layer == 'normalization':
            new_layer = tf.keras.layers.BatchNormalization(**layer.get_config())
        elif type_layer == 'core':
            if second_type_layer == 'Activation':
                new_layer = tf.keras.layers.Activation(**layer.get_config())
            elif second_type_layer == 'Dense':
                new_layer = tf.keras.layers.Dense(**layer.get_config())
            elif second_type_layer == 'Dropout':
                new_layer = tf.keras.layers.Dropout(**layer.get_config())
            elif second_type_layer == 'Lambda':
                config_lambda = layer.get_config()
                print(config_lambda)
                del config_lambda['function_type']
                del config_lambda['output_shape_type']
                new_layer = tf.keras.layers.Lambda(**config_lambda)
        elif type_layer == 'pooling':
            if second_type_layer == 'MaxPooling2D':
                new_layer = tf.keras.layers.MaxPooling2D(**layer.get_config())
            elif second_type_layer == 'AveragePooling2D':
                new_layer = tf.keras.layers.AveragePooling2D(**layer.get_config())
            elif second_type_layer == 'GlobalMaxPooling2D':
                new_layer = tf.keras.layers.GlobalMaxPooling2D(**layer.get_config())
        if new_layer == 'merge':
            new_layer = tf.keras.layers.Concatenate(**layer.get_config())
        list_layers.append(new_layer)
    model = tf.keras.Sequential(list_layers)
    return model
</code></pre>

<p>However, this is not working because of Lambda layer. In the config layer, the function is now written in the form of: </p>

<pre><code>'function': ('4wIAAAAAAAAAAgAAAAMAAABTAAAAcxQAAAB8AGQBGQB8AGQC8ARQAFwBTACkDTukAAAAA6QEA\nAACpACkC2gZpbaBXNjYWxlcgMAAAByAwAAAPp/L2dwZnMvaGFpZmEtcDYvMDMvbXNpZXZl\nX2RldjMvdXNyL3BhdWxkYS9naXRfcmVwb0hJLUltYWdlQW5hbHl0aWNzL3Jlc291cmNlcy9y\ndW5fMTE3NC9jdXN0b21fcHJldHJhaW5lZF9JbmNlcHRpb25SZXNOZXRWMi5wedoIPGxhbWJkYT6d\nAAAA8wAAAAA=\n', None, None)
</code></pre>

<p>Hence, I gave up this method hoping something else would allow to pre-process the input of my serving model.</p>
","My final goal is to use the What-If-Tool on tensorboard. In order to do that, I need to serve my Keras model on TensorflowServing, and the data in a TFRecordFile. So the data has to be transformed into tf.Examples. The tool is supposed to grab the network to run inference on the data. however, the network cannot handle tf.Examples as an input. So the served model needs to have a preprocessing function. According to the tensorflow documentation, one way is to create a tensorflow Estimator, and to use ""serving_input_receiver_fn"" to preprocess the data. This would have been perfect except for the case that I can't make an already trained native Keras model into an Estimator. It seems that the only way it to create it from a tf.keras model (and not a native keras model like I have), and to train it directly with the estimator. Another way would be to use the tf.saved_model.simple_save function, and then use TensorflowServing, but I did not find a way to preprocess the tf.Examples to make a correct input for the network. Since this is not working, I have no clue on how to resolve this. Edit: I tried to transform my native keras into a tf.keras model. My model is really big, so I build this function: However, this is not working because of Lambda layer. In the config layer, the function is now written in the form of: Hence, I gave up this method hoping something else would allow to pre-process the input of my serving model.",https://stackoverflow.com/questions/56754293,10545585,Documentation Replication on Other Examples
56802840,What exactly tensorflow.gather() does?,"<p>I saw code for triplet loss that contains the function tf.gather(). What this function does?</p>

<p>I have gone through the tensorflow's official website for definition but still unable to get it.</p>

<pre><code>def margin_triplet_loss(y_true, y_pred, margin, batch_size):
    anchor = tf.gather(y_pred, tf.range(0, batch_size, 3))
    positive = tf.gather(y_pred, tf.range(1, batch_size, 3))
    negative = tf.gather(y_pred, tf.range(2, batch_size, 3))

    loss = K.maximum(margin
                 + K.sum(K.square(anchor-positive), axis=1)
                 - K.sum(K.square(anchor-negative), axis=1),
                 0.0)
    return K.mean(loss)
</code></pre>
",I saw code for triplet loss that contains the function tf.gather(). What this function does? I have gone through the tensorflow's official website for definition but still unable to get it.,https://stackoverflow.com/questions/56802840,9678047,Lack of Alternative Solutions/Documentation
56804123,Tensorflow-Lite : Exporting a GraphDef from tf.Session convert failed,"<p>I want to convert my tensorflow Spectrogram session to <code>.tflite</code> file for using in Android.
I try to follow the TFLite <a href=""https://www.tensorflow.org/lite/convert/python_api#exporting_a_graphdef_from_tfsession_"" rel=""nofollow noreferrer"">official example</a> but it is failed during the convert procedure.
Please give me some advice for fixing this error. Thanks a lots!</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.signal import stft

# Waveform placeholder
waveform = tf.placeholder(name=""waveform"", dtype=tf.float32, shape=(4000))

# Compute the spectrogram and get the asolute values
stfts = stft(waveform, frame_length=256, frame_step=64, fft_length=256)
spectrograms = tf.abs(stfts)   

def TF_spectrogram(audio):
    # Run the spectrogram session
    with tf.Session() as sess:
        # Run the computation graph and save the png encoded image to a file
        spectrogram = sess.run(spectrograms, feed_dict={waveform: audio}) 
        print(""The Spectrogram size (shape) is : "" + str(spectrogram.shape) + "", type : "" + str(type(spectrogram)))  
        return spectrogram

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    converter = tf.lite.TFLiteConverter.from_session(sess, [waveform], [spectrograms])
    tflite_model = converter.convert()
    open(""spectrogram.tflite"", ""wb"").write(tflite_model)

</code></pre>

<p>I got the error message from Jupyter Notebook</p>

<pre><code>---------------------------------------------------------------------------
ConverterError                            Traceback (most recent call last)
&lt;ipython-input-22-a94c03f6a5ae&gt; in &lt;module&gt;
      2     sess.run(tf.global_variables_initializer())
      3     converter = tf.lite.TFLiteConverter.from_session(sess, [waveform], [spectrograms])
----&gt; 4     tflite_model = converter.convert()
      5     open(""converted_model.tflite"", ""wb"").write(tflite_model)

F:\Anaconda3\envs\tf_py36\lib\site-packages\tensorflow\lite\python\lite.py in convert(self)
    896           input_tensors=self._input_tensors,
    897           output_tensors=self._output_tensors,
--&gt; 898           **converter_kwargs)
    899     else:
    900       result = _toco_convert_graph_def(

F:\Anaconda3\envs\tf_py36\lib\site-packages\tensorflow\lite\python\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, *args, **kwargs)
    402   data = toco_convert_protos(model_flags.SerializeToString(),
    403                              toco_flags.SerializeToString(),
--&gt; 404                              input_data.SerializeToString())
    405   return data
    406 

F:\Anaconda3\envs\tf_py36\lib\site-packages\tensorflow\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str)
    170       stderr = _try_convert_to_unicode(stderr)
    171       raise ConverterError(
--&gt; 172           ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
    173   finally:
    174     # Must manually cleanup files.

ConverterError: TOCO failed. See console for info.
2019-06-28 17:04:12.512364: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: RFFT
2019-06-28 17:04:12.512817: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""NoOp"" device_type: ""CPU""') for unknown op: NoOp
2019-06-28 17:04:12.513084: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""NoOp"" device_type: ""GPU""') for unknown op: NoOp
2019-06-28 17:04:12.513269: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_HostRecv"" device_type: ""GPU"" host_memory_arg: ""tensor""') for unknown op: _HostRecv
2019-06-28 17:04:12.513564: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_Send"" device_type: ""CPU""') for unknown op: _Send
2019-06-28 17:04:12.513819: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_HostRecv"" device_type: ""CPU""') for unknown op: _HostRecv
2019-06-28 17:04:12.514068: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_Send"" device_type: ""GPU""') for unknown op: _Send
2019-06-28 17:04:12.514254: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_Recv"" device_type: ""CPU""') for unknown op: _Recv
2019-06-28 17:04:12.514431: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_HostSend"" device_type: ""GPU"" host_memory_arg: ""tensor""') for unknown op: _HostSend
2019-06-28 17:04:12.514637: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_Recv"" device_type: ""GPU""') for unknown op: _Recv
2019-06-28 17:04:12.514803: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_HostSend"" device_type: ""CPU""') for unknown op: _HostSend
2019-06-28 17:04:12.515019: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""CPU""') for unknown op: WrapDatasetVariant
2019-06-28 17:04:12.515234: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: WrapDatasetVariant
2019-06-28 17:04:12.515498: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""CPU""') for unknown op: UnwrapDatasetVariant
2019-06-28 17:04:12.515718: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: UnwrapDatasetVariant
2019-06-28 17:04:12.516175: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: RFFT
2019-06-28 17:04:12.516336: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: ComplexAbs
2019-06-28 17:04:12.516712: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 7 operators, 17 arrays (0 quantized)
2019-06-28 17:04:12.517018: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 7 operators, 17 arrays (0 quantized)
2019-06-28 17:04:12.517417: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 7 operators, 16 arrays (0 quantized)
2019-06-28 17:04:12.517682: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:118] Check failed: dim_x == dim_y (64 vs. 256)Dimensions must match
Fatal Python error: Aborted

Current thread 0x000033dc (most recent call first):
  File ""f:\anaconda3\envs\tf_py36\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 33 in execute
  File ""f:\anaconda3\envs\tf_py36\lib\site-packages\absl\app.py"", line 251 in _run_main
  File ""f:\anaconda3\envs\tf_py36\lib\site-packages\absl\app.py"", line 300 in run
  File ""f:\anaconda3\envs\tf_py36\lib\site-packages\tensorflow\python\platform\app.py"", line 40 in run
  File ""f:\anaconda3\envs\tf_py36\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 59 in main
  File ""F:\Anaconda3\envs\tf_py36\Scripts\toco_from_protos.exe\__main__.py"", line 9 in &lt;module&gt;
  File ""f:\anaconda3\envs\tf_py36\lib\runpy.py"", line 85 in _run_code
  File ""f:\anaconda3\envs\tf_py36\lib\runpy.py"", line 193 in _run_module_as_main
</code></pre>

<p>I want to make customize Spectrogram library by <a href=""https://www.tensorflow.org/api_docs/python/tf/signal/stft"" rel=""nofollow noreferrer""><code>tf.signal.stft</code></a> function, and save to <code>.tflite</code> file for using in Android device.
I can't find anymore tutorial about TFLite converter, only the official document.😭 Please help me to fix this error, thanks a lots !! 🙏</p>
","I want to convert my tensorflow Spectrogram session to .tflite file for using in Android. I try to follow the TFLite official example but it is failed during the convert procedure. Please give me some advice for fixing this error. Thanks a lots! I got the error message from Jupyter Notebook I want to make customize Spectrogram library by tf.signal.stft function, and save to .tflite file for using in Android device. I can't find anymore tutorial about TFLite converter, only the official document.😭 Please help me to fix this error, thanks a lots !! 🙏",https://stackoverflow.com/questions/56804123,7651473,Documentation Replication on Other Examples
56905939,Effective way to read images from a csv file and return a tf.data.Dataset object,"<p>I have a csv file that contains two columns:</p>

<ol>
<li>the file path of the image which is stored as <code>numpy</code> arrays</li>
<li>the label of the image</li>
</ol>

<p>Each row in the csv corresponds to one item (sample).</p>

<p>I want to create a <code>tf.data</code> pipeline that reads the file path and loads the numpy array and the label associated with it. How would I go about doing so so that I can return a <code>tf.data.Dataset</code> object?</p>

<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data"" rel=""nofollow noreferrer"">documentation</a> on the website is not very informative and I cannot figure out where to start from.</p>
",I have a csv file that contains two columns: Each row in the csv corresponds to one item (sample). I want to create a tf.data pipeline that reads the file path and loads the numpy array and the label associated with it. How would I go about doing so so that I can return a tf.data.Dataset object? The documentation on the website is not very informative and I cannot figure out where to start from.,https://stackoverflow.com/questions/56905939,5052482,Lack of Alternative Solutions/Documentation
56939282,How do you feed a tf.data.Dataset dynamically in eager execution mode where initializable_iterator isn't available?,"<p>What is the new approach (under eager execution) to feeding data through a dataset pipeline in a dynamic fashion, when we need to feed it sample by sample? </p>

<p>I have a <code>tf.data.Dataset</code> which performs some preprocessing steps and reads data from a generator, drawing from a large dataset during training. </p>

<p>Let's say that dataset is represented as:</p>

<pre><code>ds = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])
ds = ds.map(tf.square).shuffle(2).batch(2)
iterator = tf.data.make_one_shot_iterator(ds)
</code></pre>

<p>After training I want to produce various visualizations which require that I feed one sample at a time through the network for inference. I've now got this dataset preprocessing pipeline that I need to feed my raw sample through to be sized and shaped appropriately for the network input.</p>

<p>This seems like a use case for the initializable iterator:</p>

<pre><code>placeholder = tf.placeholder(tf.float32, shape=None)
ds = tf.data.Dataset.from_tensor_slices(placeholder)
ds = ds.map(tf.square).shuffle(2).batch(2)
iterator = tf.data.make_initializable_iterator(ds) 
# now re-initialize for each sample
</code></pre>

<blockquote>
  <p>Keep in mind that the map operation in this example represents a long sequence of preprocessing operations that can't be duplicated for each new data sample being feed in.</p>
</blockquote>

<p><strong>This doesn't work with eager execution</strong>, you can't use the placeholder. The documentation examples all seem to assume a static input such as in the first example here.</p>

<p>The only way I can think of doing this is with a queue and <code>tf.data.Dataset.from_generator(...)</code> which reads from the queue that I push to before predicting on the data. But this feels both hacky, and appears prone to deadlocks that I've yet to solve.</p>

<p>TF 1.14.0</p>
","What is the new approach (under eager execution) to feeding data through a dataset pipeline in a dynamic fashion, when we need to feed it sample by sample? I have a tf.data.Dataset which performs some preprocessing steps and reads data from a generator, drawing from a large dataset during training. Let's say that dataset is represented as: After training I want to produce various visualizations which require that I feed one sample at a time through the network for inference. I've now got this dataset preprocessing pipeline that I need to feed my raw sample through to be sized and shaped appropriately for the network input. This seems like a use case for the initializable iterator: This doesn't work with eager execution, you can't use the placeholder. The documentation examples all seem to assume a static input such as in the first example here. The only way I can think of doing this is with a queue and tf.data.Dataset.from_generator(...) which reads from the queue that I push to before predicting on the data. But this feels both hacky, and appears prone to deadlocks that I've yet to solve. TF 1.14.0",https://stackoverflow.com/questions/56939282,4790871,Documentation Replication on Other Examples
56969703,How to use `tf.scatter_nd` with multi-dimensional tensors,"<p>I'm trying to create a new tensor (<code>output</code>) with the values of another tensor (<code>updates</code>) placed according to <code>idx</code> tensor. The shape of <code>output</code> should be <code>[batch_size, 1, 4, 4]</code> (like an image of 2x2 pixels and one channel) and <code>update</code> has shape <code>[batch_size, 3]</code>.</p>

<p>I've read Tensorflow documentation (I'm working with gpu version 1.13.1) and found <code>tf.scatter_nd</code> should work for my problem. The issue is that I cannot make it work, I think I'm having problems understanding how I have to arange <code>idx</code>. </p>

<p>Let's consider <code>batch_size = 2</code>, so what I'm doing is:</p>

<pre class=""lang-py prettyprint-override""><code>updates = tf.constant([[1, 2, 3], [4, 5, 6]])  # shape [2, 3]
output_shape = tf.constant([2, 1, 4, 4])
idx = tf.constant([[[1, 0], [1, 1], [1, 0]], [[0, 0], [0, 1], [0, 2]]])  # shape [2, 3, 2]
idx_expanded = tf.expand_dims(idx, 1)  # so I have shape [2, 1, 3, 2]
output = tf.scatter_nd(idx_expanded, updates, output_shape)
</code></pre>

<p>I expect it to work, but it doesn't, it gives me this error:</p>

<p><code>ValueError: The outer 3 dimensions of indices.shape=[2,1,3,2] must match the outer 3 dimensions of updates.shape=[2,3]: Shapes must be equal rank, but are 3 and 2 for 'ScatterNd_7' (op: 'ScatterNd') with input shapes: [2,1,3,2], [2,3], [4]</code></p>

<p>I don't understand why it's expecting <code>updates</code> to have dimension 3. I thought <code>idx</code> has to make sense with <code>output_shape</code> (that's why I used <code>expand_dims</code>) and also with <code>updates</code> (specify the two indices for the three points), but it's obvious I'm missing something here.</p>

<p>Any help would be appreciated.</p>
","I'm trying to create a new tensor (output) with the values of another tensor (updates) placed according to idx tensor. The shape of output should be [batch_size, 1, 4, 4] (like an image of 2x2 pixels and one channel) and update has shape [batch_size, 3]. I've read Tensorflow documentation (I'm working with gpu version 1.13.1) and found tf.scatter_nd should work for my problem. The issue is that I cannot make it work, I think I'm having problems understanding how I have to arange idx. Let's consider batch_size = 2, so what I'm doing is: I expect it to work, but it doesn't, it gives me this error: ValueError: The outer 3 dimensions of indices.shape=[2,1,3,2] must match the outer 3 dimensions of updates.shape=[2,3]: Shapes must be equal rank, but are 3 and 2 for 'ScatterNd_7' (op: 'ScatterNd') with input shapes: [2,1,3,2], [2,3], [4] I don't understand why it's expecting updates to have dimension 3. I thought idx has to make sense with output_shape (that's why I used expand_dims) and also with updates (specify the two indices for the three points), but it's obvious I'm missing something here. Any help would be appreciated.",https://stackoverflow.com/questions/56969703,7327257,Documentation Replication on Other Examples
56970612,Fitted values and weights in tensorflow (tesorflow DNNRegressor),"<p>I am using tensorflow version 2.0.0-beta1. I have created the input function to feed into tf.estimator.DNNRegressor.</p>

<pre><code>input_func = tf.compat.v1.estimator.inputs.pandas_input_fn(x=X_train,
                                                y=y_train,
                                                batch_size=10,
                                                num_epochs=1000,
                                                shuffle=True)
</code></pre>

<p>Below is the model that I am creating using DNNRegressor. </p>

<pre><code>model = tf.estimator.DNNRegressor(feature_columns=feature_col, hidden_units=[1024, 800, 512, 256])
model.train(input_fn=input_func, steps=10000)
</code></pre>

<p>Now I want to identify </p>

<p><strong>1) Fitted values of my model on training data.</strong></p>

<p><strong>2) Weights associated with each variable in model(i.e. tf.estimator.DNNRegressor)</strong></p>

<p>I have search through the documentation of tensorflow and other sources but didn't get this information. </p>
",I am using tensorflow version 2.0.0-beta1. I have created the input function to feed into tf.estimator.DNNRegressor. Below is the model that I am creating using DNNRegressor. Now I want to identify 1) Fitted values of my model on training data. 2) Weights associated with each variable in model(i.e. tf.estimator.DNNRegressor) I have search through the documentation of tensorflow and other sources but didn't get this information.,https://stackoverflow.com/questions/56970612,6244166,Lack of Alternative Solutions/Documentation
57083881,Tensorflow 2.0 Saving trained parameters to be restored in a new file,"<p>I need to save trained variables of a TensorFlow 2.0 model using one of TF's built in functions like tf.train.Checkpoint or any other, and want to call them in a new file. I am not using tf.Keras.Sequantial and don't want to use something like model.save_weights()</p>

<p>I have tried tf.train.Checkpoint to save variables, but not sure how to restore them. I used to work with tf.train.Saver() in TF 1.0 to save variables using sessions and restore them using tf.train.import_meta_graph and tf.train.latest_checkpoint. However, I haven't been able to find equivalent functionalities in TF 2.0 documentation so far. </p>

<h1>try checkpoint saver in tensorflow 2.0 format to save trained parameters W, b_v, b_h</h1>

<p>saver = tf.train.Checkpoint()</p>

<p>saver.listed = [W, b_v, b_h]</p>

<p>saver.mapped = {'W':saver.listed[0],'b_v':saver.listed[1],
 'b_h':saver.listed[2]}</p>

<p>save_path = saver.save('trained_parameters')</p>

<h1>in a new file:</h1>

<p>restorer = tf.train.Checkpoint()</p>

<p>restorer.restore('trained_parameters')</p>

<h1>calling the parameters by their previously mapped names doesn't work, not sure how to go about this</h1>
","I need to save trained variables of a TensorFlow 2.0 model using one of TF's built in functions like tf.train.Checkpoint or any other, and want to call them in a new file. I am not using tf.Keras.Sequantial and don't want to use something like model.save_weights() I have tried tf.train.Checkpoint to save variables, but not sure how to restore them. I used to work with tf.train.Saver() in TF 1.0 to save variables using sessions and restore them using tf.train.import_meta_graph and tf.train.latest_checkpoint. However, I haven't been able to find equivalent functionalities in TF 2.0 documentation so far. saver = tf.train.Checkpoint() saver.listed = [W, b_v, b_h] saver.mapped = {'W':saver.listed[0],'b_v':saver.listed[1], 'b_h':saver.listed[2]} save_path = saver.save('trained_parameters') restorer = tf.train.Checkpoint() restorer.restore('trained_parameters')",https://stackoverflow.com/questions/57083881,11799681,Documentation Replication on Other Examples
57120680,Deep copy of tensor in tensorflow python,"<p>In some of my code, I have created a neural network using tensorflow and have access to a tensor representing that network's output. I want to make a copy of this tensor so that even if I train the neural network more, I can access the original value of the tensor.</p>

<p>Following other answers and tensorflow documentation, I have tried the tf.identity() function, but it does not seem to be doing what I need. Some other links suggested the use of tf.tile(), but this did not help either. I do not wish to use sess.run(), evaluate the tensor, and store it elsewhere.</p>

<p>Here is a toy example that describes what I need to do:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

t1 = tf.placeholder(tf.float32, [None, 1])
t2 = tf.layers.dense(t1, 1, activation=tf.nn.relu)
expected_out = tf.placeholder(tf.float32, [None, 1])

loss = tf.reduce_mean(tf.square(expected_out - t2))
train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)

sess = tf.Session()

sess.run(tf.global_variables_initializer())

print(sess.run(t2, feed_dict={t1: np.array([1]).reshape(-1,1)}))
t3 = tf.identity(t2) # Need to make copy here
print(sess.run(t3, feed_dict={t1: np.array([1]).reshape(-1,1)}))

print(""\nTraining \n"")

for i in range(1000):
    sess.run(train_op, feed_dict={t1: np.array([1]).reshape(-1,1), expected_out: np.array([1]).reshape(-1,1)})

print(sess.run(t2, feed_dict={t1: np.array([1]).reshape(-1,1)}))
print(sess.run(t3, feed_dict={t1: np.array([1]).reshape(-1,1)}))
</code></pre>

<p>The result of the above code is that <code>t2</code> and <code>t3</code> have the same value. </p>

<pre><code>[[1.5078927]]
[[1.5078927]]

Training

[[1.3262703]]
[[1.3262703]]
</code></pre>

<p>What I want is for <code>t3</code> to keep its value from being copied.</p>

<pre><code>[[1.5078927]]
[[1.5078927]]

Training

[[1.3262703]]
[[1.5078927]]
</code></pre>

<p>Thanks in advance for your help.</p>
","In some of my code, I have created a neural network using tensorflow and have access to a tensor representing that network's output. I want to make a copy of this tensor so that even if I train the neural network more, I can access the original value of the tensor. Following other answers and tensorflow documentation, I have tried the tf.identity() function, but it does not seem to be doing what I need. Some other links suggested the use of tf.tile(), but this did not help either. I do not wish to use sess.run(), evaluate the tensor, and store it elsewhere. Here is a toy example that describes what I need to do: The result of the above code is that t2 and t3 have the same value. What I want is for t3 to keep its value from being copied. Thanks in advance for your help.",https://stackoverflow.com/questions/57120680,8083568,Documentation Ambiguity
57134808,tf.keras.optimizers.Adam with tf.estimator model in Tensorflow 2.0.beta is crashing,"<p>I am using <code>Tensorflow 2.0.beta</code> with <code>Python 3.6.6</code> on <code>Mac OS</code> (nightly: <code>tf-nightly-2.0-preview</code> <code>2.0.0.dev20190721</code> but I never managed to have it working with compat module in <code>Tensorflow 2.0</code>).</p>

<p>I am traying to migrate a <code>tf.estimator</code> model from <code>Tensorflow 1.12</code> (fully working) to <code>Tensorflow 2.0</code>. Here is the code:</p>

<pre><code># estimator model
def baseline_estimator_model(features, labels, mode, params):
    """"""
    Model function for Estimator
    """"""
    print('model based on keras layer but return an estimator model')

    # gettings the bulding blocks
    model = keras_building_blocks(params['dim_input'], params['num_classes'])

    dense_inpout = features['dense_input']

    # Logits layer
    if mode == tf.estimator.ModeKeys.TRAIN:
        logits = model(dense_inpout, training=True)
    else:
        logits = model(dense_inpout, training=False)


    # Compute predictions
    probabilities = tf.nn.softmax(logits)
    classes = tf.argmax(input=probabilities, axis=1, )

    # made prediction
    predictions = {
        'classes': classes,
        'probabilities': probabilities,
    }

    # to be tested
    predictions_output = tf.estimator.export.PredictOutput(predictions)

    # Provide an estimator spec for `ModeKeys.PREDICT`
    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode,
                                          predictions=predictions,
                                          export_outputs={tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output})

    # Compute loss for both TRAIN and EVAL modes
    # old -&gt; loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(labels, logits)

    # Generate necessary evaluation metrics
    # old -&gt; accuracy = tf.compat.v1.metrics.accuracy(labels=tf.argmax(input=labels, axis=1), predictions=classes, name='accuracy')
    accuracy = tf.keras.metrics.CategoricalAccuracy()
    accuracy.update_state(labels, logits)

    eval_metrics = {'accuracy': accuracy}

    tf.summary.scalar('accuracy', accuracy.result())

    # Provide an estimator spec for `ModeKeys.EVAL`
    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          eval_metric_ops=eval_metrics)

    # Provide an estimator spec for `ModeKeys.TRAIN`
    if mode == tf.estimator.ModeKeys.TRAIN:

        # old but working -&gt; optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)
        # crashing
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, epsilon=1e-07)

        # old -&gt; train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())
        train_op = optimizer.minimize(loss,var_list=model.weights)

        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op)
</code></pre>

<p>predictions=predictions, loss=loss, train_op=train_op, export_outputs=predictions_output)</p>

<p>If I keep the compat.v1 module it is working:</p>

<pre><code>optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)
</code></pre>

<p>If I try to use something without compat.v1 it is crashing:</p>

<pre><code>optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9,epsilon=1e-07)
</code></pre>

<p>with the following error (I am running the code locally for the moment, not on <code>GCP</code>):</p>

<pre><code>I0721 17:33:04.812453 4526515648 estimator.py:209] Using config: {'_model_dir': 'results/Models/Mnist/tf_1_12/estimator/v3/ckpt/', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': 10, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x1c37b11b70&gt;, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0721 17:33:04.815697 4526515648 estimator_training.py:186] Not using Distribute Coordinator.
I0721 17:33:04.817899 4526515648 training.py:612] Running training and evaluation locally (non-distributed).
I0721 17:33:04.818665 4526515648 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10 or save_checkpoints_secs None.
I0721 17:33:04.834385 4526515648 model.py:211] input_dataset_fn: TRAIN, train

using keras layer and estimator (recommended way)
exporter &lt;tensorflow_estimator.python.estimator.exporter.LatestExporter object at 0x1c37b115f8&gt;

I0721 17:33:05.117963 4526515648 estimator.py:1145] Calling model_fn.

model based on keras layer but return an estimator model

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

~/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/src/model_mnist_2_0_v1/trainer/model.py in train_and_evaluate(FLAGS, use_keras)
    589                                       exporters=exporter)
    590 
--&gt; 591     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
    592 
    593 def train_and_evaluate_old(FLAGS, use_keras):

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)
    471         '(with task id 0).  Given task id {}'.format(config.task_id))
    472 
--&gt; 473   return executor.run()
    474 
    475 

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run(self)
    611         config.task_type != run_config_lib.TaskType.EVALUATOR):
    612       logging.info('Running training and evaluation locally (non-distributed).')
--&gt; 613       return self.run_local()
    614 
    615     # Distributed case.

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)
    712         max_steps=self._train_spec.max_steps,
    713         hooks=train_hooks,
--&gt; 714         saving_listeners=saving_listeners)
    715 
    716     eval_result = listener_for_eval.eval_result or _EvalResult(

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    365 
    366       saving_listeners = _check_listeners_type(saving_listeners)
--&gt; 367       loss = self._train_model(input_fn, hooks, saving_listeners)
    368       logging.info('Loss for final step: %s.', loss)
    369       return self

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1156       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1157     else:
-&gt; 1158       return self._train_model_default(input_fn, hooks, saving_listeners)
   1159 
   1160   def _train_model_default(self, input_fn, hooks, saving_listeners):

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1186       worker_hooks.extend(input_hooks)
   1187       estimator_spec = self._call_model_fn(
-&gt; 1188           features, labels, ModeKeys.TRAIN, self.config)
   1189       global_step_tensor = training_util.get_global_step(g)
   1190       return self._train_with_estimator_spec(estimator_spec, worker_hooks,

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1144 
   1145     logging.info('Calling model_fn.')
-&gt; 1146     model_fn_results = self._model_fn(features=features, **kwargs)
   1147     logging.info('Done calling model_fn.')
   1148 

~/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/src/model_mnist_2_0_v1/trainer/model.py in baseline_estimator_model(features, labels, mode, params)
    442         #train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())
    443         #train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())
--&gt; 444         train_op = optimizer.minimize(loss,var_list=model.weights)
    445 
    446         print('step 8')

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in minimize(self, loss, var_list, grad_loss, name)
    315     """"""
    316     grads_and_vars = self._compute_gradients(
--&gt; 317         loss, var_list=var_list, grad_loss=grad_loss)
    318 
    319     return self.apply_gradients(grads_and_vars, name=name)

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _compute_gradients(self, loss, var_list, grad_loss)
    349       if not callable(var_list):
    350         tape.watch(var_list)
--&gt; 351       loss_value = loss()
    352     if callable(var_list):
    353       var_list = var_list()

TypeError: 'Tensor' object is not callable
</code></pre>

<p>Any idea how to fix that ? The error messages was changing over time since <code>Tensorflow 2.0 alpha</code>.</p>

<p>I am also looking for a full working example of tf.estimator working with <code>Tensorflow 2.0</code>. I have issue to export the model as well. In the official documentation of <code>Tensorflow 2.0</code> they only use in their example <code>compat.v1</code> and don't export the model. All the online course on tf.estimator from GCP are using  older version of Tensorflow (1.12 - 1.14).</p>
","I am using Tensorflow 2.0.beta with Python 3.6.6 on Mac OS (nightly: tf-nightly-2.0-preview 2.0.0.dev20190721 but I never managed to have it working with compat module in Tensorflow 2.0). I am traying to migrate a tf.estimator model from Tensorflow 1.12 (fully working) to Tensorflow 2.0. Here is the code: predictions=predictions, loss=loss, train_op=train_op, export_outputs=predictions_output) If I keep the compat.v1 module it is working: If I try to use something without compat.v1 it is crashing: with the following error (I am running the code locally for the moment, not on GCP): Any idea how to fix that ? The error messages was changing over time since Tensorflow 2.0 alpha. I am also looking for a full working example of tf.estimator working with Tensorflow 2.0. I have issue to export the model as well. In the official documentation of Tensorflow 2.0 they only use in their example compat.v1 and don't export the model. All the online course on tf.estimator from GCP are using older version of Tensorflow (1.12 - 1.14).",https://stackoverflow.com/questions/57134808,6430839,Inadequate Examples
57140835,How to convert a tf.estimator to a keras model?,"<p>In package <code>tf.estimator</code>, there's a lot of defined estimators. I want to use them in Keras.</p>
<p>I checked TF docs, there's only one converting method that could convert <code>keras. Model</code> to <code>tf. estimator</code>, but no way to convert from <code>estimator</code> to <code>Model</code>.</p>
<p>For example, if we want to convert the following estimator:</p>
<pre><code>tf.estimator.DNNLinearCombinedRegressor
</code></pre>
<p>How could it be converted into Keras Model?</p>
","In package tf.estimator, there's a lot of defined estimators. I want to use them in Keras. I checked TF docs, there's only one converting method that could convert keras. Model to tf. estimator, but no way to convert from estimator to Model. For example, if we want to convert the following estimator: How could it be converted into Keras Model?",https://stackoverflow.com/questions/57140835,5777564,Documentation Replicability
57170737,Cannot run tflite model on GPU (Jetson Nano) using Python,"<p>I have a quantized tflite model that I'd like to benchmark for inference on a Nvidia Jetson Nano. I use tf.lite.Interpreter() method for inference. The process doesn't seem to run on the GPU as the inference times on both CPU and GPU are the same.</p>
<p>Is there any way to run a tflite model on GPU using Python?</p>
<p>I tried to force GPU usage by setting tf.device() method but still doesn't work. The official documentation has something called delegates for GPU acceleration but I can't seem to find anything for Python.</p>
<pre><code>with tf.device('/device:GPU:0'):

    interpreter = tf.lite.Interpreter(model_path=&quot;model.tflite&quot;)

    interpreter.allocate_tensors()

    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    input_shape = input_details[0]['shape']
    input_data = np.array(np.random.random_sample(input_shape), dtype=np.uint8)
    interpreter.set_tensor(input_details[0]['index'], input_data)

    start_time = time.time()

    interpreter.invoke()

    elapsed_time = time.time() - start_time
    print(elapsed_time)

    output_data = interpreter.get_tensor(output_details[0]['index'])
</code></pre>
",I have a quantized tflite model that I'd like to benchmark for inference on a Nvidia Jetson Nano. I use tf.lite.Interpreter() method for inference. The process doesn't seem to run on the GPU as the inference times on both CPU and GPU are the same. Is there any way to run a tflite model on GPU using Python? I tried to force GPU usage by setting tf.device() method but still doesn't work. The official documentation has something called delegates for GPU acceleration but I can't seem to find anything for Python.,https://stackoverflow.com/questions/57170737,8380398,Documentation Replication on Other Examples
57175343,Multiple inputs of keras model with tf.data.Dataset.from_generator in Tensorflow 2,"<p>I am trying to implement a model in keras that will have multiple inputs:</p>

<ul>
<li>image (200x200)</li>
<li>some numbers (1x50)</li>
<li>three 1d signals (1x50000, 2x100000)</li>
</ul>

<p>To feed that model, I want to write a generator to use with <code>tf.data.Dataset.from_generator</code>. From the <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#from_generator"" rel=""noreferrer"">docs of from_generator</a>, its not clear to me how I should provide its parameters <code>output_types</code>, <code>output_shapes</code>. Can anyone help me with this?</p>
","I am trying to implement a model in keras that will have multiple inputs: To feed that model, I want to write a generator to use with tf.data.Dataset.from_generator. From the docs of from_generator, its not clear to me how I should provide its parameters output_types, output_shapes. Can anyone help me with this?",https://stackoverflow.com/questions/57175343,1113765,Inadequate Examples
57279754,"What are the Tensorflow qint8, quint8, qint32, qint16, and quint16 datatypes?","<p>I'm looking at the Tensorflow tf.nn.quantized_conv2d function and I'm wondering what exactly the qint8, etc. dataypes are, particularly if they are the datatypes used for the ""fake quantization nodes"" in tf.contrib.quantize or are actually stored using 8 bits (for qint8) in memory.</p>

<p>I know that they are defined in tf.dtypes.DType, but that doesn't have any information about what they actually are.</p>
","I'm looking at the Tensorflow tf.nn.quantized_conv2d function and I'm wondering what exactly the qint8, etc. dataypes are, particularly if they are the datatypes used for the ""fake quantization nodes"" in tf.contrib.quantize or are actually stored using 8 bits (for qint8) in memory. I know that they are defined in tf.dtypes.DType, but that doesn't have any information about what they actually are.",https://stackoverflow.com/questions/57279754,11860666,Documentation Replication on Other Examples
57316557,"tf.keras.layers.pop() doesn't work, but tf.keras._layers.pop() does","<p>I want to pop the last layer of the model. So I use the <code>tf.keras.layers.pop()</code>, but it doesn't work.</p>

<pre><code>base_model.summary()
</code></pre>

<p><a href=""https://i.stack.imgur.com/wRz02.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wRz02.png"" alt=""enter image description here""></a></p>

<pre><code>base_model.layers.pop()

base_model.summary()
</code></pre>

<p><a href=""https://i.stack.imgur.com/msGnY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/msGnY.png"" alt=""enter image description here""></a></p>

<p>When I use <code>tf.keras._layers.pop()</code>, it works.</p>

<pre><code>base_model.summary()
</code></pre>

<p><a href=""https://i.stack.imgur.com/CAq45.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CAq45.png"" alt=""enter image description here""></a></p>

<pre><code>base_model._layers.pop()
base_model.summary()
</code></pre>

<p><a href=""https://i.stack.imgur.com/viCxT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/viCxT.png"" alt=""enter image description here""></a></p>

<p>I don't find docs about this usage. Could someone help explain this?</p>
","I want to pop the last layer of the model. So I use the tf.keras.layers.pop(), but it doesn't work. When I use tf.keras._layers.pop(), it works. I don't find docs about this usage. Could someone help explain this?",https://stackoverflow.com/questions/57316557,11255365,Documentation Replication on Other Examples
57349824,"Recurrent neural network, time series prediction with newer Tensorflow 1.14","<p>How to use new tf.keras API with recurrent neural network? I have checked the documentation but there is no example of such a situation.
There is this great book Hands on machine learning from 2017. Since that year the API of tensorflow has evolved and I am trying to rewrite recurrent neural network for time series prediction with using version <code>1.14</code> code.
The code from the book is using older <code>tf.nn.dynamic_rnn</code> and <code>tf.nn.rnn_cell.BasicRNNCell</code>:</p>

<pre><code>n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1
learning_rate = 0.001

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])
cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)
rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
loss = tf.reduce_mean(tf.square(outputs - y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()
n_iterations = 500
batch_size = 50

with tf.Session() as sess:
    init.run()
        for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})
</code></pre>

<p>And this code works just fine (except that it throws warnings about deprecation left and right). I wanted to use <code>tf.keras</code> API as suggested in warning. My code is the same except:</p>

<pre><code>cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)  
rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
</code></pre>

<p>But this yields following exception:</p>

<pre><code>InvalidArgumentError: Input to reshape is a tensor with 50 values, but the requested shape requires a multiple of 20
 [[node Reshape_1 (defined at &lt;ipython-input-9-879361be49dd&gt;:3) ]]
</code></pre>

<p>so I understand that the problematic line is</p>

<pre><code>outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
</code></pre>

<p>After checking and comparing documentation for both cells <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a> and 
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN</a> I can't find the culprit.</p>

<p><strong>What is the difference with these two cells? How to use tf.keras API with time series?</strong></p>

<p>Full old code: <a href=""https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb"" rel=""nofollow noreferrer"">https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb</a></p>

<p>Full ""my"" code:</p>

<pre><code>import numpy as np
import tensorflow as tf
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd
from utils import shuffle_batch, variable_summaries
import os


dir_path = os.getcwd()

now = datetime.utcnow().strftime(""%Y%m%d%H%M%S"")
root_logdir = ""tf_logs""
logdir = ""{}/run-{}/"".format(root_logdir, now)
print(dir_path)


t_min, t_max = -5, 5
section_start = (t_max + t_min) / 2
resolution = 0.1
n_steps = 20

def time_series(t):
    return np.sin(t)

def next_batch(batch_size, n_steps):
    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)
    Ts = t0 + np.arange(0., n_steps + 1) * resolution
    ys = time_series(Ts)
    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)


t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))

t_instance = np.linspace(start = section_start, stop = section_start + resolution * (n_steps + 1),num = n_steps + 1)

plt.figure(figsize=(11,4))
plt.subplot(121)
plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"")
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)
#plt.axis([-10, 10, -17, 13])
plt.xlabel(""Time"")
plt.ylabel(""Value"")

plt.subplot(122)
plt.title(""A training instance"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""c*"", markersize=10, label=""target"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")


# In[6]:


n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])


# In[7]:


cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)                        


rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
print(rnn_outputs.get_shape())


stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons], name='reshape1')
stacked_outputs = tf.keras.layers.Dense(n_outputs,name=""hidden2"")(stacked_rnn_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs], name='reshape2')


learning_rate = 0.001

loss = tf.reduce_mean(tf.square(outputs - y)) # MSE
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()

n_iterations = 1500
batch_size = 50
save_path =os.path.join(dir_path,""model"",""recurrent_sinus_model"")

with tf.Session() as sess:
    init.run()
    for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    saver.save(sess, save_path)


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})


plt.title(""Testing the model"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""w*"", markersize=10, label=""target"")
plt.plot(t_instance[1:], y_pred[0,:,0], ""r."", markersize=10, label=""prediction"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")

plt.show()


# In[ ]:


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t.reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})



plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"",linewidth=5,c='r')
plt.plot(t[:-1], time_series(t[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)

plt.xlabel(""Time"")
plt.ylabel(""Value"")
</code></pre>
","How to use new tf.keras API with recurrent neural network? I have checked the documentation but there is no example of such a situation. There is this great book Hands on machine learning from 2017. Since that year the API of tensorflow has evolved and I am trying to rewrite recurrent neural network for time series prediction with using version 1.14 code. The code from the book is using older tf.nn.dynamic_rnn and tf.nn.rnn_cell.BasicRNNCell: And this code works just fine (except that it throws warnings about deprecation left and right). I wanted to use tf.keras API as suggested in warning. My code is the same except: But this yields following exception: so I understand that the problematic line is After checking and comparing documentation for both cells https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn and https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN I can't find the culprit. What is the difference with these two cells? How to use tf.keras API with time series? Full old code: https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb Full ""my"" code:",https://stackoverflow.com/questions/57349824,2710943,Documentation Replication on Other Examples
57392510,TensorFlow simple example help - custom gradient,"<p>How do you pass a custom gradient into a gradient optimization function in TensorFlow.</p>

<p>I have illustrated what I am trying to do, with a simple example (trying to minimize z = 2x^2 + y^2 + 2).</p>

<p>I have been looking at:
<a href=""https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/train/Optimizer</a></p>

<p>The problem seems to work if you pass in <code>optimizer = tf.train.GradientDescentOptimizer(0.55)</code> and <code>train = optimizer.minimize(z)</code></p>

<p>This code works:</p>

<pre><code>import tensorflow as tf

x = tf.Variable(11, name='x', dtype=tf.float32)
y = tf.Variable(11, name='x', dtype=tf.float32)
const = tf.constant(2.0, dtype=tf.float32)

z = x**2 + y**2 + const


optimizer = tf.train.GradientDescentOptimizer(0.55)
train = optimizer.minimize(z)

init = tf.global_variables_initializer()

def optimize():
  with tf.Session() as session:
    session.run(init)
    print(""starting at"", ""x:"", session.run(x), ""y:"", session.run(y), ""z:"", session.run(z))
    for step in range(10):  
      session.run(train)
      print(""step"", step, ""x:"", session.run(x), ""y:"", session.run(y), ""z:"", session.run(z))


optimize()
</code></pre>

<p>But I want to specify the gradient in the problem.
aka I am trying to do this:</p>

<pre><code>def function_to_minimize(x,y, const):
    # z = 2x^2 + y^2 + constant
    z = 2*x**2 + y**2 + const
    return z

def calc_grad(x,y):
    # z = 2x^2 + y^2 + constant
    dz_dx = 4*x
    dz_dy = 2*y
    return [(dz_dx, x), (dz_dy, y)]

x = tf.Variable(3, name='x', dtype=tf.float32)
y = tf.Variable(3, name='y', dtype=tf.float32)
const = tf.constant(2.0, dtype=tf.float32)


z = function_to_minimize(x,y, const)
grad = calc_grad(x,y)


init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
print(sess.run(z))
print(sess.run(grad))


optimizer = tf.train.GradientDescentOptimizer(0.5)

grads_and_vars = calc_grad(x,y)

optimizer.apply_gradients(grads_and_vars)

# minimize() takes care of both computing the gradients and applying them to the variables.
#If you want to process the gradients before applying them you can instead use the optimizer in three steps:
#     1. Compute the gradients with compute_gradients().
#     2. Process the gradients as you wish.
#     3. Apply the processed gradients with apply_gradients()
</code></pre>

<p>How do you do this properly?</p>
","How do you pass a custom gradient into a gradient optimization function in TensorFlow. I have illustrated what I am trying to do, with a simple example (trying to minimize z = 2x^2 + y^2 + 2). I have been looking at: https://www.tensorflow.org/api_docs/python/tf/train/Optimizer The problem seems to work if you pass in optimizer = tf.train.GradientDescentOptimizer(0.55) and train = optimizer.minimize(z) This code works: But I want to specify the gradient in the problem. aka I am trying to do this: How do you do this properly?",https://stackoverflow.com/questions/57392510,11895031,Documentation Replication on Other Examples
57403472,How do I add a new feature column to a tf.data.Dataset object?,"<p>I am building an input pipeline for proprietary data using Tensorflow 2.0's data module and using the tf.data.Dataset object to store my features. Here is my issue - the data source is a CSV file that has only 3 columns, a label column and then two columns which just hold strings referring to JSON files where that data is stored. I have developed functions that access all the data I need, and am able to use Dataset's map function on the columns to get the data, but I don't see how I can add a new column to my tf.data.Dataset object to hold the new data. So if anyone could help with the following questions, it would really help:</p>

<ol>
<li>How can a new feature be appended to a tf.data.Dataset object?</li>
<li>Should this process be done on the entire Dataset before iterating through it, or during (I think during iteration would allow utilization of the performance boost, but I don't know how this functionality works)?</li>
</ol>

<p>I have all the methods for taking the input as the elements from the columns and performing everything required to get the features for each element, I just don't understand how to get this data into the dataset. I could do ""hacky"" workarounds, using a Pandas Dataframe as a  ""mediator"" or something along those lines, but I want to keep everything within the Tensorflow Dataset and pipeline process, for both performance gains and higher quality code.</p>

<p>I have looked through the Tensorflow 2.0 documentation for the Dataset class (<a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset"" rel=""noreferrer"">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset</a>), but haven't been able to find a method that can manipulate the structure of the object.</p>

<p>Here is the function I use to load the original dataset:</p>

<pre><code>def load_dataset(self):
    # TODO: Function to get max number of available CPU threads
    dataset = tf.data.experimental.make_csv_dataset(self.dataset_path,
                                                    self.batch_size,
                                                    label_name='score',
                                                    shuffle_buffer_size=self.get_dataset_size(),
                                                    shuffle_seed=self.seed,
                                                    num_parallel_reads=1)
    return dataset
</code></pre>

<p>Then, I have methods which allow me to take a string input (column element) and return the actual feature data. And I am able to access the elements from the Dataset using a function like "".map"". But how do I add that as a column?</p>
","I am building an input pipeline for proprietary data using Tensorflow 2.0's data module and using the tf.data.Dataset object to store my features. Here is my issue - the data source is a CSV file that has only 3 columns, a label column and then two columns which just hold strings referring to JSON files where that data is stored. I have developed functions that access all the data I need, and am able to use Dataset's map function on the columns to get the data, but I don't see how I can add a new column to my tf.data.Dataset object to hold the new data. So if anyone could help with the following questions, it would really help: I have all the methods for taking the input as the elements from the columns and performing everything required to get the features for each element, I just don't understand how to get this data into the dataset. I could do ""hacky"" workarounds, using a Pandas Dataframe as a ""mediator"" or something along those lines, but I want to keep everything within the Tensorflow Dataset and pipeline process, for both performance gains and higher quality code. I have looked through the Tensorflow 2.0 documentation for the Dataset class (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset), but haven't been able to find a method that can manipulate the structure of the object. Here is the function I use to load the original dataset: Then, I have methods which allow me to take a string input (column element) and return the actual feature data. And I am able to access the elements from the Dataset using a function like "".map"". But how do I add that as a column?",https://stackoverflow.com/questions/57403472,7508594,Inadequate Examples
57414387,Meaning of tf.keras.layers.LSTM parameters,"<p>I am having trouble understanding some of the parameters of <code>LSTM</code> layers in the <code>tf.keras.layers</code> API. </p>

<p>I am investigating using <code>CuDNNLSTM</code> layers instead of <code>LSTM</code> layers (to speed up training), but before I commit to <code>CuDNN</code> layers, I would like to have a full understanding of the parameters that I lose by using a <code>CuDNNLSTM</code> instead of a <code>LSTM</code> layer. I have read the docs, but they seem to assume some prior knowledge of <code>LSTM</code>s that I do not have.</p>

<p>I have listed the pararameters that <code>CuDNNLSTM</code> does not have (that <code>LSTM</code> has) and interspersed with my questions about them, respectively.</p>

<ul>
<li><code>activation</code></li>
<li><code>recurrent_activation</code>

<ol>
<li>What is the difference between <code>activation</code> and <code>recurrent_activation</code>? I am assuming it has something to do with the activation for a cell vs. the activation for the full <code>LSTM</code> layer, but am unsure.</li>
</ol></li>
<li><code>use_bias</code>

<ol start=""2"">
<li>If <code>use_bias</code> is True, where is this bias applied?</li>
</ol></li>
<li><code>dropout</code></li>
<li><code>recurrent_dropout</code>

<ol start=""3"">
<li>Again, what is the difference between <code>dropout</code> and <code>recurrent_dropout</code>? If <code>recurrent_dropout</code> is dropout between the LSTM cells, that does not make sense to me, because you would be ignoring the previous output, which I thought would defeat the purpose of having an RNN.</li>
<li>Can either of these dropout parameters be substituted with a dropout layer before/after the LSTM layer (i.e. <code>tf.keras.models.sequential([Input(...), LSTM(...), Dropout(0.5)])</code> or <code>tf.keras.models.sequential([Input(...), Dropout(0.5), LSTM(...)])</code> instead of <code>tf.keras.models.sequential([Input(...), LSTM(..., dropout=0.5)])</code>)</li>
</ol></li>
<li><code>implementation</code>

<ol start=""5"">
<li>I understand why this parameter is not in <code>CuDNN</code> layers, since it would probably make it harder to parallelize. However, in <code>LSTM</code>s, does this impact the result (i.e. with the same seed, will <code>implementation=1</code> converge to the same or different result as <code>implementation=2</code>)?</li>
</ol></li>
<li><code>unroll</code></li>
</ul>

<p>I've read a lot about <code>LSTM</code>s, and am at a point where I've decided to start training things, otherwise I won't absorb much more hypothetical knowledge. I've tried a lot of things in modeling, too, but the network I'm training is really simple so nothing seems to impact the results.</p>
","I am having trouble understanding some of the parameters of LSTM layers in the tf.keras.layers API. I am investigating using CuDNNLSTM layers instead of LSTM layers (to speed up training), but before I commit to CuDNN layers, I would like to have a full understanding of the parameters that I lose by using a CuDNNLSTM instead of a LSTM layer. I have read the docs, but they seem to assume some prior knowledge of LSTMs that I do not have. I have listed the pararameters that CuDNNLSTM does not have (that LSTM has) and interspersed with my questions about them, respectively. I've read a lot about LSTMs, and am at a point where I've decided to start training things, otherwise I won't absorb much more hypothetical knowledge. I've tried a lot of things in modeling, too, but the network I'm training is really simple so nothing seems to impact the results.",https://stackoverflow.com/questions/57414387,4982425,Documentation Replicability
57449484,What is trainable parameter in tensorflow?,"<p>tf.compat.v1.layers.batch_normalization takes <code>trainable</code> as an input. The documentation says:</p>

<blockquote>
  <p>Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).</p>
</blockquote>

<p>I think only scaling factor (gamma) and offset (beta) should be added to trainable variables and I am skeptical if even moving averages will get added to GraphKeys.TRAINABLE_VARIABLES. Can somebody tell me how trainable input is influencing the behavior of batch_normalization</p>
",tf.compat.v1.layers.batch_normalization takes trainable as an input. The documentation says: I think only scaling factor (gamma) and offset (beta) should be added to trainable variables and I am skeptical if even moving averages will get added to GraphKeys.TRAINABLE_VARIABLES. Can somebody tell me how trainable input is influencing the behavior of batch_normalization,https://stackoverflow.com/questions/57449484,6546694,Documentation Replicability
57460127,Tensorflow 2 creating custom dataset,"<p>I am trying to build a custom dataset-loader, which laods <a href=""https://rrc.cvc.uab.es/?ch=4"" rel=""nofollow noreferrer"">ICDAR</a>-Dataset.
My frist step was to embed a dataset inside my loader as suggested also
<a href=""https://stackoverflow.com/questions/54373780/create-tensorflow-dataset-with-custom-file-format"">here</a> in this post, but the problem is that you have to implement all the nice features that the tenfsoflow-2 class ""Dataset"" offers manually.</p>

<p>My second try was to subclass the Dataset-Class, something like:</p>

<pre><code>class MyDataset(tf.data.Dataset):
  def __init__(self):
    super(MyDataset, self).init()

  def preprocess_images(self):
    pass
</code></pre>

<p>But the problem is i did not find any documentation what dataset-class internally really does, the only implementation i found was <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L2349"" rel=""nofollow noreferrer"">this one</a>.</p>

<p>So question is does anybody know how to build a custom ""dataset"" in tf2 by subclassing tf.data.Dataset.</p>

<p>By the way i also tried tensorflow_datasets, bit it does not really worked, shince it will downlaod the dataset, and split them manually which is in this is alreay seperated by train and test and also ICDAr can not be downlaoded without registration.</p>

<p><strong>The content of the ICDAR-Dataset is as following:</strong></p>

<blockquote>
  <p>An Image </p>
  
  <p>A List of all texts in each image </p>
  
  <p>A List of Bouding-boxes for each text in each image</p>
</blockquote>

<p><strong>Image:</strong> 
@<a href=""https://rrc.cvc.uab.es/?ch=4"" rel=""nofollow noreferrer"">https://rrc.cvc.uab.es/?ch=4</a> owns the copyrights of this image.
<a href=""https://i.stack.imgur.com/YCVX2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YCVX2.jpg"" alt=""enter image description here""></a></p>

<p>Words and bounding boxes for the above image:</p>

<pre><code>377,117,463,117,465,130,378,130,Genaxis Theatre
493,115,519,115,519,131,493,131,[06]
374,155,409,155,409,170,374,170,###
492,151,551,151,551,170,492,170,62-03
376,198,422,198,422,212,376,212,Carpark
494,190,539,189,539,205,494,206,###
374,1,494,0,492,85,372,86,###
</code></pre>

<p>Thanks
does anyone know how to </p>
","I am trying to build a custom dataset-loader, which laods ICDAR-Dataset. My frist step was to embed a dataset inside my loader as suggested also here in this post, but the problem is that you have to implement all the nice features that the tenfsoflow-2 class ""Dataset"" offers manually. My second try was to subclass the Dataset-Class, something like: But the problem is i did not find any documentation what dataset-class internally really does, the only implementation i found was this one. So question is does anybody know how to build a custom ""dataset"" in tf2 by subclassing tf.data.Dataset. By the way i also tried tensorflow_datasets, bit it does not really worked, shince it will downlaod the dataset, and split them manually which is in this is alreay seperated by train and test and also ICDAr can not be downlaoded without registration. The content of the ICDAR-Dataset is as following: Image: @https://rrc.cvc.uab.es/?ch=4 owns the copyrights of this image. Words and bounding boxes for the above image: Thanks does anyone know how to",https://stackoverflow.com/questions/57460127,2335224,Lack of Alternative Solutions/Documentation
57570385,"How to generate custom mini-batches using Tensorflow 2.0, such as those in the paper ""In defense of the triplet loss""?","<p>I want to implement a custom mini-batch generator in Tensorflow 2.0 using tf.data.Dataset API. Concretely, I have image data, 100 classes with ~200 examples each. For each mini-batch, I want to randomly sample P classes, and K images from each class, for a total of P*K examples in a mini-batch (as described in the paper <a href=""https://arxiv.org/pdf/1703.07737.pdf"" rel=""nofollow noreferrer"">In Defense of the Triplet Loss for Person Re-Identification</a>]).</p>

<p>I've been searching through documentation for <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">tf.data.Dataset</a>, but can't seem to find the right method. I've looked into the <code>from_generator</code> method, but it doesn't seem suitable for this, since it generates a whole dataset from scratch as I understood.</p>

<p>It seems to me that one way to do it would be to make a new class similar to <code>BatchDataset</code> which can be found in <a href=""https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/data/ops/dataset_ops.py#L90-L1536"" rel=""nofollow noreferrer"">tf.data.Dataset source code</a>, where I would somehow implement the logic, but I'm hoping for an easier solution to be honest.</p>
","I want to implement a custom mini-batch generator in Tensorflow 2.0 using tf.data.Dataset API. Concretely, I have image data, 100 classes with ~200 examples each. For each mini-batch, I want to randomly sample P classes, and K images from each class, for a total of P*K examples in a mini-batch (as described in the paper In Defense of the Triplet Loss for Person Re-Identification]). I've been searching through documentation for tf.data.Dataset, but can't seem to find the right method. I've looked into the from_generator method, but it doesn't seem suitable for this, since it generates a whole dataset from scratch as I understood. It seems to me that one way to do it would be to make a new class similar to BatchDataset which can be found in tf.data.Dataset source code, where I would somehow implement the logic, but I'm hoping for an easier solution to be honest.",https://stackoverflow.com/questions/57570385,4301911,Requesting (Additional) Resources
57651287,How to swap tensor axes efficiently in tensorflow?,"<p>I have to swap tensor's axes using <code>tf.transpose</code> to do the batch matrix multiplication (as the code shown below). </p>

<p>tensor input_a: shape [10000, 10000] </p>

<p>tensor input_b: shape [batch_size, 10000, 10] </p>

<p>tensor output:  shape [batch_size, 10000, 10] </p>

<pre><code># reshape_input_b: shape [10000, batch_size, 10]
transpose_input_b = tf.transpose(input_b, [1, 0, 2])

# transpose_input_b : shape [10000, batch_size * 10]
reshape_input_b = tf.reshape(transpose_input_b , [10000, -1])

# ret: shape [10000, batch_size * 10]
ret = tf.matmul(input_a, reshape_input_b, a_is_sparse = True)

# reshape_ret: [10000, batch_size, 10]
reshape_ret = tf.reshape(ret, [10000, -1, 10])

# output : [batch_size, 10000, 10]
output = tf.transpose(reshape_ret, [1, 0, 2])
</code></pre>

<p>However, it seems very slow. I noticed this in the document page of <code>tf.transpose</code>:</p>

<blockquote>
  <p>In numpy transposes are memory-efficient constant time operations as they simply return a new view of the same data with adjusted strides.</p>
  
  <p>TensorFlow does not support strides, <b>so transpose returns a new tensor with the items permuted</b>.</p>
</blockquote>

<p>So, I think it might be the reason why my code run slowly? Is there any way to swap tensor's axes, or do the batch matrix multiplication efficiently?</p>
","I have to swap tensor's axes using tf.transpose to do the batch matrix multiplication (as the code shown below). tensor input_a: shape [10000, 10000] tensor input_b: shape [batch_size, 10000, 10] tensor output: shape [batch_size, 10000, 10] However, it seems very slow. I noticed this in the document page of tf.transpose: So, I think it might be the reason why my code run slowly? Is there any way to swap tensor's axes, or do the batch matrix multiplication efficiently?",https://stackoverflow.com/questions/57651287,9427280,Inadequate Examples
57717004,Tensorflow: Modern way to load large data,"<p>I want to train a convolutional neural network (using tf.keras from Tensorflow version 1.13) using numpy arrays as input data. The training data (which I currently store in a single &gt;30GB '.npz' file) does not fit in RAM all at once. <strong>What is the best way to save and load large data-sets into a neural network for training?</strong> Since I didn't manage to find a good answer to this (surely ubiquitous?) problem, I'm hoping to hear one here. Thank you very much in advance for any help!</p>
<h3>Sources</h3>
<p>Similar questions seem to have been asked many times (e.g. <a href=""https://stackoverflow.com/questions/49169016/training-classifier-from-tfrecords-in-tensorflow"">training-classifier-from-tfrecords-in-tensorflow</a>, <a href=""https://stackoverflow.com/questions/40467990/tensorflow-synchronize-readings-from-tfrecord"">tensorflow-synchronize-readings-from-tfrecord</a>, <a href=""https://stackoverflow.com/questions/51357223/how-to-load-data-parallelly-in-tensorflow"">how-to-load-data-parallelly-in-tensorflow</a>) but are several years old and usually contain no conclusive answer.</p>
<p>My current understanding is that using TFRecord files is a good way to approach this problem. The most promising tutorial I found so far explaining how to use TFRecord files with keras is <a href=""https://medium.com/@moritzkrger/speeding-up-keras-with-tfrecord-datasets-5464f9836c36"" rel=""noreferrer"">medium.com</a>. Other helpful sources were <a href=""http://machinelearninguru.com/deep_learning/data_preparation/tfrecord/tfrecord.html"" rel=""noreferrer"">machinelearninguru.com</a> and <a href=""https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564"" rel=""noreferrer"">medium.com_source2</a> and sources therin.</p>
<p>The official tensorflow documentation and tutorials (on <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer"">tf.data.Dataset</a>, <a href=""https://www.tensorflow.org/guide/datasets"" rel=""noreferrer"">Importing Data</a>, <a href=""https://www.tensorflow.org/tutorials/load_data/tf_records"" rel=""noreferrer"">tf_records</a> etc.) did not help me. In particular, several of the examples given there didn't work for me even without modifications.</p>
<h3>My Attempt at using TFRecord files</h3>
<p>I'm assuming TFRecords are a good way to solve my problem but I'm having a hard time using them. Here is an example I made based on the tutorial <a href=""https://medium.com/@moritzkrger/speeding-up-keras-with-tfrecord-datasets-5464f9836c36"" rel=""noreferrer"">medium.com</a>. I stripped down the code as much as I could.</p>
<pre><code># python 3.6, tensorflow 1.13.
# Adapted from https://medium.com/@moritzkrger/speeding-up-keras-with-tfrecord-datasets-5464f9836c36
import tensorflow as tf
import numpy as np
from tensorflow.python import keras as keras


# Helper functions (see also https://www.tensorflow.org/tutorials/load_data/tf_records)
def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))


def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


def writeTFRecords():
    number_of_samples = 100  # create some random data to play with
    images, labels = (np.random.sample((number_of_samples, 256, 256, 1)), np.random.randint(0, 30, number_of_samples))

    writer = tf.python_io.TFRecordWriter(&quot;bla.tfrecord&quot;)

    for index in range(images.shape[0]):
        image = images[index]
        label = labels[index]

        feature = {'image':  _bytes_feature(tf.compat.as_bytes(image.tostring())),
                   'label':  _int64_feature(int(label))}

        example = tf.train.Example(features=tf.train.Features(feature=feature))
        writer.write(example.SerializeToString())
    writer.close()


def loadTFRecord(data_path):
    with tf.Session() as sess:
        feature = {'train/image': tf.FixedLenFeature([], tf.string),
                   'train/label': tf.FixedLenFeature([], tf.int64)}
        # Create a list of filenames and pass it to a queue
        filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)
        # Define a reader and read the next record
        reader = tf.TFRecordReader()
        _, serialized_example = reader.read(filename_queue)
        # Decode the record read by the reader
        features = tf.parse_single_example(serialized_example, features=feature)
        # Convert the image data from string back to the numbers
        image = tf.decode_raw(features['train/image'], tf.float32)

        # Cast label data into int32
        label = tf.cast(features['train/label'], tf.int32)
        # Reshape image data into the original shape
        image = tf.reshape(image, [256, 256, 1])

        return image, label  # I'm not 100% sure that's how this works...


# ######### generate a TFRecords file in the working directory containing random data. #################################
writeTFRecords()
# ######## Load the TFRecords file and use it to train a simple example neural network. ################################
image, label = loadTFRecord(&quot;bla.tfrecord&quot;)

model_input = keras.layers.Input(tensor=image)
model_output = keras.layers.Flatten(input_shape=(-1, 256, 256, 1))(model_input)
model_output = keras.layers.Dense(16, activation='relu')(model_output)

train_model = keras.models.Model(inputs=model_input, outputs=model_output)
train_model.compile(optimizer=keras.optimizers.RMSprop(lr=0.0001),  
                    loss='mean_squared_error',
                    target_tensors=[label])

print(&quot;\n \n start training \n \n&quot;) # Execution gets stuck on fitting
train_model.fit(epochs=1, steps_per_epoch=10)  # no output or error messages.

</code></pre>
<p>The code creates a TFRecord file and starts fitting, then just gets stuck with no output or error messages. I don't know what the problem is or how I could try to fix it.</p>
","I want to train a convolutional neural network (using tf.keras from Tensorflow version 1.13) using numpy arrays as input data. The training data (which I currently store in a single &gt;30GB '.npz' file) does not fit in RAM all at once. What is the best way to save and load large data-sets into a neural network for training? Since I didn't manage to find a good answer to this (surely ubiquitous?) problem, I'm hoping to hear one here. Thank you very much in advance for any help! Similar questions seem to have been asked many times (e.g. training-classifier-from-tfrecords-in-tensorflow, tensorflow-synchronize-readings-from-tfrecord, how-to-load-data-parallelly-in-tensorflow) but are several years old and usually contain no conclusive answer. My current understanding is that using TFRecord files is a good way to approach this problem. The most promising tutorial I found so far explaining how to use TFRecord files with keras is medium.com. Other helpful sources were machinelearninguru.com and medium.com_source2 and sources therin. The official tensorflow documentation and tutorials (on tf.data.Dataset, Importing Data, tf_records etc.) did not help me. In particular, several of the examples given there didn't work for me even without modifications. I'm assuming TFRecords are a good way to solve my problem but I'm having a hard time using them. Here is an example I made based on the tutorial medium.com. I stripped down the code as much as I could. The code creates a TFRecord file and starts fitting, then just gets stuck with no output or error messages. I don't know what the problem is or how I could try to fix it.",https://stackoverflow.com/questions/57717004,9988487,Documentation Replication on Other Examples
57719398,Unable to save model with tensorflow 2.0.0 beta1,"<p>I have tried all the options described in the documentation but none of them allowed me to save my model in tensorflow 2.0.0 beta1. I've also tried to upgrade to the (also unstable) TF2-RC but that ruined even the code I had working in beta so I quickly rolled back for now to beta.</p>

<p>See a minimal reproduction code below.</p>

<p>What I have tried: </p>

<ol>
<li><pre><code>model.save(""mymodel.h5"") 
</code></pre></li>
</ol>

<blockquote>
  <p>NotImplementedError: Saving the model to HDF5 format requires the
  model to be a Functional model or a Sequential model. It does not work
  for subclassed models, because such models are defined via the body of
  a Python method, which isn't safely serializable. Consider saving to
  the Tensorflow SavedModel format (by setting save_format=""tf"") or
  using <code>save_weights</code>.</p>
</blockquote>

<ol start=""2"">
<li><pre><code>model.save(""mymodel"", format='tf')
</code></pre></li>
</ol>

<blockquote>
  <p>ValueError: Model &lt;<strong>main</strong>.CVAE object at 0x7f1cac2e7c50> cannot be
  saved because the input shapes have not been set. Usually, input
  shapes are automatically determined from calling .fit() or .predict().
  To manually set the shapes, call model._set_inputs(inputs).</p>
</blockquote>

<p>3.</p>

<pre><code>model._set_input(input_sample)
model.save(""mymodel"", format='tf') 
</code></pre>

<blockquote>
  <p>AssertionError: tf.saved_model.save is not supported inside a traced
  @tf.function. Move the call to the outer eagerly-executed context.</p>
</blockquote>

<p>And this is where I am stuck now because it gives me no reasonable hint whatsoever. That's because I am NOT calling the save() function from a @tf.function, I'm already calling it from the outermost scope possible. In fact, I have no @tf.function at all in this minimal reproduction script below and still getting the same error.</p>

<p>So I really have no idea how to save my model, I've tried every options and they all throw errors and provide no hints.</p>

<p>The minimal reproduction example below works fine if you set save_model=False and it reproduces the error when save_model=True. </p>

<p>It may seem unnecessary in this simplified auto-encoder code example to use a subclassed model but I have lots of custom functions added to it in my original VAE code that I need it for.</p>

<p>Code:</p>

<pre><code>import tensorflow as tf

save_model = True

learning_rate = 1e-4
BATCH_SIZE = 100
TEST_BATCH_SIZE = 10
color_channels = 1
imsize = 28

(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()

train_images = train_images[:5000, ::]
test_images = train_images[:1000, ::]
train_images = train_images.reshape(-1, imsize, imsize, 1).astype('float32')
test_images = test_images.reshape(-1, imsize, imsize, 1).astype('float32')
train_images /= 255.
test_images /= 255.
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).batch(BATCH_SIZE)
test_dataset = tf.data.Dataset.from_tensor_slices(test_images).batch(TEST_BATCH_SIZE)

class AE(tf.keras.Model):
    def __init__(self):
        super(AE, self).__init__()
        self.network = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=(imsize, imsize, color_channels)),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(50),
            tf.keras.layers.Dense(imsize**2 * color_channels),
            tf.keras.layers.Reshape(target_shape=(imsize, imsize, color_channels)),
        ])
    def decode(self, input):
        logits = self.network(input)
        return logits

optimizer = tf.keras.optimizers.Adam(learning_rate)
model = AE()

def compute_loss(data):
    logits = model.decode(data)
    loss = tf.reduce_mean(tf.losses.mean_squared_error(logits, data))
    return loss

def train_step(data):
    with tf.GradientTape() as tape:
        loss = compute_loss(data)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss, 0

def test_step(data):
    loss = compute_loss(data)
    return loss

input_shape_set = False
epoch = 0
epochs = 20
for epoch in range(epochs):
    for train_x in train_dataset:
        train_step(train_x)
    if epoch % 1 == 0:
        loss = 0.0
        num_batches = 0
        for test_x in test_dataset:
            loss += test_step(test_x)
            num_batches += 1
        loss /= num_batches
        print(""Epoch: {}, Loss: {}"".format(epoch, loss))

        if save_model:
            print(""Saving model..."")
            if not input_shape_set:
                # Note: Why set input shape manually and why here:
                # 1. If I do not set input shape manually: ValueError: Model &lt;main.CVAE object at 0x7f1cac2e7c50&gt; cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).
                # 2. If I set input shape manually BEFORE the first actual train step, I get: RuntimeError: Attempting to capture an EagerTensor without building a function.
                model._set_inputs(train_dataset.__iter__().next())
                input_shape_set = True
            # Note: Why choose tf format: model.save('MNIST/Models/model.h5') will return NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=""tf"") or using save_weights.
            model.save('MNIST/Models/model', save_format='tf')
</code></pre>
","I have tried all the options described in the documentation but none of them allowed me to save my model in tensorflow 2.0.0 beta1. I've also tried to upgrade to the (also unstable) TF2-RC but that ruined even the code I had working in beta so I quickly rolled back for now to beta. See a minimal reproduction code below. What I have tried: 3. And this is where I am stuck now because it gives me no reasonable hint whatsoever. That's because I am NOT calling the save() function from a @tf.function, I'm already calling it from the outermost scope possible. In fact, I have no @tf.function at all in this minimal reproduction script below and still getting the same error. So I really have no idea how to save my model, I've tried every options and they all throw errors and provide no hints. The minimal reproduction example below works fine if you set save_model=False and it reproduces the error when save_model=True. It may seem unnecessary in this simplified auto-encoder code example to use a subclassed model but I have lots of custom functions added to it in my original VAE code that I need it for. Code:",https://stackoverflow.com/questions/57719398,4515762,Documentation Replicability
57813806,Apply feature columns without tf.Estimator (Tensorflow 2.0.0-rc0),"<p>In the Tensorflow tf.Estimator and tf.feature_column docs it is well documented, how to use feature columns together with an Estimator e.g. in order to one-hot encode the categorical features in the dataset being used.</p>

<p>However, I want to ""apply"" my feature columns directly to a tf.dataset which I create from a .csv file (with two columns: UserID, MovieID), without even defining a model or an Estimator. (Reason: I want to check what's happening exactly in my datapipeline, i.e. I'd like to be able to run a batch of samples through my the pipeline, and then see in the output how the features got encoded.)</p>

<p>This is what I have tried so far:</p>

<pre><code>column_names = ['UserID', 'MovieID']

user_col = tf.feature_column.categorical_column_with_hash_bucket(key='UserID', hash_bucket_size=1000)
movie_col = tf.feature_column.categorical_column_with_hash_bucket(key='MovieID', hash_bucket_size=1000)
feature_columns = [tf.feature_column.indicator_column(user_col), tf.feature_column.indicator_column(movie_col)]

feature_layer = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)

def process_csv(line):
  fields = tf.io.decode_csv(line, record_defaults=[tf.constant([], dtype=tf.int32)]*2, field_delim="";"")
  features = dict(zip(column_names, fields))

  return features 

ds = tf.data.TextLineDataset(csv_filepath)
ds = ds.map(process_csv, num_parallel_calls=4)
ds = ds.batch(10)
ds.map(lambda x: feature_layer(x))
</code></pre>

<p>However the last line with the map call raises the following error:</p>

<blockquote>
  <p>ValueError: Column dtype and SparseTensors dtype must be compatible.
  key: MovieID, column dtype: , tensor dtype: </p>
</blockquote>

<p>I'm not sure what this error means...
I also tried to define a tf.keras model with only the feature_layer I defined, and then run .predict() on my dataset - instead of using ds.map(lambda x: feature_layer(x)):</p>

<pre><code>model = tf.keras.Sequential([feature_layer])
model.compile()
model.predict(ds)
</code></pre>

<p>However, this results exactly in the same error as above.
Does anybody have an idea what is going wrong? Is there maybe an easier way to achieve this?</p>
","In the Tensorflow tf.Estimator and tf.feature_column docs it is well documented, how to use feature columns together with an Estimator e.g. in order to one-hot encode the categorical features in the dataset being used. However, I want to ""apply"" my feature columns directly to a tf.dataset which I create from a .csv file (with two columns: UserID, MovieID), without even defining a model or an Estimator. (Reason: I want to check what's happening exactly in my datapipeline, i.e. I'd like to be able to run a batch of samples through my the pipeline, and then see in the output how the features got encoded.) This is what I have tried so far: However the last line with the map call raises the following error: I'm not sure what this error means... I also tried to define a tf.keras model with only the feature_layer I defined, and then run .predict() on my dataset - instead of using ds.map(lambda x: feature_layer(x)): However, this results exactly in the same error as above. Does anybody have an idea what is going wrong? Is there maybe an easier way to achieve this?",https://stackoverflow.com/questions/57813806,8725045,Documentation Replication on Other Examples
57823210,How to combine tf.data.Dataset and tf.estimator.DNNRegressor properly,"<p>I am currently learning to use tensorflow and have troubles getting started.
I would like to use the newest API, namely estimator and dataset. But if I run the code presented below I get an Error.</p>

<p>On the tensorflow page <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor</a> I found, that ""The function should construct and return one of the following: * A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below.""</p>

<p>I thought my code would provide that, but there seems to be a problem and I am out of ideas.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
def input_evaluation_set():
    data = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]
    labels = []
    for d in data:
        labels.append(1)
    return tf.data.Dataset.from_tensor_slices((tf.constant(data), tf.constant(labels)))

point = tf.feature_column.numeric_column('points')
estimator = tf.estimator.DNNRegressor(feature_columns = [point],hidden_units = [100,100,100])

estimator.train(input_fn = input_evaluation_set)
</code></pre>

<p>I expect to run a training session on a deep neural network with 3 hidden layers a' 100 neurons in order to approximate the 'constant 1' function;
instead I get the Error ""ValueError: features should be a dictionary of 'Tensor's. Given type: class, 'tensorflow.python.framework.ops.Tensor'</p>
","I am currently learning to use tensorflow and have troubles getting started. I would like to use the newest API, namely estimator and dataset. But if I run the code presented below I get an Error. On the tensorflow page https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor I found, that ""The function should construct and return one of the following: * A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below."" I thought my code would provide that, but there seems to be a problem and I am out of ideas. I expect to run a training session on a deep neural network with 3 hidden layers a' 100 neurons in order to approximate the 'constant 1' function; instead I get the Error ""ValueError: features should be a dictionary of 'Tensor's. Given type: class, 'tensorflow.python.framework.ops.Tensor'",https://stackoverflow.com/questions/57823210,12030773,Documentation Replication on Other Examples
57872334,parallel inference in tensorflow (CPUs),"<p>this is a really basic question, but I can't get the answer anywhere.</p>

<p>Using tensorflow, can I do inference (<code>tf.keras.Model.predict()</code>) on multiple CPUs in parallel? I am using <code>tf.data.Dataset</code> to represent my data, but from the <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model#predict"" rel=""nofollow noreferrer"">documentation</a> it seems multiprocessing can be used only with generators or tf.keras.Sequence objects. Why is that? Am I supposed to somehow create an ordinary python generator from the <code>Dataset</code> or to manage the parallelism on my own with <code>multiprocessing</code> package? What would be the standard way of doing this?</p>

<p>Thanks for any hints.</p>
","this is a really basic question, but I can't get the answer anywhere. Using tensorflow, can I do inference (tf.keras.Model.predict()) on multiple CPUs in parallel? I am using tf.data.Dataset to represent my data, but from the documentation it seems multiprocessing can be used only with generators or tf.keras.Sequence objects. Why is that? Am I supposed to somehow create an ordinary python generator from the Dataset or to manage the parallelism on my own with multiprocessing package? What would be the standard way of doing this? Thanks for any hints.",https://stackoverflow.com/questions/57872334,3447343,Documentation Replication on Other Examples
57921463,How tf.data.experimental.group_by_window() operates in Tensorflow 2.0,"<p>I am trying to understand the tf.data.experimental.group_by_window() method in Tensorflow 2 but I have some difficulties.</p>

<p>For a reproducible example I use the one presented in the documentation:</p>

<pre><code>components = np.arange(100).astype(np.int64)
dataset20 = tf.data.Dataset.from_tensor_slices(components)
dataset20 = dataset.apply(tf.data.experimental.group_by_window(key_func=lambda x: x%2, reduce_func=lambda _,\
                                                          els: els.batch(10), window_size=100))

i = 0

for elem in dataset20:

    print('i is {0}\n'.format(i))

    print('elem is {0}'.format(elem.numpy()))

    i += 1

    print('\n--------------------------------\n')

i is 0

elem is [0 2 4 6 8]

--------------------------------

i is 1

elem is [1 3 5 7 9]

--------------------------------
</code></pre>
",I am trying to understand the tf.data.experimental.group_by_window() method in Tensorflow 2 but I have some difficulties. For a reproducible example I use the one presented in the documentation:,https://stackoverflow.com/questions/57921463,8270077,Documentation Ambiguity
57929803,What is the proper way to convert Tracing Code using RunOptions to Tensorflow 2.0?,"<p>I'm having difficulty finding any documentation on how to migrate tracing code from 1.x to 2.0.</p>

<p>In tensorflow 1.x you could do the following:</p>

<pre><code>run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
run_metadata = tf.compat.v1.RunMetadata()
final_result = sess.run(result, feed_dict={...},
                        options=run_options,
                        run_metadata=run_metadata)

trace = fetched_timeline = timeline.Timeline(run_metadata.step_stats)
chrome_trace = fetched_timeline.generate_chrome_trace_format()
with open('timeline.json', 'w') as f:
    f.write(chrome_trace)
</code></pre>

<p>How can you do a similar thing with a @tf.function call?</p>

<pre><code>@tf.function
def predict(x1, x2):
    ...
#=============
# Set run options and RunMetadata variables
#=============
#=============
final_result = predict(x1_val, x2_val)

#=============
# Dump Trace (assuming run_metadata is the RunMetaData object we configured previously)
#=============
trace = fetched_timeline = timeline.Timeline(run_metadata.step_stats)
chrome_trace = fetched_timeline.generate_chrome_trace_format()
with open('timeline.json', 'w') as f:
    f.write(chrome_trace)
#=============

</code></pre>
",I'm having difficulty finding any documentation on how to migrate tracing code from 1.x to 2.0. In tensorflow 1.x you could do the following: How can you do a similar thing with a @tf.function call?,https://stackoverflow.com/questions/57929803,314864,Inadequate Examples
57970717,Using pretrained convolutional network as a GAN discriminator,"<p>I've pulled some code from TF2.0 documentation to generate images from a custom dataset. The code is <a href=""https://www.tensorflow.org/beta/tutorials/generative/dcgan"" rel=""nofollow noreferrer"">here</a> </p>

<p>Since the documentation uses Keras i figured i might change the discriminator network to a pretrained network e.g InceptionV3, and only train the top layers. I've found <a href=""https://keras.io/applications/"" rel=""nofollow noreferrer"">this</a> code (Fine-tune InceptionV3 on a new set of classes). I cant seem to figure out how to replace the the one with the other. I understand that im trying to replace Sequential mode with the Functional API. But i guess they are somehow interconnected. However, im not a frequent Keras user.</p>

<p>My questions is: How do i replace a custom CNN in Sequential mode with a pretrained one from the Functional API to use as a discriminator?</p>

<p>EDIT: I would be happy if anyone has examples of doing it with the GANEstimator instead as im more used to TF.</p>

<p><strong>Use the generator to generate a random image</strong></p>

<pre><code>def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 3)

    return model

generator = make_generator_model()
noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)
</code></pre>

<p><strong>The current discriminator and helpers (Outputs tf.Tensor([[-0.0003378]], shape=(1, 1), dtype=float32))</strong></p>

<pre><code>def make_discriminator_model():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                                     input_shape=[28, 28, 1]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1))

    return model

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

discriminator = make_discriminator_model()
decision = discriminator(generated_image)
print (decision)
</code></pre>

<p><strong>The desired discriminator</strong></p>

<pre><code>def make_discriminator_model():
    # create the base pre-trained model
    model = InceptionV3(weights='imagenet', include_top=False)

    # ADD TOP LAYERS

    # FREEZE ALL LAYERS EXCEPT TOP LAYERS

    return model

# COMPILE

def discriminator_loss(real_output, fake_output):
    real_loss = ??? # Real Loss
    fake_loss = ??? # Fake loss
    total_loss = real_loss + fake_loss
    return total_loss

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)

discriminator = make_discriminator_model()
decision = discriminator(generated_image)
print (decision)
</code></pre>

<p><strong>All imports</strong></p>

<pre><code>  from __future__ import absolute_import, division, print_function, unicode_literals

try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf
print('TF version: {}'.format(tf.__version__))

import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
from PIL import Image
from tensorflow.keras import layers
import time

from IPython import display
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import vgg16
import os.path
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras import backend as K
</code></pre>

<p><strong>EDIT:
This was the discriminator i ended up with! Thanks to @pandrey</strong></p>

<pre><code>def make_discriminator_model():
    pre_trained = tf.keras.applications.InceptionV3(
        weights='imagenet', include_top=False, input_shape=IMG_SHAPE
    )
    pre_trained.trainable = False  # mark all weights as non-trainable
    model = tf.keras.Sequential([pre_trained])
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dense(1))   
    return model
</code></pre>
","I've pulled some code from TF2.0 documentation to generate images from a custom dataset. The code is here Since the documentation uses Keras i figured i might change the discriminator network to a pretrained network e.g InceptionV3, and only train the top layers. I've found this code (Fine-tune InceptionV3 on a new set of classes). I cant seem to figure out how to replace the the one with the other. I understand that im trying to replace Sequential mode with the Functional API. But i guess they are somehow interconnected. However, im not a frequent Keras user. My questions is: How do i replace a custom CNN in Sequential mode with a pretrained one from the Functional API to use as a discriminator? EDIT: I would be happy if anyone has examples of doing it with the GANEstimator instead as im more used to TF. Use the generator to generate a random image The current discriminator and helpers (Outputs tf.Tensor([[-0.0003378]], shape=(1, 1), dtype=float32)) The desired discriminator All imports EDIT: This was the discriminator i ended up with! Thanks to @pandrey",https://stackoverflow.com/questions/57970717,11825110,Requesting (Additional) Resources
57995171,Need a clear simple approach to distributed learning in Tensorflow/Keras,"<p>I have multiple machines, some with GPUs and some others not. I also have a keras model that works fine on a single machine but I want to train it in a distributed mode because I want to test it with a huge dataset and with a bigger number of layers.
There is quite a lot of pages discussing the distribution strategy in tf.distribute, but at the same time there are a lot other pages showing how to do it with encapsulating keras model with an estimator, setting up the TF_CONFIG parameter and then call <code>tf.estimator.train_and_evaluate</code>.
I used the second approach personally as it was more straightforward and am struggling to tune and debug it. It works anyway, but I am very confused what is the point in all of strategy-related stuff as I don't see any use them in the second approach, and the documentation is not helping to clear it.</p>

<p>I also have some doubt if my file setting environment is correct: My understanding is that the PS server is going to hold the model parameters and the chief server is going to administer the whole training process, distributing data, and saving summaries and checkpoints. So I assume that:</p>

<p>0- I need only one chief server, at least one PS server, possibly some workers, and one evaluator. The data and parameter sharing and communication between all of these servers is done by system and I am not engaged in it.</p>

<p>1- The main python code for all machines should be exactly the same, except in TF_CONFIG definition that defines the task and index for that specific machine.</p>

<p>2- I should have one shared copy of data in a folder available to all chief and workers.</p>

<p>3- I should have one shared log directory accessible by all machines as defined in tf.estimator.RunConfig.</p>

<p>4- Having this setting then a piece of code such as below will do the job (assuming the <code>model</code> has been defined elsewhere and the <code>read_datasets</code> function returns features and labels for running the model):</p>

<pre><code>runConfig = tf.estimator.RunConfig(
        session_config=config,
        model_dir=log_dir,
        save_summary_steps=1,
        save_checkpoints_steps=train_steps
        )
estimator = tf.keras.estimator.model_to_estimator(model, model_dir=log_dir, config=runConfig)
train_spec = tf.estimator.TrainSpec(input_fn=lambda: read_datasets(...), max_steps=epochs*train_steps)
eval_spec = tf.estimator.EvalSpec(input_fn=lambda: read_datasets(...), start_delay_secs=1, throttle_secs=1)
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
</code></pre>

<p>Although the above approach seems to work fine, I still have some difficulties understanding how the chief is partitioning the dataset among the workers and how to set the train_step and batch_size in this approach. Also I don't know how can I report accuracy and other metrics such as precision/recall/F1 in addition to loss when running the <code>tf.estimator.evaluate</code> without writing a custom model_fn for my encapsulated keras estimator.</p>
","I have multiple machines, some with GPUs and some others not. I also have a keras model that works fine on a single machine but I want to train it in a distributed mode because I want to test it with a huge dataset and with a bigger number of layers. There is quite a lot of pages discussing the distribution strategy in tf.distribute, but at the same time there are a lot other pages showing how to do it with encapsulating keras model with an estimator, setting up the TF_CONFIG parameter and then call tf.estimator.train_and_evaluate. I used the second approach personally as it was more straightforward and am struggling to tune and debug it. It works anyway, but I am very confused what is the point in all of strategy-related stuff as I don't see any use them in the second approach, and the documentation is not helping to clear it. I also have some doubt if my file setting environment is correct: My understanding is that the PS server is going to hold the model parameters and the chief server is going to administer the whole training process, distributing data, and saving summaries and checkpoints. So I assume that: 0- I need only one chief server, at least one PS server, possibly some workers, and one evaluator. The data and parameter sharing and communication between all of these servers is done by system and I am not engaged in it. 1- The main python code for all machines should be exactly the same, except in TF_CONFIG definition that defines the task and index for that specific machine. 2- I should have one shared copy of data in a folder available to all chief and workers. 3- I should have one shared log directory accessible by all machines as defined in tf.estimator.RunConfig. 4- Having this setting then a piece of code such as below will do the job (assuming the model has been defined elsewhere and the read_datasets function returns features and labels for running the model): Although the above approach seems to work fine, I still have some difficulties understanding how the chief is partitioning the dataset among the workers and how to set the train_step and batch_size in this approach. Also I don't know how can I report accuracy and other metrics such as precision/recall/F1 in addition to loss when running the tf.estimator.evaluate without writing a custom model_fn for my encapsulated keras estimator.",https://stackoverflow.com/questions/57995171,6450489,Documentation Replication on Other Examples
58096095,How does tf.audio.decode_wav get its contents?,"<p>I'm trying to pull some audio files into Tensorflow by using <code>tf.audio.decode_wav</code>.</p>

<p>I can see someone is looking into providing more info in the docs, but does anyone have any examples of how this should work?</p>

<pre><code>tf.audio.decode_wav(
 contents,
 desired_channels=-1,
 desired_samples=-1,
 name=None
)
</code></pre>

<p><strong>Args:</strong></p>

<ul>
<li>contents: A Tensor of type string. The WAV-encoded audio, usually from a file.</li>
<li>desired_channels: An optional int. Defaults to -1. Number of sample channels wanted.</li>
<li>desired_samples: An optional int. Defaults to -1. Length of audio requested.</li>
<li>name: A name for the operation (optional).</li>
</ul>

<p>I'm guessing the contents is a tensor which has already been pulled from a file rather than a path?</p>
","I'm trying to pull some audio files into Tensorflow by using tf.audio.decode_wav. I can see someone is looking into providing more info in the docs, but does anyone have any examples of how this should work? Args: I'm guessing the contents is a tensor which has already been pulled from a file rather than a path?",https://stackoverflow.com/questions/58096095,12118244,Inadequate Examples
58112355,"What, exactly, is eager execution from a programming point of view?","<p>I am trying to understand eager execution. Pages returned by Google describe what it does for you, and I'm ok with that. I am trying to understand it from the point of view of program code. Here is an example from <a href=""https://towardsdatascience.com/eager-execution-tensorflow-8042128ca7be"" rel=""nofollow noreferrer"">this article</a>.</p>

<pre><code>a = tf.constant([[1,2],[3,4]])
</code></pre>

<p>The article says this statement does something different depending on whether you are in eager mode or not. Without eager mode, print(a) gives:</p>

<pre><code>Tensor(""Const:0"", shape=(2, 2), dtype=int32)
</code></pre>

<p>With eager mode, print(a) gives:</p>

<pre><code>tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32)
</code></pre>

<p>Please could someone explain what these two return values are. If they are two different object types, a Tensor and a tf.Tensor, what is the difference between these objects?</p>

<p>I have searched the TensorFlow documentation and can't see anything that addresses this distinction. Any pointers gratefully received.</p>

<p>Thanks,</p>

<p>Julian</p>
","I am trying to understand eager execution. Pages returned by Google describe what it does for you, and I'm ok with that. I am trying to understand it from the point of view of program code. Here is an example from this article. The article says this statement does something different depending on whether you are in eager mode or not. Without eager mode, print(a) gives: With eager mode, print(a) gives: Please could someone explain what these two return values are. If they are two different object types, a Tensor and a tf.Tensor, what is the difference between these objects? I have searched the TensorFlow documentation and can't see anything that addresses this distinction. Any pointers gratefully received. Thanks, Julian",https://stackoverflow.com/questions/58112355,6691564,Inadequate Examples
58126494,How to Translate CSV Data into TFRecord Files,"<p>Currently I am working on a system that can take data from a CSV file and import it into a TFRecord file, However I have a few questions.</p>

<p>For starters, I need to know what type a TFRecord file can take, when using CSV types are removed.</p>

<p>Secondly, How can I convert data type:object into a type that a TFRecord can take?</p>

<p>I have two columns (will post example below) of two objects types that are strings, How can I convert that data to the correct type for TFRecords?</p>

<p>When importing Im hoping to append data from each row at a time into the TFRecord file, any advice or documentation would be great, I have been looking for some time at this problem and it seems there can only be ints,floats inputted into a TFRecord but what about a list/array of Integers?</p>

<p>Thankyou for reading!</p>

<p>Quick Note, I am using PANDAS to create a dataframe of the CSV file</p>

<p>Some Example Code Im using </p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from ast import literal_eval
import numpy as np
import tensorflow as tf


tf.compat.v1.enable_eager_execution()


def Start():
    db = pd.read_csv(""I:\Github\ClubKeno\Keno Project\Database\..\LotteryDatabase.csv"")

    pd.DataFrame = db
    print(db['Winning_Numbers'])
    print(db.dtypes)

    training_dataset = (
        tf.data.Dataset.from_tensor_slices(
            (
                tf.cast(db['Draw_Number'].values, tf.int64),
                tf.cast(db['Winning_Numbers'].values, tf.int64),
                tf.cast(db['Extra_Numbers'].values, tf.int64),
                tf.cast(db['Kicker'].values, tf.int64)
            )
        )
    )

    for features_tensor, target_tensor in training_dataset:
        print(f'features:{features_tensor} target:{target_tensor}')
</code></pre>

<p>Error Message:</p>

<p><img src=""https://cdn.discordapp.com/attachments/279786369902051328/626967249395122213/Capture.PNG"" alt=""Error Message""></p>

<p><a href=""https://cdn.discordapp.com/attachments/502661247809093673/626946732239880194/LotteryDatabase.csv"" rel=""nofollow noreferrer"">CSV Data</a></p>

<p>Update:
Got Two Columns of dating working using the following function...</p>

<pre class=""lang-py prettyprint-override""><code>dataset = tf.data.experimental.make_csv_dataset(
        file_pattern=databasefile,
        column_names=['Draw_Number', 'Kicker'],
        column_defaults=[tf.int64, tf.int64],
    )
</code></pre>

<p>However when trying to include my two other column object types
(What data looks like in both those columns)
<code>""3,9,11,16,25,26,28,29,36,40,41,46,63,66,67,69,72,73,78,80""</code></p>

<p>I get an error, here is the function I tried for that</p>

<pre class=""lang-py prettyprint-override""><code>    dataset = tf.data.experimental.make_csv_dataset(
        file_pattern=databasefile,
        column_names=['Draw_Number', 'Winning_Numbers', 'Extra_Numbers', 'Kicker'],
        column_defaults=[tf.int64, tf.compat.as_bytes, tf.compat.as_bytes, tf.int64],
        header=True,
        batch_size=100,
        field_delim=',',
        na_value='NA'
    )
</code></pre>

<p>This Error Appears:</p>

<pre><code>TypeError: Failed to convert object of type &lt;class 'function'&gt; to Tensor. Contents: &lt;function as_bytes at 0x000000EA530908C8&gt;. Consider casting elements to a supported type.
</code></pre>

<p>Should I try to Cast those two types outside the function and try combining it later into the TFRecord file alongside the tf.data from the <code>make_csv_dataset</code> function? </p>
","Currently I am working on a system that can take data from a CSV file and import it into a TFRecord file, However I have a few questions. For starters, I need to know what type a TFRecord file can take, when using CSV types are removed. Secondly, How can I convert data type:object into a type that a TFRecord can take? I have two columns (will post example below) of two objects types that are strings, How can I convert that data to the correct type for TFRecords? When importing Im hoping to append data from each row at a time into the TFRecord file, any advice or documentation would be great, I have been looking for some time at this problem and it seems there can only be ints,floats inputted into a TFRecord but what about a list/array of Integers? Thankyou for reading! Quick Note, I am using PANDAS to create a dataframe of the CSV file Some Example Code Im using Error Message: CSV Data Update: Got Two Columns of dating working using the following function... However when trying to include my two other column object types (What data looks like in both those columns) ""3,9,11,16,25,26,28,29,36,40,41,46,63,66,67,69,72,73,78,80"" I get an error, here is the function I tried for that This Error Appears: Should I try to Cast those two types outside the function and try combining it later into the TFRecord file alongside the tf.data from the make_csv_dataset function?",https://stackoverflow.com/questions/58126494,9873122,Requesting (Additional) Resources
58162110,How to port a tf.Session to a tf.train.MonitoredSession call while allowing graph modifications,"<p>The code I'm working on is <a href=""https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/object_detection/object_detection.py"" rel=""nofollow noreferrer"">this</a>.<br>
The code uses tf.session call to take in a graph for object detection tasks.  <a href=""https://github.com/tensorflow/tensorrt/blob/526f3650a550d6ffcceddfd73d112391080066ad/tftrt/examples/object_detection/object_detection.py#L593-L676"" rel=""nofollow noreferrer"">Link</a><br>
My aim here is to profile this code for Nvidia GPUs using the nvtx-plugins-tf to analyze the time taken for different ops. <a href=""https://nvtx-plugins.readthedocs.io/en/latest/templates/api.html#session-hooks"" rel=""nofollow noreferrer"">Link to docs</a></p>

<p>The plugin library provides a function hook for a tf.train.MonitoredSession as given in their example code <a href=""https://github.com/NVIDIA/nvtx-plugins/blob/a5d47ba6cb0548f2a6c15814d409a14aca4d33f8/examples/tf_session_example.py#L19-L95"" rel=""nofollow noreferrer"">here</a>.<br>
The code linked above uses tf.session along with a tf.config and when I try to modify the tf.session call to a tf.train.MonitoredSession call, I can't get my code to work and it fails with an error that graph can't be modified. I went through the tensorflow APIs and it turns out that tf.session doesn't support hook callbacks and tf.train.MonitoredSession doesn't support tf_config as a function argument.</p>

<pre><code>Traceback (most recent call last):
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/mayroy13/Mayank/Mayank/test/tensorrt/tftrt/examples/object_detection/test.py"", line 105, in &lt;module&gt;
    test(args.test_config_path)
  File ""/home/mayroy13/Mayank/Mayank/test/tensorrt/tftrt/examples/object_detection/test.py"", line 81, in test
    **test_config['benchmark_config'])
  File ""/home/mayroy13/Mayank/Mayank/test/tensorrt/tftrt/examples/object_detection/object_detection.py"", line 608, in benchmark_model
    tf.import_graph_def(frozen_graph, name='')
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 443, in import_graph_def
    _ProcessNewOps(graph)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 236, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3751, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3751, in &lt;listcomp&gt;
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3640, in _create_op_from_tf_operation
    self._check_not_finalized()
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3225, in _check_not_finalized
    raise RuntimeError(""Graph is finalized and cannot be modified."")
RuntimeError: Graph is finalized and cannot be modified.
</code></pre>

<p>Any directions to go in would be appreciated. If there are ways in tensorflow to use hooks in conjunction with tf.session, that will also work for me.</p>
","The code I'm working on is this. The code uses tf.session call to take in a graph for object detection tasks. Link My aim here is to profile this code for Nvidia GPUs using the nvtx-plugins-tf to analyze the time taken for different ops. Link to docs The plugin library provides a function hook for a tf.train.MonitoredSession as given in their example code here. The code linked above uses tf.session along with a tf.config and when I try to modify the tf.session call to a tf.train.MonitoredSession call, I can't get my code to work and it fails with an error that graph can't be modified. I went through the tensorflow APIs and it turns out that tf.session doesn't support hook callbacks and tf.train.MonitoredSession doesn't support tf_config as a function argument. Any directions to go in would be appreciated. If there are ways in tensorflow to use hooks in conjunction with tf.session, that will also work for me.",https://stackoverflow.com/questions/58162110,5232265,Documentation Replication on Other Examples
58176215,Tensorflow difference between tf.stop_gradient and feed variables to optimizer?,"<p>I'm trying to train a model in <strong>self-supervised learning</strong>. The flow chart is something like the following:
<a href=""https://i.stack.imgur.com/g2yMn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g2yMn.jpg"" alt=""enter image description here""></a></p>

<p>Let's assume that <code>N1</code> is already trained and we want to train just <code>N2</code>. This is my current implementation:</p>

<pre><code>x_1 = tf.placeholder(tf.float32, [None, 128, 128, 1])
x_2 = tf.placeholder(tf.float32, [None, 128, 128, 1])

s_t1 = tf.stop_gradient(N1(x_1))  # treat s_t1 as a constant
s_t2_pred = N2(s_t1)) 
s_t2 = tf.stop_gradient(N1(x_2))  # treat s_t2 as a constant

loss = some_loss_function(s_t2, s_t2_pred)
train_op = tf.train.AdamOptimizer(lr).minimize(loss)
</code></pre>

<p>In this way, I should be optimizing only <code>N2</code>. What makes me confused is the fact that if I were to use the following code I would obtain very different results (much better than the above): </p>

<pre><code># treat everything as a variable:
s_t1 = N1(x_1)
s_t2_pred = N2(s_t1)
s_t2 = N1(x_2)

loss = some_loss_function(s_t2, s_t2_pred)
var_list = take_all_variables_in_N2()
train_op = tf.train.AdamOptimizer(lr).minimize(loss, var_list)
</code></pre>

<p>I wonder what is the problem with the first implementation. What is exactly the behaviour of <code>tf.stop_gradient</code> (the documentation is a bit poor)? How does this differ from the second approach?</p>

<p>From a practical perspective in semi-supervised learning: what is the difference between the two? Which one is the correct approach?</p>

<p>Thank you :) </p>

<hr>

<hr>

<p><strong>I added a possible solution to the problem in the comments below. I would still be happy to receive any feedback from more experienced users and to share some opinions on the best approach to structure a self-supervised learning problem in tensorflow.</strong></p>

<p>Bye, G.</p>
","I'm trying to train a model in self-supervised learning. The flow chart is something like the following: Let's assume that N1 is already trained and we want to train just N2. This is my current implementation: In this way, I should be optimizing only N2. What makes me confused is the fact that if I were to use the following code I would obtain very different results (much better than the above): I wonder what is the problem with the first implementation. What is exactly the behaviour of tf.stop_gradient (the documentation is a bit poor)? How does this differ from the second approach? From a practical perspective in semi-supervised learning: what is the difference between the two? Which one is the correct approach? Thank you :) I added a possible solution to the problem in the comments below. I would still be happy to receive any feedback from more experienced users and to share some opinions on the best approach to structure a self-supervised learning problem in tensorflow. Bye, G.",https://stackoverflow.com/questions/58176215,6470174,Documentation Replication on Other Examples
58225926,Tensorflow Gradient Tape returning None,"<p>I'm using TensorFlow 1.14.0 with Python(3.6.8). I'm trying to use tensorflow_probability's lbfgs optimizer implementation(<a href=""https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer/lbfgs_minimize"" rel=""nofollow noreferrer"">documentation/example</a>).</p>

<p>If I run the example code provided in the documentation it works fine. I tried to follow the same procedure for my own code which uses the <code>tf.GradientTape()</code> approach for computing the objective function. When doing it that way, the gradients come back as <code>None</code> type.</p>

<p>I'm not seeing why one is working, but the other is not.</p>

<p>Edit: I realized that running eager execution using the gradients wouldn't work, so I adjusted the example to be able to be run with eager execution.</p>

<p>Non-working example(using GradientTape) with eager execution</p>

<pre><code>import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

tf.enable_eager_execution()

# A high-dimensional quadratic bowl.
ndims = 3
minimum = np.ones([ndims], dtype='float64')
scales = np.arange(ndims, dtype='float64') + 1.0

# The objective function and the gradient.
def quadratic(x):
    with tf.GradientTape() as g:
        value = tf.reduce_sum(scales * (x - minimum) ** 2)
    grads = g.gradient(value, x)
    print('Gradients: ')
    print(grads)
    return value, grads

start = np.arange(ndims, 0, -1, dtype='float64')
optim_results = tfp.optimizer.lbfgs_minimize(quadratic, initial_position=start, num_correction_pairs=10,tolerance=1e-8)

print('results')
print(optim_results)
# Check that the search converged
assert(optim_results.converged)
# Check that the argmin is close to the actual value.
np.testing.assert_allclose(optim_results.position, minimum)
</code></pre>
","I'm using TensorFlow 1.14.0 with Python(3.6.8). I'm trying to use tensorflow_probability's lbfgs optimizer implementation(documentation/example). If I run the example code provided in the documentation it works fine. I tried to follow the same procedure for my own code which uses the tf.GradientTape() approach for computing the objective function. When doing it that way, the gradients come back as None type. I'm not seeing why one is working, but the other is not. Edit: I realized that running eager execution using the gradients wouldn't work, so I adjusted the example to be able to be run with eager execution. Non-working example(using GradientTape) with eager execution",https://stackoverflow.com/questions/58225926,3667142,Documentation Replication on Other Examples
58338310,Predefined layers inside Custom layers,"<p>I want to use predefined layers from tf.keras.layers inside a custom layer. I want to create a custom layer that is a combination of dense and 1D Convolution layers.
Is it possible to do something like that? I could not find an example in the tensorflow pages.</p>
",I want to use predefined layers from tf.keras.layers inside a custom layer. I want to create a custom layer that is a combination of dense and 1D Convolution layers. Is it possible to do something like that? I could not find an example in the tensorflow pages.,https://stackoverflow.com/questions/58338310,11698102,Lack of Alternative Solutions/Documentation
58384884,'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction',"<p>I am using Huber loss implementation in tf.keras in tensorflow 1.14.0 as follows:</p>

<pre><code>huber_keras_loss = tf.keras.losses.Huber(
        delta=delta,
        reduction=tf.keras.losses.Reduction.SUM,
        name='huber_loss'
    )
</code></pre>

<p>I am getting the error 
AttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction'</p>

<p>I have tried using tf.losses.Reduction, tf.compat.v2.losses.Reduction nothing seems to work.</p>

<p>Did tensorflow remove Reduction from tf.keras.losses, it is strange if they did so because their documentation still shows:
<a href=""https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/Huber#args"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/Huber#args</a></p>
","I am using Huber loss implementation in tf.keras in tensorflow 1.14.0 as follows: I am getting the error AttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction' I have tried using tf.losses.Reduction, tf.compat.v2.losses.Reduction nothing seems to work. Did tensorflow remove Reduction from tf.keras.losses, it is strange if they did so because their documentation still shows: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/Huber#args",https://stackoverflow.com/questions/58384884,11986203,Documentation Ambiguity
58412668,Hparams plugin with tf.keras (tensorflow 2.0),"<p>I try to follow the example from the <a href=""https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams"" rel=""noreferrer"">tensorflow docs</a> and setup hyperparameter logging. It also mentions that, if you use <code>tf.keras</code>, you can just use the callback <code>hp.KerasCallback(logdir, hparams)</code>. However, if I use the callback I don't get my metrics (only the outcome). </p>
","I try to follow the example from the tensorflow docs and setup hyperparameter logging. It also mentions that, if you use tf.keras, you can just use the callback hp.KerasCallback(logdir, hparams). However, if I use the callback I don't get my metrics (only the outcome).",https://stackoverflow.com/questions/58412668,7441757,Documentation Ambiguity
58520594,"tf.Data.Dataset - On each Epoch, only train with a sub sample of the full dataset","<p>I have an image dataset with a large imbalance of positive and negatives samples (many more negatives). I would like to create a tf.data.Dataset where each epoch it will train with all of the positive samples but only (ratio * len(positive) ) of the negative samples. </p>

<p>I am currently using a datagen inherited from keras.util.Sequence to achieve this and using this subsampling policy is performing much better than training on all data.</p>

<p>However reading the docs on Dataset, I cannot seem to find a way to do it, is it possible?</p>

<p>In my existing data generator, I am doing this:</p>

<pre class=""lang-py prettyprint-override""><code># List if indicies of the positive and negative samples
positives = np.where(self.labels == 1)[0]
negatives = np.where(self.labels == 0)[0]
# How many of the negatives do we want to use?
n_negatives = np.clip(int(len(positives) * self.config.DATASET_NEGSUBSAMPLE_RATIO), 1, len(negatives))
# Choose random negatives
subsampled_negatives = np.random.choice(negatives, n_negatives, replace=False)
# Create the incidies array from the positive and subsamples negative indicies
self.indexes = np.concatenate((positives, subsampled_negatives))
# Shuffle them together
np.random.shuffle(self.indexes)
</code></pre>
","I have an image dataset with a large imbalance of positive and negatives samples (many more negatives). I would like to create a tf.data.Dataset where each epoch it will train with all of the positive samples but only (ratio * len(positive) ) of the negative samples. I am currently using a datagen inherited from keras.util.Sequence to achieve this and using this subsampling policy is performing much better than training on all data. However reading the docs on Dataset, I cannot seem to find a way to do it, is it possible? In my existing data generator, I am doing this:",https://stackoverflow.com/questions/58520594,2252698,Lack of Alternative Solutions/Documentation
58630393,Does tf.keras.metrics.AUC work on multi-class problems?,"<p>I have a multi-class classification problem and I want to measure AUC on training and test data.</p>
<p>tf.keras has implemented AUC metric (tf.keras.metrics.AUC), but I'm not be able to see whether this metric could safely be used in multi-class problems. Even, the example &quot;Classification on imbalanced data&quot; on the official Web page is dedicated to a binary classification problem.</p>
<p>I have implemented a CNN model that predicts six classes, having a softmax layer that gives the probabilities of all the classes. I used this metric as follows</p>
<pre><code>self.model.compile(loss='categorical_crossentropy',
                       optimizer=Adam(hp.get(&quot;learning_rate&quot;)),
                       metrics=['accuracy', AUC()]),
</code></pre>
<p>and the code was executed without any problem. However, sometimes I see some results that are quite strange for me. For example, the model reported an accuracy of 0.78333336 and AUC equal to 0.97327775, Is this possible? Can a model have a low accuracy and an AUC so high?</p>
<p>I wonder that, although the code does not give any error, the AUC metric is computing wrong.</p>
<p>Somebody may confirm me whether or not this metrics support multi-class classification problems?</p>
","I have a multi-class classification problem and I want to measure AUC on training and test data. tf.keras has implemented AUC metric (tf.keras.metrics.AUC), but I'm not be able to see whether this metric could safely be used in multi-class problems. Even, the example ""Classification on imbalanced data"" on the official Web page is dedicated to a binary classification problem. I have implemented a CNN model that predicts six classes, having a softmax layer that gives the probabilities of all the classes. I used this metric as follows and the code was executed without any problem. However, sometimes I see some results that are quite strange for me. For example, the model reported an accuracy of 0.78333336 and AUC equal to 0.97327775, Is this possible? Can a model have a low accuracy and an AUC so high? I wonder that, although the code does not give any error, the AUC metric is computing wrong. Somebody may confirm me whether or not this metrics support multi-class classification problems?",https://stackoverflow.com/questions/58630393,9176854,Documentation Replication on Other Examples
58631390,What is the purpose of tf.compat?,"<p>What's the purpose of tf.compat module? It looks like just the entire Tensorflow API is replicated inside this module.
The documentation states</p>

<blockquote>
  <p>Functions for Python 2 vs. 3 compatibility.</p>
</blockquote>

<p>So why there is a ""v1"" and a ""v2"" submodule? What are the compatibility problems address by tf.compat specifically?</p>
","What's the purpose of tf.compat module? It looks like just the entire Tensorflow API is replicated inside this module. The documentation states So why there is a ""v1"" and a ""v2"" submodule? What are the compatibility problems address by tf.compat specifically?",https://stackoverflow.com/questions/58631390,1719931,Documentation Completeness
58699961,Problem of predicting with a loaded tensorflow estimator trained beforehand,"<p>I had trained a tensorflow NN estimator to predict something in Python. And I saved the model in my Google drive via Google colab.</p>

<p>Today, I loaded the model and it was a pretty hard work. I finally succeeded to load the estimator using <code>tf.compat.v2.saved_model.load</code> and <code>.signature</code> method. it seems WrappedFunction. This is my code till this step.</p>

<ol>
<li>code for saving</li>
</ol>

<pre><code>serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
  tf.feature_column.make_parse_example_spec(input_column))
#tf.feature_column.make_parse_example_spec([input_column]))
export_path = dnn_regressor.export_saved_model('/content/gdrive/My Drive',serving_input_fn)
#https://www.tensorflow.org/guide/saved_model#savedmodel%EC%9D%98_%EC%A0%9C%EC%96%B4_%ED%9D%90%EB%A6%84
</code></pre>

<ol start=""2"">
<li>code for loading</li>
</ol>

<pre><code>imported = tf.compat.v2.saved_model.load('/content/gdrive/My Drive/1572596260', tags=None)
infer = imported.signatures[""predict""]
</code></pre>

<p>But still I fail to put a test data to model to make a prediction.</p>

<p>first, I tried:</p>

<pre><code>test_predictions = infer(test_data)
test_predictions = np.array([itempredictions'][0] for item in test_predictions])
</code></pre>

<p>And it return an error</p>

<blockquote>
  <p>ValueError: All inputs to <code>ConcreteFunction</code>s must be Tensors; on invocation >of pruned, the 0-th input ( My data [2513 rows x 45 columns]) was not a >Tensor.</p>
</blockquote>

<p>Secondly, looked for some tensorflow document, I wrote this code</p>

<pre><code>test_predictions = infer(tf.constant(test_data))
test_predictions = np.array([itempredictions'][0] for item in test_predictions])
</code></pre>

<p>This time it return</p>

<blockquote>
  <p>The argument 'examples' (value Tensor(""Const_10:0"", shape=(2513, 45), >dtype=float64)) is not compatible with the shape this function was traced >with. Expected shape (?,), but got shape (2513, 45).</p>
  
  <p>If you called get_concrete_function, you may need to pass a >tf.TensorSpec(..., shape=...) with a less specific shape, having None on axes >which can vary.</p>
</blockquote>

<p>I found dynamic/static shape of tensorflow. But I couldn't fully understand those concept and failed to reshaping.
How can I get the result? Thanks.</p>
","I had trained a tensorflow NN estimator to predict something in Python. And I saved the model in my Google drive via Google colab. Today, I loaded the model and it was a pretty hard work. I finally succeeded to load the estimator using tf.compat.v2.saved_model.load and .signature method. it seems WrappedFunction. This is my code till this step. But still I fail to put a test data to model to make a prediction. first, I tried: And it return an error Secondly, looked for some tensorflow document, I wrote this code This time it return I found dynamic/static shape of tensorflow. But I couldn't fully understand those concept and failed to reshaping. How can I get the result? Thanks.",https://stackoverflow.com/questions/58699961,10541065,Documentation Ambiguity
58730460,Freeze sublayers in tensorflow 2,"<p>I have a model which is composed of custom layers. Each custom layer contains many tf.keras.layers. The problem is that if I want to freeze those layers after defining my model, the loop:</p>

<pre><code>for i, layer in enumerate(model.layers):
    print(i, layer.name)
</code></pre>

<p>only prints the ""outer"" custom layers and not those who exist inside. Is there any way to access the inner layers so I can freeze them?</p>

<p>an example of a custom layer from the <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models"" rel=""nofollow noreferrer"">official tf docs</a>:</p>

<pre><code>class MLPBlock(layers.Layer):

  def __init__(self):
    super(MLPBlock, self).__init__()
    self.linear_1 = Linear(32)
    self.linear_2 = Linear(32)
    self.linear_3 = Linear(1)

  def call(self, inputs):
    x = self.linear_1(inputs)
    x = tf.nn.relu(x)
    x = self.linear_2(x)
    x = tf.nn.relu(x)
    return self.linear_3(x)
</code></pre>
","I have a model which is composed of custom layers. Each custom layer contains many tf.keras.layers. The problem is that if I want to freeze those layers after defining my model, the loop: only prints the ""outer"" custom layers and not those who exist inside. Is there any way to access the inner layers so I can freeze them? an example of a custom layer from the official tf docs:",https://stackoverflow.com/questions/58730460,7248145,Documentation Replication on Other Examples
58802573,Pre processing keras dataset using keras tokenizer,"<p>I am trying to do some pre processing using the keras tokenizer on data I read using the following code:</p>

<pre><code> dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.interleave(lambda x:
        tf.data.TFRecordDataset(x).prefetch(params.num_parallel_readers),
                                     cycle_length=params.num_parallel_readers,
                                     block_length=1)
        dataset = dataset.map(_parse_example, num_parallel_calls = params.num_parallel_calls)
</code></pre>

<p>Now that I have the parsed example (output of _parse_example map function) I want to do some pre-processing on the text using <code>tf.keras.preprocessing.text.Tokenizer</code> method <code>texts_to_sequences</code>.
However, texts_to_sequences expects an input of python strings and I get Tensors in the parsed_example.</p>

<p>I can work around it by using <code>py_func</code> to wrap my code (see <strong>'emb': tf.py_func..</strong> in the code below), but then I will not be able to serialize my model (according to the <code>py_func</code> documentation).</p>

<pre><code>dataset = dataset.map(lambda features, labels: 
                              ({'window': features['window'],
                                'winSize': features['winSize'],
                                'LandingPage': features['LandingPage'],
                                'emb': tf.py_func(getEmb, [features['window']], tf.int32)},
                                tf.one_hot(labels, hparams.numClasses) ))
</code></pre>

<p>Looking for a way to do that (or a link to some similar example)</p>
","I am trying to do some pre processing using the keras tokenizer on data I read using the following code: Now that I have the parsed example (output of _parse_example map function) I want to do some pre-processing on the text using tf.keras.preprocessing.text.Tokenizer method texts_to_sequences. However, texts_to_sequences expects an input of python strings and I get Tensors in the parsed_example. I can work around it by using py_func to wrap my code (see 'emb': tf.py_func.. in the code below), but then I will not be able to serialize my model (according to the py_func documentation). Looking for a way to do that (or a link to some similar example)",https://stackoverflow.com/questions/58802573,6092553,Lack of Alternative Solutions/Documentation
58818679,Why a model using tf.py_function can not be serialized?,"<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""nofollow noreferrer"">documentation</a> of ty.py_function a model using it can't be serialized.</p>

<blockquote>
  <p>The body of the function (i.e. func) will not be serialized in a GraphDef. Therefore, you should not use this function if you need to serialize your model and restore it in a different environment.</p>
</blockquote>

<p>Why is serialization not possible?</p>

<p>I was looking for an explanation to why this is the case and alternatives to using tf.py_function but did not find helpful ones.</p>

<p>In my specific case I want to use the Keras Tokenizer and its methods expect numpy arrays - so I am calling it using tf.py_function.</p>
",According to the documentation of ty.py_function a model using it can't be serialized. Why is serialization not possible? I was looking for an explanation to why this is the case and alternatives to using tf.py_function but did not find helpful ones. In my specific case I want to use the Keras Tokenizer and its methods expect numpy arrays - so I am calling it using tf.py_function.,https://stackoverflow.com/questions/58818679,6092553,Lack of Alternative Solutions/Documentation
58822319,How to use tfa.seq2seq.BahdanauAttention with tf.keras functional API?,"<p>I want to use <a href=""https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/BahdanauAttention"" rel=""nofollow noreferrer""><code>tfa.seq2seq.BahdanauAttention</code></a> with functional API of <code>tf.keras</code>. I have looked at the example given at <a href=""https://github.com/tensorflow/nmt/blob/master/nmt/attention_model.py"" rel=""nofollow noreferrer"">tensorflow/nmt/attention_model.py</a>. But I couldn't figure out how to use it with <code>tf.keras</code>'s functional API.</p>

<p>So I would like to use <code>tfa.seq2seq.BahdanauAttention</code> for a lipreading task, something like this:</p>

<pre class=""lang-py prettyprint-override""><code>
    # Using tf.keras functional API
    encoder_out = a tensor of shape (batch_size, time_steps, units)
    decoder_out = a tensor of shape (batch_size, time_steps, units)
    attn_out = attention_mechanism()(encoder_out, decoder_out)  # Need help figuring this out

</code></pre>

<p>Thanks in advance.</p>
","I want to use tfa.seq2seq.BahdanauAttention with functional API of tf.keras. I have looked at the example given at tensorflow/nmt/attention_model.py. But I couldn't figure out how to use it with tf.keras's functional API. So I would like to use tfa.seq2seq.BahdanauAttention for a lipreading task, something like this: Thanks in advance.",https://stackoverflow.com/questions/58822319,7618706,Documentation Ambiguity
58842107,How do I update a model using a pre-release version of Tensorflow to run in a Google Colab instance?,"<p>I'm trying to use the <a href=""https://github.com/google-research-datasets/wiki-reading"" rel=""nofollow noreferrer"">WikiReading</a> dataset and model in a project and train it using a Google Colaboratory instance. For that purpose, I'm adapting the code to a Jupyter Notebook, which also uses a more recent version of Tensorflow than the provided baseline Bag of Words model did. The original paper and accompanying baseline model was published in August 2016, which predates Tensorflow 1.0.0.</p>

<p>I've gone some way towards the process of updating the model to Tensorflow v1.x, but I appear to have hit a roadblock. From my (fairly limited) understanding, the last step in the model is to apply a softmax function on the results. In the original model, this was done using the following function call:</p>

<pre><code>softmax, loss = tf.contrib.learn.ops.softmax_classifier(
        joint_enc, answers, answer_embeddings, answer_biases)
</code></pre>

<p>This is then used in this statement for the optimization:</p>

<pre><code>train_op = tf.contrib.layers.optimize_loss(
        loss, tf.contrib.framework.get_global_step(),
        learning_rate=LEARNING_RATE,
        optimizer='Adam')
</code></pre>

<p>I can't seem to find any documentation relating to the <code>tf.contrib.learn.ops.softmax_classifier()</code> function online. I'm assuming it takes in 4 tensors in some order, most likely something like the first holds the batch to classify, the second one holds a list of the answers to predict with the 3rd and 4th holding the embedding and biases for each answer.</p>

<p>My problem is that I cannot find a function that maps neatly to that format with the same output and I'm not sure what transformations to apply to my tensors to get a similar result using something like <code>tf.nn.softmax()</code> without access to the documentation of <code>tf.contrib.learn.ops.softmax_classifier()</code>. How should I go about tackling this problem? Should I just rewrite the model_fcn()?</p>

<h2>Original model_fn()</h2>

<pre class=""lang-py prettyprint-override""><code>def bow_model(features, target):
    document = utils.prune_out_of_vocab_ids(features['document_sequence'], VOCAB_SIZE)
    question = utils.prune_out_of_vocab_ids(features['question_sequence'], VOCAB_SIZE)
    answers = tf.squeeze(tf.one_hot(target, ANSWER_NUM, 1.0, 0.0),
                         squeeze_dims=[1])
    embeddings = tf.get_variable('embeddings', [VOCAB_SIZE, EMBED_DIM])
    doc_enc = layers.safe_embedding_lookup_sparse(
        [embeddings], document, None, combiner='sum')
    question_enc = layers.safe_embedding_lookup_sparse(
        [embeddings], question, None, combiner='sum')
    joint_enc = tf.concat(1, [doc_enc, question_enc])
    answer_embeddings = tf.get_variable(
        'answer_embeddings', [ANSWER_DIM, ANSWER_NUM])
    answer_biases = tf.get_variable('answer_biases', [ANSWER_NUM])
    softmax, loss = learn.ops.softmax_classifier(
        joint_enc, answers, answer_embeddings, answer_biases)
    train_op = layers.optimize_loss(
        loss, tf.contrib.framework.get_global_step(),
        learning_rate=LEARNING_RATE,
        optimizer='Adam')
    return softmax, loss, train_op
</code></pre>

<h2>Partially updated model_fn():</h2>

<pre class=""lang-py prettyprint-override""><code>def bow_model(features, labels, mode):
    document = prune_out_of_vocab_ids(features['document_sequence'], VOCAB_SIZE)
    question = prune_out_of_vocab_ids(features['question_sequence'], VOCAB_SIZE)

    answers = tf.squeeze(tf.one_hot(labels, ANSWER_NUM, 1.0, 0.0))

    embeddings = tf.get_variable('embeddings',
                                 [VOCAB_SIZE, EMBED_DIM])
    answer_embeddings = tf.get_variable('answer_embeddings',
                                        [ANSWER_DIM, ANSWER_NUM])
    answer_biases = tf.get_variable('answer_biases',
                                    [ANSWER_NUM])

    doc_enc = layers.safe_embedding_lookup_sparse(
        [embeddings], document, None, combiner='sum')
    question_enc = layers.safe_embedding_lookup_sparse(
        [embeddings], question, None, combiner='sum')
    joint_enc = tf.concat(axis=1, values=[doc_enc, question_enc])

    # softmax, loss = tf.contrib.learn.ops.softmax_classifier(
    #     joint_enc, answers, answer_embeddings, answer_biases)
    # replaced by:
    logits = tf.nn.xw_plus_b(doc_enc, answer_embeddings, answer_biases)
    softmax = tf.nn.softmax(logits)
    loss = tf.losses.softmax_cross_entropy(onehot_labels=answers,
                                           logits=logits,
                                           weights=answer_embeddings)

    train_op = layers.optimize_loss(
        loss, tf.train.get_global_step(),
        learning_rate=LEARNING_RATE,
        optimizer='Adam')

    return learn.EstimatorSpec(mode=mode,
                               predictions=softmax,
                               loss=loss,
                               train_op=train_op)
</code></pre>

<h2>Original input_fn():</h2>

<pre class=""lang-py prettyprint-override""><code>def get_wikireading_input():
    filename = ""../train-*""
    feature_info = {k: tf.VarLenFeature(dtype=tf.int64) for k in SPARSE_FEATURES}
    feature_info['answer_ids'] = tf.VarLenFeature(dtype=tf.int64)
    def input_fn():
        features = learn.read_batch_features(
            filename, BATCH_SIZE, feature_info,
            reader=tf.TFRecordReader)
        target = features.pop('answer_ids')
        target = utils.resize_axis(tf.sparse_tensor_to_dense(target), 1, 1)
        return features, target
    return input_fn
</code></pre>

<h2>Updated input_fn()</h2>

<pre class=""lang-py prettyprint-override""><code>def input_fn():
    features = {'document_sequence': tf.VarLenFeature(dtype=tf.int64),
                'question_sequence': tf.VarLenFeature(dtype=tf.int64),
                'answer_ids': tf.VarLenFeature(dtype=tf.int64)}

    files = tf.data.Dataset.list_files(file_pattern=filename)
    dataset = files.interleave(tf.data.TFRecordDataset,
                          cycle_length=AUTOTUNE,
                          num_parallel_calls=AUTOTUNE)

    def parse_fn(serialized):
        example = tf.io.parse_single_sequence_example(serialized=serialized,
                                                      sequence_features=features)[1]
        labels = example.pop('answer_ids')
        labels = resize_axis(tf.sparse_tensor_to_dense(labels), 1, 1)
        return example, labels

    dataset = dataset.map(map_func=parse_fn, num_parallel_calls=AUTOTUNE)
    dataset = dataset.batch(batch_size=BATCH_SIZE)
    dataset = dataset.shuffle(buffer_size=BATCH_SIZE)
    dataset = dataset.prefetch(buffer_size=AUTOTUNE)
    return dataset
</code></pre>

<p>The full Jupyter notebook can be accessed <a href=""https://github.com/ThierrySt-Arnaud/wiki-reading/blob/colab-conversion/colab/wiki_reading_training_en.ipynb"" rel=""nofollow noreferrer"">here</a></p>

<h2>Edit:</h2>

<p>I have found the source for the mentioned function <a href=""https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/contrib/learn/python/learn/ops/losses_ops.py"" rel=""nofollow noreferrer"">here</a>. The deprecation warning  says this about updating this function:</p>

<blockquote>
  <p>'Use <code>tf.losses.softmax_cross_entropy</code> and explicit logits computation.'</p>
</blockquote>

<p>Now, I'm replicating the operations of this function as follows:</p>

<pre class=""lang-py prettyprint-override""><code>  logits = tf.nn.xw_plus_b(doc_enc, answer_embeddings, answer_biases)
  softmax = tf.nn.softmax(logits)
  loss = tf.losses.softmax_cross_entropy(answers, logits, answer_embeddings)
</code></pre>

<p>but this gives me:</p>

<blockquote>
  <p>ValueError Dimensions must be equal, but are 20 and 40 for 'xw_plus_b/MatMul' (op: 'BatchMatMulV2') with input shapes: [?,?,20], [40,5000].</p>
</blockquote>

<p>This makes perfect sense when I look at the constant definitions:</p>

<pre class=""lang-py prettyprint-override""><code>VOCAB_SIZE = 10000
EMBED_DIM = 20
ANSWER_DIM = 2 * EMBED_DIM
ANSWER_NUM = 5000
BATCH_SIZE = 128
LEARNING_RATE = 0.01
HIDDEN_SIZE = 128
</code></pre>

<p>I guess my questions now are:</p>

<ul>
<li>Why is ANSWER_NUM and ANSWER_DIM not equal to VOCAB_SIZE and EMBED_DIM, respectively? Shouldn't they have the same size?</li>
<li>How could this have worked before?</li>
</ul>

<h3>Edit 2:</h3>

<p>As I try to update and train this model, I have grown more and more confused by the way it is defined. This is largely due to my inexperience with machine learning in general and TensorFlow in particular, but there are some things that don't make sense, at least to me. I have adjusted the ANSWER_NUM and ANSWER_DIM as I mention above and updated other functions and parameters (seen in the updated model_fn above) which gives me a valid data graph but the following error when trying to fit:</p>

<blockquote>
  <p>(0) Invalid argument: assertion failed: [weights can not be broadcast to values.] [weights.shape=] [answer_embeddings/read:0] [20 10000] [values.shape=] [softmax_cross_entropy_loss/xentropy/Reshape_2:0] [128 0] [is_scalar=] [0]</p>
</blockquote>

<p>This probably just requires a formatting step (that I'm not entirely sure of yet) before feeding running softmax_cross_entropy(). However, I also noticed that there is no definition of the hidden layers anywhere in the original model and HIDDEN_SIZE is unused. Is there an implicit definition of those layers somewhere in the model that I'm missing?</p>

<p>At this point, I feel like it will be easier to just use a Keras functional model in TensorFlow 2.x to get as close as possible to the <em>perceived</em> original model.</p>
","I'm trying to use the WikiReading dataset and model in a project and train it using a Google Colaboratory instance. For that purpose, I'm adapting the code to a Jupyter Notebook, which also uses a more recent version of Tensorflow than the provided baseline Bag of Words model did. The original paper and accompanying baseline model was published in August 2016, which predates Tensorflow 1.0.0. I've gone some way towards the process of updating the model to Tensorflow v1.x, but I appear to have hit a roadblock. From my (fairly limited) understanding, the last step in the model is to apply a softmax function on the results. In the original model, this was done using the following function call: This is then used in this statement for the optimization: I can't seem to find any documentation relating to the tf.contrib.learn.ops.softmax_classifier() function online. I'm assuming it takes in 4 tensors in some order, most likely something like the first holds the batch to classify, the second one holds a list of the answers to predict with the 3rd and 4th holding the embedding and biases for each answer. My problem is that I cannot find a function that maps neatly to that format with the same output and I'm not sure what transformations to apply to my tensors to get a similar result using something like tf.nn.softmax() without access to the documentation of tf.contrib.learn.ops.softmax_classifier(). How should I go about tackling this problem? Should I just rewrite the model_fcn()? The full Jupyter notebook can be accessed here I have found the source for the mentioned function here. The deprecation warning says this about updating this function: Now, I'm replicating the operations of this function as follows: but this gives me: This makes perfect sense when I look at the constant definitions: I guess my questions now are: As I try to update and train this model, I have grown more and more confused by the way it is defined. This is largely due to my inexperience with machine learning in general and TensorFlow in particular, but there are some things that don't make sense, at least to me. I have adjusted the ANSWER_NUM and ANSWER_DIM as I mention above and updated other functions and parameters (seen in the updated model_fn above) which gives me a valid data graph but the following error when trying to fit: This probably just requires a formatting step (that I'm not entirely sure of yet) before feeding running softmax_cross_entropy(). However, I also noticed that there is no definition of the hidden layers anywhere in the original model and HIDDEN_SIZE is unused. Is there an implicit definition of those layers somewhere in the model that I'm missing? At this point, I feel like it will be easier to just use a Keras functional model in TensorFlow 2.x to get as close as possible to the perceived original model.",https://stackoverflow.com/questions/58842107,12367101,Lack of Alternative Solutions/Documentation
58963793,ValueError: Shapes must be equal rank in assign_add(),"<p>I am reading <a href=""https://www.tensorflow.org/api_docs/python/tf/Variable"" rel=""nofollow noreferrer"">tf.Variable in Tensorflow r2.0</a> in TF2: </p>

<pre><code>import tensorflow as tf

# Create a variable.
w = tf.constant([1, 2, 3, 4], tf.float32, shape=[2, 2])

# Use the variable in the graph like any Tensor.
y = tf.matmul(w,tf.constant([7, 8, 9, 10], tf.float32, shape=[2, 2]))
v= tf.Variable(w)
# The overloaded operators are available too.
z = tf.sigmoid(w + y)
tf.shape(z)
# Assign a new value to the variable with `assign()` or a related method.
v.assign(w + 1)
v.assign_add(tf.constant([1.0, 21]))
</code></pre>

<blockquote>
  <p>ValueError: Shapes must be equal rank, but are 2 and 1 for
  'AssignAddVariableOp_4' (op: 'AssignAddVariableOp') with input shapes:
  [], <a href=""https://youtu.be/Up9CvRLIIIw?t=113"" rel=""nofollow noreferrer"">2</a>.</p>
</blockquote>

<p>And also how come the following returns false?</p>

<pre><code>tf.shape(v) == tf.shape(tf.constant([1.0, 21],tf.float32))
</code></pre>

<p>My other question is that when we are in TF 2, we should not use tf.Session() anymore, correct? It seems <a href=""https://youtu.be/Up9CvRLIIIw?t=113"" rel=""nofollow noreferrer"">we should never run session.run()</a>, but the API document keys doing it with tf.compat.v1, etc. So why they are using it in TF2 docs?</p>

<p>Any help would be appreciated.</p>

<p>CS</p>
","I am reading tf.Variable in Tensorflow r2.0 in TF2: And also how come the following returns false? My other question is that when we are in TF 2, we should not use tf.Session() anymore, correct? It seems we should never run session.run(), but the API document keys doing it with tf.compat.v1, etc. So why they are using it in TF2 docs? Any help would be appreciated. CS",https://stackoverflow.com/questions/58963793,2277812,Documentation Completeness
59056872,Why class name change after saving a keras model?,"<p>i wrote a basic keras model (tf.keras.__version = 2.2.4-tf) using tensorflow (2.0.0) :</p>

<pre><code>import tensorflow as tf

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, activation='linear',input_shape=(1,),name='equation'))
model.compile(optimizer='RMSprop', loss='mean_squared_error')
model.save('c:\\tmp\\oneneuron')
print(""Model saved type : "", type(model))
loaded_model = tf.keras.models.load_model('c:\\tmp\\oneneuron')
print(""Model loaded type : "", type(loaded_model))
print(""compare object model with loaded_model type : "",isinstance(model,type(loaded_model)))
print(""compare object loaded_model with model type : "",isinstance(loaded_model,type(model)))
print(""compare sublclass loaded_model and model type : "",issubclass(type(loaded_model),type(model)))
</code></pre>

<p>Results are </p>

<pre><code>Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; exec(open(r'C:\tmp\myPython\test_type_model.py').read())
2019-11-26 18:49:39.071088: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-11-26 18:49:39.574113: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING: Logging before flag parsing goes to stderr.
W1126 18:49:39.627490 11772 deprecation.py:506] From F:\Program Files\Python\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model saved type :  &lt;class 'tensorflow.python.keras.engine.sequential.Sequential'&gt;
Model loaded type :  &lt;class 'tensorflow.python.keras.saving.saved_model.load.Sequential'&gt;
compare object model with loaded_model type :  False
compare object loaded_model with model type :  True
compare sublclass loaded_model and model type :  True
</code></pre>

<p>Where can I find the difference between tensorflow.python.keras.saving.saved_model.load.Sequential and tensorflow.python.keras.engine.sequential.Sequential in tensorflow or keras documentation?</p>
",i wrote a basic keras model (tf.keras.__version = 2.2.4-tf) using tensorflow (2.0.0) : Results are Where can I find the difference between tensorflow.python.keras.saving.saved_model.load.Sequential and tensorflow.python.keras.engine.sequential.Sequential in tensorflow or keras documentation?,https://stackoverflow.com/questions/59056872,5128148,Lack of Alternative Solutions/Documentation
59074659,Best practice for allocating GPU and CPU resources in TensorFlow,"<p>I'm wondering what is the correct way to set devices for creating/training a model in order to optimize resource usage for speedy training in TensorFlow with the Keras API? I have 1 CPU and 2 GPUs at my disposal. I was initially using a <code>tf.device</code> context to create my model and train on GPUs only, but then I saw in the TensorFlow documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model"" rel=""nofollow noreferrer""><code>tf.keras.utils.multi_gpu_model</code></a>, they suggest explicitly instantiating the model on the CPU:</p>

<pre><code># Instantiate the base model (or ""template"" model).
# We recommend doing this with under a CPU device scope,
# so that the model's weights are hosted on CPU memory.
# Otherwise they may end up hosted on a GPU, which would
# complicate weight sharing.
with tf.device('/cpu:0'):
    model = Xception(weights=None,
                     input_shape=(height, width, 3),
                     classes=num_classes)

# Replicates the model on 8 GPUs.
# This assumes that your machine has 8 available GPUs.
parallel_model = multi_gpu_model(model, gpus=8)
parallel_model.compile(loss='categorical_crossentropy',
                       optimizer='rmsprop')
</code></pre>

<p>I did this, and now when I train I see my CPU usage go way up with all 8 cores at about 70% usage each, and my GPU memory is maxed out. Would things go faster if the model were created on one of the GPUs? Even if I have just 1 GPU, is it still better to create model on CPU and use <code>tf.device</code> context to train the model on the GPU?</p>
","I'm wondering what is the correct way to set devices for creating/training a model in order to optimize resource usage for speedy training in TensorFlow with the Keras API? I have 1 CPU and 2 GPUs at my disposal. I was initially using a tf.device context to create my model and train on GPUs only, but then I saw in the TensorFlow documentation for tf.keras.utils.multi_gpu_model, they suggest explicitly instantiating the model on the CPU: I did this, and now when I train I see my CPU usage go way up with all 8 cores at about 70% usage each, and my GPU memory is maxed out. Would things go faster if the model were created on one of the GPUs? Even if I have just 1 GPU, is it still better to create model on CPU and use tf.device context to train the model on the GPU?",https://stackoverflow.com/questions/59074659,3711266,Documentation Replication on Other Examples
59142986,serving_input_receiver_fn() function without the deprecated tf.placeholder method in TF 2.0,"<p>I have a functioning tf.estimator pipeline build in TF 1, but now I made the decision to move to TF 2.0, and I have problems in the end of my pipeline, when I want to save the model in the .pb format</p>

<p>I'm using this high level estimator export_saved_model method:</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesRegressor#export_saved_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesRegressor#export_saved_model</a></p>

<p>I have two numeric features, 'age' and 'time_spent'</p>

<p>They're defined using tf.feature_column as such: </p>

<pre><code>age = tf.feature_column.numeric_column('age')
time_spent = tf.feature_column.numeric_column('time_spent')

features = [age,time_spent]
</code></pre>

<p>After the model has been trained I turn the list of features into a dict using the method feature_column_make_parse_example_spec()  and feed it to another method build_parsing_serving_input_receiver_fn() excactly as outlied on tensorflow's webpage, <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/saved_model</a> under estimators. </p>

<pre><code>columns_dict = tf.feature_column_make_parse_example_spec(features)
input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(columns_dict)
model.export_saved_model(export_dir,input_receiver_fn)
</code></pre>

<p>I then inspect the output using the CLI tools</p>

<pre><code>saved_model_cli show --dir mydir --all: 
</code></pre>

<p>Resulting in the following: </p>

<p><a href=""https://i.stack.imgur.com/vL8Qh.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>Somehow Tensorflow squashes my two usefull numeric features into a useless string input crap called ""inputs"". </p>

<p>In TF 1 this could be circumvented by creating a custom input_receiver_fn() function using some tf.placeholder method, and I'd get the correct output with two distinct numeric features. But tf.placeholder doesn't exist in TF 2, so now it's pretty useless. </p>

<p>Sorry about the raging, but Tensorflow is horribly documented, and I'm really working with high level API's and it should just be straight out on the horse, but no. </p>

<p>I'd really appreciate any help :)</p>
","I have a functioning tf.estimator pipeline build in TF 1, but now I made the decision to move to TF 2.0, and I have problems in the end of my pipeline, when I want to save the model in the .pb format I'm using this high level estimator export_saved_model method: https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesRegressor#export_saved_model I have two numeric features, 'age' and 'time_spent' They're defined using tf.feature_column as such: After the model has been trained I turn the list of features into a dict using the method feature_column_make_parse_example_spec() and feed it to another method build_parsing_serving_input_receiver_fn() excactly as outlied on tensorflow's webpage, https://www.tensorflow.org/guide/saved_model under estimators. I then inspect the output using the CLI tools Resulting in the following: enter image description here Somehow Tensorflow squashes my two usefull numeric features into a useless string input crap called ""inputs"". In TF 1 this could be circumvented by creating a custom input_receiver_fn() function using some tf.placeholder method, and I'd get the correct output with two distinct numeric features. But tf.placeholder doesn't exist in TF 2, so now it's pretty useless. Sorry about the raging, but Tensorflow is horribly documented, and I'm really working with high level API's and it should just be straight out on the horse, but no. I'd really appreciate any help :)",https://stackoverflow.com/questions/59142986,12453038,Documentation Ambiguity
59174710,default tf.gradients in TensorFlow - total or partial derivatives?,"<p>so I'm reading about tf.gradients() in the documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/gradients"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/gradients</a>) and I'm a bit confused.</p>

<p>I've seen people stating that the results of tf.gradients() are </p>

<blockquote>
  <p>symbolic partial derivatives of sum of ys w.r.t. x in xs.</p>
</blockquote>

<p>This is also what I was thinking first. But then the documentation describes one optional arguments of this function as follows:</p>

<blockquote>
  <p>stop_gradients is a Tensor or a list of tensors to be considered constant with respect to all xs. These tensors will not be backpropagated through, as though they had been explicitly disconnected using stop_gradient. Among other things, this allows computation of partial derivatives as opposed to total derivatives. </p>
</blockquote>

<p>So is it only possible to calculate the partial derivatives if I use 'stop_gradient' and otherwise the default values returned in a vector with len(xs) are total derivatives? Probably it's just my misunderstanding, it would be much appreciated if someone could elaborate on this a bit.</p>

<p>Thanks a lot!</p>
","so I'm reading about tf.gradients() in the documentation (https://www.tensorflow.org/api_docs/python/tf/gradients) and I'm a bit confused. I've seen people stating that the results of tf.gradients() are This is also what I was thinking first. But then the documentation describes one optional arguments of this function as follows: So is it only possible to calculate the partial derivatives if I use 'stop_gradient' and otherwise the default values returned in a vector with len(xs) are total derivatives? Probably it's just my misunderstanding, it would be much appreciated if someone could elaborate on this a bit. Thanks a lot!",https://stackoverflow.com/questions/59174710,10274289,Requesting (Additional) Resources
59177677,Custom aggregation for tf.GradientTape().gradient? (TF2.0),"<p>As far as I know, the tf.gradients function provides option to choose the aggregation method for summarizing the gradients from multiple sources.
<a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/gradients"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/gradients</a></p>

<p>However according to the Tensorflow API documentation, the tf.GradientTape().gradient method has no such option.
<a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/GradientTape</a></p>

<p>So my questions are as follows:</p>

<ol>
<li><p>Is there any way to change the aggregation method in tf.GradientTape().gradient?</p></li>
<li><p>If not, is there any way to obtain the gradient with a custom aggregation method which is compatible with eager execution?</p></li>
</ol>
","As far as I know, the tf.gradients function provides option to choose the aggregation method for summarizing the gradients from multiple sources. https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/gradients However according to the Tensorflow API documentation, the tf.GradientTape().gradient method has no such option. https://www.tensorflow.org/api_docs/python/tf/GradientTape So my questions are as follows:",https://stackoverflow.com/questions/59177677,7386954,Documentation Ambiguity
59361689,Redundancies in tf.keras.backend and tensorflow libraries,"<p>I have been working in TensorFlow for about a year now, and I am transitioning from TF 1.x to TF 2.0, and I am looking for some guidance on how to use the <code>tf.keras.backend</code> library in TF 2.0. I understand that the transition to TF 2.0 is supposed to remove a lot of redundancies in modeling and building graphs, since there were many ways to create equivalent layers in earlier TensorFlow versions (and I'm insanely grateful for that change!), but I'm getting stuck on understanding when to use <code>tf.keras.backend</code>, because the operations appear redundant with other TensorFlow libraries. </p>

<p>I see that some of the functions in <code>tf.keras.backend</code> are redundant with other TensorFlow libraries. For instance, <code>tf.keras.backend.abs</code> and <code>tf.math.abs</code> are not aliases (or at least, they're not listed as aliases in the documentation), but both take the absolute value of a tensor. After examining the source code, it looks like <code>tf.keras.backend.abs</code> calls the <code>tf.math.abs</code> function, and so I really do not understand why they are not aliases. Other <code>tf.keras.backend</code> operations don't appear to be duplicated in TensorFlow libraries, but it looks like there are TensorFlow functions that can do equivalent things. For instance, <code>tf.keras.backend.cast_to_floatx</code> can be substituted with <code>tf.dtypes.cast</code> as long as you explicitly specify the dtype. I am wondering two things:</p>

<ol>
<li>when is it best to use the <code>tf.keras.backend</code> library instead of the equivalent TensorFlow functions?</li>
<li>is there a difference in these functions (and other equivalent <code>tf.keras.backend</code> functions) that I am missing?</li>
</ol>
","I have been working in TensorFlow for about a year now, and I am transitioning from TF 1.x to TF 2.0, and I am looking for some guidance on how to use the tf.keras.backend library in TF 2.0. I understand that the transition to TF 2.0 is supposed to remove a lot of redundancies in modeling and building graphs, since there were many ways to create equivalent layers in earlier TensorFlow versions (and I'm insanely grateful for that change!), but I'm getting stuck on understanding when to use tf.keras.backend, because the operations appear redundant with other TensorFlow libraries. I see that some of the functions in tf.keras.backend are redundant with other TensorFlow libraries. For instance, tf.keras.backend.abs and tf.math.abs are not aliases (or at least, they're not listed as aliases in the documentation), but both take the absolute value of a tensor. After examining the source code, it looks like tf.keras.backend.abs calls the tf.math.abs function, and so I really do not understand why they are not aliases. Other tf.keras.backend operations don't appear to be duplicated in TensorFlow libraries, but it looks like there are TensorFlow functions that can do equivalent things. For instance, tf.keras.backend.cast_to_floatx can be substituted with tf.dtypes.cast as long as you explicitly specify the dtype. I am wondering two things:",https://stackoverflow.com/questions/59361689,4982425,Documentation Replication on Other Examples
59497372,Is there an alternative to tf.py_function() for custom Python code?,"<p>I have started using TensorFlow 2.0 and have a little uncertainty with regard to one aspect.</p>

<p>Suppose I have this use case: while ingesting data with the <code>tf.data.Dataset</code> I want to apply some specific augmentation operations upon some images. However, the external libraries that I am using <strong>require</strong> that the <strong>image is a numpy array</strong>, <strong>not a tensor</strong>.</p>

<p>When using <code>tf.data.Dataset.from_tensor_slices()</code>, the flowing data needs to be of type Tensor. Concrete example:</p>

<pre><code>def my_function(tensor_image):
   print(tensor_image.numpy()
   return


data = tf.data.Dataset.from_tensor_slices(tensor_images).map(my_function)
</code></pre>

<p>The code above does not work yielding an </p>

<blockquote>
  <p>'Tensor' object has no attribute 'numpy' error.</p>
</blockquote>

<p>I have read the documentation on TensorFlow 2.0 stating that if one wants to use an arbitrary python logic, one should use <code>tf.py_function</code> <strong>or only TensorFlow primitives</strong> according to:
<a href=""https://stackoverflow.com/questions/56075037/how-to-convert-tensor-to-numpy-array-in-tensorflow"">How to convert &quot;tensor&quot; to &quot;numpy&quot; array in tensorflow?</a> </p>

<p><strong>My question is the following</strong>: Is there another way to use arbitrary python code in a function with a custom decorator/an easier way than to use <code>tf.py_function</code>?</p>

<p>To me honestly it seems that there must be a more elegant way than passing to a <code>tf.py_function</code>, transforming to a numpy array, perform operations A,B,C,D and then retransform to a tensor and yield the result. </p>
","I have started using TensorFlow 2.0 and have a little uncertainty with regard to one aspect. Suppose I have this use case: while ingesting data with the tf.data.Dataset I want to apply some specific augmentation operations upon some images. However, the external libraries that I am using require that the image is a numpy array, not a tensor. When using tf.data.Dataset.from_tensor_slices(), the flowing data needs to be of type Tensor. Concrete example: The code above does not work yielding an I have read the documentation on TensorFlow 2.0 stating that if one wants to use an arbitrary python logic, one should use tf.py_function or only TensorFlow primitives according to: How to convert ""tensor"" to ""numpy"" array in tensorflow? My question is the following: Is there another way to use arbitrary python code in a function with a custom decorator/an easier way than to use tf.py_function? To me honestly it seems that there must be a more elegant way than passing to a tf.py_function, transforming to a numpy array, perform operations A,B,C,D and then retransform to a tensor and yield the result.",https://stackoverflow.com/questions/59497372,6117017,Lack of Alternative Solutions/Documentation
59531864,Why does TensorFlow calculate 2D convolutions when 1D convolution is called?,"<p>In the documentation of tf.nn.conv1d, it is stated that</p>

<blockquote>
  <p>Internally, this op reshapes the input tensors and invokes tf.nn.conv2d. For example, if data_format does not start with ""NC"", a tensor of shape [batch, in_width, in_channels] is reshaped to [batch, 1, in_width, in_channels], and the filter is reshaped to [1, filter_width, in_channels, out_channels]. The result is then reshaped back to [batch, out_width, out_channels] (where out_width is a function of the stride and padding as in conv2d) and returned to the caller.</p>
</blockquote>

<p>I get that the operations are equivalent, but I am a bit confused about the implications of this implementation detail. </p>

<p>Does the reshaping create some computational overhead? 
The 3D convolution has its own implementation, so why not the 1D convolution?</p>

<p>Thanks for any explanation that helps me and others to understand this implementation detail of TensorFlow!</p>
","In the documentation of tf.nn.conv1d, it is stated that I get that the operations are equivalent, but I am a bit confused about the implications of this implementation detail. Does the reshaping create some computational overhead? The 3D convolution has its own implementation, so why not the 1D convolution? Thanks for any explanation that helps me and others to understand this implementation detail of TensorFlow!",https://stackoverflow.com/questions/59531864,6251391,Documentation Replicability
59555206,keras to tf.keras Conversion: Dense layer dimensions not defined?,"<p>So I've built a convnet using pure <code>keras</code>. It compiles and operates exactly as intended, but I need to convert it to use <code>tf.keras</code> so that I can make use of <code>tfmot</code>. Having read documentation, I attempted to convert it, only to get the following error:</p>

<p><code>The last dimension of the inputs to Dense should be defined. Found None.</code> </p>

<p>Any idea what I'm doing wrong?</p>

<p>Thanks!</p>

<p>Original <code>keras</code> model:</p>

<pre><code>input_layer = keras.layers.Input(shape=(100,))
reshape_layer = keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = keras.layers.Flatten()(conv_layer_5)
label_layer = keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = keras.layers.Dense(1, activation=""linear"")(label_layer)

model = keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>Converted <code>tf.keras</code> model:</p>

<pre><code>input_layer = tf.keras.layers.InputLayer(input_shape=(100,))
reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)
label_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>EDIT 1:</p>

<p>I thought maybe I could get around the issue by saving the <code>keras</code> model after creation and loading it as a <code>tf.keras</code> model immediately before compilation / training. That throws the same error! </p>
","So I've built a convnet using pure keras. It compiles and operates exactly as intended, but I need to convert it to use tf.keras so that I can make use of tfmot. Having read documentation, I attempted to convert it, only to get the following error: The last dimension of the inputs to Dense should be defined. Found None. Any idea what I'm doing wrong? Thanks! Original keras model: Converted tf.keras model: EDIT 1: I thought maybe I could get around the issue by saving the keras model after creation and loading it as a tf.keras model immediately before compilation / training. That throws the same error!",https://stackoverflow.com/questions/59555206,6639365,Documentation Replication on Other Examples
59607363,Tensorflow 2.0 dataset batching not working properly,"<p>Tensorflow 2.0 dataset api's batch is not working as I expected it to work.</p>

<p>I've made a dataset like this.</p>

<pre><code>self.train_dataset = tf.data.Dataset.from_generator(generator=train_generator, output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6]), tf.TensorShape([])))
</code></pre>

<p>This yields DatasetV1Adapter shapes: ((6,), ()), types: (tf.float32, tf.float32),
and to this dataset I applied batch function from tf.data.Dataset.</p>

<pre><code>self.train_dataset.batch(1024)
</code></pre>

<p>yields DatasetV1Adapter shapes: ((None, 6), (None,)), types: (tf.float32, tf.float32), and changing the batch size doesn't help at all. </p>

<p>From official description of the batch, </p>

<blockquote>
  <p>The components of the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced.</p>
</blockquote>

<p>The way I thought this function would work, was to make [batch, 6], [batch,] but didn't work out well. </p>

<p>I originally used pytorch, and started using TF 2.0 recently, and need some help on proper batching. Thanks in advance.</p>
","Tensorflow 2.0 dataset api's batch is not working as I expected it to work. I've made a dataset like this. This yields DatasetV1Adapter shapes: ((6,), ()), types: (tf.float32, tf.float32), and to this dataset I applied batch function from tf.data.Dataset. yields DatasetV1Adapter shapes: ((None, 6), (None,)), types: (tf.float32, tf.float32), and changing the batch size doesn't help at all. From official description of the batch, The way I thought this function would work, was to make [batch, 6], [batch,] but didn't work out well. I originally used pytorch, and started using TF 2.0 recently, and need some help on proper batching. Thanks in advance.",https://stackoverflow.com/questions/59607363,10690874,Documentation Replication on Other Examples
59743351,Tensorflow 2.0.0: AttributeError: 'TensorSliceDataset' object has no attribute 'as_numpy_iterator',"<p>I am testing tensorflow <code>tf.data.Dataset</code> method <code>as_numpy_iterator</code> using <code>tensorflow 2.0.0</code>. According to the official documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#as_numpy_iterator"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#as_numpy_iterator</a>, this function allows directly inspecting the content of a tensorflow dataset. But when I try the given example:</p>

<pre><code>dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) 
for element in dataset.as_numpy_iterator(): 
  print(element) 
</code></pre>

<p>There occurs an error: <code>AttributeError: 'TensorSliceDataset' object has no attribute 'as_numpy_iteractor'</code>. I am wondering if this method is just newly added, beyond the support of tensorflow 2.0.0. If so, is there an alternative to checking the dataset content as the <code>as_numpy_iterator()</code>?</p>
","I am testing tensorflow tf.data.Dataset method as_numpy_iterator using tensorflow 2.0.0. According to the official documentation https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#as_numpy_iterator, this function allows directly inspecting the content of a tensorflow dataset. But when I try the given example: There occurs an error: AttributeError: 'TensorSliceDataset' object has no attribute 'as_numpy_iteractor'. I am wondering if this method is just newly added, beyond the support of tensorflow 2.0.0. If so, is there an alternative to checking the dataset content as the as_numpy_iterator()?",https://stackoverflow.com/questions/59743351,6807211,Documentation Replicability
59998335,Constantly update tf.cond based on bool value,"<p>I am using <code>tf.cond</code> for controlling the flow of the Tensorflow graph. I went through the documentation and was able to implement <code>tf.cond</code> based branching successfully. But my concern is that while the graph is being loaded the value of the <code>bool</code> variable is checked and the branching decision is made at the initialization step itself. Any further changes in the <code>bool</code> is not tracked. Following is the MWE that better describes the problem:</p>

<pre class=""lang-py prettyprint-override""><code>def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    global foo
    if i &gt; 10:
        foo = False
    print(sess.run(x))    
</code></pre>

<p>This prints only <code>32</code>s. </p>

<p>I tried with <code>eager_execution</code> too with the following code:</p>

<pre class=""lang-py prettyprint-override""><code>tf.enable_eager_execution()
def funa():
    return tf.constant(32)

def funb():
    return tf.constant(21)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(x)
</code></pre>

<p>Still the same result.</p>

<p>So my question is how can I write code such that one part of the graph is chosen dynamically, based on the updates to the <code>bool</code> variable (if possible)? Thanks. I am using Tensorflow v1.14.</p>
","I am using tf.cond for controlling the flow of the Tensorflow graph. I went through the documentation and was able to implement tf.cond based branching successfully. But my concern is that while the graph is being loaded the value of the bool variable is checked and the branching decision is made at the initialization step itself. Any further changes in the bool is not tracked. Following is the MWE that better describes the problem: This prints only 32s. I tried with eager_execution too with the following code: Still the same result. So my question is how can I write code such that one part of the graph is chosen dynamically, based on the updates to the bool variable (if possible)? Thanks. I am using Tensorflow v1.14.",https://stackoverflow.com/questions/59998335,6997665,Documentation Replication on Other Examples
60013980,tf.nn.embedding_lookup_sparse 3D sparse tensor input,"<p>I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup_sparse</code></a> I found it only supports 2D sparse tensors,</p>

<blockquote>
  <p>sp_ids: N x M SparseTensor of int64 ids where N is typically batch size and M is arbitrary.</p>
</blockquote>

<p>My example code here</p>

<pre><code>import numpy as np
import tensorflow as tf
tf.enable_eager_execution()

# [feature number, embedding dim] 
w = tf.get_variable(""w"", [4, 4], initializer=tf.random_normal_initializer())

z = np.array(
     [
      [
        [0, 1, 2, 3],   # get the vector of row 0, 1, 2, 3 of the embedding matrix w and get the sum
        [2, 3]
      ],

      [
        [1, 3],
        [2]
      ],

      [
        [0, 1, 3],
        [1, 2]
      ]
     ])

sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],
                     indices=[[0,0,0],[0,0,1],[0,0,2],[0,0,3],[0,1,2],
                              [0,1,3],[1,0,1],[1,0,3],[1,1,2],[2,0,0],
                              [2,0,1],[2,0,3],[2,1,1],[2,1,2]],
                     dense_shape=[3, 2, 4])

tf.nn.embedding_lookup_sparse(w, sp, None, combiner='sum')
# the outputs
&lt;tf.Tensor: id=970, shape=(3, 4), dtype=float32, numpy=
array([[-5.8729677 , -1.3900641 ,  0.8126096 , -3.1223912 ],
       [-1.0788026 , -1.1324122 ,  0.34160078,  0.23714277],
       [-2.497394  , -2.7855003 ,  3.0201516 , -1.8009453 ]],
      dtype=float32)&gt;

print(w)
&lt;tf.Variable 'w:0' shape=(4, 4) dtype=float32, numpy=
array([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],
       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],
      dtype=float32)&gt;
</code></pre>

<p>But the expected output is a matrix with a dimension of <code>3x2x4</code>, not <code>3x4</code>. Does <code>tf.nn.embedding_lookup_sparse</code> support this operation?</p>
","I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of tf.nn.embedding_lookup_sparse I found it only supports 2D sparse tensors, My example code here But the expected output is a matrix with a dimension of 3x2x4, not 3x4. Does tf.nn.embedding_lookup_sparse support this operation?",https://stackoverflow.com/questions/60013980,5046896,Documentation Replicability
60047705,Repeated use of GradientTape for multiple Jacobian calculations,"<p>I am attempting to compute the Jacobian of a TensorFlow neural network's outputs with respect to its inputs. This is easily achieved with the <code>tf.GradientTape.jacobian</code> method. The trivial example provided in the TensorFlow documentation is as follows:</p>

<pre><code>with tf.GradientTape() as g:
  x  = tf.constant([1.0, 2.0])
  g.watch(x)
  y = x * x
jacobian = g.jacobian(y, x)
</code></pre>

<p>This is fine if I want only want to compute the Jacobian of a single instance of the input tensor <code>x</code>. However, I need to repeatedly evaluate this Jacobian many, many times for various instances of <code>x</code>. For a non-trivial Jacobian calculation (e.g. for a deep convolutional neural network with non-linear activation functions), this is incredibly expensive to repeatedly rerun the GradientTape calculation and evaluate the <code>jacobian</code> method. I know from the <a href=""https://www.tensorflow.org/tutorials/customization/autodiff"" rel=""nofollow noreferrer"">TensorFlow documentation</a> that the gradients (and hence the Jacobian) are computed via automatic differentiation.  I have to imagine there is some internal storage of the analytical gradient of the network (computed by automatic differentiation) which is evaluated at the given inputs. </p>

<p>My question: am I correct in assuming that TensorFlow builds and stores (at least parts of) the analytical gradients needed to compute the Jacobian? And if so, is there a way to save this analytical gradient and re-evaluate the Jacobian with new inputs without having to reconstruct it via the GradientTape method?</p>

<p>A ""persistent"" GradientTape does not seem to solve this issue: it only allows for the repeated evaluation of a single GradientTape instance with respect to multiple internal arguments of the computation.</p>
","I am attempting to compute the Jacobian of a TensorFlow neural network's outputs with respect to its inputs. This is easily achieved with the tf.GradientTape.jacobian method. The trivial example provided in the TensorFlow documentation is as follows: This is fine if I want only want to compute the Jacobian of a single instance of the input tensor x. However, I need to repeatedly evaluate this Jacobian many, many times for various instances of x. For a non-trivial Jacobian calculation (e.g. for a deep convolutional neural network with non-linear activation functions), this is incredibly expensive to repeatedly rerun the GradientTape calculation and evaluate the jacobian method. I know from the TensorFlow documentation that the gradients (and hence the Jacobian) are computed via automatic differentiation. I have to imagine there is some internal storage of the analytical gradient of the network (computed by automatic differentiation) which is evaluated at the given inputs. My question: am I correct in assuming that TensorFlow builds and stores (at least parts of) the analytical gradients needed to compute the Jacobian? And if so, is there a way to save this analytical gradient and re-evaluate the Jacobian with new inputs without having to reconstruct it via the GradientTape method? A ""persistent"" GradientTape does not seem to solve this issue: it only allows for the repeated evaluation of a single GradientTape instance with respect to multiple internal arguments of the computation.",https://stackoverflow.com/questions/60047705,11737392,Documentation Replication on Other Examples
60143153,Is there a way in tensorflow to load batches of data each time?,"<p>So I'm running tensorflow 2+ python in google colab.</p>

<p>Each of my data file is a 3d image with shape [563, 563, 563, 1], so loading all of them throws a resource exhaustion error.</p>

<p>I've spent days and hours searching for a way to load only a batch of my dataset as tensor and unloading/loading new batch each iteration. I'm guessing there might be a way using tf.data.Dataset.list_files, but I can't find the exact way.</p>

<p>Is there any good suggestions on a way to do it or any documents I could try to read? I've read the tf.data document from tensorflow, but couldn't find the information I needed.</p>

<p>Thank you!</p>

<h1>Edit</h1>

<p>so this is the function I want to use to load my image</p>

<pre><code>def load_image(ind):
    file_brain = ""/content/drive/My Drive/brain/"" + str(ind) + "".mgz""
    file_mask = ""/content/drive/My Drive/mask/"" + str(ind) + "".mgz""
    data_brain, affine = load_nifti(file_brain)
    data_mask, affine = load_nifti(file_mask)
    data_brain = affine_transform(data_brain, affine)
    data_mask = affine_transform(data_mask, affine)
    data_brain = normalize(data_brain)
    data_brain = zoom(data_brain, (563/256, 563/256, 563/256))
    data_brain = tf.expand_dims(data_brain, axis=-1)
    data_mask = tf.expand_dims(data_mask, axis=-1)
    return data_brain, data_mask
</code></pre>

<p>and this was the way I was loading the dataset before, which exhausted the resource;</p>

<pre><code>def create_dataset():
    train_data = []
    train_label = []
    test_data = []
    test_label = []
    test_n = np.random.randint(1, 10, 1)
    for i in range(1, 10):
        data_brain, data_mask = load_image(i)
        if i in test_n:
            test_data.append(data_brain)
            test_label.append(data_mask)
            continue
        train_data.append(data_brain)
        train_label.append(data_mask)
        shifted_data = data_brain + tf.random.uniform(shape=(), minval=-0.05, maxval=0.05)
        scaled_data = data_brain * tf.random.uniform(shape=(), minval=0.85, maxval=1.3)
        train_data.append(shifted_data)
        train_label.append(data_mask)
        train_data.append(scaled_data)
        train_label.append(data_mask)
""""""
train_data = tf.data.Dataset.from_tensor_slices(train_data)
train_label = tf.data.Dataset.from_tensor_slices(train_label)
test_data = tf.data.Dataset.from_tensor_slices(test_data)
test_label = tf.data.Dataset.from_tensor_slices(test_label)
return train_data, train_label, test_data, test_label
""""""
</code></pre>
","So I'm running tensorflow 2+ python in google colab. Each of my data file is a 3d image with shape [563, 563, 563, 1], so loading all of them throws a resource exhaustion error. I've spent days and hours searching for a way to load only a batch of my dataset as tensor and unloading/loading new batch each iteration. I'm guessing there might be a way using tf.data.Dataset.list_files, but I can't find the exact way. Is there any good suggestions on a way to do it or any documents I could try to read? I've read the tf.data document from tensorflow, but couldn't find the information I needed. Thank you! so this is the function I want to use to load my image and this was the way I was loading the dataset before, which exhausted the resource;",https://stackoverflow.com/questions/60143153,12869645,Requesting (Additional) Resources
60213882,Using Tensorflow Interleave to Improve Performance,"<p>I have an input pipe that is performing poorly with low CPU, GPU, and disk utilization. I've been reading the tensorflow ""Better performance with tf.data API"" doc and the Dataset docs, but I don't understand what's going on well enough to apply it to my situation. Here's my current setup:</p>

<pre><code>img_files = sorted(tf.io.gfile.glob(...))
imgd = tf.data.FixedLengthRecordDataset(img_files, inrez*inrez)
#POINT1A
imgd = imgd.map(lambda s: tf.reshape(tf.io.decode_raw(s, tf.int8), (inrez,inrez,1)))
imgd = imgd.map(lambda x: tf.cast(x, dtype=tf.float32))

out_files = sorted(tf.io.gfile.glob(...))
outd = tf.data.FixedLengthRecordDataset(out_files, 4, compression_type=""GZIP"")
#POINT1B
outd = outd.map(lambda s: tf.io.decode_raw(s, tf.float32))

xsrc = tf.data.Dataset.zip((imgd, outd)).batch(batchsize)
xsrc = xsrc.repeat()        # indefinitely
#POINT2
xsrc = xsrc.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
</code></pre>

<p>Should I interleave the whole pipe right at the end (POINT2), before the prefetch? Or interleave imgd and outd separately, after each FixedLengthRecordDataset (POINT1A, POINT1B), and parallelize the maps? (need to keep the imgd and outd synced up!) What's up with Dataset.range(rvalue)---seems it's necessary but not obvious what rvalue to use? Is there a better overall plan?</p>

<p>Note that the datasets are very large and do not fit in RAM.</p>
","I have an input pipe that is performing poorly with low CPU, GPU, and disk utilization. I've been reading the tensorflow ""Better performance with tf.data API"" doc and the Dataset docs, but I don't understand what's going on well enough to apply it to my situation. Here's my current setup: Should I interleave the whole pipe right at the end (POINT2), before the prefetch? Or interleave imgd and outd separately, after each FixedLengthRecordDataset (POINT1A, POINT1B), and parallelize the maps? (need to keep the imgd and outd synced up!) What's up with Dataset.range(rvalue)---seems it's necessary but not obvious what rvalue to use? Is there a better overall plan? Note that the datasets are very large and do not fit in RAM.",https://stackoverflow.com/questions/60213882,2076669,Documentation Replicability
60215970,What's the cleanest and most efficient way to pass two stereo images to a loss function in Keras?,"<p>First off, why am I using Keras? I'm trying to stay as high level as possible, which doesn't mean I'm scared of low-level Tensorflow; I just want to see how far I can go while keeping my code as simple and readable as possible.</p>

<p>I need my Keras model (custom-built using the Keras functional API) to read the left image from a stereo pair and minimize a loss function that needs to access both the right and left images. I want to store the data in a <code>tf.data.Dataset</code>.</p>

<p>What I tried:</p>

<ol>
<li>Reading the dataset as <code>(left image, right image)</code>, i.e. as tensors with shape <code>((W, H, 3), (W, H, 3))</code>, then use function closure: define a <code>keras_loss(left_images)</code> that returns a <code>loss(y_true, y_pred)</code>, with <code>y_true</code> being a <code>tf.Tensor</code> that holds the right image. The problem with this approach is that <code>left_images</code> is a <code>tf.data.Dataset</code> and Tensorflow complains (rightly so) that I'm trying to operate on a dataset instead of a tensor. </li>
<li><p>Reading the dataset as <code>(left image, (left image, right image))</code>, which should make <code>y_true</code> a <code>tf.Tensor</code> with shape <code>((W, H, 3), (W, H, 3))</code> that holds both the right and left images. The problem with this approach is that it...does not work and raises the following error:</p>

<pre><code>ValueError: Error when checking model target: the list of Numpy arrays 
that you are passing to your model is not the size the model expected. 
Expected to see 1 array(s), for inputs ['tf_op_layer_resize/ResizeBilinear'] 
but instead got the following list of 2 arrays: [&lt;tf.Tensor 'args_1:0' 
shape=(None, 512, 256, 3) dtype=float32&gt;, &lt;tf.Tensor 'args_2:0' 
shape=(None, 512, 256, 3) dtype=float32&gt;]...
</code></pre></li>
</ol>

<p>So, is there anything I did not consider? I read the documentation and found nothing about what gets considered as <code>y_pred</code> and what as <code>y_true</code>, nor about how to convert a dataset into a tensor smartly and without loading it all in memory. </p>

<p>My model is designed as such:</p>

<pre><code> def my_model(input_shape):
     width = input_shape[0]
     height = input_shape[1]
     inputs = tf.keras.Input(shape=input_shape)
     # &lt; a few more layers &gt;
     outputs = tf.image.resize(tf.nn.sigmoid(tf.slice(disp6, [0, 0, 0, 0], [-1, -1, -1, 2])), tf.Variable([width, height]))
     model = tf.keras.Model(inputs=inputs, outputs=outputs)
     return model
</code></pre>

<p>And my dataset is built as such (in case 2, while in case 1 only the function <code>read_stereo_pair_from_line()</code> changes):</p>

<pre><code>def read_img_from_file(file_name):
    img = tf.io.read_file(file_name)
    # convert the compressed string to a 3D uint8 tensor
    img = tf.image.decode_png(img, channels=3)
    # Use `convert_image_dtype` to convert to floats in the [0,1] range.
    img = tf.image.convert_image_dtype(img, tf.float32)
    # resize the image to the desired size.
    return tf.image.resize(img, [args.input_width, args.input_height])


def read_stereo_pair_from_line(line):
    split_line = tf.strings.split(line, ' ')
    return read_img_from_file(split_line[0]), (read_img_from_file(split_line[0]), read_img_from_file(split_line[1]))

# Dataset loading
list_ds = tf.data.TextLineDataset('test/files.txt')
images_ds = list_ds.map(lambda x: read_stereo_pair_from_line(x))
images_ds = images_ds.batch(1)
</code></pre>
","First off, why am I using Keras? I'm trying to stay as high level as possible, which doesn't mean I'm scared of low-level Tensorflow; I just want to see how far I can go while keeping my code as simple and readable as possible. I need my Keras model (custom-built using the Keras functional API) to read the left image from a stereo pair and minimize a loss function that needs to access both the right and left images. I want to store the data in a tf.data.Dataset. What I tried: So, is there anything I did not consider? I read the documentation and found nothing about what gets considered as y_pred and what as y_true, nor about how to convert a dataset into a tensor smartly and without loading it all in memory. My model is designed as such: And my dataset is built as such (in case 2, while in case 1 only the function read_stereo_pair_from_line() changes):",https://stackoverflow.com/questions/60215970,5623016,Lack of Alternative Solutions/Documentation
60311184,how to loop over a tensor object until a condition met,"<p>I have a tensor like this:</p>

<pre><code>masked_bad_col = [[False  True  True False  True  True  True  True  True  True  True False]]
</code></pre>

<p>I want to loop through this tensor untill all elements get <code>True</code>.
So I have another function, which will update this tensor, lets call it <code>uniqueness</code>.</p>

<pre><code>def uniqueness():

   'blah blah blha'
   return tensor1, updated_masked_bad_col
</code></pre>

<p>I looked at the documentation and got to know that I can do that using <code>tf.while_loop</code>. Although, I could not find any example working on boolean stuff.
This is what I have done so far:</p>

<pre><code>tensor1, _ = tf.while_loop(masked_bad_col != True, uniqueness)
</code></pre>

<p>It is obviously incorrect, but don't know how to use each element of <code>masked_bad_col</code> as a condition to continue looping through <code>uniqueness</code> function.</p>

<p><strong>Update 1</strong>
This is the method I am trying to call in the loop:</p>

<pre><code>corpus = load_corpus('path_to_corpus/train.corpus')
topics = []
vocab, docs = corpus['vocab'], corpus['docs']
number_of_topics = 0
encoder_model = load_keras_model(
    'path_to_model/encoder_model',
    custom_objects={""KCompetitive"": KCompetitive})
weights = encoder_model.get_weights()[0]
for idx in range(encoder_model.output_shape[1]):
    token_idx = np.argsort(weights[:, idx])[::-1][:20]
    topics.append([(revdict(vocab)[x]) for x in token_idx])
    number_of_topics += 1

nparr = np.asarray(topics)
# print nparr.shape

unique, indices, count = np.unique(nparr, return_inverse=True, return_counts=True)

tensor1 = (np.sum(count[indices].reshape(nparr.shape), axis=1).reshape(1, nparr.shape[0]) / (
        number_of_topics * 20))

def uniqueness_score():
    corpus = load_corpus('path_to_corpus/train.corpus')
    topics = []
    vocab, docs = corpus['vocab'], corpus['docs']
    number_of_topics = 0
    encoder_model = load_keras_model(
        'path_to_model/encoder_model',
        custom_objects={""KCompetitive"": KCompetitive})
    weights = encoder_model.get_weights()[0]
    for idx in range(encoder_model.output_shape[1]):
        token_idx = np.argsort(weights[:, idx])[::-1][:20]
        topics.append([(revdict(vocab)[x]) for x in token_idx])
        number_of_topics += 1

    nparr = np.asarray(topics)

    unique, indices, count = np.unique(nparr, return_inverse=True, return_counts=True)

    tensor1 = (np.sum(count[indices].reshape(nparr.shape), axis=1).reshape(1, nparr.shape[0]) / (
            number_of_topics * 20))
    return tensor1
</code></pre>

<p>And this is the way I called this method in the <code>while_loop</code></p>

<pre><code>with tf.Session() as sess:

        tensor2, _ = tf.while_loop(
            # Loop condition (negated goal condition)
            lambda tensor1: ~tf.math.reduce_all(tensor1 &gt; tf.reduce_mean(tensor1)),
            # Loop body
            lambda tensor1: uniqueness_score(),
            # Loop variables
            [tensor1])
        # Returned loop value
        print(tensor2.eval())
</code></pre>
","I have a tensor like this: I want to loop through this tensor untill all elements get True. So I have another function, which will update this tensor, lets call it uniqueness. I looked at the documentation and got to know that I can do that using tf.while_loop. Although, I could not find any example working on boolean stuff. This is what I have done so far: It is obviously incorrect, but don't know how to use each element of masked_bad_col as a condition to continue looping through uniqueness function. Update 1 This is the method I am trying to call in the loop: And this is the way I called this method in the while_loop",https://stackoverflow.com/questions/60311184,7934786,Inadequate Examples
60314717,How does shuffle and batch work in tf.data.dataset?,"<p>I'm working on a large dataset with around 10million datapoints so I've decided to use tf.data.dataset api for fetching dataset.</p>

<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices((data))
train = train_dataset.shuffle(100000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)
</code></pre>

<p>I've few doubts which isn't clear from tensorflow docs. I hope someone can address them.</p>

<p>How does the shuffle work in my case? Because I have 10 million datapoints should I shuffle all 10 million (or) will 100k be enough? Will it have any performance impact choosing a large shuffle? </p>

<p>Will the batch is considered only from shuffled dataset (or) the original dataset?</p>
",I'm working on a large dataset with around 10million datapoints so I've decided to use tf.data.dataset api for fetching dataset. I've few doubts which isn't clear from tensorflow docs. I hope someone can address them. How does the shuffle work in my case? Because I have 10 million datapoints should I shuffle all 10 million (or) will 100k be enough? Will it have any performance impact choosing a large shuffle? Will the batch is considered only from shuffled dataset (or) the original dataset?,https://stackoverflow.com/questions/60314717,11816060,Documentation Replication on Other Examples
60384790,Difference - tf.gradients vs tf.keras.backend.gradients,"<p>Being new to Tensorflow, I am trying to understand the difference between underlying functionality of tf.gradients and tf.keras.backend.gradients.</p>

<p>The latter finds the gradient of input feature values w.r.t cost function. </p>

<p>But I couldn't get a clear idea on the former whether it computes the gradient over cost function or output probabilities (For example, consider the case of binary classification using a simple feed forward network. Output probability here is referred to the Sigmoid activation outcome of final layer with single neuron. Cost is given by Binary cross entropy)</p>

<p>I have referred the official documentation for tf.gradients, but it is short and vague (for me), and I did not get a clear picture - The documentation mentions it as just 'y' - is it cost or output probability? </p>

<p>Why I need the gradients? 
To implement a basic gradient based feature attribution. </p>
","Being new to Tensorflow, I am trying to understand the difference between underlying functionality of tf.gradients and tf.keras.backend.gradients. The latter finds the gradient of input feature values w.r.t cost function. But I couldn't get a clear idea on the former whether it computes the gradient over cost function or output probabilities (For example, consider the case of binary classification using a simple feed forward network. Output probability here is referred to the Sigmoid activation outcome of final layer with single neuron. Cost is given by Binary cross entropy) I have referred the official documentation for tf.gradients, but it is short and vague (for me), and I did not get a clear picture - The documentation mentions it as just 'y' - is it cost or output probability? Why I need the gradients? To implement a basic gradient based feature attribution.",https://stackoverflow.com/questions/60384790,11606547,Requesting (Additional) Resources
60398554,"Should we apply repeat, batch shuffle to tf.data.Dataset when passing it to fit function?","<p>I still don't after having read documentation about <code>tf.keras.Model.fit</code> and <code>tf.data.Dataset</code>, when passing <code>tf.data.Dataset</code> to fit function, should I call <code>repeat</code> and <code>batch</code> on the dataset object or should I provide the <code>batch_size</code> and <code>epochs</code> arguments to fit instead? or both? Should I apply the same treatment to the validation set?</p>

<p>And while I'm here, can I <code>shuffle</code> the dataset before the <code>fit</code>? (seems like it's an obvious yes)
If so, before, after calling <code>Dataset.batch</code> and <code>Dataset.repeat</code> (if calling them)?</p>

<p><strong>Edit:</strong> When using <code>batch_size</code> argument, and without having called <code>Dataset.batch(batch_size)</code> previously, I am getting the following error:</p>

<pre><code>ValueError: The `batch_size` argument must not be specified for the given input type.
Received input: &lt;MapDataset shapes: ((&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;), (&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;)), 
types: ((tf.float32, tf.float32, tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32))&gt;, 
batch_size: 1
</code></pre>

<p>Thanks</p>
","I still don't after having read documentation about tf.keras.Model.fit and tf.data.Dataset, when passing tf.data.Dataset to fit function, should I call repeat and batch on the dataset object or should I provide the batch_size and epochs arguments to fit instead? or both? Should I apply the same treatment to the validation set? And while I'm here, can I shuffle the dataset before the fit? (seems like it's an obvious yes) If so, before, after calling Dataset.batch and Dataset.repeat (if calling them)? Edit: When using batch_size argument, and without having called Dataset.batch(batch_size) previously, I am getting the following error: Thanks",https://stackoverflow.com/questions/60398554,7483509,Documentation Replication on Other Examples
60469970,How does tf.function compile a python function with autograph?,"<p>How does <code>tf.function</code> compile a python function operating on tensors into a graph, especially wrt autograph? The <a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""nofollow noreferrer"">docs</a> don't go into detail</p>

<blockquote>
  <p><code>tf.function</code> constructs a callable that executes a TensorFlow graph (<code>tf.Graph</code>) created by trace-compiling the TensorFlow operations in <code>func</code>, effectively executing <code>func</code> as a TensorFlow graph.</p>
</blockquote>

<p>Does it use the special methods called by conditionals (<code>__bool__</code>) and loops (<code>__iter__</code>) to 'trace' the function's implementation? For example</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

@tf.function
def op(t: tf.Tensor) -&gt; tf.Tensor:
    if tf.reduce_sum(t) == 0:
        for _ in t:
            ...
</code></pre>

<p>could use the fact that the <code>if</code> results in <code>Tensor.__bool__(...)</code> and <code>for _ in t</code> results in <code>Tensor.__iter__(...)</code></p>
","How does tf.function compile a python function operating on tensors into a graph, especially wrt autograph? The docs don't go into detail Does it use the special methods called by conditionals (__bool__) and loops (__iter__) to 'trace' the function's implementation? For example could use the fact that the if results in Tensor.__bool__(...) and for _ in t results in Tensor.__iter__(...)",https://stackoverflow.com/questions/60469970,5986907,Documentation Replicability
60516977,Difficulties in understanding higher order derivatives for tf.custom_gradient(),"<p>Based on the example as quoted in tensorflow's website here: <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/custom_gradient</a></p>

<pre><code>@tf.custom_gradient
def op_with_fused_backprop(x):
     y, x_grad = fused_op(x)

     def first_order_gradient(dy):
         @tf.custom_gradient
         def first_order_custom(unused_x):
             def second_order_and_transpose(ddy):
                 return second_order_for_x(...), gradient_wrt_dy(...)
             return x_grad, second_order_and_transpose
         return dy * first_order_custom(x)
     return y, first_order_gradient
</code></pre>

<p>There is a lack of details on why <code>second_order_and_transpose(ddy)</code> returns two objects. Based on the documentation of tf.custom_gradient, the <code>grad_fn</code> (<em>i.e.</em> <code>second_order_and_transpose()</code>) should return a list of Tensors which are the derivatives of dy w.r.t. <code>unused_x</code>. It is also not even clear why did they name it <code>unused_x</code>. Anyone has any idea on this example or in general create custom gradients for higher order derivatives?</p>
","Based on the example as quoted in tensorflow's website here: https://www.tensorflow.org/api_docs/python/tf/custom_gradient There is a lack of details on why second_order_and_transpose(ddy) returns two objects. Based on the documentation of tf.custom_gradient, the grad_fn (i.e. second_order_and_transpose()) should return a list of Tensors which are the derivatives of dy w.r.t. unused_x. It is also not even clear why did they name it unused_x. Anyone has any idea on this example or in general create custom gradients for higher order derivatives?",https://stackoverflow.com/questions/60516977,4723266,Documentation Completeness
60525257,How to apply Non max suppression on batch of images in tensorflow 1.14?,"<p>I have batch of cropped images from original image on which I have to perform object detection, I am trying to apply tensorflow NMS operation. </p>

<p>I looked into tensorflow api docs, and found <code>tf.image.combined_non_max_suppression()</code>, but I am unable to understand it properly.</p>

<p>The flow in my pipeline is of two step.</p>

<ol>
<li>I get some image and apply object detection to get desired region of interests.</li>
<li>On each of these ROIs I have to apply object detection again, so I am passing it as batch.</li>
</ol>

<p>For the first step, I use simple <code>tf.image.non_max_suppression()</code> followed by <code>tf.gather()</code>, but I am not able to understand, how to do it for second step. </p>

<p>Please refer to code snippets below:</p>

<pre><code>with tf.Session(graph = self.detection_graph) as sess:

    # input image tensor
    image_tensor1 = self.detection_graph.get_tensor_by_name('import/image_tensor:0')

    # boxes, scores and classes for first step
    boxesop1 = self.detection_graph.get_tensor_by_name('import/detection_boxes:0')
    scoresop1 = self.detection_graph.get_tensor_by_name('import/detection_scores:0')
    classesop1 = self.detection_graph.get_tensor_by_name('import/detection_classes:0')

    # getting first values, since we are predicting on single image
    boxesop1 = boxesop1[0]
    classesop1 = classesop1[0]
    scoresop1 = scoresop1[0]

    # applying NMS for the first step
    selected_indices1 = tf.image.non_max_suppression(
        boxesop1, scoresop1, 20, iou_threshold = 0.5
    )

    boxesop1 = tf.gather(boxesop1, selected_indices1)
    classesop1 = tf.gather(classesop1, selected_indices1)
    scoresop1 = tf.gather(scoresop1, selected_indices1)


    # boxes, scores and classes for second step
    boxesop2 = self.detection_graph.get_tensor_by_name('import_1/detection_boxes:0')
    scoresop2 = self.detection_graph.get_tensor_by_name('import_1/detection_scores:0')
    classesop2 = self.detection_graph.get_tensor_by_name('import_1/detection_classes:0')

    # applying NMS for the second step
    boxesop2, scoresop2, classesop2, valid_detections = tf.image.combined_non_max_suppression(
        boxesop2, scoresop2, max_output_size_per_class = 10, max_total_size = 30,
        iou_threshold = 0.5
    )

    # predicting for each images
    for imgPath, imgID in img_files:

        # reading image data
        img = cv2.imread(imgPath)
        imageHeight, imageWidth = img.shape[:2]

        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
        image_np_expanded = np.expand_dims(img, axis=0)

        # Run inference
        (boxes1, scores1, classes1, boxes2, scores2, classes2) = sess.run(
            [boxesop1, scoresop1, classesop1, boxesop2, scoresop2, classesop2],
            feed_dict={image_tensor1: image_np_expanded}
        )
</code></pre>

<p>But I got following error, when tried running above:</p>

<pre><code>Traceback (most recent call last):
  File ""../env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InvalidArgumentError: boxes must be 4-D[20,300,4]
         [[{{node combined_non_max_suppression/CombinedNonMaxSuppression}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/prediction.py"", line 159, in predict
    feed_dict={image_tensor1: image_np_expanded}
  File ""../env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1173, in _run
    feed_dict_tensor, options, run_metadata)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
    run_metadata)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: boxes must be 4-D[20,300,4]
         [[node combined_non_max_suppression/CombinedNonMaxSuppression (defined at /home/prediction.py:130) ]]

Errors may have originated from an input operation.
Input Source operations connected to node combined_non_max_suppression/CombinedNonMaxSuppression:
 import_1/detection_boxes (defined at /home/prediction.py:94)

Original stack trace for 'combined_non_max_suppression/CombinedNonMaxSuppression':
  File ""/home/prediction.py"", line 130, in predict
    iou_threshold = 0.5
  File ""../env/lib/python3.5/site-packages/tensorflow/python/ops/image_ops_impl.py"", line 3707, in combined_non_max_suppression
    score_threshold, pad_per_class, clip_boxes)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/ops/gen_image_ops.py"", line 431, in combined_non_max_suppression
    clip_boxes=clip_boxes, name=name)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3616, in create_op
    op_def=op_def)
  File ""../env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2005, in __init__
    self._traceback = tf_stack.extract_stack()
</code></pre>

<p><strong>How to solve it and apply NMS for batch of images in tensorflow ?</strong></p>
","I have batch of cropped images from original image on which I have to perform object detection, I am trying to apply tensorflow NMS operation. I looked into tensorflow api docs, and found tf.image.combined_non_max_suppression(), but I am unable to understand it properly. The flow in my pipeline is of two step. For the first step, I use simple tf.image.non_max_suppression() followed by tf.gather(), but I am not able to understand, how to do it for second step. Please refer to code snippets below: But I got following error, when tried running above: How to solve it and apply NMS for batch of images in tensorflow ?",https://stackoverflow.com/questions/60525257,7112271,Documentation Replicability
60590333,Increasing each element of a tensor by the predecessor in Tensorflow 2.0,"<p>I'm new to <em>tensorflow 2.0</em>, and haven't done much except designing and training some artificial neural networks from boilerplate code. I'm trying to solve an <em>exercise for newcomers</em> into the new tensorflow. I created some code, but it doesn't work. Below is the <strong><em>problem definition</em></strong>:</p>

<hr>

<p>Assuming we have tensor <code>M</code> of rational numbers in shape of <code>(a, b, c)</code> and scalar <code>p ∈ (0, 1)</code> (memory factor), let’s create a function that will return tensor <code>N</code> in shape of <code>(a, b, c)</code>. Each element of <code>N</code> tensors moving along axis <em>c</em> should be increased by the value of predecessor multiplied by <code>p</code>.</p>

<p>Assuming we have tensor:</p>

<pre><code>T = [x1, x2, x3, x4]
</code></pre>

<p>in shape of <code>(1, 1, 4)</code>, we would like to get vector:</p>

<pre><code>[x1, x2+x1·p, x3+(x2+x1·p)·p, x4+(x3+(x2+x1·p)·p)*p] 
</code></pre>

<p>Solution should be created in <em>Tensorflow 2.0</em> and should be focused on delivering the shortest execution time on CPU. Created graph should allow to efficiently calculate derivative both on tensor <code>M</code> and value <code>p</code>.</p>

<hr>

<p>This is the <strong>code I created till now</strong>:</p>

<pre><code>import tensorflow as tf

@tf.function
def vectorize_predec(t, p):
    last_elem = 0
    result = []
    for el in t:
        result.append(el + (p * last_elem))
        last_elem = el + (p * last_elem)
    return result

p = tf.Variable(0.5, dtype='double')

m = tf.constant([[0, 1, 2, 3, 4],
          [1, 3, 5, 7, 10],
          [1, 1, 1, -1, 0]])

vectorize_predec(m, p)
</code></pre>

<p>But it throws a <code>TypeError</code>.</p>

<p>I looked around documentation, I've seen functions like <code>cumsum</code> and <code>polyeval</code>, but I'm not sure they fit my needs. To my understanding, I need to write my own customer function annotated with <code>@tf.function</code>. I'm also not sure how to handle 3-dimension tensors properly according to the problem definition (adding the predecessor should happen on the last (<em>""c""</em>) axis). </p>

<p>I've seen in documentation (here: <a href=""https://www.tensorflow.org/tutorials/customization/performance"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/customization/performance</a>) that there are ways to measure size of the produced graph. Although, I'm not sure how ""graph"" allows to efficiently calculate <em>derivative</em> both on tensor <code>M</code> and value <code>p</code>. ELI5 answers appreciated, or at least some materials I can read to educate myself better.</p>

<p>Thanks a lot! </p>
","I'm new to tensorflow 2.0, and haven't done much except designing and training some artificial neural networks from boilerplate code. I'm trying to solve an exercise for newcomers into the new tensorflow. I created some code, but it doesn't work. Below is the problem definition: Assuming we have tensor M of rational numbers in shape of (a, b, c) and scalar p ∈ (0, 1) (memory factor), let’s create a function that will return tensor N in shape of (a, b, c). Each element of N tensors moving along axis c should be increased by the value of predecessor multiplied by p. Assuming we have tensor: in shape of (1, 1, 4), we would like to get vector: Solution should be created in Tensorflow 2.0 and should be focused on delivering the shortest execution time on CPU. Created graph should allow to efficiently calculate derivative both on tensor M and value p. This is the code I created till now: But it throws a TypeError. I looked around documentation, I've seen functions like cumsum and polyeval, but I'm not sure they fit my needs. To my understanding, I need to write my own customer function annotated with @tf.function. I'm also not sure how to handle 3-dimension tensors properly according to the problem definition (adding the predecessor should happen on the last (""c"") axis). I've seen in documentation (here: https://www.tensorflow.org/tutorials/customization/performance) that there are ways to measure size of the produced graph. Although, I'm not sure how ""graph"" allows to efficiently calculate derivative both on tensor M and value p. ELI5 answers appreciated, or at least some materials I can read to educate myself better. Thanks a lot!",https://stackoverflow.com/questions/60590333,1554153,Lack of Alternative Solutions/Documentation
60639731,Tensorboard for custom training loop in Tensorflow 2,"<p>I want to create a custom training loop in tensorflow 2 and use tensorboard for visualization. Here is an example I've created based on tensorflow documentation:</p>

<pre><code>import tensorflow as tf
import datetime

os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""    # which gpu to use

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))

train_dataset = train_dataset.shuffle(60000).batch(64)
test_dataset = test_dataset.batch(64)


def create_model():
    return tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28), name='Flatten_1'),
        tf.keras.layers.Dense(512, activation='relu', name='Dense_1'),
        tf.keras.layers.Dropout(0.2, name='Dropout_1'),
        tf.keras.layers.Dense(10, activation='softmax', name='Dense_2')
    ], name='Network')


# Loss and optimizer
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

# Define our metrics
train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')
test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')

@tf.function
def train_step(model, optimizer, x_train, y_train):
    with tf.GradientTape() as tape:
        predictions = model(x_train, training=True)
        loss = loss_object(y_train, predictions)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))

    train_loss(loss)
    train_accuracy(y_train, predictions)

@tf.function
def test_step(model, x_test, y_test):
    predictions = model(x_test)
    loss = loss_object(y_test, predictions)

    test_loss(loss)
    test_accuracy(y_test, predictions)


current_time = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
train_log_dir = '/NAS/Dataset/logs/gradient_tape/' + current_time + '/train'
test_log_dir = '/NAS/Dataset/logs/gradient_tape/' + current_time + '/test'
train_summary_writer = tf.summary.create_file_writer(train_log_dir)
test_summary_writer = tf.summary.create_file_writer(test_log_dir)

model = create_model()  # reset our model

EPOCHS = 5


for epoch in range(EPOCHS):
    for (x_train, y_train) in train_dataset:
        train_step(model, optimizer, x_train, y_train)
    with train_summary_writer.as_default():
        tf.summary.scalar('loss', train_loss.result(), step=epoch)
        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)

    for (x_test, y_test) in test_dataset:
        test_step(model, x_test, y_test)
    with test_summary_writer.as_default():
        tf.summary.scalar('loss', test_loss.result(), step=epoch)
        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)

    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
    print(template.format(epoch + 1,
                          train_loss.result(),
                          train_accuracy.result() * 100,
                          test_loss.result(),
                          test_accuracy.result() * 100))

    # Reset metrics every epoch
    train_loss.reset_states()
    test_loss.reset_states()
    train_accuracy.reset_states()
    test_accuracy.reset_states()
</code></pre>

<p>I am accessing tensorboard with the following command on terminal:</p>

<pre><code>tensorboard --logdir=.....
</code></pre>

<p>The code above produce summaries for losses and metrics. My question is:</p>

<ul>
<li><strong>How can i produce the graph of this process?</strong></li>
</ul>

<p>I've tried to use the recommended commands from tensorflow: <strong>tf.summary.trace_on()</strong> and <strong>tf.summary.trace_export()</strong>, but I haven't managed to plot the graph. Maybe I am using them wrong. I whould really appreciate any suggestion on how to do this.</p>
","I want to create a custom training loop in tensorflow 2 and use tensorboard for visualization. Here is an example I've created based on tensorflow documentation: I am accessing tensorboard with the following command on terminal: The code above produce summaries for losses and metrics. My question is: I've tried to use the recommended commands from tensorflow: tf.summary.trace_on() and tf.summary.trace_export(), but I haven't managed to plot the graph. Maybe I am using them wrong. I whould really appreciate any suggestion on how to do this.",https://stackoverflow.com/questions/60639731,10687511,Documentation Replication on Other Examples
60708695,"How can I make ""element wise"" comparsion inside of the tf.function?","<p>I try to make my own activation function in TensorFlow 2 and the function looks like this:</p>

<pre><code>@tf.function
def f(x):
  r = 2
  if x&gt;=0:
    return (r**2 * x + 1)**(1/r) - 1/r
  else:
    return K.exp(r*x) - 1/r
</code></pre>

<p>The problem is that it cant take as argument <code>tf.constant([2.0, 3.0])</code>because there is an issue with conditions. I have tried <code>tf.math.qreater_equal(x, 0)</code> which lead to same output also <code>tf.cond()</code>. I have had no luck with documentation examples either.
It returns error:</p>

<pre><code>InvalidArgumentError:  The second input must be a scalar, but it has shape [2]
     [[{{node cond/switch_pred/_2}}]] [Op:__inference_f_7469065]
</code></pre>

<p>Thanks!</p>
","I try to make my own activation function in TensorFlow 2 and the function looks like this: The problem is that it cant take as argument tf.constant([2.0, 3.0])because there is an issue with conditions. I have tried tf.math.qreater_equal(x, 0) which lead to same output also tf.cond(). I have had no luck with documentation examples either. It returns error: Thanks!",https://stackoverflow.com/questions/60708695,10962934,Documentation Ambiguity
60782077,How do you use tensorflow ctc_batch_cost function with keras?,"<p>I have been trying to implement a CTC loss function in keras for several days now.</p>
<p>Unfortunately, I have yet to find a simple way to do this that fits well with keras. I found tensorflow's <code>tf.keras.backend.ctc_batch_cost</code> function but there is not much documentation on it. I am confused about a few things. First, what are the <code>input_length</code> and <code>label_length</code> parameters? I am trying to make a handwriting recognition model and my images are 32x128, my RNN has 32 time steps, and my character list has a length of 80. I have tried to use 32 for both parameters and this gives me the error below.</p>
<p>Shouldn't the function already know the <code>input_length</code> and <code>label_length</code> from the shape of the first two parameters (<code>y_true</code> and <code>y_pred</code>)?</p>
<p>Secondly, do I need to encode my training data? Is this all done automatically?</p>
<p>I know tensorflow also has a function called <code>tf.keras.backend.ctc_decode</code>. Is this only used when making predictions?</p>
<pre><code>def ctc_cost(y_true, y_pred):
    return tf.keras.backend.ctc_batch_cost(
        y_true, y_pred, 32, 32)


model = tf.keras.Sequential([
    layers.Conv2D(32, 5, padding=&quot;SAME&quot;, input_shape=(32, 128, 1)),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D(2, 2),
    layers.Conv2D(64, 5, padding=&quot;SAME&quot;),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D(2, 2),
    layers.Conv2D(128, 3, padding=&quot;SAME&quot;),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D((1, 2), (1, 2)),
    layers.Conv2D(128, 3, padding=&quot;SAME&quot;),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D((1, 2), (1, 2)),
    layers.Conv2D(256, 3, padding=&quot;SAME&quot;),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D((1, 2), (1, 2)),
    layers.Reshape((32, 256)),
    layers.Bidirectional(layers.LSTM(256, return_sequences=True)),
    layers.Bidirectional(layers.LSTM(256, return_sequences=True)),
    layers.Reshape((-1, 32, 512)),
    layers.Conv2D(80, 1, padding=&quot;SAME&quot;),
    layers.Softmax(-1)
])

print(model.summary())

model.compile(tf.optimizers.RMSprop(0.001), ctc_cost)
</code></pre>
<p><strong>Error:</strong></p>
<p><em>tensorflow.python.framework.errors_impl.InvalidArgumentError: squeeze_dims[0] not in [0,0). for 'loss/softmax_loss/Squeeze' (op: 'Squeeze') with input shapes: []</em></p>
<p><strong>Model:</strong></p>
<pre><code>Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 32, 128, 32)       832
batch_normalization (BatchNo (None, 32, 128, 32)       128
activation (Activation)      (None, 32, 128, 32)       0
max_pooling2d (MaxPooling2D) (None, 16, 64, 32)        0
conv2d_1 (Conv2D)            (None, 16, 64, 64)        51264
batch_normalization_1 (Batch (None, 16, 64, 64)        256
activation_1 (Activation)    (None, 16, 64, 64)        0
max_pooling2d_1 (MaxPooling2 (None, 8, 32, 64)         0
conv2d_2 (Conv2D)            (None, 8, 32, 128)        73856
batch_normalization_2 (Batch (None, 8, 32, 128)        512
activation_2 (Activation)    (None, 8, 32, 128)        0
max_pooling2d_2 (MaxPooling2 (None, 8, 16, 128)        0
conv2d_3 (Conv2D)            (None, 8, 16, 128)        147584
batch_normalization_3 (Batch (None, 8, 16, 128)        512
activation_3 (Activation)    (None, 8, 16, 128)        0
max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0
conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168
batch_normalization_4 (Batch (None, 8, 8, 256)         1024
activation_4 (Activation)    (None, 8, 8, 256)         0
max_pooling2d_4 (MaxPooling2 (None, 8, 4, 256)         0
reshape (Reshape)            (None, 32, 256)           0
bidirectional (Bidirectional (None, 32, 512)           1050624
bidirectional_1 (Bidirection (None, 32, 512)           1574912
reshape_1 (Reshape)          (None, None, 32, 512)     0
conv2d_5 (Conv2D)            (None, None, 32, 80)      41040     
softmax (Softmax)            (None, None, 32, 80)      0
</code></pre>
<p><strong>Here is the tensorflow documentation I was referencing:</strong></p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_batch_cost"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_batch_cost</a></p>
","I have been trying to implement a CTC loss function in keras for several days now. Unfortunately, I have yet to find a simple way to do this that fits well with keras. I found tensorflow's tf.keras.backend.ctc_batch_cost function but there is not much documentation on it. I am confused about a few things. First, what are the input_length and label_length parameters? I am trying to make a handwriting recognition model and my images are 32x128, my RNN has 32 time steps, and my character list has a length of 80. I have tried to use 32 for both parameters and this gives me the error below. Shouldn't the function already know the input_length and label_length from the shape of the first two parameters (y_true and y_pred)? Secondly, do I need to encode my training data? Is this all done automatically? I know tensorflow also has a function called tf.keras.backend.ctc_decode. Is this only used when making predictions? Error: tensorflow.python.framework.errors_impl.InvalidArgumentError: squeeze_dims[0] not in [0,0). for 'loss/softmax_loss/Squeeze' (op: 'Squeeze') with input shapes: [] Model: Here is the tensorflow documentation I was referencing: https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_batch_cost",https://stackoverflow.com/questions/60782077,11900339,Documentation Ambiguity
60801403,NotFoundError: No registered 'PyFunc' OpKernel for 'CPU' devices compatible with node {{node PyFunc}} . Registered: <no registered kernels>,"<p>I am getting an error while trying to access data from a tf.data.Dataset object.
The dataset object is built from a generator. Any help will be appreciated.
I'm using TensorFlow 2 and trying to run the example from <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator</a></p>

<pre><code>def gen(): 
  for i in itertools.count(1): 
    yield (i, [1] * i) 

dataset = tf.data.Dataset.from_generator( 
     gen, 
     (tf.int64, tf.int64), 
     (tf.TensorShape([]), tf.TensorShape([None]))) 

list(dataset.take(3).as_numpy_iterator()) 
</code></pre>

<p>The  error is : </p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)
    662         # Fast path for the case `self._structure` is not a nested structure.
--&gt; 663         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    664       except AttributeError:

AttributeError: 'tuple' object has no attribute '_from_compatible_tensor_list'

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py in execution_mode(mode)
   1896     ctx.executor = executor_new
-&gt; 1897     yield
   1898   finally:



...  
NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-25-ac0e933e02b3&gt; in &lt;module&gt;
----&gt; 1 list(dataset.take(3).as_numpy_iterator())

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in __next__(self)
   3640 
   3641   def __next__(self):
-&gt; 3642     return self.next()
   3643 
   3644 

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in next(self)
   3637 
   3638   def next(self):
-&gt; 3639     return nest.map_structure(lambda x: x.numpy(), next(self._iterator))
   3640 
   3641   def __next__(self):

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in __next__(self)
    628 
    629   def __next__(self):  # For Python 3 compatibility
--&gt; 630     return self.next()
    631 
    632   def _next_internal(self):

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)
    672     """"""Returns a nested structure of `Tensor`s containing the next element.""""""
    673     try:
--&gt; 674       return self._next_internal()
    675     except errors.OutOfRangeError:
    676       raise StopIteration

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)
    663         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    664       except AttributeError:
--&gt; 665         return structure.from_compatible_tensor_list(self._element_spec, ret)
    666 
    667   @property

/opt/conda/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)
     97                 value = type()
     98             try:
---&gt; 99                 self.gen.throw(type, value, traceback)
    100             except StopIteration as exc:
    101                 # Suppress StopIteration *unless* it's the same exception that

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py in execution_mode(mode)
   1898   finally:
   1899     ctx.executor = executor_old
-&gt; 1900     executor_new.wait()
   1901 
   1902 

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/executor.py in wait(self)
     65   def wait(self):
     66     """"""Waits for ops dispatched in this executor to finish.""""""
---&gt; 67     pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     68 
     69   def clear_error(self):

NotFoundError: No registered 'PyFunc' OpKernel for 'CPU' devices compatible with node {{node PyFunc}}
    .  Registered:  &lt;no registered kernels&gt;
</code></pre>
",I am getting an error while trying to access data from a tf.data.Dataset object. The dataset object is built from a generator. Any help will be appreciated. I'm using TensorFlow 2 and trying to run the example from https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator The error is :,https://stackoverflow.com/questions/60801403,7849569,Documentation Replicability
61059725,Why does tf.constant give a dtype error if we pass in a tensor?,"<p>The following code</p>

<pre><code>a = tf.range(10)
b = tf.constant(a, dtype=tf.float32)
</code></pre>

<p>gives the following error:</p>

<pre><code>TypeError: Expected tensor with type tf.float32 not tf.int32
</code></pre>

<p>Although from the <a href=""https://www.tensorflow.org/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">documentation</a>, setting <code>dtype</code> means that <code>tf.constant</code> is supposed to cast <code>a</code> to the specified data type. So I don't see why this should give a type error.</p>

<p>I also know that:</p>

<pre><code>a = np.arange(10)
b = tf.constant(a, dtype=tf.float32)
</code></pre>

<p>does not give an error.</p>

<p>So actually, I'm mainly wondering about what's happening under the hood here.</p>
","The following code gives the following error: Although from the documentation, setting dtype means that tf.constant is supposed to cast a to the specified data type. So I don't see why this should give a type error. I also know that: does not give an error. So actually, I'm mainly wondering about what's happening under the hood here.",https://stackoverflow.com/questions/61059725,4391249,Documentation Ambiguity
61136605,Why tf2 can't save a tf_function model as .pb file?,"<p>I tried to saved a model like the official code of transformer on <a href=""https://www.tensorflow.org/tutorials/text/transformer?hl=zh-cn"" rel=""nofollow noreferrer"">official website</a>, but when i want to save the train_step graph or trace on it with tf.summary.trace_on ,it errors.the error is as </p>

<pre><code>    ---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in convert(x)
    936         try:
--&gt; 937           x = ops.convert_to_tensor_or_composite(x)
    938         except (ValueError, TypeError):

15 frames
TypeError: Can't convert Operation 'PartitionedFunctionCall' to Tensor (target dtype=None, name=None, as_ref=False)

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in convert(x)
    941               ""must return zero or more Tensors; in compilation of %s, found ""
    942               ""return value of type %s, which is not a Tensor."" %
--&gt; 943               (str(python_func), type(x)))
    944       if add_control_dependencies:
    945         x = deps_ctx.mark_as_return(x)

TypeError: To be compatible with tf.contrib.eager.defun, Python functions must return zero or more Tensors; in compilation of &lt;function canonicalize_signatures.&lt;locals&gt;.signature_wrapper at 0x7fcf794b47b8&gt;, found return value of type &lt;class 'tensorflow.python.framework.ops.Operation'&gt;, which is not a Tensor.
</code></pre>

<p>I supposed it was some error on tensor operation and write an other demo to confirm my idea:</p>

<pre><code>    import tensorflow as tf
sig=[tf.TensorSpec(shape=(None, None), dtype=tf.int64),tf.TensorSpec(shape=(None, None), dtype=tf.int64)]
@tf.function(input_signature=sig)
def cal(a,d):
  b=a[1:]
root=tf.Module()
root.func = cal

# 获取具体函数。
concrete_func = root.func.get_concrete_function(
      tf.TensorSpec(shape=(None, None), dtype=tf.int64),tf.TensorSpec(shape=(None, None), dtype=tf.int64)
)
tf.saved_model.save(root, '/correct', concrete_func)
</code></pre>

<p>the error occurs as supposed. But how can i fixed it? the positional_encoding requires and i have no idea about how to replace this operation.</p>
","I tried to saved a model like the official code of transformer on official website, but when i want to save the train_step graph or trace on it with tf.summary.trace_on ,it errors.the error is as I supposed it was some error on tensor operation and write an other demo to confirm my idea: the error occurs as supposed. But how can i fixed it? the positional_encoding requires and i have no idea about how to replace this operation.",https://stackoverflow.com/questions/61136605,10796214,Documentation Replicability
61175291,Why is optimizer.minimize not working if we pass loss as tf.constant?,"<p>I simply have <code>train = optimizer.minimize(loss = tf.constant(4,dtype=""float32""))</code> Line of code that i change before everything is working. <br/></p>

<p>Why it is giving error ? Because documentation say it can be tensor <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#minimize"" rel=""nofollow noreferrer"">Here is Docs</a> </p>

<pre><code>W = tf.Variable([0.5],tf.float32)
b = tf.Variable([0.1],tf.float32)
x = tf.placeholder(tf.float32)
y= tf.placeholder(tf.float32)
discounted_reward = tf.placeholder(tf.float32,shape=[4,], name=""discounted_reward"")
linear_model = W*x + b

squared_delta = tf.square(linear_model - y)
print(squared_delta)
loss = tf.reduce_sum(squared_delta*discounted_reward)
print(loss)
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss = tf.constant(4,dtype=""float32""))
init = tf.global_variables_initializer()
sess = tf.Session()

sess.run(init)

for i in range(3):
    sess.run(train,{x:[1,2,3,4],y:[0,-1,-2,-3],discounted_reward:[1,2,3,4]})

print(sess.run([W,b]))
</code></pre>

<hr>

<p>I really need this thing to work. In this particular example we can have other ways to solve it but i need it to work as my actual code can do this only </p>

<p><hr/> Error is</p>

<pre><code>&gt; ValueError: No gradients provided for any variable, check your graph
&gt; for ops that do not support gradients, between variables
&gt; [""&lt;tf.Variable 'Variable:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_2:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_3:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_4:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_5:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_6:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_7:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_8:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_9:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_10:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_11:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_12:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_13:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_14:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_15:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_16:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_17:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_18:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_19:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_20:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_21:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_22:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_23:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_24:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_25:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_26:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_27:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_28:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_29:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_30:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_31:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_32:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_33:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_34:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_35:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_36:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_37:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_38:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_39:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_40:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_41:0' shape=(1,) dtype=float32_ref&gt;""] and loss
&gt; Tensor(""Const_4:0"", shape=(), dtype=float32).
</code></pre>
","I simply have train = optimizer.minimize(loss = tf.constant(4,dtype=""float32"")) Line of code that i change before everything is working. Why it is giving error ? Because documentation say it can be tensor Here is Docs I really need this thing to work. In this particular example we can have other ways to solve it but i need it to work as my actual code can do this only Error is",https://stackoverflow.com/questions/61175291,6543342,Documentation Replication on Other Examples
61305781,Using Tensorflow embedded columns raises All feature_columns must be _FeatureColumn instances error,"<p>I am new to tensorflow and I was trying to follow the official documentation where I came across 
tf.feature_column.categorical_column_with_vocabulary_list</p>

<p>The code I tested is: </p>

<pre><code>key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), default_value=0)
columns = [[tfc.embedding_column(colors, 3)], ...]
features = tf.io.parse_example(..., features=tfc.make_parse_example_spec(columns))
dense_tensor = tfc.input_layer(features, columns)
</code></pre>

<p>However , when I run this sample code I get the following error : 
 ValueError: All feature_columns must be _FeatureColumn instances. Given: [EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), dtype=tf.string, default_value=0, num_oov_buckets=0), dimension=3, combiner='mean', initializer=, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)]</p>

<p>What I am doing wrong?  </p>
","I am new to tensorflow and I was trying to follow the official documentation where I came across tf.feature_column.categorical_column_with_vocabulary_list The code I tested is: However , when I run this sample code I get the following error : ValueError: All feature_columns must be _FeatureColumn instances. Given: [EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), dtype=tf.string, default_value=0, num_oov_buckets=0), dimension=3, combiner='mean', initializer=, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)] What I am doing wrong?",https://stackoverflow.com/questions/61305781,11041539,Documentation Replication on Other Examples
61355289,When will tf.print ACTUALLY WORK as expected (i.e. print the values of tensors and variables)?,"<p>First of all, I am using TensorFlow 2.0 and I only care about this version or higher (and I am already caring too much for such a piece of software that only produces headaches).</p>

<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/print"" rel=""nofollow noreferrer"">TensorFlow documentation</a> of <code>tf.print</code> says </p>

<blockquote>
  <p>Print the specified inputs.</p>
</blockquote>

<p>and then</p>

<blockquote>
  <p>A TensorFlow operator that prints the specified inputs to a desired output stream or logging level. The inputs may be </p>
  
  <ul>
  <li><strong>dense</strong> or </li>
  <li><strong>sparse Tensors</strong>, </li>
  <li><strong>primitive python objects</strong>, </li>
  <li><strong>data structures that contain tensors</strong>, and </li>
  <li><strong>printable Python objects</strong>. </li>
  </ul>
  
  <p>Printed tensors will recursively show the first and last elements of each dimension to summarize. </p>
</blockquote>

<p>This is all very nice, but I still don't get where <code>tf.print</code> will ACTUALLY WORK (i.e. print the VALUES of variables and tensors) in my code. Of course, needless to say, I couldn't care less about the symbolic representations of tensors, variables or whatever. Whenever I try to use <code>tf.print</code>, I want to see the VALUES (real numbers, vectors or matrices). </p>

<p>I've tried to use <code>tf.print</code> in multiple cases and in multiple places, e.g. </p>

<ul>
<li><p>in a method that is called from the <code>__init__</code> method of a custom layer that is called during model building (so before compiling the model) in order to print the value of a tensor (at least, this is what the <code>type(my_var)</code> returns, i.e. it returns <code>&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;</code>), but nothing is printed. If I try to add <code>@tf.function</code> (I still don't get the usage of this function!), nothing changes. According to the documentation above <code>tf.print</code> is supposed to print tensors, my variable is a tensor and TensorFlow decides to ignore my call, and then one wonders why did I decide to use TF? Why? </p>

<p>Also, I am using TF 2.0 and, even if I don't use the decorator <code>@tf.function</code>, <code>print(tf.executing_eagerly())</code> prints False, which is really what I was expecting.</p></li>
<li><p>in a custom loss function, where a similar behaviour happens (i.e. sometimes something is printed, sometimes it is not, sometimes I try to add the decorator <code>@tf.function</code> to the custom loss function and see if something changes, but nothing changes, or maybe yes).</p></li>
</ul>

<p>Ok, so, as you can see, I have no idea where <code>tf.print</code> will do what I want, i.e. I want to see the values of tensors. If something is a tensor, it must have a value. Similarly for variables.</p>

<p>So, when will <code>tf.print</code> ACTUALLY PRINT THE VALUES OF TENSORS? </p>

<p>I am looking for answers that say e.g., ""<code>tf.print</code> will NEVER work"" or ""it will only work if you are dreaming"". Apart from the jokes and sarcasm, I am really looking for answers that tell me exactly in which places of my code or which stages of developing a model with TF <code>tf.print</code>  will actually do what it is supposed to do. Please, don't tell me that <code>tf.print</code> will work when the input is a tensor!! </p>
","First of all, I am using TensorFlow 2.0 and I only care about this version or higher (and I am already caring too much for such a piece of software that only produces headaches). The TensorFlow documentation of tf.print says and then This is all very nice, but I still don't get where tf.print will ACTUALLY WORK (i.e. print the VALUES of variables and tensors) in my code. Of course, needless to say, I couldn't care less about the symbolic representations of tensors, variables or whatever. Whenever I try to use tf.print, I want to see the VALUES (real numbers, vectors or matrices). I've tried to use tf.print in multiple cases and in multiple places, e.g. Ok, so, as you can see, I have no idea where tf.print will do what I want, i.e. I want to see the values of tensors. If something is a tensor, it must have a value. Similarly for variables. So, when will tf.print ACTUALLY PRINT THE VALUES OF TENSORS? I am looking for answers that say e.g., ""tf.print will NEVER work"" or ""it will only work if you are dreaming"". Apart from the jokes and sarcasm, I am really looking for answers that tell me exactly in which places of my code or which stages of developing a model with TF tf.print will actually do what it is supposed to do. Please, don't tell me that tf.print will work when the input is a tensor!!",https://stackoverflow.com/questions/61355289,3924118,Documentation Ambiguity
61355474,Why does tf.executing_eagerly() return False in TensorFlow 2?,"<p>Let me explain my set up. I am using TensorFlow 2.1, the Keras version shipped with TF, and TensorFlow Probability 0.9.</p>

<p>I have a function <code>get_model</code> that creates (with the functional API) and returns a model using Keras and custom layers. In the <code>__init__</code> method of these custom layers <code>A</code>, I call a method <code>A.m</code>, which executes the statement <code>print(tf.executing_eagerly())</code>, but it returns <code>False</code>. Why?</p>

<p>To be more precise, this is roughly my setup</p>

<pre><code>def get_model():
    inp = Input(...)
    x = A(...)(inp) 
    x = A(...)(x)
    ...
    model = Model(inp, out)
    model.compile(...)
    return model

class A(tfp.layers.DenseFlipout): # TensorFlow Probability
    def __init__(...):
        self.m()

    def m(self): 
        print(tf.executing_eagerly()) # Prints False
</code></pre>

<p>The documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/executing_eagerly"" rel=""nofollow noreferrer""><code>tf.executing_eagerly</code></a> says</p>

<blockquote>
  <p>Eager execution is enabled by default and this API returns True in most of cases. However, this API might return False in the following use cases.</p>
  
  <ul>
  <li>Executing inside <code>tf.function</code>, unless under <code>tf.init_scope</code> or <code>tf.config.experimental_run_functions_eagerly(True)</code> is previously called.</li>
  <li>Executing inside a transformation function for <code>tf.dataset</code>.</li>
  <li><code>tf.compat.v1.disable_eager_execution()</code> is called.</li>
  </ul>
</blockquote>

<p>But these cases are not my case, so <code>tf.executing_eagerly()</code> should return <code>True</code> in my case, but no. Why?</p>

<p>Here's a simple complete example (in TF 2.1) that illustrates the problem.</p>

<pre><code>import tensorflow as tf


class MyLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        tf.print(""tf.executing_eagerly() ="", tf.executing_eagerly())
        return inputs


def get_model():
    inp = tf.keras.layers.Input(shape=(1,))
    out = MyLayer(8)(inp)
    model = tf.keras.Model(inputs=inp, outputs=out)
    model.summary()
    return model


def train():
    model = get_model()
    model.compile(optimizer=""adam"", loss=""mae"")
    x_train = [2, 3, 4, 1, 2, 6]
    y_train = [1, 0, 1, 0, 1, 1]
    model.fit(x_train, y_train)


if __name__ == '__main__':
    train()
</code></pre>

<p>This example prints <code>tf.executing_eagerly() = False</code>.</p>

<p>See <a href=""https://github.com/tensorflow/tensorflow/issues/38775"" rel=""nofollow noreferrer"">the related Github issue</a>.</p>
","Let me explain my set up. I am using TensorFlow 2.1, the Keras version shipped with TF, and TensorFlow Probability 0.9. I have a function get_model that creates (with the functional API) and returns a model using Keras and custom layers. In the __init__ method of these custom layers A, I call a method A.m, which executes the statement print(tf.executing_eagerly()), but it returns False. Why? To be more precise, this is roughly my setup The documentation of tf.executing_eagerly says But these cases are not my case, so tf.executing_eagerly() should return True in my case, but no. Why? Here's a simple complete example (in TF 2.1) that illustrates the problem. This example prints tf.executing_eagerly() = False. See the related Github issue.",https://stackoverflow.com/questions/61355474,3924118,Documentation Replicability
61428918,tensorflow2: keras: model.fit() callbacks and eager mode,"<p>I am running Tensorflow 2.1 with keras API. I am following the following coding style:</p>

<pre><code>    model = tf.keras.Sequential()
    ...
    model.fit(..., callbacks=callbacks)
</code></pre>

<p>Now, I would like to save some intermediate layer tensor value as image summary (as a sample what is happening at n-th training step). In order to do this, I've implemented my own callback class. I've also learned how <code>keras.callbacks.TensorBoard</code> is implemented, since it can save layer weights as image summaries.
I do the following in my <code>on_epoch_end</code>:</p>

<pre><code>tensor = self.model.get_layer(layer_name).output

with context.eager_mode():
    with ops.init_scope():
        tensor = tf.keras.backend.get_value(tensor)
    tf.summary.image(layer_name, tensor, step=step, max_outputs=1)
</code></pre>

<p>Unfortunately, I am still getting issue related to eager/graph modes:</p>

<pre><code>    tensor = tf.keras.backend.get_value(tensor)
  File ""/home/matwey/lab/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 3241, in get_value
    return x.numpy()
AttributeError: 'Tensor' object has no attribute 'numpy'
</code></pre>

<p>Unfortunately, there is a little to no documentation on how to correctly combine keras callbacks and <code>tf.summary.image</code>. How could I overcome this issue?</p>

<p><strong>upd:</strong> tf_nightly-2.2.0.dev20200427 has the same behaviour.</p>
","I am running Tensorflow 2.1 with keras API. I am following the following coding style: Now, I would like to save some intermediate layer tensor value as image summary (as a sample what is happening at n-th training step). In order to do this, I've implemented my own callback class. I've also learned how keras.callbacks.TensorBoard is implemented, since it can save layer weights as image summaries. I do the following in my on_epoch_end: Unfortunately, I am still getting issue related to eager/graph modes: Unfortunately, there is a little to no documentation on how to correctly combine keras callbacks and tf.summary.image. How could I overcome this issue? upd: tf_nightly-2.2.0.dev20200427 has the same behaviour.",https://stackoverflow.com/questions/61428918,1879547,Lack of Alternative Solutions/Documentation
61522019,Is it still necessary to implement `compute_output_shape()` when defining a custom tf.keras Layer?,"<p>I have implemented a custom <code>Layer</code> in <code>tf.keras</code>, using TensorFlow 2.1.0.</p>

<p>In the past, when using the stand-alone Keras, it was important to define the <code>compute_output_shape(input_shape)</code> method in any custom layer so that the computational graph could be created. </p>

<p>Now, having moved to TF2, I found out that even if I remove that method from my custom implementation the layer still works as expected. Apparently, it works both in eager and graph mode.
This is an example of what I mean: </p>

<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.layers import Layer, Input
from tensorflow.keras.models import Sequential
import numpy as np


class MyLayer(Layer):
    def call(self, inputs):
        return inputs[:, :-1]  # Do something that changes the shape


m = Sequential([MyLayer(), MyLayer()])
m.predict(np.ones((10, 3)))  # This would not have worked in the past
</code></pre>

<p>Is it safe to say that <code>compute_output_shape()</code> is not necessary anymore? Am I missing something important?</p>

<p>In the documentation there's no explicit mention of removing <code>compute_output_shape()</code>, although none of the examples implements it explicitly. </p>

<p>Thanks</p>
","I have implemented a custom Layer in tf.keras, using TensorFlow 2.1.0. In the past, when using the stand-alone Keras, it was important to define the compute_output_shape(input_shape) method in any custom layer so that the computational graph could be created. Now, having moved to TF2, I found out that even if I remove that method from my custom implementation the layer still works as expected. Apparently, it works both in eager and graph mode. This is an example of what I mean: Is it safe to say that compute_output_shape() is not necessary anymore? Am I missing something important? In the documentation there's no explicit mention of removing compute_output_shape(), although none of the examples implements it explicitly. Thanks",https://stackoverflow.com/questions/61522019,5499527,Inadequate Examples
61526556,Serializing a tensor and writing to tfrecord from within a graph,"<p>I would like to write tensorflow example records to a TFRecordWriter from inside an AutoGraph generated graph.</p>

<p>The documentation for tensorflow 2.0 states the following:</p>

<blockquote>
  <p>The simplest way to handle non-scalar features is to use tf.serialize_tensor to convert tensors to binary-strings. Strings are scalars in tensorflow.</p>
</blockquote>

<p>However, <code>tf.io.serialize_tensor</code> returns a tensor of byte-string. Creating an Example proto requires a bytes list, not a tensor. </p>

<p>How do I write a tf.train.Example to a tf record from inside a graph?</p>

<p>Code to reproduce:</p>

<pre><code>%tensorflow_version 2.x
import tensorflow as tf

@tf.function
def example_write():
  writer = tf.io.TFRecordWriter(""test.tfr"")
  x = tf.constant([[0, 1], [2, 3]])
  x = tf.io.serialize_tensor(x)
  feature = {
      ""data"": tf.train.Features(
        bytes_list=tf.train.BytesList(value=[x]))
  }
  ex = tf.train.Example(features=tf.train.Features(
      feature=feature))
  writer.write(ex.SerializeToString())

example_write()
</code></pre>

<p>and the error</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-6-df8a97eb17c9&gt; in &lt;module&gt;()
     12   writer.write(ex.SerializeToString())
     13 
---&gt; 14 example_write()

8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

TypeError: in user code:

    &lt;ipython-input-6-df8a97eb17c9&gt;:6 example_write  *
        feature = {

    TypeError: &lt;tf.Tensor 'SerializeTensor:0' shape=() dtype=string&gt; has type Tensor, but expected one of: bytes
</code></pre>
","I would like to write tensorflow example records to a TFRecordWriter from inside an AutoGraph generated graph. The documentation for tensorflow 2.0 states the following: However, tf.io.serialize_tensor returns a tensor of byte-string. Creating an Example proto requires a bytes list, not a tensor. How do I write a tf.train.Example to a tf record from inside a graph? Code to reproduce: and the error",https://stackoverflow.com/questions/61526556,1189782,Documentation Replication on Other Examples
61720708,How do you save a Tensorflow dataset to a file?,"<p>There are at least two more questions like this on SO but not a single one has been answered.</p>

<p>I have a dataset of the form:</p>

<pre><code>&lt;TensorSliceDataset shapes: ((512,), (512,), (512,), ()), types: (tf.int32, tf.int32, tf.int32, tf.int32)&gt;
</code></pre>

<p>and another of the form:</p>

<pre><code>&lt;BatchDataset shapes: ((None, 512), (None, 512), (None, 512), (None,)), types: (tf.int32, tf.int32, tf.int32, tf.int32)&gt;
</code></pre>

<p>I have looked and looked but I can't find the code to save these datasets to files that can be loaded later. The closest I got was <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter"" rel=""noreferrer"">this page in the TensorFlow docs</a>, which suggests serializing the tensors using <code>tf.io.serialize_tensor</code> and then writing them to a file using <code>tf.data.experimental.TFRecordWriter</code>.</p>

<p>However, when I tried this using the code:</p>

<pre><code>dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter('mydata.tfrecord')
writer.write(dataset)
</code></pre>

<p>I get an error on the first line:</p>

<blockquote>
  <p>TypeError: serialize_tensor() takes from 1 to 2 positional arguments but 4 were given</p>
</blockquote>

<p>How can I modify the above (or do something else) to accomplish my goal?</p>
","There are at least two more questions like this on SO but not a single one has been answered. I have a dataset of the form: and another of the form: I have looked and looked but I can't find the code to save these datasets to files that can be loaded later. The closest I got was this page in the TensorFlow docs, which suggests serializing the tensors using tf.io.serialize_tensor and then writing them to a file using tf.data.experimental.TFRecordWriter. However, when I tried this using the code: I get an error on the first line: How can I modify the above (or do something else) to accomplish my goal?",https://stackoverflow.com/questions/61720708,424306,Documentation Replication on Other Examples
61743921,can we build object detection model using Tensorflow or it is only possible with the help f tf.keras,"<p>Is there any way to build object detection model using Tensorflow without any help of tf.keras module?</p>

<p>From Tensorflow documentation I'm  not able to find any example which helps to create model without Keras.</p>
",Is there any way to build object detection model using Tensorflow without any help of tf.keras module? From Tensorflow documentation I'm not able to find any example which helps to create model without Keras.,https://stackoverflow.com/questions/61743921,1490940,Lack of Alternative Solutions/Documentation
61767803,Tensorflow 1.x to Tensorflow 2.1.0,"<p>I am trying to update code written in Tensorflow 1.x to code in Tensorflow 2.1.0. I have been converting codes using Tensorflow 2.1.0 documentation, and I had no problems until this code.</p>

<pre><code>loss = tf.losses.softmax_cross_entropy(one_hot_labels, logits)
</code></pre>

<p>Above code is Tensorflow 1.x version, and I think, according to Tensorflow 2.1.0 documentation, the properly updated code is </p>

<pre><code>loss = tf.nn.softmax_cross_entropy_with_logits(one_hot_labels, logits)
</code></pre>

<p>Then, when I run</p>

<pre><code>return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)
</code></pre>

<p>I get the following error.</p>

<pre><code>Loss must be scalar, given: Tensor(""softmax_cross_entropy_with_logits/Reshape_2:0"", shape=(512,), dtype=float32)**
</code></pre>

<p>So, I am guessing in Tensorflow 1.x version, the loss was passed as 'tensor' to tf.estimator.EstimatorSpec, but in Tensorflow 2.1.0, the loss has to be passed as <code>scalar</code> to <code>tf.estimator.EstimatorSpec</code>? Loss (the way it is defined here) in both Tensorflow 1.x and 2.1.0 is tensor if I remember it correctly.</p>

<p>So, does anyone know how to convert tensor to scalar (which I don't think will be sufficient nor efficient in building the CNN model) or better yet, how to solve this dilemma?</p>

<p>Or did I convert the original code the wrong way?</p>

<p>I would very much appreciate if compat.v1. is not used unless absolutely necessary (i.e. no other way to use the code in Tensorflow 2.1.0 than compat.v1.)</p>
","I am trying to update code written in Tensorflow 1.x to code in Tensorflow 2.1.0. I have been converting codes using Tensorflow 2.1.0 documentation, and I had no problems until this code. Above code is Tensorflow 1.x version, and I think, according to Tensorflow 2.1.0 documentation, the properly updated code is Then, when I run I get the following error. So, I am guessing in Tensorflow 1.x version, the loss was passed as 'tensor' to tf.estimator.EstimatorSpec, but in Tensorflow 2.1.0, the loss has to be passed as scalar to tf.estimator.EstimatorSpec? Loss (the way it is defined here) in both Tensorflow 1.x and 2.1.0 is tensor if I remember it correctly. So, does anyone know how to convert tensor to scalar (which I don't think will be sufficient nor efficient in building the CNN model) or better yet, how to solve this dilemma? Or did I convert the original code the wrong way? I would very much appreciate if compat.v1. is not used unless absolutely necessary (i.e. no other way to use the code in Tensorflow 2.1.0 than compat.v1.)",https://stackoverflow.com/questions/61767803,12997689,Documentation Replication on Other Examples
61790621,List of tensors and just tensors,"<p>I am updating codes from tensorflow 1.x to 2.1.0.</p>

<p>I changed tensorflow 1.x code</p>

<pre><code>labels = tf.cast(labels, tf.int64)
predict = tf.argmax(input=logits, axis=1)
tf.metrics.accuracy(labels=labels, predictions=predict)
</code></pre>

<p>to tensorflow 2.1.0 code.</p>

<pre><code>labels = tf.cast(labels, tf.int64)
predict = tf.argmax(input=logits, axis=1)
tf.keras.metrics.Accuracy.update_state(labels, predict) #updated code
</code></pre>

<p>But, when I run the updated code, I got the following error.</p>

<pre><code>TypeError: update_state() missing 1 required positional argument: 'y_pred'
</code></pre>

<p>So, I checked the tensorflow 2.1.0 document, and parameters for <code>tf.keras.metrics.Accuracy.update_state()</code> seem to be a list (in form of [ , , , ]). Then, I searched for a way to convert tensor to a list, which is</p>

<pre><code>labels = tf.make_tensor_proto(labels)
labels = tf.make_ndarray(labels)
</code></pre>

<p>After I run this code, it gives the following error.</p>

<pre><code>TypeError: List of Tensors when single Tensor expected
</code></pre>

<p>So, I tried to turn a list of Tensors into Tensors with</p>

<pre><code>labels = tf.stack(labels)
#or
labels = torch.stack(labels)
</code></pre>

<p><code>tf.stack()</code> did not work, as it gave the same initial TypeError saying 'y_pred' is missing at the updated code.</p>

<p><code>torch.stack()</code>, however, gave the following error.</p>

<pre><code>TypeError: stack() : argument 'tensors' (position 1) must be tuple of Tensors, not Tensor
</code></pre>

<p>So, I am guessing <code>torch.stack()</code> only accepts a tuple, <strong>NOT a list</strong>.
But, <code>tf.stack()</code> seems to accept a list, but it does not turn it into a Tensor?</p>

<p>Are my labels and predict even a list of Tensors in the first place? If so, why would tf.stack() not turn them into Tensors? How can I correctly convert labels and predict so that they can be passed into <code>tf.keras.metrics.Accuracy.update_state()</code>?</p>

<p>I would very much appreciate if not using <code>compat.v1.</code> unless absolutely necessary.</p>
","I am updating codes from tensorflow 1.x to 2.1.0. I changed tensorflow 1.x code to tensorflow 2.1.0 code. But, when I run the updated code, I got the following error. So, I checked the tensorflow 2.1.0 document, and parameters for tf.keras.metrics.Accuracy.update_state() seem to be a list (in form of [ , , , ]). Then, I searched for a way to convert tensor to a list, which is After I run this code, it gives the following error. So, I tried to turn a list of Tensors into Tensors with tf.stack() did not work, as it gave the same initial TypeError saying 'y_pred' is missing at the updated code. torch.stack(), however, gave the following error. So, I am guessing torch.stack() only accepts a tuple, NOT a list. But, tf.stack() seems to accept a list, but it does not turn it into a Tensor? Are my labels and predict even a list of Tensors in the first place? If so, why would tf.stack() not turn them into Tensors? How can I correctly convert labels and predict so that they can be passed into tf.keras.metrics.Accuracy.update_state()? I would very much appreciate if not using compat.v1. unless absolutely necessary.",https://stackoverflow.com/questions/61790621,12997689,Documentation Replicability
61829273,Error trying to feed a tf.keras model with a tf.data.Dataset instead of tensors,"<p>Why does the following tf2 tf.keras model 'work' when fitted with tensors but generates a ValueError when attempting to fit the same tensors in tf.data.Dataset.from_tensor_slices form?</p>

<p>EDIT: Put another way, having developed/fitted/tested etc the model below using numpy arrays. How do those same numpy arrays need to be reshaped(?) so that they can be used to create a dataset with tf.data.Dataset.from_tensor_slices that works with the model?</p>

<pre><code>embed = ""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1""
hub_layer = hub.KerasLayer(embed, output_shape=[20], input_shape=[], 
                           dtype=tf.string, trainable=True, name='hub_layer')

# from tf hub docs.  hub_layer takes a 1D tensor of strings.

input_tensor = tf.keras.Input(shape=(), name=""input_enquiry"", dtype=tf.string) # Note tf.string. Ref: https://github.com/tensorflow/hub/issues/483
hub_tensor = hub_layer(input_tensor)
x = tf.keras.layers.Dense(16, activation='relu')(hub_tensor)
main_output = tf.keras.layers.Dense(units=4, activation='softmax', name='main_output')(x)

model = tf.keras.models.Model(inputs=[input_tensor], outputs=[main_output])
model.compile(optimizer='adam', loss=tf.losses.CategoricalCrossentropy(),metrics='acc')

# Input and target
X = tf.constant([['The quick brown fox'], ['Hello World']])
y = tf.constant([[0,0,0,1], [0,0,1,0]])

# Works OK
model.fit(X, y) # fit on tensors

X_ds = tf.data.Dataset.from_tensor_slices(X)

# Works OK
model.predict(X_ds) # predict on dataset

y_ds = tf.data.Dataset.from_tensor_slices(y)

ds = tf.data.Dataset.zip((X_ds, y_ds))

# Fails with ValueError
model.fit(ds)
</code></pre>

<p>ValueError:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
 in 
     30 
     31 # Fails with ValueError
---&gt; 32 model.fit(ds)
     33 
     34 

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    846                 batch_size=batch_size):
    847               callbacks.on_train_batch_begin(step)
--&gt; 848               tmp_logs = train_function(iterator)
    849               # Catch OutOfRangeError for Datasets of unknown size.
    850               # This blocks until the batch has finished executing.

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    578         xla_context.Exit()
    579     else:
--&gt; 580       result = self._call(*args, **kwds)
    581 
    582     if tracing_count == self._get_tracing_count():

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    609       # In this case we have created variables on the first call, so we run the
    610       # defunned version which is guaranteed to never create variables.
--&gt; 611       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    612     elif self._stateful_fn is not None:
    613       # Release the lock early so that multiple threads can perform the call

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2417     """"""Calls a graph function specialized to the inputs.""""""
   2418     with self._lock:
-&gt; 2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
   2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2421 

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2772           and self.input_signature is None
   2773           and call_context_key in self._function_cache.missed):
-&gt; 2774         return self._define_function_with_shape_relaxation(args, kwargs)
   2775 
   2776       self._function_cache.missed.add(call_context_key)

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _define_function_with_shape_relaxation(self, args, kwargs)
   2704         relaxed_arg_shapes)
   2705     graph_function = self._create_graph_function(
-&gt; 2706         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)
   2707     self._function_cache.arg_relaxed[rank_only_cache_key] = graph_function
   2708 

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2665             arg_names=arg_names,
   2666             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2667             capture_by_value=self._capture_by_value),
   2668         self._function_attributes,
   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    440         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    442     weak_wrapped_fn = weakref.ref(wrapped_fn)
    443 

~/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in user code:

    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:143 __call__
        losses = self.call(y_true, y_pred)
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:246 call
        return self.fn(y_true, y_pred, **self._fn_kwargs)
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy
        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy
        target.shape.assert_is_compatible_with(output.shape)
    /home/karl/projects/email_analysis/email_venv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with
        raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))

    ValueError: Shapes (4, 1) and (1, 4) are incompatible
</code></pre>

<p>If instead of using "".from_tensor_slices"" we use "".from_tensors"" to create X_ds and y_ds then, after zipping, all works well.  However, the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">docs</a> give me the impression "".from_tensors"" is memory heavy and not desirable.  Also, I believe that the single element "".from_tensors"" dataset is simply providing the model with two 2D tensors whereas the from_tensor_slices version is a sequence of 1D elements. </p>
","Why does the following tf2 tf.keras model 'work' when fitted with tensors but generates a ValueError when attempting to fit the same tensors in tf.data.Dataset.from_tensor_slices form? EDIT: Put another way, having developed/fitted/tested etc the model below using numpy arrays. How do those same numpy arrays need to be reshaped(?) so that they can be used to create a dataset with tf.data.Dataset.from_tensor_slices that works with the model? ValueError: If instead of using "".from_tensor_slices"" we use "".from_tensors"" to create X_ds and y_ds then, after zipping, all works well. However, the docs give me the impression "".from_tensors"" is memory heavy and not desirable. Also, I believe that the single element "".from_tensors"" dataset is simply providing the model with two 2D tensors whereas the from_tensor_slices version is a sequence of 1D elements.",https://stackoverflow.com/questions/61829273,9763431,Documentation Replication on Other Examples
61884176,Understanding tf.name_scope,"<p>I am trying to understand tf.name_scope. The documentation mentions the following:</p>

<p>""This context manager pushes a name scope, which will make the name of all operations added within it have a prefix.</p>

<p>For example, to define a new Python op called my_op:</p>

<pre><code>def my_op(a, b, c, name=None):
  with tf.name_scope(""MyOp"") as scope:
    a = tf.convert_to_tensor(a, name=""a"")
    b = tf.convert_to_tensor(b, name=""b"")
    c = tf.convert_to_tensor(c, name=""c"")
    # Define some computation that uses `a`, `b`, and `c`.
    return foo_op(..., name=scope)
</code></pre>

<p>When executed, the Tensors a, b, c, will have names MyOp/a, MyOp/b, and MyOp/c.""</p>

<p>My understanding is that the with block does not introduce a new local scope in Python. Under normal situation, the tensor variable a will also refer to the local parameter a of function my_op.   How is the name prefixing with ""MyOp/"" implemented using Python context? In the source code link for tf.name_scope (<a href=""https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/framework/ops.py#L6423-L6442"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/framework/ops.py#L6423-L6442</a>) there is an invocation of </p>

<pre><code>ctx = context.context()
</code></pre>

<p>but I could not find the semantics of context.context(). Most context manager discussion talk about <strong>enter</strong> and <strong>exit</strong>, but no mention of variable renaming with some prefix. Is this some introspective mechanism in Python that allows the manipulation of Python variable scopes? Many thanks for any insights.</p>
","I am trying to understand tf.name_scope. The documentation mentions the following: ""This context manager pushes a name scope, which will make the name of all operations added within it have a prefix. For example, to define a new Python op called my_op: When executed, the Tensors a, b, c, will have names MyOp/a, MyOp/b, and MyOp/c."" My understanding is that the with block does not introduce a new local scope in Python. Under normal situation, the tensor variable a will also refer to the local parameter a of function my_op. How is the name prefixing with ""MyOp/"" implemented using Python context? In the source code link for tf.name_scope (https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/framework/ops.py#L6423-L6442) there is an invocation of but I could not find the semantics of context.context(). Most context manager discussion talk about enter and exit, but no mention of variable renaming with some prefix. Is this some introspective mechanism in Python that allows the manipulation of Python variable scopes? Many thanks for any insights.",https://stackoverflow.com/questions/61884176,13572195,Documentation Replicability
61885570,Reading a tfrecord: DecodeError: Error parsing message,"<p>I am using colab to run a <a href=""https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb"" rel=""nofollow noreferrer"">tutorial</a> on tensorflow ranking. It uses wget to fetch the tfrecord:</p>

<pre><code>!wget -O ""/tmp/train.tfrecords"" ""http://ciir.cs.umass.edu/downloads/Antique/tf-ranking/ELWC/train.tfrecords""
</code></pre>

<p>I am using this code to try to look at the structure of the tfrecord:</p>

<pre><code>for example in tf.compat.v1.python_io.tf_record_iterator(""/tmp/train.tfrecords""):
    print(tf.train.Example.FromString(example))
    break
</code></pre>

<p>And I am getting:</p>

<pre><code>DecodeError: Error parsing message
</code></pre>

<p>How to generally look at the structure of tfrecords instead?</p>

<p>A second question: Where to find documentation on classes like <code>tf.train.Example</code>? I just find this <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Example"" rel=""nofollow noreferrer"">empty page</a>.</p>
",I am using colab to run a tutorial on tensorflow ranking. It uses wget to fetch the tfrecord: I am using this code to try to look at the structure of the tfrecord: And I am getting: How to generally look at the structure of tfrecords instead? A second question: Where to find documentation on classes like tf.train.Example? I just find this empty page.,https://stackoverflow.com/questions/61885570,8183621,Requesting (Additional) Resources
61946509,tf.estimator input_fn and eager mode,"<p>I tried to use <code>numpy</code> inside <code>cnn_model.evaluate()</code>, but it gave <code>AttributeError: 'Tensor' object has no attribute 'numpy'</code>. I used <code>numpy</code> to calculate accuracy and mean squared error using <code>tf.keras.metrics.Accuracy()</code> and <code>tf.keras.metrics.MeanSquaredError()</code> inside <code>cnn_model.evaluate()</code></p>

<p>I googled it, and in tensorflow documentation, it said </p>

<p><strong>""Calling methods of Estimator will work while eager execution is enabled. However, the model_fn and input_fn is not executed eagerly, Estimator will switch to graph mode before calling all user-provided functions (incl. hooks), so their code has to be compatible with graph mode execution.""</strong>  </p>

<p>So, I was wondering how I can update the current tf 1.x code to tf 2.1.0 code, while also using above information.</p>

<p>My current code is:</p>

<pre><code>eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(
    x={""x"": np.array(train_inputs, dtype=np.float32)},
    y=np.array(train_labels, dtype=np.float32),
    #y=np.array(train_labels),
    batch_size=1,
    num_epochs=1,
    shuffle=False)

eval_results = CNN.evaluate(input_fn=eval_input_fn)
</code></pre>

<p>What I have tried so far is add <code>tf.compat.v1.enable_eager_execution()</code> to the 1) beginning of the code after all the imports, 2) next line right after importing tf, 3) line right before declaring eval_input_fn, 4) line right before calling eval_results, 5) inside CNN model definition. It all failed to turn on the eager mode.</p>

<p>One other option that I found was remove @tf.function decorator, but I have no idea what that means and how to pass input_fn if @tf.function is removed.</p>
","I tried to use numpy inside cnn_model.evaluate(), but it gave AttributeError: 'Tensor' object has no attribute 'numpy'. I used numpy to calculate accuracy and mean squared error using tf.keras.metrics.Accuracy() and tf.keras.metrics.MeanSquaredError() inside cnn_model.evaluate() I googled it, and in tensorflow documentation, it said ""Calling methods of Estimator will work while eager execution is enabled. However, the model_fn and input_fn is not executed eagerly, Estimator will switch to graph mode before calling all user-provided functions (incl. hooks), so their code has to be compatible with graph mode execution."" So, I was wondering how I can update the current tf 1.x code to tf 2.1.0 code, while also using above information. My current code is: What I have tried so far is add tf.compat.v1.enable_eager_execution() to the 1) beginning of the code after all the imports, 2) next line right after importing tf, 3) line right before declaring eval_input_fn, 4) line right before calling eval_results, 5) inside CNN model definition. It all failed to turn on the eager mode. One other option that I found was remove @tf.function decorator, but I have no idea what that means and how to pass input_fn if @tf.function is removed.",https://stackoverflow.com/questions/61946509,12997689,Documentation Replication on Other Examples
61988657,Why does tensorflow.rank always return shape with null value,"<p>Being a beginner to TensorFlow <strong>I couldn't get why does tensorflow.rank always return shape with null value?</strong></p>

<p><strong>This is what I am working on:</strong></p>

<pre><code>import tensorflow as tf
%tensorflow_version 2.x

list_2d = [[1,2,3,4],
             [5,6,7,8],
             [9,10,11,12]
]
tensor_2d = tf.Variable(list_2d)

print(tensor_2d.shape)
print(tf.rank(tensor_2d))
</code></pre>

<p><strong>and the output is</strong> </p>

<pre><code>(3, 4)
tf.Tensor(2, shape=(), dtype=int32)
</code></pre>

<p>So <strong>my question is what is this <code>shape=()</code> from <code>tf.rank</code> output</strong>?</p>

<p>I couldn't get much from here - <a href=""https://www.tensorflow.org/api_docs/python/tf/rank"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/rank</a></p>
",Being a beginner to TensorFlow I couldn't get why does tensorflow.rank always return shape with null value? This is what I am working on: and the output is So my question is what is this shape=() from tf.rank output? I couldn't get much from here - https://www.tensorflow.org/api_docs/python/tf/rank,https://stackoverflow.com/questions/61988657,5176364,Documentation Replicability
61994285,TensorFlow: Error using weighted_categorical_column,"<p>I work on a binary classification problem containing a field STREET. In a first step I used the Tokenization to the get a word list (frequency of how often one word appears in the different datasets). Then I used this information to create two columns in my Dataframe describing the word and how often it was used:</p>

<pre><code>def buildWeightList(indexes, tokenizer):
    weights = []
    for index in indexes:
        if index == 0:
            weights.append(0)
        else:
            weights.append(tokenizer.index_docs.get(index))
    return weights
</code></pre>

<pre><code>street_tokenized = ts.texts_to_sequences(data['STREETPRO'])
data['STREETPRO'] = tf.keras.preprocessing.sequence.pad_sequences(street_tokenized, maxlen=1)
data['STREETFREQ']  = buildWeightList(data['STREETPRO'], ts)
</code></pre>

<p>After I converted the Dataframe to a TensorFlow Dataset I have used the following code to add it to my future columns:</p>

<pre><code>vocabulary_list = np.arange(0, street_num_words + 1, 1).tolist()
street_voc = tf.feature_column.categorical_column_with_vocabulary_list(
    key='STREETPRO', vocabulary_list=vocabulary_list, dtype=tf.dtypes.int64)

weighted_street = tf.feature_column.weighted_categorical_column(categorical_column=street_voc, weight_feature_key='STREETFREQ', dtype=tf.dtypes.int64)
street_one_hot = feature_column.indicator_column(weighted_street)

feature_columns.append(street_one_hot)
</code></pre>

<p>As you can see I used the function tf.feature_column.weighted_categorical_column. Unfortunately I get the following error when I try to train my model:</p>

<pre><code>InvalidArgumentError:  indices and values rows (indexing dimension) must match. (indices = 5, values = 1)
     [[node sequential/dense_features_2/STREETPRO_weighted_by_STREETFREQ_indicator/SparseMerge/SparseReorder (defined at &lt;ipython-input-40-964101dd1dc8&gt;:3) ]] [Op:__inference_train_function_986]
</code></pre>

<p>Furthermore I get the following warning:</p>

<pre><code>WARNING:tensorflow:From ...\feature_column\feature_column_v2.py:4366: sparse_merge (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
</code></pre>

<p>Now I have two questions:</p>

<p><strong>First</strong>: does it make sense to use this function for my described problem? Unfortunately, I couldn’t find a detailed description how this function works (only this short documentations: <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/weighted_categorical_column"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/feature_column/weighted_categorical_column</a>) </p>

<p><strong>Second</strong>: How can I fix the described error?</p>
","I work on a binary classification problem containing a field STREET. In a first step I used the Tokenization to the get a word list (frequency of how often one word appears in the different datasets). Then I used this information to create two columns in my Dataframe describing the word and how often it was used: After I converted the Dataframe to a TensorFlow Dataset I have used the following code to add it to my future columns: As you can see I used the function tf.feature_column.weighted_categorical_column. Unfortunately I get the following error when I try to train my model: Furthermore I get the following warning: Now I have two questions: First: does it make sense to use this function for my described problem? Unfortunately, I couldn’t find a detailed description how this function works (only this short documentations: https://www.tensorflow.org/api_docs/python/tf/feature_column/weighted_categorical_column) Second: How can I fix the described error?",https://stackoverflow.com/questions/61994285,11986067,Lack of Alternative Solutions/Documentation
62086633,Conditional branches using tf.function,"<p>I am having problems calculating the gradient using the gradient tape when using a <code>tf.function</code> with conditional branches.</p>

<p>Inside a gradient tape scope, I am trying to calculate the gradient of <code>z</code> w.r.t <code>self.LMn</code>. This works perfectly fine when I do not annotate the function with <code>@tf.function</code>. The error originates in a subclassed <code>tf.keras.layers.Layer</code> call function:</p>

<pre><code>def call(self, x, training=None, labels=None, cur_switch=None, basis_filters=None):
    if basis_filters is None:
        basis_filters = self.initial_basis_filters

    gap = tf.reduce_mean(x, axis=[1, 2])
    z = tf.einsum('bc,cd-&gt;bd', gap, self.LMn)

    # ... code for out

    return out, z
</code></pre>

<p>The actual error is given as follows:</p>

<pre><code>....
C:\...\ops\cond_v2.py:387 &lt;lambda&gt;
    lambda: _grad_fn(func_graph, grads), [], {},
C:\...\ops\cond_v2.py:363 _grad_fn
    assert len(func_graph.outputs) == len(grads)

AssertionError: 
</code></pre>

<p>More specifically,</p>

<pre><code>func_graphs.outputs = [&lt;tf.Tensor 'vgg16/block1a/block1a_conv/cond_2/Identity:0' shape=(128, 32, 32, None) dtype=float32&gt;, &lt;tf.Tensor 'vgg16/block1a/block1a_conv/cond_2/OptionalFromValue:0' shape=() dtype=variant&gt;, &lt;tf.Tensor 'vgg16/block1a/block1a_conv/cond_2/OptionalFromValue_1:0' shape=() dtype=variant&gt;]
</code></pre>

<p>and </p>

<pre><code>grads = (&lt;tf.Tensor 'gradient_tape/vgg16/block1a/block1a_conv/strided_slice_3/StridedSliceGrad_1:0' shape=(128, 32, 32, None) dtype=float32&gt;,)
</code></pre>

<p>I can assume that each of these outputs corresponds to the cases for some set of conditional branches outputs that are then fed into the <code>tf.einsum</code> function. I have read over all of the edge cases and precautions in the gradient tape documentation as this seems to the problem. Just a note that I am only performing conditional computation using hyperparameters (passed into <code>tf.function</code> as pythonic variables, such as <code>basis_filters</code>). There are also some conditional branches using (tensorflow ops) functions of these hyperparameters, is this allowed? or do I need to compute these values outside <code>tf.function</code> and pass these in as pythonic variables too?</p>

<p>I know the question is not completely clear and I can provide any extra information if needed. It would very helpful to have some guidance on what to look for with this kind of problem.</p>

<p>Thanks!</p>
","I am having problems calculating the gradient using the gradient tape when using a tf.function with conditional branches. Inside a gradient tape scope, I am trying to calculate the gradient of z w.r.t self.LMn. This works perfectly fine when I do not annotate the function with @tf.function. The error originates in a subclassed tf.keras.layers.Layer call function: The actual error is given as follows: More specifically, and I can assume that each of these outputs corresponds to the cases for some set of conditional branches outputs that are then fed into the tf.einsum function. I have read over all of the edge cases and precautions in the gradient tape documentation as this seems to the problem. Just a note that I am only performing conditional computation using hyperparameters (passed into tf.function as pythonic variables, such as basis_filters). There are also some conditional branches using (tensorflow ops) functions of these hyperparameters, is this allowed? or do I need to compute these values outside tf.function and pass these in as pythonic variables too? I know the question is not completely clear and I can provide any extra information if needed. It would very helpful to have some guidance on what to look for with this kind of problem. Thanks!",https://stackoverflow.com/questions/62086633,1582331,Documentation Replication on Other Examples
62211822,TF2 Keras - Feature Engineering in Keras saved model via Tensorflow Serving,"<p>The Tensorflow 2 documentation for preprocessing / feature engineering over a Keras model seems to be quite confusing and isn't very friendly.</p>

<p>Currently I have a simple Keras N-layer model with TF feature columns feeding as dense layer. For training I have CSV files read using <code>tf.dataset</code> API and I have written a feature engineering function that creates new features using <code>dataset.map</code> function.</p>

<pre><code>def feature_engg_features(features):
  #Add new features
  features['nodlgrbyvpatd'] = features['NODLGR'] / features['VPATD']

  return(features)
</code></pre>

<p>I can save the model easily using <code>tf.keras.models.save_model</code> method. However I am having trouble figuring out how to attach the <code>feature_engineering</code> steps in the serving function.</p>

<p><strong>Requirement</strong>: Now I want to take the same feature engineering function above and attach it to my <code>serving function</code> so that in JSON input via <code>tensorflow_model_server</code> the same feature engineering steps are applied. I know about the lambda Layer option in Keras but I want to do this via <code>saved_model</code> method but there are a lot of difficulties here.</p>

<p>For Example, below code gives error:</p>

<pre><code>def feature_engg_features(features):
  #Add new features
  features['nodlgrbyvpatd'] = features['NODLGR'] / features['VPATD']
  return(features)

@tf.function
def serving(data):
    data = tf.map_fn(feature_engg_features, data, dtype=tf.float32)

    # Predict
    predictions = m_(data)

version = ""1""
tf.keras.models.save_model(
    m_,
    ""./exported_model/"" + version,
    overwrite=True,
    include_optimizer=True,
    save_format=None,
    signatures=serving,
    options=None
)
</code></pre>

<p>Error:</p>

<pre><code>Only `tf.functions` with an input signature or concrete functions can be used as a signature.
</code></pre>

<p>The above error is because I have not provided InputSignature of my Keras model but I am not able to understand that I have 13 input fields, what is expected as input signature.</p>

<p>So I wanted to know if anyone knows the shortest way of solving this out. This is a very basic requirement and Tensorflow seems to have kept this quite complicated for Keras Tensorflow model serving.</p>

<p>GIST: <a href=""https://colab.research.google.com/gist/rafiqhasan/6abe93ac454e942317005febef59a459/copy-of-dl-e2e-structured-mixed-data-tf-2-keras-estimator.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/gist/rafiqhasan/6abe93ac454e942317005febef59a459/copy-of-dl-e2e-structured-mixed-data-tf-2-keras-estimator.ipynb</a></p>

<p><strong>EDIT:</strong>
I fixed it, so TensorSpec has to be generated and passed for each feature and also model( ) has to be called in serving function.</p>

<pre><code>@tf.function
def serving(WERKS, DIFGRIRD, SCENARIO, TOTIRQTY, VSTATU, EKGRP, TOTGRQTY, VPATD, EKORG, NODLGR, DIFGRIRV, NODLIR, KTOKK):
    ##Feature engineering
    nodlgrbyvpatd = tf.cast(NODLGR / VPATD, tf.float32)

    payload = {
        'WERKS': WERKS,
        'DIFGRIRD': DIFGRIRD,
        'SCENARIO': SCENARIO,
        'TOTIRQTY': TOTIRQTY,
        'VSTATU': VSTATU,
        'EKGRP': EKGRP,
        'TOTGRQTY': TOTGRQTY,
        'VPATD': VPATD,
        'EKORG': EKORG,
        'NODLGR': NODLGR,
        'DIFGRIRV': DIFGRIRV,
        'NODLIR': NODLIR,
        'KTOKK': KTOKK,
        'nodlgrbyvpatd': nodlgrbyvpatd,        
    }

    ## Predict
    ##IF THERE IS AN ERROR IN NUMBER OF PARAMS PASSED HERE OR DATA TYPE THEN IT GIVES ERROR, ""COULDN'T COMPUTE OUTPUT TENSOR""
    predictions = m_(payload)
    return predictions

serving = serving.get_concrete_function(WERKS=tf.TensorSpec([None,], dtype= tf.string, name='WERKS'), 
                                        DIFGRIRD=tf.TensorSpec([None,], name='DIFGRIRD'),
                                        SCENARIO=tf.TensorSpec([None,], dtype= tf.string, name='SCENARIO'), 
                                        TOTIRQTY=tf.TensorSpec([None,], name='TOTIRQTY'),
                                        VSTATU=tf.TensorSpec([None,], dtype= tf.string, name='VSTATU'), 
                                        EKGRP=tf.TensorSpec([None,], dtype= tf.string, name='EKGRP'),
                                        TOTGRQTY=tf.TensorSpec([None,], name='TOTGRQTY'), 
                                        VPATD=tf.TensorSpec([None,], name='VPATD'),
                                        EKORG=tf.TensorSpec([None,], dtype= tf.string, name='EKORG'), 
                                        NODLGR=tf.TensorSpec([None,], name='NODLGR'),
                                        DIFGRIRV=tf.TensorSpec([None,], name='DIFGRIRV'),
                                        NODLIR=tf.TensorSpec([None,], name='NODLIR'),
                                        KTOKK=tf.TensorSpec([None,], dtype= tf.string, name='KTOKK')
                                        )

version = ""1""
tf.saved_model.save(
    m_,
    ""./exported_model/"" + version,
    signatures=serving
)
</code></pre>
","The Tensorflow 2 documentation for preprocessing / feature engineering over a Keras model seems to be quite confusing and isn't very friendly. Currently I have a simple Keras N-layer model with TF feature columns feeding as dense layer. For training I have CSV files read using tf.dataset API and I have written a feature engineering function that creates new features using dataset.map function. I can save the model easily using tf.keras.models.save_model method. However I am having trouble figuring out how to attach the feature_engineering steps in the serving function. Requirement: Now I want to take the same feature engineering function above and attach it to my serving function so that in JSON input via tensorflow_model_server the same feature engineering steps are applied. I know about the lambda Layer option in Keras but I want to do this via saved_model method but there are a lot of difficulties here. For Example, below code gives error: Error: The above error is because I have not provided InputSignature of my Keras model but I am not able to understand that I have 13 input fields, what is expected as input signature. So I wanted to know if anyone knows the shortest way of solving this out. This is a very basic requirement and Tensorflow seems to have kept this quite complicated for Keras Tensorflow model serving. GIST: https://colab.research.google.com/gist/rafiqhasan/6abe93ac454e942317005febef59a459/copy-of-dl-e2e-structured-mixed-data-tf-2-keras-estimator.ipynb EDIT: I fixed it, so TensorSpec has to be generated and passed for each feature and also model( ) has to be called in serving function.",https://stackoverflow.com/questions/62211822,9334184,Documentation Replication on Other Examples
62236460,How to set bounds and constraints on Tensorflow Variables (tf.Variable),"<p>I am using Tensorflow to minimize a function. The function takes about 10 parameters. Every single parameter has bounds, e.g. a minimum and a maximum value the parameter is allowed to take. For example, the parameter x1 needs to be between 1 and 10.</p>

<p>I also have a pair of parameters that need to have the following constraint x2 > x3. In other words, x2 must always be bigger than x3. (In addition to this, x2 and x3 also have bounds, similarly to the example of x1 above.)</p>

<p>I know that tf.Variable has a ""constraint"" argument, however I can't really find any examples or documentation on how to use this to achieve the bounds and constraints as mentioned above.</p>

<p>Thank you!</p>
","I am using Tensorflow to minimize a function. The function takes about 10 parameters. Every single parameter has bounds, e.g. a minimum and a maximum value the parameter is allowed to take. For example, the parameter x1 needs to be between 1 and 10. I also have a pair of parameters that need to have the following constraint x2 &gt; x3. In other words, x2 must always be bigger than x3. (In addition to this, x2 and x3 also have bounds, similarly to the example of x1 above.) I know that tf.Variable has a ""constraint"" argument, however I can't really find any examples or documentation on how to use this to achieve the bounds and constraints as mentioned above. Thank you!",https://stackoverflow.com/questions/62236460,9762137,Documentation Replicability
62249084,What is the numpy equivalent of TensorFlow Xavier initializer for CNN?,"<p>I would like to re-create the Xavier initialization in NumPy (using basic functions) in the same way that TensorFlow2 does for CNN. 
Here is how I learned to do Xavier initialization in NumPy:</p>

<pre><code># weights.shape = (2,2)
np.random.seed(0)
nodes_in = 2*2
weights = np.random.rand(2,2) * np.sqrt(1/nodes_in)

&gt;&gt;&gt;array([[0.27440675, 0.35759468],
          [0.30138169, 0.27244159]])
</code></pre>

<p>This is the way I learned Xavier initialization for the logistic regression model. It seems that for Convolution Neural Network it should be different but I don't know how.</p>

<pre><code>initializer = tf.initializers.GlorotUniform(seed=0)
tf.Variable(initializer(shape=[2,2],dtype=tf.float32))

&gt;&gt;&gt;&lt;tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
   array([[-0.7078647 ,  0.50461936],
          [ 0.73500216,  0.6633029 ]], dtype=float32)&gt;
</code></pre>

<p>I'm confused by the TensorFlow <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform"" rel=""noreferrer"">documentation</a> when they explain the ""fan_in"" and ""fan_out"". I'm guessing this is where the problem is. Can somebody dumb it down for me, please? </p>

<p>Much appreciate it!</p>

<p><em>[UPDATE]:</em></p>

<p>When I follow the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform"" rel=""noreferrer"">tf.keras.initializers.GlorotUniform</a> documentation I still don't come to the same results:</p>

<pre><code># weights.shape = (2,2)
np.random.seed(0)
fan_in = 2*2
fan_out = 2*2
limit = np.sqrt(6/(fan_in + fan_out))
np.random.uniform(-limit,limit,size=(2,2))
&gt;&gt;&gt;array([[0.08454747, 0.37271892],
          [0.17799139, 0.07773995]])
</code></pre>
","I would like to re-create the Xavier initialization in NumPy (using basic functions) in the same way that TensorFlow2 does for CNN. Here is how I learned to do Xavier initialization in NumPy: This is the way I learned Xavier initialization for the logistic regression model. It seems that for Convolution Neural Network it should be different but I don't know how. I'm confused by the TensorFlow documentation when they explain the ""fan_in"" and ""fan_out"". I'm guessing this is where the problem is. Can somebody dumb it down for me, please? Much appreciate it! [UPDATE]: When I follow the tf.keras.initializers.GlorotUniform documentation I still don't come to the same results:",https://stackoverflow.com/questions/62249084,5040482,Documentation Replication on Other Examples
62284095,What are the parameters to tf.GradientTape()'s __exit__ function?,"<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">documentation</a> for <code>tf.GradientTape</code>, its <code>__exit__()</code> method takes three positional arguments: <code>typ, value, traceback</code>.</p>

<p><strong>What exactly are these parameters?</strong> </p>

<p>How does the <code>with</code> statement infer them? </p>

<p>What values should I give them in the code below (where I'm <em>not</em> using a <code>with</code> statement):</p>

<pre><code>x = tf.Variable(5)

gt = tf.GradientTape()
gt.__enter__()
y = x ** 2
gt.__exit__(typ = __, value = __, traceback = __)
</code></pre>
","According to the documentation for tf.GradientTape, its __exit__() method takes three positional arguments: typ, value, traceback. What exactly are these parameters? How does the with statement infer them? What values should I give them in the code below (where I'm not using a with statement):",https://stackoverflow.com/questions/62284095,1403340,Documentation Replicability
62476015,TF 2.2 Saved_model from keras with custom signatures and preprocessing,"<p>I am trying to use the <code>tf.saved_model.save</code> after a training a Transformer Model in order to deploy it.
My model has multiple inputs and outputs. If I am using the saved_model function for serving, I add some issue about the input shape with the first input, and the second input is not visible when I am using the <code>saved_model_cli show</code> function. I found a way to solve that issue by wrapping the main transformer block by a model module and then save the model.</p>
<pre><code>def build_model(MAX_LEN, transformer_layer):
    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
    inp2 = Input(shape=(MAX_LEN,), dtype=tf.int32, name=&quot;attention_mask&quot;)
    sequence_output, pooled_output = transformer_layer(inp, attention_mask = inp2)
    out = Dense(1, activation='sigmoid', name='outputs')(pooled_output)
    model = Model(inputs=[inp], outputs=[out])
    return model
</code></pre>
<p>But, I see in the documentation there is another way which consist in using signatures to indicate the inputs/outputs during the  <code>tf.saved_model.save</code>. To do that, we need to use the <code>tf.Module</code> class but I haven't understood how to use it in the case we have multiple inputs/outputs exactly (how to make understand the module that inputs tensor should be related to that inputs for the model?) .
Does anyone know how to do that with the second method ?  Morever can we do preprocessing of the data through the signature ?https://www.tensorflow.org/guide/saved_model</p>
<pre><code>class CustomModule(tf.Module):

  def __init__(self):
    super(CustomModule, self).__init__()
    self.v = tf.Variable(1.)

  @tf.function
  def __call__(self, x):
    print('Tracing with', x)
    return x * self.v

  @tf.function(input_signature=[tf.TensorSpec([], tf.float32)])
  def mutate(self, new_v):
    self.v.assign(new_v)

module = CustomModule()
</code></pre>
","I am trying to use the tf.saved_model.save after a training a Transformer Model in order to deploy it. My model has multiple inputs and outputs. If I am using the saved_model function for serving, I add some issue about the input shape with the first input, and the second input is not visible when I am using the saved_model_cli show function. I found a way to solve that issue by wrapping the main transformer block by a model module and then save the model. But, I see in the documentation there is another way which consist in using signatures to indicate the inputs/outputs during the tf.saved_model.save. To do that, we need to use the tf.Module class but I haven't understood how to use it in the case we have multiple inputs/outputs exactly (how to make understand the module that inputs tensor should be related to that inputs for the model?) . Does anyone know how to do that with the second method ? Morever can we do preprocessing of the data through the signature ?https://www.tensorflow.org/guide/saved_model",https://stackoverflow.com/questions/62476015,8401969,Documentation Replication on Other Examples
62611459,How to support mixed precision in custom Tensorflow layers?,"<p><strong>When developing my own custom layers for <code>tf.keras</code>: how am I supposed to support mixed precision?</strong></p>
<p>The <a href=""https://www.tensorflow.org/guide/mixed_precision"" rel=""nofollow noreferrer"">documentation of mixed precision</a> - a feature which is currently marked as experimental in Tensorflow 2.2 - only explains how to use it from a consumers perspective with predefined layers such as the <code>tf.keras.layers.Dense</code> one.</p>
<p>I already tried to guess it myself and found two - maybe relevant - details:</p>
<ul>
<li><p>The <code>dtype</code> property stays as <code>float32</code> by default when using 16-bit mixed precision.</p>
</li>
<li><p>There is a <code>mixed_precision.get_layer_policy(layer)</code> method (see <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/get_layer_policy"" rel=""nofollow noreferrer"">docs</a>) and a <code>mixed_precision.global_policy()</code> method (see <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/global_policy"" rel=""nofollow noreferrer"">docs</a>) which could be used to retrieve the configured <code>compute_dtype</code> and <code>variable_dtype</code>.</p>
</li>
</ul>
<p>Am I supposed to use the above <code>get_layer_policy</code>-method and just cast my variables into <code>compute_dtype</code> within the <code>call(...)</code> method of my layer? (And pass <code>variable_dtype</code> in my layers <code>build(...)</code> method to <code>add_weight(...)</code> when creating variables?)</p>
<p>For example, here is naive sample implementation of a standard dense neuron layer:</p>
<pre class=""lang-py prettyprint-override""><code>  def call(self, input):
    policy = mixed_precision.get_layer_policy(self)
    bias = tf.cast(self._bias, policy.compute_dtype)
    weights = tf.cast(self._weights, policy.compute_dtype)
    y = tf.nn.bias_add(tf.matmul(input, weights), bias)
    outputs = self._activation(y)
    return outputs
</code></pre>
<p>Sure, nobody would implement such basic stuff themselves, that one is just for demonstration. But, would this be the way the Tensorflow team expects us to implement the <code>call(...)</code> methods of our custom layers?</p>
","When developing my own custom layers for tf.keras: how am I supposed to support mixed precision? The documentation of mixed precision - a feature which is currently marked as experimental in Tensorflow 2.2 - only explains how to use it from a consumers perspective with predefined layers such as the tf.keras.layers.Dense one. I already tried to guess it myself and found two - maybe relevant - details: Am I supposed to use the above get_layer_policy-method and just cast my variables into compute_dtype within the call(...) method of my layer? (And pass variable_dtype in my layers build(...) method to add_weight(...) when creating variables?) For example, here is naive sample implementation of a standard dense neuron layer: Sure, nobody would implement such basic stuff themselves, that one is just for demonstration. But, would this be the way the Tensorflow team expects us to implement the call(...) methods of our custom layers?",https://stackoverflow.com/questions/62611459,1037200,Documentation Replication on Other Examples
62670041,batch_size in tf model.fit() vs. batch_size in tf.data.Dataset,"<p>I have a large dataset that can fit in host memory. However, when I use tf.keras to train the model, it yields GPU out-of-memory problem. Then I look into tf.data.Dataset and want to use its batch() method to batch the training dataset so that it can execute the model.fit() in GPU. According to its documentation, an example is as follows:</p>
<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))

BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
test_dataset = test_dataset.batch(BATCH_SIZE)
</code></pre>
<p>Is the BATCH_SIZE in dataset.from_tensor_slices().batch() the same as the batch_size in the tf.keras modelt.fit()?</p>
<p>How should I choose BATCH_SIZE so that GPU has sufficient data to run efficiently and yet its memory is not overflown?</p>
","I have a large dataset that can fit in host memory. However, when I use tf.keras to train the model, it yields GPU out-of-memory problem. Then I look into tf.data.Dataset and want to use its batch() method to batch the training dataset so that it can execute the model.fit() in GPU. According to its documentation, an example is as follows: Is the BATCH_SIZE in dataset.from_tensor_slices().batch() the same as the batch_size in the tf.keras modelt.fit()? How should I choose BATCH_SIZE so that GPU has sufficient data to run efficiently and yet its memory is not overflown?",https://stackoverflow.com/questions/62670041,6227592,Documentation Replication on Other Examples
62752605,Loss function in tf.nn.sampled_softmax_loss,"<p>I have a question regarding Tensorflow:</p>
<p>Which loss function is used in <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer""><code>tf.nn.sampled_softmax_loss</code></a>?</p>
<p>I believe it's <em><strong>cross-entropy</strong></em>, but it is not written on the official website. Can anyone confirm my guess?</p>
","I have a question regarding Tensorflow: Which loss function is used in tf.nn.sampled_softmax_loss? I believe it's cross-entropy, but it is not written on the official website. Can anyone confirm my guess?",https://stackoverflow.com/questions/62752605,13874745,Documentation Replicability
62877768,Input shape of tf.data.Dataset not accepted by model.fit(),"<p>I would like to feed with data my model by applying a <code>tf.data.Dataset</code>.</p>
<p>Having checked the documentation of TF 2.0 I found that the <code>.fit()</code> function (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit</a>) accepts:</p>
<blockquote>
<p>x - A tf.data dataset. Should return a tuple of either (inputs, targets)
or (inputs, targets, sample_weights).</p>
</blockquote>
<p>So, I wrote the following minial proof of concept code:</p>
<pre><code>from sklearn.datasets import make_blobs
import tensorflow as tf
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.metrics import Accuracy, AUC

X, Y = make_blobs(n_samples=500, n_features=2, cluster_std=3.0, random_state=1)

def define_model():
    model = Sequential()
    model.add(Dense(units=1, activation=&quot;sigmoid&quot;, input_shape=(2,)))
    model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[AUC(), Accuracy()])
    return model

model = define_model()

X_ds = tf.data.Dataset.from_tensor_slices(X)
Y_ds = tf.data.Dataset.from_tensor_slices(Y)
dataset = tf.data.Dataset.zip((X_ds, Y_ds))

for elem in dataset.take(1):
    print(type(elem))
    print(elem)

model.fit(x=dataset) #&lt;-- does not work
#model.fit(x=X, y=Y) &lt;-- does work without any problems....
</code></pre>
<p>As mentioned in the second comment, the code that does not apply a <code>tf.data.Dataset</code> works fine.</p>
<p>However, when applying the Dataset object I get the following error message:</p>
<pre><code>&lt;class 'tuple'&gt;
(&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([-10.42729974,  -0.85439721])&gt;, &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;)
... other output here...
ValueError: Error when checking input: expected dense_19_input to have
shape (2,) but got array with shape (1,)
</code></pre>
<p>From my understanding of the documentation, the dataset I have constructed should be exactly the tuple object the fit method expects.</p>
<p>I do not understand this error message.</p>
<p>What am I doing wrong here?</p>
","I would like to feed with data my model by applying a tf.data.Dataset. Having checked the documentation of TF 2.0 I found that the .fit() function (https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) accepts: So, I wrote the following minial proof of concept code: As mentioned in the second comment, the code that does not apply a tf.data.Dataset works fine. However, when applying the Dataset object I get the following error message: From my understanding of the documentation, the dataset I have constructed should be exactly the tuple object the fit method expects. I do not understand this error message. What am I doing wrong here?",https://stackoverflow.com/questions/62877768,1020704,Documentation Ambiguity
62956096,Is it possible to extract trained class names from tflite model?,"<p>I have tried to search everywhere, tried everything in <code>tflite_interpreter = tf.lite.Interpreter(model_path='model.tflite')</code>, read tflite documentation but I cannot find the method to extract the class names from the model.</p>
<p>Is it possible?</p>
","I have tried to search everywhere, tried everything in tflite_interpreter = tf.lite.Interpreter(model_path='model.tflite'), read tflite documentation but I cannot find the method to extract the class names from the model. Is it possible?",https://stackoverflow.com/questions/62956096,10688345,Lack of Alternative Solutions/Documentation
62962147,TensorFlow - Fashion MNIST Steps Per Epoch,"<p>I'm working with the Kera's Fashion MNIST dataset. When I fit my model, I noticed to complete one epoch it would have to go through 1500 steps.</p>
<pre><code>history = model.fit(x_train, y_train, epochs=30, validation_split=0.2)

Epoch 3/30
1500/1500 [==============================] - 3s 2ms/step - loss: 0.4494 - sparse_categorical_accuracy: 0.8438 - val_loss: 0.4691 - val_sparse_categorical_accuracy: 0.8308
Epoch 4/30
964/1500 [==================&gt;...........] - ETA: 0s - loss: 0.4294 - sparse_categorical_accuracy: 0.8504
</code></pre>
<p>I was looking at the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">docs</a> for the fit function, but couldn't understand why the default steps were set to 1500. I understand when the <code>steps_per_epoch</code> is <code>None</code> the behavior is dependent on the data type of the dataset, but how can I check if the data type is a tensor or tf.data?</p>
","I'm working with the Kera's Fashion MNIST dataset. When I fit my model, I noticed to complete one epoch it would have to go through 1500 steps. I was looking at the docs for the fit function, but couldn't understand why the default steps were set to 1500. I understand when the steps_per_epoch is None the behavior is dependent on the data type of the dataset, but how can I check if the data type is a tensor or tf.data?",https://stackoverflow.com/questions/62962147,6179818,Documentation Replication on Other Examples
62994289,Saving TFrecords with TPU,"<p>I'm trying to use <code>tf.data.experimental.TFRecordWriter</code> to save dataset on Google cloud bucket using TPU. The code from the example in documentation works:</p>
<pre><code>dataset = tf.data.Dataset.range(3)
dataset = dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter(&quot;gs://oleg-zyablov/test.tfrec&quot;)
writer.write(dataset)
</code></pre>
<p>But I have dataset of tuples (string, int64), where first is jpg-encoded image and second is label. When I pass it to writer.write() method, it says: 'tuple' object has no attribute 'is_compatible_with'.</p>
<p>I guess I have to pack image and label into tf.train.Example to make it work. I use the following code:</p>
<pre><code>def serialize(image, class_idx):
  tfrecord = tf.train.Example(features = tf.train.Features(feature = {
    'image': tf.train.Feature(bytes_list = tf.train.BytesList(value = [image.numpy()])),
    'class': tf.train.Feature(int64_list = tf.train.Int64List(value = [class_idx.numpy()]))
  }))
  return tfrecord.SerializeToString()

#saving_pipeline is a dataset of (string, int64) tuples
saving_pipeline_serialized = saving_pipeline.map(serialize)

writer = tf.data.experimental.TFRecordWriter(&quot;gs://oleg-zyablov/car-classification/train_tfrecords/test.tfrecord&quot;)
writer.write(saving_pipeline_serialized)
</code></pre>
<p>But I get the following error:</p>
<pre><code>'Tensor' object has no attribute 'numpy'
</code></pre>
<p>Although I didn't turn off eager mode and this code <code>tf.constant([], dtype = float).numpy()</code> works. Maybe TPU works not in eager mode? Ok, I changed .numpy() to .eval() in the code above. Then I get the foloowing error:</p>
<pre><code>Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`
</code></pre>
<p>What session does TPU use and how do I specify it? When I run the code below:</p>
<pre><code>with tf.compat.v1.Session():
  saving_pipeline_serialized = saving_pipeline.map(serialize)
</code></pre>
<p>I get an error:</p>
<pre><code>Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`.
</code></pre>
<p>But I don't know how to get the current graph and pass it to tf.compat.v1.Session(). When I go another way and type:</p>
<pre><code>image.eval(session = tf.compat.v1.get_default_session())
</code></pre>
<p>It says:</p>
<pre><code>Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`
</code></pre>
<p>Is it possible to use .eval() on TPU? Or how do I perform my task another way?</p>
","I'm trying to use tf.data.experimental.TFRecordWriter to save dataset on Google cloud bucket using TPU. The code from the example in documentation works: But I have dataset of tuples (string, int64), where first is jpg-encoded image and second is label. When I pass it to writer.write() method, it says: 'tuple' object has no attribute 'is_compatible_with'. I guess I have to pack image and label into tf.train.Example to make it work. I use the following code: But I get the following error: Although I didn't turn off eager mode and this code tf.constant([], dtype = float).numpy() works. Maybe TPU works not in eager mode? Ok, I changed .numpy() to .eval() in the code above. Then I get the foloowing error: What session does TPU use and how do I specify it? When I run the code below: I get an error: But I don't know how to get the current graph and pass it to tf.compat.v1.Session(). When I go another way and type: It says: Is it possible to use .eval() on TPU? Or how do I perform my task another way?",https://stackoverflow.com/questions/62994289,13959198,Documentation Replication on Other Examples
63004540,How to pad 1 dimensinal vector in tensorflow? Getting InvalidArgumentError: paddings must be a matrix with 2 columns with tf.pad,"<p>I am trying to use tf.pad. Here is my attempt to pad the tensor to length 20, with values 10.</p>
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), paddings=20, constant_values=10)
</code></pre>
<p>I get this error message</p>
<pre><code>InvalidArgumentError: paddings must be a matrix with 2 columns: [2,1] [Op:PadV2]
</code></pre>
<p>I am looking at the documentation</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/pad</a></p>
<blockquote>
<p>paddings is an integer tensor with shape [n, 2], where n is the rank of tensor. For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension</p>
</blockquote>
<p>But I am unable to figure out how to shape the pad value</p>
","I am trying to use tf.pad. Here is my attempt to pad the tensor to length 20, with values 10. I get this error message I am looking at the documentation https://www.tensorflow.org/api_docs/python/tf/pad But I am unable to figure out how to shape the pad value",https://stackoverflow.com/questions/63004540,3259896,Documentation Replicability
63020800,"Understanding Tensorflow Object-Detection API, kwargs for Checkpoint class, what is `_base_tower_layers_for_heads`?","<p>Currently, I've been learning how to use Object-Detection API from Tensorflow. I follow a quick start tutorial for training with custom data with <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb"" rel=""nofollow noreferrer"">this notebook</a> as suggested by them. In the effort to understanding each line of the code, I stumbled upon this snippet code in the &quot;Create Model and Restore Weight&quot; part.</p>
<pre><code>fake_box_predictor = tf.compat.v2.train.Checkpoint(
    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,
    # _prediction_heads=detection_model._box_predictor._prediction_heads,
    #    (i.e., the classification head that we *will not* restore)
    _box_prediction_head=detection_model._box_predictor._box_prediction_head,
    )
</code></pre>
<p>I don't really understand what are the keyword arguments that are available for the <code>Checkpoint</code> class in that particular snippet code. My question is; is there any documentation out there that shows the list of the keyword arguments? or at least explain what are <code>_base_tower_layers_for_heads</code> and<code>_box_prediction_head</code>?</p>
<p>I've read the <code>tf.train.Checkpoint</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint"" rel=""nofollow noreferrer"">documentation</a>. It says that we can provide <code>models</code> or <code>optimizers</code> for the constructor's keyword argument. I am already familiar with this class to restore the weights to my model, however, I find it is alien to see <code>_base_tower_layers_for_heads</code> or <code>_box_prediction_head</code> for the keyword argument.</p>
<p>I do know about 'heads' and different types of 'heads' in the object detection architecture and their relation to transfer learning, what I don't understand is in the context of their data structure. How do I know, these keyword arguments exist? and is there any other else? I would really appreciate it if somebody could give me insights or at least tell me where can I find documentation that I can read to understand it more.</p>
","Currently, I've been learning how to use Object-Detection API from Tensorflow. I follow a quick start tutorial for training with custom data with this notebook as suggested by them. In the effort to understanding each line of the code, I stumbled upon this snippet code in the ""Create Model and Restore Weight"" part. I don't really understand what are the keyword arguments that are available for the Checkpoint class in that particular snippet code. My question is; is there any documentation out there that shows the list of the keyword arguments? or at least explain what are _base_tower_layers_for_heads and_box_prediction_head? I've read the tf.train.Checkpoint documentation. It says that we can provide models or optimizers for the constructor's keyword argument. I am already familiar with this class to restore the weights to my model, however, I find it is alien to see _base_tower_layers_for_heads or _box_prediction_head for the keyword argument. I do know about 'heads' and different types of 'heads' in the object detection architecture and their relation to transfer learning, what I don't understand is in the context of their data structure. How do I know, these keyword arguments exist? and is there any other else? I would really appreciate it if somebody could give me insights or at least tell me where can I find documentation that I can read to understand it more.",https://stackoverflow.com/questions/63020800,8410038,Requesting (Additional) Resources
63146831,What is the analytic interpretation for Tensorflow custom gradient?,"<p>In the official <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">tf.custom_gradient</a> documentation it shows how to define custom gradients for <code>log(1 + exp(x))</code></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.math.log(1 + e), grad
</code></pre>
<p>When <code>y = log(1 + exp(x))</code>, analytically the derivative comes out to be <code>dy/dx = (1 - 1 / (1 + exp(x)))</code>.</p>
<p>However in the code <code>def grad</code> says its <code>dy * (1 - 1 / (1 + exp(x)))</code>.
<code>dy/dx = dy * (1 - 1 / (1 + exp(x)))</code> is not a valid equation. While <code>dx = dy * (1 - 1 / (1 + exp(x)))</code> is wrong as it should be the reciprocal.</p>
<p>What does the <code>grad</code> function equate to?</p>
","In the official tf.custom_gradient documentation it shows how to define custom gradients for log(1 + exp(x)) When y = log(1 + exp(x)), analytically the derivative comes out to be dy/dx = (1 - 1 / (1 + exp(x))). However in the code def grad says its dy * (1 - 1 / (1 + exp(x))). dy/dx = dy * (1 - 1 / (1 + exp(x))) is not a valid equation. While dx = dy * (1 - 1 / (1 + exp(x))) is wrong as it should be the reciprocal. What does the grad function equate to?",https://stackoverflow.com/questions/63146831,1217998,Documentation Replication on Other Examples
63158314,Tensorflow 2.3.0 - Warning: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version,"<p>I've just updated to TF-2.3. In a model using <code>tf.data.Dataset.from_tensor_slices</code> as data source, I get the folowing warning:</p>
<pre><code>WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
</code></pre>
<p>I didn't find the instructions on the documentation on how to use the updated methods.</p>
<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices(
    (
        {&quot;input_1&quot;: x1_train, 
         &quot;input_2&quot;: x2_train}, 
        {&quot;output&quot;: y_train},
    )
)

train_batches = train_dataset.batch(GLOBAL_BATCH_SIZE)
</code></pre>
<p>Training:</p>
<pre><code>history = model.fit(
    x = train_batches,
    epochs=30,
    verbose = 1,
)
</code></pre>
<p>Thanks in advance.</p>
","I've just updated to TF-2.3. In a model using tf.data.Dataset.from_tensor_slices as data source, I get the folowing warning: I didn't find the instructions on the documentation on how to use the updated methods. Training: Thanks in advance.",https://stackoverflow.com/questions/63158314,11524722,Lack of Alternative Solutions/Documentation
63379008,How to make a diagonal tensor and why doesn't Tensorflow linalg.tensor_diag do that?,"<p>What I would consider a diagonal tensor is a tensor t of shape (d1, ..., dr) which is all zero except when the components are equal.
So t[i,j,k,l] = 0 unless i == j == k == l.
A function to create such a tensor should take in a shape (d1, ..., dr) and a vector [a1, ..., ak] of length min(d1, ..., dr), placing these values along the diagonal.</p>
<p>I would like to do this in Tensorflow, and the most relevant function I could find was <a href=""https://www.tensorflow.org/api_docs/python/tf/linalg/tensor_diag"" rel=""nofollow noreferrer"">tf.linalg.tensor_diag</a>, but it doesn't do what I want. For instance, the diagonal input is a tensor, and the output tensor always has twice the rank, and so it can never output tensors of odd rank.</p>
<p>The documentation says &quot;Given a diagonal, this operation returns a tensor with the diagonal and everything else padded with zeros&quot;, but I don't know how to square that with its actual behavior.</p>
<p>My question is two parts:</p>
<ol>
<li><p>What is the best way in TF to do create what I am calling a diagonal tensor. Is there another name for this?</p>
</li>
<li><p>Why does linalg.tensor_diag work like this? What is the intended use?</p>
</li>
</ol>
<p>Here is an example output:</p>
<pre><code>&gt;&gt;&gt; tf.linalg.tensor_diag([1,2],[3,4]])

&lt;tf.Tensor: shape=(2, 2, 2, 2), dtype=int32, numpy=
array([[[[1, 0],
         [0, 0]],

        [[0, 2],
         [0, 0]]],


       [[[0, 0],
         [3, 0]],

        [[0, 0],
         [0, 4]]]], dtype=int32)&gt;```
</code></pre>
","What I would consider a diagonal tensor is a tensor t of shape (d1, ..., dr) which is all zero except when the components are equal. So t[i,j,k,l] = 0 unless i == j == k == l. A function to create such a tensor should take in a shape (d1, ..., dr) and a vector [a1, ..., ak] of length min(d1, ..., dr), placing these values along the diagonal. I would like to do this in Tensorflow, and the most relevant function I could find was tf.linalg.tensor_diag, but it doesn't do what I want. For instance, the diagonal input is a tensor, and the output tensor always has twice the rank, and so it can never output tensors of odd rank. The documentation says ""Given a diagonal, this operation returns a tensor with the diagonal and everything else padded with zeros"", but I don't know how to square that with its actual behavior. My question is two parts: Here is an example output:",https://stackoverflow.com/questions/63379008,6667924,Documentation Ambiguity
63383594,How does Tensorflow build() work from tf.keras.layers.Layer,"<p>I was wondering if anyone knew how the <code>build()</code> function works from the <code>tf.keras.layers.Layer</code> class under the hood. According to the <a href=""https://www.tensorflow.org/tutorials/customization/custom_layers"" rel=""noreferrer"">documentation</a>:</p>
<blockquote>
<p><em>build is called when you know the shapes of the input tensors and can
do the rest of the initialization</em></p>
</blockquote>
<p>so to me it seems like the class is behaving similar to this:</p>
<pre><code>class MyDenseLayer:
  def __init__(self, num_outputs):
    self.num_outputs = num_outputs

  def build(self, input_shape):
    self.kernel = self.add_weight(&quot;kernel&quot;,
                                  shape=[int(input_shape[-1]), self.num_outputs])

  def __call__(self, input):
    self.build(input.shape) ## build is called here when input shape is known
    return tf.matmul(input, self.kernel)
</code></pre>
<p>I can't imagine <code>build()</code> would be called for ever <code>__call__</code>, but it is the only place where the input is passed in. Does anyone know how exactly this works under the hood?</p>
","I was wondering if anyone knew how the build() function works from the tf.keras.layers.Layer class under the hood. According to the documentation: so to me it seems like the class is behaving similar to this: I can't imagine build() would be called for ever __call__, but it is the only place where the input is passed in. Does anyone know how exactly this works under the hood?",https://stackoverflow.com/questions/63383594,11065415,Documentation Replication on Other Examples
63399368,Error with exporting TF2.2.0 model with tf.lookup.StaticHashTable for Serving,"<p>I'm using <code>StaticHashTable</code> as in one Lambda layer after the output layer of my tf.keras model. It's quite simple actually: I've a text classification models and I'm adding a simple lambda layer that takes the <code>model.output</code> and convert the model_id to more general labels. I can save this version of model with model.save(... as H5 format..) without any issue, and can load it back and use it without any problem.</p>
<p>Issue is, when I try to export my TF2.2.0 model for TF-Serving, I can't find how I can export it. Here is what I can do with TF1.X or with <code>TF2.X + tf.compat.v1.disable_eager_execution()</code></p>
<pre class=""lang-py prettyprint-override""><code>tf.compat.v1.disable_eager_execution()
version = 1
name = 'tmp_model'
export_path = f'/opt/tf_serving/{name}/{version}'
builder = saved_model_builder.SavedModelBuilder(export_path)

model_signature = tf.compat.v1.saved_model.predict_signature_def(
    inputs={
        'input': model.input
    }, 
    outputs={
        'output': model.output
    }
)

with tf.compat.v1.keras.backend.get_session() as sess:
    builder.add_meta_graph_and_variables(
        sess=sess,
        tags=[tf.compat.v1.saved_model.tag_constants.SERVING],
        signature_def_map={
            'predict': model_signature
        },
        # For initializing Hashtables
        main_op=tf.compat.v1.tables_initializer()
    )
    builder.save()
</code></pre>
<p>This will save my models with TF1.X format for serving and I can use it without any issue. Things is, I'm using LSTM layer and I want to use my model on GPU. By the documentation, if I disable the eager mode, I can't use the GPU-version of LSTM with TF2.2. And without going through above mentioned code, I can't save my model for serving wrt TF2.2 standard and StaticHashTables.</p>
<p>Here is how I'm trying to export my TF2.2 model which is using StaticHashTables in final layer; and which is giving error as below:</p>
<pre class=""lang-py prettyprint-override""><code>class MyModule(tf.Module):

    def __init__(self, model):
        super(MyModule, self).__init__()
        self.model = model
    
    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 16), dtype=tf.int32, name='input')])
    def predict(self, input):
        result = self.model(input)
        return {&quot;output&quot;: result}

version = 1
name = 'tmp_model'
export_path = f'/opt/tf_serving/{name}/{version}'

module = MyModule(model)
tf.saved_model.save(module, export_path, signatures={&quot;predict&quot;: module.predict.get_concrete_function()})
</code></pre>
<p><strong>Error:</strong></p>
<pre class=""lang-sh prettyprint-override""><code>AssertionError: Tried to export a function which references untracked object Tensor(&quot;2907:0&quot;, shape=(), dtype=resource).
TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
</code></pre>
<p>Any suggestion or am I missing anything on exporting TF2.2 model which is using the <code>StaticHashTables</code> in final Lambda layer for TensorFlow Serving?</p>
<p>More info here: <a href=""https://github.com/tensorflow/serving/issues/1719"" rel=""noreferrer"">https://github.com/tensorflow/serving/issues/1719</a></p>
<p>Thanks!</p>
","I'm using StaticHashTable as in one Lambda layer after the output layer of my tf.keras model. It's quite simple actually: I've a text classification models and I'm adding a simple lambda layer that takes the model.output and convert the model_id to more general labels. I can save this version of model with model.save(... as H5 format..) without any issue, and can load it back and use it without any problem. Issue is, when I try to export my TF2.2.0 model for TF-Serving, I can't find how I can export it. Here is what I can do with TF1.X or with TF2.X + tf.compat.v1.disable_eager_execution() This will save my models with TF1.X format for serving and I can use it without any issue. Things is, I'm using LSTM layer and I want to use my model on GPU. By the documentation, if I disable the eager mode, I can't use the GPU-version of LSTM with TF2.2. And without going through above mentioned code, I can't save my model for serving wrt TF2.2 standard and StaticHashTables. Here is how I'm trying to export my TF2.2 model which is using StaticHashTables in final layer; and which is giving error as below: Error: Any suggestion or am I missing anything on exporting TF2.2 model which is using the StaticHashTables in final Lambda layer for TensorFlow Serving? More info here: https://github.com/tensorflow/serving/issues/1719 Thanks!",https://stackoverflow.com/questions/63399368,4496896,Documentation Replication on Other Examples
63482945,How does tf.nn.dilation2d compute gradient and learn its filters,"<p>I want to understand how the tf.nn.dilation2d is working and how the &quot;filters&quot;, which refers to a structural element are learned.</p>
<p>The official documentation is available here : <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d</a></p>
<p>The documentation didn't reference a scientific paper, and I found a lot of different idea in scientific literature, about morphological filters in deep learnign. Here just some examples :</p>
<ul>
<li><a href=""https://arxiv.org/abs/1906.01751"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1906.01751</a></li>
<li><a href=""https://arxiv.org/abs/1909.01532"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1909.01532</a></li>
<li><a href=""https://doi.org/10.1109/TNNLS.2018.2890334"" rel=""nofollow noreferrer"">https://doi.org/10.1109/TNNLS.2018.2890334</a></li>
<li><a href=""https://doi.org/10.1142/S0218001419540247"" rel=""nofollow noreferrer"">https://doi.org/10.1142/S0218001419540247</a></li>
</ul>
<p>I searched into the code (<a href=""https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/nn_ops.py#L327-L392"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/nn_ops.py#L327-L392</a>), but the tf.nn.dilation2d only call gen_nn_ops.dilation2d</p>
<pre><code>@tf_export(&quot;nn.dilation2d&quot;, v1=[])
@dispatch.add_dispatch_support
def dilation2d_v2(input, filters, strides, padding, data_format, dilations, name=None):
  if data_format != &quot;NHWC&quot;:
    raise ValueError(&quot;Data formats other than NHWC are not yet supported&quot;)
  return gen_nn_ops.dilation2d(input=input,
                               filter=filters,
                               strides=strides,
                               rates=dilations,
                               padding=padding,
                               name=name)
</code></pre>
<p>I searched into gen_nn_ops.py (which I found inside my python lib folder, probably because it's generated from somewhere else) but I didn't understand what the code is doing.</p>
<pre><code>def dilation2d(input, filter, strides, rates, padding, name=None):
  r&quot;&quot;&quot;Computes the grayscale dilation of 4-D `input` and 3-D `filter` tensors.

  The `input` tensor has shape `[batch, in_height, in_width, depth]` and the
  `filter` tensor has shape `[filter_height, filter_width, depth]`, i.e., each
  input channel is processed independently of the others with its own structuring
  function. The `output` tensor has shape
  `[batch, out_height, out_width, depth]`. The spatial dimensions of the output
  tensor depend on the `padding` algorithm. We currently only support the default
  &quot;NHWC&quot; `data_format`.

  In detail, the grayscale morphological 2-D dilation is the max-sum correlation
  (for consistency with `conv2d`, we use unmirrored filters):

      output[b, y, x, c] =
         max_{dy, dx} input[b,
                            strides[1] * y + rates[1] * dy,
                            strides[2] * x + rates[2] * dx,
                            c] +
                      filter[dy, dx, c]

  Max-pooling is a special case when the filter has size equal to the pooling
  kernel size and contains all zeros.

  Note on duality: The dilation of `input` by the `filter` is equal to the
  negation of the erosion of `-input` by the reflected `filter`.

  Args:
    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
      4-D with shape `[batch, in_height, in_width, depth]`.
    filter: A `Tensor`. Must have the same type as `input`.
      3-D with shape `[filter_height, filter_width, depth]`.
    strides: A list of `ints` that has length `&gt;= 4`.
      The stride of the sliding window for each dimension of the input
      tensor. Must be: `[1, stride_height, stride_width, 1]`.
    rates: A list of `ints` that has length `&gt;= 4`.
      The input stride for atrous morphological dilation. Must be:
      `[1, rate_height, rate_width, 1]`.
    padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;`.
      The type of padding algorithm to use.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  &quot;&quot;&quot;
  _ctx = _context._context or _context.context()
  tld = _ctx._thread_local_data
  if tld.is_eager:
    try:
      _result = pywrap_tfe.TFE_Py_FastPathExecute(
        _ctx._context_handle, tld.device_name, &quot;Dilation2D&quot;, name,
        tld.op_callbacks, input, filter, &quot;strides&quot;, strides, &quot;rates&quot;, rates,
        &quot;padding&quot;, padding)
      return _result
    except _core._FallbackException:
      try:
        return dilation2d_eager_fallback(
            input, filter, strides=strides, rates=rates, padding=padding,
            name=name, ctx=_ctx)
      except _core._SymbolicException:
        pass  # Add nodes to the TensorFlow graph.
    except _core._NotOkStatusException as e:
      _ops.raise_from_not_ok_status(e, name)
  # Add nodes to the TensorFlow graph.
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _, _, _op, _outputs = _op_def_library._apply_op_helper(
        &quot;Dilation2D&quot;, input=input, filter=filter, strides=strides,
                      rates=rates, padding=padding, name=name)
  _result = _outputs[:]
  if _execute.must_record_gradient():
    _attrs = (&quot;T&quot;, _op._get_attr_type(&quot;T&quot;), &quot;strides&quot;,
              _op.get_attr(&quot;strides&quot;), &quot;rates&quot;, _op.get_attr(&quot;rates&quot;),
              &quot;padding&quot;, _op.get_attr(&quot;padding&quot;))
    _inputs_flat = _op.inputs
    _execute.record_gradient(
        &quot;Dilation2D&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result

Dilation2D = tf_export(&quot;raw_ops.Dilation2D&quot;)(_ops.to_raw_op(dilation2d))


def dilation2d_eager_fallback(input, filter, strides, rates, padding, name, ctx):
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter], ctx)
  (input, filter) = _inputs_T
  _inputs_flat = [input, filter]
  _attrs = (&quot;T&quot;, _attr_T, &quot;strides&quot;, strides, &quot;rates&quot;, rates, &quot;padding&quot;,
  padding)
  _result = _execute.execute(b&quot;Dilation2D&quot;, 1, inputs=_inputs_flat,
                             attrs=_attrs, ctx=ctx, name=name)
  if _execute.must_record_gradient():
    _execute.record_gradient(
        &quot;Dilation2D&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result


def dilation2d_backprop_filter(input, filter, out_backprop, strides, rates, padding, name=None):
  r&quot;&quot;&quot;Computes the gradient of morphological 2-D dilation with respect to the filter.

  Args:
    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
      4-D with shape `[batch, in_height, in_width, depth]`.
    filter: A `Tensor`. Must have the same type as `input`.
      3-D with shape `[filter_height, filter_width, depth]`.
    out_backprop: A `Tensor`. Must have the same type as `input`.
      4-D with shape `[batch, out_height, out_width, depth]`.
    strides: A list of `ints` that has length `&gt;= 4`.
      1-D of length 4. The stride of the sliding window for each dimension of
      the input tensor. Must be: `[1, stride_height, stride_width, 1]`.
    rates: A list of `ints` that has length `&gt;= 4`.
      1-D of length 4. The input stride for atrous morphological dilation.
      Must be: `[1, rate_height, rate_width, 1]`.
    padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;`.
      The type of padding algorithm to use.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  &quot;&quot;&quot;
  _ctx = _context._context or _context.context()
  tld = _ctx._thread_local_data
  if tld.is_eager:
    try:
      _result = pywrap_tfe.TFE_Py_FastPathExecute(
        _ctx._context_handle, tld.device_name, &quot;Dilation2DBackpropFilter&quot;,
        name, tld.op_callbacks, input, filter, out_backprop, &quot;strides&quot;,
        strides, &quot;rates&quot;, rates, &quot;padding&quot;, padding)
      return _result
    except _core._FallbackException:
      try:
        return dilation2d_backprop_filter_eager_fallback(
            input, filter, out_backprop, strides=strides, rates=rates,
            padding=padding, name=name, ctx=_ctx)
      except _core._SymbolicException:
        pass  # Add nodes to the TensorFlow graph.
    except _core._NotOkStatusException as e:
      _ops.raise_from_not_ok_status(e, name)
  # Add nodes to the TensorFlow graph.
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d_backprop_filter' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d_backprop_filter' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _, _, _op, _outputs = _op_def_library._apply_op_helper(
        &quot;Dilation2DBackpropFilter&quot;, input=input, filter=filter,
                                    out_backprop=out_backprop,
                                    strides=strides, rates=rates,
                                    padding=padding, name=name)
  _result = _outputs[:]
  if _execute.must_record_gradient():
    _attrs = (&quot;T&quot;, _op._get_attr_type(&quot;T&quot;), &quot;strides&quot;,
              _op.get_attr(&quot;strides&quot;), &quot;rates&quot;, _op.get_attr(&quot;rates&quot;),
              &quot;padding&quot;, _op.get_attr(&quot;padding&quot;))
    _inputs_flat = _op.inputs
    _execute.record_gradient(
        &quot;Dilation2DBackpropFilter&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result

Dilation2DBackpropFilter = tf_export(&quot;raw_ops.Dilation2DBackpropFilter&quot;)(_ops.to_raw_op(dilation2d_backprop_filter))


def dilation2d_backprop_filter_eager_fallback(input, filter, out_backprop, strides, rates, padding, name, ctx):
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d_backprop_filter' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d_backprop_filter' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter, out_backprop], ctx)
  (input, filter, out_backprop) = _inputs_T
  _inputs_flat = [input, filter, out_backprop]
  _attrs = (&quot;T&quot;, _attr_T, &quot;strides&quot;, strides, &quot;rates&quot;, rates, &quot;padding&quot;,
  padding)
  _result = _execute.execute(b&quot;Dilation2DBackpropFilter&quot;, 1,
                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,
                             name=name)
  if _execute.must_record_gradient():
    _execute.record_gradient(
        &quot;Dilation2DBackpropFilter&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result


def dilation2d_backprop_input(input, filter, out_backprop, strides, rates, padding, name=None):
  r&quot;&quot;&quot;Computes the gradient of morphological 2-D dilation with respect to the input.

  Args:
    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
      4-D with shape `[batch, in_height, in_width, depth]`.
    filter: A `Tensor`. Must have the same type as `input`.
      3-D with shape `[filter_height, filter_width, depth]`.
    out_backprop: A `Tensor`. Must have the same type as `input`.
      4-D with shape `[batch, out_height, out_width, depth]`.
    strides: A list of `ints` that has length `&gt;= 4`.
      1-D of length 4. The stride of the sliding window for each dimension of
      the input tensor. Must be: `[1, stride_height, stride_width, 1]`.
    rates: A list of `ints` that has length `&gt;= 4`.
      1-D of length 4. The input stride for atrous morphological dilation.
      Must be: `[1, rate_height, rate_width, 1]`.
    padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;`.
      The type of padding algorithm to use.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  &quot;&quot;&quot;
  _ctx = _context._context or _context.context()
  tld = _ctx._thread_local_data
  if tld.is_eager:
    try:
      _result = pywrap_tfe.TFE_Py_FastPathExecute(
        _ctx._context_handle, tld.device_name, &quot;Dilation2DBackpropInput&quot;,
        name, tld.op_callbacks, input, filter, out_backprop, &quot;strides&quot;,
        strides, &quot;rates&quot;, rates, &quot;padding&quot;, padding)
      return _result
    except _core._FallbackException:
      try:
        return dilation2d_backprop_input_eager_fallback(
            input, filter, out_backprop, strides=strides, rates=rates,
            padding=padding, name=name, ctx=_ctx)
      except _core._SymbolicException:
        pass  # Add nodes to the TensorFlow graph.
    except _core._NotOkStatusException as e:
      _ops.raise_from_not_ok_status(e, name)
  # Add nodes to the TensorFlow graph.
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d_backprop_input' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d_backprop_input' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _, _, _op, _outputs = _op_def_library._apply_op_helper(
        &quot;Dilation2DBackpropInput&quot;, input=input, filter=filter,
                                   out_backprop=out_backprop, strides=strides,
                                   rates=rates, padding=padding, name=name)
  _result = _outputs[:]
  if _execute.must_record_gradient():
    _attrs = (&quot;T&quot;, _op._get_attr_type(&quot;T&quot;), &quot;strides&quot;,
              _op.get_attr(&quot;strides&quot;), &quot;rates&quot;, _op.get_attr(&quot;rates&quot;),
              &quot;padding&quot;, _op.get_attr(&quot;padding&quot;))
    _inputs_flat = _op.inputs
    _execute.record_gradient(
        &quot;Dilation2DBackpropInput&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result

Dilation2DBackpropInput = tf_export(&quot;raw_ops.Dilation2DBackpropInput&quot;)(_ops.to_raw_op(dilation2d_backprop_input))


def dilation2d_backprop_input_eager_fallback(input, filter, out_backprop, strides, rates, padding, name, ctx):
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d_backprop_input' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d_backprop_input' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter, out_backprop], ctx)
  (input, filter, out_backprop) = _inputs_T
  _inputs_flat = [input, filter, out_backprop]
  _attrs = (&quot;T&quot;, _attr_T, &quot;strides&quot;, strides, &quot;rates&quot;, rates, &quot;padding&quot;,
  padding)
  _result = _execute.execute(b&quot;Dilation2DBackpropInput&quot;, 1,
                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,
                             name=name)
  if _execute.must_record_gradient():
    _execute.record_gradient(
        &quot;Dilation2DBackpropInput&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result
</code></pre>
<p>Thank you for your time.</p>
","I want to understand how the tf.nn.dilation2d is working and how the ""filters"", which refers to a structural element are learned. The official documentation is available here : https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d The documentation didn't reference a scientific paper, and I found a lot of different idea in scientific literature, about morphological filters in deep learnign. Here just some examples : I searched into the code (https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/nn_ops.py#L327-L392), but the tf.nn.dilation2d only call gen_nn_ops.dilation2d I searched into gen_nn_ops.py (which I found inside my python lib folder, probably because it's generated from somewhere else) but I didn't understand what the code is doing. Thank you for your time.",https://stackoverflow.com/questions/63482945,14130094,Documentation Replicability
63600026,What are mixed layers in tf.keras.applications.InceptionV3?,"<p>I am currently trying to understand the architecture of Inseption v3 as implemented in <code>tf.keras.applications.InceptionV3</code>.</p>
<p>I am looking at the list of names in the model's layers:</p>
<pre class=""lang-py prettyprint-override""><code>print([layer.name for layer in model.layers])

#Outputs:
['input_1',
 'conv2d',
 'batch_normalization',
 'activation',
 'conv2d_1',
 'batch_normalization_1',
 'activation_1',
 'conv2d_2',
 ...
]

</code></pre>
<p>I understand how batch normalization, pooling and conv layers transform inputs, but deeper we have layers named <code>mixed1, mixed2, ...</code> and so on. I am trying to understand how they (mixed layers) are transforming their inputs.</p>
<p>So far, I couldn't find any information about them.
How does a mixed layer work? What does it do?</p>
","I am currently trying to understand the architecture of Inseption v3 as implemented in tf.keras.applications.InceptionV3. I am looking at the list of names in the model's layers: I understand how batch normalization, pooling and conv layers transform inputs, but deeper we have layers named mixed1, mixed2, ... and so on. I am trying to understand how they (mixed layers) are transforming their inputs. So far, I couldn't find any information about them. How does a mixed layer work? What does it do?",https://stackoverflow.com/questions/63600026,14142345,Lack of Alternative Solutions/Documentation
63715707,Does image_dataset_from_directory load all the images into memory at once?,"<p>I'm new to machine learning, and I am trying to create an image classifier, I want to load the dataset, but I want to do it in a way such that it does not take up all of my memory. Reading the tensorflow documentation, it says that iteration of a dataset happens in streaming fashion, and I am wondering if tf.keras.preprocessing.image_dataset_from_directory will load the images at once or &quot;stream&quot; it a batch at a time. If not I was thinking of making a generator to read file names one at a time and load them when the batches are ready with keras.utils.Sequence.</p>
","I'm new to machine learning, and I am trying to create an image classifier, I want to load the dataset, but I want to do it in a way such that it does not take up all of my memory. Reading the tensorflow documentation, it says that iteration of a dataset happens in streaming fashion, and I am wondering if tf.keras.preprocessing.image_dataset_from_directory will load the images at once or ""stream"" it a batch at a time. If not I was thinking of making a generator to read file names one at a time and load them when the batches are ready with keras.utils.Sequence.",https://stackoverflow.com/questions/63715707,10171841,Documentation Replication on Other Examples
63730066,How to use tf.data.Dataset with kedro?,"<p>I am using <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code></a> to prepare a streaming dataset which is used to train a tf.kears model. With <a href=""https://kedro.readthedocs.io/en/stable/index.html"" rel=""nofollow noreferrer"">kedro</a>, is there a way to create a node and return the created <code>tf.data.Dataset</code> to use it in the next training node?</p>
<p>The <a href=""https://kedro.readthedocs.io/en/stable/kedro.io.MemoryDataSet.html?highlight=memorydataset#"" rel=""nofollow noreferrer""><code>MemoryDataset</code></a> will probably not work because <code>tf.data.Dataset</code> cannot be pickled (<code>deepcopy</code> isn't possible), see also <a href=""https://stackoverflow.com/questions/56862492/how-to-save-tf-data-dataset-object"">this SO question</a>. According to <a href=""https://github.com/quantumblacklabs/kedro/issues/91"" rel=""nofollow noreferrer"">issue #91</a> the deep copy in <code>MemoryDataset</code> is done to avoid modifying the data by some other node. Can someone please elaborate a bit more on why/how this concurrent modification could happen?</p>
<p>From the <a href=""https://kedro.readthedocs.io/en/stable/kedro.io.MemoryDataSet.html#kedro.io.MemoryDataSet.__init__"" rel=""nofollow noreferrer"">docs</a>, there seems to be a <code>copy_mode = &quot;assign&quot;</code>. Would it be possible to use this option in case the data is not picklable?</p>
<p>Another solution (also mentioned in issue 91) is to use just a function to generate the streaming <code>tf.data.Dataset</code> inside the training node, without having the preceding dataset generation node. However, I am not sure what the drawbacks of this approach will be (if any). Would be greate if someone could give some examples.</p>
<p>Also, I would like to avoid storing the complete output of the streaming dataset, for example using <a href=""https://www.tensorflow.org/tutorials/load_data/tfrecord"" rel=""nofollow noreferrer""><code>tfrecords</code></a> or <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/save"" rel=""nofollow noreferrer""><code>tf.data.experimental.save</code></a> as these options would use a lot of disk storage.</p>
<p>Is there a way to pass just the created <code>tf.data.Dataset</code> object to use it for the training node?</p>
","I am using tf.data.Dataset to prepare a streaming dataset which is used to train a tf.kears model. With kedro, is there a way to create a node and return the created tf.data.Dataset to use it in the next training node? The MemoryDataset will probably not work because tf.data.Dataset cannot be pickled (deepcopy isn't possible), see also this SO question. According to issue #91 the deep copy in MemoryDataset is done to avoid modifying the data by some other node. Can someone please elaborate a bit more on why/how this concurrent modification could happen? From the docs, there seems to be a copy_mode = ""assign"". Would it be possible to use this option in case the data is not picklable? Another solution (also mentioned in issue 91) is to use just a function to generate the streaming tf.data.Dataset inside the training node, without having the preceding dataset generation node. However, I am not sure what the drawbacks of this approach will be (if any). Would be greate if someone could give some examples. Also, I would like to avoid storing the complete output of the streaming dataset, for example using tfrecords or tf.data.experimental.save as these options would use a lot of disk storage. Is there a way to pass just the created tf.data.Dataset object to use it for the training node?",https://stackoverflow.com/questions/63730066,2137370,Inadequate Examples
63851431,How to Augment the Training Set using the tf.keras.utils.Sequence API?,"<p>TensorFlow documentation have the following example that can illustrate how to create a batch generator to feed a training set in batches to a model when the training set is too large to fit in memory:</p>
<pre class=""lang-py prettyprint-override""><code>from skimage.io import imread
from skimage.transform import resize
import tensorflow as tf
import numpy as np
import math

# Here, `x_set` is list of path to the images
# and `y_set` are the associated classes.

class CIFAR10Sequence(tf.keras.utils.Sequence):

    def __init__(self, x_set, y_set, batch_size):
        self.x, self.y = x_set, y_set
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) *
        self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) *
        self.batch_size]

        return np.array([
            resize(imread(file_name), (200, 200))
               for file_name in batch_x]), np.array(batch_y)
</code></pre>
<p>My intention is to further increase the diversity of the training set by rotating each image 3x by 90º. In each Epoch of the training process, the model would first be fed with the &quot;0º training set&quot; and next with the 90º, 180º and 270º rotating sets, respectively.</p>
<p>How can I modify the previous piece of code to perform this operation inside the <code>CIFAR10Sequence()</code> data generator?</p>
<p>Please don't use <code>tf.keras.preprocessing.image.ImageDataGenerator()</code> so that the answer does not lose its generality for another type of similar problems that are of a different nature.</p>
<p>NB: The idea would be to create the new data &quot;in real time&quot; as the model is fed instead of creating (in advance) and storing on disk a new and augmented training set bigger than the original one to be used later (also in batches) during the training process of the model.</p>
<p>Thx in advance</p>
","TensorFlow documentation have the following example that can illustrate how to create a batch generator to feed a training set in batches to a model when the training set is too large to fit in memory: My intention is to further increase the diversity of the training set by rotating each image 3x by 90º. In each Epoch of the training process, the model would first be fed with the ""0º training set"" and next with the 90º, 180º and 270º rotating sets, respectively. How can I modify the previous piece of code to perform this operation inside the CIFAR10Sequence() data generator? Please don't use tf.keras.preprocessing.image.ImageDataGenerator() so that the answer does not lose its generality for another type of similar problems that are of a different nature. NB: The idea would be to create the new data ""in real time"" as the model is fed instead of creating (in advance) and storing on disk a new and augmented training set bigger than the original one to be used later (also in batches) during the training process of the model. Thx in advance",https://stackoverflow.com/questions/63851431,14230555,Documentation Replication on Other Examples
63919438,TensorFlow keras model fit() parameters steps_per_epoch and epochs behavior on train set,"<p>I'm using a tf.data dataset containing my training data consisting of (lets say) 100k images.
I'm also using a tf.data dataset containing my validation set.
Since an epoch of all 100k images takes quite long (in my case approximately one hour) before I get any feedback on performance on the validation set, I set the <code>steps_per_epoch</code> parameter in tf.keras.Model <code>fit()</code> to <code>10000</code>.
Using a batch size of 1 this results into having 10 validation scores when reaching 100k of images.
In order to complete one epoch of 100k images of my entire training dataset, I set the <code>epochs</code> parameter to <code>10</code></p>
<p>However, I'm not sure if using <code>steps_per_epoch</code> and <code>epochs</code> this way has any other consequences. Is it correct to use these parameters in order to get more frequent feedback on performance?
And also a more specific question, does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'?
I already dug into the <a href=""https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">TensorFlow docs</a> and read several different stack overflow questions, but I couldn't find anything conclusive to answer my own question. Hope you can help!</p>
<p>Tensorflow version I'm using is 2.2.0.</p>
","I'm using a tf.data dataset containing my training data consisting of (lets say) 100k images. I'm also using a tf.data dataset containing my validation set. Since an epoch of all 100k images takes quite long (in my case approximately one hour) before I get any feedback on performance on the validation set, I set the steps_per_epoch parameter in tf.keras.Model fit() to 10000. Using a batch size of 1 this results into having 10 validation scores when reaching 100k of images. In order to complete one epoch of 100k images of my entire training dataset, I set the epochs parameter to 10 However, I'm not sure if using steps_per_epoch and epochs this way has any other consequences. Is it correct to use these parameters in order to get more frequent feedback on performance? And also a more specific question, does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'? I already dug into the TensorFlow docs and read several different stack overflow questions, but I couldn't find anything conclusive to answer my own question. Hope you can help! Tensorflow version I'm using is 2.2.0.",https://stackoverflow.com/questions/63919438,3908025,Inadequate Examples
63958039,How do I interpret tf.keras.Model.predict() output?,"<p>I am having trouble finding the documentation I need on this. To summarize the issue, I have trained a tf.keras model using two classes of images, labeled as '0' or '1'. I now want to use this model to predict whether new images are a '0' or '1'. My question is as follows: <code>model.predict()</code> returns a number between 1 and 0, but I can't seem to find what exactly this is. Is it correct to say that this is it's prediction (ie, closer to 1 means the image is likely a 1, and closer to 0 means the image is likely a 0)? Or is there something else going on here. I have included the code, and some output, below. In this case, is <code>pred</code> the probability the image is a 1, and <code>1 - pred</code> the probability the image is a 0?</p>
<p>Thanks for any and all help.</p>
<pre><code>for img_path in test_filenames:
   img = tf.keras.preprocessing.image.load_img(img_path, target_size=(IMAGE_SIZE,IMAGE_SIZE))
   img_array = tf.keras.preprocessing.image.img_to_array(img)
   img_array = tf.expand_dims(img_array, 0)
   pred = model.predict(img_array)
   print(pred)
</code></pre>
<p>Returns</p>
<pre><code>[[0.8361757]]
[[0.26765466]]
[[0.2722953]]
[[0.81938094]]
[[0.24995388]]
[[0.45974937]]
</code></pre>
","I am having trouble finding the documentation I need on this. To summarize the issue, I have trained a tf.keras model using two classes of images, labeled as '0' or '1'. I now want to use this model to predict whether new images are a '0' or '1'. My question is as follows: model.predict() returns a number between 1 and 0, but I can't seem to find what exactly this is. Is it correct to say that this is it's prediction (ie, closer to 1 means the image is likely a 1, and closer to 0 means the image is likely a 0)? Or is there something else going on here. I have included the code, and some output, below. In this case, is pred the probability the image is a 1, and 1 - pred the probability the image is a 0? Thanks for any and all help. Returns",https://stackoverflow.com/questions/63958039,14301246,Lack of Alternative Solutions/Documentation
64081367,Slicing a tensor with a tensor of indices and tf.gather,"<p>I am trying to slice a tensor with a indices tensor. For this purpose I am trying to use <code>tf.gather</code>.
However, I am having a hard time understanding the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer"">documentation</a> and don't get it to work as I would expect it to:</p>
<p>I have two tensors. An <code>activations</code> tensor with a shape of <code>[1,240,4]</code> and an <code>ids</code> tensor with the shape <code>[1,1,120]</code>. I want to slice the second dimension of the <code>activations</code> tensor with the indices provided in the third dimension of the <code>ids</code> tensor:</p>
<pre><code>downsampled_activations = tf.gather(activations, ids, axis=1)
</code></pre>
<p>I have given it the <code>axis=1</code> option since that is the axis in the <code>activations</code> tensor I want to slice.</p>
<p>However, this does not render the expected result and only gives me the following error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0,0,1] = 1 is not in [0, 1)
</code></pre>
<p>I have tried various combinations of the <code>axis</code> and <code>batch_dims</code> options, but to no avail so far and the documentation doesn't really help me on my path. Anybody care to explain the parameters in more detail or on the example above would be very helpful!</p>
<p><strong>Edit:</strong>
The IDs are precomputed before runtime and come in through an input pipeline as such:</p>
<pre><code>features = tf.io.parse_single_example(
            serialized_example,
            features={ 'featureIDs': tf.io.FixedLenFeature([], tf.string)}
</code></pre>
<p>They are then reshaped into the previous format:</p>
<pre><code>feature_ids_raw = tf.decode_raw(features['featureIDs'], tf.int32)
feature_ids_shape = tf.stack([batch_size, (num_neighbours * 4)])
feature_ids = tf.reshape(feature_ids_raw, feature_ids_shape)
feature_ids = tf.expand_dims(feature_ids, 0)
</code></pre>
<p>Afterwards they have the previously mentioned shape (<code>batch_size = 1</code> and <code>num_neighbours = 30</code> -&gt; <code>[1,1,120]</code>) and I want to use them to slice the <code>activations</code> tensor.</p>
<p><strong>Edit2:</strong> I would like the output to be <code>[1,120,4]</code>. (So I would like to gather the entries along the second dimension of the <code>activations</code> tensor in accordance with the IDs stored in my <code>ids</code> tensor.)</p>
","I am trying to slice a tensor with a indices tensor. For this purpose I am trying to use tf.gather. However, I am having a hard time understanding the documentation and don't get it to work as I would expect it to: I have two tensors. An activations tensor with a shape of [1,240,4] and an ids tensor with the shape [1,1,120]. I want to slice the second dimension of the activations tensor with the indices provided in the third dimension of the ids tensor: I have given it the axis=1 option since that is the axis in the activations tensor I want to slice. However, this does not render the expected result and only gives me the following error: I have tried various combinations of the axis and batch_dims options, but to no avail so far and the documentation doesn't really help me on my path. Anybody care to explain the parameters in more detail or on the example above would be very helpful! Edit: The IDs are precomputed before runtime and come in through an input pipeline as such: They are then reshaped into the previous format: Afterwards they have the previously mentioned shape (batch_size = 1 and num_neighbours = 30 -&gt; [1,1,120]) and I want to use them to slice the activations tensor. Edit2: I would like the output to be [1,120,4]. (So I would like to gather the entries along the second dimension of the activations tensor in accordance with the IDs stored in my ids tensor.)",https://stackoverflow.com/questions/64081367,8183516,Documentation Ambiguity
64100466,How to use tf.Dataset in Keras model.fit without specifying targets?,"<p>I want to use an AutoEncoder model with Keras functional API. Also I want to use <code>tf.data.Dataset</code> as an input pipeline for the model. However, there is limitation that I can pass the dataset to the <code>keras.model.fit</code> only with tuple <code>(inputs, targets)</code> accroding to the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">docs</a>:</p>
<blockquote>
<p>Input data. It could be: A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights).</p>
</blockquote>
<p>So here is the question: can I pass the <code>tf.data.Dataset</code> without repeating inputs like that <code>(inputs, inputs)</code> and more like <code>(inputs, None)</code>. And if I can't, will the repeated inputs double the GPU memory for my model?</p>
","I want to use an AutoEncoder model with Keras functional API. Also I want to use tf.data.Dataset as an input pipeline for the model. However, there is limitation that I can pass the dataset to the keras.model.fit only with tuple (inputs, targets) accroding to the docs: So here is the question: can I pass the tf.data.Dataset without repeating inputs like that (inputs, inputs) and more like (inputs, None). And if I can't, will the repeated inputs double the GPU memory for my model?",https://stackoverflow.com/questions/64100466,9048235,Documentation Replication on Other Examples
64119612,Join ragged character tensor,"<p>I have a ragged tensor of characters (copy/pastable code to reproduce):</p>
<pre class=""lang-py prettyprint-override""><code>flat_values = [
    b'7', b'2', b'1', b'0', b'4', b'1', b'4', b'5', b'0', b'6', b'0',
    b'1', b'5', b'7', b'8', b'4', b'6', b'6', b'5', b'4', b'0', b'7',
    b'4', b'0', b'1', b'3', b'1', b'3', b'4', b'7', b'2', b'7', b'1',
    b'2', b'1', b'1', b'7', b'4', b'2', b'3', b'5', b'1', b'2', b'4',
    b'4', b'6', b'3', b'5', b'5', b'6', b'0', b'4', b'1', b'5', b'7',
    b'8', b'3', b'7', b'4', b'6', b'4', b'3', b'0', b'7', b'0', b'2',
    b'1', b'7', b'3', b'2', b'7', b'7', b'6', b'2', b'7', b'8', b'4',
    b'7', b'3', b'6', b'1', b'3', b'6', b'3', b'1', b'4', b'1', b'7',
    b'6', b'6', b'0', b'5', b'4', b'2', b'1', b'4', b'8', b'7', b'3',
    b'7', b'4', b'4', b'2', b'5', b'4', b'7', b'6', b'7', b'0', b'5',
    b'8', b'5', b'6', b'6', b'5', b'7', b'8', b'1', b'0', b'1', b'6',
    b'4', b'6', b'7', b'3', b'1', b'7', b'1', b'8', b'2', b'0', b'2',
]

row_lengths = [
    6, 4, 4, 5, 6, 6, 6, 6, 6, 5, 5, 6,
    5, 5, 6, 5, 5, 4, 4, 4, 5, 6, 6, 6, 6,
]

x = tf.RaggedTensor.from_nested_row_lengths(
    flat_values,
    (row_lengths,),
)
</code></pre>
<p>I want to join the rows as strings, but I would like to do it in the graph. I can accomplish this by evaluating the tensor:</p>
<pre><code>&gt;&gt;&gt; [''.join([c.decode() for c in i]) for i in x.to_list()]
['721041',
 '4506',
 '0157',
 '84665',
 '407401',
 '313472',
 '712117',
 '423512',
 '446355',
 '60415',
 '78374',
 '643070',
 '21732',
 '77627',
 '847361',
 '36314',
 '17660',
 '5421',
 '4873',
 '7442',
 '54767',
 '058566',
 '578101',
 '646731',
 '718202']
</code></pre>
<p>But as this is the output of my network (trained in graph mode) I would like to be able to express this operation in tensorflow so that it can be evaluated in validation steps. Two things I've tried that do not work:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; tf.strings.join(x)
InvalidArgumentError: Input shapes do not match: [6] vs. [4] [Op:StringJoin]

&gt;&gt;&gt; tf.ragged.map_flat_values(tf.strings.join, x)
ValueError: Shape () must have rank at least 1
</code></pre>
<p>Frustratingly, the <a href=""https://tensorflow.google.cn/api_docs/python/tf/strings/join#used-in-the-notebooks"" rel=""nofollow noreferrer"">documentation for <code>tf.strings.join</code></a> mentions ragged tensors but does not give an example of them. What am I missing? It seems there should be an obvious solution to this.</p>
","I have a ragged tensor of characters (copy/pastable code to reproduce): I want to join the rows as strings, but I would like to do it in the graph. I can accomplish this by evaluating the tensor: But as this is the output of my network (trained in graph mode) I would like to be able to express this operation in tensorflow so that it can be evaluated in validation steps. Two things I've tried that do not work: Frustratingly, the documentation for tf.strings.join mentions ragged tensors but does not give an example of them. What am I missing? It seems there should be an obvious solution to this.",https://stackoverflow.com/questions/64119612,3015734,Inadequate Examples
64326029,Load tensorflow images and create patches,"<p>I am using <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer"">image_dataset_from_directory</a> to load a very large RGB imagery dataset from disk into a <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">Dataset</a>. For example,</p>
<pre><code>dataset = tf.keras.preprocessing.image_dataset_from_directory(
    &lt;directory&gt;,
    label_mode=None,
    seed=1,
    subset='training',
    validation_split=0.1)
</code></pre>
<p>The Dataset has, say, 100000 images grouped into batches of size 32 yielding a <code>tf.data.Dataset</code> with spec <code>(batch=32, width=256, height=256, channels=3)</code></p>
<p>I would like to extract patches from the images to create a new <code>tf.data.Dataset</code> with image spatial dimensions of, say, 64x64.</p>
<p>Therefore, I would like to create a new Dataset with 400000 patches still in batches of 32 with a <code>tf.data.Dataset</code> with spec <code>(batch=32, width=64, height=64, channels=3)</code></p>
<p>I've looked at the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window"" rel=""nofollow noreferrer"">window</a> method and the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/extract_patches"" rel=""nofollow noreferrer"">extract_patches</a> function but it's not clear from the documentation how to use them to create a new Dataset I need to start training on the patches. The <code>window</code> seems to be geared toward 1D tensors and the <code>extract_patches</code> seems to work with arrays and not with Datasets.</p>
<p>Any suggestions on how to accomplish this?</p>
<p>UPDATE:</p>
<p>Just to clarify my needs. I am trying to avoid manually creating the patches on disk. One, that would be untenable disk wise. Two, the patch size is not fixed. The experiments will be conducted over several patch sizes. So, I do not want to manually perform the patch creation either on disk or manually load the images in memory and perform the patching. I would prefer to have tensorflow handle the patch creation as part of the pipeline workflow to minimize disk and memory usage.</p>
","I am using image_dataset_from_directory to load a very large RGB imagery dataset from disk into a Dataset. For example, The Dataset has, say, 100000 images grouped into batches of size 32 yielding a tf.data.Dataset with spec (batch=32, width=256, height=256, channels=3) I would like to extract patches from the images to create a new tf.data.Dataset with image spatial dimensions of, say, 64x64. Therefore, I would like to create a new Dataset with 400000 patches still in batches of 32 with a tf.data.Dataset with spec (batch=32, width=64, height=64, channels=3) I've looked at the window method and the extract_patches function but it's not clear from the documentation how to use them to create a new Dataset I need to start training on the patches. The window seems to be geared toward 1D tensors and the extract_patches seems to work with arrays and not with Datasets. Any suggestions on how to accomplish this? UPDATE: Just to clarify my needs. I am trying to avoid manually creating the patches on disk. One, that would be untenable disk wise. Two, the patch size is not fixed. The experiments will be conducted over several patch sizes. So, I do not want to manually perform the patch creation either on disk or manually load the images in memory and perform the patching. I would prefer to have tensorflow handle the patch creation as part of the pipeline workflow to minimize disk and memory usage.",https://stackoverflow.com/questions/64326029,14438185,Documentation Replication on Other Examples
64356209,How does Model.fit() method's shuffle deals with Batches when using a tf.data.Dataset?,"<p>I am using tensorflow 2.</p>
<p>When using the <code>Model.fit()</code> method with a <code>tf.data.Dataset</code>, the argument '<code>batch_size</code>' is ignored. Thus to train my model on batches, I have to first change my dataset of samples into a dataset of batches of samples by calling <code>tf.data.Dataset.batch(batch_size)</code>.</p>
<p>Then, after reading the documentation, I don't understand clearly how the <code>.fit()</code> method will shuffle my dataset at each epoch.</p>
<p><strong>Since my dataset is a dataset of batches, will it shuffle the batches among each other</strong> (the batches remain unchanged) <strong>? Or will it shuffle all the samples and then regroup them into new batches</strong> (which is the desired behaviour) <strong>?</strong></p>
<p>Thanks a lot for your help.</p>
","I am using tensorflow 2. When using the Model.fit() method with a tf.data.Dataset, the argument 'batch_size' is ignored. Thus to train my model on batches, I have to first change my dataset of samples into a dataset of batches of samples by calling tf.data.Dataset.batch(batch_size). Then, after reading the documentation, I don't understand clearly how the .fit() method will shuffle my dataset at each epoch. Since my dataset is a dataset of batches, will it shuffle the batches among each other (the batches remain unchanged) ? Or will it shuffle all the samples and then regroup them into new batches (which is the desired behaviour) ? Thanks a lot for your help.",https://stackoverflow.com/questions/64356209,14449900,Documentation Replicability
64424397,Keras - Custom layer with multiple inputs,"<p>I would like to implement a custom <code>tf.keras</code> layer called <code>MyLayer</code> which has three inputs and contains a sub layer which in turn has three inputs, like in the figure below:</p>
<p><a href=""https://i.stack.imgur.com/Um7yn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Um7yn.jpg"" alt=""MyLayer"" /></a></p>
<p>I assume that the right thing to do would be to create a <code>MyLayer</code> class that extends <code>tf.keras.layers.Layer</code> and implement the <code>__init__</code>, <code>build</code> and <code>call</code> methods, as mentioned in the <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models"" rel=""nofollow noreferrer"">official documentation</a>.</p>
<p>Now, the examples provided in the documentation are relative to pretty simple layers that are composed of several sublayers connected in a sequential manner, that is one after the other. For instance, the <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_are_recursively_composable"" rel=""nofollow noreferrer""><code>MLPBlock</code></a> layer consists of 3 linear layers ordered sequentially.</p>
<p>In general, however, sublayers are not ordered sequentially, but can form branches. This suggests that those layers could be run in parallel, since they are not connected to one another.
Going back to the custom layer I would like to implement, you can see that <code>Layer1</code>, <code>Layer2</code> and <code>Layer3</code> could be run in parallel. Once their outputs are computed, they can be fed to <code>Layer4</code>. The point is: how do I run them in parallel? I couldn't find any &quot;ParallelCombinator&quot; or things like that among the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers"" rel=""nofollow noreferrer"">available Keras layers</a>.</p>
<p>If I were to follow the examples provided in the documentation, I would write something along these lines:</p>
<pre class=""lang-py prettyprint-override""><code>class MyLayer(keras.layers.Layer):
    def __init__(self, ...):
        super(MyLayer, self).__init__()
        self.layer_1 = Layer1(...)
        self.layer_2 = Layer2(...)
        self.layer_3 = Layer3(...)
        self.layer_4 = Layer4(...)

    def call(self, inputs):
        tmp_1 = self.layer_1(inputs[0])
        tmp_2 = self.layer_2(inputs[1])
        tmp_3 = self.layer_3(inputs[2])
        return self.layer4([tmp_1, tmp_2, tmp_3])
</code></pre>
<p>This, however, would imply that <code>Layer1</code>, <code>Layer2</code> and <code>Layer3</code> are run sequentially, not in parallel.</p>
<p>One possible solution that I came up with involves structuring <code>MyLayer</code> as a <code>tf.keras.Model</code> built with Keras's functional API rather than as a subclass of <code>tf.keras.Layer</code>, like so:</p>
<pre class=""lang-py prettyprint-override""><code>def MyLayer(...):
    input_1 = tf.keras.layers.Input(...)
    input_2 = tf.keras.layers.Input(...)
    input_3 = tf.keras.layers.Input(...)

    layer_1 = Layer1(...)(input_1)
    layer_2 = Layer2(...)(input_2)
    layer_3 = Layer3(...)(input_3)
    output_1 = Layer4(...)([layer_1, layer_2, layer_3])

    return tf.keras.Model(inputs=[input_1, input_2, input_3], outputs=output_1)

if __name__ == '__main__':
    my_layer = MyLayer(...)
    input_1 = ...
    input_2 = ...
    input_3 = ...
    output = my_layer([input_1, input_2, input_3])
</code></pre>
<p>The reason why I think this would work is that I assume that when I feed some inputs to a <code>tf.keras.Model</code>, as in <code>output = my_layer([input_1, input_2, input_3])</code>, the layers that <em>can</em> be run in parallel are effectively run in parallel (or are they?). This solution, however, feels like a hack to me, as <code>MyLayer</code> is supposed to be a layer, <em>not</em> a model. In fact, a <code>tf.keras.Model</code> instance exposes methods like <code>fit(...)</code> that aren't meant to be called on a layer.</p>
<p>Does anybody know what's the best approach to implement <code>MyLayer</code>?</p>
","I would like to implement a custom tf.keras layer called MyLayer which has three inputs and contains a sub layer which in turn has three inputs, like in the figure below: I assume that the right thing to do would be to create a MyLayer class that extends tf.keras.layers.Layer and implement the __init__, build and call methods, as mentioned in the official documentation. Now, the examples provided in the documentation are relative to pretty simple layers that are composed of several sublayers connected in a sequential manner, that is one after the other. For instance, the MLPBlock layer consists of 3 linear layers ordered sequentially. In general, however, sublayers are not ordered sequentially, but can form branches. This suggests that those layers could be run in parallel, since they are not connected to one another. Going back to the custom layer I would like to implement, you can see that Layer1, Layer2 and Layer3 could be run in parallel. Once their outputs are computed, they can be fed to Layer4. The point is: how do I run them in parallel? I couldn't find any ""ParallelCombinator"" or things like that among the available Keras layers. If I were to follow the examples provided in the documentation, I would write something along these lines: This, however, would imply that Layer1, Layer2 and Layer3 are run sequentially, not in parallel. One possible solution that I came up with involves structuring MyLayer as a tf.keras.Model built with Keras's functional API rather than as a subclass of tf.keras.Layer, like so: The reason why I think this would work is that I assume that when I feed some inputs to a tf.keras.Model, as in output = my_layer([input_1, input_2, input_3]), the layers that can be run in parallel are effectively run in parallel (or are they?). This solution, however, feels like a hack to me, as MyLayer is supposed to be a layer, not a model. In fact, a tf.keras.Model instance exposes methods like fit(...) that aren't meant to be called on a layer. Does anybody know what's the best approach to implement MyLayer?",https://stackoverflow.com/questions/64424397,9079812,Documentation Replication on Other Examples
64552543,Tensorflow 2.2.0 :- WARNING:tensorflow:Gradients do not exist for variables when minimizing the loss,"<p>After implementing Custom Loss class as per the tensorflow api documentation and when invoking model.fit , facing this warning alongwith below error:- This is reference link on github and they have asked to raise here in stack overflow.https://github.com/tensorflow/tensorflow/issues/42542# TypeError: An op outside of the function building code is being passed a &quot;Graph&quot; tensor. It is possible to have Graph tensors leak out of the function building context by including a tf.init_scope in your function building code. For example, the following function will fail:</p>
<hr />
<blockquote>
<p>@tf.function   def has_init_scope():
my_constant = tf.constant(1.)
with tf.init_scope():
added = my_constant * 2 The graph tensor has name: ident33/Relu_5:0</p>
</blockquote>
","After implementing Custom Loss class as per the tensorflow api documentation and when invoking model.fit , facing this warning alongwith below error:- This is reference link on github and they have asked to raise here in stack overflow.https://github.com/tensorflow/tensorflow/issues/42542# TypeError: An op outside of the function building code is being passed a ""Graph"" tensor. It is possible to have Graph tensors leak out of the function building context by including a tf.init_scope in your function building code. For example, the following function will fail:",https://stackoverflow.com/questions/64552543,6926418,Documentation Replication on Other Examples
64611137,port TensorFlow 1 code to TensorFlow 2 (model learning process without sess.run),"<p>I have this piece of tf1 code, which was taken from nice book &quot;Deep Learning&quot; by S. Nikolenko.</p>
<p>It's a simple linear regression that learns <code>k</code> and <code>b</code> to 2 and 1 respectively.</p>
<pre><code>%tensorflow_version 1.x

import numpy as np,tensorflow as tf
import pandas as pd

n_samples, batch_size, num_steps = 1000, 100, 20000 #set learning constants
X_data = np.random.uniform(1, 10, (n_samples, 1)) #generate array x from 1 to 10 of shape (1000,1)
print(X_data.shape)
y_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1)) #generate right answer and add noise to it (to make it scatter)

X = tf.placeholder(tf.float32, shape=(batch_size, 1)) #defining placeholders to put into session.run
y = tf.placeholder(tf.float32, shape=(batch_size, 1))

with tf.variable_scope('linear-regression'):
  k = tf.Variable(tf.random_normal((1, 1)), name='slope') #defining 2 variables with shape (1,1)
  b = tf.Variable(tf.zeros((1,)), name='bias') # and (1,)
  print(k.shape,b.shape)

y_pred = tf.matmul(X, k) + b # all predicted y in batch, represents linear formula k*x + b
loss = tf.reduce_sum((y - y_pred) ** 2)  # mean square
optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)
display_step = 100

with tf.Session() as sess:
  sess.run(tf.initialize_variables([k,b]))
  for i in range(num_steps):
    indices = np.random.choice(n_samples, batch_size) # taking random indices
    X_batch, y_batch = X_data[indices], y_data[indices] # taking x and y from generated examples
    _, loss_val, k_val, b_val = sess.run([optimizer, loss, k, b ],
      feed_dict = { X : X_batch, y : y_batch })
    if (i+1) % display_step == 0:
      print('Epoch %d: %.8f, k=%.4f, b=%.4f' %
        (i+1, loss_val, k_val, b_val))

</code></pre>
<p>I'm striving to port it on TensorFlow 2</p>
<p>And for long time I can't wrap my head what should I use instead of <code>sess.run()</code> and <code>feed_dict</code>, which doing magic behind the scenes, official documentation go into to details with writing model class and so on, but I'm want to keep this as flat as possible.</p>
<p>Also it's suggested to calculate derivatives with <code>tf.GradientTape</code>, but I'm struggling with applying it right to my example</p>
<pre><code>%tensorflow_version 2.x

import numpy as np,tensorflow as tf
import pandas as pd

n_samples, batch_size, num_steps = 1000, 100, 200
X_data = np.random.uniform(1, 10, (n_samples, 1))
y_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1))

X = tf.Variable(tf.zeros((batch_size, 1)), dtype=tf.float32, shape=(batch_size, 1))
y = tf.Variable(tf.zeros((batch_size, 1)), dtype=tf.float32, shape=(batch_size, 1))

k = tf.Variable(tf.random.normal((1, 1)), name='slope')
b = tf.Variable(tf.zeros((1,)), name='bias')

loss = lambda: tf.reduce_sum((y - (tf.matmul(X, k) + b)) ** 2)
optimizer = tf.keras.optimizers.SGD(0.01).minimize(loss, [k, b, X, y])
display_step = 100


for i in range(num_steps):
  indices = np.random.choice(n_samples, batch_size)
  X_batch, y_batch = X_data[indices], y_data[indices]
  
</code></pre>
<p>I need SGD optimizer minimize that given loss function and learn k and b values, how can I achieve it from this point?</p>
","I have this piece of tf1 code, which was taken from nice book ""Deep Learning"" by S. Nikolenko. It's a simple linear regression that learns k and b to 2 and 1 respectively. I'm striving to port it on TensorFlow 2 And for long time I can't wrap my head what should I use instead of sess.run() and feed_dict, which doing magic behind the scenes, official documentation go into to details with writing model class and so on, but I'm want to keep this as flat as possible. Also it's suggested to calculate derivatives with tf.GradientTape, but I'm struggling with applying it right to my example I need SGD optimizer minimize that given loss function and learn k and b values, how can I achieve it from this point?",https://stackoverflow.com/questions/64611137,6634635,Documentation Replicability
64642944,Steps of tf.summary.* operations in TensorBoard are always 0,"<p>When I'm training my model with TensorFlow 2.3, I want to visualize some intermediate tensors calculated using the weight in the computation graph of my customized <code>tf.keras.layers.Layer</code>.</p>
<p>So I use <code>tf.summary.image()</code> to record these tensors and visualize them as images like this:</p>
<pre><code>class CustomizedLayer(tf.keras.layers.Layer):
    def call(self, inputs, training=None):
        # ... some code ...
        tf.summary.image(name=&quot;some_weight_map&quot;, data=some_weight_map)
        # ... some code ...
</code></pre>
<p>But in TensorBoard, no matter how many steps passed, there is only one image of step 0 shown.</p>
<p>And I tried to set the parameter <em><strong>step</strong></em> of <code>tf.summary.image()</code> to the value obtained from <code>tf.summary.experimental.get_step()</code>:</p>
<pre><code>tf.summary.image(name=&quot;weight_map&quot;, data=weight_map, step=tf.summary.experimental.get_step())
</code></pre>
<p>And update the step by calling <strong>tf.summary.experimental.set_step</strong> from a customized Callback using a tf.Variable like codes shown below:</p>
<pre><code>class SummaryCallback(tf.keras.callbacks.Callback):
def __init__(self, step_per_epoch):
    super().__init__()
    self.global_step = tf.Variable(initial_value=0, trainable=False, name=&quot;global_step&quot;)
    self.global_epoch = 0
    self.step_per_epoch = step_per_epoch
    tf.summary.experimental.set_step(self.global_step)

def on_batch_end(self, batch, logs=None):
    self.global_step = batch + self.step_per_epoch * self.global_epoch
    tf.summary.experimental.set_step(self.global_step)  
    # whether the line above is commented, calling tf.summary.experimental.get_step() in computation graph code always returns 0.
    # tf.print(self.global_step)

def on_epoch_end(self, epoch, logs=None):
    self.global_epoch += 1
</code></pre>
<p>This Callback's instance is passed in the argument <em><strong>callbacks</strong></em> in <code>model.fit()</code> function.</p>
<p>But the value <code>tf.summary.experimental.get_step()</code> returned is still 0.</p>
<p>The TensorFlow document of &quot;<code>tf.summary.experimental.set_step()</code>&quot; says:</p>
<blockquote>
<p>when using this with @tf.functions, the step value will be captured at the time the function is traced, so changes to the step outside the function will not be reflected inside the function unless using a tf.Variable step.</p>
</blockquote>
<p>Accroding to the document, I am already using a Variable to store the steps, but it's changes are still not reflected inside the function (or keras.Model).</p>
<p>Note: My code produces expected results in TensorFlow 1.x with just a simple line of <code>tf.summary.image()</code> before I migrate it to TensorFlow 2.</p>
<p>So I want to know if my approach is wrong in TensorFlow 2?</p>
<p>In TF2, how can I <strong>get training steps inside the computation graph</strong>?</p>
<p>Or there is other solution to <strong>summarize tensors (as scalar, image, etc.) inside a model in TensorFlow 2</strong>?</p>
","When I'm training my model with TensorFlow 2.3, I want to visualize some intermediate tensors calculated using the weight in the computation graph of my customized tf.keras.layers.Layer. So I use tf.summary.image() to record these tensors and visualize them as images like this: But in TensorBoard, no matter how many steps passed, there is only one image of step 0 shown. And I tried to set the parameter step of tf.summary.image() to the value obtained from tf.summary.experimental.get_step(): And update the step by calling tf.summary.experimental.set_step from a customized Callback using a tf.Variable like codes shown below: This Callback's instance is passed in the argument callbacks in model.fit() function. But the value tf.summary.experimental.get_step() returned is still 0. The TensorFlow document of ""tf.summary.experimental.set_step()"" says: Accroding to the document, I am already using a Variable to store the steps, but it's changes are still not reflected inside the function (or keras.Model). Note: My code produces expected results in TensorFlow 1.x with just a simple line of tf.summary.image() before I migrate it to TensorFlow 2. So I want to know if my approach is wrong in TensorFlow 2? In TF2, how can I get training steps inside the computation graph? Or there is other solution to summarize tensors (as scalar, image, etc.) inside a model in TensorFlow 2?",https://stackoverflow.com/questions/64642944,14562728,Documentation Replication on Other Examples
64687375,Get labels from dataset when using tensorflow image_dataset_from_directory,"<p>I wrote a simple CNN using tensorflow (v2.4) + keras in python (v3.8.3). I am trying to optimize the network, and I want more info on what it is failing to predict. I am trying to add a confusion matrix, and I need to feed tensorflow.math.confusion_matrix() the test labels.</p>
<p>My problem is that I cannot figure out how to access the labels from the dataset object created by tf.keras.preprocessing.image_dataset_from_directory()</p>
<p>My images are organized in directories having the label as the name. The documentation says the function returns a tf.data.Dataset object.</p>
<blockquote>
<pre><code>If label_mode is None, it yields float32 tensors of shape (batch_size, image_size[0], image_size[1], num_channels), encoding
</code></pre>
<p>images (see below for rules regarding num_channels).
Otherwise, it yields a tuple (images, labels), where images has shape (batch_size, image_size[0], image_size[1], num_channels), and
labels follows the format described below.</p>
</blockquote>
<p>Here is the code:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers
#import matplotlib.pyplot as plt
import numpy as np
import random

import PIL
import PIL.Image

import os
import pathlib

#load the IMAGES
dataDirectory = '/p/home/username/tensorflow/newBirds'

dataDirectory = pathlib.Path(dataDirectory)
imageCount = len(list(dataDirectory.glob('*/*.jpg')))
print('Image count: {0}\n'.format(imageCount))

#test display an image
# osprey = list(dataDirectory.glob('OSPREY/*'))
# ospreyImage = PIL.Image.open(str(osprey[random.randint(1,100)]))
# ospreyImage.show()

# nFlicker = list(dataDirectory.glob('NORTHERN FLICKER/*'))
# nFlickerImage = PIL.Image.open(str(nFlicker[random.randint(1,100)]))
# nFlickerImage.show()

#set parameters
batchSize = 32
height=224
width=224

(trainData, trainLabels) = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    validation_split=0.2,
    subset='training',
    seed=324893,
    image_size=(height,width),
    batch_size=batchSize)

testData = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    validation_split=0.2,
    subset='validation',
    seed=324893,
    image_size=(height,width),
    batch_size=batchSize)

#class names and sampling a few images
classes = trainData.class_names
testClasses = testData.class_names
#plt.figure(figsize=(10,10))
# for images, labels in trainData.take(1):
#     for i in range(9):
#         ax = plt.subplot(3, 3, i+1)
#         plt.imshow(images[i].numpy().astype(&quot;uint8&quot;))
#         plt.title(classes[labels[i]])
#         plt.axis(&quot;off&quot;)
# plt.show()

#buffer to hold the data in memory for faster performance
autotune = tf.data.experimental.AUTOTUNE
trainData = trainData.cache().shuffle(1000).prefetch(buffer_size=autotune)
testData = testData.cache().prefetch(buffer_size=autotune)

#augment the dataset with zoomed and rotated images
#use convolutional layers to maintain spatial information about the images
#use max pool layers to reduce
#flatten and then apply a dense layer to predict classes
model = tf.keras.Sequential([
    #layers.experimental.preprocessing.RandomFlip('horizontal', input_shape=(height, width, 3)),
    #layers.experimental.preprocessing.RandomRotation(0.1),
    #layers.experimental.preprocessing.RandomZoom(0.1),
    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(height, width, 3)),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(256, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    # layers.Conv2D(512, 3, padding='same', activation='relu'),
    # layers.MaxPooling2D(),
    #layers.Conv2D(1024, 3, padding='same', activation='relu'),
    #layers.MaxPooling2D(),
    #dropout prevents overtraining by not allowing each node to see each datapoint
    #layers.Dropout(0.5),
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dense(len(classes))
    ])

model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.summary()
    
epochs=2
history = model.fit(
    trainData,
    validation_data=testData,
    epochs=epochs
    )

#create confusion matrix
predictions = model.predict_classes(testData)
confusionMatrix = tf.math.confusion_matrix(labels=testClasses, predictions=predictions).numpy()
</code></pre>
<p>I have tried using (foo, foo1) = tf.keras.preprocessing.image_dataset_from_directory(dataDirectory, etc), but I get
(trainData, trainLabels) = tf.keras.preprocessing.image_dataset_from_directory(
ValueError: too many values to unpack (expected 2)</p>
<p>And if I try to return as one variable and then split it as so:</p>
<pre><code>train = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    validation_split=0.2,
    subset='training',
    seed=324893,
    image_size=(height,width),
    batch_size=batchSize)
trainData = train[0]
trainLabels = train[1]
</code></pre>
<p>I get TypeError: 'BatchDataset' object is not subscriptable</p>
<p>I can access the labels via testClasses = testData.class_names, but I get:</p>
<blockquote>
<p>2020-11-03 14:15:14.643300: W
tensorflow/core/framework/op_kernel.cc:1740] OP_REQUIRES failed at
cast_op.cc:121 : Unimplemented: Cast string to int64 is not supported
Traceback (most recent call last):   File &quot;birdFake.py&quot;, line 115, in

confusionMatrix = tf.math.confusion_matrix(labels=testClasses, predictions=predictions).numpy()   File
&quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py&quot;,
line 201, in wrapper
return target(*args, **kwargs)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/confusion_matrix.py&quot;,
line 159, in confusion_matrix
labels = math_ops.cast(labels, dtypes.int64)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py&quot;,
line 201, in wrapper
return target(*args, **kwargs)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py&quot;,
line 966, in cast
x = gen_math_ops.cast(x, base_type, name=name)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py&quot;,
line 1827, in cast
_ops.raise_from_not_ok_status(e, name)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py&quot;,
line 6862, in raise_from_not_ok_status
six.raise_from(core._status_to_exception(e.code, message), None)   File &quot;&quot;, line 3, in raise_from
tensorflow.python.framework.errors_impl.UnimplementedError: Cast
string to int64 is not supported [Op:Cast]</p>
</blockquote>
<p>I am open to any method to get those labels into the confusion matrix. Any ideas as to why what I am doing is not working would also be appreciated.</p>
<p>UPDATE: I tried the method proposed by Alexandre Catalano, and I get the following error</p>
<blockquote>
<p>Traceback (most recent call last):   File &quot;./birdFake.py&quot;, line 118,
in 
labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])   File &quot;&lt;<strong>array_function</strong> internals&gt;&quot;, line 5, in concatenate
ValueError: all the input arrays must have same number of dimensions,
but the array at index 0 has 1 dimension(s) and the array at index 1
has 0 dimension(s)</p>
</blockquote>
<p>I printed the first element of the labels array, and it is zero</p>
","I wrote a simple CNN using tensorflow (v2.4) + keras in python (v3.8.3). I am trying to optimize the network, and I want more info on what it is failing to predict. I am trying to add a confusion matrix, and I need to feed tensorflow.math.confusion_matrix() the test labels. My problem is that I cannot figure out how to access the labels from the dataset object created by tf.keras.preprocessing.image_dataset_from_directory() My images are organized in directories having the label as the name. The documentation says the function returns a tf.data.Dataset object. Here is the code: I have tried using (foo, foo1) = tf.keras.preprocessing.image_dataset_from_directory(dataDirectory, etc), but I get (trainData, trainLabels) = tf.keras.preprocessing.image_dataset_from_directory( ValueError: too many values to unpack (expected 2) And if I try to return as one variable and then split it as so: I get TypeError: 'BatchDataset' object is not subscriptable I can access the labels via testClasses = testData.class_names, but I get: I am open to any method to get those labels into the confusion matrix. Any ideas as to why what I am doing is not working would also be appreciated. UPDATE: I tried the method proposed by Alexandre Catalano, and I get the following error I printed the first element of the labels array, and it is zero",https://stackoverflow.com/questions/64687375,6419985,Documentation Replication on Other Examples
64759627,TensorFlow custom training step with different loss functions,"<h1>Background</h1>
<p>According to the <a href=""https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"" rel=""nofollow noreferrer"">TensorFlow documentation</a>, a custom training step can be performed with the following</p>
<pre><code># Fake sample data for testing
x_batch_train = tf.zeros([32, 3, 1], dtype=&quot;float32&quot;)
y_batch_train = tf.zeros([32], dtype=&quot;float32&quot;)
</code></pre>
<pre><code>loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
with tf.GradientTape() as tape:
    logits = model(x_batch_train, training=True)
    loss_value = loss_fn(y_batch_train, logits)

grads = tape.gradient(loss_value, model.trainable_weights)
optimizer.apply_gradients(zip(grads, model.trainable_weights))
</code></pre>
<br>
<p>But if I want to use a different loss function like categorical cross-entropy I would need to argmax the logits created in the gradient tape:</p>
<pre><code>loss_fn = tf.keras.lossees.get(&quot;categorical_crossentropy&quot;)
with tf.GradientTape() as tape:
    logits = model(x_batch_train, training=True)
    prediction = tf.cast(tf.argmax(logits, axis=-1), y_batch_train.dtype)
    loss_value = loss_fn(y_batch_train, prediction)

grads = tape.gradient(loss_value, model.trainable_weights)
optimizer.apply_gradients(zip(grads, model.trainable_weights))
</code></pre>
<hr>
<h1>Problem</h1>
<p>The problem with this is that the <code>tf.argmax</code> function is not differentiable, so TensorFlow wouldn't be able to compute the gradients and you would get the error:</p>
<pre><code>ValueError: No gradients provided for any variable: [...]
</code></pre>
<hr>
<p><strong>My question:</strong> Without changing the loss function how could I make the second example work?</p>
","According to the TensorFlow documentation, a custom training step can be performed with the following But if I want to use a different loss function like categorical cross-entropy I would need to argmax the logits created in the gradient tape: The problem with this is that the tf.argmax function is not differentiable, so TensorFlow wouldn't be able to compute the gradients and you would get the error: My question: Without changing the loss function how could I make the second example work?",https://stackoverflow.com/questions/64759627,,Documentation Replication on Other Examples
64769187,"Tensorflow - Interpreting the tf.estimator.ProfilerHook ""_Send"" op","<p>I have a deep CNN/RNN that I train on Google AI platform. I distribute the training on 8 GPUs using the <code>tf.distribute.MirroredStrategy</code>. I recently upgraded my runtime version from 1.13 to 1.15 and my training is more than 2x slower than before. I read that <code>tf.estimator.ProfilerHook</code> can be used to identify performance bottlenecks. So I collected the profiling information and rendered it at <code>chrome://tracing</code>. I got this</p>
<p><a href=""https://i.stack.imgur.com/l5hDZ.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/l5hDZ.jpg"" alt=""profiling screenshot"" /></a></p>
<p>A training step spends an entire 1 second on these <code>_Send</code> ops. What is this? I can't find any documentation on the op or why it's in my graph. What does this mean?</p>
",I have a deep CNN/RNN that I train on Google AI platform. I distribute the training on 8 GPUs using the tf.distribute.MirroredStrategy. I recently upgraded my runtime version from 1.13 to 1.15 and my training is more than 2x slower than before. I read that tf.estimator.ProfilerHook can be used to identify performance bottlenecks. So I collected the profiling information and rendered it at chrome://tracing. I got this A training step spends an entire 1 second on these _Send ops. What is this? I can't find any documentation on the op or why it's in my graph. What does this mean?,https://stackoverflow.com/questions/64769187,6078821,Lack of Alternative Solutions/Documentation
64826405,Tensorflow: 'axis' argument in dot product,"<p>Can someone show me the way I should use the <code>axis</code> argument in <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>?</p>
<p>I read the documentation but it was complicated and I'm still confused. I saw <a href=""https://stackoverflow.com/questions/48082900/in-tensorflow-what-is-the-argument-axis-in-the-function-tf-one-hot"">another question</a> that asks about <code>axis</code> in <code>tf.one_hot</code> and in the answers were some good insights about the matter, but that didn't help me with <code>tf.tensordot</code>. I thought you can give me some insights on this too.</p>
<p>For example, I know I can dot product a vector and a tensor like this:</p>
<pre><code>my_vector = tf.random.uniform(shape=[n])
my_tensor = tf.random.uniform(shape=[m, n])

dp = tf.tensordot(my_tensor, my_vector, 1)
</code></pre>
<p>But when I <em><strong>batch</strong></em> them and add one dimension to them to be of the shape <code>(b, n)</code> and <code>(b, m, n)</code> to obtain a <code>(b, m, 1)</code>, now I don't know how to dot product every batch.</p>
","Can someone show me the way I should use the axis argument in tf.tensordot? I read the documentation but it was complicated and I'm still confused. I saw another question that asks about axis in tf.one_hot and in the answers were some good insights about the matter, but that didn't help me with tf.tensordot. I thought you can give me some insights on this too. For example, I know I can dot product a vector and a tensor like this: But when I batch them and add one dimension to them to be of the shape (b, n) and (b, m, n) to obtain a (b, m, 1), now I don't know how to dot product every batch.",https://stackoverflow.com/questions/64826405,7339624,Documentation Ambiguity
65157852,How to mix tensorflow keras model and transformers,"<p>I am trying to import a pretrained model from Huggingface's transformers library and extend it with a few layers for classification using tensorflow keras. When I directly use transformers model (Method 1), the model trains well and reaches a validation accuracy of 0.93 after 1 epoch. However, when trying to use the model as a layer within a tf.keras model (Method 2), the model can't get above 0.32 accuracy. As far as I can tell based on the documentation, the two approaches should be equivalent. My goal is to get Method 2 working so that I can add more layers to it instead of directly using the logits produced by Huggingface's classifier head but I'm stuck at this stage.</p>
<pre><code>import tensorflow as tf

from transformers import TFRobertaForSequenceClassification
</code></pre>
<p>Method 1:</p>
<pre><code>model = TFRobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;, num_labels=6)
</code></pre>
<p>Method 2:</p>
<pre><code>input_ids = tf.keras.Input(shape=(128,), dtype='int32')

attention_mask = tf.keras.Input(shape=(128, ), dtype='int32')

transformer = TFRobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;, num_labels=6)

encoded = transformer([input_ids, attention_mask])

logits = encoded[0]

model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = logits)

</code></pre>
<p>Rest of the code for either method is identical,</p>
<pre><code>model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])
</code></pre>
<p>I am using Tensorflow 2.3.0 and have tried with transformers versions 3.5.0 and 4.0.0.</p>
","I am trying to import a pretrained model from Huggingface's transformers library and extend it with a few layers for classification using tensorflow keras. When I directly use transformers model (Method 1), the model trains well and reaches a validation accuracy of 0.93 after 1 epoch. However, when trying to use the model as a layer within a tf.keras model (Method 2), the model can't get above 0.32 accuracy. As far as I can tell based on the documentation, the two approaches should be equivalent. My goal is to get Method 2 working so that I can add more layers to it instead of directly using the logits produced by Huggingface's classifier head but I'm stuck at this stage. Method 1: Method 2: Rest of the code for either method is identical, I am using Tensorflow 2.3.0 and have tried with transformers versions 3.5.0 and 4.0.0.",https://stackoverflow.com/questions/65157852,8840524,Documentation Replication on Other Examples
65277703,image normalization and TPU,"<p>I'm trying to incorporate image normalization in my keras model to run on Google's cloud TPU. Therefore I inserted a line into my code:</p>
<pre><code>with strategy.scope():
     input_shape=(128,128,3)
     image_0 = Input(shape=input_shape)
     **image_1 = tf.image.per_image_standardization(image_0)**
     ...
</code></pre>
<p>There was nor error thrown, but according the documentation of google tf.image.per_image_standardization
is not a supported function. Does anybody know if it works anyhow, or does anybody have an idea how to check if it works?</p>
","I'm trying to incorporate image normalization in my keras model to run on Google's cloud TPU. Therefore I inserted a line into my code: There was nor error thrown, but according the documentation of google tf.image.per_image_standardization is not a supported function. Does anybody know if it works anyhow, or does anybody have an idea how to check if it works?",https://stackoverflow.com/questions/65277703,14818604,Documentation Replicability
65436819,Keras: How to use `image_dataset_from_directory` to load test set?,"<p>I am using <code>tf.keras.preprocessing.image_dataset_from_directory</code> to load dataset as follows,</p>
<pre><code>train_dataset = tf.keras.preprocessing.image_dataset_from_directory(train_dir, 
                                                                    labels='inferred', 
                                                                    label_mode='categorical',
                                                                    batch_size=32,
                                                                    image_size=(224, 224))


val_dataset = tf.keras.preprocessing.image_dataset_from_directory(val_dir, 
                                                                  labels='inferred', 
                                                                  label_mode='categorical',
                                                                  batch_size=32,
                                                                  image_size=(224, 224))

</code></pre>
<p>However, when I check the document looks like this argument <code>labels</code> seem to be a must-have one,  but my test data has no labels, so how can I load test data? Is there a convenient and unified way to do this?</p>
","I am using tf.keras.preprocessing.image_dataset_from_directory to load dataset as follows, However, when I check the document looks like this argument labels seem to be a must-have one, but my test data has no labels, so how can I load test data? Is there a convenient and unified way to do this?",https://stackoverflow.com/questions/65436819,9444831,Documentation Replicability
65437493,convert string to float array in csv using tf.data,"<p>I have a csv like this :</p>
<pre><code>kw_text,kw_text_weight
amazon google,0.5 0.5
google facebook microsoft,0.5 0.3 0.2
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>kw_text</th>
<th>kw_text_weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>amazon google</td>
<td>0.5 0.5</td>
</tr>
<tr>
<td>google facebook microsoft</td>
<td>0.5 0.3 0.2</td>
</tr>
</tbody>
</table>
</div>
<p>I want to convert column <code>text_weight</code> to <code>tf.data</code> . But I find nothing about it in tensorflow document website .</p>
",I have a csv like this : I want to convert column text_weight to tf.data . But I find nothing about it in tensorflow document website .,https://stackoverflow.com/questions/65437493,6862189,Lack of Alternative Solutions/Documentation
65464181,An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?,"<ol>
<li><p>task : object_detection</p>
</li>
<li><p>environment: AWS sagemaker</p>
</li>
<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1</p>
</li>
<li><p>Main file to be run: <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/model_main_tf2.py"" rel=""nofollow noreferrer"">original</a></p>
</li>
<li><p>Problematic code segment from the main file:</p>
<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
    FLAGS.tpu_name)
    tf.config.experimental_connect_to_cluster(resolver)
    tf.tpu.experimental.initialize_tpu_system(resolver)
    strategy = tf.distribute.experimental.TPUStrategy(resolver)
    elif FLAGS.num_workers &gt; 1:
        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    else:
        strategy = tf.compat.v2.distribute.MirroredStrategy()
</code></pre>
</li>
<li><p>Problem : Can't find the proper value to be given as <code>tpu_name</code> argument.</p>
</li>
<li><p>My research on the problem:</p>
</li>
</ol>
<p>According to the tensorflow documentation in <a href=""https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver"" rel=""nofollow noreferrer"">tf.distribute.cluster_resolver.TPUClusterResolver</a>, it says that this resolver works only on Google Cloud platform.</p>
<blockquote>
<p>This is an implementation of cluster resolvers for the Google Cloud
TPU service.</p>
<p>TPUClusterResolver supports the following distinct environments:
Google Compute Engine Google Kubernetes Engine Google internal</p>
<p>It can be passed into tf.distribute.TPUStrategy to support TF2
training on Cloud TPUs.</p>
</blockquote>
<p>But from <a href=""https://github.com/tensorflow/tensorflow/issues/39721"" rel=""nofollow noreferrer"">this issue in github</a>, I found out that a similar code also works in Azure.</p>
<ol start=""8"">
<li>My question :</li>
</ol>
<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker</strong> ?</p>
<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?</p>
","According to the tensorflow documentation in tf.distribute.cluster_resolver.TPUClusterResolver, it says that this resolver works only on Google Cloud platform. But from this issue in github, I found out that a similar code also works in Azure. Is there a way I can bypass this resolver and initialize my tpu in sagemaker ? Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?",https://stackoverflow.com/questions/65464181,9279666,Documentation Replicability
65481591,Keras Generator to tf.data.Dataset,"<p>I am working with the Mask RCNN keras implementation but the data generator hard locks on my systems when using <code>use_multiprocessing=True</code>. The data generator runs fine in single thread. I am trying to convert the data generator to a <code>tf.data.Dataset</code> as recommended by tensorflow. I have no idea how to do this and have been unable to find any documentation on this.</p>
<p>Mask RCNN data generator:</p>
<pre><code>class DataGenerator(KU.Sequence):
    &quot;&quot;&quot;An iterable that returns images and corresponding target class ids,
        bounding box deltas, and masks. It inherits from keras.utils.Sequence to avoid data redundancy
        when multiprocessing=True.

        dataset: The Dataset object to pick data from
        config: The model config object
        shuffle: If True, shuffles the samples before every epoch
        augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.
            For example, passing imgaug.augmenters.Fliplr(0.5) flips images
            right/left 50% of the time.
        random_rois: If &gt; 0 then generate proposals to be used to train the
                     network classifier and mask heads. Useful if training
                     the Mask RCNN part without the RPN.
        detection_targets: If True, generate detection targets (class IDs, bbox
            deltas, and masks). Typically for debugging or visualizations because
            in trainig detection targets are generated by DetectionTargetLayer.

        Returns a Python iterable. Upon calling __getitem__() on it, the
        iterable returns two lists, inputs and outputs. The contents
        of the lists differ depending on the received arguments:
        inputs list:
        - images: [batch, H, W, C]
        - image_meta: [batch, (meta data)] Image details. See compose_image_meta()
        - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)
        - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
        - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs
        - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]
        - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width
                    are those of the image unless use_mini_mask is True, in which
                    case they are defined in MINI_MASK_SHAPE.

        outputs list: Usually empty in regular training. But if detection_targets
            is True then the outputs list contains target class_ids, bbox deltas,
            and masks.
        &quot;&quot;&quot;

    def __init__(self, dataset, config, shuffle=True, augmentation=None,
                 random_rois=0, detection_targets=False):

        self.image_ids = np.copy(dataset.image_ids)
        self.dataset = dataset
        self.config = config

        # Anchors
        # [anchor_count, (y1, x1, y2, x2)]
        self.backbone_shapes = compute_backbone_shapes(config, config.IMAGE_SHAPE)
        self.anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,
                                                      config.RPN_ANCHOR_RATIOS,
                                                      self.backbone_shapes,
                                                      config.BACKBONE_STRIDES,
                                                      config.RPN_ANCHOR_STRIDE)

        self.shuffle = shuffle
        self.augmentation = augmentation
        self.random_rois = random_rois
        self.batch_size = self.config.BATCH_SIZE
        self.detection_targets = detection_targets

    def __len__(self):
        return int(np.ceil(len(self.image_ids) / float(self.batch_size)))

    def __getitem__(self, idx):
        b = 0
        image_index = -1
        while b &lt; self.batch_size:
            
            # Increment index to pick next image. Shuffle if at the start of an epoch.
            image_index = (image_index + 1) % len(self.image_ids)

            if self.shuffle and image_index == 0:
                np.random.shuffle(self.image_ids)

            # Get GT bounding boxes and masks for image.
            image_id = self.image_ids[image_index]
            image, image_meta, gt_class_ids, gt_boxes, gt_masks = \
                load_image_gt(self.dataset, self.config, image_id,
                              augmentation=self.augmentation)

            # Skip images that have no instances. This can happen in cases
            # where we train on a subset of classes and the image doesn't
            # have any of the classes we care about.
            if not np.any(gt_class_ids &gt; 0):
                continue

            # RPN Targets
            rpn_match, rpn_bbox = build_rpn_targets(image.shape, self.anchors,
                                                    gt_class_ids, gt_boxes, self.config)

            # Mask R-CNN Targets
            if self.random_rois:
                rpn_rois = generate_random_rois(
                    image.shape, self.random_rois, gt_class_ids, gt_boxes)
                if self.detection_targets:
                    rois, mrcnn_class_ids, mrcnn_bbox, mrcnn_mask = \
                        build_detection_targets(
                            rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config)

            # Init batch arrays
            if b == 0:
                batch_image_meta = np.zeros((self.batch_size,) + image_meta.shape, dtype=image_meta.dtype)
                batch_rpn_match = np.zeros([self.batch_size, self.anchors.shape[0], 1], dtype=rpn_match.dtype)
                batch_rpn_bbox = np.zeros([self.batch_size, self.config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)
                batch_images = np.zeros((self.batch_size,) + image.shape, dtype=np.float32)
                batch_gt_class_ids = np.zeros((self.batch_size, self.config.MAX_GT_INSTANCES), dtype=np.int32)
                batch_gt_boxes = np.zeros((self.batch_size, self.config.MAX_GT_INSTANCES, 4), dtype=np.int32)
                batch_gt_masks = np.zeros((self.batch_size, gt_masks.shape[0], gt_masks.shape[1],self.config.MAX_GT_INSTANCES), dtype=gt_masks.dtype)
                if self.random_rois:
                    batch_rpn_rois = np.zeros((self.batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype)
                    if self.detection_targets:
                        batch_rois = np.zeros((self.batch_size,) + rois.shape, dtype=rois.dtype)
                        batch_mrcnn_class_ids = np.zeros((self.batch_size,) + mrcnn_class_ids.shape, dtype=mrcnn_class_ids.dtype)
                        batch_mrcnn_bbox = np.zeros((self.batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype)
                        batch_mrcnn_mask = np.zeros((self.batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)

            # If more instances than fits in the array, sub-sample from them.
            if gt_boxes.shape[0] &gt; self.config.MAX_GT_INSTANCES:
                ids = np.random.choice(
                    np.arange(gt_boxes.shape[0]), self.config.MAX_GT_INSTANCES, replace=False)
                gt_class_ids = gt_class_ids[ids]
                gt_boxes = gt_boxes[ids]
                gt_masks = gt_masks[:, :, ids]

            # Add to batch
            batch_image_meta[b] = image_meta
            batch_rpn_match[b] = rpn_match[:, np.newaxis]
            batch_rpn_bbox[b] = rpn_bbox
            batch_images[b] = mold_image(image.astype(np.float32), self.config)
            batch_gt_class_ids[b, :gt_class_ids.shape[0]] = gt_class_ids
            batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes
            batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks
            if self.random_rois:
                batch_rpn_rois[b] = rpn_rois
                if self.detection_targets:
                    batch_rois[b] = rois
                    batch_mrcnn_class_ids[b] = mrcnn_class_ids
                    batch_mrcnn_bbox[b] = mrcnn_bbox
                    batch_mrcnn_mask[b] = mrcnn_mask
            b += 1

        inputs = [batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox,
                  batch_gt_class_ids, batch_gt_boxes, batch_gt_masks]
        outputs = []

        if self.random_rois:
            inputs.extend([batch_rpn_rois])
            if self.detection_targets:
                inputs.extend([batch_rois])
                # Keras requires that output and targets have the same number of dimensions
                batch_mrcnn_class_ids = np.expand_dims(
                    batch_mrcnn_class_ids, -1)
                outputs.extend(
                    [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask])

        return inputs, outputs
</code></pre>
<p>I have tried to use the <code>tf.data.Dataset.from_generator()</code> however it requires the <code>output_types=</code> argument and the Mask RCNN outputs a number of lists, I can not figure out how to define <code>output_types=</code>.</p>
<p>I am using <code>python3.7</code>, <code>keras==2.2.5</code>, <code>tensorflow==2.2.0</code></p>
","I am working with the Mask RCNN keras implementation but the data generator hard locks on my systems when using use_multiprocessing=True. The data generator runs fine in single thread. I am trying to convert the data generator to a tf.data.Dataset as recommended by tensorflow. I have no idea how to do this and have been unable to find any documentation on this. Mask RCNN data generator: I have tried to use the tf.data.Dataset.from_generator() however it requires the output_types= argument and the Mask RCNN outputs a number of lists, I can not figure out how to define output_types=. I am using python3.7, keras==2.2.5, tensorflow==2.2.0",https://stackoverflow.com/questions/65481591,2600161,Documentation Replication on Other Examples
65640885,CRNN tf.keras.backend.ctc_decode. What is log probability?,"<p>Based on <a href=""https://docs.w3cub.com/tensorflow%7Epython/tf/keras/backend/ctc_decode"" rel=""nofollow noreferrer"">the documentation</a>, the function <code>tf.keras.backend.ctc_decode</code> is supposed to return a <code>Tuple</code>. Its first field contains the best path (let's assume we use greedy search), whereas the second one contains its <code>log probability</code>.</p>
<p>Is this probability actually the accuracy of the prediction?</p>
<p>If not how am I supposed to calculate it?</p>
<p>I've tried on some test images and this was my output:</p>
<pre><code>True value: test0, prediction: test0, acc: 1.841524362564087
True value: test1, prediction: test1, acc: 0.9661365151405334
True value: test2, prediction: test2, acc: 1.0634151697158813
True value: test3, prediction: test3, acc: 2.471940755844116
True value: test4, prediction: test4, acc: 1.4866207838058472
True value: test5, prediction: test5, acc: 0.7630811333656311
True value: test6, prediction: test6, acc: 0.35642576217651367
True value: test7, prediction: test7, acc: 1.5693446397781372
True value: test8, prediction: test8, acc: 0.9700028896331787
True value: test9, prediction: test9, acc: 1.4783780574798584
</code></pre>
<p>During the training part, the final CTC loss was around 0.1 and the prediction is always correct. However what I think it's the probability seems not to be what I expect. They looks like completely random numbers, even grater than 1 or 2! What am I doing wrong?</p>
","Based on the documentation, the function tf.keras.backend.ctc_decode is supposed to return a Tuple. Its first field contains the best path (let's assume we use greedy search), whereas the second one contains its log probability. Is this probability actually the accuracy of the prediction? If not how am I supposed to calculate it? I've tried on some test images and this was my output: During the training part, the final CTC loss was around 0.1 and the prediction is always correct. However what I think it's the probability seems not to be what I expect. They looks like completely random numbers, even grater than 1 or 2! What am I doing wrong?",https://stackoverflow.com/questions/65640885,14967854,Documentation Replication on Other Examples
65712409,How to convert tf2 model so it will run on tflite interpreter,"<p><strong>Background:</strong>
I am trying to convert the tf2 model for SSD MobileNet V2 FPNLite 320x320 (for example) from the official tf zoo. The model should run eventually on raspberry pi, so I would like it to run on the tflite interpreter (without full tf). The docs imply that ssd model conversion is supported.</p>
<p><strong>Whats happening:</strong>
the process is detailed in <a href=""https://colab.research.google.com/drive/1P7T-ghA68mrNIRXjDPc_A59-lLxfq5yw?usp=sharing"" rel=""nofollow noreferrer"">this colab notebook</a>. It is failing with the error:</p>
<pre><code>ConverterError: &lt;unknown&gt;:0: error: loc(callsite(callsite(&quot;Postprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/SortByField_1/Size@__inference___call___23519&quot; at &quot;StatefulPartitionedCall@__inference_signature_wrapper_25508&quot;) at &quot;StatefulPartitionedCall&quot;)): 'tf.Size' op is neither a custom op nor a flex op
&lt;unknown&gt;:0: note: loc(&quot;StatefulPartitionedCall&quot;): called from
&lt;unknown&gt;:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
    tf.Size {device = &quot;&quot;}
</code></pre>
<p>if I add the flag tf.lite.OpsSet.SELECT_TF_OPS, it works but wont run on the rpi, as it does not have the ops.</p>
<p>Can this be done? Has anyone succeeded?</p>
","Background: I am trying to convert the tf2 model for SSD MobileNet V2 FPNLite 320x320 (for example) from the official tf zoo. The model should run eventually on raspberry pi, so I would like it to run on the tflite interpreter (without full tf). The docs imply that ssd model conversion is supported. Whats happening: the process is detailed in this colab notebook. It is failing with the error: if I add the flag tf.lite.OpsSet.SELECT_TF_OPS, it works but wont run on the rpi, as it does not have the ops. Can this be done? Has anyone succeeded?",https://stackoverflow.com/questions/65712409,9694304,Documentation Replication on Other Examples
65779087,How to use tf.gradients within a model and still use a custom training loop?,"<p>I would like to make a TensorFlow model where the outputs respect a mathematical condition, namely that output 0 is a scalar function and all subsequent outputs are its partial derivatives w.r.t. the input. This is because my observations are the scalar function and its partials, and not using the partials for training would be a waste of information.</p>
<p>For now, using simply tf.gradients works if I don't build a custom training loop, i.e. when I don't utilize eager execution. The model is built like this, and training works as expected:</p>
<pre><code>import tensorflow as tf


from tensorflow.keras import losses
from tensorflow.keras import optimizers
from tensorflow.keras import callbacks

# Creating a model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Dense,
    Dropout,
    Flatten,
    Concatenate,
    Input,
    Lambda,
)

# Custom activation function
from tensorflow.keras.layers import Activation
from tensorflow.keras import backend as K

import numpy
import matplotlib.pyplot as plt

import tensorboard

layer_width = 200
dense_layer_number = 3

def lambda_gradient(args):
    layer = args[0]
    inputs = args[1]
    return tf.gradients(layer, inputs)[0]

# Input is a 2 dimensional vector
inputs = tf.keras.Input(shape=(2,), name=&quot;coordinate_input&quot;)

# Build `dense_layer_number` times a dense layers of width `layer_width`
stream = inputs
for i in range(dense_layer_number):
    stream = Dense(
        layer_width, activation=&quot;relu&quot;, name=f&quot;dense_layer_{i}&quot;
    )(stream)

# Build one dense layer that reduces the 200 nodes to a scalar output
scalar = Dense(1, name=&quot;network_to_scalar&quot;, activation=custom_activation)(stream)

# Take the gradient of the scalar w.r.t. the model input
gradient = Lambda(lambda_gradient, name=&quot;gradient_layer&quot;)([scalar, inputs])

# Combine them to form the model output
concat = Concatenate(name=&quot;concat_scalar_gradient&quot;)([scalar, gradient])

# Wrap everything in a model
model = tf.keras.Model(inputs=inputs, outputs=concat)

loss = &quot;MSE&quot;
optimizer = &quot;Adam&quot;

# And compile
model.compile(loss=loss, optimizer=optimizer)
</code></pre>
<p>However, them problem now comes when I want to do online training (i.e. with an incremental dataset). In this case, I wouldn't compile my model at the very end. Instead, I write a loop as such (before calling model.compile):</p>
<pre><code># ... continue from previous minus model.compile

loss_fn = tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.Adam()

# Iterate over the batches of a dataset and train.
for i_batch in range(number_of_batches):

    with tf.GradientTape() as tape:
        # Predict w.r.t. the inputs X
        prediction_Y = model(batches_X[i_batch])
        
        # Compare batch prediction to batch observation
        loss_value = loss_fn(batches_Y[i_batch], prediction_Y)

    gradients = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(gradients, model.trainable_weights))
</code></pre>
<p>This however gives the following exception at <code>prediction_Y = model(batches_X[i_batch])</code>:</p>
<pre><code>RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.
</code></pre>
<p>As most examples, tutorials and documentation solely deal with using gradients to do training, and not within the model, I can't find any good resources how to deal with this. I tried to find how to use gradient tape, but I can't figure out how to use it in the model design phase. Any pointers would be appreciated!</p>
<p>Versions used:</p>
<pre><code>$ python --version                                         
Python 3.8.5
$ python -c &quot;import tensorflow as tf;print(tf.__version__);print(tf.keras.__version__)&quot;
2.2.0
2.3.0-tf
</code></pre>
","I would like to make a TensorFlow model where the outputs respect a mathematical condition, namely that output 0 is a scalar function and all subsequent outputs are its partial derivatives w.r.t. the input. This is because my observations are the scalar function and its partials, and not using the partials for training would be a waste of information. For now, using simply tf.gradients works if I don't build a custom training loop, i.e. when I don't utilize eager execution. The model is built like this, and training works as expected: However, them problem now comes when I want to do online training (i.e. with an incremental dataset). In this case, I wouldn't compile my model at the very end. Instead, I write a loop as such (before calling model.compile): This however gives the following exception at prediction_Y = model(batches_X[i_batch]): As most examples, tutorials and documentation solely deal with using gradients to do training, and not within the model, I can't find any good resources how to deal with this. I tried to find how to use gradient tape, but I can't figure out how to use it in the model design phase. Any pointers would be appreciated! Versions used:",https://stackoverflow.com/questions/65779087,6848887,Inadequate Examples
65794527,"Example of output_signature , output_types & output_shapes for complex object called by tf.data.Dataset.from_generator","<p>I've a generator function that yields the following tuple: <code>yield (transformed_input_array, set_y)</code></p>
<p><em>transformed_input_array</em> is a list of ndarrays with the following shape: <em>(1024, 104), (1024, 142), (1024, 1), (1024, 1), (1024, 1), (1024, 1), (1024, 140)</em>  and the following types: <em>tf.float64, tf.float64, tf.int8, tf.int16, tf.int8, tf.int8, tf.float64</em>
<em>set_y</em> is a ndarray of shape <em>1024</em> and type of <em>int64</em></p>
<p>I've wrapped my generator with tf.data.Dataset.from_generator function, here is the code:</p>
<pre><code>dataset = tf.data.Dataset.from_generator(
    generator,
    # output_signature=(
    #     tf.TensorSpec(shape=(), dtype=(tf.float64, tf.float64, tf.int8, tf.int16, tf.int8, tf.int8, tf.float64)),
    #     tf.TensorSpec(shape=1024, dtype=tf.int64))
    output_types=(tf.float64, tf.float64, tf.int8, tf.int16, tf.int8, tf.int8, tf.float64, tf.int64),
    output_shapes=((1024, 104), (1024, 142), (1024, 1), (1024, 1), (1024, 1), (1024, 1), (1024, 140), 1024)
)
</code></pre>
<p>But when I run the training, I get the following error:</p>
<blockquote>
<p>ValueError: Data is expected to be in format <code>x</code>, <code>(x,)</code>, <code>(x, y)</code>,
or <code>(x, y, sample_weight)</code>, found: (&lt;tf.Tensor 'IteratorGetNext:0'
shape=(1024, 104) dtype=float64&gt;, &lt;tf.Tensor 'IteratorGetNext:1'
shape=(1024, 142) dtype=float64&gt;, &lt;tf.Tensor 'IteratorGetNext:2'
shape=(1024, 1) dtype=int8&gt;, &lt;tf.Tensor 'It eratorGetNext:3'
shape=(1024, 1) dtype=int16&gt;, &lt;tf.Tensor 'IteratorGetNext:4'
shape=(1024, 1) dtype=int8&gt;, &lt;tf.Tensor 'IteratorGetNext:5'
shape=(1024, 1) dtype=int8&gt;, &lt;tf.Tensor 'IteratorGetNext:6'
shape=(1024, 140) dtype=float64&gt;, &lt;tf.Tensor 'ExpandDims:0'
shape=(1024, 1) dtype=int64&gt;)</p>
</blockquote>
<p>If I try to run with output_signature param (commented out code), I get the following error:</p>
<blockquote>
<p>TypeError: Cannot convert value (tf.float64, tf.float64, tf.int8,
tf.int16, tf.int8, tf.int8, tf.float64) to a TensorFlow DType.</p>
</blockquote>
<p><strong>Can someone provide an example, of how I should treat complex type (list of ndarrays)?</strong> Couldn't find any example in TF documentation..</p>
","I've a generator function that yields the following tuple: yield (transformed_input_array, set_y) transformed_input_array is a list of ndarrays with the following shape: (1024, 104), (1024, 142), (1024, 1), (1024, 1), (1024, 1), (1024, 1), (1024, 140) and the following types: tf.float64, tf.float64, tf.int8, tf.int16, tf.int8, tf.int8, tf.float64 set_y is a ndarray of shape 1024 and type of int64 I've wrapped my generator with tf.data.Dataset.from_generator function, here is the code: But when I run the training, I get the following error: If I try to run with output_signature param (commented out code), I get the following error: Can someone provide an example, of how I should treat complex type (list of ndarrays)? Couldn't find any example in TF documentation..",https://stackoverflow.com/questions/65794527,336558,Inadequate Examples
65835387,ValueError: too many values to unpack (expected 2) when using tf.keras.preprocessing.image_dataset_from_directory,"<p>I want to create a dataset-variable as well as a labels-variable using the function tf.keras.preprocessing.image_dataset_from_directory (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory</a>).
The documentation states:</p>
<blockquote>
<p>Returns:
A tf.data.Dataset object.
If label_mode is None, it yields
float32 tensors of shape (batch_size, image_size[0], image_size[1],
num_channels), encoding images (see below for rules regarding
num_channels).
Otherwise, it yields a tuple (images, labels), where
images has shape (batch_size, image_size[0], image_size[1],
num_channels), and labels follows the format described below.</p>
</blockquote>
<p>My code is the following:</p>
<pre><code>train_ds, labels = tf.keras.preprocessing.image_dataset_from_directory(
  directory = data_dir,
  labels='inferred',
  label_mode = &quot;int&quot;,
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
</code></pre>
<p>I expect to get a tuple as return values, but instead I get the error message:</p>
<pre><code>Found 2160 files belonging to 2160 classes.
Using 1728 files for training.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-168-ed9d42ed2ab9&gt; in &lt;module&gt;
      7   seed=123,
      8   image_size=(img_height, img_width),
----&gt; 9   batch_size=batch_size)

ValueError: too many values to unpack (expected 2)
</code></pre>
<p>When I save the output in one variable (just train_ds) and I inspect the variable, I get the following output:</p>
<pre><code>&lt;BatchDataset shapes: ((None, 120, 30, 3), (None,)), types: (tf.float32, tf.int32)&gt;
</code></pre>
<p>How can I access the two tuples inside seperatly?</p>
","I want to create a dataset-variable as well as a labels-variable using the function tf.keras.preprocessing.image_dataset_from_directory (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory). The documentation states: My code is the following: I expect to get a tuple as return values, but instead I get the error message: When I save the output in one variable (just train_ds) and I inspect the variable, I get the following output: How can I access the two tuples inside seperatly?",https://stackoverflow.com/questions/65835387,12336925,Documentation Ambiguity
65863738,Changing order of Input Image in 3D convolutions,"<p>According to the official documentation of tf.keras.layers.Conv3D</p>
<blockquote>
<p>5+D tensor with shape: batch_shape + (channels, conv_dim1, conv_dim2,
conv_dim3) if data_format='channels_first' or 5+D tensor with shape:
batch_shape + (conv_dim1, conv_dim2, conv_dim3, channels) if
data_format='channels_last'</p>
</blockquote>
<p>. Now the whole idea around channels and batch shape makes sense, but will changing the general order of (conv_dim1, conv_dim2,conv_dim2) as (x,y,z) to say (z,x,y) affect the performance.</p>
<p>Does Conv3D worry about order of x-y-z dimension ?</p>
<p>I was training a U-net segmentation model and upon changing the order of axis I saw difference in performance. (x,y,z) order converges faster as compared to (y,x,z).</p>
<p>I just wanted to make sure what's the correct way..</p>
","According to the official documentation of tf.keras.layers.Conv3D . Now the whole idea around channels and batch shape makes sense, but will changing the general order of (conv_dim1, conv_dim2,conv_dim2) as (x,y,z) to say (z,x,y) affect the performance. Does Conv3D worry about order of x-y-z dimension ? I was training a U-net segmentation model and upon changing the order of axis I saw difference in performance. (x,y,z) order converges faster as compared to (y,x,z). I just wanted to make sure what's the correct way..",https://stackoverflow.com/questions/65863738,5066344,Documentation Ambiguity
65953591,Which format should have time series input for LSTM-Model in Tensorflow?,"<p>I have a problem with the input for the fit-function of an LSTM-Model in TensorFlow. I have an input with the following shape:<br />
(5, 128, 78, 80)<br />
The fields are: (number of samples, timesteps, feature1, feature2)</p>
<p>The output has the shape: (5, 128, 78, 2)</p>
<p>This is my model:</p>
<pre><code>from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation

batch_size=5

time_model = tf.keras.Sequential()
time_model.add(tf.keras.layers.LSTM(512,return_sequences=True,input_shape=(128,2)))
time_model.add(Activation('sigmoid'))
time_model.add(Dense(2,name=&quot;dense&quot;))
time_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')


time_model.fit(x=time_input,y=time_output, epochs=10, batch_size=batch_size)
</code></pre>
<p>I get the following error:<br />
<code>ValueError: Input 0 of layer sequential_38 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (5, 128, 78, 80)</code></p>
<p>So I think, I have to change the shape of my data, but I don't know how. I tried already different values for input and  input_shape-attribute.<br />
I read in <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM</a> that the input has to be a tensor with shape <code>[batch, timesteps, feature]</code>. So I put the two features in a nested array, and gave <code>[batch, timesteps, array of the features]</code> to the fit-function. But it told me that the data could not be converted to a tensor. Also explicit converting with <code>tf.convert_to_tensor</code> did not work.</p>
<p>I would be really glad, if someone could explain me, how I can pass input data with two features to an LSTM-model.</p>
","I have a problem with the input for the fit-function of an LSTM-Model in TensorFlow. I have an input with the following shape: (5, 128, 78, 80) The fields are: (number of samples, timesteps, feature1, feature2) The output has the shape: (5, 128, 78, 2) This is my model: I get the following error: ValueError: Input 0 of layer sequential_38 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (5, 128, 78, 80) So I think, I have to change the shape of my data, but I don't know how. I tried already different values for input and input_shape-attribute. I read in https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM that the input has to be a tensor with shape [batch, timesteps, feature]. So I put the two features in a nested array, and gave [batch, timesteps, array of the features] to the fit-function. But it told me that the data could not be converted to a tensor. Also explicit converting with tf.convert_to_tensor did not work. I would be really glad, if someone could explain me, how I can pass input data with two features to an LSTM-model.",https://stackoverflow.com/questions/65953591,11535546,Documentation Ambiguity
66019998,"How to get a processed dataset, if the processing steps are not tensor operations?","<p>I have an instance of <code>tf.data.Dataset()</code>, of images, basically, acquired this way:</p>
<pre><code>import tensorflow as tf

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_directory,
    image_size = (image_height, image_width),
    batch_size = batch_size
)
</code></pre>
<p>So, this dataset has <code>(data, label)</code> where the data is a tensor of shape <code>(batch_size, image_height, image_width, channels)</code> [I don't really need the labels it assigns]. So far so good. The problem is, I need to process this dataset, applying certain operations to the images, and, this dataset is too big to load everything in memory (that's why <code>batch_size</code> is there). According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer"">tensorflow documentation</a>, <code>tf.data.Dataset.map()</code> is the function I need (or so I assume....).</p>
<pre><code>def image_processing(data):
    print(data.shape)
    
    # Do some operations.
    # Do some copies [because np.arrays help me more...].
    copy = np.array(data, copy=True)

    # Change some pixels, like, zero out a square in this image
    # It sad that TensorFlow can't do this assignment if it were a tf.Tensor:
    copy[10:80,10:80] = np.array([0,0,0])

    # Do more things, and when done return.
    return something


processed_dataset = dataset.map(lambda image, label: (image_processing(image), label))
</code></pre>
<p>First of all, the shape returned by the print: <code>(None, 200, 200, 3)</code> instead of <code>(32, 200, 200, 3)</code>, or, instead of <code>(200, 200, 3)</code> [which is what I'd expect from reading the documentation] [let's assume batch of 32, and images 200x200], and this is messing my code, because, I need to do assigments, like, take the ith image, and change a couple pixels: <code>data[i][12:15,40:50] = np.array([1,2,3])</code> and things like that.</p>
<p>Basically, that's the error message: <code>TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'</code>.</p>
<hr />
<p><strong>In summary, my question:</strong> How can I get a <code>processed_dataset</code>, where the processing steps will not be whole tensor operations, but instead, will be changing individual values in the data (say, individual pixels), for certain images (say, the ith image, jth image, etc)?</p>
<hr />
<p>If you must know, I am running this in Ubuntu. Tensorflow version is:</p>
<pre><code>&gt;&gt;&gt; tf.__version__
'2.4.0'
</code></pre>
","I have an instance of tf.data.Dataset(), of images, basically, acquired this way: So, this dataset has (data, label) where the data is a tensor of shape (batch_size, image_height, image_width, channels) [I don't really need the labels it assigns]. So far so good. The problem is, I need to process this dataset, applying certain operations to the images, and, this dataset is too big to load everything in memory (that's why batch_size is there). According to the tensorflow documentation, tf.data.Dataset.map() is the function I need (or so I assume....). First of all, the shape returned by the print: (None, 200, 200, 3) instead of (32, 200, 200, 3), or, instead of (200, 200, 3) [which is what I'd expect from reading the documentation] [let's assume batch of 32, and images 200x200], and this is messing my code, because, I need to do assigments, like, take the ith image, and change a couple pixels: data[i][12:15,40:50] = np.array([1,2,3]) and things like that. Basically, that's the error message: TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'. In summary, my question: How can I get a processed_dataset, where the processing steps will not be whole tensor operations, but instead, will be changing individual values in the data (say, individual pixels), for certain images (say, the ith image, jth image, etc)? If you must know, I am running this in Ubuntu. Tensorflow version is:",https://stackoverflow.com/questions/66019998,15049194,Documentation Replication on Other Examples
66038861,Why are both branches in tf.cond being executed? And why does tf.while_loop finish the loop even though the condition still true?,"<p>I am using keras for a while now, but usually I don't have to use customized layers or perform some more complex flow control, so I'm struggling trying to understand somethings.</p>
<p>I am modeling a neural network with a customized layer on the top. This customized layer calls another function (<code>search_sigma</code>)  and inside this function I execute <code>tf.while_loop</code> and inside of <code>tf.while_loop</code> I execute <code>tf.cond</code>.</p>
<p>I cannot understand why the conditions are not working.</p>
<ul>
<li><code>tf.while_loop</code> stops even though the condition (<code>l1</code>) still true</li>
<li><code>tf.cond executes</code> both <code>f1</code> and <code>f2</code> (callables <code>true_fn</code> and <code>false_fn</code>)</li>
</ul>
<p>Could someone help me understand what I am missing?</p>
<p>I already tried to change both tf.cond and tf.while_loop conditions for true tensors, just to see what would happen. The behavior (exactly same errors) remained the same.</p>
<p>I also tried to write this code without implementing a class (using just functions). Nothing changed.</p>
<p>I tried to find solutions looking at tensorflow documentation, other stack overflow doubts and websites talking about tf.while_loop and tf.cond.</p>
<p>I left some <code>print()</code>s in the body of the code to try to track what was happening.</p>
<pre><code>class find_sigma:
    
    def __init__ (self, t_inputs,  inputs,  expected_perp=10. ):       
        self.sigma, self.cluster = t_inputs
        self.inputs = inputs
        self.expected_perp = expected_perp
        self.min_sigma=tf.constant([0.01],tf.float32)
        self.max_sigma=tf.constant([50.],tf.float32)
 

    def search_sigma(self):

        
        def cond(s,sigma_not_found): return sigma_not_found


        def body(s,sigma_not_found):   

            print('loop')
            pi = K.exp( - K.sum( (K.expand_dims(self.inputs, axis=1) - self.cluster)**2, axis=2  )/(2*s**2) )        
            pi = pi / K.sum(pi)
            MACHINE_EPSILON = np.finfo(np.double).eps
            pi = K.maximum(pi, MACHINE_EPSILON)
            H = - K.sum ( pi*(K.log(pi)/K.log(2.)) , axis=0 )
            perp = 2**H

            print('0')

            l1 = tf.logical_and (tf.less(perp , self.expected_perp), tf.less(0.01, self.max_sigma-s))
            l2 = tf.logical_and (tf.less(  self.expected_perp , perp) , tf.less(0.01, s-self.min_sigma) )
    
            def f1():
                print('f1')
                self.min_sigma = s 
                s2 = (s+self.max_sigma)/2 
                return  [s2, tf.constant([True])]
                

            def f2(l2): 
                tf.cond( l2, true_fn=f3 , false_fn = f4)

            def f3(): 
                print('f3')
                self.max_sigma = s 
                s2 = (s+self.min_sigma)/2
                return [s2, tf.constant([True])]

            def f4(): 
                print('f4')
                return [s, tf.constant([False])]
            
            output = tf.cond( l1, f1 ,  f4 ) #colocar f2 no lugar de f4

            s, sigma_not_found = output
            
            print('sigma_not_found = ',sigma_not_found)
            return [s,sigma_not_found]

        print('01')

        sigma_not_found = tf.constant([True])

        new_sigma,sigma_not_found=sigma_not_found = tf.while_loop(
            cond , body, loop_vars=[self.sigma,sigma_not_found]
        )

        print('saiu')
        
        print(new_sigma)

        return new_sigma
</code></pre>
<p>The piece of code that calls the above code is:</p>
<pre><code>self.sigma = tf.map_fn(fn=lambda t: find_sigma(t,  inputs).search_sigma() , elems=(self.sigma,self.clusters), dtype=tf.float32)
</code></pre>
<p>'inputs' is a <code>(None, 10)</code> size tensor</p>
<p>'self.sigma' is a <code>(10,)</code> size tensor</p>
<p>'self.clusters' is a <code>(N, 10)</code> size tensor</p>
","I am using keras for a while now, but usually I don't have to use customized layers or perform some more complex flow control, so I'm struggling trying to understand somethings. I am modeling a neural network with a customized layer on the top. This customized layer calls another function (search_sigma) and inside this function I execute tf.while_loop and inside of tf.while_loop I execute tf.cond. I cannot understand why the conditions are not working. Could someone help me understand what I am missing? I already tried to change both tf.cond and tf.while_loop conditions for true tensors, just to see what would happen. The behavior (exactly same errors) remained the same. I also tried to write this code without implementing a class (using just functions). Nothing changed. I tried to find solutions looking at tensorflow documentation, other stack overflow doubts and websites talking about tf.while_loop and tf.cond. I left some print()s in the body of the code to try to track what was happening. The piece of code that calls the above code is: 'inputs' is a (None, 10) size tensor 'self.sigma' is a (10,) size tensor 'self.clusters' is a (N, 10) size tensor",https://stackoverflow.com/questions/66038861,15141021,Inadequate Examples
66049816,Custom layer in sequential model tensorflow,"<p>I'm trying to create a custom layer for my model, which can be used the classic Dense layer of Keras. Here my custom layer:</p>
<pre><code>class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, num_outputs):
        super(MyDenseLayer, self).__init__()
        self.num_outputs = num_outputs
    def build(self, input_shape):
        self.kernel = self.add_weight(&quot;kernel&quot;, 
                                      shape=[int(input_shape[-1]),
                                      self.num_outputs])
    def call(self, input):
        return tf.matmul(input, self.kernel)
</code></pre>
<p>It does not do anything 'custom' for now.</p>
<p>But when I add it to my model</p>
<pre><code>def build_model():
    model = keras.Sequential([
        MyDenseLayer(10)(normed_x_train),
        layers.Activation(tf.nn.relu),
        layers.Dense(1, activation=tf.nn.relu)
        ])
    return model
</code></pre>
<p>I get this:</p>
<pre><code>The added layer must be an instance of class Layer. Found: tf.Tensor(
[....])
</code></pre>
<p>Because probably I'm creating directly the object of class Custom Layer. But I do not find in the tf documentation how to add other properties to make it work as a normal layer, i.e. as something like <code>layers.Dense(100, activation=tf.nn.relu)</code></p>
<p>Is there a way to make it work like that ?</p>
","I'm trying to create a custom layer for my model, which can be used the classic Dense layer of Keras. Here my custom layer: It does not do anything 'custom' for now. But when I add it to my model I get this: Because probably I'm creating directly the object of class Custom Layer. But I do not find in the tf documentation how to add other properties to make it work as a normal layer, i.e. as something like layers.Dense(100, activation=tf.nn.relu) Is there a way to make it work like that ?",https://stackoverflow.com/questions/66049816,12338521,Lack of Alternative Solutions/Documentation
66231467,How to set a minimum number of epoch in Optuna SuccessiveHalvingPruner()?,"<p>I'm using Optuna 2.5 to optimize a couple of hyperparameters on a tf.keras CNN model. I want to use pruning so that the optimization skips the less promising corners of the hyperparameters space. I'm using something like this:</p>
<pre><code>study0 = optuna.create_study(study_name=study_name,
                             storage=storage_name,
                             direction='minimize', 
                             sampler=TPESampler(n_startup_trials=25, multivariate=True, seed=123),
                             pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource='auto',
                             reduction_factor=4, min_early_stopping_rate=0),
                             load_if_exists=True)
</code></pre>
<p>Sometimes the model stops after 2 epochs, some other times it stops after 12 epochs, 48 and so forth. What I want is to ensure that the model always trains at least 30 epochs before being pruned. I guess that the parameter <code>min_early_stopping_rate</code> might have some control on this but I've tried to change it from 0 to 30 and then  the models never get pruned. Can someone explain me a bit better than the Optuna documentation, what these parameters in the  <code>SuccessiveHalvingPruner()</code> really do (specially <code>min_early_stopping_rate</code>)?
Thanks</p>
","I'm using Optuna 2.5 to optimize a couple of hyperparameters on a tf.keras CNN model. I want to use pruning so that the optimization skips the less promising corners of the hyperparameters space. I'm using something like this: Sometimes the model stops after 2 epochs, some other times it stops after 12 epochs, 48 and so forth. What I want is to ensure that the model always trains at least 30 epochs before being pruned. I guess that the parameter min_early_stopping_rate might have some control on this but I've tried to change it from 0 to 30 and then the models never get pruned. Can someone explain me a bit better than the Optuna documentation, what these parameters in the SuccessiveHalvingPruner() really do (specially min_early_stopping_rate)? Thanks",https://stackoverflow.com/questions/66231467,11433296,Requesting (Additional) Resources
66385626,How does TensorFlow SavedModel handle additional dependencies,"<p>I am trying to export my TF model using the <code>tf.saved_model.save()</code> function. However, I'm using additional external libraries during preprocessing. A working code using the example of nltk looks like this:</p>
<pre><code>import tensorflow as tf
import nltk
import tensorflow_datasets as tfds
from tensorflow.python.ops import gen_string_ops
from tensorflow.python.ops import string_ops

train_data, val_data, test_data = train_data, validation_data, test_data = tfds.load(
    name=&quot;imdb_reviews&quot;, 
    split=('train[:60%]', 'train[60%:]', 'test'),
    as_supervised=True)


def remove_stops(text):
    nltk.download('stopwords')
    stop_words = nltk.corpus.stopwords.words('english')
    for w in stop_words:
        text = string_ops.regex_replace(text, '\\b{}\\b'.format(w),'')  # remove all stopwords
    text = tf.strings.strip(text)  # remove trailing whitespaces
    return text

vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization( # this gives me a BOW representation of my data...
    max_tokens=1000,
    standardize=remove_stops,  # ...using my method to remove stopwords
    output_mode='count') 

train_feats = list(map(lambda x: x[0], train_data)) # bc PrefetchDatasets don't allow direct access
vectorize_layer.adapt(train_feats)  # BOW needs to be trained beforehand to get vector reps

# define some simple network
input_layer = tf.keras.Input(shape=(), name='input_text', dtype=tf.string)
model = tf.keras.Sequential()
model.add(input_layer)
model.add(vectorize_layer)
model.add(tf.keras.layers.Dense(units=64, activation='softmax')) 
model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy', metrics = ['binary_accuracy'])
model.fit(train_data.shuffle(10000).batch(512),
             epochs=2,
             validation_data=val_data.batch(512))

tf.saved_model.save(model, './save_here')
</code></pre>
<p>As far as I've understood the <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">docs</a>, a savedModel includes the trained parameters and computations, but NOT my code. But when I load the same model in another script like so (i.e. without importing my external library at all)</p>
<pre><code>import tensorflow as tf
loaded_model = tf.saved_model.load('./save_here')
loaded_model(tf.constant(['test me pls']))
</code></pre>
<p>I get my output without any errors</p>
<pre><code>&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.5086063]], dtype=float32)&gt;
</code></pre>
<p>This is especially baffling to me as I am even using nltk to download a dataset of stopwords. How exactly do SavedModels deal with these external libraries during exportation?</p>
","I am trying to export my TF model using the tf.saved_model.save() function. However, I'm using additional external libraries during preprocessing. A working code using the example of nltk looks like this: As far as I've understood the docs, a savedModel includes the trained parameters and computations, but NOT my code. But when I load the same model in another script like so (i.e. without importing my external library at all) I get my output without any errors This is especially baffling to me as I am even using nltk to download a dataset of stopwords. How exactly do SavedModels deal with these external libraries during exportation?",https://stackoverflow.com/questions/66385626,8921867,Documentation Replicability
66546321,Proper way to input a scalar into a Tensorflow 2 model,"<p>In my Tensorflow 2 model, I want my batch size to be parametric, such that I can build tensors which have appropriate batch size dynamically. I have the following code:</p>
<pre><code>batch_size_param = 128

tf_batch_size = tf.keras.Input(shape=(), name=&quot;tf_batch_size&quot;, dtype=tf.int32)
batch_indices = tf.range(0, tf_batch_size, 1)

md = tf.keras.Model(inputs={&quot;tf_batch_size&quot;: tf_batch_size}, outputs=[batch_indices])
res = md(inputs={&quot;tf_batch_size&quot;: batch_size_param})
</code></pre>
<p>The code throws an error in <code>tf.range</code>:</p>
<pre><code>ValueError: Shape must be rank 0 but is rank 1
 for 'limit' for '{{node Range}} = Range[Tidx=DT_INT32](Range/start, tf_batch_size, Range/delta)' with input shapes: [], [?], []
</code></pre>
<p>I think the problem is with the fact that <code>tf.keras.Input</code> automatically tries to expand the input array at the first dimension, since it expects the partial shape of the input without the batch size and will attach the batch size according to the shape of the input array, which in my case a scalar. I can just feed the scalar value as a constant integer into <code>tf.range</code> but this time, I won't be able to change it after the model graph has been compiled.</p>
<p>Interestingly, I failed to find a proper way to input only a scalar into a TF-2 model even though I checked the documentation, too. So, what would be the best way to handle such a case?</p>
","In my Tensorflow 2 model, I want my batch size to be parametric, such that I can build tensors which have appropriate batch size dynamically. I have the following code: The code throws an error in tf.range: I think the problem is with the fact that tf.keras.Input automatically tries to expand the input array at the first dimension, since it expects the partial shape of the input without the batch size and will attach the batch size according to the shape of the input array, which in my case a scalar. I can just feed the scalar value as a constant integer into tf.range but this time, I won't be able to change it after the model graph has been compiled. Interestingly, I failed to find a proper way to input only a scalar into a TF-2 model even though I checked the documentation, too. So, what would be the best way to handle such a case?",https://stackoverflow.com/questions/66546321,1538049,Documentation Replication on Other Examples
66711706,"Jax, jit and dynamic shapes: a regression from Tensorflow?","<p>The <a href=""https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#to-jit-or-not-to-jit"" rel=""noreferrer"">documentation for JAX</a> says,</p>
<blockquote>
<p>Not all JAX code can be JIT compiled, as it requires array shapes to be static &amp; known at compile time.</p>
</blockquote>
<p>Now I am somewhat surprised because tensorflow has operations like <code>tf.boolean_mask</code> that does what JAX seems incapable of doing when compiled.</p>
<ol>
<li>Why is there such a regression from Tensorflow? I was under the assumption that the underlying XLA representation was shared between the two frameworks, but I may be mistaken. I don't recall Tensorflow ever having troubles with dynamic shapes, and functions such as <code>tf.boolean_mask</code> have been around forever.</li>
<li>Can we expect this gap to close in the future? If not, why makes it impossible to do in JAX' jit what Tensorflow (among others) enables?</li>
</ol>
<p><strong>EDIT</strong></p>
<p>The gradient passes through <code>tf.boolean_mask</code> (obviously not on mask values, which are discrete); case in point here using TF1-style graphs where values are unknown, so TF cannot rely on them:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

x1 = tf.placeholder(tf.float32, (3,))
x2 = tf.placeholder(tf.float32, (3,))
y = tf.boolean_mask(x1, x2 &gt; 0)
print(y.shape)  # prints &quot;(?,)&quot;
dydx1, dydx2 = tf.gradients(y, [x1, x2])
assert dydx1 is not None and dydx2 is None
</code></pre>
","The documentation for JAX says, Now I am somewhat surprised because tensorflow has operations like tf.boolean_mask that does what JAX seems incapable of doing when compiled. EDIT The gradient passes through tf.boolean_mask (obviously not on mask values, which are discrete); case in point here using TF1-style graphs where values are unknown, so TF cannot rely on them:",https://stackoverflow.com/questions/66711706,9973879,Documentation Replication on Other Examples
66874943,Why iterations over the same tf.data.Dataset give different data each iteration?,"<p>I'm trying to understand how <strong>tf.data.Dataset</strong> works.</p>
<p>It says on the documentation that <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""nofollow noreferrer"">take</a> returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch):</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds

# Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=True)

# Build your input pipeline
ds = ds.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)
# ...
</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([2 0 6 6 8 8 6 0 3 4 8 7 5 2 5 7 8 7 1 1 1 8 6 4 0 4 3 2 4 2 1 9], shape=(32,), dtype=int64)
</code></pre>
<p>However, iterating over it again, gives different labels: (continuation of last code)</p>
<pre class=""lang-py prettyprint-override""><code>for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([7 3 5 6 3 1 7 9 6 1 9 3 9 8 6 7 7 1 9 7 5 2 0 7 8 1 7 8 7 0 5 0], shape=(32,), dtype=int64)
tf.Tensor([1 3 6 1 8 8 0 4 1 3 2 9 5 3 8 7 4 2 1 8 1 0 8 5 4 5 6 7 3 4 4 1], shape=(32,), dtype=int64)
</code></pre>
<p>Shouldn't the labels be the same, given that the dataset is the same?</p>
","I'm trying to understand how tf.data.Dataset works. It says on the documentation that take returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch): Outputs: However, iterating over it again, gives different labels: (continuation of last code) Outputs: Shouldn't the labels be the same, given that the dataset is the same?",https://stackoverflow.com/questions/66874943,2076973,Documentation Replicability
66879748,What is the difference between tf.keras.model and tf.keras.sequential?,"<p>In some <code>tf. keras</code> tutorials, I've seen them instantiated their model class like this:</p>
<p><code>model = tf.keras.Sequential()</code></p>
<p>While in some places, they use something like this:</p>
<p><code>model = tf.keras.Model(inputs=input, outputs=output)</code></p>
<p>But seeing here in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model"" rel=""noreferrer"">docs</a>, they do seem the same, but I am not sure nor is it explicitly mentioned. What are the differences between the two?</p>
","In some tf. keras tutorials, I've seen them instantiated their model class like this: model = tf.keras.Sequential() While in some places, they use something like this: model = tf.keras.Model(inputs=input, outputs=output) But seeing here in the docs, they do seem the same, but I am not sure nor is it explicitly mentioned. What are the differences between the two?",https://stackoverflow.com/questions/66879748,8648710,Documentation Ambiguity
67066760,Configuring labels in TensorFlow BinaryCrossentropy loss function,"<p>I want to compute cross-entropy loss using <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy"" rel=""nofollow noreferrer"">tf.keras.losses.BinaryCrossentropy</a>. The documentation has the following example, and specifies that true labels and predicted labels should have the shape <code>[batch_size]</code>:</p>
<pre><code>y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]

bce = tf.keras.losses.BinaryCrossentropy()
bce(y_true, y_pred).numpy()
</code></pre>
<p>From the example, it is inferred that each sample's label should be formatted as [probability of belonging to Class 0, probability of belonging to Class 1]. Is it correct? If it is, why <code>y_true[1]</code> probabilities do not add up to 1?</p>
","I want to compute cross-entropy loss using tf.keras.losses.BinaryCrossentropy. The documentation has the following example, and specifies that true labels and predicted labels should have the shape [batch_size]: From the example, it is inferred that each sample's label should be formatted as [probability of belonging to Class 0, probability of belonging to Class 1]. Is it correct? If it is, why y_true[1] probabilities do not add up to 1?",https://stackoverflow.com/questions/67066760,5425172,Documentation Ambiguity
67197448,How to extract multiple rows from tensor at the same time?,"<p>TL;DR:
TensorFlow tensor is of shape <code>(50, 50, 6)</code>, want these indices (:, :, (0, 2, 3)). How to extract them?</p>
<p>Here is an example array I am working with:</p>
<pre><code>import numpy as np

a = np.random.randint(0,10, (50, 50, 6))
</code></pre>
<p>I want to extract the the first, third, and fourth row; in other words I need all these entries <code>(:, :, (1, 3))</code>, which works for numpy arrays:</p>
<pre><code>out = a[:,:, [0, 2, 3]]
out.shape #(50, 50, 3)

</code></pre>
<p>Working with a tensor <code>t = tf.convert_to_tensor(a)</code> and then calling the index like</p>
<pre><code>t[:,:, [0, 2, 3]]
</code></pre>
<p>throws an error:</p>
<pre><code>TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got [0, 1, 3]
</code></pre>
<p>For numpy I have found the following relevant questions, but they naturally focus on numpy arrays:</p>
<p><a href=""https://stackoverflow.com/questions/46227095/how-to-slice-a-2d-array-non-consecutively-in-python?noredirect=1&amp;lq=1"">How to slice a 2D array non-consecutively in Python</a></p>
<p><a href=""https://stackoverflow.com/questions/24398708/slicing-a-numpy-array-along-a-dynamically-specified-axis"">Slicing a numpy array along a dynamically specified axis</a></p>
<p>Looking at the TF documentation I found <code>gather_nd</code> and <code>boolean_mask</code>, which I feel are helpful, but I must freely admit that I have not understood the docs at this part. On SO I found this question <a href=""https://stackoverflow.com/questions/58052967/how-to-select-elements-of-a-tensor-along-a-specific-axis-in-tensorflow"">How to select elements of a tensor along a specific axis in TensorFlow</a>, which focuses on single elements; I am looking for complete dimensions (if that's the right wording here).</p>
<p>How can I do the numpy thing in TensorFlow?</p>
","TL;DR: TensorFlow tensor is of shape (50, 50, 6), want these indices (:, :, (0, 2, 3)). How to extract them? Here is an example array I am working with: I want to extract the the first, third, and fourth row; in other words I need all these entries (:, :, (1, 3)), which works for numpy arrays: Working with a tensor t = tf.convert_to_tensor(a) and then calling the index like throws an error: For numpy I have found the following relevant questions, but they naturally focus on numpy arrays: How to slice a 2D array non-consecutively in Python Slicing a numpy array along a dynamically specified axis Looking at the TF documentation I found gather_nd and boolean_mask, which I feel are helpful, but I must freely admit that I have not understood the docs at this part. On SO I found this question How to select elements of a tensor along a specific axis in TensorFlow, which focuses on single elements; I am looking for complete dimensions (if that's the right wording here). How can I do the numpy thing in TensorFlow?",https://stackoverflow.com/questions/67197448,12859833,Documentation Replication on Other Examples
67211152,Tensorlow - please decipher what the tf.where document says,"<p>Please decipher what the <a href=""https://www.tensorflow.org/api_docs/python/tf/where"" rel=""nofollow noreferrer"">tf.where</a> documentation says about what it does when both x and y are provided.</p>
<p>I suppose it tries to say it will produce a result by:</p>
<ol>
<li>Broadcast y to the result shape.</li>
<li>Broadcast x to the result shape.</li>
<li>Update y with x elements where the condition is true.</li>
</ol>
<p>Is this correct?</p>
<blockquote>
<p>If x and y are provided (both have non-None values):
tf.where will choose an output shape from the shapes of condition, x, and y that all three shapes are broadcastable to.</p>
<p><strong>Returns</strong>
If x and y are provided: A Tensor with the same type as x and y, and shape that is broadcast from condition, x, and y. Otherwise, a Tensor with shape (num_true, dim_size(condition)).</p>
</blockquote>
",Please decipher what the tf.where documentation says about what it does when both x and y are provided. I suppose it tries to say it will produce a result by: Is this correct?,https://stackoverflow.com/questions/67211152,4281353,Documentation Replicability
67523944,"Tensorflow2 - Use ""tf.data.experimental.make_csv_dataset"" with ""tf.keras.preprocessing.timeseries_dataset_from_array""","<p>I am trying to get TensorFlow to read +100 CSV files that <em><strong>don't</strong></em> fit in memory (+1GB size each). The files contain time series data (EEG signals), with the labels in the first column. From the TensorFlow documentation it seems like I should be able to use the <em>tf.data</em> API to load my data off-disk.</p>
<p>For the sake of simplicity and reproducibility, let's consider the following &quot;<em>sample_data.csv</em>&quot; dataset:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Label</th>
<th>Feature 1</th>
<th>Feature 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Apple</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>Banana</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>Coconut</td>
<td>5</td>
<td>6</td>
</tr>
<tr>
<td>Durian</td>
<td>7</td>
<td>8</td>
</tr>
</tbody>
</table>
</div>
<p>I've tried using <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset"" rel=""nofollow noreferrer"">tf.data.experimental.make_csv_dataset</a> to load the CSV files into <em>tf.data.Dataset</em> objects, and then <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array"" rel=""nofollow noreferrer"">tf.keras.preprocessing.timeseries_dataset_from_array</a> to process the data into sliding windows with overlap. For the dataset above, I would do:</p>
<pre><code>import tensorflow as tf

input_data = tf.data.experimental.make_csv_dataset(
    'sample_data.csv',
    batch_size=1,
    column_names=['Label', 'Feature 1', 'Feature 2']
    label_name='Label',
    num_epochs=1,
    shuffle=False
)
</code></pre>
<p>Which we can check works correctly by looking at the output from <code>list(input_data.as_numpy_iterator())</code>. We can then feed <code>input_data</code> to the next function:</p>
<pre><code>my_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data,
    targets=None,
    sequence_length=3,
    sequence_stride=2,
    sampling_rate=1,  
    batch_size=1,
    shuffle=False
)
</code></pre>
<p>Which unfortunately <strong>throws this error</strong>:</p>
<blockquote>
<p>TypeError: dataset length is unknown.</p>
</blockquote>
<p>I also tried using <code>my_dataset = input_data.window(3, shift=2)</code> (see the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window"" rel=""nofollow noreferrer"">tf.data.Dataset.window</a> documentation) and it didn't throw an error, but
it seems to be returning an <strong>empty dataset</strong>? See &quot;<em>_VariantDataset shapes: (None,)</em>&quot; in the output:</p>
<pre><code>list(input_data.window(3, shift=2))

[344]:
[(OrderedDict([('Feature 1',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;),
               ('Feature 2',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;)]),
  &lt;_VariantDataset shapes: (None,), types: tf.string&gt;),
 (OrderedDict([('Feature 1',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;),
               ('Feature 2',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;)]),
  &lt;_VariantDataset shapes: (None,), types: tf.string&gt;),
 (OrderedDict([('Feature 1',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;),
               ('Feature 2',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;)]),
  &lt;_VariantDataset shapes: (None,), types: tf.string&gt;)]
</code></pre>
<p>If I load the &quot;<em>sample_data.csv</em>&quot; in memory using pandas and then feed the <em>timeseries_dataset_from_array</em> function a numpy array instead, it works correctly.</p>
<p>Any ideas on how to solve this? <strong>What's the best method to input overlapping windows from off-memory time-series data into TensorFlow</strong>?</p>
<p>Thank you!</p>
","I am trying to get TensorFlow to read +100 CSV files that don't fit in memory (+1GB size each). The files contain time series data (EEG signals), with the labels in the first column. From the TensorFlow documentation it seems like I should be able to use the tf.data API to load my data off-disk. For the sake of simplicity and reproducibility, let's consider the following ""sample_data.csv"" dataset: I've tried using tf.data.experimental.make_csv_dataset to load the CSV files into tf.data.Dataset objects, and then tf.keras.preprocessing.timeseries_dataset_from_array to process the data into sliding windows with overlap. For the dataset above, I would do: Which we can check works correctly by looking at the output from list(input_data.as_numpy_iterator()). We can then feed input_data to the next function: Which unfortunately throws this error: I also tried using my_dataset = input_data.window(3, shift=2) (see the tf.data.Dataset.window documentation) and it didn't throw an error, but it seems to be returning an empty dataset? See ""_VariantDataset shapes: (None,)"" in the output: If I load the ""sample_data.csv"" in memory using pandas and then feed the timeseries_dataset_from_array function a numpy array instead, it works correctly. Any ideas on how to solve this? What's the best method to input overlapping windows from off-memory time-series data into TensorFlow? Thank you!",https://stackoverflow.com/questions/67523944,6395699,Documentation Replication on Other Examples
67563475,How to convert a tensorflow model and load as tfds,"<p>I need help converting my dataset from how I usually make it using
<code>tf.keras.preprocessing.image_dataset_from_directory</code>
To be used to replace this in an example</p>
<pre><code>dataset, info = tfds.load(name='mnist', split=split, with_info=True,

as_supervised=True, try_gcs=True)
</code></pre>
<p>How can I do so? I am unable to find related documentation so if you can link that it would be amazing.
Thanks</p>
<p>This is how the dataset is used in the example</p>
<pre><code>  split = 'train' if is_training else 'test'
  dataset, info = tfds.load(name='mnist', split=split, with_info=True,
                            as_supervised=True, try_gcs=True)


  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255.0

    return image, label

  dataset = dataset.map(scale)
</code></pre>
",I need help converting my dataset from how I usually make it using tf.keras.preprocessing.image_dataset_from_directory To be used to replace this in an example How can I do so? I am unable to find related documentation so if you can link that it would be amazing. Thanks This is how the dataset is used in the example,https://stackoverflow.com/questions/67563475,15760012,Documentation Ambiguity
67723809,Memory leak when using tf.data Datasets with shuffle,"<p>I have a memory leak somehow when I create my tf.data.dataset pipeline, but I don't know where.
My code works fine with ImageDataGenerator but is really slow.
Reading a lot of documentation I thought it might be albumentations.</p>
<p>However I now switched my transform to be entirely in tensorflow:</p>
<pre><code>def map_data(inputs, outputs):
image = inputs['image_input']
image = parse_image(image)
image = tf.cast(image, tf.float32) / 255.0
image = tf.image.resize(image, size = [224, 224])
image = tf.image.random_flip_left_right(image)
image = tf.image.random_flip_up_down(image)
image = tf.image.random_brightness(image, max_delta = 0.5)
#image = tf.expand_dims(image, axis=3)
other = parse_image(inputs['other_input'])
other = tf.cast(other, tf.float32) / 255.0
other = tf.image.resize(other, size = [224, 224])
other = tf.image.random_flip_left_right(other)
other = tf.image.random_flip_up_down(other)
other = tf.image.random_brightness(other, max_delta = 0.5)


return {'image_input': image, 'other_input': other}, outputs
</code></pre>
<p>And I made the shuffle buffer extremely small:</p>
<pre><code>        #dataset = dataset.prefetch(tf.data.AUTOTUNE)
    AUTOTUNE = tf.data.AUTOTUNE
    dataset = (dataset
        .shuffle(32)
        .map(map_data, num_parallel_calls=AUTOTUNE)
        .cache()
        .batch(32)
        .prefetch(AUTOTUNE)
    )
</code></pre>
<p>Could autotune cause this?
On Colab I usually hit the RAM restart at 500 batches</p>
<p>I would like to use tf.data.datasets because it's really much faster if possible.</p>
<p>Thank you for anyone who can point me to the flaw in my code, I have always used generators and only recently made the switch.</p>
","I have a memory leak somehow when I create my tf.data.dataset pipeline, but I don't know where. My code works fine with ImageDataGenerator but is really slow. Reading a lot of documentation I thought it might be albumentations. However I now switched my transform to be entirely in tensorflow: And I made the shuffle buffer extremely small: Could autotune cause this? On Colab I usually hit the RAM restart at 500 batches I would like to use tf.data.datasets because it's really much faster if possible. Thank you for anyone who can point me to the flaw in my code, I have always used generators and only recently made the switch.",https://stackoverflow.com/questions/67723809,11106507,Documentation Replication on Other Examples
67740346,TimeDistributed layer to apply several convolutional layers error,"<p>I have an issue with the tf.keras.layers.TimeDistributed layer (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed</a>).</p>
<p>I am aware that TimeDistributed can be used to apply a single layer (dense, convolutional...) to a set of inputs, obtaining a set of outputs.</p>
<p>Not only that, but until recently I was able to use it to apply an entire &quot;submodel&quot; to all the inputs. That is, a series of layers, not just one. An example of this is explained here by Patrice Ferlet (<a href=""https://medium.com/smileinnovation/training-neural-network-with-image-sequence-an-example-with-video-as-input-c3407f7a0b0f"" rel=""nofollow noreferrer"">https://medium.com/smileinnovation/training-neural-network-with-image-sequence-an-example-with-video-as-input-c3407f7a0b0f</a>).</p>
<p>Using that source as example I can define a sequential &quot;submodel&quot; like this:</p>
<pre><code>import keras
from keras.layers import Conv2D, BatchNormalization, \
MaxPool2D, GlobalMaxPool2D

def build_convnet(shape=(112, 112, 3)):
  momentum = .9
  model = keras.Sequential()
  model.add(Conv2D(64, (3,3), input_shape=shape,
      padding='same', activation='relu'))
  model.add(Conv2D(64, (3,3), padding='same', activation='relu'))
  model.add(BatchNormalization(momentum=momentum))

  model.add(MaxPool2D())

  model.add(Conv2D(128, (3,3), padding='same', activation='relu'))
  model.add(Conv2D(128, (3,3), padding='same', activation='relu'))
  model.add(BatchNormalization(momentum=momentum))

  model.add(MaxPool2D())

  model.add(Conv2D(256, (3,3), padding='same', activation='relu'))
  model.add(Conv2D(256, (3,3), padding='same', activation='relu'))
  model.add(BatchNormalization(momentum=momentum))

  model.add(MaxPool2D())

  model.add(Conv2D(512, (3,3), padding='same', activation='relu'))
  model.add(Conv2D(512, (3,3), padding='same', activation='relu'))
  model.add(BatchNormalization(momentum=momentum))

  # flatten...
  model.add(GlobalMaxPool2D())
  return model
</code></pre>
<p>And then include this submodel inside a superior model which calls TimeDistributed with the whole initial submodel (convnet).</p>
<pre><code>from keras.layers import TimeDistributed, GRU, Dense, Dropout

def action_model(shape=(5, 112, 112, 3), nbout=3):
  # Create our convnet with (112, 112, 3) input shape
  convnet = build_convnet(shape[1:])

  # then create our final model
  model = keras.Sequential()
  # add the convnet with (5, 112, 112, 3) shape
  model.add(TimeDistributed(convnet, input_shape=shape))
  # here, you can also use GRU or LSTM
  model.add(GRU(64))
  # and finally, we make a decision network
  model.add(Dense(1024, activation='relu'))
  model.add(Dropout(.5))
  model.add(Dense(512, activation='relu'))
  model.add(Dropout(.5))
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(.5))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(nbout, activation='softmax'))
  return model
</code></pre>
<p>Now this works well, and I can get the model structure calling</p>
<pre><code>mod=action_model()
mod.summary()
</code></pre>
<p>But if instead of this I define the convnet model using as backbone a predefined arquitecture from keras, like VGG16, there seems to be an error. (I also needed to change as well keras.Sequential by tf.keras.models.Sequential)</p>
<pre><code>import tensorflow as tf
from keras.layers import Flatten

def build_convnet():

    prevModel = tf.keras.applications.vgg16.VGG16(
        include_top=False,
        input_shape=(112, 112, 3),
        weights='imagenet'  # ImageNet weights
    )

    model = tf.keras.models.Sequential()

    model.add(prevModel)
    model.add(Flatten())

    return model

def action_model(shape=(5, 112, 112, 3), nbout=3):
  # Create our convnet with (112, 112, 3) input shape
  convnet = build_convnet()

  # then create our final model
  model = tf.keras.models.Sequential()
  # add the convnet with (5, 112, 112, 3) shape
  model.add(TimeDistributed(convnet, input_shape=shape))
  # here, you can also use GRU or LSTM
  model.add(GRU(64))
  # and finally, we make a decision network
  model.add(Dense(1024, activation='relu'))
  model.add(Dropout(.5))
  model.add(Dense(512, activation='relu'))
  model.add(Dropout(.5))
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(.5))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(nbout, activation='softmax'))
  return model
</code></pre>
<p>When I run this after defining my VGG16-based architecture</p>
<pre><code>mod=action_model()
mod.summary()
</code></pre>
<p>I get the following error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-106-c8c6108a1d66&gt; in &lt;module&gt;()
----&gt; 1 mod=action_model()
      2 mod.summary()

1 frames
/usr/local/lib/python3.7/dist-packages/keras/layers/wrappers.py in __init__(self, layer, **kwargs)
    121           'Please initialize `TimeDistributed` layer with a '
    122           '`tf.keras.layers.Layer` instance. You passed: {input}'.format(
--&gt; 123               input=layer))
    124     super(TimeDistributed, self).__init__(layer, **kwargs)
    125     self.supports_masking = True

ValueError: Please initialize `TimeDistributed` layer with a `tf.keras.layers.Layer` instance. You passed: &lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x7fbb36266a50&gt;
</code></pre>
<p>So now it appears like python is complaining that I am using an input for TimeDistributed that is not a single layer. This doesn't make any sense, since the initial example works well and also involves using several layers with TimeDistributed. Apart from that, the VGG16 model also worked fine some weeks ago.</p>
<p>I am running all of this in Google CoLab.</p>
<p>Could someone help me figure out what is going on here? Is this caused by the new tensorflow 2.5.0 version? Everywhere I look I see people using TimeDistributed to apply a single layer, but applying a whole sequential model worked just fine until now (despite no apparent mention in the documentation).</p>
<p>Thank you!</p>
","I have an issue with the tf.keras.layers.TimeDistributed layer (https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed). I am aware that TimeDistributed can be used to apply a single layer (dense, convolutional...) to a set of inputs, obtaining a set of outputs. Not only that, but until recently I was able to use it to apply an entire ""submodel"" to all the inputs. That is, a series of layers, not just one. An example of this is explained here by Patrice Ferlet (https://medium.com/smileinnovation/training-neural-network-with-image-sequence-an-example-with-video-as-input-c3407f7a0b0f). Using that source as example I can define a sequential ""submodel"" like this: And then include this submodel inside a superior model which calls TimeDistributed with the whole initial submodel (convnet). Now this works well, and I can get the model structure calling But if instead of this I define the convnet model using as backbone a predefined arquitecture from keras, like VGG16, there seems to be an error. (I also needed to change as well keras.Sequential by tf.keras.models.Sequential) When I run this after defining my VGG16-based architecture I get the following error: So now it appears like python is complaining that I am using an input for TimeDistributed that is not a single layer. This doesn't make any sense, since the initial example works well and also involves using several layers with TimeDistributed. Apart from that, the VGG16 model also worked fine some weeks ago. I am running all of this in Google CoLab. Could someone help me figure out what is going on here? Is this caused by the new tensorflow 2.5.0 version? Everywhere I look I see people using TimeDistributed to apply a single layer, but applying a whole sequential model worked just fine until now (despite no apparent mention in the documentation). Thank you!",https://stackoverflow.com/questions/67740346,16059221,Documentation Replication on Other Examples
67947583,"Defining a callable ""loss"" function","<p>I am trying to optimize a loss function (defined using evidence lower bound) with <code>tf.train.AdamOptimizer.minimize()</code> on Tensorflow version <code>1.15.2</code> with eager execution enabled. I tried the following:</p>
<pre><code>learning_rate = 0.01
optim = tf.train.AdamOptimizer(learning_rate=learning_rate)
train_op = optim.minimize(loss)
</code></pre>
<p>and got the following : <code>RuntimeError: &quot;loss&quot; passed to Optimizer.compute_gradients should be a function when eager execution is enabled.</code></p>
<p>This works fine if I disable eager execution but since I need to save a tensorflow variable as a <code>numpy</code> array so I need eager execution enabled. The documentation mentions that when eager execution is enabled, the loss must be a <strong>callable</strong>. So the loss function should be defined in a way that it takes no inputs but gives out loss. I am not exactly sure how do I achieve such a thing.</p>
<p>I tried <code>train_op = optim.minimize(lambda: loss)</code> but got <code>ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [] and loss &lt;function &lt;lambda&gt; at 0x7f3c67a93b00&gt;</code></p>
","I am trying to optimize a loss function (defined using evidence lower bound) with tf.train.AdamOptimizer.minimize() on Tensorflow version 1.15.2 with eager execution enabled. I tried the following: and got the following : RuntimeError: ""loss"" passed to Optimizer.compute_gradients should be a function when eager execution is enabled. This works fine if I disable eager execution but since I need to save a tensorflow variable as a numpy array so I need eager execution enabled. The documentation mentions that when eager execution is enabled, the loss must be a callable. So the loss function should be defined in a way that it takes no inputs but gives out loss. I am not exactly sure how do I achieve such a thing. I tried train_op = optim.minimize(lambda: loss) but got ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [] and loss &lt;function &lt;lambda&gt; at 0x7f3c67a93b00&gt;",https://stackoverflow.com/questions/67947583,6639856,Documentation Replication on Other Examples
68319579,tfa.optimizers.MultiOptimizer - TypeError: 'Not JSON Serializable:',"<p>I'm trying to use tfa.optimizers.MultiOptimizer(). I did everything according to the docs (<a href=""https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/MultiOptimizer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/MultiOptimizer</a>) yet I'm getting the following error:</p>
<p>TypeError: ('Not JSON Serializable:', &lt;tf.Tensor 'gradient_tape/model_80/dense_3/Tensordot/MatMul/MatMul:0' shape=(1, 1) dtype=float32&gt;)</p>
<p>Below is a minimal, working example that reproduces the error, just copy and paste it. The error occurs when the first epoch is finished and the callback trys to save the model.</p>
<pre><code>##############################################################################

import tensorflow as tf
import tensorflow_addons as tfa
import tensorflow.keras.layers as l
import tensorflow_addons.layers as la
import tensorflow.keras as ke
import numpy as np

##############################################################################

def build_model_1():

    model_input = l.Input(shape=(32,1))

    x = l.Dense(1)(model_input)

    model = ke.Model(inputs=model_input, outputs=x)

##########  
    
    optimizers = [tf.keras.optimizers.Adam(),
                  tf.keras.optimizers.Adam()]
    
    optimizers_and_layers = [(optimizers[0], model.layers[:5]), (optimizers[1], model.layers[5:])]
    
    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)
    
    model.compile(optimizer=optimizer, loss='mse', metrics='mse')

    test = tf.keras.optimizers.serialize(optimizer)

    return model

##############################################################################

input_data =  np.arange( 0, 10000, 1).reshape(10000,1)
target_data = np.arange(-10000, 0, 1).reshape(10000,1)

model = build_model_1()

model_checkpoint = ke.callbacks.ModelCheckpoint('best_model.h5',
                                                monitor='val_mse',
                                                mode='min',
                                                save_best_only=True,
                                                verbose=1)

training_history = model.fit(x = input_data,
                             y = target_data,
                             validation_split = 0.2,
                             epochs = 5,
                             verbose = 1,
                             callbacks = [model_checkpoint])
    
##############################################################################
</code></pre>
","I'm trying to use tfa.optimizers.MultiOptimizer(). I did everything according to the docs (https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/MultiOptimizer) yet I'm getting the following error: TypeError: ('Not JSON Serializable:', &lt;tf.Tensor 'gradient_tape/model_80/dense_3/Tensordot/MatMul/MatMul:0' shape=(1, 1) dtype=float32&gt;) Below is a minimal, working example that reproduces the error, just copy and paste it. The error occurs when the first epoch is finished and the callback trys to save the model.",https://stackoverflow.com/questions/68319579,16300082,Documentation Replicability
68354367,Getting an error when using tf.keras.metrics.Mean in functional Keras API,"<p>I'm trying to add a Mean metric to a Keras functional model (Tensorflow 2.5), and am getting the following error:</p>
<pre><code>ValueError: Expected a symbolic Tensor for the metric value, received: tf.Tensor(0.0, shape=(), dtype=float32)
</code></pre>
<p>Here is the code:</p>
<pre><code>x = [1, 2, 3, 4, 5, 6, 7, 8]
y = [5 + i * 3 for i in x]
a = Input(shape=(1,))
output = Dense(1)(a)
model = Model(inputs=a,outputs=output)
model.add_metric(tf.keras.metrics.Mean()(output))
model.compile(loss='mse')
model.fit(x=x, y=y, epochs=100)
</code></pre>
<p>If I remove the following line (from which the exception is thrown):</p>
<pre><code>model.add_metric(tf.keras.metrics.Mean()(output))
</code></pre>
<p>the code works as expected.
<br><br>I Tried disabling eager execution, but I get the following error instead:</p>
<pre><code>ValueError: Using the result of calling a `Metric` object when calling `add_metric` on a Functional Model is not supported. Please pass the Tensor to monitor directly.
</code></pre>
<p>The above usage was pretty much copied from the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Mean"" rel=""nofollow noreferrer"">tf.keras.metrics.Mean</a> documentation (see <em>Usage with compile() API</em>)</p>
","I'm trying to add a Mean metric to a Keras functional model (Tensorflow 2.5), and am getting the following error: Here is the code: If I remove the following line (from which the exception is thrown): the code works as expected. I Tried disabling eager execution, but I get the following error instead: The above usage was pretty much copied from the tf.keras.metrics.Mean documentation (see Usage with compile() API)",https://stackoverflow.com/questions/68354367,6133787,Lack of Alternative Solutions/Documentation
68422887,Slow tensorflow code; can I batch evaluate and obtain multiple loss scores?,"<p>I recently switched from using the 'keras' package in Python to using 'tensorflow.keras', since this seems to be preferred now. The latest version of Keras was also giving me issues that seemed like I'd have to modify the internal Keras code to fix, whereas tf.keras works fine. However, upon making this switch, some of my code was slowed down by a factor of 30-40. I've identified the following calls to &quot;model.evaluate&quot; as a bottleneck, though I'm not sure why it's so much slower than before. The code is structured something like this:</p>
<pre><code># 'model' is a tensorflow.keras.models.Sequential

n_scores = 10000

inputs = np.zeros((n_scores, 8, 10), dtype=np.bool)
outputs = np.zeros((n_scores, 10), dtype=np.bool)

# [populate inputs and outputs]

scores = []
for i in range(n_scores):
    score = model.evaluate(inputs[i,:,:], outputs[i,:])
    scores.append(score)

return scores
</code></pre>
<p>I'm thinking the major bottleneck is that I'm making a bunch of SMALL calls to tensorflow, rather than one LARGE call. Using a GPU actually makes it even slower, presumably due to all the loading/unloading.</p>
<p>I'd like to just make a call like</p>
<pre><code>scores = model.evaluate(inputs, outputs)
</code></pre>
<p>but model.evaluate() seems to always output a single scalar, when I need the whole list of 10000 loss scores. I have been unable to find a solution in the documentation, is there a builtin way to do sort of a &quot;batch evaluate&quot; but get individual loss scores out for each sample?</p>
","I recently switched from using the 'keras' package in Python to using 'tensorflow.keras', since this seems to be preferred now. The latest version of Keras was also giving me issues that seemed like I'd have to modify the internal Keras code to fix, whereas tf.keras works fine. However, upon making this switch, some of my code was slowed down by a factor of 30-40. I've identified the following calls to ""model.evaluate"" as a bottleneck, though I'm not sure why it's so much slower than before. The code is structured something like this: I'm thinking the major bottleneck is that I'm making a bunch of SMALL calls to tensorflow, rather than one LARGE call. Using a GPU actually makes it even slower, presumably due to all the loading/unloading. I'd like to just make a call like but model.evaluate() seems to always output a single scalar, when I need the whole list of 10000 loss scores. I have been unable to find a solution in the documentation, is there a builtin way to do sort of a ""batch evaluate"" but get individual loss scores out for each sample?",https://stackoverflow.com/questions/68422887,16470540,Documentation Replication on Other Examples
68431633,tf.image.stateless_random_crop VS. tf.image.random_crop. Shouldn't these be the same thing?,"<p>In tf 2.5, there are two functions for cropping an image: <code>tf.image.stateless_random_crop</code>, and <code>tf.image.random_crop</code>. The documentation states that <code>stateless_random_crop</code> is deterministic (always returns the same crop given one seed). However, <code>random_crop</code> has a seed parameter and is also deterministic, one would think. What is the actual difference between these two functions? I cannot find information about statelessness in Tensorflow anywhere.</p>
<p>The differences between <code>tf.image.stateless_random_crop</code>, and <code>tf.image.random_crop</code> are one line where stateless_random_uniform is used instead of a random_uniform:
stateless_random_crop: <a href=""https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465</a>
random_crop: <a href=""https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412</a></p>
<p>I always thought that <code>random_crop</code> would always return the same crop given a seed, but it looks like maybe that wasn't always true? Any enlightenment about statelessness in Tensorflow is greatly appreciated!</p>
","In tf 2.5, there are two functions for cropping an image: tf.image.stateless_random_crop, and tf.image.random_crop. The documentation states that stateless_random_crop is deterministic (always returns the same crop given one seed). However, random_crop has a seed parameter and is also deterministic, one would think. What is the actual difference between these two functions? I cannot find information about statelessness in Tensorflow anywhere. The differences between tf.image.stateless_random_crop, and tf.image.random_crop are one line where stateless_random_uniform is used instead of a random_uniform: stateless_random_crop: https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465 random_crop: https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412 I always thought that random_crop would always return the same crop given a seed, but it looks like maybe that wasn't always true? Any enlightenment about statelessness in Tensorflow is greatly appreciated!",https://stackoverflow.com/questions/68431633,11632499,Documentation Ambiguity
68550779,Slicing a 2D tensor similar to numpy np.ix_,"<p>I have learned how to slice a tensor on one dimension <a href=""https://stackoverflow.com/questions/64081367/slicing-a-tensor-with-a-tensor-of-indices-and-tf-gather"">here</a>.</p>
<p>I have learned how to slice a 2D tensor giving a 1D tensor of specific values <a href=""https://stackoverflow.com/questions/38492608/tensorflow-indexing-into-2d-tensor-with-1d-tensor"">here</a>.</p>
<p>Both use <code>tf.gather()</code> but I'm pretty sure I need <code>tf.gather_nd()</code> though I'm obviously using it wrong.</p>
<p>In numpy, I have a 5x5 2D array, and I can slice a 2x2 array by using <code>np.ix_()</code> with row and column indices (I always need the same indices for rows and columns, resulting in a squared matrix):</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

a = np.array([[1,2,3,4,5],[2,1,6,7,8],[3,6,1,9,10],[4,7,9,1,11],[5,8,10,11,1]])

a
</code></pre>
<blockquote>
<pre><code>array([[ 1,  2,  3,  4,  5],
      [ 2,  1,  6,  7,  8],
      [ 3,  6,  1,  9, 10],
      [ 4,  7,  9,  1, 11],
      [ 5,  8, 10, 11,  1]])
</code></pre>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>a[np.ix_([1,3], [1,3])]
</code></pre>
<blockquote>
<pre><code>array([[1, 7],
      [7, 1]])
</code></pre>
</blockquote>
<p>Reading over the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather_nd"" rel=""nofollow noreferrer""><code>tf.gather_nd()</code></a> docs I assumed this is the way to do it in TF, but I'm using it wrong:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

a = tf.constant([[1,2,3,4,5],[2,1,6,7,8],[3,6,1,9,10],[4,7,9,1,11],[5,8,10,11,1]])

tf.gather_nd(a, [[1,3], [1,3]])
</code></pre>
<blockquote>
<pre><code>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([7, 7])&gt;
</code></pre>
</blockquote>
<p>I would have to do something like:</p>
<pre class=""lang-py prettyprint-override""><code>tf.gather_nd(a, [[[1,1], [1,3]],[[3,1],[3,3]]])
</code></pre>
<blockquote>
<pre><code>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1, 7],
      [7, 1]])&gt;
</code></pre>
</blockquote>
<p>Which leads me down another rabbit hole I'm not keen on. My indices vector is a lot longer of course.</p>
<p>My indices, BTW, are 1D integer tensors themselves. So bottom-line I want to slice <code>a</code> with the same indices for rows and columns as I do with <code>np._ix()</code>, and my indices are something like:</p>
<pre class=""lang-py prettyprint-override""><code>idx = tf.constant([1, 3])

# tf.gather_nd(a, indices = &quot;something with idx&quot;)
</code></pre>
","I have learned how to slice a tensor on one dimension here. I have learned how to slice a 2D tensor giving a 1D tensor of specific values here. Both use tf.gather() but I'm pretty sure I need tf.gather_nd() though I'm obviously using it wrong. In numpy, I have a 5x5 2D array, and I can slice a 2x2 array by using np.ix_() with row and column indices (I always need the same indices for rows and columns, resulting in a squared matrix): Reading over the tf.gather_nd() docs I assumed this is the way to do it in TF, but I'm using it wrong: I would have to do something like: Which leads me down another rabbit hole I'm not keen on. My indices vector is a lot longer of course. My indices, BTW, are 1D integer tensors themselves. So bottom-line I want to slice a with the same indices for rows and columns as I do with np._ix(), and my indices are something like:",https://stackoverflow.com/questions/68550779,4095235,Documentation Replication on Other Examples
68878231,tf.gradients() vs tf.gradientTape.gradient() in graph mode,"<p>I had a question regarding the behavior of tf.gradients() as opposed tf.gradientTape.gradient() in graph mode.</p>
<p>Given a differentiable function y = f(x), where x and y are each single tensorflow tensors, is there any difference between the behavior of tf.gradient(y, x) vs tape.gradient(y, x) where tape is an instance of tf.gradientTape (assuming the use of graph mode) ?</p>
<p>Not sure why tensorflow has two different gradient methods which can be used with graph mode - maybe there are some subtle differences in the implementations? I’ve looked at the documentation for gradientTape and tf.gradients but it’s not clear whether there is any difference between the behavior of these methods for a single (x, y) pair, or whether it’s just that tf.gradients() can be used in this case for a speedup when using graph mode.</p>
<p>Thank you so much for your help!</p>
","I had a question regarding the behavior of tf.gradients() as opposed tf.gradientTape.gradient() in graph mode. Given a differentiable function y = f(x), where x and y are each single tensorflow tensors, is there any difference between the behavior of tf.gradient(y, x) vs tape.gradient(y, x) where tape is an instance of tf.gradientTape (assuming the use of graph mode) ? Not sure why tensorflow has two different gradient methods which can be used with graph mode - maybe there are some subtle differences in the implementations? I’ve looked at the documentation for gradientTape and tf.gradients but it’s not clear whether there is any difference between the behavior of these methods for a single (x, y) pair, or whether it’s just that tf.gradients() can be used in this case for a speedup when using graph mode. Thank you so much for your help!",https://stackoverflow.com/questions/68878231,16569549,Documentation Replication on Other Examples
68984841,How can I understand the kernel of tf.keras.layers.Dense for rank >2?,"<p>How can I understand the kernel of <code>tf.keras.layers.Dense</code> for rank &gt;2?</p>
<p>The official API doc states that:</p>
<blockquote>
<p>Note: If the input to the layer has a rank greater than 2, then Dense
computes the dot product between the inputs and the kernel along the
last axis of the inputs and axis 0 of the kernel (using tf.tensordot).
For example, if input has dimensions (batch_size, d0, d1), then we
create a kernel with shape (d1, units), and the kernel operates along
axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there
are batch_size * d0 such sub-tensors). The output in this case will
have shape (batch_size, d0, units).</p>
</blockquote>
<p>My understanding is that for a rank larger than 2 (for example rank 3) only <strong>one</strong> kernel is created and thus the same kernel is applied on all slices of the second dimension, like above.
That would consequently mean that the outputs for different indices of the second dimension are <strong>not independent</strong> of each other (especially during training).</p>
<p>Is my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?</p>
","How can I understand the kernel of tf.keras.layers.Dense for rank &gt;2? The official API doc states that: My understanding is that for a rank larger than 2 (for example rank 3) only one kernel is created and thus the same kernel is applied on all slices of the second dimension, like above. That would consequently mean that the outputs for different indices of the second dimension are not independent of each other (especially during training). Is my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?",https://stackoverflow.com/questions/68984841,16787662,Documentation Replicability
69195950,Problem with inputs when building a model with TFBertModel and AutoTokenizer from HuggingFace's transformers,"<p>I'm trying to build the model illustrated in this picture:
<a href=""https://i.stack.imgur.com/4eiAK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4eiAK.png"" alt=""enter image description here"" /></a></p>
<p>I obtained a pre-trained BERT and respective tokenizer from HuggingFace's <code>transformers</code> in the following way:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, TFBertModel
model_name = &quot;dbmdz/bert-base-italian-xxl-cased&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
bert = TFBertModel.from_pretrained(model_name)
</code></pre>
<p>The model will be fed a sequence of italian tweets and will need to determine if they are ironic or not.</p>
<p>I'm having problems building the initial part of the model, which takes the inputs and feeds them to the tokenizer in order to get a representation I can feed to BERT.</p>
<p>I can do it outside of the model-building context:</p>
<pre><code>my_phrase = &quot;Ciao, come va?&quot;
# an equivalent version is tokenizer(my_phrase, other parameters)
bert_input = tokenizer.encode(my_phrase, add_special_tokens=True, return_tensors='tf', max_length=110, padding='max_length', truncation=True) 
attention_mask = bert_input &gt; 0
outputs = bert(bert_input, attention_mask)['pooler_output']
</code></pre>
<p>but I'm having troubles building a model that does this. Here is the code for building such a model (the problem is in the first 4 lines ):</p>
<pre><code>def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = tokenizer(text_input, return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  
  X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(net)
  X = tf.keras.layers.Concatenate(axis=-1)([X, input_layer])
  X = tf.keras.layers.MaxPooling1D(20)(X)
  X = tf.keras.layers.SpatialDropout1D(0.4)(X)
  X = tf.keras.layers.Flatten()(X)
  X = tf.keras.layers.Dense(128, activation=&quot;relu&quot;)(X)
  X = tf.keras.layers.Dropout(0.25)(X)
  X = tf.keras.layers.Dense(2, activation='softmax')(X)

  model = tf.keras.Model(inputs=text_input, outputs = X) 
  
  return model
</code></pre>
<p>And when I call the function for creating this model I get this error:</p>
<blockquote>
<p>text input must of type <code>str</code> (single example), <code>List[str]</code> (batch or single pretokenized example) or <code>List[List[str]]</code> (batch of pretokenized examples).</p>
</blockquote>
<p>One thing I thought was that maybe I had to use the <code>tokenizer.batch_encode_plus</code> function which works with lists of strings:</p>
<pre class=""lang-py prettyprint-override""><code>class BertPreprocessingLayer(tf.keras.layers.Layer):
  def __init__(self, tokenizer, maxlength):
    super().__init__()
    self._tokenizer = tokenizer
    self._maxlength = maxlength
  
  def call(self, inputs):
    print(type(inputs))
    print(inputs)
    tokenized = tokenizer.batch_encode_plus(inputs, add_special_tokens=True, return_tensors='tf', max_length=self._maxlength, padding='max_length', truncation=True)
    return tokenized

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = BertPreprocessingLayer(tokenizer, 100)(text_input)
  outputs = bert(encoder_inputs)
  net = outputs['pooler_output']
  # ... same as above
</code></pre>
<p>but I get this error:</p>
<blockquote>
<p>batch_text_or_text_pairs has to be a list (got &lt;class 'keras.engine.keras_tensor.KerasTensor'&gt;)</p>
</blockquote>
<p>and beside the fact I haven't found a way to convert that tensor to a list with a quick google search, it seems weird that I have to go in and out of tensorflow in this way.</p>
<p>I've also looked up on the huggingface's <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"" rel=""noreferrer"">documentation</a> but there is only a single usage example, with a single phrase, and what they do is analogous at my &quot;out of model-building context&quot; example.</p>
<p>EDIT:</p>
<p>I also tried with <code>Lambda</code>s in this way:</p>
<pre><code>tf.executing_eagerly()

def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  return tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string, name='text')
  
  encoder_inputs = tf.keras.layers.Lambda(tokenize_tensor, name='tokenize')(text_input)
  ...
  
  outputs = bert(encoder_inputs)
</code></pre>
<p>but I get the following error:</p>
<blockquote>
<p>'Tensor' object has no attribute 'numpy'</p>
</blockquote>
<p>EDIT 2:</p>
<p>I also tried the approach suggested by @mdaoust of wrapping everything in a <code>tf.py_function</code> and got this error.</p>
<pre class=""lang-py prettyprint-override""><code>def py_func_tokenize_tensor(tensor):
  return tf.py_function(tokenize_tensor, [tensor], Tout=[tf.int32, tf.int32, tf.int32])
</code></pre>
<blockquote>
<p>eager_py_func() missing 1 required positional argument: 'Tout'</p>
</blockquote>
<p>Then I defined Tout as the type of the value returned by the tokenizer:</p>
<p><code>transformers.tokenization_utils_base.BatchEncoding</code></p>
<p>and got the following error:</p>
<blockquote>
<p>Expected DataType for argument 'Tout' not &lt;class
'transformers.tokenization_utils_base.BatchEncoding'&gt;</p>
</blockquote>
<p>Finally I unpacked the value in the BatchEncoding in the following way:</p>
<pre class=""lang-py prettyprint-override""><code>def tokenize_tensor(tensor):
  t = tensor.numpy()
  t = np.array([str(s, 'utf-8') for s in t])
  dictionary = tokenizer(t.tolist(), return_tensors='tf', add_special_tokens=True, max_length=110, padding='max_length', truncation=True)
  #unpacking
  input_ids = dictionary['input_ids']
  tok_type = dictionary['token_type_ids']
  attention_mask = dictionary['attention_mask']
  return input_ids, tok_type, attention_mask
</code></pre>
<p>And get an error in the line below:</p>
<pre class=""lang-py prettyprint-override""><code>...
outputs = bert(encoder_inputs)
</code></pre>
<blockquote>
<p>ValueError: Cannot take the length of shape with unknown rank.</p>
</blockquote>
","I'm trying to build the model illustrated in this picture: I obtained a pre-trained BERT and respective tokenizer from HuggingFace's transformers in the following way: The model will be fed a sequence of italian tweets and will need to determine if they are ironic or not. I'm having problems building the initial part of the model, which takes the inputs and feeds them to the tokenizer in order to get a representation I can feed to BERT. I can do it outside of the model-building context: but I'm having troubles building a model that does this. Here is the code for building such a model (the problem is in the first 4 lines ): And when I call the function for creating this model I get this error: One thing I thought was that maybe I had to use the tokenizer.batch_encode_plus function which works with lists of strings: but I get this error: and beside the fact I haven't found a way to convert that tensor to a list with a quick google search, it seems weird that I have to go in and out of tensorflow in this way. I've also looked up on the huggingface's documentation but there is only a single usage example, with a single phrase, and what they do is analogous at my ""out of model-building context"" example. EDIT: I also tried with Lambdas in this way: but I get the following error: EDIT 2: I also tried the approach suggested by @mdaoust of wrapping everything in a tf.py_function and got this error. Then I defined Tout as the type of the value returned by the tokenizer: transformers.tokenization_utils_base.BatchEncoding and got the following error: Finally I unpacked the value in the BatchEncoding in the following way: And get an error in the line below:",https://stackoverflow.com/questions/69195950,11579184,Inadequate Examples
69458522,What does tf.squeeze does to the audio and how can I load an mp3?,"<p>I'm using TensorFlow and I would like to be able to load audio and generate a spectrogram from it. I have little knowledge of how audio internally works.
Currently, this is the code I'm using:</p>
<pre><code>import pathlib
import tensorflow as tf
import tensorflow_io as tfio
import matplotlib.pyplot as plt

from IPython.display import Audio

data_dir = pathlib.Path('recordings')
sample_file = data_dir/'testA.mp3'
audio = tfio.audio.AudioIOTensor(str(sample_file))

# remove last dimension
audio_slice = audio[100:]
audio_tensor = tf.squeeze(audio_slice, axis=[-1])
#audio_tensor = audio.to_tensor()
tensor = tf.cast(audio_tensor, tf.float32) / 32768.0

print(&quot;Audio Tensor: &quot; + str(tensor))

plt.figure()
plt.plot(tensor.numpy())
plt.show()

# Convert to spectrogram
spectrogram = tfio.audio.spectrogram(tensor, nfft=512, window=512, stride=256)
    
plt.figure()
plt.imshow(tf.math.log(spectrogram).numpy())
plt.show()
</code></pre>
<p>I have been reading the documentation and in order to create a tensor I need to either use the tf.squeeze method or audio.to_tensor(). I have no clue what the tf.squeeze method does, but when I use it I get the error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[1], expected a dimension of 1, got 2 [Op:Squeeze]
</code></pre>
<p>If I instead use the method audio.to_tensor(), I'm unable to display the created spectrogram on the plt and instead I get the following error:</p>
<pre><code>TypeError: Invalid shape (28224, 1, 257) for image data
</code></pre>
","I'm using TensorFlow and I would like to be able to load audio and generate a spectrogram from it. I have little knowledge of how audio internally works. Currently, this is the code I'm using: I have been reading the documentation and in order to create a tensor I need to either use the tf.squeeze method or audio.to_tensor(). I have no clue what the tf.squeeze method does, but when I use it I get the error: If I instead use the method audio.to_tensor(), I'm unable to display the created spectrogram on the plt and instead I get the following error:",https://stackoverflow.com/questions/69458522,4999534,Documentation Replication on Other Examples
69509388,TF BERT input packer on more than two inputs,"<p>Some of the TensorFlow examples using BERT models show a use of the BERT preprocessor to &quot;pack&quot; inputs. E.g. in <a href=""https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb"" rel=""nofollow noreferrer"">this example</a>,</p>
<p><code>text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20))</code></p>
<p>The documentation implies that this works equally well with more than two input sentences, such that (I would expect) one can do something like:</p>
<p><code>text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok, tok], tf.constant(20))</code></p>
<p>However, so doing causes the error at the bottom[1] of this post.</p>
<p>I get that there isn't a matching signature; if I read this correctly (and I may not!), there's a signature for a single input and one for two. But what's the recommended way to pack more than two sentences into input suitable for a classification task, as suggested in the above colab?</p>
<p>1.</p>
<pre><code>ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (2 total):
    * [tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(&quot;inputs:0&quot;, shape=(None,), dtype=int32), row_splits=Tensor(&quot;inputs_2:0&quot;, shape=(None,), dtype=int64)), row_splits=Tensor(&quot;inputs_1:0&quot;, shape=(2,), dtype=int64)), tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(&quot;inputs_3:0&quot;, shape=(None,), dtype=int32), row_splits=Tensor(&quot;inputs_5:0&quot;, shape=(None,), dtype=int64)), row_splits=Tensor(&quot;inputs_4:0&quot;, shape=(2,), dtype=int64)), tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(&quot;inputs_6:0&quot;, shape=(None,), dtype=int32), row_splits=Tensor(&quot;inputs_8:0&quot;, shape=(None,), dtype=int64)), row_splits=Tensor(&quot;inputs_7:0&quot;, shape=(2,), dtype=int64))]
    * Tensor(&quot;seq_length:0&quot;, shape=(), dtype=int32)
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}

Option 2:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64), RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}

Option 3:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64), RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}

Option 4:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}```
</code></pre>
","Some of the TensorFlow examples using BERT models show a use of the BERT preprocessor to ""pack"" inputs. E.g. in this example, text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20)) The documentation implies that this works equally well with more than two input sentences, such that (I would expect) one can do something like: text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok, tok], tf.constant(20)) However, so doing causes the error at the bottom[1] of this post. I get that there isn't a matching signature; if I read this correctly (and I may not!), there's a signature for a single input and one for two. But what's the recommended way to pack more than two sentences into input suitable for a classification task, as suggested in the above colab? 1.",https://stackoverflow.com/questions/69509388,211714,Inadequate Examples
69587392,How to apply map() when working with a batched Dataset?,"<p>I am creating a timeseries Dataset using <code>tf.keras.utils.timeseries_dataset_from_array</code>.
According to the docs, it returns a <code>tf.data.Dataset</code> instance.
I also pass the batch size argument when calling the <code>timeseries_dataset_from_array</code> function, so my dataset is a BatchDataset.
I am using map on this batched Dataset (<code>ds</code>), passing <code>my_fun</code>.
Data in the code below is a pandas dataframe containing continuous timesteps.</p>
<p>What does the <code>my_fun</code> function expect as input parameters - aka what does it apply on each iteration? Whole batches of shape (samples, sequence_length, features) or a single element of shape (None, sequence_length, features)?</p>
<p>I am confused because when I print the single argument that I define in my <code>my_fun</code> function, it yields a shape of (None, None, features), but I cannot inspect it further...</p>
<p>My code is inspired from the TF tutorial (see split_window function)
<a href=""https://www.tensorflow.org/tutorials/structured_data/time_series"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/structured_data/time_series</a></p>
<pre class=""lang-py prettyprint-override""><code>def split_window(self, features):
   inputs = features[:, self.input_slice, :]
   labels = features[:, self.labels_slice, :]
   if self.label_columns is not None:
     labels = tf.stack(
         [labels[:, :, self.column_indices[name]] for name in 
          self.label_columns],
             axis=-1)

   # Slicing doesn't preserve static shape information, so set the shapes
   # manually. This way the `tf.data.Datasets` are easier to inspect.
   inputs.set_shape([None, self.input_width, None])
   labels.set_shape([None, self.label_width, None])

   return inputs, labels

data = np.array(data, dtype=np.float32)
ds = timeseries_dataset_from_array(
        data=data,
        targets=None,
        sequence_length=24,
        sequence_stride=1,
        shuffle=True,
        batch_size=32)
 
ds = ds.map(split_window)
</code></pre>
","I am creating a timeseries Dataset using tf.keras.utils.timeseries_dataset_from_array. According to the docs, it returns a tf.data.Dataset instance. I also pass the batch size argument when calling the timeseries_dataset_from_array function, so my dataset is a BatchDataset. I am using map on this batched Dataset (ds), passing my_fun. Data in the code below is a pandas dataframe containing continuous timesteps. What does the my_fun function expect as input parameters - aka what does it apply on each iteration? Whole batches of shape (samples, sequence_length, features) or a single element of shape (None, sequence_length, features)? I am confused because when I print the single argument that I define in my my_fun function, it yields a shape of (None, None, features), but I cannot inspect it further... My code is inspired from the TF tutorial (see split_window function) https://www.tensorflow.org/tutorials/structured_data/time_series",https://stackoverflow.com/questions/69587392,17161135,Documentation Replication on Other Examples
69672777,Compute Hessian of lossfunction in Tensorflow,"<p>I would like to compute the hessian of a loss function of a neural network in Tensorflow with respect to all the parameters (or trainable variables). By modifying the example code from the Tensorflow documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/GradientTape</a>) I managed to compute the hessian w.r.t the weight matrix for the first layer (if I'm not mistaken):</p>
<pre><code>with tf.GradientTape(persistent=True) as tape:
    loss = tf.reduce_mean(model(x,training=True)**2)
    g = tape.gradient(loss,model.trainable_variables[0]) 
    h=tape.jacobian(g,model.trainable_variables[0])
</code></pre>
<p>If I try to compute it w.r.t model.trainable_variables instead the tape.jacobian complains that 'list object has no attribute shape'. I instead tried to flatten the model.trainable_variables and compute it w.r.t the flattened vector:</p>
<pre><code>with tf.GradientTape(persistent=True) as tape:
    loss = tf.reduce_mean(model(x,training=True)**2)
    source = tf.concat([tf.reshape(x,[-1]) for x in model.trainable_variables],axis=0)
    g = tape.gradient(loss,source) 
    h=tape.jacobian(g,source)
   
</code></pre>
<p>The problem now is that g is empty (NoneType) for some reason. I noticed that source is tf.Tensor-type but model.trainable_variables[0] was of type tf.ResourceVariable so I tried changing this by declaring source as</p>
<pre><code>source = resource_variable_ops.ResourceVariable(tf.concat([tf.reshape(x,[-1]) for x in model.trainable_variables],axis=0))
</code></pre>
<p>This didn't change anything though, so I'm guessing that this is not the issue. I also thought that the problem might be that the source-variable is not watched, but it seems that it is set to trainable and even if i do tape.watch(source), g is still empty.</p>
<p>Does anybody know how I can solve this?</p>
","I would like to compute the hessian of a loss function of a neural network in Tensorflow with respect to all the parameters (or trainable variables). By modifying the example code from the Tensorflow documentation (https://www.tensorflow.org/api_docs/python/tf/GradientTape) I managed to compute the hessian w.r.t the weight matrix for the first layer (if I'm not mistaken): If I try to compute it w.r.t model.trainable_variables instead the tape.jacobian complains that 'list object has no attribute shape'. I instead tried to flatten the model.trainable_variables and compute it w.r.t the flattened vector: The problem now is that g is empty (NoneType) for some reason. I noticed that source is tf.Tensor-type but model.trainable_variables[0] was of type tf.ResourceVariable so I tried changing this by declaring source as This didn't change anything though, so I'm guessing that this is not the issue. I also thought that the problem might be that the source-variable is not watched, but it seems that it is set to trainable and even if i do tape.watch(source), g is still empty. Does anybody know how I can solve this?",https://stackoverflow.com/questions/69672777,9163968,Documentation Replication on Other Examples
69777802,How to create the same structure of tf.data.experimental.make_csv_dataset from pandas,"<p><code>tf.data.experimental.make_csv_dataset</code> creates a TF Dataset ready for Kears supervised training.</p>
<pre><code>titanic_file = tf.keras.utils.get_file(&quot;titanic_train.csv&quot;, &quot;https://storage.googleapis.com/tf-datasets/titanic/train.csv&quot;)
titanic = tf.data.experimental.make_csv_dataset(
    titanic_file,
    label_name=&quot;survived&quot;,
    batch_size=1,   # To compre with the head of CSV
    shuffle=False,  # To compre with the head of CSV
    header=True,
)
for row in titanic.take(1):  # Take the first batch 
    features = row[0]        # Diectionary
    label = row[1]
    
    for feature, value in features.items():
        print(f&quot;{feature:20s}: {value}&quot;)
    
    print(f&quot;label/survived      : {label}&quot;)    
-----
sex                 : [b'male']
age                 : [22.]
n_siblings_spouses  : [1]
parch               : [0]
fare                : [7.25]
class               : [b'Third']
deck                : [b'unknown']
embark_town         : [b'Southampton']
alone               : [b'n']
label/survived      : [0]
</code></pre>
<p>How to create the same from Pandas? Tried below but the label is dictionary instead of int32.</p>
<pre><code>df = pd.read_csv(titanic_file)
titanic_from_pandas = tf.data.Dataset.from_tensor_slices((
    dict(df.loc[:, df.columns != 'survived']),
    dict(df.loc[:, ['survived']])
))
for row in titanic_from_pandas.batch(1).take(1):  # Take the first batch 
    features = row[0]        # Diectionary
    label = row[1]
    
    for feature, value in features.items():
        print(f&quot;{feature:20s}: {value}&quot;)
    
    print(f&quot;label/survived      : {label}&quot;)    
---
sex                 : [b'male']
age                 : [22.]
n_siblings_spouses  : [1]
parch               : [0]
fare                : [7.25]
class               : [b'Third']
deck                : [b'unknown']
embark_town         : [b'Southampton']
alone               : [b'n']
label/survived      : {'survived': &lt;tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])&gt;}  &lt;-----
</code></pre>
<p>By the way, the data structure ready for Keras supervised training is (features, labels) but which document defines it?</p>
","tf.data.experimental.make_csv_dataset creates a TF Dataset ready for Kears supervised training. How to create the same from Pandas? Tried below but the label is dictionary instead of int32. By the way, the data structure ready for Keras supervised training is (features, labels) but which document defines it?",https://stackoverflow.com/questions/69777802,4281353,Lack of Alternative Solutions/Documentation
69792031,Explanation of tf.keras.layers.CategoryEncoding output_mode='multi_hot' behavior,"<h1>Question</h1>
<p>Please help understand the definition of <strong>multi hot encoding</strong> of tf.keras.layers.CategoryEncoding and the behavior of <code>output_mode='multi_hot'</code>.</p>
<h1>Background</h1>
<p>According to <a href=""https://stats.stackexchange.com/a/467672"">What exactly is multi-hot encoding and how is it different from one-hot?</a>:</p>
<blockquote>
<p>If you would use multi-hot-encoding you would first label-encode your classes, thus having only a single number which represents the presence of a class (e.g. 1 for 'dog') and then convert the numerical labels to binary vectors of size log2(5)=3.<br />
Examples:</p>
<pre><code>'cat'  = [0,0,0]  
'dog'  = [0,0,1]  
'fish' = [0,1,0]  
'bird' = [0,1,1]  
'ant'  = [1,0,0]   
</code></pre>
</blockquote>
<h1>Behaviour of <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/CategoryEncoding"" rel=""nofollow noreferrer"">tf.keras.layers.CategoryEncoding</a></h1>
<p>The document says <code>num_tokens</code> is the total number of tokens the layer should support.</p>
<blockquote>
<h3>args</h3>
<h4>num_tokens</h4>
<p>The total number of tokens the layer should support. All inputs to the layer must integers in the range 0 &lt;= value &lt; num_tokens, or an error will be thrown.</p>
<h4>output_mode</h4>
<ul>
<li>&quot;one_hot&quot;: Encodes each individual element in the input into an array of num_tokens size, containing a 1 at the element index. If the last dimension is size 1, will encode on that dimension. If the last dimension is not size 1, will append a new dimension for the encoded output.</li>
<li>&quot;multi_hot&quot;: Encodes each sample in the input into <strong>a single array of num_tokens size, containing a 1 for each vocabulary term present in the sample</strong>. Treats the last dimension as the sample dimension, if input shape is (..., sample_length), output shape will be (..., num_tokens).</li>
</ul>
</blockquote>
<p>According to the definitions of multi hot encoding above, I expected <code>tf.keras.layers.CategoryEncoding(num_tokens=5, output_mode=&quot;multi_hot&quot;)</code> encodes 5 tokens into an array of size 3.</p>
<p>However, the document says &quot;multi_hot&quot; encodes each sample into <strong>a single array of num_tokens size</strong>, containing a 1 for each vocabulary term present in the sample, and behaves as such.</p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices(tf.constant(['cat', 'dog', 'fish', 'bird']))

lookup = tf.keras.layers.StringLookup(max_tokens=5, oov_token='[UNK]')
lookup.adapt(dataset)
lookup.get_vocabulary()
---
['[UNK]', 'fish', 'dog', 'cat', 'bird']

mhe = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size(), output_mode=&quot;multi_hot&quot;)
print(f&quot;cat: {mhe(lookup(tf.constant('cat'))).numpy()}&quot;)
print(f&quot;dog: {mhe(lookup(tf.constant('dog'))).numpy()}&quot;)
---
cat: [0. 0. 0. 1. 0.]
dog: [0. 0. 1. 0. 0.]
</code></pre>
<p>Which has no difference from One Hot Encoding.</p>
<pre><code>ohe = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size(), output_mode=&quot;one_hot&quot;)
print(f&quot;cat: {ohe(lookup(tf.constant('cat'))).numpy()}&quot;)
print(f&quot;dog: {ohe(lookup(tf.constant('dog'))).numpy()}&quot;)
---
cat: [0. 0. 0. 1. 0.]
dog: [0. 0. 1. 0. 0.]
</code></pre>
<p>For multi value inputs, multi_hot only handles the first value.</p>
<pre><code>print(ohe(lookup(tf.constant(['cat', 'dog']))).numpy())
---
[[0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0.]]

print(mhe(lookup(tf.constant(['cat', 'dog']))).numpy())
---
[0. 0. 1. 1. 0.]
</code></pre>
<p>To handle multiple inputs, need to be 2D array.</p>
<pre><code>print(mhe(lookup(tf.constant([['cat'], ['dog']]))).numpy())
---
[[0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0.]]
</code></pre>
<p>Apparently the definition of <strong>multi hot encoding</strong> of <code>tf.keras.layers.CategoryEncoding</code> is not the same with the one in <a href=""https://stats.stackexchange.com/a/467672"">What exactly is multi-hot encoding and how is it different from one-hot?</a>.</p>
<h1>Related</h1>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/issues/52892"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/52892</a></li>
</ul>
","Please help understand the definition of multi hot encoding of tf.keras.layers.CategoryEncoding and the behavior of output_mode='multi_hot'. According to What exactly is multi-hot encoding and how is it different from one-hot?: The document says num_tokens is the total number of tokens the layer should support. According to the definitions of multi hot encoding above, I expected tf.keras.layers.CategoryEncoding(num_tokens=5, output_mode=""multi_hot"") encodes 5 tokens into an array of size 3. However, the document says ""multi_hot"" encodes each sample into a single array of num_tokens size, containing a 1 for each vocabulary term present in the sample, and behaves as such. Which has no difference from One Hot Encoding. For multi value inputs, multi_hot only handles the first value. To handle multiple inputs, need to be 2D array. Apparently the definition of multi hot encoding of tf.keras.layers.CategoryEncoding is not the same with the one in What exactly is multi-hot encoding and how is it different from one-hot?.",https://stackoverflow.com/questions/69792031,4281353,Lack of Alternative Solutions/Documentation
69843239,How does tf.keras.util.array_to_image() work with regards to memory?,"<p>I have image data that I want to use in a TensorFlow model, but I have to retrieve the image as an (Numpy) array of pixel values. From what I've read, TensorFlow has to read an image in as some image format and from some location. I know that <code>tf.keras.util.array_to_image()</code> can convert an array to a PIL instance of an image, and I know that there are several other libraries that have similar functionality, such as <code>PIL.Image.fromarray()</code>.</p>
<p>My problem is that I don't want to duplicate the image data by copying it to a new format. The API documentation for the <code>tf.keras.util.array_to_image()</code> says that it returns a &quot;PIL image instance&quot;. Does that mean that it is copying all array values to a new data structure and returning that, or is it creating an image data structure that references the original array pixel values?</p>
<p>As a follow-up question, if the keras method does duplicate the data (by having both the original array and the image instance have independent values), is there a way to have TensorFlow accept an array representation of an image without needing to duplicate it as a separate image file?</p>
","I have image data that I want to use in a TensorFlow model, but I have to retrieve the image as an (Numpy) array of pixel values. From what I've read, TensorFlow has to read an image in as some image format and from some location. I know that tf.keras.util.array_to_image() can convert an array to a PIL instance of an image, and I know that there are several other libraries that have similar functionality, such as PIL.Image.fromarray(). My problem is that I don't want to duplicate the image data by copying it to a new format. The API documentation for the tf.keras.util.array_to_image() says that it returns a ""PIL image instance"". Does that mean that it is copying all array values to a new data structure and returning that, or is it creating an image data structure that references the original array pixel values? As a follow-up question, if the keras method does duplicate the data (by having both the original array and the image instance have independent values), is there a way to have TensorFlow accept an array representation of an image without needing to duplicate it as a separate image file?",https://stackoverflow.com/questions/69843239,11626498,Documentation Replication on Other Examples
70196272,Overcoming incompatibilities between tensorflow 1.x and 2.x when trying to view layer activity with backend,"<p>I would like to run newer tensorflow routines like:</p>
<pre><code>    from tensorflow.keras.preprocessing import image_dataset_from_directory
</code></pre>
<p>for which I get error in 1.x:</p>
<p>ImportError: cannot import name image_dataset_from_directory</p>
<p>while preserving older functionality of 1.x like running the routine to see activations in various layers like:</p>
<pre><code>    K=tf.keras.backend
    func = K.function([base_model.input, K.learning_phase()],[layer.output for layer in base_model.layers if layer.output is not base_model.input]) 
</code></pre>
<p>for which I get the following error in tf 2.x:</p>
<p>ValueError: Input tensors to a Functional must come from <code>tf.keras.Input</code>. Received: 0 (missing previous layer metadata).</p>
<p>The code:</p>
<pre><code>    import tensorflow as tf
    from tensorflow.keras.preprocessing import image_dataset_from_directory

    IMG_SHAPE = (160, 160) + (3,)
    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                                   include_top=False,
                                                   weights='imagenet')

    K=tf.keras.backend
    func = K.function([base_model.input, K.learning_phase()],[layer.output for layer in base_model.layers if layer.output is not base_model.input])
</code></pre>
<p>The documentation I looked at suggests the problem may have something to do with eager computation mode eg <a href=""https://github.com/tensorflow/tensorflow/issues/34201"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/34201</a></p>
<p>But I cannot figure out how to resolve this.
Thank you for suggestions!</p>
",I would like to run newer tensorflow routines like: for which I get error in 1.x: ImportError: cannot import name image_dataset_from_directory while preserving older functionality of 1.x like running the routine to see activations in various layers like: for which I get the following error in tf 2.x: ValueError: Input tensors to a Functional must come from tf.keras.Input. Received: 0 (missing previous layer metadata). The code: The documentation I looked at suggests the problem may have something to do with eager computation mode eg https://github.com/tensorflow/tensorflow/issues/34201 But I cannot figure out how to resolve this. Thank you for suggestions!,https://stackoverflow.com/questions/70196272,11726927,Lack of Alternative Solutions/Documentation
70328363,Extra dimension to MaxPool1D layer from Conv1D layer,"<p>I'm very new to Tensorflow (this is my first project using it), and I don't really understand how input shapes work. I am trying to train a CNN-LSTM on a set of financial time series data.</p>
<p>For my use case, I have a <code>tf.keras.data.DataLoader</code> object which is meant to serve batches of training data to the model.</p>
<p>One training instance corresponds to the price history over the last 30 days, and hence should have shape <code>(30,)</code>.</p>
<p>running the following code:</p>
<pre><code>for x, y in train_ds:
    print(x, y)
    print(x.shape)
    break
</code></pre>
<p>I get that <code>x.shape</code> is <code>(4, 30)</code>, where the <code>Dataset</code> object I have defined serves training instances in batches of 4.</p>
<p>Here is my code:</p>
<pre><code># driver code for experiments
import keras
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras import layers

WIDTH = 30
BATCH_SIZE = 4

# load datasets (prepended with 'n' for 'normalized' )

nXtrain = np.load('cad_90s_nXtrain.npy')
nytrain = np.load('cad_90s_nytrain.npy')
nXval = np.load('cad_90s_nXval.npy')
nyval = np.load('cad_90s_nyval.npy')
nXtest = np.load('cad_90s_nXtest.npy')
nytest = np.load('cad_90s_nytest.npy')

# instantiate tensorflow Datasets
train_ds = tf.data.Dataset.from_tensor_slices((nXtrain, nytrain)).batch(BATCH_SIZE)
val_ds = tf.data.Dataset.from_tensor_slices((nXval, nyval)).batch(BATCH_SIZE)
test_ds = tf.data.Dataset.from_tensor_slices((nXtest, nytest)).batch(BATCH_SIZE)


input_shape = (BATCH_SIZE, WIDTH, 1 )

testnet = tf.keras.Sequential([
    layers.InputLayer(input_shape=input_shape),
    layers.Conv1D(filters=32,
                  kernel_size=3,
                  activation='tanh',
                  padding='same',
                  strides=1),
    layers.MaxPool1D(pool_size=2,
                     padding='same'),
    layers.ReLU(),
    layers.LSTM(units=64, dropout=0.2, activation='tanh'),
    layers.Dense(units=1)
])

testnet.build()
testnet.summary()
</code></pre>
<p>with accompanying error message:</p>
<pre><code>ValueError: Input 0 of layer &quot;max_pooling1d&quot; is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 4, 30, 32)

</code></pre>
<p>I don't understand what's going on--why is there an extra dimension coming out of the <code>Conv1D</code> layer? I mean, should the output of 1-D convolution not simply be
<code>(BATCH_SIZE, WIDTH, 32)</code> (padding was set to <code>'same'</code>)?</p>
<p>I apologize if this is addressed in the documentation, but I have been looking everywhere for an answer and I can't seem to fix this problem. I would really appreciate some help here.</p>
<p>Thanks!</p>
","I'm very new to Tensorflow (this is my first project using it), and I don't really understand how input shapes work. I am trying to train a CNN-LSTM on a set of financial time series data. For my use case, I have a tf.keras.data.DataLoader object which is meant to serve batches of training data to the model. One training instance corresponds to the price history over the last 30 days, and hence should have shape (30,). running the following code: I get that x.shape is (4, 30), where the Dataset object I have defined serves training instances in batches of 4. Here is my code: with accompanying error message: I don't understand what's going on--why is there an extra dimension coming out of the Conv1D layer? I mean, should the output of 1-D convolution not simply be (BATCH_SIZE, WIDTH, 32) (padding was set to 'same')? I apologize if this is addressed in the documentation, but I have been looking everywhere for an answer and I can't seem to fix this problem. I would really appreciate some help here. Thanks!",https://stackoverflow.com/questions/70328363,8163401,Lack of Alternative Solutions/Documentation
70363340,Question about tensorflow.tile with a tensor of 5 dimensions,"<p>I'm trying to understand the following thing from an implementation of a paper I'm currently reading:</p>
<p>In <code>tensorflow</code>, if I have a tensor <code>x</code> of shape <code>(4,64,5,5)</code></p>
<ul>
<li><p>Then I create a new dimension by doing</p>
<pre><code>x = x[:,:,tf.newaxis]
</code></pre>
<p>ending with a new tensor of <code>shape</code> <code>(4,64,1,5,5)</code></p>
</li>
<li><p>Then I do</p>
<pre><code>x = tf.tile(x, (1, 1, 5, 1, 1))
</code></pre>
</li>
</ul>
<p>ending up with something of shape <code>(4,64,5,5,5)</code></p>
<p>Reading the documentation for <code>tf.tile</code>, I still don't understand what is it exactly doing in this case. Am I replicating the new dimension for 5 times? And if yes, what is exactly placed in the new dimension by tensorflow? What am I exactly replicating?</p>
","I'm trying to understand the following thing from an implementation of a paper I'm currently reading: In tensorflow, if I have a tensor x of shape (4,64,5,5) ending up with something of shape (4,64,5,5,5) Reading the documentation for tf.tile, I still don't understand what is it exactly doing in this case. Am I replicating the new dimension for 5 times? And if yes, what is exactly placed in the new dimension by tensorflow? What am I exactly replicating?",https://stackoverflow.com/questions/70363340,14824108,Documentation Ambiguity
70747499,Using tf.map_fn when the function has multiple outputs,"<p>I can easily use tf.map_fn when the function has one output:</p>
<pre><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    return x[0]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p><strong>output:</strong></p>
<pre><code>tf.Tensor([1. 4.], shape=(2,), dtype=float32)
</code></pre>
<p>But, when the function has two outputs:</p>
<pre><code>def my_fun(x):
    return [x[0],x[1]]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p>I get an error. Not sure what is going on. I read the information about tf.map_fn in here <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/map_fn</a>, but not sure how to fix this:</p>
<p>map_fn also supports functions with multi-arity inputs and outputs:</p>
<p><em>If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems).
If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures.</em></p>
<p><strong>Output:</strong></p>
<pre><code>~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    317     _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types,
--&gt; 318                                            expand_composites)
    319   except (ValueError, TypeError) as e:

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-36-5b11c7fef461&gt; in &lt;module&gt;
      5     return [x[0],x[1]]
      6 
----&gt; 7 print(tf.map_fn(my_fun,tensaki))

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)
    266         back_prop=back_prop,
    267         swap_memory=swap_memory,
--&gt; 268         maximum_iterations=n)
    269     results_flat = [r.stack() for r in r_a]
    270 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   2712                                               list(loop_vars))
   2713       while cond(*loop_vars):
-&gt; 2714         loop_vars = body(*loop_vars)
   2715         if try_to_pack and not isinstance(loop_vars, (list, _basetuple)):
   2716           packed = True

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in &lt;lambda&gt;(i, lv)
   2703         cond = lambda i, lv: (  # pylint: disable=g-long-lambda
   2704             math_ops.logical_and(i &lt; maximum_iterations, orig_cond(*lv)))
-&gt; 2705         body = lambda i, lv: (i + 1, orig_body(*lv))
   2706       try_to_pack = False
   2707 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in compute(i, tas)
    256       packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])
    257       packed_fn_values = fn(packed_values)
--&gt; 258       nest.assert_same_structure(dtype or elems, packed_fn_values)
    259       flat_fn_values = output_flatten(packed_fn_values)
    260       tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    323                   &quot;Entire first structure:\n%s\n&quot;
    324                   &quot;Entire second structure:\n%s&quot;
--&gt; 325                   % (str(e), str1, str2))
    326 
    327 

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not
Entire first structure:
.
Entire second structure:
[., .]```
</code></pre>
","I can easily use tf.map_fn when the function has one output: output: But, when the function has two outputs: I get an error. Not sure what is going on. I read the information about tf.map_fn in here https://www.tensorflow.org/api_docs/python/tf/map_fn, but not sure how to fix this: map_fn also supports functions with multi-arity inputs and outputs: If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems). If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures. Output:",https://stackoverflow.com/questions/70747499,11861082,Documentation Replication on Other Examples
70880589,what does cardinality mean in relation to an image dataset?,"<p>After successfully creating a tensorflow image <code>Dataset</code> with:</p>
<p><code>dataset = tf.keras.utils.image_dataset_from_directory(...)</code></p>
<p>which returns</p>
<p><em>Found 21397 files belonging to 5 classes.
Using 17118 files for training.</em></p>
<p>There is the cardinality method:</p>
<p><code>dataset.cardinality()</code></p>
<p>which returns a tensor containing the single value</p>
<p><em>tf.Tensor(535, shape=(), dtype=int64)</em></p>
<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality"" rel=""nofollow noreferrer"">docs here</a> but I don't understand what 535 represents or why its different to the number of files?</p>
<p>I ask, because I would like to understand how cardinality plays into this equation:</p>
<p><code>steps_per_epoch = dataset.cardinality().numpy() // batch_size</code></p>
","After successfully creating a tensorflow image Dataset with: dataset = tf.keras.utils.image_dataset_from_directory(...) which returns Found 21397 files belonging to 5 classes. Using 17118 files for training. There is the cardinality method: dataset.cardinality() which returns a tensor containing the single value tf.Tensor(535, shape=(), dtype=int64) I've read the docs here but I don't understand what 535 represents or why its different to the number of files? I ask, because I would like to understand how cardinality plays into this equation: steps_per_epoch = dataset.cardinality().numpy() // batch_size",https://stackoverflow.com/questions/70880589,14777655,Lack of Alternative Solutions/Documentation
71019644,Equivalent tensorflow expression to numpy mask,"<p>I have a numpy array named PixelData of unknown shape, and I am using the following condition to filter values in the array greater than some value x using a mask:</p>
<pre><code>PixelData[PixelData&gt;=x] = PixelData[PixelData&gt;=x] - x
</code></pre>
<p>When I convert this numpy array to a tensor, I cannot perform the same masking operation. I have tried using tf.where as follows:</p>
<pre><code>PixelData = tf.where(PixelData&gt;=x, PixelData - x, PixelData)
</code></pre>
<p>In the official documentation, they always seem to define the mask dimensions in advance to equal the dimensions of the tensor being masked, but then they talk about the dimensions being broadcasted automatically, so I am a bit confused. Are these two functions equivalent? Are there any situations where they may produce different outputs?</p>
","I have a numpy array named PixelData of unknown shape, and I am using the following condition to filter values in the array greater than some value x using a mask: When I convert this numpy array to a tensor, I cannot perform the same masking operation. I have tried using tf.where as follows: In the official documentation, they always seem to define the mask dimensions in advance to equal the dimensions of the tensor being masked, but then they talk about the dimensions being broadcasted automatically, so I am a bit confused. Are these two functions equivalent? Are there any situations where they may produce different outputs?",https://stackoverflow.com/questions/71019644,17815854,Documentation Ambiguity
71090570,How do I create a tf.Tensor from a pandas DataFrame containing arrays?,"<p>I have a pandas DataFrame like below.</p>
<pre><code>import pandas as pd
import numpy as np
import tensorflow as tf  # Version 2.8.0
df = pd.DataFrame({&quot;id&quot;: 
                   [&quot;i123&quot;, &quot;i456&quot;],  
                   &quot;col&quot;: [np.array([&quot;igh&quot;, &quot;ghdd&quot;, &quot;yu&quot;]),
                           np.array([&quot;uh&quot;, &quot;lkk&quot;, &quot;nj&quot;])]})
print(df)
</code></pre>
<p>Output:</p>
<pre><code>    id      col
0   i123    [igh, ghdd, yu]
1   i456    [uh, lkk, nj]
</code></pre>
<p>I would to create a <code>Tensor</code> from the values of the <code>col</code> column, in order to use them in a specific use case. I have tried converting the values like</p>
<pre><code>values = df[&quot;col&quot;].to_numpy()
values
</code></pre>
<p>Which looks like:</p>
<pre><code>array([array(['igh', 'ghdd', 'yu'], dtype='&lt;U4'),
       array(['uh', 'lkk', 'nj'], dtype='&lt;U3')], dtype=object)
</code></pre>
<p>When I try to convert this to a Tensor, by</p>
<pre><code>tf.constant(values)
</code></pre>
<p>I get an exception:</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).
</code></pre>
<p>I can see from the <a href=""https://www.tensorflow.org/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">documentation</a> that the tf.constant method should work on a very similar array
<a href=""https://i.stack.imgur.com/zscVM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zscVM.png"" alt=""TF docs"" /></a></p>
<p>The <code>values</code> variable I create have <code>.shape</code> like <code>(2,)</code> while the image below have <code>(2, 3)</code>, which might be the problem.
I can't seem to get the dtype and/or shape to match exactly, and I'm unsure how to get it to work. Any ideas?</p>
","I have a pandas DataFrame like below. Output: I would to create a Tensor from the values of the col column, in order to use them in a specific use case. I have tried converting the values like Which looks like: When I try to convert this to a Tensor, by I get an exception: I can see from the documentation that the tf.constant method should work on a very similar array The values variable I create have .shape like (2,) while the image below have (2, 3), which might be the problem. I can't seem to get the dtype and/or shape to match exactly, and I'm unsure how to get it to work. Any ideas?",https://stackoverflow.com/questions/71090570,11764097,Documentation Replicability
71129505,"Is it possible to split a tensorflow dataset into train, validation AND test datasets when using image_dataset_from_directory?","<p>I am using <code>tf.keras.utils.image_dataset_from_directory</code> to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the <code>validation_split</code> parameter), I want to split it into training, testing, and validation subsets.</p>
<p>I have tried using <code>dataset.skip()</code> and <code>dataset.take()</code> to further split one of the resulting subsets, but these functions return a <code>SkipDataset</code> and a <code>TakeDataset</code> respectively (by the way, contrary to <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#take"" rel=""nofollow noreferrer"">the documentation</a>, where it is claimed that these functions return a <code>Dataset</code>). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history.</p>
<p>So, my question is: is there a way to split a <code>Dataset</code> into three subsets for training, validation and testing, so that all three subsets are also <code>Dataset</code> objects?</p>
<p><strong>Code used to load the data</strong></p>
<pre><code>def load_data_tf(data_path: str, img_shape=(256,256), batch_size: int=8):
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.2,
        subset=&quot;training&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.3,
        subset=&quot;validation&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    return train_ds, val_ds

train_dataset, test_val_ds = load_data_tf('data_folder', img_shape = (256,256), batch_size=8)
test_dataset = test_val_ds.take(686)
val_dataset = test_val_ds.skip(686)
</code></pre>
<p><strong>Model compilation and fitting</strong></p>
<pre><code>model.compile(optimizer='sgd',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])
history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, verbose=1)
</code></pre>
<p><strong>When using a normal <code>Dataset</code>, <code>val_accuracy</code> and <code>val_loss</code> are present in the history of the model:</strong></p>
<p><a href=""https://i.stack.imgur.com/Qn1Yf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qn1Yf.png"" alt=""Expected behaviour: when using a Dataset, validation metrics are calculated"" /></a></p>
<p><strong>But when using a <code>SkipDataset</code>, they are not:</strong></p>
<p><a href=""https://i.stack.imgur.com/GMnBM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GMnBM.png"" alt=""Using the SkipDataset produced by test_val_ds.take() leads to validation metrics disappearing from model history"" /></a></p>
<p><a href=""https://i.stack.imgur.com/omU5U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/omU5U.png"" alt=""val_accuracy and val_loss are not present in history keys when using a SkipDataset or a TakeDataset"" /></a></p>
","I am using tf.keras.utils.image_dataset_from_directory to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the validation_split parameter), I want to split it into training, testing, and validation subsets. I have tried using dataset.skip() and dataset.take() to further split one of the resulting subsets, but these functions return a SkipDataset and a TakeDataset respectively (by the way, contrary to the documentation, where it is claimed that these functions return a Dataset). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history. So, my question is: is there a way to split a Dataset into three subsets for training, validation and testing, so that all three subsets are also Dataset objects? Code used to load the data Model compilation and fitting When using a normal Dataset, val_accuracy and val_loss are present in the history of the model: But when using a SkipDataset, they are not:",https://stackoverflow.com/questions/71129505,11212528,Documentation Replication on Other Examples
71130645,Correct axes to use dot product to evaluate the final output of a listwise learning to rank model,"<p>I'm not being able to find the correct configuration to pass to a tf.keras.layers.Dot to make a pairwise dot product when the entries each have lists of values, like from a listwise learning to rank model. For instance, suppose:</p>
<pre><code>repeated_query_vector = [
  [[1, 2], [1, 2]],
  [[3, 4], [3, 4]]
]

document_vectors = [
  [[5, 6], [7, 8]],
  [[9, 10], [11, 12]],
]
</code></pre>
<p>Calling tf.keras.layers.Dot(axes=??)([repeated_query_vector, document_vectors]) I want the output to be like:</p>
<pre><code>[
  [1*5 + 2*6, 1*7 + 2*8]
  [3*9 + 4*10, 3*11 + 4*12]
]
</code></pre>
<p>All examples I found in the documentation have one dimension less than my use case. What would be the correct value of axes for this call?</p>
","I'm not being able to find the correct configuration to pass to a tf.keras.layers.Dot to make a pairwise dot product when the entries each have lists of values, like from a listwise learning to rank model. For instance, suppose: Calling tf.keras.layers.Dot(axes=??)([repeated_query_vector, document_vectors]) I want the output to be like: All examples I found in the documentation have one dimension less than my use case. What would be the correct value of axes for this call?",https://stackoverflow.com/questions/71130645,13262684,Inadequate Examples
71149271,"How to remove single feature from tensorflow dataset, how to use apply on single feture?","<p>I created dataset from csv file with dataset = tf.data.experimental.make_csv_dataset() function but My dataset has categorical and numeric features.</p>
<pre><code>dataset=
color  price weight
red    120    1.2
blue    80     2.0
green   90     3
</code></pre>
<p>Question 1:
The question is how can I  modify  only single feature, for example weight +2, to:</p>
<pre><code>dataset=
color  price weight
red    120    3.2
blue    80     4.0
green   90     5
</code></pre>
<p>I try to do something like:</p>
<pre><code>dataset = dataset.apply(lambda x: x['weight']+2)
</code></pre>
<p>but the error is: &quot;TypeError: 'FilterDataset' object is not subscriptable&quot;</p>
<p>Example from the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply</a> doesn't show it.</p>
<p>Question 2:
How can I remove single feature ? Is there any equivalent to pandas drop column?</p>
","I created dataset from csv file with dataset = tf.data.experimental.make_csv_dataset() function but My dataset has categorical and numeric features. Question 1: The question is how can I modify only single feature, for example weight +2, to: I try to do something like: but the error is: ""TypeError: 'FilterDataset' object is not subscriptable"" Example from the documentation https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply doesn't show it. Question 2: How can I remove single feature ? Is there any equivalent to pandas drop column?",https://stackoverflow.com/questions/71149271,13824257,Documentation Replication on Other Examples
71294464,@tf_gradient peculiar implementation in StyleGan,"<p>I've been reading the source code for the StyleGAN implementation, and I cannot understand the peculiar use of the <code>@tf_gradient</code> decorator. Let us take the concrete example of their implementation of <code>Leaky_Relu</code>. The way I would do it is as follows :</p>
<pre><code>def myLRelu(x,alpha=0.2):
    alpha = tf.constant(alpha, dtype=x.dtype, name='alpha')
    @tf.custom_gradient
    def func(x):
        y = tf.maximum(x, x * alpha)
        def grad(dy):
            dx = tf.where(y &gt;= 0, dy, dy * alpha)
            return dx
        return y, grad
    return func(x)
</code></pre>
<p>Which follows the tf documentation for the use of tf.custom_gradient. But in the styleGan paper, they implement it as follows (I removed the &quot;variable_scope&quot; in my implementation as I'm not sure what it does):</p>
<pre><code>def leaky_relu(x, alpha=0.2):
    with tf.variable_scope('LeakyReLU'):
        alpha = tf.constant(alpha, dtype=x.dtype, name='alpha')
        @tf.custom_gradient
        def func(x):
            y = tf.maximum(x, x * alpha)
            @tf.custom_gradient
            def grad(dy):
                dx = tf.where(y &gt;= 0, dy, dy * alpha)
                return dx, lambda ddx: tf.where(y &gt;= 0, ddx, ddx * alpha)
            return y, grad
        return func(x)
</code></pre>
<p>There are two <code>@tf.custom_gradient</code> decorators used, and I don't understand why since there clearly aren't any second order derivatives being computed (as they are identically 0 anyway for LRelu). Is this a trick to somehow speed up computations ? If so, how does it work ?</p>
<p>EDIT : To clarify why I think this is somehow a &quot;trick&quot; to make computations of gradients faster, the authors make the following comment in the code :</p>
<pre><code># High-level ops for manipulating 4D activation tensors.
# The gradients of these are meant to be as efficient as possible.
</code></pre>
<p>And for completeness, here is the <a href=""https://github.com/NVlabs/stylegan/"" rel=""nofollow noreferrer"">repo</a> from which I took the code from</p>
","I've been reading the source code for the StyleGAN implementation, and I cannot understand the peculiar use of the @tf_gradient decorator. Let us take the concrete example of their implementation of Leaky_Relu. The way I would do it is as follows : Which follows the tf documentation for the use of tf.custom_gradient. But in the styleGan paper, they implement it as follows (I removed the ""variable_scope"" in my implementation as I'm not sure what it does): There are two @tf.custom_gradient decorators used, and I don't understand why since there clearly aren't any second order derivatives being computed (as they are identically 0 anyway for LRelu). Is this a trick to somehow speed up computations ? If so, how does it work ? EDIT : To clarify why I think this is somehow a ""trick"" to make computations of gradients faster, the authors make the following comment in the code : And for completeness, here is the repo from which I took the code from",https://stackoverflow.com/questions/71294464,3842374,Documentation Replication on Other Examples
71335830,What is the difference between tf.keras.layers.Input() and tf.keras.layers.Flatten(),"<p>I have seen multiple uses of both <code>tf.keras.layers.Flatten()</code> (ex. <a href=""https://www.tensorflow.org/tutorials/generative/autoencoder#first_example_basic_autoencoder"" rel=""nofollow noreferrer"">here</a>) and <code>tf.keras.layers.Input()</code> (ex. <a href=""https://www.tensorflow.org/tutorials/generative/autoencoder#define_a_convolutional_autoencoder"" rel=""nofollow noreferrer"">here</a>). After reading the documentation, it is not clear to me</p>
<ol>
<li>whether either of them uses the other</li>
<li>whether both can be used interchangeably when introducing to a model an input layer (let's say with dimensions <code>(64, 64)</code>)</li>
</ol>
","I have seen multiple uses of both tf.keras.layers.Flatten() (ex. here) and tf.keras.layers.Input() (ex. here). After reading the documentation, it is not clear to me",https://stackoverflow.com/questions/71335830,9758352,Documentation Ambiguity
71588962,Solving a set of linear systems in tensorflow,"<p>I'm having a problem understanding the working mechanism of tensorflow's function: tf.linalg.solve.
I want to solve a set of linear systems (AX = Y), where the linear coefficients (A) were shared but there are multiple batches of Y, which are different.
Using numpy, I can simply do it via:</p>
<pre><code>np.random.seed(0)
mtx = np.random.uniform(size= (1,4,4))
vec = np.random.uniform(size= (100,4,1))
solution = np.linalg.solve(mtx,vec)
print(abs(np.matmul(mtx,solution) - vec).max())
# 5.551115123125783e-16
</code></pre>
<p>which gives me a quite consistent solution.
But when I switch to tensorflow, it gives me the results:</p>
<pre><code>mtx = tf.random.uniform(shape = (1,4,4))
vec = tf.random.uniform(shape = (100,4,1))
solution = tf.linalg.solve(mtx,vec)
print(tf.math.reduce_max(abs(tf.matmul(mtx,solution) - vec))) 
# tf.Tensor(1.3136615, shape=(), dtype=float32)
</code></pre>
<p>According to the document, I assume the solution should be solved according to the corresponding vec. But it does not seem to give me the expected results in tensorflow. Since I'm a new user, I could have messed up something.
It would be appreciated if any information could be offered.</p>
","I'm having a problem understanding the working mechanism of tensorflow's function: tf.linalg.solve. I want to solve a set of linear systems (AX = Y), where the linear coefficients (A) were shared but there are multiple batches of Y, which are different. Using numpy, I can simply do it via: which gives me a quite consistent solution. But when I switch to tensorflow, it gives me the results: According to the document, I assume the solution should be solved according to the corresponding vec. But it does not seem to give me the expected results in tensorflow. Since I'm a new user, I could have messed up something. It would be appreciated if any information could be offered.",https://stackoverflow.com/questions/71588962,12416654,Documentation Ambiguity
71596616,Predicting Data using an Untrained Keras Model,"<p>Essentially, I want to propagate data through a Keras model, without first training the Keras model. I tried using both predict() and feeding in raw tensors into the model.</p>
<p>The data is a 2D Numpy float64 array with shape (3, 3), filled entirely with zeros.</p>
<p>The model itself is outlined below:</p>
<pre><code>inputs = keras.Input(shape=(3,), batch_size=1)
FFNNlayer1 = keras.layers.Dense(100, activation='relu')(inputs)
FFNNlayer2 = keras.layers.Dense(100, activation='relu')(FFNNlayer1)
numericalOutput = keras.layers.Dense(3, activation='sigmoid')(FFNNlayer2)
categoricalOutput = keras.layers.Dense(9, activation='softmax')(FFNNlayer2)
outputs = keras.layers.concatenate([numericalOutput, categoricalOutput])
hyperparameters = keras.Model(inputs=inputs, outputs=outputs, name=&quot;hyperparameters&quot;)
hyperparameters.summary()
</code></pre>
<p>The model needed two different activation functions in it's output layer, hence why I used Functional API.</p>
<p>I first attempted to use <code>hyperparameter.predict(data[0])</code>, but kept getting the following error:</p>
<pre><code>WARNING:tensorflow:Model was constructed with shape (1, 3) for input KerasTensor(type_spec=TensorSpec(shape=(1, 3), dtype=tf.float32, name='input_15'), name='input_15', description=&quot;created by layer 'input_15'&quot;), but it was called on an input with incompatible shape (None,).
Traceback (most recent call last):

  File &quot;&lt;ipython-input-144-4c4a629eaefa&gt;&quot;, line 1, in &lt;module&gt;
    mainNet.hyperparameters.predict([dataset_info[0]])

  File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File &quot;C:\Users\hudso\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\func_graph.py&quot;, line 1129, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)

ValueError: in user code:

    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1621, in predict_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1611, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1604, in run_step  **
        outputs = model.predict_step(data)
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\training.py&quot;, line 1572, in predict_step
        return self(x, training=False)
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\input_spec.py&quot;, line 227, in assert_input_compatibility
        raise ValueError(f'Input {input_index} of layer &quot;{layer_name}&quot; '

    ValueError: Exception encountered when calling layer &quot;hyperparameters&quot; (type Functional).
    
    Input 0 of layer &quot;dense_20&quot; is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)
    
    Call arguments received:
      • inputs=('tf.Tensor(shape=(None,), dtype=float32)',)
      • training=False
      • mask=None
</code></pre>
<p>I fiddled around with array dimensions a bit, but the model continued to give the same error. I then tried feeding raw tensors into the model, with the following code:</p>
<pre><code>tensorflow_dataset_info =  tf.data.Dataset.from_tensor_slices([dataset_info[0]]).batch(1)
aaaaa = enumerate(tensorflow_dataset_info)
predictions = mainNet.hyperparameters(aaaaa)
</code></pre>
<p>This code continued to give the following error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-143-df51fe8fd203&gt;&quot;, line 1, in &lt;module&gt;
    hyperparameters = mainNet.hyperparameters(enumerate(tensorflow_dataset_info))

  File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File &quot;C:\Users\hudso\anaconda3\lib\site-packages\keras\engine\input_spec.py&quot;, line 196, in assert_input_compatibility
    raise TypeError(f'Inputs to a layer should be tensors. Got: {x}')

TypeError: Inputs to a layer should be tensors. Got: &lt;enumerate object at 0x000001F60081EA40&gt;
</code></pre>
<p>I've looked online for a while, and I've searched through the tf.data documentation, but I'm still not sure how to fix this. Again, I've tried multiple variations of this code, and I continue to get mostly the same errors.</p>
","Essentially, I want to propagate data through a Keras model, without first training the Keras model. I tried using both predict() and feeding in raw tensors into the model. The data is a 2D Numpy float64 array with shape (3, 3), filled entirely with zeros. The model itself is outlined below: The model needed two different activation functions in it's output layer, hence why I used Functional API. I first attempted to use hyperparameter.predict(data[0]), but kept getting the following error: I fiddled around with array dimensions a bit, but the model continued to give the same error. I then tried feeding raw tensors into the model, with the following code: This code continued to give the following error: I've looked online for a while, and I've searched through the tf.data documentation, but I'm still not sure how to fix this. Again, I've tried multiple variations of this code, and I continue to get mostly the same errors.",https://stackoverflow.com/questions/71596616,11442631,Documentation Ambiguity
71619495,Image normalization by tf.image.convert_image_dtype function,"<p>According to documentation <code>tf.image.convert_image_dtype</code> &quot;Images that are represented using floating point values are expected to have values in the range [0,1).&quot;</p>
<p>But in the keras tutorial(<a href=""https://keras.io/examples/vision/cutmix/"" rel=""nofollow noreferrer"">https://keras.io/examples/vision/cutmix/</a>) i have seen the following preprocessing function:</p>
<pre><code>def preprocess_image(image, label):
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    image = tf.image.convert_image_dtype(image, tf.float32) / 255.0
    return image, label
</code></pre>
<p>My question is: why did they divide by 255, when <code>tf.image.convert_image_dtype</code> already did that job?</p>
","According to documentation tf.image.convert_image_dtype ""Images that are represented using floating point values are expected to have values in the range [0,1)."" But in the keras tutorial(https://keras.io/examples/vision/cutmix/) i have seen the following preprocessing function: My question is: why did they divide by 255, when tf.image.convert_image_dtype already did that job?",https://stackoverflow.com/questions/71619495,5094589,Documentation Ambiguity
71791115,Nan Loss when training Deep neural Recommender model using tensorflow,"<p>I am trying to follow <a href=""https://www.tensorflow.org/recommenders/examples/deep_recommenders"" rel=""nofollow noreferrer"">tensorflow documentation</a> and applying same technique to one of toy dataset.</p>
<p>During training I am getting all loss as Nan. I have tried to debug the same using Debugger V2 and I could see that <code>tf.keras.layers.GlobalAveragePooling1D</code> is giving Nan due to division by 0, which is causing all values to be Nan during backpropagation. But what is not clear from the debugger V2 GUI why the sum is becoming 0. I did try to reduce the number of features and the size of the dataset, but each of this activity is giving me new error (probably I shall start a separate question thread for each issues at a later point ).</p>
<p>Below is the code for reference. I am providing the dataset as well <a href=""https://drive.google.com/file/d/1z954Djz8IntSzMP6velSdMGWTW_yBUAn/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>. I had tried below code on Google Colab.</p>
<pre><code>import os
import pprint
import tempfile

from typing import Dict, Text

import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_datasets as tfds

tf.debugging.experimental.enable_dump_debug_info(
    &quot;./tfdbg2_logdir&quot;,
    tensor_debug_mode=&quot;FULL_HEALTH&quot;,
    circular_buffer_size=-1)

!pip install -q tensorflow-recommenders
import tensorflow_recommenders as tfrs  
</code></pre>
<p>Preparing Data</p>
<pre><code>ds=pd.read_csv('train_recom.csv')
ds['year'].replace(0,1,inplace=True)
ds_song=ds.groupby(['song_id','title','release','artist_name','year']).size().reset_index().rename(columns={0:'count'})
ds_song.to_csv('songs_details.csv')
ds.to_csv('train_recom_transformed.csv')
</code></pre>
<p>Reading data to tensorflow dataset</p>
<pre><code>ratings = tf.data.experimental.make_csv_dataset(
    &quot;./train_recom_transformed.csv&quot;,
    batch_size=5,
    select_columns=['user_id', 'song_id', 'listen_count', 'title', 'release', 'artist_name',
       'year'],
    header=True,
    num_epochs=1,
    ignore_errors=False,)
songs = tf.data.experimental.make_csv_dataset(
    &quot;./songs_details.csv&quot;,
    batch_size=128,
    select_columns=['song_id','title','release','artist_name','year'],
    num_epochs=1,
    ignore_errors=True,)
ratings = ratings.unbatch().map(lambda x: {
    &quot;song_id&quot;: x[&quot;song_id&quot;],
    &quot;user_id&quot;: x[&quot;user_id&quot;],
    &quot;release&quot; : x[&quot;release&quot;],
    &quot;artist_name&quot; : x[&quot;artist_name&quot;],
    &quot;title&quot; : x[&quot;title&quot;],
    &quot;year&quot; : x[&quot;year&quot;],
    &quot;listen_count&quot;: x[&quot;listen_count&quot;]
})
songs = songs.unbatch().map(lambda x: x[&quot;song_id&quot;]) 
</code></pre>
<p>Preparing train and test dataset</p>
<pre><code>tf.random.set_seed(42)
shuffled = ratings.shuffle(16000, seed=42, reshuffle_each_iteration=False)

train = shuffled.take(12000)
test = shuffled.skip(12000).take(4000)
cached_train = train.shuffle(100_000).batch(1200).cache()
cached_test = test.batch(400).cache()

title = songs.batch(1000)
user_ids = ratings.batch(1_000_000).map(lambda x: x[&quot;user_id&quot;])
unique_song_titles = np.unique(np.concatenate(list(title)))
unique_user_ids = np.unique(np.concatenate(list(user_ids)))
year_data=np.concatenate(list(ratings.map(lambda x: x['year']).batch(4000)))
</code></pre>
<p>User model class</p>
<pre><code>class UserModel(tf.keras.Model):

    def __init__(self):
        super().__init__()

        max_tokens = 1_000_000

        embedding_dimension = 32
        self.user_embedding = tf.keras.Sequential([
            tf.keras.layers.StringLookup(
                vocabulary=unique_user_ids, mask_token=None),
            tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)
          ])

        self.release_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            max_tokens=max_tokens)
        
        self.release_text_embedding = tf.keras.Sequential([
          self.release_vectorizer,
          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True,input_length=144),
          tf.keras.layers.GlobalAveragePooling1D(),
        ])

        self.release_vectorizer.adapt(np.concatenate(list(ratings.map(lambda x: x['release']).batch(4000))))

        self.artist_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            max_tokens=max_tokens)
        self.artist_text_embedding = tf.keras.Sequential([
          self.artist_vectorizer,
          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),
          tf.keras.layers.GlobalAveragePooling1D(),
        ])
        self.artist_vectorizer.adapt(np.concatenate(list(ratings.map(lambda x: x['artist_name']).batch(4000))))
        
        self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            max_tokens=max_tokens)
        self.title_text_embedding = tf.keras.Sequential([
          self.title_vectorizer,
          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),
          tf.keras.layers.GlobalAveragePooling1D(),
        ])
        self.title_vectorizer.adapt(np.concatenate(list(ratings.map(lambda x: x['title']).batch(4000))))
        
        self.year_embedding = tf.keras.Sequential([
              tf.keras.layers.Embedding(len(year_data) + 1, 32),
            ])

    def call(self, inputs):
      return tf.concat([
          self.user_embedding(inputs['user_id']),
          self.release_text_embedding(inputs['release'])
          ,
          self.year_embedding(inputs['year']), 
          self.artist_text_embedding(inputs['artist_name']),
          self.title_text_embedding(inputs['title']),
             ], axis=1)
</code></pre>
<p>Item model</p>
<pre><code>class ItemModel(tf.keras.Model):

    def __init__(self):
        super().__init__()

        max_tokens = 10_000

        embedding_dimension = 32

        ## embed title from unique_song_titles
        self.title_embedding = tf.keras.Sequential([
        tf.keras.layers.StringLookup(
            vocabulary=unique_song_titles, mask_token=None),
        tf.keras.layers.Embedding(len(unique_song_titles) + 1, embedding_dimension)
      ])

    def call(self, inputs):
      return self.title_embedding(inputs)
</code></pre>
<p>Query model . Creating Deep model</p>
<pre><code>class QueryModel(tf.keras.Model):
  &quot;&quot;&quot;Model for encoding user queries.&quot;&quot;&quot;

  def __init__(self, layer_sizes):
    &quot;&quot;&quot;Model for encoding user queries.

    Args:
      layer_sizes:
        A list of integers where the i-th entry represents the number of units
        the i-th layer contains.
    &quot;&quot;&quot;
    super().__init__()

    # We first use the user model for generating embeddings.
    self.embedding_model = UserModel()

    # Then construct the layers.
    self.dense_layers = tf.keras.Sequential()

    # Use the ReLU activation for all but the last layer.
    for layer_size in layer_sizes[:-1]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=&quot;relu&quot;))

    # No activation for the last layer.
    for layer_size in layer_sizes[-1:]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size))

  def call(self, inputs):
    feature_embedding = self.embedding_model(inputs)
    return self.dense_layers(feature_embedding)
</code></pre>
<p>Creating deep model for the Item model</p>
<pre><code>class CandidateModel(tf.keras.Model):
  &quot;&quot;&quot;Model for encoding movies.&quot;&quot;&quot;

  def __init__(self, layer_sizes):
    &quot;&quot;&quot;Model for encoding movies.

    Args:
      layer_sizes:
        A list of integers where the i-th entry represents the number of units
        the i-th layer contains.
    &quot;&quot;&quot;
    super().__init__()

    self.embedding_model = ItemModel()

    # Then construct the layers.
    self.dense_layers = tf.keras.Sequential()

    # Use the ReLU activation for all but the last layer.
    for layer_size in layer_sizes[:-1]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=&quot;relu&quot;))

    # No activation for the last layer.
    for layer_size in layer_sizes[-1:]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size))

  def call(self, inputs):
    feature_embedding = self.embedding_model(inputs)
    return self.dense_layers(feature_embedding)
</code></pre>
<p>Combining both query and candidate model</p>
<pre><code>class SongModel(tfrs.models.Model):

    def __init__(self, layer_sizes):
        super().__init__()
        self.query_model = QueryModel(layer_sizes)
        self.candidate_model = CandidateModel(layer_sizes)
        self.task = tfrs.tasks.Retrieval(
          metrics=tfrs.metrics.FactorizedTopK(
              candidates=songs.batch(128).map(self.candidate_model),
          ),
      )

    def compute_loss(self, features, training=False):
        print('type of feature ----',type(features))

        query_embeddings = self.query_model({
            &quot;user_id&quot;: features[&quot;user_id&quot;]
            ,
                &quot;release&quot; : features[&quot;release&quot;]
                ,
                &quot;artist_name&quot; : features[&quot;artist_name&quot;],
                &quot;title&quot;: features[&quot;title&quot;],
                &quot;year&quot; : features[&quot;year&quot;],
        })

        item_embeddings = self.candidate_model(features[&quot;song_id&quot;])

        return self.task(query_embeddings, item_embeddings)
</code></pre>
<p>training the model</p>
<pre><code>model = SongModel([32])
model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))
model_hist = model.fit(cached_train, epochs=9)
</code></pre>
<p>Below id the outout that I got</p>
<pre><code>WARNING:tensorflow:Failed to read source code from path: /content/&lt;ipython-input-26-fdc864fc30cf&gt;. Reason: Source path neither exists nor can be loaded as a .par file: /content/&lt;ipython-input-26-fdc864fc30cf&gt;
WARNING:tensorflow:Failed to read source code from path: /content/&lt;ipython-input-25-e3009db55439&gt;. Reason: Source path neither exists nor can be loaded as a .par file: /content/&lt;ipython-input-25-e3009db55439&gt;
Epoch 1/9
type of feature ---- &lt;class 'dict'&gt;
WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_10_input'), name='embedding_10_input', description=&quot;created by layer 'embedding_10_input'&quot;), but it was called on an input with incompatible shape (None,).
type of feature ---- &lt;class 'dict'&gt;
WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_10_input'), name='embedding_10_input', description=&quot;created by layer 'embedding_10_input'&quot;), but it was called on an input with incompatible shape (None,).
10/10 [==============================] - 63s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0022 - factorized_top_k/top_10_categorical_accuracy: 0.0033 - factorized_top_k/top_50_categorical_accuracy: 0.0073 - factorized_top_k/top_100_categorical_accuracy: 0.0103 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 2/9
10/10 [==============================] - 9s 945ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 3/9
10/10 [==============================] - 10s 953ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 4/9
10/10 [==============================] - 9s 948ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 5/9
10/10 [==============================] - 10s 966ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 6/9
10/10 [==============================] - 10s 955ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 7/9
10/10 [==============================] - 10s 955ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 8/9
10/10 [==============================] - 10s 958ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 9/9
10/10 [==============================] - 10s 971ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
</code></pre>
","I am trying to follow tensorflow documentation and applying same technique to one of toy dataset. During training I am getting all loss as Nan. I have tried to debug the same using Debugger V2 and I could see that tf.keras.layers.GlobalAveragePooling1D is giving Nan due to division by 0, which is causing all values to be Nan during backpropagation. But what is not clear from the debugger V2 GUI why the sum is becoming 0. I did try to reduce the number of features and the size of the dataset, but each of this activity is giving me new error (probably I shall start a separate question thread for each issues at a later point ). Below is the code for reference. I am providing the dataset as well here. I had tried below code on Google Colab. Preparing Data Reading data to tensorflow dataset Preparing train and test dataset User model class Item model Query model . Creating Deep model Creating deep model for the Item model Combining both query and candidate model training the model Below id the outout that I got",https://stackoverflow.com/questions/71791115,12271381,Documentation Replication on Other Examples
71933464,How to make true_fn of tf.cond skip a for loop in tensorflow v1.0/python?,"<p>I want to use <code>tf.cond</code> to mimic the python <code>if-else</code> logic in the <code>_preprocessing_fn</code> of <code>transform.py</code>.</p>
<p>Specifically, if the condition of <code>tf.cond</code> is true, I want to skip the current iteration of the for loop.</p>
<p>This seems problematic because <code>true_fn</code> and <code>false_fn</code> parameters of <code>tf.cond</code> are expected to return Tensors according to the documentation.</p>
<p>However, in my case, I want <code>true_fn</code> (aka <code>skip_feature_fn</code>)to simply &quot;continue&quot; to the next for loop iteration. Also, I want <code>false_fn</code> to take in two inputs (<code>feature</code> and <code>sp</code>) and simply feed them to some other API (e.g. <code>tft.vocabulary</code>).
I don't expect either of <code>true_fn</code> or <code>false_fn</code> to return anything.</p>
<p>Could someone help me accomplish my goal?</p>
<p>Here is the code snippet I'm working with:</p>
<pre><code>def _preprocessing_fn(inputs, category_features=features.STRING_FEATURES):
  outputs = transform_lib.preprocessing_helper_fn(
      inputs, used_features=category_features)

  for feature in category_features:
    if feature:
      sp = outputs[feature]
      tf.cond(
          tf.equal(sp.dense_shape[1], 0), skip_feature_fn, lambda: process_feature_further(
              feature,
              sp,
          ))

  return outputs
</code></pre>
<p>Thank you.</p>
","I want to use tf.cond to mimic the python if-else logic in the _preprocessing_fn of transform.py. Specifically, if the condition of tf.cond is true, I want to skip the current iteration of the for loop. This seems problematic because true_fn and false_fn parameters of tf.cond are expected to return Tensors according to the documentation. However, in my case, I want true_fn (aka skip_feature_fn)to simply ""continue"" to the next for loop iteration. Also, I want false_fn to take in two inputs (feature and sp) and simply feed them to some other API (e.g. tft.vocabulary). I don't expect either of true_fn or false_fn to return anything. Could someone help me accomplish my goal? Here is the code snippet I'm working with: Thank you.",https://stackoverflow.com/questions/71933464,4982651,Documentation Replication on Other Examples
72165812,Connecting BatchDataset with Keras VGG16 preprocess_input,"<p>I am using <code>tf.keras.preprocessing.image_dataset_from_directory</code> to get a <code>BatchDataset</code>, where the dataset has 10 classes.</p>
<p>I am trying to integrate this <code>BatchDataset</code> with a Keras <code>VGG16</code> (<a href=""https://keras.io/api/applications/vgg/"" rel=""nofollow noreferrer"">docs</a>) network.  From the docs:</p>
<blockquote>
<p>Note: each Keras Application expects a specific kind of input preprocessing. For VGG16, call <code>tf.keras.applications.vgg16.preprocess_input</code> on your inputs before passing them to the model.</p>
</blockquote>
<p>However, I am struggling to get this <code>preprocess_input</code> working with a <code>BatchDataset</code>.  <strong>Can you please help me figure out how to connect these two dots?</strong></p>
<p>Please see the below code:</p>
<pre class=""lang-py prettyprint-override""><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_data_dir, image_size=(224, 224))
train_ds = tf.keras.applications.vgg16.preprocess_input(train_ds)
</code></pre>
<p>This will throw <code>TypeError: 'BatchDataset' object is not subscriptable</code>:</p>
<pre><code>Traceback (most recent call last):
  ...
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/vgg16.py&quot;, line 232, in preprocess_input
    return imagenet_utils.preprocess_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 117, in preprocess_input
    return _preprocess_symbolic_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 278, in _preprocess_symbolic_input
    x = x[..., ::-1]
TypeError: 'BatchDataset' object is not subscriptable
</code></pre>
<p>From <a href=""https://github.com/tensorflow/tensorflow/issues/39338"" rel=""nofollow noreferrer"">TypeError: 'DatasetV1Adapter' object is not subscriptable</a> (from <a href=""https://stackoverflow.com/q/61642569/11163122"">BatchDataset not subscriptable when trying to format Python dictionary as table</a>) the suggestion was to use:</p>
<pre class=""lang-py prettyprint-override""><code>train_ds = tf.keras.applications.vgg16.preprocess_input(
    list(train_ds.as_numpy_iterator())
)
</code></pre>
<p>However, this also fails:</p>
<pre><code>Traceback (most recent call last):
  ...
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/vgg16.py&quot;, line 232, in preprocess_input
    return imagenet_utils.preprocess_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 117, in preprocess_input
    return _preprocess_symbolic_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 278, in _preprocess_symbolic_input
    x = x[..., ::-1]
TypeError: list indices must be integers or slices, not tuple
</code></pre>
<p>This is all using <code>Python==3.10.3</code> with <code>tensorflow==2.8.0</code>.</p>
<p>How can I get this working?  Thank you in advance.</p>
","I am using tf.keras.preprocessing.image_dataset_from_directory to get a BatchDataset, where the dataset has 10 classes. I am trying to integrate this BatchDataset with a Keras VGG16 (docs) network. From the docs: However, I am struggling to get this preprocess_input working with a BatchDataset. Can you please help me figure out how to connect these two dots? Please see the below code: This will throw TypeError: 'BatchDataset' object is not subscriptable: From TypeError: 'DatasetV1Adapter' object is not subscriptable (from BatchDataset not subscriptable when trying to format Python dictionary as table) the suggestion was to use: However, this also fails: This is all using Python==3.10.3 with tensorflow==2.8.0. How can I get this working? Thank you in advance.",https://stackoverflow.com/questions/72165812,11163122,Documentation Replication on Other Examples
72329108,Is there a simple way to know which Tensorflow ops have a registered GPU kernel?,"<p>I have been trying to optimize some Tensorflow code that was pretty memory inefficient (use of large dense tensors containing very sparse information), and would thus limit batch size and scalability, by trying to make use of SparseTensors.
After some struggle I finally come up with a decent solution with satisfactory speedup on CPU and very low memory usage, and when the time comes to use a GPU I realize that the previous memory inefficient is orders of magnitude faster...</p>
<p>Using tensorboard profiling I've discovered that two of the operations I have used in my &quot;&quot;optimized&quot;&quot; version only run on CPU (namely UniqueV2 and sparse_dense_matmul), but I could not see any hint of that in the documentation.</p>
<p>The only related piece of <a href=""https://www.tensorflow.org/guide/gpu#overview"" rel=""nofollow noreferrer"">documentation</a> states:</p>
<blockquote>
<p>If a TensorFlow operation has no corresponding GPU implementation,
then the operation falls back to the CPU device. For example, since
tf.cast only has a CPU kernel, on a system with devices CPU:0 and
GPU:0, the CPU:0 device is selected to run tf.cast, even if requested
to run on the GPU:0 device.</p>
</blockquote>
<p>In turn there is nothing in the tf.cast documentation hinting that the op has no GPU kernel.</p>
<p>Thus, is there a simple way to know whether a TF ops has a registered GPU kernel, without having to use a GPU to find it out?</p>
<p>The <a href=""https://www.tensorflow.org/guide/create_op#gpu_support"" rel=""nofollow noreferrer"">custom ops</a> guide suggest that this could be seen by looking at the ops C files, but this seems a rather cumbersome way to do it...</p>
<p>I'm using TF v2.8</p>
<p>Thanks!</p>
","I have been trying to optimize some Tensorflow code that was pretty memory inefficient (use of large dense tensors containing very sparse information), and would thus limit batch size and scalability, by trying to make use of SparseTensors. After some struggle I finally come up with a decent solution with satisfactory speedup on CPU and very low memory usage, and when the time comes to use a GPU I realize that the previous memory inefficient is orders of magnitude faster... Using tensorboard profiling I've discovered that two of the operations I have used in my """"optimized"""" version only run on CPU (namely UniqueV2 and sparse_dense_matmul), but I could not see any hint of that in the documentation. The only related piece of documentation states: In turn there is nothing in the tf.cast documentation hinting that the op has no GPU kernel. Thus, is there a simple way to know whether a TF ops has a registered GPU kernel, without having to use a GPU to find it out? The custom ops guide suggest that this could be seen by looking at the ops C files, but this seems a rather cumbersome way to do it... I'm using TF v2.8 Thanks!",https://stackoverflow.com/questions/72329108,19167343,Inadequate Examples
72707453,How to save a tensorflow dataset to multiple shards without using enumerate,"<p>I have a tensorflow dataset with some elements in it, and I want to save it with <code>tf.data.Dataset.save</code> such that each element gets its own shard. Thus if the dataset contains 2,000 elements, it would be saved to 2,000 shards.</p>
<p>The documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#save"" rel=""nofollow noreferrer"">here</a> specifies how to create 1 shard only, but not how to make a shard for each element.</p>
<p>Below, I am able to do it with enumerate, but is there another way to do it without also saving the index from <code>enumerate</code>?</p>
<pre><code>tuple_data = np.array([3, 4])
data = tf.data.Dataset.from_tensor_slices(tuple_data)
data = data.enumerate()
print(list(data.as_numpy_iterator()))
# [(0, 3), (1, 4)]

data.save(path='~/Desktop/1', shard_func=lambda i, x: i)
</code></pre>
","I have a tensorflow dataset with some elements in it, and I want to save it with tf.data.Dataset.save such that each element gets its own shard. Thus if the dataset contains 2,000 elements, it would be saved to 2,000 shards. The documentation here specifies how to create 1 shard only, but not how to make a shard for each element. Below, I am able to do it with enumerate, but is there another way to do it without also saving the index from enumerate?",https://stackoverflow.com/questions/72707453,9909857,Documentation Replicability
72720129,Understanding tf.keras.metrics.Precision and Recall for multiclass classification,"<p>I am building a model for a multiclass classification problem. So I want to evaluate the model performance using the Recall and Precision.
I have 4 classes in the dataset and it is provided in <code>one hot</code> representation.</p>
<p>I was reading the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision"" rel=""nofollow noreferrer"">Precision</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall"" rel=""nofollow noreferrer"">Recall</a> <code>tf.keras</code> documentation, and have some questions:</p>
<ol>
<li>When it calculating the Precision and Recall for the multi-class classification, how can we take the average of all of the labels, meaning the global precision &amp; Recall? is it calculated with <code>macro</code> or <code>micro</code> since it is not specified in the documentation as in the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"" rel=""nofollow noreferrer"">Sikit learn</a>.</li>
<li>If I want to calculate the precision &amp; Recall for each label separately, can I use the argument <code>class_id</code> for each label to do  <code>one_vs_rest</code> or <code>binary</code> classification. Like what I have done in the code below?</li>
<li>can I use the argument <code>top_k</code> with the value <code>top_k=2</code> would be helpful here or it is not suitable for my classification of 4 classes only?</li>
<li>While I am measuring the performance of each class, What could be the difference, when I set the <code>top_k=1</code> and not setting <code>top_k</code>overall?</li>
</ol>
<pre><code>model.compile(
      optimizer='sgd',
      loss=tf.keras.losses.CategoricalCrossentropy(),
      metrics=[tf.keras.metrics.CategoricalAccuracy(),
               ##class 0
               tf.keras.metrics.Precision(class_id=0,top_k=2), 
               tf.keras.metrics.Recall(class_id=0,top_k=2),
              ##class 1
               tf.keras.metrics.Precision(class_id=1,top_k=2), 
               tf.keras.metrics.Recall(class_id=1,top_k=2),
              ##class 2
               tf.keras.metrics.Precision(class_id=2,top_k=2), 
               tf.keras.metrics.Recall(class_id=2,top_k=2),
              ##class 3
               tf.keras.metrics.Precision(class_id=3,top_k=2), 
               tf.keras.metrics.Recall(class_id=3,top_k=2),
])
</code></pre>
<p>Any clarification of this function will be appreciated.
Thanks in advance</p>
","I am building a model for a multiclass classification problem. So I want to evaluate the model performance using the Recall and Precision. I have 4 classes in the dataset and it is provided in one hot representation. I was reading the Precision and Recall tf.keras documentation, and have some questions: Any clarification of this function will be appreciated. Thanks in advance",https://stackoverflow.com/questions/72720129,17534198,Documentation Replicability
72850120,"Keras - Specifying from_logits=False when using tf.keras.layers.Dense(1,activation='sigmoid')(x)","<p>I am working on a binary classification problem, using transfer learning and image inputs and have a question regarding the</p>
<p>I have been working through using the correct activation layers (e.g. Softmax or Sigmoid - sigmoid for binary softmax for multiclass) and noticed when I specify 'sigmoid' as part of the <code>Dense()</code> output layer, I no longer need to specify <code>from_logits=True</code> during <code>model.compile()</code>.</p>
<p>This means when I am obtaining predictions, I don't use the <code>tf.nn.sigmoid()</code> function and instead simply check if the value is greater than 0.5, then 1, else 0. Is this correct? Here is my code:</p>
<pre><code>i = keras.Input(shape=(150, 150, 3))
                scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)
                mt = scale_layer(i)
                mt = base_model(model_top, training=False)
                mt = keras.layers.GlobalAveragePooling2D()(mt)
                mt = keras.layers.Dropout(dropout)(mt)  # Regularize with dropout
                o = keras.layers.Dense(1,activation='sigmoid')(mt)
                model = keras.Model(i, o)

....

model.compile(optimizer=keras.optimizers.Adam(lr),loss=keras.losses.BinaryCrossentropy(from_logits=False)
                )
</code></pre>
<p>And then when I obtain predictions, I have the following:</p>
<pre><code>                pred = model.predict(test)
                pred = tf.where(pred &lt; 0.5, 0, 1)
                pred = pred.numpy()
</code></pre>
<p>My intuition is that as I am specifying the sigmoid activation function during the Dense layer build, I no longer work with 'logits' and therefore do not need to apply the sigmoid function later on. In the documentation, I've seen both examples used but it's quite sparse on information when working with <code>model.predict()</code>, would appreciate any guidance.</p>
","I am working on a binary classification problem, using transfer learning and image inputs and have a question regarding the I have been working through using the correct activation layers (e.g. Softmax or Sigmoid - sigmoid for binary softmax for multiclass) and noticed when I specify 'sigmoid' as part of the Dense() output layer, I no longer need to specify from_logits=True during model.compile(). This means when I am obtaining predictions, I don't use the tf.nn.sigmoid() function and instead simply check if the value is greater than 0.5, then 1, else 0. Is this correct? Here is my code: And then when I obtain predictions, I have the following: My intuition is that as I am specifying the sigmoid activation function during the Dense layer build, I no longer work with 'logits' and therefore do not need to apply the sigmoid function later on. In the documentation, I've seen both examples used but it's quite sparse on information when working with model.predict(), would appreciate any guidance.",https://stackoverflow.com/questions/72850120,6419012,Documentation Replicability
72928149,Difference between Experimental Preprocessing layers and normal preprocessing layers in Tensorflow,"<pre><code>import tensorflow as tf
import keras
import tensorflow.keras.layers as tfl
from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation
</code></pre>
<p>I am trying to figure out which I should use for Data Augmentation. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers"" rel=""nofollow noreferrer"">documentation</a>, there is:</p>
<p>tf.keras.layers.RandomFlip and RandomRotation</p>
<p>Then we have in <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing"" rel=""nofollow noreferrer"">tf.keras.layers.experimental.preprocessing</a> the same things, randomFlip and RandomRotation.</p>
<p>Which should I use? I've seen <a href=""https://www.tensorflow.org/guide/keras/preprocessing_layers"" rel=""nofollow noreferrer"">guides</a> that use both.</p>
<p>This is my current code:</p>
<pre><code>def data_augmenter():
data_augmentation = tf.keras.Sequential([
    tfl.RandomFlip(),
    tfl.RandomRotation(0.2)
])
return data_augmentation
</code></pre>
<p>and this is a part of my model:</p>
<pre><code>def ResNet50(image_shape = IMG_SIZE, data_augmentation=data_augmenter()):

input_shape = image_shape + (3,)

# Remove top layer in order to put mine with the correct classification labels, get weights for imageNet
base_model = tf.keras.applications.resnet_v2.ResNet50V2(input_shape=input_shape, include_top=False, weights='imagenet')

# Freeze base model
base_model.trainable = False

# Define input layer
inputs = tf.keras.Input(shape=input_shape)

# Apply Data Augmentation
x = data_augmentation(inputs)
</code></pre>
<p>I am a bit confused here..</p>
","I am trying to figure out which I should use for Data Augmentation. In the documentation, there is: tf.keras.layers.RandomFlip and RandomRotation Then we have in tf.keras.layers.experimental.preprocessing the same things, randomFlip and RandomRotation. Which should I use? I've seen guides that use both. This is my current code: and this is a part of my model: I am a bit confused here..",https://stackoverflow.com/questions/72928149,14072615,Documentation Replication on Other Examples
73049510,How to dynamically set pool size for AveragePooling2D layer/ How to pass external value to an sequential layer,"<p>Trying to understand <a href=""https://www.tensorflow.org/recommenders/examples/listwise_ranking"" rel=""nofollow noreferrer"">listwise documentation</a></p>
<p>while trying to replicate by mixing <a href=""https://www.tensorflow.org/recommenders/examples/deep_recommenders"" rel=""nofollow noreferrer"">deep model</a> to listwise I am stuck at point where I am not able to set the pool size inside the sequential layer in an dynamic manner. For example consider below code</p>
<pre><code>!pip install -q tensorflow-recommenders
!pip install -q --upgrade tensorflow-datasets
!pip install -q tensorflow-ranking
import pprint

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_ranking as tfr
import tensorflow_recommenders as tfrs
from typing import Dict, Text
import os
import tempfile
import datetime
ratings = tfds.load(&quot;movielens/100k-ratings&quot;, split=&quot;train&quot;)
movies = tfds.load(&quot;movielens/100k-movies&quot;, split=&quot;train&quot;)

ratings = ratings.map(lambda x: {
    &quot;movie_title&quot;: x[&quot;movie_title&quot;],
    &quot;user_id&quot;: x[&quot;user_id&quot;],
    &quot;user_rating&quot;: x[&quot;user_rating&quot;],
    # &quot;timestamp&quot;: x[&quot;timestamp&quot;],
})
movies = movies.map(lambda x: x[&quot;movie_title&quot;])

unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))
unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(
    lambda x: x[&quot;user_id&quot;]))))


class MovieModel(tf.keras.Model):

  def __init__(self):
    super().__init__()

    max_tokens = 10_000_00

    self.title_vectorizer = tf.keras.layers.TextVectorization(
        max_tokens=max_tokens)

    self.title_text_embedding = tf.keras.Sequential([
      # tf.keras.layers.Flatten(),
      self.title_vectorizer,
      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),
      tf.keras.layers.AveragePooling2D(pool_size=(1,4),strides=1,    padding='valid',),
    ])
    self.title_vectorizer.adapt(movies)

  def call(self, titles):
    return self.title_text_embedding(titles)
</code></pre>
<p>After we create movie model lets try to test it before we can use it on proper movie data</p>
<p>below is the test code</p>
<pre><code>test_movie_titles = [[&quot;M*A*S*H (1970)&quot;, &quot;Dances with Wolves (1990)&quot;, &quot;Speed (1994)&quot;,&quot;Dances with Wolves (1990)&quot;, &quot;Speed (1994)&quot;]]
md = MovieModel()
test_ratings = md(tf.constant(tf.reshape(test_movie_titles,[1,5,1])) )  
test_ratings
</code></pre>
<p>This now works perfect and I will get an output as below</p>
<pre><code>&lt;tf.Tensor: shape=(1, 5, 1, 32), dtype=float32, numpy=
array([[[[ 0.00778975, -0.00899004,  0.02926993, -0.00527342,
           0.00706512,  0.02012717,  0.03438753,  0.01971687,
          -0.00543808, -0.00754605, -0.02241766,  0.00045748,
          -0.00785657, -0.00291913,  0.00670988,  0.01176082,
          -0.02052191, -0.00751739, -0.01433057,  0.008
-----
----
</code></pre>
<p>Now if you notice in the code above I have hardcoded the pool_size as 1,4 ( <code>tf.keras.layers.AveragePooling2D(pool_size=(1,4),strides=1,    padding='valid',),</code>) because the test sample I had used above only have maximum 4 words, so the vectorization will produce vector of size 4, now problem is how to I ensure the right pool size when I pass the whole dataset (movies) to the model. How can I pass such external value (pool_size) to an sequential layer from outside?</p>
<p>The above code was run on google colab using tensorflow version 2.9.1</p>
","Trying to understand listwise documentation while trying to replicate by mixing deep model to listwise I am stuck at point where I am not able to set the pool size inside the sequential layer in an dynamic manner. For example consider below code After we create movie model lets try to test it before we can use it on proper movie data below is the test code This now works perfect and I will get an output as below Now if you notice in the code above I have hardcoded the pool_size as 1,4 ( tf.keras.layers.AveragePooling2D(pool_size=(1,4),strides=1, padding='valid',),) because the test sample I had used above only have maximum 4 words, so the vectorization will produce vector of size 4, now problem is how to I ensure the right pool size when I pass the whole dataset (movies) to the model. How can I pass such external value (pool_size) to an sequential layer from outside? The above code was run on google colab using tensorflow version 2.9.1",https://stackoverflow.com/questions/73049510,12271381,Documentation Replication on Other Examples
73165980,Tensorflow: how to feed a variable-time-step input to a RNN,"<p>I have a simple X_train and Y_train data:</p>
<pre><code>x_train = [
  array([ 6,  1,  9, 10,  7,  7,  1,  9, 10,  3, 10,  1,  4]), 
  array([ 2,  8,  8,  1,  1,  4,  2,  5,  1,  2,  7,  2,  1,  1, 4,  5, 10, 4])
]
y_train = [23, 17]
</code></pre>
<p>Arrays are numpy arrays.
I am now trying to use the <code>tf.data.Dataset</code> class to load these as tensors.
Before I have done a similar thing successfully using the following code:</p>
<pre><code>    dataset = data.Dataset.from_tensor_slices((x_train, y_train))
</code></pre>
<p>As this input is fed into a RNN, I have used the expand_dims method in the first RNN layer (the expand_dimension is passed as a function to overcome an apparent bug in tensorflow: see <a href=""https://github.com/keras-team/keras/issues/5298#issuecomment-281914537"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/issues/5298#issuecomment-281914537</a>):</p>
<pre><code>def expand_dimension(x):
    from tensorflow import expand_dims
    return expand_dims(x, axis=-1)

model = models.Sequential(
    [
        layers.Lambda(expand_dimension,
                      input_shape=[None]),
        layers.LSTM(units=64, activation='tanh'),
        layers.Dense(units=1)
    ]
)
</code></pre>
<p>This worked although because I had arrays of equal length. In the example I posted instead the 1st array has 13 numbers and the 2nd one 18.
In this case the method above doesn't work, and the recommended method seems to be using <code>tf.data.Dataset.from_generator</code>.
Reading this <a href=""https://stackoverflow.com/questions/50329855/how-to-use-the-tensorflow-dataset-pipeline-for-variable-length-inputs"">How to use the Tensorflow Dataset Pipeline for Variable Length Inputs?</a>, the accepted solution shows something like the following would work (where I am not caring here about <code>y_train</code> for simplicity):</p>
<pre><code>dataset = tf.data.Dataset.from_generator(lambda: x_train, 
                                         tf.as_dtype(x_train[0].dtype),
                                         tf.TensorShape([None, ]))
</code></pre>
<p>However, the syntax in tensorflow has changed since this answer, and now it requires to use the <code>output_signature</code> argument (see <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator</a>).</p>
<p>I've tried different ways but I'm finding hard to understand from tensorflow documentation what the <code>output_signature</code> should exactly be in my case.
Any help would be much appreciated.</p>
","I have a simple X_train and Y_train data: Arrays are numpy arrays. I am now trying to use the tf.data.Dataset class to load these as tensors. Before I have done a similar thing successfully using the following code: As this input is fed into a RNN, I have used the expand_dims method in the first RNN layer (the expand_dimension is passed as a function to overcome an apparent bug in tensorflow: see https://github.com/keras-team/keras/issues/5298#issuecomment-281914537): This worked although because I had arrays of equal length. In the example I posted instead the 1st array has 13 numbers and the 2nd one 18. In this case the method above doesn't work, and the recommended method seems to be using tf.data.Dataset.from_generator. Reading this How to use the Tensorflow Dataset Pipeline for Variable Length Inputs?, the accepted solution shows something like the following would work (where I am not caring here about y_train for simplicity): However, the syntax in tensorflow has changed since this answer, and now it requires to use the output_signature argument (see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator). I've tried different ways but I'm finding hard to understand from tensorflow documentation what the output_signature should exactly be in my case. Any help would be much appreciated.",https://stackoverflow.com/questions/73165980,13454852,Documentation Ambiguity
73179836,tensorflow.py_function fails to temporarily switch to eager execution while in graph mode,"<p>I'm not sure if this is a Tensorflow bug or my misunderstanding about what this function is supposed to do, but I can't get <code>tf.py_function</code> to return an <code>EagerTensor</code> <em>while in graph mode</em>. Consequently, calling <code>.numpy()</code> on the output of this function fails.</p>
<p>The issue can be reproduced using the exact example given in the official documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/py_function</a>):</p>
<pre><code>import tensorflow as tf

tf.compat.v1.disable_eager_execution()

def log_huber(x, m):
  if tf.abs(x) &lt;= m:
    return x**2
  else:
    return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))

x = tf.constant(1.0)
m = tf.constant(2.0)

with tf.GradientTape() as t:
  t.watch([x, m])
  y = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)

dy_dx = t.gradient(y, x)
assert dy_dx.numpy() == 2.0

</code></pre>
<p>This generates the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;input&gt;&quot;, line 17, in &lt;module&gt;
  File &quot;C:\Users\...\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 446, in __getattr__
    self.__getattribute__(name)
AttributeError: 'Tensor' object has no attribute 'numpy'
</code></pre>
<h3>About version</h3>
<p>I am running Python 3.8 and Tensorflow v2.9.1.</p>
<p>Any help would be greatly appreciated!</p>
","I'm not sure if this is a Tensorflow bug or my misunderstanding about what this function is supposed to do, but I can't get tf.py_function to return an EagerTensor while in graph mode. Consequently, calling .numpy() on the output of this function fails. The issue can be reproduced using the exact example given in the official documentation (https://www.tensorflow.org/api_docs/python/tf/py_function): This generates the following error: I am running Python 3.8 and Tensorflow v2.9.1. Any help would be greatly appreciated!",https://stackoverflow.com/questions/73179836,10453038,Documentation Replicability
73213159,How to apply tf.data transformations to a DataFrame,"<p>I want to apply tf.data transformations to a panda  dataframe. According to the tensorflow docs <a href=""https://www.tensorflow.org/tutorials/load_data/pandas_dataframe"" rel=""nofollow noreferrer"">HERE</a> I can apply tf.data to a dataframe directly but the dtype of the dataframe should be uniform.</p>
<p>When I apply tf.data to my dataframe like below</p>
<pre><code>tf.data.Dataset.from_tensor_slices(df['reports'])
</code></pre>
<p>it generates this error</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
</code></pre>
<p>When I print <code>df['reports'].dtype</code> it is <code>dtype('O')</code> which seems to be not uniformed, if this is the case then how can I convert this dataframe to uniform <code>dtype</code>.</p>
","I want to apply tf.data transformations to a panda dataframe. According to the tensorflow docs HERE I can apply tf.data to a dataframe directly but the dtype of the dataframe should be uniform. When I apply tf.data to my dataframe like below it generates this error When I print df['reports.dtype it is dtype('O') which seems to be not uniformed, if this is the case then how can I convert this dataframe to uniform dtype.",https://stackoverflow.com/questions/73213159,19443650,Documentation Replicability
73279782,Tensorboard profiling a predict call using Cloud TPU Node,"<p>I've been trying to profile a predict call of a custom NN model using a Cloud TPU v2-8 Node.</p>
<p>It is important to say that my prediction call takes about 2 minutes to finish and I do it using data divided in TFRecord batches.</p>
<p>I followed the official documentation &quot;<a href=""https://cloud.google.com/tpu/docs/cloud-tpu-tools"" rel=""nofollow noreferrer"">Profile your model with Cloud TPU Tools</a>&quot; and I tryied to capture a profile:</p>
<ol>
<li>Using <a href=""https://cloud.google.com/tpu/docs/cloud-tpu-tools#capture_a_profile_using_tensorboard"" rel=""nofollow noreferrer"">Tensorboard UI</a> and</li>
<li>The &quot;<a href=""https://cloud.google.com/tpu/docs/cloud-tpu-tools#capture_a_profile_programmatically"" rel=""nofollow noreferrer"">programatic way</a>&quot; with a tf.profiler.experimental.start() and tf.profilier.experimental.stop() wrapping the predict call, but I had no success in both cases.</li>
</ol>
<pre><code># TPU Node connection is done before...

# TPU at this point is already running
logdir_path = &quot;logs/predict&quot;
tf.profiler.experimental.start(logdir_path)
# Tensorflow predict call here
tf.profiler.experimental.stop()
</code></pre>
<p>I could generate some data in both cases (Tensorboard UI and profiler call), but when I try to open it in Tensorboard pointing the logdir path, I received a &quot;No dashboard are active for the current data set&quot; message.</p>
<p><strong>Is there any way to profile a Tensorflow/Keras prediction call with a model running in a Cloud TPU Node?</strong>
<br>
<br>
<br>
<br>
<strong>Curious fact</strong> - There seems to be an inconsistency in the Tensorflow docs and Cloud TPU docs: in <a href=""https://www.tensorflow.org/guide/profiler#profiling_use_cases"" rel=""nofollow noreferrer"">Tensorflow Optimization Docs</a> we can see that tf.profiler.experimental.start/stop calls are not supported by TPU hardware, but in <a href=""https://cloud.google.com/tpu/docs/cloud-tpu-tools#capture_a_profile_programmatically"" rel=""nofollow noreferrer"">Google Cloud docs</a> this is the recommended method to capture a profile in TPU.</p>
<p>Config:</p>
<ul>
<li>Tensorflow 2.6.1</li>
<li>Tensorboard 2.9.1</li>
<li>Python 3.8</li>
<li>Cloud TPU Node v2-8</li>
</ul>
","I've been trying to profile a predict call of a custom NN model using a Cloud TPU v2-8 Node. It is important to say that my prediction call takes about 2 minutes to finish and I do it using data divided in TFRecord batches. I followed the official documentation ""Profile your model with Cloud TPU Tools"" and I tryied to capture a profile: I could generate some data in both cases (Tensorboard UI and profiler call), but when I try to open it in Tensorboard pointing the logdir path, I received a ""No dashboard are active for the current data set"" message. Is there any way to profile a Tensorflow/Keras prediction call with a model running in a Cloud TPU Node? Curious fact - There seems to be an inconsistency in the Tensorflow docs and Cloud TPU docs: in Tensorflow Optimization Docs we can see that tf.profiler.experimental.start/stop calls are not supported by TPU hardware, but in Google Cloud docs this is the recommended method to capture a profile in TPU. Config:",https://stackoverflow.com/questions/73279782,11648055,Documentation Replication on Other Examples
73645574,Why keras AUC returns zero when multi-label is set?,"<p>I'm trying to understand how <code>tf.keras.metrics.AUC(multi_label=True)</code> works. From the docs, I'm led to understand that when working with multi-label vectors, each class is computed individually, then averaged.</p>
<p>However, I can't seem to get the following trivial case to compute correctly. That is, if the prediction is the same as the expected vector, why is the output not <code>1.0</code>?</p>
<pre><code>y_true = [
    [1, 0, 0, 0, 1],
]

acc = tf.keras.metrics.AUC(multi_label=True, num_labels=5)

acc.reset_state()
acc.update_state(tf.constant(y_true), tf.constant(y_true))
acc.result().numpy()

&gt;&gt;&gt; 0.0
</code></pre>
","I'm trying to understand how tf.keras.metrics.AUC(multi_label=True) works. From the docs, I'm led to understand that when working with multi-label vectors, each class is computed individually, then averaged. However, I can't seem to get the following trivial case to compute correctly. That is, if the prediction is the same as the expected vector, why is the output not 1.0?",https://stackoverflow.com/questions/73645574,774907,Documentation Ambiguity
73794766,what is the meaning of axis=-1 in tf.keras.layers.Normalization?,"<p>I'm trying to learn deep learning using keras and tensorflow and I came across a code explaining linear regression at <a href=""https://www.tensorflow.org/tutorials/keras/regression"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/keras/regression</a> wherein they have created a normalization layer using normalizer = tf.keras.layers.Normalization(axis=-1). Someone please explain the meaning of axis =-1 . I tried looking at the API documentation but I couldnt understand the explanation from there?I know that axis=0 represent rows and axis=1 columns, right?
Thanks in advance</p>
","I'm trying to learn deep learning using keras and tensorflow and I came across a code explaining linear regression at https://www.tensorflow.org/tutorials/keras/regression wherein they have created a normalization layer using normalizer = tf.keras.layers.Normalization(axis=-1). Someone please explain the meaning of axis =-1 . I tried looking at the API documentation but I couldnt understand the explanation from there?I know that axis=0 represent rows and axis=1 columns, right? Thanks in advance",https://stackoverflow.com/questions/73794766,19986715,Documentation Completeness
74005009,How to create output_signature for tensorflow.dataset.from_generator,"<p>I have a generator yielding data and labels <code>yield data, labels</code> where the data is
an <code>numpy.ndarray</code> with variable rows and 500 columns of type <code>dtype=float32</code> and the labels are integers of <code>numpy.int64</code>.</p>
<p>I'm trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: <code>tf.data.Dataset.from_generator</code></p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">docs</a> say that the from_generator function needs a parameter <code>output_signature</code> as an input. But I'm having trouble understanding how to build this output_signature.</p>
<p>How can I make the output_signature for the generator I described?</p>
<p>Thank you!</p>
<p>Edit:
I used <code>tf.type_spec_from_value</code> to get this:</p>
<pre><code>dataset = tf.data.Dataset.from_generator(
   datagen_row,
   output_signature=(
      tf.TensorSpec(shape=(None, 512), dtype=tf.float32, name=None),
      tf.TensorSpec(shape=(), dtype=tf.int64, name=None)
   )
)
</code></pre>
<p>But is it correct to use None when the number of rows is varying for the first data type?</p>
","I have a generator yielding data and labels yield data, labels where the data is an numpy.ndarray with variable rows and 500 columns of type dtype=float32 and the labels are integers of numpy.int64. I'm trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: tf.data.Dataset.from_generator The docs say that the from_generator function needs a parameter output_signature as an input. But I'm having trouble understanding how to build this output_signature. How can I make the output_signature for the generator I described? Thank you! Edit: I used tf.type_spec_from_value to get this: But is it correct to use None when the number of rows is varying for the first data type?",https://stackoverflow.com/questions/74005009,2300622,Documentation Replicability
74029376,Tensorflow custom reduction function with axis support,"<p>I would like to get the value with the maximum absolute value in a tensor, with respect to an axis. Note that I don't want the maximum absolute value, I want the <em>value that has the maximum absolute value</em> (so I need to keep the sign).</p>
<p>Ideally, I would like something similar to <code>reduce_max</code> or <code>reduce_min</code>:</p>
<pre class=""lang-py prettyprint-override""><code>tensor = tf.constant(
  [
    [[ 1,  5, -3],
     [ 2, -3,  1],
     [ 3, -6,  2]],

    [[-2,  3, -5],
     [-1,  4,  2],
     [ 4, -1,  0]]
   ]
)
# tensor.shape = (2, 3, 3)

tensor.reduce_maxamplitude(tensor, axis=0)
# Tensor(
#  [[-2,  5, -5],
#   [ 2,  4,  2],
#   [ 4, -6,  2]]
# )
# shape: (3, 3)

tensor.reduce_maxamplitude(tensor, axis=1)
# Tensor(
#  [[3, -6, -3],
#   [4,  4, -5]]
# )
# shape: (2, 3)

tensor.reduce_maxamplitude(tensor, axis=2)
# Tensor(
#  [[5, -3, -6],
#   [-5,  4, 4]]
# )
# shape: (2, 3)
</code></pre>
<p>but I did not find anything useful in tensorflow documentation.</p>
<p>With a flat tensor, I know that I could use <code>tf.foldl</code> or <code>tf.foldr</code>:</p>
<pre class=""lang-py prettyprint-override""><code>flat = tf.reshape(tensor, -1)
tf.foldr(lambda a, x: x if tf.abs(x) &gt; tf.abs(a) else a, flat)
# -6
</code></pre>
<p>However, I don't know how to handle an axis parameter in the case of multidimensional tensors.</p>
","I would like to get the value with the maximum absolute value in a tensor, with respect to an axis. Note that I don't want the maximum absolute value, I want the value that has the maximum absolute value (so I need to keep the sign). Ideally, I would like something similar to reduce_max or reduce_min: but I did not find anything useful in tensorflow documentation. With a flat tensor, I know that I could use tf.foldl or tf.foldr: However, I don't know how to handle an axis parameter in the case of multidimensional tensors.",https://stackoverflow.com/questions/74029376,18159603,Lack of Alternative Solutions/Documentation
74060508,How to Save a Tensorflow Dataset,"<p>As the title says I'm trying to save a <code>TensorSliceDataset</code> object to file. Viewing tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">website</a> it seems that the <code>tf.data.Dataset</code> class has a save function but it is not implemented for <code>TensorSliceDataset</code> objects. Pickling also did not work for me.</p>
<p>Example code</p>
<pre><code>import tensorflow as tf
t = tf.range(10)
ds = tf.data.Dataset.from_tensor_slices(t)
ds.save()
</code></pre>
<p>returns error: <code>AttributeError: 'TensorSliceDataset' object has no attribute 'save'</code></p>
",As the title says I'm trying to save a TensorSliceDataset object to file. Viewing tensorflow's website it seems that the tf.data.Dataset class has a save function but it is not implemented for TensorSliceDataset objects. Pickling also did not work for me. Example code returns error: AttributeError: 'TensorSliceDataset' object has no attribute 'save',https://stackoverflow.com/questions/74060508,7875444,Documentation Replicability
74182037,"How to ""update"" from module tf.keras.preprocessing.image to tf.keras.utils.image_dataset_from_directory for features extraction","<p>This code part is common to both &quot;problematic&quot; codes below:</p>
<pre><code>BATCH_SIZE = 32
IM_DIR = '/content/drive/My Drive/101_ObjectCategories/'
IM_HEIGHT = 224
IM_WIDTH = 224
NUM_IM = 8686
NUM_EPOCHS = int(math.ceil(NUM_IM / BATCH_SIZE))

#load pre-trained base model
model = ResNet50(weights='imagenet',
                 include_top=False,
                 input_shape=(IM_WIDTH, IM_HEIGHT, CH),
                 pooling='max')
</code></pre>
<p>The following code I successfully use to extract features of a set of images using module <code>tf.keras.preprocessing.image</code>.</p>
<pre><code>datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)
dataset = datagen.flow_from_directory(IM_DIR,
                                      target_size=(IM_HEIGHT, IM_WIDTH),
                                      class_mode=None,
                                      shuffle=False)

feature_list = []
feature_list = model.predict(dataset, num_epochs)
</code></pre>
<p>Thereafter I train a simple nearest-neighbor model using brute-force algorithm and I'm able to find three other images that are really similar to the query image as you can see below:</p>
<p><a href=""https://i.stack.imgur.com/qPi7q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qPi7q.png"" alt=""Right results"" /></a></p>
<p>But as pointed in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image"" rel=""nofollow noreferrer"">documentation</a> this preprocessing module is deprecated.<br />
So, I would like to &quot;update&quot; the code as suggested in the documentation: &quot;Prefer loading data with <code>tf.keras.utils.image_dataset_from_directory</code>, and then transforming the output <code>tf.data.Dataset</code> with preprocessing layers&quot;.<br />
For that I'm trying the following:</p>
<pre><code>#load images
dataset = tf.keras.utils.image_dataset_from_directory(
  IM_DIR,
  labels='inferred', #'inferred', None
  label_mode='categorical',  #'int', 'categorical', 'binary' or None
  class_names=None,
  color_mode='rgb',  #'grayscale', 'rgb' or 'rgba'
  batch_size=BATCH_SIZE,
  image_size=(IM_HEIGHT, IM_WIDTH),
  shuffle=True,
  seed=51719,
  validation_split=None,
  subset=None,                #'training', 'validation' or 'both'
  interpolation='bilinear',   #'bilinear', 'nearest', 'bicubic', 'area', 'lanczos3', 'lanczos5', 'gaussian' or 'mitchellcubic'
  follow_links=False,
  crop_to_aspect_ratio=False
)

#&quot;transforming the output with preprocessing layers&quot;
#rescale (normalize) dataset
rescale_layer = tf.keras.layers.Rescaling(1./255)

rescaled_dataset = dataset.map(lambda x, y: (rescale_layer(x), y))
im_batch, labels_batch = next(iter(rescaled_dataset))


#configure dataset for performance
#https://www.tensorflow.org/tutorials/load_data/images#configure_the_dataset_for_performance

AUTOTUNE = tf.data.AUTOTUNE
tuned_dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)
</code></pre>
<p>And now I begin with the features extraction</p>
<pre><code>#features extraction
#https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict
feature_list = []

feature_list = model.predict(
    tuned_dataset,
    batch_size=None,
    verbose='auto',
    steps=None,
    callbacks=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False
)

#save features
pickle.dump(
    feature_list,
    open(DATA_DIR + 'features.pickle', 'wb'))
</code></pre>
<p>After that I do the same and train the nearest neighbor model with this features, but the results are catastrophic as you can see below:</p>
<p><a href=""https://i.stack.imgur.com/18Wsa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/18Wsa.png"" alt=""Bad results"" /></a></p>
<p>What I'm doing so wrong that I have such different results?</p>
<p><strong>== EDIT 1 ==</strong></p>
<p>Answering @DWKOT using the same image we have following results:</p>
<pre><code>#Query image with first code
im_idx = 75
distances, indices = neighbors.kneighbors([feature_list[im_idx]])
plt.imshow(mpimg.imread(filenames[im_idx]), interpolation='lanczos')
</code></pre>
<p><a href=""https://i.stack.imgur.com/V7EdQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V7EdQ.png"" alt=""Query image"" /></a></p>
<pre><code>#Similar image
plt.imshow(mpimg.imread(filenames[indices[0][1]]), interpolation='lanczos')
</code></pre>
<p><a href=""https://i.stack.imgur.com/UfUG8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UfUG8.png"" alt=""Similar image"" /></a></p>
<p>And the code that give us the distance to the 5 nearest neighbors:</p>
<pre><code>for i in range(5):
    print(distances[0][i])
</code></pre>
<p>With the following results:</p>
<pre><code>0.0
185.60701
185.75049
195.71657
196.4056
</code></pre>
<p>With the second code we have following result for query / similar image:</p>
<p><a href=""https://i.stack.imgur.com/V7EdQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V7EdQ.png"" alt=""Query image"" /></a> / <a href=""https://i.stack.imgur.com/lW4n1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lW4n1.png"" alt=""Similar image 2"" /></a></p>
<p>And following results for the first five &quot;similar&quot; images:</p>
<pre><code>0.0
0.81401
0.88622
0.92734
0.9346
</code></pre>
<p>What is also strange as I would expect similar images having values next to zero and different ones far from zero...</p>
","This code part is common to both ""problematic"" codes below: The following code I successfully use to extract features of a set of images using module tf.keras.preprocessing.image. Thereafter I train a simple nearest-neighbor model using brute-force algorithm and I'm able to find three other images that are really similar to the query image as you can see below: But as pointed in the documentation this preprocessing module is deprecated. So, I would like to ""update"" the code as suggested in the documentation: ""Prefer loading data with tf.keras.utils.image_dataset_from_directory, and then transforming the output tf.data.Dataset with preprocessing layers"". For that I'm trying the following: And now I begin with the features extraction After that I do the same and train the nearest neighbor model with this features, but the results are catastrophic as you can see below: What I'm doing so wrong that I have such different results? == EDIT 1 == Answering @DWKOT using the same image we have following results: And the code that give us the distance to the 5 nearest neighbors: With the following results: With the second code we have following result for query / similar image: / And following results for the first five ""similar"" images: What is also strange as I would expect similar images having values next to zero and different ones far from zero...",https://stackoverflow.com/questions/74182037,3499881,Documentation Replication on Other Examples
74359221,Tensorflow v2.10 mutate output of signature function to be a map of label to results,"<p>I'm trying to save my model so that when called from tf-serving the output is:</p>
<pre><code>{
   &quot;results&quot;: [
      { &quot;label1&quot;: x.xxxxx, &quot;label2&quot;: x.xxxxx },
      { &quot;label1&quot;: x.xxxxx, &quot;label2&quot;: x.xxxxx }
   ]
}
</code></pre>
<p>where <code>label1</code> and <code>label2</code> are my labels and <code>x.xxxxx</code> are the probability of that label.</p>
<p>This is what I'm trying:</p>
<pre class=""lang-py prettyprint-override""><code>class TFModel(tf.Module):

    def __init__(self, model: tf.keras.Model) -&gt; None:
        self.labels = ['label1', 'label2']
        self.model = model
            
    @tf.function(input_signature=[tf.TensorSpec(shape=(1, ), dtype=tf.string)])
    def prediction(self, pagetext: str):

        return
        { 'results': tf.constant([{k: v for dct in [{self.labels[c]: f&quot;{x:.5f}&quot;} for (c,x) in enumerate(results[i])] for k, v in dct.items()}
         for i in range(len(results.numpy()))])}


# and then save it:
tf_model_wrapper = TFModel(classifier_model)
tf.saved_model.save(tf_model_wrapper.model, 
                      saved_model_path,
                      signatures={'serving_default':tf_model_wrapper.prediction}
                   )
</code></pre>
<p><em><strong>Side Note:</strong> Apparently in TensorFlow v2.0 if <code>signatures</code> is omitted it should scan the object for the first <code>@tf.function</code> (according to this: <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/save"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/saved_model/save</a>) but in reality that doesn't seem to work. Instead, the model saves successfully with no errors and the <code>@tf.function</code> is not called, but default output is returned instead.</em></p>
<p>The error I get from the above is:</p>
<pre><code>ValueError: Got a non-Tensor value &lt;tf.Operation 'PartitionedCall' type=PartitionedCall&gt; for key 'output_0' in the output of the function __inference_prediction_125493 used to generate the SavedModel signature 'serving_default'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor.
</code></pre>
<p>I wrapped the result in <code>tf.constant</code> above because of this error, thinking it might be a quick fix, but I think it's me just being naive and not understanding Tensors properly.</p>
<p>I tried a bunch of other things before learning that [all outputs must be return values].<a href=""https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values"" rel=""nofollow noreferrer"">1</a></p>
<p>How can I change the output to be as I want it to be?</p>
","I'm trying to save my model so that when called from tf-serving the output is: where label1 and label2 are my labels and x.xxxxx are the probability of that label. This is what I'm trying: Side Note: Apparently in TensorFlow v2.0 if signatures is omitted it should scan the object for the first @tf.function (according to this: https://www.tensorflow.org/api_docs/python/tf/saved_model/save) but in reality that doesn't seem to work. Instead, the model saves successfully with no errors and the @tf.function is not called, but default output is returned instead. The error I get from the above is: I wrapped the result in tf.constant above because of this error, thinking it might be a quick fix, but I think it's me just being naive and not understanding Tensors properly. I tried a bunch of other things before learning that [all outputs must be return values].1 How can I change the output to be as I want it to be?",https://stackoverflow.com/questions/74359221,498391,Documentation Replication on Other Examples
74434308,Setting only global level seed gives same output in consecutive iterations of loop in Tensorflow,"<p>I am testing out the <code>tf.random.set_seed</code> according to the rules given at - <a href=""https://www.tensorflow.org/api_docs/python/tf/random/set_seed"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/random/set_seed</a></p>
<p>In particular I am testing the second rule - where we set only global level seed and no operation level seed.</p>
<p>According to the documentation (the link is mentioned above), the second rule is:</p>
<blockquote>
<p>If the global seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the global seed so that it gets a unique random sequence.</p>
</blockquote>
<p>To explain the second rule, the documentation uses the following snippet:</p>
<pre><code>tf.random.set_seed(1234)
print(tf.random.uniform([1]))  # generates 'A1'
print(tf.random.uniform([1]))  # generates 'A2'
</code></pre>
<p>and states that</p>
<blockquote>
<p>The reason we get 'A2' instead 'A1' on the second call of tf.random.uniform above is because the second call uses a different operation seed.</p>
</blockquote>
<p>Now, I tested this rule on a 1D tensor of shape (3,) to check if the output of shuffling the tensor does not give the same sequence within consecutive iterations of the loop as follows:</p>
<pre><code>import tensorflow as tf


&quot;&quot;&quot;
Only global level seed
&quot;&quot;&quot;

tf.random.set_seed(1234)
   
constant_tensor = tf.constant([1,2,3])

for i in range(1, 15):
    shuffled_tensor = tf.random.shuffle(constant_tensor)
    print(shuffled_tensor)
</code></pre>
<p>I got the following output:</p>
<pre><code>tf.Tensor([3 1 2], shape=(3,), dtype=int32)
tf.Tensor([2 3 1], shape=(3,), dtype=int32)
tf.Tensor([2 1 3], shape=(3,), dtype=int32)
tf.Tensor([2 3 1], shape=(3,), dtype=int32)
tf.Tensor([1 3 2], shape=(3,), dtype=int32)
tf.Tensor([3 2 1], shape=(3,), dtype=int32)
tf.Tensor([2 1 3], shape=(3,), dtype=int32)
tf.Tensor([2 1 3], shape=(3,), dtype=int32)
tf.Tensor([2 3 1], shape=(3,), dtype=int32)
tf.Tensor([2 1 3], shape=(3,), dtype=int32)
tf.Tensor([3 2 1], shape=(3,), dtype=int32)
tf.Tensor([1 2 3], shape=(3,), dtype=int32)
tf.Tensor([3 2 1], shape=(3,), dtype=int32)
tf.Tensor([3 2 1], shape=(3,), dtype=int32)
</code></pre>
<p>From the output you can see that the sequence on line number 7 and 8 match.
Also the sequence on line number 13 and 14 match.</p>
<p>According to the documentation, tensorflow should not output the same sequence in a consecutive iteration.</p>
<p>Then why am I getting this kind of output? Have I misunderstood the concept?</p>
<p>To test this further, I also tested to following snippet which I used to generate 14 1-D tensors and check if any tensor is repeated within consecutive runs as follows:</p>
<pre><code>import tensorflow as tf
tf.random.set_seed(1234)
for i in range(1, 15):
    print(tf.random.uniform(shape=[1], minval=1, maxval=15, dtype=tf.int32))
</code></pre>
<p>And I got the following output:</p>
<pre><code>tf.Tensor([12], shape=(1,), dtype=int32)
tf.Tensor([8], shape=(1,), dtype=int32)
tf.Tensor([1], shape=(1,), dtype=int32)
tf.Tensor([2], shape=(1,), dtype=int32)
tf.Tensor([4], shape=(1,), dtype=int32)
tf.Tensor([3], shape=(1,), dtype=int32)
tf.Tensor([2], shape=(1,), dtype=int32)
tf.Tensor([7], shape=(1,), dtype=int32)
tf.Tensor([13], shape=(1,), dtype=int32)
tf.Tensor([11], shape=(1,), dtype=int32)
tf.Tensor([8], shape=(1,), dtype=int32)
tf.Tensor([3], shape=(1,), dtype=int32)
tf.Tensor([1], shape=(1,), dtype=int32)
tf.Tensor([4], shape=(1,), dtype=int32)
</code></pre>
<p>You can see that no two consecutive tensors are repeated. Why didn't I see this behaviour for my first snippet?</p>
","I am testing out the tf.random.set_seed according to the rules given at - https://www.tensorflow.org/api_docs/python/tf/random/set_seed In particular I am testing the second rule - where we set only global level seed and no operation level seed. According to the documentation (the link is mentioned above), the second rule is: To explain the second rule, the documentation uses the following snippet: and states that Now, I tested this rule on a 1D tensor of shape (3,) to check if the output of shuffling the tensor does not give the same sequence within consecutive iterations of the loop as follows: I got the following output: From the output you can see that the sequence on line number 7 and 8 match. Also the sequence on line number 13 and 14 match. According to the documentation, tensorflow should not output the same sequence in a consecutive iteration. Then why am I getting this kind of output? Have I misunderstood the concept? To test this further, I also tested to following snippet which I used to generate 14 1-D tensors and check if any tensor is repeated within consecutive runs as follows: And I got the following output: You can see that no two consecutive tensors are repeated. Why didn't I see this behaviour for my first snippet?",https://stackoverflow.com/questions/74434308,7422352,Documentation Ambiguity
75136950,How to visualize tf.compat.v1 static graph in tensorboard?,"<p>For a given graph, how can we visualize the graph using tensorboard for tf.compat.v1 ?
Sharing this here after searching everywhere. Most of the documentations explains tf.keras and not for tf.compat.v1 static graphs</p>
","For a given graph, how can we visualize the graph using tensorboard for tf.compat.v1 ? Sharing this here after searching everywhere. Most of the documentations explains tf.keras and not for tf.compat.v1 static graphs",https://stackoverflow.com/questions/75136950,10545426,Documentation Replication on Other Examples
75371111,"when trying to load external tfrecord with TFDS, given tf.train.Example, how to get tfds.features?","<p><strong>What I need help with / What I was wondering</strong></p>
<p>Hi, I am trying to load external tfrecord files with TFDS. I have read the official doc <a href=""https://www.tensorflow.org/datasets/external_tfrecord"" rel=""nofollow noreferrer"">here</a>, and find I need to define the feature structure using <code>tfds.features</code>. However, since the tfrecords files are alreay generated, I do not have control the generation pipeline. I do, however, know the <code>tf.train.Example</code> structre used in <code>TFRecordWriter</code> during generation, shown as follows.</p>
<pre><code>from tensorflow.python.training.training import BytesList, Example, Feature, Features, Int64List

dict(Example=Features({
'image': Feature(bytes_list=BytesList(value=[img_str])), # img_str is jpg encoded image raw bytes
'caption': Feature(bytes_list=BytesList(value=[caption])), # caption is a string
'height': Feature(bytes_list=Int64List(value=[caption])), 
'width': Feature(bytes_list=Int64List(value=[caption])), 
})
</code></pre>
<p>The doc only describes how to translate <a href=""https://www.tensorflow.org/datasets/api_docs/python/tfds/features"" rel=""nofollow noreferrer"">tfds.features</a> into the human readable structure of the <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Example"" rel=""nofollow noreferrer"">tf.train.Example</a>. But nowhere does it mention how to translate a tf.train.Example into tfds.features, which is needed to automatically add the proper metadata fileswith <a href=""https://www.tensorflow.org/datasets/api_docs/python/tfds/folder_dataset/write_metadata"" rel=""nofollow noreferrer"">tfds.folder_dataset.write_metadata</a>.</p>
<p>I wonder how to translate the above tf.train.Example into tfds.features? Thanks a lot!</p>
<p>BTW, while I understand that it is possible to directly read the data as it is in TFRecord with <code>tf.data.TFRecordDataset</code> and then use <code>map(decode_fn)</code> for decoding as suggested <a href=""https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset"" rel=""nofollow noreferrer"">here</a>, it seems to me this approach lacks necessary metadata like <code>num_shards</code> or <code>shard_lengths</code>. In this case, I am not sure if it is still ok to use common operations like <code>cache/repeat/shuffle/map/batch</code> on that <code>tf.data.TFRecordDataset</code>. So I think it is better to stick to the tfds approach.</p>
<p><strong>What I've tried so far</strong></p>
<p>I have searched the official doc for quite some time but cannot find the answer. There is a <code>Scalar</code> class in <code>tfds.features</code>, which I assume could be used to decode <code>Int64List</code>. But How can I decode the <code>BytesList</code>?</p>
<p><strong>Environment information</strong></p>
<ul>
<li><code>tensorflow-datasets</code> version: 4.8.2</li>
<li><code>tensorflow</code> version: 2.11.0</li>
</ul>
","What I need help with / What I was wondering Hi, I am trying to load external tfrecord files with TFDS. I have read the official doc here, and find I need to define the feature structure using tfds.features. However, since the tfrecords files are alreay generated, I do not have control the generation pipeline. I do, however, know the tf.train.Example structre used in TFRecordWriter during generation, shown as follows. The doc only describes how to translate tfds.features into the human readable structure of the tf.train.Example. But nowhere does it mention how to translate a tf.train.Example into tfds.features, which is needed to automatically add the proper metadata fileswith tfds.folder_dataset.write_metadata. I wonder how to translate the above tf.train.Example into tfds.features? Thanks a lot! BTW, while I understand that it is possible to directly read the data as it is in TFRecord with tf.data.TFRecordDataset and then use map(decode_fn) for decoding as suggested here, it seems to me this approach lacks necessary metadata like num_shards or shard_lengths. In this case, I am not sure if it is still ok to use common operations like cache/repeat/shuffle/map/batch on that tf.data.TFRecordDataset. So I think it is better to stick to the tfds approach. What I've tried so far I have searched the official doc for quite some time but cannot find the answer. There is a Scalar class in tfds.features, which I assume could be used to decode Int64List. But How can I decode the BytesList? Environment information",https://stackoverflow.com/questions/75371111,7811775,Documentation Completeness
75401761,Change Verbosity of Keras train_on_batch()?,"<p>I am training a GAN using Keras's <code>train_on_batch()</code> command. This is very similar to Keras's <code>fit()</code>. However, in the documentation for <code>fit()</code>, there is a parameter for <code>verbose</code>, which changes how often a progress bar is printed to the console.</p>
<p>My model has many batches, and so it is printing tons of progress bars to the command line. Unfortunately, <code>train_on_batch()</code> does not have a <code>verbose</code> parameter. Is there a workaround for this? Is there a Keras global variable/environment variable that I can set? I don't want to disable my program from printing to the console, I just want to change the verbosity of specifically <code>train_on_batch()</code>.</p>
<p>For clarify, I am using Keras directly from the Keras package, I am not using tf.keras.</p>
","I am training a GAN using Keras's train_on_batch() command. This is very similar to Keras's fit(). However, in the documentation for fit(), there is a parameter for verbose, which changes how often a progress bar is printed to the console. My model has many batches, and so it is printing tons of progress bars to the command line. Unfortunately, train_on_batch() does not have a verbose parameter. Is there a workaround for this? Is there a Keras global variable/environment variable that I can set? I don't want to disable my program from printing to the console, I just want to change the verbosity of specifically train_on_batch(). For clarify, I am using Keras directly from the Keras package, I am not using tf.keras.",https://stackoverflow.com/questions/75401761,12276162,Documentation Replicability
75572543,What to look out for when passing a generator into model.fit in tensorflow?,"<p>I want to replace the x and y training data parameters in tf.keras.Model.fit with a generator. However, some subtlety seems to escape me, as the model accuracy doesn't improve with the generator when training.</p>
<p>As far as I understand the documentation, the generator is supposed to yield tuples <code>(x_vals,y_vals)</code>, such that <code>x_vals</code> is a concatenation of <code>batch_size</code>-many training samples along a new 0th dimension, and 'v_vals' is the concatenation of their corresponding labels.</p>
<p>As long as the generator fulfills this, as I understand it, we can just replace the x parameter in tf.keras.Model.fit with the generator and omit the y parameter, though to define an epoch, we also need to specify 'steps_per_epoch' in fit.</p>
<p>There however seems to be something here I misunderstood or forgot, because starting with a model and input data that trains (i.e. its accuracy improves) and replacing the training data array with a generator as discussed, results in a model that doesn't train (i.e. its accuracy instead goes up a little, then however goes back down till its equal to chance).</p>
<p>The corresponding code:</p>
<pre><code>import numpy as np
import tensorflow as tf

BATCH_SIZE = 32

#Loading training data:
def load_cifar():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
    assert x_train.shape == (50000, 32, 32, 3)
    assert x_test.shape == (10000, 32, 32, 3)
    assert y_train.shape == (50000, 1)
    assert y_test.shape == (10000, 1)

    #Normalize the data &amp; cast to fp32:
    x_train = np.true_divide(x_train,255,dtype=np.single)
    x_test =  np.true_divide(x_test,255,dtype=np.single)
    y_train = y_train.astype(np.single)
    y_test =  y_test.astype(np.single)

    return (x_train,y_train), (x_test,y_test)


(train_x, train_y) , (validation_x, validation_y) = load_cifar()
   


# Defining the generator:
def data_generator_dummy(input_data_x:np.ndarray,
                          input_data_y:np.ndarray,
                          batch_size=BATCH_SIZE,
                          ):
    &quot;&quot;&quot;
    Given the input_data's, generate infinitely by:
     1. Drawing batch_size-many vectors from input_data_x and input_data_y
     2. Turn the drawn vectors into a mini-batch (with shape  [None]+input_data.shape)

    :param batch_size:
    :param input_data_x, input_data_y: The data on which noise shall be added
    :return: A generator for the input data.
    &quot;&quot;&quot;
    index =0
    while True:
        # We start with a zero-vector of expected size and fill the drawn samples into it:
        samples_x = np.zeros( [batch_size] + list(input_data_x.shape[1:]),dtype=np.single)
        samples_y = np.zeros( [batch_size] + list(input_data_y.shape[1:]),dtype=np.single)
        for i in range(batch_size):
            samples_x[i] = input_data_x[index%50_000]
            samples_y[i] = input_data_y[index%50_000]
            index +=1

        yield samples_x,samples_y

# Basically a linear classifier:
def make_model():
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(10,tf.nn.softmax))
    model.build([None] +list(train_x[0,:,:,:].shape))
    return model


#Training:
generator = data_generator_dummy(train_x,train_y,batch_size=BATCH_SIZE)
model = make_model()
model.summary()
optimizer_adam=tf.keras.optimizers.Adam(learning_rate=0.0005/32,beta_1=0.9,beta_2=0.999,epsilon=1e-07)
model.compile(optimizer_adam,loss=&quot;sparse_categorical_crossentropy&quot;,metrics=&quot;accuracy&quot;)
model.fit(generator, validation_data=(validation_x,validation_y),epochs=10,
          steps_per_epoch=train_x.shape[0]//BATCH_SIZE,
          )

# This one however works:
# model.fit(train_x, train_y, validation_data=(validation_x,validation_y),epochs=30,
#           steps_per_epoch=train_x.shape[0]//BATCH_SIZE,
#           shuffle=True
#           )
</code></pre>
<hr />
<p>The model also trains if one first let's the generator generate a long list of samples and then passes those into <code>fit</code> as <code>x</code>and <code>y</code>:</p>
<pre><code>#Training:
generator = data_generator_dummy(train_x,train_y,batch_size=50000)
model = make_model()
model.summary()
optimizer_adam=tf.keras.optimizers.Adam(learning_rate=0.0005/32,beta_1=0.9,beta_2=0.999,epsilon=1e-07)
model.compile(optimizer_adam,loss=&quot;sparse_categorical_crossentropy&quot;,metrics=&quot;accuracy&quot;)
while True:
    samples_x,samples_y = next(generator)
    model.fit(samples_x,samples_y, validation_data=(validation_x,validation_y),epochs=10,batch_size=BATCH_SIZE
          )
</code></pre>
","I want to replace the x and y training data parameters in tf.keras.Model.fit with a generator. However, some subtlety seems to escape me, as the model accuracy doesn't improve with the generator when training. As far as I understand the documentation, the generator is supposed to yield tuples (x_vals,y_vals), such that x_vals is a concatenation of batch_size-many training samples along a new 0th dimension, and 'v_vals' is the concatenation of their corresponding labels. As long as the generator fulfills this, as I understand it, we can just replace the x parameter in tf.keras.Model.fit with the generator and omit the y parameter, though to define an epoch, we also need to specify 'steps_per_epoch' in fit. There however seems to be something here I misunderstood or forgot, because starting with a model and input data that trains (i.e. its accuracy improves) and replacing the training data array with a generator as discussed, results in a model that doesn't train (i.e. its accuracy instead goes up a little, then however goes back down till its equal to chance). The corresponding code: The model also trains if one first let's the generator generate a long list of samples and then passes those into fit as xand y:",https://stackoverflow.com/questions/75572543,8536211,Documentation Replication on Other Examples
75639137,TF1 to TF2 migration,"<p>Hello I am new to tensorflow and I am working on a code that I would like to migrate from tensorflow 1 to 2. I have this line of code:</p>
<pre><code>x1 = tf.compat.v1.placeholder(tf.float32, [], name=&quot;x1&quot;)
</code></pre>
<p>As mentioned in <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder</a>, I should use <code>keras.Input</code>. But even when specifying the shape, I can't have the same tensor as with compat.v1:</p>
<pre><code>x2 = tf.keras.Input(shape=[], dtype=tf.float32, name=&quot;x2&quot;)
</code></pre>
<p>To check the shape I use <code>tf.shape(x1)</code> or <code>tf.shape(x2)</code>, but the shapes are not the same. Could anyone explain to me how to have, in TF2, the same shape as in TF1 ?
Thanks and regards</p>
","Hello I am new to tensorflow and I am working on a code that I would like to migrate from tensorflow 1 to 2. I have this line of code: As mentioned in https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder, I should use keras.Input. But even when specifying the shape, I can't have the same tensor as with compat.v1: To check the shape I use tf.shape(x1) or tf.shape(x2), but the shapes are not the same. Could anyone explain to me how to have, in TF2, the same shape as in TF1 ? Thanks and regards",https://stackoverflow.com/questions/75639137,15822972,Documentation Replicability
75851842,tensorflow map function to mulitple tensors,"<p>I am using the following function in a custom layer in TensorFlow to rearrange query, key values:</p>
<pre><code>q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h = self.heads), (q, k, v)) 
</code></pre>
<p>and it throws this warning:</p>
<p><code>WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23. Instructions for updating: Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089</code></p>
<p>Is there a more TensorFlowic way of doing this?</p>
<p>I tried using map_fn as follows and it throws the the same warning and an error:</p>
<pre><code>import tensorflow as tf
from einops import rearrange
a = tf.random.uniform((1, 196, 196))
b, c, d = tf.map_fn(lambda t: rearrange(t, 'b (h n) d -&gt; b h n d', h=14), [a, a, a])
</code></pre>
<p>From documentation, it seems <code>tf.map_fn</code> but it seems to work on a stack of tensors. Will it be better to stack the tensors?</p>
","I am using the following function in a custom layer in TensorFlow to rearrange query, key values: and it throws this warning: WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23. Instructions for updating: Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089 Is there a more TensorFlowic way of doing this? I tried using map_fn as follows and it throws the the same warning and an error: From documentation, it seems tf.map_fn but it seems to work on a stack of tensors. Will it be better to stack the tensors?",https://stackoverflow.com/questions/75851842,13262692,Documentation Replication on Other Examples
75996642,Is there a good equivalent of pandas' `apply` for TensorFlow datasets?,"<p><strong>BACKGROUND</strong></p>
<p>The use of <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code></a> is promoted by TensorFlow as the best practice for implementing input pipelines due to their efficient implementation of common operations such as batching, shuffling, as well as their seamless integration with the Keras API.</p>
<p>I may just be lousy at looking up the documentation on the matter, but it seems to me that the major drawback of TensorFlow datasets is that they are quite unwieldy, if not impossible to work with, when trying to implement feature engineering tasks whereby a new column is created via the application of some generic Python function. This is in contrast to pandas' very nifty <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>apply()</code></a> function which can produce new columns from preexisting ones both efficiently (i.e., via vectorization) and in a pythonic manner.</p>
<p>To the best of my understanding, the closest thing to pandas' <code>apply()</code> is TensorFlow dataset's <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer""><code>map()</code></a>. However, one can't simply use it with arbitrary Python functions since they'd first need to be converted to tensors. This becomes very difficult as one has to have arcane knowledge of the miscellaneous tensor analogues of arbitrary Python functions (e.g., <code>tf.strings.length()</code> instead of Python's <code>len()</code>). Even when one finds such functions, the idiosyncracies of tensor operations makes them very un-pythonic and prone to obscure dimensionality or type errors.</p>
<p>I've read about TensorFlow's <a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""nofollow noreferrer""><code>py_function</code></a> as some sort of wrapper that magically converts Pythonic code into a tensor representation, but judging from the documentation, it became clear to me that this is far from the case.</p>
<p><strong>QUESTION</strong></p>
<p>Is TensorFlow's <code>tf.data</code> just not mature yet to be able to handle feature engineering in the same way that pandas <code>apply()</code> does? If not, what am I missing in my understanding?</p>
<p><strong>MINIMUM WORKING EXAMPLE</strong></p>
<p>In the code below, I compare a pandas DataFrame <code>df</code> with the equivalent TensorFlow dataset <code>ds</code>. My goal is to engineer two extra features, namely</p>
<ol>
<li>adding a suffix to the string feature <code>my_string</code>, and</li>
<li>counting the number of time a certain letter appears in any given instance of <code>my_string</code>.</li>
</ol>
<p>As you can see for yourself, the first operation works intuitively for both pandas and TensorFlow, but the second only works easily for pandas. Getting it to work with TensorFlow is either extremely complex, or just plain impossible.</p>
<pre><code>from collections import Counter
import numpy as np
import pandas as pd
import tensorflow as tf

# Create the pandas DataFrame.
df = pd.DataFrame(
    {'index': list(range(5)),
     'my_string': ['Alondra', 'Spaanbiuk', 'Ibinth', 'Liefelle', 'Yoanda'], 
     'some_other_column': np.random.rand(5),
     }).set_index('index')
print('Original pandas DataFrame:')
print(df, '\n')

# Create the TensorFlow dataset and define a function to view it 
# as a pandas DataFrame.
ds = tf.data.Dataset.from_tensors(df.to_dict(orient='list'))
def view_ds(ds):
    data = pd.concat([pd.DataFrame(k) for k in ds.take(5)], axis=0)
    # Convert byte strings to Python strings.
    object_cols = data.select_dtypes([object])
    data[object_cols.columns] = object_cols.stack().str.decode('utf-8').unstack()
    print('TensorFlow dataset:')
    print(data, '\n')
view_ds(ds)

#### Add a suffix to `my_string` as `my_string_with_suffix`.
def add_a_suffix(x):
    x['my_string_with_suffix'] = x['my_string']+'.suffix'
    return x

# Apply to the pandas DataFrame.
print('DataFrame with the suffix:')
df = df.apply(add_a_suffix, axis=1)
print(df, '\n')

# Apply to the TensorFlow dataset.
print('TensorFlow dataset with the suffix:')
ds = ds.map(lambda x: add_a_suffix(x))
view_ds(ds)

#### Count they the number of `a`'s in `my_string`:
def count_letters(x, letter='a'):
    counter = Counter(x['my_string'].lower())
    x[f'{letter}_counts'] = counter[letter]
    return x

# Apply to the pandas DataFrame.
print('DataFrame with the letter count:')
df = df.apply(count_letters, axis=1)
print(df, '\n')
    
# Apply to the TensorFlow dataset.
# HOW BUT HOW?!!
# print('TensorFlow dataset with the letter count:')
# ds = ds.apply(lambda x: count_letters(x))
# view_ds(ds)
</code></pre>
<p>The output is of the above is as follows.</p>
<pre><code>Original pandas DataFrame:
       my_string  some_other_column
index                              
0        Alondra           0.209685
1      Spaanbiuk           0.972315
2         Ibinth           0.933700
3       Liefelle           0.186369
4         Yoanda           0.667436 

TensorFlow dataset:
   my_string  some_other_column
0    Alondra           0.209685
1  Spaanbiuk           0.972315
2     Ibinth           0.933700
3   Liefelle           0.186369
4     Yoanda           0.667436 

DataFrame with the suffix:
       my_string  some_other_column my_string_with_suffix
index                                                    
0        Alondra           0.209685        Alondra.suffix
1      Spaanbiuk           0.972315      Spaanbiuk.suffix
2         Ibinth           0.933700         Ibinth.suffix
3       Liefelle           0.186369       Liefelle.suffix
4         Yoanda           0.667436         Yoanda.suffix 

TensorFlow dataset with the suffix:
TensorFlow dataset:
   my_string  some_other_column my_string_with_suffix
0    Alondra           0.209685        Alondra.suffix
1  Spaanbiuk           0.972315      Spaanbiuk.suffix
2     Ibinth           0.933700         Ibinth.suffix
3   Liefelle           0.186369       Liefelle.suffix
4     Yoanda           0.667436         Yoanda.suffix 

DataFrame with the letter count:
       my_string  some_other_column my_string_with_suffix  a_counts
index                                                              
0        Alondra           0.209685        Alondra.suffix         2
1      Spaanbiuk           0.972315      Spaanbiuk.suffix         2
2         Ibinth           0.933700         Ibinth.suffix         0
3       Liefelle           0.186369       Liefelle.suffix         0
4         Yoanda           0.667436         Yoanda.suffix         2 
</code></pre>
","BACKGROUND The use of tf.data.Dataset is promoted by TensorFlow as the best practice for implementing input pipelines due to their efficient implementation of common operations such as batching, shuffling, as well as their seamless integration with the Keras API. I may just be lousy at looking up the documentation on the matter, but it seems to me that the major drawback of TensorFlow datasets is that they are quite unwieldy, if not impossible to work with, when trying to implement feature engineering tasks whereby a new column is created via the application of some generic Python function. This is in contrast to pandas' very nifty apply() function which can produce new columns from preexisting ones both efficiently (i.e., via vectorization) and in a pythonic manner. To the best of my understanding, the closest thing to pandas' apply() is TensorFlow dataset's map(). However, one can't simply use it with arbitrary Python functions since they'd first need to be converted to tensors. This becomes very difficult as one has to have arcane knowledge of the miscellaneous tensor analogues of arbitrary Python functions (e.g., tf.strings.length() instead of Python's len()). Even when one finds such functions, the idiosyncracies of tensor operations makes them very un-pythonic and prone to obscure dimensionality or type errors. I've read about TensorFlow's py_function as some sort of wrapper that magically converts Pythonic code into a tensor representation, but judging from the documentation, it became clear to me that this is far from the case. QUESTION Is TensorFlow's tf.data just not mature yet to be able to handle feature engineering in the same way that pandas apply() does? If not, what am I missing in my understanding? MINIMUM WORKING EXAMPLE In the code below, I compare a pandas DataFrame df with the equivalent TensorFlow dataset ds. My goal is to engineer two extra features, namely As you can see for yourself, the first operation works intuitively for both pandas and TensorFlow, but the second only works easily for pandas. Getting it to work with TensorFlow is either extremely complex, or just plain impossible. The output is of the above is as follows.",https://stackoverflow.com/questions/75996642,5640161,Documentation Ambiguity
76040030,Problem using Huggingface imagenet-1k dataset in Keras / Tensorflow,"<p>I'm having a problem using the imagenet-1k dataset from Huggingface with a Keras model. I'm just experimenting with simple models, but am stuck trying to get the dataset to work with the model fit function.</p>
<p>Here is how I load the dataset:</p>
<pre><code>ds = load_dataset('imagenet-1k')  # loads a DatasetDict
ds_train = ds['train']  # get a Dataset
ds_train.set_format(type='tensorflow', columns=['image'])  # convert to tf tensor
ds_val = ds['validation']  # get a Dataset
ds_val.set_format(type='tensorflow', columns=['image'])  # convert to tf tensor
</code></pre>
<p>Here is the fit invocation:</p>
<pre><code># train the autoencoder
autoencoder.fit(ds_train, ds_train,
                epochs=10,
                shuffle=True,
                validation_data=(ds_val, ds_val))
</code></pre>
<p>I get the following error:</p>
<pre><code>ValueError: Failed to find data adapter that can handle input: &lt;class 'datasets.arrow_dataset.Dataset'&gt;, &lt;class 'datasets.arrow_dataset.Dataset'&gt;
</code></pre>
<p>When I inspect one of the elements of the datasets it looks like a tf.Tensor, so I don't understand why it can't be passed directly. None of the examples or docs I can find make it clear how to do this. Huggingface <a href=""https://huggingface.co/docs/datasets/v2.11.0/en/use_with_tensorflow"" rel=""nofollow noreferrer"">examples</a> for images produce the same format that I'm getting, but apparently there is a step I'm missing before it can be used with model.fit()</p>
","I'm having a problem using the imagenet-1k dataset from Huggingface with a Keras model. I'm just experimenting with simple models, but am stuck trying to get the dataset to work with the model fit function. Here is how I load the dataset: Here is the fit invocation: I get the following error: When I inspect one of the elements of the datasets it looks like a tf.Tensor, so I don't understand why it can't be passed directly. None of the examples or docs I can find make it clear how to do this. Huggingface examples for images produce the same format that I'm getting, but apparently there is a step I'm missing before it can be used with model.fit()",https://stackoverflow.com/questions/76040030,21668078,Documentation Replication on Other Examples
76244268,Tensorflow: Build new model from input and middle layers of another model,"<p>I'm trying to build <code>new_model</code> from another model layers for class activation mapping purposes.</p>
<pre class=""lang-py prettyprint-override""><code>def vgg_sequential():
    input_shape = IMG_SIZE + (3,)
    model = Sequential()
    model.add(tf.keras.applications.vgg16.VGG16(input_shape=input_shape, include_top=False, weights='imagenet'))
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dense(1))
    return model
</code></pre>
<pre class=""lang-py prettyprint-override""><code>cam_model = tf.keras.Model(inputs=seq_vgg.layers[0].input, outputs=(seq_vgg.layers[-3].output, seq_vgg.layers[-1].output))
</code></pre>
<p>And with this code i get the following error:</p>
<pre><code>ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 480, 480, 3), dtype=tf.float32, name='vgg16_input'), name='vgg16_input', description=&quot;created by layer 'vgg16_input'&quot;) at layer &quot;vgg16&quot;. The following previous layers were accessed without issue: ['block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_pool', 'block5_conv1']
</code></pre>
<p>Already tried functional model API, providing <code>Input()</code> layer inside <code>vgg_sequential()</code> with the same error that my Input layer is disconected from the rest of my model. Beside this when using <code>tf.keras.applications.efficientnet_v2</code> that provides input layers for rescaling and resizing images i don't have any problem.</p>
<p>Any help, information, tips or links to docs that getas me to a solution will be very much appreciated.</p>
<p>Thanks in advance.</p>
","I'm trying to build new_model from another model layers for class activation mapping purposes. And with this code i get the following error: Already tried functional model API, providing Input() layer inside vgg_sequential() with the same error that my Input layer is disconected from the rest of my model. Beside this when using tf.keras.applications.efficientnet_v2 that provides input layers for rescaling and resizing images i don't have any problem. Any help, information, tips or links to docs that getas me to a solution will be very much appreciated. Thanks in advance.",https://stackoverflow.com/questions/76244268,2103321,Inadequate Examples
76324368,Understanding tf.keras.layers.Dense(),"<p>I am trying to understand why there is a difference between calculating a dense layer operation directly and using the <code>keras</code> implementation.</p>
<p>Following the documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense</a>) <code>tf.keras.layers.Dense()</code> should implement the operation <code>output = activation(dot(input, kernel) + bias)</code> but <code>result</code> and <code>result1</code> below are not the same.</p>
<pre class=""lang-py prettyprint-override""><code>tf.random.set_seed(1)

bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32)
kernel = tf.Variable(tf.random.uniform(shape=(5,10)), dtype=tf.float32)
x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32))

result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x) + bias)
tf.print(result)

test = tf.keras.layers.Dense(units = 5, 
                            activation = 'relu',
                            use_bias = True, 
                            kernel_initializer = tf.keras.initializers.Constant(value=kernel), 
                            bias_initializer = tf.keras.initializers.Constant(value=bias), 
                            dtype=tf.float32)

result1 = test(tf.transpose(x))

print()
tf.print(result1)

</code></pre>
<p>output</p>
<pre class=""lang-py prettyprint-override""><code>
[[2.87080455]
 [3.25458574]
 [3.28776264]
 [3.14319134]
 [2.04760242]]

[[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]

</code></pre>
<p>Using <code>test.get_weights()</code> I can see that the kernel and bias (<code>b</code>) are getting set to the correct values. I am using TF version 2.12.0.</p>
","I am trying to understand why there is a difference between calculating a dense layer operation directly and using the keras implementation. Following the documentation (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) tf.keras.layers.Dense() should implement the operation output = activation(dot(input, kernel) + bias) but result and result1 below are not the same. output Using test.get_weights() I can see that the kernel and bias (b) are getting set to the correct values. I am using TF version 2.12.0.",https://stackoverflow.com/questions/76324368,18338104,Documentation Replication on Other Examples
76380927,Tensorflow decode image,"<p>I am a beginner in tensorflow and I am training a small cnn, I am using the tf.io.decode_image function but I can't figure out if this function does preprocess.
The tensorflow documentation about it doesn't say anything.
When I open images with this function the values are between 0 and 1.
The images are single channel grayscale.
This is the code.</p>
<pre><code>def decode_img(self, imgs, channels):
        # Convert the compressed string to a 3D uint8 tensor
        images = []
        for element in imgs:

            dec_image = tf.io.decode_image(element, channels=channels, dtype=tf.float32)
            try:
                img = keras.utils.img_to_array(dec_image)
            except AttributeError:
                img = keras.preprocessing.image.img_to_array(dec_image)
            images.append(img)
        images = np.array(images)
        return images
</code></pre>
<p>I would like to have more explanations</p>
","I am a beginner in tensorflow and I am training a small cnn, I am using the tf.io.decode_image function but I can't figure out if this function does preprocess. The tensorflow documentation about it doesn't say anything. When I open images with this function the values are between 0 and 1. The images are single channel grayscale. This is the code. I would like to have more explanations",https://stackoverflow.com/questions/76380927,15460221,Requesting (Additional) Resources
76391276,Custom gradient for broadcasting operation,"<p>I have an operation for which I want to define a custom gradient with <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer""><code>tf.custom_gradient</code></a>. The operation takes two broadcastable arguments and produces a result with the broadcasted shape. The problem is how to handle the broadcasting rules &quot;backwards&quot; in the custom gradient. Let's take the example for a multiplication operation from the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer""><code>tf.custom_gradient</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

@tf.custom_gradient
def bar(x, y):
  def grad(upstream):
    dz_dx = y
    dz_dy = x
    return upstream * dz_dx, upstream * dz_dy
  z = x * y
  return z, grad
</code></pre>
<p>I can use this gradient alright for the non-broadcasting case:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

with tf.GradientTape() as tape:
    a = tf.ones([5])
    b = tf.ones([5])
    tape.watch([a, b])
    c = bar(a, b)
# Works fine
grad_a, grad_b = tape.gradient(c, [a, b])
</code></pre>
<p>However, when the inputs are broadcasted, the result is not correct:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

with tf.GradientTape() as tape:
    a = tf.ones([10, 1])
    b = tf.ones([5])
    tape.watch([a, b])
    c = bar(a, b)
grad_a, grad_b = tape.gradient(c, [a, b])
print(grad_a.shape, grad_b.shape)
# (10, 5) (10, 5)
</code></pre>
<p>In fact, trying to use it in graph mode fails:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

with tf.Graph().as_default():
    a = tf.ones([10, 1])
    b = tf.ones([5])
    c = bar(a, b)
    grad_a, grad_b = tf.gradients(c, [a, b])
# ValueError: Incompatible shapes between op input and calculated input gradient.
</code></pre>
<p>Is there a way to handle this &quot;unbroadcasting&quot; of the input gradients automatically?</p>
","I have an operation for which I want to define a custom gradient with tf.custom_gradient. The operation takes two broadcastable arguments and produces a result with the broadcasted shape. The problem is how to handle the broadcasting rules ""backwards"" in the custom gradient. Let's take the example for a multiplication operation from the documentation of tf.custom_gradient: I can use this gradient alright for the non-broadcasting case: However, when the inputs are broadcasted, the result is not correct: In fact, trying to use it in graph mode fails: Is there a way to handle this ""unbroadcasting"" of the input gradients automatically?",https://stackoverflow.com/questions/76391276,1782792,Documentation Ambiguity
76396532,"Ragged tensors in dataset, tensorflow, how do I train the model","<p>I have</p>
<pre><code>def call (self, inputs):
    context, x = inputs
</code></pre>
<p>in my model, for fitting,
and my dataset contains ragged tensors, basically context and x are ragged tensor, of variable length, everything I try gives me some sort of error, for example<br> first I tried <code>[ [ context, x], ...]</code> where all the arrays were <code>np.ndarray</code>, but it said something along the lines that <code>np.ndarray</code> is an unrecognised datatype and cannot be converted to a <code>tf.Tensor</code>.<br>Then when I tried putting this in a tf.data.Dataset, it says <code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor).</code><br>
I am totally lost on how to train my model</p>
<p>I tried several different data types from list, to tf.data.Dataset, but none of them were working, and now I am totally in the dark. Consulting the documentation has not helped me figure out how does the fit function actually treat data</p>
","I have in my model, for fitting, and my dataset contains ragged tensors, basically context and x are ragged tensor, of variable length, everything I try gives me some sort of error, for example first I tried [ [ context, x], ...] where all the arrays were np.ndarray, but it said something along the lines that np.ndarray is an unrecognised datatype and cannot be converted to a tf.Tensor.Then when I tried putting this in a tf.data.Dataset, it says ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor). I am totally lost on how to train my model I tried several different data types from list, to tf.data.Dataset, but none of them were working, and now I am totally in the dark. Consulting the documentation has not helped me figure out how does the fit function actually treat data",https://stackoverflow.com/questions/76396532,13154958,Documentation Replicability
76444107,Are 'validation_steps' used if the validation_dataset is 'DirectoryIterator'?,"<p>I was trying to use the Keras API to train a given model by using its <a href=""https://keras.io/api/models/model_training_apis/"" rel=""nofollow noreferrer"">fit</a> function. In its documentation we can see the following:</p>
<blockquote>
<p>validation_steps: Only relevant if validation_data is provided and is
a tf.data dataset. Total number of steps (batches of samples) to draw
before stopping when performing validation at the end of every epoch.
If 'validation_steps' is None, validation will run until the
validation_data dataset is exhausted. In the case of an infinitely
repeated dataset, it will run into an infinite loop. If
'validation_steps' is specified and only part of the dataset will be
consumed, the evaluation will start from the beginning of the dataset
at each epoch. This ensures that the same validation samples are used
every time.</p>
</blockquote>
<p>By reading the above text I understand that if the <code>validation_data</code> is not a tf.data dataset the validation_steps is ignored. However I am not sure if in my case this principle is applied. I am using an <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"" rel=""nofollow noreferrer"">ImageDataGenerator</a> to then use its <code>flow_from_directory</code> function that returns a DirectoryIterator. In its documentation we can see the following:</p>
<blockquote>
<p>A DirectoryIterator yielding tuples of (x, y) where x is a numpy array
containing a batch of images with shape (batch_size, *target_size,
channels) and y is a numpy array of corresponding labels.</p>
</blockquote>
<p>So, by reading this I am convinced that the whole validation dataset is going to be used despite the fact that validation_steps might be set to a given number. Is there a way I can check whether it is using the whole dataset or only the given <code>validation_steps</code>? Does it simply ignore the <code>validation_steps</code> because the <code>validation_data</code> is not a tf.data and therefore there is no need to check it?</p>
<p>Thanks!</p>
<p>Code example of usage:</p>
<pre><code># model building and whatnot
# (...)
img_val = r'/content/drive/MyDrive/validation'
validation = ImageDataGenerator()
val_dataset = validation.flow_from_directory(img_val,
                                          target_size=(224, 224),
                                          batch_size=32,
                                          class_mode='categorical')

model.fit(train_dataset, validation_data=val_dataset, epochs=1000, steps_per_epoch=1875, validation_steps=375  
</code></pre>
<p>PS: I know that we can let <code>validation_steps</code> be <code>None</code> to use it fully for validating the model in each epoch. My question is especifically if the <code>validation_steps</code> parameter is ignored or not if provided while using data from a ImageDataGenerator.</p>
","I was trying to use the Keras API to train a given model by using its fit function. In its documentation we can see the following: By reading the above text I understand that if the validation_data is not a tf.data dataset the validation_steps is ignored. However I am not sure if in my case this principle is applied. I am using an ImageDataGenerator to then use its flow_from_directory function that returns a DirectoryIterator. In its documentation we can see the following: So, by reading this I am convinced that the whole validation dataset is going to be used despite the fact that validation_steps might be set to a given number. Is there a way I can check whether it is using the whole dataset or only the given validation_steps? Does it simply ignore the validation_steps because the validation_data is not a tf.data and therefore there is no need to check it? Thanks! Code example of usage: PS: I know that we can let validation_steps be None to use it fully for validating the model in each epoch. My question is especifically if the validation_steps parameter is ignored or not if provided while using data from a ImageDataGenerator.",https://stackoverflow.com/questions/76444107,11875606,Documentation Replication on Other Examples
76447508,How to retrain a model that was saved using the tf.saved_model.save() function in Tensorflow,"<p>I am building a Neural Machine Translator for English to Konkani (a local language) language using the Transformer architecture proposed by (Vaswani et, al. 2017). I am following the tutorial code from <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a>. I have trained the model and used the <code>tf.saved_model.save()</code> method to save the model files locally.</p>
<p>I now want to retrain that saved model on a new dataset that I have gathered recently, but I've realised that after loading the model using the <code>tf.saved_model.load()</code> method, I am not able to train it again as the loaded model now lacks the necessary method <code>model.fit()</code> .</p>
<p>Here is a part of the model training code:</p>
<pre class=""lang-py prettyprint-override""><code>class Transformer(tf.keras.Model):
  def __init__(self, *, num_layers, d_model, num_heads, dff,
               input_vocab_size, target_vocab_size, dropout_rate=0.1):
    super().__init__()
    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=input_vocab_size,
                           dropout_rate=dropout_rate)

    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=target_vocab_size,
                           dropout_rate=dropout_rate)

    self.final_layer = tf.keras.layers.Dense(target_vocab_size)

  def call(self, inputs):
    # To use a Keras model with `.fit` you must pass all your inputs in the
    # first argument.
    context, x  = inputs

    context = self.encoder(context)  # (batch_size, context_len, d_model)

    x = self.decoder(x, context)  # (batch_size, target_len, d_model)

    # Final linear layer output.
    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)

    try:
      # Drop the keras mask, so it doesn't scale the losses/metrics.
      # b/250038731
      del logits._keras_mask
    except AttributeError:
      pass

    # Return the final output and the attention weights.
    return logits
#-----------------------------------------------------------------------

#...&lt;code to define optimizers and loss functions&gt;...

# This Class acts as an interface for the Transformer
class Translator(tf.Module):
  def __init__(self, context_tokenizers, target_tokenizers, transformer):
    self.context_tokenizers = context_tokenizers
    self.target_tokenizers = target_tokenizers
    self.transformer = transformer

  def __call__(self, sentence, max_length=MAX_TOKENS): #max_length=MAX_TOKENS
    assert isinstance(sentence, tf.Tensor)
    if len(sentence.shape) == 0:
      sentence = sentence[tf.newaxis]

    sentence = tokenize(sentence,self.context_tokenizers).to_tensor()

    encoder_input = sentence

    # As the output language is English, initialize the output with the
    # English `[START]` token.

    start_end = tokenize('',self.target_tokenizers)[0]
    start = start_end[0][tf.newaxis]
    end = start_end[-1][tf.newaxis]

    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
    output_array = output_array.write(0, start)

    for i in tf.range(max_length):
      output = tf.transpose(output_array.stack())
      predictions = self.transformer([encoder_input, output], training=False)

      # Select the last token from the `seq_len` dimension.
      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

      predicted_id = tf.argmax(predictions, axis=-1)

      # Concatenate the `predicted_id` to the output which is given to the
      # decoder as its input.
      output_array = output_array.write(i+1, predicted_id[0])

      if predicted_id == end:
        break

    output = tf.transpose(output_array.stack())
    # The output shape is `(1, tokens)`.

    text = self.target_tokenizers.detokenize(output)

    tokens = tf.gather(target_vocab, output)

    # `tf.function` prevents us from using the attention_weights that were
    # calculated on the last iteration of the loop.
    # So, recalculate them outside the loop.
    self.transformer([encoder_input, output[:,:-1]], training=False)
    attention_weights = self.transformer.decoder.last_attn_scores

    joined_text = tf.strings.reduce_join(text[0][1:-1], separator=' ', axis=-1)
    return joined_text, tokens, attention_weights
#-----------------------------------------------------------------------

class ExportTranslator(tf.Module):
  def __init__(self, translator):
    self.translator = translator

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def __call__(self, sentence):
    (result,
     tokens,
     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)

    return result
#-----------------------------------------------------------------------

transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=context_vocab_size,
    target_vocab_size=target_vocab_size,
    dropout_rate=dropout_rate)

transformer.compile(
    loss=masked_loss,
    optimizer=optimizer,
    metrics=[masked_accuracy])

# training the model on the training data for some epochs
transformer.fit(train_batches,
                epochs=20,
                validation_data=val_batches,
                callbacks=[
                  tf.keras.callbacks.EarlyStopping(patience=3)],
                )

translator = Translator(context_tokenizer, target_tokenizer, transformer)

exp_translator = ExportTranslator(translator)

#saving the model
tf.saved_model.save(exp_translator, export_dir=MODEL_SAVED_FILES)

#-----------------------------------------------------------------------

#loading a saved model
reloaded = tf.saved_model.load(MODEL_SAVED_FILES)
</code></pre>
<p>Here's the error I get when I try to retrain the model using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>reloaded = tf.saved_model.load(MODEL_SAVED_FILES)

#retraing the model on new dataset
reloaded.translator.transformer.fit(train_batches,
                epochs=20,
                validation_data=val_batches,
                callbacks=[
                  tf.keras.callbacks.EarlyStopping(patience=3)],
                )
</code></pre>
<p>The error:</p>
<pre class=""lang-py prettyprint-override""><code>
---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-41-ad1b625ff6c0&gt; in &lt;cell line: 2&gt;()
      1 #retraing the model on new dataset
----&gt; 2 reloaded.translator.transformer.fit(train_batches,
      3                 epochs=20,
      4                 validation_data=val_batches,
      5                 callbacks=[

AttributeError: '_UserObject' object has no attribute 'fit'
</code></pre>
<p>After reading the documentation I've realised that when saving the model in the above method, the <code>model.fit()</code> and other methods are not saved hence they are not callable.</p>
<p>I need help in finding a way to retrain my saved model, It is not feasible for me to train a new model on a combined dataset as It will take up lot of time and I have very limited resources. I have been looking up on the web for days but couldn't find a solution. Any help in this regards will be appreciated!</p>
","I am building a Neural Machine Translator for English to Konkani (a local language) language using the Transformer architecture proposed by (Vaswani et, al. 2017). I am following the tutorial code from https://www.tensorflow.org/text/tutorials/transformer. I have trained the model and used the tf.saved_model.save() method to save the model files locally. I now want to retrain that saved model on a new dataset that I have gathered recently, but I've realised that after loading the model using the tf.saved_model.load() method, I am not able to train it again as the loaded model now lacks the necessary method model.fit() . Here is a part of the model training code: Here's the error I get when I try to retrain the model using the following code: The error: After reading the documentation I've realised that when saving the model in the above method, the model.fit() and other methods are not saved hence they are not callable. I need help in finding a way to retrain my saved model, It is not feasible for me to train a new model on a combined dataset as It will take up lot of time and I have very limited resources. I have been looking up on the web for days but couldn't find a solution. Any help in this regards will be appreciated!",https://stackoverflow.com/questions/76447508,16851318,Inadequate Examples