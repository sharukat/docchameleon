User,QuestionId,QuestionAPI,IssueType,Title,Question,GroundTruth,related_ids,related_answers,relevant_ids,relevant_answers,relevant_urls,union,avg_related_emb_dist,avg_relevant_emb_dist
1,51586693,tf.gather,Documentation Replication on Other Examples,"Tensor has shape [?, 0] -- how to reshape to [?,]","<p>When <code>src</code> has shape <code>[?]</code>, <code>tf.gather(src, tf.where(src != 0))</code> returns a tensor with shape <code>[?, 0]</code>. I'm not sure how a dimension can have size 0, and I'm especially unsure how to change the tensor back. I didn't find anything in the documentation to explain this, either.</p>

<p>I tried to <code>tf.transpose(tensor)[0]</code>, but the first dimension of the transposed tensor has size 0 and cannot be accessed! What's wrong?</p>
","<p>I think you should use <a href=""https://www.tensorflow.org/api_docs/python/tf/not_equal"" rel=""nofollow noreferrer""><code>tf.not_equal</code></a> to perform elementwise comparison on the tensor.</p>

<pre><code>src = tf.constant([0, 1, 1, 0], dtype=tf.int8)
tf.gather(src, tf.where(tf.not_equal(src, 0))).eval(session=tf.Session())

array([[1],
       [1]], dtype=int8)
</code></pre>

<p>You can also shorten this a bit and use <a href=""https://www.tensorflow.org/api_docs/python/tf/boolean_mask"" rel=""nofollow noreferrer""><code>tf.boolean_mask</code></a> instead of <code>tf.where</code> and <code>tf.gather</code>:</p>

<pre><code>tf.boolean_mask(src, tf.not_equal(src, 0)).eval(session=tf.Session())
array([1, 1], dtype=int8)
</code></pre>

<p>Note the difference in the shape of the outputs.</p>
","{50983969, 46465925, 58620552, 37703886, 49409488, 34141430, 70795799, 55003704, 47026042, 33705404}","[{'QuestionId': 34141430, 'AnswerId': 37241750, 'URL': 'https://stackoverflow.com/questions/34141430/tensorflow-tensor-reshape-and-pad-with-zeros/37241750#37241750', 'QuestionTitle': 'Tensorflow Tensor reshape and pad with zeros', 'Answer': '<p>Tensorflow now offers the pad function which performs padding on a tensor in a number of ways(like opencv2\'s padding function for arrays):\n<a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/pad</a></p>\n<pre><code>tf.pad(tensor, paddings, mode=\'CONSTANT\', name=None)\n</code></pre>\n<p>example from the docs above:</p>\n<pre><code># \'t\' is [[1, 2, 3],\n#         [4, 5, 6]]\n# \'paddings\' is [[1, 1], [2, 2]]\n# rank of \'t\' is 2.\npad(t, paddings, &quot;CONSTANT&quot;) ==&gt; [[0, 0, 0, 0, 0, 0, 0],\n                                  [0, 0, 1, 2, 3, 0, 0],\n                                  [0, 0, 4, 5, 6, 0, 0],\n                                  [0, 0, 0, 0, 0, 0, 0]]\n\npad(t, paddings, &quot;REFLECT&quot;) ==&gt; [[6, 5, 4, 5, 6, 5, 4],\n                                 [3, 2, 1, 2, 3, 2, 1],\n                                 [6, 5, 4, 5, 6, 5, 4],\n                                 [3, 2, 1, 2, 3, 2, 1]]\n\npad(t, paddings, &quot;SYMMETRIC&quot;) ==&gt; [[2, 1, 1, 2, 3, 3, 2],\n                                   [2, 1, 1, 2, 3, 3, 2],\n                                   [5, 4, 4, 5, 6, 6, 5],\n                                   [5, 4, 4, 5, 6, 6, 5]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1463334985}, {'QuestionId': 70795799, 'AnswerId': 70806418, 'URL': 'https://stackoverflow.com/questions/70795799/how-to-solve-input-to-reshape-is-a-tensor-with/70806418#70806418', 'QuestionTitle': 'How to solve Input to reshape is a tensor with', 'Answer': '<p>You have specified your input shape as (150, 150, 3) and your image shape is (96, 96, 3), these are incompatible.</p>\n<p>You can either resize your images to (150, 150, 3) or change your input shape to be the same as your image shape.</p>\n', 'IsAccepted': False, 'CreationDate': 1642791807}, {'QuestionId': 47026042, 'AnswerId': 47026470, 'URL': 'https://stackoverflow.com/questions/47026042/how-to-reshape-a-tensor-with-multiple-none-dimensions/47026470#47026470', 'QuestionTitle': 'How to reshape a tensor with multiple `None` dimensions?', 'Answer': '<p>In this case you can access to the dynamic shape of <code>X</code> through <code>tf.shape(X)</code>:</p>\n<pre><code>shape = [tf.shape(X)[k] for k in range(4)]\nY = tf.reshape(X, [shape[0], shape[1]*shape[2], shape[3]])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1509412244}, {'QuestionId': 46465925, 'AnswerId': 59424403, 'URL': 'https://stackoverflow.com/questions/46465925/input-to-reshape-is-a-tensor-with-37632-values-but-the-requested-shape-has-1505/59424403#59424403', 'QuestionTitle': 'Input to reshape is a tensor with 37632 values, but the requested shape has 150528', 'Answer': '<p>I had the same error just like you, and I found the reason behind it. It is because when you store your image with .tostring(), the data is stored with the format of tf.float32. Then you decode the tfrecord with decode_raw(tf.uint8), which causes the dismatch error.\nI solved it by change the code to:</p>\n\n<pre><code>image=tf.decode_raw(image_raw,tf.float32)\n</code></pre>\n\n<p>or:</p>\n\n<pre><code>image=tf.image.decode_jpeg(image_raw,channels=3)\n</code></pre>\n\n<p>if you image_raw is jpeg format originally</p>\n', 'IsAccepted': False, 'CreationDate': 1576840581}, {'QuestionId': 58620552, 'AnswerId': 58622508, 'URL': 'https://stackoverflow.com/questions/58620552/tf-reshape-is-not-giving-none-for-first-element/58622508#58622508', 'QuestionTitle': 'tf.reshape is not giving ?(None) for first element', 'Answer': '<p>The task you are trying to achieve is fundamentally wrong. You are trying to make a Partially-known shape from Fully-known shape, <a href=""https://www.tensorflow.org/api_docs/python/tf/TensorShape"" rel=""nofollow noreferrer"">see the documentation</a>. </p>\n\n<p>The partially-known shapes are used, when you don\'t know the fully-known shape during the graph construction. Anyway, you have to specify the actual shape when executing the graph. Therefore, it doesn\'t make sense to convert any tensor with fully-known shape to the partially-known one.</p>\n\n<p>If you want to perform an operation on some partially-known shape, use broadcasting (e.g. <code>add</code> operation on tensors with shape <code>(1, 2, 3)</code> and <code>(None, 2, 3)</code> results in a tensor of shape <code>(None, 2, 3)</code>)</p>\n', 'IsAccepted': False, 'CreationDate': 1572428699}, {'QuestionId': 58620552, 'AnswerId': 58622067, 'URL': 'https://stackoverflow.com/questions/58620552/tf-reshape-is-not-giving-none-for-first-element/58622067#58622067', 'QuestionTitle': 'tf.reshape is not giving ?(None) for first element', 'Answer': '<p>The ""problem"" is TensorFlow does as much shape inference as it can, which is generally something good, but it makes it more complicated if you explicitly want to have a <code>None</code> dimension. Not an ideal solution, but one possible workaround is to use a <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/placeholder_with_default"" rel=""nofollow noreferrer""><code>tf.placeholder_with_default</code></a>, for example like this:</p>\n\n<pre><code>import tensorflow as tf\n\na = tf.constant([[1, 2, 3], [4, 5, 6]])\n# This placeholder is never actually fed\nz = tf.placeholder_with_default(tf.zeros([1, 1, 1], a.dtype), [None, 1, 1])\nb = a + z\nprint(b)\n# Tensor(""add:0"", shape=(?, 2, 3), dtype=int32)\n</code></pre>\n\n<p>Or another similar option, just with reshaping:</p>\n\n<pre><code>import tensorflow as tf\n\na = tf.constant([[1, 2, 3], [4, 5, 6]])\ns = tf.placeholder_with_default([1, int(a.shape[0]), int(a.shape[1])], [3])\nb = tf.reshape(a, s)\nb.set_shape(tf.TensorShape([None]).concatenate(a.shape))\nprint(b)\n# Tensor(""Reshape:0"", shape=(?, 2, 3), dtype=int32)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1572427263}, {'QuestionId': 55003704, 'AnswerId': 55004230, 'URL': 'https://stackoverflow.com/questions/55003704/reshape-a-tensor-with-none-for-batch-size/55004230#55004230', 'QuestionTitle': 'Reshape a tensor with None for batch size', 'Answer': '<p>You should use <code>-1</code> instead of <code>None</code> to specify the dimension which should be calculated automatically. Try <code>tf.reshape(h1, [-1] + conv.shape.as_list()[1:])</code>.</p>\n', 'IsAccepted': False, 'CreationDate': 1551793256}, {'QuestionId': 46465925, 'AnswerId': 52752117, 'URL': 'https://stackoverflow.com/questions/46465925/input-to-reshape-is-a-tensor-with-37632-values-but-the-requested-shape-has-1505/52752117#52752117', 'QuestionTitle': 'Input to reshape is a tensor with 37632 values, but the requested shape has 150528', 'Answer': ""<p>I had the same error , so changed my code from this:</p>\n\n<pre><code>image = tf.decode_raw(image_raw, tf.float32)\nimage = tf.reshape(image, [img_width, img_height, 3])\n</code></pre>\n\n<p>to this: </p>\n\n<pre><code>image = tf.decode_raw(image_raw, tf.uint8)\nimage = tf.reshape(image, [img_width, img_height, 3])\n\n\n# The type is now uint8 but we need it to be float.\nimage = tf.cast(image, tf.float32)\n</code></pre>\n\n<p>It is because somehow there's a mismatch in my generate_tf_record data format. I serialized it to string instead of bytelist. I notice the difference you and me, you change your image to byte . Here's how I write my image to tfrecord.</p>\n\n<pre><code>            file_path, label = sample\n            image = Image.open(file_path)\n            image = image.resize((224, 224))\n            image_raw = np.array(image).tostring()\n\n            features = {\n                'label': _int64_feature(class_map[label]),\n                'text_label': _bytes_feature(bytes(label, encoding = 'utf-8')),\n                'image': _bytes_feature(image_raw)\n            }\n\n            example = tf.train.Example(features=tf.train.Features(feature=features))\n            writer.write(example.SerializeToString())  \n</code></pre>\n\n<p>hope it will help.</p>\n"", 'IsAccepted': False, 'CreationDate': 1539231396}, {'QuestionId': 46465925, 'AnswerId': 52240509, 'URL': 'https://stackoverflow.com/questions/46465925/input-to-reshape-is-a-tensor-with-37632-values-but-the-requested-shape-has-1505/52240509#52240509', 'QuestionTitle': 'Input to reshape is a tensor with 37632 values, but the requested shape has 150528', 'Answer': '<p>The problem is basically related to shape of Architecture of CNN.Let say I defined architecture shown in picture<a href=""https://i.stack.imgur.com/6DXZ1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6DXZ1.png"" alt=""CNN Architecture""></a> int coding we defined weights and biases in following way<a href=""https://i.stack.imgur.com/ZNwYE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZNwYE.png"" alt=""enter image description here""></a>\nIf we see (weights) Lets start with </p>\n\n<p><strong>wc1</strong>  in this layer I defined 32 filters of 3x3 size will be applied</p>\n\n<p><strong>wc2</strong>  in this layer I defined 64 filters of 3x3 size will be applied</p>\n\n<p><strong>wc3</strong>  in this layer I defined 128 filters of 3x3 size will be applied</p>\n\n<p><strong>wd1</strong>  38*38*128 is interesting (Where it comes from). </p>\n\n<p>And in Architecture we also defined maxpooling concept. \nSee Architecture pic in every step\n1.Lets Explain it  Let say your input image is 300 x 300 x 1 (in picture it is 28x28x1)\n2. (If strides defined is set to 1)Each filter will have an 300x300x1 picture so After applying 32 filter of 3x3 \nthe we will have 32 pictures of 300x300 thus collected images will be 300x300x32</p>\n\n<p>3.After Maxpooling if (Strides=2 depends what you defined usually it is 2) image size will change from 300 x 300 x 32 to <strong>150 x 150 x 32</strong></p>\n\n<ol start=""4"">\n<li>(If strides defined is set to 1)Now Each filter will have an 150x150x32 picture so After applying 64 filter of 3x3 \nthe we will have 64 pictures of 300x300 thus collected images will be 150x150x(32x64)</li>\n</ol>\n\n<p>5.After Maxpooling if (Strides=2 depends what you defined usually it is 2) image size will change from 150x150x(32x64)\n  to <strong>75 x 75 x (32x64)</strong></p>\n\n<ol start=""6"">\n<li>(If strides defined is set to 1)Now Each filter will have an 75 x 75 x (32x64) picture so After applying 64 filter of 3x3 \nthe we will have 128 pictures of 75 x 75 x (32x64) thus collected images will be 75 x 75 x (32x64x128)</li>\n</ol>\n\n<p>7.After Maxpooling <strong>since dimension of image is 75x75(odd dimension make it even) so it is needed to pad first (if padding defined =\'Same\') then it will change to 76x76(even) ** if (Strides=2 depends what you defined usually it is 2) image size will change from 76x76x(32x64x128)\n  to **38 x 38 x (32x64x128)</strong></p>\n\n<p>Now See \'wd1\' in coding picture here comes 38*38*128</p>\n', 'IsAccepted': False, 'CreationDate': 1536454730}, {'QuestionId': 50983969, 'AnswerId': 50987477, 'URL': 'https://stackoverflow.com/questions/50983969/how-tensorflow-reshape-a-tensor-with-dimension-of-none-like-this-none-2-2-1/50987477#50987477', 'QuestionTitle': 'How tensorflow reshape a tensor with dimension of None, like this [None,2,2,1]?', 'Answer': '<p>I think this is what the question and the comment with the answer means. Had to clarify for myself first. Something similar.</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nx = tf.placeholder(dtype=tf.float32, shape=(None))\nx_reshaped = tf.reshape(x, shape=[tf.shape(x)[0], 2,2,1])\n\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    _, x_reshaped_result = sess.run([x, x_reshaped], feed_dict={x: np.random.random(16).reshape(4, 2, 2, 1)})\n    print (x_reshaped_result.shape)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1529668823}, {'QuestionId': 50983969, 'AnswerId': 50987213, 'URL': 'https://stackoverflow.com/questions/50983969/how-tensorflow-reshape-a-tensor-with-dimension-of-none-like-this-none-2-2-1/50987213#50987213', 'QuestionTitle': 'How tensorflow reshape a tensor with dimension of None, like this [None,2,2,1]?', 'Answer': ""<p>To reshape the placeholder simply use <code>-1</code> for the unknown dimension. You must also feed a value to the placeholder when you use <code>sess.run</code>. This will work:</p>\n\n<pre><code>import tensorflow as tf\n\nxs = tf.placeholder(tf.float32, [None, 2, 2, 1], name='x-input')\n\nreshaped_xs = tf.reshape(xs, [-1, 4])\n\nwith tf.Session() as sess:\n    x = [[[[0.], [0.]], [[0.], [0.]]]]\n    print(sess.run(reshaped_xs, feed_dict={xs: x}))\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1529667885}, {'QuestionId': 50983969, 'AnswerId': 50984408, 'URL': 'https://stackoverflow.com/questions/50983969/how-tensorflow-reshape-a-tensor-with-dimension-of-none-like-this-none-2-2-1/50984408#50984408', 'QuestionTitle': 'How tensorflow reshape a tensor with dimension of None, like this [None,2,2,1]?', 'Answer': ""<p>You can't reshape a placeholder, it reserve a memory space for the object to be passed in feed dict, the None means the memory will be allocated dynamic,in your case you are reshaping a non allocated memory to another fixed size of memory,it might be syntax correct but i doubt if you run it in ipython it will work.</p>\n"", 'IsAccepted': False, 'CreationDate': 1529658236}, {'QuestionId': 34141430, 'AnswerId': 34143633, 'URL': 'https://stackoverflow.com/questions/34141430/tensorflow-tensor-reshape-and-pad-with-zeros/34143633#34143633', 'QuestionTitle': 'Tensorflow Tensor reshape and pad with zeros', 'Answer': '<p>As far as I know, there\'s no built-in operator that does this (<a href=""https://www.tensorflow.org/versions/master/api_docs/python/array_ops.html#reshape"" rel=""noreferrer""><code>tf.reshape()</code></a> will give you an error if the shapes do not match). However, you can achieve the same result with a few different operators:</p>\n\n<pre><code>a = tf.constant([[1, 2], [3, 4]])\n\n# Reshape `a` as a vector. -1 means ""set this dimension automatically"".\na_as_vector = tf.reshape(a, [-1])\n\n# Create another vector containing zeroes to pad `a` to (2 * 3) elements.\nzero_padding = tf.zeros([2 * 3] - tf.shape(a_as_vector), dtype=a.dtype)\n\n# Concatenate `a_as_vector` with the padding.\na_padded = tf.concat([a_as_vector, zero_padding], 0)\n\n# Reshape the padded vector to the desired shape.\nresult = tf.reshape(a_padded, [2, 3])\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1449523948}, {'QuestionId': 49409488, 'AnswerId': 49409976, 'URL': 'https://stackoverflow.com/questions/49409488/tensorflow-tensor-reshape-and-pad-with-zeros-at-the-end-of-some-rows/49409976#49409976', 'QuestionTitle': 'Tensorflow: Tensor reshape and pad with zeros at the end of some rows', 'Answer': '<p>This should do, and be easy to expand (f.e. with different sorts of padding etc). Please let me know if it works as you expected!</p>\n\n<pre><code>import tensorflow as tf\n\ndef split_and_pad_tensor(tensor, lengths):\n    """"""\n    Input: a rank 2 tensor of shape (A,B) and a collection of indexes that\n    sum up to A (otherwise tf.split crashes).\n    The tensor is then split in len(lengths) tensors of the given lengths,\n    and then each splitted tensor is zero-padded at the right until all have\n    B*max(idxs) elements. Output is then a rank 2 tensor of shape\n    (len(idxs), B*max(idxs))\n    """"""\n    length_result, max_length = len(lengths), max(lengths)\n    splitted = tf.split(tensor, lengths, 0)\n    # pad\'s second argument can be seen as [[left, right], [up, down]]\n    padded = tf.stack([tf.pad(s, [[0,max_length-l],[0,0]]) for l,s in zip(lengths, splitted)])\n    # flatten last two axes:\n    return tf.reshape(padded, [length_result, tf.shape(tensor)[1]*max_length])\n\n# make some data and test for different valid inputs:\nDATA = tf.constant([[x,x,x] for x in [1,2,4,5,6,7]])\nwith tf.Session() as sess:\n    for lengths in ([4,2], [2,3,1], [2,2,1,1]):\n        print sess.run(split_and_pad_tensor(DATA, lengths))\n</code></pre>\n\n<p>Outputs:</p>\n\n<pre><code>[[1 1 1 2 2 2 4 4 4 5 5 5]\n [6 6 6 7 7 7 0 0 0 0 0 0]]\n[[1 1 1 2 2 2 0 0 0]\n [4 4 4 5 5 5 6 6 6]\n [7 7 7 0 0 0 0 0 0]]\n[[1 1 1 2 2 2]\n [4 4 4 5 5 5]\n [6 6 6 0 0 0]\n [7 7 7 0 0 0]]\n</code></pre>\n\n<hr>\n\n<h2>Pure-TF version with placeholders:</h2>\n\n<p>The following code has the same functionality as above, but inputs are placeholders, and the <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer"">tf.map_fn</a> + <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer"">tf.gather</a> combo is used to allow full shape dynamism:</p>\n\n<pre><code>import tensorflow as tf\n\nclass SplitAndPadGraph(object):\n    def __init__(self):\n        # minimal assumptions on the placeholderes\' shapes\n        data_ph = tf.placeholder(tf.float32, shape=[None, None])\n        lengths_ph = tf.placeholder(tf.int32, shape=[None])\n        # extract information about input shapes\n        data_len = tf.shape(data_ph)[0]\n        out_dim0 = tf.shape(lengths_ph)[0]\n        out_dim1 = tf.reduce_max(lengths_ph)\n        out_dim2 = tf.shape(data_ph)[-1]\n        # create a [[x,y,z], ...] tensor, where x=start_idx, y=length, z=pad_size\n        start_idxs = tf.concat([[0], tf.cumsum(lengths_ph)], 0)[:-1]\n        pads = tf.fill([out_dim0], out_dim1)-lengths_ph\n        reconstruction_metadata = tf.stack([start_idxs, lengths_ph, pads], axis=1)\n        # pass the xyz tensor to map_fn to create a tensor with the proper indexes.\n        # then gather the indexes from data_ph and reshape\n        reconstruction_data = tf.map_fn(lambda x: tf.concat([tf.range(x[0],x[0]+x[1]),\n                                                             tf.fill([x[2]], data_len)],\n                                                            0), reconstruction_metadata)\n        output = tf.gather(tf.concat([data_ph, tf.zeros((1,out_dim2))], 0),\n                           tf.reshape(reconstruction_data, [out_dim0*out_dim1]))\n        output = tf.reshape(output, [out_dim0, out_dim1*out_dim2])\n        # graph interface to access input and output nodes from outside\n        self.data_ph = data_ph\n        self.lengths_ph = lengths_ph\n        self.output = output\n\nDATA = [[x,x,x] for x in [1,2,4,5,6,7]]\ng = SplitAndPadGraph()\nwith tf.Session() as sess:\n    for lengths in [[4,2], [2,3,1], [2,2,1,1]]:\n        print ""lengths ="", lengths\n        print sess.run(g.output, feed_dict={g.data_ph:DATA, g.lengths_ph:lengths})\n</code></pre>\n\n<p>Cheers!\nAndres</p>\n', 'IsAccepted': True, 'CreationDate': 1521645046}, {'QuestionId': 37703886, 'AnswerId': 37704866, 'URL': 'https://stackoverflow.com/questions/37703886/how-to-reshape-data-to-none-in-tensorflow/37704866#37704866', 'QuestionTitle': 'How to reshape data to None in tensorflow?', 'Answer': '<p>This is a bit subtle: in TensorFlow terminology, you don\'t actually want to <code>reshape</code> the tensor (i.e. change the number of elements in each dimension), but instead you want TensorFlow to ""forget"" a specific dimension, in order to feed values with a range of sizes.</p>\n\n<p>The <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#placeholder_with_default"" rel=""noreferrer""><code>tf.placeholder_with_default()</code></a> op is designed to support this case. It takes a default input, which in your case would be the next training batch (of shape <code>[64, ...]</code>); and a shape, which in your case would be the same shape as the input, with the first dimension set to <code>None</code>. You can then feed this placeholder with values of any batch size.</p>\n\n<p>Here\'s an example of how you\'d use it:</p>\n\n<pre><code>X_batch, Y_batch = tf.train.batch([image, label], batch_size=64)\n\n# Alternatively, `X_shape = [None, 32, 32, 3]`\nX_shape = tf.TensorShape([None]).concatenate(X_batch.get_shape()[1:])\n# Alternatively, `Y_shape = [None, 10]`\nY_shape = tf.TensorShape([None]).concatenate(Y_batch.get_shape()[1:])\n\n# Create tensors that can be fed with a less specific shape\n# than `X_batch`, `Y_batch`.\nX = tf.placeholder_with_default(X_batch, shape=X_shape)\nY = tf.placeholder_with_default(Y_batch, shape=Y_shape)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1465394829}, {'QuestionId': 33705404, 'AnswerId': 33717309, 'URL': 'https://stackoverflow.com/questions/33705404/reshaping-a-tensor-in-tensorflow-with-scalar-tensors/33717309#33717309', 'QuestionTitle': 'Reshaping a Tensor in TensorFlow with scalar tensors', 'Answer': '<p>Now that you told that you use placeholders to populate data, it started to make sense. Here is an example of how can you reshape your data in this case:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\ndata = np.random.rand(2, 3, 4)\n\nx = tf.placeholder(""float"", None)\ns = tf.shape(x)\n\nsess = tf.Session()\nshape_original = sess.run(s, feed_dict={x: data})\n\nx_ = tf.reshape(x, [shape_original[0] * shape_original[1], shape_original[2]])\ns_ = tf.shape(x_)\n\nshape_now = sess.run(s_, feed_dict={x: data})\nprint \'Original\\t\', shape_original\nprint \'Now\\t\\t\\t\', shape_now\n\nsess.close()\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1447571363}, {'QuestionId': 33705404, 'AnswerId': 33705884, 'URL': 'https://stackoverflow.com/questions/33705404/reshaping-a-tensor-in-tensorflow-with-scalar-tensors/33705884#33705884', 'QuestionTitle': 'Reshaping a Tensor in TensorFlow with scalar tensors', 'Answer': '<p>Take a look at this:</p>\n\n<pre><code>import tensorflow as tf\n\na, b, c = 2, 3, 4\nx = tf.Variable(tf.random_normal([a, b, c], mean=0.0, stddev=1.0, dtype=tf.float32))\ns = tf.shape(x)\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\nv1, v2, v3 = sess.run(s)\ny = tf.reshape(x, [v1 * v2, v3])\nshape = tf.shape(y)\n\nprint sess.run(y)\nprint sess.run(shape)\n</code></pre>\n\n<p>I am getting the shape of the variable after it\'s initialization and then use it later. Also take a look at <a href=""https://stackoverflow.com/a/33642096/1090562"">this answer</a>, as it deals with a similar thing.</p>\n', 'IsAccepted': False, 'CreationDate': 1447483411}]",{56444655},"['<p>I believe this is because you are are returning the dynamic shape by using tf.shape(x). If you use x.get_shape() this should return the static shape. However, you will need to provide or define the batch size in order to do the reshape operation.</p>\n']",{'https://stackoverflow.com/questions/56444655/reshaping-of-a-tensor-with-adaptive-shape-results-in-unexpected-resulting-shape/56448837#56448837'},,0.28288942250412474,0.291502056196581
1,56286350,tf.keras.metrics.SensitivityAtSpecificity,Documentation Replication on Other Examples,tf.keras.metrics.SpecificityAtSensitivity num_thresholds interpretation,"<p>I'm trying to get my head around <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/metrics/SensitivityAtSpecificity"" rel=""nofollow noreferrer"">tf.keras.metrics.SensitivityAtSpecificity</a>. I'm fine with the concept of sensity and specificity in isolation, but I'm unsure how the two are related in this single metric.</p>

<p>More specifically, I'm unsure how to interpret the <code>num_thresholds</code> argument. The example in documentation has <code>num_thresholds=1</code>. Setting <code>num_thresholds</code> greater than 1 with the same input data seems to always return a metric value of 1.0.</p>

<pre class=""lang-py prettyprint-override""><code>def print_metric_value(num_thresholds):
    # other values based on docs example
    m = tf.keras.metrics.SensitivityAtSpecificity(
        0.4, num_thresholds=num_thresholds)
    m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
    print('Result with num_thresholds = %d: %.1f' %
          (num_thresholds, m.result().numpy()))

print_metric_value(1)    # 0.5 - same as docs
print_metric_value(2)    # 1.0
print_metric_value(200)  # 1.0
</code></pre>
","<p>The <code>num_thresholds</code> refers to the number of thresholds. But you might ask: what is a threshold (in this context)? And the answer is that the threshold, which is in the range [0,1], is actually the value which all the predictions greater than that will be considered as positive (i.e. 1) and all the prediction lower than that will be considered as negative (i.e. 0). </p>

<p>For example, consider the prediction vector as <code>[0, 0.5, 0.3, 0.9]</code> which are actually confidences scores (e.g. probabilities). Now if we apply the threshold value of <code>0.1</code>, we get <code>[0, 1, 1, 1]</code>; or if we apply threshold value of <code>0.6</code> we get <code>[0, 0, 0, 1]</code> (i.e. only the confidence of last prediction is higher than <code>0.6</code>).   </p>

<p>Now suppose you want to monitor the changes to specificity at a fixed sensitivity. What <code>SensitivityAtSpecificity</code> metric does is that, to compute the value of sensitivity, it would first compute the specificity at different thresholds and then chooses the threshold which has the closest specificity to the specificity value you have provided (for example, in your question you have given <code>0.4</code> as the specificity value). Then the sensitivity is computed at that threshold and will be returned as the value of this metric. The same thing applies to <code>SpecificityAtSensitivity</code> metric, just swap ""specificity"" and ""sensitivity"" in this paragraph.</p>

<p>You might also ask: what are the threshold values? The answer is if <code>num_thresholds=1</code> then the only threshold is 0.5. If <code>num_thresholds &gt; 1</code> then, besides 0 and 1 as thresholds, the interval (0,1) will be split into <code>num_thresholds - 1</code> equal sub-intervals and the split points are chosen as additional threshold values. For example:</p>

<pre><code>num_threshold  |  thresholds
=============================
1              | [0.5]
2              | [0, 1]
3              | [0, 0.5, 1]
4              | [0, 0.33, 0.66, 1]
5              | [0, 0.25, 0.5, 0.75, 1]
...
</code></pre>
","{72720129, 50433094, 46440332, 58001456, 53362642, 48909330, 66182518, 47191992, 39713694, 42606207}","[{'QuestionId': 72720129, 'AnswerId': 72825886, 'URL': 'https://stackoverflow.com/questions/72720129/understanding-tf-keras-metrics-precision-and-recall-for-multiclass-classificatio/72825886#72825886', 'QuestionTitle': 'Understanding tf.keras.metrics.Precision and Recall for multiclass classification', 'Answer': '<p><strong>3. can I use the argument top_k with the value top_k=2 would be helpful here or it is not suitable for my classification of 4 classes only?</strong></p>\n<p>According to the description, it will only calculate top_k(with the function of _filter_top_k) predictions, and turn other predictions to <code>False</code> if you use this argument</p>\n<p>The example from official document link:<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision</a></p>\n<p>You may also want to read the original code:\n<a href=""https://github.com/keras-team/keras/blob/07e13740fd181fc3ddec7d9a594d8a08666645f6/keras/utils/metrics_utils.py#L487"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/blob/07e13740fd181fc3ddec7d9a594d8a08666645f6/keras/utils/metrics_utils.py#L487</a>\n<em><strong>With top_k=2, it will calculate precision over y_true[:2] and y_pred[:2]</strong></em></p>\n<pre><code>m = tf.keras.metrics.Precision(top_k=2)\nm.update_state([0, 0, 1, 1], [1, 1, 1, 1])\nm.result().numpy()\n0.0\n</code></pre>\n<p>As we can see the note posted in the example here, it will only calculate y_true[:2] and y_pred[:2], which means the precision will calculate only top 2 predictions (also turn the rest of y_pred to 0).</p>\n<p>If you want to use 4 classes classification, the argument of <code>class_id</code> maybe enough.</p>\n<p><strong>4.While I am measuring the performance of each class, What could be the difference when I set the top_k=1 and not setting top_koverall?</strong>\nThe function will calculate the precision across all the predictions your model make if you don\'t set <code>top_k</code> value. If you want to measure the perfromance.</p>\n<p>Top k may works for other model, not for classification model</p>\n', 'IsAccepted': False, 'CreationDate': 1656661063}, {'QuestionId': 72720129, 'AnswerId': 72778398, 'URL': 'https://stackoverflow.com/questions/72720129/understanding-tf-keras-metrics-precision-and-recall-for-multiclass-classificatio/72778398#72778398', 'QuestionTitle': 'Understanding tf.keras.metrics.Precision and Recall for multiclass classification', 'Answer': '<p><strong>1. Is it macro or micro ?</strong></p>\n<p>To be precise, all the metrics are reset at the beginning of every epoch and at the beginning of every validation if there is. So I guess, we can call it macro.</p>\n<p><strong>2. Class specific precision and recall ?</strong></p>\n<p>You can take a look at <code>tf.compat.v1.metrics.precision_at_k</code>  and <code>tf.compat.v1.metrics.recall_at_k</code>. It seems that it computes the respectivly the precision at the recall for a specific <strong>class k</strong>.</p>\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/metrics/precision_at_k"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/compat/v1/metrics/precision_at_k</a></p>\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/metrics/recall_at_k"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/compat/v1/metrics/recall_at_k</a></p>\n', 'IsAccepted': False, 'CreationDate': 1656364680}, {'QuestionId': 66182518, 'AnswerId': 66183535, 'URL': 'https://stackoverflow.com/questions/66182518/tensorflow-custom-metric-sensitivityatspecificity/66183535#66183535', 'QuestionTitle': 'Tensorflow Custom Metric: SensitivityAtSpecificity', 'Answer': '<p>use <code>n = tf.shape(y_predict)[0]</code> intead of <code>n = y_predict.shape[0]</code> for dynamically take into account the batch dimensionality</p>\n<p>pass also your validation data in round brackets: <code>validation_data = (x_test,y_test)</code></p>\n<p>here the running notebook: <a href=""https://colab.research.google.com/drive/1uUb3nAk8CAsLYDJXGraNt1_sYYRYVihX?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1uUb3nAk8CAsLYDJXGraNt1_sYYRYVihX?usp=sharing</a></p>\n', 'IsAccepted': True, 'CreationDate': 1613207935}, {'QuestionId': 58001456, 'AnswerId': 58001684, 'URL': 'https://stackoverflow.com/questions/58001456/tensorflow-keras-accuracy-metric-under-the-hood-implementation/58001684#58001684', 'QuestionTitle': 'TensorFlow Keras &#39;accuracy&#39; metric under the hood implementation', 'Answer': '<p>Found it: this is handled in <a href=""https://github.com/tensorflow/tensorflow/blob/41474120daa68791e09eae99c8bdd8df093eda31/tensorflow/python/keras/engine/training_utils.py#L1007-L1046"" rel=""nofollow noreferrer""><code>tensorflow.python.keras.engine.training_utils.get_metric_function</code></a>. In particular, the output shape is inspected to determine which accuracy function to use.</p>\n\n<p>To elaborate, in the current implementation <a href=""https://github.com/tensorflow/tensorflow/blob/41474120daa68791e09eae99c8bdd8df093eda31/tensorflow/python/keras/engine/training.py#L184-L400"" rel=""nofollow noreferrer""><code>Model.compile</code></a> either delegates metric processing to <a href=""https://github.com/tensorflow/tensorflow/blob/41474120daa68791e09eae99c8bdd8df093eda31/tensorflow/python/keras/engine/training.py#L1563-L1575"" rel=""nofollow noreferrer""><code>Model._compile_eagerly</code></a> (if executing eagerly) or does it directly. In either case, <a href=""https://github.com/tensorflow/tensorflow/blob/41474120daa68791e09eae99c8bdd8df093eda31/tensorflow/python/keras/engine/training.py#L1892-L1908"" rel=""nofollow noreferrer""><code>Model._cache_output_metric_attributes</code></a> is called, which calls <a href=""https://github.com/tensorflow/tensorflow/blob/41474120daa68791e09eae99c8bdd8df093eda31/tensorflow/python/keras/engine/training_utils.py#L748-L824"" rel=""nofollow noreferrer""><code>collect_per_output_metric_info</code></a> for both the unweighted and weighted metrics. This function loops over the provided metrics, calling <code>get_metric_function</code> on each one.</p>\n', 'IsAccepted': True, 'CreationDate': 1568847323}, {'QuestionId': 53362642, 'AnswerId': 53362862, 'URL': 'https://stackoverflow.com/questions/53362642/what-does-the-first-value-returned-from-tf-metrics-accuracy-represent/53362862#53362862', 'QuestionTitle': 'What does the first value returned from tf.metrics.accuracy represent', 'Answer': '<p>Based on tensorflow code and the description, first value is the accuracy computed without the latest batch of data (in case of streaming data) and the second value is the final accuracy computed with all the data. This is used by tensorflow for stream processing of data.    </p>\n', 'IsAccepted': True, 'CreationDate': 1542557165}, {'QuestionId': 50433094, 'AnswerId': 50433993, 'URL': 'https://stackoverflow.com/questions/50433094/what-do-the-return-values-of-tf-metrics-mean/50433993#50433993', 'QuestionTitle': 'What do the return values of tf.metrics mean?', 'Answer': '<p>Yes, it seems there is a bug currently in the running metrics in <code>tf.metrics</code>. A bug has been filed <a href=""https://github.com/tensorflow/tensorflow/issues/19368"" rel=""nofollow noreferrer"">here</a> and the discussion that triggered here <a href=""https://stackoverflow.com/questions/50392027/tf-constant-and-tf-placeholder-behave-differently/50395387#50395387"">there</a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1526812970}, {'QuestionId': 48909330, 'AnswerId': 48910328, 'URL': 'https://stackoverflow.com/questions/48909330/meaning-of-values-returned-by-tensorflow-accuracy-metric/48910328#48910328', 'QuestionTitle': 'Meaning of values returned by tensorflow accuracy metric', 'Answer': '<pre><code>import tensorflow as tf\nfrom sklearn.metrics import accuracy_score\n\n# true and predicted tensors\ny_p = tf.placeholder(dtype=tf.int64)\ny_t = tf.placeholder(dtype=tf.int64)\n\n# Count true positives, true negatives, false positives and false negatives.\ntp = tf.count_nonzero(y_p * y_t)\ntn = tf.count_nonzero((y_p - 1) * (y_t - 1))\nfp = tf.count_nonzero(y_p * (y_t - 1))\nfn = tf.count_nonzero((y_p - 1) * y_t)\n\nacc = tf.metrics.accuracy(predictions=y_p, labels=y_t)\n\n# Calculate accuracy, precision, recall and F1 score.\naccuracy = (tp + tn) / (tp + fp + fn + tn)\n\nwith tf.Session() as sess:\n    for i in range(4):\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n\n\n        if i == 0:\n            yop = [0,0,0,0,0,0,0,0,0,0]\n        elif i == 1:\n            yop = [0,0,0,0,0,0,0,0,1,1]\n        elif i == 2:\n            yop = [1,1,1,0,0,0,0,0,0,1]\n        else:\n            yop = [0,1,1,1,1,1,1,0,0,0]\n        print(\'accuracy_score\', accuracy_score([0,0,0,0,0,0,0,0,0,0], yop))\n        tf_a = sess.run(acc, feed_dict={y_p: [0,0,0,0,0,0,0,0,0,0], y_t: yop})\n        my_a = sess.run(accuracy, feed_dict={y_p: [0,0,0,0,0,0,0,0,0,0], y_t: yop})\n        print(""TF accuracy: {0}"".format(tf_a))\n        print(""My accuracy: {0}"".format(my_a))\n        print()\n</code></pre>\n\n<p>output:</p>\n\n<pre><code>accuracy_score 1.0\nTF accuracy: (0.0, 1.0)\nMy accuracy: 1.0\n\naccuracy_score 0.8\nTF accuracy: (0.0, 0.8)\nMy accuracy: 0.8\n\naccuracy_score 0.6\nTF accuracy: (0.0, 0.6)\nMy accuracy: 0.6\n\naccuracy_score 0.4\nTF accuracy: (0.0, 0.4)\nMy accuracy: 0.4\n</code></pre>\n\n<p>Simply moving the <code>tf.local_variables_initializer()</code> inside the loop makes sure that the values in accuracy metric tensor get\'s re-initialised.</p>\n\n<p><strong>Why does it work?</strong></p>\n\n<p>As per the documentation </p>\n\n<blockquote>\n  <p>The accuracy function creates two local variables, total and count\n  that are used to compute the frequency with which predictions matches\n  labels.</p>\n</blockquote>\n\n<p>If we don\'t reinitialise the local variables then value from previous iteration remains in it, leading to wrong results as you were experiencing.</p>\n\n<p>The other approach is to use:</p>\n\n<p><code>tf.contrib.metrics.accuracy</code> instead of <code>tf.metrics.accuracy</code>. But this gives some residual value at the end like <code>0.800000011920929</code> instead of <code>0.8</code>. It\'s also <a href=""https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/contrib/metrics/streaming_accuracy"" rel=""nofollow noreferrer"">deprecated</a> as pointed out by OP in comments.</p>\n\n<p>Source:</p>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy</a></p>\n\n<p><a href=""https://github.com/tensorflow/tensorflow/issues/3971"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/3971</a></p>\n\n<p><a href=""https://stackoverflow.com/questions/46409626/how-to-properly-use-tf-metrics-accuracy"">How to properly use tf.metrics.accuracy?</a></p>\n', 'IsAccepted': True, 'CreationDate': 1519228990}, {'QuestionId': 47191992, 'AnswerId': 47233671, 'URL': 'https://stackoverflow.com/questions/47191992/issue-with-using-tensorflow-metrics-recall-at-thresholds/47233671#47233671', 'QuestionTitle': 'Issue with using tensorflow.metrics.recall_at_thresholds', 'Answer': '<p>Here is a working example:</p>\n\n<pre><code>import tensorflow as tf\na = tf.constant([[1.0, 0.0], [0.0, 1.0], [0.0, 1.0]])\nb = tf.constant([[0.1, 0.9], [0.1, 0.9], [0.3, 0.7]])\nc = tf.metrics.recall_at_thresholds(a, b, [0.5])\nwith tf.Session() as sess:\n  sess.run(tf.local_variables_initializer())\n  print(sess.run(c))\n</code></pre>\n\n<p>The issue is that <code>recall_at_thresholds</code> creates local variables as stated in its <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/recall_at_thresholds"" rel=""nofollow noreferrer"">doc</a>. TensorFlow variables need to be initialized. The <code>sess.run(tf.local_variables_initializer())</code> line does this.</p>\n', 'IsAccepted': True, 'CreationDate': 1510366307}, {'QuestionId': 46440332, 'AnswerId': 46440637, 'URL': 'https://stackoverflow.com/questions/46440332/keras-output-metrics-interpretation/46440637#46440637', 'QuestionTitle': 'Keras output metrics interpretation', 'Answer': '<ol>\n<li><p>Regarding your first question: I respectfully recommend you familiarize yourself with the <a href=""http://www.deeplearningbook.org/contents/mlp.html"" rel=""nofollow noreferrer"">basic mechanics of a neural network</a> or look into one of the many MOOCs i.e. <a href=""http://course.fast.ai/"" rel=""nofollow noreferrer"">this excellent one from fast.ai</a>. This is also beyond the scope of this forum since it´s doesn´t seem to about programming.</p></li>\n<li><p>Your validation accuracy is calculated from the data that you provide by setting the <code>validation_data</code> parameter in your <code>model.fit_generator()</code> function. In your case you have set it to <code>test_batches</code> which is methodically very likely not correct. You need to split your data into three sets: One for training, one for validation (and by that seeing the progress of your training in regard to unseen data and getting useful information for tuning your hyperparameters) and one data set for testing (to evaluate the final score of your model).</p></li>\n</ol>\n\n<p>One more thing: <code>nb_val_samples</code> is not a parameter of <code>fit_generator</code> anymore. See <a href=""https://keras.io/models/sequential/#fit_generator"" rel=""nofollow noreferrer"">documentation here</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1506492715}, {'QuestionId': 46440332, 'AnswerId': 46440981, 'URL': 'https://stackoverflow.com/questions/46440332/keras-output-metrics-interpretation/46440981#46440981', 'QuestionTitle': 'Keras output metrics interpretation', 'Answer': '<p>Loss is the objective function that you are minimizing to train a neural network. The <code>loss</code> value is the mean value of the loss function across batches in the training set. Accuracy (<code>acc</code>) is the mean accuracy across batches, also on the training set. Accuracy is just the fraction of samples in the dataset that the model classified correctly.</p>\n\n<p>But the <code>val</code> metrics are computed on the full validation set, which is the dataset you passed on parameter <code>validation_data</code>. This is done to check for overfitting during training.</p>\n', 'IsAccepted': True, 'CreationDate': 1506494049}, {'QuestionId': 42606207, 'AnswerId': 42607110, 'URL': 'https://stackoverflow.com/questions/42606207/keras-custom-decision-threshold-for-precision-and-recall/42607110#42607110', 'QuestionTitle': 'Keras custom decision threshold for precision and recall', 'Answer': '<p>create custom metrics like this :</p>\n\n<p><strong>Edited thanks to @Marcin</strong> : Create functions that returns the desired metrics with <code>threshold_value</code> as argument</p>\n\n<pre><code>def precision_threshold(threshold=0.5):\n    def precision(y_true, y_pred):\n        """"""Precision metric.\n        Computes the precision over the whole batch using threshold_value.\n        """"""\n        threshold_value = threshold\n        # Adaptation of the ""round()"" used before to get the predictions. Clipping to make sure that the predicted raw values are between 0 and 1.\n        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n        # Compute the number of true positives. Rounding in prevention to make sure we have an integer.\n        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n        # count the predicted positives\n        predicted_positives = K.sum(y_pred)\n        # Get the precision ratio\n        precision_ratio = true_positives / (predicted_positives + K.epsilon())\n        return precision_ratio\n    return precision\n\ndef recall_threshold(threshold = 0.5):\n    def recall(y_true, y_pred):\n        """"""Recall metric.\n        Computes the recall over the whole batch using threshold_value.\n        """"""\n        threshold_value = threshold\n        # Adaptation of the ""round()"" used before to get the predictions. Clipping to make sure that the predicted raw values are between 0 and 1.\n        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n        # Compute the number of true positives. Rounding in prevention to make sure we have an integer.\n        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n        # Compute the number of positive targets.\n        possible_positives = K.sum(K.clip(y_true, 0, 1))\n        recall_ratio = true_positives / (possible_positives + K.epsilon())\n        return recall_ratio\n    return recall\n</code></pre>\n\n<p>now you can use them in</p>\n\n<pre><code>model.compile(..., metrics = [precision_threshold(0.1), precision_threshold(0.2),precision_threshold(0.8), recall_threshold(0.2,...)])\n</code></pre>\n\n<p>I hope this helps :)</p>\n', 'IsAccepted': True, 'CreationDate': 1488709089}, {'QuestionId': 39713694, 'AnswerId': 40396430, 'URL': 'https://stackoverflow.com/questions/39713694/what-does-tensorflow-mean-with-positive-threshold-0-500000-mean/40396430#40396430', 'QuestionTitle': 'What does Tensorflow mean with &#39;positive_threshold_0.500000_mean&#39;?', 'Answer': ""<p>Since you get these statistics, I assume you are doing a binary classification. </p>\n\n<p><code>baseline_target_mean</code> is the mean of the class labels in your data, i.e. in this example, assuming you have class labels 0 and 1, about 22% of your test examples belong to class 1, the remaining examples belong to class 0. Class labels can be arbitrary numbers, so the interpretation depends on your data, what I described is just one possibility because I don't know the data you are using.</p>\n\n<p><code>positive_threshold_0.500000_mean</code> just means that examples for which the prediction is above the threshold of 0.5 are considered positive examples whereas those below 0.5 are negative</p>\n"", 'IsAccepted': True, 'CreationDate': 1478160010}]",{56286350},"['0). </p>\n\n<p>For example, consider the prediction vector as <code>[0, 0.5, 0.3, 0.9]</code> which are actually confidences scores (e.g. probabilities). Now if we apply the threshold value of <code>0.1</code>, we get <code>[0, 1, 1, 1]</code>; or if we apply threshold value of <code>0.6</code> we get <code>[0, 0, 0, 1]</code> (i.e. only the confidence of last prediction is higher than <code>0.6</code>). </p>\n\n<p>Now suppose you want to monitor the changes to specificity at a fixed sensitivity. What <code>SensitivityAtSpecificity</code> metric does is that, to compute the value of sensitivity, it would first compute the specificity at different thresholds and then chooses the threshold which has the closest specificity to the specificity value you have provided (for example, in your question you have given <code>0.4</code> as the specificity value). Then the sensitivity is computed at that threshold and will be returned as the value of this metric. The same thing applies to <code>SpecificityAtSensitivity</code> metric, just swap ""specificity"" and ""sensitivity"" in this paragraph.</p>\n\n<p>You might also ask: what are the threshold values? The answer is if <code>num_thresholds=1</code> then the only threshold is 0.5. If <code>num_thresholds &gt; 1</code> then, besides 0 and 1 as thresholds, the interval (0,1) will be split into <code>num_thresholds - 1</code> equal sub-intervals and the split points are chosen as additional threshold values. For example:</p>\n\n<pre><code>num_threshold  |  thresholds\n=============================\n1              | [0.5]\n2              | [0, 1]\n3              | [0, 0.5, 1]\n4              | [0, 0.33, 0.66, 1]\n5              | [0, 0.25, 0.5, 0.75, 1]\n... </code></pre>\n', '<p>The <code>num_thresholds</code> refers to the number of thresholds. But you might ask: what is a threshold (in this context)? And the answer is that the threshold, which is in the range [0,1], is actually the value which all the predictions greater than that will be considered as positive (i.e. 1) and all the prediction lower than that will be considered as negative (i.e.', '<p>The <code>num_thresholds</code> refers to the number of thresholds. But you might ask: what is a threshold (in this context)? And the answer is that the threshold, which is in the range [0,1], is actually the value which all the predictions greater than that will be considered as positive (i.e. 1) and all the prediction lower than that will be considered as negative (i.e. 0). </p>\n\n<p>For example, consider the prediction vector as <code>[0, 0.5, 0.3, 0.9]</code> which are actually confidences scores (e.g. probabilities). Now if we apply the threshold value of <code>0.1</code>, we get <code>[0, 1, 1, 1]</code>; or if we apply threshold value of <code>0.6</code> we get <code>[0, 0, 0, 1]</code> (i.e. only the confidence of last prediction is higher than <code>0.6</code>).   </p>\n\n<p>Now suppose you want to monitor the changes to specificity at a fixed sensitivity. What <code>SensitivityAtSpecificity</code> metric does is that, to compute the value of sensitivity, it would first compute the specificity at different thresholds and then chooses the threshold which has the closest specificity to the specificity value you have provided (for example, in your question you have given <code>0.4</code> as the specificity value). Then the sensitivity is computed at that threshold and will be returned as the value of this metric. The same thing applies to <code>SpecificityAtSensitivity</code> metric, just swap ""specificity"" and ""sensitivity"" in this paragraph.</p>\n\n<p>You might also ask: what are the threshold values? The answer is if <code>num_thresholds=1</code> then the only threshold is 0.5. If <code>num_thresholds &gt; 1</code> then, besides 0 and 1 as thresholds, the interval (0,1) will be split into <code>num_thresholds - 1</code> equal sub-intervals and the split points are chosen as additional threshold values. For example:</p>\n\n<pre><code>num_threshold  |  thresholds\n=============================\n1              | [0.5]\n2              | [0, 1]\n3              | [0, 0.5, 1]\n4              | [0, 0.33, 0.66, 1]\n5              | [0, 0.25, 0.5, 0.75, 1]\n...\n</code></pre>\n']",{'https://stackoverflow.com/questions/56286350/tf-keras-metrics-specificityatsensitivity-num-thresholds-interpretation/56348713#56348713'},,0.2638158265342419,0.03544800693524904
1,74005009,tf.data.Dataset,Documentation Replicability,How to create output_signature for tensorflow.dataset.from_generator,"<p>I have a generator yielding data and labels <code>yield data, labels</code> where the data is
an <code>numpy.ndarray</code> with variable rows and 500 columns of type <code>dtype=float32</code> and the labels are integers of <code>numpy.int64</code>.</p>
<p>I'm trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: <code>tf.data.Dataset.from_generator</code></p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">docs</a> say that the from_generator function needs a parameter <code>output_signature</code> as an input. But I'm having trouble understanding how to build this output_signature.</p>
<p>How can I make the output_signature for the generator I described?</p>
<p>Thank you!</p>
<p>Edit:
I used <code>tf.type_spec_from_value</code> to get this:</p>
<pre><code>dataset = tf.data.Dataset.from_generator(
   datagen_row,
   output_signature=(
      tf.TensorSpec(shape=(None, 512), dtype=tf.float32, name=None),
      tf.TensorSpec(shape=(), dtype=tf.int64, name=None)
   )
)
</code></pre>
<p>But is it correct to use None when the number of rows is varying for the first data type?</p>
","<p>if your datagen_row() function yields input_data, label with format 500 and 1
than your output_signature should be:</p>
<pre><code>  output_signature=(
  tf.TensorSpec(shape=(None, 500), dtype=tf.float32, name=None),
  tf.TensorSpec(shape=(), dtype=tf.int64, name=None))
</code></pre>
<p>where the first TensorSpec is for the data format and the second one for the label format.
But it would be helpful if you post the function + maybe data examples or data shape here. Otherwise it is hard to help.</p>
","{63345896, 66947465, 46870058, 52443273, 51655339, 49691315, 53910012, 59142040, 57432696, 63247003, 51630204}","[{'QuestionId': 51630204, 'AnswerId': 51630551, 'URL': 'https://stackoverflow.com/questions/51630204/how-to-make-tf-data-dataset-from-generator-yield-batches-with-a-custom-generator/51630551#51630551', 'QuestionTitle': 'How to make tf.data.Dataset.from_generator yield batches with a custom generator', 'Answer': '<p>The <code>Dataset</code> construction process in that example is ill-formed. It should be done in this order, as also established by the official guide on <a href=""https://www.tensorflow.org/guide/datasets"" rel=""nofollow noreferrer"">Importing Data</a>:</p>\n<ol>\n<li>A base dataset creation function or static method should be called for establishing the original <strong>source</strong> of data (e.g. the static methods <code>from_slice_tensors</code>, <code>from_generator</code>, <code>list_files</code>, ...).</li>\n<li>At this point, <strong>transformations</strong> may be applied by chaining adapter methods (such as <code>batch</code>).</li>\n</ol>\n<p>Thus, in TensorFlow 2.9:</p>\n<pre class=""lang-py prettyprint-override""><code>dataset = tf.data.Dataset.from_generator(\n     generator,\n     output_signature=(\n         tf.TensorSpec(shape=(width, height, channels, frames), dtype=tf.float32),\n         tf.TensorSpec(shape=(width, height, channels, frames), dtype=tf.float32)\n    ).batch(batch_size)\n</code></pre>\n<p>In TensorFlow 1:</p>\n<pre class=""lang-py prettyprint-override""><code>dataset = tf.data.Dataset.from_generator(custom_generator).batch(batch_size)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1533117872}, {'QuestionId': 57432696, 'AnswerId': 69170847, 'URL': 'https://stackoverflow.com/questions/57432696/passing-multiple-arguments-to-generator-in-tf-data-datasets-from-generator/69170847#69170847', 'QuestionTitle': 'Passing Multiple Arguments to Generator in tf.data.Datasets.from_generator', 'Answer': ""<p>The solution using lambda worked in part.\nBut in my current environment(TensorFlow2.6.0), I have observed that the values generated by lambda won't reset between epochs.</p>\n<p>As a workaround, it is possible to make generators have double layers as shown below.</p>\n<pre><code>def new_gen():\n    gen_mod = gen.flow_from_directory(directory=data_path, target_size=(\n        160, 128), class_mode=None, batch_size=32, shuffle=True)\n\n    for x in gen_mod:\n        yield x\n\nreal_imgs_dataset = tf.data.Dataset.from_generator(\n    new_gen,\n    output_types=tf.float32,\n    output_shapes=([None, 160, 128, 3])\n)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1631583578}, {'QuestionId': 59142040, 'AnswerId': 60450253, 'URL': 'https://stackoverflow.com/questions/59142040/tensorflow-2-0-how-to-change-the-output-signature-while-using-tf-saved-model/60450253#60450253', 'QuestionTitle': 'Tensorflow 2.0: How to change the output signature while using tf.saved_model', 'Answer': '<p>I figured out a way to define the output signature without using tf.Module by defining a <code>tf.function</code> that returns a dictionary of outputs where the keys used in the dictionary will be the output names.</p>\n<pre class=""lang-py prettyprint-override""><code># Create the model\nmodel = ...\n\n# Train the model\nmodel.fit(...)\n\n# Define where to save the model\nexport_path = &quot;...&quot;\n\n@tf.function()\ndef my_predict(my_prediction_inputs):\n   inputs = {\n        \'my_serving_input\': my_prediction_inputs,\n   }\n   prediction = model(inputs)\n   return {&quot;my_prediction_outputs&quot;: prediction}\n\nmy_signatures = my_predict.get_concrete_function(\n   my_prediction_inputs=tf.TensorSpec([None,None], dtype=tf.dtypes.float32, name=&quot;my_prediction_inputs&quot;)\n)\n\n# Save the model.\ntf.saved_model.save(\n    model,\n    export_dir=export_path,\n    signatures=my_signatures\n)\n\n</code></pre>\n<p>This produces the following signature:</p>\n<pre><code>signature_def[\'serving_default\']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs[\'my_prediction_inputs\'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, -1)\n        name: serving_default_my_prediction_inputs:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs[\'my_prediction_outputs\'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: StatefulPartitionedCall:0\n  Method name is: tensorflow/serving/predict\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1582887199}, {'QuestionId': 66947465, 'AnswerId': 66947974, 'URL': 'https://stackoverflow.com/questions/66947465/tensorflow-dataset-how-to-get-the-shape-of-the-generator-of-data/66947974#66947974', 'QuestionTitle': 'Tensorflow dataset how to get the shape of the generator of data?', 'Answer': ""<p>Your variable <code>ds_info</code> has that information:</p>\n<pre><code>height, width, channels = ds_info.features['image'].shape\n</code></pre>\n<p>Look at it like this:</p>\n<pre><code>ds_info.features['image']\n</code></pre>\n<pre><code>Image(shape=(28, 28, 1), dtype=tf.uint8)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1617594043}, {'QuestionId': 66947465, 'AnswerId': 66947609, 'URL': 'https://stackoverflow.com/questions/66947465/tensorflow-dataset-how-to-get-the-shape-of-the-generator-of-data/66947609#66947609', 'QuestionTitle': 'Tensorflow dataset how to get the shape of the generator of data?', 'Answer': '<p>One can use <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#attributes_1"" rel=""nofollow noreferrer""><code>dataset.element_spec</code></a>:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow_datasets as tfds\n\n(ds_train, ds_test), ds_info = tfds.load(\n    &quot;mnist&quot;,\n    split=[&quot;train&quot;, &quot;test&quot;],\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True,\n)\n\nds_train.element_spec\n# (TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None),\n#  TensorSpec(shape=(), dtype=tf.int64, name=None))\n\nds_train.element_spec[0].shape\n# TensorShape([28, 28, 1])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1617589718}, {'QuestionId': 63247003, 'AnswerId': 66314338, 'URL': 'https://stackoverflow.com/questions/63247003/tensorflow-from-generator-giving-error-generator-yielded-an-element-that-co/66314338#66314338', 'QuestionTitle': 'tensorflow from_generator() giving error- &#39;generator` yielded an element that could not be converted to the expected type', 'Answer': '<p>I had a similar problem, which I believe I solved by specifying the shape that the generator generated.\nFor me, the output was 2 images 32x32x3,as well as two labels so i used this, as an argument to the from_generator method.</p>\n<pre><code>dataset = tf.data.Dataset.from_generator\n    generator,\n# this is the part that you care about #\n    output_signature=(\n      tf.TensorSpec(shape=(2,32,32,3), dtype=tf.int32),\n      tf.TensorSpec(shape=(2,1), dtype=tf.int32,))\n#\n    )\n\n</code></pre>\n<p>I tried to run your code but train_list is not defined.\nStill I believe that this could work, maybe with a change or two</p>\n<pre><code>\noutput_signature = (\n# the one is there to clarify that there is one of these objects\n      tf.TensorSpec(shape=(1,16,100,223,3),  dtype=tf.int32)\n      tf.TensorSpec(shape=(1,100,223), dtype=tf.int32)\n      tf.TensorSpec(shape=(1,16,) ,dtype=tf.int32)\n      tf.TensorSpec(shape=(2,1),dtype=tf.int32)\n)\n\n</code></pre>\n<p>Here is a link with a toy dataset that I made in order to make sense out of this mess</p>\n<p><a href=""https://colab.research.google.com/drive/17pu6jJYLGP-I1nJnzigOx2YOh3Ih4cvG?usp=sharing"" rel=""noreferrer"">https://colab.research.google.com/drive/17pu6jJYLGP-I1nJnzigOx2YOh3Ih4cvG?usp=sharing</a></p>\n', 'IsAccepted': False, 'CreationDate': 1613991260}, {'QuestionId': 52443273, 'AnswerId': 65969780, 'URL': 'https://stackoverflow.com/questions/52443273/how-do-you-send-arguments-to-a-generator-function-using-tf-data-dataset-from-gen/65969780#65969780', 'QuestionTitle': 'How do you send arguments to a generator function using tf.data.Dataset.from_generator()?', 'Answer': '<p>For Tensorflow 2.4:</p>\n<pre><code>training_dataset = tf.data.Dataset.from_generator(\n     raw_data_gen, \n     args=(1), \n     output_types=(tf.float32, tf.uint8), \n     output_shapes=([None, 1], [None]))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1612020721}, {'QuestionId': 63345896, 'AnswerId': 63478337, 'URL': 'https://stackoverflow.com/questions/63345896/dataset-from-generator-typeerror-generator-must-be-callable/63478337#63478337', 'QuestionTitle': 'Dataset.from_generator: TypeError: `generator` must be callable', 'Answer': '<p>I recently encountered a similar problem, but I\'m a beginner so not sure if this will help.</p>\n<p>Try add a <strong>call</strong> function in your class.</p>\n<p>Below are the original class which raise <code>TypeError: `generator` must be callable.</code></p>\n<pre class=""lang-py prettyprint-override""><code>class DataGen:\n  def __init__(self, files, data_path):\n    self.i = 0\n    self.files=files\n    self.data_path=data_path\n  \n  def __load__(self, files_name):\n    data_path = os.path.join(self.data_path, files_name)\n    arr_img, arr_mask = load_patch(data_path)\n    return arr_img, arr_mask\n\n  def getitem(self, index):\n    _img, _mask = self.__load__(self.files[index])\n    return _img, _mask\n\n  def __iter__(self):\n    return self\n\n  def __next__(self):\n    if self.i &lt; len(self.files):\n      img_arr, mask_arr = self.getitem(self.i)\n      self.i += 1\n    else:\n      raise StopIteration()\n    return img_arr, mask_arr\n</code></pre>\n<p>Then I revised the code as below and it worked for me.</p>\n<pre class=""lang-py prettyprint-override""><code>class DataGen:\n  def __init__(self, files, data_path):\n    self.i = 0\n    self.files=files\n    self.data_path=data_path\n  \n  def __load__(self, files_name):\n    data_path = os.path.join(self.data_path, files_name)\n    arr_img, arr_mask = load_patch(data_path)\n    return arr_img, arr_mask\n\n  def getitem(self, index):\n    _img, _mask = self.__load__(self.files[index])\n    return _img, _mask\n\n  def __iter__(self):\n    return self\n\n  def __next__(self):\n    if self.i &lt; len(self.files):\n      img_arr, mask_arr = self.getitem(self.i)\n      self.i += 1\n    else:\n      raise StopIteration()\n    return img_arr, mask_arr\n  \n  def __call__(self):\n    self.i = 0\n    return self\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1597796143}, {'QuestionId': 59142040, 'AnswerId': 61616317, 'URL': 'https://stackoverflow.com/questions/59142040/tensorflow-2-0-how-to-change-the-output-signature-while-using-tf-saved-model/61616317#61616317', 'QuestionTitle': 'Tensorflow 2.0: How to change the output signature while using tf.saved_model', 'Answer': '<p>Another way of creating the serving_default signature is:</p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text\n\nexport_dir = ""./models/use/00000001""\nmodule = hub.load(""https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3"")\n\n@tf.function\ndef my_module_encoder(text):\n   inputs = {\n        \'text\': text,\n   }\n   outputs = {\n        \'embeddings\': module(text)\n   }\n   return outputs\n\ntf.saved_model.save(\n    module, \n    export_dir, \n    signatures=my_module_encoder.get_concrete_function(\n        text=tf.TensorSpec(shape=None, dtype=tf.string)\n    ), \n    options=None\n)\n</code></pre>\n\n<p>You can look at the created <code>SignatureDefs</code> signature using <code>saved_model_cli</code> command as below:</p>\n\n<pre><code>$ saved_model_cli show --all  --dir models/use/00000001\n\nMetaGraphDef with tag-set: \'serve\' contains the following SignatureDefs:\n\nsignature_def[\'__saved_model_init_op\']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs[\'__saved_model_init_op\'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is:\n\nsignature_def[\'serving_default\']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs[\'text\'] tensor_info:\n        dtype: DT_STRING\n        shape: unknown_rank\n        name: serving_default_text:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs[\'embeddings\'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 512)\n        name: StatefulPartitionedCall:0\n  Method name is: tensorflow/serving/predict\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1588691082}, {'QuestionId': 51655339, 'AnswerId': 60110780, 'URL': 'https://stackoverflow.com/questions/51655339/parametrized-generators-while-using-tf-data-dataset-from-generator/60110780#60110780', 'QuestionTitle': 'Parametrized generators while using tf.data.Dataset.from_generator()', 'Answer': '<p>TensorFlow <code>Dataset</code> already supports parametrizing the generator through the argument <code>args</code> which is simply passed to your generator (<a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#args_35"" rel=""nofollow noreferrer"">see docs</a>). Here is a minimal working example tested on TensorFlow <code>2.0.0</code>. </p>\n\n<pre><code>import tensorflow as tf\n\nx_train = [i for i in range(0, 20, 2)]  # even\nx_val = [i for i in range(1, 20, 2)]  # odd\ny_train = [i**2 for i in x_train]  # squared\ny_val = [i**2 for i in x_val]\n\ndef gen_data_epoch(test=False):  # parametrized generator\n    train_data = x_val if test else x_train\n    label_data = y_val if test else y_train\n    n_tests = len(train_data)\n    for test_idx in range(len(train_data)):\n        yield train_data[test_idx], label_data[test_idx]\n\ndef get_dataset(test=False):\n    return tf.data.Dataset.from_generator(\n        gen_data_epoch, args=(test,),\n        output_types=(tf.int32, tf.int32))\n\nprint(""Train:"", [(i[0].numpy(), i[1].numpy()) for i in get_dataset().take(5)])\nprint(""Test: "", [(i[0].numpy(), i[1].numpy()) for i in get_dataset(test=True).take(5)])\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1581068466}, {'QuestionId': 57432696, 'AnswerId': 58295669, 'URL': 'https://stackoverflow.com/questions/57432696/passing-multiple-arguments-to-generator-in-tf-data-datasets-from-generator/58295669#58295669', 'QuestionTitle': 'Passing Multiple Arguments to Generator in tf.data.Datasets.from_generator', 'Answer': '<p>I had a similar issue that I reported there: <a href=""https://github.com/tensorflow/tensorflow/issues/33133"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/33133</a></p>\n\n<p>The proposed solution adapted to your case is:</p>\n\n<pre class=""lang-py prettyprint-override""><code>gen_mod = gen.flow_from_directory(directory=data_path, target_size=(\n    160, 128), class_mode=None, batch_size=32, shuffle=True)\n\nreal_imgs_dataset = tf.data.Dataset.from_generator(\n    lambda: gen_mod,\n    output_types=tf.float32,\n    output_shapes=([None, 160, 128, 3])\n)\n</code></pre>\n\n<p>Hope it helped! </p>\n', 'IsAccepted': True, 'CreationDate': 1570582124}, {'QuestionId': 49691315, 'AnswerId': 56641853, 'URL': 'https://stackoverflow.com/questions/49691315/invalidargumenterror-when-using-of-tensorflow-data-dataset-from-generator/56641853#56641853', 'QuestionTitle': 'InvalidArgumentError when using of tensorflow.data.Dataset.from_generator', 'Answer': '<p>In short, the reason is that from_generator can flatten a NumPy array but not a Tensor.</p>\n\n<p>Here is a shorter code that will reproduce the error:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nprint(tf.__version__)\ndef g():\n  img = tf.random_uniform([3])\n  # img = np.random.rand(3)\n  # img = tf.convert_to_tensor(img)\n  yield img\n\ndataset = tf.data.Dataset.from_generator(g, tf.float64, tf.TensorShape([3]))\niterator = dataset.make_one_shot_iterator()\nnext_iterator = iterator.get_next()\n\nsess = tf.Session()\nsess.run(next_iterator)\n</code></pre>\n\n<p>The error message in version 1.14 is very helpful. (The exact line of code will change due to different versions, but I have checked 1.12 and 1.13 I am using the cause is the same.)</p>\n\n<pre><code>InvalidArgumentError: TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was float64, but the yielded element was Tensor(""random_uniform:0"", shape=(3,), dtype=float32).\nTraceback (most recent call last):\n\n  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 530, in generator_py_func\n    ret, dtype=dtype.as_numpy_dtype))\n\n  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 169, in _convert\n    result = np.asarray(value, dtype=dtype, order=""C"")\n\n  File ""/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py"", line 538, in asarray\n    return array(a, dtype, copy=False, order=order)\n\nValueError: setting an array element with a sequence. \n</code></pre>\n\n<p>When the generated element is a Tensor, from_generator will flatten it to <code>output_types</code>. The convert function does not work.</p>\n\n<p>To solve this problem, just dont use <code>from_generator</code> when your generator generates a tensor. You can use <code>from_tensors</code> or <code>from_tensor_slices</code>.</p>\n\n<pre><code>img = tf.random_uniform([3])\n\ndataset = tf.data.Dataset.from_tensors(img).repeat()\niterator = dataset.make_initializable_iterator()\nnext_iterator = iterator.get_next()\n\nsess = tf.Session()\nsess.run(iterator.initializer)\nsess.run(next_iterator)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1560832704}, {'QuestionId': 53910012, 'AnswerId': 55832498, 'URL': 'https://stackoverflow.com/questions/53910012/tensorflow-dataset-from-generator-using-an-generator-that-yield-tensors/55832498#55832498', 'QuestionTitle': 'tensorflow Dataset.from_generator using an generator that yield tensors', 'Answer': '<p>I am able to work around this problem with the following trick. Its efficiency is OK.</p>\n\n<pre><code>tf.data.Dataset.from_tensors(0).repeat().map(lambda _: dataset_generator())\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1556116089}, {'QuestionId': 52443273, 'AnswerId': 52443952, 'URL': 'https://stackoverflow.com/questions/52443273/how-do-you-send-arguments-to-a-generator-function-using-tf-data-dataset-from-gen/52443952#52443952', 'QuestionTitle': 'How do you send arguments to a generator function using tf.data.Dataset.from_generator()?', 'Answer': ""<p>You need to define a new function based on <code>raw_data_gen</code> that doesn't take any arguments. You can use the <code>lambda</code> keyword to do this.</p>\n\n<pre><code>training_dataset = tf.data.Dataset.from_generator(lambda: raw_data_gen(train_val_or_test=1), (tf.float32, tf.uint8), ([None, 1], [None]))\n...\n</code></pre>\n\n<p>Now, we are passing a function to <code>from_generator</code> that doesn't take any arguments, but that will simply act as <code>raw_data_gen</code> with the argument set to 1. You can use the same scheme for the validation and test sets, passing 2 and 3 respectively.</p>\n"", 'IsAccepted': True, 'CreationDate': 1537533385}, {'QuestionId': 51655339, 'AnswerId': 51655340, 'URL': 'https://stackoverflow.com/questions/51655339/parametrized-generators-while-using-tf-data-dataset-from-generator/51655340#51655340', 'QuestionTitle': 'Parametrized generators while using tf.data.Dataset.from_generator()', 'Answer': '<p>I found a solution based on a functional programming concept called <strong>Partially Applied Functions</strong>. In summary:</p>\n\n<blockquote>\n  <p>a PAF is a function that takes a function with multiple parameters and returns a function with fewer parameters.</p>\n</blockquote>\n\n<p>The way I did it is the following:</p>\n\n<pre><code>from functools import partial\nimport tensorflow as tf\n\ndef generator(lo, hi):\n    for i in range(lo, hi):\n        yield float(i)\n\ndef get_generator(lo, hi):\n    return partial(generator, lo, hi)\n\ntf.data.Dataset(get_generator(lo, hi), tf.float64)\n</code></pre>\n\n<p>The <code>get_generator(lo, hi)</code> function returns a partially applied function for the generator which fixes the values for the <code>lo</code> and <code>hi</code> parameters, which is in fact the parameterless generator required by <code>tf.data.Dataset.from_generator()</code>. </p>\n', 'IsAccepted': True, 'CreationDate': 1533217798}, {'QuestionId': 49691315, 'AnswerId': 49950137, 'URL': 'https://stackoverflow.com/questions/49691315/invalidargumenterror-when-using-of-tensorflow-data-dataset-from-generator/49950137#49950137', 'QuestionTitle': 'InvalidArgumentError when using of tensorflow.data.Dataset.from_generator', 'Answer': '<p>Did you figure this out?</p>\n\n<p>I was running into the exact same type of issue and my issue was a dimension mismatch in between the generator and what I was feeding into output_shapes.</p>\n\n<p>Also looking at you code I believe you have to be feeding valid data in, such as numpy arrays, not TensorFlow constants.</p>\n', 'IsAccepted': False, 'CreationDate': 1524260632}, {'QuestionId': 46870058, 'AnswerId': 46893250, 'URL': 'https://stackoverflow.com/questions/46870058/calling-tensorflows-dataset-from-generator-method/46893250#46893250', 'QuestionTitle': 'Calling TensorFlow&#39;s Dataset.from_generator method', 'Answer': '<p>This is a known issue with TensorFlow 1.4.0<strong>rc0</strong> when running on Windows. The bug has been fixed in the nightly build of TensorFlow and <a href=""https://github.com/tensorflow/tensorflow/commit/cc5268be7d251e5116229f83aacab80ae6dd917f"" rel=""nofollow noreferrer"">cherry-picked</a> into the next release candidate of TensorFlow 1.4.0, which is <a href=""https://pypi.python.org/pypi/tensorflow/1.4.0rc1"" rel=""nofollow noreferrer"">now available</a>.</p>\n\n<p>In the meantime, there are a few options:</p>\n\n<ol>\n<li><p>Install the nightly build, using <code>pip install tf-nightly</code>. Note that this will contain some features not available in the 1.4 branch, and has not been subject to as much testing as the release branch.</p></li>\n<li><p>Build the 1.4 branch from source.</p></li>\n<li><p>Wait for the 1.4.0rc1 release candidate to be published. <strong>EDIT:</strong> This release is now available from PyPI using <code>pip install tensorflow==1.4.0rc1</code>.</p></li>\n</ol>\n', 'IsAccepted': True, 'CreationDate': 1508773039}]","{74005009, 69851165}","['<p>if your datagen_row() function yields input_data, label with format 500 and 1\nthan your output_signature should be:</p>\n<pre><code>  output_signature=(\n  tf.TensorSpec(shape=(None, 500), dtype=tf.float32, name=None),\n  tf.TensorSpec(shape=(), dtype=tf.int64, name=None))\n</code></pre>\n<p>where the first TensorSpec is for the data format and the second one for the label format.\nBut it would be helpful if you post the function + maybe data examples or data shape here. Otherwise it is hard to help.</p>\n', '<p>if your datagen_row() function yields input_data, label with format 500 and 1\nthan your output_signature should be:</p>\n<pre><code>  output_signature=(\n  tf.TensorSpec(shape=(None, 500), dtype=tf.float32, name=None),\n  tf.TensorSpec(shape=(), dtype=tf.int64, name=None))\n</code></pre>\n<p>where the first TensorSpec is for the data format and the second one for the label format. But it would be helpful if you post the function + maybe data examples or data shape here.', '<p>You could try something like the following; however, I am not sure how efficient it is:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\nclass generator:\n    def __init__(self, n=7):\n        self.n = n\n\n    def __call__(self):\n        for i in range(self.n):\n            yield (i, 10*i)\n\ndataset = tf.data.Dataset.from_generator(generator(), \n    output_signature=(tf.TensorSpec(shape=(), dtype=tf.int32), tf.TensorSpec(shape=(), dtype=tf.int32)))\n\nwindow_size = 5\nwindows = dataset.window(window_size, shift=1)\n\ndef stack(x, y):\n  x = tf.expand_dims(x, axis=1)\n  y = tf.expand_dims(y, axis=1)\n  result = tf.concat((x, y), axis=1)\n  ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n  for w in tf.range(3):\n    ta = ta.write(w, result[w: w + 3])\n  return ta.stack()\n\ndef sub_to_batch(sub1, sub2):\n    sub1 = sub1.batch(5, drop_remainder=True)\n    sub2 = sub2.batch(5, drop_remainder=True)\n\n    return tf.data.Dataset.zip((sub1, sub2)).map(stack)\n\nfinal_dset = windows.flat_map(sub_to_batch)\nfor s in final_dset.take(1):\n  print(s)\n</code></pre>\n<pre><code>tf.Tensor(\n[[[ 0  0]\n  [ 1 10]\n  [ 2 20]]\n\n [[ 1 10]\n  [ 2 20]\n  [ 3 30]]\n\n [[ 2 20]\n  [ 3 30]\n  [ 4 40]]], shape=(3, 3, 2), dtype=int32)\n</code></pre>\n<p>You could also hard-code the indices if you want to and the results will be the same:</p>\n<pre class=""lang-py prettyprint-override""><code>def stack(x, y):\n  x = tf.expand_dims(x, axis=1)\n  y = tf.expand_dims(y, axis=1)\n  result = tf.concat((x, y), axis=1)\n  return tf.stack([result[0: 3], result[1: 4], result[2: 5]])\n</code></pre>\n']","{'https://stackoverflow.com/questions/69851165/how-to-create-a-multivariate-timeseries-dataset-with-tf-data/69852786#69852786', 'https://stackoverflow.com/questions/74005009/how-to-create-output-signature-for-tensorflow-dataset-from-generator/74224171#74224171'}",,0.22104186353500332,0.07315698143478377
1,51858970,tf.gradients,Documentation Ambiguity,"tf.gradients() sums over ys, does it?","<p><a href=""https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients"" rel=""noreferrer"">https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients</a></p>

<p>In the documentation for tf.gradients(ys, xs) it states that </p>

<blockquote>
  <p>Constructs symbolic derivatives of sum of ys w.r.t. x in xs</p>
</blockquote>

<p>I am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example:</p>

<pre><code>x_dims = 3
batch_size = 4

x = tf.placeholder(tf.float32, (None, x_dims))

y = 2*(x**2)

grads = tf.gradients(y,x)

sess = tf.Session()

x_val = np.random.randint(0, 10, (batch_size, x_dims))
y_val, grads_val = sess.run([y, grads], {x:x_val})

print('x = \n', x_val)
print('y = \n', y_val)
print('dy/dx = \n', grads_val[0])
</code></pre>

<p>This gives the following output:</p>

<pre><code>x = 
 [[5 3 7]
 [2 2 5]
 [7 5 0]
 [3 7 6]]
y = 
 [[50. 18. 98.]
 [ 8.  8. 50.]
 [98. 50.  0.]
 [18. 98. 72.]]
dy/dx = 
 [[20. 12. 28.]
 [ 8.  8. 20.]
 [28. 20.  0.]
 [12. 28. 24.]]
</code></pre>

<p>This is the output I would expect, simply the derivative dy/dx for every element in the batch. I don't see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: <a href=""https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html"" rel=""noreferrer"">https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html</a>). Why is this necessary?</p>

<p>I am using Tensorflow 1.6 and Python 3.</p>
","<p>If y and x have the same shape then the sum over the dy/dx is the sum over exactly one value. However, if you have more than one y for each x, then the gradients are summed.</p>

<pre><code>import numpy as np
import tensorflow as tf

x_dims = 3
batch_size = 4

x = tf.placeholder(tf.float32, (None, x_dims))
y = 2*(x**2)
z = tf.stack([y, y]) # There are twice as many z's as x's

dy_dx = tf.gradients(y,x)
dz_dx = tf.gradients(z,x)

sess = tf.Session()

x_val = np.random.randint(0, 10, (batch_size, x_dims))
y_val, z_val, dy_dx_val, dz_dx_val = sess.run([y, z, dy_dx, dz_dx], {x:x_val})

print('x.shape =', x_val.shape)
print('x = \n', x_val)
print('y.shape = ', y_val.shape)
print('y = \n', y_val)
print('z.shape = ', z_val.shape)
print('z = \n', z_val)
print('dy/dx = \n', dy_dx_val[0])
print('dz/dx = \n', dz_dx_val[0])
</code></pre>

<p>Produces the following output:</p>

<pre><code>x.shape = (4, 3)
x = 
 [[1 4 8]
 [0 2 8]
 [2 8 1]
 [4 5 2]]

y.shape =  (4, 3)
y = 
 [[  2.  32. 128.]
 [  0.   8. 128.]
 [  8. 128.   2.]
 [ 32.  50.   8.]]

z.shape =  (2, 4, 3)
z = 
 [[[  2.  32. 128.]
  [  0.   8. 128.]
  [  8. 128.   2.]
  [ 32.  50.   8.]]

 [[  2.  32. 128.]
  [  0.   8. 128.]
  [  8. 128.   2.]
  [ 32.  50.   8.]]]

dy/dx = 
 [[ 4. 16. 32.]
 [ 0.  8. 32.]
 [ 8. 32.  4.]
 [16. 20.  8.]]
dz/dx = 
 [[ 8. 32. 64.]
 [ 0. 16. 64.]
 [16. 64.  8.]
 [32. 40. 16.]]
</code></pre>

<p>In particular, notice that the values of dz/dx are twice those of dy/dz since they are summed over the inputs to the stack.</p>
","{39993377, 64086949, 45151015, 55400041, 43022091, 63065452, 48767565, 43934225, 49603346, 59893850}","[{'QuestionId': 43022091, 'AnswerId': 70753526, 'URL': 'https://stackoverflow.com/questions/43022091/where-in-tensorflow-gradients-is-the-sum-over-the-elements-of-y-made/70753526#70753526', 'QuestionTitle': 'Where in tensorflow gradients is the sum over the elements of y made?', 'Answer': '<p>With the jacobian function in Tensorflow 2, this is a straightforward task.</p>\n<pre><code>with tf.GradientTape() as tape1:\n    with tf.GradientTape() as tape2:\n        y = layer(x)\n        loss = tf.reduce_mean(y ** 2)\n    first_order_gradient = tape2.gradient(loss, layer.trainable_weights)\nhessian = tape1.jacobian(first_order_gradient, layer.trainable_weights)\n</code></pre>\n<p><a href=""https://www.tensorflow.org/guide/advanced_autodiff#hessian"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/advanced_autodiff#hessian</a></p>\n', 'IsAccepted': False, 'CreationDate': 1642499553}, {'QuestionId': 39993377, 'AnswerId': 70077757, 'URL': 'https://stackoverflow.com/questions/39993377/tensorflow-gradients-without-automatic-implicit-sum/70077757#70077757', 'QuestionTitle': 'Tensorflow gradients: without automatic implicit sum', 'Answer': '<p>for future readers:</p>\n<p>Tensorflow has made some advancements, and as for tf2.7 (and maybe even earlier versions) <strong>the answer to this questions is to use tf.GradientTape.jacobian</strong></p>\n<p><a href=""https://www.tensorflow.org/guide/advanced_autodiff#jacobians"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/advanced_autodiff#jacobians</a></p>\n', 'IsAccepted': False, 'CreationDate': 1637657023}, {'QuestionId': 64086949, 'AnswerId': 64087468, 'URL': 'https://stackoverflow.com/questions/64086949/is-the-gradient-of-the-sum-equal-to-the-sum-of-the-gradients-for-a-neural-networ/64087468#64087468', 'QuestionTitle': 'Is the gradient of the sum equal to the sum of the gradients for a neural network in pytorch?', 'Answer': '<p>I think it should, this is just a toy example</p>\n<pre class=""lang-py prettyprint-override""><code>w = torch.tensor([2.], requires_grad=True)\nx1 = torch.tensor([3.], requires_grad=True)\nx2 = torch.tensor([4.], requires_grad=True)\ny = w * a + w * b\ny.backward() # calculate gradient\n</code></pre>\n<p>return</p>\n<pre><code>&gt;&gt;&gt; w.grad\ntensor([7.])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1601202859}, {'QuestionId': 63065452, 'AnswerId': 63066420, 'URL': 'https://stackoverflow.com/questions/63065452/how-does-tensorflow-gradienttape-accumulate-operation-within-a-loop/63066420#63066420', 'QuestionTitle': 'How does tensorflow.GradientTape() accumulate operation within a loop?', 'Answer': '<pre><code>def mean(lst):\n  return tf.reduce_mean(lst)\n  \nxs = tf.constant([2.0, 3.0, 4.0])\nfor x in xs:\n  with tf.GradientTape() as g:\n    y = x * x\n  print(y, mean(y))\n</code></pre>\n<p>output:</p>\n<pre><code>tf.Tensor(4.0, shape=(), dtype=float32) tf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(9.0, shape=(), dtype=float32)\ntf.Tensor(16.0, shape=(), dtype=float32) tf.Tensor(16.0, shape=(), dtype=float32)\n</code></pre>\n<p>As i have used reduce_mean() it only calculates mean of the tensor at that instance but, tf.keras.metrics.mean() accumulates every time its compiled. Hope, this cleared out your doubt.</p>\n<pre><code>mean = tf.keras.metrics.Mean()\nxs = tf.constant([2.0, 3.0, 4.0])\nfor i in range(0,2):\n  for x in xs:\n    with tf.GradientTape() as g:\n      y = x * x\n    print(y, mean(y))\n</code></pre>\n<p>output:</p>\n<pre><code>tf.Tensor(4.0, shape=(), dtype=float32) tf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(6.5, shape=(), dtype=float32)\ntf.Tensor(16.0, shape=(), dtype=float32) tf.Tensor(9.666667, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32) tf.Tensor(8.25, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(8.4, shape=(), dtype=float32)\ntf.Tensor(16.0, shape=(), dtype=float32) tf.Tensor(9.666667, shape=(), dtype=float32)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1595561881}, {'QuestionId': 63065452, 'AnswerId': 63065925, 'URL': 'https://stackoverflow.com/questions/63065452/how-does-tensorflow-gradienttape-accumulate-operation-within-a-loop/63065925#63065925', 'QuestionTitle': 'How does tensorflow.GradientTape() accumulate operation within a loop?', 'Answer': ""<pre><code>def get_gradient(x):\n   with tf.GradientTape() as tape:\n      y = tf.nn.sigmoid(x)\n   gradient = tape.gradient(y, x)\n   return gradient, y\n        \ntest = tf.Variable([0.5, 0.6, 0.7])\ngradient, y = get_gradient(test)\n</code></pre>\n<p>Let's consider a sigmoid activation function. A sigmoid activation function outputs values from 0 to 1.</p>\n<p>g(x) = 1/(1 + e^(-x)) = y</p>\n<p>g'(x) = g(x)(1 - g(x))</p>\n<p>Example: Let, x = 0.5</p>\n<p>g(0.5) = 1/(1 + e^(-0.5)) = 0.62245</p>\n<p>g'(0.5) = g(x)(1 - g(x)) = 0.2350</p>\n<p>########################################################################</p>\n<p><strong>with tf.GradientTape() as tape:</strong> within this you defined your function you wanted to differentiate. Here, it is y = sigmoid(x). Tape will record all the operations on x.</p>\n<p><strong>gradient = tape.gradient(y, x)</strong> y = f(x) is a function with y' = f'(x). Input parameters are tensor of y (solved at given x) and x. It returns dy/dx as a tensor.</p>\n"", 'IsAccepted': False, 'CreationDate': 1595558288}, {'QuestionId': 59893850, 'AnswerId': 62683800, 'URL': 'https://stackoverflow.com/questions/59893850/how-to-accumulate-gradients-in-tensorflow-2-0/62683800#62683800', 'QuestionTitle': 'How to accumulate gradients in tensorflow 2.0?', 'Answer': '<p>If I understand correctly from this statement:</p>\n<blockquote>\n<p>How can I accumulate the losses/gradients and then apply a single optimizer step?</p>\n</blockquote>\n<p>@Nagabhushan is trying to accumulate gradients and then apply the optimization on the (mean) accumulated gradient. The answer provided by @TensorflowSupport does not answers it.\nIn order to perform the optimization only once, and accumulate the gradient from several tapes, you can do the following:</p>\n<pre><code>for i in range(num_epochs):\n    print(f\'Epoch: {i + 1}\')\n    total_loss = 0\n\n    # get trainable variables\n    train_vars = self.model.trainable_variables\n    # Create empty gradient list (not a tf.Variable list)\n    accum_gradient = [tf.zeros_like(this_var) for this_var in train_vars]\n\n    for j in tqdm(range(num_samples)):\n        sample = samples[j]\n        with tf.GradientTape as tape:\n            prediction = self.model(sample)\n            loss_value = self.loss_function(y_true=labels[j], y_pred=prediction)\n        total_loss += loss_value\n\n        # get gradients of this tape\n        gradients = tape.gradient(loss_value, train_vars)\n        # Accumulate the gradients\n        accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n\n\n    # Now, after executing all the tapes you needed, we apply the optimization step\n    # (but first we take the average of the gradients)\n    accum_gradient = [this_grad/num_samples for this_grad in accum_gradient]\n    # apply optimization step\n    self.optimizer.apply_gradients(zip(accum_gradient,train_vars))\n        \n\n    epoch_loss = total_loss / num_samples\n    print(f\'Epoch loss: {epoch_loss}\')\n</code></pre>\n<p>Using tf.Variable() should be avoided inside the training loop, since it will produce errors when trying to execute the code as a graph. If you use tf.Variable() inside your training function and then decorate it with &quot;@tf.function&quot; or apply &quot;tf.function(my_train_fcn)&quot; to obtain a graph function (i.e. for improved performance), the execution will rise an error.\nThis happens because the tracing of the tf.Variable function results in a different behaviour than the observed in eager execution (re-utilization or creation, respectively). You can find more info on this in the <a href=""https://www.tensorflow.org/guide/function#variables"" rel=""noreferrer"">tensorflow help page</a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1593630407}, {'QuestionId': 59893850, 'AnswerId': 60170330, 'URL': 'https://stackoverflow.com/questions/59893850/how-to-accumulate-gradients-in-tensorflow-2-0/60170330#60170330', 'QuestionTitle': 'How to accumulate gradients in tensorflow 2.0?', 'Answer': '<p>In line with the <a href=""https://stackoverflow.com/a/46773161/11530462"">Stack Overflow Answer</a> and the explanation provided in <a href=""https://www.tensorflow.org/guide/eager#train_a_model"" rel=""nofollow noreferrer"">Tensorflow Website</a>, mentioned below is the code for Accumulating Gradients in Tensorflow Version 2.0:</p>\n\n<pre><code>def train(epochs):\n  for epoch in range(epochs):\n    for (batch, (images, labels)) in enumerate(dataset):\n       with tf.GradientTape() as tape:\n        logits = mnist_model(images, training=True)\n        tvs = mnist_model.trainable_variables\n        accum_vars = [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) for tv in tvs]\n        zero_ops = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\n        loss_value = loss_object(labels, logits)\n\n       loss_history.append(loss_value.numpy().mean())\n       grads = tape.gradient(loss_value, tvs)\n       #print(grads[0].shape)\n       #print(accum_vars[0].shape)\n       accum_ops = [accum_vars[i].assign_add(grad) for i, grad in enumerate(grads)]\n\n\n\n    optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n    print (\'Epoch {} finished\'.format(epoch))\n\n# Call the above function    \ntrain(epochs = 3)\n</code></pre>\n\n<p>Complete code can be found in this <a href=""https://colab.research.google.com/gist/rmothukuru/88dd02828f50d9727004d3d9db2c97d3/accumulation_of_gradients.ipynb"" rel=""nofollow noreferrer"">Github Gist</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1581428848}, {'QuestionId': 55400041, 'AnswerId': 55401848, 'URL': 'https://stackoverflow.com/questions/55400041/why-does-this-tensorflow-example-not-have-a-summation-before-the-activation-func/55401848#55401848', 'QuestionTitle': 'Why does this TensorFlow example not have a summation before the activation function?', 'Answer': '<p>The tf.matmul operator performs a matrix multiplication, which means that each element in the resulting matrix is a sum of products (which corresponds exactly to what you describe). </p>\n\n<p>Take a simple example with a row-vector and a column-vector, as would be the case if you had exactly one neuron and an input vector (as per the graphic you shared above);</p>\n\n<p>x = [2,3,1]\ny = [3,\n     1,\n     2]</p>\n\n<p>Then the result would be:</p>\n\n<p>tf.matmul(x, y) = 2*3 + 3*1 +1*2 = 11</p>\n\n<p>There you can see the weighted sum.</p>\n\n<p>p.s: tf.multiply performs element-wise multiplication, which is not what we want here.</p>\n', 'IsAccepted': True, 'CreationDate': 1553788220}, {'QuestionId': 55400041, 'AnswerId': 55400872, 'URL': 'https://stackoverflow.com/questions/55400041/why-does-this-tensorflow-example-not-have-a-summation-before-the-activation-func/55400872#55400872', 'QuestionTitle': 'Why does this TensorFlow example not have a summation before the activation function?', 'Answer': '<p>The last layer of your model <code>out_layer</code> outputs probabilities of each class <code>Prob(y=yi|X)</code> and has shape <code>[batch_size, n_classes]</code>. To calculate these probabilities the <a href=""https://en.wikipedia.org/wiki/Softmax_function"" rel=""nofollow noreferrer"">softmax</a>\nfunction is applied. For each single input data point <code>x</code> that your model receives it outputs a vector of probabilities <code>y</code> of size number of classes. You then pick the one that has highest probability by applying <code>argmax</code> on the output vector <code>class=argmax(P(y|x))</code> which can be written in tensorflow as <code>y_pred = tf.argmax(out_layer, 1)</code>.</p>\n\n<p>Consider network with a single layer. You have input matrix <code>X</code> of shape <code>[n_samples, x_dimension]</code> and you multiply it by some matrix <code>W</code> that has shape <code>[x_dimension, model_output]</code>. The summation that you\'re talking about is dot product between the row of matrix <code>X</code> and column of matrix <code>W</code>. The output will then have shape <code>[n_samples, model_output]</code>. On this output you apply activation function (if it is the final layer you probably want softmax). Perhaps the picture that you\'ve shown is a bit misleading.</p>\n\n<p>Mathematically, the layer without bias can be described as <a href=""https://i.stack.imgur.com/JrEiO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JrEiO.png"" alt=""enter image description here""></a> and suppose that the first row of matrix <a href=""https://i.stack.imgur.com/DWSvk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DWSvk.png"" alt=""enter image description here""></a> (the first row is a single input data point) is</p>\n\n<p><a href=""https://i.stack.imgur.com/owSb7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/owSb7.png"" alt=""enter image description here""></a></p>\n\n<p>and first column of <code>W</code> is</p>\n\n<p><a href=""https://i.stack.imgur.com/jViuf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jViuf.png"" alt=""enter image description here""></a></p>\n\n<p>The result of this dot product is given by</p>\n\n<p><a href=""https://i.stack.imgur.com/U3c2a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U3c2a.png"" alt=""enter image description here""></a></p>\n\n<p>which is your summation. You repeat this for each column in matrix <code>W</code> and the result is vector of size <code>model_output</code> (which correspond to the number of columns in <code>W</code>). To this vector you add bias (if needed) and then apply activation. </p>\n', 'IsAccepted': False, 'CreationDate': 1553785433}, {'QuestionId': 49603346, 'AnswerId': 49607672, 'URL': 'https://stackoverflow.com/questions/49603346/understanding-how-tf-gradients-evaluates/49607672#49607672', 'QuestionTitle': 'Understanding how tf.gradients evaluates', 'Answer': '<p><code>tf.gradient(ys, xs)</code> is returning the symbolic partial derivatives of sum of ys w.r.t. x in xs.\nIn your case, you\'re defining the partial derivative of <code>cross_entropy</code> with respect to <code>x</code> (and extracting the first (and only) element, since <code>tf.gradient</code> returns a list).</p>\n\n<p>The gradient of the cost with respect to the input gives you an indication of how much you have to update your network parameters and in which direction perform this update in order to minimize the cost.</p>\n\n<p>Hence, since you want to trick the classifier you compute the gradient of a certain input with a different label, in order to find the ""indication"" (or signal) you have to follow in order to make the network consider that input a <code>6</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1522656837}, {'QuestionId': 48767565, 'AnswerId': 48768659, 'URL': 'https://stackoverflow.com/questions/48767565/if-you-use-plus-sign-instead-of-tf-add-will-tensorflow-still-calculate-gradient/48768659#48768659', 'QuestionTitle': 'If you use plus sign instead of tf.add, will tensorflow still calculate gradients correctly?', 'Answer': '<p>As described in this <a href=""https://stackoverflow.com/a/35095052"">anwser</a>, <code>__add__</code> op (and <code>__mul__</code> as well btw) are overloaded, therefore  </p>\n\n<pre><code>cost = tf.reduce_sum(tf.add(\n    tf.multiply( y , tf.log(y/abs(yy))),\n    tf.multiply((1 - y) , tf.log((1-y)/abs(1-yy)))\n))\n</code></pre>\n\n<p>is equivalent to</p>\n\n<pre><code>cost = tf.reduce_sum(y * tf.log(y/abs(yy))  + (1 - y) * tf.log((1-y)/abs(1-yy))) \n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1518530665}, {'QuestionId': 45151015, 'AnswerId': 45151785, 'URL': 'https://stackoverflow.com/questions/45151015/how-does-tf-gradients-behave-when-passed-a-list-of-ys-tensors/45151785#45151785', 'QuestionTitle': 'How does tf.gradients behave when passed a list of `ys` tensors?', 'Answer': ""<p>This is due to the fact that you are using <code>tf.constant</code>, which in theory shouldn't be affected by inputs.</p>\n\n<p>If you replace your experiments with anything else (e.g. <code>Variables</code>) it works as expected.</p>\n\n<p>When you apply an operator to the constant (be it addition, or even identity), you obtain a new tensor that is not <code>constant</code>, even though they depand on <code>constant</code>s only -- and therefore you obtain the expected behavior.</p>\n"", 'IsAccepted': True, 'CreationDate': 1500318195}, {'QuestionId': 43934225, 'AnswerId': 43934761, 'URL': 'https://stackoverflow.com/questions/43934225/how-do-tf-gradients-work/43934761#43934761', 'QuestionTitle': 'How do tf.gradients() work?', 'Answer': '<p>TensorFlow uses <a href=""https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation"" rel=""noreferrer"">reverse accumulation</a> which is based on the chain rule, to compute the gradient value at point. In order to compute gradient of function with respect to a variable you have to define both. Also you have to specify value at which you want to compute the gradient. In this example you compute gradient of <code>y=x**2+x+1</code> with respect to <code>x</code> at <code>2</code>:</p>\n\n<pre><code>#!/usr/bin/env python3\nimport tensorflow as tf\n\nx = tf.Variable(2.0)\ny = x**2 + x - 1\n\ngrad = tf.gradients(y, x)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    grad_value = sess.run(grad)\n    print(grad_value)\n\n# output: [5.0]\n</code></pre>\n\n<p>It is also possible to compute a gradient in case your variable is a matrix. In such case the gradient will be also a matrix. Here we use a simple case when the function depends on the sum of all matrix elements:</p>\n\n<pre><code>#!/usr/bin/env python3\nimport tensorflow as tf\n\nX = tf.Variable(tf.random_normal([3, 3]))\nX_sum = tf.reduce_sum(X)\ny = X_sum**2 + X_sum - 1\n\ngrad = tf.gradients(y, X)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    grad_value = sess.run(grad)\n    print(grad_value)\n\n# output: [array([[ 9.6220665,  9.6220665,  9.6220665],\n#   [ 9.6220665,  9.6220665,  9.6220665],\n#   [ 9.6220665,  9.6220665,  9.6220665]], dtype=float32)]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1494582950}, {'QuestionId': 43022091, 'AnswerId': 43033556, 'URL': 'https://stackoverflow.com/questions/43022091/where-in-tensorflow-gradients-is-the-sum-over-the-elements-of-y-made/43033556#43033556', 'QuestionTitle': 'Where in tensorflow gradients is the sum over the elements of y made?', 'Answer': '<p>I\'ve answered it <a href=""https://github.com/tensorflow/tensorflow/issues/675#issuecomment-289242922"" rel=""noreferrer"">here</a> but I\'m guessing it\'s not very useful because you can\'t use this knowledge to be able to differentiate with respect to non-scalar <code>y</code>. Scalar y assumption is central to design of reverse AD algorithm, and there\'s not a single place you can modify to support non-scalar <code>y</code>s. Since this confusion keeps coming up, let me go in a bit more detail as to why it\'s non-trivial:</p>\n\n<p>First of all, how reverse AD works -- suppose we have a function f that\'s composition of component functions f_i. Each component function takes a vector of length n and produces a vector of length n.</p>\n\n<p>Its derivative can be expressed as a sequence of matrix multiplications. The entire expression can be expressed below.</p>\n\n<p><img src=""https://i.stack.imgur.com/Zqa8E.png""></p>\n\n<p>When differentiating, function composition becomes matrix multiplication of corresponding component function Jacobians.</p>\n\n<p>Note that this involves matrix/matrix products which proves to be too expensive for neural networks. IE, AlexNet contains 8k activations in its convnet->fc transition layer. Doing matrix multiples where each matrix is 8k x 8k would take too long. The trick that makes it efficient is to assume that last function in the chain produces a scalar. Then its Jacobian is a vector, and the whole thing can be rewritten in terms of vector-matrix multiplies, instead of matrix-matrix multiplies.</p>\n\n<p><img src=""https://i.stack.imgur.com/nmbYn.png""></p>\n\n<p>This product can be computed efficiently by doing multiplication left to right so everything you do is an nxn vector-matrix multiply instead of nxn matrix-matrix multiply.</p>\n\n<p>You can make it even more efficient by never forming those nxn derivative matrices in a first place, and associate each component function with an op that does vector x Jacobian matrix product implicitly. That\'s what TensorFlow <code>tf.RegisterGradient</code> does. Here\'s an illustration of the ""grad"" associated with an a component function.</p>\n\n<p><a href=""https://i.stack.imgur.com/pIKHv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/pIKHv.png"" alt=""enter image description here""></a></p>\n\n<p>Now, this is done for vector value functions, what if your functions are matrix valued? This is a typical situation we deal with in neural networks. IE, in a layer that does matrix multiply, matrix that you multiply by is an unknown and it is matrix valued. In that case, the last derivative has rank 2, and remaining derivatives have rank 3.</p>\n\n<p>Now to apply the chain rule you\'d have to deal with extra notation because now ""x"" in chain rule means matrix multiplication generalized to tensors of rank-3.\n<a href=""https://i.stack.imgur.com/enwmb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/enwmb.png"" alt=""enter image description here""></a></p>\n\n<p>However, note that we never have to do the multiplication explicitly since we are using a grad operator. So now in practice, this operator now takes values of rank-2 and produces values of rank-2.</p>\n\n<p><a href=""https://i.stack.imgur.com/bKJZS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bKJZS.png"" alt=""enter image description here""></a></p>\n\n<p>So in all of this, there\'s an assumption that final target is scalar which allows fully connected layers to be differentiated by passing matrices around.</p>\n\n<p>If you want to extend this to support non-scalar vector, you would need to modify the reverse AD algorithm to to propagate more info. IE, for fully connected feed-forward nets you would propagate rank-3 tensors around instead of matrices.</p>\n', 'IsAccepted': True, 'CreationDate': 1490556857}, {'QuestionId': 39993377, 'AnswerId': 40005908, 'URL': 'https://stackoverflow.com/questions/39993377/tensorflow-gradients-without-automatic-implicit-sum/40005908#40005908', 'QuestionTitle': 'Tensorflow gradients: without automatic implicit sum', 'Answer': '<p>Here is my work around just taking the derivative of each component (as also mentionned by @Yaroslav) and then packing them all together again in the case of rank 2 tensors (Matrices):</p>\n\n<pre><code>import tensorflow as tf\n\ndef twodtensor2list(tensor,m,n):\n    s = [[tf.slice(tensor,[j,i],[1,1]) for i in range(n)] for j in range(m)]\n    fs = []\n    for l in s:\n        fs.extend(l)\n    return fs\n\ndef grads_all_comp(y, shapey, x, shapex):\n    yl = twodtensor2list(y,shapey[0],shapey[1])\n    grads = [tf.gradients(yle,x)[0] for yle in yl]\n    gradsp = tf.pack(grads)\n    gradst = tf.reshape(gradsp,shape=(shapey[0],shapey[1],shapex[0],shapex[1]))\n    return gradst\n</code></pre>\n\n<p>Now <code>grads_all_comp(y, shapey, x, shapex)</code> will output the rank 4 tensor in the desired format. It is a very inefficient way because everything needs to be sliced up and repacked together, so if someone finds a better I would be very interested to see it. </p>\n', 'IsAccepted': False, 'CreationDate': 1476297096}, {'QuestionId': 39993377, 'AnswerId': 40003139, 'URL': 'https://stackoverflow.com/questions/39993377/tensorflow-gradients-without-automatic-implicit-sum/40003139#40003139', 'QuestionTitle': 'Tensorflow gradients: without automatic implicit sum', 'Answer': ""<p>There isn't a way. TensorFlow 0.11 <code>tf.gradients</code> implements standard reverse-mode AD which gives the derivative of a scalar quantity. You'd need to call <code>tf.gradients</code> for each <code>y[i,j]</code> separately</p>\n"", 'IsAccepted': False, 'CreationDate': 1476287968}]","{46517320, 51858970}","['<p>By default tf.gradients takes the gradient of the scalar you get by summing all elements of all tensors passed to tf.gradients as outputs.</p>\n', ""<p>If y and x have the same shape then the sum over the dy/dx is the sum over exactly one value. However, if you have more than one y for each x, then the gradients are summed.</p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nx_dims = 3\nbatch_size = 4\n\nx = tf.placeholder(tf.float32, (None, x_dims))\ny = 2*(x**2)\nz = tf.stack([y, y]) # There are twice as many z's as x's\n\ndy_dx = tf.gradients(y,x)\ndz_dx = tf.gradients(z,x)\n\nsess = tf.Session()\n\nx_val = np.random.randint(0, 10, (batch_size, x_dims))\ny_val, z_val, dy_dx_val, dz_dx_val = sess.run([y, z, dy_dx, dz_dx], {x:x_val})\n\nprint('x.shape =', x_val.shape)\nprint('x = \\n', x_val)\nprint('y.shape = ', y_val.shape)\nprint('y = \\n', y_val)\nprint('z.shape = ', z_val.shape)\nprint('z = \\n', z_val)\nprint('dy/dx = \\n', dy_dx_val[0])\nprint('dz/dx = \\n', dz_dx_val[0])\n</code></pre>\n\n<p>Produces the following output:</p>\n\n<pre><code>x.shape = (4, 3)\nx = \n [[1 4 8]\n [0 2 8]\n [2 8 1]\n [4 5 2]]\n\ny.shape =  (4, 3)\ny = \n [[  2.  32. 128.]\n [  0.   8. 128.]\n [  8. 128.   2.]\n [ 32.  50.   8.]]\n\nz.shape =  (2, 4, 3)\nz = \n [[[  2.  32. 128.]\n  [  0.   8. 128.]\n  [  8. 128.   2.]\n  [ 32.  50.   8.]]\n\n [[  2.  32. 128.]\n  [  0.   8. 128.]\n  [  8. 128.   2.]\n  [ 32.  50.   8.]]]\n\ndy/dx = \n [[ 4. 16. 32.]\n [ 0.  8. 32.]\n [ 8. 32.  4.]\n [16. 20.  8.]]\ndz/dx = \n [[ 8. 32. 64.]\n [ 0. 16. 64.]\n [16. 64.  8.]\n [32. 40. 16.]]\n</code></pre>\n\n<p>In particular, notice that the values of dz/dx are twice those of dy/dz since they are summed over the inputs to the stack.</p>\n""]","{'https://stackoverflow.com/questions/51858970/tf-gradients-sums-over-ys-does-it/51862290#51862290', 'https://stackoverflow.com/questions/46517320/tensorflow-what-exactly-does-tf-gradients-return/46532900#46532900'}",,0.20806512675616304,0.09150637078980789
1,60013980,tf.nn.embedding_lookup_sparse,Documentation Replicability,tf.nn.embedding_lookup_sparse 3D sparse tensor input,"<p>I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup_sparse</code></a> I found it only supports 2D sparse tensors,</p>

<blockquote>
  <p>sp_ids: N x M SparseTensor of int64 ids where N is typically batch size and M is arbitrary.</p>
</blockquote>

<p>My example code here</p>

<pre><code>import numpy as np
import tensorflow as tf
tf.enable_eager_execution()

# [feature number, embedding dim] 
w = tf.get_variable(""w"", [4, 4], initializer=tf.random_normal_initializer())

z = np.array(
     [
      [
        [0, 1, 2, 3],   # get the vector of row 0, 1, 2, 3 of the embedding matrix w and get the sum
        [2, 3]
      ],

      [
        [1, 3],
        [2]
      ],

      [
        [0, 1, 3],
        [1, 2]
      ]
     ])

sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],
                     indices=[[0,0,0],[0,0,1],[0,0,2],[0,0,3],[0,1,2],
                              [0,1,3],[1,0,1],[1,0,3],[1,1,2],[2,0,0],
                              [2,0,1],[2,0,3],[2,1,1],[2,1,2]],
                     dense_shape=[3, 2, 4])

tf.nn.embedding_lookup_sparse(w, sp, None, combiner='sum')
# the outputs
&lt;tf.Tensor: id=970, shape=(3, 4), dtype=float32, numpy=
array([[-5.8729677 , -1.3900641 ,  0.8126096 , -3.1223912 ],
       [-1.0788026 , -1.1324122 ,  0.34160078,  0.23714277],
       [-2.497394  , -2.7855003 ,  3.0201516 , -1.8009453 ]],
      dtype=float32)&gt;

print(w)
&lt;tf.Variable 'w:0' shape=(4, 4) dtype=float32, numpy=
array([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],
       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],
      dtype=float32)&gt;
</code></pre>

<p>But the expected output is a matrix with a dimension of <code>3x2x4</code>, not <code>3x4</code>. Does <code>tf.nn.embedding_lookup_sparse</code> support this operation?</p>
","<p>The most easier way to do so is to make the sparse tensor as a 2D tensor and get the weights of the embedding matrix and then reshape.</p>

<pre class=""lang-py prettyprint-override""><code># First make the z as a 2D arr and create a sparse tensor 
z = np.array([
        [0, 1, 2, 3],  # get the row 0,1,2,3 of the embedding matrix w and get the sum
        [2, 3],
        [1, 3],
        [2],
        [0, 1, 3],
        [1, 2]
      ])

sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],
                     indices=[[0,0],[0,1],[0,2],[0,3],[1,2],[1,3],[2,1],
                              [2,3],[3,2],[4,0],[4,1],[4,3],[5,1],[5,2]],
                     dense_shape=[6, 4])

res = tf.nn.embedding_lookup_sparse(w, sp, None, combiner='sum')

res.numpy()
# the output
array([[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],
       [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ],
       [ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],
       [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]],

# reshape
tf.reshape(res, [-1, 2, 4])
# that is exacly what I want.
array([[[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],
        [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ]],

       [[ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],
        [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532]],

       [[-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],
        [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]]])

# print w, and the above result is right
w.numpy()

array([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],
       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],
      dtype=float32)
</code></pre>

<p>So, forget the 3D sparse tensor, simply convert it to 2D tensor. Because you only care about the values (index of rows, which are used to get the corresponding rows of the embedding matrix) in the sparse tensor.</p>
","{50259009, 47213602, 39207587, 63069863, 38427471, 59747187, 60644500, 38893526, 35295191, 47485498, 46433596}","[{'QuestionId': 63069863, 'AnswerId': 70588416, 'URL': 'https://stackoverflow.com/questions/63069863/how-to-implement-a-sparse-embedding-in-tensorflow-2-like-pytorch-embeddingspars/70588416#70588416', 'QuestionTitle': 'How to implement a Sparse Embedding in Tensorflow 2 like Pytorch Embedding(sparse=True)?', 'Answer': '<p>I am facing the same problem.I solve it by <a href=""https://github.com/tensorflow/tensorflow/issues/33880"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/33880</a>.</p>\n<pre><code>class SparseEmbedding(tf.keras.layers.Embedding):\ndef __init__(self, *args, combiner=None, mask_zero=None, **kwargs):\n    assert mask_zero is None, &quot;Cannot use sparse embedding with mask zero!&quot;\n    self._combiner = combiner\n    super().__init__(*args, **kwargs)\n\ndef compute_mask(self, inputs, mask=None):\n    return None\n\ndef call(self, inputs):\n    dtype = backend.dtype(inputs)\n    if dtype != \'int32\' and dtype != \'int64\':\n        raise TypeError(f&quot;&quot;&quot;\n        Failed to process inputs! Expected dtype to be integers!\n        \n        expected: int32 or int64\n        received: {dtype}\n        name: {inputs.name}\n        &quot;&quot;&quot;)\n    out = embedding_ops.safe_embedding_lookup_sparse_v2(\n        embedding_weights=self.embeddings,\n        sparse_ids=inputs,\n        combiner=self._combiner)\n    if self._dtype_policy.compute_dtype != self._dtype_policy.variable_dtype:\n        # Instead of casting the variable as in most layers, cast the output, as\n        # this is mathematically equivalent but is faster.\n        out = math_ops.cast(out, self._dtype_policy.compute_dtype)\n    return out\n\ndef get_config(self):\n    base_config = super().get_config()\n    config = {&quot;combiner&quot;: self._combiner}\n    return dict(list(base_config.items()) + list(config.items()))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1641362575}, {'QuestionId': 63069863, 'AnswerId': 63109946, 'URL': 'https://stackoverflow.com/questions/63069863/how-to-implement-a-sparse-embedding-in-tensorflow-2-like-pytorch-embeddingspars/63109946#63109946', 'QuestionTitle': 'How to implement a Sparse Embedding in Tensorflow 2 like Pytorch Embedding(sparse=True)?', 'Answer': '<p>My bad... checking tf.GradientTape() tells me that gradient of tf.gather is already a sparse tensor, so this needs no bother.</p>\n', 'IsAccepted': False, 'CreationDate': 1595832121}, {'QuestionId': 60644500, 'AnswerId': 60647358, 'URL': 'https://stackoverflow.com/questions/60644500/embedding-lookup-from-a-specific-axis/60647358#60647358', 'QuestionTitle': 'Embedding lookup from a specific axis', 'Answer': '<p>You can use the following <code>tf.nn.embedding_lookup</code> or <code>tf.gather_nd</code> methods to achieve your goals.</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nm_np = np.random.randint(0,50,(64, 50, 1))\nm = tf.constant(m_np)\nn = tf.random.normal((50, 64, 128))\n\n# Method 1\ntmp = tf.nn.embedding_lookup(n,m[:,:,0]) # shape=(64,50,64,128)\ntmp = tf.transpose(tmp,[1,3,0,2]) # shape=(50,128,64,64)\nresult1 = tf.transpose(tf.matrix_diag_part(tmp),[2,0,1]) # shape=(64,50,128)\n\n# Method 2\nindices = tf.tile(tf.reshape(tf.range(64),(-1,1,1)),(1,50,1)) # shape=(64,50,1)\nindices = tf.concat([m,indices],axis=-1) # shape=(64,50,2)\nresult2 = tf.gather_nd(n,indices) # shape=(64,50,128)\n\nwith tf.Session() as sess:\n    # Randomly select a location for test\n    n_value,result_value = sess.run([n,result1])\n    print((n_value[m_np[5,4],5,:]==result_value[5,4]).all())\n\n# True\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1583984821}, {'QuestionId': 59747187, 'AnswerId': 59762718, 'URL': 'https://stackoverflow.com/questions/59747187/tensorflow-embedding-lookup-on-multiple-dimension/59762718#59762718', 'QuestionTitle': 'Tensorflow embedding_lookup on multiple dimension', 'Answer': '<p>Try the following,</p>\n\n<pre><code>A = tf.constant([[[1,1],[2,2],[3,3]], [[4,4],[5,5],[6,6]]])\nB = [1,0]\ninds = [(a,b) for a,b in zip(np.arange(len(B)), B)]\n\nC = tf.gather_nd(params=A, indices=inds)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1579146127}, {'QuestionId': 50259009, 'AnswerId': 50296942, 'URL': 'https://stackoverflow.com/questions/50259009/how-to-modify-the-return-tensor-from-tf-nn-embedding-lookup/50296942#50296942', 'QuestionTitle': 'How to modify the return tensor from tf.nn.embedding_lookup()?', 'Answer': '<p>This problem rooted from not clearly understand the tensor and variable in the tensorflow context. Later with more knowledge of the tensor, the solution came to my mind is:</p>\n\n<pre><code>   with tf.device(\'/cpu:0\'), tf.name_scope(""embedding""):\n        self.W = tf.Variable(\n            tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n            name=""W"")\n        self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n        for i in range(0,sequence_length - 1,2):\n            self.tslice = tf.slice(self.embedded_chars,[0,i,0],[0,1,128])\n            self.tslice2 = tf.slice(self.embedded_chars,[0,i+1,0],[0,1,128])\n            self.tslice3 = tf.slice(self.embedded_chars,[0,i+2,0],[0,1,128])\n            self.toffset1 = tf.subtract(self.tslice,self.tslice2)\n            self.toffset2 = tf.subtract(self.tslice2,self.tslice3)\n            self.tconcat = tf.concat([self.toffset1,self.toffset2],1)\n        self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n</code></pre>\n\n<p>the function used, tf.slice, tf.subtract, tf.concat all accept tensor as input. Just avoid using function like tf.scatter_nd_update that require variable as input.</p>\n', 'IsAccepted': False, 'CreationDate': 1526057697}, {'QuestionId': 50259009, 'AnswerId': 50259941, 'URL': 'https://stackoverflow.com/questions/50259009/how-to-modify-the-return-tensor-from-tf-nn-embedding-lookup/50259941#50259941', 'QuestionTitle': 'How to modify the return tensor from tf.nn.embedding_lookup()?', 'Answer': '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup</code></a> simply returns the slice of the larger matrix, so the simplest solution is to update the value of <em>that</em> matrix itself, in your case it\'s <code>self.W</code>:</p>\n\n<pre><code>self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n</code></pre>\n\n<p>Since it\'s a variable, it is compliant with <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update"" rel=""nofollow noreferrer""><code>tf.scatter_nd_update</code></a>. Note that you can\'t update just <em>any</em> tensor, only <em>variables</em>.</p>\n\n<p>Another option is to create a new variable just for the selected slice, assign <code>self.embedded_chars</code> to it and perform an update afterwards.</p>\n\n<hr>\n\n<p>Caveat: in both cases, you\'re blocking the gradients to train the embedding matrix, so double check that overwriting the learned value is really what you want.</p>\n', 'IsAccepted': False, 'CreationDate': 1525890709}, {'QuestionId': 47485498, 'AnswerId': 47486848, 'URL': 'https://stackoverflow.com/questions/47485498/tensorflow-tf-nn-embedding-lookup/47486848#47486848', 'QuestionTitle': 'Tensorflow tf.nn.embedding_lookup', 'Answer': '<p>Yes, the embedding is learned. You can look at the <code>tf.nn.embedding_lookup</code> operation as doing the following matrix multiplication more efficiently:</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\nimport numpy as np\n\nNUM_CATEGORIES, EMBEDDING_SIZE = 5, 3\ny = tf.placeholder(name=\'class_idx\', shape=(1,), dtype=tf.int32)\n\nRS = np.random.RandomState(42)\nW_em_init = RS.randn(NUM_CATEGORIES, EMBEDDING_SIZE)\nW_em = tf.get_variable(name=\'W_em\',\n                       initializer=tf.constant_initializer(W_em_init),\n                       shape=(NUM_CATEGORIES, EMBEDDING_SIZE))\n\n# Using tf.nn.embedding_lookup\ny_em_1 = tf.nn.embedding_lookup(W_em, y)\n\n# Using multiplication\ny_one_hot = tf.one_hot(y, depth=NUM_CATEGORIES)\ny_em_2 = tf.matmul(y_one_hot, W_em)\n\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run([y_em_1, y_em_2], feed_dict={y: [1.0]})\n# [array([[ 1.5230298 , -0.23415338, -0.23413695]], dtype=float32),\n#  array([[ 1.5230298 , -0.23415338, -0.23413695]], dtype=float32)]\n</code></pre>\n\n<p>The variable <code>W_em</code> will be trained in exactly the same way irrespective of whether you use <code>y_em_1</code> or <code>y_em_2</code> formulation; <code>y_em_1</code> is likely to be more efficient, though.</p>\n', 'IsAccepted': False, 'CreationDate': 1511618741}, {'QuestionId': 39207587, 'AnswerId': 39209361, 'URL': 'https://stackoverflow.com/questions/39207587/how-to-use-tf-nn-embedding-lookup-sparse-in-tensorflow/39209361#39209361', 'QuestionTitle': 'How to use tf.nn.embedding_lookup_sparse in TensorFlow?', 'Answer': '<p><code>tf.nn.embedding_lookup_sparse()</code> uses <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/math_ops.html#segmentation"" rel=""noreferrer"">Segmentation</a> to combine embeddings, which requires indices from SparseTensor to start at 0 and to be increasing by 1. That\'s why you get this error.</p>\n\n<p>Instead of boolean values, your sparse tensor needs to hold only the indices of every row that you want to retrieve from embeddings. Here\'s your tweaked code:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nexample = tf.SparseTensor(indices=[[0], [1], [2]], values=[3, 6, 9], dense_shape=[3])\n\nvocabulary_size = 10\nembedding_size = 1\nvar = np.array([0.0, 1.0, 4.0, 9.0, 16.0, 25.0, 36.0, 49.0, 64.0, 81.0])\nembeddings = tf.Variable(var)\n\nembed = tf.nn.embedding_lookup_sparse(embeddings, example, None)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(embed)) #\xa0prints [  9.  36.  81.]\n</code></pre>\n\n<p>In addition, you can use indices from <code>tf.SparseTensor()</code> to combine word embeddings using one of the allowed <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#embedding_lookup_sparse"" rel=""noreferrer""><code>tf.nn.embedding_lookup_sparse()</code></a> combiners:</p>\n\n<blockquote>\n  <ul>\n  <li>""sum"" computes the weighted sum of the embedding results for each row. </li>\n  <li>""mean"" is the weighted sum divided by the total weight.</li>\n  <li>""sqrtn"" is the weighted sum divided by the square root of the sum of the squares of the weights.</li>\n  </ul>\n</blockquote>\n\n<p>For example:</p>\n\n<pre><code>example = tf.SparseTensor(indices=[[0], [0]], values=[1, 2], dense_shape=[2])\n...\nembed = tf.nn.embedding_lookup_sparse(embeddings, example, None, combiner=\'sum\')\n...\nprint(sess.run(embed)) # prints [ 5.]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1472482811}, {'QuestionId': 47213602, 'AnswerId': 47226224, 'URL': 'https://stackoverflow.com/questions/47213602/tf-nn-embedding-lookup-with-float-input/47226224#47226224', 'QuestionTitle': 'tf.nn.embedding_lookup with float input?', 'Answer': '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup</code></a> can\'t allow float input, because the point of this function is to select the embeddings at the <strong>specified rows</strong>.</p>\n\n<p>Example:</p>\n\n<p><a href=""https://i.stack.imgur.com/11LeF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/11LeF.png"" alt=""embed-lookup""></a></p>\n\n<p>Here there are 5 words and 5 embedding 3D vectors, and the operation returns the 3-rd row (with 0-indexing). This is equivalent to this line in tensorflow:</p>\n\n<pre><code>embed = tf.nn.embedding_lookup(embed_matrix, [3])\n</code></pre>\n\n<p>You can\'t possibly look up a floating point index, such as <code>0.2</code> or <code>0.8</code>, because there is no <code>0.2</code> and <code>0.8</code> row index in the matrix. Highly recommend <a href=""http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"" rel=""nofollow noreferrer"">this post by Chris McCormick</a> about word2vec.</p>\n\n<p>What you describe sounds more like a softmax loss function, which outputs a probability distribution over the target classes.</p>\n', 'IsAccepted': True, 'CreationDate': 1510327836}, {'QuestionId': 46433596, 'AnswerId': 46440226, 'URL': 'https://stackoverflow.com/questions/46433596/tf-nn-embedding-lookup-row-or-column/46440226#46440226', 'QuestionTitle': 'tf.nn.embedding_lookup - row or column?', 'Answer': '<p>If <code>params</code> is a single tensor, the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup(params, ids)</code></a> operation treats <code>ids</code> as the indices of <strong>rows</strong> in <code>params</code>.  If <code>params</code> is a list of tensors or a partitioned variable, then <code>ids</code> still correspond to rows in those tensors, but the <code>partition_strategy</code> (either <code>""div""</code> or <code>""mod""</code>) determines how the <code>ids</code> map to a particular row.</p>\n\n<p>As <a href=""https://stackoverflow.com/a/46436638/3574081"">Aaron suggests</a>, it will probably be easiest to define your embedding <code>U</code> as having shape <code>[vocab_size, embedding_size]</code>, so that you can use <code>tf.nn.embedding_lookup()</code> and <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse"" rel=""nofollow noreferrer"">related functions</a>.</p>\n\n<p>Alternatively, you can use the <code>axis</code> argument to <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer""><code>tf.gather()</code></a> to select columns from <code>U</code>:</p>\n\n<pre><code>embedding = tf.gather(U, word_index, axis=1)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1506490785}, {'QuestionId': 46433596, 'AnswerId': 46436638, 'URL': 'https://stackoverflow.com/questions/46433596/tf-nn-embedding-lookup-row-or-column/46436638#46436638', 'QuestionTitle': 'tf.nn.embedding_lookup - row or column?', 'Answer': '<p>U should be vocab_size x embedding_size, the transpose of what you have now.</p>\n', 'IsAccepted': False, 'CreationDate': 1506464268}, {'QuestionId': 38893526, 'AnswerId': 38900894, 'URL': 'https://stackoverflow.com/questions/38893526/whats-the-meaning-of-tf-nn-embedding-lookup-sparse-in-tensorflow/38900894#38900894', 'QuestionTitle': 'What&#39;s the meaning of tf.nn.embedding_lookup_sparse in TensorFlow?', 'Answer': ""<p>The main difference between embedding lookup and embedding lookup sparse is that the sparse version expects the id's and weights to be of type SparseTensor.</p>\n\n<p>How embedding lookup works: </p>\n\n<p>You pass in a tensor of some size and the embedding_lookup_sparse will multiply the slices of the tensors (slices referenced by sp_ids parameter) by some weight (also passed in as sp_weight; defaults to value 1) then you are returned the new slices.  </p>\n\n<p>There is no bias term.  You can add the slices of the tensor together by referencing more than one to be included as an element in your output.</p>\n"", 'IsAccepted': False, 'CreationDate': 1470930342}, {'QuestionId': 35295191, 'AnswerId': 35296384, 'URL': 'https://stackoverflow.com/questions/35295191/tensorflow-embedding-lookup/35296384#35296384', 'QuestionTitle': 'Tensorflow embedding_lookup', 'Answer': '<p>The shape error arises because you are using a two-dimensional tensor, <code>x</code> to index into a two-dimensional embedding tensor <code>W</code>. Think of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup"" rel=""noreferrer""><code>tf.nn.embedding_lookup()</code></a> (and its close cousin <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""noreferrer""><code>tf.gather()</code></a>) as taking each integer value <code>i</code> in <code>x</code> and replacing it with the row <code>W[i, :]</code>. From the error message, one can infer that <code>n_input = 300</code> and <code>embedding_size = 128</code>. In general, the result of <code>tf.nn.embedding_lookup()</code> number of dimensions equal to <code>rank(x) + rank(W) - 1</code>&hellip; in this case, 3. The error arises when you try to multiply this result by <code>_weights[\'h1\']</code>, which is a (two-dimensional) matrix.</p>\n\n<p>To fix this code, it depends on what you\'re trying to do, and why you are passing in a matrix of inputs to the embedding. One common thing to do is to <em>aggregate</em> the embedding vectors for each input example into a single row per example using an operation like <a href=""https://www.tensorflow.org/api_docs/python/tf/reduce_sum"" rel=""noreferrer""><code>tf.reduce_sum()</code></a>. For example, you might do the following:</p>\n\n<pre><code>W = tf.Variable(\n    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0) ,name=""W"")\nembedding_layer = tf.nn.embedding_lookup(W, _X)\n\n# Reduce along dimension 1 (`n_input`) to get a single vector (row)\n# per input example.\nembedding_aggregated = tf.reduce_sum(embedding_layer, [1])\n\nlayer_1 = tf.nn.sigmoid(tf.add(tf.matmul(\n    embedding_aggregated, _weights[\'h1\']), _biases[\'b1\'])) \n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1455033253}, {'QuestionId': 35295191, 'AnswerId': 39383880, 'URL': 'https://stackoverflow.com/questions/35295191/tensorflow-embedding-lookup/39383880#39383880', 'QuestionTitle': 'Tensorflow embedding_lookup', 'Answer': '<p>One another possible solution is : Instead of adding the embedding vectors, concatenate these vectors into a single vector and increase the number of neurons in the hidden layer. <br>\nI used : <br>\n    <code>embedding_aggregated = tf.reshape(embedding_layer, [-1, embedding_size *     sequence_length])</code> <br>\nAlso, i changed the number of neurons in hidden layer to <code>embedding_size * sequence_length</code>. \nObservation : Accuracy also improved on using concatenation rather than addition.</p>\n', 'IsAccepted': False, 'CreationDate': 1473315913}, {'QuestionId': 38427471, 'AnswerId': 38438045, 'URL': 'https://stackoverflow.com/questions/38427471/why-does-tf-nn-embedding-lookup-use-a-list-of-embeddings/38438045#38438045', 'QuestionTitle': 'Why does tf.nn.embedding_lookup use a list of embeddings?', 'Answer': ""<p><code>tf.embedding_lookup</code> function assumes that the embedding matrix is <em>sharded</em>, i.e., partitioned into many pieces. Indeed, it can work when the embedding matrix is sharded one-way, in which case it acts like <code>tf.gather</code>.</p>\n\n<p>But the more interesting case is when the embedding matrix is large and you can't fit it on one machine's memory, or, you want a high bandwidth on the embedding lookup operation. In those cases, it helps to partition the matrix into pieces. The pieces can be distributed across machines to fit it all in memory, and also allow parallel reads for higher bandwidth for the lookup.</p>\n"", 'IsAccepted': True, 'CreationDate': 1468848634}]","{65746272, 60013980}","['<p>The most easier way to do so is to make the sparse tensor as a 2D tensor and get the weights of the embedding matrix and then reshape.</p>\n\n<pre class=""lang-py prettyprint-override""><code># First make the z as a 2D arr and create a sparse tensor \nz = np.array([\n        [0, 1, 2, 3],  # get the row 0,1,2,3 of the embedding matrix w and get the sum\n        [2, 3],\n        [1, 3],\n        [2],\n        [0, 1, 3],\n        [1, 2]\n      ])\n\nsp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],\n                     indices=[[0,0],[0,1],[0,2],[0,3],[1,2],[1,3],[2,1],\n                              [2,3],[3,2],[4,0],[4,1],[4,3],[5,1],[5,2]],\n                     dense_shape=[6, 4])\n\nres = tf.nn.embedding_lookup_sparse(w, sp, None, combiner=\'sum\')\n\nres.numpy()\n# the output\narray([[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],\n       [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ],\n       [ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],\n       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],\n       [-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],\n       [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]],\n\n# reshape\ntf.reshape(res, [-1, 2, 4])\n# that is exacly what I want.\narray([[[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],\n        [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ]],\n\n       [[ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],\n        [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532]],\n\n       [[-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],\n        [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]]])\n\n# print w, and the above result is right\nw.numpy()\n\narray([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],\n       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],\n       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],\n       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],\n      dtype=float32)\n</code></pre>\n\n<p>So, forget the 3D sparse tensor, simply convert it to 2D tensor. Because you only care about the values (index of rows, which are used to get the corresponding rows of the embedding matrix) in the sparse tensor.</p>\n', '<p>You can access each of the three tensors within the SparseTensor object with:</p>\n<pre><code># Here some_sparse_tensor is an object of the tf.sparse.SparseTensor class\nsome_sparse_tensor.indices\nsome_sparse_tensor.values\nsome_sparse_tensor.dense_shape\n</code></pre>\n<p>As per documentation:\n<a href=""https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor#attributes"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor#attributes</a></p>\n<p>You can not get their &quot;names&quot;, but you can assign them to their own variables:\n<code>values_of_sparse_tensor = some_sparse_tensor.values</code></p>\n', '<p>The most easier way to do so is to make the sparse tensor as a 2D tensor and get the weights of the embedding matrix and then reshape.</p>\n\n<pre class=""lang-py prettyprint-override""><code># First make the z as a 2D arr and create a sparse tensor \nz = np.array([\n        [0, 1, 2, 3],  # get the row 0,1,2,3 of the embedding matrix w and get the sum\n        [2, 3],\n        [1, 3],\n        [2],\n        [0, 1, 3],\n        [1, 2]\n      ])\n\nsp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],\n                     indices=[[0,0],[0,1],[0,2],[0,3],[1,2],[1,3],[2,1],\n                              [2,3],[3,2],[4,0],[4,1],[4,3],[5,1],[5,2]],\n                     dense_shape=[6, 4])\n\nres = tf.nn.embedding_lookup_sparse(w, sp, None, combiner=\'sum\')\n\nres.numpy()\n# the output\narray([[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],\n       [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ],\n       [ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],\n       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],\n       [-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],\n       [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]],\n\n# reshape\ntf.reshape(res, [-1, 2, 4])\n# that is exacly what I want. array([[[-3.6457794 , -1.5215762 ,  1.7455802 , -2.5802398 ],\n        [-2.227188  ,  0.13151208, -0.9329706 , -0.5421516 ]],\n\n       [[ 0.2612    , -0.6687883 ,  1.7068193 , -0.03506255],\n        [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532]],\n\n       [[-2.3057768 , -1.0579524 ,  3.1107986 , -2.8524451 ],\n        [-0.19161725, -1.7275481 , -0.0906471 ,  1.0514997 ]]])\n\n# print w, and the above result is right\nw.numpy()\n\narray([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],\n       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],\n       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],\n       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],\n      dtype=float32)\n</code></pre>\n\n<p>So, forget the 3D sparse tensor, simply convert it to 2D tensor.']","{'https://stackoverflow.com/questions/65746272/how-do-you-get-the-names-of-tensors-used-in-a-sparse-tensor/65746296#65746296', 'https://stackoverflow.com/questions/60013980/tf-nn-embedding-lookup-sparse-3d-sparse-tensor-input/60014627#60014627'}",,0.21415151813588368,0.06996960419445482
2,57349824,tf.keras,Documentation Replication on Other Examples,"Recurrent neural network, time series prediction with newer Tensorflow 1.14","<p>How to use new tf.keras API with recurrent neural network? I have checked the documentation but there is no example of such a situation.
There is this great book Hands on machine learning from 2017. Since that year the API of tensorflow has evolved and I am trying to rewrite recurrent neural network for time series prediction with using version <code>1.14</code> code.
The code from the book is using older <code>tf.nn.dynamic_rnn</code> and <code>tf.nn.rnn_cell.BasicRNNCell</code>:</p>

<pre><code>n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1
learning_rate = 0.001

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])
cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)
rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
loss = tf.reduce_mean(tf.square(outputs - y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()
n_iterations = 500
batch_size = 50

with tf.Session() as sess:
    init.run()
        for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})
</code></pre>

<p>And this code works just fine (except that it throws warnings about deprecation left and right). I wanted to use <code>tf.keras</code> API as suggested in warning. My code is the same except:</p>

<pre><code>cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)  
rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
</code></pre>

<p>But this yields following exception:</p>

<pre><code>InvalidArgumentError: Input to reshape is a tensor with 50 values, but the requested shape requires a multiple of 20
 [[node Reshape_1 (defined at &lt;ipython-input-9-879361be49dd&gt;:3) ]]
</code></pre>

<p>so I understand that the problematic line is</p>

<pre><code>outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
</code></pre>

<p>After checking and comparing documentation for both cells <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a> and 
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN</a> I can't find the culprit.</p>

<p><strong>What is the difference with these two cells? How to use tf.keras API with time series?</strong></p>

<p>Full old code: <a href=""https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb"" rel=""nofollow noreferrer"">https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb</a></p>

<p>Full ""my"" code:</p>

<pre><code>import numpy as np
import tensorflow as tf
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd
from utils import shuffle_batch, variable_summaries
import os


dir_path = os.getcwd()

now = datetime.utcnow().strftime(""%Y%m%d%H%M%S"")
root_logdir = ""tf_logs""
logdir = ""{}/run-{}/"".format(root_logdir, now)
print(dir_path)


t_min, t_max = -5, 5
section_start = (t_max + t_min) / 2
resolution = 0.1
n_steps = 20

def time_series(t):
    return np.sin(t)

def next_batch(batch_size, n_steps):
    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)
    Ts = t0 + np.arange(0., n_steps + 1) * resolution
    ys = time_series(Ts)
    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)


t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))

t_instance = np.linspace(start = section_start, stop = section_start + resolution * (n_steps + 1),num = n_steps + 1)

plt.figure(figsize=(11,4))
plt.subplot(121)
plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"")
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)
#plt.axis([-10, 10, -17, 13])
plt.xlabel(""Time"")
plt.ylabel(""Value"")

plt.subplot(122)
plt.title(""A training instance"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""c*"", markersize=10, label=""target"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")


# In[6]:


n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])


# In[7]:


cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)                        


rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
print(rnn_outputs.get_shape())


stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons], name='reshape1')
stacked_outputs = tf.keras.layers.Dense(n_outputs,name=""hidden2"")(stacked_rnn_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs], name='reshape2')


learning_rate = 0.001

loss = tf.reduce_mean(tf.square(outputs - y)) # MSE
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()

n_iterations = 1500
batch_size = 50
save_path =os.path.join(dir_path,""model"",""recurrent_sinus_model"")

with tf.Session() as sess:
    init.run()
    for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    saver.save(sess, save_path)


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})


plt.title(""Testing the model"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""w*"", markersize=10, label=""target"")
plt.plot(t_instance[1:], y_pred[0,:,0], ""r."", markersize=10, label=""prediction"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")

plt.show()


# In[ ]:


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t.reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})



plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"",linewidth=5,c='r')
plt.plot(t[:-1], time_series(t[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)

plt.xlabel(""Time"")
plt.ylabel(""Value"")
</code></pre>
","<p>So the answer is:</p>

<pre><code>rnn_outputs, rnn_states  = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"", return_state=True, return_sequences=True)(X)
</code></pre>

<p>instead of </p>

<pre><code>rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
</code></pre>

<p>so the parameter <code>return_sequences=True</code> make the RNN return the time series as well, and well, this is the point.</p>
","{35961216, 46632225, 42603459, 48253348, 44090278, 46810120, 40905736, 51355725, 45409714, 47513277, 61109886}","[{'QuestionId': 61109886, 'AnswerId': 61122453, 'URL': 'https://stackoverflow.com/questions/61109886/one-step-prediction-of-time-series-using-lstm/61122453#61122453', 'QuestionTitle': 'One step prediction of time series using LSTM', 'Answer': ""<p>So, assuming you have defined, trained, and saved a model somewhat like the following:</p>\n\n<pre><code># define model\nmodel = Sequential()\nmodel.add(LSTM(...))\n\n# compile model\nmodel.compile(...)\n\n# fit model\nmodel.fit(...)\n\n# save model\nmodel.save('lstm_model.h5')\n</code></pre>\n\n<p>To predict new values with that model, load the model and run predict with a new set of input. For example, assume you predict Y based on X. It would look something like the following:</p>\n\n<pre><code>from keras.models import load_model\n# load model\nmodel = load_model('lstm_model.h5')\n\n# define input\nX = ...\n\n# make predictions\nyhat = model.predict(X, verbose=0)\nprint(yhat) \n</code></pre>\n\n<p>It looks like you are working on a sequence regression problem where you define the time step and the LSTM predicts that value. The input X is therefore only the data/sequence necessary for making the prediction <code>yhat</code>. It does not include all training data before it.  For example, if your input to training the LSTM is between <code>1...1500</code>, then <code>X</code> would be <code>1501</code>. </p>\n\n<p>Remember to use any data preparation process you used on the training data on the inference data as well. </p>\n"", 'IsAccepted': False, 'CreationDate': 1586439428}, {'QuestionId': 51355725, 'AnswerId': 52603780, 'URL': 'https://stackoverflow.com/questions/51355725/time-series-prediction-using-rnns-keras-in-r/52603780#52603780', 'QuestionTitle': 'Time Series prediction using RNNs (Keras) in R', 'Answer': ""<p>It seems to me that you need to redefine the <code>generator</code>, you need to get only the <code>samples</code> as output. Following your example:</p>\n\n<pre><code># generator function\ngenerator &lt;- function(data, lookback, delay, min_index, max_index,\n                      shuffle = FALSE, batch_size = 128, step = 6) {\n  if (is.null(max_index))\n    max_index &lt;- nrow(data) - delay - 1\n  i &lt;- min_index + lookback\n  function() {\n    if (shuffle) {\n      rows &lt;- sample(c((min_index+lookback):max_index), size = batch_size)\n    } else {\n      if (i + batch_size &gt;= max_index)\n        i &lt;&lt;- min_index + lookback\n      rows &lt;- c(i:min(i+batch_size-1, max_index))\n      i &lt;&lt;- i + length(rows)\n    }\n\n    samples &lt;- array(0, dim = c(length(rows), \n                                lookback / step,\n                                dim(data)[[-1]]))\n    targets &lt;- array(0, dim = c(length(rows)))\n\n    for (j in 1:length(rows)) {\n      indices &lt;- seq(rows[[j]] - lookback, rows[[j]]-1, \n                     length.out = dim(samples)[[2]])\n      samples[j,,] &lt;- data[indices,]\n      targets[[j]] &lt;- data[rows[[j]] + delay,2]\n    }            \n\n    list(samples) # just the samples, (quick and dirty solution, I just removed targets)\n  }\n}\n\n# test_gen is the same\ntest_gen &lt;- generator(\n  data,\n  lookback = lookback,\n  delay = delay,\n  min_index = 300001,\n  max_index = NULL,\n  step = step,\n  batch_size = batch_size\n)\n</code></pre>\n\n<p>Now you can call <code>predict_generator</code>:</p>\n\n<pre><code>preds &lt;- model %&gt;% predict_generator(test_gen, steps = test_steps)\n</code></pre>\n\n<p>But now you need to <em>de-normalize</em> those, because you scaled each variable before the fit. </p>\n\n<pre><code>denorm_pred = preds * std + mean\n</code></pre>\n\n<p>Be careful that <code>std</code> and <code>mean</code> should be calculated on <code>T (degC)</code> <em>only</em> on the <code>train</code> data, otherwise you're overfitting.</p>\n"", 'IsAccepted': True, 'CreationDate': 1538465393}, {'QuestionId': 35961216, 'AnswerId': 49816576, 'URL': 'https://stackoverflow.com/questions/35961216/how-to-train-a-rnn-with-lstm-cells-for-time-series-prediction/49816576#49816576', 'QuestionTitle': 'How to train a RNN with LSTM cells for time series prediction', 'Answer': '<p>After reading several LSTM introduction blogs e.g. <a href=""http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction"" rel=""nofollow noreferrer"">Jakob Aungiers\'</a>, option 3 seems to be the right one for stateless LSTM.</p>\n\n<p>If your LSTMs need to remember data longer ago than your <code>num_steps</code>, your can train in a stateful way - for a Keras example see <a href=""https://philipperemy.github.io/keras-stateful-lstm/"" rel=""nofollow noreferrer"">Philippe Remy\'s blog post ""Stateful LSTM in Keras""</a>. Philippe does not show an example for batch size greater than one, however. I guess that in your case a batch size of four with stateful LSTM could be used with the following data (written as <code>input -&gt; label</code>):</p>\n\n<pre><code>batch #0:\n1,2,3,4 -&gt; 5\n2,3,4,5 -&gt; 6\n3,4,5,6 -&gt; 7\n4,5,6,7 -&gt; 8\n\nbatch #1:\n5,6,7,8 -&gt; 9\n6,7,8,9 -&gt; 10\n7,8,9,10 -&gt; 11\n8,9,10,11 -&gt; 12\n\nbatch #2:\n9,10,11,12 -&gt; 13\n...\n</code></pre>\n\n<p>By this, the state of e.g. the 2nd sample in batch #0 is correctly reused to continue training with the 2nd sample of batch #1.</p>\n\n<p>This is somehow similar to your option 4, however you are not using all available labels there.</p>\n\n<p><strong>Update:</strong></p>\n\n<p>In extension to my suggestion where <code>batch_size</code> equals the <code>num_steps</code>, Alexis Huet <a href=""https://stackoverflow.com/a/48521460/1389680"">gives an answer</a> for the case of <code>batch_size</code> being a divisor of <code>num_steps</code>, which can be used for larger <code>num_steps</code>. He <a href=""https://ahstat.github.io/RNN-Keras-time-series/"" rel=""nofollow noreferrer"">describes it nicely</a> on his blog.</p>\n', 'IsAccepted': True, 'CreationDate': 1523620994}, {'QuestionId': 46810120, 'AnswerId': 48284720, 'URL': 'https://stackoverflow.com/questions/46810120/predicting-values-in-time-series-for-future-periods-using-rnn-in-tensorflow/48284720#48284720', 'QuestionTitle': 'Predicting values in time series for future periods using RNN in Tensorflow', 'Answer': ""<p>In first step you should use real values. Then using predict value to replace last value as you want. \nHope the following code could help you. </p>\n\n<pre><code>with tf.Session() as sess:\n    saver.restore(sess, './model_saved')\n    preds = []\n    X_batch = last_n_steps_value\n    X_batch = X_batch.reshape(-1, n_steps, 1)\n    for i in range(number_you_want_to_predict):\n        pred = sess.run(outputs, feed_dict={X: X_batch})\n        preds.append(pred.reshape(7)[-1])\n        X_batch = X_batch[:, 1:]\n        # Using predict value to replace real value\n        X_batch = np.append(X_batch, pred[:, -1])\n        X_batch = X_batch.reshape(-1, n_steps, 1)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1516116618}, {'QuestionId': 48253348, 'AnswerId': 48253662, 'URL': 'https://stackoverflow.com/questions/48253348/rnn-time-series-predictions-with-multiple-time-series-dimension-with-keras-tens/48253662#48253662', 'QuestionTitle': 'RNN time series predictions with multiple time series dimension with Keras, Tensorflow', 'Answer': ""<p>Your output has also sequential nature. <code>LSTM</code> by default has a flag <code>return_sequences=False</code>. This makes your sequence to be squashed to a vector after the second <code>LSTM</code> layer. In order to change that try:</p>\n\n<pre><code>model = Sequential()\nmodel.add(Bidirectional(LSTM(20, return_sequences=True),\n                    input_shape=Train_X.shape[1:]))\nmodel.add(Bidirectional(LSTM(10, return_sequences=True)))\nmodel.add(Dense(5))\nmodel.compile(loss='mae', \n          optimizer='rmsprop')\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1515960210}, {'QuestionId': 47513277, 'AnswerId': 47516229, 'URL': 'https://stackoverflow.com/questions/47513277/time-series-prediction-with-keras/47516229#47516229', 'QuestionTitle': 'Time-series prediction with keras', 'Answer': ""<p>The message says that your input data (numpy arrays) has shape <code>(1,56,1)</code>, while your model is expecting shape <code>(any, any, 56)</code>. </p>\n\n<p>In recurrent networks, the input shape should be like <code>(batch size, time steps, input features)</code>.</p>\n\n<p>So, you need to decide whether you've got 56 time steps of the same feature, or if you've got only one time steps of 56 different features. Then you pick one of the two shapes to adjust. </p>\n\n<p>It seems logical (if you're using LSTMs), that you have sequences, so I assume you've got 56 time steps. </p>\n\n<p>Then, your input shape in the LSTM layer should be:</p>\n\n<pre><code>LSTM(doesntMatter, input_shape=(56,1), return_sequences=True)\n</code></pre>\n\n<p>Or (if you want a variable number of steps):</p>\n\n<pre><code>LSTM(doesntMatter, input_shape=(None,1), return_sequences=True)\n</code></pre>\n\n<hr>\n\n<p>Suppose you want more than one info, such as Date and Weekday, for instance. Then you've got two features. Your shape would be then <code>input_shape(None,2)</code>. </p>\n"", 'IsAccepted': True, 'CreationDate': 1511802228}, {'QuestionId': 46632225, 'AnswerId': 46635564, 'URL': 'https://stackoverflow.com/questions/46632225/keras-lstm-for-timeseries-prediction-predicting-vectors-of-features/46635564#46635564', 'QuestionTitle': 'Keras LSTM for timeseries prediction: predicting vectors of features', 'Answer': ""<p>This is a loss function I'm using for 2D highly unbalanced data, it works very well. You can replace the <code>binary_crossentropy</code> for another kind of loss.</p>\n\n<pre><code>import keras.backend as K\n\ndef weightedByBatch(yTrue,yPred):\n\n    nVec = K.ones_like(yTrue) #to sum the total number of elements in the tensor\n    percent = K.sum(yTrue) / K.sum(nVec) #percent of ones relative to total\n    percent2 = 1 - percent #percent of zeros relative to total   \n    yTrue2 = 1 - yTrue #complement of yTrue (yTrue+ yTrue2 = full of ones)   \n\n    weights = (yTrue2 * percent2) + (yTrue*percent)\n    return K.mean(K.binary_crossentropy(yTrue,yPred)/weights)\n</code></pre>\n\n<p>For your 3D data, this may work, but maybe you could work in columns, creating a pair of weights for each feature, instead of summing all features together. </p>\n\n<p>This would be done like this:</p>\n\n<pre><code>def weightedByBatch2D(yTrue,yPred):\n\n    nVec = K.ones_like(yTrue) #to sum the total number of elements in the tensor\n    percent = K.sum(K.sum(yTrue,axis=0,keepdims=True),axis=1,keepdims=True) / K.sum(K.sum(nVec,axis=0,keepdims=True),axis=1,keepdims=True) #percent of ones relative to total\n    percent2 = 1 - percent #percent of zeros relative to total   \n    yTrue2 = 1 - yTrue #complement of yTrue (yTrue+ yTrue2 = full of ones)   \n\n    weights = (yTrue2 * percent2) + (yTrue*percent)\n    return K.mean(K.binary_crossentropy(yTrue,yPred)/weights)   \n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1507493471}, {'QuestionId': 45409714, 'AnswerId': 45419859, 'URL': 'https://stackoverflow.com/questions/45409714/time-series-prediction-with-lstm-in-keras/45419859#45419859', 'QuestionTitle': 'Time Series Prediction with LSTM in Keras', 'Answer': ""<p><code>X_train</code> needs to be three-dimensional. When specifying the <code>input_shape</code> in your first layer, you are specificying <code>(timesteps, features)</code>. Then, when passing actual data to <code>fit()</code>, you need to pass in a three-dimensional array where each sample has the shape <code>(timesteps, features)</code>.</p>\n\n<pre><code>timesteps = X_train.shape[0]\nfeatures = X_train.shape[1]\n\nX_train = X_train.reshape(1, timesteps, features)\n</code></pre>\n\n<p>This leaves you with a single training sample, though, which I doubt is what you want. Without knowing what your data actually looks like it is hard to be of further help! It seems more likely you'd want to break down your dataset into sequences of some fixed amount of timesteps. Additionally, you'll want to apply similar transformations to <code>X_test</code>.</p>\n"", 'IsAccepted': False, 'CreationDate': 1501514954}, {'QuestionId': 44090278, 'AnswerId': 44092110, 'URL': 'https://stackoverflow.com/questions/44090278/keras-lstm-for-time-series-bad-prediction-and-convergance-to-unchangable-range-o/44092110#44092110', 'QuestionTitle': 'Keras LSTM for time-series bad prediction and convergance to unchangable range of values', 'Answer': '<p><code>neurons = 5</code>, this is a very low capacity model. Might not be enough to model the targeted time series function.</p>\n\n<p><code>timesteps = 1</code>, this is time series so the output must be dependent on a certain number of <code>timesteps</code> before a correct prediction should be made. <code>timesteps = 1</code> means your output is only dependent on 1 <code>timestep</code>.</p>\n\n<p><code>batch_size = 1</code>, this will take a while to converge. Normally, this should be power of 2 (16, 32, 64, 128, 256, 512).</p>\n', 'IsAccepted': False, 'CreationDate': 1495327151}, {'QuestionId': 42603459, 'AnswerId': 42606452, 'URL': 'https://stackoverflow.com/questions/42603459/lstm-for-prediction-of-future-time-series-values-with-keras/42606452#42606452', 'QuestionTitle': 'lstm for prediction of future time series values with Keras', 'Answer': '<p>You have 8x2 inputs in each sample, for every of those 8 time step you encode 512 features that you keep track of. Then again, 512 values... The number of parameters must be huge? I hope that you have enough data and variety in the patterns to make use of that huge number of parameters, otherwise you will overfit. </p>\n\n<p>I would also avise you to use dropout for the LSTM like this : </p>\n\n<pre><code>model.add(LSTM(512, return_sequences=True, dropout_W = 0.2)) #- original\n</code></pre>\n\n<p>There is also the feature dropout_U, but I would not use that one. The LSTM has multiple gates as you should know, each of these gates are dense layers, so you can choose on which one you want to apply the dropout. Best practice is to apply it on the inputs gate (dropout_W) and not on recurrent gate (dropout_U). </p>\n\n<p>Otherwise the overall architecture makes sense. </p>\n', 'IsAccepted': True, 'CreationDate': 1488704810}, {'QuestionId': 40905736, 'AnswerId': 40925981, 'URL': 'https://stackoverflow.com/questions/40905736/rnn-lstm-time-series-tensorflow-0-12-error/40925981#40925981', 'QuestionTitle': 'RNN-LSTM Time-Series Tensorflow 0.12 error', 'Answer': '<p>I changed the framework to v0.10.0 and it is working.</p>\n', 'IsAccepted': True, 'CreationDate': 1480659492}, {'QuestionId': 35961216, 'AnswerId': 37528208, 'URL': 'https://stackoverflow.com/questions/35961216/how-to-train-a-rnn-with-lstm-cells-for-time-series-prediction/37528208#37528208', 'QuestionTitle': 'How to train a RNN with LSTM cells for time series prediction', 'Answer': '<p>I\'m just about to learn LSTMs in TensorFlow and try to implement an example which (luckily) tries to predict some time-series / number-series genereated by a simple math-fuction.</p>\n\n<p>But I\'m using a different way to structure the data for training, motivated by <a href=""http://arxiv.org/pdf/1502.04681v3.pdf"" rel=""noreferrer"">Unsupervised Learning of Video Representations using LSTMs</a>:</p>\n\n<p><a href=""https://i.stack.imgur.com/1Ngkk.png"" rel=""noreferrer"">LSTM Future Predictor Model</a></p>\n\n<p><strong>Option 5:</strong></p>\n\n<pre><code>input data               label     \n1,2,3,4                  5,6,7,8\n2,3,4,5                  6,7,8,9\n3,4,5,6                  7,8,9,10\n...\n</code></pre>\n\n<p>Beside this paper, I (tried) to take inspiration by the given TensorFlow RNN examples. My current complete solution looks like this:</p>\n\n<pre><code>import math\nimport random\nimport numpy as np\nimport tensorflow as tf\n\nLSTM_SIZE = 64\nLSTM_LAYERS = 2\nBATCH_SIZE = 16\nNUM_T_STEPS = 4\nMAX_STEPS = 1000\nLAMBDA_REG = 5e-4\n\n\ndef ground_truth_func(i, j, t):\n    return i * math.pow(t, 2) + j\n\n\ndef get_batch(batch_size):\n    seq = np.zeros([batch_size, NUM_T_STEPS, 1], dtype=np.float32)\n    tgt = np.zeros([batch_size, NUM_T_STEPS], dtype=np.float32)\n\n    for b in xrange(batch_size):\n        i = float(random.randint(-25, 25))\n        j = float(random.randint(-100, 100))\n        for t in xrange(NUM_T_STEPS):\n            value = ground_truth_func(i, j, t)\n            seq[b, t, 0] = value\n\n        for t in xrange(NUM_T_STEPS):\n            tgt[b, t] = ground_truth_func(i, j, t + NUM_T_STEPS)\n    return seq, tgt\n\n\n# Placeholder for the inputs in a given iteration\nsequence = tf.placeholder(tf.float32, [BATCH_SIZE, NUM_T_STEPS, 1])\ntarget = tf.placeholder(tf.float32, [BATCH_SIZE, NUM_T_STEPS])\n\nfc1_weight = tf.get_variable(\'w1\', [LSTM_SIZE, 1], initializer=tf.random_normal_initializer(mean=0.0, stddev=1.0))\nfc1_bias = tf.get_variable(\'b1\', [1], initializer=tf.constant_initializer(0.1))\n\n# ENCODER\nwith tf.variable_scope(\'ENC_LSTM\'):\n    lstm = tf.nn.rnn_cell.LSTMCell(LSTM_SIZE)\n    multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * LSTM_LAYERS)\n    initial_state = multi_lstm.zero_state(BATCH_SIZE, tf.float32)\n    state = initial_state\n    for t_step in xrange(NUM_T_STEPS):\n        if t_step &gt; 0:\n            tf.get_variable_scope().reuse_variables()\n\n        # state value is updated after processing each batch of sequences\n        output, state = multi_lstm(sequence[:, t_step, :], state)\n\nlearned_representation = state\n\n# DECODER\nwith tf.variable_scope(\'DEC_LSTM\'):\n    lstm = tf.nn.rnn_cell.LSTMCell(LSTM_SIZE)\n    multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * LSTM_LAYERS)\n    state = learned_representation\n    logits_stacked = None\n    loss = 0.0\n    for t_step in xrange(NUM_T_STEPS):\n        if t_step &gt; 0:\n            tf.get_variable_scope().reuse_variables()\n\n        # state value is updated after processing each batch of sequences\n        output, state = multi_lstm(sequence[:, t_step, :], state)\n        # output can be used to make next number prediction\n        logits = tf.matmul(output, fc1_weight) + fc1_bias\n\n        if logits_stacked is None:\n            logits_stacked = logits\n        else:\n            logits_stacked = tf.concat(1, [logits_stacked, logits])\n\n        loss += tf.reduce_sum(tf.square(logits - target[:, t_step])) / BATCH_SIZE\n\nreg_loss = loss + LAMBDA_REG * (tf.nn.l2_loss(fc1_weight) + tf.nn.l2_loss(fc1_bias))\n\ntrain = tf.train.AdamOptimizer().minimize(reg_loss)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n\n    total_loss = 0.0\n    for step in xrange(MAX_STEPS):\n        seq_batch, target_batch = get_batch(BATCH_SIZE)\n\n        feed = {sequence: seq_batch, target: target_batch}\n        _, current_loss = sess.run([train, reg_loss], feed)\n        if step % 10 == 0:\n            print(""@{}: {}"".format(step, current_loss))\n        total_loss += current_loss\n\n    print(\'Total loss:\', total_loss)\n\n    print(\'### SIMPLE EVAL: ###\')\n    seq_batch, target_batch = get_batch(BATCH_SIZE)\n    feed = {sequence: seq_batch, target: target_batch}\n    prediction = sess.run([logits_stacked], feed)\n    for b in xrange(BATCH_SIZE):\n        print(""{} -&gt; {})"".format(str(seq_batch[b, :, 0]), target_batch[b, :]))\n        print("" `-&gt; Prediction: {}"".format(prediction[0][b]))\n</code></pre>\n\n<p>Sample output of this looks like this:</p>\n\n<pre><code>### SIMPLE EVAL: ###\n# [input seq] -&gt; [target prediction]\n#  `-&gt; Prediction: [model prediction]  \n[  33.   53.  113.  213.] -&gt; [  353.   533.   753.  1013.])\n `-&gt; Prediction: [ 19.74548721  28.3149128   33.11489105  35.06603241]\n[ -17.  -32.  -77. -152.] -&gt; [-257. -392. -557. -752.])\n `-&gt; Prediction: [-16.38951683 -24.3657589  -29.49801064 -31.58583832]\n[ -7.  -4.   5.  20.] -&gt; [  41.   68.  101.  140.])\n `-&gt; Prediction: [ 14.14126873  22.74848557  31.29668617  36.73633194]\n...\n</code></pre>\n\n<p>The model is a <em>LSTM-autoencoder</em> having 2 layers each.</p>\n\n<p>Unfortunately, as you can see in the results, this model does not learn the sequence properly. I might be the case that I\'m just doing a bad mistake somewhere, or that 1000-10000 training steps is just way to few for a LSTM. As I said, I\'m also just starting to understand/use LSTMs properly.\nBut hopefully this can give you some inspiration regarding the implementation.</p>\n', 'IsAccepted': False, 'CreationDate': 1464618735}, {'QuestionId': 35961216, 'AnswerId': 36017615, 'URL': 'https://stackoverflow.com/questions/35961216/how-to-train-a-rnn-with-lstm-cells-for-time-series-prediction/36017615#36017615', 'QuestionTitle': 'How to train a RNN with LSTM cells for time series prediction', 'Answer': '<p>I believe Option 1 is closest to the reference implementation in /tensorflow/models/rnn/ptb/reader.py</p>\n\n<pre><code>def ptb_iterator(raw_data, batch_size, num_steps):\n  """"""Iterate on the raw PTB data.\n\n  This generates batch_size pointers into the raw PTB data, and allows\n  minibatch iteration along these pointers.\n\n  Args:\n    raw_data: one of the raw data outputs from ptb_raw_data.\n    batch_size: int, the batch size.\n    num_steps: int, the number of unrolls.\n\n  Yields:\n    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n    The second element of the tuple is the same data time-shifted to the\n    right by one.\n\n  Raises:\n    ValueError: if batch_size or num_steps are too high.\n  """"""\n  raw_data = np.array(raw_data, dtype=np.int32)\n\n  data_len = len(raw_data)\n  batch_len = data_len // batch_size\n  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n  for i in range(batch_size):\n    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n\n  epoch_size = (batch_len - 1) // num_steps\n\n  if epoch_size == 0:\n    raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n  for i in range(epoch_size):\n    x = data[:, i*num_steps:(i+1)*num_steps]\n    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n    yield (x, y)\n</code></pre>\n\n<p>However, another Option is to select a pointer into your data array randomly for each training sequence.</p>\n', 'IsAccepted': False, 'CreationDate': 1458061331}]","{45027944, 57349824}","[""<p>Here's what would be helpful to keep in mind for your case specifically:</p>\n\n<p>Given the input shape of <code>[128, 300, 38]</code></p>\n\n<ul>\n<li>One call to <code>dynamic_rnn</code> will propagate through all 300 steps, and if you are using something like LSTM, the state will also be carried through those 300 steps</li>\n<li>However, each SUBSEQUENT call to <code>dynamic_rnn</code> will not automatically remember the state from the previous call. By the second call, the weights/etc. will have been updated thanks to the first call, but you will still need to pass the state that resulted from the first call into the second call. That's why <code>dynamic_rnn</code> has a parameter <code>initial_state</code> and that's why one of its outputs is <code>final_state</code> (i.e. the state after processing all 300 steps in ONE call). So you are meant to take the final state from call N and pass it back as the initial state for call N+1 to <code>dynamic_rnn</code>. This allrelates specifically to LSTM, since this is what you asked for</li>\n<li>You are right to note that elements in one batch don't necessarily relate to each other within the same batch. This is something you need to consider carefully. Because with successive calls to <code>dynamic_rnn</code>, batch elements in your input sequences have to relate to their respective counterparts in the previous/following sequence, but not to each other. I.e. element 3 in the first call may have nothing to do with the other 127 elements within the same batch, but element 3 in the NEXT call has to be the temporal/logical continuation of element 3 in the PREVIOUS call, and so forth. This way, the state that you keep passing forward makes sense continuously </li>\n</ul>\n"", '<p>So the answer is:</p>\n\n<pre><code>rnn_outputs, rnn_states  = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"", return_state=True, return_sequences=True)(X)\n</code></pre>\n\n<p>instead of </p>\n\n<pre><code>rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)\n</code></pre>\n\n<p>so the parameter <code>return_sequences=True</code> make the RNN return the time series as well, and well, this is the point.</p>\n', ""<p>Here's what would be helpful to keep in mind for your case specifically:</p>\n\n<p>Given the input shape of <code>[128, 300, 38]</code></p>\n\n<ul>\n<li>One call to <code>dynamic_rnn</code> will propagate through all 300 steps, and if you are using something like LSTM, the state will also be carried through those 300 steps</li>\n<li>However, each SUBSEQUENT call to <code>dynamic_rnn</code> will not automatically remember the state from the previous call. By the second call, the weights/etc. will have been updated thanks to the first call, but you will still need to pass the state that resulted from the first call into the second call. That's why <code>dynamic_rnn</code> has a parameter <code>initial_state</code> and that's why one of its outputs is <code>final_state</code> (i.e. the state after processing all 300 steps in ONE call). So you are meant to take the final state from call N and pass it back as the initial state for call N+1 to <code>dynamic_rnn</code>. This allrelates specifically to LSTM, since this is what you asked for</li>\n<li>You are right to note that elements in one batch don't necessarily relate to each other within the same batch. This is something you need to consider carefully. Because with successive calls to <code>dynamic_rnn</code>, batch elements in your input sequences have to relate to their respective counterparts in the previous/following sequence, but not to each other. I.e.""]","{'https://stackoverflow.com/questions/57349824/recurrent-neural-network-time-series-prediction-with-newer-tensorflow-1-14/57362313#57362313', 'https://stackoverflow.com/questions/45027944/understanding-the-functioning-of-a-recurrent-neural-network-with-lstm-cells/45032070#45032070'}",,0.26329248474778555,0.12788236889554794
2,55573670,tf.nn.sparse_softmax_cross_entropy_with_logits,Documentation Replication on Other Examples,Unexpected output for tf.nn.sparse_softmax_cross_entropy_with_logits,"<p>The TensorFlow documentation for <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> explicitly declares that I should not apply softmax to the inputs of this op:</p>

<blockquote>
  <p>This op expects unscaled logits, since it performs a softmax on logits
  internally for efficiency. Do not call this op with the output of
  softmax, as it will produce incorrect results.</p>
</blockquote>

<p>However if I use cross entropy without softmax it gives me unexpected results. According to <a href=""https://cs231n.github.io/neural-networks-3/#sanitycheck"" rel=""nofollow noreferrer"">CS231n course</a> the expected loss value is around 2.3 for CIFAR-10:</p>

<blockquote>
  <p>For example, for CIFAR-10 with a Softmax classifier we would expect
  the initial loss to be 2.302, because we expect a diffuse probability
  of 0.1 for each class (since there are 10 classes), and Softmax loss
  is the negative log probability of the correct class so: -ln(0.1) =
  2.302.</p>
</blockquote>

<p>However without softmax I get much bigger values, for example 108.91984.</p>

<p>What exactly am I doing wrong with <code>sparse_softmax_cross_entropy_with_logits</code>? The TF code is shown below.</p>

<pre><code>import tensorflow as tf
import numpy as np
from tensorflow.python import keras


(_, _), (x_test, y_test) = keras.datasets.cifar10.load_data()
x_test = np.reshape(x_test, [-1, 32, 32, 3])

y_test = np.reshape(y_test, (10000,))
y_test = y_test.astype(np.int32)

x = tf.placeholder(dtype=tf.float32, shape=(None, 32, 32, 3))
y = tf.placeholder(dtype=tf.int32, shape=(None,))

layer = tf.layers.Conv2D(filters=16, kernel_size=3)(x)
layer = tf.nn.relu(layer)
layer = tf.layers.Flatten()(layer)
layer = tf.layers.Dense(units=1000)(layer)
layer = tf.nn.relu(layer)
logits = tf.layers.Dense(units=10)(layer)

# If this line is uncommented I get expected value around 2.3
# logits = tf.nn.softmax(logits)

loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,
                                                      logits=logits)
loss = tf.reduce_mean(loss, name='cross_entropy')

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    res = sess.run(loss, feed_dict={x: x_test[0:256], y: y_test[0:256]})
    print(""loss: "", res)
    # Expected output is value close to 2.3
    # Real outputs are 108.91984, 72.82324, etc.

</code></pre>
","<p>The issue is not in the lines </p>

<pre><code># If this line is uncommented I get expected value around 2.3
# logits = tf.nn.softmax(logits)
</code></pre>

<p>Images in cifar10 dataset are in RGB, thus pixel values are in range [0, 256). If you divide your <code>x_test</code> by 255 </p>

<pre><code>x_test = np.reshape(x_test, [-1, 32, 32, 3]).astype(np.float32) / 255
</code></pre>

<p>the values will be rescaled to [0,1] and <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> will return expected values</p>
","{45489314, 47120680, 39583752, 43415177, 46446763, 48065164, 44124396, 50038585, 50179709, 57878623}","[{'QuestionId': 57878623, 'AnswerId': 57887051, 'URL': 'https://stackoverflow.com/questions/57878623/error-with-tf-nn-sparse-softmax-cross-entropy-with-logits/57887051#57887051', 'QuestionTitle': 'Error with tf.nn.sparse_softmax_cross_entropy_with_logits', 'Answer': ""<blockquote>\n  <p>I don't understand how having a shape [50,1] is not the same as being 1D.</p>\n</blockquote>\n\n<p>While you can reshape a [50, 1] 2D matrix into a [50] 1D matrix just with a simple squeeze, Tensorflow will never do that automatically. </p>\n\n<p>The only heuristic the <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> uses to check if the input shape is correct is to check the number of dimensions it has. If it's not 1D, it fails without trying other heuristics like checking if the input could be squeezed. This is a security feature.</p>\n"", 'IsAccepted': False, 'CreationDate': 1568197366}, {'QuestionId': 45489314, 'AnswerId': 45489665, 'URL': 'https://stackoverflow.com/questions/45489314/understanding-output-of-softmax-cross-entropy-with-logits/45489665#45489665', 'QuestionTitle': 'Understanding output of softmax_cross_entropy_with_logits', 'Answer': '<p><a href=""http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/"" rel=""nofollow noreferrer"">Here is a good explanation about it</a>. It works like this. First, the logits are passed through the <a href=""https://en.wikipedia.org/wiki/Softmax_function"" rel=""nofollow noreferrer"">softmax function</a>, giving you a probability distribution:</p>\n\n<pre><code>import numpy as np\n\nlogits = np.array([1., -3., 10.])\n# Softmax function\nsoftmax = np.exp(logits) / np.sum(np.exp(logits))\nprint(softmax)\n&gt;&gt;&gt; array([  1.23394297e-04,   2.26004539e-06,   9.99874346e-01])\n# It is a probability distribution because the values are in [0, 1]\n# and add up to 1\nnp.sum(softmax)\n&gt;&gt;&gt; 0.99999999999999989  # Almost, that is\n</code></pre>\n\n<p>Then, you compute the cross-entropy between the computed softmax value and the target.</p>\n\n<pre><code>target = np.array([0.1, 0.02, 0.88])\n# Cross-entropy function\ncrossentropy = -np.sum(target * np.log(softmax))\nprint(crossentropy)\n&gt;&gt;&gt; 1.1601256622376641\n</code></pre>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits"" rel=""nofollow noreferrer""><code>tf.nn.softmax_cross_entropy_with_logits</code></a> will return you one of these values ""per vector"" (by default, ""vectors"" are in the last dimension), so, for example, if your input logits and targets have size 10x3 you will end up with 10 cross-entropy values. Usually one sums or averages these all and uses the result as loss value to minimize (which is what <a href=""https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy"" rel=""nofollow noreferrer""><code>tf.losses.softmax_cross_entropy</code></a> offers). The logic behind the cross-entropy expression is that <code>target * np.log(softmax)</code> will take negative values closer to zero where <code>target</code> is more similar to <code>softmax</code> and diverge from zero (towards minus infinity) when they are different.</p>\n\n<p>Note: This is a <em>logical</em> explanation of the function. Internally, TensorFlow most likely perform different but equivalent operations for better performance and numerical stability.</p>\n', 'IsAccepted': False, 'CreationDate': 1501777125}, {'QuestionId': 50179709, 'AnswerId': 50181774, 'URL': 'https://stackoverflow.com/questions/50179709/tensorflow-softmax-returning-only-0-and-1/50181774#50181774', 'QuestionTitle': 'Tensorflow - softmax returning only 0 and 1', 'Answer': '<p>Looking at the 2nd box under <strong>The Neural Network</strong>:</p>\n\n<pre><code># output layer\nwith tf.variable_scope(\'output_lay\') as scope:\n    weights = weight_variable([4096, CLASSES])\n    bias = bias_variable([CLASSES], 0.)\n    activation = tf.nn.relu(tf.matmul(out, weights)+bias, name=scope.name)\n    out = tf.nn.softmax(activation)\nreturn tf.reshape(out, [-1, CLASSES])\n</code></pre>\n\n<p><strong>NB : <code>ReLu</code> activation is only used for hidden layers not output layer.</strong></p>\n\n<p>Then you are feeding this to cross-entropy in your <code>train</code> function</p>\n\n<pre><code>logits=AlexNet(x_tr)\n\n# loss function\ncross_entropy = -tf.reduce_sum(tf.squeeze(y_tr)*tf.log(tf.clip_by_value(tf.squeeze(logits),1e-10,1.0)))\nloss = tf.reduce_mean(cross_entropy)\n</code></pre>\n\n<p>Re-visiting <a href=""https://en.wikipedia.org/wiki/Loss_functions_for_classification#Cross_entropy_loss_(Log_Loss)"" rel=""nofollow noreferrer"">cross entropy</a>:</p>\n\n<p><code>C= −1/n * (∑[y*ln(a)+(1−y)*ln(1−a)])</code></p>\n\n<p>where <code>a = sigmoid(W(x)+b)</code>, So I suggest :</p>\n\n<pre><code>with tf.variable_scope(\'output_lay\') as scope:\n    weights = weight_variable([4096, CLASSES])\n    bias = bias_variable([CLASSES], 0.)\n    return tf.matmul(out, weights)+bias\n</code></pre>\n\n<p>and for simplicity just use inbuilt softmax function:</p>\n\n<pre><code>logits=AlexNet(x_tr)\n\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_input, logits=logits)\nloss = tf.reduce_mean(cross_entropy)\n</code></pre>\n\n<p><code>tf.nn.softmax_cross_entropy_with_logits</code> takes in <code>W(x)+b</code> and efficiently calculates the cross entropy.</p>\n', 'IsAccepted': False, 'CreationDate': 1525461427}, {'QuestionId': 50038585, 'AnswerId': 50039953, 'URL': 'https://stackoverflow.com/questions/50038585/tf-nn-softmax-cross-entropy-with-logits-v2-returing-zero-for-mlp/50039953#50039953', 'QuestionTitle': 'tf.nn.softmax_cross_entropy_with_logits_v2 returing zero for MLP', 'Answer': '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2"" rel=""nofollow noreferrer"">tf.nn.softmax_cross_entropy_with_logits_v2</a>\nalready computes softmax for you, you need to pass unbounded logits to your cross-entropy function, not the probability distribution that softmax returns. Try this: </p>\n\n<pre><code>def multilayer_perceptron(x, weights, biases):\n    # ... \n    logits = tf.matmul(layer_3, weights[\'out\']) + biases[\'out\']\n    out_layer = tf.nn.softmax(logits)\n    return logits, out_layer\n</code></pre>\n\n<p>then use <em>logits</em> to compute cross-entropy and <em>out_layer</em> for inference.</p>\n\n<pre><code>logits, pred = multilayer_perceptron(x, weights, biases)\ncost = tf.reduce_mean(\n  tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n</code></pre>\n\n<p><em>optimizer</em> is the operation that calculates gradients and applies them to variables, this operation is essentially what makes your network ""learn"", you have declared it but I don\'t see you calling it inside the loop. This should do it:</p>\n\n<pre><code>_, temp_loss = sess.run([optimizer, cost], feed_dict={x: rand_x,y: rand_y})\n</code></pre>\n\n<p>You have a binary classification problem, I would guess your labels are either 0 or 1. You also have one output neuron, softmax of that will always return 1.0. What I would suggest is to make 2 output neurons, that way softmax will calculate probability distribution over 2 classes. Then your inference is the argmax of that distribution:</p>\n\n<pre><code>correct = tf.cast(tf.equal(tf.argmax(pred, 1), y), dtype=tf.float32)\naccuracy = tf.reduce_mean(correct)\n</code></pre>\n\n<p>In this case, you need to use <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits"" rel=""nofollow noreferrer"">tf.nn.sparse_softmax_cross_entropy_with_logits()</a> to calculate cross-entropy:</p>\n\n<pre><code>cost = tf.reduce_mean(\n  tf.nn.tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n</code></pre>\n\n<p>I would further suggest you to look at <a href=""https://stackoverflow.com/a/35279095/9535747"">this post</a> for more details on what I\'m talking about.</p>\n', 'IsAccepted': True, 'CreationDate': 1524736304}, {'QuestionId': 48065164, 'AnswerId': 48065937, 'URL': 'https://stackoverflow.com/questions/48065164/tensorflow-weighted-cross-entropy-with-logits-produces-wrong-result/48065937#48065937', 'QuestionTitle': 'TensorFlow weighted_cross_entropy_with_logits produces wrong result', 'Answer': '<p>The implementation of my_binary_crossentropy_np is wrong.\nHere is the right one:</p>\n\n<pre><code>l = (weight - 1.0) * labels + 1.0\n  loss1 = np.multiply(1.0 - labels, output)\n  loss2 = np.multiply(l, np.log(1.0 + np.exp(-abs(output))) + np.maximum(-output, 0))\n  loss = loss1 + loss2\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1514917497}, {'QuestionId': 47120680, 'AnswerId': 47120911, 'URL': 'https://stackoverflow.com/questions/47120680/why-cant-the-output-of-the-network-go-through-a-softmax-when-using-softmax-cros/47120911#47120911', 'QuestionTitle': 'Why can&#39;t the output of the network go through a softmax when using softmax_cross_entropy_with_logits?', 'Answer': ""<p>Since <code>tf.nn.softmax_cross_entropy_with_logits</code> computes internally the softmax (in a numerically stable way) of its input, you have to define your network in order to use the linear activation function: <code>tf.identity</code></p>\n\n<pre><code>result = tf.layers.dense(input=dropout, classes_num, tf.identity)\n</code></pre>\n\n<p>Moreover, once the network has been trained and you want to use use the model for inference, you have to replace the activation with the softmax.</p>\n\n<p>Thus, introduce in your code a <code>is_training</code> python boolean variable, and use it to change your model definition when you're training or testing.</p>\n\n<pre><code>result = tf.layers.dense(input=dropout, classes_num,\n             tf.identity if is_training else tf.nn.softmax)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1509880478}, {'QuestionId': 47120680, 'AnswerId': 47120829, 'URL': 'https://stackoverflow.com/questions/47120680/why-cant-the-output-of-the-network-go-through-a-softmax-when-using-softmax-cros/47120829#47120829', 'QuestionTitle': 'Why can&#39;t the output of the network go through a softmax when using softmax_cross_entropy_with_logits?', 'Answer': '<p>The function that you have mentioned is <strong>tf.nn.softmax_cross_entropy_with_logits</strong>. As the name suggests, it first performs a softmax (<em>i.e</em> scaling) on logits and then calculates the entropy between logits and labels. </p>\n\n<p>Therefore, if you input logits (as <strong>result</strong> in your code) that already performed the softmax then you perform the softmax twice on your logits, which will produce incorrect results.</p>\n\n<p>Hope this helps.</p>\n', 'IsAccepted': False, 'CreationDate': 1509879895}, {'QuestionId': 46446763, 'AnswerId': 46453363, 'URL': 'https://stackoverflow.com/questions/46446763/tf-nn-sparse-softmax-cross-entropy-with-logits-rank-error/46453363#46453363', 'QuestionTitle': 'tf.nn.sparse_softmax_cross_entropy_with_logits - rank error', 'Answer': '<p>Fixed it for you. <code>x</code> needs to be a 2d vector  </p>\n\n<pre><code>with tf.Session() as sess:\n    y = tf.constant([1])\n    x = tf.expand_dims(tf.constant([0.0, 1.0, 0.0]), 0)\n    r = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n    print(r.eval())\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1506531410}, {'QuestionId': 44124396, 'AnswerId': 44125405, 'URL': 'https://stackoverflow.com/questions/44124396/tf-nn-softmax-cross-entropy-with-logits-is-giving-wrong-output-why/44125405#44125405', 'QuestionTitle': 'tf.nn.softmax_cross_entropy_with_logits is giving wrong output. Why?', 'Answer': ""<p>I believe that <strong>tf.nn.softmax_cross_entropy_with_logits</strong> calculates <em>sum(-bLog(softmax(a)) )</em>-ish and the desired output is <strong>0.794377</strong>.  If you don't include the negative sign when you do it by hand then you'll get <strong>-0.794377</strong>, which will work as a loss term but you'll have to maximize your loss, not minimize it, when training.</p>\n\n<p>It is also important to note that TF uses natural log, and not log base 10.</p>\n"", 'IsAccepted': False, 'CreationDate': 1495510783}, {'QuestionId': 43415177, 'AnswerId': 43428933, 'URL': 'https://stackoverflow.com/questions/43415177/sparse-softmax-cross-entropy-with-logits-results-is-worse-than-softmax-cross-ent/43428933#43428933', 'QuestionTitle': 'sparse_softmax_cross_entropy_with_logits results is worse than softmax_cross_entropy_with_logits', 'Answer': '<p>I found the problem, thanks to @mrry for helpful comment, actually I mistake about calculation of accuracy, in fact, ""sparse_softmax"" and ""softmax"" has the same loss(or cost) for input logits,</p>\n\n<p>for computation accuracy, I change </p>\n\n<p><code>correct_pred = tf.equal(tf.argmax(self.score_,1), tf.argmax(y,1))</code></p>\n\n<p>to</p>\n\n<p><code>correct_pred = tf.equal(tf.argmax(self.score_,1), y ))</code></p>\n\n<p>since in ""sparse_softmax"" the ground truth labels are not in one-hot vector format, but real int32 or int64 numbers.</p>\n', 'IsAccepted': True, 'CreationDate': 1492276442}, {'QuestionId': 39583752, 'AnswerId': 39602346, 'URL': 'https://stackoverflow.com/questions/39583752/nan-from-sparse-softmax-cross-entropy-with-logits-in-tensorflow/39602346#39602346', 'QuestionTitle': 'NaN from sparse_softmax_cross_entropy_with_logits in Tensorflow', 'Answer': '<p>It actually turns out that some of my labels were out of range (e.g. a label of 14000, when my logits matrix is just 150 x 10000). It turns out this results in a NaN rather than an error.</p>\n', 'IsAccepted': True, 'CreationDate': 1474399752}, {'QuestionId': 39583752, 'AnswerId': 39588174, 'URL': 'https://stackoverflow.com/questions/39583752/nan-from-sparse-softmax-cross-entropy-with-logits-in-tensorflow/39588174#39588174', 'QuestionTitle': 'NaN from sparse_softmax_cross_entropy_with_logits in Tensorflow', 'Answer': '<p>The <code>NaN</code> error probably occurs when one of the softmaxed logits gets truncated to 0, as you have said, and then it performs log(0) to compute the cross-entropy error.</p>\n\n<p>To avoid this, as it is suggested in <a href=""https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/33713196#33713196"">this other answer</a>, you could clip the values of the softmax output so that they are never zero.</p>\n\n<pre><code>out = tf.clip_by_value(out,1e-10,100.0)\n</code></pre>\n\n<p>Or you could add a small constant to avoid having zeros:</p>\n\n<pre><code>out = out + 1e-10\n</code></pre>\n\n<p>The problem with it is that the softmax function is applied on the logits internally by <code>sparse_softmax_cross_entropy_with_logits()</code> so you can not change its behavior.</p>\n\n<p>To overcome this, code the cross entropy error yourself and add the constant <code>1e-10</code> to the output of the softmax, not to the logits.</p>\n\n<pre><code>loss = -tf.reduce_sum(labels*tf.log(tf.nn.softmax(logits) + 1e-10))\n</code></pre>\n\n<p>Be aware that with the <code>sparse_softmax_cross_entropy_with_logits()</code> function the variable <code>labels</code> was the numeric value of the label, but if you implement the cross-entropy loss yourself, <code>labels</code> have to be the one-hot encoding of these numeric labels.</p>\n\n<p><strong>Update:</strong> I have corrected the answer thanks to the comment by <a href=""https://stackoverflow.com/users/997378/mdaoust"">@mdaoust</a>. As he said the zeros are only relevant after the softmax function has been applied to the logits, not before.</p>\n', 'IsAccepted': False, 'CreationDate': 1474356582}, {'QuestionId': 39583752, 'AnswerId': 39588018, 'URL': 'https://stackoverflow.com/questions/39583752/nan-from-sparse-softmax-cross-entropy-with-logits-in-tensorflow/39588018#39588018', 'QuestionTitle': 'NaN from sparse_softmax_cross_entropy_with_logits in Tensorflow', 'Answer': ""<p><code>tf.sparse_softmax_cross_entropy_with_logits</code> handles the case of <code>log(0)</code> for you, you don't have to worry about it.</p>\n\n<p>Usually a <code>NaN</code> is due to a high learning rate of your optimization algorithm. Try to lower it until <code>NaN</code> errors disappear and the loss starts to decrease </p>\n"", 'IsAccepted': False, 'CreationDate': 1474356113}]","{43394152, 41283115}","['<p>Well, usually <code>p(x)</code> in cross-entropy equation is true distribution, while <code>q(x)</code> is the distribution obtained from softmax. So, if <code>p(x)</code> is one-hot (and this is so, otherwise sparse cross-entropy could not be applied), cross entropy is just negative log for probability of true category.</p>\n\n<p>In your example, <code>softmax(logits)</code> is a vector with values <code>[0.09003057,  0.24472847,  0.66524096]</code>, so the loss is <code>-log(0.24472847) = 1.4076059</code> which is exactly what you got as output.</p>\n', ""<p>The difference is that <code>tf.nn.softmax_cross_entropy_with_logits</code> doesn't assume that the classes are mutually exclusive: </p>\n\n<blockquote>\n  <p>Measures the probability error in discrete classification tasks in\n  which each class is independent and not mutually exclusive. For\n  instance, one could perform multilabel classification where a picture\n  can contain both an elephant and a dog at the same time.</p>\n</blockquote>\n\n<p>Compare with <code>sparse_*</code>:</p>\n\n<blockquote>\n  <p>Measures the probability error in discrete classification tasks in\n  which the classes are mutually exclusive (each entry is in exactly one\n  class). For example, each CIFAR-10 image is labeled with one and only\n  one label: an image can be a dog or a truck, but not both.</p>\n</blockquote>\n\n<p>As such, with sparse functions, the dimensions of <code>logits</code> and <code>labels</code> are not the same: <code>labels</code> contain one number per example, whereas <code>logits</code> the number of classes per example, denoting probabilities.</p>\n""]","{'https://stackoverflow.com/questions/43394152/tensorflow-what-exact-formula-is-applied-in-tf-nn-sparse-softmax-cross-entropy/43395986#43395986', 'https://stackoverflow.com/questions/41283115/tensorflow-difference-between-tf-nn-softmax-cross-entropy-with-logits-and-tf-nn/41285054#41285054'}",,0.20475285336472415,0.21582586179811863
2,51612489,tf.edit_distance,Lack of Alternative Solutions/Documentation,tensorflow tf.edit_distance explanation required?,"<p>How does tensorflow <code>tf.edit_distance</code> function works?
How it compares string stored in two different sparse matrix equivalent of 2d or 3d dense matrix. </p>

<p>Example given on tensorflow web page <a href=""https://www.tensorflow.org/api_docs/python/tf/edit_distance"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/edit_distance</a> is not so obvious. Please provide explanation using some other examples. </p>

<p>Also this example is not clear.</p>

<pre><code>#'hypothesis' is a tensor of shape [2, 1] with variable-length values:
#(0,0) = [""a""] and (1,0) = [""b""]

hypothesis = tf.SparseTensor([[0, 0, 0],[1, 0, 0]],[""a"", ""b""],(2, 1, 1))

#'truth' is a tensor of shape `[2, 2]` with variable-length values:
#(0,0) = [], (0,1) = [""a""], (1,0) = [""b"", ""c""],(1,1) = [""a""]

truth = tf.SparseTensor([[0, 1, 0],[1, 0, 0],[1, 0, 1],[1, 1, 0]],[""a"", ""b"", 
""c"", ""a""],(2, 2, 2))

normalize = True

#'output' is a tensor of shape [2, 2] with edit distances normalized by 
#'truth' lengths.

output ==&gt; [[inf, 1.0],[0.5, 1.0]],

(0,0): no truth, (0,1): no hypothesis, (1,0): addition, (1,1): no hypothesis
</code></pre>

<p>How output is of dimension [2,2]?</p>

<p>What normalization is doing here?</p>
","<p>hypothesis in dense form looks like this</p>

<pre><code>[[['a']],
 [['b']]] # (2, 1, 1)
</code></pre>

<p>truth is this</p>

<pre><code>[[[],['a']],
 [['b', 'c'], ['a']]] # (2, 2, 2)
</code></pre>

<p>We are trying to find the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a> between hypothesis and truth value.
So, here is what is happening:</p>

<p>at (0,0,0) - how far is ['a'] in hypothesis from [] - no truth in that position so can't calculate distance</p>

<p>at (0,0,1) - since there is nothing in that position at hypothesis we return 1. Unlike the case above, the distance is 1 because in theory the hypothesis can be made same as truth by inserting one character (See Levenshtein distance calculations)</p>

<p>at (1,0,0) - how far is ['b'] in hyp from ['b', 'c'] in truth. This is again 1, since we can insert a character to make hyp same as truth. But, we selected to normalize the output distance. So we divide by length of truth segment, which is 2. So you get 0.5</p>

<p>at (1,0,1) - how far is [] in hyp from ['a'], since there is nothing in that position at hyp, we return 1</p>

<p>Output is (2,2) because rank of hyp and truth is 3. The function returns tensor with rank (rank-1) </p>

<p>It helps by imagining what we are trying to do here. You have 2 sequences in hypothesis and 2 sequences in the truth. So your output score will be such that you get scores for each position in each sequence.</p>

<p>Here is an example where we try to match 4 hypotheses to a truth value. I think you have to do this for each truth sequence for the use case that you describe in your comment - let me know if you find something more efficient :-)</p>

<pre><code>import tensorflow as tf

hypothesis = tf.SparseTensor(
            [[0, 0, 0],
             [1, 0, 0],
             [2, 0, 0],
             [3, 0, 0]],
             [""a"", ""b"", ""c"", ""d""],
            (4, 1, 1))

truth = tf.SparseTensor([[0, 0, 0], [0, 0, 1], [0, 1, 0]], [""b"", ""c"", ""a""], (1,2,2))
num_hyp = 4
truth = tf.sparse_concat(0, [truth] * num_hyp)

d = tf.edit_distance(hypothesis, truth)

with tf.Session() as sess:
    print(sess.run(d))
</code></pre>

<p>Output:</p>

<pre><code>[[1.  1. ]
 [0.5 1. ]
 [0.5 1. ]
 [1.  1. ]]
</code></pre>
","{42541088, 45905601, 69331522, 44264962, 31024077, 37479119, 2460177, 38020817, 58054035, 13979390}","[{'QuestionId': 2460177, 'AnswerId': 72549276, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/72549276#72549276', 'QuestionTitle': 'Edit Distance in Python', 'Answer': ""<p>following up on @krassowski's answer</p>\n<pre><code>from difflib import SequenceMatcher\n\ndef sequence_matcher_edits(word_a, word_b):\n  required_edits = [code for code in (\n      SequenceMatcher(a=word_a, b=word_b, autojunk=False).get_opcodes()\n    )\n    if code[0] != 'equal'\n  ]\n  return len(required_edits)\n\nprint(f&quot;sequence_matcher_edits {sequence_matcher_edits('kitten', 'sitting')}&quot;)\n# -&gt; sequence_matcher_edits 3\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1654706017}, {'QuestionId': 2460177, 'AnswerId': 71994577, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/71994577#71994577', 'QuestionTitle': 'Edit Distance in Python', 'Answer': '<p>Fine tuned codes based on the version from @Santosh and should address the issue brought up by @Artur Krajewski; The biggest difference is replacing an effective 2d matrix</p>\n<pre class=""lang-py prettyprint-override""><code>\ndef edit_distance(s1, s2):\n# add a blank character for both strings\n    m=len(s1)+1\n    n=len(s2)+1\n# launch a matrix\n    tbl = [[0] * n for i in range(m)] \n    for i in range(m): tbl[i][0]=i\n    for j in range(n): tbl[0][j]=j\n\n    for i in range(1, m):\n        for j in range(1, n):\n#if strings have same letters, set operation cost as 0 otherwise 1\n            cost = 0 if s1[i-1] == s2[j-1] else 1\n#find min practice\n            tbl[i][j] = min(tbl[i][j-1]+1, tbl[i-1][j]+1, tbl[i-1][j-1]+cost)\n    return tbl\n\nedit_distance(&quot;birthday&quot;, &quot;Birthdayyy&quot;)\n\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1650861596}, {'QuestionId': 2460177, 'AnswerId': 69545740, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/69545740#69545740', 'QuestionTitle': 'Edit Distance in Python', 'Answer': '<p>I would recommend not creating this kind of code on your own. There are libraries for that.</p>\n<p>For instance the <a href=""https://pypi.org/project/Levenshtein/"" rel=""noreferrer"">Levenshtein</a> library.</p>\n<pre><code>\nIn [2]: Levenshtein.distance(&quot;foo&quot;, &quot;foobar&quot;)\nOut[2]: 3\n\nIn [3]: Levenshtein.distance(&quot;barfoo&quot;, &quot;foobar&quot;)\nOut[3]: 6\n\nIn [4]: Levenshtein.distance(&quot;Buroucrazy&quot;, &quot;Bureaucracy&quot;)\nOut[4]: 3\n\nIn [5]: Levenshtein.distance(&quot;Misisipi&quot;, &quot;Mississippi&quot;)\nOut[5]: 3\n\nIn [6]: Levenshtein.distance(&quot;Misisipi&quot;, &quot;Misty Mountains&quot;)\nOut[6]: 11\n\nIn [7]: Levenshtein.distance(&quot;Buroucrazy&quot;, &quot;Born Crazy&quot;)\nOut[7]: 4\n\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1634065328}, {'QuestionId': 69331522, 'AnswerId': 69331630, 'URL': 'https://stackoverflow.com/questions/69331522/what-does-a-tensor-t-1-tf-newaxis-stands-for/69331630#69331630', 'QuestionTitle': 'What does a tensor t[..., 1, tf.newaxis] ...stands for?', 'Answer': '<p>&quot;...&quot; means &quot;all dimensions prior to&quot; the specified dimension.</p>\n<p>So, in this example:</p>\n<pre class=""lang-py prettyprint-override""><code>t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\nt.shape\n# OP: TensorShape([2, 3])\n</code></pre>\n<p>Now add <code>newaxis</code></p>\n<pre class=""lang-py prettyprint-override""><code>t[..., 1, tf.newaxis]\n# OP: &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n# array([[2.],\n#       [5.]], dtype=float32)&gt;\nt[..., 1, tf.newaxis].shape\n# OP: TensorShape([2, 1])\n</code></pre>\n<p>Without the <code>newaxis</code>, it basically selects all the rows (...) and the first column. The shape would be (2,). Now, with the <code>newaxis</code>, the shape is (2,1).</p>\n', 'IsAccepted': False, 'CreationDate': 1632626692}, {'QuestionId': 2460177, 'AnswerId': 64805642, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/64805642#64805642', 'QuestionTitle': 'Edit Distance in Python', 'Answer': ""<p>Similar to Santoshi's solution above but I made three changes:</p>\n<ol>\n<li>One line initialization instead of five</li>\n<li>No need to define cost alone (just use int(boolean) 0 or 1)</li>\n<li>Instead of double for loop use product, (this last one is only cosmetic, double loop seems unavoidable)</li>\n</ol>\n<pre><code>from itertools import product\n\ndef edit_distance(s1,s2):      \n   d={ **{(i,0):i for i in range(len(s1)+1)},**{(0,j):j for j in range(len(s2)+1)}}\n   for i, j in product(range(1,len(s1)+1), range(1,len(s2)+1)): \n       d[i,j]=min((s1[i-1]!=s2[j-1]) + d[i-1,j-1], d[i-1,j]+1, d[i,j-1]+1)\n   return d[i,j]\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1605190585}, {'QuestionId': 2460177, 'AnswerId': 55906048, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/55906048#55906048', 'QuestionTitle': 'Edit Distance in Python', 'Answer': '<p><a href=""https://docs.python.org/3.6/library/difflib.html"" rel=""noreferrer""><code>difflib</code></a> in the standard library has various utilities for sequence matching, including the <code>get_close_matches</code> method that you could use. It uses an algorithm adapted from Ratcliff and Obershelp.</p>\n<p><strong>From the docs</strong></p>\n<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; from difflib import get_close_matches\n&gt;&gt;&gt; get_close_matches(\'appel\', [\'ape\', \'apple\', \'peach\', \'puppy\'])\n[\'apple\', \'ape\']\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1556550149}, {'QuestionId': 2460177, 'AnswerId': 62136133, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/62136133#62136133', 'QuestionTitle': 'Edit Distance in Python', 'Answer': '<p>Using the <a href=""https://docs.python.org/3.8/library/difflib.html#sequencematcher-objects"" rel=""noreferrer""><code>SequenceMatcher</code></a> from Python built-in <code>difflib</code> is another way of doing it, but (as correctly pointed out in the comments), the result does not match the definition of an edit distance exactly. Bonus: it supports ignoring &quot;junk&quot; parts (e.g. spaces or punctuation).</p>\n<pre><code>from difflib import SequenceMatcher\n\na = \'kitten\'\nb = \'sitting\'\n\nrequired_edits = [\n    code\n    for code in (\n        SequenceMatcher(a=a, b=b, autojunk=False)\n        .get_opcodes()\n    )\n    if code[0] != \'equal\'\n]\nrequired_edits\n# [\n#    # (tag, i1, i2, j1, j2)\n#    (\'replace\', 0, 1, 0, 1), # replace a[0:1]=&quot;k&quot; with b[0:1]=&quot;s&quot;\n#    (\'replace\', 4, 5, 4, 5), # replace a[4:5]=&quot;e&quot; with b[4:5]=&quot;i&quot;\n#    (\'insert\', 6, 6, 6, 7),  # insert b[6:7]=&quot;g&quot; after a[6:6]=&quot;n&quot;\n# ]\n\n\n# the edit distance:\nlen(required_edits)  # == 3\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1591027387}, {'QuestionId': 44264962, 'AnswerId': 67034160, 'URL': 'https://stackoverflow.com/questions/44264962/how-tf-space-to-depth-works-in-tensorflow/67034160#67034160', 'QuestionTitle': 'how tf.space_to_depth() works in tensorflow?', 'Answer': '<p>maybe this one works:</p>\n<pre><code>sudo apt install nvidia-cuda-toolkit\n</code></pre>\n<p>it worked for me.</p>\n', 'IsAccepted': False, 'CreationDate': 1618056199}, {'QuestionId': 2460177, 'AnswerId': 32558749, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/32558749#32558749', 'QuestionTitle': 'Edit Distance in Python', 'Answer': '<p>The thing you are looking at is called an edit distance and here is a <a href=""https://en.wikipedia.org/wiki/Edit_distance"" rel=""noreferrer"">nice explanation on wiki</a>. There are a lot of ways how to define a distance between the two words and the one that you want is called Levenshtein distance and here is a DP (dynamic programming) implementation in python.</p>\n<pre><code>def levenshteinDistance(s1, s2):\n    if len(s1) &gt; len(s2):\n        s1, s2 = s2, s1\n\n    distances = range(len(s1) + 1)\n    for i2, c2 in enumerate(s2):\n        distances_ = [i2+1]\n        for i1, c1 in enumerate(s1):\n            if c1 == c2:\n                distances_.append(distances[i1])\n            else:\n                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n        distances = distances_\n    return distances[-1]\n</code></pre>\n<p>And a <a href=""https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python"" rel=""noreferrer"">couple of more implementations are here</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1442213547}, {'QuestionId': 58054035, 'AnswerId': 58054064, 'URL': 'https://stackoverflow.com/questions/58054035/how-does-tf-losses-absolute-difference-work/58054064#58054064', 'QuestionTitle': 'How does tf.losses.absolute_difference() work?', 'Answer': '<p>From the <a href=""https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/ops/losses/losses_impl.py#L206-L256"" rel=""nofollow noreferrer"">source code</a>, <code>tf.losses.absolute_difference</code> computes the <em>weighted</em> absolute difference of its inputs - and by default, <code>weights=1.0 #broadcasted</code> (which computes the <em>average</em> difference):</p>\n\n<pre class=""lang-py prettyprint-override""><code>absolute_difference(a,b) = weights .* abs(a - b) / sum(weights) # .* = dot product\n                         = [1, 1] .* abs([2, 3] - [1, 1]) / (1 + 1) # lists for simplicity\n                         = [1, 1] .* abs([1, 2]) / 2\n                         = (1 + 2) / 2\n                         = 1.5\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1569194473}, {'QuestionId': 2460177, 'AnswerId': 53599949, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/53599949#53599949', 'QuestionTitle': 'Edit Distance in Python', 'Answer': '<p>You need Minimum Edit Distance for this task.</p>\n\n<p>Following is my version of MED a.k.a Levenshtein Distance.</p>\n\n<pre><code>def MED_character(str1,str2):\n    cost=0\n    len1=len(str1)\n    len2=len(str2)\n\n    #output the length of other string in case the length of any of the string is zero\n    if len1==0:\n        return len2\n    if len2==0:\n        return len1\n\n    accumulator = [[0 for x in range(len2)] for y in range(len1)] #initializing a zero matrix\n\n    # initializing the base cases\n    for i in range(0,len1):\n        accumulator[i][0] = i;\n    for i in range(0,len2):\n        accumulator[0][i] = i;\n\n    # we take the accumulator and iterate through it row by row. \n    for i in range(1,len1):\n        char1=str1[i]\n        for j in range(1,len2):\n            char2=str2[j]\n            cost1=0\n            if char1!=char2:\n                cost1=2 #cost for substitution\n            accumulator[i][j]=min(accumulator[i-1][j]+1, accumulator[i][j-1]+1, accumulator[i-1][j-1] + cost1 )\n\n    cost=accumulator[len1-1][len2-1]\n    return cost\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1543862646}, {'QuestionId': 38020817, 'AnswerId': 38025292, 'URL': 'https://stackoverflow.com/questions/38020817/computing-edit-distance-feed-dict-error/38025292#38025292', 'QuestionTitle': 'Computing Edit Distance (feed_dict error)', 'Answer': '<p><strong>TL;DR:</strong> For the return type of <code>create_sparse_vec()</code>, use <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/sparse_ops.html#SparseTensorValue"" rel=""nofollow noreferrer""><code>tf.SparseTensorValue</code></a> instead of <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/sparse_ops.html#SparseTensor"" rel=""nofollow noreferrer""><code>tf.SparseTensor</code></a>.</p>\n\n<p>The problem here comes from the return type of <code>create_sparse_vec()</code>, which is <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/sparse_ops.html#SparseTensor"" rel=""nofollow noreferrer""><code>tf.SparseTensor</code></a>, and is not understood as a feed <em>value</em> in the call to <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/client.html#Session.run"" rel=""nofollow noreferrer""><code>sess.run()</code></a>.</p>\n\n<p>When you feed a (dense) <code>tf.Tensor</code>, the expected value type is a NumPy array (or certain objects that can be converted to an array). When you feed a <code>tf.SparseTensor</code>, the expected value type is a <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/sparse_ops.html#SparseTensorValue"" rel=""nofollow noreferrer""><code>tf.SparseTensorValue</code></a>, which is similar to a <code>tf.SparseTensor</code> but its <code>indices</code>, <code>values</code>, and <code>shape</code> properties are NumPy arrays (or certain objects that can be converted to arrays, like the lists in your example.</p>\n\n<p>The following code should work:</p>\n\n<pre class=""lang-py prettyprint-override""><code>def create_sparse_vec(word_list):\n    num_words = len(word_list)\n    indices = [[xi, 0, yi] for xi,x in enumerate(word_list) for yi,y in enumerate(x)]\n    chars = list(\'\'.join(word_list))\n    return tf.SparseTensorValue(indices, chars, [num_words,1,1])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1466832304}, {'QuestionId': 45905601, 'AnswerId': 47935405, 'URL': 'https://stackoverflow.com/questions/45905601/how-does-tf-map-fn-work/47935405#47935405', 'QuestionTitle': 'How does tf.map_fn work?', 'Answer': ""<blockquote>\n  <p>For the second: I think the result is [2, -1], because the first time\n  x=np.array([1, 2, 3]) and return 1*2, the second time x=np.array([-1,\n  1, -1]) and return 1*(-1)</p>\n</blockquote>\n\n<pre><code>In [26]: a = np.array([[1, 2, 3], [2, 4, 1], [5, 1, 7]])\nIn [27]: b = np.array([[1, -1, -1], [1, 1, 1], [-1, 1, -1]])\nIn [28]: elems = (a, b)    \nIn [29]: alternate = map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\nIn [30]: alternate.eval()\nOut[30]: \narray([[ 1, -2, -3],\n       [ 2,  4,  1],\n       [-5,  1, -7]])\n</code></pre>\n\n<p>You will see that it is the tensor in 0 dimension of each element in the elems that is applied to the function. </p>\n\n<blockquote>\n  <p>For the third: I think the shape of result is (3,2), because the first\n  time x=1 and return (1,-1), the second time x=2 and return (2,-2), the\n  third time x=3 and return (3,-3).</p>\n</blockquote>\n\n<pre><code>In [36]: elems = np.array([[1, 2, 3], [4, 5, 1], [1, 6, 1]])\nIn [37]: alternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))\nIn [38]: alternates\nOut[38]: \n(&lt;tf.Tensor 'map_6/TensorArrayStack/TensorArrayGatherV3:0' shape=(3, 3) dtype=int64&gt;,\n &lt;tf.Tensor 'map_6/TensorArrayStack_1/TensorArrayGatherV3:0' shape=(3, 3) dtype=int64&gt;)\nIn [39]: alternates[0].eval()\nOut[39]: \narray([[1, 2, 3],\n       [4, 5, 1],\n       [1, 6, 1]])\nIn [40]: alternates[1].eval()\nOut[40]: \narray([[-1, -2, -3],\n       [-4, -5, -1],\n       [-1, -6, -1]])\n</code></pre>\n\n<p>To get the results you expected: </p>\n\n<pre><code>In [8]: elems = np.array([[1], [2], [3]])                                                          \nIn [9]: alternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))\nIn [10]: sess = tf.InteractiveSession()                                                            \nIn [11]: alternates[0].eval()\nOut[11]: \narray([[1],\n       [2],\n       [3]])\n\nIn [12]: alternates[1].eval()                                                                      \nOut[12]: \narray([[-1],\n       [-2],\n       [-3]])\n</code></pre>\n\n<p>May this can help you in understanding the map_fn better. </p>\n"", 'IsAccepted': False, 'CreationDate': 1513912429}, {'QuestionId': 44264962, 'AnswerId': 47731493, 'URL': 'https://stackoverflow.com/questions/44264962/how-tf-space-to-depth-works-in-tensorflow/47731493#47731493', 'QuestionTitle': 'how tf.space_to_depth() works in tensorflow?', 'Answer': '<p>Using split and stack functions along with permute in Pytorch gives us the same result as space_to_depth in tensorflow does. Here is the code in Pytorch.\nAssume that input is in BHWC format.</p>\n\n<p>Based on block_size and input shape, we can caculate the output shape.\nFirst, it splits the input on the ""width"" dimension or dimension #2 by block_size. The result of this operation is an array of length d_width. It\'s just like you cut a cake (by block_size) into d_width pieces. \nThen for each piece, you reshape it so it has correct output height and output depth (channel). Finally, we stack those pieces together and perform a permutation.</p>\n\n<p>Hope it helps. </p>\n\n<pre><code>def space_to_depth(input, block_size)\n    block_size_sq = block_size*block_size\n    (batch_size, s_height, s_width, s_depth) = input.size()\n    d_depth = s_depth * self.block_size_sq\n    d_width = int(s_width / self.block_size)\n    d_height = int(s_height / self.block_size)\n    t_1 = input.split(self.block_size, 2)\n    stack = [t_t.contiguous().view(batch_size, d_height, d_depth) for t_t in t_1]\n    output = torch.stack(stack, 1)\n    output = output.permute(0, 2, 1, 3)\n    return output\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1512842223}, {'QuestionId': 45905601, 'AnswerId': 46154051, 'URL': 'https://stackoverflow.com/questions/45905601/how-does-tf-map-fn-work/46154051#46154051', 'QuestionTitle': 'How does tf.map_fn work?', 'Answer': '<p>Tensorflow map_fn, from the docs, </p>\n\n<blockquote>\n  <p>map on the list of tensors unpacked from elems on dimension 0.</p>\n</blockquote>\n\n<p>in this case, the only axis of the input tensor [1,2,3], or [-1,1,-1]. Operations are thus 1*-1,2*1 and 3*-1, and the results are repacked giving you the tensor shape. </p>\n', 'IsAccepted': False, 'CreationDate': 1505127447}, {'QuestionId': 44264962, 'AnswerId': 44318386, 'URL': 'https://stackoverflow.com/questions/44264962/how-tf-space-to-depth-works-in-tensorflow/44318386#44318386', 'QuestionTitle': 'how tf.space_to_depth() works in tensorflow?', 'Answer': '<p><strong>Conclusion</strong>:  <code>tf.space_to_depth()</code> only outputs a copy of the input tensor where values from the height and width dimensions are moved to the depth dimension. </p>\n\n<p>If you modify your code a little bit, like this</p>\n\n<pre><code>norm = tf.random_normal([1, 2, 2, 1], mean=0, stddev=1)\n\nwith tf.Session() as s:\n    norm = s.run(norm)\n\ntrans = tf.space_to_depth(norm,2)\n\nwith tf.Session() as s:\n    trans = s.run(trans)\n</code></pre>\n\n<p>Then you will have the following results:</p>\n\n<pre><code>Norm\n(1, 2, 2, 1)\n-0.130227\n2.04587\n-0.077691\n-0.112031\nTrans\n(1, 1, 1, 4)\n-0.130227\n2.04587\n-0.077691\n-0.112031\n</code></pre>\n\n<p>Hope this can help you.</p>\n', 'IsAccepted': False, 'CreationDate': 1496358305}, {'QuestionId': 44264962, 'AnswerId': 44269849, 'URL': 'https://stackoverflow.com/questions/44264962/how-tf-space-to-depth-works-in-tensorflow/44269849#44269849', 'QuestionTitle': 'how tf.space_to_depth() works in tensorflow?', 'Answer': '<p>A good reference for PyTorch is the implementation of the PixelShuffle module <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L563"" rel=""nofollow noreferrer"">here</a>. This shows the implementation of something equivalent to Tensorflow\'s <a href=""https://www.tensorflow.org/api_docs/python/tf/depth_to_space"" rel=""nofollow noreferrer"">depth_to_space</a>. Based on that we can implement pixel_shuffle with a scaling factor less than 1 which would be like space_to_depth. E.g., downscale_factor=0.5 is like space_to_depth with block_size=2. </p>\n\n<pre><code>def pixel_shuffle_down(input, downscale_factor):\n    batch_size, channels, in_height, in_width = input.size()\n    out_channels = channels / (downscale_factor ** 2)\n    block_size = 1 / downscale_factor\n\n    out_height = in_height * downscale_factor\n    out_width = in_width * downscale_factor\n\n    input_view = input.contiguous().view(\n        batch_size, channels, out_height, block_size, out_width, block_size)\n\n    shuffle_out = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()\n    return shuffle_out.view(batch_size, out_channels, out_height, out_width)\n</code></pre>\n\n<p>Note: I haven\'t verified this implementation yet and I\'m not sure if it\'s exactly the inverse of pixel_shuffle but this is the basic idea. I\'ve also opened an issue on the PyTorch Github about this <a href=""https://github.com/pytorch/pytorch/issues/1684"" rel=""nofollow noreferrer"">here</a>. In NumPy the equivalent code would use <code>reshape</code>and <code>transpose</code> instead of <code>view</code> and <code>permute</code> respectively. </p>\n', 'IsAccepted': False, 'CreationDate': 1496170453}, {'QuestionId': 44264962, 'AnswerId': 44266640, 'URL': 'https://stackoverflow.com/questions/44264962/how-tf-space-to-depth-works-in-tensorflow/44266640#44266640', 'QuestionTitle': 'how tf.space_to_depth() works in tensorflow?', 'Answer': '<p>This tf.space_to_depth divides your input into blocs and concatenates them.</p>\n\n<p>In your example the input is 38x38x64 (and I guess the block_size is 2). So the function divides your input into 4 (block_size x block_size) and concatenates them which gives your 19x19x256 output.</p>\n\n<p>You just need to divide each of your channel (input) into block_size*block_size patches (each patch has a size of width/block_size x height/block_size) and concatenate all of these patches. Should be pretty straightforward with numpy.</p>\n\n<p>Hope it helps.</p>\n', 'IsAccepted': False, 'CreationDate': 1496159166}, {'QuestionId': 2460177, 'AnswerId': 43157138, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/43157138#43157138', 'QuestionTitle': 'Edit Distance in Python', 'Answer': '<p>Instead of going with Levenshtein distance algo use <em>BK tree</em> or <em>TRIE</em>, as these algorithms have less complexity then edit distance. A good browse over these topic will give a detailed description.</p>\n\n<p>This <a href=""https://nullwords.wordpress.com/2013/03/13/the-bk-tree-a-data-structure-for-spell-checking/"" rel=""nofollow noreferrer"">link</a> will help you more about spell checking.</p>\n', 'IsAccepted': False, 'CreationDate': 1491051258}, {'QuestionId': 42541088, 'AnswerId': 42542428, 'URL': 'https://stackoverflow.com/questions/42541088/difficulty-understanding-tensorflow-computations/42542428#42542428', 'QuestionTitle': 'Difficulty Understanding TensorFlow Computations', 'Answer': ""<p>You don't need to define two function for generating models, you can use <code>tf.name_scope</code>, and pass a model name to the function to use it as a prefix for variable declaration. On the other hand, you defined two variables for distance, first is <code>distance</code> and second is <code>test_distance</code> . But your model will learn from train data to minimize <code>cost</code> which is only related to first distance variable. Therefore, <code>test_distance</code> is never used and the model which is related to it, will never learn anything! Again there is no need for two distance functions. You only need one. When you want to calculate <strong>train distance</strong>, you should <strong>feed it with train data</strong> and when you want to calculate <strong>test distance</strong> you should <strong>feed it with test data</strong>.\nAnyway, if you want second distance to work, you should declare another <code>optimizer</code> for it and also you have to learn it as you have done for first one. Also you should consider the fact that models are learning base on their initial values and training data. Even if you feed both models with exactly same training batches, you can't expect to have exactly similar characteristics models since initial values for weights are different and this could cause falling into different local minimum of error surface. At the end notice that whenever you call <code>neural_network_model1</code> or <code>neural_network_model2</code> you will generate new weights and biases, because <code>tf.Variable</code> is generating new variables for you.</p>\n"", 'IsAccepted': False, 'CreationDate': 1488403878}, {'QuestionId': 42541088, 'AnswerId': 42542266, 'URL': 'https://stackoverflow.com/questions/42541088/difficulty-understanding-tensorflow-computations/42542266#42542266', 'QuestionTitle': 'Difficulty Understanding TensorFlow Computations', 'Answer': '<p>Each time you call the functions <code>neural_network_model1()</code> or <code>neural_network_model2()</code>, you create a new set of variables, so there are four sets of variables in total.</p>\n\n<ul>\n<li>The call to <code>sess.run(tf.global_variables_initializer())</code> initializes all four sets of variables.</li>\n<li><p>When you train in the <code>for</code> loop, you only update the first two sets of variables, created with these lines:</p>\n\n<pre><code>prediction1 = neural_network_model1(x1)\nprediction2 = neural_network_model2(x2)\n</code></pre></li>\n<li><p>When you evaluate with <code>test_distance.eval()</code>, the tensor <code>test_distance</code> depends only on the variables that were created in the last two sets of variables, which were created with these lines:</p>\n\n<pre><code>test_result1 = neural_network_model1(x3)\ntest_result2 = neural_network_model2(x4)\n</code></pre>\n\n<p>These variables were never updated in the training loop, so the evaluation results will be based on the random initial values.</p></li>\n</ul>\n\n<p>TensorFlow does include some code for sharing weights between multiple calls to the same function, using <a href=""https://www.tensorflow.org/api_docs/python/tf/variable_scope"" rel=""nofollow noreferrer""><code>with tf.variable_scope(...):</code></a> blocks. For more information on how to use these, see the <a href=""https://www.tensorflow.org/programmers_guide/variable_scope"" rel=""nofollow noreferrer"">tutorial on variables and sharing</a> on the TensorFlow website.</p>\n', 'IsAccepted': False, 'CreationDate': 1488403237}, {'QuestionId': 37479119, 'AnswerId': 37480271, 'URL': 'https://stackoverflow.com/questions/37479119/doing-pairwise-distance-computation-with-tensorflow/37480271#37480271', 'QuestionTitle': 'Doing pairwise distance computation with TensorFlow', 'Answer': '<h3>Short answer</h3>\n\n<p>I think the simplest way to do that is to <strong>sample the pairs offline</strong> (i.e. outside of the TensorFlow graph).<br>\nYou create <code>tf.placeholder</code> for a batch of pairs along with their labels (positive or negative, i.e. same class or different class), and then you can compute in TensorFlow the corresponding loss.</p>\n\n<hr>\n\n<h3>With the code</h3>\n\n<ol>\n<li>You sample the pairs offline. You sample <code>batch_size</code> pairs of inputs, and output the <code>batch_size</code> left elements of the pairs of shape <code>[batch_size, input_size]</code>. You also output the labels of the pairs (either positive of negative) of shape <code>[batch_size,]</code></li>\n</ol>\n\n<pre class=""lang-py prettyprint-override""><code>pairs_left = np.zeros((batch_size, input_size))\npairs_right = np.zeros((batch_size, input_size))\nlabels = np.zeros((batch_size, 1))  # ex: [[0.], [1.], [1.], [0.]] for batch_size=4\n</code></pre>\n\n<ol start=""2"">\n<li>Then you create Tensorflow placeholders corresponding to these inputs. In your code, you will feed the previous inputs to these placeholders in the <code>feed_dict</code> argument of <code>sess.run()</code></li>\n</ol>\n\n<pre class=""lang-py prettyprint-override""><code>pairs_left_node = tf.placeholder(tf.float32, [batch_size, input_size])\npairs_right_node = tf.placeholder(tf.float32, [batch_size, input_size])\nlabels_node = tf.placeholder(tf.float32, [batch_size, 1])\n</code></pre>\n\n<ol start=""3"">\n<li>Now we can perform a feedforward on the inputs (let\'s say your model is a linear model).</li>\n</ol>\n\n<pre class=""lang-py prettyprint-override""><code>W = ...   # shape [input_size, feature_size]\noutput_left = tf.matmul(pairs_left_node, W)  # shape [batch_size, feature_size]\noutput_right = tf.matmul(pairs_right_node, W)  # shape [batch_size, feature_size]\n</code></pre>\n\n<ol start=""4"">\n<li>Finally we can compute the pairwise loss.\n<a href=""https://i.stack.imgur.com/hz4O4.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hz4O4.png"" alt=""Loss""></a></li>\n</ol>\n\n<pre class=""lang-py prettyprint-override""><code>l2_loss_pairs = tf.reduce_sum(tf.square(output_left - output_right), 1)\npositive_loss = l2_loss_pairs\nnegative_loss = tf.nn.relu(margin - l2_loss_pairs)\nfinal_loss = tf.mul(labels_node, positive_loss) + tf.mul(1. - labels_node, negative_loss)\n</code></pre>\n\n<hr>\n\n<p>And that\'s it ! You can now optimize on this loss, with a good offline sampling.</p>\n', 'IsAccepted': False, 'CreationDate': 1464342598}, {'QuestionId': 37479119, 'AnswerId': 37480205, 'URL': 'https://stackoverflow.com/questions/37479119/doing-pairwise-distance-computation-with-tensorflow/37480205#37480205', 'QuestionTitle': 'Doing pairwise distance computation with TensorFlow', 'Answer': ""<p>1)\nYou should do the pair sampling <strong>before</strong> feeding the data into a session. Label every pair a boolean label, say y = 1 for matched-pair, 0 otherwise. </p>\n\n<p>2) 3) Just calculate both pos/neg terms for every pair, and let the 0-1 label <strong>y</strong> to choose which to add to the loss.</p>\n\n<hr>\n\n<p>First create placeholders, y_ is for boolean labels.</p>\n\n<pre><code>dim = 64\nx1_ = tf.placeholder('float32', shape=(None, dim))\nx2_ = tf.placeholder('float32', shape=(None, dim))\ny_ = tf.placeholder('uint8', shape=[None])   # uint8 for boolean\n</code></pre>\n\n<p>Then the loss tensor can be created by the function.</p>\n\n<pre><code>def loss(x1, x2, y):\n    # Euclidean distance between x1,x2\n    l2diff = tf.sqrt( tf.reduce_sum(tf.square(tf.sub(x1, x2)),\n                                    reduction_indices=1))\n\n    # you can try margin parameters\n    margin = tf.constant(1.)     \n\n    labels = tf.to_float(y)\n\n    match_loss = tf.square(l2diff, 'match_term')\n    mismatch_loss = tf.maximum(0., tf.sub(margin, tf.square(l2diff)), 'mismatch_term')\n\n    # if label is 1, only match_loss will count, otherwise mismatch_loss\n    loss = tf.add(tf.mul(labels, match_loss), \\\n                  tf.mul((1 - labels), mismatch_loss), 'loss_add')\n\n    loss_mean = tf.reduce_mean(loss)\n    return loss_mean\n\nloss_ = loss(x1_, x2_, y_)\n</code></pre>\n\n<p>Then feed your data (random generated for example):</p>\n\n<pre><code>batchsize = 4\nx1 = np.random.rand(batchsize, dim)\nx2 = np.random.rand(batchsize, dim)\ny = np.array([0,1,1,0])\n\nl = sess.run(loss_, feed_dict={x1_:x1, x2_:x2, y_:y})\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1464342419}, {'QuestionId': 2460177, 'AnswerId': 19928962, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/19928962#19928962', 'QuestionTitle': 'Edit Distance in Python', 'Answer': '<pre><code>#this calculates edit distance not levenstein edit distance\nword1=""rice""\n\nword2=""ice""\n\nlen_1=len(word1)\n\nlen_2=len(word2)\n\nx =[[0]*(len_2+1) for _ in range(len_1+1)]#the matrix whose last element -&gt;edit distance\n\nfor i in range(0,len_1+1): #initialization of base case values\n\n    x[i][0]=i\nfor j in range(0,len_2+1):\n\n    x[0][j]=j\nfor i in range (1,len_1+1):\n\n    for j in range(1,len_2+1):\n\n        if word1[i-1]==word2[j-1]:\n            x[i][j] = x[i-1][j-1] \n\n        else :\n            x[i][j]= min(x[i][j-1],x[i-1][j],x[i-1][j-1])+1\n\nprint x[i][j]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1384258609}, {'QuestionId': 31024077, 'AnswerId': 31024178, 'URL': 'https://stackoverflow.com/questions/31024077/what-does-transposition-mean-in-edit-distance-algorithm/31024178#31024178', 'QuestionTitle': 'What does &quot;transposition&quot; mean in edit distance algorithm?', 'Answer': ""<p>Convert GAOL to GOAL:<br/>\nWithout transpositions you must do two edits: delete 'A', insert 'A' after 'O'.<br/>\nWith transpositions you must do one edit: transpose 'A' and 'O'</p>\n"", 'IsAccepted': True, 'CreationDate': 1435141680}, {'QuestionId': 13979390, 'AnswerId': 24353233, 'URL': 'https://stackoverflow.com/questions/13979390/edit-distance-explanation/24353233#24353233', 'QuestionTitle': 'Edit distance explanation', 'Answer': '<p>The code could be difficult to understand but here are some tips:</p>\n\n<ol>\n<li><p>Edit distance between any pair of characters in two string is at least edit distance of all the character pairs which has been compared before them. e.g when comparing two string <strong>abc</strong> and <strong>ade</strong> when you reach at stage comparing <strong>c</strong> and <strong>e</strong> you are sure that the edit distance of the two strings is at least going to be the minimum edit distance found so far (1 in this case)</p></li>\n<li><p>If two characters being compared are not equal, edit distance is going to increase by one, signifying replacement. So if you know the edit distance of the string before the comparison, you can add 1 to it depending upon whether the characters are equal or not.</p></li>\n</ol>\n\n<p>Armed with these two facts,</p>\n\n<p>position[i, j] is the edit distance for consideration</p>\n\n<p>position[i-1,j] would be the total edit distance of the string so far if ith row element did not exist (signify insertion cost)</p>\n\n<p>position[i,j-1] would be the total edit distance of the string so far if jth column element was removed (signify deletion cost)</p>\n\n<p>position[i-i,j-1] would be the min edit distance calculated so far for all previous elements</p>\n\n<p>position[i,j] is then calculated by taking minimum of all three possibility and adding current decision.</p>\n', 'IsAccepted': False, 'CreationDate': 1403454261}, {'QuestionId': 2460177, 'AnswerId': 24172422, 'URL': 'https://stackoverflow.com/questions/2460177/edit-distance-in-python/24172422#24172422', 'QuestionTitle': 'Edit Distance in Python', 'Answer': '<p>Here is my version for Levenshtein distance</p>\n\n<pre>\ndef edit_distance(s1, s2):\n    m=len(s1)+1\n    n=len(s2)+1\n\n    tbl = {}\n    for i in range(m): tbl[i,0]=i\n    for j in range(n): tbl[0,j]=j\n    for i in range(1, m):\n        for j in range(1, n):\n            cost = 0 if s1[i-1] == s2[j-1] else 1\n            tbl[i,j] = min(tbl[i, j-1]+1, tbl[i-1, j]+1, tbl[i-1, j-1]+cost)\n\n    return tbl[i,j]\n\nprint(edit_distance(""Helloworld"", ""HalloWorld""))\n</pre>\n', 'IsAccepted': False, 'CreationDate': 1402520161}, {'QuestionId': 13979390, 'AnswerId': 13981019, 'URL': 'https://stackoverflow.com/questions/13979390/edit-distance-explanation/13981019#13981019', 'QuestionTitle': 'Edit distance explanation', 'Answer': '<p>Your code is calculating the <a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""noreferrer"">Levenshtein distance</a> using <a href=""http://en.wikipedia.org/wiki/Dynamic_programming"" rel=""noreferrer"">dynamic programming</a>.  </p>\n\n<p>The array <code>d</code> will ultimately contain the solution to various sub-problems, where <code>d[i][j]</code> is the distance between the first <code>i</code> letters of the first word and the first <code>j</code> letters of the second.  There is a relationship between <code>d[i][j]</code> and the entries <code>d[i-1][j]</code>, <code>d[i][j-1]</code> and <code>d[i-1][j-1]</code>.  The algorithm calculates the entries of the table in such a way that required sub-problems have already been calculated (this is the dynamic programming part).</p>\n', 'IsAccepted': False, 'CreationDate': 1356039968}, {'QuestionId': 13979390, 'AnswerId': 13980874, 'URL': 'https://stackoverflow.com/questions/13979390/edit-distance-explanation/13980874#13980874', 'QuestionTitle': 'Edit distance explanation', 'Answer': ""<p>The matrix contains [at the end] the edit distances between all the prefixes of <code>word1</code> and all the prefixes of <code>word2</code>.</p>\n\n<pre><code>d[i][j] = edit distance between word1[0..(i-1)] and word2[0..(j-1)]\n</code></pre>\n\n<p>You are interested in <code>d[l1][l2]</code>. In general, to compute <code>d[i][j]</code>, you need to look at the three smaller neighbours <code>d[i-1][j]</code>, <code>d[i-1][j-1]</code> and <code>d[i][j-1]</code>. So transitively, <code>d[i][j]</code> requires all entries where at least one coordinate is smaller than <code>i</code> resp<code>j</code> (and the other not larger). The exception is when two characters <code>word1[i-1]</code> and <code>word2[j-1]</code> are equal, in which case you only need the diagonal smaller neighbour.</p>\n\n<p>If you first fill the matrix with <code>-1</code> to indicate the corresponding edit distance between the prefixes has not been evaluated, and recursively calculate <code>d[l1][l2]</code>, using the cached value of a needed <code>d[i][j]</code> if it has already been computed, computing it recursively and storing its value if not, some regions of the matrix may remain untouched. Possibly large regions if there are many pairs of equal characters [only the diagonal will be evaluated if the two words are equal], only small regions if any if there are few pairs of equal characters.</p>\n\n<p>In the generic case, most of the matrix is needed to compute <code>d[l1][l2]</code>, so then it is faster to compute the matrix completely using the simple algorithm than to recur and compute only the actually required values.</p>\n\n<p>If you don't store the values for the shorter prefixes, since they are transitively required to compute <code>d[i][j]</code>, they would have to be recalculated for each way <code>d[i-a][j-b]</code> is reached from <code>d[i][j]</code>. Since <code>d[i-a][j-b]</code> can be reached in many ways from <code>d[i][j]</code>, that would cause a <strong>lot</strong> of recalculation leading to a grossly inefficient algorithm in general.</p>\n\n<p>Since the computation for each row only uses the previous row, you could do with just two arrays of length <code>min{l1, l2} + 1</code> to save some memory, but unless the words are really long, it doesn't make a big difference, and the code is simpler with the full array.</p>\n"", 'IsAccepted': True, 'CreationDate': 1356039432}]","{51612489, 41283115}","['<p>hypothesis in dense form looks like this</p>\n\n<pre><code>[[[\'a\']],\n [[\'b\']]] # (2, 1, 1)\n</code></pre>\n\n<p>truth is this</p>\n\n<pre><code>[[[],[\'a\']],\n [[\'b\', \'c\'], [\'a\']]] # (2, 2, 2)\n</code></pre>\n\n<p>We are trying to find the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a> between hypothesis and truth value.\nSo, here is what is happening:</p>\n\n<p>at (0,0,0) - how far is [\'a\'] in hypothesis from [] - no truth in that position so can\'t calculate distance</p>\n\n<p>at (0,0,1) - since there is nothing in that position at hypothesis we return 1. Unlike the case above, the distance is 1 because in theory the hypothesis can be made same as truth by inserting one character (See Levenshtein distance calculations)</p>\n\n<p>at (1,0,0) - how far is [\'b\'] in hyp from [\'b\', \'c\'] in truth. This is again 1, since we can insert a character to make hyp same as truth. But, we selected to normalize the output distance. So we divide by length of truth segment, which is 2. So you get 0.5</p>\n\n<p>at (1,0,1) - how far is [] in hyp from [\'a\'], since there is nothing in that position at hyp, we return 1</p>\n\n<p>Output is (2,2) because rank of hyp and truth is 3. The function returns tensor with rank (rank-1) </p>\n\n<p>It helps by imagining what we are trying to do here. You have 2 sequences in hypothesis and 2 sequences in the truth. So your output score will be such that you get scores for each position in each sequence.</p>\n\n<p>Here is an example where we try to match 4 hypotheses to a truth value. I think you have to do this for each truth sequence for the use case that you describe in your comment - let me know if you find something more efficient :-)</p>\n\n<pre><code>import tensorflow as tf\n\nhypothesis = tf.SparseTensor(\n            [[0, 0, 0],\n             [1, 0, 0],\n             [2, 0, 0],\n             [3, 0, 0]],\n             [""a"", ""b"", ""c"", ""d""],\n            (4, 1, 1))\n\ntruth = tf.SparseTensor([[0, 0, 0], [0, 0, 1], [0, 1, 0]], [""b"", ""c"", ""a""], (1,2,2))\nnum_hyp = 4\ntruth = tf.sparse_concat(0, [truth] * num_hyp)\n\nd = tf.edit_distance(hypothesis, truth)\n\nwith tf.Session() as sess:\n    print(sess.run(d))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[[1.  1. ]\n [0.5 1. ]\n [0.5 1. ]\n [1.  1. ]]\n</code></pre>\n', 'For example, each CIFAR-10 image is labeled with one and only\n  one label: an image can be a dog or a truck, but not both.</p>\n</blockquote>\n\n<p>As such, with sparse functions, the dimensions of <code>logits</code> and <code>labels</code> are not the same: <code>labels</code> contain one number per example, whereas <code>logits</code> the number of classes per example, denoting probabilities.</p>\n', '<p>hypothesis in dense form looks like this</p>\n\n<pre><code>[[[\'a\']],\n [[\'b\']]] # (2, 1, 1)\n</code></pre>\n\n<p>truth is this</p>\n\n<pre><code>[[[],[\'a\']],\n [[\'b\', \'c\'], [\'a\']]] # (2, 2, 2)\n</code></pre>\n\n<p>We are trying to find the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a> between hypothesis and truth value. So, here is what is happening:</p>\n\n<p>at (0,0,0) - how far is [\'a\'] in hypothesis from [] - no truth in that position so can\'t calculate distance</p>\n\n<p>at (0,0,1) - since there is nothing in that position at hypothesis we return 1. Unlike the case above, the distance is 1 because in theory the hypothesis can be made same as truth by inserting one character (See Levenshtein distance calculations)</p>\n\n<p>at (1,0,0) - how far is [\'b\'] in hyp from [\'b\', \'c\'] in truth. This is again 1, since we can insert a character to make hyp same as truth. But, we selected to normalize the output distance. So we divide by length of truth segment, which is 2. So you get 0.5</p>\n\n<p>at (1,0,1) - how far is [] in hyp from [\'a\'], since there is nothing in that position at hyp, we return 1</p>\n\n<p>Output is (2,2) because rank of hyp and truth is 3. The function returns tensor with rank (rank-1) </p>\n\n<p>It helps by imagining what we are trying to do here. You have 2 sequences in hypothesis and 2 sequences in the truth. So your output score will be such that you get scores for each position in each sequence.</p>\n\n<p>Here is an example where we try to match 4 hypotheses to a truth value. I think you have to do this for each truth sequence for the use case that you describe in your comment - let me know if you find something more efficient :-)</p>\n\n<pre><code>import tensorflow as tf\n\nhypothesis = tf.SparseTensor(\n            [[0, 0, 0],\n             [1, 0, 0],\n             [2, 0, 0],\n             [3, 0, 0]],\n             [""a"", ""b"", ""c"", ""d""],\n            (4, 1, 1))\n\ntruth = tf.SparseTensor([[0, 0, 0], [0, 0, 1], [0, 1, 0]], [""b"", ""c"", ""a""], (1,2,2))\nnum_hyp = 4\ntruth = tf.sparse_concat(0, [truth] * num_hyp)\n\nd = tf.edit_distance(hypothesis, truth)\n\nwith tf.Session() as sess:\n    print(sess.run(d))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[[1. 1.']","{'https://stackoverflow.com/questions/41283115/tensorflow-difference-between-tf-nn-softmax-cross-entropy-with-logits-and-tf-nn/41285054#41285054', 'https://stackoverflow.com/questions/51612489/tensorflow-tf-edit-distance-explanation-required/51615875#51615875'}",,0.2557369292449667,0.08926008203800444
2,54047604,tf.custom_gradient,Documentation Replication on Other Examples,How to assign custom gradient to TensorFlow op with multiple inputs,"<p>I'm trying to use TensorFlow's <code>@tf.custom_gradient</code> functionality to assign a custom gradient to a function with multiple inputs.  I can put together a working setup for only one input, but not for two or more.</p>

<p>I've based my code on <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">TensorFlow's custom_gradient documentation</a>, which works just fine for one input, as in this example:</p>

<pre><code>import tensorflow as tf
import os

# Suppress Tensorflow startup info
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

# Custom gradient decorator on a function,
# as described in documentation
@tf.custom_gradient
def my_identity(x):

    # The custom gradient
    def grad(dy):
        return dy

    # Return the result AND the gradient
    return tf.identity(x), grad

# Make a variable, run it through the custom op
x = tf.get_variable('x', initializer=1.)
y = my_identity(x)

# Calculate loss, make an optimizer, train the variable
loss = tf.abs(y)
opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)
train = opt.minimize(loss)

# Start a TensorFlow session, initialize variables, train
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(train)
</code></pre>

<p>This example runs silently, then closes.  No issues, no errors.  The variable optimizes as expected.  However, in my application, I need to do such a calculation with multiple inputs, so something of this form:</p>

<pre><code>@tf.custom_gradient
def my_identity(x, z):

    def grad(dy):
        return dy

    return tf.identity(x*z), grad
</code></pre>

<p>Running this in place of the example (and adding another variable input to the call of <code>my_identify</code>) results in the following error output.  Best as I can tell, the last parts of the error are from the dynamic generation of the op -- the information format matches the C++ formatting required in the op establishment (though that's about all I know about it).</p>

<pre><code>Traceback (most recent call last):
  File ""testing.py"", line 27, in &lt;module&gt;
    train = opt.minimize(loss)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 400, in minimize
    grad_loss=grad_loss)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/training/optimizer.py"", line 519, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 630, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 821, in _GradientsHelper
    _VerifyGeneratedGradients(in_grads, op)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/gradients_impl.py"", line 323, in _VerifyGeneratedGradients
    ""inputs %d"" % (len(grads), op.node_def, len(op.inputs)))
ValueError: Num gradients 2 generated for op name: ""IdentityN""
op: ""IdentityN""
input: ""Identity""
input: ""x/read""
input: ""y/read""
attr {
  key: ""T""
  value {
    list {
      type: DT_FLOAT
      type: DT_FLOAT
      type: DT_FLOAT
    }
  }
}
attr {
  key: ""_gradient_op_type""
  value {
    s: ""CustomGradient-9""
  }
}
 do not match num inputs 3
</code></pre>

<p>Based on other custom gradient options, I surmised that the issue was a lack of supplied gradient for the second input argument.  So, I changed my function to this:</p>

<pre><code>@tf.custom_gradient
def my_identity(x, z):

    def grad(dy):
        return dy

    return tf.identity(x*z), grad, grad
</code></pre>

<p>This results in the following more familiar error:</p>

<pre><code>Traceback (most recent call last):
  File ""testing.py"", line 22, in &lt;module&gt;
    y = my_identity(x, z)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 111, in decorated
    return _graph_mode_decorator(f, *args, **kwargs)
  File ""/usr/lib/python3/dist-packages/tensorflow/python/ops/custom_gradient.py"", line 132, in _graph_mode_decorator
    result, grad_fn = f(*args)
ValueError: too many values to unpack (expected 2)
</code></pre>

<p>The <code>@custom_gradient</code> decorator is only identifying the last returned element as a gradient.  So, I tried putting the two gradients into a tuple as <code>(grad, grad)</code> such that there would only be ""two"" outputs for the function.  TensorFlow rejected this too, this time because it can't call a tuple like it would a Tensor -- entirely reasonable, in hindsight.</p>

<p>I've fussed around with the example some more, but to no avail.  No matter what I try, I can't get the custom-defined gradient to deal with multiple inputs.  I'm hoping that somebody with more knowledge than I regarding custom ops and gradients will have a better idea on this -- thanks in advance for the help!</p>
","<p>If we use multiple variables as input, the number of gradients return from ""grad"" function should be equals to number of input variables, though we maybe don't care about some of them. </p>

<p>For example:</p>

<pre><code>@tf.custom_gradient
def my_multiple(x,z):

def grad(dy):
    # return two gradients, one for 'x' and one for 'z'
    return (dy*z, dy*x)

return tf.identity(x*z), grad
</code></pre>

<p>Note that the second output of ""my_multiple"" is a function, not a gradient tensor. </p>
","{43256517, 50030026, 45815531, 44564397, 50341647, 52604879, 48013711, 51836242, 40060047, 39048984}","[{'QuestionId': 51836242, 'AnswerId': 55133321, 'URL': 'https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs/55133321#55133321', 'QuestionTitle': 'tf.custom_gradient with multiple inputs', 'Answer': ""<p>I ran into a similar problem yesterday and found this post, and I believe I know what you are running into. Problem is that while using @tf.custom_gradient, the function that it decorates can have multiple inputs (instead of a list of tensors). Look at the following code(note that it's just a test code with no actual meaning):</p>\n\n<pre><code>@tf.custom_gradient\ndef loop1(x,a):\n    def grad(dy):\n        return dy*3,dy*2\n    n = tf.multiply(x,a)\n    return n,grad\n</code></pre>\n\n<p>By using two inputs x and a, you have to return two gradients respectively in the grad function. dy*3 corresponds to the gradient of x and dy*2 corresponds to the gradient of a.</p>\n\n<p>I think in this function the documents make people very confusing, but you can still use multiple inputs, just make sure that you also have the same number of gradients, or else you will run into errors.</p>\n"", 'IsAccepted': False, 'CreationDate': 1552443022}, {'QuestionId': 43256517, 'AnswerId': 54026867, 'URL': 'https://stackoverflow.com/questions/43256517/how-to-register-a-custom-gradient-for-a-operation-composed-of-tf-operations/54026867#54026867', 'QuestionTitle': 'How to register a custom gradient for a operation composed of tf operations', 'Answer': '<p>See <a href=""https://stackoverflow.com/a/54015702/3623290"">this</a> answer (note that different questions might be satisfactorily answered by the same answer).</p>\n', 'IsAccepted': False, 'CreationDate': 1546535738}, {'QuestionId': 52604879, 'AnswerId': 52605552, 'URL': 'https://stackoverflow.com/questions/52604879/how-excute-custom-gradient-with-tf-multiply/52605552#52605552', 'QuestionTitle': 'How excute custom gradient with tf.multiply?', 'Answer': '<p>The TensorFlow op name for <code>tf.multiply</code> is just <code>Mul</code>, not <code>Multiply</code>. Also, <code>tf.multiply</code> has two inputs, so its gradients should have two outputs. So your code could look something like this:</p>\n\n<pre><code>import tensorflow as tf\n\n@tf.RegisterGradient(""MyopGrad"")\ndef frop_grad(op, grad):\n    x = op.inputs[0]\n    y = op.inputs[1]\n    return 1000.0 * x, 1000.0 * y\n\ninput = tf.Variable([4.0], dtype=tf.float32)\nx = tf.constant(5.0)\ng = tf.get_default_graph()\n\nwith g.gradient_override_map({""Mul"": ""MyopGrad""}): \n  output1 = tf.multiply(input, x , name = \'multiply\')\ngrad1 = tf.gradients(output1, input)\n\n# output without gradient clipping in the backwards pass for comparison:\noutput1_ori = tf.multiply(input , x)\ngrad1_ori = tf.gradients(output1_ori, input)\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  print(""with custom:"", sess.run(grad1)[0])\n  print(""without custom:"", sess.run(grad1_ori)[0])\n</code></pre>\n\n<p>Output:</p>\n\n<pre class=""lang-none prettyprint-override""><code>with custom: [4000.]\nwithout custom: [5.]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1538472213}, {'QuestionId': 51836242, 'AnswerId': 51836354, 'URL': 'https://stackoverflow.com/questions/51836242/tf-custom-gradient-with-multiple-inputs/51836354#51836354', 'QuestionTitle': 'tf.custom_gradient with multiple inputs', 'Answer': '<p>I believe you need something like this a tf Graph input:+\nn_input is the input number</p>\n\n<pre><code>x = tf.placeholder(""float"", [None, n_input])\ny = tf.placeholder(""float"", [None])\n</code></pre>\n\n<p>Does this answer your question ?</p>\n', 'IsAccepted': False, 'CreationDate': 1534233120}, {'QuestionId': 43256517, 'AnswerId': 51746951, 'URL': 'https://stackoverflow.com/questions/43256517/how-to-register-a-custom-gradient-for-a-operation-composed-of-tf-operations/51746951#51746951', 'QuestionTitle': 'How to register a custom gradient for a operation composed of tf operations', 'Answer': '<p>If you want to use <code>tf.RegisterGradient()</code> for this purpose, I\'m not sure if it is a proper solution. Because in the official documents <a href=""https://www.tensorflow.org/api_docs/python/tf/RegisterGradient"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/RegisterGradient</a> , it says:</p>\n\n<blockquote>\n  <p>This decorator is only used when defining a new op type.</p>\n</blockquote>\n\n<p>which means you need to define a new op written in C++ or wrapped  in <code>py_func</code>. I\'m not totally sure if it can apply on the group of ""tf op"" you said. </p>\n\n<hr>\n\n<p>However, You can also refer to the ""trick"" methods mentioned in this thread:</p>\n\n<p><a href=""https://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph"">How Can I Define Only the Gradient for a Tensorflow Subgraph?</a></p>\n\n<p>where you could combine <code>tf.stop_gradient()</code> and <code>tfgradient_override_map()</code> together to re-define the gradients for groups of operations</p>\n', 'IsAccepted': False, 'CreationDate': 1533731835}, {'QuestionId': 39048984, 'AnswerId': 39984513, 'URL': 'https://stackoverflow.com/questions/39048984/tensorflow-how-to-write-op-with-gradient-in-python/39984513#39984513', 'QuestionTitle': 'Tensorflow: How to write op with gradient in python?', 'Answer': '<p>Yes, as mentionned in @Yaroslav\'s answer, it is possible and the key is the links he references: <a href=""https://github.com/tensorflow/tensorflow/issues/1095"" rel=""nofollow noreferrer"">here</a> and <a href=""https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342"" rel=""nofollow noreferrer"">here</a>. I want to elaborate on this answer by giving a concret example. </p>\n\n<p><strong>Modulo opperation:</strong> Let\'s implement the element-wise modulo operation in tensorflow (it already exists but its gradient is not defined, but for the example we will implement it from scratch). </p>\n\n<p><strong>Numpy function:</strong> The first step is to define the opperation we want for numpy arrays. The element-wise modulo opperation is already implemented in numpy so it is easy:</p>\n\n<pre><code>import numpy as np\ndef np_mod(x,y):\n    return (x % y).astype(np.float32)\n</code></pre>\n\n<p>The reason for the <code>.astype(np.float32)</code> is because by default tensorflow takes float32 types and if you give it float64 (the numpy default) it will complain. </p>\n\n<p><strong>Gradient Function:</strong> Next we need to define the gradient function for our opperation for each input of the opperation as tensorflow function. The function needs to take a very specific form. It need to take the tensorflow representation of the opperation <code>op</code> and the gradient of the output <code>grad</code> and say how to propagate the gradients. In our case, the gradients of the <code>mod</code> opperation are easy, the derivative is 1 with respect to the first argument and \n<a href=""https://i.stack.imgur.com/aXvZe.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aXvZe.gif"" alt=""enter image description here""></a> with respect to the second (almost everywhere, and infinite at a finite number of spots, but let\'s ignore that, see <a href=""https://math.stackexchange.com/questions/1849280/derivative-of-remainder-function-wrt-denominator"">https://math.stackexchange.com/questions/1849280/derivative-of-remainder-function-wrt-denominator</a> for details). So we have</p>\n\n<pre><code>def modgrad(op, grad):\n    x = op.inputs[0] # the first argument (normally you need those to calculate the gradient, like the gradient of x^2 is 2x. )\n    y = op.inputs[1] # the second argument\n\n    return grad * 1, grad * tf.neg(tf.floordiv(x, y)) #the propagated gradient with respect to the first and second argument respectively\n</code></pre>\n\n<p>The grad function needs to return an n-tuple where n is the number of arguments of the operation. Notice that we need to return tensorflow functions of the input.</p>\n\n<p><strong>Making a TF function with gradients:</strong> As explained in the sources mentioned above, there is a hack to define gradients of a function using <code>tf.RegisterGradient</code> <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/framework.html#RegisterGradient"" rel=""nofollow noreferrer"">[doc]</a> and <code>tf.Graph.gradient_override_map</code> <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/framework.html"" rel=""nofollow noreferrer"">[doc]</a>. </p>\n\n<p>Copying the code from <a href=""https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342"" rel=""nofollow noreferrer"">harpone</a> we can modify the <code>tf.py_func</code> function to make it define the gradient at the same time:</p>\n\n<pre><code>import tensorflow as tf\n\ndef py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n\n    # Need to generate a unique name to avoid duplicates:\n    rnd_name = \'PyFuncGrad\' + str(np.random.randint(0, 1E+8))\n\n    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n    g = tf.get_default_graph()\n    with g.gradient_override_map({""PyFunc"": rnd_name}):\n        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n</code></pre>\n\n<p>The <code>stateful</code> option is to tell tensorflow whether the function always gives the same output for the same input (stateful = False) in which case tensorflow can simply the tensorflow graph, this is our case and will probably be the case in most situations.</p>\n\n<p><strong>Combining it all together:</strong> Now that we have all the pieces, we can combine them all together:</p>\n\n<pre><code>from tensorflow.python.framework import ops\n\ndef tf_mod(x,y, name=None):\n\n    with ops.op_scope([x,y], name, ""mod"") as name:\n        z = py_func(np_mod,\n                        [x,y],\n                        [tf.float32],\n                        name=name,\n                        grad=modgrad)  # &lt;-- here\'s the call to the gradient\n        return z[0]\n</code></pre>\n\n<p><code>tf.py_func</code> acts on lists of tensors (and returns a list of tensors), that is why we have <code>[x,y]</code> (and return <code>z[0]</code>). \nAnd now we are done. And we can test it.</p>\n\n<p><strong>Test:</strong></p>\n\n<pre><code>with tf.Session() as sess:\n\n    x = tf.constant([0.3,0.7,1.2,1.7])\n    y = tf.constant([0.2,0.5,1.0,2.9])\n    z = tf_mod(x,y)\n    gr = tf.gradients(z, [x,y])\n    tf.initialize_all_variables().run()\n\n    print(x.eval(), y.eval(),z.eval(), gr[0].eval(), gr[1].eval())\n</code></pre>\n\n<blockquote>\n  <p>[ 0.30000001  0.69999999  1.20000005  1.70000005] [ 0.2 0.5 1. 2.9000001] [ 0.10000001  0.19999999  0.20000005  1.70000005] [ 1.  1.  1.  1.] [ -1.  -1.  -1.  0.]</p>\n</blockquote>\n\n<p><strong>Success!</strong></p>\n', 'IsAccepted': True, 'CreationDate': 1476211592}, {'QuestionId': 50341647, 'AnswerId': 50357226, 'URL': 'https://stackoverflow.com/questions/50341647/apply-tensorflow-gradients-to-specific-inputs/50357226#50357226', 'QuestionTitle': 'Apply tensorflow gradients to specific inputs', 'Answer': '<p>In case anyone is wanting a full solution regarding @DomJacks answer:</p>\n\n<pre><code>from keras.models import Model\nfrom keras.layers import Dense, Input, Concatenate\nimport tensorflow as tf\nimport keras.backend as K\nimport numpy as np\n\nnum_features = 100\ninput_ = Input(shape=(num_features,))\noutput_ = Dense(10)(input_)\n\nmodel = Model(input_,output_)\n\n# input range of interest\nx_range = [50,70]\n# output indices of interest\ny_indices = [2,3,4]\n\n# If model is saved, you can load using: \n#model = keras.models.load_model(filepath)\n# then grab the input:\ninput_ = model.input\n\n# Split inputs\nuninteresting, interesting, more_uninteresting = tf.split(input_, [x_range[0], \n                                                                   x_range[1]-x_range[0], \n                                                                   num_features-x_range[1]], \n                                                          axis=1)\n# Create new process\ninputs = Concatenate()([uninteresting, interesting, more_uninteresting])\ny = model(inputs)\ny_list = tf.unstack(y[0])\nx = np.random.random((1,num_features))\n\n# Create Jacobian matrix\njacobian_matrix = []\nfor i in y_indices:\n    J = tf.gradients(y_list[i], interesting)\n    jacobian_func = K.function([input_, K.learning_phase()], J)\n    jac = jacobian_func([x, False])[0][0]\n    jacobian_matrix.append(jac)\njacobian_matrix = np.array(jacobian_matrix)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1526409873}, {'QuestionId': 50341647, 'AnswerId': 50342940, 'URL': 'https://stackoverflow.com/questions/50341647/apply-tensorflow-gradients-to-specific-inputs/50342940#50342940', 'QuestionTitle': 'Apply tensorflow gradients to specific inputs', 'Answer': ""<p>The issue is with the line <code>J = tf.gradients(y_list[i], x_list[j])</code>. <code>x_list[j]</code> was derived from <code>model.input[0]</code>, but there's no directed path from <code>x_list[j]</code> to <code>model.output[0]</code>. You need to either unstack the model input, restack then run the model, or create the derivative with respect to the entire input and just select the <code>j</code>th row from there.</p>\n\n<p>First way:</p>\n\n<pre><code>inputs = tf.keras.Inputs((100,))\nuninteresting, interesting, more_uninteresting = tf.split(inputs, [50, 10, 40], axis=1)\ninputs = tf.concat([uninteresting, interesting, more_uninteresting], axis=1)\nmodel = Model(inputs)\n...\nJ, = tf.gradients(y_list[i], interesting)\n</code></pre>\n\n<p>Second way:</p>\n\n<pre><code>J, = tf.gradients(y_list[i], model.input[0])\nJ = J[:, 50:60]\n</code></pre>\n\n<p>Having said that, this is still going to be slow for a large number of <code>y</code> indices, so I'd strongly encourage you to be absolutely sure you need the Jacobian itself (and not, for example, the result of a Jacobian-vector product).</p>\n"", 'IsAccepted': True, 'CreationDate': 1526362424}, {'QuestionId': 50030026, 'AnswerId': 50031716, 'URL': 'https://stackoverflow.com/questions/50030026/how-to-provide-custom-gradient-in-tensorflow/50031716#50031716', 'QuestionTitle': 'How to provide custom gradient in TensorFlow', 'Answer': '<p>Since your function <code>f2()</code> has two inputs, you have to provide a gradient to flow back to each of them. The error you see:</p>\n\n<blockquote>\n  <p>Num gradients 2 generated for op name: ""IdentityN"" [...]  do not match num inputs 3</p>\n</blockquote>\n\n<p>is admittedly quite cryptic, though. Supposing you never want to calculate d<strong><em>y</em></strong>/d<strong><em>A</em></strong>, you can just return None, dzByDx. The code below (tested):</p>\n\n<pre><code>import tensorflow as tf\n\n#I want to write custom gradient for this function f1\ndef f1(A,x):\n    y=tf.matmul(A,x,name=\'y\')\n    return y\n\n#for y= Ax, the derivative is: dy/dx= transpose(A)\n@tf.custom_gradient\ndef f2(A,x):\n    y=f1(A,x)\n    def grad(dzByDy): # dz/dy = 2y reaches here correctly.\n        dzByDx=tf.matmul(A,dzByDy,transpose_a=True) \n        return None, dzByDx\n    return y,grad\n\nx= tf.constant([[1.],[0.]],name=\'x\')\nA= tf.constant([ [1., 2.], [3., 4.]],name=\'A\')\n\n#y=f1(A,x) # This works as desired\ny=f2(A,x) #This line gives Error\n\nz=tf.reduce_sum(y*y,name=\'z\')\n\ng=tf.gradients(ys=z,xs=x)\n\nwith tf.Session() as sess:\n    print sess.run( g )\n</code></pre>\n\n<p>outputs:</p>\n\n<blockquote>\n  <p>[array([[20.],\n         [28.]], dtype=float32)]</p>\n</blockquote>\n\n<p>as desired.</p>\n', 'IsAccepted': True, 'CreationDate': 1524692303}, {'QuestionId': 45815531, 'AnswerId': 49312789, 'URL': 'https://stackoverflow.com/questions/45815531/tensorflow-op-with-two-inputs-return-one-of-the-two-and-override-gradient/49312789#49312789', 'QuestionTitle': 'Tensorflow op with two inputs, return one of the two and override gradient', 'Answer': '<p>I am not sure how you managed to solve your problem but the names \'op_name\' and \'some_name\' in above solution would not show on the graph. So you will not be able to use gradient_override_map({""op_name"": ""SynthGrad""}).</p>\n\n<p>One possible solution:\nIf you have a custom tensorflow op x=f(a,b) in forwardpass but you want that to behave as g(a,b) in backwardpass, you can do something like this:</p>\n\n<p>t=g(a,b)\nout=t+tf.stop_gradient(f(a,b)-t)</p>\n\n<p>However, you need to define g(a,b) in C++ as a dummy/identity operator with a name. Later, you can use gradient_override_map.</p>\n', 'IsAccepted': False, 'CreationDate': 1521171440}, {'QuestionId': 48013711, 'AnswerId': 48014285, 'URL': 'https://stackoverflow.com/questions/48013711/using-op-inputs-when-defining-custom-gradients-in-tensorflow/48014285#48014285', 'QuestionTitle': 'Using op inputs when defining custom gradients in TensorFlow', 'Answer': ""<p>There is no problem with your code:</p>\n\n<p>Let's first do the forward pass:</p>\n\n<p><code>var_foo = 5</code>  -> <code>bar = 125</code>   ->  <code>tf.identity(bar) = 125</code></p>\n\n<p>Now let's backpropagate:</p>\n\n<p>The gradient of <code>tf.identity(bar)</code> with respect to its argument <code>bar</code> equals (by your definition) to <code>bar</code>, that is, <code>125</code>. The gradient of <code>bar</code> with respect to <code>var_foo</code> equals 3 times the square of <code>var_foo</code> which is <code>75</code>. Multiply, and you get <code>9375</code>, which is indeed the output of your code.</p>\n\n<p><code>op.inputs[0]</code> contains the forward-pass value of the op. In this case, the forward pass of the <code>identity</code> op is <code>125</code>.</p>\n"", 'IsAccepted': False, 'CreationDate': 1514494606}, {'QuestionId': 45815531, 'AnswerId': 45815932, 'URL': 'https://stackoverflow.com/questions/45815531/tensorflow-op-with-two-inputs-return-one-of-the-two-and-override-gradient/45815932#45815932', 'QuestionTitle': 'Tensorflow op with two inputs, return one of the two and override gradient', 'Answer': '<p>You can add the following code during model definition to override gradient.\n<code>tf.Graph</code> has <a href=""https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/Graph#gradient_override_map"" rel=""nofollow noreferrer""><code>gradient_override_map</code></a> construct to achieve the same</p>\n\n<pre><code>g = tf.get_default_graph()\n...model, definiton, input other op etc\n\n# gradient overrring map construct with the function `f` in your case\nwith g.gradient_override_map({""op_name"": ""SynthGrad""}):\n    f_out = f(op_in_1, op_in_2, name=""op_name"")\n\n...\n# code related to custom function and custom gradient from your question\ndef f(a, b, name=\'some_name\'):\n    ... some stuffs\n    return a    \n\n@tf.RegisterGradient(""SynthGrad"")\ndef _SynthGrad(op, grad):\n    dim1 = tf.shape(op.inputs[1])[1]\n    dim2 = tf.shape(op.inputs[0])[1]\n    B = tf.random_normal([dim1, dim2])\n    synth_grad = tf.matmul(op.inputs[1], B)\n    return synth_grad\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1503399906}, {'QuestionId': 44564397, 'AnswerId': 44832468, 'URL': 'https://stackoverflow.com/questions/44564397/how-do-i-define-a-gradient-for-a-custom-op-working-on-complex-tensors-in-tensorf/44832468#44832468', 'QuestionTitle': 'How do I define a gradient for a custom op working on complex tensors in tensorflow?', 'Answer': '<p>Looks like <code>tf.arg</code> will exist soon: <a href=""https://github.com/tensorflow/tensorflow/pull/10643"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/pull/10643</a>.  If you want something before then, I\'d recommend</p>\n\n<pre><code>def arg(z):\n  return tf.atan2(tf.imag(z), tf.real(z))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1498760803}, {'QuestionId': 43256517, 'AnswerId': 44592784, 'URL': 'https://stackoverflow.com/questions/43256517/how-to-register-a-custom-gradient-for-a-operation-composed-of-tf-operations/44592784#44592784', 'QuestionTitle': 'How to register a custom gradient for a operation composed of tf operations', 'Answer': '<p>You need to define the op within the scope of <code>with g.gradient_override_map({\'Myop\': \'MyopGrad\'})</code></p>\n\n<p>Also, you need to map <code>Identity</code> rather than the name <code>Myop</code> to your new gradient.</p>\n\n<p>Here is the full code:</p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n@ops.RegisterGradient(""MyopGrad"")\ndef frop_grad(op, grad):\n    x = op.inputs[0]\n    return 0 * x  # zero out to see the difference:\n\ndef fprop(x):\n    x = tf.sqrt(x)\n    out = tf.maximum(x, .2)\n    return out\n\na = tf.Variable(tf.constant([5., 4., 3., 2., 1.], dtype=tf.float32))\nh = fprop(a)\n\ng = tf.get_default_graph()\nwith g.gradient_override_map({\'Identity\': \'MyopGrad\'}):\n    h = tf.identity(h, name=""Myop"")\n    grad = tf.gradients(h, a)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    result = sess.run(grad)\n\nprint(result[0])\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[ 0.  0.  0.  0.  0.]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1497626348}, {'QuestionId': 40060047, 'AnswerId': 40065673, 'URL': 'https://stackoverflow.com/questions/40060047/tensorflow-how-can-i-process-in-numpy-op-outputs-in-py-func-gradient/40065673#40065673', 'QuestionTitle': 'Tensorflow -- how can I process in numpy op outputs in py_func gradient?', 'Answer': '<p>Use <code>tf.py_func</code>\nIE, to square <code>x</code> using numpy</p>\n\n<pre><code>def _MySquareGrad(op, grad):\n   x = op.inputs[0] # how can I process this op.inputs[0] in numpy??\n   xx = tf.py_func(np.square, [x], [tf.float32])\n   return xx[0]*grad*2 # sample gradient\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1476578349}, {'QuestionId': 39048984, 'AnswerId': 39049106, 'URL': 'https://stackoverflow.com/questions/39048984/tensorflow-how-to-write-op-with-gradient-in-python/39049106#39049106', 'QuestionTitle': 'Tensorflow: How to write op with gradient in python?', 'Answer': '<p>Here\'s an example of adding gradient to a specific <code>py_func</code>\n<a href=""https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342"" rel=""nofollow"">https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342</a></p>\n\n<p>Here\'s the issue <a href=""https://github.com/tensorflow/tensorflow/issues/1095#issuecomment-239406220"" rel=""nofollow"">discussion</a></p>\n', 'IsAccepted': False, 'CreationDate': 1471650446}]","{54047604, 46876063}","['<p>In short, the correct version of _custom_square_grad should be:</p>\n\n<pre><code>@tf.RegisterGradient(""CustomSquare"")                                             \ndef _custom_square_grad(op, grad):                                               \n    x = op.inputs[0]                                                            \n    return 2.0 * (grad * 2.0 * x)\n</code></pre>\n\n<p>In order to understand the code, you need to know how <code>gradient</code> works. When you define <code>tf.RegisterGradient</code>, it is supposed to BACK-PROPAGATE the gradients from outputs to inputs.', '<p>If we use multiple variables as input, the number of gradients return from ""grad"" function should be equals to number of input variables, though we maybe don\'t care about some of them. </p>\n\n<p>For example:</p>\n\n<pre><code>@tf.custom_gradient\ndef my_multiple(x,z):\n\ndef grad(dy):\n    # return two gradients, one for \'x\' and one for \'z\'\n    return (dy*z, dy*x)\n\nreturn tf.identity(x*z), grad\n</code></pre>\n\n<p>Note that the second output of ""my_multiple"" is a function, not a gradient tensor.', '<p>In short, the correct version of _custom_square_grad should be:</p>\n\n<pre><code>@tf.RegisterGradient(""CustomSquare"")                                             \ndef _custom_square_grad(op, grad):                                               \n    x = op.inputs[0]                                                            \n    return 2.0 * (grad * 2.0 * x)\n</code></pre>\n\n<p>In order to understand the code, you need to know how <code>gradient</code> works. When you define <code>tf.RegisterGradient</code>, it is supposed to BACK-PROPAGATE the gradients from outputs to inputs. For <code>tf.squre</code>, the default gradient function is like this:</p>\n\n<pre><code># Given y = tf.square(x) =&gt; y\' = 2x\ngrad_x = grad_y * 2.0 * x\n</code></pre>\n\n<p>Since you want to double the gradient in your customized gradient   function, you can simply change it to <code>grad_x = 2.0 * (grad_y * 2.0 * x)</code>.</p>\n']","{'https://stackoverflow.com/questions/46876063/tensorflow-custom-gradients/46880761#46880761', 'https://stackoverflow.com/questions/54047604/how-to-assign-custom-gradient-to-tensorflow-op-with-multiple-inputs/54917180#54917180'}",,0.19934635991408406,0.11163766207418317
3,59555206,tf.keras,Documentation Replication on Other Examples,keras to tf.keras Conversion: Dense layer dimensions not defined?,"<p>So I've built a convnet using pure <code>keras</code>. It compiles and operates exactly as intended, but I need to convert it to use <code>tf.keras</code> so that I can make use of <code>tfmot</code>. Having read documentation, I attempted to convert it, only to get the following error:</p>

<p><code>The last dimension of the inputs to Dense should be defined. Found None.</code> </p>

<p>Any idea what I'm doing wrong?</p>

<p>Thanks!</p>

<p>Original <code>keras</code> model:</p>

<pre><code>input_layer = keras.layers.Input(shape=(100,))
reshape_layer = keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = keras.layers.Flatten()(conv_layer_5)
label_layer = keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = keras.layers.Dense(1, activation=""linear"")(label_layer)

model = keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>Converted <code>tf.keras</code> model:</p>

<pre><code>input_layer = tf.keras.layers.InputLayer(input_shape=(100,))
reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)
label_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>EDIT 1:</p>

<p>I thought maybe I could get around the issue by saving the <code>keras</code> model after creation and loading it as a <code>tf.keras</code> model immediately before compilation / training. That throws the same error! </p>
","<p>There's few issues with your code. Fix them and you should be good to go,</p>

<h2>Issue 1: Use <code>Input</code> instead of <code>InputLayer</code></h2>

<p>The standard is to use <code>Input</code> layer instead of <code>InputLayer</code> (which infact uses <code>InputLayer</code> internally). You also need to change <code>input_shape</code> to <code>shape</code> if you are using <code>Input</code> layer.</p>

<pre><code>input_layer = tf.keras.layers.Input(shape=(100,))
</code></pre>

<h2>Issue 2: 2 <code>None</code> dimensions in the output</h2>

<p>When you execute the following line, you get two <code>None</code> dimensions in your output. </p>

<pre><code>reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)
</code></pre>

<p>Which is why you are getting the above error. When defining the <code>Reshape</code> layer you don't define the <code>batch</code> dimension, which will be None. And that's the only dimension you can have as <code>None</code> if you want to use a <code>Dense</code> layer. Otherwise, the <code>Dense</code> layer cannot infer the shape of its weights (which is why you get the error). So change that to,</p>

<pre><code>reshape_layer = tf.keras.layers.Reshape((1, 100, 1))(input_layer)
</code></pre>

<p>The rest stays the same.</p>

<pre><code>conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)
label_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.summary()
</code></pre>
","{59791364, 64512293, 72484910, 53200335, 60032337, 51735090, 47214001, 50081333, 48415957, 59781176}","[{'QuestionId': 72484910, 'AnswerId': 72485731, 'URL': 'https://stackoverflow.com/questions/72484910/tensorflow-incompatible-with-the-layer-expected-min-ndim-3-found-ndim-2-error/72485731#72485731', 'QuestionTitle': 'Tensorflow incompatible with the layer: expected min_ndim=3, found ndim=2. error', 'Answer': '<p><code>train_X</code> and <code>test_X</code> are missing a dimension. A <code>Conv1D </code> layer needs data with the shape (samples, timesteps, features). So the easiest thing you can do is add a dimension before calling model.fit(*):</p>\n<pre><code>train_X = tf.expand_dims(train_X, axis=-1)\ntest_X = tf.expand_dims(test_X, axis=-1)\n</code></pre>\n<p>Also, replace the last <code>MaxPooling1D</code> layer in your model with a <code>GlobalMaxPool1D</code> layer.</p>\n<p>The <code>MaxPooling1D</code> layers calculates the max value of a tensor for a given window size and stride. It then outputs a 3D tensor <code>(batch_size, timesteps, features)</code>, whereas <code>Dense</code> layers usually work with 2D Tensors <code>(batch_size, features)</code>. The <code>GlobalMaxPool1D</code> layer downsamples the input representation by taking the maximum value over the time dimension and returns a 2D tensor.</p>\n', 'IsAccepted': True, 'CreationDate': 1654239139}, {'QuestionId': 64512293, 'AnswerId': 64513181, 'URL': 'https://stackoverflow.com/questions/64512293/input-dense-is-incompatible-with-the-layer-invalid-shape/64513181#64513181', 'QuestionTitle': 'Input dense is incompatible with the layer invalid shape', 'Answer': ""<p>When specifying the input shape, you only need to specify the number of features. Keras doesn't want to know the number of sample because it can accept any size. So, when you do this:</p>\n<pre><code>states = Input(shape=(len(inputFinal),))\n</code></pre>\n<p>You're telling Keras that your input has 328 columns, which isn't the case. Keras realizes this when you feed the input, and crashes.</p>\n<p>If <code>inputFinal</code> is a 2D NumPy array, try:</p>\n<pre><code>Input(shape=inputFinal.shape[1:])\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1603541457}, {'QuestionId': 59791364, 'AnswerId': 61837269, 'URL': 'https://stackoverflow.com/questions/59791364/keras-conv2d-value-error-negative-dimension-size/61837269#61837269', 'QuestionTitle': 'keras Conv2d value error: Negative dimension size', 'Answer': '<p>I was able to recreate your error </p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nimport numpy as np\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n             activation=\'relu\',\n             input_shape=(3,201,1), data_format=\'channels_first\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=""tf"",strides=(1, 1)))\nmodel.add(Conv2D(32, (3, 3), activation=\'relu\', data_format=\'channels_first\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), dim_ordering=""tf"",strides=(1, 1)))\nmodel.add(Flatten())\nmodel.add(Dense(1000, activation=""tanh"", kernel_initializer=""uniform""))\nmodel.add(Dense(4, activation=""relu"", kernel_initializer=""uniform""))\n\nmodel.summary()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)\n   1653   try:\n-&gt; 1654     c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\n   1655   except errors.InvalidArgumentError as e:\n\nInvalidArgumentError: Negative dimension size caused by subtracting 3 from 1 for \'{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv2d_input, conv2d/Conv2D/ReadVariableOp)\' with input shapes: [?,3,201,1], [3,3,3,32].\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n14 frames\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)\n   1655   except errors.InvalidArgumentError as e:\n   1656     # Convert to ValueError for backwards compatibility.\n-&gt; 1657     raise ValueError(str(e))\n   1658 \n   1659   return c_op\n\nValueError: Negative dimension size caused by subtracting 3 from 1 for \'{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv2d_input, conv2d/Conv2D/ReadVariableOp)\' with input shapes: [?,3,201,1], [3,3,3,32].\n</code></pre>\n\n<p><strong>Solution:</strong></p>\n\n<p>This issue can be fixed by including <code>padding=\'same\'</code> to all <code>convolution</code> and <code>maxpooling</code> layers through which we can get same <code>dimensionality</code>( in default behavior \ni.e <code>padding = \'valid\'</code>  there is a automatic dimensionality reduction occurs during convolution and maxpooling and will face negative dimensions issue).</p>\n\n<p>While fixing above issue, came across with another issue is due <code>dim_ordering</code> in <code>MaxPooling</code> layer. This option was changed to `data_format\' in Keras 2.</p>\n\n<p><code>dim_ordering=\'tf\'</code> is equivalent to <code>data_format=""channels_last""</code> accordingly i made changes to the network</p>\n\n<pre><code>model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n             activation=\'relu\',\n             input_shape=(3,201,1), padding=\'same\', data_format=\'channels_first\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding=\'same\', data_format=\'channels_last\',strides=(1, 1)))\nmodel.add(Conv2D(32, (3, 3), activation=\'relu\', padding=\'same\', data_format=\'channels_first\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding=\'same\', data_format=\'channels_last\',strides=(1, 1)))\nmodel.add(Flatten())\nmodel.add(Dense(1000, activation=""tanh"", kernel_initializer=""uniform""))\nmodel.add(Dense(4, activation=""relu"", kernel_initializer=""uniform""))\n\nmodel.summary()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>Model: ""sequential_7""\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_8 (Conv2D)            (None, 32, 201, 1)        896       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 32, 201, 1)        0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 32, 201, 1)        9248      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 32, 201, 1)        0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 6432)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1000)              6433000   \n_________________________________________________________________\ndense_3 (Dense)              (None, 4)                 4004      \n=================================================================\nTotal params: 6,447,148\nTrainable params: 6,447,148\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1589634486}, {'QuestionId': 60032337, 'AnswerId': 60033924, 'URL': 'https://stackoverflow.com/questions/60032337/tensorflow-keras-dimension-error-for-input-layer/60033924#60033924', 'QuestionTitle': 'TensorFlow Keras dimension error for input layer', 'Answer': '<p>When you made your preprocessing, you might have read the image in grayscale mode with a library OpenCV/PIL.</p>\n\n<p>When you read them, your library considers a grayscale image of size (48,48), not a (48,48,1), hence the issue that you have.</p>\n\n<p>Solve the issue as soon as possible, not before feeding to your model; in your code, wherever you read those images, before appending to your list/arrays, ensure the right shape of the array is picked. You can see down below an OpenCV example:</p>\n\n<pre><code>image = cv2.imread(filepath, 0)\n#Before this np_expand_dims, image has shape (48,48)\nimage = np.expand_dims(image , axis=2)\n#After this step, image has shape (48,48,1)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1580710127}, {'QuestionId': 60032337, 'AnswerId': 60033139, 'URL': 'https://stackoverflow.com/questions/60032337/tensorflow-keras-dimension-error-for-input-layer/60033139#60033139', 'QuestionTitle': 'TensorFlow Keras dimension error for input layer', 'Answer': '<p><em>Reshape</em> your training data to have 4-dimensions before calling <code>model.fit()</code> such as:</p>\n\n<pre><code>image_train = np.reshape(image_train, (21005, 48, 48, 1))\n</code></pre>\n\n<p>This is needed because the first <code>Conv2D</code> layer expects an image to have an <code>input_shape</code> of (48,48,1)</p>\n', 'IsAccepted': True, 'CreationDate': 1580704522}, {'QuestionId': 59781176, 'AnswerId': 59783269, 'URL': 'https://stackoverflow.com/questions/59781176/incorrect-dimension-going-into-convolutional-layer-in-tensorflow/59783269#59783269', 'QuestionTitle': 'Incorrect dimension going into convolutional layer in tensorflow', 'Answer': '<p><code>numpy.expand_dims(images, axis=4)</code> work for me</p>\n', 'IsAccepted': False, 'CreationDate': 1579247949}, {'QuestionId': 59781176, 'AnswerId': 59781197, 'URL': 'https://stackoverflow.com/questions/59781176/incorrect-dimension-going-into-convolutional-layer-in-tensorflow/59781197#59781197', 'QuestionTitle': 'Incorrect dimension going into convolutional layer in tensorflow', 'Answer': '<p>You need to add the channel dimension back into your batch. Reshape to <code>(15,28,28,1)</code> or <code>(15,1,28,28)</code> depending on your configuration (default is channels last in keras).</p>\n', 'IsAccepted': False, 'CreationDate': 1579235268}, {'QuestionId': 50081333, 'AnswerId': 54607337, 'URL': 'https://stackoverflow.com/questions/50081333/valueerror-the-last-dimension-of-the-inputs-to-dense-should-be-defined-found/54607337#54607337', 'QuestionTitle': 'ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`', 'Answer': '<p>You are cropping your input image too much. The <code>cropping</code> argument is <a href=""https://keras.io/layers/convolutional/#Cropping2D"" rel=""nofollow noreferrer"">interpreted</a> as follows:</p>\n\n<blockquote>\n  <p>If tuple of 2 tuples of 2 ints: interpreted as  ((top_crop,\n  bottom_crop), (left_crop, right_crop))</p>\n</blockquote>\n\n<p>Consider the following example from the Keras docs:</p>\n\n<pre><code># Crop the input 2D images or feature maps\nmodel = Sequential()\nmodel.add(Cropping2D(cropping=((2, 2), (4, 4)),\n                     input_shape=(28, 28, 3)))\n# now model.output_shape == (None, 24, 20, 3)\n</code></pre>\n\n<p>In your code, <strong>you are cropping 79 pixels from the top and 145 pixels from the bottom, whereas the height of your images is only 160 pixels</strong>. With less cropping, your code runs fine, eg:</p>\n\n<pre><code>model.add(Cropping2D(cropping=((10, 10), (10, 10)), input_shape=(160,320,3)))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1549723701}, {'QuestionId': 53200335, 'AnswerId': 53202984, 'URL': 'https://stackoverflow.com/questions/53200335/tensorflow-valueerror-the-last-dimension-of-the-inputs-to-dense-should-be-de/53202984#53202984', 'QuestionTitle': 'Tensorflow: ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`', 'Answer': ""<p>If you set the shape after batching, you will need to set it to <code>[None, 100]</code> to include the batch axis:</p>\n\n<pre><code>x['reviews'].set_shape([None, 100])\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1541661351}, {'QuestionId': 51735090, 'AnswerId': 51736367, 'URL': 'https://stackoverflow.com/questions/51735090/input-to-tf-keras-conv2d-layer-not-of-appropriate-size/51736367#51736367', 'QuestionTitle': 'Input to tf.keras Conv2D layer not of appropriate size', 'Answer': '<p>It looks like you skipped reshaping part from tutorial:</p>\n\n<pre><code># Reshape input data from (28, 28) to (28, 28, 1)\nw, h = 28, 28\nx_train = x_train.reshape(x_train.shape[0], w, h, 1)\nx_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\nx_test = x_test.reshape(x_test.shape[0], w, h, 1)\n</code></pre>\n\n<p>The idea here is that your samples are 28x28x1 (one color, 28x28 pixels), and the first dimension - the number of the sample (60000 in your case).</p>\n', 'IsAccepted': False, 'CreationDate': 1533681524}, {'QuestionId': 48415957, 'AnswerId': 48416073, 'URL': 'https://stackoverflow.com/questions/48415957/keras-tensorflow-backend-getting-typeerror-unhashable-type-dimension/48416073#48416073', 'QuestionTitle': 'Keras (tensorflow backend) getting &quot;TypeError: unhashable type: &#39;Dimension&#39;&quot;', 'Answer': ""<p>This is because you are using <code>tf.reshape</code>, which returns a Tensor, and the <code>fit</code> method of Keras models don't work well with tensors.</p>\n\n<p>Consider using <code>np.reshape</code> instead, which will do the exact same thing.</p>\n"", 'IsAccepted': False, 'CreationDate': 1516775171}, {'QuestionId': 47214001, 'AnswerId': 47225894, 'URL': 'https://stackoverflow.com/questions/47214001/unable-to-convert-model-from-keras-to-tensorflow/47225894#47225894', 'QuestionTitle': 'Unable to convert model from Keras to Tensorflow', 'Answer': '<p>Conversion was correct. I wrote unittests for convolution layers from Keras and Tensorflow and found they produce numerically identical results.</p>\n\n<p>Additionally, I replaced optimization goal from just log-loss to <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits"" rel=""nofollow noreferrer""><code>sigmoid_cross_entropy_with_logits</code></a> but this didn\'t helped alone.</p>\n\n<p>The problem was with too small <code>stdev</code> of initialization values. </p>\n\n<p>I was thinking it is enough to have it very small to break symmetry, and was setting it to <code>1e-8</code> or <code>1e-5</code>, but this was wrong: such small values were nearly identical to zeros and after several layers network was starting to produce identical results for all samples.</p>\n\n<p>After I changed <code>stdev</code> to <code>1e-1</code>, then netwrok started to perfor as in <code>Keras</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1510326874}]","{57690048, 59555206}","[""<p>There's few issues with your code. Fix them and you should be good to go,</p>\n\n<h2>Issue 1: Use <code>Input</code> instead of <code>InputLayer</code></h2>\n\n<p>The standard is to use <code>Input</code> layer instead of <code>InputLayer</code> (which infact uses <code>InputLayer</code> internally). You also need to change <code>input_shape</code> to <code>shape</code> if you are using <code>Input</code> layer.</p>\n\n<pre><code>input_layer = tf.keras.layers.Input(shape=(100,))\n</code></pre>\n\n<h2>Issue 2: 2 <code>None</code> dimensions in the output</h2>\n\n<p>When you execute the following line, you get two <code>None</code> dimensions in your output. </p>\n\n<pre><code>reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)\n</code></pre>\n\n<p>Which is why you are getting the above error. When defining the <code>Reshape</code> layer you don't define the <code>batch</code> dimension, which will be None. And that's the only dimension you can have as <code>None</code> if you want to use a <code>Dense</code> layer."", '<p>There\'s few issues with your code. Fix them and you should be good to go,</p>\n\n<h2>Issue 1: Use <code>Input</code> instead of <code>InputLayer</code></h2>\n\n<p>The standard is to use <code>Input</code> layer instead of <code>InputLayer</code> (which infact uses <code>InputLayer</code> internally). You also need to change <code>input_shape</code> to <code>shape</code> if you are using <code>Input</code> layer.</p>\n\n<pre><code>input_layer = tf.keras.layers.Input(shape=(100,))\n</code></pre>\n\n<h2>Issue 2: 2 <code>None</code> dimensions in the output</h2>\n\n<p>When you execute the following line, you get two <code>None</code> dimensions in your output. </p>\n\n<pre><code>reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)\n</code></pre>\n\n<p>Which is why you are getting the above error. When defining the <code>Reshape</code> layer you don\'t define the <code>batch</code> dimension, which will be None. And that\'s the only dimension you can have as <code>None</code> if you want to use a <code>Dense</code> layer. Otherwise, the <code>Dense</code> layer cannot infer the shape of its weights (which is why you get the error). So change that to,</p>\n\n<pre><code>reshape_layer = tf.keras.layers.Reshape((1, 100, 1))(input_layer)\n</code></pre>\n\n<p>The rest stays the same.</p>\n\n<pre><code>conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)\nconv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)\nconv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)\nconv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)\nconv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)\nflatten_layer = tf.keras.layers.Flatten()(conv_layer_5)\nlabel_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)\noutput_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)\n\nmodel = tf.keras.Model(inputs=input_layer, outputs=output_layer)\nmodel.summary()\n</code></pre>\n', ""Dense layers are defined by having &quot;one&quot; connection between each item of the output and each item of the input. So even though you have 5 &quot;2x3 things&quot; in your output, they each just have one solitary weight associated with them about how they are connected to the 2x3 thing that is the input. Keras also defaults to using a bias <em>vector</em> (not bias tensor), so if the dense layer has dimension <code>k</code> and the final dimension of the previous layer is <code>n</code> you should expect <code>(n+1)k</code> trainable parameters. These will always be used with numpy-like broadcasting to make the lesser dimensional shape of the weight and bias vectors conformable to the actual shapes of the input tensors.</p>\n<p>It is customary to use Flatten as in your first example if you want to enforce the exact size of the coming dense layer. You would use multidimensional Dense layer when you want different &quot;(n - 1)D&quot; groups of connections to each Dense node. This is probably extremely rare for higher dimensional inputs because you'd typically want a CNN type of operation, but I could imagine maybe in some cases where a model predicts pixel-wise values or if you are generating a full nD output, like from the decoder portion of an encoder-decoder network, you might want a dense array of cells that match the dimensions of some expected structured output type like an image or video.</p>\n""]","{'https://stackoverflow.com/questions/57690048/understanding-output-of-dense-layer-for-higher-dimension/57693306#57693306', 'https://stackoverflow.com/questions/59555206/keras-to-tf-keras-conversion-dense-layer-dimensions-not-defined/59556499#59556499'}",,0.2211622866125691,0.07326105182924521
3,57813806,tf.feature_column,Documentation Replication on Other Examples,Apply feature columns without tf.Estimator (Tensorflow 2.0.0-rc0),"<p>In the Tensorflow tf.Estimator and tf.feature_column docs it is well documented, how to use feature columns together with an Estimator e.g. in order to one-hot encode the categorical features in the dataset being used.</p>

<p>However, I want to ""apply"" my feature columns directly to a tf.dataset which I create from a .csv file (with two columns: UserID, MovieID), without even defining a model or an Estimator. (Reason: I want to check what's happening exactly in my datapipeline, i.e. I'd like to be able to run a batch of samples through my the pipeline, and then see in the output how the features got encoded.)</p>

<p>This is what I have tried so far:</p>

<pre><code>column_names = ['UserID', 'MovieID']

user_col = tf.feature_column.categorical_column_with_hash_bucket(key='UserID', hash_bucket_size=1000)
movie_col = tf.feature_column.categorical_column_with_hash_bucket(key='MovieID', hash_bucket_size=1000)
feature_columns = [tf.feature_column.indicator_column(user_col), tf.feature_column.indicator_column(movie_col)]

feature_layer = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)

def process_csv(line):
  fields = tf.io.decode_csv(line, record_defaults=[tf.constant([], dtype=tf.int32)]*2, field_delim="";"")
  features = dict(zip(column_names, fields))

  return features 

ds = tf.data.TextLineDataset(csv_filepath)
ds = ds.map(process_csv, num_parallel_calls=4)
ds = ds.batch(10)
ds.map(lambda x: feature_layer(x))
</code></pre>

<p>However the last line with the map call raises the following error:</p>

<blockquote>
  <p>ValueError: Column dtype and SparseTensors dtype must be compatible.
  key: MovieID, column dtype: , tensor dtype: </p>
</blockquote>

<p>I'm not sure what this error means...
I also tried to define a tf.keras model with only the feature_layer I defined, and then run .predict() on my dataset - instead of using ds.map(lambda x: feature_layer(x)):</p>

<pre><code>model = tf.keras.Sequential([feature_layer])
model.compile()
model.predict(ds)
</code></pre>

<p>However, this results exactly in the same error as above.
Does anybody have an idea what is going wrong? Is there maybe an easier way to achieve this?</p>
","<p>Just found the issue:
tf.feature_column.categorical_column_with_hash_bucket() takes an optional argument dtype, which is set to tf.dtypes.string by default.
However, the datatype of my columns is numerical (tf.dtypes.int32).
This solved the issue:</p>

<pre><code>tf.feature_column.categorical_column_with_hash_bucket(key='UserID', hash_bucket_size=1000, dtype=tf.dtypes.int32)
</code></pre>
","{71191874, 73213159, 46904972, 57403472, 52266000, 61148307, 48087861, 49853718, 50818584, 58979037, 60174815}","[{'QuestionId': 73213159, 'AnswerId': 73216861, 'URL': 'https://stackoverflow.com/questions/73213159/how-to-apply-tf-data-transformations-to-a-dataframe/73216861#73216861', 'QuestionTitle': 'How to apply tf.data transformations to a DataFrame', 'Answer': ""<p>Try using a ragged structure:</p>\n<pre><code>import tensorflow as tf\nimport pandas as pd\n\ndf = pd.DataFrame(data={'reports': [[2.0, 3.0, 4.0], [2.0, 3.0], [2.0]]})\n\ndataset = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(df['reports']))\n\nfor x in dataset:\n  print(x)\n</code></pre>\n<pre><code>tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\ntf.Tensor([2. 3.], shape=(2,), dtype=float32)\ntf.Tensor([2.], shape=(1,), dtype=float32)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1659506319}, {'QuestionId': 73213159, 'AnswerId': 73213382, 'URL': 'https://stackoverflow.com/questions/73213159/how-to-apply-tf-data-transformations-to-a-dataframe/73213382#73213382', 'QuestionTitle': 'How to apply tf.data transformations to a DataFrame', 'Answer': ""<p>You can try forcing your <code>df[&quot;reports&quot;]</code> to a specific type. Assuming that you want to convert this column to numbers you can easily do it like this:</p>\n<pre><code>df['reports'] = pd.to_numeric(df['reports'])\n</code></pre>\n<p>Anyway, I suggest you to investigate the cause of your non-uniform <code>dtype('O')</code>. You could have some mistake in your data.</p>\n"", 'IsAccepted': False, 'CreationDate': 1659471835}, {'QuestionId': 71191874, 'AnswerId': 71196640, 'URL': 'https://stackoverflow.com/questions/71191874/cant-load-dataframe-columns-by-tf-data-dataset-from-tensor-slices/71196640#71196640', 'QuestionTitle': 'Can&#39;t load dataframe columns by tf.data.Dataset.from_tensor_slices()', 'Answer': '<p>If the columns <code>Text</code> and <code>Media_location</code> have the same data type your code will work:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\nimport pandas as pd\n\ndf = pd.DataFrame(data={\'Text\': [\'some text\', \'some more text\'],\n                        \'Media_location\': [\'/path/to/file1\', \'/path/to/file2\']})\n\nfeatures = df[[\'Text\', \'Media_location\']]\ndataset = tf.data.Dataset.from_tensor_slices((features))\n\nfor x in dataset:\n  print(x)\n</code></pre>\n<pre><code>tf.Tensor([b\'some text\' b\'/path/to/file1\'], shape=(2,), dtype=string)\ntf.Tensor([b\'some more text\' b\'/path/to/file2\'], shape=(2,), dtype=string)\n</code></pre>\n<p>However, if both have different data types, you will get your error or a similar one, since a tensor cannot have mixed data types. So try something like this:</p>\n<pre><code>df = pd.DataFrame(data={\'Text\': [0.29, 0.58],\n                        \'Media_location\': [\'/path/to/file1\', \'/path/to/file2\']})\n\ndataset = tf.data.Dataset.from_tensor_slices((df[\'Text\'], df[\'Media_location\']))\n\nfor x in dataset:\n  print(x)\n</code></pre>\n<pre><code>(&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.29&gt;, &lt;tf.Tensor: shape=(), dtype=string, numpy=b\'/path/to/file1\'&gt;)\n(&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.58&gt;, &lt;tf.Tensor: shape=(), dtype=string, numpy=b\'/path/to/file2\'&gt;)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1645376138}, {'QuestionId': 60174815, 'AnswerId': 65977786, 'URL': 'https://stackoverflow.com/questions/60174815/best-way-to-add-columns-to-a-tf-data-dataset/65977786#65977786', 'QuestionTitle': 'Best way to add &quot;columns&quot; to a tf.data.dataset', 'Answer': ""<p>y_dataset = x_dataset.map(fn1)</p>\n<p>You can define fn1 as you want</p>\n<pre><code>@tf.function\ndef fn1(x):\n    ##use x to derive additional columns u want. Set the shape as well\n    y = {}\n    y.update(x)\n    y['new1'] = new1\n    y['new2'] = new2\n    return y\n\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1612087111}, {'QuestionId': 61148307, 'AnswerId': 61149485, 'URL': 'https://stackoverflow.com/questions/61148307/how-to-create-add-columns-i-e-features-on-a-tf-dataset/61149485#61149485', 'QuestionTitle': 'How to create add columns (i.e., features) on a tf.Dataset?', 'Answer': '<p>Assuming <code>tensorflow 2.0</code>:</p>\n\n<pre><code>import tensorflow as tf\ncities_ds = tf.data.Dataset.from_tensor_slices([""Rome"",""Brussels""])\nages_ds = tf.data.Dataset.from_tensor_slices([5,7])\nds = tf.data.Dataset.zip((cities_ds, ages_ds)) \nds = ds.map(lambda city, age: (city, age, tf.strings.length(city)))\nfor i in ds:\n  print(i[0].numpy(), i[1].numpy(), i[2].numpy())\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1586557108}, {'QuestionId': 58979037, 'AnswerId': 58993925, 'URL': 'https://stackoverflow.com/questions/58979037/howto-tf-estimator-with-continuous-and-categorical-columns/58993925#58993925', 'QuestionTitle': 'HOWTO tf.estimator with continuous and categorical columns', 'Answer': ""<p>You need to wrap categorical columns before sending to DNN:</p>\n\n<pre><code>cat_feature_cols = [ tf.feature_column.sequence_categorical_column_with_identity('cat_col', num_buckets=4)) ]\nfeature_cols = [tf.feature_column.indicator_column(c) for c in cat_feature_cols]\n</code></pre>\n\n<p>Use indicator column to one-hot encode, or embedded column to embed.</p>\n"", 'IsAccepted': True, 'CreationDate': 1574424207}, {'QuestionId': 57403472, 'AnswerId': 57403695, 'URL': 'https://stackoverflow.com/questions/57403472/how-do-i-add-a-new-feature-column-to-a-tf-data-dataset-object/57403695#57403695', 'QuestionTitle': 'How do I add a new feature column to a tf.data.Dataset object?', 'Answer': '<p>Wow, this is embarassing, but I have found the solution and it\'s simplicity literally makes me feel like an idiot for asking this. But I will leave the answer up just in case anyone else is ever facing this issue.</p>\n\n<p>You first create a new tf.data.Dataset object using any function that returns a Dataset, such as "".map"".</p>\n\n<p>Then you create a new Dataset by zipping the original and the one with the new data:</p>\n\n<pre><code>dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1565223333}, {'QuestionId': 49853718, 'AnswerId': 54957675, 'URL': 'https://stackoverflow.com/questions/49853718/how-to-combine-feature-columns-model-to-estimator-and-dataset-api-in-tensorflow/54957675#54957675', 'QuestionTitle': 'How to combine feature_columns, model_to_estimator and dataset API in Tensorflow', 'Answer': '<p>Some example codes:</p>\n\n<pre><code>from tensorflow.python.feature_column import feature_column_v2 as fc\n\nfeature_layer = fc.FeatureLayer(your_feature_columns)\n\nmodel = tf.keras.Sequential([\n  feature_layer,\n  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n  tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n</code></pre>\n\n<p>please refer to <a href=""https://github.com/tensorflow/docs/blob/b4d8d7096099c2b0a7df6a0564bf6eca8c96c4a0/site/en/tutorials/structured_data/feature_cols_keras.ipynb"" rel=""nofollow noreferrer"">feature_cols_keras</a></p>\n', 'IsAccepted': False, 'CreationDate': 1551523787}, {'QuestionId': 52266000, 'AnswerId': 52276382, 'URL': 'https://stackoverflow.com/questions/52266000/avoiding-tf-data-dataset-from-tensor-slices-with-estimator-api/52276382#52276382', 'QuestionTitle': 'Avoiding tf.data.Dataset.from_tensor_slices with estimator api', 'Answer': ""<p>To use either initializable or reinitializable iterators, you must create a class that inherits from tf.train.SessionRunHook, which has access to the session at multiple times during training and evaluation steps.</p>\n\n<p>You can then use this new class to initialize the iterator has you would normally do in a classic setting. You simply need to pass this newly created hook to the training/evaluation functions or to the correct train spec.</p>\n\n<p>Here is quick example that you can adapt to your needs :</p>\n\n<pre><code>class IteratorInitializerHook(tf.train.SessionRunHook):\n    def __init__(self):\n        super(IteratorInitializerHook, self).__init__()\n        self.iterator_initializer_func = None # Will be set in the input_fn\n\n    def after_create_session(self, session, coord):\n        # Initialize the iterator with the data feed_dict\n        self.iterator_initializer_func(session) \n\n\ndef get_inputs(X, y):\n    iterator_initializer_hook = IteratorInitializerHook()\n\n    def input_fn():\n        X_pl = tf.placeholder(X.dtype, X.shape)\n        y_pl = tf.placeholder(y.dtype, y.shape)\n\n        dataset = tf.data.Dataset.from_tensor_slices((X_pl, y_pl))\n        dataset = ...\n        ...\n\n        iterator = dataset.make_initializable_iterator()\n        next_example, next_label = iterator.get_next()\n\n\n        iterator_initializer_hook.iterator_initializer_func = lambda sess: sess.run(iterator.initializer,\n                                                                                    feed_dict={X_pl: X, y_pl: y})\n\n        return next_example, next_label\n\n    return input_fn, iterator_initializer_hook\n\n...\n\ntrain_input_fn, train_iterator_initializer_hook = get_inputs(X_train, y_train)\ntest_input_fn, test_iterator_initializer_hook = get_inputs(X_test, y_test)\n\n...\n\nestimator.train(input_fn=train_input_fn,\n                hooks=[train_iterator_initializer_hook]) # Don't forget to pass the hook !\nestimator.evaluate(input_fn=test_input_fn,\n                   hooks=[test_iterator_initializer_hook])\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1536670208}, {'QuestionId': 49853718, 'AnswerId': 50858817, 'URL': 'https://stackoverflow.com/questions/49853718/how-to-combine-feature-columns-model-to-estimator-and-dataset-api-in-tensorflow/50858817#50858817', 'QuestionTitle': 'How to combine feature_columns, model_to_estimator and dataset API in Tensorflow', 'Answer': '<p>To connect the <code>feature_columns</code> with the estimator created via <code>model_to_estimator(keras_model=model)</code>, you have to make the name of the feature_column match the name of the input layer of your model.</p>\n\n<p>For example, your <code>input_fn()</code> could look like this:</p>\n\n<pre><code>def input_fn(features, labels, batch_size):\n    dataset = D.Dataset.from_tensor_slices((dict(features), labels))\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n    iterator = dataset.make_initializable_iterator()\n    tf.add_to_collection(\n        tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)\n    features, labels = iterator.get_next()\n    return {""dense_1_input"": features}, labels\n</code></pre>\n\n<p>So, whatever name your input layer has, the keras model expects a feature column of that name with <code>_input</code> added:</p>\n\n<pre><code>model = tf.keras.models.Sequential()\nmodel.add(L.Dense(10, activation=\'relu\', input_dim=9, name=""MY_NAME""))\n\ndef input_fn(features, labels, batch_size):\n    ...\n    return {""MY_NAME_input"": features}, labels\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1528983344}, {'QuestionId': 50818584, 'AnswerId': 50821391, 'URL': 'https://stackoverflow.com/questions/50818584/can-i-make-a-custom-tf-estimator-without-using-feature-columns/50821391#50821391', 'QuestionTitle': 'Can i make a custom tf.estimator without using feature columns?', 'Answer': '<p>Yes. The <code>features</code> argument to your <code>model_fn</code> can simply be a tensor, or a dict mapping strings to tensors in the case of multiple inputs. This also means that your <code>input_fn</code> can simply return such objects.</p>\n', 'IsAccepted': True, 'CreationDate': 1528819142}, {'QuestionId': 48087861, 'AnswerId': 48179106, 'URL': 'https://stackoverflow.com/questions/48087861/tf-estimator-inputs-pandas-input-fn-throws-numericcolumn-object-has-no-attribu/48179106#48179106', 'QuestionTitle': 'tf.estimator.inputs.pandas_input_fn throws _NumericColumn&#39; object has no attribute &#39;insert_transformed_feature', 'Answer': '<p>This is happening because you\'re mixing the SVM estimator from <code>tf.contrib</code> with the feature columns from core TensorFlow (<code>tf.feature_column.numeric_column</code>).</p>\n\n<p>Try using the contrib version of the feature columns.  Replace <code>tf.feature_column.numeric_column(k)</code> with <code>tf.contrib.layers.real_valued_column(k)</code>.</p>\n\n<p><a href=""https://towardsdatascience.com/how-to-move-from-tf-contrib-learn-estimator-to-core-tensorflow-tf-estimator-af07b2d21f34"" rel=""nofollow noreferrer"">This article</a> gives some more context on why this is an issue.</p>\n', 'IsAccepted': False, 'CreationDate': 1515548208}, {'QuestionId': 46904972, 'AnswerId': 46911617, 'URL': 'https://stackoverflow.com/questions/46904972/how-to-create-a-tf-feature-column-by-multiplying-two-other-tf-feature-columns/46911617#46911617', 'QuestionTitle': 'How to create a tf.feature_column by multiplying two other tf.feature_columns?', 'Answer': '<p>You can declare a custom numerical column and add it to the dataframe in your <a href=""https://www.tensorflow.org/get_started/input_fn"" rel=""nofollow noreferrer"">input function</a>:</p>\n\n<pre><code># Existing features\nage = tf.feature_column.numeric_column(""age"")\neducation_num = tf.feature_column.numeric_column(""education_num"")\n# Declare a custom column just like other columns\nmy_feature = tf.feature_column.numeric_column(""my_feature"")\n\n...\n# Add to the list of features\nfeature_columns = { ... age, education_num, my_feature, ... }\n\n...\ndef input_fn():\n  df_data = pd.read_csv(""input.csv"")\n  df_data = df_data.dropna(how=""any"", axis=0)\n  # Manually update the dataframe\n  df_data[""my_feature""] = df_data[""age""] * df_data[""education_num""]\n\n  return tf.estimator.inputs.pandas_input_fn(x=df_data,\n                                             y=labels,\n                                             batch_size=100,\n                                             num_epochs=10)\n\n...\nmodel.train(input_fn=input_fn())\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1508851026}]","{50334755, 57813806}","[""However, the datatype of my columns is numerical (tf.dtypes.int32). This solved the issue:</p>\n\n<pre><code>tf.feature_column.categorical_column_with_hash_bucket(key='UserID', hash_bucket_size=1000, dtype=tf.dtypes.int32)\n</code></pre>\n"", 'They have very different weights, and our machine learning algorithm can learn to ""understand"" that an animal with a heavy weight is more likely to be an elephant than it is to be a squirrel. In a real scenario you would generally have more than one feature.</p>\n\n<p>I\'m not sure why you would say that Keras does not use features. They are a fundamental aspect of many classification problems. Some datasets may contain labelled data or labelled features, like this one: <a href=""https://keras.io/datasets/#cifar100-small-image-classification"" rel=""nofollow noreferrer"">https://keras.io/datasets/#cifar100-small-image-classification</a></p>\n\n<p>When we ""don\'t use features"", I think a more accurate way to state that would be that the data is unlabelled. In this case, a machine learning algorithm can still find relationships in the data, but without human labels applied to the data.</p>\n\n<p>If you <code>Ctrl+F</code> for the word ""features"" on this page you will see places where Keras accepts them as an argument: <a href=""https://keras.io/layers/core/"" rel=""nofollow noreferrer"">https://keras.io/layers/core/</a></p>\n\n<p>I am not a machine learning expert so if anyone is able to correct my answer, I would appreciate that too.</p>\n\n<p><strong>In Tensorflow</strong></p>\n\n<p>My understanding of <a href=""https://www.tensorflow.org/get_started/feature_columns"" rel=""nofollow noreferrer"">Tensorflow\'s feature columns</a> implementation in particular is that they allow you to cast raw data into a typed column that allow the algorithm to better distinguish what type of data you are passing. For example Latitude and Longitude could be passed as two numerical columns, but as the docs say <a href=""https://www.tensorflow.org/get_started/feature_columns#crossed_column"" rel=""nofollow noreferrer"">here</a>, using a Crossed Column for Latitude X Longitude may allow the model to train on the data in a more meaningful/effective way. After all, what ""Latitude"" and ""Longitude"" really mean is ""Location."" As for why Keras does not have this functionality, I am not sure, hopefully someone else can offer insight on this topic.</p>\n', '<p>Just found the issue:\ntf.feature_column.categorical_column_with_hash_bucket() takes an optional argument dtype, which is set to tf.dtypes.string by default.']","{'https://stackoverflow.com/questions/57813806/apply-feature-columns-without-tf-estimator-tensorflow-2-0-0-rc0/57814782#57814782', 'https://stackoverflow.com/questions/50334755/how-feature-columns-work-in-tensorflow/50334977#50334977'}",,0.23275245642317222,0.10660069011538846
3,76324368,tf.keras.layers.Dense,Documentation Replication on Other Examples,Understanding tf.keras.layers.Dense(),"<p>I am trying to understand why there is a difference between calculating a dense layer operation directly and using the <code>keras</code> implementation.</p>
<p>Following the documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense</a>) <code>tf.keras.layers.Dense()</code> should implement the operation <code>output = activation(dot(input, kernel) + bias)</code> but <code>result</code> and <code>result1</code> below are not the same.</p>
<pre class=""lang-py prettyprint-override""><code>tf.random.set_seed(1)

bias = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32)
kernel = tf.Variable(tf.random.uniform(shape=(5,10)), dtype=tf.float32)
x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32))

result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x) + bias)
tf.print(result)

test = tf.keras.layers.Dense(units = 5, 
                            activation = 'relu',
                            use_bias = True, 
                            kernel_initializer = tf.keras.initializers.Constant(value=kernel), 
                            bias_initializer = tf.keras.initializers.Constant(value=bias), 
                            dtype=tf.float32)

result1 = test(tf.transpose(x))

print()
tf.print(result1)

</code></pre>
<p>output</p>
<pre class=""lang-py prettyprint-override""><code>
[[2.87080455]
 [3.25458574]
 [3.28776264]
 [3.14319134]
 [2.04760242]]

[[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]

</code></pre>
<p>Using <code>test.get_weights()</code> I can see that the kernel and bias (<code>b</code>) are getting set to the correct values. I am using TF version 2.12.0.</p>
","<p>After some experimentation I realized that the <code>kernel</code> for the dense layer needs to be of <code>shape=(10,5)</code> as apposed to <code>(5,10)</code> as in the code from the original question above. This is implicit because <code>units=5</code> so a vector of size <code>10</code> needs to be passed (hence why <code>input_shape=(10,)</code> is commented out as a reminder). Below is the corrected code:</p>
<pre class=""lang-py prettyprint-override""><code>tf.random.set_seed(1)

bias   = tf.Variable(tf.random.uniform(shape=(5,1)), dtype=tf.float32)
kernel = tf.Variable(tf.random.uniform(shape=(10,5)), dtype=tf.float32)
x = tf.constant(tf.random.uniform(shape=(10,1), dtype=tf.float32))

result = tf.nn.relu(tf.linalg.matmul(a=kernel, b=x, transpose_a=True) + bias)
tf.print(result)

test = tf.keras.layers.Dense(units = 5, 
                            # input_shape=(10,),
                            activation = 'relu',
                            use_bias = True, 
                            kernel_initializer = tf.keras.initializers.Constant(value=kernel), 
                            bias_initializer = tf.keras.initializers.Constant(value=bias), 
                            dtype=tf.float32)

result1 = test(tf.transpose(x))

print()
tf.print(result1)

</code></pre>
<pre class=""lang-py prettyprint-override""><code>[[2.38769]
 [3.63470697]
 [2.62423944]
 [3.31286287]
 [2.91121125]]

[[2.38769 3.63470697 2.62423944 3.31286287 2.91121125]]
</code></pre>
<p>Ultimately, I am not entirely sure what was happening under the hood and why <code>keras</code> did not raise an error. I will check with the <code>tf.keras.layers.Dense()</code> implementation but any thoughts or suggestions by someone who knows the code already are highly appreciated!</p>
","{52254253, 38714959, 55613969, 51914294, 64754524}","[{'QuestionId': 38714959, 'AnswerId': 50018200, 'URL': 'https://stackoverflow.com/questions/38714959/understanding-keras-lstms/50018200#50018200', 'QuestionTitle': 'Understanding Keras LSTMs', 'Answer': ""<p>When you have return_sequences in your last layer of RNN you cannot use a simple Dense layer instead use TimeDistributed.</p>\n<p>Here is an example piece of code this might help others.</p>\n<pre><code>words = keras.layers.Input(batch_shape=(None, self.maxSequenceLength), name = &quot;input&quot;)\n\n# Build a matrix of size vocabularySize x EmbeddingDimension \n# where each row corresponds to a &quot;word embedding&quot; vector.\n# This layer will convert replace each word-id with a word-vector of size Embedding Dimension.\nembeddings = keras.layers.embeddings.Embedding(self.vocabularySize, self.EmbeddingDimension,\n    name = &quot;embeddings&quot;)(words)\n# Pass the word-vectors to the LSTM layer.\n# We are setting the hidden-state size to 512.\n# The output will be batchSize x maxSequenceLength x hiddenStateSize\nhiddenStates = keras.layers.GRU(512, return_sequences = True, \n                                    input_shape=(self.maxSequenceLength,\n                                    self.EmbeddingDimension),\n                                    name = &quot;rnn&quot;)(embeddings)\nhiddenStates2 = keras.layers.GRU(128, return_sequences = True, \n                                    input_shape=(self.maxSequenceLength, self.EmbeddingDimension),\n                                    name = &quot;rnn2&quot;)(hiddenStates)\n\ndenseOutput = TimeDistributed(keras.layers.Dense(self.vocabularySize), \n    name = &quot;linear&quot;)(hiddenStates2)\npredictions = TimeDistributed(keras.layers.Activation(&quot;softmax&quot;), \n    name = &quot;softmax&quot;)(denseOutput)  \n\n# Build the computational graph by specifying the input, and output of the network.\nmodel = keras.models.Model(input = words, output = predictions)\n# model.compile(loss='kullback_leibler_divergence', \\\nmodel.compile(loss='sparse_categorical_crossentropy', \\\n    optimizer = keras.optimizers.Adam(lr=0.009, \\\n        beta_1=0.9,\\\n        beta_2=0.999, \\\n        epsilon=None, \\\n        decay=0.01, \\\n        amsgrad=False))\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1524647136}, {'QuestionId': 38714959, 'AnswerId': 68847256, 'URL': 'https://stackoverflow.com/questions/38714959/understanding-keras-lstms/68847256#68847256', 'QuestionTitle': 'Understanding Keras LSTMs', 'Answer': '<p>Refer this blog for more details <a href=""https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45"" rel=""nofollow noreferrer"">Animated RNN, LSTM and GRU</a>.</p>\n<p>The figure below gives you a better view of LSTM. It\'s a LSTM cell.\n<img src=""https://i.stack.imgur.com/Csa3E.png"" alt=""This figure"" /></p>\n<p>As you can see, X has 3 <code>features</code> (green circles) so input of this cell is a vector of dimension 3 and hidden state has 2 <code>units</code> (red circles) so the output of this cell (and also cell state) is a vector of dimension 2.</p>\n<p>An example of one LSTM layer with 3 timesteps (3 LSTM cells) is shown in the figure below:</p>\n<p><img src=""https://i.stack.imgur.com/7n2Fc.png"" alt=""This image"" /></p>\n<p>** A model can have multiple LSTM layers.</p>\n<p>Now I use <em>Daniel Möller</em>\'s example again for better understanding:\nWe have 10 oil tanks. For each of them we measure 2 features: temperature, pressure  every one hour for 5 times.\nnow parameters are:</p>\n<ul>\n<li><strong>batch_size</strong> = number of samples used in one forward/backward pass (default=32) --&gt; for example if you have 1000 samples and you set up the batch_size to 100 then the model will take 10 iterations to pass all of the samples once through network (1 epoch). The higher the batch size, the more memory space you\'ll need. Because the number of samples in this example are low, we consider batch_size equal to all of samples = 10</li>\n<li><strong>timesteps</strong> = 5</li>\n<li><strong>features</strong> = 2</li>\n<li><strong>units</strong> = It\'s a positive integer and determines the dimension of hidden state and cell state or in other words the number of parameters passed to next LSTM cell. It can be chosen arbitrarily or empirically based on the features and timesteps. Using more units will result in more accuracy and also more computational time. But it may cause over fitting.</li>\n<li><strong>input_shape</strong> = (batch_size, timesteps, features) = (10,5,2)</li>\n<li><strong>output_shape</strong>:\n<ul>\n<li>(batch_size, timesteps, units) if <em>return_sequences=True</em></li>\n<li>(batch_size, units) if <em>return_sequences=False</em></li>\n</ul>\n</li>\n</ul>\n', 'IsAccepted': False, 'CreationDate': 1629373549}, {'QuestionId': 55613969, 'AnswerId': 66073158, 'URL': 'https://stackoverflow.com/questions/55613969/what-exactly-is-dense-in-lstm-model-description/66073158#66073158', 'QuestionTitle': 'what exactly is Dense in LSTM model description?', 'Answer': '<p>The code you wrote is not for LSTM, this is a simple neural network of two fully connected layers also known as dense layers, here sequential means the output of one layer will be directly passed to next layer, which is not sequential learning like LSTM.</p>\n', 'IsAccepted': False, 'CreationDate': 1612580246}, {'QuestionId': 64754524, 'AnswerId': 64755380, 'URL': 'https://stackoverflow.com/questions/64754524/why-i-cannot-run-a-tensor-and-got-error-the-variable-was-uninitialized/64755380#64755380', 'QuestionTitle': 'Why I cannot run a tensor and got error &quot;the variable was uninitialized&quot;', 'Answer': '<p>It may be that you need to initialize variables (in the dense layer) before you use them e.g with <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/initializers/global_variables"" rel=""nofollow noreferrer"">tf.global_variables_initializer</a>. Try adding the following line after declaring the variables but before using them:</p>\n<pre><code>sess.run(tf.global_variables_initializer())\n</code></pre>\n<p>You can also call <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/initializers/variables"" rel=""nofollow noreferrer"">tf.variables_initializer</a> to initialize variables from a list.</p>\n', 'IsAccepted': True, 'CreationDate': 1604939058}, {'QuestionId': 52254253, 'AnswerId': 57319659, 'URL': 'https://stackoverflow.com/questions/52254253/how-does-tf-layers-dense-interact-with-inputs-of-higher-dim/57319659#57319659', 'QuestionTitle': 'How does tf.layers.dense() interact with inputs of higher dim?', 'Answer': ""<p>You can verify your expectation by checking the shape of the dense kernel as follows.</p>\n\n<pre><code>&gt;&gt;&gt; inputx = tf.placeholder(float, shape=[2,3,4])\n&gt;&gt;&gt; dense_layer = tf.layers.dense(inputx, 128, tf.nn.relu)\n&gt;&gt;&gt; g=tf.get_default_graph()\n&gt;&gt;&gt; g.get_collection('variables')\n[&lt;tf.Variable 'dense/kernel:0' shape=(4, 128) dtype=float32_ref&gt;, &lt;tf.Variable 'dense/bias:0' shape=(128,) dtype=float32_ref&gt;]\n</code></pre>\n\n<p>The behavior of the dense layer is the same as a conv layer. </p>\n\n<p>You can consider inputx as an image which has width=2, height=3 and channel=4 and the dense layer as a conv layer which has 128 filters and filters size is 1*1.</p>\n"", 'IsAccepted': False, 'CreationDate': 1564717599}, {'QuestionId': 55613969, 'AnswerId': 55614088, 'URL': 'https://stackoverflow.com/questions/55613969/what-exactly-is-dense-in-lstm-model-description/55614088#55614088', 'QuestionTitle': 'what exactly is Dense in LSTM model description?', 'Answer': '<p>Another name for dense layer is Fully-connected layer. It\'s actually the layer where each neuron is connected to all of the neurons from the next layer. It implements the operation <code>output = X * W + b</code> where <code>X</code> is input to the layer, and <code>W</code> and <code>b</code> are weights and bias of the layer. <code>W</code> ad <code>b</code> are actually the things you\'re trying to learn. If you want a more detailed explanation, please refer to <a href=""https://towardsdatascience.com/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528"" rel=""nofollow noreferrer"">this</a> article.</p>\n', 'IsAccepted': False, 'CreationDate': 1554903660}, {'QuestionId': 55613969, 'AnswerId': 55614077, 'URL': 'https://stackoverflow.com/questions/55613969/what-exactly-is-dense-in-lstm-model-description/55614077#55614077', 'QuestionTitle': 'what exactly is Dense in LSTM model description?', 'Answer': '<p>A dense layer is a fully-connected layer, i.e. every neurons of the layer N are connected to every neurons of the layer N+1</p>\n', 'IsAccepted': False, 'CreationDate': 1554903629}, {'QuestionId': 38714959, 'AnswerId': 50235563, 'URL': 'https://stackoverflow.com/questions/38714959/understanding-keras-lstms/50235563#50235563', 'QuestionTitle': 'Understanding Keras LSTMs', 'Answer': '<p>As a complement to the accepted answer, this answer shows keras behaviors and how to achieve each picture.</p>\n\n<h2>General Keras behavior</h2>\n\n<p>The standard keras internal processing is always a many to many as in the following picture (where I used <code>features=2</code>, pressure and temperature, just as an example):</p>\n\n<p><a href=""https://i.stack.imgur.com/vTYa5.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vTYa5.jpg"" alt=""ManyToMany""></a></p>\n\n<p>In this image, I increased the number of steps to 5, to avoid confusion with the other dimensions.</p>\n\n<p>For this example:</p>\n\n<ul>\n<li>We have N oil tanks    </li>\n<li>We spent 5 hours taking measures hourly (time steps)    </li>\n<li>We measured two features:\n\n<ul>\n<li>Pressure P</li>\n<li>Temperature T</li>\n</ul></li>\n</ul>\n\n<p>Our input array should then be something shaped as <code>(N,5,2)</code>:</p>\n\n<pre><code>        [     Step1      Step2      Step3      Step4      Step5\nTank A:    [[Pa1,Ta1], [Pa2,Ta2], [Pa3,Ta3], [Pa4,Ta4], [Pa5,Ta5]],\nTank B:    [[Pb1,Tb1], [Pb2,Tb2], [Pb3,Tb3], [Pb4,Tb4], [Pb5,Tb5]],\n  ....\nTank N:    [[Pn1,Tn1], [Pn2,Tn2], [Pn3,Tn3], [Pn4,Tn4], [Pn5,Tn5]],\n        ]\n</code></pre>\n\n<h2>Inputs for sliding windows</h2>\n\n<p>Often, LSTM layers are supposed to process the entire sequences. Dividing windows may not be the best idea. The layer has internal states about how a sequence is evolving as it steps forward. Windows eliminate the possibility of learning long sequences, limiting all sequences to the window size. </p>\n\n<p>In windows, each window is part of a long original sequence, but by Keras they will be seen each as an independent sequence: </p>\n\n<pre><code>        [     Step1    Step2    Step3    Step4    Step5\nWindow  A:  [[P1,T1], [P2,T2], [P3,T3], [P4,T4], [P5,T5]],\nWindow  B:  [[P2,T2], [P3,T3], [P4,T4], [P5,T5], [P6,T6]],\nWindow  C:  [[P3,T3], [P4,T4], [P5,T5], [P6,T6], [P7,T7]],\n  ....\n        ]\n</code></pre>\n\n<p>Notice that in this case, you have initially only one sequence, but you\'re dividing it in many sequences to create windows. </p>\n\n<p>The concept of ""what is a sequence"" is abstract. The important parts are:</p>\n\n<ul>\n<li>you can have batches with many individual sequences    </li>\n<li>what makes the sequences be sequences is that they evolve in steps (usually time steps)    </li>\n</ul>\n\n<h1>Achieving each case with ""single layers""</h1>\n\n<h3>Achieving standard many to many:</h3>\n\n<p><a href=""https://i.stack.imgur.com/RXYW2.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RXYW2.jpg"" alt=""StandardManyToMany""></a></p>\n\n<p>You can achieve many to many with a simple LSTM layer, using <code>return_sequences=True</code>:</p>\n\n<pre><code>outputs = LSTM(units, return_sequences=True)(inputs)\n\n#output_shape -&gt; (batch_size, steps, units)\n</code></pre>\n\n<h3>Achieving many to one:</h3>\n\n<p>Using the exact same layer, keras will do the exact same internal preprocessing, but when you use <code>return_sequences=False</code> (or simply ignore this argument), keras will automatically discard the steps previous to the last:</p>\n\n<p><a href=""https://i.stack.imgur.com/GMe8r.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/GMe8r.jpg"" alt=""ManyToOne""></a></p>\n\n<pre><code>outputs = LSTM(units)(inputs)\n\n#output_shape -&gt; (batch_size, units) --&gt; steps were discarded, only the last was returned\n</code></pre>\n\n<h2>Achieving one to many</h2>\n\n<p>Now, this is not supported by keras LSTM layers alone. You will have to create your own strategy to multiplicate the steps. There are two good approaches:</p>\n\n<ul>\n<li>Create a constant multi-step input by repeating a tensor   </li>\n<li>Use a <code>stateful=True</code> to recurrently take the output of one step and serve it as the input of the next step (needs <code>output_features == input_features</code>)   </li>\n</ul>\n\n<h3>One to many with repeat vector</h3>\n\n<p>In order to fit to keras standard behavior, we need inputs in steps, so, we simply repeat the inputs for the length we want:</p>\n\n<p><a href=""https://i.stack.imgur.com/ZADR6.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZADR6.jpg"" alt=""OneToManyRepeat""></a></p>\n\n<pre><code>outputs = RepeatVector(steps)(inputs) #where inputs is (batch,features)\noutputs = LSTM(units,return_sequences=True)(outputs)\n\n#output_shape -&gt; (batch_size, steps, units)\n</code></pre>\n\n<h2>Understanding stateful = True</h2>\n\n<p>Now comes one of the possible usages of  <code>stateful=True</code> (besides avoiding loading data that can\'t fit your computer\'s memory at once)</p>\n\n<p>Stateful allows us to input ""parts"" of the sequences in stages. The difference is:</p>\n\n<ul>\n<li>In <code>stateful=False</code>, the second batch contains whole new sequences, independent from the first batch   </li>\n<li>In <code>stateful=True</code>, the second batch continues the first batch, extending the same sequences.  </li>\n</ul>\n\n<p>It\'s like dividing the sequences in windows too, with these two main differences: </p>\n\n<ul>\n<li>these windows do not superpose!!      </li>\n<li><code>stateful=True</code> will see these windows connected as a single long sequence   </li>\n</ul>\n\n<p>In <code>stateful=True</code>, every new batch will be interpreted as continuing the previous batch (until you call <code>model.reset_states()</code>). </p>\n\n<ul>\n<li>Sequence 1 in batch 2 will continue sequence 1 in batch 1. </li>\n<li>Sequence 2 in batch 2 will continue sequence 2 in batch 1.   </li>\n<li>Sequence n in batch 2 will continue sequence n in batch 1. </li>\n</ul>\n\n<p>Example of inputs, batch 1 contains steps 1 and 2, batch 2 contains steps 3 to 5:</p>\n\n<pre><code>                   BATCH 1                           BATCH 2\n        [     Step1      Step2        |    [    Step3      Step4      Step5\nTank A:    [[Pa1,Ta1], [Pa2,Ta2],     |       [Pa3,Ta3], [Pa4,Ta4], [Pa5,Ta5]],\nTank B:    [[Pb1,Tb1], [Pb2,Tb2],     |       [Pb3,Tb3], [Pb4,Tb4], [Pb5,Tb5]],\n  ....                                |\nTank N:    [[Pn1,Tn1], [Pn2,Tn2],     |       [Pn3,Tn3], [Pn4,Tn4], [Pn5,Tn5]],\n        ]                                  ]\n</code></pre>\n\n<p>Notice the alignment of tanks in batch 1 and batch 2! That\'s why we need <code>shuffle=False</code> (unless we are using only one sequence, of course).</p>\n\n<p>You can have any number of batches, indefinitely. (For having variable lengths in each batch, use <code>input_shape=(None,features)</code>. </p>\n\n<h2>One to many with stateful=True</h2>\n\n<p>For our case here, we are going to use only 1 step per batch, because we want to get one output step and make it be an input.</p>\n\n<p>Please notice that the behavior in the picture is not ""caused by"" <code>stateful=True</code>. We will force that behavior in a manual loop below. In this example, <code>stateful=True</code> is what ""allows"" us to stop the sequence, manipulate what we want, and continue from where we stopped. </p>\n\n<p><a href=""https://i.stack.imgur.com/ihAFT.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ihAFT.jpg"" alt=""OneToManyStateful""></a></p>\n\n<p>Honestly, the repeat approach is probably a better choice for this case. But since we\'re looking into <code>stateful=True</code>, this is a good example. The best way to use this is the next ""many to many"" case.</p>\n\n<p>Layer:</p>\n\n<pre><code>outputs = LSTM(units=features, \n               stateful=True, \n               return_sequences=True, #just to keep a nice output shape even with length 1\n               input_shape=(None,features))(inputs) \n    #units = features because we want to use the outputs as inputs\n    #None because we want variable length\n\n#output_shape -&gt; (batch_size, steps, units) \n</code></pre>\n\n<p>Now, we\'re going to need a manual loop for predictions:</p>\n\n<pre><code>input_data = someDataWithShape((batch, 1, features))\n\n#important, we\'re starting new sequences, not continuing old ones:\nmodel.reset_states()\n\noutput_sequence = []\nlast_step = input_data\nfor i in steps_to_predict:\n\n    new_step = model.predict(last_step)\n    output_sequence.append(new_step)\n    last_step = new_step\n\n #end of the sequences\n model.reset_states()\n</code></pre>\n\n<h3>Many to many with stateful=True</h3>\n\n<p>Now, here, we get a very nice application: given an input sequence, try to predict its future unknown steps. </p>\n\n<p>We\'re using the same method as in the ""one to many"" above, with the difference that:</p>\n\n<ul>\n<li>we will use the sequence itself to be the target data, one step ahead</li>\n<li>we know part of the sequence (so we discard this part of the results).   </li>\n</ul>\n\n<p><a href=""https://i.stack.imgur.com/4HPZB.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4HPZB.jpg"" alt=""ManyToManyStateful""></a></p>\n\n<p>Layer (same as above):</p>\n\n<pre><code>outputs = LSTM(units=features, \n               stateful=True, \n               return_sequences=True, \n               input_shape=(None,features))(inputs) \n    #units = features because we want to use the outputs as inputs\n    #None because we want variable length\n\n#output_shape -&gt; (batch_size, steps, units) \n</code></pre>\n\n<p><strong>Training:</strong></p>\n\n<p>We are going to train our model to predict the next step of the sequences:</p>\n\n<pre><code>totalSequences = someSequencesShaped((batch, steps, features))\n    #batch size is usually 1 in these cases (often you have only one Tank in the example)\n\nX = totalSequences[:,:-1] #the entire known sequence, except the last step\nY = totalSequences[:,1:] #one step ahead of X\n\n#loop for resetting states at the start/end of the sequences:\nfor epoch in range(epochs):\n    model.reset_states()\n    model.train_on_batch(X,Y)\n</code></pre>\n\n<p><strong>Predicting:</strong></p>\n\n<p>The first stage of our predicting involves ""ajusting the states"". That\'s why we\'re going to predict the entire sequence again, even if we already know this part of it:</p>\n\n<pre><code>model.reset_states() #starting a new sequence\npredicted = model.predict(totalSequences)\nfirstNewStep = predicted[:,-1:] #the last step of the predictions is the first future step\n</code></pre>\n\n<p>Now we go to the loop as in the one to many case. But <strong>don\'t reset states here!</strong>. We want the model to know in which step of the sequence it is (and it knows it\'s at the first new step because of the prediction we just made above)</p>\n\n<pre><code>output_sequence = [firstNewStep]\nlast_step = firstNewStep\nfor i in steps_to_predict:\n\n    new_step = model.predict(last_step)\n    output_sequence.append(new_step)\n    last_step = new_step\n\n #end of the sequences\n model.reset_states()\n</code></pre>\n\n<p>This approach was used in these answers and file:</p>\n\n<ul>\n<li><a href=""https://stackoverflow.com/questions/47594861/predicting-a-multiple-time-step-forward-of-a-time-series-using-lstm/47719094#47719094"">Predicting a multiple forward time step of a time series using LSTM</a>  </li>\n<li><a href=""https://stackoverflow.com/questions/48760472/how-to-use-the-keras-model-to-forecast-for-future-dates-or-events/48807811#48807811"">how to use the Keras model to forecast for future dates or events?</a>    </li>\n<li><a href=""https://github.com/danmoller/TestRepo/blob/master/TestBookLSTM.ipynb"" rel=""noreferrer"">https://github.com/danmoller/TestRepo/blob/master/TestBookLSTM.ipynb</a></li>\n</ul>\n\n<h1>Achieving complex configurations</h1>\n\n<p>In all examples above, I showed the behavior of ""one layer"". </p>\n\n<p>You can, of course, stack many layers on top of each other, not necessarly all following the same pattern, and create your own models. </p>\n\n<p>One interesting example that has been appearing is the ""autoencoder"" that has a ""many to one encoder"" followed by a ""one to many"" decoder:</p>\n\n<p><strong>Encoder:</strong></p>\n\n<pre><code>inputs = Input((steps,features))\n\n#a few many to many layers:\noutputs = LSTM(hidden1,return_sequences=True)(inputs)\noutputs = LSTM(hidden2,return_sequences=True)(outputs)    \n\n#many to one layer:\noutputs = LSTM(hidden3)(outputs)\n\nencoder = Model(inputs,outputs)\n</code></pre>\n\n<p><strong>Decoder:</strong></p>\n\n<p>Using the ""repeat"" method;</p>\n\n<pre><code>inputs = Input((hidden3,))\n\n#repeat to make one to many:\noutputs = RepeatVector(steps)(inputs)\n\n#a few many to many layers:\noutputs = LSTM(hidden4,return_sequences=True)(outputs)\n\n#last layer\noutputs = LSTM(features,return_sequences=True)(outputs)\n\ndecoder = Model(inputs,outputs)\n</code></pre>\n\n<p><strong>Autoencoder:</strong></p>\n\n<pre><code>inputs = Input((steps,features))\noutputs = encoder(inputs)\noutputs = decoder(outputs)\n\nautoencoder = Model(inputs,outputs)\n</code></pre>\n\n<p>Train with <code>fit(X,X)</code></p>\n\n<h1>Additional explanations</h1>\n\n<p>If you want details about how steps are calculated in LSTMs, or details about the <code>stateful=True</code> cases above, you can read more in this answer: <a href=""https://stackoverflow.com/questions/53955093/doubts-regarding-understanding-keras-lstms"">Doubts regarding `Understanding Keras LSTMs`</a></p>\n', 'IsAccepted': False, 'CreationDate': 1525788209}, {'QuestionId': 51914294, 'AnswerId': 51979623, 'URL': 'https://stackoverflow.com/questions/51914294/tfe-py-recordgradient-error-using-keras-with-tensorflow-back-end/51979623#51979623', 'QuestionTitle': 'TFE_Py_RecordGradient error using Keras with Tensorflow back end', 'Answer': '<p>From the stacktrace, I gather you\'re using <a href=""https://keras.io/#installation"" rel=""nofollow noreferrer""><code>keras</code></a> and not <a href=""https://www.tensorflow.org/api_docs/python/tf/keras"" rel=""nofollow noreferrer""><code>tf.keras</code></a>, which is packaged with the TensorFlow installation.</p>\n\n<p><code>tf.keras</code> implements the API spec defined in keras.io but adds additional TensorFlow-specific functionality, such as support for eager execution.</p>\n\n<p>So, I suspect all you need to do is change from something like:</p>\n\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense\n</code></pre>\n\n<p>to:</p>\n\n<pre><code>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n</code></pre>\n\n<p>Hope that helps.</p>\n', 'IsAccepted': False, 'CreationDate': 1535006058}, {'QuestionId': 38714959, 'AnswerId': 38737941, 'URL': 'https://stackoverflow.com/questions/38714959/understanding-keras-lstms/38737941#38737941', 'QuestionTitle': 'Understanding Keras LSTMs', 'Answer': '<p>First of all, you choose great tutorials(<a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"">1</a>,<a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"">2</a>) to start.</p>\n\n<p><strong>What Time-step means</strong>: <code>Time-steps==3</code> in X.shape (Describing data shape) means there are three pink boxes. Since in Keras each step requires an input, therefore the number of the green boxes should usually equal to the number of red boxes. Unless you hack the structure.</p>\n\n<p><strong>many to many vs. many to one</strong>: In keras, there is a <code>return_sequences</code> parameter when your initializing <code>LSTM</code> or <code>GRU</code> or <code>SimpleRNN</code>. When <code>return_sequences</code> is <code>False</code> (by default), then it is <strong>many to one</strong> as shown in the picture. Its return shape is <code>(batch_size, hidden_unit_length)</code>, which represent the last state. When <code>return_sequences</code> is <code>True</code>, then it is <strong>many to many</strong>. Its return shape is <code>(batch_size, time_step, hidden_unit_length)</code></p>\n\n<p><strong>Does the features argument become relevant</strong>: Feature argument means <strong>""How big is your red box""</strong> or what is the input dimension each step. If you want to predict from, say, 8 kinds of market information, then you can generate your data with <code>feature==8</code>.</p>\n\n<p><strong>Stateful</strong>: You can look up <a href=""https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py#L223"">the source code</a>. When initializing the state, if <code>stateful==True</code>, then the state from last training will be used as the initial state, otherwise it will generate a new state. I haven\'t turn on <code>stateful</code> yet. However, I disagree with that the <code>batch_size</code> can only be 1 when <code>stateful==True</code>. </p>\n\n<p>Currently, you generate your data with collected data. Image your stock information is coming as stream, rather than waiting for a day to collect all sequential, you would like to generate input data <strong>online</strong> while training/predicting with network. If you have 400 stocks sharing a same network, then you can set <code>batch_size==400</code>. </p>\n', 'IsAccepted': True, 'CreationDate': 1470211799}]","{57690048, 68984841, 69620683}","[""Dense layers are defined by having &quot;one&quot; connection between each item of the output and each item of the input. So even though you have 5 &quot;2x3 things&quot; in your output, they each just have one solitary weight associated with them about how they are connected to the 2x3 thing that is the input. Keras also defaults to using a bias <em>vector</em> (not bias tensor), so if the dense layer has dimension <code>k</code> and the final dimension of the previous layer is <code>n</code> you should expect <code>(n+1)k</code> trainable parameters. These will always be used with numpy-like broadcasting to make the lesser dimensional shape of the weight and bias vectors conformable to the actual shapes of the input tensors.</p>\n<p>It is customary to use Flatten as in your first example if you want to enforce the exact size of the coming dense layer. You would use multidimensional Dense layer when you want different &quot;(n - 1)D&quot; groups of connections to each Dense node. This is probably extremely rare for higher dimensional inputs because you'd typically want a CNN type of operation, but I could imagine maybe in some cases where a model predicts pixel-wise values or if you are generating a full nD output, like from the decoder portion of an encoder-decoder network, you might want a dense array of cells that match the dimensions of some expected structured output type like an image or video.</p>\n"", '<p>Yes, your understanding is correct.</p>\n<p>To achieve what you want, you need to define a custom keras layer. Let\'s suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original <code>Dense</code> layer (link: <a href=""https://github.com/tensorflow/tensorflow/blob/22ffec3a9c44133cba2182d60678d49bb372f020/tensorflow/python/keras/layers/core.py#L1077"" rel=""nofollow noreferrer"">github</a>), except that</p>\n<ol>\n<li>In the <code>build</code> function, the shape of <code>self.kernel</code> is (d0, i0, units) instead. You can get the value of <code>d0</code> as well as <code>i0</code> from <code>input_shape</code>.</li>\n<li>In the <code>call</code> function, to do the specified tensor multiplication between <code>inputs</code> and <code>self.kernel</code>, use <code>tf.einsum</code> with this equation: <code>tf.einsum(\'abc,bcg-&gt;abg\', inputs, self.kernel)</code></li>\n</ol>\n', ""Instead, if you call tf.keras.layers.Dense(), tensorflow basically creates a new dense layer and returns it to you, which you can then use to process your data. You could actually write this in two lines to make it more clear:</p>\n<pre><code>dense_layer = layers.Dense(128, activation='relu')   # We define a new dense layer\ndense_layer_output = dense_layer(pretrained_model.output)    # We apply the dense layer to the data\n</code></pre>\n""]","{'https://stackoverflow.com/questions/57690048/understanding-output-of-dense-layer-for-higher-dimension/57693306#57693306', 'https://stackoverflow.com/questions/69620683/explain-x-tf-keras-layers-dense128-activation-relupretrained-model-out/69621825#69621825', 'https://stackoverflow.com/questions/68984841/how-can-i-understand-the-kernel-of-tf-keras-layers-dense-for-rank-2/69001257#69001257'}",,0.2354899364129847,0.18725399314088478
4,68984841,tf.keras.layers.Dense,Documentation Replicability,How can I understand the kernel of tf.keras.layers.Dense for rank >2?,"<p>How can I understand the kernel of <code>tf.keras.layers.Dense</code> for rank &gt;2?</p>
<p>The official API doc states that:</p>
<blockquote>
<p>Note: If the input to the layer has a rank greater than 2, then Dense
computes the dot product between the inputs and the kernel along the
last axis of the inputs and axis 0 of the kernel (using tf.tensordot).
For example, if input has dimensions (batch_size, d0, d1), then we
create a kernel with shape (d1, units), and the kernel operates along
axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there
are batch_size * d0 such sub-tensors). The output in this case will
have shape (batch_size, d0, units).</p>
</blockquote>
<p>My understanding is that for a rank larger than 2 (for example rank 3) only <strong>one</strong> kernel is created and thus the same kernel is applied on all slices of the second dimension, like above.
That would consequently mean that the outputs for different indices of the second dimension are <strong>not independent</strong> of each other (especially during training).</p>
<p>Is my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?</p>
","<p>Yes, your understanding is correct.</p>
<p>To achieve what you want, you need to define a custom keras layer. Let's suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original <code>Dense</code> layer (link: <a href=""https://github.com/tensorflow/tensorflow/blob/22ffec3a9c44133cba2182d60678d49bb372f020/tensorflow/python/keras/layers/core.py#L1077"" rel=""nofollow noreferrer"">github</a>), except that</p>
<ol>
<li>In the <code>build</code> function, the shape of <code>self.kernel</code> is (d0, i0, units) instead. You can get the value of <code>d0</code> as well as <code>i0</code> from <code>input_shape</code>.</li>
<li>In the <code>call</code> function, to do the specified tensor multiplication between <code>inputs</code> and <code>self.kernel</code>, use <code>tf.einsum</code> with this equation: <code>tf.einsum('abc,bcg-&gt;abg', inputs, self.kernel)</code></li>
</ol>
","{64140960, 67053409, 57690048, 56127936, 51625702, 53694252, 52254253, 60662574, 60783216, 63239226, 45693020}","[{'QuestionId': 63239226, 'AnswerId': 77141527, 'URL': 'https://stackoverflow.com/questions/63239226/usage-of-tf-keras-layers-densefeatures/77141527#77141527', 'QuestionTitle': 'usage of tf.keras.layers.DenseFeatures', 'Answer': '<p>Using tf.keras.models.Sequential() configuration, instead of the Input-Output configuration with, tf.keras.layers.DenseFeatures(feature_columns) gives the same results. The only difference is that when you wish to look at the model with model.summary() the preprocessing layers are not individually shown.</p>\n', 'IsAccepted': False, 'CreationDate': 1695205748}, {'QuestionId': 67053409, 'AnswerId': 67054597, 'URL': 'https://stackoverflow.com/questions/67053409/dimension-of-output-in-dense-layer-keras/67054597#67054597', 'QuestionTitle': 'Dimension of output in Dense layer Keras', 'Answer': ""<p>The thing is after checking the <code>input shape</code> of the model from the <strong>first layer</strong>, it won't check or deal with other declared <code>input shape</code> inside that same model. For example, if you write your model the following way</p>\n<pre><code>sample_model.add(layers.Dense(32, input_shape=(4,)))\nsample_model.add(layers.Dense(16, input_shape = (44,)))\nsample_model.add(layers.Dense(8, input_shape = (32,)))\n</code></pre>\n<p>The program will always check the first declared <code>input shape</code> layer and discard the rest. So, if you start your first layer with <code>input_shape = (44,)</code>, you need to pass exact feature numbers to your model as input such as:</p>\n<pre><code>sam_x = np.random.rand(10,44)\nsam_y = np.array([0,1,1,0,1,0,0,1,0,1,])\nsample_model.fit(sam_x,sam_y)\n</code></pre>\n<p>Additionally, if you look at the <strong>Functional API</strong>, unlike the <strong>Sequential</strong> model, you must create and define a standalone <code>Input</code> layer that specifies the shape of input data. It's not learnable but simply a spec layer. It's a kind of gateway of the input data for the model. That means even if we define <code>input_shape</code> inside the other layers, they all will be discarded. For example:</p>\n<pre><code>nputs = keras.Input(shape=(4,))\n\ndense = layers.Dense(64, input_shape=(8,)) # dicard input_shape \nx = dense(inputs)\n\nx = layers.Dense(64, input_shape=(16,))(x) # dicard input_shape \noutputs = layers.Dense(10)(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs, name=&quot;mnist_model&quot;)\n</code></pre>\n<hr />\n<p>Here is a more complex example with <code>Conv2D</code> and MNIST.</p>\n<pre><code>encoder_input = keras.Input(shape=(28, 28, 1),)\nx = layers.Conv2D(16, 3, activation=&quot;relu&quot;,  input_shape=[32,32,3])(encoder_input)\nx = layers.Conv2D(32, 3, activation=&quot;relu&quot;,  input_shape=[64,64,3])(x) \nx = layers.MaxPooling2D(3)(x)\nx = layers.Conv2D(32, 3, activation=&quot;relu&quot;,  input_shape=[224,321,3])(x)\nx = layers.Conv2D(16, 3, activation=&quot;relu&quot;,  input_shape=[420,32,3])(x)\nx = layers.GlobalMaxPooling2D()(x)\nout = layers.Dense(10, activation='softmax')(x)\n\nencoder = keras.Model(encoder_input, out, name=&quot;encoder&quot;)\nencoder.summary()\n</code></pre>\n<pre><code>Model: &quot;encoder&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_15 (InputLayer)        [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 26, 26, 16)        160       \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 24, 24, 32)        4640      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 6, 6, 32)          9248      \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 4, 4, 16)          4624      \n_________________________________________________________________\nglobal_max_pooling2d_2 (Glob (None, 16)                0         \n_________________________________________________________________\ndense_56 (Dense)             (None, 10)                170       \n=================================================================\nTotal params: 18,842\nTrainable params: 18,842\nNon-trainable params: 0\n</code></pre>\n<pre><code>def pre_process(image, label):\n    return (image / 256)[...,None].astype('float32'), \n            tf.keras.utils.to_categorical(label, num_classes=10)\n\n(x, y), (_, _) = tf.keras.datasets.mnist.load_data('mnist')\n</code></pre>\n<pre><code>encoder.compile(\n          loss      = tf.keras.losses.CategoricalCrossentropy(),\n          metrics   = tf.keras.metrics.CategoricalAccuracy(),\n          optimizer = tf.keras.optimizers.Adam())\n\nencoder.fit(x, y, batch_size=256)\n4s 14ms/step - loss: 1.4303 - categorical_accuracy: 0.5279\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1618214295}, {'QuestionId': 67053409, 'AnswerId': 67054325, 'URL': 'https://stackoverflow.com/questions/67053409/dimension-of-output-in-dense-layer-keras/67054325#67054325', 'QuestionTitle': 'Dimension of output in Dense layer Keras', 'Answer': '<p>The <code>input_shape</code> keyword argument has an effect only on the first layer of a <code>Sequential</code>. The shape of the input of the other layers will be derived from their previous layer.</p>\n<p>That behaviour is hinted in the doc of <code>tf.keras.layers.InputShape</code>:</p>\n<blockquote>\n<p>When using InputLayer with Keras Sequential model, it can be skipped by moving the input_shape parameter to the first layer after the InputLayer.</p>\n</blockquote>\n<p>And in <a href=""https://www.tensorflow.org/guide/keras/sequential_model"" rel=""nofollow noreferrer"">the Sequential Model guide</a>.</p>\n<p>The behaviour can be confirmed by looking at the source of the <a href=""https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/engine/sequential.py#L159-L236"" rel=""nofollow noreferrer""><code>Sequential.add</code> method</a>:</p>\n<pre><code>if not self._layers:\n  if isinstance(layer, input_layer.InputLayer):\n    # Case where the user passes an Input or InputLayer layer via `add`.\n    set_inputs = True\n  else:\n    batch_shape, dtype = training_utils.get_input_shape_and_dtype(layer)\n    if batch_shape:\n      # Instantiate an input layer.\n      x = input_layer.Input(\n          batch_shape=batch_shape, dtype=dtype, name=layer.name + \'_input\')\n      # This will build the current layer\n      # and create the node connecting the current layer\n      # to the input layer we just created.\n      layer(x)\n      set_inputs = True \n</code></pre>\n<p>If there is no layers yet in the model, then an <code>Input</code> will be added to the model with the shape derived from the first layer of the model. This is done <strong>only</strong> if no layer is present yet in the model.</p>\n<p>That shape is either fully known (if <code>input_shape</code> has been passed to the first layer of the model) or will be fully known once the model is built (for example, with a call to <code>model.build(input_shape)</code>).</p>\n', 'IsAccepted': True, 'CreationDate': 1618213174}, {'QuestionId': 67053409, 'AnswerId': 67054321, 'URL': 'https://stackoverflow.com/questions/67053409/dimension-of-output-in-dense-layer-keras/67054321#67054321', 'QuestionTitle': 'Dimension of output in Dense layer Keras', 'Answer': ""<p>I think Keras will create (or preserves to create) an additional Input Layer - but as the second dense layer is added using <code>model.add()</code> it will automatically be connected to the layer before, and thus the extra input layer stays unconnected and <strong>is not part of the model</strong>.\n(I agree that it would be nice of Keras to hint at unconnected layers, I sometimes created unconnected layers when using the functional API and changed the inputs. Keras doesn't remind me that I had jumped several layers, I just wondered why the <code>summary()</code> was so short...)</p>\n"", 'IsAccepted': False, 'CreationDate': 1618213153}, {'QuestionId': 63239226, 'AnswerId': 63286157, 'URL': 'https://stackoverflow.com/questions/63239226/usage-of-tf-keras-layers-densefeatures/63286157#63286157', 'QuestionTitle': 'usage of tf.keras.layers.DenseFeatures', 'Answer': ""<p>Yes, your idea is reasonable. And actually you are free to choose either <strong>Keras functional API</strong> or <strong>Keras Sequential API</strong> when specifying your deep learning architecture.</p>\n<p>To complete your work, I would remove the last line and make some additional tweaks. What follows is a code snippet for completing the work you left by using Keras functional APIs:</p>\n<pre><code>feature_columns = []\nbins = [-125, -75, -50, -25, 0, 25, 50, 75, 125]\ntemp_num = feature_column.numeric_column('temp')\ntemp_buckets = feature_column.bucketized_column(temp_num, boundaries=bins)\nfeature_columns.append(temp_buckets)\nfeature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n\n# create a dictionary to associate column names with column values\ninputs = {}\ninputs[&quot;temp_num&quot;] = tf.keras.Input(shape=(1,), name=&quot;temp_num&quot;) \n\n# convert FeatureColumns into a single tensor layer\nx = feature_layer(inputs)\n\nx = tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.Dropout(.1)(x)\nout = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.Model(inputs=dict(inputs), outputs=out)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1596725251}, {'QuestionId': 64140960, 'AnswerId': 64141032, 'URL': 'https://stackoverflow.com/questions/64140960/result-of-dense-layer-in-keras/64141032#64141032', 'QuestionTitle': 'Result of Dense layer in keras', 'Answer': '<p>you can create a sub-model and make predictions</p>\n<pre><code>new_model = Model(model.input, model.layers[312].output)\nnew_model.predict(...)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1601480236}, {'QuestionId': 45693020, 'AnswerId': 45693132, 'URL': 'https://stackoverflow.com/questions/45693020/is-tf-layers-dense-a-single-layer/45693132#45693132', 'QuestionTitle': 'Is tf.layers.dense a single layer?', 'Answer': '<p><code>tf.layers.dense</code> (<code>tf.compat.v1.layers.dense</code>) is only one layer with a amount of nodes. You can check on TensorFlow web site about <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/layers/dense"" rel=""nofollow noreferrer"">tf.layers.dense (tf.compat.v1.layers.dense)</a></p>\n\n<pre><code>layer1 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\nlayer2 = tf.layers.dense(inputs=layer1, units=1024, activation=tf.nn.relu)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1502801173}, {'QuestionId': 60783216, 'AnswerId': 60783438, 'URL': 'https://stackoverflow.com/questions/60783216/what-exactly-does-tf-keras-layers-dense-do/60783438#60783438', 'QuestionTitle': 'What exactly does tf.keras.layers.Dense do?', 'Answer': '<p>Dense implements the operation: <code>output = activation(dot(input, kernel) + bias)</code> where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).</p>\n\n<blockquote>\n  <p>Note: If the input to the layer has a rank greater than 2, then it is\n  flattened prior to the initial dot product with kernel.</p>\n</blockquote>\n\n<p>Example:</p>\n\n<pre><code># as first layer in a sequential model:\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(16,)))\n# now the model will take as input arrays of shape (*, 16)\n# and output arrays of shape (*, 32)\n\n# after the first layer, you don\'t need to specify\n# the size of the input anymore:\nmodel.add(Dense(32))\n</code></pre>\n\n<p>Arguments :</p>\n\n<pre><code>&gt; units: Positive integer, dimensionality of the output space.\n\n&gt; activation: Activation function to use. If you don\'t specify anything,\n\n&gt; no activation is applied (ie. ""linear"" activation: a(x) = x).\n\n&gt; use_bias: Boolean, whether the layer uses a bias vector.\n\n&gt; kernel_initializer: Initializer for the kernel weights matrix.\n\n&gt; bias_initializer: Initializer for the bias vector. \n\n&gt;kernel_regularizer:Regularizer function applied to the kernel weights matrix.\n&gt; bias_regularizer: Regularizer function applied to the bias vector.\n\n&gt; activity_regularizer: Regularizer function applied to the output of the layer (its ""activation"").. \n\n&gt;kernel_constraint: Constraint function applied to the kernel weights matrix. \n\n&gt;bias_constraint: Constraint function applied to the bias vector.\n</code></pre>\n\n<p>Input shape:</p>\n\n<p>N-D tensor with shape: (batch_size, ..., input_dim). The most common situation would be a 2D input with shape (batch_size, input_dim).</p>\n\n<p>Output shape:</p>\n\n<p>N-D tensor with shape: (batch_size, ..., units). For instance, for a 2D input with shape (batch_size, input_dim), the output would have shape (batch_size, units).</p>\n', 'IsAccepted': True, 'CreationDate': 1584751901}, {'QuestionId': 60662574, 'AnswerId': 60662788, 'URL': 'https://stackoverflow.com/questions/60662574/dimensionality-of-keras-dense-layer/60662788#60662788', 'QuestionTitle': 'Dimensionality of Keras Dense layer', 'Answer': '<p>Depending on your application you could flatten after the Conv2D layer:</p>\n\n<pre><code>input_layer = Input((1, 1710))\nx = Reshape((38, 5, 9))(input_layer)\nx = Flatten()(x)\nx = Dense(512)(x)\nx = Dense(368)(x)\n\nLayer (type)                 Output Shape              Param #   \n_________________________________________________________________\ninput_1 (InputLayer)         [(None, 1, 1710)]         0         \n_________________________________________________________________\nreshape (Reshape)            (None, 38, 5, 9)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1710)              0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               876032    \n_________________________________________________________________\ndense_1 (Dense)              (None, 368)               188784    \n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1584053376}, {'QuestionId': 60662574, 'AnswerId': 60662718, 'URL': 'https://stackoverflow.com/questions/60662574/dimensionality-of-keras-dense-layer/60662718#60662718', 'QuestionTitle': 'Dimensionality of Keras Dense layer', 'Answer': ""<p>It's the <code>Conv2D</code> layer. The convolutional layer is producing 38x5 outputs of length 9, and then your <code>Dense</code> layer is taking each of the 38x5 length 9 sequences as input and converting it to a length 512 sequence as output.</p>\n\n<p>To get rid of the spatial dependence, you'll want to use something like a pooling layer, possibly a <code>GlobalMaxPool2D</code>. This will consolidate the data into only the channel dimension, and produce a <code>(None, 9)</code> shaped output, which will lead to your expected shapes from the <code>Dense</code> layers.</p>\n"", 'IsAccepted': False, 'CreationDate': 1584052804}, {'QuestionId': 57690048, 'AnswerId': 57693306, 'URL': 'https://stackoverflow.com/questions/57690048/understanding-output-of-dense-layer-for-higher-dimension/57693306#57693306', 'QuestionTitle': 'Understanding output of Dense layer for higher dimension', 'Answer': ""<p>This is tricky but it does fit with the documentation from Keras on dense layers,</p>\n<blockquote>\n<p>Output shape</p>\n<p><code>nD</code> tensor with shape: <code>(batch_size, ..., units)</code>. For instance, for a 2D input with shape <code>(batch_size, input_dim)</code>, the output would have shape <code>(batch_size, units)</code></p>\n</blockquote>\n<p>Note it is not the clearest, but they are saying with the <code>...</code> that the final dimension of the input shape will be elided by the number of dense connections. Basically, for each item of the final dimension, create a connection to each of the requested dense nodes in the coming dense layer.</p>\n<p>In your case you have something which is 2 x 3 x 1. So there is &quot;one thing&quot; (the 2 x 3 thing) to be connected to each of the 5 dense layer nodes, hense 2 x 3 x 5. You can think of it like channels of a CNN layer in this particular case. There is a distinct 2 x 3 sheet of outputs for each of the 5 output &quot;nodes&quot;.</p>\n<p>In a purely 2-D case <code>(batch_size, units)</code> ... then each item iterated by the final dimension <code>units</code> is itself a scalar value, so you end up with something of exactly the size of the number of dense nodes requested.</p>\n<p>But in a higher-dimensional case, each item you iterate along the final dimension of the input will itself still be a higher-dimensional thing, and so the output is k distinct &quot;clones&quot; of those higher-dimensional things, where k is the dense layer size requested, and by &quot;clone&quot; we mean the output for a single dense connection has the same shape as the the items in the final dimension of the input.</p>\n<p>Then the Dense-ness means that each specific element of that output has a connection to each element of the corresponding set of inputs. But be careful about this. Dense layers are defined by having &quot;one&quot; connection between each item of the output and each item of the input. So even though you have 5 &quot;2x3 things&quot; in your output, they each just have one solitary weight associated with them about how they are connected to the 2x3 thing that is the input. Keras also defaults to using a bias <em>vector</em> (not bias tensor), so if the dense layer has dimension <code>k</code> and the final dimension of the previous layer is <code>n</code> you should expect <code>(n+1)k</code> trainable parameters. These will always be used with numpy-like broadcasting to make the lesser dimensional shape of the weight and bias vectors conformable to the actual shapes of the input tensors.</p>\n<p>It is customary to use Flatten as in your first example if you want to enforce the exact size of the coming dense layer. You would use multidimensional Dense layer when you want different &quot;(n - 1)D&quot; groups of connections to each Dense node. This is probably extremely rare for higher dimensional inputs because you'd typically want a CNN type of operation, but I could imagine maybe in some cases where a model predicts pixel-wise values or if you are generating a full nD output, like from the decoder portion of an encoder-decoder network, you might want a dense array of cells that match the dimensions of some expected structured output type like an image or video.</p>\n"", 'IsAccepted': True, 'CreationDate': 1566998035}, {'QuestionId': 52254253, 'AnswerId': 57319659, 'URL': 'https://stackoverflow.com/questions/52254253/how-does-tf-layers-dense-interact-with-inputs-of-higher-dim/57319659#57319659', 'QuestionTitle': 'How does tf.layers.dense() interact with inputs of higher dim?', 'Answer': ""<p>You can verify your expectation by checking the shape of the dense kernel as follows.</p>\n\n<pre><code>&gt;&gt;&gt; inputx = tf.placeholder(float, shape=[2,3,4])\n&gt;&gt;&gt; dense_layer = tf.layers.dense(inputx, 128, tf.nn.relu)\n&gt;&gt;&gt; g=tf.get_default_graph()\n&gt;&gt;&gt; g.get_collection('variables')\n[&lt;tf.Variable 'dense/kernel:0' shape=(4, 128) dtype=float32_ref&gt;, &lt;tf.Variable 'dense/bias:0' shape=(128,) dtype=float32_ref&gt;]\n</code></pre>\n\n<p>The behavior of the dense layer is the same as a conv layer. </p>\n\n<p>You can consider inputx as an image which has width=2, height=3 and channel=4 and the dense layer as a conv layer which has 128 filters and filters size is 1*1.</p>\n"", 'IsAccepted': False, 'CreationDate': 1564717599}, {'QuestionId': 56127936, 'AnswerId': 56128078, 'URL': 'https://stackoverflow.com/questions/56127936/what-is-the-rank-in-the-context-of-keras/56128078#56128078', 'QuestionTitle': 'What is the Rank in the context of Keras?', 'Answer': '<p>It\'s just the number of dimensions of a Tensor.</p>\n\n<blockquote>\n  <p>The <strong>rank</strong> of a <code>tf.Tensor</code> object is its number of dimensions. Synonyms for rank include <strong>order</strong> or <strong>degree</strong> or <strong>n-dimension</strong>. Note that rank in TensorFlow is not the same as matrix rank in mathematics. <a href=""https://www.tensorflow.org/guide/tensors#rank"" rel=""nofollow noreferrer"">[...]</a></p>\n</blockquote>\n', 'IsAccepted': True, 'CreationDate': 1557828971}, {'QuestionId': 56127936, 'AnswerId': 56128073, 'URL': 'https://stackoverflow.com/questions/56127936/what-is-the-rank-in-the-context-of-keras/56128073#56128073', 'QuestionTitle': 'What is the Rank in the context of Keras?', 'Answer': '<p>It means the dimensions of the input tensor, and not the rank of the input matrix.</p>\n', 'IsAccepted': False, 'CreationDate': 1557828956}, {'QuestionId': 53694252, 'AnswerId': 53698066, 'URL': 'https://stackoverflow.com/questions/53694252/tf-sparse-to-dense-shape-must-be-rank-1-but-is-rank-0/53698066#53698066', 'QuestionTitle': 'tf.sparse_to_dense: Shape must be rank 1 but is rank 0', 'Answer': '<p>The specific error you\'ve seen is trying to say your <code>output_shape</code> should be a 1-D Tensor, like <code>(32,)</code>, rather than 0-D Tensor as you had there, <code>32</code>. But I worried this simple change will not solve your problem.</p>\n\n<p>One thing I don\'t understand is why your <code>x</code> is a 3-D tensor when you said you have just 10 indices. Technically speaking, <a href=""https://www.tensorflow.org/api_docs/python/tf/sparse_to_dense"" rel=""nofollow noreferrer""><code>sparse_indices</code> can be a 2-D tensor at most</a>. My understanding of <code>tf.sparse_to_dense</code> is that it\'s quite similar to making a sparse tensor. So the number <code>2</code> in your <code>(10, 2)</code> already decided that the output tensor will be 2-D. The <code>None</code>, like variant sample size, should be handled differently.</p>\n\n<p>Following this logic, another problem you may find is the <code>output_shape</code> should be <code>(32, 32)</code> rather than <code>(32,)</code> as the simple fix mentioned above. The length of the tuple should match the shape (last axis specifically) of <code>sparse_indices</code>.</p>\n\n<p>With all these in mind, I think a tensorflow only MVCE mimicking your example could be:</p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nx = tf.placeholder(tf.float32, shape=(10, 2))\nidx = tf.cast(x*15.5+15.5, tf.int32)\nz = tf.sparse_to_dense(idx, (32, 32), 1.0, 0.0, name=\'sparse_tensor\')\n\nwith tf.Session() as sess:\n    print(sess.run(\n        z, feed_dict={x: np.arange(20, dtype=np.float32).reshape((10, 2))/20})\n    )\n</code></pre>\n\n<p>Just to point out: <a href=""https://www.tensorflow.org/api_docs/python/tf/sparse_to_dense"" rel=""nofollow noreferrer"">The <code>tf.sparse_to_dense</code> ""FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Create a tf.sparse.SparseTensor and use tf.sparse.to_dense instead.""</a></p>\n', 'IsAccepted': False, 'CreationDate': 1544401556}, {'QuestionId': 51625702, 'AnswerId': 51626334, 'URL': 'https://stackoverflow.com/questions/51625702/how-does-tf-layers-dense-create-the-inputs-kernel-weight-matrix/51626334#51626334', 'QuestionTitle': 'How does tf.layers.dense create the inputs.kernel weight matrix?', 'Answer': '<p>They have started mixing <code>tf.layers</code> with <code>tf.keras</code>. In you referenced implementation the class inherits all methods from <a href=""https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/layers/core.py#L37"" rel=""nofollow noreferrer""><code>keras_layers</code></a></p>\n\n<p>""Following the money"", the implementation is <a href=""https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/keras/layers/core.py#L878-L921"" rel=""nofollow noreferrer"">here</a> and boils down to</p>\n\n<pre><code>def build(...):\n    self.kernel = self.add_variable(\'kernel\', ...\n    self.bias = self.add_variable\ndef call(...):\n    # ...\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\n    # ...\n    if self.activation is not None:\n        return self.activation(outputs)\n    return outputs\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1533103479}, {'QuestionId': 45693020, 'AnswerId': 45712481, 'URL': 'https://stackoverflow.com/questions/45693020/is-tf-layers-dense-a-single-layer/45712481#45712481', 'QuestionTitle': 'Is tf.layers.dense a single layer?', 'Answer': '<p><code>tf.layers.dense</code> adds a single layer to your network. The second argument is the number of neurons/nodes of the layer. For example:</p>\n\n<pre><code># no hidden layers, dimension output layer = 1\noutput = tf.layers.dense(tf_x, 1, tf.nn.relu)\n\n# one hidden layer, dimension hidden layer = 10,  dimension output layer = 1\nhidden = tf.layers.dense(tf_x, 10, tf.nn.relu)\noutput = tf.layers.dense(hidden, 1, tf.nn.relu)\n</code></pre>\n\n<blockquote>\n  <p>My network seemed to work properly with just 1 layer, so I was curious about the setup.</p>\n</blockquote>\n\n<p>That is possible, for some tasks you will get decent results without hidden layers.</p>\n', 'IsAccepted': True, 'CreationDate': 1502883266}]",{68984841},"[""<p>Yes, your understanding is correct.</p>\n<p>To achieve what you want, you need to define a custom keras layer. Let's suppose the input to the layer is of shape (batch_size, d0, i0)."", '<p>Yes, your understanding is correct.</p>\n<p>To achieve what you want, you need to define a custom keras layer. Let\'s suppose the input to the layer is of shape (batch_size, d0, i0). Most part of the layer will be similar to the original <code>Dense</code> layer (link: <a href=""https://github.com/tensorflow/tensorflow/blob/22ffec3a9c44133cba2182d60678d49bb372f020/tensorflow/python/keras/layers/core.py#L1077"" rel=""nofollow noreferrer"">github</a>), except that</p>\n<ol>\n<li>In the <code>build</code> function, the shape of <code>self.kernel</code> is (d0, i0, units) instead. You can get the value of <code>d0</code> as well as <code>i0</code> from <code>input_shape</code>.</li>\n<li>In the <code>call</code> function, to do the specified tensor multiplication between <code>inputs</code> and <code>self.kernel</code>, use <code>tf.einsum</code> with this equation: <code>tf.einsum(\'abc,bcg-&gt;abg\', inputs, self.kernel)</code></li>\n</ol>\n']",{'https://stackoverflow.com/questions/68984841/how-can-i-understand-the-kernel-of-tf-keras-layers-dense-for-rank-2/69001257#69001257'},,0.20295551811512724,0.03773022132945003
4,53079436,tf.cond,Documentation Replication on Other Examples,tensorflow Tf.cond giving unexpected output,"<p>I seem to be having a misunderstanding on how <code>tf.cond</code> works. In the tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer"">documentation</a>, it gives the following example:</p>

<pre><code>z = tf.multiply(a, b)
result = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))
</code></pre>

<p>The result of the example, if <code>x&lt;y</code> is <code>True</code> is <code>tf.add(x,z)</code> else <code>tf.square(y)</code></p>

<p>Following this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation.</p>

<p>in my example, <code>deterministic_action = 4</code>, <code>random_action = 11</code>, <code>chose_random=False</code>. The <code>stochastic_action</code> should be <code>4</code>, instead it is <code>1</code>.
Where did the value 1 come from?</p>

<pre><code>#!/usr/bin/env python3

import tensorflow as tf
import numpy as np

with tf.Graph().as_default():
    with tf.device('/cpu:0'):
        stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"")
        eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0))
        with tf.variable_scope('test_cond') as sc:
            deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4
            random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11
            chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) &lt; eps # False because eps = 0
            stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1
            #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action)


    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init, feed_dict={stochastic_ph: True})
    print (""s_ph = "", stochastic_ph)
    d_action = sess.run(deterministic_action)
    print (""det_action= "", d_action)
    r_action = sess.run(random_action)
    print (""rand_action= "", r_action)
    e = sess.run(eps)
    c_action = sess.run(chose_random)
    print (""chose_rand= "", c_action)
    s_action = sess.run(stochastic_action)
    print (""s_action= "", s_action)
    #output = sess.run(output_action)
</code></pre>

<p>here is the output:</p>

<pre><code>python random_vec.py
2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  1
</code></pre>
","<p>This is because you are evaluating again in a new sess.run. 
Since you are generating a random number for deterministic_action, the result turns out to be the next random number after 4, which is 1.
Here is the result of your code, when I extract the value of deterministic_action as well in the last step.</p>

<p>Modification:</p>

<pre><code>print (""s_ph = "", stochastic_ph)
d_action = sess.run(deterministic_action)
print (""det_action= "", d_action)
r_action = sess.run(random_action)
print (""rand_action= "", r_action)
e = sess.run(eps)
c_action = sess.run(chose_random)
print (""chose_rand= "", c_action)
s_action, d_action = sess.run([stochastic_action, deterministic_action])
print (""s_action= "", s_action)
print (""det_action= "", d_action)
</code></pre>

<p>Result:</p>

<pre><code>s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  1
det_action=  1
</code></pre>

<p>Now all you need to do is run everything in one sess.run</p>

<pre><code>d_action, r_action, e,  c_action, s_action = sess.run([deterministic_action, random_action, eps, chose_random, stochastic_action])
print (""det_action= "", d_action)
print (""rand_action= "", r_action)
print (""chose_rand= "", c_action)
print (""s_action= "", s_action)
</code></pre>

<p>Result:</p>

<pre><code>s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  4
</code></pre>

<p>Update:</p>

<p>I was not clear on why the random_uniform generates different values when seed is set. This is because the code is running with the same session object that it initialized the variables with.
Modifying the code with a new session object, this is what happens:</p>

<pre><code>print (""s_ph = "", stochastic_ph)
d_action = sess.run(deterministic_action)
print (""det_action= "", d_action)
sess.close()
sess = tf.Session()
sess.run(init, feed_dict={stochastic_ph: True})
s_action = sess.run(stochastic_action)
print (""s_action= "", s_action)
</code></pre>

<p>Result:</p>

<pre><code>s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
s_action=  4
</code></pre>
","{57063872, 70981958, 55805576, 47537064, 50224523, 53966350, 68379727, 37063952, 51422833, 48518895, 47430801}","[{'QuestionId': 68379727, 'AnswerId': 68382747, 'URL': 'https://stackoverflow.com/questions/68379727/tf-cond-returning-a-tensor-of-shape-unknown/68382747#68382747', 'QuestionTitle': 'tf.cond() returning a tensor of shape unknown', 'Answer': '<p>You have a few problems in your code.</p>\n<ol>\n<li><a href=""https://www.tensorflow.org/api_docs/python/tf/math/segment_mean"" rel=""nofollow noreferrer""><code>tf.math.segment_mean()</code></a> expects <code>class_segments</code> to have the same shape as first dimension of your input <code>t</code>. So <code>None</code> must be equal <code>6</code> in order for your code to run. This is most likely cause of you getting the <em>unknown</em> shape -  because the shape of your tensors depends on <code>None</code> which is determined on runtime. You could apply transformation for your code to run (not sure if that is what you are trying to achieve), eg.</li>\n</ol>\n<pre class=""lang-py prettyprint-override""><code>a = tf.math.segment_mean(tf.transpose(t), class_segments)\n</code></pre>\n<ol start=""2"">\n<li>In <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer""><code>tf.cond()</code></a> <code>true_fn</code> and <code>false_fn</code> must return tensors of same shape. In your case  <code>true_fn</code> returns <code>(None, 6)</code> because of <a href=""https://numpy.org/doc/stable/user/basics.broadcasting.html"" rel=""nofollow noreferrer"">broadcasting</a> and <code>false_fn</code> returns tensor of shape <code>(6,)</code>.</li>\n<li>The predicate in <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer""><code>tf.cond()</code></a> must be reduced to a rank 0. For example, if you were to apply\n<code>b = tf.math.argmax(tf.math.segment_mean(tf.transpose(t), class_segments), 0)</code>\nthen the shape of <code>b</code> would be <code>(None)</code> and the predicate <code>pred</code> in <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer""><code>tf.cond()</code></a> will be <a href=""https://numpy.org/doc/stable/user/basics.broadcasting.html"" rel=""nofollow noreferrer"">broadcasted</a> to the same shape (which will raise an error).</li>\n</ol>\n', 'IsAccepted': True, 'CreationDate': 1626284059}, {'QuestionId': 70981958, 'AnswerId': 70982571, 'URL': 'https://stackoverflow.com/questions/70981958/tensorflow-typeerror-when-using-tf-cond/70982571#70982571', 'QuestionTitle': 'Tensorflow - TypeError when using tf.cond()', 'Answer': '<p>The problem is that <code>tf.cond</code> will not work with <code>KerasTensors</code> (tensors that are returned by <code>Keras</code> layers). You could try wrapping <code>tf.cond</code> in a custom layer:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\nclass ConditionalActivationLayer(tf.keras.layers.Layer):\n\n  def call(self, inputs):\n    x1, x = inputs[0], inputs[1]\n    return tf.cond(tf.less(x1,5), lambda :tf.nn.relu(x), lambda :tf.nn.leaky_relu(x, alpha=0.1))\n\nx = tf.keras.Input(shape=(224, 224, 3), batch_size=None)\nx1= tf.keras.Input(1, dtype=tf.int32)\ny = ConditionalActivationLayer()([x1, x])\nmodel=tf.keras.models.Model(inputs=[x,x1], outputs=[y])\nmodel.summary()\n</code></pre>\n<pre><code>odel: &quot;model&quot;\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_6 (InputLayer)           [(None, 1)]          0           []                               \n                                                                                                  \n input_5 (InputLayer)           [(None, 224, 224, 3  0           []                               \n                                )]                                                                \n                                                                                                  \n conditional_activation_layer (  (None, 224, 224, 3)  0          [\'input_6[0][0]\',                \n ConditionalActivationLayer)                                      \'input_5[0][0]\']                \n                                                                                                  \n==================================================================================================\nTotal params: 0\nTrainable params: 0\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code></pre>\n<p>Also works with:</p>\n<pre><code>return tf.cond(tf.less(x1,5), lambda :tf.keras.layers.ReLU()(x), lambda :tf.keras.layers.LeakyReLU(alpha=0.1)(x))\n</code></pre>\n<p>It is just a matter of taste.</p>\n<p>You could also disable eager execution and it should work, since the input layers are now normal tensors:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\nx = tf.keras.Input(shape=(224, 224, 3), batch_size=None)\nx1=tf.keras.Input(1, dtype=tf.int32)\nprint(type(x1), type(x)) #&lt;class \'tensorflow.python.framework.ops.Tensor\'&gt; &lt;class \'tensorflow.python.framework.ops.Tensor\'&gt;\ny = tf.cond(tf.less(x1,5), lambda :tf.keras.layers.ReLU()(x), lambda :tf.keras.layers.LeakyReLU(alpha=0.1)(x))\nmodel=tf.keras.models.Model(inputs=[x,x1], outputs=[y])\nmodel.summary()\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1643958288}, {'QuestionId': 37063952, 'AnswerId': 37064128, 'URL': 'https://stackoverflow.com/questions/37063952/confused-by-the-behavior-of-tf-cond/37064128#37064128', 'QuestionTitle': 'Confused by the behavior of `tf.cond`', 'Answer': '<p><strong>TL;DR:</strong> If you want <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/control_flow_ops.html#cond"" rel=""nofollow noreferrer""><code>tf.cond()</code></a> to perform a side effect (like an assignment) in one of the branches, you must create the op that performs the side effect <strong>inside</strong> the function that you pass to <code>tf.cond()</code>.</p>\n<p>The behavior of <code>tf.cond()</code> is a little unintuitive. Because execution in a TensorFlow graph flows forward through the graph, all operations that you refer to in <strong>either</strong> branch must execute before the conditional is evaluated. This means that both the true and the false branches receive a control dependency on the <code>tf.assign()</code> op, and so <code>y</code> always gets set to <code>2</code>, even if pred is <code>False</code>.</p>\n<p>The solution is to create the <code>tf.assign()</code> op inside the function that defines the true branch. For example, you could structure your code as follows:</p>\n<pre class=""lang-py prettyprint-override""><code>pred = tf.placeholder(tf.bool, shape=[])\nx = tf.Variable([1])\ndef update_x_2():\n  with tf.control_dependencies([tf.assign(x, [2])]):\n    return tf.identity(x)\ny = tf.cond(pred, update_x_2, lambda: tf.identity(x))\nwith tf.Session() as session:\n  session.run(tf.initialize_all_variables())\n  print(y.eval(feed_dict={pred: False}))  # ==&gt; [1]\n  print(y.eval(feed_dict={pred: True}))   # ==&gt; [2]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1462508113}, {'QuestionId': 51422833, 'AnswerId': 64806807, 'URL': 'https://stackoverflow.com/questions/51422833/why-does-tf-print-not-work/64806807#64806807', 'QuestionTitle': 'Why does tf.Print() not work?', 'Answer': ""<p>Although, @Ignacio Peletier answer is a fully helpful one it depends on an external site. I find it peculiar that noone mentioned that here. Anyway, for sticking with the rules I provide the answer also here (without having to visit the external link that is) with some extra information:</p>\n<p>For <code>tf.Print</code> to actually print something it should belong to the graph. To do so, you simply reuse the returned Tensor from the <code>tf.Print</code> and pass it to the next op. <em>Passing it</em> over to the next op is crucial for actually displaying the message.</p>\n<p>So, to use your example it could be rewritten:</p>\n<pre><code>def cum_sum(prev, cur):\n    non_zeros = tf.equal(cur, 0.)\n    non_zeros = tf.Print(non_zeros, [non_zeros], &quot;message &quot;)\n    cur = tf.Print(cur, [cur])\n    return cur\n</code></pre>\n<p>which will print <code>cur</code> but not <code>non_zeros</code> since this node is dangling. Also, I am not sure I can rewrite your code so that <code>non_zeros</code> is displayed since it is not actually used in your code after the point you define them (so tensorflow will simply ignore it in non-eager mode).</p>\n<p>In general for (a non dangling) node, let's name it <code>result</code> somewhere in your code:</p>\n<pre><code>result = tf.some_op_here()\n# the following is actually displaying the contents of result\nresult = tf.Print(result, [result], 'some arbitrary message here to be displayed before the actual message')\ntf.another_op_here_using_result(result)\n</code></pre>\n<p>will print the (possibly concatenated) contents of <code>result</code>. For a control over the amount of information shown you can also use the parameter <code>summarize=x</code> where <code>x</code> is the number of parameters being displayed.</p>\n"", 'IsAccepted': False, 'CreationDate': 1605194775}, {'QuestionId': 53966350, 'AnswerId': 53970881, 'URL': 'https://stackoverflow.com/questions/53966350/tf-print-has-no-result/53970881#53970881', 'QuestionTitle': 'tf.print has no result', 'Answer': '<p>From the <a href=""https://www.tensorflow.org/api_docs/python/tf/Print"" rel=""nofollow noreferrer"">documentation of <code>tf.Print</code></a> - that\'s deprecated and suggests to use <code>tf.print</code>:</p>\n\n<p>Note that tf.print returns a no-output operator that directly prints the output. <strong>Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in <code>session.run</code> or used as a control dependency for other operators.</strong></p>\n\n<p>This is only a concern in graph mode. Below is an example of how to ensure <code>tf.print</code> executes in graph mode:</p>\n\n<pre><code>sess = tf.Session()\nwith sess.as_default():\n    tensor = tf.range(10)\n    print_op = tf.print(tensor)\n    with tf.control_dependencies([print_op]):\n      out = tf.add(tensor, tensor)\n    sess.run(out)\n</code></pre>\n\n<p>Hence, if you enable the eager mode your code will work as you expected, if you want to continue using the static-graph mode you have to use <code>sess.run</code></p>\n', 'IsAccepted': False, 'CreationDate': 1546097692}, {'QuestionId': 57063872, 'AnswerId': 57064127, 'URL': 'https://stackoverflow.com/questions/57063872/weird-tf-print-bug/57064127#57064127', 'QuestionTitle': 'Weird tf.Print bug', 'Answer': '<p>Found answer here:</p>\n\n<p><a href=""https://epcsirmaz.blogspot.com/2018/06/display-full-value-of-tensor-in.html"" rel=""nofollow noreferrer"">https://epcsirmaz.blogspot.com/2018/06/display-full-value-of-tensor-in.html</a></p>\n\n<p>Basically, when using Keras, you have to wrap it in a lambda layer.</p>\n', 'IsAccepted': False, 'CreationDate': 1563304884}, {'QuestionId': 55805576, 'AnswerId': 55810588, 'URL': 'https://stackoverflow.com/questions/55805576/tensorflow-reports-typeerror-list-of-tensors-when-single-tensor-expected-when/55810588#55810588', 'QuestionTitle': 'Tensorflow reports &#39;TypeError: List of Tensors when single Tensor expected&#39; when using tf.cond()', 'Answer': '<p>I have tried that <code>tf.convert_to_tensor([1, src_shape[0]])</code> works. It is an alternative solution.</p>\n', 'IsAccepted': False, 'CreationDate': 1556020209}, {'QuestionId': 55805576, 'AnswerId': 55806515, 'URL': 'https://stackoverflow.com/questions/55805576/tensorflow-reports-typeerror-list-of-tensors-when-single-tensor-expected-when/55806515#55806515', 'QuestionTitle': 'Tensorflow reports &#39;TypeError: List of Tensors when single Tensor expected&#39; when using tf.cond()', 'Answer': '<p>One way would be to use tf.stack, which stacks a list of rank-R tensors into one rank-(R+1) tensor.</p>\n\n<pre><code>lambda: tf.stack([1, src_shape[0]], axis=0)\n</code></pre>\n\n<p>Another solution would be using tf.concat using the right tf.reshape commands. </p>\n', 'IsAccepted': True, 'CreationDate': 1556005652}, {'QuestionId': 53966350, 'AnswerId': 53966421, 'URL': 'https://stackoverflow.com/questions/53966350/tf-print-has-no-result/53966421#53966421', 'QuestionTitle': 'tf.print has no result', 'Answer': ""<pre><code>import tensorflow as tf\n\na = tf.constant([1.0, 3.0])\n\ninit_op = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n    print(sess.run(a))\n</code></pre>\n\n<p>is what I'd do. Import tensorflow, set up your variables, set up and run the initializer for them, and then print the session evaluating the constant</p>\n"", 'IsAccepted': False, 'CreationDate': 1546053578}, {'QuestionId': 51422833, 'AnswerId': 51423239, 'URL': 'https://stackoverflow.com/questions/51422833/why-does-tf-print-not-work/51423239#51423239', 'QuestionTitle': 'Why does tf.Print() not work?', 'Answer': '<p>tf.Print does not work like that. Print nodes need to be into the graph in order to exectue. I highly recommend you to check out <a href=""https://towardsdatascience.com/using-tf-print-in-tensorflow-aa26e1cff11e"" rel=""noreferrer"">this</a> tutorial to learn how to use it.</p>\n\n<p>If you have any questions feel free to ask.</p>\n', 'IsAccepted': True, 'CreationDate': 1532004849}, {'QuestionId': 37063952, 'AnswerId': 50460472, 'URL': 'https://stackoverflow.com/questions/37063952/confused-by-the-behavior-of-tf-cond/50460472#50460472', 'QuestionTitle': 'Confused by the behavior of `tf.cond`', 'Answer': '<pre><code>pred = tf.constant(False)\nx = tf.Variable([1])\n\ndef update_x_2():\n    assign_x_2 = tf.assign(x, [2])\n    with tf.control_dependencies([assign_x_2]):\n        return tf.identity(x)\ny = tf.cond(pred, update_x_2, lambda: tf.identity(x))\nwith tf.Session() as session:\n  session.run(tf.initialize_all_variables())\n  print(y.eval())\n</code></pre>\n\n<p>This will get the result of <code>[1]</code>.</p>\n\n<p>This answer is quite the same as the above answer. But what I wanna share is you can put every ops you would like to use in its branch function. Because, given your example code, tensor <code>x</code> is can be directly used by the <code>update_x_2</code> function.</p>\n', 'IsAccepted': False, 'CreationDate': 1526967319}, {'QuestionId': 50224523, 'AnswerId': 50225388, 'URL': 'https://stackoverflow.com/questions/50224523/why-does-tf-cond-appear-to-run-both-branches/50225388#50225388', 'QuestionTitle': 'Why does tf.cond appear to run both branches?', 'Answer': ""<p>The way that <code>tf.cond</code> works is that it runs both branches of the if statement and then it makes sure that the right one is the one that has it's value assigned into the output. That's why you are seeing the print statement show up when you don't expect it to.</p>\n\n<p>It looks like the purpose of your <code>tf.cond</code> statements is just to enable or disable dropout. The way I do that in my own code is to make the dropout probability be a placeholder with a default of 1.0. Then during training I feed in the proper dropout probability and during validation/testing I leave the default value and that effectively disables the dropout.</p>\n"", 'IsAccepted': False, 'CreationDate': 1525750298}, {'QuestionId': 48518895, 'AnswerId': 48518911, 'URL': 'https://stackoverflow.com/questions/48518895/tensorflow-wrong-ouput-in-print/48518911#48518911', 'QuestionTitle': 'Tensorflow wrong ouput in print', 'Answer': '<p>Problem is </p>\n\n<pre><code>print (addn)\n</code></pre>\n\n<p>Printing data just gives the name of the </p>\n\n<pre><code> Tensor(""addn:0"", shape=(), dtype=int32) \n</code></pre>\n\n<p>Tensor ,shape and its data type </p>\n\n<p>doesn\'t give  value it hold any point of time.\nThis is because above code is not run/executed.\nIt has just constructed the Graph in tensorflow but haven\'t executed to get the result To Execute it <code>session</code> is required  </p>\n\n<p>you can just add few lines ,create a session then print</p>\n\n<pre><code>sess = tf.Session()\nprint(sess.run(addn))\n</code></pre>\n\n<p><strong>output</strong>\n   you will get output 20</p>\n\n<blockquote>\n  <p>a*b + c/d = 6*3 + 10/5 = 18 + 2 = 20</p>\n</blockquote>\n\n<p>Complete Code</p>\n\n<pre><code>d = tf.constant(5,name=\'contant_d\')\n\nmul = tf.multiply(a,b,name=\'mul\')\ndiv = tf.div(c,d,name=""div"")\n\n# Output of the multiplication what needs to be added\naddn = tf.add_n([mul,div],name=""addn"")\nprint (addn)\n\n""""""\nPrinting data just gives the name of the Tensor ,shape and its data type\ndoesn\'t give  value it hold anypoint of time\nThis is because above code is not run\nIt has just constructed the Graph in tensorflow but haven\'t executed to get the result\nTo Execute it session is required  \n""""""\nsess = tf.Session()\nprint(sess.run(addn))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1517307377}, {'QuestionId': 47537064, 'AnswerId': 47539340, 'URL': 'https://stackoverflow.com/questions/47537064/tensorflow-incompatible-return-types-from-tf-cond/47539340#47539340', 'QuestionTitle': 'Tensorflow Incompatible return types from tf.cond', 'Answer': '<p>If you read the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer"">tf.cond</a>, you will get this description:</p>\n\n<p>true_fn and false_fn both return lists of output tensors. true_fn and false_fn must have the <strong>same</strong> non-zero number and <strong>type of outputs</strong>.</p>\n\n<p>Now let\'s look into the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/TFRecordReader"" rel=""nofollow noreferrer"">TFRecordReader</a>\nThese are the return types of the following functions:</p>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/TFRecordReader#read_up_to"" rel=""nofollow noreferrer""><code>read_up_to</code></a>: A tuple of Tensors (keys, values). keys: A 1-D string Tensor. values: A 1-D string Tensor.</p>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/TFRecordReader#read"" rel=""nofollow noreferrer""><code>read</code></a>: A tuple of Tensors (key, value). key: A string scalar Tensor. value: A string scalar Tensor.</p>\n\n<p>To give further explanation, you will have to give more amount of code, but the concept of mistake has been highlighted.</p>\n', 'IsAccepted': True, 'CreationDate': 1511896925}, {'QuestionId': 47430801, 'AnswerId': 47448845, 'URL': 'https://stackoverflow.com/questions/47430801/tf-print-doesnt-well-with-tf-assign/47448845#47448845', 'QuestionTitle': 'tf.Print doesn&#39;t well with tf.assign', 'Answer': '<p>The issue is that <code>tf.Print</code> returns a tensor rather than a <code>tf.Variable</code>.  You can fix this by assigning the result to a new variable, although this feels a bit kludgy to me and there may be a more elegant way.  If you run the following:</p>\n\n<pre><code>import tensorflow as tf\n\nentcoeff = tf.Variable([0], dtype=tf.float32, trainable=False)\nprint(entcoeff)\nentcoeff = tf.Variable(tf.Print(entcoeff, [entcoeff], message=""\\n\\nprinting""))\nprint(entcoeff)\n\nentcoeff = tf.assign(entcoeff, [-1.])   # This returns an op which you need to run for the assignment to happen\nprint(entcoeff)\ninit_op = tf.global_variables_initializer()\n\nsess = tf.Session()\nsess.run(init_op)           # This triggers the tf.Print\nprint(sess.run(entcoeff))   # This causes entcoeff to be assigned -1.\n</code></pre>\n\n<p>you should get</p>\n\n<pre><code>&lt;tf.Variable \'Variable:0\' shape=(1,) dtype=float32_ref&gt;\n&lt;tf.Variable \'Variable_1:0\' shape=(1,) dtype=float32_ref&gt;\n&lt;tf.Variable \'Variable_1:0\' shape=(1,) dtype=float32_ref&gt;\n[ -1.]\n2017-11-23 00:49:56.450723: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\35\\tensorflow\\core\\kernels\\logging_ops.cc:79]\n\nprinting[0]\n</code></pre>\n\n<p>Note that the output of the <code>tf.Print</code> doesn\'t show up until the end, because <code>tf.Print</code> works by writing to standard error.  Also note the <code>\\n\\n</code> I added to the output message to make it more visible.</p>\n', 'IsAccepted': False, 'CreationDate': 1511418046}]","{68444180, 45151015}","['The printed messages actually implies all of the branches were added to the tensorflow graph but it does not imply all branches will be executed all the time in graph mode. <code>tf.print</code> should be used instead for the debugging.</p>\n<p>For more information: <a href=""https://www.tensorflow.org/guide/function#conditionals"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/function#conditionals</a></p>\n<p>Demonstration:</p>\n<pre><code>def a():\n  tf.print(\'a\')\n  return tf.constant(10)\n\ndef b():\n  tf.print(\'b\')\n  return tf.constant(11)\n\ndef c():\n  tf.print(\'c\')\n  return tf.constant(12)\n\n\n@tf.function\ndef cond_fn(x):\n  return tf.switch_case(x, {0:a,1:b}, default=c)\n\nprint(cond_fn(tf.constant(0)))\nprint(cond_fn(tf.constant(1)))\nprint(cond_fn(tf.constant(2)))\n</code></pre>\n<p>Expected outputs:</p>\n<pre><code>a\ntf.Tensor(10, shape=(), dtype=int32)\nb\ntf.Tensor(11, shape=(), dtype=int32)\nc\ntf.Tensor(12, shape=(), dtype=int32)\n</code></pre>\n<hr />\n<p>The <code>ValueError</code> error message is because tensorflow graph does not support this kind of feature very well, at least not with <code>tf.einsum</code>. One way of the workarounds is to have a graph that supports variable-shaped inputs by using <code>tf.function(f).get_concrete_function(tf.TensorSpec(shape=[None,None,None]))</code>.</p>\n<p>Besides, <code>tf.einsum</code> is problematic in the process and have to be replaced by <code>tf.transpose</code> and <code>tf.tensordot</code>.</p>\n<p>Example Codes:</p>\n<pre><code>x = tf.random.normal((3,2,1))\ny = tf.random.normal((1,2,3))\nz = tf.random.normal((4,3,5))\nk = tf.random.normal((3,5,5))\n\n#for checking the values\ndef f2(t):\n    p = tf.case([\n        (tf.equal(tf.shape(t)[0], 3), lambda:tf.einsum(&quot;ijk,lmi&quot;, t, y)),\n        (tf.equal(tf.shape(t)[1], 3), lambda:tf.einsum(&quot;ijk,ljm&quot;, t, z)),\n    ], default=lambda:tf.einsum(&quot;ijk,klm&quot;, t, k), exclusive=True)\n    return p\n\n#work around\ndef f(t):\n    if tf.shape(t)[0] == 3:\n      tf.print(\'branch a executed\')\n      return tf.tensordot(tf.transpose(t,[1,2,0]), tf.transpose(y,[2,0,1]),1)\n    elif tf.shape(t)[1] == 3:\n      tf.print(\'branch b executed\')\n      return tf.tensordot(tf.transpose(t,[0,2,1]), tf.transpose(z,[1,0,2]),1)\n    else:\n      tf.print(\'branch c executed\')\n      return tf.tensordot(t, k,1)\n\ngraph_f=tf.function(f).get_concrete_function(tf.TensorSpec(shape=[None,None,None]))\n\nprint(np.allclose(graph_f(x),f2(x)))\nprint(np.allclose(graph_f(y),f2(y)))\nprint(np.allclose(graph_f(z),f2(z)))\n</code></pre>\n<p>Expected outputs:</p>\n<pre><code>branch a executed\nTrue\nbranch c executed\nTrue\nbranch b executed\nTrue\n</code></pre>\n', ""<p>This is due to the fact that you are using <code>tf.constant</code>, which in theory shouldn't be affected by inputs.</p>\n\n<p>If you replace your experiments with anything else (e.g. <code>Variables</code>) it works as expected.</p>\n\n<p>When you apply an operator to the constant (be it addition, or even identity), you obtain a new tensor that is not <code>constant</code>, even though they depand on <code>constant</code>s only -- and therefore you obtain the expected behavior.</p>\n""]","{'https://stackoverflow.com/questions/68444180/tf-case-and-tf-cond-executes-all-the-functions-within-in-tensorflow/68445819#68445819', 'https://stackoverflow.com/questions/45151015/how-does-tf-gradients-behave-when-passed-a-list-of-ys-tensors/45151785#45151785'}",,0.2499697861691067,0.23229161330022824
4,61305781,tf.feature_column.categorical_column_with_vocabulary_list,Documentation Replication on Other Examples,Using Tensorflow embedded columns raises All feature_columns must be _FeatureColumn instances error,"<p>I am new to tensorflow and I was trying to follow the official documentation where I came across 
tf.feature_column.categorical_column_with_vocabulary_list</p>

<p>The code I tested is: </p>

<pre><code>key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), default_value=0)
columns = [[tfc.embedding_column(colors, 3)], ...]
features = tf.io.parse_example(..., features=tfc.make_parse_example_spec(columns))
dense_tensor = tfc.input_layer(features, columns)
</code></pre>

<p>However , when I run this sample code I get the following error : 
 ValueError: All feature_columns must be _FeatureColumn instances. Given: [EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), dtype=tf.string, default_value=0, num_oov_buckets=0), dimension=3, combiner='mean', initializer=, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)]</p>

<p>What I am doing wrong?  </p>
","<p><code>make_parse_example_spec</code> expects <code>FeatureColumn instances</code>. You can create the FeatureColumn instance using the below method for the category list.</p>

<pre><code>colors = feature_column.categorical_column_with_vocabulary_list(key='colors',vocabulary_lis=('R', 'G', 'B', 'Y'),num_oov_buckets=2)
my_feature_columns = [feature_column.indicator_column(colors)]
feature_column.make_parse_example_spec(my_feature_columns)
</code></pre>

<p>Output :</p>

<pre><code>{'colors': VarLenFeature(dtype=tf.string)}  
</code></pre>

<p>If you want to create a dense embedding tensor on your categorical column, you can follow the below example.  </p>

<pre><code>data = {'colors': ['X', 'R', 'G', 'B', 'Y']}

df = pd.DataFrame(data)

colors = feature_column.categorical_column_with_vocabulary_list('colors', df['colors'].unique())

colors_embedding = feature_column.embedding_column(colors, dimension=4)

dense_tensor = tf.keras.layers.DenseFeatures(colors_embedding)(data)
</code></pre>

<p>Result: </p>

<pre><code>tf.Tensor(
[[ 0.17071894  0.29407692 -0.26661882  0.07768019]
 [ 0.26196313  0.14372464 -0.41102907 -0.7207164 ]
 [-0.7888006  -0.07049363 -0.49007863  0.45744416]
 [ 0.56329435 -0.7051675   0.04742934 -0.69377   ]
 [-0.52031726  0.488502   -0.37031132 -0.44338205]], shape=(5, 4), dtype=float32)
</code></pre>
","{46882307, 48614819, 38808643, 44812261, 39249704, 51237419, 63138030, 52183856, 58003448, 46037627}","[{'QuestionId': 63138030, 'AnswerId': 64080413, 'URL': 'https://stackoverflow.com/questions/63138030/typeerror-could-not-build-a-typespec-for-a-column/64080413#64080413', 'QuestionTitle': 'TypeError: Could not build a TypeSpec for a column', 'Answer': ""<p>This is basically with the <code>null</code> values present in the data which you have taken, you need to handle it when you load the data.</p>\n<p>I have done couple of changes.</p>\n<ol>\n<li>To drop the record of the null value, you can also perform <code>df.fillna</code> based on the columns and the values you need to fill in it considering data type.</li>\n<li>I have changed the column <code>Year</code> datatype from <code>float</code> to <code>int</code>. Since it would lead to another problem for <code>tensor_slices</code>.</li>\n</ol>\n<p>Below is the modified code with the same data you have taken.</p>\n<pre><code>df = pd.read_csv('/content/vgsales.csv')\n# print(df.head())\nprint(df[df.isnull().any(axis=1)])\n# df.fillna('', inplace=True)\ndf.dropna(how=&quot;any&quot;,inplace = True)\ndf.Year = df.Year.astype(int) \n\nCATEGORICAL_COLUMNS = ['Name', 'Platform', 'Genre', 'Publisher']\nNUMERIC_COLUMNS = ['Year'] \n\nfeature_columns = []\nfor feature_name in CATEGORICAL_COLUMNS:\n  vocabulary = df[feature_name].unique()  # gets a list of all unique values from given feature column\n  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n\nfor feature_name in NUMERIC_COLUMNS:\n  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.int64))\n\nprint(feature_columns)\n\ndef make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n  def input_function():  \n    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  \n    if shuffle:\n      ds = ds.shuffle(1000)  \n    ds = ds.batch(batch_size).repeat(num_epochs)  \n    return ds\n  return input_function  \n\ntrain_input_fn = make_input_fn(df, y_train)  \nlinear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1601139803}, {'QuestionId': 51237419, 'AnswerId': 61214436, 'URL': 'https://stackoverflow.com/questions/51237419/feature-column-pre-trained-embedding/61214436#61214436', 'QuestionTitle': 'Feature Column Pre-trained Embedding', 'Answer': ""<p>You can also wrap your array into a function like this:</p>\n\n<pre><code>some_matrix = np.array([[0,1,2],[0,2,3],[5,6,7]])\n\ndef custom_init(shape, dtype):\n    return some_matrix\n\nembedding_feature = tf.feature_column.embedding_column(itemx_vocab, \n                                                       dimension=3, \n                                                       initializer=custom_init\n                                                       )\n\n</code></pre>\n\n<p>It's an hacky way but does the job. </p>\n"", 'IsAccepted': False, 'CreationDate': 1586888063}, {'QuestionId': 52183856, 'AnswerId': 60928514, 'URL': 'https://stackoverflow.com/questions/52183856/tensorflow-feature-columns-attributeerror-tuple-object-has-no-attribute-nam/60928514#60928514', 'QuestionTitle': 'Tensorflow Feature Columns: AttributeError: &#39;tuple&#39; object has no attribute &#39;name&#39;', 'Answer': ""<p>Im sure you figured it out yourself by now, but for anyone who has the same error and probably just copied it out from the list with the comma at the end.</p>\n\n<p>The trailing comma makes 'col4' a tuple </p>\n\n<pre><code>col4 = tf.feature_column.categorical_column_with_identity(key='col4', num_buckets=3), \n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1585563960}, {'QuestionId': 46882307, 'AnswerId': 57154217, 'URL': 'https://stackoverflow.com/questions/46882307/attributeerror-module-tensorflow-has-no-attribute-feature-column/57154217#57154217', 'QuestionTitle': 'AttributeError: module &#39;tensorflow&#39; has no attribute &#39;feature_column&#39;', 'Answer': '<p>I faced a similar error while running a session using the Tensorflow 2.0 beta. I used the following form for running a session:</p>\n\n<pre><code>import tensorflow as tf\nconstant = tf.constant([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\nwith tf.compat.v1.Session() as sess:\n        print(sess.run(constant))\n</code></pre>\n\n<p>instead of:</p>\n\n<pre><code>import tensorflow as tf\nconstant = tf.constant([[1, 2, 3],[4, 5, 6],[7, 8, 9]])\nwith tf.Session() as sess:\n      print(sess.run(constant))\n</code></pre>\n\n<p>Also, </p>\n\n<pre><code>tf.compat.v1.Session()\n</code></pre>\n\n<p>is backward compatible. You might face a similar error when using other functions in Tensorflow 2.0 beta like print, get_variable, etc. Use a similar form as shown above in the example.</p>\n', 'IsAccepted': False, 'CreationDate': 1563832368}, {'QuestionId': 58003448, 'AnswerId': 58004067, 'URL': 'https://stackoverflow.com/questions/58003448/valueerror-items-of-feature-columns-must-be-either-a-densecolumn-or-categorical/58004067#58004067', 'QuestionTitle': 'ValueError: Items of feature_columns must be either a DenseColumn or CategoricalColumn', 'Answer': '<p>The following code works fine for training and prediction.</p>\n\n<pre><code>x_data = np.linspace(0, 10.0, 1000)\nprint(x_data.shape)\ny_true = (0.5 * x_data) + 5\nprint(y_true.shape)\nx_train, x_eval, y_train, y_eval = train_test_split(x_data, y_true, test_size=0.25, random_state=101)\ntrain_func = tf.estimator.inputs.numpy_input_fn({\'x\': x_train}, y_train, batch_size=8, num_epochs=10, shuffle=True)\n\nfeatures = [tf.contrib.layers.real_valued_column(""x"", dimension=1)]\nestimator = tf.estimator.LinearRegressor(feature_columns=features)\n\nestimator.train(input_fn=train_func, steps=100) # Fit the model to training data.\n\neval_func = tf.estimator.inputs.numpy_input_fn({\'x\': x_eval}, batch_size=1, num_epochs=1, shuffle=False)\n\nresult = estimator.predict(eval_func) # Predict scores\n\nprint(""predict_scores"", list(result))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1568869701}, {'QuestionId': 48614819, 'AnswerId': 51966850, 'URL': 'https://stackoverflow.com/questions/48614819/items-of-feature-columns-must-be-a-featurecolumn-given-vocabularylistcategori/51966850#51966850', 'QuestionTitle': 'Items of feature_columns must be a _FeatureColumn Given: _VocabularyListCategoricalColumn', 'Answer': '<p>There is another way According to <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/feature_column/feature_column.py"" rel=""nofollow noreferrer"">This Source Code</a>:</p>\n\n<p>So:</p>\n\n<pre><code>categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(key=""Sex"", vocabulary_list=[""male"", ""female""], default_value=0)\n\nmy_feature_columns = [\n    tf.feature_column.numeric_column(key=\'Pclass\'),\n    embedded_group_column = tf.feature_column.embedding_column(categorical_column,dimension=number_of_categories)\n    tf.feature_column.numeric_column(key=\'Age\')\n]\n</code></pre>\n\n<p>Good Luck!</p>\n', 'IsAccepted': False, 'CreationDate': 1534940692}, {'QuestionId': 46882307, 'AnswerId': 51389187, 'URL': 'https://stackoverflow.com/questions/46882307/attributeerror-module-tensorflow-has-no-attribute-feature-column/51389187#51389187', 'QuestionTitle': 'AttributeError: module &#39;tensorflow&#39; has no attribute &#39;feature_column&#39;', 'Answer': '<p><strong>Upgrading your tensorflow might help.</strong></p>\n\n<pre><code>pip install --upgrade tensorflow\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1531856937}, {'QuestionId': 51237419, 'AnswerId': 51280185, 'URL': 'https://stackoverflow.com/questions/51237419/feature-column-pre-trained-embedding/51280185#51280185', 'QuestionTitle': 'Feature Column Pre-trained Embedding', 'Answer': '<p>I also take a issue here <a href=""https://github.com/tensorflow/tensorflow/issues/20663"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/20663</a></p>\n\n<p>finally I got a right way with to solve it. although. i\'m not clear why answer above is not effective!!  if you know the question, Thanks to give some suggestion to me!!</p>\n\n<p>ok~~~~here is current solvement. Actually from here <a href=""https://stackoverflow.com/questions/47523374/feature-columns-embedding-lookup"">Feature Columns Embedding lookup</a></p>\n\n<blockquote>\n  <p>code: </p>\n</blockquote>\n\n<pre><code>itemx_vocab = tf.feature_column.categorical_column_with_vocabulary_file(\n    key=\'itemx\',\n    vocabulary_file=FLAGS.vocabx)\n\nembedding_initializer_x = tf.contrib.framework.load_embedding_initializer(\n    ckpt_path=\'model.ckpt\',\n    embedding_tensor_name=\'w_in\',\n    new_vocab_size=itemx_vocab.vocabulary_size,\n    embedding_dim=emb_size,\n    old_vocab_file=\'FLAGS.vocab_emb\',\n    new_vocab_file=FLAGS.vocabx\n)\nitemx_emb = tf.feature_column.embedding_column(itemx_vocab,\n                                               dimension=128,\n                                               initializer=embedding_initializer_x,\n                                               trainable=False)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1531296039}, {'QuestionId': 46882307, 'AnswerId': 50430102, 'URL': 'https://stackoverflow.com/questions/46882307/attributeerror-module-tensorflow-has-no-attribute-feature-column/50430102#50430102', 'QuestionTitle': 'AttributeError: module &#39;tensorflow&#39; has no attribute &#39;feature_column&#39;', 'Answer': '<p>If you\'re importing Tensorflow in a project that uses Keras, import Keras modules first, then Tensorflow.  That solved the problem for me.</p>\n\n<p><strong>Do this</strong>: (notice the order)</p>\n\n<pre class=""lang-python prettyprint-override""><code>from keras.backend.tensorflow_backend import set_session\nfrom keras.models import Sequential\nfrom keras import applications\n\nimport tensorflow as tf\n</code></pre>\n\n<p><strong>Do not do</strong> this:</p>\n\n<pre class=""lang-python prettyprint-override""><code>import tensorflow as tf\n\nfrom keras.backend.tensorflow_backend import set_session\nfrom keras.models import Sequential\nfrom keras import applications\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1526769387}, {'QuestionId': 48614819, 'AnswerId': 48614916, 'URL': 'https://stackoverflow.com/questions/48614819/items-of-feature-columns-must-be-a-featurecolumn-given-vocabularylistcategori/48614916#48614916', 'QuestionTitle': 'Items of feature_columns must be a _FeatureColumn Given: _VocabularyListCategoricalColumn', 'Answer': '<p>Reading again <a href=""https://www.tensorflow.org/versions/master/get_started/feature_columns"" rel=""noreferrer"">the documentation</a> I realised that </p>\n\n<blockquote>\n  <p>Indicator columns and embedding columns never work on features directly</p>\n</blockquote>\n\n<p>So I corrected my code:</p>\n\n<pre><code>categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(key=""Sex"", vocabulary_list=[""male"", ""female""], default_value=0)\n\nmy_feature_columns = [\n    tf.feature_column.numeric_column(key=\'Pclass\'),\n    tf.feature_column.indicator_column(categorical_column),\n    tf.feature_column.numeric_column(key=\'Age\')\n]\n</code></pre>\n\n<p>And works like a charm!</p>\n', 'IsAccepted': True, 'CreationDate': 1517795922}, {'QuestionId': 38808643, 'AnswerId': 48367461, 'URL': 'https://stackoverflow.com/questions/38808643/tf-contrib-layers-embedding-column-from-tensor-flow/48367461#48367461', 'QuestionTitle': 'tf.contrib.layers.embedding_column from tensor flow', 'Answer': '<p>I had a similar doubt about embeddings.</p>\n\n<p>Here is the main point:</p>\n\n<blockquote>\n  <p>The ability of adding an embedding layer along with tradition wide linear models allows for accurate predictions by reducing sparse dimensionality down to low dimensionality.</p>\n</blockquote>\n\n<p><a href=""https://i.stack.imgur.com/EVlqN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EVlqN.png"" alt=""enter image description here""></a></p>\n\n<p>Here is a <a href=""https://research.googleblog.com/2016/06/wide-deep-learning-better-together-with.html"" rel=""nofollow noreferrer"">good post</a> about it! </p>\n\n<p>And here is a <a href=""http://camron.xyz/index.php/2016/09/13/hybrid_learning/"" rel=""nofollow noreferrer"">simple example</a> combining embedding layers. Using the Titanic Kaggle data to predict whether or not the passenger will survive based on certain attributes like Name, Sex, what ticket they had, the fare they paid the cabin they stayed in etc.</p>\n', 'IsAccepted': False, 'CreationDate': 1516542471}, {'QuestionId': 46882307, 'AnswerId': 46882595, 'URL': 'https://stackoverflow.com/questions/46882307/attributeerror-module-tensorflow-has-no-attribute-feature-column/46882595#46882595', 'QuestionTitle': 'AttributeError: module &#39;tensorflow&#39; has no attribute &#39;feature_column&#39;', 'Answer': '<p>Tensorflow 1.3 should support feature_column well. You might accidentally used an old version. Try the following code to verify your version:</p>\n\n<pre><code>import tensorflow as tf\nprint(tf.__version__)\nprint(dir(tf.feature_column))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1508737221}, {'QuestionId': 44812261, 'AnswerId': 46262448, 'URL': 'https://stackoverflow.com/questions/44812261/module-object-has-no-attribute-feature-column/46262448#46262448', 'QuestionTitle': '&#39;module&#39; object has no attribute &#39;feature_column&#39;', 'Answer': '<p>This could also happen from updating Google Cloud SDK, in which case <code>pip install --upgrade tensorflow</code> should do the job.</p>\n', 'IsAccepted': False, 'CreationDate': 1505639827}, {'QuestionId': 46037627, 'AnswerId': 46039856, 'URL': 'https://stackoverflow.com/questions/46037627/tensorflow-contrib-learn-dnnclassifier-missing-1-required-positional-argument/46039856#46039856', 'QuestionTitle': 'tensorflow.contrib.learn.DNNclassifier missing 1 required positional argument: &#39;feature_columns&#39;', 'Answer': '<p>You need to pass correct arguments <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier"" rel=""nofollow noreferrer""><code>tf.contrib.learn.DNNClassifier</code></a>, here you didn\'t pass<code>feature_columns</code> argument. </p>\n\n<p>For example, you can use <code>real_valued_column</code> as features_columns</p>\n\n<pre><code>feature_columns = [tf.contrib.layers.real_valued_column("""", dimension=1)]\nclassifier = learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=2)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1504537695}, {'QuestionId': 44812261, 'AnswerId': 45783971, 'URL': 'https://stackoverflow.com/questions/44812261/module-object-has-no-attribute-feature-column/45783971#45783971', 'QuestionTitle': '&#39;module&#39; object has no attribute &#39;feature_column&#39;', 'Answer': '<p>I faced this problem with an outdated version of tensorflow. Updating it to the latest build helped.</p>\n', 'IsAccepted': False, 'CreationDate': 1503243999}, {'QuestionId': 39249704, 'AnswerId': 39359007, 'URL': 'https://stackoverflow.com/questions/39249704/tensorflow-valueerror-duplicate-feature-column-key-found-for-column/39359007#39359007', 'QuestionTitle': 'Tensorflow : ValueError Duplicate feature column key found for column', 'Answer': '<p>First of all, as Jesse suggested use the master (no branch on git clone) when you are downloading the tutorial source code from tensorflow website.\nSecond, you only need to pass the first two parameters to the class. You will get some warnings on the way; just ignore them and wait for the program to finish.\nSo use the following code on top of the tutorial source code: or download the \nmodified code from here: <a href=""https://raw.githubusercontent.com/henaras/tensorflow/66bebf4c03e15686f91c692be6efee671107bf32/tensorflow/examples/learn/wide_n_deep_tutorial.py"" rel=""nofollow"">Modified Version</a></p>\n\n<pre><code>from tensorflow.contrib.layers.python.layers.feature_column import _EmbeddingColumn\n\nclass _MonkeyEmbeddingColumn(_EmbeddingColumn):\n  # override the key property\n  @property\n  def key(self):\n    return ""{}"".format(self)\n\ndef monkey_embedding_column(sparse_id_column,\n                     dimension,\n                     combiner=""mean"",\n                     initializer=None,\n                     ckpt_to_load_from=None,\n                     tensor_name_in_ckpt=None):\n  return _MonkeyEmbeddingColumn(sparse_id_column, dimension)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1473202986}, {'QuestionId': 39249704, 'AnswerId': 39268045, 'URL': 'https://stackoverflow.com/questions/39249704/tensorflow-valueerror-duplicate-feature-column-key-found-for-column/39268045#39268045', 'QuestionTitle': 'Tensorflow : ValueError Duplicate feature column key found for column', 'Answer': '<p><strong>Final update</strong> I had this problem with the recommended 0.10rc0 branch, but after reinstalling using the master (no branch on git clone) this problem went away.  I checked the source code and they fixed it.  Python 3 now gets the same results as Python 2 for wide_n_deep mode, after fixing the urllib.request thing you already mentioned.</p>\n\n<p>For anyone coming later and still using 0.10rc0 branch, feel free to read on:</p>\n\n<p>Had the same problem, and did some debugging.  Looks like a bug in tensorflow/contrib/layers/python/layers/feature_column.py in the _EmbeddingColumn class.  The key(self) property is plagued by this bug:\n<a href=""https://bugs.python.org/issue24931"" rel=""nofollow"">https://bugs.python.org/issue24931</a></p>\n\n<p>So instead of coming out with a nice unique key, we get the following key for all _EmbeddingColumn instances:\n\'_EmbeddingColumn()\'</p>\n\n<p>This causes the feature_column_ops.py\'s check_feature_columns() function to determine that the second _EmbeddingColumn instance is a duplicate since they keys of all of them are the same.</p>\n\n<p>I\'m kind of a Python noob, and I can\'t figure out how to monkey patch a property.  So I fixed this problem by creating a subclass at the top of the wide_n_deep tutorial file:</p>\n\n<pre><code># EmbeddingColumn for Python 3.4 has a problem with key property\n# can\'t monkey patch a property, so subclass it and make a method to create the \n# subclass to use instead of ""embedding_column""\nfrom tensorflow.contrib.layers.python.layers.feature_column import _EmbeddingColumn\nclass _MonkeyEmbeddingColumn(_EmbeddingColumn):\n  # override the key property\n  @property\n  def key(self):\n    return ""{}"".format(self)\n\ndef monkey_embedding_column(sparse_id_column,\n                     dimension,\n                     combiner=""mean"",\n                     initializer=None,\n                     ckpt_to_load_from=None,\n                     tensor_name_in_ckpt=None):\n  return _MonkeyEmbeddingColumn(sparse_id_column, dimension, combiner, initializer, ckpt_to_load_from, tensor_name_in_ckpt)\n</code></pre>\n\n<p>Then find the calls like this:</p>\n\n<pre><code>tf.contrib.layers.embedding_column(workclass, dimension=8)\n</code></pre>\n\n<p>and replace ""tf.contrib.layers."" with ""monkey_"" so you now have:</p>\n\n<pre><code>  deep_columns = [\n      monkey_embedding_column(workclass, dimension=8),\n      monkey_embedding_column(education, dimension=8),\n      monkey_embedding_column(marital_status,\n                                         dimension=8),\n      monkey_embedding_column(gender, dimension=8),\n      monkey_embedding_column(relationship, dimension=8),\n      monkey_embedding_column(race, dimension=8),\n      monkey_embedding_column(native_country,\n                                         dimension=8),\n      monkey_embedding_column(occupation, dimension=8),\n      age,\n      education_num,\n      capital_gain,\n      capital_loss,\n      hours_per_week,\n  ]\n</code></pre>\n\n<p>So now it uses the MonkeyEmbeddingColumn class with the modified key property (works like all the other key properties from feature_column.py).  This lets the code run to completion, but I\'m not 100% sure it\'s correct as it reports the accuracy as:</p>\n\n<pre><code>accuracy: 0.818316\n</code></pre>\n\n<p>As this is slightly worse than the wide-only training, I wonder if it has this accuracy in Python 2 or if my fix is lowering the accuracy by causing a training problem.</p>\n\n<p><strong>Update</strong> I installed in Python 2 and the wide_n_deep gets over 0.85 accuracy, so this ""fix"" lets the code run but seems to be doing the wrong thing.  I\'ll debug and see what Python 2 gets for these values and see if it can be fixed properly in Python 3.  I\'m curious too.</p>\n', 'IsAccepted': False, 'CreationDate': 1472722926}, {'QuestionId': 38808643, 'AnswerId': 38980687, 'URL': 'https://stackoverflow.com/questions/38808643/tf-contrib-layers-embedding-column-from-tensor-flow/38980687#38980687', 'QuestionTitle': 'tf.contrib.layers.embedding_column from tensor flow', 'Answer': '<p>I\'ve been wondering about this too. It\'s not really clear to me what they\'re doing, but this is what I found.</p>\n\n<p>In the <a href=""http://arxiv.org/pdf/1606.07792v1.pdf"" rel=""noreferrer"">paper on wide and deep learning</a>, they describe the embedding vectors as being randomly initialized and then adjusted during training to minimize error.</p>\n\n<p>Normally when you do embeddings, you take some arbitrary vector representation of the data (such as one-hot vectors) and then multiply it by a matrix that represents the embedding. This matrix can be found by PCA or while training by something like t-SNE or word2vec.</p>\n\n<p>The actual code for the embedding_column is <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column.py"" rel=""noreferrer"">here</a>, and it\'s implemented as a class called _EmbeddingColumn which is a subclass of _FeatureColumn. It stores the embedding matrix inside its sparse_id_column attribute. Then, the method to_dnn_input_layer applies this embedding matrix to produce the embeddings for the next layer.</p>\n\n<pre><code> def to_dnn_input_layer(self,\n                         input_tensor,\n                         weight_collections=None,\n                         trainable=True):\n    output, embedding_weights = _create_embedding_lookup(\n        input_tensor=self.sparse_id_column.id_tensor(input_tensor),\n        weight_tensor=self.sparse_id_column.weight_tensor(input_tensor),\n        vocab_size=self.length,\n        dimension=self.dimension,\n        weight_collections=_add_variable_collection(weight_collections),\n        initializer=self.initializer,\n        combiner=self.combiner,\n        trainable=trainable)\n</code></pre>\n\n<p>So as far as I can see, it seems like the embeddings are formed by applying whatever learning rule you\'re using (gradient descent, etc.) to the embedding matrix.</p>\n', 'IsAccepted': False, 'CreationDate': 1471367195}]","{48614819, 57813806}","['<p>Just found the issue:\ntf.feature_column.categorical_column_with_hash_bucket() takes an optional argument dtype, which is set to tf.dtypes.string by default.', '<p>Reading again <a href=""https://www.tensorflow.org/versions/master/get_started/feature_columns"" rel=""noreferrer"">the documentation</a> I realised that </p>\n\n<blockquote>\n  <p>Indicator columns and embedding columns never work on features directly</p>\n</blockquote>\n\n<p>So I corrected my code:</p>\n\n<pre><code>categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(key=""Sex"", vocabulary_list=[""male"", ""female""], default_value=0)\n\nmy_feature_columns = [\n    tf.feature_column.numeric_column(key=\'Pclass\'),\n    tf.feature_column.indicator_column(categorical_column),\n    tf.feature_column.numeric_column(key=\'Age\')\n]\n</code></pre>\n\n<p>And works like a charm!</p>\n']","{'https://stackoverflow.com/questions/48614819/items-of-feature-columns-must-be-a-featurecolumn-given-vocabularylistcategori/48614916#48614916', 'https://stackoverflow.com/questions/57813806/apply-feature-columns-without-tf-estimator-tensorflow-2-0-0-rc0/57814782#57814782'}",{48614819},0.2425506314024536,0.1855951087766876
4,49987839,tf.clip_by_global_norm,Documentation Replicability,How to handle None in tf.clip_by_global_norm?,"<p>I have read in answers to <a href=""https://stackoverflow.com/questions/36498127/how-to-apply-gradient-clipping-in-tensorflow"">this question here</a> that tf.clip_by_global_norm() handles None values by simply ignoring them (comment by danijar in comments to the answer by @danijar) but when i try to apply it i seem to be doing something wrong as it throws </p>

<p>ValueError: None values not supported.</p>

<pre><code>tf.reset_default_graph()
z = tf.get_variable(name = 'z', shape = [1])
b = tf.get_variable('b', [1])
c = b*b - 2*b + 1
optimizer = tf.train.AdamOptimizer(0.1)
gradients, variables = zip(*optimizer.compute_gradients(c))
gradients = tf.clip_by_global_norm(gradients, 2.5)
train_op = optimizer.apply_gradients(zip(gradients, variables))
</code></pre>

<p>Can somebody please tell me what am i doing wrong or if tf.clip_by_global_norm() does not handle None gradients and i have to take care of them manually</p>

<p>The official documentation seems to agree with @danijar's comments. see <a href=""https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/clip_by_global_norm"" rel=""nofollow noreferrer"">here</a></p>

<blockquote>
  <p>Any of the entries of t_list that are of type None are ignored.</p>
</blockquote>
","<p>There's a small problem in your code: you're assigning the return value of <code>tf.clip_by_global_norm</code> to a single variable, when this function returns a pair of values.</p>

<p>The documentation says:</p>

<blockquote>
  <p>Returns:</p>
  
  <p>list_clipped: A list of Tensors of the same type as list_t.</p>
  
  <p>global_norm: A 0-D (scalar) Tensor representing the global norm.</p>
</blockquote>

<p>Hence, the problem arises when you try to apply the gradients to the variables, in the next line.</p>

<p>You can easily fix your code ignoring the global_norm returned value.</p>

<pre><code>gradients, _ = tf.clip_by_global_norm(gradients, 2.5)
</code></pre>
","{39295136, 46545986, 42194051, 43144785, 48431282, 56428659, 49787924, 44796793, 48455294, 45906143}","[{'QuestionId': 44796793, 'AnswerId': 44798131, 'URL': 'https://stackoverflow.com/questions/44796793/difference-between-tf-clip-by-value-and-tf-clip-by-global-norm-for-rnns-and-how/44798131#44798131', 'QuestionTitle': 'Difference between tf.clip_by_value and tf.clip_by_global_norm for RNN&#39;s and how to decide max value to clip on?', 'Answer': '<p><strong>TL;DR</strong>: use <code>tf.clip_by_global_norm</code> for gradient clipping, with &quot;some high value&quot; as max value.</p>\n<h3>clip_by_value</h3>\n<p><code>tf.clip_by_value</code> clips each value inside one tensor, regardless of the other values in the tensor. For instance,</p>\n<pre><code>tf.clip_by_value([-1, 2, 10], 0, 3)  -&gt; [0, 2, 3]  # Only the values below 0 or above 3 are changed\n</code></pre>\n<p>Consequently, it can change the direction of the tensor, so it should be used if the values in the tensor are decorrelated one from another (which is not the case for gradient clipping), or to avoid zero / infinite values in a tensor that could lead to Nan / infinite values elsewhere (by clipping with a minimum of epsilon=1e-8 and a very big max value for instance).</p>\n<h3>clip_by_norm</h3>\n<p><code>tf.clip_by_norm</code> rescales one tensor if necessary, so that its L2 norm does not exceed a certain threshold. It\'s useful typically to avoid exploding gradient on one tensor, because you keep the gradient direction. For instance:</p>\n<pre><code>tf.clip_by_norm([-2, 3, 6], 5)  -&gt; [-2, 3, 6]*5/7  # The original L2 norm is 7, which is &gt;5, so the final one is 5\ntf.clip_by_norm([-2, 3, 6], 9)  -&gt; [-2, 3, 6]  # The original L2 norm is 7, which is &lt;9, so it is left unchanged\n</code></pre>\n<p>However, <code>clip_by_norm</code> works on only one gradient, so if you use it on all your gradient tensors, you\'ll unbalance them (some will be rescaled, others not, and not all with the same scale).</p>\n<p>Note that the two first ones work on only one tensor, while the last one is used on a list of tensors.</p>\n<h3>clip_by_global_norm</h3>\n<p><code>tf.clip_by_global_norm</code> rescales a list of tensors so that the total norm of the vector of all their norms does not exceed a threshold. The goal is the same as  <code>clip_by_norm</code> (avoid exploding gradient, keep the gradient directions), but it works on all the gradients at once rather than on each one separately (that is, all of them are rescaled by the same factor if necessary, or none of them are rescaled). This is better, because the balance between the different gradients is maintained.</p>\n<p>For instance:</p>\n<pre><code>tf.clip_by_global_norm([tf.constant([-2, 3, 6]),tf.constant([-4, 6, 12])] , 14.5)\n</code></pre>\n<p>will rescale both tensors by a factor <code>14.5/sqrt(49 + 196)</code>, because the first tensor has a L2 norm of 7, the second one 14, and <code>sqrt(7^2+ 14^2)&gt;14.5</code></p>\n<p>This (<code>tf.clip_by_global_norm</code>) is the one that you should use for gradient clipping. See <a href=""https://arxiv.org/pdf/1211.5063.pdf"" rel=""nofollow noreferrer"">this</a> for instance for more information.</p>\n<h3>Choosing the value</h3>\n<p>Choosing the max value is the hardest part. You should use the biggest value such that you don\'t have exploding gradient (whose effects can be <code>Nan</code>s or <code>infinite</code> values appearing in your tensors, constant loss /accuracy after a few training steps). The value should be bigger for <code>tf.clip_by_global_norm</code> than for the others, since the global L2 norm will be mechanically bigger than the other ones due to the number of tensors implied.</p>\n', 'IsAccepted': True, 'CreationDate': 1498640662}, {'QuestionId': 42194051, 'AnswerId': 62683812, 'URL': 'https://stackoverflow.com/questions/42194051/filter-out-non-zero-values-in-a-tensor/62683812#62683812', 'QuestionTitle': 'Filter out non-zero values in a tensor', 'Answer': '<p>Casting numbers to bool identifies zeros as <code>False</code>. Then you can mask as usual. Example:</p>\n<pre><code>x = [1,0,2]\nmask = tf.cast(x, dtype=tf.bool)  # [True, False, True]\nnonzero_x = tf.boolean_mask(x, mask)  # [1, 2]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1593630474}, {'QuestionId': 56428659, 'AnswerId': 58823510, 'URL': 'https://stackoverflow.com/questions/56428659/how-to-implement-clip-gradients-by-norm-in-tensorflow-2-0/58823510#58823510', 'QuestionTitle': 'How to implement clip_gradients_by_norm in TensorFlow 2.0?', 'Answer': '<p>Looking at a comment on this issue <a href=""https://github.com/tensorflow/tensorflow/issues/28707#issuecomment-502336827"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/28707#issuecomment-502336827</a>,</p>\n\n<p>I discovered that you can modify your code to look like this:</p>\n\n<pre><code># Use gradient descent as the optimizer for training the model.\nfrom tensorflow.keras import optimizers\nmy_optimizer = optimizers.SGD(lr=0.0000001, clipnorm=5.0)\n\n# Configure the linear regression model with our feature columns and optimizer.\n# Set a learning rate of 0.0000001 for Gradient Descent.\nlinear_regressor = tf.estimator.LinearRegressor(\n    feature_columns=feature_columns,\n    optimizer=my_optimizer\n)\n</code></pre>\n\n<p>Instead of:</p>\n\n<pre><code># Use gradient descent as the optimizer for training the model.\nmy_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n\n# Configure the linear regression model with our feature columns and optimizer.\n# Set a learning rate of 0.0000001 for Gradient Descent.\nlinear_regressor = tf.estimator.LinearRegressor(\n    feature_columns=feature_columns,\n    optimizer=my_optimizer\n)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1573578700}, {'QuestionId': 46545986, 'AnswerId': 52950658, 'URL': 'https://stackoverflow.com/questions/46545986/how-to-use-tf-clip-by-value-on-sliced-tensor-in-tensorflow/52950658#52950658', 'QuestionTitle': 'How to use tf.clip_by_value() on sliced tensor in tensorflow?', 'Answer': '<p>The following is not documented on the man page <a href=""https://www.tensorflow.org/api_docs/python/tf/clip_by_value"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/clip_by_value</a>, but in my tests it seems to work: clip_by_value should support broadcasting. If so, the easiest (as in: not creating temporary tensors) way to do this clipping is the following:</p>\n\n<pre><code>outputs = tf.clip_by_value(outputs, [[-2147483647, 0]], [[2147483647, 100]])\n</code></pre>\n\n<p>Here I\'m assuming you\'re using <code>tf.int32</code> dtype, hence the min and max values for the field you don\'t want to clip. Admittedly it\'s not super nice, it looks better for floats where you can use <code>-numpy.inf</code> and <code>numpy.inf</code> instead.</p>\n', 'IsAccepted': False, 'CreationDate': 1540302268}, {'QuestionId': 49787924, 'AnswerId': 49921758, 'URL': 'https://stackoverflow.com/questions/49787924/tensorflow-how-to-determine-the-value-of-clip-norm-when-using-clip-by-norm-or-c/49921758#49921758', 'QuestionTitle': 'Tensorflow: How to determine the value of clip_norm when using clip_by_norm or clip_by_global_norm?', 'Answer': '<p>To view norm or global norm in Tensorboard you can manually calculate it.</p>\n\n<p>For the norm (not global) you obtain gradient as usual using <code>compute_gradients</code> and compute gradient l2 norm using <a href=""https://www.tensorflow.org/api_docs/python/tf/norm"" rel=""nofollow noreferrer""><code>tf.norm</code></a>. This gives you a scalar value. You can add it to Tensorboard using <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/scalar"" rel=""nofollow noreferrer""><code>tf.summary.scalar</code></a>.</p>\n\n<p>For the global norm you can compute it using <a href=""https://www.tensorflow.org/api_docs/python/tf/global_norm"" rel=""nofollow noreferrer""><code>tf.global_norm</code></a> and then add it to Tensorboard (also scalar summary).</p>\n\n<p><strong>EDIT</strong>:</p>\n\n<p>Please note that <a href=""https://www.tensorflow.org/api_docs/python/tf/norm"" rel=""nofollow noreferrer""><code>tf.norm</code></a> expects single tensor. So you need to compute norm and add to to Tensorboard for each gradients tensor. The same as you do when using <code>tf.summary.histogram</code> to view values.</p>\n', 'IsAccepted': True, 'CreationDate': 1524142695}, {'QuestionId': 48455294, 'AnswerId': 48456088, 'URL': 'https://stackoverflow.com/questions/48455294/clippingfiltering-tf-placeholder-values/48456088#48456088', 'QuestionTitle': 'Clipping(Filtering) tf.placeholder values', 'Answer': '<p>Yes, it\'s even easier than you think:</p>\n\n<pre><code>x = tf.placeholder(tf.float32, None)\n# create a bool tensor the same shape as x\ncondition = x &lt; SmallConst \n\n# create tensor same shape as x, with values greater than SmallConst set to 0\nto_remove = x*tf.to_float(condition)\n\n# set all values of x less than SmallConst to 0\nx_clipped = x - to_remove \n</code></pre>\n\n<p>I\'d normally just put that into one line like:</p>\n\n<pre><code>x_clipped = x - x*tf.to_float(x &lt; small_const)\n</code></pre>\n\n<p><em>note:</em> using <code>tf.to_float</code> on a tensor of type <code>bool</code> will give you <code>0.0</code>s in place of <code>False</code>s and <code>1.0</code>s in place of <code>True</code>s</p>\n\n<hr>\n\n<p>Additional information for cleaner code:</p>\n\n<p>The numerical operators (e.g. <code>&lt;</code>, <code>&gt;=</code>, <code>+</code>, <code>-</code> etc, but not <code>==</code>) are overloaded for tensorflow tensors such that you can use native python variables with tensors to get a new tensor that is the result of that operation. So <code>tf.constant()</code> is actually fairly rarely actually needed. Example of this in action:</p>\n\n<pre><code>a = tf.placeholder(tf.int32)\nb = a + 1\nc = a &gt; 0\nprint(b) # gives ""&lt;tf.Tensor \'add:0\' shape=&lt;unknown&gt; dtype=int32&gt;""\nprint(c) # gives ""&lt;tf.Tensor \'Greater:0\' shape=&lt;unknown&gt; dtype=bool&gt;""\nsess.run(b, {a: 1}) # gives scalar int32 numpy array with value 2\nsess.run(c, {a: 1}) # gives scalar bool numpy array with value True\n</code></pre>\n\n<p>This is also true of numpy.</p>\n\n<hr>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/assign"" rel=""nofollow noreferrer""><code>tf.assign()</code></a> only works on Variables because it will</p>\n\n<blockquote>\n  <p>Update \'ref\' by assigning \'value\' to it.</p>\n</blockquote>\n\n<p>Tensors in tensorflow are immutable. The result of any operation on a tensor is another tensor, but the original tensor will never change. Variables, however are mutable, and you change their value with <code>tf.assign()</code></p>\n', 'IsAccepted': True, 'CreationDate': 1516943360}, {'QuestionId': 48431282, 'AnswerId': 48437314, 'URL': 'https://stackoverflow.com/questions/48431282/only-evaluate-non-zero-values-of-tf-tensor/48437314#48437314', 'QuestionTitle': 'Only evaluate non-zero values of tf.Tensor', 'Answer': '<p>I would write the metric in tensorflow on my own like:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n\ndata = np.array([0, 1, 2, 0, 0, 3, 7, 0]).astype(np.float32)\npred = np.random.randn(8).astype(np.float32)\ngt = np.random.randn(8).astype(np.float32)\n\ndata_op = tf.convert_to_tensor(data)\npred_op = tf.convert_to_tensor(pred)\ngt_op = tf.convert_to_tensor(gt)\n\nexpected = np.sqrt(((gt[data != 0] - pred[data != 0]) ** 2).mean())\n\n\ndef nonzero_mean(gt_op, pred_op, data_op):\n    mask_op = 1 - tf.cast(tf.equal(data_op, 0), tf.float32)\n\n    actual_op = ((gt_op - pred_op) * mask_op)**2\n    actual_op = tf.reduce_sum(actual_op) / tf.cast(tf.count_nonzero(mask_op), tf.float32)\n    actual_op = tf.sqrt(actual_op)\n    return actual_op\n\nwith tf.Session() as sess:\n\n    actual = sess.run(nonzero_mean(gt_op, pred_op, data_op))\n\nprint actual, expected\n</code></pre>\n\n<p>The <code>y_true != 0</code> is not possible in plain Tensorflow. Not sure, if keras does some magic here.</p>\n', 'IsAccepted': False, 'CreationDate': 1516863805}, {'QuestionId': 48431282, 'AnswerId': 48432269, 'URL': 'https://stackoverflow.com/questions/48431282/only-evaluate-non-zero-values-of-tf-tensor/48432269#48432269', 'QuestionTitle': 'Only evaluate non-zero values of tf.Tensor', 'Answer': '<p>Yes, indeed the problem lies in using <code>numpy</code> function. Here is a quick fix:</p>\n\n<pre><code>def evaluation_metric(y_true, y_pred):\n\n    y_true = y_true * (y_true != 0) \n    y_pred = y_pred * (y_true != 0)\n\n    error = sqrt(mean_squared_error(y_true, y_pred))\n    return error\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1516830810}, {'QuestionId': 46545986, 'AnswerId': 46546528, 'URL': 'https://stackoverflow.com/questions/46545986/how-to-use-tf-clip-by-value-on-sliced-tensor-in-tensorflow/46546528#46546528', 'QuestionTitle': 'How to use tf.clip_by_value() on sliced tensor in tensorflow?', 'Answer': '<p>I think the most straightforward (but maybe not optimal) way is to split <code>outputs</code> along the second dimension using <code>tf.split</code>, then apply the clipping and concatenate back (if needed).</p>\n\n<pre><code>temperature, humidity = tf.split(output, 2, axis=1)\nhumidity = tf.clip_by_value(humidity, 0, 100)\n\n# optional concat\nclipped_output = tf.concat([temperature, humidity], axis=1)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1507039427}, {'QuestionId': 46545986, 'AnswerId': 46546365, 'URL': 'https://stackoverflow.com/questions/46545986/how-to-use-tf-clip-by-value-on-sliced-tensor-in-tensorflow/46546365#46546365', 'QuestionTitle': 'How to use tf.clip_by_value() on sliced tensor in tensorflow?', 'Answer': '<p>If your <code>outputs</code> is a variable, you can use <code>tf.assign</code>:</p>\n\n<pre><code>tf.assign(outputs[:,1], tf.clip_by_value(outputs[:,1], 0, 100))\n</code></pre>\n\n<hr>\n\n<pre><code>import tensorflow as tf\na = tf.Variable([[23, 78],\n [24, 79],\n [25, 78],\n [23, 81],\n [27, 82],\n [21, 87],\n [28, 88],\n [23, 90]])\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    clipped_value = tf.clip_by_value(a[:,1], 80, 85)\n    sess.run(tf.assign(a[:,1], clipped_value))\n    print(sess.run(a))\n\n#[[23 80]\n# [24 80]\n# [25 80]\n# [23 81]\n# [27 82]\n# [21 85]\n# [28 85]\n# [23 85]]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1507038962}, {'QuestionId': 45906143, 'AnswerId': 45907228, 'URL': 'https://stackoverflow.com/questions/45906143/how-to-return-the-equivalent-of-none-in-tensorflow/45907228#45907228', 'QuestionTitle': 'How to return the equivalent of &quot;None&quot; in Tensorflow', 'Answer': '<p>Tensors are containers for numerical data types, e.g.\n<code>tf.convert_to_tensor(None)</code> raises <code>ValueError: None values not supported.</code>. So there is <strong>no</strong> <em>None</em>-Tensor.</p>\n\n<p>I would do it like:</p>\n\n<pre><code>mask = tf.less(0, 1) # return a tensor of type bool\nfiltered = tf.cast(mask, unfiltered.dtype) * unfiltered\n</code></pre>\n\n<p>I would <em>never</em> add NaNs on my own to the computation. They strongly indicate that something went wrong.</p>\n', 'IsAccepted': False, 'CreationDate': 1503852731}, {'QuestionId': 43144785, 'AnswerId': 43151823, 'URL': 'https://stackoverflow.com/questions/43144785/how-to-clip-the-gradient-norm-on-the-grad-and-var-tuple-in-tensorflow-r1-0/43151823#43151823', 'QuestionTitle': 'How to clip the gradient norm on the grad_and_var tuple in tensorflow-r1.0?', 'Answer': '<p>you can clip norm and add gradient noise like this:</p>\n\n<pre><code>opt = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate)\ngvs = opt.compute_gradients(self.loss)\ngvs = [(tf.clip_by_norm(grad,self.config.max_grad), val) for grad,val in gvs]\ngvs = [(tf.add(grad, tf.random_normal(tf.shape(grad),stddev=self.config.grad_noise)), val) for grad,val in gvs]\nself.train_op = opt.apply_gradients(gvs)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1491010294}, {'QuestionId': 43144785, 'AnswerId': 43145062, 'URL': 'https://stackoverflow.com/questions/43144785/how-to-clip-the-gradient-norm-on-the-grad-and-var-tuple-in-tensorflow-r1-0/43145062#43145062', 'QuestionTitle': 'How to clip the gradient norm on the grad_and_var tuple in tensorflow-r1.0?', 'Answer': '<p>One possible approach that I have seen is to zip <code>clipped_gradients</code> and your variables and to use <code>opt.apply_gradients</code> on the zipped list, like in the code below (taken from <a href=""https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/model.py"" rel=""nofollow noreferrer"">here</a>, lines 78-83):</p>\n\n<pre><code>tvars = tf.trainable_variables()\ngrads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n                args.grad_clip)\nwith tf.name_scope(\'optimizer\'):\n    optimizer = tf.train.AdamOptimizer(self.lr)\nself.train_op = optimizer.apply_gradients(zip(grads, tvars))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1490976173}, {'QuestionId': 42194051, 'AnswerId': 42194197, 'URL': 'https://stackoverflow.com/questions/42194051/filter-out-non-zero-values-in-a-tensor/42194197#42194197', 'QuestionTitle': 'Filter out non-zero values in a tensor', 'Answer': '<p>First create a boolean mask to identify where your condition is true; then apply the mask to your tensor, as shown below. You can if you want use tf.where to index - however it returns a tensor using x&amp;y with the same rank as the input so without further work the best you could achieve would be something like [[[1 -1 3 5 -1 8 6]]] changing -1 with something that you would identify to remove later. Just using where (without x&amp;y) will give you the index of all values where your condition is true so a solution can be created using indexes if that is what you prefer. My recommendation is below for the most clarity.  </p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\ninput = np.array([[1,0,3,5,0,8,6]])\nX = tf.placeholder(tf.int32,[None,7])\nzeros = tf.cast(tf.zeros_like(X),dtype=tf.bool)\nones = tf.cast(tf.ones_like(X),dtype=tf.bool)\nloc = tf.where(input!=0,ones,zeros)\nresult=tf.boolean_mask(input,loc)\nwith tf.Session() as sess:\n out = sess.run([result],feed_dict={X:input})\n print (np.array(out))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1486939786}, {'QuestionId': 39295136, 'AnswerId': 39295309, 'URL': 'https://stackoverflow.com/questions/39295136/gradient-clipping-appears-to-choke-on-none/39295309#39295309', 'QuestionTitle': 'Gradient clipping appears to choke on None', 'Answer': ""<p>So, one option that seems to work is this:</p>\n\n<pre><code>    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    if gradient_clipping:\n        gradients = optimizer.compute_gradients(loss)\n\n        def ClipIfNotNone(grad):\n            if grad is None:\n                return grad\n            return tf.clip_by_value(grad, -1, 1)\n        clipped_gradients = [(ClipIfNotNone(grad), var) for grad, var in gradients]\n        opt = optimizer.apply_gradients(clipped_gradients, global_step=global_step)\n    else:\n        opt = optimizer.minimize(loss, global_step=global_step)\n</code></pre>\n\n<p>It looks like compute_gradients returns None instead of a zero tensor when the gradient would be a zero tensor and tf.clip_by_value does not support a None value. So just don't pass None to it and preserve None values.</p>\n"", 'IsAccepted': True, 'CreationDate': 1472827882}]","{43671393, 49987839}","['<p>It is because <code>c</code> is calculated based on <code>a</code>, not <code>e</code>. You could change the line of gradient tensor as below. </p>\n\n<pre><code>d = tf.gradients(c,a)\n</code></pre>\n\n<p>BTW, in your original code, if you print <code>d</code>, you will find it is a <code>[None]</code></p>\n', ""<p>There's a small problem in your code: you're assigning the return value of <code>tf.clip_by_global_norm</code> to a single variable, when this function returns a pair of values.</p>\n\n<p>The documentation says:</p>\n\n<blockquote>\n  <p>Returns:</p>\n  \n  <p>list_clipped: A list of Tensors of the same type as list_t.</p>\n  \n  <p>global_norm: A 0-D (scalar) Tensor representing the global norm.</p>\n</blockquote>\n\n<p>Hence, the problem arises when you try to apply the gradients to the variables, in the next line.</p>\n\n<p>You can easily fix your code ignoring the global_norm returned value.</p>\n\n<pre><code>gradients, _ = tf.clip_by_global_norm(gradients, 2.5)\n</code></pre>\n""]","{'https://stackoverflow.com/questions/49987839/how-to-handle-none-in-tf-clip-by-global-norm/49988099#49988099', 'https://stackoverflow.com/questions/43671393/tensorflow-error-when-using-tf-gradients-and-tf-hessian-typeerror-fetch-argume/43673464#43673464'}",,0.21850858621071706,0.10851148137429839
5,55560676,tf.while_loop,Documentation Replicability,How to use tf.while_loop with eager execution?,"<p>In the documentation, the body of a tf.while_loop needs to be a python callable.</p>

<pre><code>i = tf.constant(0)
b = lambda i: tf.add(i,1)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>works but</p>

<pre><code>def b(i):
    tf.add(i,1)

i = tf.constant(0)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>throws a ValueError: Attempt to convert a value (None) with an unsupported type() to a Tensor</p>

<p>In 2.0, eager execution is default, I wonder what's the problem?!</p>
","<p>You forgot to add return statement to your function:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

def b(i):
    return tf.add(i, 1)

i = tf.constant(0)
c = lambda i: tf.less(i, 10)
tf.while_loop(c, b, [i]) # &lt;tf.Tensor: id=51, shape=(), dtype=int32, numpy=10&gt;
</code></pre>

<p>Note that in your first example function <code>b</code> does return incremented value:</p>

<pre class=""lang-py prettyprint-override""><code>i = tf.constant(0)
b = lambda i: tf.add(i,1)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
print(b(1).numpy()) # 2
</code></pre>
","{50486241, 59332392, 64484810, 50237486, 55632511, 49658802, 59299060, 37441140, 58588600, 62405567}","[{'QuestionId': 64484810, 'AnswerId': 64485371, 'URL': 'https://stackoverflow.com/questions/64484810/eager-execution-on-tensorflow2-3-0/64485371#64485371', 'QuestionTitle': 'eager execution on tensorflow2.3.0', 'Answer': '<p>if you want to have eager execution - import tf the regular way, not <code>tensorflow.compat.v1</code>. Then there is no need to use <code>session</code> at all. just enter formulas and print results:</p>\n<pre><code>import tensorflow as tf\n\nx = tf.constant([3,5,7])\ny = tf.constant([1,2,3])\nz1 = tf.add(x,y)\nz2 = x*y\nz3 = z2-z1\n\nprint(z1)\nprint(z2)\nprint(z3)\n\ntf.Tensor([ 4  7 10], shape=(3,), dtype=int32)\ntf.Tensor([ 3 10 21], shape=(3,), dtype=int32)\ntf.Tensor([-1  3 11], shape=(3,), dtype=int32)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1603379926}, {'QuestionId': 62405567, 'AnswerId': 62407166, 'URL': 'https://stackoverflow.com/questions/62405567/how-to-skip-the-current-iteration-of-tf-while-loop/62407166#62407166', 'QuestionTitle': 'How to skip the current iteration of tf.while_loop()?', 'Answer': '<p>There are several issues in the code, you shouldn\'t use <code>tf.Variable</code> objects for this, those <code>tf.map_fn</code> are avoidable and <code>tf.cond</code> must always have two branches. Here is a possible implementation of the code you linked in TensorFlow, adapted to work on batches of images. Each image in the batch is independently modified with the given probability on a different box. I broken down the logic in several functions for clarity.</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\n@tf.function\ndef random_erasing(img, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=[0.4914, 0.4822, 0.4465]):\n    \'\'\'\n    img is a 4-D variable (ex: tf.Variable(image, validate_shape=False) ) and NHWC order\n\n    probability: The probability that the operation will be performed.\n    sl: min erasing area\n    sh: max erasing area\n    r1: min aspect ratio\n    mean: erasing value\n    \'\'\'\n    return tf.where(tf.random.uniform([tf.shape(img)[0], 1, 1, 1]) &gt; probability,\n                    img,\n                    _do_random_erasing(img, sl, sh, r1, mean))\n\ndef _do_random_erasing(img, sl, sh, r1, mean):\n    s = tf.shape(img, out_type=tf.int32)\n    # Sample random h and w values\n    def sample_hw(h, w):\n        s = tf.shape(img)\n        area = s[1] * s[2]\n        target_area = tf.random.uniform([s[0]], sl, sh)\n        target_area *= tf.dtypes.cast(area, target_area.dtype)\n        aspect_ratio = tf.random.uniform([s[0]], r1, 1 / r1)\n        h_new = tf.dtypes.cast(tf.math.round(tf.math.sqrt(target_area * aspect_ratio)), tf.int32)\n        w_new = tf.dtypes.cast(tf.math.round(tf.math.sqrt(target_area / aspect_ratio)), tf.int32)\n        # Only replace values that are still wrong\n        m = (h &gt;= s[0]) | (w &gt;= s[1])\n        h = tf.where(m, h_new, h)\n        w = tf.where(m, w_new, w)\n        return h, w\n    # Loop\n    _, h, w = tf.while_loop(\n        # While there are iterations to go and h and w are not good\n        lambda i, h, w: (i &lt; 100) &amp; tf.reduce_any((h &gt;= s[1]) | (w &gt;= s[2])),\n        # Get new h and w values\n        lambda i, h, w: (i + 1, *sample_hw(h, w)),\n        [0, tf.fill([s[0]], s[1]), tf.fill([s[0]], s[2])])\n    # Erase box if we got valid h and w values\n    return tf.cond(tf.reduce_all((h &lt; s[1]) &amp; (w &lt; s[2])),\n                   lambda: _erase_random_box(img, h, w, mean),\n                   lambda: img)\n\ndef _erase_random_box(img, h, w, mean):\n    # Make box boundaries\n    s = tf.shape(img, out_type=tf.int32)\n    # Add extra dimensions for later\n    h = tf.reshape(h, [-1, 1, 1])\n    w = tf.reshape(w, [-1, 1, 1])\n    # Sample random boundaries\n    h_max = tf.dtypes.cast(s[1] - h + 1, tf.float32)\n    x1 = tf.dtypes.cast(tf.random.uniform(tf.shape(h)) * h_max, h.dtype)\n    w_max = tf.dtypes.cast(s[2] - w + 1, tf.float32)\n    y1 = tf.dtypes.cast(tf.random.uniform(tf.shape(w)) * w_max, w.dtype)\n    # Replacement mask\n    _, ii, jj = tf.meshgrid(tf.range(s[0]), tf.range(s[1]), tf.range(s[2]), indexing=\'ij\')\n    mask = (ii &gt;= x1) &amp; (ii &lt; x1 + h) &amp; (jj &gt;= y1) &amp; (jj &lt; y1 + w)\n    # Replace box\n    result = tf.where(tf.expand_dims(mask, axis=-1),\n                      tf.dtypes.cast(mean, img.dtype),\n                      img)\n    # Maybe can use tfa.image.cutout for this function?\n    return result\n\n# Test\ntf.random.set_seed(100)\n# Example batch of three 10x8 single-channel random images\nimg = tf.random.uniform([3, 8, 10, 1], dtype=tf.float32)\n# Apply erasing\nerased = random_erasing(img, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=[-1])\n# Check results\nwith np.printoptions(precision=2, suppress=True):\n    erased_np = erased.numpy()\n    print(erased_np[0, :, :, 0])\n    # [[ 0.25  0.48  0.39  0.82  0.24  0.39  0.96  0.74  0.31  0.78]\n    #  [ 0.36  0.44  0.39  0.41 -1.   -1.   -1.    0.99  0.08  0.7 ]\n    #  [ 0.3   0.69  0.95  0.65 -1.   -1.   -1.    0.37  0.5   0.66]\n    #  [ 0.42  0.64  0.71  0.86 -1.   -1.   -1.    0.78  0.16  0.19]\n    #  [ 0.47  0.66  0.97  0.63 -1.   -1.   -1.    0.66  0.41  0.18]\n    #  [ 0.56  0.33  0.58  0.03 -1.   -1.   -1.    0.01  0.44  0.29]\n    #  [ 0.77  0.63  0.61  0.09  0.77  0.25  0.15  0.18  0.75  0.6 ]\n    #  [ 0.74  0.4   0.15  0.18  0.18  0.07  0.53  0.16  0.61  0.42]]\n    print(erased_np[1, :, :, 0])\n    # [[0.55 0.31 0.67 0.42 0.93 0.31 0.1  0.67 0.11 0.3 ]\n    #  [0.99 0.66 0.57 0.51 0.01 0.76 0.69 0.28 0.1  0.6 ]\n    #  [0.91 0.63 0.23 0.   0.21 0.7  0.85 0.16 0.35 0.18]\n    #  [0.67 0.83 0.66 0.4  0.51 0.84 0.07 0.62 0.8  0.66]\n    #  [0.62 0.23 0.29 0.99 0.9  0.7  0.68 0.09 0.92 0.67]\n    #  [0.36 0.75 0.51 0.76 0.68 0.56 0.07 0.68 0.57 0.58]\n    #  [0.98 0.75 0.22 0.87 0.28 0.55 0.77 0.65 0.8  0.28]\n    #  [0.76 0.46 0.11 0.85 0.3  0.35 0.81 0.48 0.24 0.81]]\n    print(erased_np[2, :, :, 0])\n    # [[ 0.42  0.33  0.44  0.68  0.89  0.88  0.8   0.72  0.5   0.61]\n    #  [ 0.54 -1.   -1.   -1.   -1.    0.56  0.33  0.24  0.98  0.89]\n    #  [ 0.06 -1.   -1.   -1.   -1.    0.64  0.76  0.26  0.1   0.57]\n    #  [ 0.39 -1.   -1.   -1.   -1.    0.09  0.24  0.47  0.92  0.2 ]\n    #  [ 0.46 -1.   -1.   -1.   -1.    0.61  0.11  0.5   0.52  0.06]\n    #  [ 0.71  0.74  0.03  0.77  0.87  0.51  0.42  0.87  0.73  0.01]\n    #  [ 0.18  0.71  0.38  0.17  0.18  0.56  0.58  0.7   0.1   0.87]\n    #  [ 0.46  0.19  0.98  0.19  0.19  0.41  0.95  0.    0.82  0.05]]\n</code></pre>\n\n<p>One caveat with this function is that the <code>tf.while_loop</code> tries to find good <code>h</code> and <code>w</code> values for <em>all</em> images in the batch, but if it fails to sample a good pair of values in the 100 loop iterations even for just one of the images, then if will not apply the erasing to <em>any</em> image. You might tweak the code in one way or another to work around that, although I suppose just giving a reasonable number of iterations should be fine.</p>\n', 'IsAccepted': True, 'CreationDate': 1592306121}, {'QuestionId': 49658802, 'AnswerId': 60601644, 'URL': 'https://stackoverflow.com/questions/49658802/how-can-i-use-tf-data-datasets-in-eager-execution-mode/60601644#60601644', 'QuestionTitle': 'How can I use tf.data Datasets in eager execution mode?', 'Answer': '<p>With <strong>TF 2.1</strong>,</p>\n\n<p>You can create an iterator like so:</p>\n\n<pre><code>iterator = iter(dataset)\n</code></pre>\n\n<p>And get the next batch of values:</p>\n\n<pre><code>batch = iterator.get_next()\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1583760712}, {'QuestionId': 59332392, 'AnswerId': 59350211, 'URL': 'https://stackoverflow.com/questions/59332392/tf-function-and-tf-while-loop-in-tensorflow-2-0/59350211#59350211', 'QuestionTitle': 'tf.function and tf.while loop in Tensorflow 2.0', 'Answer': '<p>The code in your first snippet (the one without the <code>@tf.function</code>) takes advantage of TensorFlow 2\'s eager execution to manipulate a numpy array (i.e., your outer <code>iteration</code> object) directly. With <code>@tf.function</code>, this doesn\'t work because @tf.function tries to compile your code into a tf.Graph, which cannot operate on a numpy array directly (it can only process tensorflow tensors). To get around this issue, use a tf.Variable and keep assigning value into its slices.</p>\n\n<p>With <code>@tf.function</code>, what you are trying to do is actually achievable with simpler code, by taking advantage of <code>@tf.function</code>\'s automatic Python-to-graph transformation feature (known as AutoGraph). You just write a normal Python while loop (using <code>tf.less()</code> in lieu of the <code>&lt;</code> operator), and the while loop will be compiled by AutoGraph into a tf.while_loop under the hood. </p>\n\n<p>The code looks something like: </p>\n\n<pre class=""lang-py prettyprint-override""><code>result = tf.Variable(np.zeros([10], dtype=np.int32))\n\n@tf.function\ndef run_graph():\n  i = tf.constant(0, dtype=tf.int32)\n  while tf.less(i, 10):\n    result[i].assign(i)  # Performance may require tuning here.\n    i += 1\n\nrun_graph()\nprint(result.read_value())\n\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1576467054}, {'QuestionId': 59299060, 'AnswerId': 59323903, 'URL': 'https://stackoverflow.com/questions/59299060/tf-2-0-while-loop-and-parallel-iterations/59323903#59323903', 'QuestionTitle': 'TF 2.0 while_loop and parallel_iterations', 'Answer': '<p><code>parallel_iterations</code> doesn\'t mean anything when running in eager mode, but you can always use <code>tf.function</code> decorator and gain significant speedups. This can be seen in this picture: <a href=""https://i.stack.imgur.com/nz3GL.png"" rel=""nofollow noreferrer"">running times</a></p>\n\n<p>You can wrap your <code>tf.while_loop</code> with <code>tf.function</code> like this </p>\n\n<pre><code>@tf.function\ndef run_graph():\n    iteration = tf.constant(0)\n    r = tf.while_loop(c, print_fun, [iteration], parallel_iterations=4)\n</code></pre>\n\n<p>and then call <code>run_graph</code> when required.</p>\n', 'IsAccepted': False, 'CreationDate': 1576245067}, {'QuestionId': 58588600, 'AnswerId': 58592157, 'URL': 'https://stackoverflow.com/questions/58588600/relationship-between-eager-execution-and-tf-function/58592157#58592157', 'QuestionTitle': 'Relationship between Eager Execution and tf.function', 'Answer': '<p>Let me try to explain it. Hope it will be useful.</p>\n\n<ul>\n<li>@tf.function decorator work only when eager execution is disabled ?\n<strong>no, actually tf.function is something to accelerate execution when eager mode is enabled</strong></li>\n<li>What is the relationship between @tf.function decorator and eager mode of execution ?\n<strong>@tf.function will cause tensorflow autograph working and accelerate execution for those operation inside it.</strong></li>\n<li>How does TensorFlow switch between eager mode and non-eager mode internally ?\n<strong>tf.function and AutoGraph work by generating code and tracing it into TensorFlow graphs. So when you call a @tf.function decorated function the first time, tensorflow will first convert it into a graph, then execute it, after that, when you can the function again, it will just execute the graph.</strong></li>\n</ul>\n\n<p>You can also check the tensorflow documentation.\n<a href=""https://www.tensorflow.org/guide/function"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/function</a></p>\n', 'IsAccepted': False, 'CreationDate': 1572270694}, {'QuestionId': 55632511, 'AnswerId': 55633498, 'URL': 'https://stackoverflow.com/questions/55632511/eager-execution-of-tf-dataset-instances/55633498#55633498', 'QuestionTitle': 'Eager execution of tf.dataset instances', 'Answer': ""<p>A little change. You're first trying to slice numpy array with tensors and then convert result to tensor. But instead you first need to convert <code>large_array</code> to tensor and then slice. So instead of </p>\n\n<pre><code>return tf.convert_to_tensor(large_array[heigth: heigth+heigth_exapnd,\n                                  width: width+width_exapnd, :])\n</code></pre>\n\n<p>Do </p>\n\n<pre><code>return tf.convert_to_tensor(large_array)[heigth: heigth+heigth_exapnd,\n                                  width: width+width_exapnd, :]\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1554988770}, {'QuestionId': 37441140, 'AnswerId': 37444810, 'URL': 'https://stackoverflow.com/questions/37441140/how-to-use-tf-while-loop-in-tensorflow/37444810#37444810', 'QuestionTitle': 'How to use tf.while_loop() in tensorflow', 'Answer': ""<p>What is stopping you from adding more functionality to the body? You can build whatever complex computational graph you like in the body and take whatever inputs you like from the enclosing graph. Also, outside of the loop, you can then do whatever you want with whatever outputs you return. As you can see from the amount of 'whatevers', TensorFlow's control flow primitives were built with much generality in mind. Below is another 'simple' example, in case it helps.</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndef body(x):\n    a = tf.random_uniform(shape=[2, 2], dtype=tf.int32, maxval=100)\n    b = tf.constant(np.array([[1, 2], [3, 4]]), dtype=tf.int32)\n    c = a + b\n    return tf.nn.relu(x + c)\n\ndef condition(x):\n    return tf.reduce_sum(x) &lt; 100\n\nx = tf.Variable(tf.constant(0, shape=[2, 2]))\n\nwith tf.Session():\n    tf.global_variables_initializer().run()\n    result = tf.while_loop(condition, body, [x])\n    print(result.eval())\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1464200238}, {'QuestionId': 50486241, 'AnswerId': 50488872, 'URL': 'https://stackoverflow.com/questions/50486241/tensorflow-tf-while-loop-with-vector-condition/50488872#50488872', 'QuestionTitle': 'Tensorflow: tf.while_loop() with vector condition', 'Answer': '<p>The solution is to use tf.reduce_all():</p>\n\n<pre><code>i = tf.constant(0)\nc = lambda i: tf.reduce_all(tf.greater([10,10],[i,i]))\nb = lambda i: tf.add(i, 1)\nr = tf.while_loop(c, b, [i])\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1527079751}, {'QuestionId': 50237486, 'AnswerId': 50237643, 'URL': 'https://stackoverflow.com/questions/50237486/tf-data-iterator-get-next-how-to-advance-in-tf-while-loop/50237643#50237643', 'QuestionTitle': 'tf.data.Iterator.get_next(): How to advance in tf.while_loop?', 'Answer': '<p>You need to call <code>iterator.get_next()</code> every time you want to ""iterate inside one run"".</p>\n\n<p>For instance in your toy example, just replace your <code>body_op</code> with:</p>\n\n<pre class=""lang-python prettyprint-override""><code> body_op=lambda i: tf.Print(i, [iterator.get_next()], message=""This is sample: "")\n# This is sample: [0]\n# This is sample: [1]\n# This is sample: [2]\n# This is sample: [3]\n# This is sample: [4]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1525794573}, {'QuestionId': 49658802, 'AnswerId': 49659358, 'URL': 'https://stackoverflow.com/questions/49658802/how-can-i-use-tf-data-datasets-in-eager-execution-mode/49659358#49659358', 'QuestionTitle': 'How can I use tf.data Datasets in eager execution mode?', 'Answer': '<p><code>make_one_shot_iterator()</code> should work in TensorFlow 1.8, but for now (i.e., for TensorFlow 1.7), do the following:</p>\n\n<pre><code>import tensorflow.contrib.eager as tfe\n\ndataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([50, 10]))\ndataset = dataset.batch(5)\nfor batch in tfe.Iterator(dataset):\n     print(batch)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1522871667}]","{49265723, 49889359}","['<p>The issue I reported on tensorflow got this answer:</p>\n\n<blockquote>\n  <p>Thank you for the bug report. We have a fix for this issue, that will\n  show up on GitHub soon.</p>\n</blockquote>\n\n<p>See issue  <a href=""https://github.com/tensorflow/tensorflow/issues/18642"" rel=""nofollow noreferrer"">#18642</a> on GITHUB for Tensorflow.</p>\n\n<p>Based on this, I understand that method <code>fit()</code> of Keras models will be supported with eager execution, once the bug is fixed.</p>\n', '<p>It is best to write code that\'s compatible with both graph mode and eager execution. From the <a href=""https://www.tensorflow.org/programmers_guide/eager"" rel=""noreferrer"">documentation</a>:</p>\n\n<blockquote>\n  <ul>\n  <li>Use tf.data for input processing instead of queues. It\'s faster and easier.</li>\n  <li>Use object-oriented layer APIs—like tf.keras.layers and tf.keras.Model—since they have explicit storage for variables.</li>\n  <li>Most model code works the same during eager and graph execution, but there are exceptions. (For example, dynamic models using Python\n  control flow to change the computation based on inputs.)</li>\n  <li>Once eager execution is enabled with tf.enable_eager_execution, it cannot be turned off. Start a new Python session to return to graph\n  execution.</li>\n  </ul>\n</blockquote>\n\n<p>That said, it is possible to use eager execution while in graph mode by using <code>tfe.py_func()</code>. Here is the code example from the documentation (I just added the imports and asserts):</p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ndef my_py_func(x):\n    assert tf.executing_eagerly()\n    x = tf.matmul(x, x)  # You can use tf ops\n    print(x)  # but it\'s eager!\n    return x\n\nassert not tf.executing_eagerly()\nwith tf.Session() as sess:\n    x = tf.placeholder(dtype=tf.float32)\n    # Call eager function in graph!\n    pf = tfe.py_func(my_py_func, [x], tf.float32)\n    sess.run(pf, feed_dict={x: [[2.0]]})  # [[4.0]]\n</code></pre>\n\n<p>The reverse is also possible, as Alex Passos explains in <a href=""https://youtu.be/T8AW0fKP0Hs?t=788"" rel=""noreferrer"">this video</a>. Here is an example inspired by the one in the video:</p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\ntf.enable_eager_execution()\n\ndef my_graph_func(x):\n    assert not tf.executing_eagerly()\n    w = tfe.Variable(2.0)\n    b = tfe.Variable(4.0)\n    return x * w + b\n\nassert tf.executing_eagerly()\ng = tfe.make_template(""g"", my_graph_func, create_graph_function_=True)\nprint(g(3))\n</code></pre>\n\n<p>There\'s also an unofficial way to switch modes, using the <code>eager_mode</code> and <code>graph_mode</code> contexts defined in <code>tensorflow.python.eager.context</code> like this:</p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nfrom tensorflow.python.eager.context import eager_mode, graph_mode\n\nwith eager_mode():\n    print(""Eager mode"")\n    assert tf.executing_eagerly()\n    x1 = tfe.Variable(5.0)\n    print(x1.numpy())\n\nprint()\nwith graph_mode():\n    print(""Graph mode"")\n    assert not tf.executing_eagerly()\n\n    x2 = tfe.Variable(5.0)\n    with tf.Session():\n        x2.initializer.run()\n        print(x2.eval())\n</code></pre>\n\n<p>As it is not official, you should probably avoid it in production code, but it may come in handy when debugging, or in a Jupyter notebook. One last option is to use this <code>switch_to()</code> function:</p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nfrom tensorflow.python.eager.context import context, EAGER_MODE, GRAPH_MODE\n\ndef switch_to(mode):\n    ctx = context()._eager_context\n    ctx.mode = mode\n    ctx.is_eager = mode == EAGER_MODE\n\nswitch_to(EAGER_MODE)\nassert tf.executing_eagerly()\nv = tfe.Variable(3.0)\nprint(v.numpy())\nassert tf.get_default_graph().get_operations() == []\n\nswitch_to(GRAPH_MODE)\nassert not tf.executing_eagerly()\nv = tfe.Variable(3.0)\ninit = tf.global_variables_initializer()\nassert len(tf.get_default_graph().get_operations()) &gt; 0\nwith tf.Session():\n    init.run()\n    print(v.eval())\n</code></pre>\n\n<p>It is really a hack, but it may be useful in a Jupyter notebook, if you don\'t like nesting all your code in <code>with</code> blocks.</p>\n']","{'https://stackoverflow.com/questions/49889359/does-tensorflow-support-keras-models-fit-method-with-eager-execution/49929139#49929139', 'https://stackoverflow.com/questions/49265723/how-to-use-tensorflow-eager-execution-only-in-specific-parts-of-the-application/50297773#50297773'}",,0.20496062383765834,0.2439428935706383
5,64826405,tf.tensordot,Documentation Ambiguity,Tensorflow: 'axis' argument in dot product,"<p>Can someone show me the way I should use the <code>axis</code> argument in <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>?</p>
<p>I read the documentation but it was complicated and I'm still confused. I saw <a href=""https://stackoverflow.com/questions/48082900/in-tensorflow-what-is-the-argument-axis-in-the-function-tf-one-hot"">another question</a> that asks about <code>axis</code> in <code>tf.one_hot</code> and in the answers were some good insights about the matter, but that didn't help me with <code>tf.tensordot</code>. I thought you can give me some insights on this too.</p>
<p>For example, I know I can dot product a vector and a tensor like this:</p>
<pre><code>my_vector = tf.random.uniform(shape=[n])
my_tensor = tf.random.uniform(shape=[m, n])

dp = tf.tensordot(my_tensor, my_vector, 1)
</code></pre>
<p>But when I <em><strong>batch</strong></em> them and add one dimension to them to be of the shape <code>(b, n)</code> and <code>(b, m, n)</code> to obtain a <code>(b, m, 1)</code>, now I don't know how to dot product every batch.</p>
","<p>The operation that you want to do cannot be done (in an effective way) with <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>. There is, however, a dedicated function for that operation, <a href=""https://www.tensorflow.org/api_docs/python/tf/linalg/matvec"" rel=""nofollow noreferrer""><code>tf.linalg.matvec</code></a>, which will work with batches out of the box. And you can also do the same thing with <a href=""https://www.tensorflow.org/api_docs/python/tf/einsum"" rel=""nofollow noreferrer""><code>tf.einsum</code></a>, like <code>tf.einsum('bmn,bn-&gt;bm', my_tensors, my_vectors)</code>.</p>
<p>With respect to <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>, in general it computes an &quot;all vs all&quot; product of the two given tensors, but matching and reducing some axes. When no axes are given (you have to explicitly pass <code>axes=[[], []]</code> to do this), it creates a tensor with the dimensions of both inputs concatenated. So, if you have <code>my_tensors</code> with shape <code>(b, m, n)</code> and <code>my_vectors</code> with shape <code>(b, n)</code> and you do:</p>
<pre class=""lang-py prettyprint-override""><code>res = tf.tensordot(my_tensors, my_vectors, axes=[[], []])
</code></pre>
<p>You get <code>res</code> with shape <code>(b, m, n, b, n)</code>, such that <code>res[p, q, r, s, t] == my_tensors[p, q, r] * my_vectors[s, t]</code>.</p>
<p>The <code>axes</code> argument is used to specify dimensions in the input tensors that are &quot;matched&quot;. Values along matched axes are multiplied and summed (like a dot product), so those matched dimensions are reduced from the output. <code>axes</code> can take two different forms:</p>
<ul>
<li>If it is a single integer, <code>N</code> then the last <code>N</code> dimensions of the first parameter are matched against the first <code>N</code> dimensions of <code>b</code>. In your example, that corresponds to the dimensions with <code>n</code> elements in <code>my_tensor</code> and <code>my_vector</code>.</li>
<li>If it is a list, it must contain two sublists, <code>axes_a</code> and <code>axes_b</code>, each with the same number <code>N</code> of integers. In this form, you are explicitly indicating which dimensions of the given values are matched. So, in your example, you could have passed <code>axes=[[1], [0]]</code>, which means &quot;match the dimension <code>1</code> of the first parameter (<code>my_tensor</code>) to the dimension <code>0</code> of the second parameter (<code>my_vector</code>)&quot;.</li>
</ul>
<p>If you have now <code>my_tensors</code>  with shape <code>(b, m, n)</code> and <code>my_vectors</code> with shape <code>(b, n)</code>, then you would want to match the dimension <code>2</code> of the first one to the dimension <code>1</code> of the second one, so you could pass <code>axes=[[2], [1]]</code>. However, that will give you a result <code>res</code> with shape <code>(b, m, b)</code> such that <code>res[i, :, j]</code> is the product of matrix <code>my_tensors[i]</code> and vector <code>my_vectors[j]</code>. You could take then only the results that you want (those where <code>i == j</code>), with something more or less convoluted like <code>tf.transpose(tf.linalg.diag_part(tf.transpose(res, [1, 0, 2])))</code>, but you would be doing far more computation than you need to get the same result.</p>
","{40670370, 62100004, 46240646, 64283338, 39432138, 61599630, 59749107, 36030963, 70010037, 44539034}","[{'QuestionId': 40670370, 'AnswerId': 74407854, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/74407854#74407854', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': ""<p><strong>Use tf.reduce_sum(tf.multiply(x,y)) if you want the dot product of 2 vectors.</strong></p>\n<p>To be clear, using tf.matmul(x,tf.transpose(y)) won't get you the dot product, even if you add all the elements of the matrix together afterward.</p>\n<p>I'm only mentioning this because of how often it comes up in the above answers when it has nothing to do with the question being asked. I'd just make a comment, but don't have the rep to do that.</p>\n"", 'IsAccepted': False, 'CreationDate': 1668198882}, {'QuestionId': 70010037, 'AnswerId': 70023364, 'URL': 'https://stackoverflow.com/questions/70010037/tensorflow-multiplication-along-axis/70023364#70023364', 'QuestionTitle': 'TensorFlow multiplication along axis', 'Answer': '<p>I\'m not quite sure what result you\'re expecting, but if I understood you correctly, you could do something like this:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\na = tf.concat([tf.ones([1, 4, 3], dtype=tf.float32), \n               tf.ones([1, 4, 3], dtype=tf.float32) * 3, \n               tf.zeros([2, 4, 3], dtype=tf.float32)], axis=0)\nb = tf.constant(7, dtype=tf.float32)\n\ntensor = tf.slice(a,\n               begin=[1, 0, 0],\n               size=[1, 4, 3])\nc = tensor * b\nresult = tf.tensor_scatter_nd_update(a, [[1]], c)\nprint(\'a --&gt;\', a, \'\\n\')\nprint(\'c --&gt;\', c, \'\\n\')\nprint(\'result --&gt;\', result)\n</code></pre>\n<pre><code>a --&gt; tf.Tensor(\n[[[1. 1. 1.]\n  [1. 1. 1.]\n  [1. 1. 1.]\n  [1. 1. 1.]]\n\n [[3. 3. 3.]\n  [3. 3. 3.]\n  [3. 3. 3.]\n  [3. 3. 3.]]\n\n [[0. 0. 0.]\n  [0. 0. 0.]\n  [0. 0. 0.]\n  [0. 0. 0.]]\n\n [[0. 0. 0.]\n  [0. 0. 0.]\n  [0. 0. 0.]\n  [0. 0. 0.]]], shape=(4, 4, 3), dtype=float32) \n\nc --&gt; tf.Tensor(\n[[[21. 21. 21.]\n  [21. 21. 21.]\n  [21. 21. 21.]\n  [21. 21. 21.]]], shape=(1, 4, 3), dtype=float32) \n\nresult --&gt; tf.Tensor(\n[[[ 1.  1.  1.]\n  [ 1.  1.  1.]\n  [ 1.  1.  1.]\n  [ 1.  1.  1.]]\n\n [[21. 21. 21.]\n  [21. 21. 21.]\n  [21. 21. 21.]\n  [21. 21. 21.]]\n\n [[ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]]\n\n [[ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]\n  [ 0.  0.  0.]]], shape=(4, 4, 3), dtype=float32)\n</code></pre>\n<p>Update: slicing the data the way you want is not what you think it is:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\na = tf.concat([tf.ones([1, 2, 3], dtype=tf.float32), \n               tf.zeros([1, 2, 3], dtype=tf.float32)], axis=0)\n\nb = tf.ones([2, 2, 3], dtype=tf.float32)\n\nprint(a.shape, a[...,0])\nprint(b.shape, b[...,0])\n</code></pre>\n<pre><code>(2, 2, 3) tf.Tensor(\n[[1. 1.]\n [0. 0.]], shape=(2, 2), dtype=float32)\n(2, 2, 3) tf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32)\n</code></pre>\n<p>As you can see when using a (2, 2, 3) tensor with mixed values, you do not only get ones like you would expect, when slicing along the last axis.</p>\n', 'IsAccepted': False, 'CreationDate': 1637252667}, {'QuestionId': 64283338, 'AnswerId': 64285103, 'URL': 'https://stackoverflow.com/questions/64283338/how-to-select-numpy-tensordot-axes/64285103#64285103', 'QuestionTitle': 'How to select numpy tensordot axes', 'Answer': ""<p>The problem is likely much simpler than you are making it. If you apply <code>np.tensordot</code> to a pair of arrays of shape <code>(w, h, 2)</code> along the last axis, you will get a result of shape <code>(w, h, w, h)</code>. This is not what you want. There are three simple approaches here. In addition to showing the options, I've shown a few tips and tricks for making the code simpler without changing any of the basic functionality:</p>\n<ol>\n<li><p>Do the sum-reduction manually (using <code>+</code> and <code>*</code>):</p>\n<pre><code>def average_angular_error(estimated_oc : np.ndarray, target_oc : np.ndarray):\n    # If you want to do in-place normalization, do x /= ... instead of x = x / ...\n    estimated_oc = estimated_oc / np.linalg.norm(estimated_oc, axis=-1, keepdims=True)\n    target_oc = target_oc / np.linalg.norm(target_oc, axis=-1, keepdims=True)\n    # Use plain element-wise multiplication\n    dots = np.sum(estimated_oc * target_oc, axis=-1)\n    return np.arccos(dots).mean()\n</code></pre>\n</li>\n<li><p>Use <code>np.matmul</code> (a.k.a. <code>@</code>) with properly broadcasted dimensions:</p>\n<pre><code>def average_angular_error(estimated_oc : np.ndarray, target_oc : np.ndarray):\n    estimated_oc = estimated_oc / np.linalg.norm(estimated_oc, axis=-1, keepdims=True)\n    target_oc = target_oc / np.linalg.norm(target_oc, axis=-1, keepdims=True)\n    # Matrix multiplication needs two dimensions to operate on\n    dots = estimated_oc[..., None, :] @ target_oc[..., :, None]\n    return np.arccos(dots).mean()\n</code></pre>\n<p><code>np.matmul</code> and <code>np.dot</code> both require the last dimension of the first array to match the second to last of the second, like with normal matrix multiplication. <code>None</code> is an alias for <code>np.newaxis</code>, which introduces a new axis of size 1 at the location of your choice. In this case, I made the first array <code>(w, h, 1, 2)</code> and the second <code>(w, h, 2, 1)</code>. That ensures that the last two dimensions are multiplied as a transposed vector and a regular vector at every corresponding element.</p>\n</li>\n<li><p>Use <code>np.einsum</code>:</p>\n<pre><code>def average_angular_error(estimated_oc : np.ndarray, target_oc : np.ndarray):\n    estimated_oc = estimated_oc / np.linalg.norm(estimated_oc, axis=-1, keepdims=True)\n    target_oc = target_oc / np.linalg.norm(target_oc, axis=-1, keepdims=True)\n    # Matrix multiplication needs two dimensions to operate on\n    dots = np.einsum('ijk,ijk-&gt;ik', estimated_oc, target_oc)\n    return np.arccos(dots).mean()\n</code></pre>\n</li>\n</ol>\n<p>You can't use <code>np.dot</code> or <code>np.tensordot</code> for this. <code>dot</code> and <code>tensordot</code> keep the untouched dimensions of both arrays, as explained earlier. <code>matmul</code> broadcasts them together, which is what you want.</p>\n"", 'IsAccepted': True, 'CreationDate': 1602266487}, {'QuestionId': 62100004, 'AnswerId': 62103791, 'URL': 'https://stackoverflow.com/questions/62100004/numpy-dot-product-along-specific-axes/62103791#62103791', 'QuestionTitle': 'Numpy dot product along specific axes', 'Answer': '<p>As you suspected, <code>np.einsum</code> can take care of this. If <code>input</code> and <code>some_other_array</code> have shapes <code>(64, 8, 64, 8)</code>, then if you write</p>\n\n<pre><code>output = np.einsum(\'ijkl,ilkm-&gt;ijkm\', input, some_other_array)  \n</code></pre>\n\n<p>then <code>output</code> will also have shape <code>(64, 8, 64, 8)</code>, where matrix multiplication (i.e. <code>np.dot</code>) has been done only on axes <code>1</code> and <code>3</code>.</p>\n\n<p>The string argument to <code>np.einsum</code> looks complicated, but really it\'s a combination of two things. First, matrix multiplication is given by <code>jl,lm-&gt;jm</code> (see e.g. <a href=""https://stackoverflow.com/questions/26089893/understanding-numpys-einsum"">this answer on einsum</a>). Second, we don\'t want to do anything to axis <code>0</code> and <code>2</code>, so for them I just write <code>ik,ik-&gt;ik</code>. Combining the two gives <code>ijkl,ilkm-&gt;ijkm</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1590848601}, {'QuestionId': 62100004, 'AnswerId': 62101260, 'URL': 'https://stackoverflow.com/questions/62100004/numpy-dot-product-along-specific-axes/62101260#62101260', 'QuestionTitle': 'Numpy dot product along specific axes', 'Answer': ""<p>They'll work if you reorder them a bit. If input and some_other_array are both shaped (64,8,64,8), then:</p>\n\n<pre><code>input = input.transpose(0,2,1,3)\nsome_other_array = some_other_array.transpose(0,2,1,3)\n</code></pre>\n\n<p>This will reorder them to 64,64,8,8. At this point you can compute a matrix multiplication. Do note that you need matmul to compute the block products, and not dot, which will try to multiply the entire matrices.</p>\n\n<pre><code>output = input @ some_other_array\noutput = output.transpose(0,2,1,3)\noutput = output.reshape(512,512)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1590835388}, {'QuestionId': 61599630, 'AnswerId': 61599838, 'URL': 'https://stackoverflow.com/questions/61599630/tensorflow-axis-definition-starts-from-0-or-1/61599838#61599838', 'QuestionTitle': 'tensorflow axis definition starts from 0 or 1?', 'Answer': '<p>So you have a tensor <code>a</code>, with shape <code>[x, y, z, w]</code>. When using a function which expects the axis parameter, the axis numbering for <code>a</code> will be <code>[0, 1, 2, 3]</code>. Reducing axes 1 and 3 results in those axes ""disappearing"" (because you are reducing those axes), so the output will consist of original axes <code>[0, 2]</code> which means the resulting shape is <code>[x, z]</code>. </p>\n', 'IsAccepted': True, 'CreationDate': 1588619583}, {'QuestionId': 59749107, 'AnswerId': 59752145, 'URL': 'https://stackoverflow.com/questions/59749107/move-axis-in-tensorflow/59752145#59752145', 'QuestionTitle': 'Move axis in tensorflow', 'Answer': '<p>You can use the tensorflow <strong>scatter</strong> function <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_nd"" rel=""nofollow noreferrer""><code>tf.scatter_nd</code></a> for achieving this.</p>\n\n<p>Define your <code>input</code> tensor:</p>\n\n<pre><code>input = tf.constant([[[ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217]],\n\n   [[ 450,  607,  493,  662],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[ 950, 1277, 1028, 1335],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]]])\n</code></pre>\n\n<p>Since we are interested in only the first 3 elements along zeroth dimension, let\'s slice it into a new tensor:</p>\n\n<pre><code>sliced_input = tf.slice(input, [0, 0, 0], [3, -1, -1])\n</code></pre>\n\n<p>Define your target <code>indices</code>:</p>\n\n<pre><code>indices = tf.constant([[0], [2], [5]])\n</code></pre>\n\n<p>Define <code>shapes</code> of your target <code>output</code>, here same as your <code>input</code> shape:</p>\n\n<pre><code>shape = tf.shape(input)\n</code></pre>\n\n<p>Now use the <strong>scatter</strong> function to get your <code>output</code>:</p>\n\n<pre><code>output = tf.scatter_nd(indices, sliced_input, shape)\n</code></pre>\n\n<p><code>output</code>:</p>\n\n<pre><code>array([[[ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217],\n    [ 298, 1217,  298, 1217]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[ 450,  607,  493,  662],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[ 950, 1277, 1028, 1335],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]],\n\n   [[   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0],\n    [   0,    0,    0,    0]]], dtype=int32)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1579093813}, {'QuestionId': 40670370, 'AnswerId': 45845122, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/45845122#45845122', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': '<p>One of the easiest way to calculate dot product between two tensors (vector is 1D tensor) is using <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""noreferrer""><code>tf.tensordot</code></a></p>\n\n<pre><code>a = tf.placeholder(tf.float32, shape=(5))\nb = tf.placeholder(tf.float32, shape=(5))\n\ndot_a_b = tf.tensordot(a, b, 1)\n\nwith tf.Session() as sess:\n    print(dot_a_b.eval(feed_dict={a: [1, 2, 3, 4, 5], b: [6, 7, 8, 9, 10]}))\n# results: 130.0\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1503506180}, {'QuestionId': 40670370, 'AnswerId': 46661442, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/46661442#46661442', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': ""<p>Let us assume that you have two column vectors</p>\n\n<pre><code>u = tf.constant([[2.], [3.]])\nv = tf.constant([[5.], [7.]])\n</code></pre>\n\n<p>If you want a 1x1 matrix you can use</p>\n\n<pre><code>tf.einsum('ij,ik-&gt;jk',x,y)\n</code></pre>\n\n<p>If you are interested in a scalar you can use</p>\n\n<pre><code>tf.einsum('ij,ik-&gt;',x,y)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1507622271}, {'QuestionId': 40670370, 'AnswerId': 46659548, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/46659548#46659548', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': '<h2>Just use * and reduce_sum</h2>\n\n<pre><code>ab = tf.reduce_sum(a*b)\n</code></pre>\n\n<p>Take a simple example as follows:</p>\n\n<pre><code>import tensorflow as tf\na = tf.constant([1,2,3])\nb = tf.constant([2,3,4])\n\nprint(a.get_shape())\nprint(b.get_shape())\n\nc = a*b\nab = tf.reduce_sum(c)\n\nwith tf.Session() as sess:\n    print(c.eval())\n    print(ab.eval())\n\n# output\n# (3,)\n# (3,)\n# [2 6 12]\n# 20\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1507615082}, {'QuestionId': 46240646, 'AnswerId': 46241024, 'URL': 'https://stackoverflow.com/questions/46240646/tensor-multiply-along-axis-in-tensorflow/46241024#46241024', 'QuestionTitle': 'Tensor multiply along axis in tensorflow', 'Answer': '<p>Given a 2-dimensional tensor <code>x</code> and a vector <code>y</code>, you just need to do:</p>\n\n<pre><code>result = x * tf.expand_dims(y, axis=-1)\n</code></pre>\n\n<p>Or, if you like it more:</p>\n\n<pre><code>result = x * y[:, tf.newaxis]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1505482891}, {'QuestionId': 44539034, 'AnswerId': 44541214, 'URL': 'https://stackoverflow.com/questions/44539034/why-tensorflows-division-hasnt-have-axis-parameter/44541214#44541214', 'QuestionTitle': 'Why TensorFlow&#39;s division hasn&#39;t have axis parameter?', 'Answer': '<p>Your second solution with <code>A / tf.transpose(v)</code> should work. <a href=""https://www.tensorflow.org/api_docs/python/tf/div"" rel=""nofollow noreferrer"">tf.div()</a> does not have an axis parameter because it <em>""Divides x / y elementwise""</em>. So both tensors should have the same dimensions.</p>\n\n<p>In your case it works with different dimensions because <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops/arithmetic_operators#div"" rel=""nofollow noreferrer"">tf.div supports broadcasting</a>. More about broadcasting is <a href=""https://www.tensorflow.org/performance/xla/broadcasting#what_is_broadcasting"" rel=""nofollow noreferrer"">here</a>.</p>\n\n<p>Also it is as efficient as it could be, so no need to look for anything else.</p>\n', 'IsAccepted': True, 'CreationDate': 1497433772}, {'QuestionId': 40670370, 'AnswerId': 40672159, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/40672159#40672159', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': '<p>In addition to <code>tf.reduce_sum(tf.multiply(x, y))</code>, you can also do <code>tf.matmul(x, tf.reshape(y, [-1, 1]))</code>.</p>\n', 'IsAccepted': False, 'CreationDate': 1479456585}, {'QuestionId': 40670370, 'AnswerId': 42865229, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/42865229#42865229', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': '<p>Maybe with the new docs you can just set the transpose option to true for either the first argument of the dot product or the second argument:</p>\n\n<pre><code>tf.matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None)\n</code></pre>\n\n<p>leading:</p>\n\n<pre><code>tf.matmul(a, b, transpose_a=True, transpose_b=False)\ntf.matmul(a, b, transpose_a=False, transpose_b=True)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1489777010}, {'QuestionId': 40670370, 'AnswerId': 42213093, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/42213093#42213093', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': '<pre><code>import tensorflow as tf\n\nx = tf.Variable([1, -2, 3], tf.float32, name=\'x\')\ny = tf.Variable([-1, 2, -3], tf.float32, name=\'y\')\n\ndot_product = tf.reduce_sum(tf.multiply(x, y))\n\nsess = tf.InteractiveSession()\ninit_op = tf.global_variables_initializer()\nsess.run(init_op)\n\ndot_product.eval()\n\nOut[46]: -14\n</code></pre>\n\n<p>Here, x and y are both vectors. We can do element wise product and then use tf.reduce_sum to sum the elements of the resulting vector. This solution is easy to read and does not require reshaping.</p>\n\n<p>Interestingly, it does not seem like there is a built in dot product operator in the <a href=""https://www.tensorflow.org/api_docs/python/math_ops/"" rel=""nofollow noreferrer"">docs</a>. </p>\n\n<p>Note that you can easily check intermediate steps:</p>\n\n<pre><code>In [48]: tf.multiply(x, y).eval()\nOut[48]: array([-1, -4, -9], dtype=int32)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1487018274}, {'QuestionId': 40670370, 'AnswerId': 41941688, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/41941688#41941688', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': '<p>you can use tf.matmul and tf.transpose</p>\n\n<pre><code>tf.matmul(x,tf.transpose(y))\n</code></pre>\n\n<p>or </p>\n\n<pre><code>tf.matmul(tf.transpose(x),y)\n</code></pre>\n\n<p>depending on the dimensions of x and y</p>\n', 'IsAccepted': False, 'CreationDate': 1485796562}, {'QuestionId': 40670370, 'AnswerId': 41939481, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/41939481#41939481', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': '<p>In newer versions (I think since 0.12), you should be able to do</p>\n\n<pre><code>tf.einsum(\'i,i-&gt;\', x, y)\n</code></pre>\n\n<p>(<a href=""https://github.com/tensorflow/tensorflow/blame/8532897352ada1d8ecd3ca1dd17aaa869a42d4b8/tensorflow/python/ops/special_math_ops.py#L100"" rel=""nofollow noreferrer"">Before that</a>, the reduction to a scalar seemed not to be allowed/possible.)</p>\n', 'IsAccepted': False, 'CreationDate': 1485789826}, {'QuestionId': 40670370, 'AnswerId': 40670426, 'URL': 'https://stackoverflow.com/questions/40670370/dot-product-of-two-vectors-in-tensorflow/40670426#40670426', 'QuestionTitle': 'Dot product of two vectors in tensorflow', 'Answer': '<p>You can do tf.mul(x,y), followed by tf.reduce_sum()</p>\n', 'IsAccepted': False, 'CreationDate': 1479449435}, {'QuestionId': 39432138, 'AnswerId': 39437803, 'URL': 'https://stackoverflow.com/questions/39432138/multiplying-along-an-arbitrary-axis/39437803#39437803', 'QuestionTitle': 'Multiplying along an arbitrary axis?', 'Answer': '<p>To answer your second question, ""is there a name for this kind of multiplication that I didn\'t know to search for?"", it is an element-wise multiplication with broadcasting. Broadcasting refers to operations that implicitly replicate elements of a tensor to make it compatible with a second tensor used in the element-wise operation. Many Tensorflow operations use the same broadcasting methods used by Numpy, which are further described <a href=""http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""nofollow"">here</a> </p>\n', 'IsAccepted': False, 'CreationDate': 1473608395}, {'QuestionId': 39432138, 'AnswerId': 39436721, 'URL': 'https://stackoverflow.com/questions/39432138/multiplying-along-an-arbitrary-axis/39436721#39436721', 'QuestionTitle': 'Multiplying along an arbitrary axis?', 'Answer': '<p>This is straightforward. Just multiply both tensors. For example:</p>\n\n<pre><code>import tensorflow as tf\n\ntensor = tf.Variable(tf.ones([2, 2, 2, 3]))\ndepth = tf.constant([4, 5, 6], dtype=tf.float32)\nresult = tensor * depth\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nprint(sess.run(result))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1473601485}, {'QuestionId': 39432138, 'AnswerId': 39435921, 'URL': 'https://stackoverflow.com/questions/39432138/multiplying-along-an-arbitrary-axis/39435921#39435921', 'QuestionTitle': 'Multiplying along an arbitrary axis?', 'Answer': ""<p>I've come up with the following: </p>\n\n\n\n<pre><code>a = tf.placeholder(tf.float32, shape = [5, 2])\nb = tf.placeholder(tf.float32, shape = 2)\n\nc = tf.concat(1, [tf.mul(x, y) for x, y in zip(tf.split(0, 2, b), tf.split(1, 2, a))])\n\nsess = tf.Session()\nprint sess.run(c, feed_dict = {a: np.ones([5, 2]), b: [5, 6]})\n</code></pre>\n\n<p>Looks a little bit odd, but it seems to work fine for me. I've used 2d tensor, but you definetely could extend this to your case. </p>\n"", 'IsAccepted': False, 'CreationDate': 1473595804}, {'QuestionId': 36030963, 'AnswerId': 36041342, 'URL': 'https://stackoverflow.com/questions/36030963/dot-product-along-third-axis/36041342#36041342', 'QuestionTitle': 'Dot product along third axis', 'Answer': '<p>Using <code>.dot</code> works just fine for me:</p>\n\n<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.array([[[1, 1, 1],\n                     [0, 0, 0],\n                     [2, 2, 2]],\n\n                    [[0, 0, 0],\n                     [4, 4, 4],\n                     [0, 0, 0]]])\n&gt;&gt;&gt; arr.dot([1, 1, 1])\narray([[ 3,  0,  6],\n       [ 0, 12,  0]])\n</code></pre>\n\n<p>Although interestingly is slower than all the other suggestions</p>\n', 'IsAccepted': False, 'CreationDate': 1458145256}, {'QuestionId': 36030963, 'AnswerId': 36031086, 'URL': 'https://stackoverflow.com/questions/36030963/dot-product-along-third-axis/36031086#36031086', 'QuestionTitle': 'Dot product along third axis', 'Answer': '<p>The reduction is along <code>axis=2</code> for <code>arr</code> and <code>axis=0</code> for <code>w</code>. Thus, with <a href=""http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.tensordot.html"" rel=""noreferrer""><code>np.tensordot</code></a>, the solution would be -</p>\n\n<pre><code>np.tensordot(arr,w,axes=([2],[0]))\n</code></pre>\n\n<p>Alternatively, one can also use <a href=""http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.einsum.html"" rel=""noreferrer""><code>np.einsum</code></a> -</p>\n\n<pre><code>np.einsum(\'ijk,k-&gt;ij\',arr,w)\n</code></pre>\n\n<p><a href=""http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.matmul.html"" rel=""noreferrer""><code>np.matmul</code></a> also works</p>\n\n<pre><code>np.matmul(arr, w)\n</code></pre>\n\n<p>Runtime test -</p>\n\n<pre><code>In [52]: arr = np.random.rand(200,300,300)\n\nIn [53]: w = np.random.rand(300)\n\nIn [54]: %timeit np.tensordot(arr,w,axes=([2],[0]))\n100 loops, best of 3: 8.75 ms per loop\n\nIn [55]: %timeit np.einsum(\'ijk,k-&gt;ij\',arr,w)\n100 loops, best of 3: 9.78 ms per loop\n\nIn [56]: %timeit np.matmul(arr, w)\n100 loops, best of 3: 9.72 ms per loop\n</code></pre>\n\n<p>hlin117 tested on Macbook Pro OS X El Capitan, numpy version 1.10.4.</p>\n', 'IsAccepted': True, 'CreationDate': 1458119344}]","{64826405, 59582206}","['<p><strong>Use</strong> <code>tf.linalg.tensordot()</code>. <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot?version=stable"" rel=""noreferrer"">See the documentation</a></p>\n\n<p>As you have mentioned in the question that you are trying to find dot product. In this case <code>tf.matmul()</code> will not work, as it is only for cross product of metrices.</p>\n\n<p><strong>Demo code snippet</strong></p>\n\n<pre><code>import tensorflow as tf\n\nA = tf.constant([[1,4,6],[2,1,5],[3,2,4]])\nx = tf.constant([3,2,7])\nresult = tf.linalg.tensordot(tf.transpose(x), A, axes=1)\nresult = tf.linalg.tensordot(result, x, axes=1)\nprint(result)\n</code></pre>\n\n<p>And the result will be</p>\n\n<pre><code>&gt;&gt;&gt;tf.Tensor(532, shape=(), dtype=int32)\n</code></pre>\n\n<p><strong>Few points I want to mention here</strong></p>\n\n<ol>\n<li><p>Don\'t forget the <code>axes</code> argument inside <code>tf.linalg.tensordot()</code></p></li>\n<li><p>When you create <code>tf.zeros(5)</code> it will create a list of shape 5 and it will be like <code>[0,0,0,0,0]</code>, when you transpose this it will give you the same list. But if you create it like <code>tf.zeros((5,1))</code>, it would be a vector of shape <code>(5,1)</code> and the result will be </p>\n\n<pre><code>[\n[0],[0],[0],[0],[0]\n]\n</code></pre>\n\n<p>Now you can transpose this and the result will be different, but I recommend you do the code snippet I have mentioned. In case of dot product you don\'t have to bother much about this. </p></li>\n</ol>\n\n<p>If you are still facing issues, will be very happy to help you.', '<p>The operation that you want to do cannot be done (in an effective way) with <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>. There is, however, a dedicated function for that operation, <a href=""https://www.tensorflow.org/api_docs/python/tf/linalg/matvec"" rel=""nofollow noreferrer""><code>tf.linalg.matvec</code></a>, which will work with batches out of the box. And you can also do the same thing with <a href=""https://www.tensorflow.org/api_docs/python/tf/einsum"" rel=""nofollow noreferrer""><code>tf.einsum</code></a>, like <code>tf.einsum(\'bmn,bn-&gt;bm\', my_tensors, my_vectors)</code>.</p>\n<p>With respect to <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>, in general it computes an &quot;all vs all&quot; product of the two given tensors, but matching and reducing some axes. When no axes are given (you have to explicitly pass <code>axes=[[], []]</code> to do this), it creates a tensor with the dimensions of both inputs concatenated. So, if you have <code>my_tensors</code> with shape <code>(b, m, n)</code> and <code>my_vectors</code> with shape <code>(b, n)</code> and you do:</p>\n<pre class=""lang-py prettyprint-override""><code>res = tf.tensordot(my_tensors, my_vectors, axes=[[], []])\n</code></pre>\n<p>You get <code>res</code> with shape <code>(b, m, n, b, n)</code>, such that <code>res[p, q, r, s, t] == my_tensors[p, q, r] * my_vectors[s, t]</code>.</p>\n<p>The <code>axes</code> argument is used to specify dimensions in the input tensors that are &quot;matched&quot;. Values along matched axes are multiplied and summed (like a dot product), so those matched dimensions are reduced from the output. <code>axes</code> can take two different forms:</p>\n<ul>\n<li>If it is a single integer, <code>N</code> then the last <code>N</code> dimensions of the first parameter are matched against the first <code>N</code> dimensions of <code>b</code>. In your example, that corresponds to the dimensions with <code>n</code> elements in <code>my_tensor</code> and <code>my_vector</code>.</li>\n<li>If it is a list, it must contain two sublists, <code>axes_a</code> and <code>axes_b</code>, each with the same number <code>N</code> of integers. In this form, you are explicitly indicating which dimensions of the given values are matched. So, in your example, you could have passed <code>axes=[[1], [0]]</code>, which means &quot;match the dimension <code>1</code> of the first parameter (<code>my_tensor</code>) to the dimension <code>0</code> of the second parameter (<code>my_vector</code>)&quot;.</li>\n</ul>\n<p>If you have now <code>my_tensors</code>  with shape <code>(b, m, n)</code> and <code>my_vectors</code> with shape <code>(b, n)</code>, then you would want to match the dimension <code>2</code> of the first one to the dimension <code>1</code> of the second one, so you could pass <code>axes=[[2], [1]]</code>. However, that will give you a result <code>res</code> with shape <code>(b, m, b)</code> such that <code>res[i, :, j]</code> is the product of matrix <code>my_tensors[i]</code> and vector <code>my_vectors[j]</code>. You could take then only the results that you want (those where <code>i == j</code>), with something more or less convoluted like <code>tf.transpose(tf.linalg.diag_part(tf.transpose(res, [1, 0, 2])))</code>, but you would be doing far more computation than you need to get the same result.</p>\n', '<p>The operation that you want to do cannot be done (in an effective way) with <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>. There is, however, a dedicated function for that operation, <a href=""https://www.tensorflow.org/api_docs/python/tf/linalg/matvec"" rel=""nofollow noreferrer""><code>tf.linalg.matvec</code></a>, which will work with batches out of the box. And you can also do the same thing with <a href=""https://www.tensorflow.org/api_docs/python/tf/einsum"" rel=""nofollow noreferrer""><code>tf.einsum</code></a>, like <code>tf.einsum(\'bmn,bn-&gt;bm\', my_tensors, my_vectors)</code>.</p>\n<p>With respect to <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer""><code>tf.tensordot</code></a>, in general it computes an &quot;all vs all&quot; product of the two given tensors, but matching and reducing some axes. When no axes are given (you have to explicitly pass <code>axes=[[], []]</code> to do this), it creates a tensor with the dimensions of both inputs concatenated. So, if you have <code>my_tensors</code> with shape <code>(b, m, n)</code> and <code>my_vectors</code> with shape <code>(b, n)</code> and you do:</p>\n<pre class=""lang-py prettyprint-override""><code>res = tf.tensordot(my_tensors, my_vectors, axes=[[], []])\n</code></pre>\n<p>You get <code>res</code> with shape <code>(b, m, n, b, n)</code>, such that <code>res[p, q, r, s, t] == my_tensors[p, q, r] * my_vectors[s, t]</code>.</p>\n<p>The <code>axes</code> argument is used to specify dimensions in the input tensors that are &quot;matched&quot;. Values along matched axes are multiplied and summed (like a dot product), so those matched dimensions are reduced from the output.']","{'https://stackoverflow.com/questions/59582206/triple-tensor-product-with-tensorflow/59589204#59589204', 'https://stackoverflow.com/questions/64826405/tensorflow-axis-argument-in-dot-product/64826641#64826641'}",,0.18879004585780712,0.03906568558489928
5,70880589,tf.data.experimental.cardinality,Lack of Alternative Solutions/Documentation,what does cardinality mean in relation to an image dataset?,"<p>After successfully creating a tensorflow image <code>Dataset</code> with:</p>
<p><code>dataset = tf.keras.utils.image_dataset_from_directory(...)</code></p>
<p>which returns</p>
<p><em>Found 21397 files belonging to 5 classes.
Using 17118 files for training.</em></p>
<p>There is the cardinality method:</p>
<p><code>dataset.cardinality()</code></p>
<p>which returns a tensor containing the single value</p>
<p><em>tf.Tensor(535, shape=(), dtype=int64)</em></p>
<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality"" rel=""nofollow noreferrer"">docs here</a> but I don't understand what 535 represents or why its different to the number of files?</p>
<p>I ask, because I would like to understand how cardinality plays into this equation:</p>
<p><code>steps_per_epoch = dataset.cardinality().numpy() // batch_size</code></p>
","<p>The cardinality, in your case, is simply the rounded number of batches:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import pathlib

dataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=123,
  image_size=(180, 180),
  batch_size=batch_size)

print(train_ds.cardinality())
</code></pre>
<pre><code>Found 3670 files belonging to 5 classes.
Using 2936 files for training.
tf.Tensor(92, shape=(), dtype=int64)
</code></pre>
<p>The equation is: <code>2936/32 = cardinality</code>, so it depends on your batch size.</p>
","{71714912, 60997186, 50606178, 72936231, 66670862, 45124719, 70992022, 52144570, 61882270, 54678975}","[{'QuestionId': 72936231, 'AnswerId': 72937310, 'URL': 'https://stackoverflow.com/questions/72936231/behavior-of-dataset-map-in-tensorflow/72937310#72937310', 'QuestionTitle': 'Behavior of Dataset.map in Tensorflow', 'Answer': '<p>You should use <code>tf.shape</code> to get the dynamic shape of a tensor in <code>graph</code> mode:</p>\n<pre><code>token_length = tf.shape(tokens)[0]\n</code></pre>\n<p>And another problem you have is using a scalar tensor as the number of splits in <code>graph</code> mode. That won\'t work either.</p>\n<p>Try this:</p>\n<pre><code>import tensorflow as tf\n\ndef body(i, m, n):\n  n = n.write(n.size(), m[i:i+chunk_size])\n  return tf.add(i,chunk_size), m, n \n\ndef split_data(data, chunk_size):\n    length = tf.shape(data)[0]\n    x = data[:(length // chunk_size) * chunk_size]\n    ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n    i0 = tf.constant(0)\n    c = lambda i, m, n: tf.less(i, tf.shape(x)[0] - 1)\n    _, _, out = tf.while_loop(c, body, loop_vars=[i0, x, ta])\n    return out.stack()\n\nchunk_size = 4\n\ndataset = tf.data.Dataset.from_tensor_slices(\n    tf.ragged.constant([[1, 2, 3, 4, 5], [4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7, 8, 9]])).map(lambda x: split_data(x, 4)).flat_map(tf.data.Dataset.from_tensor_slices)\n\nfor item in dataset:\n  print(item)\n</code></pre>\n<pre><code>tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\ntf.Tensor([4 5 6 7], shape=(4,), dtype=int32)\ntf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\ntf.Tensor([5 6 7 8], shape=(4,), dtype=int32)\n</code></pre>\n<p>And see my other answer <a href=""https://stackoverflow.com/questions/70680271/split-examples-of-a-tensorflow-tf-data-dataset-in-graph-execution-mode"">here</a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1657536305}, {'QuestionId': 71714912, 'AnswerId': 71716342, 'URL': 'https://stackoverflow.com/questions/71714912/how-to-get-the-right-cardinality-of-a-dataset-for-lstm-tensorflow-network/71716342#71716342', 'QuestionTitle': 'how to get the right cardinality of a dataset for lstm tensorflow network?', 'Answer': ""<p>After this line:</p>\n<pre><code>X_train, y_train = X.iloc[0:107452], y.iloc[0:107452]\nX_train= np.asarray(X_train).astype('float32')\n\n</code></pre>\n<p>Try running:</p>\n<pre><code>y_train = tf.keras.utils.to_categorical(y_train, 3) # 3 classes\nX_train = tf.expand_dims(X_train, axis=-1)\n</code></pre>\n<p>And it should work. The same applies to your test data.</p>\n"", 'IsAccepted': True, 'CreationDate': 1648891629}, {'QuestionId': 70992022, 'AnswerId': 70992365, 'URL': 'https://stackoverflow.com/questions/70992022/how-to-get-the-correct-cardinality-of-a-tensorflow-dataset-after-filtering/70992365#70992365', 'QuestionTitle': 'How to get the correct cardinality of a Tensorflow dataset after filtering', 'Answer': '<p>Checking <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cardinality"" rel=""nofollow noreferrer"">the docs</a> of this method, there are special integer codes for infinite as well as unknown cardinalities. <a href=""https://www.tensorflow.org/api_docs/python/tf/data#UNKNOWN_CARDINALITY"" rel=""nofollow noreferrer"">Way at the bottom</a>, wee see that -2 codes for unknown cardinality. That is, the method was not able to infer the dataset size. Actually, <code>filter</code> is used as an example for a dataset with unknown cardinality.</p>\n<p>Why this is the case, I\'m not sure. Digging in the code, the implementation for <code>cardinality()</code> is <a href=""https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/data/ops/dataset_ops.py#L2696-L2722"" rel=""nofollow noreferrer"">here</a>. This leads to <code>gen_dataset_ops.dataset_cardinality</code>. However I cannot find <code>gen_dataset_ops</code> in the codebase. It might be a file that is automatically generated from somewhere else.</p>\n<p>I would assume that this method only performs a very rudimentary analysis (e.g. for a <code>range</code> Dataset it is very easy to say how many elements there are) without actually evaluating any of the dataset elements, and if this simple method cannot succeed (as it\'s not clear which elements will pass the filter without actually looking at the elements), it returns &quot;unknown&quot;.</p>\n', 'IsAccepted': True, 'CreationDate': 1644005677}, {'QuestionId': 45124719, 'AnswerId': 45949434, 'URL': 'https://stackoverflow.com/questions/45124719/memory-management-in-tensorflows-dataset-api/45949434#45949434', 'QuestionTitle': 'Memory management in Tensorflow&#39;s Dataset API', 'Answer': '<p>Yes. An example from official guide (Using the Dataset API for TensorFlow Input Pipelines, <a href=""https://www.tensorflow.org/programmers_guide/datasets"" rel=""nofollow noreferrer"">https://www.tensorflow.org/programmers_guide/datasets</a>)</p>\n<pre><code>filenames = [&quot;/var/data/file1.tfrecord&quot;, &quot;/var/data/file2.tfrecord&quot;]\ndataset = tf.contrib.data.TFRecordDataset(filenames)\ndataset = dataset.map(...) ## Parsing data with a user specified function\ndataset = dataset.shuffle(buffer_size=10000) ## 10000: size of sample/record pool for random selection\ndataset = dataset.repeat() ## None: keep repeating\ndataset = dataset.batch(32) ## 32: number of samples/records per batch (to be read into memory)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1504047233}, {'QuestionId': 66670862, 'AnswerId': 66671555, 'URL': 'https://stackoverflow.com/questions/66670862/what-does-the-dataset-dataset0num-images-code-does/66671555#66671555', 'QuestionTitle': 'What does the dataset = dataset[0:num_images, :, :] code does?', 'Answer': '<p>What you have in dataset is a tensor (or a multidimensional array). In your case it has 3 dimensions.</p>\n<pre class=""lang-py prettyprint-override""><code>dataset[0:num_images, :, :]\n</code></pre>\n<p>What you are doing in the first dimension is selecting a set of images from 0 to num_images, and then the \':\' are saying to keep the rest of information.</p>\n<p>So its just a tensor filtering on the first dimension while keeping the rest the same.</p>\n', 'IsAccepted': True, 'CreationDate': 1615978175}, {'QuestionId': 61882270, 'AnswerId': 61882378, 'URL': 'https://stackoverflow.com/questions/61882270/what-is-the-difference-between-a-tensor-and-a-dataset-in-tensorflow-2-0/61882378#61882378', 'QuestionTitle': 'What is the difference between a Tensor and a DataSet in TensorFlow 2.0?', 'Answer': '<p>You should read more of the documentation. The links are in your question, but for clarity, here it is for <a href=""https://www.tensorflow.org/guide/tensor"" rel=""nofollow noreferrer"">Tensors</a> and for <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">Dataset</a>.</p>\n\n<p>Tensors are n-dimensional, meaning they can have an arbitrary number of dimensions. They are not rectangular, which would be restricted to 2-dimensional.</p>\n\n<p>Datasets are an API for inputting data. If your data are in Dataset form, then you can use the various processing methods that the <code>tf.data</code> module provides, like parallel processing, shuffling, batching, and others. You can also match your features an labels in a single Dataset, as opposed to using two separate Tensor objects.</p>\n\n<p>Once you iterate over a Dataset, you get Tensor objects.</p>\n\n<p>Perhaps the distinction would also be clearer if you took a look at <a href=""https://www.tensorflow.org/datasets"" rel=""nofollow noreferrer"">TensorFlow Datasets</a>. That project distributes data in Dataset form.</p>\n', 'IsAccepted': False, 'CreationDate': 1589856252}, {'QuestionId': 60997186, 'AnswerId': 61677552, 'URL': 'https://stackoverflow.com/questions/60997186/understanding-how-to-use-tf-dataset-map/61677552#61677552', 'QuestionTitle': 'Understanding how to use tf.dataset.map()', 'Answer': '<p>In the below code, I am using <code>tf.data.Dataset.list_files</code> to read a file_path of a image. In the <code>map</code> function I am loading the image and doing the <code>crop_central</code>(basically crops the center part of the image for the given percentage, here I have specified the percentage by <code>np.random.uniform(0.50, 1.00)</code>). </p>\n\n<p>As you rightly mentioned, it is difficult to read the file as the the file path is of <code>tf.string</code> type and the <code>load_img</code> or any other function to read the image file would require simple <code>string</code> type.</p>\n\n<p>So here is how you can do it - </p>\n\n<ol>\n<li>You need to decorate your map function with <code>tf.py_function(load_file_and_process, [x], [tf.float32]).</code> You can find more about it <a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""noreferrer"">here</a>.</li>\n<li>You can retrieve the <code>string</code> from the <code>tf.string</code> using <code>bytes.decode(path.numpy()</code>.</li>\n</ol>\n\n<p>Below is the complete code for you reference. You can replace it with your image path while you run this code. </p>\n\n<pre><code>%tensorflow_version 2.x\nimport tensorflow as tf\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array, array_to_img\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef load_file_and_process(path):\n    image = load_img(bytes.decode(path.numpy()), target_size=(224, 224))\n    image = img_to_array(image)\n    image = tf.image.central_crop(image, np.random.uniform(0.50, 1.00))\n    return image\n\ntrain_dataset = tf.data.Dataset.list_files(\'/content/bird.jpg\')\ntrain_dataset = train_dataset.map(lambda x: tf.py_function(load_file_and_process, [x], [tf.float32]))\n\nfor f in train_dataset:\n  for l in f:\n    image = np.array(array_to_img(l))\n    plt.imshow(image)\n</code></pre>\n\n<p>Output - </p>\n\n<p><a href=""https://i.stack.imgur.com/4mRsr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4mRsr.png"" alt=""enter image description here""></a></p>\n\n<p>Hope this answers your question. Happy Learning.</p>\n', 'IsAccepted': False, 'CreationDate': 1588935401}, {'QuestionId': 54678975, 'AnswerId': 54679387, 'URL': 'https://stackoverflow.com/questions/54678975/tensorflow-dataset-api-explanation-of-behavior/54679387#54679387', 'QuestionTitle': 'Tensorflow Dataset API - explanation of behavior', 'Answer': '<ol>\n<li>Data in Dataset API is lazy loaded, so it depends on later operations. Now you load 1024 samples at time because of the size of shuffle buffer. It needs to fill the shuffle buffer. Data will be then loaded lazily, when you will be fetching values from the iterator.</li>\n<li>You repeat loaded data, because the repeating is after the map function. This is why it\'s advised to shuffle before parsing data, because it\'s more memory friendly.</li>\n<li>The shuffle loads some data (depending on size od shuffle buffer), and shuffles that data. </li>\n<li>Yes, you can repeat, shuffle and then map, it is even advised in the <a href=""https://www.tensorflow.org/guide/performance/datasets"" rel=""nofollow noreferrer"">performance guide</a>. And there is also function which merges <code>repeat</code> and <code>shuffle</code> together <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/shuffle_and_repeat"" rel=""nofollow noreferrer"">here</a>.</li>\n</ol>\n', 'IsAccepted': True, 'CreationDate': 1550091858}, {'QuestionId': 54678975, 'AnswerId': 54679253, 'URL': 'https://stackoverflow.com/questions/54678975/tensorflow-dataset-api-explanation-of-behavior/54679253#54679253', 'QuestionTitle': 'Tensorflow Dataset API - explanation of behavior', 'Answer': '<ol>\n<li>Here you\'re loading entire dataset. It\'s usually not a good idea to apply map prior to batch. Tensorflow has a hard limit 2GB on tensor size. num_parallel_calls means number of map functions applied in parallel. </li>\n<li><code>dataset.repeat()</code> without specified epoch value will repeat dataset indefinitely.</li>\n<li>Shuffle will randomly shuffle dataset with specified buffer value. In order to properly shuffle it\'s usually good to set this value to dataset length, and apply this function prior to batch.</li>\n<li><p><code>tf.data.TFRecordDataset</code>expects filenames as input. Generally, preferred order is</p>\n\n<pre><code>dataset = dataset.shuffle(shuffle_buffer).repeat()\ndataset = dataset.batch(batch_size)\ndataset = dataset.map(map_func)\n</code></pre></li>\n</ol>\n\n<p>Take a look at <a href=""https://www.tensorflow.org/guide/performance/datasets"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/performance/datasets</a></p>\n', 'IsAccepted': False, 'CreationDate': 1550091258}, {'QuestionId': 52144570, 'AnswerId': 52148700, 'URL': 'https://stackoverflow.com/questions/52144570/tensorflow-dataset-api/52148700#52148700', 'QuestionTitle': 'Tensorflow dataset api', 'Answer': '<p>With this example data:</p>\n\n<pre><code>review, rating\nBest film ever, 5\nrather meh, 2\n</code></pre>\n\n<p>You should be able to use tf.data.map() as explained <a href=""https://www.tensorflow.org/guide/datasets"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer"">here</a> in tensorflow 1.10:</p>\n\n<pre><code>def create_tokens(sentence):\n    return tf.string_split(sentence[\'review\'])\n\ndataset = tf.contrib.data.make_csv_dataset(\'test.csv\', batch_size=2)\ndataset = dataset.map(create_tokens)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1535973475}, {'QuestionId': 50606178, 'AnswerId': 50608469, 'URL': 'https://stackoverflow.com/questions/50606178/tensorflow-tf-data-dataset-and-bucketing/50608469#50608469', 'QuestionTitle': 'TensorFlow tf.data.Dataset and bucketing', 'Answer': '<p>Various <code>bucketing</code> use cases using <code>Dataset API</code> are explained well <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/kernel_tests/bucketing_test.py"" rel=""nofollow noreferrer"">here</a>.</p>\n\n<p><strong><code>bucket_by_sequence_length()</code> example:</strong></p>\n\n<pre><code>def elements_gen():\n   text = [[1, 2, 3], [3, 4, 5, 6, 7], [1, 2], [8, 9, 0, 2]]\n   label = [1, 2, 1, 2]\n   for x, y in zip(text, label):\n       yield (x, y)\n\ndef element_length_fn(x, y):\n   return tf.shape(x)[0]\n\ndataset = tf.data.Dataset.from_generator(generator=elements_gen,\n                                     output_shapes=([None],[]),\n                                     output_types=(tf.int32, tf.int32))\n\ndataset =   dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=element_length_fn,\n                                                              bucket_batch_sizes=[2, 2, 2],\n                                                              bucket_boundaries=[0, 8]))\n\nbatch = dataset.make_one_shot_iterator().get_next()\n\nwith tf.Session() as sess:\n\n   for _ in range(2):\n      print(\'Get_next:\')\n      print(sess.run(batch))\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>Get_next:\n(array([[1, 2, 3, 0, 0],\n   [3, 4, 5, 6, 7]], dtype=int32), array([1, 2], dtype=int32))\nGet_next:\n(array([[1, 2, 0, 0],\n   [8, 9, 0, 2]], dtype=int32), array([1, 2], dtype=int32))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1527694139}, {'QuestionId': 45124719, 'AnswerId': 45124940, 'URL': 'https://stackoverflow.com/questions/45124719/memory-management-in-tensorflows-dataset-api/45124940#45124940', 'QuestionTitle': 'Memory management in Tensorflow&#39;s Dataset API', 'Answer': '<p>If you will specify the number of records via <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset"" rel=""nofollow noreferrer"">batch_size</a>. In this case TF will grab only batch_size elements from the file. You can also specify <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset#shuffle"" rel=""nofollow noreferrer"">shuffle</a> and this will guarantee that all the time in the memory will be at maximum <code>buffer_size</code> elements.</p>\n\n<p>I verified it on my tfrecords files. I have 100 tfrecords files, each of them is ~10Gb (which is more than the memory on my laptop). And everything works fine.</p>\n', 'IsAccepted': False, 'CreationDate': 1500178834}]","{70880589, 70992022}","['<p>Checking <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cardinality"" rel=""nofollow noreferrer"">the docs</a> of this method, there are special integer codes for infinite as well as unknown cardinalities. <a href=""https://www.tensorflow.org/api_docs/python/tf/data#UNKNOWN_CARDINALITY"" rel=""nofollow noreferrer"">Way at the bottom</a>, wee see that -2 codes for unknown cardinality. That is, the method was not able to infer the dataset size. Actually, <code>filter</code> is used as an example for a dataset with unknown cardinality.</p>\n<p>Why this is the case, I\'m not sure. Digging in the code, the implementation for <code>cardinality()</code> is <a href=""https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/data/ops/dataset_ops.py#L2696-L2722"" rel=""nofollow noreferrer"">here</a>. This leads to <code>gen_dataset_ops.dataset_cardinality</code>. However I cannot find <code>gen_dataset_ops</code> in the codebase.', '<p>The cardinality, in your case, is simply the rounded number of batches:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\nimport pathlib\n\ndataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;\ndata_dir = tf.keras.utils.get_file(\'flower_photos\', origin=dataset_url, untar=True)\ndata_dir = pathlib.Path(data_dir)\n\nbatch_size = 32\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=&quot;training&quot;,\n  seed=123,\n  image_size=(180, 180),\n  batch_size=batch_size)\n\nprint(train_ds.cardinality())\n</code></pre>\n<pre><code>Found 3670 files belonging to 5 classes.\nUsing 2936 files for training.\ntf.Tensor(92, shape=(), dtype=int64)\n</code></pre>\n<p>The equation is: <code>2936/32 = cardinality</code>, so it depends on your batch size.</p>\n', '<p>Checking <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cardinality"" rel=""nofollow noreferrer"">the docs</a> of this method, there are special integer codes for infinite as well as unknown cardinalities. <a href=""https://www.tensorflow.org/api_docs/python/tf/data#UNKNOWN_CARDINALITY"" rel=""nofollow noreferrer"">Way at the bottom</a>, wee see that -2 codes for unknown cardinality. That is, the method was not able to infer the dataset size. Actually, <code>filter</code> is used as an example for a dataset with unknown cardinality.</p>\n<p>Why this is the case, I\'m not sure. Digging in the code, the implementation for <code>cardinality()</code> is <a href=""https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/data/ops/dataset_ops.py#L2696-L2722"" rel=""nofollow noreferrer"">here</a>. This leads to <code>gen_dataset_ops.dataset_cardinality</code>. However I cannot find <code>gen_dataset_ops</code> in the codebase. It might be a file that is automatically generated from somewhere else.</p>\n<p>I would assume that this method only performs a very rudimentary analysis (e.g. for a <code>range</code> Dataset it is very easy to say how many elements there are) without actually evaluating any of the dataset elements, and if this simple method cannot succeed (as it\'s not clear which elements will pass the filter without actually looking at the elements), it returns &quot;unknown&quot;.</p>\n']","{'https://stackoverflow.com/questions/70992022/how-to-get-the-correct-cardinality-of-a-tensorflow-dataset-after-filtering/70992365#70992365', 'https://stackoverflow.com/questions/70880589/what-does-cardinality-mean-in-relation-to-an-image-dataset/70880952#70880952'}",{70992022},0.21694975949115683,0.11881909200298586
5,59531864,tf.nn.conv1d,Documentation Replicability,Why does TensorFlow calculate 2D convolutions when 1D convolution is called?,"<p>In the documentation of tf.nn.conv1d, it is stated that</p>

<blockquote>
  <p>Internally, this op reshapes the input tensors and invokes tf.nn.conv2d. For example, if data_format does not start with ""NC"", a tensor of shape [batch, in_width, in_channels] is reshaped to [batch, 1, in_width, in_channels], and the filter is reshaped to [1, filter_width, in_channels, out_channels]. The result is then reshaped back to [batch, out_width, out_channels] (where out_width is a function of the stride and padding as in conv2d) and returned to the caller.</p>
</blockquote>

<p>I get that the operations are equivalent, but I am a bit confused about the implications of this implementation detail. </p>

<p>Does the reshaping create some computational overhead? 
The 3D convolution has its own implementation, so why not the 1D convolution?</p>

<p>Thanks for any explanation that helps me and others to understand this implementation detail of TensorFlow!</p>
","<p>Digging through the source code, I conclude that it's likely done for convenience and minimalism of implementation - details below. </p>

<p>First, there is no ""reshaping"", only expanding, squeezing, and re-ordering dims, which bears a tiny overhead; no array elements are actually being moved in memory - only the tensor object's indexing specifiers are changed.</p>

<p>Second, all <code>conv</code> ultimately route to <a href=""https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/nn_ops.py#L942"" rel=""nofollow noreferrer""><code>tf.nn_ops.convolution_internal</code></a>, which then routes to either <code>gen_nn_ops.conv2d</code> or <code>gen_nn_ops.conv3d</code>; a <code>conv1d</code> does not exist in <code>gen_nn_ops.py</code>. Note that for some reason, you won't find that file in the Git respository - but it should be in your local install, <code>/python/ops/gen_nn_ops.py</code>.</p>

<p>Lastly, to get a real answer on why there isn't a dedicated <code>conv1d</code> implementation, you'll need to ask the cuDNN developers behind the convolution algorithms found in <code>gen_nn_ops.py</code>; it's possible that they found no performance improvements, and that <code>conv2d</code> works just as fast. From a low-level standpoint, this makes sense, as the number of matrix multiplications in sliding a kernel with <code>N x 1</code> elements along an <code>M x 1</code> input is identical to that of <code>N</code> along <code>M</code> - again, the only difference is in indexing.</p>

<p>Unfortunately devs decided to encapsulate the ultimate call, that is to <code>_pywrap_tensorflow_internal.TFE_Py_FastPathExecute</code>; the module consists of a <code>.lib</code> and a <code>.pyd</code> file - basically, compiled C (Cython) code that requires disassembly for introspection. </p>

<hr>

<p>TL;DR (1) the ""reshaping"" has a trivial overhead; (2) lack of a dedicated <code>conv1d</code> implementation is likely per sparing redundancy as <code>conv2d</code> is just as fast; (3) I'm not a cuDNN expert, so if you need to be sure, better ask over at <a href=""https://developer.nvidia.com/cudnn"" rel=""nofollow noreferrer"">cuDNN</a>, or read their <a href=""https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html"" rel=""nofollow noreferrer"">SDK Documentation</a>. Alternatively, a dev at <a href=""https://github.com/tensorflow/tensorflow/issues"" rel=""nofollow noreferrer"">TF Github</a> may help. I haven't seen cuDNN devs answer on SO for years now, so posting here may not be the best bet.</p>

<hr>

<p><strong>Dim reordering performance demo</strong>:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
from time import time

x = np.random.randn(700, 800, 900) # 504,000,000 elements

t0 = time()
for i in range(1000):
    if i % 2 == 0:
        x = x.reshape(700, 900, 800)
    else:
        x = x.reshape(700, 800, 900)
print(time() - t0)
</code></pre>

<pre><code>0.0009968280792236328
</code></pre>
","{38114534, 61943686, 66114952, 51198890, 58753902, 42618543, 53033425, 54194233, 65790139, 42743199}","[{'QuestionId': 65790139, 'AnswerId': 74139047, 'URL': 'https://stackoverflow.com/questions/65790139/why-does-nn-conv1d-work-on-2d-feature-b-c-h-w/74139047#74139047', 'QuestionTitle': 'Why does nn.Conv1d work on 2d feature [b, c, h, w]?', 'Answer': '<p>The current accepted answer is incorrect, so I write this one.</p>\n<p>In the example the asker gives, the two convolutions are the same, up to random initialization of parameters.\nThis is because both use the same underlying implementation, and just pass different parameters such as kernel size. <code>nn.Conv1d</code>, <code>nn.Conv2d</code> and <code>nn.Conv3d</code> interpret their input differently, e.g. <code>kernel_size=3</code> will become <code>(3,3)</code> for <code>nn.Conv2d</code> but <code>(3,)</code> for <code>nn.Conv1d</code>.</p>\n<p>However, you can force these parameters to be the correct shape.\nNote that stride and dilation need to be specified in some of the instances below:</p>\n<pre class=""lang-py prettyprint-override""><code>import torch\nfrom torch import nn\n\nconv1d = nn.Conv1d(1, 1, 3, padding=\'same\', bias=False)\nconv2d = nn.Conv2d(1, 1, (3,), stride=(1,), dilation=(1,), padding=\'same\', bias=False)\nconv3d = nn.Conv3d(1, 1, (3,), stride=(1,), dilation=(1,), padding=\'same\', bias=False)\nconv1d.weight.data.fill_(1)\nconv2d.weight.data.fill_(1)\nconv3d.weight.data.fill_(1)\nx = torch.rand(1, 1, 100)\nassert (conv1d(x) == conv2d(x)).all() and (conv1d(x) == conv3d(x)).all()\n\nconv1d = nn.Conv1d(1, 1, (3,3), padding=\'same\', bias=False)\nconv2d = nn.Conv2d(1, 1, 3, padding=\'same\', bias=False)\nconv3d = nn.Conv3d(1, 1, (3,3), stride=(1,1), dilation=(1,1), padding=\'same\', bias=False)\nconv1d.weight.data.fill_(1)\nconv2d.weight.data.fill_(1)\nconv3d.weight.data.fill_(1)\nx = torch.rand(1, 1, 100, 100)\nassert (conv1d(x) == conv2d(x)).all() and (conv1d(x) == conv3d(x)).all()\n\nconv1d = nn.Conv1d(1, 1, (3,3,3), stride=(1,1,1), dilation=(1,1,1), padding=\'same\', bias=False)\nconv2d = nn.Conv2d(1, 1, (3,3,3), stride=(1,1,1), dilation=(1,1,1), padding=\'same\', bias=False)\nconv3d = nn.Conv3d(1, 1, 3, padding=\'same\', bias=False)\nconv1d.weight.data.fill_(1)\nconv2d.weight.data.fill_(1)\nconv3d.weight.data.fill_(1)\nx = torch.rand(1, 1, 100, 100, 100)\nassert (conv1d(x) == conv2d(x)).all() and (conv1d(x) == conv3d(x)).all()\n</code></pre>\n<p>This equality would not work if, as is stated in the currently accepted answer, nn.Conv1d could only &quot;move along one direction only&quot;, as both spatial dimensions are much larger than the kernel size. nn.Conv1d could not have generated the full 100x100 output if it were locked to move only in one direction.</p>\n<p>You can read more in <a href=""https://discuss.pytorch.org/t/conv1d-kernel-size-explained/84323/4"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/conv1d-kernel-size-explained/84323/4</a>, as pointed out by <a href=""https://stackoverflow.com/users/10935717/trialnerror"">@trialNerror</a> in a comment to the question.</p>\n', 'IsAccepted': False, 'CreationDate': 1666264924}, {'QuestionId': 61943686, 'AnswerId': 67610299, 'URL': 'https://stackoverflow.com/questions/61943686/why-are-input-channels-for-conv2d-limited-to-1-3-4-with-the-tensor-dimension-lim/67610299#67610299', 'QuestionTitle': 'Why are input channels for Conv2D limited to 1,3,4 with the tensor dimension limited to 4-D in tensorflow?', 'Answer': ""<p>I'm not sure about tensorflow, but in Pytorch, you can use something like .squeeze() to eliminate the last dimension, and the size will become [60,128,9,9], then it should be good.</p>\n"", 'IsAccepted': False, 'CreationDate': 1621455391}, {'QuestionId': 66114952, 'AnswerId': 66115178, 'URL': 'https://stackoverflow.com/questions/66114952/understanding-2d-convolution-output-size/66115178#66115178', 'QuestionTitle': 'Understanding 2D convolution output size', 'Answer': '<p>Well, it reminded me <a href=""https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"" rel=""nofollow noreferrer"">AlexNet paper</a> where there was a similar mistake. Your calculation is correct. I think they mistakenly write 256x256 instead of 224x224, in which case the calculation for the input layer is,</p>\n<p>(224-11+2*0)/4 + 1 = 54.25 ~ 54</p>\n<p>It\'s highly possible that authors mistakenly wrote 256x256 instead of the real architecture input size being 224x224 (that was the case in AlexNet also), or the other less possible option is they wrote 256x256 which was the real architecture input size, but do the calculations for 224x224. The latter is ignorable as I think it is a very silly mistake and I don\'t think that\'s even an option.</p>\n<p>Thus, I believe the true input size was 224x224 instead of 256x256.</p>\n', 'IsAccepted': False, 'CreationDate': 1612858350}, {'QuestionId': 65790139, 'AnswerId': 65794327, 'URL': 'https://stackoverflow.com/questions/65790139/why-does-nn-conv1d-work-on-2d-feature-b-c-h-w/65794327#65794327', 'QuestionTitle': 'Why does nn.Conv1d work on 2d feature [b, c, h, w]?', 'Answer': '<p><em>&quot;I want to know why conv1d works and what it mean by 2d kernel size in 1d convolution&quot;</em></p>\n<p>It doesn\'t have any reason not to work. Under the hood all this &quot;convolution&quot; means is &quot;Dot Product&quot;, now it could be between matrix and vector, matrix and matrix, vector and vector, etc. Simply put, <strong>the real distinction</strong> between 1D and 2D convolution <strong>is the freedom</strong> one has <strong>to move along the spatial dimension</strong> of input. This means If you look at <strong>1D convolution</strong>, It can move along one direction only, that is, the temporal dimension of the input (Note the kernel could be a vector, matrix whatever that doesn\'t matter). On the other hand, <strong>2D convolution has the freedom to move along 2 dimensions</strong> (height and width) of the input that is the spatial dimension. If it still seems confusing, have a look at the gifs below.</p>\n<h3>1D Convolution in action:</h3>\n<p><em>Note: It\'s a 1D convolution with kernel size <code>3x3</code>, look how it only moves down the input which is the temporal dimension.</em>\n<a href=""https://i.stack.imgur.com/DSDzc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DSDzc.png"" alt=""1d conv"" /></a></p>\n<h3>2D Connvolution in action:</h3>\n<p><em>Note: It\'s a 2D convolution with kernel size <code>3x3</code>, look how it moves along both width and height of the input which is the spatial dimension.</em>\n<a href=""https://i.stack.imgur.com/QrX7M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QrX7M.png"" alt=""2d conv"" /></a></p>\n<p>I think It\'s clear now what is the actual difference between 1D and 2D conv and why they both would produce different results for the same input.</p>\n', 'IsAccepted': True, 'CreationDate': 1611069430}, {'QuestionId': 58753902, 'AnswerId': 58754368, 'URL': 'https://stackoverflow.com/questions/58753902/different-2d-convolution-results-between-keras-and-scipy/58754368#58754368', 'QuestionTitle': 'Different 2D convolution results between keras and scipy', 'Answer': '<p>I don\'t know for certain without reading the source code for these two libraries, but there is more than one straightforward way to write a convolution algorithm, and evidently these two libraries implement it in different ways.</p>\n\n<p>One way is to ""paint"" the kernel onto the output, for each pixel of the image:</p>\n\n<pre class=""lang-py prettyprint-override""><code>from itertools import product\n\ndef convolve_paint(img, ker):\n    img_w, img_h = len(img[0]), len(img)\n    ker_w, ker_h = len(ker[0]), len(ker)\n    out_w, out_h = img_w + ker_w - 1, img_h + ker_h - 1\n    out = [[0]*out_w for i in range(out_h)]\n    for x,y in product(range(img_w), range(img_h)):\n        for dx,dy in product(range(ker_w), range(ker_h)):\n            out[y+dy][x+dx] += img[y][x] * ker[dy][dx]\n    return out\n</code></pre>\n\n<p>Another way is to ""sum"" the contributing amounts at each pixel in the output:</p>\n\n<pre class=""lang-py prettyprint-override""><code>def convolve_sum(img, ker):\n    img_w, img_h = len(img[0]), len(img)\n    ker_w, ker_h = len(ker[0]), len(ker)\n    out_w, out_h = img_w + ker_w - 1, img_h + ker_h - 1\n    out = [[0]*out_w for i in range(out_h)]\n    for x,y in product(range(out_w), range(out_h)):\n        for dx,dy in product(range(ker_w), range(ker_h)):\n            if 0 &lt;= y-dy &lt; img_h and 0 &lt;= x-dx &lt; img_w:\n                out[y][x] += img[y-dy][x-dx] * ker[dy][dx]\n    return out\n</code></pre>\n\n<p>These two functions produce the same output. However, notice that the second one has <code>y-dy</code> and <code>x-dx</code> instead of <code>y+dy</code> and <code>x+dx</code>. If the second algorithm is written with <code>+</code> instead of <code>-</code>, as might seem natural, then the results will be as if the kernel is rotated by 180 degrees, which is as you\'ve observed.</p>\n\n<p>It\'s unlikely that either library uses such a simple algorithm to do convolution. For larger images and kernels it\'s more efficient to use a Fourier transform, applying the <a href=""https://en.wikipedia.org/wiki/Convolution_theorem"" rel=""nofollow noreferrer"">convolution theorem</a>. But the difference between the two libraries is likely to be caused by something similar to this.</p>\n', 'IsAccepted': False, 'CreationDate': 1573148723}, {'QuestionId': 58753902, 'AnswerId': 58754039, 'URL': 'https://stackoverflow.com/questions/58753902/different-2d-convolution-results-between-keras-and-scipy/58754039#58754039', 'QuestionTitle': 'Different 2D convolution results between keras and scipy', 'Answer': '<p>What is usually called convolution in neural networks (and image processing) is not exactly the mathematical concept of <a href=""https://en.wikipedia.org/wiki/Convolution"" rel=""nofollow noreferrer"">convolution</a>, which is what <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html"" rel=""nofollow noreferrer""><code>convolve2d</code></a> implements, but the similar one of <a href=""https://en.wikipedia.org/wiki/Cross-correlation"" rel=""nofollow noreferrer"">correlation</a>, which is implemented by <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate2d.html"" rel=""nofollow noreferrer""><code>correlate2d</code></a>:</p>\n\n<pre><code>res_scipy = correlate2d(image, kernel.T, mode=\'same\')\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1573147361}, {'QuestionId': 38114534, 'AnswerId': 44091550, 'URL': 'https://stackoverflow.com/questions/38114534/basic-1d-convolution-in-tensorflow/44091550#44091550', 'QuestionTitle': 'Basic 1d convolution in tensorflow', 'Answer': '<p>In the new versions of TF (starting from 0.11) you have <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv1d"" rel=""nofollow noreferrer"">conv1d</a>, so there is no need to use 2d convolution to do 1d convolution. Here is a simple example of how to use conv1d:</p>\n\n<pre><code>import tensorflow as tf\ni = tf.constant([1, 0, 2, 3, 0, 1, 1], dtype=tf.float32, name=\'i\')\nk = tf.constant([2, 1, 3], dtype=tf.float32, name=\'k\')\n\ndata   = tf.reshape(i, [1, int(i.shape[0]), 1], name=\'data\')\nkernel = tf.reshape(k, [int(k.shape[0]), 1, 1], name=\'kernel\')\n\nres = tf.squeeze(tf.nn.conv1d(data, kernel, stride=1, padding=\'VALID\'))\nwith tf.Session() as sess:\n    print sess.run(res)\n</code></pre>\n\n<p>To understand how conv1d is calculates, take a look at <a href=""http://www.riptutorial.com/tensorflow/example/30750/math-behind-1d-convolution-with-advanced-examples-in-tf"" rel=""nofollow noreferrer"">various examples</a></p>\n', 'IsAccepted': False, 'CreationDate': 1495320995}, {'QuestionId': 54194233, 'AnswerId': 54277252, 'URL': 'https://stackoverflow.com/questions/54194233/convert-a-2d-convolution-into-a-1d-convolution-affine-transformation-in-tensor/54277252#54277252', 'QuestionTitle': 'Convert a 2D Convolution into a 1D Convolution + Affine Transformation in TensorFlow?', 'Answer': '<p>You may be able to use the <a href=""https://en.wikipedia.org/wiki/Multidimensional_discrete_convolution#The_Helix_Transform"" rel=""nofollow noreferrer"">Helix Transform</a> in order to create 1D convolution which is equivalence of multi dimensional convolution.</p>\n', 'IsAccepted': True, 'CreationDate': 1547993147}, {'QuestionId': 53033425, 'AnswerId': 53034370, 'URL': 'https://stackoverflow.com/questions/53033425/how-to-accomplish-2d-convolution-using-1d-convolution/53034370#53034370', 'QuestionTitle': 'How to accomplish 2D convolution using 1D convolution?', 'Answer': '<p>The reason that your 1D convolutions combine to give you the same results as the 2D convolution is that your filter is <a href=""https://en.wikipedia.org/wiki/Separable_filter"" rel=""nofollow noreferrer"">separable</a>. Steve Eddins discussed separable convolutions on his MATLAB blog <a href=""https://blogs.mathworks.com/steve/2006/10/04/separable-convolution/"" rel=""nofollow noreferrer"">here</a>.</p>\n\n<p>Your filter is separable because:</p>\n\n<pre><code>[1;1;1] * [2,0,1] = \n\n   2   0   1\n   2   0   1\n   2   0   1\n</code></pre>\n\n<p>But in general, not all 2D filters are separable and only those that are can be turned into separate 1D convolutions.</p>\n', 'IsAccepted': True, 'CreationDate': 1540748424}, {'QuestionId': 53033425, 'AnswerId': 53033594, 'URL': 'https://stackoverflow.com/questions/53033425/how-to-accomplish-2d-convolution-using-1d-convolution/53033594#53033594', 'QuestionTitle': 'How to accomplish 2D convolution using 1D convolution?', 'Answer': '<p>I suppose because your matrix <code>v</code> is just the same row 3 times. Therefore you can simply use a 2nd convolution with <code>[1 1 1]</code> over the columns  </p>\n', 'IsAccepted': False, 'CreationDate': 1540743681}, {'QuestionId': 42743199, 'AnswerId': 52771066, 'URL': 'https://stackoverflow.com/questions/42743199/tensorflow-convolution-of-2d-array/52771066#52771066', 'QuestionTitle': 'TensorFlow convolution of 2D array', 'Answer': ""<p>In addition to kaufmanu's answer, you can use <code>tf.squeeze</code> if you need a 2D result. I.e.,</p>\n\n<pre><code>c=tf.squeeze(tf.nn.conv2d(a_tensor, k_weight,padding='VALID',strides=[1, 1, 1, 1]))\n</code></pre>\n\n<p>to perform the convolution.</p>\n"", 'IsAccepted': False, 'CreationDate': 1539308553}, {'QuestionId': 51198890, 'AnswerId': 51199942, 'URL': 'https://stackoverflow.com/questions/51198890/convolution-2d-vs-1d-2-pass-giving-different-results/51199942#51199942', 'QuestionTitle': 'convolution: 2d vs 1d 2-pass giving different results', 'Answer': ""<p>The issue is that you convolve twice in the same direction, rather than convolving once along each image axis:</p>\n\n<pre><code>result2 = signal.convolve(signal.convolve(image, kernel, mode='same'), kernel.T, mode='same')\n#                                                                      ^^^^^^^^\n</code></pre>\n\n<p>This gives me an average absolute difference (per pixel) with <code>result1</code> in the order of <code>1e-15</code>.</p>\n"", 'IsAccepted': True, 'CreationDate': 1530824942}, {'QuestionId': 38114534, 'AnswerId': 38117279, 'URL': 'https://stackoverflow.com/questions/38114534/basic-1d-convolution-in-tensorflow/38117279#38117279', 'QuestionTitle': 'Basic 1d convolution in tensorflow', 'Answer': '<p>I am sorry to say that, but your first code was almost right. You just inverted <code>x</code> and <code>phi</code> in <code>tf.nn.conv2d</code>:</p>\n\n<pre class=""lang-py prettyprint-override""><code>g = tf.Graph()\nwith g.as_default():\n    # data shape is ""[batch, in_height, in_width, in_channels]"",\n    x = tf.Variable(np.array([0.0, 0.0, 0.0, 0.0, 1.0]).reshape(1, 1, 5, 1), name=""x"")\n    # filter shape is ""[filter_height, filter_width, in_channels, out_channels]""\n    phi = tf.Variable(np.array([0.0, 0.5, 1.0]).reshape(1, 3, 1, 1), name=""phi"")\n    conv = tf.nn.conv2d(\n        x,\n        phi,\n        strides=[1, 1, 1, 1],\n        padding=""SAME"",\n        name=""conv"")\n</code></pre>\n\n<hr>\n\n<p><strong>Update:</strong> TensorFlow now supports 1D convolution since version r0.11, using <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv1d"" rel=""noreferrer""><code>tf.nn.conv1d</code></a>. I previously made a guide to use them in the stackoverflow documentation (now extinct) that I\'m pasting here:</p>\n\n<hr>\n\n<h2>Guide to 1D convolution</h2>\n\n<p>Consider a basic example with an input of length <code>10</code>, and dimension <code>16</code>. The batch size is <code>32</code>. We therefore have a placeholder with input shape <code>[batch_size, 10, 16]</code>.</p>\n\n<pre class=""lang-py prettyprint-override""><code>batch_size = 32\nx = tf.placeholder(tf.float32, [batch_size, 10, 16])\n</code></pre>\n\n<p>We then create a filter with width 3, and we take <code>16</code> channels as input, and output also <code>16</code> channels.</p>\n\n<pre class=""lang-py prettyprint-override""><code>filter = tf.zeros([3, 16, 16])  # these should be real values, not 0\n</code></pre>\n\n<hr>\n\n<p>Finally we apply <code>tf.nn.conv1d</code> with a stride and a padding:\n- <strong>stride</strong>: integer <code>s</code>\n- <strong>padding</strong>: this works like in 2D, you can choose between <code>SAME</code> and <code>VALID</code>. <code>SAME</code> will output the same input length, while <code>VALID</code> will not add zero padding.</p>\n\n<p>For our example we take a stride of 2, and a valid padding.\n</p>\n\n<pre><code>output = tf.nn.conv1d(x, filter, stride=2, padding=""VALID"")\n</code></pre>\n\n<p>The output shape should be <code>[batch_size, 4, 16]</code>.<br>\nWith <code>padding=""SAME""</code>, we would have had an output shape of <code>[batch_size, 5, 16]</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1467273751}, {'QuestionId': 42743199, 'AnswerId': 42746143, 'URL': 'https://stackoverflow.com/questions/42743199/tensorflow-convolution-of-2d-array/42746143#42746143', 'QuestionTitle': 'TensorFlow convolution of 2D array', 'Answer': '<p>The answers posted so far all miss one important point: Tensorflow does <em>not</em> compute a convolution, but a cross-correlation as is stated in the <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/convolution"" rel=""nofollow noreferrer"">doc</a>:</p>\n\n<blockquote>\n  <p>Note that although these ops are called ""convolution"", they are\n  strictly speaking ""cross-correlation"" since the filter is combined\n  with an input window without reversing the filter.</p>\n</blockquote>\n\n<p>If you really want to compute a convolution, you will have to reverse the kernel before passing it into <code>conv2d</code>, i.e. flip it once on the horizontal then on the vertical axis. Using Miriam\'s answer, this could look like this:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\na = np.random.randint(2, size=(10,10))\nk = np.array([[1,1,1],[1,1,1],[1,1,1]],dtype=np.float32)\nflip = [slice(None, None, -1), slice(None, None, -1)]\nk = k[flip]\n\na=a.astype(np.float32)\na_tensor = tf.reshape(a, [1, 10, 10, 1])\nk_weight = tf.reshape(np.array(k), [3,3,1,1])\nc=tf.nn.conv2d(a_tensor, k_weight,padding=\'VALID\',strides=[1, 1, 1, 1])\nsess=tf.Session()\nc.eval(session=sess)\n</code></pre>\n\n<p>Note that in this specific example flipping the kernel is technically in vain, because for symmetric kernels convolution and cross-correlation is the same thing. However, as soon as you have a non-symmetric kernels, you must flip it if you want Tensorflow to actually compute a convolution.</p>\n', 'IsAccepted': False, 'CreationDate': 1489313630}, {'QuestionId': 42743199, 'AnswerId': 42743379, 'URL': 'https://stackoverflow.com/questions/42743199/tensorflow-convolution-of-2d-array/42743379#42743379', 'QuestionTitle': 'TensorFlow convolution of 2D array', 'Answer': '<pre><code>import tensorflow as tf\nimport numpy as np\n\na = np.random.randint(2, size=(10,10))\nk = [[1,1,1],[1,1,1],[1,1,1]]\n\ntensor_a = tf.constant(a, tf.float32)\ntensor_k = tf.constant(k, tf.float32)\n\ntensor_res = tf.nn.convolution(tf.reshape(tensor_a, [1, 10, 10, 1]), tf.reshape(tensor_k, [3, 3, 1, 1]), padding=\'VALID\')\n\nsess = tf.Session()\nprint(sess.run(tensor_res))\n</code></pre>\n\n<p>The Computational Graph tutorial is <a href=""https://www.tensorflow.org/get_started/get_started"" rel=""nofollow noreferrer"">here</a></p>\n\n<p>Convolution helper <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/convolution"" rel=""nofollow noreferrer"">doc</a></p>\n', 'IsAccepted': False, 'CreationDate': 1489289178}, {'QuestionId': 42618543, 'AnswerId': 42624588, 'URL': 'https://stackoverflow.com/questions/42618543/performing-1d-convolution-using-2d-kernel-in-keras/42624588#42624588', 'QuestionTitle': 'Performing 1d convolution using 2d kernel in keras', 'Answer': ""<p>Assuming that your image <code>shape=(dim_x, dim_y, img_channels)</code> you can obtain a <code>1D</code> convolution by setting:</p>\n\n<pre><code>conv1d_on_image = Convolution2D(output_channels, 1, dim_y, border_mode='valid')(input)\n</code></pre>\n\n<p>Remember that the output from this layer would have shape <code>(dim_x, 1, output_channels)</code>. If you want your input to be sequential you may use the <code>Reshape</code> layer by setting:</p>\n\n<pre><code>conv1d_on_image = Reshape((dim_x, output_channels))(conv1d_on_image)\n</code></pre>\n\n<p>This would produce output with shape <code>(dim_x, output_channels)</code>.</p>\n\n<p>An interesting fact is that this is exactly the way how <code>Conv1D</code> works in <code>Keras</code> with <code>tf</code> backend.</p>\n"", 'IsAccepted': True, 'CreationDate': 1488800067}, {'QuestionId': 38114534, 'AnswerId': 38857776, 'URL': 'https://stackoverflow.com/questions/38114534/basic-1d-convolution-in-tensorflow/38857776#38857776', 'QuestionTitle': 'Basic 1d convolution in tensorflow', 'Answer': '<p>I think I got it to work with the requirements that I needed. The comments/details of how it works are on the code:</p>\n\n<pre><code>import numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntask_name = \'task_MNIST_flat_auto_encoder\'\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\nX_train, Y_train = mnist.train.images, mnist.train.labels # N x D\nX_cv, Y_cv = mnist.validation.images, mnist.validation.labels\nX_test, Y_test = mnist.test.images, mnist.test.labels\n\n# data shape is ""[batch, in_height, in_width, in_channels]"",\n# X_train = N x D\nN, D = X_train.shape\n# think of it as N images with height 1 and width D.\nX_train = X_train.reshape(N,1,D,1)\nx = tf.placeholder(tf.float32, shape=[None,1,D,1], name=\'x-input\')\n#x = tf.Variable( X_train , name=\'x-input\')\n# filter shape is ""[filter_height, filter_width, in_channels, out_channels]""\nfilter_size, nb_filters = 10, 12 # filter_size , number of hidden units/units\n# think of it as having nb_filters number of filters, each of size filter_size\nW = tf.Variable( tf.truncated_normal(shape=[1, filter_size, 1,nb_filters], stddev=0.1) )\nstride_convd1 = 2 # controls the stride for 1D convolution\nconv = tf.nn.conv2d(input=x, filter=W, strides=[1, 1, stride_convd1, 1], padding=""SAME"", name=""conv"")\n\nwith tf.Session() as sess:\n    sess.run( tf.initialize_all_variables() )\n    sess.run(fetches=conv, feed_dict={x:X_train})\n</code></pre>\n\n<p>thanks to Olivier for the help (see the discussion in his comments for further clarification). </p>\n\n<hr>\n\n<p>Manually check it:</p>\n\n<pre><code>X_train_org = np.array([[0,1,2,3]])\nN, D = X_train_org.shape\nX_train_1d = X_train_org.reshape(N,1,D,1)\n#X_train = tf.constant( X_train_org )\n# think of it as N images with height 1 and width D.\nxx = tf.placeholder(tf.float32, shape=[None,1,D,1], name=\'xx-input\')\n#x = tf.Variable( X_train , name=\'x-input\')\n# filter shape is ""[filter_height, filter_width, in_channels, out_channels]""\nfilter_size, nb_filters = 2, 2 # filter_size , number of hidden units/units\n# think of it as having nb_filters number of filters, each of size filter_size\nfilter_w = np.array([[1,3],[2,4]]).reshape(1,filter_size,1,nb_filters)\n#W = tf.Variable( tf.truncated_normal(shape=[1,filter_size,1,nb_filters], stddev=0.1) )\nW = tf.Variable( tf.constant(filter_w, dtype=tf.float32) )\nstride_convd1 = 2 # controls the stride for 1D convolution\nconv = tf.nn.conv2d(input=xx, filter=W, strides=[1, 1, stride_convd1, 1], padding=""SAME"", name=""conv"")\n\n#C = tf.constant( (np.array([[4,3,2,1]]).T).reshape(1,1,1,4) , dtype=tf.float32 ) #\n#tf.reshape( conv , [])\n#y_tf = tf.matmul(conv, C)\n\n\n##\nx = tf.placeholder(tf.float32, shape=[None,D], name=\'x-input\') # N x 4\nW1 = tf.Variable( tf.constant( np.array([[1,2,0,0],[3,4,0,0]]).T, dtype=tf.float32 ) ) # 2 x 4\ny1 = tf.matmul(x,W1) # N x 2 = N x 4 x 4 x 2\nW2 = tf.Variable( tf.constant( np.array([[0,0,1,2],[0,0,3,4]]).T, dtype=tf.float32 ))\ny2 = tf.matmul(x,W2) # N x 2 = N x 4 x 4 x 2\nC1 = tf.constant( np.array([[4,3]]).T, dtype=tf.float32 ) # 1 x 2\nC2 = tf.constant( np.array([[2,1]]).T, dtype=tf.float32 )\n\np1 = tf.matmul(y1,C1)\np2 = tf.matmul(y2,C2)\ny = p1 + p2\nwith tf.Session() as sess:\n    sess.run( tf.initialize_all_variables() )\n    print \'manual conv\'\n    print sess.run(fetches=y1, feed_dict={x:X_train_org})\n    print sess.run(fetches=y2, feed_dict={x:X_train_org})\n    #print sess.run(fetches=y, feed_dict={x:X_train_org})\n    print \'tf conv\'\n    print sess.run(fetches=conv, feed_dict={xx:X_train_1d})\n    #print sess.run(fetches=y_tf, feed_dict={xx:X_train_1d})\n</code></pre>\n\n<p>outputs:</p>\n\n<pre><code>manual conv\n[[ 2.  4.]]\n[[  8.  18.]]\ntf conv\n[[[[  2.   4.]\n   [  8.  18.]]]]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1470766517}]","{59531864, 41853767}","['<p>Digging through the source code, I conclude that it\'s likely done for convenience and minimalism of implementation - details below. </p>\n\n<p>First, there is no ""reshaping"", only expanding, squeezing, and re-ordering dims, which bears a tiny overhead; no array elements are actually being moved in memory - only the tensor object\'s indexing specifiers are changed.</p>\n\n<p>Second, all <code>conv</code> ultimately route to <a href=""https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/nn_ops.py#L942"" rel=""nofollow noreferrer""><code>tf.nn_ops.convolution_internal</code></a>, which then routes to either <code>gen_nn_ops.conv2d</code> or <code>gen_nn_ops.conv3d</code>; a <code>conv1d</code> does not exist in <code>gen_nn_ops.py</code>. Note that for some reason, you won\'t find that file in the Git respository - but it should be in your local install, <code>/python/ops/gen_nn_ops.py</code>.</p>\n\n<p>Lastly, to get a real answer on why there isn\'t a dedicated <code>conv1d</code> implementation, you\'ll need to ask the cuDNN developers behind the convolution algorithms found in <code>gen_nn_ops.py</code>; it\'s possible that they found no performance improvements, and that <code>conv2d</code> works just as fast. From a low-level standpoint, this makes sense, as the number of matrix multiplications in sliding a kernel with <code>N x 1</code> elements along an <code>M x 1</code> input is identical to that of <code>N</code> along <code>M</code> - again, the only difference is in indexing.</p>\n\n<p>Unfortunately devs decided to encapsulate the ultimate call, that is to <code>_pywrap_tensorflow_internal.TFE_Py_FastPathExecute</code>; the module consists of a <code>.lib</code> and a <code>.pyd</code> file - basically, compiled C (Cython) code that requires disassembly for introspection. </p>\n\n<hr>\n\n<p>TL;DR (1) the ""reshaping"" has a trivial overhead; (2) lack of a dedicated <code>conv1d</code> implementation is likely per sparing redundancy as <code>conv2d</code> is just as fast; (3) I\'m not a cuDNN expert, so if you need to be sure, better ask over at <a href=""https://developer.nvidia.com/cudnn"" rel=""nofollow noreferrer"">cuDNN</a>, or read their <a href=""https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html"" rel=""nofollow noreferrer"">SDK Documentation</a>. Alternatively, a dev at <a href=""https://github.com/tensorflow/tensorflow/issues"" rel=""nofollow noreferrer"">TF Github</a> may help.', '<p>The problem that you see is because TF does not really calculate the convolution. If you will take a look at the explanation of what <a href=""https://en.wikipedia.org/wiki/Convolution"" rel=""noreferrer"">convolution actually does</a> (check for <strong>Visual explanations of convolution</strong>), you will see that the second function is flipped:</p>\n\n<ol>\n<li>Express each function in terms of a dummy variable </li>\n<li>Reflect one of the functions <strong>(this is the flip)</strong></li>\n<li>..... Some other stuff which I will not copy here.</li>\n</ol>\n\n<p>TF does everything except of that flip.', '<p>Digging through the source code, I conclude that it\'s likely done for convenience and minimalism of implementation - details below. </p>\n\n<p>First, there is no ""reshaping"", only expanding, squeezing, and re-ordering dims, which bears a tiny overhead; no array elements are actually being moved in memory - only the tensor object\'s indexing specifiers are changed.</p>\n\n<p>Second, all <code>conv</code> ultimately route to <a href=""https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/ops/nn_ops.py#L942"" rel=""nofollow noreferrer""><code>tf.nn_ops.convolution_internal</code></a>, which then routes to either <code>gen_nn_ops.conv2d</code> or <code>gen_nn_ops.conv3d</code>; a <code>conv1d</code> does not exist in <code>gen_nn_ops.py</code>. Note that for some reason, you won\'t find that file in the Git respository - but it should be in your local install, <code>/python/ops/gen_nn_ops.py</code>.</p>\n\n<p>Lastly, to get a real answer on why there isn\'t a dedicated <code>conv1d</code> implementation, you\'ll need to ask the cuDNN developers behind the convolution algorithms found in <code>gen_nn_ops.py</code>; it\'s possible that they found no performance improvements, and that <code>conv2d</code> works just as fast. From a low-level standpoint, this makes sense, as the number of matrix multiplications in sliding a kernel with <code>N x 1</code> elements along an <code>M x 1</code> input is identical to that of <code>N</code> along <code>M</code> - again, the only difference is in indexing.</p>\n\n<p>Unfortunately devs decided to encapsulate the ultimate call, that is to <code>_pywrap_tensorflow_internal.TFE_Py_FastPathExecute</code>; the module consists of a <code>.lib</code> and a <code>.pyd</code> file - basically, compiled C (Cython) code that requires disassembly for introspection. </p>\n\n<hr>\n\n<p>TL;DR (1) the ""reshaping"" has a trivial overhead; (2) lack of a dedicated <code>conv1d</code> implementation is likely per sparing redundancy as <code>conv2d</code> is just as fast; (3) I\'m not a cuDNN expert, so if you need to be sure, better ask over at <a href=""https://developer.nvidia.com/cudnn"" rel=""nofollow noreferrer"">cuDNN</a>, or read their <a href=""https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html"" rel=""nofollow noreferrer"">SDK Documentation</a>. Alternatively, a dev at <a href=""https://github.com/tensorflow/tensorflow/issues"" rel=""nofollow noreferrer"">TF Github</a> may help. I haven\'t seen cuDNN devs answer on SO for years now, so posting here may not be the best bet.</p>\n\n<hr>\n\n<p><strong>Dim reordering performance demo</strong>:</p>\n\n<pre class=""lang-py prettyprint-override""><code>import numpy as np\nfrom time import time\n\nx = np.random.randn(700, 800, 900) # 504,000,000 elements\n\nt0 = time()\nfor i in range(1000):\n    if i % 2 == 0:\n        x = x.reshape(700, 900, 800)\n    else:\n        x = x.reshape(700, 800, 900)\nprint(time() - t0)\n</code></pre>\n\n<pre><code>0.0009968280792236328\n</code></pre>\n']","{'https://stackoverflow.com/questions/41853767/difference-between-tensorflow-convolution-and-numpy-convolution/44093772#44093772', 'https://stackoverflow.com/questions/59531864/why-does-tensorflow-calculate-2d-convolutions-when-1d-convolution-is-called/59544249#59544249'}",,0.2211657065950711,0.09569078954661932
5,58631390,tf.compat,Documentation Completeness,What is the purpose of tf.compat?,"<p>What's the purpose of tf.compat module? It looks like just the entire Tensorflow API is replicated inside this module.
The documentation states</p>

<blockquote>
  <p>Functions for Python 2 vs. 3 compatibility.</p>
</blockquote>

<p>So why there is a ""v1"" and a ""v2"" submodule? What are the compatibility problems address by tf.compat specifically?</p>
","<p><a href=""https://www.tensorflow.org/api_docs/python/tf/compat"" rel=""noreferrer""><code>tf.compat</code></a> allows you to write code that works both in TensorFlow 1.x and 2.x. For example, the following piece of code:</p>

<pre><code>import tensorflow as tf

tf.compat.v1.disable_v2_behavior()
with tf.compat.v1.Session() as sess:
    x = tf.compat.v1.placeholder(tf.float32, [2])
    x2 = tf.square(x)
    print(sess.run(x2, feed_dict={x: [2, 3]}))
    # [4. 9.]
</code></pre>

<p>Runs the same on TensorFlow 1.15.0 and 2.0.0, even though session and placeholders were deprecated in 2.x. Likewise, <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v2"" rel=""noreferrer""><code>tf.compat.v2</code></a> allows you to use things introduced in 2.x from 1.x. Also, these APIs provide also backwards compatibility for the future too, so if at some point a 3.x version is released, the mechanism to write version-independent code will already be there since the first version of 2.x.</p>

<p>EDIT: The documentation for the module about Python should actually be changed. Originally, <code>tf.compat</code> only held functions for that purpose (and it was like that until 1.13, <a href=""https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/compat"" rel=""noreferrer"">see all module documentation</a>). However, it was later repurposed for TensorFlow version compatibility.</p>
","{33932901, 58897927, 40404519, 42256938, 37689802, 61628845, 54894799, 41505746, 39543348, 38060825}","[{'QuestionId': 54894799, 'AnswerId': 54897167, 'URL': 'https://stackoverflow.com/questions/54894799/why-should-i-use-tf-data/54897167#54897167', 'QuestionTitle': 'Why should I use tf.data?', 'Answer': '<ul>\n<li>The <code>tf.data</code> module has specific tools which help in building a input pipeline for your ML model. A input pipeline takes in the raw data, processes it and then feeds it to the model.</li>\n</ul>\n<blockquote>\n<p>When should I use <code>tf.data</code> module?</p>\n</blockquote>\n<p>The <code>tf.data</code> module is useful when you have a large dataset in the form of a file such as .csv or .tfrecord. <code>tf.data.Dataset</code> can perform shuffling and batching of samples efficiently. Useful for large datasets as well as small datasets. It could combine train and test datasets.</p>\n<blockquote>\n<p>How can I create batches and iterate through them for training?</p>\n</blockquote>\n<p>I think you can efficiently do this with NumPy and <code>np.reshape</code> method. Pandas can read data files for you. Then, you just need a <code>for ... in ...</code> loop to get each batch amd pass it to your model.</p>\n<blockquote>\n<p>How can I feed NumPy data to a TensorFlow model?</p>\n</blockquote>\n<p>There are two options to use <code>tf.placeholder()</code> or <code>tf.data.Dataset</code>.</p>\n<ol>\n<li>The <code>tf.data.Dataset</code> is a much easier implementation. I recommend to use it. Also, has some good set of methods.</li>\n<li>The <code>tf.placeholder</code> creates a placeholder tensor which feeds the data to a TensorFlow graph. This process would consume more time feeding in the data.</li>\n</ol>\n', 'IsAccepted': True, 'CreationDate': 1551234885}, {'QuestionId': 33932901, 'AnswerId': 52421144, 'URL': 'https://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/52421144#52421144', 'QuestionTitle': 'What&#39;s the purpose of tf.app.flags in TensorFlow?', 'Answer': '<p>Short Answer:</p>\n<p>At Google, they use flag systems to set default values for arguments. It\'s similar to argparse. They use their own flag system instead of argparse or sys.argv.</p>\n<p>Source: I worked there before.</p>\n<p>Long Answer:</p>\n<p>For the arguments you have in that example, they are called <a href=""https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)"" rel=""nofollow noreferrer"">hyperparameters</a>. In neural network there are multiple parameters you can optimize in order to get a a desired results. For example, for <a href=""https://keras.io/api/optimizers/#optimizers"" rel=""nofollow noreferrer"">batch_size</a>, it\'s the number of data vector (This can be image, text, or raw data points) that can be passed in a single shot to the <a href=""https://keras.io/api/optimizers/#optimizers"" rel=""nofollow noreferrer"">optimizer</a>.</p>\n<p>You can Google the name of the argument, and see what\'s the purpose of it of it. If you want to learn about Deep Learning, I recommend you take Andrew Ng course.</p>\n', 'IsAccepted': False, 'CreationDate': 1537433550}, {'QuestionId': 61628845, 'AnswerId': 61630018, 'URL': 'https://stackoverflow.com/questions/61628845/alternative-to-tf-compat-v1/61630018#61630018', 'QuestionTitle': 'Alternative to tf.compat.v1', 'Answer': '<p>For converting from TF1 to TF2  code, you can do it easily by using the tf.keras api in TF2. For example the functions your provided.</p>\n\n<p><code>tf.losses.softmax_cross_entropy()</code> to\n<code>tf.keras.losses.CategoricalCrossentropy(from_logits=True)</code>\n<code>from_logits</code> parameter here specifies whether y_pred is expected to be a logits tensor. </p>\n\n<p><code>tf.train.MomentumOptimizer()</code> to <code>tf.keras.optimizers.SGD(momentum=...)</code>\nby providing the <code>momentum</code> parameter.</p>\n\n<p>As for the final function its still being used in the TF2.0 documentation so maybe there isn\'t an equivalent version in 2.0. </p>\n\n<p>You can check the this guide from the TF2.0 website whic would provide a great refrence for you. <a href=""https://www.tensorflow.org/guide/migrate"" rel=""nofollow noreferrer"">Migrate your TensorFlow 1 code to TensorFlow 2</a></p>\n', 'IsAccepted': True, 'CreationDate': 1588751535}, {'QuestionId': 58897927, 'AnswerId': 58899003, 'URL': 'https://stackoverflow.com/questions/58897927/why-we-need-tf-convert-to-tensor/58899003#58899003', 'QuestionTitle': 'why we need tf.convert_to_tensor?', 'Answer': '<p>In this example, it only serves a simple demonstrative purpose (i.e. showing how Eager works) - otherwise, it\'s redundant. If the question is ""why use tensors is Eager?"" - \nTensorFlow ops can only operate on tensor instances (e.g. <code>Tensor</code>, <code>EagerTensor</code>) - even in Eager execution. Eager isn\'t actually devoid of graph - it\'s <em>mostly</em> graph, \'executed graph\'.</p>\n\n<p>As for \'why\' - long question, but short answer is ""optimization""; before actually executing, TensorFlow intelligently organizes and type-casts inputs - and doing so requires inputs to have certain attributes and methods, which are inherited once they are cast to tensors. When using high-level ops, this is done automatically, but if writing custom functionality (e.g. optimizers), you may need to convert manually.</p>\n', 'IsAccepted': True, 'CreationDate': 1573981928}, {'QuestionId': 33932901, 'AnswerId': 51821086, 'URL': 'https://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/51821086#51821086', 'QuestionTitle': 'What&#39;s the purpose of tf.app.flags in TensorFlow?', 'Answer': '<p>After trying many times I found this to print all FLAGS key as well as actual value -</p>\n\n<pre><code>for key in tf.app.flags.FLAGS.flag_values_dict():\n\n  print(key, FLAGS[key].value)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1534158964}, {'QuestionId': 33932901, 'AnswerId': 52420982, 'URL': 'https://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/52420982#52420982', 'QuestionTitle': 'What&#39;s the purpose of tf.app.flags in TensorFlow?', 'Answer': '<p>The <code>tf.app.flags</code> module is a functionality provided by Tensorflow to implement command line flags for your Tensorflow program. As an example, the code you came across would do the following:</p>\n\n<pre><code>flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n</code></pre>\n\n<p>The first parameter defines the name of the flag while the second defines the default value in case the flag is not specified while executing the file.</p>\n\n<p>So if you run the following:</p>\n\n<pre><code>$ python fully_connected_feed.py --learning_rate 1.00\n</code></pre>\n\n<p>then the learning rate is set to 1.00 and will remain 0.01 if the flag is not specified.</p>\n\n<p>As mentioned <a href=""https://medium.com/@zzzuzum/command-line-flags-python-and-tensorflow-85ab217dbd5"" rel=""noreferrer"" title=""Command line flags: Python and Tensorflow"">in this article</a>, the docs are probably not present because this might be something that Google requires internally for its developers to use.</p>\n\n<p>Also, as mentioned in the post, there are several advantages of using Tensorflow flags over flag functionality provided by other Python packages such as <code>argparse</code> especially when dealing with Tensorflow models, the most important being that you can supply Tensorflow specific information to the code such as information about which GPU to use.</p>\n', 'IsAccepted': False, 'CreationDate': 1537433089}, {'QuestionId': 33932901, 'AnswerId': 50462209, 'URL': 'https://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/50462209#50462209', 'QuestionTitle': 'What&#39;s the purpose of tf.app.flags in TensorFlow?', 'Answer': '<p>When you use <code>tf.app.run()</code>, you can transfer the variable very conveniently between threads using <code>tf.app.flags</code>. See <a href=""https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10"" rel=""noreferrer"">this</a> for further usage of <code>tf.app.flags</code>.</p>\n', 'IsAccepted': False, 'CreationDate': 1526974556}, {'QuestionId': 37689802, 'AnswerId': 42379195, 'URL': 'https://stackoverflow.com/questions/37689802/what-is-tensorflow-compat-as-str/42379195#42379195', 'QuestionTitle': 'What is tensorflow.compat.as_str()?', 'Answer': '<p>Basically, it comes from the fact that in Python 2, strings were dealt with primarily as bytes, not unicode.<br>\nIn Python 3, all strings are natively unicode.<br>\nThe purpose of the function is to ensure that whichever Python version you\'re using, you won\'t be bothered, hence the <code>compat</code> module name standing for compatibility.</p>\n\n<p>Under the hood, <code>tensorflow.compat.as_str</code> converts both <code>bytes</code> and <code>unicode</code> strings to <code>unicode</code> strings.  </p>\n\n<pre><code>Signature: tensorflow.compat.as_str(bytes_or_text, encoding=\'utf-8\')\nDocstring:\nReturns the given argument as a unicode string.\n\nArgs:\n  bytes_or_text: A `bytes`, `str, or `unicode` object.\n  encoding: A string indicating the charset for decoding unicode.\n\nReturns:\n  A `unicode` (Python 2) or `str` (Python 3) object.\n\nRaises:\n  TypeError: If `bytes_or_text` is not a binary or unicode string.\n</code></pre>\n\n<p>The library is documented <a href=""https://www.tensorflow.org/api_docs/python/tf/compat"" rel=""nofollow noreferrer"">here</a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1487716153}, {'QuestionId': 42256938, 'AnswerId': 42936969, 'URL': 'https://stackoverflow.com/questions/42256938/what-does-tf-gfile-do-in-tensorflow/42936969#42936969', 'QuestionTitle': 'What does tf.gfile do in TensorFlow?', 'Answer': '<p>For anyone landing here, the following answer was provided (by a googler) on: <a href=""https://stackoverflow.com/questions/42922948/why-use-tensorflow-gfile-for-file-i-o/"">Why use tensorflow gfile? (for file I/O)</a></p>\n\n<blockquote>\n  <p>The main roles of the tf.gfile module are:</p>\n  \n  <ol>\n  <li><p>To provide an API that is close to Python\'s file objects, and</p></li>\n  <li><p>To provide an implementation based on TensorFlow\'s C++ FileSystem API.</p></li>\n  </ol>\n  \n  <p>The C++ FileSystem API supports multiple file system implementations,\n  including local files, Google Cloud Storage (using a <code>gs://</code> prefix),\n  and HDFS (using an <code>hdfs://</code> prefix). TensorFlow exports these as\n  <code>tf.gfile</code>, so that you can use these implementations for saving and\n  loading checkpoints, writing TensorBoard logs, and accessing training\n  data (among other uses). However, if all of your files are local, you\n  can use the regular Python file API without any problem.</p>\n</blockquote>\n', 'IsAccepted': False, 'CreationDate': 1490126112}, {'QuestionId': 38060825, 'AnswerId': 38062223, 'URL': 'https://stackoverflow.com/questions/38060825/what-is-the-purpose-of-the-tf-contrib-module-in-tensorflow/38062223#38062223', 'QuestionTitle': 'What is the purpose of the tf.contrib module in Tensorflow?', 'Answer': '<p>In general, <code>tf.contrib</code> contains <em>contrib</em>uted code. It is meant to contain features and contributions that eventually should get merged into core TensorFlow, but whose interfaces may still change, or which require some testing to see whether they can find broader acceptance.</p>\n\n<p>The code in <code>tf.contrib</code> isn\'t supported by the Tensorflow team. It is included in the hope that it is helpful, but it might change or be removed at any time; there are no guarantees.</p>\n\n<p>The source of <code>tf.contrib.layers.sparse_column_with_hash_bucket</code> can be found at</p>\n\n<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column.py#L365"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/feature_column.py#L365</a></p>\n', 'IsAccepted': True, 'CreationDate': 1467057409}, {'QuestionId': 37689802, 'AnswerId': 44856079, 'URL': 'https://stackoverflow.com/questions/37689802/what-is-tensorflow-compat-as-str/44856079#44856079', 'QuestionTitle': 'What is tensorflow.compat.as_str()?', 'Answer': '<p>In a current version of TF, the whole <a href=""https://www.tensorflow.org/api_docs/python/tf/compat"" rel=""nofollow noreferrer""><code>tf.compat</code></a> group is nicely documented. </p>\n\n<p>Basically some things behaves differently in python 2 and 3 (might be slightly inaccurate, python gurus can help me with this). Python3 uses 64-bit floats and python2 32-bit floats, there are also difference with respect to <a href=""http://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html#unicode"" rel=""nofollow noreferrer"">strings</a>. Compat module tries things to behave in the same way (if you will check the <a href=""https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/util/compat.py"" rel=""nofollow noreferrer"">source code</a> you will see that they do different things depending on whether you use 2 or 3).</p>\n\n<hr>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/compat/as_bytes#tfcompatas_str"" rel=""nofollow noreferrer""><code>tf.compat.as_str</code></a>:</p>\n\n<blockquote>\n  <p>Converts either bytes or unicode to bytes, using utf-8 encoding for\n  text.</p>\n</blockquote>\n\n<p>This can be helpful if you save the data in tfrecords and want to make sure it will be saved in the same way no matter which python version is used.</p>\n', 'IsAccepted': False, 'CreationDate': 1498866350}, {'QuestionId': 42256938, 'AnswerId': 42259977, 'URL': 'https://stackoverflow.com/questions/42256938/what-does-tf-gfile-do-in-tensorflow/42259977#42259977', 'QuestionTitle': 'What does tf.gfile do in TensorFlow?', 'Answer': '<p>As you correctly point out <code>tf.gfile</code> is an abstraction for accessing the filesystem and is documented <a href=""https://www.tensorflow.org/api_docs/python/tf/gfile"" rel=""nofollow noreferrer"">here</a>. It is recommended over using plain python API since it provides some level of portability.</p>\n', 'IsAccepted': False, 'CreationDate': 1487192516}, {'QuestionId': 41505746, 'AnswerId': 41508451, 'URL': 'https://stackoverflow.com/questions/41505746/what-is-the-use-of-tf-select/41508451#41508451', 'QuestionTitle': 'What is the use of tf.select', 'Answer': ""<p>Best way you can understand it is by trying it out yourself:</p>\n\n<pre><code>In [86]: s = tf.InteractiveSession()\n\nIn [87]: inputs = tf.random_uniform([10], 0., 1.)\n\nIn [88]: positives = tf.ones([10])\n\nIn [89]: negatives = tf.zeros([10])    \n\nIn [90]: s.run([inputs, tf.select(inputs &gt; .5, positives, negatives)])\nOut[90]: \n[array([ 0.13187623,  0.77344072,  0.29853749,  0.29245567,  0.53489852,\n         0.34861541,  0.15090156,  0.40595055,  0.34910154,  0.24349082], dtype=float32),\n array([ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.], dtype=float32)]\n</code></pre>\n\n<p>For every value > <code>0.5</code> in tensor <code>inputs</code> you'll get a <code>1.</code> at the same index, otherwise the value is <code>0.</code>.</p>\n\n<p>The result of <code>inputs &gt; .5</code> is a tensor of booleans (<code>True</code> for values that meet the condition, <code>False</code> otherwise).</p>\n\n<pre><code>In [92]: s.run(inputs &gt; .5)\nOut[92]: array([ True, False,  True,  True,  True,  True,  True,  True, False,  True], dtype=bool)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1483715184}, {'QuestionId': 41505746, 'AnswerId': 41507765, 'URL': 'https://stackoverflow.com/questions/41505746/what-is-the-use-of-tf-select/41507765#41507765', 'QuestionTitle': 'What is the use of tf.select', 'Answer': '<p>Umm, RTD(Read the docs)! </p>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/control_flow_ops/comparison_operators#select"" rel=""nofollow noreferrer"">tf.select</a> selects elements from <code>positive</code> or <code>negative</code> tensors based on the <strong>boolness</strong> of the elements in the <code>condition</code> tensor. </p>\n\n<blockquote>\n  <p><code>tf.select(condition, t, e, name=None)</code><br>\n  <strong>Selects elements from t or e, depending on condition.</strong><br>\n  The t, and e tensors must all have the same shape, and the output will also have that shape.</p>\n</blockquote>\n\n<p>(from the official docs.)</p>\n\n<p>So in your case:</p>\n\n<p><code>threshold = tf.select(input &gt; RLSA_THRESHOLD, positive, negative)</code></p>\n\n<p><code>input &gt; RLSA_THRESHOLD</code> will be a tensor of <code>bool</code> or logical values (<code>0</code> or <code>1</code> symbolically), which will help choose a value from either the <code>positive</code> vector or the <code>negative</code> vector.</p>\n\n<p>For example, say you have a <code>RLSA_THRESHOLD</code> of 0.5 and your <code>input</code> vector is a 4-dimensional vector of real continuous values ranging from 0 to 1. Your <code>positive</code> and <code>negative</code> vectors are essentially <code>[1, 1, 1, 1]</code> and <code>[0, 0, 0, 0]</code>, respectively. <code>input</code> is <code>[0.8, 0.2, 0.5, 0.6]</code>.</p>\n\n<p><code>threshold</code> will be <code>[1, 0, 0, 1]</code>.</p>\n\n<p><strong>NOTE:</strong> <code>positive</code> and <code>negative</code> could be any kind of tensor as long as the dimensions agree with the <code>condition</code> tensor. Had <code>positive</code> and <code>negative</code> been, say, <code>[2, 4, 6, 8]</code> and <code>[1, 3, 5, 7]</code> respectively, your <code>threshold</code> would have been <code>[2, 3, 5, 8]</code>.</p>\n\n<hr>\n\n<blockquote>\n  <p>The code snippet seems reasonably advanced for me to assume that the authors would have just used <code>input &gt; RLSA_THRESHOLD</code> if there was no specific reason for the <code>tf.select</code>.</p>\n</blockquote>\n\n<p>There is a very good reason for that. <code>input &gt; RLSA_THRESHOLD</code> would simply return a tensor of logical (boolean) values. Logical values <strong>do not</strong> mix well with numerical values. You cannot use them for any realistic numerical computation. Had the <code>positive</code> and/or <code>negative</code> tensors been real valued, you might have required your <code>threshold</code> tensor to also have real values, in case you planned to use them further along.</p>\n\n<hr>\n\n<blockquote>\n  <p>Is the <code>tf.select</code> equivalent to <code>input &gt; RLSA_THRESHOLD</code>? If not, why not?</p>\n</blockquote>\n\n<p>No they are not. One is a function, the other is a tensor. </p>\n\n<p>I am going to give you the benefit of doubt and assume you meant to ask:</p>\n\n<blockquote>\n  <p>Is the <code>threshold</code> equivalent to <code>input &gt; RLSA_THRESHOLD</code>? If not, why not?</p>\n</blockquote>\n\n<p>No they are not. As explained above, <code>input &gt; RLSA_THRESHOLD</code> is a logical  tensor with a data type of <code>bool</code>. <code>threshold</code>, on the other hand, is a tensor with the same data type as <code>positive</code> and <code>negative</code>.</p>\n\n<p><strong>NOTE:</strong> You can always cast your logical tensors to numerical (or any other supported data type) tensors using any of the <a href=""https://www.tensorflow.org/api_docs/python/array_ops/casting"" rel=""nofollow noreferrer"">casting</a> methods available in <a href=""/questions/tagged/tensorflow"" class=""post-tag"" title=""show questions tagged &#39;tensorflow&#39;"" rel=""tag"">tensorflow</a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1483712747}, {'QuestionId': 40404519, 'AnswerId': 40407585, 'URL': 'https://stackoverflow.com/questions/40404519/what-is-the-intended-use-for-tf-contrib-framework-functions/40407585#40407585', 'QuestionTitle': 'What is the intended use for tf.contrib.framework functions?', 'Answer': '<p>If you look at the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/framework/python/ops/variables.py#L176"" rel=""nofollow noreferrer"">source code</a>, variable() seems to be just a wrapper around get_variable().  The only additional thing it does is set the ""collections"" parameter to all variables in graph, if it wasn\'t set.  </p>\n\n<p>contrib.framework is more or less just a bunch of utility functions.</p>\n', 'IsAccepted': True, 'CreationDate': 1478193404}, {'QuestionId': 39543348, 'AnswerId': 39577949, 'URL': 'https://stackoverflow.com/questions/39543348/why-does-tf-cond-accept-python-functions-as-arguments-instead-of-tensors/39577949#39577949', 'QuestionTitle': 'Why does `tf.cond` accept python functions as arguments instead of tensors?', 'Answer': '<p>They provide an abstraction to define the subgraphs during the construction of the true and false branch of the conditional.</p>\n\n<p>Similar to all the other operators, tf.cond only adds a bunch of nodes in the graph that is used in run calls.</p>\n', 'IsAccepted': True, 'CreationDate': 1474302900}, {'QuestionId': 33932901, 'AnswerId': 33938519, 'URL': 'https://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/33938519#33938519', 'QuestionTitle': 'What&#39;s the purpose of tf.app.flags in TensorFlow?', 'Answer': '<p>The <code>tf.app.flags</code> module is presently a thin wrapper around <strike>python-gflags, so the <a href=""https://github.com/gflags/python-gflags"">documentation for that project</a> is the best resource for how to use it</strike> <a href=""https://docs.python.org/2.7/library/argparse.html""><code>argparse</code></a>, which implements a subset of the functionality in <a href=""https://github.com/gflags/python-gflags""><code>python-gflags</code></a>.</p>\n\n<p>Note that this module is currently packaged as a convenience for writing demo apps, and is not technically part of the public API, so it may change in future.</p>\n\n<p>We recommend that you implement your own flag parsing using <code>argparse</code> or whatever library you prefer.</p>\n\n<p><strong>EDIT:</strong> The <code>tf.app.flags</code> module is not in fact implemented using <code>python-gflags</code>, but it uses a similar API. </p>\n', 'IsAccepted': False, 'CreationDate': 1448540245}, {'QuestionId': 37689802, 'AnswerId': 37690601, 'URL': 'https://stackoverflow.com/questions/37689802/what-is-tensorflow-compat-as-str/37690601#37690601', 'QuestionTitle': 'What is tensorflow.compat.as_str()?', 'Answer': '<ol>\n<li><p><code>tf.compat.as_str</code> converts input into a string</p></li>\n<li><p>I couldn\'t find any documentation, but you can look at the source code <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/compat.py"">here</a></p></li>\n<li><p>Tensorflow functions as a python module. The <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/framework.html#core-graph-data-structures"">graph context</a> is used to define a graph (mathematical computations) that will be used to train the model.</p></li>\n</ol>\n\n<blockquote>\n  <p>typical usage involves the Graph.as_default() context manager, which overrides the current default graph for the lifetime of the contex</p>\n</blockquote>\n', 'IsAccepted': False, 'CreationDate': 1465339502}]",{58631390},"['<p><a href=""https://www.tensorflow.org/api_docs/python/tf/compat"" rel=""noreferrer""><code>tf.compat</code></a> allows you to write code that works both in TensorFlow 1.x and 2.x. For example, the following piece of code:</p>\n\n<pre><code>import tensorflow as tf\n\ntf.compat.v1.disable_v2_behavior()\nwith tf.compat.v1.Session() as sess:\n    x = tf.compat.v1.placeholder(tf.float32, [2])\n    x2 = tf.square(x)\n    print(sess.run(x2, feed_dict={x: [2, 3]}))\n    # [4. 9.]\n</code></pre>\n\n<p>Runs the same on TensorFlow 1.15.0 and 2.0.0, even though session and placeholders were deprecated in 2.x. Likewise, <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v2"" rel=""noreferrer""><code>tf.compat.v2</code></a> allows you to use things introduced in 2.x from 1.x. Also, these APIs provide also backwards compatibility for the future too, so if at some point a 3.x version is released, the mechanism to write version-independent code will already be there since the first version of 2.x.</p>\n\n<p>EDIT: The documentation for the module about Python should actually be changed. Originally, <code>tf.compat</code> only held functions for that purpose (and it was like that until 1.13, <a href=""https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/compat"" rel=""noreferrer"">see all module documentation</a>). However, it was later repurposed for TensorFlow version compatibility.</p>\n', '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/compat"" rel=""noreferrer""><code>tf.compat</code></a> allows you to write code that works both in TensorFlow 1.x and 2.x. For example, the following piece of code:</p>\n\n<pre><code>import tensorflow as tf\n\ntf.compat.v1.disable_v2_behavior()\nwith tf.compat.v1.Session() as sess:\n    x = tf.compat.v1.placeholder(tf.float32, [2])\n    x2 = tf.square(x)\n    print(sess.run(x2, feed_dict={x: [2, 3]}))\n    # [4. 9.]\n</code></pre>\n\n<p>Runs the same on TensorFlow 1.15.0 and 2.0.0, even though session and placeholders were deprecated in 2.x. Likewise, <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v2"" rel=""noreferrer""><code>tf.compat.v2</code></a> allows you to use things introduced in 2.x from 1.x.', 'Also, these APIs provide also backwards compatibility for the future too, so if at some point a 3.x version is released, the mechanism to write version-independent code will already be there since the first version of 2.x.</p>\n\n<p>EDIT: The documentation for the module about Python should actually be changed. Originally, <code>tf.compat</code> only held functions for that purpose (and it was like that until 1.13, <a href=""https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/compat"" rel=""noreferrer"">see all module documentation</a>). However, it was later repurposed for TensorFlow version compatibility.</p>\n']",{'https://stackoverflow.com/questions/58631390/what-is-the-purpose-of-tf-compat/58631589#58631589'},,0.24058110135043836,0.03843466590672943
6,55422537,tf.estimator.DNNClassifier,Documentation Replication on Other Examples,Testing TF serving model fails with bytes as strings and strings as bytes confusion,"<p>I'm having a problem serving my text classification model on <code>Tensorflow 1.12</code>. I'm using <code>tf.estimator.inputs.pandas_input_fn</code> to read in my data, and <code>tf.estimator.DNNClassifier</code> to train/evaluate. I'd then like to serve my model.
(Apologies in advance, it's tough to provide a full working example here, but it's very much like the example TF provides at <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier</a>  )</p>

<p>I'm currently saving my model with ...</p>

<pre class=""lang-py prettyprint-override""><code>...
estimator.export_savedmodel(""./TEST_SERVING/"", self.serving_input_receiver_fn, strip_default_attrs=True)
...
def serving_input_receiver_fn(self):
      """"""An input receiver that expects a serialized tf.Example.""""""

      # feature spec dictionary  determines our input parameters for the model
      feature_spec = {
          'Headline': tf.VarLenFeature(dtype=tf.string),
          'Description': tf.VarLenFeature(dtype=tf.string)
      }

      # the inputs will be initially fed as strings with data serialized by
      # Google ProtoBuffers
      serialized_tf_example = tf.placeholder(
          dtype=tf.string, shape=None, name='input_example_tensor')
      receiver_tensors = {'examples': serialized_tf_example}

      # deserialize input
      features = tf.parse_example(serialized_tf_example, feature_spec)
      return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)


</code></pre>

<p>This actually fails to run with the error:</p>

<pre class=""lang-sh prettyprint-override""><code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensor'&gt; to Tensor. Contents: SparseTensor(indices=Tensor(""ParseExample/ParseExample:0"", shape=(?, 2), 
dtype=int64), values=Tensor(""ParseExample/ParseExample:2"", shape=(?,), dtype=string), dense_shape=Tensor(""ParseExample/ParseExample:4"", shape=(2,), dtype=int64)). Consider casting elements to a supported type.

</code></pre>

<p>I tried to save a second way doing:</p>

<pre class=""lang-py prettyprint-override""><code>def serving_input_receiver_fn(self):
  """"""Build the serving inputs.""""""
  INPUT_COLUMNS = [""Headline"",""Description""]
  inputs = {}
  for feat in INPUT_COLUMNS:
    inputs[feat] = tf.placeholder(shape=[None], dtype=tf.string, name=feat)
  return tf.estimator.export.ServingInputReceiver(inputs, inputs)
</code></pre>

<p>This actually works, until I try testing it with the <code>saved_model_cli</code>.
Some output for <code>saved_model_cli show --all --dir TEST_SERVING/1553879255/</code>:</p>

<pre class=""lang-sh prettyprint-override""><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['predict']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['Description'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Description:0
    inputs['Headline'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Headline:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['class_ids'] tensor_info:
        dtype: DT_INT64
        shape: (-1, 1)
        name: dnn/head/predictions/ExpandDims:0
    outputs['classes'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: dnn/head/predictions/str_classes:0
    outputs['logits'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/logits/BiasAdd:0
    outputs['probabilities'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/head/predictions/probabilities:0
  Method name is: tensorflow/serving/predict

</code></pre>

<p>But now I can't seem to test it.</p>

<pre class=""lang-sh prettyprint-override""><code>&gt;&gt;&gt; saved_model_cli run --dir TEST_SERVING/1553879255/ --tag_set serve --signature_def predict --input_examples 'inputs=[{""Description"":[""What is going on""],""Headline"":[""Help me""]}]'
Traceback (most recent call last):
 ...
  File ""/Users/Josh/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 489, in _create_example_string
    feature_list)
TypeError: 'What is going on' has type str, but expected one of: bytes

</code></pre>

<p>Ok, lets turn it into a bytes object by changing to <code>b[""What is going on""]</code> and <code>b[""Help me""]</code>...</p>

<pre class=""lang-sh prettyprint-override""><code>ValueError: Type &lt;class 'bytes'&gt; for value b'What is going on' is not supported for tf.train.Feature.
</code></pre>

<p>Any ideas/thoughts??
Thanks!</p>
","<p>Ok, so eventually I found the answer, quoted in <a href=""https://stackoverflow.com/questions/51482730/tensorflow-how-to-export-estimator-using-tensorhub-module"">TensorFlow: how to export estimator using TensorHub module?</a> </p>

<p>The problem was with serialization stuff I don't really understand. The solution allows to pass raw strings to <code>tf.estimator.export.build_raw_serving_input_receiver_fn</code> instead.</p>

<p>My saving funciton now looks like this:</p>

<pre class=""lang-py prettyprint-override""><code>  def save_serving_model(self,estimator):
      feature_placeholder = {'Headline': tf.placeholder('string', [1], name='headline_placeholder'),
      'Description': tf.placeholder('string', [1], name='description_placeholder')}
      serving_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholder)

      estimator.export_savedmodel(""TEST_SERVING/"", serving_input_fn)
</code></pre>

<p>where using the <code>saved_model_cli</code> works. I.e.:</p>

<pre class=""lang-sh prettyprint-override""><code>saved_model_cli run --dir /path/to/model/ --tag_set serve --signature_def predict --input_exprs=""Headline=['Finally, it works'];Description=['Yay, it works']"" 

</code></pre>

<pre class=""lang-sh prettyprint-override""><code>Result for output key class_ids:
[[2]]
Result for output key classes:
[[b'2']]
Result for output key logits:
[[-0.56755465  0.31625098  0.39260274]]
Result for output key probabilities:
[[0.16577701 0.40119565 0.4330274 ]]
</code></pre>
","{58003010, 46753508, 46862662, 44665255, 52614665, 47572144, 53499409, 46800819, 58978261, 58172536}","[{'QuestionId': 58978261, 'AnswerId': 62220976, 'URL': 'https://stackoverflow.com/questions/58978261/tfrecords-parsing-cant-parse-serialized-example/62220976#62220976', 'QuestionTitle': 'TFRecords parsing: Can&#39;t parse serialized Example', 'Answer': '<p>I was able to reproduce the error using the below code -</p>\n\n<pre><code>%tensorflow_version 2.x\nimport tensorflow as tf\nprint(tf.__version__)\n\ndef write_date_tfrecord():  \n    #writes 10 dummy values to replicate the issue\n    Output = [20191221 + x for x in range(0,10)]\n    print(""Writing Output - "", Output)\n\n    example = tf.train.Example(\n            features = tf.train.Features(\n                feature = {                    \n                    \'Output\':tf.train.Feature(float_list=tf.train.FloatList(value=Output))                    \n                     }\n                ))\n\n\n    writer = tf.io.TFRecordWriter(""Output.tf_record"")\n    writer.write(example.SerializeToString())\n\ndef parse_function(serialized_example):\n        features = {\n            \'Output\': tf.io.FixedLenFeature([], tf.float32) \n             }\n        features = tf.io.parse_single_example(serialized=serialized_example, features=features)  \n        Output = features[\'Output\']\n        return Output\n\ndef dataset_generator():\n    trRecordDataset = tf.data.TFRecordDataset(""Output.tf_record"")\n    trRecordDataset = trRecordDataset.map(parse_function, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n    return trRecordDataset    \n\nif __name__ == \'__main__\':\n    write_date_tfrecord()\n    generator = dataset_generator()\n    for Output in generator:\n        print(Output)\n</code></pre>\n\n<p><strong>Output -</strong></p>\n\n<pre><code>2.2.0\nWriting Output -  [20191221, 20191222, 20191223, 20191224, 20191225, 20191226, 20191227, 20191228, 20191229, 20191230]\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\n   1985       ctx.executor = executor_new\n-&gt; 1986       yield\n   1987     finally:\n\n10 frames\nInvalidArgumentError: Key: Output.  Can\'t parse serialized Example.\n     [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]\n\nDuring handling of the above exception, another exception occurred:\n\nInvalidArgumentError                      Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)\n     65   def wait(self):\n     66     """"""Waits for ops dispatched in this executor to finish.""""""\n---&gt; 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n     68 \n     69   def clear_error(self):\n\nInvalidArgumentError: Key: Output.  Can\'t parse serialized Example.\n     [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]]\n</code></pre>\n\n<p><strong>Solution -</strong> I was able to fix the issue and run the code successfully after modifying the code inside <code>parse_function</code>. In your case modify it in <code>parseExample</code> function. Modify,</p>\n\n<pre><code>\'Output\': tf.io.FixedLenFeature([], tf.float32)      \n</code></pre>\n\n<p>to</p>\n\n<pre><code>\'Output\': tf.io.FixedLenSequenceFeature([], tf.float32,allow_missing=True) \n</code></pre>\n\n<p><strong>Fixed Code -</strong></p>\n\n<pre><code>%tensorflow_version 2.x\nimport tensorflow as tf\nprint(tf.__version__)\n\ndef write_date_tfrecord():  \n    #writes 10 dummy values to replicate the issue\n    Output = [20191221 + x for x in range(0,10)]\n    print(""Writing Output - "", Output)\n\n    example = tf.train.Example(\n            features = tf.train.Features(\n                feature = {                    \n                    \'Output\':tf.train.Feature(float_list=tf.train.FloatList(value=Output))                    \n                     }\n                ))\n\n\n    writer = tf.io.TFRecordWriter(""Output.tf_record"")\n    writer.write(example.SerializeToString())\n\ndef parse_function(serialized_example):\n        features = {\n            \'Output\': tf.io.FixedLenSequenceFeature([], tf.float32,allow_missing=True) \n             }\n        features = tf.io.parse_single_example(serialized=serialized_example, features=features)  \n        Output = features[\'Output\']\n        return Output\n\ndef dataset_generator():\n    trRecordDataset = tf.data.TFRecordDataset(""Output.tf_record"")\n    trRecordDataset = trRecordDataset.map(parse_function, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n    return trRecordDataset\n\nif __name__ == \'__main__\':\n    write_date_tfrecord()\n    generator = dataset_generator()\n    for Output in generator:\n        print(Output)\n</code></pre>\n\n<p><strong>Output -</strong></p>\n\n<pre><code>2.2.0\nWriting Output -  [20191221, 20191222, 20191223, 20191224, 20191225, 20191226, 20191227, 20191228, 20191229, 20191230]\ntf.Tensor(\n[20191220. 20191222. 20191224. 20191224. 20191224. 20191226. 20191228.\n 20191228. 20191228. 20191230.], shape=(10,), dtype=float32)\n</code></pre>\n\n<p>Hope this answers your question. Happy Learning.</p>\n', 'IsAccepted': False, 'CreationDate': 1591377947}, {'QuestionId': 58172536, 'AnswerId': 58216669, 'URL': 'https://stackoverflow.com/questions/58172536/tensorflow-serving-type-object-is-not-of-expected-type-uint8/58216669#58216669', 'QuestionTitle': 'Tensorflow Serving Type: Object is not of expected type: uint8', 'Answer': '<p>Try loading the image in opencv and convert it to a list and send that. You don\'t have to encode it in base64 format. It should work then. </p>\n\n<pre><code>import json\nimport numpy as np\nimport cv2\nimport base64\nimport requests\nimport base64\nimport json\n\nimage = ""./frames/IMG_0474.MOV/IMG_0474_100.jpg""\nURL = ""http://localhost:8501/v1/models/saved_model/versions/1:predict"" \nheaders = {""content-type"": ""application/json""}\nimage_content = cv2.imread(image,1).astype(\'uint8\').tolist()\nbody = {""instances"": [{""inputs"": image_content}]}\nr = requests.post(URL, data=json.dumps(body), headers = headers) \nprint(r.text)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1570096618}, {'QuestionId': 58003010, 'AnswerId': 58020973, 'URL': 'https://stackoverflow.com/questions/58003010/tensorflow-tfrecords-cannot-parse-serialized-example/58020973#58020973', 'QuestionTitle': 'tensorflow TFRecords cannot parse serialized example', 'Answer': '<p>EDIT : My old answer is below this answer</p>\n\n<p>The right answer was to provide a <code>default_value</code> to the feature spec</p>\n\n<pre><code>peritem_feature_spec = {\n    \'relevance\':tf.FixedLenFeature([], tf.float32, default_value=0.0),\n    \'encoded_clust_index\':tf.VarLenFeature(tf.float32, default_value=0.0)\n    }\n</code></pre>\n\n<p>My <strong>old</strong> (works, but incorrect) answer is below</p>\n\n<p>So the issue came down to how my features were being padded in the tensorflow_ranking library. It was padding a list feature like : </p>\n\n<pre><code>def pad_fn():\n    return tf.pad(\n        tensor=serialized_list,\n        paddings=[[0, 0], [0, list_size -cur_list_size]],\n            constant_values="""")\n</code></pre>\n\n<p>This method appends empty bytes to the end of the tensor.  The parser looks for a <code>feature_name</code> in the empty tensors, and responds that it cant find it.  My workaround was to append serialized TFRecord example protos instead of an empty byte string.  I accomplished this like so : </p>\n\n<pre><code>def pad_fn():\n    # Create feature spec for tf.train.Example to append\n    pad_spec = {}\n    # Default values are 0 or an empty byte string depending on \n    # original serialized data type\n    dtype_map = {tf.float32:tf.train.Feature(\n            float_list=tf.train.FloatList(value=[0.0])), \n                 tf.int32:tf.train.Feature(\n                         int64_list=tf.train.Int64List(value=[0])), \n                 tf.string:tf.train.Feature(\n                         bytes_list=tf.train.BytesList(\n                                 value=[bytes(\'\', encoding=\'UTF-8\')]))}\n     # Create the feature spec\n    for key, item in peritem_feature_spec.items():\n        dtype = item.dtype\n        pad_spec[key] = dtype_map[dtype]\n    # Make and serialize example to append\n    constant_values = tf.train.Example(\n            features=tf.train.Features(feature=pad_spec))\n    constant_val_str = constant_values.SerializeToString()\n\n    # Add serialized padding to end of list\n    return tf.pad(\n        tensor=serialized_list,\n        paddings=[[0, 0], [0, list_size - cur_list_size]],\n        constant_values=constant_val_str)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1568946132}, {'QuestionId': 58003010, 'AnswerId': 58012472, 'URL': 'https://stackoverflow.com/questions/58003010/tensorflow-tfrecords-cannot-parse-serialized-example/58012472#58012472', 'QuestionTitle': 'tensorflow TFRecords cannot parse serialized example', 'Answer': '<p>Instead of <code>VarLenFeature</code> can you try <code>tf.FixedLenSequenceFeature([], tf.float32, allow_missing = True ,default_value=0)</code> explained <a href=""https://www.tensorflow.org/api_docs/python/tf/io/FixedLenSequenceFeature"" rel=""nofollow noreferrer"">here</a> and see if that works?</p>\n', 'IsAccepted': False, 'CreationDate': 1568900670}, {'QuestionId': 53499409, 'AnswerId': 57990442, 'URL': 'https://stackoverflow.com/questions/53499409/tensorflow-tfrecord-cant-parse-serialized-example/57990442#57990442', 'QuestionTitle': 'Tensorflow TFRecord: Can&#39;t parse serialized example', 'Answer': '<p>As an alternative, if your input features lengths are not fixed and are of arbitrary sizes then you can also use <code>tf.io.FixedLenSequenceFeature()</code> with arguments <code>allow_missing = True</code> and <code>default_value=0</code> (in case of type int and 0.0 for float) which does not require the input feature to be of fixed size unlike <code>tf.io.FixedLenFeature()</code>. You can find more information <a href=""https://www.tensorflow.org/api_docs/python/tf/io/FixedLenSequenceFeature"" rel=""noreferrer"">here</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1568801889}, {'QuestionId': 53499409, 'AnswerId': 53504855, 'URL': 'https://stackoverflow.com/questions/53499409/tensorflow-tfrecord-cant-parse-serialized-example/53504855#53504855', 'QuestionTitle': 'Tensorflow TFRecord: Can&#39;t parse serialized example', 'Answer': '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature"" rel=""noreferrer"">tf.FixedLenFeature()</a> is used for reading the fixed size arrays of data. And the shape of the data should be defined beforehand. Updating the parse function to</p>\n\n<pre><code>def parse(tfrecord):\n   return tf.parse_single_example(tfrecord, features={\n       \'label\': tf.FixedLenFeature([3], tf.int64, default_value=[0,0,0]),\n       \'test\': tf.FixedLenFeature([3], tf.float32, default_value=[0.0, 0.0, 0.0]),\n   })\n</code></pre>\n\n<p>Should do the job.</p>\n', 'IsAccepted': True, 'CreationDate': 1543338906}, {'QuestionId': 52614665, 'AnswerId': 52616053, 'URL': 'https://stackoverflow.com/questions/52614665/tensorflow-parses-the-record-incorrectly/52616053#52616053', 'QuestionTitle': 'Tensorflow parses the record incorrectly', 'Answer': '<p>A smaller example would be better.</p>\n\n<p>Each <code>session.run</code> will evaluate the tensor and run the graph. That means if you evaluate <code>image_name</code> and <code>raycast_name</code> separately, then you will get them from different runs and they won\'t be a pair.</p>\n\n<p>You could get the pair by evaluating both at the same time, e.g.:</p>\n\n<pre><code>current_image_name, current_raycast_name = session.run([\n    image_name, raycast_name\n])\n</code></pre>\n\n<p>I would also recommend to use the newer <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">Dataset API</a> over the queues.</p>\n', 'IsAccepted': True, 'CreationDate': 1538512678}, {'QuestionId': 47572144, 'AnswerId': 49139264, 'URL': 'https://stackoverflow.com/questions/47572144/has-type-unicode-but-expected-one-of-bytes-tf-train-example/49139264#49139264', 'QuestionTitle': 'has type unicode but expected one of :bytes tf.train.example', 'Answer': '<p>You have to encode text in utf-8 instead of unicode for it to be byte compatible :</p>\n\n<pre><code>text=u\'地离开对方\'\ntext = text.encode(""utf8"")\nexample = tf.train.Example(features=tf.train.Features(feature={\'text\':_bytes_feature([text])}))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1520367329}, {'QuestionId': 46753508, 'AnswerId': 49006839, 'URL': 'https://stackoverflow.com/questions/46753508/tensorflow-serving-response/49006839#49006839', 'QuestionTitle': 'Tensorflow Serving Response', 'Answer': ""<p>I am not sure how you are handling your <code>Predict.future(request_form, 5.0)</code>, but the same should apply for the sync response handling; tf offers a utility function <code>make_ndarray</code>:</p>\n\n<pre><code>res = stub.Predict(request, timeout).outputs[tensor_name]\narr = tf.make_ndarray(res)\n</code></pre>\n\n<p><code>arr</code> will be an np array of the correct dims.</p>\n\n<p>Where <code>tensor_name</code> is the name defined in your signature e.g. </p>\n\n<pre><code>tf.saved_model.signature_def_utils.build_signature_def(\n    inputs={'images': inp_tensor_info},\n    outputs={'scores': out_tensor_info},\n    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n)\n</code></pre>\n\n<p>would require </p>\n\n<pre><code>res = stub.Predict(request, timeout).outputs['scores']\narr = tf.make_ndarray(res)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1519728514}, {'QuestionId': 46862662, 'AnswerId': 46870942, 'URL': 'https://stackoverflow.com/questions/46862662/getting-error-str-object-has-no-attribute-dtype-when-exporting-textsum-model/46870942#46870942', 'QuestionTitle': 'Getting error &#39;str&#39; object has no attribute &#39;dtype&#39; when exporting textsum model for TensorFlow Serving', 'Answer': ""<p>PREDICT signature work with tensors, if res is 'str' type python variable, then res_tensor will be of dtype tf.string</p>\n\n<pre><code>res_tensor = tf.convert_to_tensor(res) \n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1508651654}, {'QuestionId': 46800819, 'AnswerId': 46868989, 'URL': 'https://stackoverflow.com/questions/46800819/tensorflow-serving-error-when-connecting-with-client-input-size-does-not-match/46868989#46868989', 'QuestionTitle': 'Tensorflow serving error when connecting with client &quot;input size does not match signature&quot;', 'Answer': '<p>QuantumLicht I again just want to thank you for your assistance here as it was one part of my issue. It seemed to have something to do with the keys used in the feature config. I am still using TF 1.2 and I remember reading sometime back that there were some fixes performed for proper key names being able to be used now in newer versions.  That said, as I debugged I noticed that it was expecting a single input named ""inputs"". So I removed ""abstract"" and set article to inputs. I then had to modify the output of decode and the final issue was related to the fact that I was only loading the model but never running the function against the model to get back the output that I needed to then send into tensor_info_outputs. </p>\n', 'IsAccepted': True, 'CreationDate': 1508627505}, {'QuestionId': 46800819, 'AnswerId': 46814198, 'URL': 'https://stackoverflow.com/questions/46800819/tensorflow-serving-error-when-connecting-with-client-input-size-does-not-match/46814198#46814198', 'QuestionTitle': 'Tensorflow serving error when connecting with client &quot;input size does not match signature&quot;', 'Answer': ""<p>It looks like you should specify a shape of <code>[1]</code> both in your client and graph definition.\n<strong>export_textsum.py</strong></p>\n\n<pre><code>feature_configs = {\n                'article': tf.FixedLenFeature(shape=[1], dtype=tf.string),\n                'abstract': tf.FixedLenFeature(shape=[1], dtype=tf.string),\n            }\n</code></pre>\n\n<p><strong>textsumclient.py</strong></p>\n\n<pre><code>  request.inputs['article'].CopyFrom(\n      tf.contrib.util.make_tensor_proto([test_data_set[0]], shape=[1]))\n  request.inputs['abstract'].CopyFrom(\n      tf.contrib.util.make_tensor_proto([test_data_set[1]], shape=[1]))\n</code></pre>\n\n<p>Or perhaps using <code>shape=[len(test_data_set[0])]</code> would be more appropriate</p>\n"", 'IsAccepted': False, 'CreationDate': 1508341454}, {'QuestionId': 44665255, 'AnswerId': 44665300, 'URL': 'https://stackoverflow.com/questions/44665255/tensorflow-input-data-protocol-buffer-tf-train-example-typeerror-for-feature-w/44665300#44665300', 'QuestionTitle': 'TensorFlow input data protocol buffer (tf.train.Example) TypeError for Feature with string type', 'Answer': '<p>It seems that they expect <code>s1</code> to be a byte string, so you need to add <code>b</code> before the <code>""</code>:</p>\n\n<pre><code>import tensorflow as tf\ntf_example = tf.train.Example()\ns1 = b""sample string 1""\ntf_example.features.feature[\'str1\'].bytes_list.value.extend([s1])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1498007227}]",{55422537},"['I.e.:</p>\n\n<pre class=""lang-sh prettyprint-override""><code>saved_model_cli run --dir /path/to/model/ --tag_set serve --signature_def predict --input_exprs=""Headline=[\'Finally, it works\'];Description=[\'Yay, it works\']"" \n\n</code></pre>\n\n<pre class=""lang-sh prettyprint-override""><code>Result for output key class_ids:\n[[2]]\nResult for output key classes:\n[[b\'2\']]\nResult for output key logits:\n[[-0.56755465  0.31625098  0.39260274]]\nResult for output key probabilities:\n[[0.16577701 0.40119565 0.4330274 ]]\n</code></pre>\n', '<p>Ok, so eventually I found the answer, quoted in <a href=""https://stackoverflow.com/questions/51482730/tensorflow-how-to-export-estimator-using-tensorhub-module"">TensorFlow: how to export estimator using TensorHub module?</a> </p>\n\n<p>The problem was with serialization stuff I don\'t really understand. The solution allows to pass raw strings to <code>tf.estimator.export.build_raw_serving_input_receiver_fn</code> instead.</p>\n\n<p>My saving funciton now looks like this:</p>\n\n<pre class=""lang-py prettyprint-override""><code>  def save_serving_model(self,estimator):\n      feature_placeholder = {\'Headline\': tf.placeholder(\'string\', [1], name=\'headline_placeholder\'),\n      \'Description\': tf.placeholder(\'string\', [1], name=\'description_placeholder\')}\n      serving_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholder)\n\n      estimator.export_savedmodel(""TEST_SERVING/"", serving_input_fn)\n</code></pre>\n\n<p>where using the <code>saved_model_cli</code> works. I.e.:</p>\n\n<pre class=""lang-sh prettyprint-override""><code>saved_model_cli run --dir /path/to/model/ --tag_set serve --signature_def predict --input_exprs=""Headline=[\'Finally, it works\'];Description=[\'Yay, it works\']"" \n\n</code></pre>\n\n<pre class=""lang-sh prettyprint-override""><code>Result for output key class_ids:\n[[2]]\nResult for output key classes:\n[[b\'2\']]\nResult for output key logits:\n[[-0.56755465  0.31625098  0.39260274]]\nResult for output key probabilities:\n[[0.16577701 0.40119565 0.4330274 ]]\n</code></pre>\n', '<p>Ok, so eventually I found the answer, quoted in <a href=""https://stackoverflow.com/questions/51482730/tensorflow-how-to-export-estimator-using-tensorhub-module"">TensorFlow: how to export estimator using TensorHub module?</a> </p>\n\n<p>The problem was with serialization stuff I don\'t really understand. The solution allows to pass raw strings to <code>tf.estimator.export.build_raw_serving_input_receiver_fn</code> instead.</p>\n\n<p>My saving funciton now looks like this:</p>\n\n<pre class=""lang-py prettyprint-override""><code>  def save_serving_model(self,estimator):\n      feature_placeholder = {\'Headline\': tf.placeholder(\'string\', [1], name=\'headline_placeholder\'),\n      \'Description\': tf.placeholder(\'string\', [1], name=\'description_placeholder\')}\n      serving_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholder)\n\n      estimator.export_savedmodel(""TEST_SERVING/"", serving_input_fn)\n</code></pre>\n\n<p>where using the <code>saved_model_cli</code> works.']",{'https://stackoverflow.com/questions/55422537/testing-tf-serving-model-fails-with-bytes-as-strings-and-strings-as-bytes-confus/55439968#55439968'},,0.2296046562176262,0.06511087994711988
6,71019644,tf.where,Documentation Ambiguity,Equivalent tensorflow expression to numpy mask,"<p>I have a numpy array named PixelData of unknown shape, and I am using the following condition to filter values in the array greater than some value x using a mask:</p>
<pre><code>PixelData[PixelData&gt;=x] = PixelData[PixelData&gt;=x] - x
</code></pre>
<p>When I convert this numpy array to a tensor, I cannot perform the same masking operation. I have tried using tf.where as follows:</p>
<pre><code>PixelData = tf.where(PixelData&gt;=x, PixelData - x, PixelData)
</code></pre>
<p>In the official documentation, they always seem to define the mask dimensions in advance to equal the dimensions of the tensor being masked, but then they talk about the dimensions being broadcasted automatically, so I am a bit confused. Are these two functions equivalent? Are there any situations where they may produce different outputs?</p>
","<p>Not sure what <code>PixelData</code> looks like, but here is working example with both methods:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf

x = 2
np_pixel_data = np.array([[3, 4, 5, 1],
                       [6, 4, 2, 5]], dtype=np.float32)

np_pixel_data[np_pixel_data&gt;=x] = np_pixel_data[np_pixel_data&gt;=x] - x

tf_pixel_data = tf.constant([[3, 4, 5, 1],
                       [6, 4, 2, 5]], dtype=tf.float32)

tf_pixel_data = tf.where(tf.greater_equal(tf_pixel_data, x), tf_pixel_data - x, tf_pixel_data)

print(np_pixel_data)
print(tf_pixel_data)
</code></pre>
<pre><code>[[1. 2. 3. 1.]
 [4. 2. 0. 3.]]
tf.Tensor(
[[1. 2. 3. 1.]
 [4. 2. 0. 3.]], shape=(2, 4), dtype=float32)
</code></pre>
<p>You might have some minor rounding differences, but nothing significant.</p>
","{44636319, 50637921, 58669730, 59568868, 58564613, 55011844, 73263182, 45784815, 51474033, 62196703}","[{'QuestionId': 73263182, 'AnswerId': 73263595, 'URL': 'https://stackoverflow.com/questions/73263182/creating-a-mask-tensor-from-an-index-tensor/73263595#73263595', 'QuestionTitle': 'creating a mask tensor from an index tensor', 'Answer': '<p>This can be accomplished using <code>tf.one_hot</code>, e.g.:</p>\n<pre class=""lang-py prettyprint-override""><code>mask = tf.one_hot(indices, depth=hidden_dim, axis=-1)  # [batch, seq_len, k, hidden_dim]\n</code></pre>\n<p>I wasn\'t clear on what you\'d like to happen to <code>k</code>. <code>tf.one_hot()</code> will keep the axis as is, i.e. you\'ll get a delta distribution for each [batch-index, seq-index, k-index] tuple.</p>\n', 'IsAccepted': True, 'CreationDate': 1659823935}, {'QuestionId': 62196703, 'AnswerId': 62205402, 'URL': 'https://stackoverflow.com/questions/62196703/elegant-way-to-get-unique-mask-in-tensorflow/62205402#62205402', 'QuestionTitle': 'Elegant way to get unique mask in tensorflow?', 'Answer': '<p>I made a custom mask_row function. this function selects the first unique elements in the row and creates a mask. this function is iterated on all the rows with map_fn</p>\n\n<pre><code>sequences = tf.constant([\n        [""1"", ""2"", ""3"", ""2""],\n        [""4"", ""2"", ""5"", ""6""],\n        [""7"", ""7"", ""4"", ""7""]])\n\n\ndef mask_row(row):\n    mask = tf.map_fn(lambda x: tf.math.equal(tf.range(row.shape[-1]), \n                                        tf.cast(tf.math.reduce_min(tf.where(tf.math.equal(row, x))), tf.int32)), \n                     row, dtype=tf.bool)\n    return tf.math.reduce_any(mask, axis=0)\n\n\ntf.map_fn(mask_row, sequences, dtype=tf.bool)\n</code></pre>\n\n<p>result</p>\n\n<pre><code>&lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy=\narray([[ True,  True,  True, False],\n       [ True,  True,  True,  True],\n       [ True, False,  True, False]])&gt;\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1591310899}, {'QuestionId': 59568868, 'AnswerId': 59569552, 'URL': 'https://stackoverflow.com/questions/59568868/equivalent-for-setting-numpy-like-mask-values-in-tensorflow/59569552#59569552', 'QuestionTitle': 'Equivalent for setting Numpy-like mask values in Tensorflow?', 'Answer': ""<p>You can do the following. It's a bit cluttery than numpy though.</p>\n\n<h2>Option 1</h2>\n\n<pre><code>import tensorflow as tf\n\nm = tf.constant([[True, False],[False, True]])\na = tf.constant([[2.,3.],[1.,2.]])\nval = 5\nb = a * tf.cast(tf.logical_not(m), tf.float32) + val * tf.cast(m, tf.float32)\n\nwith tf.Session() as sess:\n  print(sess.run(b))\n</code></pre>\n\n<h2>Option 2</h2>\n\n<pre><code>m = tf.constant([[True, False],[False, True]])\na = tf.constant([[2.,3.],[1.,2.]])\nval = 5\nval_arr = tf.ones_like(a)*val\n\nc = tf.where(tf.equal(m,False), a, val_arr)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1577996529}, {'QuestionId': 58669730, 'AnswerId': 58670292, 'URL': 'https://stackoverflow.com/questions/58669730/tensorflow-mask-tensor-element-with-condition/58670292#58670292', 'QuestionTitle': 'Tensorflow - mask tensor element with condition', 'Answer': '<p>I prefer to use <code>tf.tile()</code> operation to expand the mask:</p>\n\n<pre><code>data = tf.constant([[[0, 0], [1, 1], [2, 2]], [[3, 3], [4, 4], [5, 5]]], dtype=tf.float32)\nmask = tf.constant([[True, True, False], [False, True, True]])\n\nmask_expand = tf.tile(tf.expand_dims(mask, axis=-1), multiples=[1,1, tf.shape(data)[-1]])\n\nminus_ones = tf.fill(tf.shape(data), tf.constant(-1, dtype=data.dtype))\ndata = tf.where(mask_expand, data, minus_ones)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1572691179}, {'QuestionId': 58669730, 'AnswerId': 58669973, 'URL': 'https://stackoverflow.com/questions/58669730/tensorflow-mask-tensor-element-with-condition/58669973#58669973', 'QuestionTitle': 'Tensorflow - mask tensor element with condition', 'Answer': ""<p>I do a simple workaround (change the shape of mask), maybe there is a better method, but I can't figure out now.</p>\n\n<pre><code># reshape mask to the same shape with data\nbatch_size, total_timestep, feature_dimension = tf.shape(data)\n\n# mask = [[[True], [True], [False]], [[False], [True], [True]]]\nmask = tf.reshape(mask, [batch_size, total_timestep, 1]) # shape=(2, 3, 1)\n# mask = [[[True, True], [True, True], [False, False]], [[False, False], [True, True], [True, True]]]\nmask = tf.broadcast_to(mask, [batch_size, total_timestep, feature_dimension]) # shape=(2, 3, 2)\n\n# adapt mask\ndata = tf.where(mask, data, tf.constant(-1, dtype=data.dtype) )\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1572688454}, {'QuestionId': 58564613, 'AnswerId': 58565329, 'URL': 'https://stackoverflow.com/questions/58564613/tensorflow-boolean-mask-inverse/58565329#58565329', 'QuestionTitle': 'tensorflow boolean_mask inverse?', 'Answer': '<p>Similar to your own solution but with <code>tf.scatter_nd</code>.</p>\n\n<pre class=""lang-py prettyprint-override""><code>true_mask = tf.cast(tf.where(boolean_mask), tf.int32)\nfalse_mask = tf.cast(tf.where(~boolean_mask), tf.int32)\nt_foo = tf.scatter_nd(true_mask, true_bar, shape=tf.shape(foo))\nf_foo = tf.scatter_nd(false_mask, false_bar, shape=tf.shape(foo))\nres = t_foo + f_foo\n# array([[ 2,  4],\n#        [ 9, 12],\n#        [10, 12],\n#        [21, 24]], dtype=int32)\n</code></pre>\n\n<p>Basically, you can scatter the <code>true_bar</code> and <code>false_bar</code> to two different tensors, and add them together. </p>\n', 'IsAccepted': True, 'CreationDate': 1572035256}, {'QuestionId': 58564613, 'AnswerId': 58564859, 'URL': 'https://stackoverflow.com/questions/58564613/tensorflow-boolean-mask-inverse/58564859#58564859', 'QuestionTitle': 'tensorflow boolean_mask inverse?', 'Answer': ""<p>This is what I'm currently doing, but it seems unnecessarily complicated:</p>\n\n<pre><code>def boolean_mask_inverse(boolean_mask, true_bar, false_bar):\n    stacked_bar = tf.concat((true_bar, false_bar), axis=0)\n    index_mapping = tf.where(boolean_mask)\n    true_index_mapping = tf.where_v2(boolean_mask)[:,0]\n    false_index_mapping = tf.where_v2(tf.logical_not(boolean_mask))[:,0]\n    stacked_index_mapping = tf.concat((true_index_mapping, false_index_mapping), axis=0)\n    basic_indices = tf.range(tf.shape(stacked_index_mapping)[0])\n    inverse_index_mapping = tf.gather(basic_indices, stacked_index_mapping)\n    return tf.gather(stacked_bar, inverse_index_mapping)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1572032867}, {'QuestionId': 55011844, 'AnswerId': 55012263, 'URL': 'https://stackoverflow.com/questions/55011844/how-to-get-a-mask-in-the-tensorflow-graph-by-only-tensor-operations/55012263#55012263', 'QuestionTitle': 'How to get a mask in the tensorflow graph by only tensor operations', 'Answer': '<p>Creating one-hot encoding of indices and adding along the first dimension should give you the mask. i.e.</p>\n\n<pre><code>mask = tf.reduce_sum(tf.one_hot(idx, 5), axis=0)\n</code></pre>\n\n<p>The mask size (hard-coded 5) can be replaced with <code>d1.shape[0]</code>. </p>\n', 'IsAccepted': False, 'CreationDate': 1551823192}, {'QuestionId': 51474033, 'AnswerId': 51516840, 'URL': 'https://stackoverflow.com/questions/51474033/tensorflow-how-to-implement-this-complex-mask-operation/51516840#51516840', 'QuestionTitle': 'tensorflow: how to implement this complex mask operation', 'Answer': '<p>I think you are looking for <code>tf.sequence_mask</code>. <a href=""https://www.tensorflow.org/api_docs/python/tf/sequence_mask"" rel=""nofollow noreferrer"">See here</a>. This basically implements creating the mask you are wondering about. Usage as follows:</p>\n\n<pre><code>mask = tf.sequence_mask(t2, dtype=tf.float32)\nresult = t1 * mask\n</code></pre>\n\n<p>If <code>dtype</code> is not given this would create a boolean mask which would likely result in problems when trying to multiply with <code>t1</code>, which is why we specifically ask for <code>float32</code>.  </p>\n\n<p>In case the maximum element in <code>t2</code> can be smaller than <code>data_size</code>, you should use</p>\n\n<pre><code>mask = tf.sequence_mask(t2, maxlen=data_size, dtype=tf.float32)\n</code></pre>\n\n<p>to prevent a shape mismatch between <code>t1</code> and <code>mask</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1532515101}, {'QuestionId': 50637921, 'AnswerId': 50639432, 'URL': 'https://stackoverflow.com/questions/50637921/weighted-masking-in-tensorflow/50639432#50639432', 'QuestionTitle': 'Weighted masking in TensorFlow', 'Answer': '<p>Here is a simple solution using <code>tf.sequence_mask</code>:</p>\n\n<pre><code>import tensorflow as tf\n\nv = tf.constant([0.5, 0.1, 0.7])\nw = tf.constant([2, 3, 0])\n\nm = tf.sequence_mask(w)\nv2 = tf.tile(v[:, None], [1, tf.shape(m)[1]])\nres = tf.boolean_mask(v2, m)\n\nsess = tf.InteractiveSession()\nprint(res.eval())\n# array([0.5, 0.5, 0.1, 0.1, 0.1], dtype=float32)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1527842639}, {'QuestionId': 45784815, 'AnswerId': 45784993, 'URL': 'https://stackoverflow.com/questions/45784815/how-best-to-implement-a-matrix-mask-operation-in-tensorflow/45784993#45784993', 'QuestionTitle': 'How best to implement a matrix mask operation in tensorflow?', 'Answer': '<p>I think these three lines might do what you want. First, you make a mask. Then, you create the random data. Finally, fill in the masked values with the random data.</p>\n\n<pre><code>mask = tf.equal(a, 0.0)\nr = tf.random_uniform(tf.shape(a), minval = 0.0,maxval=1.0,dtype=tf.float32)\ntargets = tf.where(mask, r, a)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1503250375}, {'QuestionId': 45784815, 'AnswerId': 45785156, 'URL': 'https://stackoverflow.com/questions/45784815/how-best-to-implement-a-matrix-mask-operation-in-tensorflow/45785156#45785156', 'QuestionTitle': 'How best to implement a matrix mask operation in tensorflow?', 'Answer': '<p>You can use <code>tf.where</code> to achieve the same:</p>\n\n<pre><code>A = tf.Variable(a)\nB = tf.where(A==0., tf.random_normal(A.get_shape()), tf.cast(A, tf.float32))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1503251362}, {'QuestionId': 44636319, 'AnswerId': 44637587, 'URL': 'https://stackoverflow.com/questions/44636319/create-boolean-mask-on-tensorflow/44637587#44637587', 'QuestionTitle': 'Create boolean mask on TensorFlow', 'Answer': '<p>Try this:</p>\n\n<pre><code>import tensorflow as tf \n\nindices = tf.constant([[0, 1],[3, 5]], dtype=tf.int64)\nvalues = tf.constant([1, 1])\ns = (10, 7)\n\nst = tf.SparseTensor(indices, values, s)\nst_ordered = tf.sparse_reorder(st)\nresult = tf.sparse_tensor_to_dense(st_ordered)\n\nsess = tf.Session()\nsess.run(result)\n</code></pre>\n\n<p>Here is the output:</p>\n\n<pre><code>array([[0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n</code></pre>\n\n<p>I slightly modified your indexes so you can see the <code>x,y</code> format of the indices</p>\n\n<p>To obtain what you originally asked, set:</p>\n\n<pre><code>indices = tf.constant([[0, 0], [1, 0],[3, 0], [5, 0]], dtype=tf.int64)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>array([[1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0]], dtype=int32)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1497897175}]","{51586693, 65577783}","['<p>Not sure if you ever received an answer, but in case you haven\'t, you might look at this: <a href=""https://keras.io/guides/understanding_masking_and_padding/"" rel=""nofollow noreferrer"">Masking Guide</a></p>\n<p>I was just going to look to see if Dense supports masking also. Here is the relevant quote from the linked guide:</p>\n<blockquote>\n<p>If you have a custom layer that does not modify the time dimension, and if you want it to be able to propagate the current input mask, you should set <code>self.supports_masking = True</code> in the layer constructor. In this case, the default behavior of <code>compute_mask()</code> is to just pass the current mask through.</p>\n</blockquote>\n<p>This to me, says that Dense will propagate the mask.</p>\n', '<p>I think you should use <a href=""https://www.tensorflow.org/api_docs/python/tf/not_equal"" rel=""nofollow noreferrer""><code>tf.not_equal</code></a> to perform elementwise comparison on the tensor.</p>\n\n<pre><code>src = tf.constant([0, 1, 1, 0], dtype=tf.int8)\ntf.gather(src, tf.where(tf.not_equal(src, 0))).eval(session=tf.Session())\n\narray([[1],\n       [1]], dtype=int8)\n</code></pre>\n\n<p>You can also shorten this a bit and use <a href=""https://www.tensorflow.org/api_docs/python/tf/boolean_mask"" rel=""nofollow noreferrer""><code>tf.boolean_mask</code></a> instead of <code>tf.where</code> and <code>tf.gather</code>:</p>\n\n<pre><code>tf.boolean_mask(src, tf.not_equal(src, 0)).eval(session=tf.Session())\narray([1, 1], dtype=int8)\n</code></pre>\n\n<p>Note the difference in the shape of the outputs.</p>\n', '<p>Not sure if you ever received an answer, but in case you haven\'t, you might look at this: <a href=""https://keras.io/guides/understanding_masking_and_padding/"" rel=""nofollow noreferrer"">Masking Guide</a></p>\n<p>I was just going to look to see if Dense supports masking also. Here is the relevant quote from the linked guide:</p>\n<blockquote>\n<p>If you have a custom layer that does not modify the time dimension, and if you want it to be able to propagate the current input mask, you should set <code>self.supports_masking = True</code> in the layer constructor.']","{'https://stackoverflow.com/questions/51586693/tensor-has-shape-0-how-to-reshape-to/51586823#51586823', 'https://stackoverflow.com/questions/65577783/seeking-masking-support-for-dense-layer-in-keras/66270858#66270858'}",,0.2315629168677133,0.25391942394740236
7,62877768,tf.keras.Model,Documentation Ambiguity,Input shape of tf.data.Dataset not accepted by model.fit(),"<p>I would like to feed with data my model by applying a <code>tf.data.Dataset</code>.</p>
<p>Having checked the documentation of TF 2.0 I found that the <code>.fit()</code> function (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit</a>) accepts:</p>
<blockquote>
<p>x - A tf.data dataset. Should return a tuple of either (inputs, targets)
or (inputs, targets, sample_weights).</p>
</blockquote>
<p>So, I wrote the following minial proof of concept code:</p>
<pre><code>from sklearn.datasets import make_blobs
import tensorflow as tf
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.metrics import Accuracy, AUC

X, Y = make_blobs(n_samples=500, n_features=2, cluster_std=3.0, random_state=1)

def define_model():
    model = Sequential()
    model.add(Dense(units=1, activation=&quot;sigmoid&quot;, input_shape=(2,)))
    model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[AUC(), Accuracy()])
    return model

model = define_model()

X_ds = tf.data.Dataset.from_tensor_slices(X)
Y_ds = tf.data.Dataset.from_tensor_slices(Y)
dataset = tf.data.Dataset.zip((X_ds, Y_ds))

for elem in dataset.take(1):
    print(type(elem))
    print(elem)

model.fit(x=dataset) #&lt;-- does not work
#model.fit(x=X, y=Y) &lt;-- does work without any problems....
</code></pre>
<p>As mentioned in the second comment, the code that does not apply a <code>tf.data.Dataset</code> works fine.</p>
<p>However, when applying the Dataset object I get the following error message:</p>
<pre><code>&lt;class 'tuple'&gt;
(&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([-10.42729974,  -0.85439721])&gt;, &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;)
... other output here...
ValueError: Error when checking input: expected dense_19_input to have
shape (2,) but got array with shape (1,)
</code></pre>
<p>From my understanding of the documentation, the dataset I have constructed should be exactly the tuple object the fit method expects.</p>
<p>I do not understand this error message.</p>
<p>What am I doing wrong here?</p>
","<p>When you pass a dataset to <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""noreferrer""><code>fit</code></a>, it is expected that it will directly generate batches, not individual examples. You just need to batch your dataset before training.</p>
<pre class=""lang-py prettyprint-override""><code>dataset = dataset.batch(batch_size)
model.fit(x=dataset)
</code></pre>
","{52218500, 60442117, 50809257, 59155985, 64770484, 71003062, 54055707, 53643164, 56218014, 73613375}","[{'QuestionId': 73613375, 'AnswerId': 73613662, 'URL': 'https://stackoverflow.com/questions/73613375/tensorflow-dataset-or-model-is-in-wrong-shape/73613662#73613662', 'QuestionTitle': 'Tensorflow: dataset or model is in wrong shape', 'Answer': ""<ol>\n<li>You need batch dimension: <code>from_tensors(([img], [mask]))</code></li>\n<li>You need to modify your model because it produces the output with shape that does not match a mask shape</li>\n</ol>\n<p>Complete example:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nimg = np.zeros((720, 1280, 3), dtype=np.uint8)\nmask = np.zeros((720, 1280, 1), dtype=np.uint8)\ntrain_data = tf.data.Dataset.from_tensors(([img], [mask]))\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(720, 1280, 3)),\n    tf.keras.layers.Conv2D(128, 5, padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(128, 5, padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2DTranspose(128, kernel_size=3, strides=2, padding='same', activation='relu'),\n    tf.keras.layers.Conv2DTranspose(2, kernel_size=3, strides=2, padding='same', activation='relu'),\n])\n\nmodel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\nmodel.summary()\n\nmodel.fit(train_data)\nmodel.evaluate(train_data)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1662404175}, {'QuestionId': 73613375, 'AnswerId': 73613574, 'URL': 'https://stackoverflow.com/questions/73613375/tensorflow-dataset-or-model-is-in-wrong-shape/73613574#73613574', 'QuestionTitle': 'Tensorflow: dataset or model is in wrong shape', 'Answer': '<p>I think it is because you have your images with shape <code>(720, 1280, 3)</code>. But, even if you are providing only one image, you should add another dimension, that indicates the number of samples in your dataset. In this case, since you have only one element, the correct shapes are:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n# sample images\nimg = np.ones((1, 720, 1280, 3))\nmask = np.ones((1, 720, 1280, 3))\ntrain_data = tf.data.Dataset.from_tensors((img, mask))\n</code></pre>\n<p>To convert an existing image in this format:</p>\n<pre><code>img = np.ones((720, 1280, 3))\nimg = np.expand_dims(img, axis=0)  # new shape: (1, 720, 1280, 3)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1662403469}, {'QuestionId': 52218500, 'AnswerId': 71914845, 'URL': 'https://stackoverflow.com/questions/52218500/using-tf-data-dataset-as-training-input-to-keras-model-not-working/71914845#71914845', 'QuestionTitle': 'Using tf.data.Dataset as training input to Keras model NOT working', 'Answer': '<blockquote>\n<p>I am wondering how Keras is able to do 5 epochs when the\nmake_one_shot_iterator() which only supports iterating once through a\ndataset?</p>\n</blockquote>\n<p>could be given smth like <em>iterations = len(y_train) * epochs</em> - <a href=""https://medium.com/ymedialabs-innovation/how-to-use-dataset-and-iterators-in-tensorflow-with-code-samples-3bb98b6b74ab"" rel=""nofollow noreferrer"">here</a> shown for tf.v1</p>\n<p>the code from <em>Mohan Radhakrishnan</em> still works in <strong>tf.v2</strong> with little corrections in objects\' belongings to new classes (in tf.v2) fixings - to make the code up-to-date... No more make_one_shot_iterator() needed</p>\n<pre><code># &gt;&gt; author: Mohan Radhakrishnan\n\nimport tensorflow as tf\nimport tensorflow.keras\nimport numpy as np\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Input\n\nnp.random.seed(1)\ntf.random.set_seed(1)\n\nbatch_size = 128\nNUM_CLASSES = 10\n\nprint(tf.__version__)\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n#x_train, x_test = x_train / 255.0, x_test / 255.0 #normalizing\n\ndef tfdata_generator(images, labels, is_training, batch_size=128):\n    \'\'\'Construct a data generator using tf.Dataset\'\'\'\n\n    def preprocess_fn(image, label):\n        \'\'\'A transformation function to preprocess raw data\n        into trainable input. \'\'\'\n        x = tf.reshape(tf.cast(image, tf.float32), (28, 28, 1))\n        y = tf.one_hot(tf.cast(label, tf.uint8), NUM_CLASSES)\n        return x, y\n\n    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n    if is_training:\n        dataset = dataset.shuffle(1000)  # depends on sample size\n\n    # Transform and batch data at the same time\n    dataset = dataset.apply( tf.data.experimental.map_and_batch(\n        preprocess_fn, batch_size,\n        num_parallel_batches=2,  # cpu cores\n        drop_remainder=True if is_training else False))\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch( tf.data.experimental.AUTOTUNE)\n\n    return dataset\n\ntraining_set = tfdata_generator(x_train, y_train,is_training=True, batch_size=batch_size)\ntesting_set  = tfdata_generator(x_test, y_test, is_training=False, batch_size=batch_size)\n\ninputs = Input(shape=(28, 28, 1))\nx = Conv2D(32, (3, 3), activation=\'relu\', padding=\'valid\')(inputs)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Conv2D(64, (3, 3), activation=\'relu\')(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Flatten()(x)\nx = Dense(512, activation=\'relu\')(x)\nx = Dropout(0.5)(x)\noutputs = Dense(NUM_CLASSES, activation=\'softmax\')(x)\n\nkeras_model =  tf.keras.Model(inputs, outputs)\n\n#Compile the model\nkeras_model.compile(\'adam\', \'categorical_crossentropy\', metrics=[\'acc\'])\n\n#Train with tf.data datasets\n# training_set.make_one_shot_iterator() - \'PrefetchDataset\' object has no attribute \'make_one_shot_iterator\'\nkeras_training_history = keras_model.fit(\n                            training_set,\n                            steps_per_epoch=len(x_train) // batch_size,\n                            epochs=5,\n                            validation_data=testing_set,\n                            validation_steps=len(x_test) // batch_size,\n                            verbose=1)\nprint(keras_training_history.history)\n</code></pre>\n<p>not loading data locally, just easy DataFlow - that is very convinient - Thanks a lot - hope my corrections are proper</p>\n', 'IsAccepted': False, 'CreationDate': 1650301680}, {'QuestionId': 56218014, 'AnswerId': 71884643, 'URL': 'https://stackoverflow.com/questions/56218014/how-to-acquire-tf-data-datasets-shape/71884643#71884643', 'QuestionTitle': 'How to acquire tf.data.dataset&#39;s shape?', 'Answer': '<p>As of 4/15/2022 with the TF v2.8, you can get the results by using</p>\n<p><code>dataset.cardinality().numpy()</code></p>\n<p>ref: <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cardinality"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cardinality</a></p>\n', 'IsAccepted': False, 'CreationDate': 1650029256}, {'QuestionId': 71003062, 'AnswerId': 71007445, 'URL': 'https://stackoverflow.com/questions/71003062/fit-a-dataframe-into-a-model-using-tfds-data/71007445#71007445', 'QuestionTitle': 'Fit a DataFrame into a model using TFDS data', 'Answer': '<p>I would recommend using the <code>tf.data.Dataset</code> API. Check out this <a href=""https://www.tensorflow.org/tutorials/load_data/pandas_dataframe"" rel=""nofollow noreferrer"">tutorial</a>. Here is a working example based on your data structure:</p>\n<pre><code>import pandas as pd\nimport tensorflow as tf\n\ndf = pd.DataFrame(data= {\'text\': [\'some text\', \'some more text\'], \'class\': [0, 1]})\nlabels = df.pop(\'class\')\ndataset = tf.data.Dataset.from_tensor_slices((df, labels))\n\nfor x, y in dataset:\n  print(x, y)\n</code></pre>\n<pre><code>tf.Tensor([b\'some text\'], shape=(1,), dtype=string) tf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor([b\'some more text\'], shape=(1,), dtype=string) tf.Tensor(1, shape=(), dtype=int64)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1644152883}, {'QuestionId': 56218014, 'AnswerId': 68164257, 'URL': 'https://stackoverflow.com/questions/56218014/how-to-acquire-tf-data-datasets-shape/68164257#68164257', 'QuestionTitle': 'How to acquire tf.data.dataset&#39;s shape?', 'Answer': '<p><em>To see element shapes and types, print dataset elements directly instead of using as_numpy_iterator.</em> - <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset</a></p>\n<pre><code>dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\n</code></pre>\n<p>break from for loop to see the shape of any tensor</p>\n<pre><code>dataset = tf.data.Dataset.from_tensor_slices((X_s, y_s))\nfor element in dataset:\n  print(element)\n  break\n</code></pre>\n<p>Output here as two numpy arrays and shape of each is printed</p>\n<pre><code>(&lt;tf.Tensor: shape=(13,), dtype=float32, numpy=\narray([ 0.9521966 ,  0.68100524,  1.973123  ,  0.7639558 , -0.2563337 ,\n        2.394438  , -1.0058318 ,  0.01544279, -0.69663054,  1.0873381 ,\n       -2.2745786 , -0.71442884, -2.1488726 ], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)&gt;)\n\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1624887131}, {'QuestionId': 56218014, 'AnswerId': 67527025, 'URL': 'https://stackoverflow.com/questions/56218014/how-to-acquire-tf-data-datasets-shape/67527025#67527025', 'QuestionTitle': 'How to acquire tf.data.dataset&#39;s shape?', 'Answer': '<p>Code as below:</p>\n<pre><code>dataset_to_numpy = list(dataset.as_numpy_iterator())\nshape = tf.shape(dataset_to_numpy)\nprint(shape)\n</code></pre>\n<p>It produces output like this:</p>\n<pre><code>tf.Tensor([1080   64   64    3], shape=(4,), dtype=int32)\n</code></pre>\n<p>It\'s simple to write the code, but it still costs time to iterate the dataset.\nFor more info about <code>tf.data.Dataset</code>, check this <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#as_numpy_iterator"" rel=""nofollow noreferrer"">link</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1620945404}, {'QuestionId': 64770484, 'AnswerId': 64772183, 'URL': 'https://stackoverflow.com/questions/64770484/issue-tf-data-dataset-for-keras-multi-input-model/64772183#64772183', 'QuestionTitle': 'Issue tf.data.Dataset for Keras multi-input model', 'Answer': '<p>You can combine datasets using the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip"" rel=""nofollow noreferrer""><code>zip</code> function</a>. The <code>zip</code> function can take nested datasets as an argument, so we just need to reproduce the way you feed data in the fit function with numpy arrays :</p>\n<pre class=""lang-py prettyprint-override""><code>ds_meta = tf.data.Dataset.from_tensor_slices((dict_meta))\nds_text = tf.data.Dataset.from_tensor_slices((dict_text))\nds_label = tf.data.Dataset.from_tensor_slices((label))\ncombined_dataset = tf.data.Dataset.zip(((ds_text,ds_meta),ds_label))\ncombined_dataset = combined_dataset.batch(5)\n</code></pre>\n<p>Running it :</p>\n<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; model.fit(combined_dataset)\n1/1 [==============================] - 0s 212us/step - loss: 2.2895\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1605023399}, {'QuestionId': 60442117, 'AnswerId': 60936291, 'URL': 'https://stackoverflow.com/questions/60442117/model-fit-with-tensorflow-2-1-and-tf-data-dataset-valueerror-attempt-to-convert/60936291#60936291', 'QuestionTitle': 'model.fit with tensorflow 2.1 and tf.data.Dataset ValueError: Attempt to convert a value TensorSpec', 'Answer': ""<p>The problem appears to be that Keras cannot handle nested dictionaries. If you flatten the dictionary then it resolves the error. For example:</p>\n\n<pre><code>{\n    'contextual_one_hot': ...,\n    'contextual_multi_hot': ...,\n    'contextual_dense': ...,\n    'sequential_one_hot': ...,\n    'sequential_multi_hot': ...,\n    'sequential_one_hot': ...,\n    'sequential_dense': ...\n}\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1585590038}, {'QuestionId': 59155985, 'AnswerId': 59272146, 'URL': 'https://stackoverflow.com/questions/59155985/cant-get-tensorflow-dataset-to-be-accepted-by-the-model-fit-function-some-prob/59272146#59272146', 'QuestionTitle': 'Can&#39;t get tensorflow dataset to be accepted by the model.fit function. Some problem with dimensionality', 'Answer': ""<p>For the benefit of anyone who may encounter this in the future, I've managed to find the problem. The clue is in the error message. I was confused as my model clearly stated that the input would be (40, 40, 3) and yet the error said that it expected 4 dims and got an input (40, 40, 3).</p>\n\n<p>But that's the answer right there. The models 'fit' function expects a 4D input. I was trying to feed in the whole dataset object (which is a 3D object) and it was expecting 4D.</p>\n\n<p>It expects 4D because the other dimension is the batch size, which is why when you batch up the dataset and feed that in, it suddenly works. Batching up the data, add that extra dimension.</p>\n"", 'IsAccepted': False, 'CreationDate': 1575997094}, {'QuestionId': 53643164, 'AnswerId': 59306221, 'URL': 'https://stackoverflow.com/questions/53643164/tf-data-dataset-object-as-input-to-tf-keras-model-valueerror/59306221#59306221', 'QuestionTitle': 'tf.data.Dataset object as input to tf.Keras model -- ValueError', 'Answer': '<p>I just got stuck with a similar problem trying to distribute my </p>\n\n<p><code>&lt;DatasetV1Adapter shapes: &lt;unknown&gt;, types: tf.float32&gt;"" dataset using strategy.experimental_distribute_dataset() with tf.distribute.MirroredStrategy() as strategy. I got the same error as above ("" raise ValueError(""Cannot take the length of shape with unknown rank</code>\nValueError: Cannot take the length of shape with unknown rank.\n"") For anyone who gets stuck with a similar problem, my solution was to use my DatasetV1Adapter dataset and create a new dataset using data.Dataset.from_generator as follows:</p>\n\n<pre><code>        def generator(dataset):\n            # dataset of type DatasetV1Adapter \n            for datapoint in dataset:\n                yield datapoint\n\n    dataset = tf.data.Dataset.from_generator(generator, (tf.float32), output_shapes=([None, None, None, None]))\n\ndataset_dist = strategy.experimental_distribute_dataset(dataset)\n</code></pre>\n\n<p>Worked for me!</p>\n', 'IsAccepted': False, 'CreationDate': 1576158967}, {'QuestionId': 53643164, 'AnswerId': 57683053, 'URL': 'https://stackoverflow.com/questions/53643164/tf-data-dataset-object-as-input-to-tf-keras-model-valueerror/57683053#57683053', 'QuestionTitle': 'tf.data.Dataset object as input to tf.Keras model -- ValueError', 'Answer': '<p>I had this issue recently; you probably need to provide the <code>output_shapes</code> argument:</p>\n\n<pre><code>dataset = data.Dataset.from_generator(train_generator, (tf.int64, tf.int64), output_shapes=(tf.TensorShape([None, None, None, None]), tf.TensorShape([None])))\n</code></pre>\n\n<p>assuming a 4-dimensional input image and a 1-dimensional output array.</p>\n', 'IsAccepted': False, 'CreationDate': 1566945821}, {'QuestionId': 56218014, 'AnswerId': 56220759, 'URL': 'https://stackoverflow.com/questions/56218014/how-to-acquire-tf-data-datasets-shape/56220759#56220759', 'QuestionTitle': 'How to acquire tf.data.dataset&#39;s shape?', 'Answer': ""<p>Where the length is known you can call:</p>\n\n<pre><code>tf.data.experimental.cardinality(dataset)\n</code></pre>\n\n<p>but if this fails then, it's important to know that a TensorFlow <code>Dataset</code> is (in general) lazily evaluated so this means that in the general case we may need to iterate over every record before we can find the length of the dataset.</p>\n\n<p>For example, assuming you have eager execution enabled and its a small 'toy' dataset that fits comfortably in memory you could just <code>enumerate</code> it into a new list and grab the last index (then add 1 because lists are zero-indexed):</p>\n\n<pre><code>dataset_length = [i for i,_ in enumerate(dataset)][-1] + 1\n</code></pre>\n\n<p>Of course this is inefficient at best and, for large datasets, will fail entirely  because everything needs to fit into memory for the list. in such circumstances I can't see any alternative other than to iterate through the records keeping a manual count.</p>\n"", 'IsAccepted': True, 'CreationDate': 1558354218}, {'QuestionId': 54055707, 'AnswerId': 54062683, 'URL': 'https://stackoverflow.com/questions/54055707/keras-model-fit-with-tf-dataset-fails-while-using-tf-train-works-fine/54062683#54062683', 'QuestionTitle': 'Keras model.fit() with tf.dataset fails while using tf.train works fine', 'Answer': '<p>Probably tf.py_func produces an unknown shape which Keras cannot infer. We can set the shape of the tensor returned by it using set_shape(your_shape) method and that would help Keras infer the shape of the result.</p>\n', 'IsAccepted': True, 'CreationDate': 1546786226}, {'QuestionId': 52218500, 'AnswerId': 52222670, 'URL': 'https://stackoverflow.com/questions/52218500/using-tf-data-dataset-as-training-input-to-keras-model-not-working/52222670#52222670', 'QuestionTitle': 'Using tf.data.Dataset as training input to Keras model NOT working', 'Answer': '<p>I just upgraded to Tensorflow 1.10 to execute <a href=""https://gist.github.com/datlife/abfe263803691a8864b7a2d4f87c4ab8"" rel=""nofollow noreferrer"">this code</a>. I think that is the answer which is also discussed in the other <a href=""https://stackoverflow.com/questions/46135499/how-to-properly-combine-tensorflows-dataset-api-and-keras"">Stackoverflow thread</a></p>\n\n<p>This code executes but only if I remove the normalization as that line seems to use too much CPU memory. I see messages indicating that. I also reduced the cores.</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Input\n\nnp.random.seed(1)\ntf.set_random_seed(1)\n\nbatch_size = 128\nNUM_CLASSES = 10\n\nprint(tf.__version__)\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n#x_train, x_test = x_train / 255.0, x_test / 255.0 #normalizing\n\ndef tfdata_generator(images, labels, is_training, batch_size=128):\n    \'\'\'Construct a data generator using tf.Dataset\'\'\'\n\n    def preprocess_fn(image, label):\n        \'\'\'A transformation function to preprocess raw data\n        into trainable input. \'\'\'\n        x = tf.reshape(tf.cast(image, tf.float32), (28, 28, 1))\n        y = tf.one_hot(tf.cast(label, tf.uint8), NUM_CLASSES)\n        return x, y\n\n    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n    if is_training:\n        dataset = dataset.shuffle(1000)  # depends on sample size\n\n    # Transform and batch data at the same time\n    dataset = dataset.apply(tf.contrib.data.map_and_batch(\n        preprocess_fn, batch_size,\n        num_parallel_batches=2,  # cpu cores\n        drop_remainder=True if is_training else False))\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n\n    return dataset\n\ntraining_set = tfdata_generator(x_train, y_train,is_training=True, batch_size=batch_size)\ntesting_set  = tfdata_generator(x_test, y_test, is_training=False, batch_size=batch_size)\n\ninputs = Input(shape=(28, 28, 1))\nx = Conv2D(32, (3, 3), activation=\'relu\', padding=\'valid\')(inputs)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Conv2D(64, (3, 3), activation=\'relu\')(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Flatten()(x)\nx = Dense(512, activation=\'relu\')(x)\nx = Dropout(0.5)(x)\noutputs = Dense(NUM_CLASSES, activation=\'softmax\')(x)\n\nkeras_model =  tf.keras.Model(inputs, outputs)\n\n#Compile the model\nkeras_model.compile(\'adam\', \'categorical_crossentropy\', metrics=[\'acc\'])\n\n#Train with tf.data datasets\nkeras_training_history = keras_model.fit(\n                            training_set.make_one_shot_iterator(),\n                            steps_per_epoch=len(x_train) // batch_size,\n                            epochs=5,\n                            validation_data=testing_set.make_one_shot_iterator(),\n                            validation_steps=len(x_test) // batch_size,\n                            verbose=1)\nprint(keras_training_history.history)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1536323559}, {'QuestionId': 52218500, 'AnswerId': 52226501, 'URL': 'https://stackoverflow.com/questions/52218500/using-tf-data-dataset-as-training-input-to-keras-model-not-working/52226501#52226501', 'QuestionTitle': 'Using tf.data.Dataset as training input to Keras model NOT working', 'Answer': '<p>Installing the tf-nightly build, together with changing dtypes of some tensors (the error changes after installing tf-nightly), solved the problem, so it is an issue which (hopefully) will be solved in 1.11.</p>\n\n<p>Related material: <a href=""https://github.com/tensorflow/tensorflow/issues/21894"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/21894</a></p>\n', 'IsAccepted': True, 'CreationDate': 1536337930}, {'QuestionId': 50809257, 'AnswerId': 50869818, 'URL': 'https://stackoverflow.com/questions/50809257/returning-dataset-from-tf-data-dataset-map-causes-tensorslicedataset-object/50869818#50869818', 'QuestionTitle': 'Returning dataset from tf.data.Dataset.map() causes &#39;TensorSliceDataset&#39; object has no attribute &#39;get_shape&#39; error', 'Answer': '<p><a href=""https://stackoverflow.com/a/50811745/3574081"">DomJack\'s answer</a> is absolutely correct about the signature of <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer""><code>Dataset.map()</code></a>: it expects the return value of the passed <code>mapped_fn</code> to be one or more tensors (or sparse tensors).</p>\n\n<p>If you do have a function that returns a <code>Dataset</code>, you can use <code>Dataset.flat_map()</code> to flatten and concatenate all of the returned datasets into a single dataset, as follows:</p>\n\n<pre><code>def mapped_fn(_):\n    X = tf.random_uniform([3,3])\n    y = tf.random_uniform([3,1])\n    dataset = tf.data.Dataset.from_tensor_slices((X,y))\n    return dataset\n\n# Generate 100 dummy elements.\nunimportant_dataset = tf.data.Dataset.range(100)\n\n# Convert each dummy element into a dataset of 3 nested elements, and concatenate them.\ndataset = unimportant_dataset.flat_map(mapped_fn)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1529042030}, {'QuestionId': 50809257, 'AnswerId': 50811745, 'URL': 'https://stackoverflow.com/questions/50809257/returning-dataset-from-tf-data-dataset-map-causes-tensorslicedataset-object/50811745#50811745', 'QuestionTitle': 'Returning dataset from tf.data.Dataset.map() causes &#39;TensorSliceDataset&#39; object has no attribute &#39;get_shape&#39; error', 'Answer': '<p>The <code>map_fn</code> passed to <code>tf.data.Dataset.map</code> should take the tensors of a single example from the calling dataset and return the tensors of the returned dataset.</p>\n\n<p>e.g.</p>\n\n<pre><code>def map_fn(example_proto):\n  features, labels = parse_example_proto(example_proto)\n  # do data augmentation here\n  return features, labels\n\ndataset = tf.data.TfRecordsDataset(filenames)\ndataset = dataset.repeat().shuffle().map(\n  map_fn, num_parallel_calls=8).prefetch(1)\nfeatures, labels = dataset.make_one_shot_iterator().get_next()\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1528789204}]","{58752538, 63926554}","['<p>The error happens because a <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer"">tf.Dataset</a> is provided to the argument <code>validation_data</code> of <a href=""https://keras.io/models/model/#fit"" rel=""noreferrer"">Model.fit</a>, but Keras does not know how many steps to validate for. To solve this problem, you can just set the argument <code>validation_steps</code>. For example:</p>\n\n<pre><code>model.fit(train_data,\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n    validation_data=(validation_inputs, validation_targets),\n    validation_steps=10)\n</code></pre>\n', ""<p>The error is just that <code>tokenizer.tokenize</code> expects a string and you're giving it a list. This simple edit will work. I just made a loop that gives all strings to the tokenizer instead of giving it a list of strings.</p>\n<pre><code>dataset = tf.data.experimental.make_csv_dataset(\n    'test.csv',\n    batch_size=2,\n    label_name='target',\n    num_epochs=1)\n\ntokenizer = tfds.features.text.Tokenizer()\n\nlowercase = True\nvocabulary = Counter()\nfor features, _ in dataset:\n    text = features['text']\n    if lowercase:\n        text = tf.strings.lower(text)\n    for t in text:\n        tokens = tokenizer.tokenize(t.numpy())\n        vocabulary.update(tokens)\n</code></pre>\n"", '<p>The error happens because a <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer"">tf.Dataset</a> is provided to the argument <code>validation_data</code> of <a href=""https://keras.io/models/model/#fit"" rel=""noreferrer"">Model.fit</a>, but Keras does not know how many steps to validate for. To solve this problem, you can just set the argument <code>validation_steps</code>.']","{'https://stackoverflow.com/questions/58752538/tensorflow-batch-size-or-steps-is-required-for-tensor-or-numpy-input-da/58755261#58755261', 'https://stackoverflow.com/questions/63926554/referencing-and-tokenizing-single-feature-column-in-multi-feature-tensorflow-dat/63992841#63992841'}",,0.23543950920984905,0.19838626665015902
7,52572275,tf.scatter_nd,Documentation Replication on Other Examples,tensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)?,"<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/manip/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd documentation</a> and run the example code for 1D and 3D tensors... and now I'm trying to do it for a 2D tensor.  I want to 'interleave' the columns of two tensors.  For 1D tensors, one can do this via</p>

<pre><code>'''
We want to interleave elements of 1D tensors arr1 and arr2, where
arr1 = [10, 11, 12]
arr2 = [1, 2, 3, 4, 5, 6]
such that
desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12]
'''

import tensorflow as tf

with tf.Session() as sess:

    updates1 = tf.constant([1,2,3,4,5,6])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)

    updates2 = tf.constant([10,11,12])
    indices2 = tf.constant([[2], [5], [8]])
    scatter2 = tf.scatter_nd(indices2, updates2, shape)

    result = scatter1 + scatter2

    print(sess.run(result))
</code></pre>

<p>(aside: is there a <em>better</em> way to do this?  I'm all ears.)</p>

<p>This gives the output</p>

<p><code>[ 1  2 10  3  4 11  5  6 12]</code></p>

<p>Yay! that worked!</p>

<p>Now lets' try to extend this to 2D.</p>

<pre><code>    '''
    We want to interleave the *columns* (not rows; rows would be easy!) of

    arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]
    arr2 = [[10 11 12], [10 11 12], [10 11 12]]
    such that
    desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]]
    '''

    updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([3, 9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)
</code></pre>

<p>This gives the error
<code>ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1
dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but
are 6 and 3. Shapes are [6] and [3]. for 'ScatterNd_2' (op: 'ScatterNd') with
input shapes: [6,1], [3,6], [2].</code></p>

<p>Seems like my <code>indices</code> is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean
I need to <em>explicitly</em> specify every single pair of indices for every element in <code>updates1</code>?
Or is there some kind of 'wildcard' specification I can use for the rows? (Note <code>indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]])</code> gives syntax errors, as it probably should.)</p>

<p>Would it be easier to just do a transpose, interleave the rows, then transpose back?
Because I tried that...</p>

<pre><code>scatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))
print(sess.run(tf.transpose(scatter1)))
</code></pre>

<p>...and got a <em>much</em> longer error message, that I don't feel like posting unless someone requests it. </p>

<p>PS- I searched to make sure this isn't a duplicate -- I find it hard to imagine that someone else hasn't asked this before -- but turned up nothing. </p>
","<p>This is pure slicing but I didn't know that syntax like <code>arr1[0:,:][:,:2]</code> actually works. It seems it does but not sure if it is better.</p>

<p>This may be the <em>wildcard</em> slicing mechanism you are looking for.</p>

<pre><code>arr1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,7],[1,2,3,4,5,8]])
arr2 = tf.constant([[10, 11, 12], [10, 11, 12], [10, 11, 12]])

with tf.Session() as sess :
    sess.run( tf.global_variables_initializer() )
    print(sess.run(tf.concat([arr1[0:,:][:,:2], arr2[0:,:] [:,:1],
                              arr1[0:,:][:,2:4],arr2[0:, :][:, 1:2],
                              arr1[0:,:][:,4:6],arr2[0:, :][:, 2:3]],axis=1)))
</code></pre>

<p>Output is</p>

<pre><code>[[ 1  2 10  3  4 11  5  6 12]
 [ 1  2 10  3  4 11  5  7 12]
 [ 1  2 10  3  4 11  5  8 12]]
</code></pre>

<p>So, for example,</p>

<p><code>arr1[0:,:]</code> returns</p>

<pre><code>[[1 2 3 4 5 6]
 [1 2 3 4 5 7]
 [1 2 3 4 5 8]]
</code></pre>

<p>and <code>arr1[0:,:][:,:2]</code> returns the first two columns</p>

<pre><code>[[1 2]
 [1 2]
 [1 2]]
</code></pre>

<p>axis is 1.</p>
","{42207554, 56888197, 56969703, 40320585, 52872239, 52194706, 72120212, 54930932, 45162998, 47343228}","[{'QuestionId': 72120212, 'AnswerId': 72122326, 'URL': 'https://stackoverflow.com/questions/72120212/with-tensorflow-how-to-combine-two-arrays-tensors-with-interchanging-index-from/72122326#72122326', 'QuestionTitle': 'With Tensorflow, How to combine two arrays/tensors with interchanging index from each array?', 'Answer': '<p>You could also try using <code>tf.tensor_scatter_nd_update</code>:</p>\n<pre><code>import tensorflow as tf\n\na = tf.constant([1,2,3])\nb = tf.constant([10,20,30])\nshape = tf.shape(a)[0] + tf.shape(b)[0]\n\nc = tf.tensor_scatter_nd_update(tf.zeros(shape, dtype=tf.int32), \n                            tf.expand_dims(tf.concat([tf.range(start=0, limit=shape, delta=2), tf.range(start=1, limit=shape, delta=2) ], axis=0), axis=-1), \n                            tf.concat([a, b], axis=0))\n\n# tf.Tensor([ 1 10  2 20  3 30], shape=(6,), dtype=int32)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1651728566}, {'QuestionId': 72120212, 'AnswerId': 72121557, 'URL': 'https://stackoverflow.com/questions/72120212/with-tensorflow-how-to-combine-two-arrays-tensors-with-interchanging-index-from/72121557#72121557', 'QuestionTitle': 'With Tensorflow, How to combine two arrays/tensors with interchanging index from each array?', 'Answer': '<pre><code>a = tf.constant([1,2,3])\nb = tf.constant([10,20,30])\nc = tf.stack([a,b]) #combine a,b as a matrix\nd = tf.transpose(c) #transpose matrix to get the right order\ne = tf.reshape(d, [-1]) #reshape to 1-d tensor\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1651720902}, {'QuestionId': 56888197, 'AnswerId': 71304607, 'URL': 'https://stackoverflow.com/questions/56888197/how-can-i-merge-two-3d-tensors-by-interleaving-them-along-a-certain-axis/71304607#71304607', 'QuestionTitle': 'How can I merge two 3D tensors by interleaving them along a certain axis?', 'Answer': ""<p>Here's a simpler way to do interleaving I discovered:</p>\n<pre><code>import tensorflow as tf\na = tf.constant([1,2,3])\nb = tf.constant([9,8,7])\n\n# to interleave on axis N, stack on axis N+1\nc = tf.stack([a,b], axis=1)\n# reshape to remove the temp N+1 axis\nc = tf.reshape(c, (-1,))\n\nprint(c)\n# Output: [1, 9, 2, 8, 3, 7]\n</code></pre>\n<p>For OP's example, <code>tf.stack</code> won't work out-of-the-box, since A has 3 elements to interleave, but B has only 2 elements. We can get around this easily by padding B up to 3 elements:</p>\n<pre><code>import tensorflow as tf\na = tf.constant([\n    [[1,1],[2,2],[3,3]],\n    [[4,4],[5,5],[6,6]],\n    [[7,7],[8,8],[9,9]]\n])\nb = tf.constant([\n    [[10,10],[11,11]],\n    [[12,12],[13,13]],\n    [[14,14],[15,15]]\n])\n\n# pad b to match a\nb = tf.pad(b, [[0,0],[0,1],[0,0]])\n# interleave on axis 1\nc = tf.stack([a,b], axis=2)\nc = tf.reshape(c, (a.shape[0],-1,a.shape[-1]))\n# discard the padding elements\nc = c[:,:-1]\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1646116674}, {'QuestionId': 47343228, 'AnswerId': 66278266, 'URL': 'https://stackoverflow.com/questions/47343228/interleaving-tf-data-datasets/66278266#66278266', 'QuestionTitle': 'Interleaving tf.data.Datasets', 'Answer': '<p>Pavel\'s answer works great if you don\'t mind the order of interleaving. If you <em>do</em> care...</p>\n<h3>Option 1</h3>\n<p>A variant on mrry\'s answer that works with an arbitrary number of input datasets:</p>\n<pre class=""lang-py prettyprint-override""><code>ds0 = tf.data.Dataset.range(0, 10, 3)\nds1 = tf.data.Dataset.range(1, 10, 3)\nds2 = tf.data.Dataset.range(2, 10, 3)\ndatasets = (ds0, ds1, ds2)\n\n# Note: `datasets` should be a *tuple*, not a list.\ndatasets_zipped = tf.data.Dataset.zip(datasets)\n# Each element of the dataset will now be a tuple, e.g. (0, 1, 2).\ndatasets_zipped_tensor = datasets_zipped.map(lambda *args: tf.stack(args))\n# Each element will now be a Tensor, e.g. Tensor([0, 1, 2]).\ndatasets_interleaved = datasets_zipped_tensor.unbatch()\n</code></pre>\n<p>However, note that because of the way <code>zip</code> works, the dataset this produces <strong>limited by the length of the shortest input dataset</strong>. For example, using the above code with</p>\n<pre class=""lang-py prettyprint-override""><code>datasets = [\n    tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5]),\n    tf.data.Dataset.from_tensor_slices([10, 20]),\n)\n</code></pre>\n<p>would yield a dataset comprising just <code>[1, 10, 2, 20]</code>.</p>\n<h3>Option 2</h3>\n<p><code>Dataset.interleave</code> doesn\'t suffer from this problem. In <em>some</em> cases you can use <code>interleave</code> with:</p>\n<pre class=""lang-py prettyprint-override""><code># Note: `datasets` should be a *list*, not a tuple\ntf.data.Dataset.from_tensor_slices(datasets).interleave(lambda x: x)\n</code></pre>\n<p>But this doesn\'t seem to work for all kinds of dataset; <strong>calling <code>from_tensor_slices</code> on your datasets may not work</strong>.</p>\n<h3>Option 3</h3>\n<p>If option 2 doesn\'t work, you might be able to use <code>interleave</code> at an earlier stage of your dataset pipeline. For example, you might be able to change from calling <code>interleave</code> on pre-existing datasets to calling <code>interleave</code> on the file names from which the individual datasets were created:</p>\n<pre class=""lang-py prettyprint-override""><code>filenames = [\'foo\', \'bar\']\nfilesnames_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n\ndef read_dataset(filename): ...\n\ninterleaved_dataset = filenames_dataset.interleave(read_dataset)\n</code></pre>\n<p>But <strong>this will only work if your <code>read_dataset</code> function accepts a <code>Tensor</code> argument</strong>.</p>\n<h3>Option 4</h3>\n<p>If none of the other options work for you, I think the only solution is to implement the interleaving yourself, with something like:</p>\n<pre class=""lang-py prettyprint-override""><code>element_spec = datasets[0].element_spec\nassert all(dataset.element_spec == element_spec for dataset in datasets)\n\ndef interleave_generator():\n  iters_not_exhausted = [iter(dataset) for dataset in datasets]\n  while iters_not_exhausted:\n    for dataset_iter in iters_not_exhausted:\n      try:\n        x = next(dataset_iter)\n      except StopIteration:\n        iters_not_exhausted.remove(dataset_iter)\n      else:\n        yield x\n\ndatasets_interleaved = tf.data.Dataset.from_generator(\n    interleave_generator,\n    output_signature=element_spec,\n)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1613740562}, {'QuestionId': 45162998, 'AnswerId': 64658190, 'URL': 'https://stackoverflow.com/questions/45162998/proper-usage-of-tf-scatter-nd-in-tensorflow-r1-2/64658190#64658190', 'QuestionTitle': 'Proper usage of `tf.scatter_nd` in tensorflow-r1.2', 'Answer': '<p>I think you might be looking for this.</p>\n<pre><code>def permute_batched_tensor(batched_x, batched_perm_ids):\n    indices = tf.tile(tf.expand_dims(batched_perm_ids, 2), [1,1,batched_x.shape[2]])\n\n    # Create additional indices\n    i1, i2 = tf.meshgrid(tf.range(batched_x.shape[0]),\n                     tf.range(batched_x.shape[2]), indexing=&quot;ij&quot;)\n    i1 = tf.tile(i1[:, tf.newaxis, :], [1, batched_x.shape[1], 1])\n    i2 = tf.tile(i2[:, tf.newaxis, :], [1, batched_x.shape[1], 1])\n    # Create final indices\n    idx = tf.stack([i1, indices, i2], axis=-1)\n    temp = tf.scatter_nd(idx, batched_x, batched_x.shape)\n    return temp\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1604387284}, {'QuestionId': 52872239, 'AnswerId': 61272034, 'URL': 'https://stackoverflow.com/questions/52872239/can-tf-scatter-update-or-tf-scatter-nd-update-be-used-to-update-column-slices-of/61272034#61272034', 'QuestionTitle': 'Can tf.scatter_update or tf.scatter_nd_update be used to update column slices of a tensor?', 'Answer': '<p>Refer to the Tensorflow2 documentation for tf.Variable</p>\n\n<blockquote>\n  <p><code>__getitem__</code>(\n      var, slice_spec )</p>\n  \n  <p>Creates a slice helper object given a variable.</p>\n  \n  <p>This allows creating a sub-tensor from part of the current contents of\n  a variable. See tf.Tensor.getitem for detailed examples of slicing.</p>\n  \n  <p><strong>This function in addition also allows assignment to a sliced range</strong>.\n  This is similar to <code>__setitem__</code> functionality in Python. However, the\n  syntax is different so that the user can capture the assignment\n  operation for grouping or passing to sess.run(). For example,</p>\n  \n  <p>...</p>\n</blockquote>\n\n<p>Here is a minimal working example:</p>\n\n<pre class=""lang-python3 prettyprint-override""><code>import tensorflow as tf\nimport numpy as np\nvar = tf.Variable(np.random.rand(3,3,3))\nprint(var)\n# update the last column of the three (3x3) matrices to random integer values\n# note that the update values needs to have the same shape\n# as broadcasting is not supported as of TF2\nvar[:,:,2].assign(np.random.randint(10,size=(3,3)))\nprint(var)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1587127564}, {'QuestionId': 47343228, 'AnswerId': 60203823, 'URL': 'https://stackoverflow.com/questions/47343228/interleaving-tf-data-datasets/60203823#60203823', 'QuestionTitle': 'Interleaving tf.data.Datasets', 'Answer': '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/sample_from_datasets"" rel=""noreferrer"">tf.data.experimental.sample_from_datasets</a> method could be also useful if you do not need to preserve the strict order for the items you want to interleave. </p>\n\n<p>In my case I had to interleave a real life data with some synthetic data, so the order was not an issue for me. Then this can be easily done as follows</p>\n\n<pre class=""lang-python prettyprint-override""><code>dataset = tf.data.experimental.sample_from_datasets([ds0, ds1])\n</code></pre>\n\n<p>Note that the result will be <strong>non-deterministic</strong> and some items could be taken from same dataset twice, but in general it will be very similar to regular interleave.</p>\n\n<p>The advantages of this approach:</p>\n\n<ul>\n<li>you can use multiple datasets in one method call</li>\n<li>you can specify fraction of the samples for each dataset using <code>weights</code> parameter (e.g. I wanted to have only small fraction of the data to be generated so I used <code>weights=[0.9, 0.1]</code>)</li>\n</ul>\n', 'IsAccepted': False, 'CreationDate': 1581584219}, {'QuestionId': 56969703, 'AnswerId': 56988219, 'URL': 'https://stackoverflow.com/questions/56969703/how-to-use-tf-scatter-nd-with-multi-dimensional-tensors/56988219#56988219', 'QuestionTitle': 'How to use `tf.scatter_nd` with multi-dimensional tensors', 'Answer': '<p>I\'ve been playing around with the function and I have found my mistake. If anyone is facing this problem, this is what I did to solve it:</p>\n\n<p>Considering <code>batch_size=2</code> and <code>3</code> points, <code>idx</code> tensor must have shape <code>[2, 3, 4]</code>, where first dimension correspond to the batch from where we are taking <code>update</code>value, second dimension must be equal to the second dimension of <code>updates</code> (number of points per batch) and the third dimension is <code>4</code> because we need <code>4</code> indices: [batch_number, channel, row, col]. Following the example in the question:</p>\n\n<pre class=""lang-py prettyprint-override""><code>updates = tf.constant([[1., 2., 3.], [4., 5., 6.]])  # [2, 3]\nidx = tf.constant([[[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 1, 0]], [[1, 0, 1, 1], [1, 0, 0, 0], [1, 0, 1, 0]]])  # [2, 3, 4]\noutput = tf.scatter_nd(idx, updates, [2, 1, 4, 4])\n\nsess = tf.Session()\nprint(sess.run(output))\n\n[[[[2. 1. 0. 0.]\n   [3. 0. 0. 0.]\n   [0. 0. 0. 0.]\n   [0. 0. 0. 0.]]]\n\n\n [[[5. 0. 0. 0.]\n   [6. 4. 0. 0.]\n   [0. 0. 0. 0.]\n   [0. 0. 0. 0.]]]]\n\n</code></pre>\n\n<p>This way it\'s possible to place specific numbers in a new tensor.</p>\n', 'IsAccepted': True, 'CreationDate': 1562844748}, {'QuestionId': 56888197, 'AnswerId': 56897751, 'URL': 'https://stackoverflow.com/questions/56888197/how-can-i-merge-two-3d-tensors-by-interleaving-them-along-a-certain-axis/56897751#56897751', 'QuestionTitle': 'How can I merge two 3D tensors by interleaving them along a certain axis?', 'Answer': '<p>You can first  reorder the index of the column.</p>\n\n<pre><code>import tensorflow as tf\n\na = [[[1,1],[2,2],[3,3]],\n     [[4,4],[5,5],[6,6]],\n     [[7,7],[8,8],[9,9]]]\n\nb = [[[10,10],[11,11]],\n     [[12,12],[13,13]],\n     [[14,14],[15,15]]]\n\na_tf = tf.constant(a)\nb_tf = tf.constant(b)\n\na_tf_column = tf.range(a_tf.shape[1])*2\n# [0 2 4]\nb_tf_column = tf.range(b_tf.shape[1])*2+1\n# [1 3]\n\ncolumn_indices = tf.concat([a_tf_column,b_tf_column],axis=-1)\n# Before TF v1.13\ncolumn_indices = tf.contrib.framework.argsort(column_indices)\n## From TF v1.13\n# column_indices = tf.argsort(column_indices)\n\n# [0 3 1 4 2]\n</code></pre>\n\n<p>Then you should create new indices for <code>tf.gather_nd()</code>.</p>\n\n<pre><code>column,row = tf.meshgrid(column_indices,tf.range(a_tf.shape[0]))\ncombine_indices = tf.stack([row,column],axis=-1)\n# [[[0,0],[0,3],[0,1],[0,4],[0,2]],\n#  [[1,0],[1,3],[1,1],[1,4],[1,2]],\n#  [[2,0],[2,3],[2,1],[2,4],[2,2]]]\n</code></pre>\n\n<p>Finally you should concat the value of <code>a</code> and <code>b</code> and use <code>tf.gather_nd()</code> to get the result.</p>\n\n<pre><code>combine_value = tf.concat([a_tf,b_tf],axis=1)\nresult = tf.gather_nd(combine_value,combine_indices)\n\nwith tf.Session() as sess:\n    print(sess.run(result))\n\n# [[[1,1],[10,10],[2,2],[11,11],[3,3]],\n#  [[4,4],[12,12],[5,5],[13,13],[6,6]],\n#  [[7,7],[14,14],[8,8],[15,15],[9,9]]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1562308675}, {'QuestionId': 54930932, 'AnswerId': 55278194, 'URL': 'https://stackoverflow.com/questions/54930932/tensorflow-an-alternative-to-tf-scatter-update/55278194#55278194', 'QuestionTitle': 'TensorFlow: An alternative to tf.scatter_update', 'Answer': ""<p>I will add here also my solution. This utility function works pretty much the same as <code>scatter_update</code>, but without using Variables:</p>\n\n<pre><code>def scatter_update_tensor(x, indices, updates):                                                                                                                                                                                                                       \n    '''                                                                                                                                                                                                                                                               \n    Utility function similar to `tf.scatter_update`, but performing on Tensor                                                                                                                                                                                         \n    '''                                                                                                                                                                                                                                                               \n    x_shape = tf.shape(x)                                                                                                                                                                                                                                             \n    patch = tf.scatter_nd(indices, updates, x_shape)                                                                                                                                                                                                                  \n    mask = tf.greater(tf.scatter_nd(indices, tf.ones_like(updates), x_shape), 0)                                                                                                                                                                                      \n    return tf.where(mask, patch, x)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1553163543}, {'QuestionId': 54930932, 'AnswerId': 54936423, 'URL': 'https://stackoverflow.com/questions/54930932/tensorflow-an-alternative-to-tf-scatter-update/54936423#54936423', 'QuestionTitle': 'TensorFlow: An alternative to tf.scatter_update', 'Answer': '<p>This one should work. A bit twisted, but no variable used.</p>\n\n<pre><code>import tensorflow as tf\ntemplate = tf.convert_to_tensor([[1, 1, 0.5, 0.5, 0.3, 0.3],                                                                       \n                                 [2, 2, 0.75, 0.5, 0.3, 0.3],                                                                      \n                                 [3, 3, 0.5, 0.75, 0.3, 0.3],                                                                      \n                                 [4, 4, 0.75, 0.75, 0.3, 0.3]])   \n\npatch = tf.convert_to_tensor([[1, 1, 1, 0.17, 0.4, 0.4],                                                                           \n                              [3, 3, 3, 0.22, 0.53, 0.6]])\n\nind = tf.constant([1,3])\nrn_t = tf.range(0, template.shape[0])\n\ndef index1d(t, val):\n    return tf.reduce_min(tf.where(tf.equal([t], val)))\n\ndef index1dd(t,val):\n    return tf.argmax(tf.cast(tf.equal(t,val), tf.int64), axis=0)\n\nr = tf.map_fn(lambda x: tf.where(tf.equal(index1d(ind, x), 0), patch[index1dd(ind, x)] , template[x]), rn_t, dtype=tf.float32)\n\nwith tf.Session() as sess:\n   print(sess.run([r]))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1551401344}, {'QuestionId': 52872239, 'AnswerId': 52872757, 'URL': 'https://stackoverflow.com/questions/52872239/can-tf-scatter-update-or-tf-scatter-nd-update-be-used-to-update-column-slices-of/52872757#52872757', 'QuestionTitle': 'Can tf.scatter_update or tf.scatter_nd_update be used to update column slices of a tensor?', 'Answer': '<p>Here is a small demonstration of how to update rows or columns. The idea is that you specify the row and column indices of the variables where you want each element in the update to end up. That is easy to do with <a href=""https://www.tensorflow.org/api_docs/python/tf/meshgrid"" rel=""nofollow noreferrer""><code>tf.meshgrid</code></a>.</p>\n\n<pre><code>import tensorflow as tf\n\nvar = tf.get_variable(\'var\', [4, 3], tf.float32, initializer=tf.zeros_initializer())\nupdates = tf.placeholder(tf.float32, [None, None])\nindices = tf.placeholder(tf.int32, [None])\n# Update rows\nvar_update_rows = tf.scatter_update(var, indices, updates)\n# Update columns\ncol_indices_nd = tf.stack(tf.meshgrid(tf.range(tf.shape(var)[0]), indices, indexing=\'ij\'), axis=-1)\nvar_update_cols = tf.scatter_nd_update(var, col_indices_nd, updates)\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    print(\'Rows updated:\')\n    print(sess.run(var_update_rows, feed_dict={updates: [[1, 2, 3], [4, 5, 6]], indices: [3, 1]}))\n    print(\'Columns updated:\')\n    print(sess.run(var_update_cols, feed_dict={updates: [[1, 5], [2, 6], [3, 7], [4, 8]], indices: [0, 2]}))\n</code></pre>\n\n<p>Output:</p>\n\n<pre class=""lang-none prettyprint-override""><code>Rows updated:\n[[0. 0. 0.]\n [4. 5. 6.]\n [0. 0. 0.]\n [1. 2. 3.]]\nColumns updated:\n[[1. 0. 5.]\n [2. 5. 6.]\n [3. 0. 7.]\n [4. 2. 8.]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1539861397}, {'QuestionId': 52194706, 'AnswerId': 52200014, 'URL': 'https://stackoverflow.com/questions/52194706/fast-and-efficient-interleaving-of-tensors-in-tensorflow/52200014#52200014', 'QuestionTitle': 'Fast and efficient interleaving of tensors in TensorFlow', 'Answer': ""<p>My attempt is this. If I assume the given shapes then this produces the desired output. </p>\n\n<pre><code>A = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nB = tf.constant([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\nC = tf.constant([[19, 20, 21], [22, 23, 24], [25, 26, 27]])\n\nwith tf.Session() as sess :\n\n    sess.run(tf.global_variables_initializer())\n\n    ODD = tf.concat( [A[0::2],B[0::2],C[0::2]], axis=0)\n\n    EVENANDODD = tf.concat([ODD[0::2],\n                           tf.concat([A[1::2], B[1::2], C[1::2]], axis=0)], axis=0)\n\n    FINAL = tf.concat([EVENANDODD,\n                      ODD[1::2]], axis=0)\n\n\n    print( sess.run(FINAL) )\n</code></pre>\n\n<blockquote>\n  <p>[[ 1  2  3]\n  [10 11 12]\n  [19 20 21]\n  [ 4  5  6]\n  [13 14 15]\n  [22 23 24]\n  [ 7  8  9]\n  [16 17 18]\n  [25 26 27]]</p>\n</blockquote>\n\n<p>Note : I couldn't address the backprop and performance points.</p>\n"", 'IsAccepted': True, 'CreationDate': 1536223422}, {'QuestionId': 52194706, 'AnswerId': 52195797, 'URL': 'https://stackoverflow.com/questions/52194706/fast-and-efficient-interleaving-of-tensors-in-tensorflow/52195797#52195797', 'QuestionTitle': 'Fast and efficient interleaving of tensors in TensorFlow', 'Answer': '<p>In your specific case, A, B, C are 3x3 matrices and you wanna get a 9x3 matrix. The most intuitive way is to concatenate them. Below I show it with numpy.</p>\n\n<pre><code>import numpy as np\n\na = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\nc = np.array([[19, 20, 21], [22, 23, 24], [25, 26, 27]])\n\nabc = np.concatenate((a, b, c), axis=0)\nabc = abc.reshape((3, 3, 3))\nabc = abc.transpose((1, 0, 2))\nabc = abc.reshape((9, 3))  #  this gives your objective\n</code></pre>\n\n<p>However, in general, you may wanna stack irregular rows from each matrix, e.g, [A0, B2, A2, A1, C1, C2, B0, C0, B1]. <code>tf.gather</code> is an option.</p>\n\n<pre><code>import tensorflow as tf\n\na = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = tf.constant([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\nc = tf.constant([[19, 20, 21], [22, 23, 24], [25, 26, 27]])\n\nabc = tf.concat((a, b, c), axis=0)\nabc = tf.gather(abc, [0, 5, 2, 1, 7, 8, 3, 5, 6, 4], axis=0)\nsess = tf.InteractiveSession()\nprint(sess.run(abc))\n</code></pre>\n\n<p>If not mistaken, in tf.gather, backprop can be done w.r.t. elements of a, b, c (it is just 1 or 0), but not w.r.t. the index (in this case, <code>[0, 5, 2, 1, 7, 8, 3, 5, 6, 4]</code>). </p>\n', 'IsAccepted': False, 'CreationDate': 1536202031}, {'QuestionId': 40320585, 'AnswerId': 50207625, 'URL': 'https://stackoverflow.com/questions/40320585/use-tf-scatter-update-in-a-two-dimensional-tf-variable/50207625#50207625', 'QuestionTitle': 'Use tf.scatter_update in a two dimensional tf.Variable', 'Answer': '<p>I found something <a href=""https://github.com/tensorflow/tensorflow/issues/4638"" rel=""nofollow noreferrer"">here</a>\nI made a variable name U = [[1, 2, 3], [4, 5, 6]] and wanted to update it like as U[:,1] = [2, 3] so I did U[:,1].assign(cast_into_tensor[2,3])</p>\n\n<p>here a simple code </p>\n\n<pre><code>x = tf.Variable([[1,2,3],[4,5,6]])\nprint K.eval(x)\ny = [0, 0]\nwith tf.control_dependencies([x[:,1].assign(y)]):\n    x = tf.identity(x)\nprint K.eval(x)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1525671339}, {'QuestionId': 42207554, 'AnswerId': 48019530, 'URL': 'https://stackoverflow.com/questions/42207554/swapping-elements-within-a-matrix-rows-and-columns-tensorflow-scatter-nd/48019530#48019530', 'QuestionTitle': 'Swapping elements within a matrix rows and columns - TensorFlow scatter_nd', 'Answer': '<p>Suppose you want to swap elements in the second dimension either keeping the first dimension order or not. </p>\n\n<pre><code>import tensorflow as tf\nsess = tf.InteractiveSession()\n\n\ndef prepare_fd(fd_indices, sd_dims):\n    fd_indices = tf.expand_dims(fd_indices, 1)\n    fd_indices = tf.tile(fd_indices, [1, sd_dims])\n    return fd_indices\n\n# define the updates\nupdates = tf.constant([[11, 12, 13, 14],\n                       [21, 22, 23, 24],\n                       [31, 32, 33, 34]])\nsd_dims = tf.shape(updates)[1]\n\nsd_indices = tf.constant([[1, 0, 2, 3], [0, 2, 1, 3], [0, 1, 3, 2]])\nfd_indices_range = tf.range(0, limit=tf.shape(updates)[0])\nfd_indices_custom = tf.constant([2, 0, 1])\n\n# define the indices\nindices1 = tf.stack((prepare_fd(fd_indices_range, sd_dims), sd_indices), axis=2)\nindices2 = tf.stack((prepare_fd(fd_indices_custom, sd_dims), sd_indices), axis=2)\n\n# define the shape\nshape = tf.shape(updates)\n\nscatter1 = tf.scatter_nd(indices1, updates, shape)\nscatter2 = tf.scatter_nd(indices2, updates, shape)\n\nprint(scatter1.eval())\n\n# array([[12, 11, 13, 14],\n#        [21, 23, 22, 24],\n#        [31, 32, 34, 33]], dtype=int32)\n\nprint(scatter2.eval())\n\n# array([[21, 23, 22, 24],\n#        [31, 32, 34, 33],\n#        [12, 11, 13, 14]], dtype=int32)\n</code></pre>\n\n<p>May this example help. </p>\n', 'IsAccepted': False, 'CreationDate': 1514535165}, {'QuestionId': 47343228, 'AnswerId': 47344405, 'URL': 'https://stackoverflow.com/questions/47343228/interleaving-tf-data-datasets/47344405#47344405', 'QuestionTitle': 'Interleaving tf.data.Datasets', 'Answer': '<p>MattScarpino is on the right track in <a href=""https://stackoverflow.com/questions/47343228/interleaving-tf-data-datasets#comment81639602_47343228"">his comment</a>. You can use <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip"" rel=""noreferrer""><code>Dataset.zip()</code></a> along with <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map"" rel=""noreferrer""><code>Dataset.flat_map()</code></a> to flatten a multi-element dataset:</p>\n\n<pre><code>ds0 = tf.data.Dataset.range(0, 10, 2)\nds1 = tf.data.Dataset.range(1, 10, 2)\n\n# Zip combines an element from each input into a single element, and flat_map\n# enables you to map the combined element into two elements, then flattens the\n# result.\ndataset = tf.data.Dataset.zip((ds0, ds1)).flat_map(\n    lambda x0, x1: tf.data.Dataset.from_tensors(x0).concatenate(\n        tf.data.Dataset.from_tensors(x1)))\n\niter = dataset.make_one_shot_iterator()\nval = iter.get_next()\n</code></pre>\n\n<p>Having said this, your intuition about using <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave"" rel=""noreferrer""><code>Dataset.interleave()</code></a> is pretty sensible. We\'re investigating ways that you can do this more easily.</p>\n\n<hr>\n\n<p>PS. As an alternative, you <em>can</em> use <code>Dataset.interleave()</code> to solve the problem if you change how <code>ds0</code> and <code>ds1</code> are defined:</p>\n\n<pre><code>dataset = tf.data.Dataset.range(2).interleave(\n    lambda x: tf.data.Dataset.range(x, 10, 2), cycle_length=2, block_length=1)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1510899459}, {'QuestionId': 45162998, 'AnswerId': 45164376, 'URL': 'https://stackoverflow.com/questions/45162998/proper-usage-of-tf-scatter-nd-in-tensorflow-r1-2/45164376#45164376', 'QuestionTitle': 'Proper usage of `tf.scatter_nd` in tensorflow-r1.2', 'Answer': '<p>So assuming you have:</p>\n\n<ul>\n<li>A tensor <code>updates</code> with shape <code>[batch_size, sequence_len, sampled_size]</code>.</li>\n<li>A tensor <code>indices</code> with shape <code>[batch_size, sequence_len, sampled_size]</code>.</li>\n</ul>\n\n<p>Then you do:</p>\n\n<pre><code>import tensorflow as tf\n\n# Create updates and indices...\n\n# Create additional indices\ni1, i2 = tf.meshgrid(tf.range(batch_size),\n                     tf.range(sequence_len), indexing=""ij"")\ni1 = tf.tile(i1[:, :, tf.newaxis], [1, 1, sampled_size])\ni2 = tf.tile(i2[:, :, tf.newaxis], [1, 1, sampled_size])\n# Create final indices\nidx = tf.stack([i1, i2, indices], axis=-1)\n# Output shape\nto_shape = [batch_size, sequence_len, vocab_size]\n# Get scattered tensor\noutput = tf.scatter_nd(idx, updates, to_shape)\n</code></pre>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_nd"" rel=""noreferrer""><code>tf.scatter_nd</code></a> takes an <code>indices</code> tensor, an <code>updates</code> tensor and some shape. <code>updates</code> is the original tensor, and the shape is just the desired output shape, so <code>[batch_size, sequence_len, vocab_size]</code>. Now, <code>indices</code> is more complicated. Since your output has 3 dimensions (rank 3), for each of the elements in <code>updates</code> you need 3 indices to determine where in the output each element is going to be placed. So the shape of the <code>indices</code> parameter should be the same as <code>updates</code> with an additional dimension of size 3. In this case, we want the first to dimensions to be the same, but we still have to specify the 3 indices. So we use <a href=""https://www.tensorflow.org/api_docs/python/tf/meshgrid"" rel=""noreferrer""><code>tf.meshgrid</code></a> to generate the indices that we need and we tile them along the third dimension (the first and second index for each element vector in the last dimension of <code>updates</code> is the same). Finally, we stack these indices with the previously created mapping indices and we have our full 3-dimensional indices.</p>\n', 'IsAccepted': False, 'CreationDate': 1500374258}, {'QuestionId': 42207554, 'AnswerId': 43953233, 'URL': 'https://stackoverflow.com/questions/42207554/swapping-elements-within-a-matrix-rows-and-columns-tensorflow-scatter-nd/43953233#43953233', 'QuestionTitle': 'Swapping elements within a matrix rows and columns - TensorFlow scatter_nd', 'Answer': '<p>The following swaps the elements of each row of each row using scatter_nd</p>\n\n<pre><code>indices = tf.constant([[[0, 1], [0, 0], [0, 2], [0, 3]], \n                      [[1, 1], [1, 0], [1, 2], [1, 3]]])\nupdates = tf.constant([ [5, 6, 7, 8],\n                        [1, 2, 3, 4] ])\nshape = tf.constant([2, 4])\nscatter1 = tf.scatter_nd(indices, updates, shape)\nwith tf.Session() as sess:\n    print(sess.run(scatter1))\n</code></pre>\n\n<p>Giving an output of:<br>\n<code>[[6 5 7 8]\n [2 1 3 4]]\n</code></p>\n\n<p>The locations of the coordinate in <code>indices</code> define where the values are being taken from in <code>updates</code> and the actual cordinates define where the values will be placed in <code>scatter1</code>. </p>\n\n<p>This answer is a few months late but hopefully still helpful. </p>\n', 'IsAccepted': False, 'CreationDate': 1494678965}, {'QuestionId': 40320585, 'AnswerId': 40321883, 'URL': 'https://stackoverflow.com/questions/40320585/use-tf-scatter-update-in-a-two-dimensional-tf-variable/40321883#40321883', 'QuestionTitle': 'Use tf.scatter_update in a two dimensional tf.Variable', 'Answer': '<p>In tensorflow you cannot update a Tensor but you can update a Variable.</p>\n\n<p>The <code>scatter_update</code> operator can update only the first dimension of the variable.\nYou have to pass always a reference tensor to the scatter update (<code>a</code> instead of <code>a[line]</code>).</p>\n\n<p>This is how you can update the first element of the variable:</p>\n\n<pre><code>import tensorflow as tf\n\ng = tf.Graph()\nwith g.as_default():\n    a = tf.Variable(initial_value=[[0, 0, 0, 0],[0, 0, 0, 0]])\n    b = tf.scatter_update(a, [0, 1], [[1, 0, 0, 0], [1, 0, 0, 0]])\n\nwith tf.Session(graph=g) as sess:\n   sess.run(tf.initialize_all_variables())\n   print sess.run(a)\n   print sess.run(b)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[[0 0 0 0]\n [0 0 0 0]]\n[[1 0 0 0]\n [1 0 0 0]]\n</code></pre>\n\n<p>But having to change again the whole tensor it might be faster to just assign a completely new one.</p>\n', 'IsAccepted': True, 'CreationDate': 1477763205}]","{42159946, 56969703}","['<p>I\'ve been playing around with the function and I have found my mistake. If anyone is facing this problem, this is what I did to solve it:</p>\n\n<p>Considering <code>batch_size=2</code> and <code>3</code> points, <code>idx</code> tensor must have shape <code>[2, 3, 4]</code>, where first dimension correspond to the batch from where we are taking <code>update</code>value, second dimension must be equal to the second dimension of <code>updates</code> (number of points per batch) and the third dimension is <code>4</code> because we need <code>4</code> indices: [batch_number, channel, row, col]. Following the example in the question:</p>\n\n<pre class=""lang-py prettyprint-override""><code>updates = tf.constant([[1., 2., 3.], [4., 5., 6.]])  # [2, 3]\nidx = tf.constant([[[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 1, 0]], [[1, 0, 1, 1], [1, 0, 0, 0], [1, 0, 1, 0]]])  # [2, 3, 4]\noutput = tf.scatter_nd(idx, updates, [2, 1, 4, 4])\n\nsess = tf.Session()\nprint(sess.run(output))\n\n[[[[2. 1.', '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/where"" rel=""noreferrer"">tf.where</a> sounds like what you want: a vectorized selection between Tensors.</p>\n\n<p><code>tf.cond</code> is a control flow modifier: it determines which ops are <em>executed</em>, and so it\'s difficult to think of useful batch semantics.</p>\n\n<p>We can also put together a mixture of these operations: an operation which slices based on a condition and passes those slices to two branches.</p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.util import nest\n\ndef slicing_where(condition, full_input, true_branch, false_branch):\n  """"""Split `full_input` between `true_branch` and `false_branch` on `condition`. Args:\n    condition: A boolean Tensor with shape [B_1, ..., B_N]. full_input: A Tensor or nested tuple of Tensors of any dtype, each with\n      shape [B_1, ..., B_N, ...], to be split between `true_branch` and\n      `false_branch` based on `condition`. true_branch: A function taking a single argument, that argument having the\n      same structure and number of batch dimensions as `full_input`. Receives\n      slices of `full_input` corresponding to the True entries of\n      `condition`. Returns a Tensor or nested tuple of Tensors, each with batch\n      dimensions matching its inputs. false_branch: Like `true_branch`, but receives inputs corresponding to the\n      false elements of `condition`. Returns a Tensor or nested tuple of Tensors\n      (with the same structure as the return value of `true_branch`), but with\n      batch dimensions matching its inputs. Returns:\n    Interleaved outputs from `true_branch` and `false_branch`, each Tensor\n    having shape [B_1, ..., B_N, ...]. """"""\n  full_input_flat = nest.flatten(full_input)\n  true_indices = tf.where(condition)\n  false_indices = tf.where(tf.logical_not(condition))\n  true_branch_inputs = nest.pack_sequence_as(\n      structure=full_input,\n      flat_sequence=[tf.gather_nd(params=input_tensor, indices=true_indices)\n                     for input_tensor in full_input_flat])\n  false_branch_inputs = nest.pack_sequence_as(\n      structure=full_input,\n      flat_sequence=[tf.gather_nd(params=input_tensor, indices=false_indices)\n                     for input_tensor in full_input_flat])\n  true_outputs = true_branch(true_branch_inputs)\n  false_outputs = false_branch(false_branch_inputs)\n  nest.assert_same_structure(true_outputs, false_outputs)\n  def scatter_outputs(true_output, false_output):\n    batch_shape = tf.shape(condition)\n    scattered_shape = tf.concat(\n        [batch_shape, tf.shape(true_output)[tf.rank(batch_shape):]],\n        0)\n    true_scatter = tf.scatter_nd(\n        indices=tf.cast(true_indices, tf.int32),\n        updates=true_output,\n        shape=scattered_shape)\n    false_scatter = tf.scatter_nd(\n        indices=tf.cast(false_indices, tf.int32),\n        updates=false_output,\n        shape=scattered_shape)\n    return true_scatter + false_scatter\n  result = nest.pack_sequence_as(\n      structure=true_outputs,\n      flat_sequence=[\n          scatter_outputs(true_single_output, false_single_output)\n          for true_single_output, false_single_output\n          in zip(nest.flatten(true_outputs), nest.flatten(false_outputs))])\n  return result\n</code></pre>\n\n<p>Some examples:</p>\n\n<pre><code>vector_test = slicing_where(\n    condition=tf.equal(tf.range(10) % 2, 0),\n    full_input=tf.range(10, dtype=tf.float32),\n    true_branch=lambda x: 0.2 + x,\n    false_branch=lambda x: 0.1 + x)\n\ncross_range = (tf.range(10, dtype=tf.float32)[:, None]\n               * tf.range(10, dtype=tf.float32)[None, :])\nmatrix_test = slicing_where(\n    condition=tf.equal(tf.range(10) % 3, 0),\n    full_input=cross_range,\n    true_branch=lambda x: -x,\n    false_branch=lambda x: x + 0.1)\n\nwith tf.Session():\n  print(vector_test.eval())\n  print(matrix_test.eval())\n</code></pre>\n\n<p>Prints:</p>\n\n<pre><code>[ 0.2         1.10000002  2.20000005  3.0999999   4.19999981  5.0999999\n  6.19999981  7.0999999   8.19999981  9.10000038]\n[[  0. 0.', '<p>I\'ve been playing around with the function and I have found my mistake. If anyone is facing this problem, this is what I did to solve it:</p>\n\n<p>Considering <code>batch_size=2</code> and <code>3</code> points, <code>idx</code> tensor must have shape <code>[2, 3, 4]</code>, where first dimension correspond to the batch from where we are taking <code>update</code>value, second dimension must be equal to the second dimension of <code>updates</code> (number of points per batch) and the third dimension is <code>4</code> because we need <code>4</code> indices: [batch_number, channel, row, col]. Following the example in the question:</p>\n\n<pre class=""lang-py prettyprint-override""><code>updates = tf.constant([[1., 2., 3.], [4., 5., 6.]])  # [2, 3]\nidx = tf.constant([[[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 1, 0]], [[1, 0, 1, 1], [1, 0, 0, 0], [1, 0, 1, 0]]])  # [2, 3, 4]\noutput = tf.scatter_nd(idx, updates, [2, 1, 4, 4])\n\nsess = tf.Session()\nprint(sess.run(output))\n\n[[[[2. 1. 0. 0.]\n   [3. 0. 0. 0.]\n   [0. 0. 0. 0.]\n   [0. 0. 0. 0.]]]\n\n\n [[[5. 0. 0. 0.]\n   [6. 4. 0. 0.]\n   [0. 0. 0. 0.]\n   [0. 0. 0. 0.]]]]\n\n</code></pre>\n\n<p>This way it\'s possible to place specific numbers in a new tensor.</p>\n']","{'https://stackoverflow.com/questions/56969703/how-to-use-tf-scatter-nd-with-multi-dimensional-tensors/56988219#56988219', 'https://stackoverflow.com/questions/42159946/how-to-use-tf-cond-for-batch-processing/42164494#42164494'}",{56969703},0.2037736435705936,0.20967264310645659
8,71129505,tf.data.Dataset,Documentation Replication on Other Examples,"Is it possible to split a tensorflow dataset into train, validation AND test datasets when using image_dataset_from_directory?","<p>I am using <code>tf.keras.utils.image_dataset_from_directory</code> to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the <code>validation_split</code> parameter), I want to split it into training, testing, and validation subsets.</p>
<p>I have tried using <code>dataset.skip()</code> and <code>dataset.take()</code> to further split one of the resulting subsets, but these functions return a <code>SkipDataset</code> and a <code>TakeDataset</code> respectively (by the way, contrary to <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#take"" rel=""nofollow noreferrer"">the documentation</a>, where it is claimed that these functions return a <code>Dataset</code>). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history.</p>
<p>So, my question is: is there a way to split a <code>Dataset</code> into three subsets for training, validation and testing, so that all three subsets are also <code>Dataset</code> objects?</p>
<p><strong>Code used to load the data</strong></p>
<pre><code>def load_data_tf(data_path: str, img_shape=(256,256), batch_size: int=8):
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.2,
        subset=&quot;training&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.3,
        subset=&quot;validation&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    return train_ds, val_ds

train_dataset, test_val_ds = load_data_tf('data_folder', img_shape = (256,256), batch_size=8)
test_dataset = test_val_ds.take(686)
val_dataset = test_val_ds.skip(686)
</code></pre>
<p><strong>Model compilation and fitting</strong></p>
<pre><code>model.compile(optimizer='sgd',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])
history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, verbose=1)
</code></pre>
<p><strong>When using a normal <code>Dataset</code>, <code>val_accuracy</code> and <code>val_loss</code> are present in the history of the model:</strong></p>
<p><a href=""https://i.stack.imgur.com/Qn1Yf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qn1Yf.png"" alt=""Expected behaviour: when using a Dataset, validation metrics are calculated"" /></a></p>
<p><strong>But when using a <code>SkipDataset</code>, they are not:</strong></p>
<p><a href=""https://i.stack.imgur.com/GMnBM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GMnBM.png"" alt=""Using the SkipDataset produced by test_val_ds.take() leads to validation metrics disappearing from model history"" /></a></p>
<p><a href=""https://i.stack.imgur.com/omU5U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/omU5U.png"" alt=""val_accuracy and val_loss are not present in history keys when using a SkipDataset or a TakeDataset"" /></a></p>
","<p>The issue is that you are not taking and skipping samples when you do <code>test_val_ds.take(686)</code> and <code>test_val_ds.skip(686)</code>, but actually batches. Try running <code>print(val_dataset.cardinality())</code> and you will see how many batches you really have reserved for validation. I am guessing <code>val_dataset</code> is empty, because you do not have 686 batches for validation. Here is a working example:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import pathlib

dataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=123,
  image_size=(180, 180),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;validation&quot;,
  seed=123,
  image_size=(180, 180),
  batch_size=batch_size)

test_dataset = val_ds.take(5)
val_ds = val_ds.skip(5)

print('Batches for testing --&gt;', test_dataset.cardinality())
print('Batches for validating --&gt;', val_ds.cardinality())

model = tf.keras.Sequential([
  tf.keras.layers.Rescaling(1./255, input_shape=(180, 180, 3)),
  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(5)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

epochs=1
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=1
)
</code></pre>
<pre><code>Found 3670 files belonging to 5 classes.
Using 2936 files for training.
Found 3670 files belonging to 5 classes.
Using 734 files for validation.
Batches for testing --&gt; tf.Tensor(5, shape=(), dtype=int64)
Batches for validating --&gt; tf.Tensor(18, shape=(), dtype=int64)
92/92 [==============================] - 96s 1s/step - loss: 1.3516 - accuracy: 0.4489 - val_loss: 1.1332 - val_accuracy: 0.5645
</code></pre>
<p>In this example, with a <code>batch_size</code> of 32, you can clearly see that the validation set reserved 23 batches. Afterwards, 5 batches were given to the test set and 18 batches remained for the validation set.</p>
","{59905761, 65390914, 48213766, 60655280, 51125266, 61535668, 65715958, 73295865, 69848315, 58402973}","[{'QuestionId': 51125266, 'AnswerId': 77375102, 'URL': 'https://stackoverflow.com/questions/51125266/how-do-i-split-tensorflow-datasets/77375102#77375102', 'QuestionTitle': 'How do I split Tensorflow datasets?', 'Answer': '<p><strong>I will first explain why the accepted answer is wrong</strong> and secondly <strong>will provide a simple working solution</strong>, using <code>take()</code>, <code>skip()</code> and <code>seed</code>.</p>\n<p>When working with pipelines, such as TF/Torch Datasets, <strong>beware of lazy evaluation</strong>. Avoid:</p>\n<pre class=""lang-py prettyprint-override""><code># DONT\nfull_dataset = full_dataset.shuffle(10)\ntrain_dataset = full_dataset.take(train_size)\ntest_dataset = full_dataset.skip(train_size)\n</code></pre>\n<p>because take and skip will synchronize to single shuffle, but rather gets executed as <code>shuffle+take</code> and <code>shuffle+skip</code> separately (yes !), overlapping typically in 80%*20%=16% of cases. So, <strong>information leak</strong>.</p>\n<p>Play with this code in case of doubt</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\ndef gen_data():\n    return iter(range(10))\n\nfull_dataset = tf.data.Dataset.from_generator(\n  gen_data, \n  output_signature=tf.TensorSpec(shape=(),dtype=tf.int32,name=&quot;element&quot;))\n\ntrain_size = 8\n\n# WRONG WAY\nfull_dataset = full_dataset.shuffle(10)\ntrain_dataset = full_dataset.take(train_size)\ntest_dataset = full_dataset.skip(train_size)\n\nA = set(train_dataset.as_numpy_iterator())\nB = set(test_dataset.as_numpy_iterator())\n\n# EXPECT OVERLAP\nassert A.intersection(B)==set()\n\nprint(list(A))\nprint(list(B))\n</code></pre>\n<p>Now, what works is <strong>repeating and seeding shuffle in both</strong> train and test datasets, which is also good for reproducibility. This should work with any <em>deterministically ordered</em> iterator:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\ndef gen_data():\n    return iter(range(10))\n\nds = tf.data.Dataset.from_generator(\n    gen_data, \n    output_signature=tf.TensorSpec(shape=(),dtype=tf.int32,name=&quot;element&quot;))\n\nSEED = 42 # NOTE: change this\n\nds_train = ds.shuffle(100,seed=SEED).take(8).shuffle(100)\nds_test  = ds.shuffle(100,seed=SEED).skip(8)\n\nA = set(ds_train.as_numpy_iterator())\nB = set(ds_test.as_numpy_iterator())\n\nassert A.intersection(B)==set()\n\nprint(list(A))\nprint(list(B))\n</code></pre>\n<p>By playing with <code>SEED</code> you can for instance inspect/estimate generalization (bootstraping in place of cross-validation).</p>\n', 'IsAccepted': False, 'CreationDate': 1698417873}, {'QuestionId': 48213766, 'AnswerId': 77375171, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/77375171#77375171', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': '<p><strong>Beware of lazy evaluation</strong> which produces two pipelines <code>shuffle+take</code> and <code>shuffle+skip</code> that do overlap. Due to this, some of the high-scored answers produce information leaks. Here is the correct way by <strong>repeating and seeding shuffle in both train and test</strong> datasets.</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\ndef gen_data():\n    return iter(range(10))\n\nds = tf.data.Dataset.from_generator(\n    gen_data, \n    output_signature=tf.TensorSpec(shape=(),dtype=tf.int32,name=&quot;element&quot;))\n\nSEED = 42 # NOTE: with no seed, you overlap train and test!\n\nds_train = ds.shuffle(100,seed=SEED).take(8).shuffle(100)\nds_test  = ds.shuffle(100,seed=SEED).skip(8)\n\nA = set(ds_train.as_numpy_iterator())\nB = set(ds_test.as_numpy_iterator())\n\nassert A.intersection(B)==set()\n\nprint(list(A))\nprint(list(B))\n</code></pre>\n<p>NOTE: This works for any <strong>deterministically ordered</strong> iterator.</p>\n', 'IsAccepted': False, 'CreationDate': 1698418459}, {'QuestionId': 58402973, 'AnswerId': 75753348, 'URL': 'https://stackoverflow.com/questions/58402973/how-to-create-train-test-and-validation-splits-in-tensorflow-2-0/75753348#75753348', 'QuestionTitle': 'How to create train, test and validation splits in tensorflow 2.0', 'Answer': ""<p><strong>Importing tensorflow datasets :</strong></p>\n<pre><code>import tensorflow_datasets as tfds\n</code></pre>\n<p><strong>MNIST_info used to save the MNIST dataset once the MNIST dataset gets loaded:</strong></p>\n<pre><code>MNIST_dataset, MNIST_info = tfds.load(name='MNIST', with_info= True, as_supervised= True)\n</code></pre>\n<p><strong>Splitting the MNIST dataset into two parts, train and test dataset :</strong></p>\n<pre><code>MNIST_train, MNIST_test = MNIST_dataset['train'],MNIST_dataset['test']\n\nnum_validation_samples=0.1*MNIST_info.splits['train'].num_examples\n# (allocating 10 percent of the training dataset to create the validation dataset.)\n</code></pre>\n<p><strong>Once validation dataset gets created, we can then have the samples convert to integer.</strong></p>\n<pre><code>num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n</code></pre>\n<p><strong>Similarily, we have created the test samples in a similar way,</strong></p>\n<pre><code>num_test_samples = MNIST_info.splits['test'].num_examples    \nnum_test_samples = tf.cast(num_test_samples, tf.int64)    \nnum_train_samples = 0.8*MNIST_info_splits['train'].num_examples\n</code></pre>\n<p><strong>(allocating 80 percent out of the test dataset to create the training dataset.)</strong></p>\n<pre><code>num_train_samples = tf.cast(num_train_samples, tf.int64)\n</code></pre>\n<p>Hope this has answered your question 🙂👍</p>\n"", 'IsAccepted': False, 'CreationDate': 1678952838}, {'QuestionId': 48213766, 'AnswerId': 73591823, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/73591823#73591823', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': '<p>The upcoming TensorFlow 2.10.0 will have a <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/split_dataset"" rel=""noreferrer""><code>tf.keras.utils.split_dataset function</code></a>, see the <a href=""https://github.com/tensorflow/tensorflow/releases/tag/v2.10.0-rc3"" rel=""noreferrer"">rc3 release notes</a>:</p>\n<blockquote>\n<p>Added <code>tf.keras.utils.split_dataset</code> utility to split a <code>Dataset</code> object or a list/tuple of arrays into two <code>Dataset</code> objects (e.g. train/test).</p>\n</blockquote>\n', 'IsAccepted': False, 'CreationDate': 1662203925}, {'QuestionId': 73295865, 'AnswerId': 73296710, 'URL': 'https://stackoverflow.com/questions/73295865/what-is-the-correct-way-of-splitting-dataset-into-train-validation-and-test/73296710#73296710', 'QuestionTitle': 'what is the correct way of splitting dataset into train, validation and test?', 'Answer': ""<ol>\n<li>It looks like <code>Structure 2</code> just splits whatever is available, which is fundamentally correct. In reality, you'll most likely be using <code>Structure 1</code> when using <code>flow_from_directory()</code>.  You can't perform <code>evaluate()</code> without labels, so your <code>test_generator</code> is more akin to a validation set, but you can technically evaluate using that &quot;test&quot; data since they would created the same way, but ideally used differently.</li>\n<li><code>flow_from_directory()</code> outputs <code>Dataset</code>, which contains features <code>classes</code> and labels <code>class_indices</code>. When you pass a <code>Dataset</code> object to <code>evaluate()</code>, the method uses both features and labels from the passed variable. If you want to extract the labels from an object from <code>flow_from_directory()</code>, if the variable is <code>x</code>, it's <code>x.class_indices</code>, which will be a dictionary. When you pass a <code>Dataset</code> to <code>predict()</code>, only the features are used. The labels are ignored. Unless you need to manually retrieve something within the <code>Dataset</code> object, you don't need to access anything within that object when evaluating or predicting.</li>\n<li><code>split_folder</code> does not do anything with your model.</li>\n</ol>\n<p>The subfolders named after classes for train and val is when comparing the image to the label (the folder name - the class). This is how <code>flow_from_directory()</code> keeps track of each image's class. Since prediction is supposed to use unseen data as input, it wouldn't have a label to compare to, hence no labels (or subfolder containing classes) when you split your test folder out.</p>\n<p>Another thing you could do which is common is, just splitting your train and test set, then creating your validation set from your training set. But both methods are fundamentally the same.</p>\n"", 'IsAccepted': True, 'CreationDate': 1660070717}, {'QuestionId': 59905761, 'AnswerId': 71614317, 'URL': 'https://stackoverflow.com/questions/59905761/split-train-data-to-train-and-validation-by-using-tensorflow-datasets-load-tf-2/71614317#71614317', 'QuestionTitle': 'Split train data to train and validation by using tensorflow_datasets.load (TF 2.1)', 'Answer': '<p>If you need to allocate <code>training</code>, <code>validation</code>, and <code>test</code> subsets (70%, 15%, 15%), this is the code (got it from <a href=""https://stackabuse.com/split-train-test-and-validation-sets-with-tensorflow-datasets-tfds/"" rel=""nofollow noreferrer"">here</a>)</p>\n<pre><code>(training_set, validation_set, test_set), dataset_info = tfds.load(\n    \'tf_flowers\',\n    split=[\'train[:70%]\', \'train[70%:85%]\', \'train[85%:]\',\n    with_info=True,\n    as_supervised=True,\n    )\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1648198598}, {'QuestionId': 48213766, 'AnswerId': 70905839, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/70905839#70905839', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': '<p>A robust way to split dataset into two parts is to first deterministically map every item in the dataset into a bucket with, for example, <a href=""https://www.tensorflow.org/api_docs/python/tf/strings/to_hash_bucket_fast"" rel=""nofollow noreferrer""><code>tf.strings.to_hash_bucket_fast</code></a>. Then you can split the dataset into two by filtering by the bucket. If you split your data into five buckets, you get 80-20 split assuming that the split is even.</p>\n<p>As an example, assume that your dataset contains dictionaries with key <code>filename</code>. We split the data into five buckets based on this key. With this <code>add_fold</code> function, we add the key <code>&quot;fold&quot;</code> in the dictionaries:</p>\n<pre class=""lang-py prettyprint-override""><code>def add_fold(buckets: int):\n    def add_(sample, label):\n        fold = tf.strings.to_hash_bucket(sample[&quot;filename&quot;], num_buckets=buckets)\n        return {**sample, &quot;fold&quot;: fold}, label\n\n    return add_\n\ndataset = dataset.map(add_fold(buckets=5))\n</code></pre>\n<p>Now we can split the dataset into two disjoint datasets with <code>Dataset.filter</code>:</p>\n<pre class=""lang-py prettyprint-override""><code>def pick_fold(fold: int):\n    def filter_fn(sample, _):\n        return tf.math.equal(sample[&quot;fold&quot;], fold)\n\n    return filter_fn\n\n\ndef skip_fold(fold: int):\n    def filter_fn(sample, _):\n        return tf.math.not_equal(sample[&quot;fold&quot;], fold)\n\n    return filter_fn\n\ntrain_dataset = dataset.filter(skip_fold(0))\nval_dataset = dataset.filter(pick_fold(0))\n</code></pre>\n<p>The key that you use for hashing should be one that captures the correlations in the dataset. For example, if your samples collected by the same person are correlated and you want all samples with the same collector end up in the same bucket (and the same split), you should use the collector name or ID as the hashing column.</p>\n<p>Of course, you can skip the part with <code>dataset.map</code> and do the hashing and filtering in one <code>filter</code> function. Here\'s a full example:</p>\n<pre class=""lang-py prettyprint-override""><code>dataset = tf.data.Dataset.from_tensor_slices([f&quot;value-{i}&quot; for i in range(10000)])\n\ndef to_bucket(sample):\n    return tf.strings.to_hash_bucket_fast(sample, 5)\n\ndef filter_train_fn(sample):\n    return tf.math.not_equal(to_bucket(sample), 0)\n\ndef filter_val_fn(sample):\n    return tf.math.logical_not(filter_train_fn(sample))\n\ntrain_ds = dataset.filter(filter_train_fn)\nval_ds = dataset.filter(filter_val_fn)\n\nprint(f&quot;Length of training set: {len(list(train_ds.as_numpy_iterator()))}&quot;)\nprint(f&quot;Length of validation set: {len(list(val_ds.as_numpy_iterator()))}&quot;)\n</code></pre>\n<p>This prints:</p>\n<pre><code>Length of training set: 7995\nLength of validation set: 2005\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1643462618}, {'QuestionId': 69848315, 'AnswerId': 69849293, 'URL': 'https://stackoverflow.com/questions/69848315/how-can-i-split-the-dataset-obtained-from-image-dataset-from-directory-into-data/69849293#69849293', 'QuestionTitle': 'How can I split the dataset obtained from image_dataset_from_directory into data and labels?', 'Answer': '<p>You can use the <code>subset</code> parameter to separate your data into <code>training</code> and <code>validation</code>.</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\nimport pathlib\n\ndataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;\ndata_dir = tf.keras.utils.get_file(\'flower_photos\', origin=dataset_url, untar=True)\ndata_dir = pathlib.Path(data_dir)\n\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=&quot;training&quot;,\n  image_size=(256, 256),\n  seed=1,\n  batch_size=32)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=&quot;validation&quot;,\n  seed=1,\n  image_size=(256, 256),\n  batch_size=32)\n\nfor x, y in train_ds.take(1):\n  print(\'Image --&gt; \', x.shape, \'Label --&gt; \',  y.shape)\n</code></pre>\n<pre><code>Found 3670 files belonging to 5 classes.\nUsing 2936 files for training.\nFound 3670 files belonging to 5 classes.\nUsing 734 files for validation.\nImage --&gt;  (32, 256, 256, 3) Label --&gt;  (32,)\n</code></pre>\n<p>As for your labels, according to the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory"" rel=""noreferrer"">docs</a>:</p>\n<blockquote>\n<p>Either &quot;inferred&quot; (labels are generated from the directory structure),\nNone (no labels), or a list/tuple of integer labels of the same size\nas the number of image files found in the directory. Labels should be\nsorted according to the alphanumeric order of the image file paths\n(obtained via os.walk(directory) in Python).</p>\n</blockquote>\n<p>So just try iterating over the <code>train_ds</code> and see if they are there. You can also use the parameters <code>label_mode</code> to refer to the kind of labels you have and <code>class_names</code> to explicitly list your classes.</p>\n<p>If your classes are inbalanced, you can use the <code>class_weights</code> parameter of <code>model.fit(*)</code>. For more information, check out this <a href=""https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras"">post</a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1636093118}, {'QuestionId': 61535668, 'AnswerId': 69694228, 'URL': 'https://stackoverflow.com/questions/61535668/how-to-split-data-in-3-folds-train-validation-test-using-imagedatagenerator-wh/69694228#69694228', 'QuestionTitle': 'how to Split data in 3 folds (train,validation,test) using ImageDataGenerator when data is in different directories of each class', 'Answer': '<p>A much better alternative is to use split-folders library. It will create train, validation and test set folders for you.</p>\n<p><strong>source</strong> - <a href=""https://stackoverflow.com/questions/53074712/how-to-split-folder-of-images-into-test-training-validation-sets-with-stratified"">How to split folder of images into test/training/validation sets with stratified sampling?</a></p>\n<p><strong>Library</strong> - <a href=""https://pypi.org/project/split-folders/"" rel=""nofollow noreferrer"">https://pypi.org/project/split-folders/</a></p>\n', 'IsAccepted': False, 'CreationDate': 1635055162}, {'QuestionId': 58402973, 'AnswerId': 67548655, 'URL': 'https://stackoverflow.com/questions/58402973/how-to-create-train-test-and-validation-splits-in-tensorflow-2-0/67548655#67548655', 'QuestionTitle': 'How to create train, test and validation splits in tensorflow 2.0', 'Answer': ""<p><em>Francesco Boi Soultion works good for me.</em></p>\n<pre><code>splits, info = tfds.load('fashion_mnist', with_info=True, as_supervised=True, split=['train+test[:80]','train+test[80:90]', 'train+test[90:]'])\n\n(train_examples, validation_examples, test_examples) = splits\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1621094179}, {'QuestionId': 60655280, 'AnswerId': 60662739, 'URL': 'https://stackoverflow.com/questions/60655280/how-to-split-an-image-dataset-in-x-train-y-train-x-test-y-test-by-tensorflow/60662739#60662739', 'QuestionTitle': 'How to split an image dataset in X_train, y_train, X_test, y_test by tensorflow?', 'Answer': ""<p>You don't have to use tensorflow or keras to divide your dataset.\nIf you have sklearn package installed then you can simply use it:</p>\n<pre><code>from sklearn.model_selection import train_test_split\nX = ...\nY = ...\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n</code></pre>\n<p>You can also use numpy for the same purpose:</p>\n<pre><code>import numpy\nX = ...\nY = ...\ntest_size = 0.2\ntrain_nsamples = (1-test_size) * len(Y)\nx_train, x_test, y_train, y_test = X[:train_nsamples,:], X[train_nsamples:, :], Y[:train_nsamples, ], Y[train_nsamples:,]\n</code></pre>\n<p>Good Luck!</p>\n"", 'IsAccepted': False, 'CreationDate': 1584052955}, {'QuestionId': 59905761, 'AnswerId': 59922289, 'URL': 'https://stackoverflow.com/questions/59905761/split-train-data-to-train-and-validation-by-using-tensorflow-datasets-load-tf-2/59922289#59922289', 'QuestionTitle': 'Split train data to train and validation by using tensorflow_datasets.load (TF 2.1)', 'Answer': '<p>According to the <a href=""https://www.tensorflow.org/datasets/splits#specifying_a_percentage_slice"" rel=""nofollow noreferrer"">Tensorflow Dataset docs</a> the approach you presented is now supported. Splitting is possible by passing split parameter to <code>tfds.load</code> like so <code>split=&quot;test[:70%]&quot;</code>.</p>\n<pre class=""lang-py prettyprint-override""><code>(training_set, validation_set), dataset_info = tfds.load(\n    \'tf_flowers\',\n    split=[\'train[:70%]\', \'train[70%:]\'],\n    with_info=True,\n    as_supervised=True,\n)\n</code></pre>\n<p>With the above code the <code>training_set</code> has 2569 entries, while <code>validation_set</code> has 1101.</p>\n<p>Thank you <a href=""https://stackoverflow.com/users/2959717/saman"">Saman</a> for the comment on API deprecation:<br />\nIn previous Tensorflow version it was possible to use <code>tfds.Split</code> API which is now deprecated:</p>\n<pre class=""lang-py prettyprint-override""><code>(training_set, validation_set), dataset_info = tfds.load(\n    \'tf_flowers\',\n    split=[\n        tfds.Split.TRAIN.subsplit(tfds.percent[:70]),\n        tfds.Split.TRAIN.subsplit(tfds.percent[70:])\n    ],\n    with_info=True,\n    as_supervised=True,\n)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1580069388}, {'QuestionId': 65715958, 'AnswerId': 65723021, 'URL': 'https://stackoverflow.com/questions/65715958/to-split-the-main-data-directory-into-train-validation-test-set/65723021#65723021', 'QuestionTitle': 'To split the main data directory into Train/validation/test Set', 'Answer': ""<p>I have had a need to do this often do I developed a thorough function to accomplish the splitting. It is rather lengthy because it does a lot of checks etc. The code is posted below.</p>\n<pre><code>import os\nimport shutil\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\ndef tr_te_val_split(s_dir, dest_dir, train_size, test_size): \n    if train_size &lt;0 or train_size &gt;1:\n        print('*** Train size must be a float between 0.0 and 1.0, process terminated ***')\n        return\n    if test_size &lt;0 or test_size &gt;1:\n        print('*** Test size must be a float between 0.0 and 1.0, process terminated ***')\n        return\n    if test_size + train_size &gt;1:\n        print ('*** The sum of the train size plus the test size must be &lt;= 1, process terminating ***')\n        return\n    \n    remainder= 1-train_size # percent available for test and validation\n    test_size= test_size/remainder\n    if os.path.isdir(dest_dir)==False:\n        os.mkdir(dest_dir)\n        print ('The dest_dir you specified ', dest_dir, ' does not exist, created it for you ')        \n    dest_list=os.listdir(dest_dir) # list content of destination directory\n    for d in ['train', 'test', 'valid']:\n        d_path=os.path.join(dest_dir,d)\n        if d not in dest_list:\n            os.mkdir(d_path)  # create train, test and valid directories in the destination directory\n        else: # check to see if there are any files in these directories\n            d_list=os.listdir(d_path)\n            if len(d_list) &gt; 0:  # there are files or directories in d\n                cycle=True\n                print('*** WARNING***  there is content in ', d_path)\n                while cycle:\n                    ans=input(' enter D to delete content, C to continue and keep content or Q to Quit ')\n                    if ans not in ['D', 'd', 'C', 'c', 'Q', 'q']:\n                        print('your response ', ans, ' was not a  D, C or Q, try again')\n                    else:\n                        cycle=False\n                        if ans in ['Q', 'q']:\n                            print ('**** PROCESS TERMINATED BY USER ****')\n                            return\n                        else:\n                            if ans in ['D', 'd']:\n                                print(' Removing all files and sub directories in ', d_path)\n                                for f in d_list:\n                                    f_path=os.path.join (d_path,f)\n                                    if os.path.isdir(f_path):                                        \n                                        shutil.rmtree(f_path)                                        \n                                    else:\n                                        os.remove(f_path)\n            \n    class_list=os.listdir(s_dir)  # listof classes     \n    for klass in tqdm(class_list): # iterate through the classes\n        klass_path=os.path.join(s_dir, klass) # path to class directory\n        f_list=os.listdir(klass_path) # get the list of file names\n        ftrain, ftv= train_test_split(f_list, train_size=train_size, random_state=123 )\n        ftest, fvalid= train_test_split(ftv, train_size= test_size, random_state=123 )        \n        for d in ['train', 'test', 'valid']:\n            d_path=os.path.join(dest_dir,d)\n            d_class_path=os.path.join(d_path,klass)\n            if os.path.isdir(d_class_path)==False:\n                os.mkdir(d_class_path)\n            if d=='train':\n                fx=ftrain\n            elif d=='test':\n                fx=ftest\n            else:\n                fx=fvalid\n            for f in fx:\n                f_path=os.path.join(klass_path, f)\n                d_f_path=os.path.join(d_class_path,f)\n                shutil.copy(f_path, d_f_path)\n    for d in ['train', 'test', 'valid']:\n        file_count=0\n        d_path=os.path.join(dest_dir, d)\n        d_list=os.listdir(d_path)\n        for klass in d_list:\n            klass_path=os.path.join(d_path, klass)\n            klass_list=os.listdir(klass_path)\n            d_count=len(klass_list)\n            file_count=file_count + d_count\n            if d == 'train':\n                tr_count=file_count\n            elif d =='test':\n                te_count=file_count\n            else:\n                tv_count=file_count\n    print ('Process Completed ', tr_count, ' training files ', te_count, ' test files and ', tv_count, ' validation files were partitioned')\n</code></pre>\n<p>This function splits the files in the s_dir into train, test, and validation files stored in the dest_dir.\ns_dir is the full path to the directory containing the files to be split\ndest_dir is the full path to the destination directory. If it does not exist it is created.\ntrain_size is a float between 0.0 and 1.0 indicating the percentage of file to be allocated as training files\ntest_size is a float between 0.0 and 1.0 indicating the percentage of file to be allocated as test files\nIn the dest_dir three sub directories 'train', 'test' and 'valid' are created and used to store the training files,\ntest files and validation files.\nIf these sub directories already exist they are check for existing content.If content is found a notice is printed\nto that effect. The user is then prompted to enter 'D' to delete the content, 'Q' to terminate program execution\nor 'C' to continue. If 'C' is selected the content is not removed however files may be over written if any existing\nfiles have the same file name as the new files being added to the sub directory.\nNote if the test, train and valid directories exist and have content, and the user elects 'c' to continue\nsub directories and files from the s_dir are appended to the content of the test, train and valid subdirectories\nin the dest_dir\nThis function utlilizes tqdm and sklearn which must be installed in your working environment¶</p>\n"", 'IsAccepted': False, 'CreationDate': 1610642212}, {'QuestionId': 65390914, 'AnswerId': 65394085, 'URL': 'https://stackoverflow.com/questions/65390914/tensorflow-dataset-train-test-split/65394085#65394085', 'QuestionTitle': 'TensorFlow Dataset train/test split', 'Answer': '<p>See the documentation of <a href=""https://www.tensorflow.org/datasets/splits"" rel=""nofollow noreferrer"">Tensorflow Datasets: Splits and Slicing</a>. What you need is this:</p>\n<pre><code>tfds.load(\'coil100\', split=[\'train[:7000]\', \'train[7000:]\'])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1608559119}, {'QuestionId': 48213766, 'AnswerId': 60503037, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/60503037#60503037', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': ""<p>Most of the answers here use <code>take()</code> and <code>skip()</code>, which requires knowing the size of your dataset before hand. This isn't always possible, or is difficult/intensive to ascertain.</p>\n<p>Instead what you can do is to essentially slice the dataset up so that 1 every N records becomes a validation record.</p>\n<p>To accomplish this, lets start with a simple dataset of 0-9:</p>\n<pre><code>dataset = tf.data.Dataset.range(10)\n# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>\n<p>Now for our example, we're going to slice it so that we have a 3/1 train/validation split. Meaning 3 records will go to training, then 1 record to validation, then repeat.</p>\n<pre><code>split = 3\ndataset_train = dataset.window(split, split + 1).flat_map(lambda ds: ds)\n# [0, 1, 2, 4, 5, 6, 8, 9]\ndataset_validation = dataset.skip(split).window(1, split + 1).flat_map(lambda ds: ds)\n# [3, 7]\n</code></pre>\n<p>So the first <code>dataset.window(split, split + 1)</code> says to grab <code>split</code> number <em>(3)</em> of elements, then advance <code>split + 1</code> elements, and repeat. That <code>+ 1</code> effectively skips the 1 element we're going to use in our validation dataset.<br />\nThe <code>flat_map(lambda ds: ds)</code> is because <code>window()</code> returns the results in batches, which we don't want. So we flatten it back out.</p>\n<p>Then for the validation data we first <code>skip(split)</code>, which skips over the first <code>split</code> number <em>(3)</em> of elements that were grabbed in the first training window, so we start our iteration on the 4th element. The <code>window(1, split + 1)</code> then grabs 1 element, advances <code>split + 1</code> <em>(4)</em>, and repeats.</p>\n<p>\xa0</p>\n<p>Note on nested datasets:<br />\nThe above example works well for simple datasets, but <code>flat_map()</code> will generate an error if the dataset is nested. To address this, you can swap out the <code>flat_map()</code> with a more complicated version that can handle both simple and nested datasets:</p>\n<pre><code>.flat_map(lambda *ds: ds[0] if len(ds) == 1 else tf.data.Dataset.zip(ds))\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1583224325}, {'QuestionId': 58402973, 'AnswerId': 64459146, 'URL': 'https://stackoverflow.com/questions/58402973/how-to-create-train-test-and-validation-splits-in-tensorflow-2-0/64459146#64459146', 'QuestionTitle': 'How to create train, test and validation splits in tensorflow 2.0', 'Answer': '<p>I had the <a href=""https://stackoverflow.com/q/64451516/1714692"">same problem</a></p>\n<p>It depends on the dataset, most of which have a train and test set. In this case you can do the following (assuming 80-10-10 split):</p>\n<pre><code>splits, info = tfds.load(\'fashion_mnist\', with_info=True, as_supervised=True,\nsplit=[\'train+test[:80]\',\'train+test[80:90]\', \'train+test[90:]\'],\ndata_dir=filePath)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1603267220}, {'QuestionId': 61535668, 'AnswerId': 61593001, 'URL': 'https://stackoverflow.com/questions/61535668/how-to-split-data-in-3-folds-train-validation-test-using-imagedatagenerator-wh/61593001#61593001', 'QuestionTitle': 'how to Split data in 3 folds (train,validation,test) using ImageDataGenerator when data is in different directories of each class', 'Answer': ""<p>As you rightly mentioned, splitting the Data into 3 Folds is not possible in one line of code using Keras <code>ImageDataGenerator</code>. </p>\n\n<p>Work around would be to store the Images corresponding to <code>Test Data</code> in a separate Folder and apply <code>ImageDataGenerator</code>, as shown below:</p>\n\n<pre><code># Path to Training Directory\ntrain_dir = 'Dogs_Vs_Cats_Small/train'\n\n# Path to Test Directory\ntest_dir = 'Dogs_Vs_Cats_Small/test'\n\nTrain_Gen = ImageDataGenerator(1./255)\nTest_Gen = ImageDataGenerator(1./255)\n\n\nTrain_Generator = Train_Gen.flow_from_directory(train_dir, target_size = (150,150), batch_size = 20, class_mode = 'binary')\n\nTest_Generator = Test_Gen.flow_from_directory(test_dir, target_size = (150, 150), class_mode = 'binary', batch_size = 20)\n</code></pre>\n\n<p>Sample Code to extract some images from the Original Directory and place them in two separate folders, <code>train</code> and <code>test</code>, which may help you, is shown below:</p>\n\n<pre><code>import os, shutil\n\n# Path to the directory where the original dataset was uncompressed\noriginal_dataset_dir = 'Dogs_Vs_Cats'\n\n# Directory where you’ll store your smaller dataset\nbase_dir = 'Dogs_Vs_Cats_Small2'\n\nos.mkdir(base_dir)\n\n# Directory for the training splits\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\n\n# Directory for the test splits\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)\n\n# Directory with training cat pictures\ntrain_cats_dir = os.path.join(train_dir, 'cats')\nos.mkdir(train_cats_dir)\n\n# Directory with training dog pictures\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nos.mkdir(train_dogs_dir)\n\n# Directory with Test Cat Pictures\ntest_cats_dir = os.path.join(test_dir, 'cats')\nos.mkdir(test_cats_dir)\n\n# Directory with Test Dog Pictures\ntest_dogs_dir = os.path.join(test_dir, 'dogs')\nos.mkdir(test_dogs_dir)\n\n# Copies the first 1,000 cat images to train_cats_dir. \nfnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, 'train', fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)\n\n# Copies the next 500 cat images to test_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, 'train', fname)\n    dst = os.path.join(test_cats_dir, fname)\n    shutil.copyfile(src, dst)\n\n# Copies the first 1,000 dog images to train_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, 'train', fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\n# Copies the next 500 dog images to test_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, 'train', fname)\n    dst = os.path.join(test_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\n# Sanity Check to ensure that Training, Validation and Test Folders have the expected number of images\n\nprint('Number of Cat Images in Training Directory is {}'.format(len(os.listdir(train_cats_dir))))\nprint('Number of Dog Images in Training Directory is {}'.format(len(os.listdir(train_dogs_dir))))\nprint('Number of Cat Images in Testing Directory is {}'.format(len(os.listdir(test_cats_dir))))\nprint('Number of Dog Images in Testing Directory is {}'.format(len(os.listdir(test_dogs_dir))))\n</code></pre>\n\n<p>Hope this helps. </p>\n"", 'IsAccepted': True, 'CreationDate': 1588597814}, {'QuestionId': 48213766, 'AnswerId': 60894496, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/60894496#60894496', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': ""<p>@ted's answer will cause some overlap. Try this.</p>\n\n<pre><code>train_ds_size = int(0.64 * full_ds_size)\nvalid_ds_size = int(0.16 * full_ds_size)\n\ntrain_ds = full_ds.take(train_ds_size)\nremaining = full_ds.skip(train_ds_size)  \nvalid_ds = remaining.take(valid_ds_size)\ntest_ds = remaining.skip(valid_ds_size)\n</code></pre>\n\n<p>use code below to test.</p>\n\n<pre><code>tf.enable_eager_execution()\n\ndataset = tf.data.Dataset.range(100)\n\ntrain_size = 20\nvalid_size = 30\ntest_size = 50\n\ntrain = dataset.take(train_size)\nremaining = dataset.skip(train_size)\nvalid = remaining.take(valid_size)\ntest = remaining.skip(valid_size)\n\nfor i in train:\n    print(i)\n\nfor i in valid:\n    print(i)\n\nfor i in test:\n    print(i)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1585345417}, {'QuestionId': 60655280, 'AnswerId': 60841589, 'URL': 'https://stackoverflow.com/questions/60655280/how-to-split-an-image-dataset-in-x-train-y-train-x-test-y-test-by-tensorflow/60841589#60841589', 'QuestionTitle': 'How to split an image dataset in X_train, y_train, X_test, y_test by tensorflow?', 'Answer': '<p>For example, you have folder like this</p>\n\n<pre><code>full_dataset\n|--horse (40 images)\n|--donkey (30 images)\n|--cow ((50 images)\n|--zebra (70 images)\n</code></pre>\n\n<p>FIRST WAY</p>\n\n<pre><code>import glob\nhorse = glob.glob(\'full_dataset/horse/*.*\')\ndonkey = glob.glob(\'full_dataset/donkey/*.*\')\ncow = glob.glob(\'full_dataset/cow/*.*\')\nzebra = glob.glob(\'full_dataset/zebra/*.*\')\n\ndata = []\nlabels = []\n\nfor i in horse:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode=\'RGB\', \n    target_size= (280,280))\n    image=np.array(image)\n    data.append(image)\n    labels.append(0)\nfor i in donkey:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode=\'RGB\', \n    target_size= (280,280))\n    image=np.array(image)\n    data.append(image)\n    labels.append(1)\nfor i in cow:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode=\'RGB\', \n    target_size= (280,280))\n    image=np.array(image)\n    data.append(image)\n    labels.append(2)\nfor i in zebra:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode=\'RGB\', \n    target_size= (280,280))\n    image=np.array(image)\n    data.append(image)\n    labels.append(3)\n\ndata = np.array(data)\nlabels = np.array(labels)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, ytrain, ytest = train_test_split(data, labels, test_size=0.2,\n                                                random_state=42)\n</code></pre>\n\n<p>SECOND WAY</p>\n\n<pre><code>image_generator = ImageDataGenerator(rescale=1/255, validation_split=0.2)    \n\ntrain_dataset = image_generator.flow_from_directory(batch_size=32,\n                                                 directory=\'full_dataset\',\n                                                 shuffle=True,\n                                                 target_size=(280, 280), \n                                                 subset=""training"",\n                                                 class_mode=\'categorical\')\n\nvalidation_dataset = image_generator.flow_from_directory(batch_size=32,\n                                                 directory=\'full_dataset\',\n                                                 shuffle=True,\n                                                 target_size=(280, 280), \n                                                 subset=""validation"",\n                                                 class_mode=\'categorical\')\n</code></pre>\n\n<p>Main drawback from Second way, you can\'t use for display a picture. It will error if you write <code>validation_dataset[1]</code>. But it worked if I use first way : <code>X_test[1]</code></p>\n', 'IsAccepted': False, 'CreationDate': 1585098022}, {'QuestionId': 58402973, 'AnswerId': 60740009, 'URL': 'https://stackoverflow.com/questions/58402973/how-to-create-train-test-and-validation-splits-in-tensorflow-2-0/60740009#60740009', 'QuestionTitle': 'How to create train, test and validation splits in tensorflow 2.0', 'Answer': '<p>Please refer below code to create train, test and validation splits using tensorflow dataset ""oxford_flowers102"" </p>\n\n<pre><code>!pip install tensorflow==2.0.0\n\nimport tensorflow as tf\nprint(tf.__version__)\nimport tensorflow_datasets as tfds\n\nlabeled_ds, summary = tfds.load(\'oxford_flowers102\', split=\'train+test+validation\', with_info=True)\n\nlabeled_all_length = [i for i,_ in enumerate(labeled_ds)][-1] + 1\n\ntrain_size = int(0.8 * labeled_all_length)\nval_test_size = int(0.1 * labeled_all_length)\n\ndf_train = labeled_ds.take(train_size)\ndf_test = labeled_ds.skip(train_size)\ndf_val = df_test.skip(val_test_size)\ndf_test = df_test.take(val_test_size)\n\ndf_train_length = [i for i,_ in enumerate(df_train)][-1] + 1\ndf_val_length = [i for i,_ in enumerate(df_val)][-1] + 1\ndf_test_length = [i for i,_ in enumerate(df_test)][-1] + 1\n\nprint(\'Original: \', labeled_all_length)\nprint(\'Train: \', df_train_length)\nprint(\'Validation :\', df_val_length)\nprint(\'Test :\', df_test_length)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1584535915}, {'QuestionId': 48213766, 'AnswerId': 60495805, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/60495805#60495805', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': ""<p>Can't comment, but above answer has overlap and is incorrect. Set BUFFER_SIZE to DATASET_SIZE for perfect shuffle. Try different sized val/test size to verify. Answer should be:</p>\n\n<pre><code>DATASET_SIZE = tf.data.experimental.cardinality(full_dataset).numpy()\ntrain_size = int(0.7 * DATASET_SIZE)\nval_size = int(0.15 * DATASET_SIZE)\ntest_size = int(0.15 * DATASET_SIZE)\n\nfull_dataset = full_dataset.shuffle(BUFFER_SIZE)\ntrain_dataset = full_dataset.take(train_size)\ntest_dataset = full_dataset.skip(train_size)\nval_dataset = test_dataset.take(val_size)\ntest_dataset = test_dataset.skip(val_size)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1583179253}, {'QuestionId': 48213766, 'AnswerId': 59919797, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/59919797#59919797', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': '<p>In case size of the dataset is known:</p>\n\n<pre><code>from typing import Tuple\nimport tensorflow as tf\n\ndef split_dataset(dataset: tf.data.Dataset, \n                  dataset_size: int, \n                  train_ratio: float, \n                  validation_ratio: float) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset]:\n    assert (train_ratio + validation_ratio) &lt; 1\n\n    train_count = int(dataset_size * train_ratio)\n    validation_count = int(dataset_size * validation_ratio)\n    test_count = dataset_size - (train_count + validation_count)\n\n    dataset = dataset.shuffle(dataset_size)\n\n    train_dataset = dataset.take(train_count)\n    validation_dataset = dataset.skip(train_count).take(validation_count)\n    test_dataset = dataset.skip(validation_count + train_count).take(test_count)\n\n    return train_dataset, validation_dataset, test_dataset\n</code></pre>\n\n<p>Example:</p>\n\n<pre><code>size_of_ds = 1001\ntrain_ratio = 0.6\nval_ratio = 0.2\n\nds = tf.data.Dataset.from_tensor_slices(list(range(size_of_ds)))\ntrain_ds, val_ds, test_ds = split_dataset(ds, size_of_ds, train_ratio, val_ratio)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1580052484}, {'QuestionId': 51125266, 'AnswerId': 58452268, 'URL': 'https://stackoverflow.com/questions/51125266/how-do-i-split-tensorflow-datasets/58452268#58452268', 'QuestionTitle': 'How do I split Tensorflow datasets?', 'Answer': '<p>This question is similar to <a href=""https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test"">this one</a> and <a href=""https://stackoverflow.com/questions/50204609/is-there-a-way-to-partition-a-tf-dataset-with-tensorflow-s-dataset-api-not-a-p?noredirect=1&amp;lq=1"">this one</a>, and I am afraid we have not had a satisfactory answer yet.</p>\n\n<ul>\n<li><p>Using <code>take()</code> and <code>skip()</code> requires knowing the dataset size. What if I don\'t know that, or don\'t want to find out?</p></li>\n<li><p>Using <code>shard()</code> only gives <code>1 / num_shards</code> of dataset. What if I want the rest?</p></li>\n</ul>\n\n<p>I try to present a better solution below, tested on <strong>TensorFlow 2</strong> only. Assuming you already have a <em>shuffled</em> dataset, you can then use <code>filter()</code> to split it into two:</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\nall = tf.data.Dataset.from_tensor_slices(list(range(1, 21))) \\\n        .shuffle(10, reshuffle_each_iteration=False)\n\ntest_dataset = all.enumerate() \\\n                    .filter(lambda x,y: x % 4 == 0) \\\n                    .map(lambda x,y: y)\n\ntrain_dataset = all.enumerate() \\\n                    .filter(lambda x,y: x % 4 != 0) \\\n                    .map(lambda x,y: y)\n\nfor i in test_dataset:\n    print(i)\n\nprint()\n\nfor i in train_dataset:\n    print(i)\n</code></pre>\n\n<p>The parameter <code>reshuffle_each_iteration=False</code> is important. It makes sure the original dataset is shuffled once and no more. Otherwise, the two resulting sets may have some overlaps.</p>\n\n<p>Use <code>enumerate()</code> to add an index.</p>\n\n<p>Use <code>filter(lambda x,y: x % 4 == 0)</code> to take 1 sample out of 4. Likewise, <code>x % 4 != 0</code> takes 3 out of 4.</p>\n\n<p>Use <code>map(lambda x,y: y)</code> to strip the index and recover the original sample.</p>\n\n<p>This example achieves a 75/25 split.</p>\n\n<p><code>x % 5 == 0</code> and <code>x % 5 != 0</code> gives a 80/20 split.</p>\n\n<p>If you really want a 70/30 split, <code>x % 10 &lt; 3</code> and <code>x % 10 &gt;= 3</code> should do.</p>\n\n<p><strong>UPDATE:</strong></p>\n\n<p>As of TensorFlow 2.0.0, above code may result in some warnings due to <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md"" rel=""noreferrer"">AutoGraph\'s limitations</a>. To eliminate those warnings, declare all lambda functions separately:</p>\n\n<pre class=""lang-py prettyprint-override""><code>def is_test(x, y):\n    return x % 4 == 0\n\ndef is_train(x, y):\n    return not is_test(x, y)\n\nrecover = lambda x,y: y\n\ntest_dataset = all.enumerate() \\\n                    .filter(is_test) \\\n                    .map(recover)\n\ntrain_dataset = all.enumerate() \\\n                    .filter(is_train) \\\n                    .map(recover)\n</code></pre>\n\n<p>This gives no warning on my machine. And making <code>is_train()</code> to be <code>not is_test()</code> is definitely a good practice.</p>\n', 'IsAccepted': False, 'CreationDate': 1571406685}, {'QuestionId': 51125266, 'AnswerId': 51126863, 'URL': 'https://stackoverflow.com/questions/51125266/how-do-i-split-tensorflow-datasets/51126863#51126863', 'QuestionTitle': 'How do I split Tensorflow datasets?', 'Answer': '<p>You may use <code>Dataset.take()</code> and <code>Dataset.skip()</code>:</p>\n\n<pre><code>train_size = int(0.7 * DATASET_SIZE)\nval_size = int(0.15 * DATASET_SIZE)\ntest_size = int(0.15 * DATASET_SIZE)\n\nfull_dataset = tf.data.TFRecordDataset(FLAGS.input_file)\nfull_dataset = full_dataset.shuffle()\ntrain_dataset = full_dataset.take(train_size)\ntest_dataset = full_dataset.skip(train_size)\nval_dataset = test_dataset.skip(test_size)\ntest_dataset = test_dataset.take(test_size)\n</code></pre>\n\n<p>For more generality, I gave an example using a 70/15/15 train/val/test split but if you don\'t need a test or a val set, just ignore the last 2 lines.</p>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""noreferrer""><strong>Take</strong></a>:</p>\n\n<blockquote>\n  <p>Creates a Dataset with at most count elements from this dataset.</p>\n</blockquote>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#skip"" rel=""noreferrer""><strong>Skip</strong></a>:</p>\n\n<blockquote>\n  <p>Creates a Dataset that skips count elements from this dataset.</p>\n</blockquote>\n\n<p>You may also want to look into <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard"" rel=""noreferrer""><code>Dataset.shard()</code></a>:</p>\n\n<blockquote>\n  <p>Creates a Dataset that includes only 1/num_shards of this dataset.</p>\n</blockquote>\n', 'IsAccepted': True, 'CreationDate': 1530477606}, {'QuestionId': 48213766, 'AnswerId': 53419125, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/53419125#53419125', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': '<p>You can use <code>shard</code>:</p>\n\n<pre><code>dataset = dataset.shuffle()  # optional\ntrainset = dataset.shard(2, 0)\ntestset = dataset.shard(2, 1)\n</code></pre>\n\n<p>See:\n<a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard</a></p>\n', 'IsAccepted': False, 'CreationDate': 1542827879}, {'QuestionId': 48213766, 'AnswerId': 51258695, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/51258695#51258695', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': '<p>You may use <code>Dataset.take()</code> and <code>Dataset.skip()</code>:</p>\n\n<pre><code>train_size = int(0.7 * DATASET_SIZE)\nval_size = int(0.15 * DATASET_SIZE)\ntest_size = int(0.15 * DATASET_SIZE)\n\nfull_dataset = tf.data.TFRecordDataset(FLAGS.input_file)\nfull_dataset = full_dataset.shuffle()\ntrain_dataset = full_dataset.take(train_size)\ntest_dataset = full_dataset.skip(train_size)\nval_dataset = test_dataset.skip(val_size)\ntest_dataset = test_dataset.take(test_size)\n</code></pre>\n\n<p>For more generality, I gave an example using a 70/15/15 train/val/test split but if you don\'t need a test or a val set, just ignore the last 2 lines.</p>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""noreferrer""><strong>Take</strong></a>:</p>\n\n<blockquote>\n  <p>Creates a Dataset with at most count elements from this dataset.</p>\n</blockquote>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#skip"" rel=""noreferrer""><strong>Skip</strong></a>:</p>\n\n<blockquote>\n  <p>Creates a Dataset that skips count elements from this dataset.</p>\n</blockquote>\n\n<p>You may also want to look into <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard"" rel=""noreferrer""><code>Dataset.shard()</code></a>:</p>\n\n<blockquote>\n  <p>Creates a Dataset that includes only 1/num_shards of this dataset.</p>\n</blockquote>\n\n<hr>\n\n<p>Disclaimer I stumbled upon this question after answering <a href=""https://stackoverflow.com/questions/51125266/how-do-i-split-tensorflow-datasets"">this one</a> so I thought I\'d spread the love</p>\n', 'IsAccepted': False, 'CreationDate': 1531204928}, {'QuestionId': 48213766, 'AnswerId': 50185329, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/50185329#50185329', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': '<p>Assuming you have <code>all_dataset</code> variable of <code>tf.data.Dataset</code> type:</p>\n\n<pre><code>test_dataset = all_dataset.take(1000) \ntrain_dataset = all_dataset.skip(1000)\n</code></pre>\n\n<p>Test dataset now has first 1000 elements and the rest goes for training.</p>\n', 'IsAccepted': False, 'CreationDate': 1525489804}, {'QuestionId': 48213766, 'AnswerId': 49230994, 'URL': 'https://stackoverflow.com/questions/48213766/split-a-dataset-created-by-tensorflow-dataset-api-in-to-train-and-test/49230994#49230994', 'QuestionTitle': 'Split a dataset created by Tensorflow dataset API in to Train and Test?', 'Answer': ""<p>Now Tensorflow doesn't contain any tools for that.<br>\nYou could use <code>sklearn.model_selection.train_test_split</code> to generate train/eval/test dataset, then create <code>tf.data.Dataset</code> respectively. </p>\n"", 'IsAccepted': False, 'CreationDate': 1520843707}]","{71129505, 70535683}","['<p>I would suggest unbatching your dataset and using <code>tf.data.Dataset.map</code>:</p>\n<pre class=""lang-py prettyprint-override""><code>import numpy as np\nimport tensorflow as tf\n\ndataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;\ndata_dir = tf.keras.utils.get_file(\'flower_photos\', origin=dataset_url, untar=True)\ndata_dir = pathlib.Path(data_dir)\nbatch_size = 32\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=&quot;training&quot;,\n  seed=123,\n  image_size=(180, 180),\n  batch_size=batch_size,\n  shuffle=False)\n\ntrain_ds = train_ds.unbatch()\nimages = np.asarray(list(train_ds.map(lambda x, y: x)))\nlabels = np.asarray(list(train_ds.map(lambda x, y: y)))\n</code></pre>\n<p>Or as suggested in the comments, you could also try just working with the batches and concatenating them afterwards:</p>\n<pre class=""lang-py prettyprint-override""><code>images = np.concatenate(list(train_ds.map(lambda x, y: x)))\nlabels = np.concatenate(list(train_ds.map(lambda x, y: y)))\n</code></pre>\n<p>Or set <code>shuffle=True</code> and use <code>tf.TensorArray</code>:</p>\n<pre class=""lang-py prettyprint-override""><code>images = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\nlabels = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n\nfor x, y in train_ds.unbatch():\n  images = images.write(images.size(), x)\n  labels = labels.write(labels.size(), y)\n\nimages = tf.stack(images.stack(), axis=0)\nlabels = tf.stack(labels.stack(), axis=0)\n</code></pre>\n', '<p>The issue is that you are not taking and skipping samples when you do <code>test_val_ds.take(686)</code> and <code>test_val_ds.skip(686)</code>, but actually batches. Try running <code>print(val_dataset.cardinality())</code> and you will see how many batches you really have reserved for validation. I am guessing <code>val_dataset</code> is empty, because you do not have 686 batches for validation. Here is a working example:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\nimport pathlib\n\ndataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;\ndata_dir = tf.keras.utils.get_file(\'flower_photos\', origin=dataset_url, untar=True)\ndata_dir = pathlib.Path(data_dir)\n\nbatch_size = 32\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=&quot;training&quot;,\n  seed=123,\n  image_size=(180, 180),\n  batch_size=batch_size)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=&quot;validation&quot;,\n  seed=123,\n  image_size=(180, 180),\n  batch_size=batch_size)\n\ntest_dataset = val_ds.take(5)\nval_ds = val_ds.skip(5)\n\nprint(\'Batches for testing --&gt;\', test_dataset.cardinality())\nprint(\'Batches for validating --&gt;\', val_ds.cardinality())\n\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Rescaling(1./255, input_shape=(180, 180, 3)),\n  tf.keras.layers.Conv2D(16, 3, padding=\'same\', activation=\'relu\'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Conv2D(32, 3, padding=\'same\', activation=\'relu\'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Conv2D(64, 3, padding=\'same\', activation=\'relu\'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(128, activation=\'relu\'),\n  tf.keras.layers.Dense(5)\n])\n\nmodel.compile(optimizer=\'adam\',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\'accuracy\'])\n\nepochs=1\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=1\n)\n</code></pre>\n<pre><code>Found 3670 files belonging to 5 classes.\nUsing 2936 files for training.\nFound 3670 files belonging to 5 classes.\nUsing 734 files for validation.\nBatches for testing --&gt; tf.Tensor(5, shape=(), dtype=int64)\nBatches for validating --&gt; tf.Tensor(18, shape=(), dtype=int64)\n92/92 [==============================] - 96s 1s/step - loss: 1.3516 - accuracy: 0.4489 - val_loss: 1.1332 - val_accuracy: 0.5645\n</code></pre>\n<p>In this example, with a <code>batch_size</code> of 32, you can clearly see that the validation set reserved 23 batches. Afterwards, 5 batches were given to the test set and 18 batches remained for the validation set.</p>\n', '<p>The issue is that you are not taking and skipping samples when you do <code>test_val_ds.take(686)</code> and <code>test_val_ds.skip(686)</code>, but actually batches. Try running <code>print(val_dataset.cardinality())</code> and you will see how many batches you really have reserved for validation. I am guessing <code>val_dataset</code> is empty, because you do not have 686 batches for validation. Here is a working example:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\nimport pathlib\n\ndataset_url = &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;\ndata_dir = tf.keras.utils.get_file(\'flower_photos\', origin=dataset_url, untar=True)\ndata_dir = pathlib.Path(data_dir)\n\nbatch_size = 32\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=&quot;training&quot;,\n  seed=123,\n  image_size=(180, 180),\n  batch_size=batch_size)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=&quot;validation&quot;,\n  seed=123,\n  image_size=(180, 180),\n  batch_size=batch_size)\n\ntest_dataset = val_ds.take(5)\nval_ds = val_ds.skip(5)\n\nprint(\'Batches for testing --&gt;\', test_dataset.cardinality())\nprint(\'Batches for validating --&gt;\', val_ds.cardinality())\n\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Rescaling(1./255, input_shape=(180, 180, 3)),\n  tf.keras.layers.Conv2D(16, 3, padding=\'same\', activation=\'relu\'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Conv2D(32, 3, padding=\'same\', activation=\'relu\'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Conv2D(64, 3, padding=\'same\', activation=\'relu\'),\n  tf.keras.layers.MaxPooling2D(),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(128, activation=\'relu\'),\n  tf.keras.layers.Dense(5)\n])\n\nmodel.compile(optimizer=\'adam\',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\'accuracy\'])\n\nepochs=1\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=1\n)\n</code></pre>\n<pre><code>Found 3670 files belonging to 5 classes. Using 2936 files for training.']","{'https://stackoverflow.com/questions/70535683/extract-data-from-tensorflow-dataset-e-g-to-numpy/70584413#70584413', 'https://stackoverflow.com/questions/71129505/is-it-possible-to-split-a-tensorflow-dataset-into-train-validation-and-test-dat/71137470#71137470'}",,0.18621679812801392,0.05489735444047428
8,56802840,tf.gather,Lack of Alternative Solutions/Documentation,What exactly tensorflow.gather() does?,"<p>I saw code for triplet loss that contains the function tf.gather(). What this function does?</p>

<p>I have gone through the tensorflow's official website for definition but still unable to get it.</p>

<pre><code>def margin_triplet_loss(y_true, y_pred, margin, batch_size):
    anchor = tf.gather(y_pred, tf.range(0, batch_size, 3))
    positive = tf.gather(y_pred, tf.range(1, batch_size, 3))
    negative = tf.gather(y_pred, tf.range(2, batch_size, 3))

    loss = K.maximum(margin
                 + K.sum(K.square(anchor-positive), axis=1)
                 - K.sum(K.square(anchor-negative), axis=1),
                 0.0)
    return K.mean(loss)
</code></pre>
","<p>tf.gather is a function to index an array. You gather the elements which you specify by the index argument. This is not natively posible for tensorflow tensors. </p>

<p>tf.gather(y_pred, tf.range(0, batch_size, 3)) is equivalent in numpy to y_pred[0:batch_size:3], which means that you return every third element starting from the first one. </p>
","{50999977, 56452714, 51143210, 43981134, 42608175, 44691406, 66456914, 36035092, 36764791, 40362011}","[{'QuestionId': 50999977, 'AnswerId': 67623659, 'URL': 'https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms/67623659#67623659', 'QuestionTitle': 'What does the gather function do in pytorch in layman terms?', 'Answer': '<p>This is based on @Ritesh <a href=""https://stackoverflow.com/a/54706716/2000548"">answer</a> (thanks @Ritesh!) with some real codes.</p>\n<p>The <code>torch.gather</code> API is</p>\n<p><code>torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor</code></p>\n<p><strong>Example 1</strong></p>\n<p>When <code>dim = 0</code>,</p>\n<p><a href=""https://i.stack.imgur.com/ZnXZD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZnXZD.png"" alt=""enter image description here"" /></a></p>\n<pre><code>dim = 0\ninput = torch.tensor([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\nindex = torch.tensor([[0, 1, 2], [1, 2, 0]])\n\noutput = torch.gather(input, dim, index)\n# tensor([[10, 14, 18],\n#         [13, 17, 12]])\n</code></pre>\n<p><strong>Example 2</strong></p>\n<p>When <code>dim = 1</code>,</p>\n<p><a href=""https://i.stack.imgur.com/JWu4L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JWu4L.png"" alt=""enter image description here"" /></a></p>\n<pre><code>dim = 1\ninput = torch.tensor([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\nindex = torch.tensor([[0, 1], [1, 2], [2, 0]])\n\noutput = torch.gather(input, dim, index)\n# tensor([[10, 11],\n#         [14, 15],\n#         [18, 16]])\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1621525078}, {'QuestionId': 50999977, 'AnswerId': 75480097, 'URL': 'https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms/75480097#75480097', 'QuestionTitle': 'What does the gather function do in pytorch in layman terms?', 'Answer': '<p><code>gather</code> allows you to take tensor indexing</p>\n<pre><code>&gt;&gt;&gt; torch.arange(6)[torch.tensor([1,5])]\ntensor([1, 5])\n</code></pre>\n<p>and do it in batches</p>\n<pre><code>&gt;&gt;&gt; a = torch.stack((torch.arange(6),torch.arange(6)), dim=0)\n&gt;&gt;&gt; torch.gather(a, dim=1, index=torch.tensor([[5,1],[5,1]]))\ntensor([[5, 1],\n        [5, 1]])\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1676605730}, {'QuestionId': 56452714, 'AnswerId': 71806700, 'URL': 'https://stackoverflow.com/questions/56452714/replacement-of-tf-gather-nd/71806700#71806700', 'QuestionTitle': 'replacement of &quot;tf.gather_nd&quot;', 'Answer': '<p>For other people who would like implement tf.gather_nd in pytorch, see <a href=""https://discuss.pytorch.org/t/how-to-do-the-tf-gather-nd-in-pytorch/6445/37"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/how-to-do-the-tf-gather-nd-in-pytorch/6445/37</a> and his colab notebook.\nI adapt it a little bit to implement it by numpy:</p>\n<pre><code>def gather_nd(params, indices, batch_dims=0):\n    &quot;&quot;&quot; use numpy and tensorflow to implement tf.gather_nd\n    Adapt from : https://discuss.pytorch.org/t/how-to-do-the-tf-gather-nd-in-pytorch/6445/37\n    &quot;&quot;&quot;\n    # firstly, convert to numpy type, then use numpy to execute operations\n    if isinstance(params, tf.Tensor):\n      params = params.numpy()\n    else:\n      if not isinstance(indices, np.ndarray):\n        raise ValueError(f\'params must be `tf.Tensor` or `numpy.ndarray`. Got {type(params)}\')\n    if isinstance(indices, tf.Tensor):\n      indices = indices.numpy()\n    else:\n      if not isinstance(indices, np.ndarray):\n        raise ValueError(f\'indices must be `tf.Tensor` or `numpy.ndarray`. Got {type(indices)}\')\n\n    if batch_dims == 0:\n        orig_shape = list(indices.shape)\n        num_samples = int(np.prod(orig_shape[:-1]))\n        m = orig_shape[-1]\n        n = len(params.shape)\n\n        if m &lt;= n:\n            out_shape = orig_shape[:-1] + list(params.shape[m:])\n        else:\n            raise ValueError(\n                f\'the last dimension of indices must less or equal to the rank of params. Got indices:{indices.shape}, params:{params.shape}. {m} &gt; {n}\'\n            )\n        # indices_ = tf.transpose(tf.reshape(indices, [num_samples, m]), perm=[1, 0])\n        indices = indices.reshape((num_samples, m)).transpose().tolist()\n        output = params[indices]    # (num_samples, ...)\n\n        return tf.reshape(output,out_shape)  # or return numpy type: output.reshape(out_shape)\n    else:\n        batch_shape = params.shape[:batch_dims]\n        orig_indices_shape = list(indices.shape)\n        orig_params_shape = list(params.shape)\n        assert (\n            batch_shape == indices.shape[:batch_dims]\n        ), f\'if batch_dims is not 0, then both &quot;params&quot; and &quot;indices&quot; have batch_dims leading batch dimensions that exactly match.\'\n        mbs = np.prod(batch_shape)\n        if batch_dims != 1:\n            params = params.reshape(mbs, *(params.shape[batch_dims:]))\n            indices = indices.reshape(mbs, *(indices.shape[batch_dims:]))\n        output = []\n        for i in range(mbs):\n            output.append(gather_nd(params[i], indices[i], batch_dims=0))\n        output =np.stack(output, axis=0)\n        output_shape = orig_indices_shape[:-1] + list(orig_params_shape[orig_indices_shape[-1]+batch_dims:])\n        return tf.reshape(output,output_shape)  # or return numpy type: output.reshape(output_shape)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1649495648}, {'QuestionId': 36035092, 'AnswerId': 39357384, 'URL': 'https://stackoverflow.com/questions/36035092/multi-dimensional-gather-in-tensorflow/39357384#39357384', 'QuestionTitle': 'Multi-dimensional gather in Tensorflow', 'Answer': '<p>It sounds like you want <a href=""https://www.tensorflow.org/api_docs/python/tf/gather_nd"" rel=""nofollow noreferrer"">gather_nd</a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1473193917}, {'QuestionId': 66456914, 'AnswerId': 66457805, 'URL': 'https://stackoverflow.com/questions/66456914/using-tf-gather-or-tf-gather-nd/66457805#66457805', 'QuestionTitle': 'Using tf.gather or tf.gather_nd', 'Answer': '<p>Use <code>tf.gather()</code> with <code>batch_dims</code> argument:</p>\n<pre><code>inds = tf.constant([[[0, 1], [1, 2], [0, 2]]])\noutput = tf.gather(inp, inds, batch_dims=2)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1614777178}, {'QuestionId': 50999977, 'AnswerId': 61262046, 'URL': 'https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms/61262046#61262046', 'QuestionTitle': 'What does the gather function do in pytorch in layman terms?', 'Answer': ""<p>@Ritesh and @cleros gave great answers (with <em>lots</em> of upvotes), but after reading them I was still a bit confused, and I know why. This post will perhaps help folks like me.</p>\n<p>For these sorts of exercises with rows and columns I think it <em>really</em> helps to use a non-square object, so let's start with a larger 4x3 <code>source</code> (<code>torch.Size([4, 3])</code>) using <code>source = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])</code>. This will give us</p>\n<pre><code>\\\\ This is the source tensor\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12]])\n</code></pre>\n<p>Now let's start indexing along the columns (<code>dim=1</code>) and create <code>index = torch.tensor([[0,0],[1,1],[2,2],[0,1]])</code>, which is a list of lists. Here's the <strong>key</strong>: since our dimension is columns, and the source has <code>4</code> rows, the <code>index</code> must contain <code>4</code> lists! We need a list for each row. Running <code>source.gather(dim=1, index=index)</code> will give us</p>\n<pre><code>tensor([[ 1,  1],\n        [ 5,  5],\n        [ 9,  9],\n        [10, 11]])\n</code></pre>\n<p>So, each list within <code>index</code> gives us the columns from which to pull the values. The 1st list of the <code>index</code> (<code>[0,0]</code>) is telling us to take to look at the 1st row of the <code>source</code> and take the 1st column of that row (it's zero-indexed) twice, which is <code>[1,1]</code>. The 2nd list of the <code>index</code> (<code>[1,1]</code>) is telling us to take to look at the 2nd row of <code>source</code> and take the 2nd column of that row twice, which is <code>[5,5]</code>. Jumping to the 4th list of the <code>index</code> (<code>[0,1]</code>), which is asking us to look at the 4th and final row of the <code>source</code>, is asking us to take the 1st column (<code>10</code>) and then the 2nd column (<code>11</code>) which gives us <code>[10,11]</code>.</p>\n<p>Here's a nifty thing: each list of your <code>index</code> has to be the same length, but they may be as long as you like! For example, with <code>index = torch.tensor([[0,1,2,1,0],[2,1,0,1,2],[1,2,0,2,1],[1,0,2,0,1]])</code>, <code>source.gather(dim=1, index=index)</code> will give us</p>\n<pre><code>tensor([[ 1,  2,  3,  2,  1],\n        [ 6,  5,  4,  5,  6],\n        [ 8,  9,  7,  9,  8],\n        [11, 10, 12, 10, 11]])\n</code></pre>\n<p>The output will always have the same number of rows as the <code>source</code>, but the number of columns will equal the length of each list in <code>index</code>. For example, the 2nd list of the <code>index</code> (<code>[2,1,0,1,2]</code>) is going to the 2nd row of the <code>source</code> and pulling, respectively, the 3rd, 2nd, 1st, 2nd and 3rd items, which is <code>[6,5,4,5,6]</code>. Note, the value of every element in <code>index</code> has to be less than the number of columns of <code>source</code> (in this case <code>3</code>), otherwise you get an <code>out of bounds</code> error.</p>\n<p>Switching to <code>dim=0</code>, we'll now be using the rows as opposed to the columns. Using the same <code>source</code>, we now need an <code>index</code> where the length of each list equals the number of columns in the <code>source</code>. Why? Because each element in the list represents the row from <code>source</code> as we move column by column.</p>\n<p>Therefore, <code>index = torch.tensor([[0,0,0],[0,1,2],[1,2,3],[3,2,0]])</code> will then have <code>source.gather(dim=0, index=index)</code> give us</p>\n<pre><code>tensor([[ 1,  2,  3],\n        [ 1,  5,  9],\n        [ 4,  8, 12],\n        [10,  8,  3]])\n</code></pre>\n<p>Looking at the 1st list in the <code>index</code> (<code>[0,0,0]</code>), we can see that we're moving across the 3 columns of <code>source</code> picking the 1st element (it's zero-indexed) of each column, which is <code>[1,2,3]</code>. The 2nd list in the <code>index</code> (<code>[0,1,2]</code>) tells us to move across the columns taking the 1st, 2nd and 3rd items, respectively, which is <code>[1,5,9]</code>. And so on.</p>\n<p>With <code>dim=1</code> our <code>index</code> had to have a number of lists equal to the number of rows in the <code>source</code>,  but each list could be as long, or short, as you like. With <code>dim=0</code>, each list in our <code>index</code> has to be the same length as the number of columns in the <code>source</code>, but we can now have as many lists as we like. Each value in <code>index</code>, however, needs to be less than the number of row in <code>source</code> (in this case <code>4</code>).</p>\n<p>For example, <code>index = torch.tensor([[0,0,0],[1,1,1],[2,2,2],[3,3,3],[0,1,2],[1,2,3],[3,2,0]])</code> would have <code>source.gather(dim=0, index=index)</code> give us</p>\n<pre><code>tensor([[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12],\n        [ 1,  5,  9],\n        [ 4,  8, 12],\n        [10,  8,  3]])\n</code></pre>\n<p>With <code>dim=1</code> the output always has the same number of rows as the <code>source</code>, although the number of columns will equal the length of the lists in <code>index</code>. The number of lists in <code>index</code> has to equal the number of rows in <code>source</code>. Each value in <code>index</code>, however, needs to be less than the number of columns in <code>source</code>.</p>\n<p>With <code>dim=0</code> the output always has the same number of columns as the <code>source</code>, but the number of rows will equal the number of lists in <code>index</code>. The length of each list in <code>index</code> has to equal the number of columns in <code>source</code>. Each value in <code>index</code>, however, needs to be less than the number of row in <code>source</code>.</p>\n<p>That's it for two dimensions. Moving beyond that will follow the same patterns.</p>\n"", 'IsAccepted': False, 'CreationDate': 1587081439}, {'QuestionId': 56452714, 'AnswerId': 56457345, 'URL': 'https://stackoverflow.com/questions/56452714/replacement-of-tf-gather-nd/56457345#56457345', 'QuestionTitle': 'replacement of &quot;tf.gather_nd&quot;', 'Answer': '<p>This function should do an equivalent work:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndef my_gather_nd(params, indices):\n    idx_shape = tf.shape(indices)\n    params_shape = tf.shape(params)\n    idx_dims = idx_shape[-1]\n    gather_shape = params_shape[idx_dims:]\n    params_flat = tf.reshape(params, tf.concat([[-1], gather_shape], axis=0))\n    axis_step = tf.cumprod(params_shape[:idx_dims], exclusive=True, reverse=True)\n    indices_flat = tf.reduce_sum(indices * axis_step, axis=-1)\n    result_flat = tf.gather(params_flat, indices_flat)\n    return tf.reshape(result_flat, tf.concat([idx_shape[:-1], gather_shape], axis=0))\n\n# Test\nnp.random.seed(0)\nwith tf.Graph().as_default(), tf.Session() as sess:\n    params = tf.constant(np.random.rand(10, 20, 30).astype(np.float32))\n    indices = tf.constant(np.stack([np.random.randint(10, size=(5, 8)),\n                                    np.random.randint(20, size=(5, 8))], axis=-1))\n    result1, result2 = sess.run((tf.gather_nd(params, indices),\n                                 my_gather_nd(params, indices)))\n    print(np.allclose(result1, result2))\n    # True\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1559725365}, {'QuestionId': 42608175, 'AnswerId': 55055527, 'URL': 'https://stackoverflow.com/questions/42608175/what-does-tf-gather-nd-intuitively-do/55055527#55055527', 'QuestionTitle': 'What does tf.gather_nd intuitively do?', 'Answer': '<p>You provide a tensor and indices representing locations in that tensor. It returns the elements of the tensor corresponding to the indices you provide. </p>\n\n<p>EDIT: An example</p>\n\n<pre><code>import tensorflow as tf\nsess = tf.Session()\nx = [[1,2,3],[4,5,6]]\ny = tf.gather_nd(x, [[1,1],[1,2]])\nprint(sess.run(y))\n</code></pre>\n\n<blockquote>\n  <p>[5, 6]</p>\n</blockquote>\n', 'IsAccepted': False, 'CreationDate': 1552009217}, {'QuestionId': 50999977, 'AnswerId': 54706716, 'URL': 'https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms/54706716#54706716', 'QuestionTitle': 'What does the gather function do in pytorch in layman terms?', 'Answer': '<p><code>torch.gather</code> creates a new tensor from the input tensor by taking the values from each row along the input dimension <code>dim</code>. The values in <code>torch.LongTensor</code>, passed as <code>index</code>, specify which value to take from each \'row\'. The dimension of the output tensor is same as the dimension of index tensor. Following illustration from the official docs explains it more clearly:\n<a href=""https://i.stack.imgur.com/nudGq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nudGq.png"" alt=""Pictoral representation from the docs"" /></a></p>\n<p>(Note: In the illustration, indexing starts from 1 and not 0).</p>\n<p>In first example, the dimension given is along rows (top to bottom), so for (1,1) position of <code>result</code>, it takes row value from the <code>index</code> for the <code>src</code> that is <code>1</code>. At (1,1) in source value is <code>1</code> so,  outputs <code>1</code> at (1,1) in <code>result</code>.\nSimilarly for (2,2) the row value from the index for <code>src</code> is  <code>3</code>. At (3,2) the value in <code>src</code> is <code>8</code> and hence outputs <code>8</code> and so on.</p>\n<p>Similarly for second example, indexing is along columns, and hence at (2,2)  position of the <code>result</code>, the column value from the index for <code>src</code> is <code>3</code>, so at (2,3) from <code>src</code> ,<code>6</code> is taken and outputs to <code>result</code> at (2,2)</p>\n', 'IsAccepted': False, 'CreationDate': 1550224496}, {'QuestionId': 51143210, 'AnswerId': 51143657, 'URL': 'https://stackoverflow.com/questions/51143210/batched-gather-gathernd/51143657#51143657', 'QuestionTitle': 'Batched Gather/GatherND', 'Answer': '<p>Not sure if this qualifies as ""simple"", but you can use <code>gather_nd</code> for this:</p>\n\n<pre><code>def batched_gather(values, indices):\n    row_indices = tf.range(0, tf.shape(values)[0])[:, tf.newaxis]\n    row_indices = tf.tile(row_indices, [1, tf.shape(indices)[-1]])\n    indices = tf.stack([row_indices, indices], axis=-1)\n    return tf.gather_nd(values, indices)\n</code></pre>\n\n<p>Explanation: The idea is to construct index vectors such as <code>[0, 1]</code> meaning ""the value in the 0th row and 1st column"".<br>\nThe column indices are already given in the <code>indices</code> argument to the function.<br>\nThe row indices are a simple progression from 0 to e.g. 128 (in your example), but are repeated (tiled) in accordance with the number of column indices for each row (3 in your example; could hardcode this instead of using <code>tf.shape</code> if this number is fixed).<br>\nThe row and column indices are then stacked to produce the index vectors. In your example, the resulting indices would be</p>\n\n<pre><code>array([[[0, 2],\n        [0, 3],\n        [0, 6]],\n\n       [[1, 0],\n        [1, 2],\n        [1, 3]]])\n</code></pre>\n\n<p>and <code>gather_nd</code> produces the desired result.</p>\n', 'IsAccepted': True, 'CreationDate': 1530565120}, {'QuestionId': 50999977, 'AnswerId': 51032153, 'URL': 'https://stackoverflow.com/questions/50999977/what-does-the-gather-function-do-in-pytorch-in-layman-terms/51032153#51032153', 'QuestionTitle': 'What does the gather function do in pytorch in layman terms?', 'Answer': '\n\n<p>The <code>torch.gather</code> function (or <code>torch.Tensor.gather</code>)  is a multi-index selection method. Look at the following example from the official docs:</p>\n\n<pre class=""lang-py prettyprint-override""><code>t = torch.tensor([[1,2],[3,4]])\nr = torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))\n# r now holds:\n# tensor([[ 1,  1],\n#        [ 4,  3]])\n</code></pre>\n\n<p>Let\'s start with going through the semantics of the different arguments: The first argument, <code>input</code>, is the source tensor that we want to select elements from. The second, <code>dim</code>, is the dimension (or axis in tensorflow/numpy) that we want to collect along. And finally, <code>index</code> are the indices to index <code>input</code>.\nAs for the semantics of the operation, this is how the official docs explain it:</p>\n\n<pre class=""lang-py prettyprint-override""><code>out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n</code></pre>\n\n<p>So let\'s go through the example.</p>\n\n<p>the input tensor is <code>[[1, 2], [3, 4]]</code>, and the dim argument is <code>1</code>, i.e. we want to collect from the second dimension. The indices for the second dimension are given as <code>[0, 0]</code> and <code>[1, 0]</code>. </p>\n\n<p>As we ""skip"" the first dimension (the dimension we want to collect along is <code>1</code>), the first dimension of the result is implicitly given as the first dimension of the <code>index</code>. That means that the indices hold the second dimension, or the column indices, but not the row indices. Those are given by the indices of the <code>index</code> tensor itself.\nFor the example, this means that the output will have in its first row a selection of the elements of the <code>input</code> tensor\'s first row as well, as given by the first row of the <code>index</code> tensor\'s first row. As the column-indices are given by <code>[0, 0]</code>, we therefore select the first element of the first row of the input twice, resulting in <code>[1, 1]</code>. Similarly, the elements of the second row of the result are a result of indexing the second row of the <code>input</code> tensor by the elements of the second row of the <code>index</code> tensor, resulting in <code>[4, 3]</code>. </p>\n\n<p>To illustrate this even further, let\'s swap the dimension in the example:</p>\n\n<pre class=""lang-py prettyprint-override""><code>t = torch.tensor([[1,2],[3,4]])\nr = torch.gather(t, 0, torch.tensor([[0,0],[1,0]]))\n# r now holds:\n# tensor([[ 1,  2],\n#        [ 3,  2]])\n</code></pre>\n\n<p>As you can see, the indices are now collected along the first dimension.</p>\n\n<p>For the example you referred, </p>\n\n<pre class=""lang-py prettyprint-override""><code>current_Q_values = Q(obs_batch).gather(1, act_batch.unsqueeze(1))\n</code></pre>\n\n<p><code>gather</code> will index the rows of the q-values (i.e. the per-sample q-values in a batch of q-values) by the batch-list of actions. The result will be the same as if you had done the following (though it will be much faster than a loop):</p>\n\n<pre class=""lang-py prettyprint-override""><code>q_vals = []\nfor qv, ac in zip(Q(obs_batch), act_batch):\n    q_vals.append(qv[ac])\nq_vals = torch.cat(q_vals, dim=0)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1529962810}, {'QuestionId': 42608175, 'AnswerId': 42608263, 'URL': 'https://stackoverflow.com/questions/42608175/what-does-tf-gather-nd-intuitively-do/42608263#42608263', 'QuestionTitle': 'What does tf.gather_nd intuitively do?', 'Answer': ""<p>Ok, so think about it like this:</p>\n\n<p>You are providing a list of index values to index the provided tensor to get those slices. The first dimension of the indices you provide is for each index you will perform. Let's pretend that tensor is just a list of lists.</p>\n\n<p>[[0]] means you want to get one specific slice(list) at index 0 in the provided tensor. Just like this:</p>\n\n<pre><code>[tensor[0]]\n</code></pre>\n\n<p>[[0], [1]] means you want get two specific slices at indices 0 and 1 like this:</p>\n\n<pre><code>[tensor[0], tensor[1]]\n</code></pre>\n\n<p>Now what if tensor is more than one dimensions? We do the same thing: </p>\n\n<p>[[0, 0]] means you want to get one slice at index [0,0] of the 0-th list. Like this:</p>\n\n<pre><code>[tensor[0][0]]\n</code></pre>\n\n<p>[[0, 1], [2, 3]] means you want return two slices at the indices and dimensions provided. Like this: </p>\n\n<pre><code>[tensor[0][1], tensor[2][3]]\n</code></pre>\n\n<p>I hope that makes sense. I tried using Python indexing to help explain how it would look in Python to do this to a list of lists.</p>\n"", 'IsAccepted': False, 'CreationDate': 1488716336}, {'QuestionId': 36764791, 'AnswerId': 45394214, 'URL': 'https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension/45394214#45394214', 'QuestionTitle': 'In Tensorflow, how to use tf.gather() for the last dimension?', 'Answer': '<p>As of TensorFlow 1.3 <code>tf.gather</code> has an <code>axis</code> parameter, so the various workarounds here are no longer necessary.</p>\n\n<p><a href=""https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/gather"" rel=""noreferrer"">https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/gather</a>\n<a href=""https://github.com/tensorflow/tensorflow/issues/11223"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/11223</a></p>\n', 'IsAccepted': False, 'CreationDate': 1501363624}, {'QuestionId': 36764791, 'AnswerId': 45216470, 'URL': 'https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension/45216470#45216470', 'QuestionTitle': 'In Tensorflow, how to use tf.gather() for the last dimension?', 'Answer': '<p>You can try this way, for instance(in most cases in NLP at the least),    </p>\n\n<p>The parameter is of shape <code>[batch_size, depth]</code> and the indices are [i, j, k, n, m] of which the length is batch_size. Then <code>gather_nd</code> can be helpful.  </p>\n\n<pre><code>parameters = tf.constant([\n                          [11, 12, 13], \n                          [21, 22, 23], \n                          [31, 32, 33], \n                          [41, 42, 43]])    \ntargets = tf.constant([2, 1, 0, 1])    \nbatch_nums = tf.range(0, limit=parameters.get_shape().as_list()[0])     \nindices = tf.stack((batch_nums, targets), axis=1) # the axis is the dimension number   \nitems = tf.gather_nd(parameters, indices)  \n# which is what we want: [13, 22, 31, 42]\n</code></pre>\n\n<p>This snippet first find the fist dimension through the batch_num and then fetch the item along that dimension by the target number. </p>\n', 'IsAccepted': False, 'CreationDate': 1500557990}, {'QuestionId': 44691406, 'AnswerId': 44692798, 'URL': 'https://stackoverflow.com/questions/44691406/how-to-understand-tf-get-collection-in-tensorflow/44692798#44692798', 'QuestionTitle': 'How to understand tf.get_collection() in TensorFlow', 'Answer': '<p>As described in the string doc:</p>\n\n<blockquote>\n  <ul>\n  <li><code>TRAINABLE_VARIABLES</code>: the subset of <code>Variable</code> objects that will\n  be trained by an optimizer.  </li>\n  </ul>\n</blockquote>\n\n<p>and</p>\n\n<blockquote>\n  <p>scope: (Optional.) A string. If supplied, the resulting list is filtered\n          to include only items whose <code>name</code> attribute matches <code>scope</code> using\n          <code>re.match</code>. Items without a <code>name</code> attribute are never returned if a\n          scope is supplied. The choice of <code>re.match</code> means that a <code>scope</code> without\n          special tokens filters by prefix.</p>\n</blockquote>\n\n<p>So it will return the list of trainable variables in the given scope.</p>\n', 'IsAccepted': False, 'CreationDate': 1498116369}, {'QuestionId': 44691406, 'AnswerId': 44693753, 'URL': 'https://stackoverflow.com/questions/44691406/how-to-understand-tf-get-collection-in-tensorflow/44693753#44693753', 'QuestionTitle': 'How to understand tf.get_collection() in TensorFlow', 'Answer': '<p>A collection is nothing but a named set of values.</p>\n\n<p>Every value is a node of the computational graph.</p>\n\n<p>Every node has its name and the name is composed by the concatenation of scopes, <code>/</code> and values, like: <code>preceding/scopes/in/that/way/value</code></p>\n\n<p><code>get_collection</code>, without <code>scope</code> allow fetching every value in the collection without applying any filter operation.</p>\n\n<p>When the <code>scope</code> parameter is present, every element of the collection is filtered and its returned only if the name of the node starts with the specified <code>scope</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1498119089}, {'QuestionId': 36764791, 'AnswerId': 44056276, 'URL': 'https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension/44056276#44056276', 'QuestionTitle': 'In Tensorflow, how to use tf.gather() for the last dimension?', 'Answer': ""<p>A  correct version of @Andrei's answer would read</p>\n\n<pre><code>cat_idx = tf.stack([tf.range(0, tf.shape(x)[0]), indices_for_dim1], axis=1)\nresult = tf.gather_nd(matrix, cat_idx)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1495135531}, {'QuestionId': 43981134, 'AnswerId': 43983596, 'URL': 'https://stackoverflow.com/questions/43981134/tensorflow-gather-or-gather-nd/43983596#43983596', 'QuestionTitle': 'tensorflow gather or gather_nd', 'Answer': ""<p>If I've understood your question correctly, you'll want to use </p>\n\n<pre><code>output = tf.gather_nd(tensor2, indices)\n</code></pre>\n\n<p>with <code>indices</code> being a matrix of shape <code>(batch_size, 48, 48, 3)</code> such that</p>\n\n<pre><code>indices[sample][i][j] = [i, row, col]\n</code></pre>\n\n<p>where<code>(row, col)</code> are the coordinates of the value you want to fetch in <code>tensor2</code>. They are a translation of the content given in <code>tensor1</code>, coded in 2 numbers instead of 1: </p>\n\n<pre><code>(row, col) = (tensor1[i, j] / 32, tensor1[i, j] % 32)\n</code></pre>\n\n<p>To create <code>indices</code> dynamically, something like that should do it: </p>\n\n<pre><code>batch_size = tf.shape(tensor1)[0]\ni_mat = tf.transpose(tf.reshape(tf.tile(tf.range(batch_size), [48*48]),\n                                   [48, 48, batch_size]))\n# i_mat should be such that i_matrix[i, j, k, l]=i\nmat_32 = tf.fill(value=tf.constant(32, dtype=tf.int32), dims=[batch_size, 48, 48])\nrow_mat = tf.floor_div(tensor1, mat_32)\ncol_mat = tf.mod(tensor1, mat_32)\nindices = tf.stack([i_mat, row_mat, col_mat], axis=-1)\n\noutput = tf.gather_nd(tensor2, indices)\n</code></pre>\n\n<p><strong>EDIT 2</strong></p>\n\n<p>The code above has changed a bit.</p>\n\n<p>The code above considers that your input tensors are actually of shape <code>(batch_size, 48, 48)</code> and <code>(batch_size, 32, 32)</code>, as opposed to <code>(batch_size, 48, 48, 1)</code> and <code>(batch_size, 32, 32, 1)</code>. To correct that, use for instance </p>\n\n<pre><code>tensor1=tf.squeeze(tensor1, axis=-1)\ntensor2=tf.squeeze(tensor2, axis=-1)\n</code></pre>\n\n<p>before my code above, and</p>\n\n<pre><code>output = tf.expand_dims(tf.gather_nd(tensor2, indices), axis=-1)\ntensor1= tf.expand_dims(tensor1, axis=-1)\ntensor2= tf.expand_dims(tensor2, axis=-1)\n</code></pre>\n\n<p>at the end</p>\n"", 'IsAccepted': False, 'CreationDate': 1494862785}, {'QuestionId': 36764791, 'AnswerId': 42380139, 'URL': 'https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension/42380139#42380139', 'QuestionTitle': 'In Tensorflow, how to use tf.gather() for the last dimension?', 'Answer': '<p>Tensor doesn\'t have attribute shape, but get_shape() method. Below is runnable by Python 2.7</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\nimport numpy as np\nx = tf.constant([[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]])\nidx = tf.constant([1, 0, 2])\nidx_flattened = tf.range(0, x.get_shape()[0]) * x.get_shape()[1] + idx\ny = tf.gather(tf.reshape(x, [-1]),  # flatten input\n              idx_flattened)  # use flattened indices\n\nwith tf.Session(\'\'):\n  print y.eval()  # [2 4 9]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1487720803}, {'QuestionId': 36764791, 'AnswerId': 42534224, 'URL': 'https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension/42534224#42534224', 'QuestionTitle': 'In Tensorflow, how to use tf.gather() for the last dimension?', 'Answer': '<p>Yet another solution using tf.unstack(...), tf.gather(...) and tf.stack(..)</p>\n\n<p>Code:</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\nimport numpy as np\n\nshape = [2, 2, 2, 10] \nL = np.arange(np.prod(shape))\nL = np.reshape(L, shape)\n\nindices = [0, 2, 3, 8]\naxis = -1 # last dimension\n\ndef gather_axis(params, indices, axis=0):\n    return tf.stack(tf.unstack(tf.gather(tf.unstack(params, axis=axis), indices)), axis=axis)\n\nprint(L)\nwith tf.Session() as sess:\n    partL = sess.run(gather_axis(L, indices, axis))\n    print(partL)\n</code></pre>\n\n<p>Result:</p>\n\n<pre class=""lang-py prettyprint-override""><code>L = \n[[[[ 0  1  2  3  4  5  6  7  8  9]\n   [10 11 12 13 14 15 16 17 18 19]]\n\n  [[20 21 22 23 24 25 26 27 28 29]\n   [30 31 32 33 34 35 36 37 38 39]]]\n\n\n [[[40 41 42 43 44 45 46 47 48 49]\n   [50 51 52 53 54 55 56 57 58 59]]\n\n  [[60 61 62 63 64 65 66 67 68 69]\n   [70 71 72 73 74 75 76 77 78 79]]]]\n\npartL = \n[[[[ 0  2  3  8]\n   [10 12 13 18]]\n\n  [[20 22 23 28]\n   [30 32 33 38]]]\n\n\n [[[40 42 43 48]\n   [50 52 53 58]]\n\n  [[60 62 63 68]\n   [70 72 73 78]]]]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1488378237}, {'QuestionId': 36764791, 'AnswerId': 41845855, 'URL': 'https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension/41845855#41845855', 'QuestionTitle': 'In Tensorflow, how to use tf.gather() for the last dimension?', 'Answer': ""<p>With gather_nd you can now do this as follows:</p>\n\n<pre><code>cat_idx = tf.concat([tf.range(0, tf.shape(x)[0]), indices_for_dim1], axis=0)\nresult = tf.gather_nd(matrix, cat_idx)\n</code></pre>\n\n<p>Also, as reported by user Nova in a thread referenced by @Yaroslav Bulatov's:</p>\n\n<pre><code>x = tf.constant([[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]])\nidx = tf.constant([1, 0, 2])\nidx_flattened = tf.range(0, x.shape[0]) * x.shape[1] + idx\ny = tf.gather(tf.reshape(x, [-1]),  # flatten input\n              idx_flattened)  # use flattened indices\n\nwith tf.Session(''):\n  print y.eval()  # [2 4 9]\n</code></pre>\n\n<p>The gist is flatten the tensor and use strided 1D addressing with tf.gather(...).</p>\n"", 'IsAccepted': False, 'CreationDate': 1485329713}, {'QuestionId': 40362011, 'AnswerId': 40368073, 'URL': 'https://stackoverflow.com/questions/40362011/how-to-gather-data-in-my-case-using-gather-nd-in-tensorflow/40368073#40368073', 'QuestionTitle': 'How to gather data in my case using gather_nd in tensorflow?', 'Answer': ""<p>A mixture of tf.range() and some tiling seems to work:</p>\n\n<pre><code>def index_matrix_to_pairs(index_matrix):\n  replicated_first_indices = tf.tile(\n      tf.expand_dims(tf.range(tf.shape(index_matrix)[0]), dim=1), \n      [1, tf.shape(index_matrix)[1]])\n  return tf.pack([replicated_first_indices, index_matrix], axis=2)\n\nstart = [[4, 1, 6, 2],\n [1, 4, 0, 9],\n [5, 1, 9, 6]]\nwith tf.Session():\n  print(index_matrix_to_pairs(start).eval())\n</code></pre>\n\n<p>Gives:</p>\n\n<pre><code>[[[0 4]\n  [0 1]\n  [0 6]\n  [0 2]]\n\n [[1 1]\n  [1 4]\n  [1 0]\n  [1 9]]\n\n [[2 5]\n  [2 1]\n  [2 9]\n  [2 6]]]\n</code></pre>\n\n<p>It's just generating the first part of each pair with a tiled tf.range() op, then packing that with the specified indices.</p>\n"", 'IsAccepted': True, 'CreationDate': 1478031927}, {'QuestionId': 36764791, 'AnswerId': 40314311, 'URL': 'https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension/40314311#40314311', 'QuestionTitle': 'In Tensorflow, how to use tf.gather() for the last dimension?', 'Answer': '<p>Implementing 2. from @Yaroslav Bulatov\'s:</p>\n\n<pre><code>#Your indices\nindices = [0, 2, 3, 8]\n\n#Remember for final reshaping\nn_indices = tf.shape(indices)[0]\n\nflattened_L = tf.reshape(L, [-1])\n\n#Walk strided over the flattened array\noffset = tf.expand_dims(tf.range(0, tf.reduce_prod(tf.shape(L)), tf.shape(L)[-1]), 1)\nflattened_indices = tf.reshape(tf.reshape(indices, [-1])+offset, [-1])\nselected_rows = tf.gather(flattened_L, flattened_indices)\n\n#Final reshape\npartL = tf.reshape(selected_rows, tf.concat(0, [tf.shape(L)[:-1], [n_indices]]))\n</code></pre>\n\n<p>Credit to <a href=""https://stackoverflow.com/questions/36088277/how-to-select-rows-from-a-3-d-tensor-in-tensorflow"">How to select rows from a 3-D Tensor in TensorFlow?</a></p>\n', 'IsAccepted': False, 'CreationDate': 1477695505}, {'QuestionId': 36764791, 'AnswerId': 36777781, 'URL': 'https://stackoverflow.com/questions/36764791/in-tensorflow-how-to-use-tf-gather-for-the-last-dimension/36777781#36777781', 'QuestionTitle': 'In Tensorflow, how to use tf.gather() for the last dimension?', 'Answer': '<p>There\'s a tracking bug to support this use-case here: <a href=""https://github.com/tensorflow/tensorflow/issues/206"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/206</a></p>\n\n<p>For now you can:</p>\n\n<ol>\n<li><p>transpose your matrix so that dimension to gather is first (transpose is  expensive)</p></li>\n<li><p>reshape your tensor into 1d (reshape is cheap) and turn your gather column indices into a list of individual element indices at linear indexing, then reshape back</p></li>\n<li>use <code>gather_nd</code>. Will still need to turn your column indices into list of individual element indices.</li>\n</ol>\n', 'IsAccepted': True, 'CreationDate': 1461263599}]","{56802840, 66999579}","['<p>tf.gather is a function to index an array. You gather the elements which you specify by the index argument.', '<p>tf.gather is a function to index an array. You gather the elements which you specify by the index argument. This is not natively posible for tensorflow tensors. </p>\n\n<p>tf.gather(y_pred, tf.range(0, batch_size, 3)) is equivalent in numpy to y_pred[0:batch_size:3], which means that you return every third element starting from the first one. </p>\n', '<p>You can use <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer""><code>tf.gather</code></a>.</p>\n<pre><code>&gt;&gt;&gt; tf.gather(words,[0,2,5])\n&lt;tf.Tensor: shape=(3, 1), dtype=string, numpy=\narray([[b\'\'],\n       [b\'the\'],\n       [b\'is\']], dtype=object)&gt;\n</code></pre>\n<p>Read more in the guide: <a href=""https://www.tensorflow.org/guide/tensor_slicing"" rel=""nofollow noreferrer"">Introduction to tensor slicing</a></p>\n']","{'https://stackoverflow.com/questions/56802840/what-exactly-tensorflow-gather-does/56802999#56802999', 'https://stackoverflow.com/questions/66999579/what-is-the-tensorflow-equivalent-of-numpy-tuple-array-indexing/67003695#67003695'}",,0.20335095031109476,0.09433773634592517
8,59361689,tf.keras.backend,Documentation Replication on Other Examples,Redundancies in tf.keras.backend and tensorflow libraries,"<p>I have been working in TensorFlow for about a year now, and I am transitioning from TF 1.x to TF 2.0, and I am looking for some guidance on how to use the <code>tf.keras.backend</code> library in TF 2.0. I understand that the transition to TF 2.0 is supposed to remove a lot of redundancies in modeling and building graphs, since there were many ways to create equivalent layers in earlier TensorFlow versions (and I'm insanely grateful for that change!), but I'm getting stuck on understanding when to use <code>tf.keras.backend</code>, because the operations appear redundant with other TensorFlow libraries. </p>

<p>I see that some of the functions in <code>tf.keras.backend</code> are redundant with other TensorFlow libraries. For instance, <code>tf.keras.backend.abs</code> and <code>tf.math.abs</code> are not aliases (or at least, they're not listed as aliases in the documentation), but both take the absolute value of a tensor. After examining the source code, it looks like <code>tf.keras.backend.abs</code> calls the <code>tf.math.abs</code> function, and so I really do not understand why they are not aliases. Other <code>tf.keras.backend</code> operations don't appear to be duplicated in TensorFlow libraries, but it looks like there are TensorFlow functions that can do equivalent things. For instance, <code>tf.keras.backend.cast_to_floatx</code> can be substituted with <code>tf.dtypes.cast</code> as long as you explicitly specify the dtype. I am wondering two things:</p>

<ol>
<li>when is it best to use the <code>tf.keras.backend</code> library instead of the equivalent TensorFlow functions?</li>
<li>is there a difference in these functions (and other equivalent <code>tf.keras.backend</code> functions) that I am missing?</li>
</ol>
","<p>Short answer: Prefer tensorflow's native API such as <code>tf.math.*</code> to the<code>tf.keras.backend.*</code> API wherever possible.</p>

<p>Longer answer:</p>

<ul>
<li>The <code>tf.keras.backend.*</code> API can be <em>mostly</em> viewed as a remnant of the <code>keras.backend.*</code> API. The latter is a design that serves the ""exchangeable backend"" design of the original (non-TF-specific) keras. This relates to the historical aspect of keras, which supports multiple backend libraries, among which tensorflow used to be just one of them. Back in 2015 and 2016, other backends, such as Theano and MXNet were quite popular too. But going into 2017 and 2018, tensorflow became the dominant backend of keras users. Eventually keras became a part of the tensorflow API (in 2.x and later minor versions of 1.x). In the old multi-backend world, the <code>backend.*</code> API provides a backend-independent abstraction over the myriad of supported backend. But in the tf.keras world, the value of the backend API is much more limited.</li>
<li>The various functions in <code>tf.keras.backend.*</code> can be divided into a few categories:

<ol>
<li>Thin wrappers around the equivalent or mostly-equivalent tensorflow native API. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2363"" rel=""noreferrer"">tf.keras.backend.less</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2431"" rel=""noreferrer"">tf.keras.backend.sin</a></li>
<li>Slightly thicker wrappers around tensorflow native APIs, with more features included. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2589"" rel=""noreferrer"">tf.keras.backend.batch_normalization</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869"" rel=""noreferrer"">tf.keras.backend.conv2d</a>(<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869</a>). They often perform proprocessing and implement other logics, which make your life easier than using native tensorflow API.</li>
<li>Unique functions that don't have equivalent in the native tensorflow API. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L3809"" rel=""noreferrer"">tf.keras.backend.rnn</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L342"" rel=""noreferrer"">tf.keras.backend.set_learning_phase</a></li>
</ol></li>
</ul>

<p>For category 1, use native tensorflow APIs. For categories 2 and 3, you may want to use the <code>tf.keras.backend.*</code> API, as long as you can find it in the documentation page: <a href=""https://www.tensorflow.org/api_docs/python/"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/</a>, because the documented ones have backward compatibility guarantees, so that you don't need to worry about a future version of tensorflow removing it or changing its behavior.</p>
","{55466308, 59847045, 59359113, 54231821, 45869776, 54083955, 53589333, 45172725, 60505369, 47104415}","[{'QuestionId': 59847045, 'AnswerId': 64076758, 'URL': 'https://stackoverflow.com/questions/59847045/should-i-use-tf-function-for-all-functions/64076758#64076758', 'QuestionTitle': 'Should I use @tf.function for all functions?', 'Answer': '<p>Per my understanding and according to the documentation, using <code>tf.function</code> is highly recommended mainly for speeding up your code since the code wrapped by <code>tf.function</code> would be converted to a graph and therefore there is a room for some optimizations (e.g. op pruning, folding, etc.) to be done which may not be performed when the same code is run eagerly.</p>\n<p>However, there are also a few cases where using <code>tf.function</code> might incur additional overhead or does not result in noticeable speedups. One notable case is when the wrapped function is <strong>small</strong> and only <strong>used a few times</strong> in your code and therefore the overhead of calling the graph might be relatively large. Another case is when most of the computations are already done on an accelerator device (e.g. GPU, TPU), and therefore the speedups gained by graph computation might not be significant.</p>\n<p>There is also <a href=""https://www.tensorflow.org/guide/intro_to_graphs#seeing_the_speed_up"" rel=""nofollow noreferrer"">a section in the documentation</a> where the speedups are discussed in various scenarios, and at the beginning of this section the two cases above have been mentioned:</p>\n<blockquote>\n<p>Just wrapping a tensor-using function in <code>tf.function</code> does not automatically speed up your code. For small functions called a few times on a single machine, the overhead of calling a graph or graph fragment may dominate runtime. Also, if most of the computation was already happening on an accelerator, such as stacks of GPU-heavy convolutions, the graph speedup won\'t be large.</p>\n<p>For complicated computations, graphs can provide a significant speedup. This is because graphs reduce the Python-to-device communication and perform some speedups.</p>\n</blockquote>\n<p>But at the end of the day, <em>if it\'s applicable to your workflow</em>, I think the best way to determine this for your specific use case and environment is to profile your code when it gets executed in eager mode (i.e. without using <code>tf.function</code>) vs. when it gets executed in graph mode (i.e. using <code>tf.function</code> extensively).</p>\n', 'IsAccepted': False, 'CreationDate': 1601117456}, {'QuestionId': 59847045, 'AnswerId': 61516502, 'URL': 'https://stackoverflow.com/questions/59847045/should-i-use-tf-function-for-all-functions/61516502#61516502', 'QuestionTitle': 'Should I use @tf.function for all functions?', 'Answer': '<p>TLDR: It depends on your function and whether you are in production or development. Don\'t use <code>tf.function</code> if you want to be able to debug your function easily, or if it falls under the limitations of AutoGraph or tf.v1 code compatibility.\nI would highly recommend watching the Inside TensorFlow talks about <a href=""https://www.youtube.com/watch?v=NIEgzljyDyI"" rel=""noreferrer"">AutoGraph</a> and <a href=""https://www.youtube.com/watch?v=MSXouZPyTrc"" rel=""noreferrer"">Functions, not Sessions</a>.</p>\n\n<p>In the following I\'ll break down the reasons, which are all taken from information made available online by Google.</p>\n\n<p>In general, the <code>tf.function</code> decorator causes a function to be compiled as a callable that executes a TensorFlow graph. This entails:</p>\n\n<ul>\n<li>Conversion of the code through AutoGraph if required (including any functions called from an annotated function)</li>\n<li>Tracing and executing the generated graph code</li>\n</ul>\n\n<p><a href=""https://github.com/tensorflow/community/blob/master/rfcs/20180918-functions-not-sessions-20.md"" rel=""noreferrer"">There is detailed information available on the design ideas behind this.</a></p>\n\n<h2>Benefits of decorating a function with <code>tf.function</code></h2>\n\n<h3>General benefits</h3>\n\n<ul>\n<li><em>Faster execution</em>, especially if the function consists of many small ops <a href=""https://www.tensorflow.org/guide/function"" rel=""noreferrer"">(Source)</a></li>\n</ul>\n\n<h3>For functions with Python code / Using AutoGraph via <code>tf.function</code> decoration</h3>\n\n<p>If you want to use AutoGraph, using <code>tf.function</code> is highly recommended over calling AutoGraph directly.\nReasons for this include: Automatic control dependencies, it is required for some APIs, more caching, and exception helpers <a href=""https://www.youtube.com/watch?v=NIEgzljyDyI&amp;t=1450s"" rel=""noreferrer"">(Source)</a>.</p>\n\n<h2>Drawbacks of decorating a function with <code>tf.function</code></h2>\n\n<h3>General drawbacks</h3>\n\n<ul>\n<li>If the function only consists of few expensive ops, there will not be much speedup <a href=""https://www.tensorflow.org/guide/function"" rel=""noreferrer"">(Source)</a></li>\n</ul>\n\n<h3>For functions with Python code / Using AutoGraph via <code>tf.function</code> decoration</h3>\n\n<ul>\n<li>No exception catching (should be done in eager mode; outside of the decorated function) <a href=""https://www.youtube.com/watch?v=NIEgzljyDyI&amp;t=2406s"" rel=""noreferrer"">(Source)</a></li>\n<li>Debugging is much harder</li>\n<li>Limitations due to hidden side effects and TF control flow</li>\n</ul>\n\n<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md"" rel=""noreferrer"">Detailed information on AutoGraph limitations is available.</a></p>\n\n<h3>For functions with tf.v1 code</h3>\n\n<ul>\n<li>It is not allowed to create variables more than once in <code>tf.function</code>, but this is subject to change as tf.v1 code is phased out <a href=""https://www.youtube.com/watch?v=MSXouZPyTrc&amp;t=1018s"" rel=""noreferrer"">(Source)</a></li>\n</ul>\n\n<h3>For functions with tf.v2 code</h3>\n\n<ul>\n<li>No specific drawbacks</li>\n</ul>\n\n<h2>Examples of limitations</h2>\n\n<h3>Creating variables more than once</h3>\n\n<p>It is not allowed to create variables more than once, such as <code>v</code> in the following example:</p>\n\n<pre><code>@tf.function\ndef f(x):\n    v = tf.Variable(1)\n    return tf.add(x, v)\n\nf(tf.constant(2))\n\n# =&gt; ValueError: tf.function-decorated function tried to create variables on non-first call.\n</code></pre>\n\n<p>In the following code, this is mitigated by making sure that <code>self.v</code> is only created once:</p>\n\n<pre><code>class C(object):\n    def __init__(self):\n        self.v = None\n    @tf.function\n    def f(self, x):\n        if self.v is None:\n            self.v = tf.Variable(1)\n        return tf.add(x, self.v)\n\nc = C()\nprint(c.f(tf.constant(2)))\n\n# =&gt; tf.Tensor(3, shape=(), dtype=int32)\n</code></pre>\n\n<h3>Hidden side effects not captured by AutoGraph</h3>\n\n<p>Changes such as to <code>self.a</code> in this example can\'t be hidden, which leads to an error since cross-function analysis is not done (yet) <a href=""https://www.youtube.com/watch?v=NIEgzljyDyI&amp;t=2134"" rel=""noreferrer"">(Source)</a>:</p>\n\n<pre><code>class C(object):\n    def change_state(self):\n        self.a += 1\n\n    @tf.function\n    def f(self):\n        self.a = tf.constant(0)\n        if tf.constant(True):\n            self.change_state() # Mutation of self.a is hidden\n        tf.print(self.a)\n\nx = C()\nx.f()\n\n# =&gt; InaccessibleTensorError: The tensor \'Tensor(""add:0"", shape=(), dtype=int32)\' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=cond_true_5, id=5477800528); accessed from: FuncGraph(name=f, id=5476093776).\n</code></pre>\n\n<p>Changes in plain sight are no problem:</p>\n\n<pre><code>class C(object):\n    @tf.function\n    def f(self):\n        self.a = tf.constant(0)\n        if tf.constant(True):\n            self.a += 1 # Mutation of self.a is in plain sight\n        tf.print(self.a)\n\nx = C()\nx.f()\n\n# =&gt; 1\n</code></pre>\n\n<h3>Example of limitation due to TF control flow</h3>\n\n<p>This if statement leads to an error because the value for else needs to be defined for TF control flow:</p>\n\n<pre><code>@tf.function\ndef f(a, b):\n    if tf.greater(a, b):\n        return tf.constant(1)\n\n# If a &lt;= b would return None\nx = f(tf.constant(3), tf.constant(2))   \n\n# =&gt; ValueError: A value must also be returned from the else branch. If a value is returned from one branch of a conditional a value must be returned from all branches.\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1588224608}, {'QuestionId': 59847045, 'AnswerId': 61516289, 'URL': 'https://stackoverflow.com/questions/59847045/should-i-use-tf-function-for-all-functions/61516289#61516289', 'QuestionTitle': 'Should I use @tf.function for all functions?', 'Answer': '<p>tf.function is useful in creating and using computational graphs, they should be used in training and in deployment, however it isnt needed for most of your functions. </p>\n\n<p>Lets say that we are building a special layer that will be apart of a larger model. We would not want to have the tf.function decorator above the function that constructs that layer because it is merely a definition of what the layer will look like.</p>\n\n<p>On the other hand, lets say that we are going to either make a prediction or continue our training using some function. We would want to have the decorator tf.function because we are actually using the computational graph to get some value. </p>\n\n<p>A great example would be constructing a encoder-decoder model. \nDONT put the decorator around the function the create the encoder or decoder or any layer, that is only a definition of what it will do. \nDO put the decorator around the ""train"" or ""predict"" method because those are actually going to use the computational graph for computation.</p>\n', 'IsAccepted': False, 'CreationDate': 1588223251}, {'QuestionId': 60505369, 'AnswerId': 60505624, 'URL': 'https://stackoverflow.com/questions/60505369/using-tensorflow-functions-with-tf-keras/60505624#60505624', 'QuestionTitle': 'using tensorflow functions with tf.keras', 'Answer': '<p>Just subclass <code>tf.keras.Layer</code> and you will be good to go. Great reference here: <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/custom_layers_and_models</a>. Your layer should look something like this:</p>\n\n<pre><code>class SubPixel1D(tf.keras.layers.Layer):\n  def __init__(self, r)\n      super(SubPixel1D, self).__init__()\n      self.r = r\n\n  def call(self, inputs):\n      with tf.name_scope(\'subpixel\'):\n          X = tf.transpose(inputs, [2,1,0]) # (r, w, b)\n          X = tf.batch_to_space_nd(X, [self.r], [[0,0]]) # (1, r*w, b)\n          X = tf.transpose(X, [2,1,0])\n     return X\n</code></pre>\n\n<p>and then call it when defining your model</p>\n\n<pre><code>inputdata = keras.Input(shape=(2048, 1))\n  x = layers.Conv1D(16, 3, activation=\'relu\')(inputdata)\n  x = layers.Conv1D(32, 3, activation=\'relu\')(x)\n  x = SubPixel1D(2)(x)\n  x = layers.Conv1D(64, 3, activation=\'relu\')(x)\n</code></pre>\n\n<p>I don\'t know how <code>tf.name_scope</code> will behave, but I don\'t see any immediate issues.</p>\n', 'IsAccepted': True, 'CreationDate': 1583232812}, {'QuestionId': 59359113, 'AnswerId': 59363499, 'URL': 'https://stackoverflow.com/questions/59359113/what-is-the-use-of-tf-keras-backend-nowadays-is-it-safer-more-future-proof-to-c/59363499#59363499', 'QuestionTitle': 'What is the use of tf.keras.backend nowadays, is it safer/more future proof to code w/ or w/o it?', 'Answer': '<p>It is difficult to say either is better at this point. Because keras backend offers unique feature(s) (still).</p>\n\n<p>For example, <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/backend/rnn"" rel=""nofollow noreferrer""><code>K.rnn</code></a> is a very valuable function provided by Keras backend. This can be used to iterate the temporal output of a sequential model (LSTM/GRU) on the temporal dimension. This is pretty useful when you have to do a <code>map()</code> like function on each temporal output of a sequential model (e.g. computing attention vector for each LSTM output of the encoder). This is a very convenient functions to achieve the above because, (as far as I know) doing this with <code>tf.*</code> involves <code>tf.gather</code> and can become ugly (especially in TF 1.x). I am not really sure about other functions that might offer a unique advantage over <code>tf.*</code>. But probably there are a few (e.g. <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/backend/foldl?version=stable"" rel=""nofollow noreferrer""><code>K.foldl</code></a>). </p>\n\n<p>On the other hand, <code>tf.*</code> does offer many more functions than what the Keras backend offers. </p>\n\n<p>In conclusion, I think it\'s too early to completely avoid Keras backend. But I do feel like the keras backend will get merged to <code>tf.*</code> at some point in order to offer a more consistent API.</p>\n', 'IsAccepted': False, 'CreationDate': 1576526431}, {'QuestionId': 54083955, 'AnswerId': 54117754, 'URL': 'https://stackoverflow.com/questions/54083955/should-i-use-the-standalone-keras-library-or-tf-keras/54117754#54117754', 'QuestionTitle': 'Should I use the standalone Keras library or tf.keras?', 'Answer': '<p>You are mixing things up:</p>\n\n<ul>\n<li>Keras (<a href=""https://keras.io/"" rel=""nofollow noreferrer"">https://keras.io/</a>) is a library independent from TensorFlow, which specifies a high-level API for building and training neural networks and is capable of using one of multiple backends (among which, TensorFlow) for low-level tensor computation.</li>\n<li><code>tf.keras</code> (<a href=""https://www.tensorflow.org/guide/keras"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras</a>) implements the Keras API specification within TensorFlow. In addition, the <code>tf.keras</code> API is optimized to work well with other TensorFlow modules: you can pass a <code>tf.data</code> Dataset to the <code>.fit()</code> method of a <code>tf.keras</code> model, for instance, or convert a <code>tf.keras</code> model to a TensorFlow estimator with <code>tf.keras.estimator.model_to_estimator</code>. Currently, the <code>tf.keras</code> API is the high-level API to look for when building models within TensorFlow, and the integration with other TensorFlow features will continue in the future.</li>\n</ul>\n\n<p>So to answer your question: <strong>no, you don\'t need to convert Keras code to tf.keras code</strong>. Keras code uses the Keras library, potentially even runs on top of a different backend than TensorFlow, and will continue to work just fine in the future. Even more, it\'s important to not just mix up Keras and <code>tf.keras</code> objects within the same script, since this might produce incompatabilities, as you can see for example <a href=""https://stackoverflow.com/questions/53821855/why-keras-does-not-allow-to-add-a-convolutional-layer-in-this-way/53821915#53821915"">in this question</a>.</p>\n\n<p><strong>Update</strong>: Keras will be abandoned in favor of tf.keras: <a href=""https://twitter.com/fchollet/status/1174019423541157888"" rel=""nofollow noreferrer"">https://twitter.com/fchollet/status/1174019423541157888</a></p>\n', 'IsAccepted': True, 'CreationDate': 1547065342}, {'QuestionId': 54231821, 'AnswerId': 56515486, 'URL': 'https://stackoverflow.com/questions/54231821/tensorflow-2-and-keras/56515486#56515486', 'QuestionTitle': 'TensorFlow 2 and Keras:', 'Answer': '<p>The Keras API (<a href=""https://keras.io/"" rel=""nofollow noreferrer"">https://keras.io/</a>) has multiple implementations, including the original and reference implementation (<a href=""https://github.com/keras-team/keras"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras</a>), but also various other implementations, including tf.keras, which is part of TensorFlow.</p>\n\n<p>So there are two ways you can use Keras with TensorFlow:</p>\n\n<ol>\n<li>Using the reference implementation with the TensorFlow backend. However, this implementation has not been updated to support TensorFlow 2 yet (as of June 2019).</li>\n<li>Using TensorFlow\'s implementation, tf.keras. This one works fine with TF 2.</li>\n</ol>\n\n<p>To use tf.keras, you must make sure to use the correct imports:</p>\n\n<pre class=""lang-py prettyprint-override""><code>from tensorflow import keras\n# NOT: import keras\n</code></pre>\n\n<p>Similarly, use:</p>\n\n<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.layers import Dense\n# Not from keras.layers import Dense\n</code></pre>\n\n<p>Hope this helps.</p>\n', 'IsAccepted': False, 'CreationDate': 1560090909}, {'QuestionId': 55466308, 'AnswerId': 55472462, 'URL': 'https://stackoverflow.com/questions/55466308/difference-btwn-high-and-low-level-libraries/55472462#55472462', 'QuestionTitle': 'Difference btwn high and low level libraries', 'Answer': '<p>Keras is a high level Deep learning(DL) \'API\'. Key components of the API are:</p>\n\n<ul>\n<li><p><em>Model</em> - to define the Neural network(NN).    </p></li>\n<li><p><em>Layers</em> - building blocks of the NN model (e.g. Dense, Convolution).    </p></li>\n<li><p><em>Optimizers</em> - different methods for doing gradient descent to learn weights of NN (e.g. SGD, Adam).    </p></li>\n<li><p><em>Losses</em> - objective functions that the optimizer should minimize for use cases like classification, regression (e.g. categorical_crossentropy, MSE).</p></li>\n</ul>\n\n<p>Moreover, it provides reasonable defaults for the APIs e.g. learning rates for Optimizers, which would work for the common use cases. This reduces the cognitive load on the user during the learning phase.</p>\n\n<p>The <strong>\'Guiding Principles\'</strong> section here is very informative:</p>\n\n<p><a href=""https://keras.io/"" rel=""nofollow noreferrer"">https://keras.io/</a></p>\n\n<p>The mathematical operations involved in running the Neural networks themselves like Convolutions, Matrix Multiplications etc. are delegated to the backend. One \nof the backends supported by Keras is Tensorflow.</p>\n\n<p>To highlight the differences with a code snippet:</p>\n\n<p><strong>Keras</strong></p>\n\n<pre><code># Define Neural Network\nmodel = Sequential()  \n# Add Layers to the Network\nmodel.add(Dense(512, activation=\'relu\', input_shape=(784,)))\n....\n# Define objective function and optimizer\nmodel.compile(loss=\'categorical_crossentropy\',\n          optimizer=Adam(),\n          metrics=[\'accuracy\'])\n\n# Train the model for certain number of epochs by feeding train/validation data\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\n</code></pre>\n\n<p><strong>Tensorflow</strong></p>\n\n<p>It ain\'t a code snippet anymore :) since you need to define everything starting from the Variables that would store the weights, the connections between the layers, the training loop, creating batches of data to do the training etc. </p>\n\n<p>You can refer the below links to understand the code complexity with training a MNIST(DL Hello world example) in Keras vs Tensorflow.</p>\n\n<p><a href=""https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py</a></p>\n\n<p><a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/multilayer_perceptron.py"" rel=""nofollow noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/multilayer_perceptron.py</a></p>\n\n<p>Considering the benefits that come with Keras, Tensorflow has made <strong>tf.keras the high-level API</strong> in Tensorflow 2.0.</p>\n\n<p><a href=""https://www.tensorflow.org/tutorials/"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/</a></p>\n', 'IsAccepted': True, 'CreationDate': 1554200684}, {'QuestionId': 55466308, 'AnswerId': 55466431, 'URL': 'https://stackoverflow.com/questions/55466308/difference-btwn-high-and-low-level-libraries/55466431#55466431', 'QuestionTitle': 'Difference btwn high and low level libraries', 'Answer': ""<p>Keras sits on top of Tensorflow, and thus the framework is relatively 'higher-level' than Tensorflow itself.</p>\n\n<p>A 'high' level language or framework is typically defined as one that has a greater number of dependencies or has a greater distance from core binary code, relative to a lower-level language or framework.</p>\n\n<p>E.g., jQuery would be considered higher-level than JavaScript, as it depends on Javascript. Whereas Javascript would be considered higher-level than assembly code, as it's transpiled to assembly.</p>\n"", 'IsAccepted': False, 'CreationDate': 1554174555}, {'QuestionId': 55466308, 'AnswerId': 55466365, 'URL': 'https://stackoverflow.com/questions/55466308/difference-btwn-high-and-low-level-libraries/55466365#55466365', 'QuestionTitle': 'Difference btwn high and low level libraries', 'Answer': ""<p>High level means that your interactions are closer to writing English, and the code you write is essentially more understandable to humans.</p>\n\n<p>An example of low level would be a language in which you would have to do things such as allocate memory, copy data from one memory address to another etc.</p>\n\n<p>Keras is considered high level because you can make a neural network in just a few lines of code, the library will handle all the complexity for you.</p>\n\n<p>In tensorflow (I haven't used it), you probably have to write many more lines of code to achieve the same thing, but probably have a greater degree of control. Reading tensorflow code for a NN would be less meaningful to a layman than reading keras code for a NN.</p>\n"", 'IsAccepted': False, 'CreationDate': 1554174071}, {'QuestionId': 54231821, 'AnswerId': 55346753, 'URL': 'https://stackoverflow.com/questions/54231821/tensorflow-2-and-keras/55346753#55346753', 'QuestionTitle': 'TensorFlow 2 and Keras:', 'Answer': '<p>Since TensorFlow 2 defaults to eager execution Keras will need some changes in order to be compatible with it, but until then a previous version of TensorFlow is required.</p>\n', 'IsAccepted': False, 'CreationDate': 1553549699}, {'QuestionId': 53589333, 'AnswerId': 53594060, 'URL': 'https://stackoverflow.com/questions/53589333/keras-backend-tensorflow/53594060#53594060', 'QuestionTitle': 'Keras backend Tensorflow', 'Answer': '<p>If the three nets are different:</p>\n\n<pre><code>visible = Input((64,64,3))\n\nRGB = Lambda(lambda x: tf.split(x, 3, axis=-1))(visible)\n\nnet1 = Conv2D(....)(RGB[0])\nnet1 = Activation(....)(net1)\nnet1 = Conv2D(....)(net1)\nnet1 = Activatoin(....)(net1)\n\nnet2 = Conv2D(....)(RGB[1])\n....\n\nnet3 = Conv2D(....)(RGB[2])\n.....\n\njoined = Concatenate()([net1,net2,net3])\n\nmodel = Model(visible, joined)\n</code></pre>\n\n<p>If the three nets are the same:</p>\n\n<pre><code>visible = Input((64,64,3))\n\nout = Lambda(lambda x: K.permute_dimensions(x,(0,3,1,2)))(visible)\nout = Reshape((3,64,64,1))(out)\n\nout = TimeDistributed(Conv2D(...))(out)\nout = TimeDistributed(Activation(...))(out)\nout = TimeDistributed(Conv2D(...))(out)\n....\n\nout = Reshape((3,64,64))(out)\nout = Lambda(lambda x: K.permute_dimensions(x, (0,2,3,1)))(out)\n\nmodel = Model(visible,out)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1543840815}, {'QuestionId': 53589333, 'AnswerId': 53589634, 'URL': 'https://stackoverflow.com/questions/53589333/keras-backend-tensorflow/53589634#53589634', 'QuestionTitle': 'Keras backend Tensorflow', 'Answer': '<p>When you input an RGB image to your model, you\'re actually inputting a tensor of size <code>(height ,width ,3)</code> where  <code>3</code> represents the 3 channels ( red, green, yellow)</p>\n\n<p><a href=""https://i.stack.imgur.com/mGwM9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mGwM9.png"" alt=""enter image description here""></a></p>\n\n<p>You can separate the channels by :</p>\n\n<pre><code>b, g, r    = image_array[:, :, 0], image_array[:, :, 1], image_array[:, :, 2]\n</code></pre>\n\n<p>just make sure that your channels are aligned correctly ( be careful to remove the alpha channel if it exists)</p>\n\n<p>Also you can use <a href=""https://opencv-python-tutroals.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">OpenCV</a> which will make it much easier to deal with images </p>\n\n<pre><code>import cv2\nb, g, r    = cv2.split(image_array)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1543823946}, {'QuestionId': 53589333, 'AnswerId': 53589605, 'URL': 'https://stackoverflow.com/questions/53589333/keras-backend-tensorflow/53589605#53589605', 'QuestionTitle': 'Keras backend Tensorflow', 'Answer': '<p>For multi-input, multi-output models use functional api: <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""nofollow noreferrer"">https://keras.io/getting-started/functional-api-guide/</a></p>\n\n<p>For merging results of your conv layers you can use keras\' concatenate layer. <a href=""https://keras.io/layers/merge/#concatenate"" rel=""nofollow noreferrer"">https://keras.io/layers/merge/#concatenate</a></p>\n\n<pre><code>imgs, y = read_data()\nR = imgs[:][:][:][0]\nG = imgs[:][:][:][1]\nB = imgs[:][:][:][2]\nmodel.fit([R,G,B], y, ...)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1543823837}, {'QuestionId': 45172725, 'AnswerId': 45173679, 'URL': 'https://stackoverflow.com/questions/45172725/tensorflow-why-are-there-so-many-similar-or-even-duplicate-functions-in-tf-nn/45173679#45173679', 'QuestionTitle': 'Tensorflow - Why are there so many similar or even duplicate functions in tf.nn and tf.layers / tf.losses / tf.contrib.layers etc?', 'Answer': '<p>Tensorflow proposes on the one hand a low level API (<code>tf.</code>, <code>tf.nn.</code>...), and on the other hand, a higher level API (<code>tf.layers.</code>, <code>tf.losses.</code>,...).</p>\n\n<p>The goal of the higher level API is to provide functions that greatly simplify the design of the most common neural nets. The lower level API is there for people with special needs, or who wishes to keep a finer control of what is going on.</p>\n\n<p>The situation is a bit confused though, because some functions have the same or similar names, and also, there is no clear way to distinguish at first sight which namespace correspond to which level of the API.</p>\n\n<p>Now, let\'s look at <code>conv2d</code> for example. A striking difference between <code>tf.nn.conv2d</code> and <code>tf.layers.conv2d</code> is that the later takes care of all the variables needed for  weights and biases. A single line of code, and <em>voilà</em>, you just created a convolutional layer. With <code>tf.nn.conv2d</code>, you have to take declare the weights variable yourself before passing it to the function. And as for the biases, well, they are actually not even handled: you need to add them yourself later.</p>\n\n<p>Add to that that <code>tf.layers.conv2d</code> also proposes to add regularization and activation in the same function call, you can imagine how this can reduce code size when one\'s need is covered by the higher-level API.</p>\n\n<p>The higher level also makes some decisions by default that could be considered as best practices. For example, losses in <code>tf.losses</code> are added to the <code>tf.GraphKeys.LOSSES</code> collection by default, which makes recovery and summation of the various component easy and somewhat standardized. If you use the lower level API, you would need to do all of that yourself. Obviously, you would need to be careful when you start mixing low and high level API functions there.</p>\n\n<p>The higher-level API is also an answer to a great need from people that have been otherwise used to similarly high-level function in other frameworks, Theano aside. This is rather obvious when one ponders the number of alternative higher level APIs built on top of tensorflow, such as keras 2 (now <a href=""https://blog.keras.io/introducing-keras-2.html"" rel=""noreferrer"">part of the official tensorflow API</a>), slim (in <code>tf.contrib.slim</code>), tflearn, tensorlayer, and the likes.</p>\n\n<p>Finally, if I may add an advice: if you are beginning with tensorflow and do not have a preference towards a particular API, I would personnally encourage you to stick to the <code>tf.keras.*</code> API:</p>\n\n<ul>\n<li>Its API is friendly and at least as good as the other high-level APIs built on top of the low-level tensorflow API</li>\n<li>It has a clear namespace within tensorflow (although it can -- and sometimes should -- be used with parts from other namespaces, such as <code>tf.data</code>)</li>\n<li>It is now a first-class citizen of tensorflow (it used to be in <code>tf.contrib.keras</code>), and care is taken to make new tensorflow features (such as <code>eager</code>) compatible with keras.</li>\n<li>Its generic implementation can use other toolkits such as CNTK, and so does not lock you to tensorflow.</li>\n</ul>\n', 'IsAccepted': True, 'CreationDate': 1500399698}, {'QuestionId': 47104415, 'AnswerId': 47105546, 'URL': 'https://stackoverflow.com/questions/47104415/how-do-i-use-tensorflow-backend-in-keras-without-changing-keras-json/47105546#47105546', 'QuestionTitle': 'How do I use TensorFlow backend in keras without changing keras.json?', 'Answer': ""<p>Using @GPhilo documentation reference;</p>\n\n<p>I successfully used <code>TensorFlow</code> backend by adding following lines at the starting of my code:</p>\n\n<pre><code>import os\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n# rest of the code\n</code></pre>\n\n<p>Make sure you have activated your virtual environment.</p>\n"", 'IsAccepted': True, 'CreationDate': 1509750192}, {'QuestionId': 47104415, 'AnswerId': 47104915, 'URL': 'https://stackoverflow.com/questions/47104415/how-do-i-use-tensorflow-backend-in-keras-without-changing-keras-json/47104915#47104915', 'QuestionTitle': 'How do I use TensorFlow backend in keras without changing keras.json?', 'Answer': '<p>From <a href=""https://keras.io/backend/"" rel=""nofollow noreferrer"">Keras\' documentation</a>:</p>\n\n<blockquote>\n  <p>You can also define the environment variable <code>KERAS_BACKEND</code> and this\n  will override what is defined in your config file :</p>\n  \n  <p><code>KERAS_BACKEND=tensorflow python -c ""from keras import backend""\n  Using TensorFlow backend.</code></p>\n</blockquote>\n', 'IsAccepted': False, 'CreationDate': 1509746067}, {'QuestionId': 45869776, 'AnswerId': 45870334, 'URL': 'https://stackoverflow.com/questions/45869776/tensor-math-with-tensorflow-backend/45870334#45870334', 'QuestionTitle': 'Tensor math with tensorflow backend', 'Answer': '<p>One easy way to handle this issue is to use a callback instead. Following the logic from this <a href=""https://github.com/fchollet/keras/issues/4506"" rel=""nofollow noreferrer"">issue</a>, you could specify a metrics call back that calculates any metric using sci-kit learn. For example, if you wanted to calculate f1, you could do the following:</p>\n\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Masking, Dropout\nfrom keras.optimizers import SGD, Adam, RMSprop\nimport keras.backend as K\nfrom keras.callbacks import Callback\nimport numpy as np\n\nfrom sklearn.metrics import f1_score\n\n_Xtrain = np.random.rand(1000,21,47)\n_ytrain = np.random.randint(2, size=1000)\n\n_Xtest = np.random.rand(200,21,47)\n_ytest = np.random.randint(2, size=200)\n\nclass MetricsCallback(Callback):\n    def __init__(self, train_data, validation_data):\n        super().__init__()\n        self.validation_data = validation_data\n        self.train_data = train_data\n        self.f1_scores = []\n        self.cutoff = .5\n\n    def on_epoch_end(self, epoch, logs={}):\n        X_val = self.validation_data[0]\n        y_val = self.validation_data[1]\n\n        preds = self.model.predict(X_val)\n\n        f1 = f1_score(y_val, (preds &gt; self.cutoff).astype(int))\n        self.f1_scores.append(f1)\n\n\ndef build_model():\n    model = Sequential()\n    model.add(Masking(mask_value=0, input_shape=(21, _Xtrain[0].shape[1])))\n    model.add(LSTM(32, return_sequences=True))\n    model.add(LSTM(64, return_sequences=False))\n    model.add(Dense(1, activation=\'sigmoid\'))\n    rms = RMSprop(lr=.001, decay=.001)\n    model.compile(loss=\'binary_crossentropy\', optimizer=rms, metrics=[\'acc\'])\n    return model\n\nmodel = build_model()\n\nhist = model.fit(_Xtrain, _ytrain, epochs=2, batch_size=5, validation_data=(_Xtest, _ytest), shuffle=True,\n                callbacks=[MetricsCallback((_Xtrain, _ytrain), (_Xtest, _ytest))])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1503606406}]",{59361689},"['<p>Short answer: Prefer tensorflow\'s native API such as <code>tf.math.*</code> to the<code>tf.keras.backend.*</code> API wherever possible.</p>\n\n<p>Longer answer:</p>\n\n<ul>\n<li>The <code>tf.keras.backend.*</code> API can be <em>mostly</em> viewed as a remnant of the <code>keras.backend.*</code> API. The latter is a design that serves the ""exchangeable backend"" design of the original (non-TF-specific) keras. This relates to the historical aspect of keras, which supports multiple backend libraries, among which tensorflow used to be just one of them. Back in 2015 and 2016, other backends, such as Theano and MXNet were quite popular too. But going into 2017 and 2018, tensorflow became the dominant backend of keras users. Eventually keras became a part of the tensorflow API (in 2.x and later minor versions of 1.x). In the old multi-backend world, the <code>backend.*</code> API provides a backend-independent abstraction over the myriad of supported backend. But in the tf.keras world, the value of the backend API is much more limited.</li>\n<li>The various functions in <code>tf.keras.backend.*</code> can be divided into a few categories:\n\n<ol>\n<li>Thin wrappers around the equivalent or mostly-equivalent tensorflow native API. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2363"" rel=""noreferrer"">tf.keras.backend.less</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2431"" rel=""noreferrer"">tf.keras.backend.sin</a></li>\n<li>Slightly thicker wrappers around tensorflow native APIs, with more features included. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2589"" rel=""noreferrer"">tf.keras.backend.batch_normalization</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869"" rel=""noreferrer"">tf.keras.backend.conv2d</a>(<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869</a>). They often perform proprocessing and implement other logics, which make your life easier than using native tensorflow API.</li>\n<li>Unique functions that don\'t have equivalent in the native tensorflow API. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L3809"" rel=""noreferrer"">tf.keras.backend.rnn</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L342"" rel=""noreferrer"">tf.keras.backend.set_learning_phase</a></li>\n</ol></li>\n</ul>\n\n<p>For category 1, use native tensorflow APIs. For categories 2 and 3, you may want to use the <code>tf.keras.backend.*</code> API, as long as you can find it in the documentation page: <a href=""https://www.tensorflow.org/api_docs/python/"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/</a>, because the documented ones have backward compatibility guarantees, so that you don\'t need to worry about a future version of tensorflow removing it or changing its behavior.</p>\n', 'This relates to the historical aspect of keras, which supports multiple backend libraries, among which tensorflow used to be just one of them. Back in 2015 and 2016, other backends, such as Theano and MXNet were quite popular too. But going into 2017 and 2018, tensorflow became the dominant backend of keras users. Eventually keras became a part of the tensorflow API (in 2.x and later minor versions of 1.x). In the old multi-backend world, the <code>backend.*</code> API provides a backend-independent abstraction over the myriad of supported backend. But in the tf.keras world, the value of the backend API is much more limited.</li>\n<li>The various functions in <code>tf.keras.backend.*</code> can be divided into a few categories:\n\n<ol>\n<li>Thin wrappers around the equivalent or mostly-equivalent tensorflow native API. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2363"" rel=""noreferrer"">tf.keras.backend.less</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2431"" rel=""noreferrer"">tf.keras.backend.sin</a></li>\n<li>Slightly thicker wrappers around tensorflow native APIs, with more features included. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L2589"" rel=""noreferrer"">tf.keras.backend.batch_normalization</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869"" rel=""noreferrer"">tf.keras.backend.conv2d</a>(<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4869</a>). They often perform proprocessing and implement other logics, which make your life easier than using native tensorflow API.</li>\n<li>Unique functions that don\'t have equivalent in the native tensorflow API. Examples: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L3809"" rel=""noreferrer"">tf.keras.backend.rnn</a>, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L342"" rel=""noreferrer"">tf.keras.backend.set_learning_phase</a></li>\n</ol></li>\n</ul>\n\n<p>For category 1, use native tensorflow APIs. For categories 2 and 3, you may want to use the <code>tf.keras.backend.*</code> API, as long as you can find it in the documentation page: <a href=""https://www.tensorflow.org/api_docs/python/"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/</a>, because the documented ones have backward compatibility guarantees, so that you don\'t need to worry about a future version of tensorflow removing it or changing its behavior.</p>\n', '<p>Short answer: Prefer tensorflow\'s native API such as <code>tf.math.*</code> to the<code>tf.keras.backend.*</code> API wherever possible.</p>\n\n<p>Longer answer:</p>\n\n<ul>\n<li>The <code>tf.keras.backend.*</code> API can be <em>mostly</em> viewed as a remnant of the <code>keras.backend.*</code> API. The latter is a design that serves the ""exchangeable backend"" design of the original (non-TF-specific) keras.']",{'https://stackoverflow.com/questions/59361689/redundancies-in-tf-keras-backend-and-tensorflow-libraries/59364358#59364358'},,0.2010510244723116,0.026160834042768144
9,56047272,tf.constant,Documentation Replication on Other Examples,Explicit vs implicit type definition in TensorFlow,"<p>I'm just beginning to learn TensorFlow. Quoting from the <a href=""https://www.tensorflow.org/guide/low_level_intro#graph"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Let's build a simple computational graph. The most basic operation is a constant. The Python function that builds the operation takes a tensor value as input. The resulting operation takes no inputs. When run, it outputs the value that was passed to the constructor. We can create two floating point constants a and b as follows:</p>
</blockquote>

<pre><code>a = tf.constant(3.0, dtype=tf.float32)
b = tf.constant(4.0) # also tf.float32 implicitly
total = a + b
print(a)
print(b)
print(total)
</code></pre>

<p>The second constant is implicitly typed as a float32. Is that based on the explicit typing of the first constant? And does that imply that the first <code>dtype</code> is required? <a href=""https://www.tensorflow.org/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">tf.constant documentation</a> would imply that it does not:</p>

<blockquote>
  <p>If the argument dtype is not specified, then the type is inferred from the type of <code>value</code>.</p>
</blockquote>

<p>But then it would be unnecessary to explicitly type the 3.0 constant above.</p>

<p>I'm just looking for some clarification on this, since, like I said, I'm just starting out.</p>
","<blockquote>
  <p>But then it would be unnecessary to explicitly type the 3.0 constant
  above.</p>
</blockquote>

<p>Absolutely correct. </p>

<pre><code>a = tf.constant(3.0, dtype=tf.float32)
</code></pre>

<p>is equivalent to:</p>

<pre><code>a = tf.constant(3.0)
</code></pre>

<p>The documentation is just demonstrating the different overloads. We might choose to explicitly provide the type if we want a different numerical precision (or even just to aid human readability) but if you want the default data type TF infers, then it's entirely unnecessary.</p>
","{66968102, 51237606, 63194184, 44376936, 42758315, 31966126, 37958706, 64056979, 63508052, 44167676, 66977630}","[{'QuestionId': 63508052, 'AnswerId': 78335145, 'URL': 'https://stackoverflow.com/questions/63508052/difference-between-typevart-and-any/78335145#78335145', 'QuestionTitle': 'Difference between TypeVar(&#39;T&#39;) and Any', 'Answer': '<p>In your case, there is no practical difference.</p>\n<p>However there are cases where using <code>TypeVar</code> differs from using <code>Any</code>:</p>\n<pre class=""lang-py prettyprint-override""><code>from typing import Any, TypeVar\n\nT = TypeVar(&quot;T&quot;)\n\ndef function1(input: Any) -&gt; Any:\n    ...\n\ndef function2(input: T) -&gt; T:\n    ...\n</code></pre>\n<p>For <code>function1</code> the input can be any type and the output can be any type, independent of each other.\nHowever, for <code>function2</code> the input and output can be any type, but they have to be equal to each other.</p>\n', 'IsAccepted': False, 'CreationDate': 1713276261}, {'QuestionId': 63508052, 'AnswerId': 63508090, 'URL': 'https://stackoverflow.com/questions/63508052/difference-between-typevart-and-any/63508090#63508090', 'QuestionTitle': 'Difference between TypeVar(&#39;T&#39;) and Any', 'Answer': '<p>One could think that the difference is:</p>\n<p><code>foo(*args: T)</code> means that the function can take any amount of arguments and the arguments can be of any type but must all be of the same type.</p>\n<p><code>foo(*args: Any)</code> means that the function can take any amount of any type arguments.</p>\n<p>But this is actually not the case: <a href=""https://github.com/python/mypy/issues/6559#issuecomment-473864640"" rel=""nofollow noreferrer"">https://github.com/python/mypy/issues/6559#issuecomment-473864640</a></p>\n<p>In reality <strong>there is no difference between the two definitions</strong>.</p>\n', 'IsAccepted': False, 'CreationDate': 1597936338}, {'QuestionId': 66977630, 'AnswerId': 66977865, 'URL': 'https://stackoverflow.com/questions/66977630/why-python-int-cannot-be-converted-into-tensorflow-dtype-whereas-float-can/66977865#66977865', 'QuestionTitle': 'Why Python &quot;int&quot; cannot be converted into Tensorflow dtype whereas &quot;float&quot; can?', 'Answer': '<p>Function states that:</p>\n<blockquote>\n<p>A value that can be converted to a tf.DType object. This may currently\nbe a tf.DType object, a DataType enum, a string type name, or a\nnumpy.dtype.</p>\n</blockquote>\n<p>You can check the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/types.proto"" rel=""nofollow noreferrer"">source code</a>:</p>\n<pre><code>  ...\n  // Data types that all computation devices are expected to be\n  // capable to support.\n  DT_FLOAT = 1;\n  DT_DOUBLE = 2;\n  DT_INT32 = 3;\n  DT_UINT8 = 4;\n  DT_INT16 = 5;\n  ...\n</code></pre>\n<p>It casts to <code>float32</code> when you pass <code>float</code> but there is no infer for <code>int</code>, directly. You need to indicate that you are passing <code>int32</code> or whatever.</p>\n<pre><code>print(&quot;Python int equivalent in TF is %s&quot; % tf.dtypes.as_dtype(\'int32\'))\n--&gt; Python int equivalent in TF is &lt;dtype: \'int32\'&gt;\n</code></pre>\n<p>Also:</p>\n<pre><code>print(&quot;Python int equivalent in TF is %s&quot; % tf.dtypes.as_dtype(3)) # order\nPython int equivalent in TF is &lt;dtype: \'int32\'&gt;\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1617752713}, {'QuestionId': 66968102, 'AnswerId': 66971071, 'URL': 'https://stackoverflow.com/questions/66968102/python-type-hint-can-tensorflow-data-type-be-used/66971071#66971071', 'QuestionTitle': 'python type hint - can tensorflow data type be used?', 'Answer': '<p>I assume you would like your function to accept:</p>\n<ul>\n<li><code>tf.float32</code></li>\n<li><code>np.float32</code></li>\n<li><code>float</code></li>\n<li><code>tf.int32</code></li>\n<li><code>np.int32</code></li>\n<li><code>int</code></li>\n</ul>\n<p>and always return, say, <code>tf.float32</code>.  Not completely sure if this covers your use case, but I would put a broad type for your input argument and cast to the desired type in your function.</p>\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""nofollow noreferrer""><code>experimental_follow_type_hints</code></a> can be used along with type annotations to improve performance by reducing the number of expensive graph retracings. For example, an argument annotated with tf.Tensor is converted to Tensor even when the input is a non-Tensor value.</p>\n<pre><code>from typing import TYPE_CHECKING\nimport tensorflow as tf\nimport numpy as np\n\n\n@tf.function(experimental_follow_type_hints=True)\ndef foo(x: tf.Tensor) -&gt; tf.float32:\n    if x.dtype == tf.int32:\n        x = tf.dtypes.cast(x, tf.float32)\n    return x * 2\n\na = tf.cast(1.0, dtype=tf.float32)\nb = tf.cast(1.0, dtype=tf.int32)\n\nc = np.float32(1.0)\nd = np.int32(1.0)\n\ne = 1.0\nf = 1\n\nfor var in [a, b, c, d, e, f]:\n    print(f&quot;input: {var},\\tinput type: {type(var)},\\toutput: {foo(var)}\\toutput type: {type(foo(var))}&quot;)\n\nif TYPE_CHECKING:\n    reveal_locals()\n</code></pre>\n<p>Output of <code>python3 stack66968102.py</code>:</p>\n<pre><code>input: 1.0,     input type: &lt;class \'tensorflow.python.framework.ops.EagerTensor\'&gt;,      output: 2.0     output dtype: &lt;dtype: \'float32\'&gt;\ninput: 1,       input type: &lt;class \'tensorflow.python.framework.ops.EagerTensor\'&gt;,      output: 2.0     output dtype: &lt;dtype: \'float32\'&gt;\ninput: 1.0,     input type: &lt;class \'numpy.float32\'&gt;,    output: 2.0     output dtype: &lt;dtype: \'float32\'&gt;\ninput: 1,       input type: &lt;class \'numpy.int32\'&gt;,      output: 2.0     output dtype: &lt;dtype: \'float32\'&gt;\ninput: 1.0,     input type: &lt;class \'float\'&gt;,    output: 2.0     output dtype: &lt;dtype: \'float32\'&gt;\ninput: 1,       input type: &lt;class \'int\'&gt;,      output: 2.0     output dtype: &lt;dtype: \'float32\'&gt;\n</code></pre>\n<p>Output of <code>mypy stack66968102.py  --ignore-missing-imports</code>:</p>\n<pre><code>stack66968102.py:27: note: Revealed local types are:\nstack66968102.py:27: note:     a: Any\nstack66968102.py:27: note:     b: Any\nstack66968102.py:27: note:     c: numpy.floating[numpy.typing._32Bit*]\nstack66968102.py:27: note:     d: numpy.signedinteger[numpy.typing._32Bit*]\nstack66968102.py:27: note:     e: builtins.float\nstack66968102.py:27: note:     f: builtins.int\nstack66968102.py:27: note:     tf: Any\nstack66968102.py:27: note:     var: Any\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1617720867}, {'QuestionId': 64056979, 'AnswerId': 64057847, 'URL': 'https://stackoverflow.com/questions/64056979/pytorch-difference-between-typea-a-type-a-type/64057847#64057847', 'QuestionTitle': 'PyTorch: difference between type(a), a.type, a.type()', 'Answer': '<p><a href=""https://docs.python.org/3/library/functions.html#type"" rel=""nofollow noreferrer""><code>type</code></a> is the python in-built method.</p>\n<ul>\n<li>It will return type of object. like <code>&lt;class \'torch.Tensor\'&gt;</code></li>\n</ul>\n<p><a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor.type"" rel=""nofollow noreferrer""><code>torch.Tensor.type</code></a> (<code>x.type()</code>) is pytorch in-built method.</p>\n<ul>\n<li>It will return type of data stored inside tensor. like <code>torch.DoubleTensor</code>, <a href=""https://pytorch.org/docs/stable/tensors.html#torch-tensor"" rel=""nofollow noreferrer"">etc.</a></li>\n</ul>\n<hr />\n<p>Edit:</p>\n<p>And about <code>x.type()</code> vs <code>x.type</code> -\nWhen you write a function name with parentheses <code>x.type ()</code> it will actually execute the function and return its value. Whereas without parentheses <code>x.type</code> it is simply a reference to function.</p>\n', 'IsAccepted': False, 'CreationDate': 1601008380}, {'QuestionId': 64056979, 'AnswerId': 64057839, 'URL': 'https://stackoverflow.com/questions/64056979/pytorch-difference-between-typea-a-type-a-type/64057839#64057839', 'QuestionTitle': 'PyTorch: difference between type(a), a.type, a.type()', 'Answer': ""<pre><code>type(a) - returns the class type\na.type - returns built method type\na.type() - return the data type\n\n&gt;&gt;&gt; a = torch.Tensor()\n&gt;&gt;&gt; type(a)\n&lt;class 'torch.Tensor'&gt;\n&gt;&gt;&gt; a.type\n&lt;built-in method type of Tensor object at 0x7f6a0acfc140&gt;\n&gt;&gt;&gt; a.type()    \n'torch.FloatTensor'                                                     \n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1601008247}, {'QuestionId': 63194184, 'AnswerId': 63195599, 'URL': 'https://stackoverflow.com/questions/63194184/difference-between-2-and-2-in-tf-constant-dynamic-assignment-of-datatypes-i/63195599#63195599', 'QuestionTitle': 'Difference between 2 and 2. in tf.constant() - Dynamic Assignment of Datatypes in Tensorflow', 'Answer': ""<p>In such cases, it is always suggested that you check the data types explicitly using, say, the following example:</p>\n<p>g = tf.constant([[1, 2.0]])</p>\n<p>print(g.dtype) # output is &lt;dtype: 'float32'&gt;</p>\n"", 'IsAccepted': True, 'CreationDate': 1596212570}, {'QuestionId': 42758315, 'AnswerId': 42758506, 'URL': 'https://stackoverflow.com/questions/42758315/how-to-get-the-type-of-a-tensor/42758506#42758506', 'QuestionTitle': 'How to get the type of a Tensor?', 'Answer': '<p>You can use <a href=""https://www.tensorflow.org/versions/r0.11/resources/faq#tensor_shapes"" rel=""noreferrer"">get_shape()</a> to get the shape of a tensorflow variable.</p>\n\n<pre><code>&gt;&gt;&gt; x = tf.Variable(tf.random_normal([256, 100]))\n&gt;&gt;&gt; x.get_shape()\n(256, 100)\n</code></pre>\n\n<p>You can use <a href=""https://www.tensorflow.org/api_docs/python/tf/dtypes/DType"" rel=""noreferrer"">dtype</a> property to get the type of a tensorflow variable.</p>\n\n<pre><code>&gt;&gt;&gt; x = tf.Variable(tf.random_normal([256, 100]))\n&gt;&gt;&gt; x.dtype\n&lt;dtype: \'float32_ref\'&gt;\n</code></pre>\n\n<p>You can use <a href=""https://www.tensorflow.org/api_docs/python/tf/dtypes/DType"" rel=""noreferrer"">as_numpy_dtype</a> property of dtype to convert from <a href=""https://www.tensorflow.org/api_docs/python/tf/dtypes/DType"" rel=""noreferrer"">tf.dtype</a> to <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html"" rel=""noreferrer"">numpy dtype</a>.</p>\n\n<pre><code>&gt;&gt;&gt; x = tf.Variable(tf.random_normal([256, 100]))\n&gt;&gt;&gt; x.dtype.as_numpy_dtype\n&lt;class \'numpy.float32\'&gt;\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1489390656}, {'QuestionId': 51237606, 'AnswerId': 51238155, 'URL': 'https://stackoverflow.com/questions/51237606/typecasting-error-in-tensorflow/51238155#51238155', 'QuestionTitle': 'Typecasting error in TensorFlow', 'Answer': '<ol>\n<li><p><em>Oh, you should never</em> use <code>tf.Variable</code> unless you have a very good reason. You should use <code>tf.get_variable</code> instead to avoid <a href=""https://stackoverflow.com/q/51167542/7443104"">issues</a>.</p></li>\n<li><p><em>Oh, you should never</em> use float64 as the data type, unless you do have a good reason. NumPy uses float64 as a default, so you should write something like</p>\n\n<pre><code>W = tf.get_variable(""w"", initializer=np.random.randn().astype(np.float32))\n</code></pre></li>\n</ol>\n', 'IsAccepted': True, 'CreationDate': 1531109823}, {'QuestionId': 44376936, 'AnswerId': 44377194, 'URL': 'https://stackoverflow.com/questions/44376936/why-is-the-type-of-tf-variable8-0-tf-float64-float-32-rather-than-float-64/44377194#44377194', 'QuestionTitle': 'Why is the type of tf.Variable([8.0], tf.float64) float 32 rather than float 64 in TensorFlow?', 'Answer': '<p><em>Update:</em></p>\n\n<p>Use dtype when using tf.Variable()</p>\n\n<p><em>Original Question:</em></p>\n\n<p>making <em>ref</em> type to float32 is working for me.</p>\n\n<p>Do </p>\n\n<pre><code>ref = tf.Variable(np.arange(0, 12).reshape((4, 3)).astype(np.float32))\n</code></pre>\n\n<p>or</p>\n\n<pre><code>ref = tf.Variable(np.arange(0, 12).reshape((4, 3)),dtype=tf.float32)\n</code></pre>\n\n<p>I guess, ScatterNdUpdate operation works only for float32 not float64.</p>\n', 'IsAccepted': False, 'CreationDate': 1496694603}, {'QuestionId': 44376936, 'AnswerId': 44377551, 'URL': 'https://stackoverflow.com/questions/44376936/why-is-the-type-of-tf-variable8-0-tf-float64-float-32-rather-than-float-64/44377551#44377551', 'QuestionTitle': 'Why is the type of tf.Variable([8.0], tf.float64) float 32 rather than float 64 in TensorFlow?', 'Answer': '<p>The reason is really simple: your code create a <code>tf.Variable</code> which is trainable (your <code>tf.float64</code> is interpreted as a <code>True</code> for the <code>trainable</code> argument. If you just add <code>dtype</code>, it will work:</p>\n\n<pre><code>    updates = tf.Variable([8.0], dtype=tf.float64)\n</code></pre>\n\n<p>Actually, there was a similar <a href=""https://stackoverflow.com/a/42899188/4282745"">Q&amp;A</a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1496696147}, {'QuestionId': 44167676, 'AnswerId': 44168826, 'URL': 'https://stackoverflow.com/questions/44167676/what-python-types-does-tensorflow-accept-for-attrs-of-type-tensor/44168826#44168826', 'QuestionTitle': 'What Python types does TensorFlow accept for Attr&#39;s of type &quot;tensor&quot;?', 'Answer': '<p>After much more hunting, I found <code>tf.contrib.util.make_tensor_proto</code>, a function that converts a python scalar, python list, numpy ndarray, or numpy scalar into a <code>tf.TensorProto</code> object. The following works:</p>\n\n<pre><code>A = tf.contrib.util.make_tensor_proto([[1.0, 2.0, 3.0],[4.0, 5.0, 6.0]])\nX = tf.Variable(tf.constant(1.0))\n\nY = dostufflib.do_stuff(X, A)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1495661603}, {'QuestionId': 42758315, 'AnswerId': 42758444, 'URL': 'https://stackoverflow.com/questions/42758315/how-to-get-the-type-of-a-tensor/42758444#42758444', 'QuestionTitle': 'How to get the type of a Tensor?', 'Answer': '<p>To get the type you can do</p>\n\n<pre><code>x.dtype\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1489390416}, {'QuestionId': 37958706, 'AnswerId': 37959933, 'URL': 'https://stackoverflow.com/questions/37958706/in-tensorflow-what-is-the-difference-between-a-tensor-that-has-a-type-ending-in/37959933#37959933', 'QuestionTitle': 'In Tensorflow, what is the difference between a tensor that has a type ending in _ref and a tensor that does not?', 'Answer': '<p>A reference-typed tensor is <strong>mutable</strong>. The most common way to create a reference-typed tensor is to define a <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#Variable"" rel=""noreferrer""><code>tf.Variable</code></a>: defining a <code>tf.Variable</code> whose initial value has dtype <code>tf.float32</code> will create a reference-typed tensor with dtype <code>tf.float32_ref</code>. You can mutate a reference-typed tensor by passing it as the first argument to <code>tf.assign()</code>.</p>\n\n<p>(Note that reference-typed tensors are something of an implementation detail in the present version of TensorFlow. We\'d encourage you to use higher-level wrappers like <code>tf.Variable</code>, which may migrate to alternative representations for mutable state in the future.)</p>\n', 'IsAccepted': True, 'CreationDate': 1466576070}, {'QuestionId': 31966126, 'AnswerId': 32140234, 'URL': 'https://stackoverflow.com/questions/31966126/what-is-the-difference-between-x-type-and-typex-in-python/32140234#32140234', 'QuestionTitle': 'what is the difference between x.type and type(x) in Python?', 'Answer': '<p>As others have mentioned, <code>type(x)</code> is Python\'s <a href=""https://docs.python.org/2/library/functions.html#type"" rel=""nofollow"">builtin function</a> that returns the type of the object.  It has nothing to do with Theano per se.  This builtin function can be applied to any Python object (and everything in Python is an object).  For example,</p>\n\n<ul>\n<li><code>type(1)</code> is <code>int</code>,</li>\n<li><code>type(True)</code> is <code>bool</code>,</li>\n<li><code>type(lambda x: x * x)</code> is <code>function</code>, etc.</li>\n</ul>\n\n<p>Interestingly, you can call <code>type</code> on <code>type</code> itself (everything, including <code>type</code>, is an object) - <code>type(type)</code> is <code>type</code>.</p>\n\n<p>Incidentally, <code>type(T.dscalar)</code> is <code>TensorType</code> (<code>theano.tensor.type.TensorType</code> to be precise).</p>\n\n<p><code>x.type</code>, as others have mentioned, is an attribute of the object <code>x</code>.  It points back to <code>type(T.dscalar)</code>.  <code>x.type</code> returns <code>TensorType(float64, scalar)</code> - this not only shows you the type of <code>T.dscalar</code>, it also tells you that <code>x</code> is scalar and it is 64-bit float.</p>\n\n<p>Other examples of the type attribute:</p>\n\n<pre><code>&gt;&gt;&gt; iv = T.ivector()\n&gt;&gt;&gt; iv.type\nTensorType(int32, vector)    # iv is a vector of 32-bit ints\n&gt;&gt;&gt; fm = T.fmatrix()\n&gt;&gt;&gt; fm.type\nTensorType(float32, matrix)   # fm is a matrix of 32-bit floats\n&gt;&gt;&gt; lt3 = T.ltensor3()\n&gt;&gt;&gt; lt3.type\nTensorType(int64, 3D)    # lt3 is a 3D array of 64-bit ints\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1440159717}, {'QuestionId': 31966126, 'AnswerId': 31966191, 'URL': 'https://stackoverflow.com/questions/31966126/what-is-the-difference-between-x-type-and-typex-in-python/31966191#31966191', 'QuestionTitle': 'what is the difference between x.type and type(x) in Python?', 'Answer': '<p><code>theano.tensor</code> has an attribute <a href=""http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor._tensor_py_operators.type"" rel=""nofollow""><code>type</code></a> which you are looking at when you say</p>\n\n<pre><code>x.type\n</code></pre>\n\n<p>This is analagous to numpy objects <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html"" rel=""nofollow""><code>dtype</code></a> attribute that many of their objects carry (if you are familiar with that library).</p>\n\n<p>On the other hand <a href=""https://docs.python.org/3/library/functions.html#type"" rel=""nofollow""><code>type</code></a> is a Python function that looks at the actual type of the object you pass in, which for <code>type(x)</code> is indeed a </p>\n\n<pre><code>theano.tensor.var.TensorVariable\n</code></pre>\n\n<p>So long story short, you are comparing an attribute to the actual object type.</p>\n', 'IsAccepted': True, 'CreationDate': 1439384852}, {'QuestionId': 31966126, 'AnswerId': 31966200, 'URL': 'https://stackoverflow.com/questions/31966126/what-is-the-difference-between-x-type-and-typex-in-python/31966200#31966200', 'QuestionTitle': 'what is the difference between x.type and type(x) in Python?', 'Answer': ""<p><code>type(x)</code> is a builtin.</p>\n\n<p><code>x.type</code> is an attribute that's defined in your object.</p>\n\n<p>They are completely seperate, <code>type(x)</code> returns what type of object <code>x</code> is and <code>x.type</code> does whatever the object wants it to. In this case, it returns some information on the type of object it is</p>\n"", 'IsAccepted': False, 'CreationDate': 1439384871}]",{56047272},"[""We might choose to explicitly provide the type if we want a different numerical precision (or even just to aid human readability) but if you want the default data type TF infers, then it's entirely unnecessary.</p>\n"", ""<blockquote>\n  <p>But then it would be unnecessary to explicitly type the 3.0 constant\n  above.</p>\n</blockquote>\n\n<p>Absolutely correct. </p>\n\n<pre><code>a = tf.constant(3.0, dtype=tf.float32)\n</code></pre>\n\n<p>is equivalent to:</p>\n\n<pre><code>a = tf.constant(3.0)\n</code></pre>\n\n<p>The documentation is just demonstrating the different overloads. We might choose to explicitly provide the type if we want a different numerical precision (or even just to aid human readability) but if you want the default data type TF infers, then it's entirely unnecessary.</p>\n"", '<blockquote>\n  <p>But then it would be unnecessary to explicitly type the 3.0 constant\n  above.</p>\n</blockquote>\n\n<p>Absolutely correct. </p>\n\n<pre><code>a = tf.constant(3.0, dtype=tf.float32)\n</code></pre>\n\n<p>is equivalent to:</p>\n\n<pre><code>a = tf.constant(3.0)\n</code></pre>\n\n<p>The documentation is just demonstrating the different overloads.']",{'https://stackoverflow.com/questions/56047272/explicit-vs-implicit-type-definition-in-tensorflow/56058558#56058558'},,0.22558537805977555,0.056523523278489196
9,56553579,tf.estimator.BestExporter,Documentation Ambiguity,How to export Estimator's best model?,"<p>I am training a simple CNN based on a Custom Estimator with TF Records.
I am trying to export the best model in terms of validation loss during the <code>train_and_evaluate</code> phase. </p>

<p>According to the documentation of the <code>tf.estimator.BestExporter</code>, I should feed a function that returns a <code>ServingInputReceiver</code> but after doing so, the <code>train_and_evaluate</code> phase crashes with a <code>NotFoundError: model/m01/eval; No such file or directory</code>.</p>

<p>Seems like if the BestExporter does not permit saving the evaluation results as it would do without the exporter. I tried with different <code>ServingInputReceiver</code> but I keep getting the same error.</p>

<p>As defined <a href=""https://www.tensorflow.org/guide/saved_model#using_savedmodel_with_estimators"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>feature_spec = {
        'shape': tf.VarLenFeature(tf.int64),
        'image_raw': tf.FixedLenFeature((), tf.string),
        'label_raw': tf.FixedLenFeature((43), tf.int64)
    }

def serving_input_receiver_fn():
  serialized_tf_example = tf.placeholder(dtype=tf.string,
                                         shape=[120, 120, 3],
                                         name='input_example_tensor')
  receiver_tensors = {'image': serialized_tf_example}
  features = tf.parse_example(serialized_tf_example, feature_spec)
  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)
</code></pre>

<p>and <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/BestExporter#__init__"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>def serving_input_receiver_fn():
    feature_spec = {
            'image': tf.FixedLenFeature((), tf.string)
        }
    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)
</code></pre>

<p>Here are my exporter and training procedure:</p>

<pre><code>exporter = tf.estimator.BestExporter(
    name=""best_exporter"",
    serving_input_receiver_fn=serving_input_receiver_fn,
    exports_to_keep=5)

train_spec = tf.estimator.TrainSpec(
    input_fn=lambda: imgs_input_fn(train_path, True, epochs, batch_size))

eval_spec = tf.estimator.EvalSpec(
    input_fn=lambda: imgs_input_fn(eval_path, perform_shuffle=False, batch_size=1),
    exporters=exporter)

tf.estimator.train_and_evaluate(ben_classifier, train_spec, eval_spec)
</code></pre>

<p><a href=""https://gist.github.com/hichameyessou/f2710391066f6ed5786693892ac93dbe"" rel=""nofollow noreferrer"">This is a gist</a> with the output.
What's the correct way to define a <code>ServingInputReceiver</code> for the <code>BestExporter</code>?</p>
","<p>Can you try the code shown below:</p>

<pre><code>def serving_input_receiver_fn():
    """"""
    This is used to define inputs to serve the model.
    :return: ServingInputReciever
    """"""
    reciever_tensors = {
        # The size of input image is flexible.
        'image': tf.placeholder(tf.float32, [None, None, None, 1]),
    }

    # Convert give inputs to adjust to the model.
    features = {
        # Resize given images.
        'image': tf.reshape(reciever_tensors[INPUT_FEATURE], [-1, INPUT_SHAPE])
    }
    return tf.estimator.export.ServingInputReceiver(receiver_tensors=reciever_tensors,
                                                    features=features)
</code></pre>

<p>Then use <code>tf.estimator.BestExporter</code> as shown below:</p>

<pre><code>best_exporter = tf.estimator.BestExporter(
        serving_input_receiver_fn=serving_input_receiver_fn,
        exports_to_keep=1)
    exporters = [best_exporter]
    eval_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={input_name: eval_data},
        y=eval_labels,
        num_epochs=1,
        shuffle=False)
    eval_spec = tf.estimator.EvalSpec(
        input_fn=eval_input_fn,
        throttle_secs=10,
        start_delay_secs=10,
        steps=None,
        exporters=exporters)

    # Train and evaluate the model.
    tf.estimator.train_and_evaluate(classifier, train_spec=train_spec, eval_spec=eval_spec)
</code></pre>

<p>For more info, refer the link:
<a href=""https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_keras_estimator.py"" rel=""nofollow noreferrer"">https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_keras_estimator.py</a></p>
","{42835809, 55095907, 46239139, 46539142, 44460362, 51482730, 51537682, 47440114, 43966073, 50612923}","[{'QuestionId': 44460362, 'AnswerId': 59432802, 'URL': 'https://stackoverflow.com/questions/44460362/how-to-save-estimator-in-tensorflow-for-later-use/59432802#59432802', 'QuestionTitle': 'How to save estimator in Tensorflow for later use?', 'Answer': '<p><strong>Update to <a href=""https://stackoverflow.com/users/4268517/david-valenzuela-urrutia"">David Valenzuela Urrutia\'s</a> answer(codes)</strong></p>\n\n<p>David Valenzuela Urrutia\'s answer was for  Python 3.6.3, Tensorflow 1.4.0 so i thought of updating the answer(code samples) to Tensorflow 2.x because some funtionalities like <strong>tf.Session</strong> is not supported in Tensorflow version 2 so you need to replace it with <strong>tf.compat.v1.Session</strong> for it to work. Visit this <a href=""https://www.tensorflow.org/guide/effective_tf2"" rel=""nofollow noreferrer"">link</a> to know more about the  changes added to tensorflow version 2</p>\n\n<p><strong>Training script updated code</strong></p>\n\n<pre><code>def serving_input_receiver_fn():\n   serialized_tf_example = tf.compat.v1.placeholder(dtype=tf.string, shape=[None], \n       name=\'input_tensors\')\n   receiver_tensors      = {""predictor_inputs"": serialized_tf_example}\n   feature_spec          = {""words"": tf.io.FixedLenFeature([25],tf.int64)}\n   features              = tf.io.parse_example(serialized=serialized_tf_example, \n       features=feature_spec)\n   return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\ndef estimator_spec_for_softmax_classification(logits, labels, mode):\n   predicted_classes = tf.argmax(input=logits, axis=1)\n   if (mode == tf.estimator.ModeKeys.PREDICT):\n      export_outputs = {\'predict_output\': \n   tf.estimator.export.PredictOutput({""pred_output_classes"": predicted_classes, \'probabilities\': tf.nn.softmax(logits)})}\n   return tf.estimator.EstimatorSpec(mode=mode, predictions={\'class\': predicted_classes, \'prob\': tf.nn.softmax(logits)}, export_outputs=export_outputs) # IMPORTANT!!!\n   onehot_labels = tf.one_hot(labels, 31, 1, 0)\n   loss        =tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n   if (mode == tf.estimator.ModeKeys.TRAIN):\n       optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\n       train_op  = optimizer.minimize(loss, global_step=tf.compat.v1.train.get_global_step())\n       return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n   eval_metric_ops = {\'accuracy\': tf.compat.v1.metrics.accuracy(labels=labels, predictions=predicted_classes)}\n   return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n\ndef model_custom(features, labels, mode):\n   bow_column           = tf.feature_column.categorical_column_with_identity(""words"", num_buckets=1000)\n   bow_embedding_column = tf.feature_column.embedding_column(bow_column, dimension=50)   \n   bow                  = tf.compat.v1.feature_column.input_layer(features, feature_columns=[bow_embedding_column])\n   logits               = tf.compat.v1.layers.dense(bow, 31, activation=None)\n   return estimator_spec_for_softmax_classification(logits=logits, labels=labels, mode=mode)\n\ndef main():\n   # ...\n   # preprocess-&gt; features_train_set and labels_train_set\n   # ...\n   classifier     = tf.estimator.Estimator(model_fn = model_custom)\n   train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(x={""words"": features_train_set}, y=labels_train_set, batch_size=batch_size_param, num_epochs=None, shuffle=True)\n   classifier.train(input_fn=train_input_fn, steps=100)\n   full_model_dir = classifier.export_savedmodel(export_dir_base=""C:/models/directory_base"", serving_input_receiver_fn=serving_input_receiver_fn)\n</code></pre>\n\n<p><strong>Testing script updated code</strong></p>\n\n<pre><code>def main():\n   # ...\n   # preprocess-&gt; features_test_set\n   # ...\n   with tf.compat.v1.Session() as sess:\n       tf.compat.v1.saved_model.loader.load(sess, [tf.saved_model.SERVING], full_model_dir)\n       predictor   = tf.contrib.predictor.from_saved_model(full_model_dir)\n       model_input = tf.train.Example(features=tf.train.Features( feature={""words"": tf.train.Feature(int64_list=tf.train.Int64List(value=features_test_set)) })) \n       model_input = model_input.SerializeToString()\n       output_dict = predictor({""predictor_inputs"":[model_input]})\n       y_predicted = output_dict[""pred_output_classes""][0]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1576890358}, {'QuestionId': 55095907, 'AnswerId': 55104789, 'URL': 'https://stackoverflow.com/questions/55095907/how-to-save-tensorflow-model-with-tf-estimator/55104789#55104789', 'QuestionTitle': 'how to save tensorflow model with tf.estimator', 'Answer': ""<p>You create a function with a dictionary of input features. Placeholder should match the shape of your image, with first dimension for batch_size.</p>\n\n<pre><code>def serving_input_receiver_fn():\n  x = tf.placeholder(tf.float32, [None, Shape])\n  inputs = {'x': x}\n  return tf.estimator.export.ServingInputReceiver(features=inputs, receiver_tensors=inputs)\n</code></pre>\n\n<p>Or you can use <code>TensorServingInputReceiver</code> which doesn't required dict mapping</p>\n\n<pre><code>inputs = tf.placeholder(tf.float32, [None, 32*32*3])\ntf.estimator.export.TensorServingInputReceiver(inputs, inputs)\n</code></pre>\n\n<p>This function returns new instance of <code>ServingInputReceiver</code>, which is passed to <code>export_savedmodel</code> or <code>tf.estimator.FinalExporter</code></p>\n\n<pre><code>...\nimage_classifier.export_savedmodel(saved_dir, serving_input_receiver_fn)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1552316550}, {'QuestionId': 43966073, 'AnswerId': 47920968, 'URL': 'https://stackoverflow.com/questions/43966073/prediction-from-model-saved-with-tf-estimator-estimator-in-tensorflow/47920968#47920968', 'QuestionTitle': 'Prediction from model saved with `tf.estimator.Estimator` in Tensorflow', 'Answer': '<p>As for the TypeError, I solve it in this way.</p>\n\n<p>First, name the placeholder:</p>\n\n<pre><code>feature_spec = {""input_image"": tf.placeholder(dtype=tf.float32, shape=[None, 784], name=\'input_image\')}\n</code></pre>\n\n<p>Then you can use it like this:</p>\n\n<pre><code>feed_dict={""input_image:0"": input_data}\n</code></pre>\n\n<p>Hope it can help someone.</p>\n\n<hr>\n\n<p>EDIT</p>\n\n<p>In this question, after<code>estimator.export_savedmodel(...)</code>\nyou can do prediction like this:</p>\n\n<pre><code>with tf.Session(graph=tf.Graph()) as sess:\n    meta_graph_def = tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], model_path)\n    signature = meta_graph_def.signature_def\n    x_tensor_name = signature[\'classes\'].inputs[\'input_image\'].name\n    y_tensor_name = signature[\'classes\'].outputs[\'labels\'].name\n    x = sess.graph.get_tensor_by_name(x_tensor_name)\n    y = sess.graph.get_tensor_by_name(y_tensor_name)\n    predictions = sess.run(y, {x: mnist.test.images[:5]})\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1513845247}, {'QuestionId': 42835809, 'AnswerId': 48089083, 'URL': 'https://stackoverflow.com/questions/42835809/how-to-export-estimator-model-with-export-savedmodel-function/48089083#48089083', 'QuestionTitle': 'How to export Estimator model with export_savedmodel function', 'Answer': '<p>Do it like this:</p>\n\n<pre><code>your_feature_spec = {\n    ""some_feature"": tf.FixedLenFeature([], dtype=tf.string, default_value=""""),\n    ""some_feature"": tf.VarLenFeature(dtype=tf.string),\n}\n\ndef _serving_input_receiver_fn():\n    serialized_tf_example = tf.placeholder(dtype=tf.string, shape=None, \n                                           name=\'input_example_tensor\')\n    # key (e.g. \'examples\') should be same with the inputKey when you \n    # buid the request for prediction\n    receiver_tensors = {\'examples\': serialized_tf_example}\n    features = tf.parse_example(serialized_tf_example, your_feature_spec)\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\nestimator.export_savedmodel(export_dir, _serving_input_receiver_fn)\n</code></pre>\n\n<p>Then you can request the served model with ""predict"" signature name by batch.</p>\n\n<p>Source: <a href=""https://www.tensorflow.org/guide/saved_model#prepare_serving_inputs"" rel=""noreferrer"">https://www.tensorflow.org/guide/saved_model#prepare_serving_inputs</a></p>\n', 'IsAccepted': False, 'CreationDate': 1515043028}, {'QuestionId': 43966073, 'AnswerId': 51997008, 'URL': 'https://stackoverflow.com/questions/43966073/prediction-from-model-saved-with-tf-estimator-estimator-in-tensorflow/51997008#51997008', 'QuestionTitle': 'Prediction from model saved with `tf.estimator.Estimator` in Tensorflow', 'Answer': ""<p>I predict successfully by using <code>tensorflow.contrib.predictor</code>:</p>\n\n<pre><code>from tensorflow.contrib import predictor\n\npredict_fn = predictor.from_saved_model(\n    export_dir='model/1535012949',  # your model path\n    signature_def_key='predict', \n)\n\npredictions = predict_fn({'examples': examples})  # FYI, rename to `input_image`\n</code></pre>\n\n<p>But I want to predict by <code>session</code> and <code>tensors</code> also, so that I can use the traning model with other languages. Expect some perfect answer!</p>\n"", 'IsAccepted': False, 'CreationDate': 1535081089}, {'QuestionId': 51537682, 'AnswerId': 51977746, 'URL': 'https://stackoverflow.com/questions/51537682/how-do-i-export-an-estimator-tf-estimator-dnnclassifier/51977746#51977746', 'QuestionTitle': 'How do I export an Estimator, tf.estimator.DNNClassifier', 'Answer': '<p>You might read this before.\n<a href=""https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model"">Tensorflow: how to save/restore a model?</a></p>\n\n<p>A serving_input_receiver_fn should be defined.</p>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/export/build_parsing_serving_input_receiver_fn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/export/build_parsing_serving_input_receiver_fn</a></p>\n\n<p>This document introduce a valuable method to build the serving_input_receiver_fn.</p>\n\n<p>Here is the example:</p>\n\n<pre><code># first you should prepare feature_spec. it include the speciation of your feature columns. \n\nfeature_spec = tf.feature_column.make_parse_example_spec(my_feature_columns)\nprint feature_spec\nserving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\n\nexport_model = classifier.export_savedmodel(\'./iris/\', serving_input_receiver_fn)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1534993131}, {'QuestionId': 51537682, 'AnswerId': 51540810, 'URL': 'https://stackoverflow.com/questions/51537682/how-do-i-export-an-estimator-tf-estimator-dnnclassifier/51540810#51540810', 'QuestionTitle': 'How do I export an Estimator, tf.estimator.DNNClassifier', 'Answer': '<p>All I had to do is add <strong><em>model_dir= os.getcwd()+\'\\Model\'</em></strong> to the estimator</p>\n\n<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">\r\n<div class=""snippet-code"">\r\n<pre class=""snippet-code-html lang-html prettyprint-override""><code>model_dir= os.getcwd()+\'\\Model\'</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>this is the new Code , I have created a new Folder and named it model. </p>\n\n<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">\r\n<div class=""snippet-code"">\r\n<pre class=""snippet-code-html lang-html prettyprint-override""><code>estimator = tf.estimator.DNNClassifier(\r\n    hidden_units=[10, 20],\r\n    feature_columns=[embedded_text_feature_column],\r\n    n_classes=4,\r\n    optimizer=tf.train.AdagradOptimizer(learning_rate=0.003),\r\n    model_dir= os.getcwd()+\'\\Model\')</code></pre>\r\n</div>\r\n</div>\r\n</p>\n', 'IsAccepted': False, 'CreationDate': 1532613947}, {'QuestionId': 51537682, 'AnswerId': 51539450, 'URL': 'https://stackoverflow.com/questions/51537682/how-do-i-export-an-estimator-tf-estimator-dnnclassifier/51539450#51539450', 'QuestionTitle': 'How do I export an Estimator, tf.estimator.DNNClassifier', 'Answer': '<p>You can esily get a serving_input_fn with <code>tf.estimator.export.build_parsing_serving_input_receiver_fn</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/export/build_parsing_serving_input_receiver_fn"" rel=""nofollow noreferrer"">link</a>)</p>\n\n<p>In your case do something like:</p>\n\n<pre><code>serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n        [embedded_text_feature_column])\n</code></pre>\n\n<p>If you expect to pass tensors directly there\'s also <code>build_raw_serving_input_receiver_fn</code> in the same package.</p>\n', 'IsAccepted': False, 'CreationDate': 1532609937}, {'QuestionId': 51482730, 'AnswerId': 51495979, 'URL': 'https://stackoverflow.com/questions/51482730/tensorflow-how-to-export-estimator-using-tensorhub-module/51495979#51495979', 'QuestionTitle': 'TensorFlow: how to export estimator using TensorHub module?', 'Answer': '<p>If you want to feed raw strings, you might want to consider using the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/export/build_raw_serving_input_receiver_fn"" rel=""nofollow noreferrer"">raw input receiver</a>. This code:</p>\n\n<pre><code>feature_placeholder = {\'title\': tf.placeholder(\'string\', [1], name=\'title_placeholder\')}\nserving_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholder)\n\nestimator.export_savedmodel(dir_path, serving_input_fn)\n</code></pre>\n\n<p>will give you a <code>SavedModel</code> with the following input specification according to the <code>SavedModel</code> CLI:</p>\n\n<pre><code>saved_model_cli show --dir ./ --tag_set serve --signature_def serving_default\n\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs[\'inputs\'] tensor_info:\n    dtype: DT_STRING\n    shape: (-1)\n    name: title_placeholder_1:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs[\'classes\'] tensor_info:\n    dtype: DT_STRING\n    shape: (-1, 2)\n    name: linear/head/Tile:0\n  outputs[\'scores\'] tensor_info:\n    dtype: DT_FLOAT\n    shape: (-1, 2)\n    name: linear/head/predictions/probabilities:0\n</code></pre>\n\n<p>You can provide a python expression to the CLI to serve an input to the model to validate that it works:</p>\n\n<pre><code>saved_model_cli run --dir ./ --tag_set serve --signature_def \\\nserving_default --input_exprs ""inputs=[\'this is a test sentence\']""\n\nResult for output key classes:\n[[b\'0\' b\'1\']]\nResult for output key scores:\n[[0.5123377 0.4876624]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1532427048}, {'QuestionId': 50612923, 'AnswerId': 50613631, 'URL': 'https://stackoverflow.com/questions/50612923/tensorflow-export-estimators-for-prediction/50613631#50613631', 'QuestionTitle': 'Tensorflow export estimators for prediction', 'Answer': '<p>The <code>Estimator</code> has <code>model_dir</code> args where the model will be saved. So during prediction we use the <code>Estimator</code> and call the <code>predict</code> method which recreates the graph and the checkpoints are loaded.</p>\n\n<p>For the <code>MNIST</code> example, the prediction code would be:</p>\n\n<pre><code>tf.reset_default_graph()\n\n# An input-function to predict the class of new data.\npredict_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={""x"": eval_data},\n    num_epochs=1,\n    shuffle=False)\n\nmnist_classifier = tf.estimator.Estimator(\n      model_fn=cnn_model_fn, model_dir=""/tmp/mnist_convnet_model"")\n\n#Prediction call\npredictions = mnist_classifier.predict(input_fn=predict_input_fn)\n\npred_class = np.array([p[\'classes\'] for p in predictions]).squeeze()\nprint(pred_class)\n\n# Output\n# [7 2 1 ... 4 5 6]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1527715951}, {'QuestionId': 46239139, 'AnswerId': 46287309, 'URL': 'https://stackoverflow.com/questions/46239139/tensorflow-optimize-for-inference-a-savedmodel-exported-by-estimator/46287309#46287309', 'QuestionTitle': 'TensorFlow: Optimize for Inference a SavedModel exported by Estimator', 'Answer': '<p><strong>Update:</strong> There are good instructions at <a href=""https://www.tensorflow.org/mobile/prepare_models"" rel=""nofollow noreferrer"">https://www.tensorflow.org/mobile/prepare_models</a> which include an explaination of what to do with SavedModels. You can freeze your SavedModel using the <a href=""https://github.com/tensorflow/tensorflow/blob/cfd0ea3bfb85d92cdb32760ea024f1e38618d717/tensorflow/python/tools/freeze_graph.py#L362"" rel=""nofollow noreferrer"">--input_saved_model_dir</a> to freeze_graph.py.</p>\n\n<p>They\'re both protocol buffers (.pb), but unfortunately they\'re different messages (i.e. different file formats). Theoretically you could first extract a <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/meta_graph.proto"" rel=""nofollow noreferrer"">MetaGraph</a> from the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/saved_model.proto"" rel=""nofollow noreferrer"">SavedModel</a>, then ""freeze"" the MetaGraph\'s GraphDef (move variables into constants), then run this script on the frozen GraphDef. In that case you\'d want your input_fn to be just placeholders.</p>\n\n<p>You could also add a plus one emoji on one of the <a href=""https://github.com/tensorflow/tensorflow/issues/12750"" rel=""nofollow noreferrer"">""SavedModel support for Android"" Github issues</a>. Medium-term we\'d like to standardize on SavedModel; sorry you\'ve run into this!</p>\n', 'IsAccepted': False, 'CreationDate': 1505764943}, {'QuestionId': 46539142, 'AnswerId': 48554153, 'URL': 'https://stackoverflow.com/questions/46539142/how-to-save-a-tensorflow-estimator-by-export-savedmodel-then-load-and-use-it-lo/48554153#48554153', 'QuestionTitle': 'How to save a tensorflow estimator by export_savedmodel, then load and use it locally later?', 'Answer': '<pre><code>def serving_input_receiver_fn():\n    """"""\n    input placeholder\n    """"""\n    inputs = {""x"": tf.placeholder(shape=[feature_size], dtype=tf.float32)}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n# export model and weights\nexport_dir = est_inception_v3.export_savedmodel(export_dir_base=""/export_dir"", \n    serving_input_receiver_fn=serving_input_receiver_fn)\n\n# restore from disk\nwith tf.Session() as sess:\n    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], export_dir)\n    predictor = SavedModelPredictor(export_dir)\n    print(predictor({""x"": test_x_mat}))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1517449660}, {'QuestionId': 44460362, 'AnswerId': 48329873, 'URL': 'https://stackoverflow.com/questions/44460362/how-to-save-estimator-in-tensorflow-for-later-use/48329873#48329873', 'QuestionTitle': 'How to save estimator in Tensorflow for later use?', 'Answer': '<p>Almost all real applications of machine learning seek to train a model once and then save it for future uses with new data. Most classifiers spend hours in the training stage and just few seconds in the testing stage, so is fundamental learn how to save successfully a trained model.</p>\n\n<p>I\'m going to explain how to export ""high level"" Tensorflow models (using <strong>export_savedmodel</strong>).\nThe function <strong>export_savedmodel</strong> requires the argument serving_input_receiver_fn, that is a function without arguments, which defines the input from the model and the predictor. Therefore, you must create your own <strong>serving_input_receiver_fn</strong>, where the model input type match with the model input in the training script, and the predictor input type match with the predictor input in the testing script.\nOn the other hand, if you create a custom model, you must define the export_outputs,  defined by the function <strong>tf.estimator.export.PredictOutput</strong>, which input is a dictionary that define the name that has to match with the name of the predictor output in the testing script.</p>\n\n<p>For example:</p>\n\n<h2>TRAINING SCRIPT</h2>\n\n<pre><code>def serving_input_receiver_fn():\n    serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[None], name=\'input_tensors\')\n    receiver_tensors      = {""predictor_inputs"": serialized_tf_example}\n    feature_spec          = {""words"": tf.FixedLenFeature([25],tf.int64)}\n    features              = tf.parse_example(serialized_tf_example, feature_spec)\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\ndef estimator_spec_for_softmax_classification(logits, labels, mode):\n    predicted_classes = tf.argmax(logits, 1)\n    if (mode == tf.estimator.ModeKeys.PREDICT):\n        export_outputs = {\'predict_output\': tf.estimator.export.PredictOutput({""pred_output_classes"": predicted_classes, \'probabilities\': tf.nn.softmax(logits)})}\n        return tf.estimator.EstimatorSpec(mode=mode, predictions={\'class\': predicted_classes, \'prob\': tf.nn.softmax(logits)}, export_outputs=export_outputs) # IMPORTANT!!!\n    onehot_labels = tf.one_hot(labels, 31, 1, 0)\n    loss          = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n    if (mode == tf.estimator.ModeKeys.TRAIN):\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n        train_op  = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n    eval_metric_ops = {\'accuracy\': tf.metrics.accuracy(labels=labels, predictions=predicted_classes)}\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\ndef model_custom(features, labels, mode):\n    bow_column           = tf.feature_column.categorical_column_with_identity(""words"", num_buckets=1000)\n    bow_embedding_column = tf.feature_column.embedding_column(bow_column, dimension=50)   \n    bow                  = tf.feature_column.input_layer(features, feature_columns=[bow_embedding_column])\n    logits               = tf.layers.dense(bow, 31, activation=None)\n    return estimator_spec_for_softmax_classification(logits=logits, labels=labels, mode=mode)\ndef main():\n    # ...\n    # preprocess-&gt; features_train_set and labels_train_set\n    # ...\n    classifier     = tf.estimator.Estimator(model_fn = model_custom)\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(x={""words"": features_train_set}, y=labels_train_set, batch_size=batch_size_param, num_epochs=None, shuffle=True)\n    classifier.train(input_fn=train_input_fn, steps=100)\n    full_model_dir = classifier.export_savedmodel(export_dir_base=""C:/models/directory_base"", serving_input_receiver_fn=serving_input_receiver_fn)\n</code></pre>\n\n<h2>TESTING SCRIPT</h2>\n\n<pre><code>def main():\n    # ...\n    # preprocess-&gt; features_test_set\n    # ...\n    with tf.Session() as sess:\n        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], full_model_dir)\n        predictor   = tf.contrib.predictor.from_saved_model(full_model_dir)\n        model_input = tf.train.Example(features=tf.train.Features( feature={""words"": tf.train.Feature(int64_list=tf.train.Int64List(value=features_test_set)) })) \n        model_input = model_input.SerializeToString()\n        output_dict = predictor({""predictor_inputs"":[model_input]})\n        y_predicted = output_dict[""pred_output_classes""][0]\n</code></pre>\n\n<p>(Code tested in Python 3.6.3, Tensorflow 1.4.0)</p>\n', 'IsAccepted': False, 'CreationDate': 1516307738}, {'QuestionId': 42835809, 'AnswerId': 44474427, 'URL': 'https://stackoverflow.com/questions/42835809/how-to-export-estimator-model-with-export-savedmodel-function/44474427#44474427', 'QuestionTitle': 'How to export Estimator model with export_savedmodel function', 'Answer': '<p>You have 2 options:</p>\n\n<h2>Export your model to work with JSON dictionaries</h2>\n\n<p>In my <a href=""https://github.com/Fematich/mlengine-boilerplate/blob/master/trainer/task.py"" rel=""nofollow noreferrer"">mlengine-boilerplate repository</a>, I use this to export estimator models to Cloud ML Engine to easily use this with online predictions (<a href=""https://github.com/Fematich/mlengine-boilerplate/blob/master/predictions/predict.py"" rel=""nofollow noreferrer"">sample code for the predictions</a>). Essential part:</p>\n\n<pre><code>def serving_input_fn():\n    feature_placeholders = {\n        \'id\': tf.placeholder(tf.string, [None], name=""id_placeholder""),\n        \'feat\': tf.placeholder(tf.float32, [None, FEAT_LEN], name=""feat_placeholder""),\n        #label is not required since serving is only used for inference\n    }\n    return input_fn_utils.InputFnOps(\n        feature_placeholders,\n        None,\n        feature_placeholders)\n</code></pre>\n\n<h2>Export your model to work with Tensorflow Examples</h2>\n\n<p><a href=""https://github.com/MtDersvan/tf_playground/blob/master/wide_and_deep_tutorial/wide_and_deep_basic_serving.md"" rel=""nofollow noreferrer"">This tutorial</a> shows how you can use <code>export_savedmodel</code> to serve\n   the Wide &amp; Deep Model implemented with estimators and how to feed Tensorflow examples into the exported model. The essential\n   part:</p>\n\n<pre><code>from tensorflow.contrib.learn.python.learn.utils import input_fn_utils      \nserving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1497104750}, {'QuestionId': 42835809, 'AnswerId': 47645149, 'URL': 'https://stackoverflow.com/questions/42835809/how-to-export-estimator-model-with-export-savedmodel-function/47645149#47645149', 'QuestionTitle': 'How to export Estimator model with export_savedmodel function', 'Answer': '<p>You need to have tf.train.Example and tf.train.Feature and pass the input to input receiver function and invoke the model.\n You can take a look at this example\n<a href=""https://github.com/tettusud/tensorflow-examples/tree/master/estimators"" rel=""nofollow noreferrer"">https://github.com/tettusud/tensorflow-examples/tree/master/estimators</a></p>\n', 'IsAccepted': False, 'CreationDate': 1512441009}, {'QuestionId': 47440114, 'AnswerId': 47470281, 'URL': 'https://stackoverflow.com/questions/47440114/export-tensorflow-estimator/47470281#47470281', 'QuestionTitle': 'Export Tensorflow Estimator', 'Answer': '<p>Please look at <a href=""https://github.com/tensorflow/tensorflow/issues/9952"" rel=""nofollow noreferrer"">this issue</a> on github, it looks like you have the same problem. Apparently (at least when using <code>estimator.export_savedmodel</code>)  you should load the graph with <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/loader.h#L50"" rel=""nofollow noreferrer"">LoadSavedModel</a> instead of ReadBinaryProto, because it\'s not saved as a graphdef file.</p>\n\n<p>You\'ll find <a href=""https://www.tensorflow.org/programmers_guide/saved_model#loading_a_savedmodel_in_c"" rel=""nofollow noreferrer"">here</a> a bit more instructions about how to use it:</p>\n\n<pre><code> const string export_dir = ...\nSavedModelBundle bundle;\n...\nLoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagTrain},\n               &amp;bundle);\n</code></pre>\n\n<p>I can\'t seem to find the <code>SavedModelBundle</code> documentation for c++ to use it afterwards, but it\'s likely close to <a href=""https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/SavedModelBundle"" rel=""nofollow noreferrer"">the same class in Java</a>, in which case it basically contains the session and the graph you\'ll be using.</p>\n', 'IsAccepted': False, 'CreationDate': 1511516236}, {'QuestionId': 43966073, 'AnswerId': 44535601, 'URL': 'https://stackoverflow.com/questions/43966073/prediction-from-model-saved-with-tf-estimator-estimator-in-tensorflow/44535601#44535601', 'QuestionTitle': 'Prediction from model saved with `tf.estimator.Estimator` in Tensorflow', 'Answer': '<p>I am working with tf.contrib.learn.Estimator. \nAs I see, the syntax and method signatures are almost the same so I believe the differences are related to the several Tensorflow versions.\nSo you can create Estimator as usual with something like</p>\n\n<pre><code>estimator = learn.Estimator(\n         model_fn=your_model,\n         model_dir=""tmp"",\n         config=tf.contrib.learn.RunConfig(\n             save_checkpoints_steps=10,\n             save_summary_steps=10,\n             save_checkpoints_secs=None\n         )\n     )\n</code></pre>\n\n<p>Then you do the train as<code>estimator.fit(input_fn=input_function, steps=100)</code></p>\n\n<p>And then you can do the prediction calling <br></p>\n\n<pre><code>estimator .predict(prediction)\n</code></pre>\n\n<p>Mote, there is a trick, related with the Tensorflow\'s <a href=""https://github.com/tensorflow/tensorflow/issues/3208"" rel=""nofollow noreferrer"">known issue</a>. Calling <code>predict</code> does not properly initialize the Estimator, so you need to call </p>\n\n<pre><code>estimator.evaluate(x=prediction, y=label_array, steps=1)\n</code></pre>\n\n<p>before calling <code>predict</code>.</p>\n\n<p>Hope, this helps.</p>\n', 'IsAccepted': False, 'CreationDate': 1497415342}, {'QuestionId': 44460362, 'AnswerId': 44460773, 'URL': 'https://stackoverflow.com/questions/44460362/how-to-save-estimator-in-tensorflow-for-later-use/44460773#44460773', 'QuestionTitle': 'How to save estimator in Tensorflow for later use?', 'Answer': '<p><code>Estimator</code> has an <code>export_savedmodel</code> member function for that purpose. You will find the docs <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Estimator#export_savedmodel"" rel=""nofollow noreferrer"">here</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1497019720}, {'QuestionId': 43966073, 'AnswerId': 43973072, 'URL': 'https://stackoverflow.com/questions/43966073/prediction-from-model-saved-with-tf-estimator-estimator-in-tensorflow/43973072#43973072', 'QuestionTitle': 'Prediction from model saved with `tf.estimator.Estimator` in Tensorflow', 'Answer': '<p>The name of your input Tensor probably is <code>input_image:0</code>.</p>\n\n<p>You can list the signature of your saved model by calling:</p>\n\n<p><code>print(estimator.signature_def[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY])</code></p>\n\n<p>That should list the expected input/output Tensors.</p>\n', 'IsAccepted': False, 'CreationDate': 1494830938}, {'QuestionId': 42835809, 'AnswerId': 42932112, 'URL': 'https://stackoverflow.com/questions/42835809/how-to-export-estimator-model-with-export-savedmodel-function/42932112#42932112', 'QuestionTitle': 'How to export Estimator model with export_savedmodel function', 'Answer': ""<p>if you are using tensorflow straight from the master branch there's a module tensorflow.python.estimator.export that provides a function for that:</p>\n\n<pre><code>from tensorflow.python.estimator.export import export\nfeature_spec = {'MY_FEATURE': tf.constant(2.0, shape=[1, 1])}\nserving_input_fn = export.build_raw_serving_input_receiver_fn(feature_spec)\n</code></pre>\n\n<p>Unfortunately at least for me it will not go further than that but I'm not sure if my model is really correct so maybe you have more luck than I do.</p>\n\n<p>Alternatively, there are the following functions for the current version installed from pypi:</p>\n\n<pre><code>serving_input_fn = tf.contrib.learn.utils.build_parsing_serving_input_fn(feature_spec)\nserving_input_fn = tf.contrib.learn.utils.build_default_serving_input_fn(feature_spec)\n</code></pre>\n\n<p>But I couldn't get them to work, too.</p>\n\n<p>Probably, I'm not understanding this correctly so I hope you'll have more luck.</p>\n\n<p>chris</p>\n"", 'IsAccepted': False, 'CreationDate': 1490111673}]",{56553579},"['features = {\n        # Resize given images. \'image\': tf.reshape(reciever_tensors[INPUT_FEATURE], [-1, INPUT_SHAPE])\n    }\n    return tf.estimator.export.ServingInputReceiver(receiver_tensors=reciever_tensors,\n                                                    features=features)\n</code></pre>\n\n<p>Then use <code>tf.estimator.BestExporter</code> as shown below:</p>\n\n<pre><code>best_exporter = tf.estimator.BestExporter(\n        serving_input_receiver_fn=serving_input_receiver_fn,\n        exports_to_keep=1)\n    exporters = [best_exporter]\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={input_name: eval_data},\n        y=eval_labels,\n        num_epochs=1,\n        shuffle=False)\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=eval_input_fn,\n        throttle_secs=10,\n        start_delay_secs=10,\n        steps=None,\n        exporters=exporters)\n\n    # Train and evaluate the model. tf.estimator.train_and_evaluate(classifier, train_spec=train_spec, eval_spec=eval_spec)\n</code></pre>\n\n<p>For more info, refer the link:\n<a href=""https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_keras_estimator.py"" rel=""nofollow noreferrer"">https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_keras_estimator.py</a></p>\n', '<p>Can you try the code shown below:</p>\n\n<pre><code>def serving_input_receiver_fn():\n    """"""\n    This is used to define inputs to serve the model. :return: ServingInputReciever\n    """"""\n    reciever_tensors = {\n        # The size of input image is flexible. \'image\': tf.placeholder(tf.float32, [None, None, None, 1]),\n    }\n\n    # Convert give inputs to adjust to the model.', '<p>Can you try the code shown below:</p>\n\n<pre><code>def serving_input_receiver_fn():\n    """"""\n    This is used to define inputs to serve the model.\n    :return: ServingInputReciever\n    """"""\n    reciever_tensors = {\n        # The size of input image is flexible.\n        \'image\': tf.placeholder(tf.float32, [None, None, None, 1]),\n    }\n\n    # Convert give inputs to adjust to the model.\n    features = {\n        # Resize given images.\n        \'image\': tf.reshape(reciever_tensors[INPUT_FEATURE], [-1, INPUT_SHAPE])\n    }\n    return tf.estimator.export.ServingInputReceiver(receiver_tensors=reciever_tensors,\n                                                    features=features)\n</code></pre>\n\n<p>Then use <code>tf.estimator.BestExporter</code> as shown below:</p>\n\n<pre><code>best_exporter = tf.estimator.BestExporter(\n        serving_input_receiver_fn=serving_input_receiver_fn,\n        exports_to_keep=1)\n    exporters = [best_exporter]\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={input_name: eval_data},\n        y=eval_labels,\n        num_epochs=1,\n        shuffle=False)\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=eval_input_fn,\n        throttle_secs=10,\n        start_delay_secs=10,\n        steps=None,\n        exporters=exporters)\n\n    # Train and evaluate the model.\n    tf.estimator.train_and_evaluate(classifier, train_spec=train_spec, eval_spec=eval_spec)\n</code></pre>\n\n<p>For more info, refer the link:\n<a href=""https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_keras_estimator.py"" rel=""nofollow noreferrer"">https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_keras_estimator.py</a></p>\n']",{'https://stackoverflow.com/questions/56553579/how-to-export-estimators-best-model/56679248#56679248'},,0.16291067497647152,0.04230501376132423
9,71149271,tf.data.Dataset,Documentation Replication on Other Examples,"How to remove single feature from tensorflow dataset, how to use apply on single feture?","<p>I created dataset from csv file with dataset = tf.data.experimental.make_csv_dataset() function but My dataset has categorical and numeric features.</p>
<pre><code>dataset=
color  price weight
red    120    1.2
blue    80     2.0
green   90     3
</code></pre>
<p>Question 1:
The question is how can I  modify  only single feature, for example weight +2, to:</p>
<pre><code>dataset=
color  price weight
red    120    3.2
blue    80     4.0
green   90     5
</code></pre>
<p>I try to do something like:</p>
<pre><code>dataset = dataset.apply(lambda x: x['weight']+2)
</code></pre>
<p>but the error is: &quot;TypeError: 'FilterDataset' object is not subscriptable&quot;</p>
<p>Example from the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply</a> doesn't show it.</p>
<p>Question 2:
How can I remove single feature ? Is there any equivalent to pandas drop column?</p>
","<p>You can remove features by only filtering the features that you want. This how you can modify only one feature:</p>
<pre><code>import tensorflow as tf
import pandas as pd

df = pd.DataFrame(data={'color': ['red', 'blue','green'], 'price': [120, 80, 90], 'weight': [3.2, 4.0, 5]})
df.to_csv('data.csv', index=False)

dataset = tf.data.experimental.make_csv_dataset('/content/data.csv', batch_size=1, num_epochs = 1, shuffle=False)
dataset = dataset.map(lambda x: (x['color'], x['price'], x['weight']+2))

for x in dataset:
  print(x[0], x[1], x[2])
</code></pre>
<pre><code>tf.Tensor([b'red'], shape=(1,), dtype=string) tf.Tensor([120], shape=(1,), dtype=int32) tf.Tensor([5.2], shape=(1,), dtype=float32)
tf.Tensor([b'blue'], shape=(1,), dtype=string) tf.Tensor([80], shape=(1,), dtype=int32) tf.Tensor([6.], shape=(1,), dtype=float32)
tf.Tensor([b'green'], shape=(1,), dtype=string) tf.Tensor([90], shape=(1,), dtype=int32) tf.Tensor([7.], shape=(1,), dtype=float32)
</code></pre>
","{66986310, 74881158, 62303911, 52661127, 47447183, 63704145, 62925173, 64869590, 28296670, 67521759}","[{'QuestionId': 74881158, 'AnswerId': 74881433, 'URL': 'https://stackoverflow.com/questions/74881158/deleat-row-with-nan-in-a-tensorflow-dataset/74881433#74881433', 'QuestionTitle': 'deleat row with nan in a tensorflow dataset', 'Answer': '<p>You can create a <code>map</code> function to filter out the rows containing <code>nan</code> using <code>tf.is_nan()</code>,</p>\n<pre><code>ds = ds.map(lambda x: tf.boolean_mask(x, tf.reduce_all(~tf.math.is_nan(x), axis=-1)))\n</code></pre>\n<p>The boolean mask skips rows which contains <code>nan</code> as any element.</p>\n<p>Testing the complete code,</p>\n<pre><code>simple_data_samples = np.array([\n    [1, 11, 111, -1, -11],\n    [2, np.nan, 222, -2, -22],\n    [3, 33, 333, -3, -33],\n    [4, 44, 444, -4, -44],\n    [5, 55, 555, -5, -55],\n    [6, 66, 666, -6, -66],\n    [7, 77, 777, -7, -77],\n    [8, 88, 888, -8, -88],\n    [9, 99, 999, -9, np.nan],\n    [10, 100, 1000, -10, -100],\n    [11, 111, 1111, -11, -111],\n    [12, 122, 122, -12, -122]\n])\n\nds = tf.data.Dataset.from_tensor_slices(simple_data_samples)\n\nds = ds.window(4, shift=1, drop_remainder=True)\nds = ds.flat_map(lambda x: x).batch(4)\nds = ds.shuffle(10)\nds = ds.map(lambda x: tf.boolean_mask(x, tf.reduce_all(~tf.math.is_nan(x), axis=-1)))\n\nfor data in ds.take(1):#just printing the first sample\n    print(data)\n#output\n tf.Tensor(\n[[  1.  11. 111.  -1. -11.]\n [  3.  33. 333.  -3. -33.]\n [  4.  44. 444.  -4. -44.]]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1671654251}, {'QuestionId': 67521759, 'AnswerId': 70557259, 'URL': 'https://stackoverflow.com/questions/67521759/how-to-extract-data-without-label-from-tensorflow-dataset/70557259#70557259', 'QuestionTitle': 'How to extract data without label from tensorflow dataset', 'Answer': '<p>You can use TF Dataset method <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#unbatch"" rel=""nofollow noreferrer"">unbatch</a>() to unbatch the dataset, then you can easily retrieve the data and the labels from it:</p>\n<pre><code>data=[]\nfor images, labels in ds.unbatch():\n    data.append(images)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1641137352}, {'QuestionId': 62925173, 'AnswerId': 62957244, 'URL': 'https://stackoverflow.com/questions/62925173/how-to-remove-certain-classes-labels-and-images-from-existing-tensorflow-datas/62957244#62957244', 'QuestionTitle': 'How to remove certain classes (labels and images) from existing TensorFlow datasets? (Fashion MNIST)', 'Answer': '<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.datasets import fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n# sorting based on index\nidx = np.argsort(train_labels)\ntrain_images = train_images[idx]\ntrain_labels = train_labels[idx]\n\nidx = np.argsort(test_labels)\ntest_images = test_images[idx]\ntest_labels = test_labels[idx]\n\nlabels = [&quot;T-Shirt&quot;, &quot;Trouser&quot;, &quot;Pullover&quot;, &quot;Dress&quot;, &quot;Coat&quot;, \n          &quot;Sandal&quot;, &quot;Shirt&quot;, &quot;Sneaker&quot;, &quot;Bag&quot;, &quot;Ankle boot&quot;]\n\nlabel_mapping = dict(zip(labels, range(10)))\n\ndef get_data(mapping, classes):\n    X_train, X_test, y_train, y_test = [], [], [], []\n    for cls in classes:\n        idx = mapping[cls]\n        start = idx*6000\n        end = idx*6000+6000\n        X_train.append(train_images[start: end])\n        y_train.append(train_labels[start: end])\n        start = idx*1000\n        end = idx*1000+1000\n        X_test.append(test_images[start: end])\n        y_test.append(test_labels[start: end])\n    return X_train, X_test, y_train, y_test\n\n\nX_train, X_test, y_train, y_test = get_data(label_mapping, \n                                            classes=[&quot;T-Shirt&quot;, &quot;Shirt&quot;, &quot;Trouser&quot;])\n</code></pre>\n<p>You can find the mapping between classes and their labels <a href=""https://github.com/zalandoresearch/fashion-mnist"" rel=""nofollow noreferrer"">here</a></p>\n', 'IsAccepted': True, 'CreationDate': 1595000071}, {'QuestionId': 67521759, 'AnswerId': 67526013, 'URL': 'https://stackoverflow.com/questions/67521759/how-to-extract-data-without-label-from-tensorflow-dataset/67526013#67526013', 'QuestionTitle': 'How to extract data without label from tensorflow dataset', 'Answer': '<p>If you did not apply further transformations to the dataset it will be a <code>BatchDataset</code>. You can create two lists to iterate over dataset. Here in total I have <strong>2936</strong> images.</p>\n<pre><code>x_train, y_train = [], []\n\nfor images, labels in train_ds:\n  x_train.append(images.numpy())\n  y_train.append(labels.numpy())\n\nnp.array(x_train).shape &gt;&gt; (92,)\n</code></pre>\n<p>It was generating batches. You can use <code>np.concatenate</code> to concat them.</p>\n<pre><code>x_train = np.concatenate(x_train, axis = 0) \nx_train.shape &gt;&gt; (2936,28,28,3)\n</code></pre>\n<p>Or you can unbatch the dataset and iterate over it:</p>\n<pre><code>for images, labels in train_ds.unbatch():\n  x_train.append(images.numpy())\n  y_train.append(labels.numpy())\n\nx_train = np.array(x_train)\nx_train.shape &gt;&gt; (2936,28,28,3)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1620938694}, {'QuestionId': 66986310, 'AnswerId': 66986420, 'URL': 'https://stackoverflow.com/questions/66986310/drop-bad-data-from-dataset-tensorflow/66986420#66986420', 'QuestionTitle': 'Drop bad data from dataset Tensorflow', 'Answer': ""<p>Assuming that <code>element</code> is a data frame in your code, then it would be:</p>\n<pre><code>def parse_function(element):\n    element = element.query('height&gt;0')\n\n    labels = element['label']\n    features['height'] = element['height']\n\n    return features, labels\n\nds = tf.data.Dataset.from_tensor_slices(ds_files)\nclean_ds = ds.map(parse_function)\n</code></pre>\n<p>`</p>\n"", 'IsAccepted': False, 'CreationDate': 1617799843}, {'QuestionId': 66986310, 'AnswerId': 66986371, 'URL': 'https://stackoverflow.com/questions/66986310/drop-bad-data-from-dataset-tensorflow/66986371#66986371', 'QuestionTitle': 'Drop bad data from dataset Tensorflow', 'Answer': '<p>You can use <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter"" rel=""nofollow noreferrer""><code>tf.data.Dataset.filter</code></a>:</p>\n<pre><code>def filter_func(elem):\n    &quot;&quot;&quot; return True if the element is to be kept &quot;&quot;&quot;\n    return tf.math.greater(elem[\'height\'],0)\n\nds = tf.data.Dataset.from_tensor_slices(ds_files)\nclean_ds = ds.filter(filter_func)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1617799660}, {'QuestionId': 64869590, 'AnswerId': 64869689, 'URL': 'https://stackoverflow.com/questions/64869590/how-can-i-remove-or-omit-data-using-map-method-for-tf-data-dataset-objects/64869689#64869689', 'QuestionTitle': 'How can I remove or omit data using map method for tf.data.Dataset objects?', 'Answer': '<p>Shortly no, you cannot filter data using <code>map</code>. Map functions apply some transformation to every element of the dataset. What you want is to check every element for some predicate and get only those elements that satisfy the predicate.</p>\n<p>And that function is <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter"" rel=""noreferrer""><code>filter()</code></a>.</p>\n<p>So you can do:</p>\n<pre><code>gen = gen.filter(lambda x: x % 2 != 0)\n</code></pre>\n<hr />\n<p><strong>Update:</strong></p>\n<p>If you want to use a custom function instead of <code>lambda</code>, you can do something like:</p>\n<pre><code>def filter_func(x):\n    if x**2 &lt; 500:\n        return True\n    return False\ngen = gen.filter(filter_func)\n</code></pre>\n<p>If this function is passed to <code>filter</code> all numbers whose square is less than 500 will be returned.</p>\n', 'IsAccepted': False, 'CreationDate': 1605590044}, {'QuestionId': 63704145, 'AnswerId': 63704560, 'URL': 'https://stackoverflow.com/questions/63704145/how-to-remove-a-vector-which-is-specific-value-from-tensor-in-tensorflow/63704560#63704560', 'QuestionTitle': 'How to remove a vector which is specific value from tensor in tensorflow?', 'Answer': '<p>You can do that like this:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\ndef remove_row(m, q):\n    # Assumes m is 2D\n    mask = tf.math.reduce_any(tf.not_equal(m, q), axis=-1)\n    return tf.boolean_mask(m, mask)\n\n# Test\nm = tf.constant([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\nq = tf.constant([2, 2, 2])\ntf.print(remove_row(m, q))\n# [[1 1 1]\n#  [3 3 3]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1599046069}, {'QuestionId': 62303911, 'AnswerId': 62373588, 'URL': 'https://stackoverflow.com/questions/62303911/how-to-select-only-one-part-of-a-tensorflow-dataset-and-change-the-dimensions/62373588#62373588', 'QuestionTitle': 'How to select only one part of a Tensorflow dataset, and change the dimensions', 'Answer': '<p>The solution to this was simply to download the dataset files elsewhere, so I had a list of .avi files in my directory, and then preprocess these files outside of tensorflow. I used the cv2 library and the following code, where I borrowed the two functions from elsewhere:</p>\n\n<pre><code># Utilities to open video files using CV2\ndef crop_center_square(frame):\n  y, x = frame.shape[0:2]\n  min_dim = min(y, x)\n  start_x = (x // 2) - (min_dim // 2)\n  start_y = (y // 2) - (min_dim // 2)\n  return frame[start_y:start_y+min_dim,start_x:start_x+min_dim]\n\ndef load_video(path, max_frames=0, resize=(256, 256)):\n  cap = cv2.VideoCapture(path)\n  frames = []\n  try:\n    while True:\n      ret, frame = cap.read()\n      if not ret:\n        break\n      frame = crop_center_square(frame)\n      frame = cv2.resize(frame, resize)\n      frame = frame[:, :, [2, 1, 0]]\n      frames.append(frame)\n\n      if len(frames) == max_frames:\n        break\n  finally:\n    cap.release()\n  return np.array(frames) / 255.0\n\n\nfiles = [f for f in glob.glob(""**/*.avi"", recursive=True)]\n\nfor video_path in files:\n  video = load_video(video_path)\n  video_name = video_path[video_path.find(\'/\')+1:]\n  num_frames = video.shape[0]\n  print(""Video in "" + video_path + "" has "" + str(num_frames) + "" frames."")\n  for seg_num in range(math.floor(num_frames/10)):\n    result = video[seg_num*10:(seg_num+1)*10, ...]\n    new_filepath = video_name[:-4] + ""_"" + str(seg_num).zfill(2) + "".avi""\n    print(new_filepath)\n    out = cv2.VideoWriter(new_filepath,0, 25.0, (256,256))\n    for frame_n in range(0,10):\n      out.write(np.uint8(255*result[frame_n, ...]))\n    out.release()\n    del result\n  del video\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1592143457}, {'QuestionId': 62303911, 'AnswerId': 62304792, 'URL': 'https://stackoverflow.com/questions/62303911/how-to-select-only-one-part-of-a-tensorflow-dataset-and-change-the-dimensions/62304792#62304792', 'QuestionTitle': 'How to select only one part of a Tensorflow dataset, and change the dimensions', 'Answer': ""<p>Forgive me for the approximate answer, because I won't download the 6GB dataset to test my answer.</p>\n\n<p>Why don't you just select the video when you iterate through the dataset:</p>\n\n<pre><code>next(iter(x_train))['video']\n</code></pre>\n\n<p>To select the dimensions, you can use normal <code>numpy</code> indexing. That would be an example with <code>mnist</code>:</p>\n\n<pre><code>import tensorflow_datasets as tfds\n\ndata = tfds.load('mnist', split='train', batch_size=16)\n</code></pre>\n\n<pre><code>&lt;PrefetchDataset shapes: {image: (None, 28, 28, 1), \n    label: (None,)}, types: {image: tf.uint8, label: tf.int64}&gt;\n</code></pre>\n\n<p>Now let's select only <code>image</code>, and select the first 10 observations.</p>\n\n<pre><code>dim = lambda x: x['image'][:10, ...]\n\nnext(iter(data.map(dim))).shape\n</code></pre>\n\n<pre><code>TensorShape([10, 28, 28, 1])\n</code></pre>\n\n<p>See how I removed a <code>None</code> in the shape with simple indexing. </p>\n"", 'IsAccepted': False, 'CreationDate': 1591795667}, {'QuestionId': 52661127, 'AnswerId': 52661186, 'URL': 'https://stackoverflow.com/questions/52661127/how-to-remove-one-dictionary-from-dataframe/52661186#52661186', 'QuestionTitle': 'How to remove one dictionary from dataframe', 'Answer': '<p>I believe you need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.transform.html"" rel=""nofollow noreferrer""><code>transform</code></a> <code>size</code> and then filter by <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing"" rel=""nofollow noreferrer""><code>boolean indexing</code></a>:</p>\n\n<pre><code>df = pd.concat([dfs])\ndf = df[df.groupby(\'appId\')[\'appId\'].transform(\'size\') &gt;= 30]\n#alternative 1\n#df = df[df.groupby(\'appId\')[\'appId\'].transform(\'size\').ge(30)]\n#alternative 2 (slowier in large data)\n#df = df.groupby(\'appId\').filter(lambda x: len(x) &gt;= 30)\n</code></pre>\n\n<p>Another approach is filter <code>dictionary</code>:</p>\n\n<pre><code>dfs = {k: v for k, v in dfs.items() if len(v) &gt;= 30}\n</code></pre>\n\n<p>EDIT:</p>\n\n<pre><code> timeseries = timeseries[timeseries.groupby(\'appId\')[\'appId\'].transform(\'size\') &gt;= 30] \n dfs = dict(tuple(timeseries.groupby(\'appId\')))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1538727226}, {'QuestionId': 28296670, 'AnswerId': 52644080, 'URL': 'https://stackoverflow.com/questions/28296670/remove-a-specific-feature-in-scikit-learn/52644080#52644080', 'QuestionTitle': 'Remove a specific feature in scikit learn', 'Answer': '<p>While there is no built in class in sklearn to do this you can easily create one with the standard fit and transform methods:</p>\n\n<pre><code>from sklearn.base import TransformerMixin\n\nclass ManualFeatureSelector(TransformerMixin):\n    """"""\n    Transformer for manual selection of features using sklearn style \n    transform \n    method.  \n    """"""\n\n    def __init__(self, features):\n        self.features = features\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[:,self.features]\n</code></pre>\n\n<p>Generally it is better to do manual feature selection like this outside of the sklearn framework but I have come across situations where manual feature selection as part of a Pipeline is helpful.  </p>\n\n<p>For example if an object passes an array to both a classifier and some other object like a display function, you may want to pass only certain fields to the classifier. This is most easily done by changing the classifier to a Pipeline incorporating the above transform and the original classifier.</p>\n\n<p>Hope that helps!</p>\n', 'IsAccepted': False, 'CreationDate': 1538647784}, {'QuestionId': 47447183, 'AnswerId': 47449700, 'URL': 'https://stackoverflow.com/questions/47447183/remove-a-set-of-tensors-from-a-tensor-in-tensorflow/47449700#47449700', 'QuestionTitle': 'Remove a set of tensors from a tensor in Tensorflow', 'Answer': '<p>Another way to do this would be either making use of split or slice function. This would be useful especially if tensor is huge.</p>\n\n<p><strong>Method 1:</strong> Making use of <a href=""https://www.tensorflow.org/api_docs/python/tf/split"" rel=""nofollow noreferrer"">split</a> function.</p>\n\n<pre><code>a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name=\'a\')\nsplit1, split2, split3 = tf.split(a, [1, 1, 1], 1)\na_new = tf.concat([split1, split3], 1)\n</code></pre>\n\n<p><strong>Method 2:</strong> Making use of <a href=""https://www.tensorflow.org/api_docs/python/tf/slice"" rel=""nofollow noreferrer"">slice</a> function.</p>\n\n<pre><code>slice1 = tf.slice(a, [0, 0], [2, 1])\nslice2 = tf.slice(a, [0, 2], [2, 1])\na_new = tf.concat([slice1, slice2], 1)\n</code></pre>\n\n<p>In both the cases, a_new will have </p>\n\n<pre><code>[[ 1.  3.]\n [ 4.  6.]]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1511421708}, {'QuestionId': 47447183, 'AnswerId': 47447462, 'URL': 'https://stackoverflow.com/questions/47447183/remove-a-set-of-tensors-from-a-tensor-in-tensorflow/47447462#47447462', 'QuestionTitle': 'Remove a set of tensors from a tensor in Tensorflow', 'Answer': ""<p>You can call <code>tf.unstack</code> to obtain a list of sub-tensors. Then you can modify the list and call <code>tf.stack</code> to construct a tensor from the list. For example, the following code removes the [2.0, 5.0] column from a:</p>\n\n<pre><code>a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\na_vecs = tf.unstack(a, axis=1)\ndel a_vecs[1]\na_new = tf.stack(a_vecs, 1)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1511410552}, {'QuestionId': 28296670, 'AnswerId': 28326733, 'URL': 'https://stackoverflow.com/questions/28296670/remove-a-specific-feature-in-scikit-learn/28326733#28326733', 'QuestionTitle': 'Remove a specific feature in scikit learn', 'Answer': '<p>There is no such think as a scikit-learn dataset. scikit-learn uses common datastructures are just numpy arrays (or scipy sparse matrices):</p>\n\n<pre><code>&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; iris = load_iris\n&gt;&gt;&gt; type(iris.data)\n&lt;class \'numpy.ndarray\'&gt;\n</code></pre>\n\n<p>You can use regular numpy array indexing to generate a new version of data. For instance to drop the second feature with boolean mask:</p>\n\n<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = iris.data\n&gt;&gt;&gt; mask = np.array([True, False, True, True])\n&gt;&gt;&gt; X_masked = X[:, mask]\n</code></pre>\n\n<p>Note, the <code>:</code> sign in the first location means ""all the rows"".</p>\n\n<p>To check, you can print the first 5 rows of each array:</p>\n\n<pre><code>&gt;&gt;&gt; print(X[:5])\n[[ 5.1  3.5  1.4  0.2]\n [ 4.9  3.   1.4  0.2]\n [ 4.7  3.2  1.3  0.2]\n [ 4.6  3.1  1.5  0.2]\n [ 5.   3.6  1.4  0.2]]\n&gt;&gt;&gt; print(X_masked[:5])\n[[ 5.1  1.4  0.2]\n [ 4.9  1.4  0.2]\n [ 4.7  1.3  0.2]\n [ 4.6  1.5  0.2]\n [ 5.   1.4  0.2]]\n</code></pre>\n\n<p>You can also use integer based fancy indexing to get the same result:</p>\n\n<pre><code>&gt;&gt;&gt; index = np.array([0, 2, 3])\n&gt;&gt;&gt; X_indexed = X[:, index]\n&gt;&gt;&gt; print(X_indexed[:5])\n[[ 5.1  1.4  0.2]\n [ 4.9  1.4  0.2]\n [ 4.7  1.3  0.2]\n [ 4.6  1.5  0.2]\n [ 5.   1.4  0.2]]\n</code></pre>\n\n<p>To learn more about basic numpy operations, have a look at a tutorial such as:</p>\n\n<p><a href=""http://scipy-lectures.github.io/"" rel=""nofollow"">http://scipy-lectures.github.io/</a></p>\n', 'IsAccepted': True, 'CreationDate': 1423068077}]","{47340607, 71149271}","['<p>I am guessing that the parsing spec created by tf.transform is different from what we normally get. Can you share the output of transformed_metadata.schema.as_feature_spec()?</p>\n\n<p>As a work-around try adding this line to your input_fn after features = tf.train.shuffle_batch(...):</p>\n\n<p><code>\nfeatures = {feature_name: tf.reshape(feature_value, [-1, 1]) for\n            feature_name, feature_value in features.items()}</code></p>\n', ""<p>You can remove features by only filtering the features that you want. This how you can modify only one feature:</p>\n<pre><code>import tensorflow as tf\nimport pandas as pd\n\ndf = pd.DataFrame(data={'color': ['red', 'blue','green'], 'price': [120, 80, 90], 'weight': [3.2, 4.0, 5]})\ndf.to_csv('data.csv', index=False)\n\ndataset = tf.data.experimental.make_csv_dataset('/content/data.csv', batch_size=1, num_epochs = 1, shuffle=False)\ndataset = dataset.map(lambda x: (x['color'], x['price'], x['weight']+2))\n\nfor x in dataset:\n  print(x[0], x[1], x[2])\n</code></pre>\n<pre><code>tf.Tensor([b'red'], shape=(1,), dtype=string) tf.Tensor([120], shape=(1,), dtype=int32) tf.Tensor([5.2], shape=(1,), dtype=float32)\ntf.Tensor([b'blue'], shape=(1,), dtype=string) tf.Tensor([80], shape=(1,), dtype=int32) tf.Tensor([6.], shape=(1,), dtype=float32)\ntf.Tensor([b'green'], shape=(1,), dtype=string) tf.Tensor([90], shape=(1,), dtype=int32) tf.Tensor([7.], shape=(1,), dtype=float32)\n</code></pre>\n""]","{'https://stackoverflow.com/questions/47340607/tensorflow-gradientboosteddecisiontreeclassifier-error-dense-float-feature-mu/47358487#47358487', 'https://stackoverflow.com/questions/71149271/how-to-remove-single-feature-from-tensorflow-dataset-how-to-use-apply-on-single/71154993#71154993'}",,0.2326627659646246,0.10586791012337482
10,51706848,tf.reshape,Documentation Completeness,How does tf.reshape() work internally ?,"<p>I'm trying to understand how tf.reshape works. Let's have an example:</p>

<pre><code>embeddings = tf.placeholder(tf.float32, shape=[N0,N1])
M_2D = tf.placeholder(tf.float32, shape=[N0,None])
M_3D = tf.reshape(M_2D, [-1,N0,1])
weighted_embeddings = tf.multiply(embeddings, M_3D)
</code></pre>

<p>Here I have a 2D tensor M_2D whose columns represent coefficients for the N0 embeddings of dimension N1. I want to create a 3D tensor where each column of M_2D is placed in the first dimension of M_3D, and columns are keep in the same order. My final goal is to create a 3D tensor of 2D embeddings, each weighted by the columns of M_2D. </p>

<p>How can I be sure that reshape actually place each column in the new dimension of M_3D. Is it possible that it places the rows instead ? Is there somewhere in tensorflow documentation a clear explanation on the internal working process of tf.reshape, particularly when -1 is provided ?    </p>
","<p>Tensor before and after <code>tf.reshape</code> have the <strong>same flatten order</strong>.</p>

<p>In tensorflow runtime, a Tensor is consists of raw data(byte array), shape, and dtype, <code>tf.reshape</code> only change shape, with raw data and dtype not changed. <code>-1</code> or <code>None</code> in <code>tf.reshape</code> means that this value can be calculated.</p>

<p>For example,</p>

<pre><code># a tensor with 6 elements, with shape [3,2]
a = tf.constant([[1,2], [3,4], [5,6]]) 
# reshape tensor to [2, 3, 1], 2 is calculated by 6/3/1
b = tf.reshape(a, [-1, 3, 1])
</code></pre>

<p>In this example, <code>a</code> and <code>b</code> have the same flatten order, namely <code>[1,2,3,4,5,6]</code>, <code>a</code> has the shape <code>[3,2]</code>, its value is <code>[[1,2], [3,4], [5,6]]</code>, <code>b</code> has the shape <code>[2,3,1]</code>, its value is <code>[[[1],[2],[3]],[[4],[5],[6]]]</code>.</p>
","{50933697, 42082913, 66862298, 38426827, 46958414, 38837519, 57236922, 64739606, 51443898, 40441503}","[{'QuestionId': 66862298, 'AnswerId': 66862606, 'URL': 'https://stackoverflow.com/questions/66862298/how-to-reshape-data-in-tensorflow-dataset/66862606#66862606', 'QuestionTitle': 'How to reshape data in Tensorflow dataset?', 'Answer': ""<p>That's easy. Do this in your split function</p>\n<pre><code>def split(window):\n    return window[:-label_length, tf.newaxis], window[-label_length, tf.newaxis, tf.newaxis]\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1617058108}, {'QuestionId': 64739606, 'AnswerId': 64766913, 'URL': 'https://stackoverflow.com/questions/64739606/what-does-fc1-tf-reshapeconv2-1-weightswd1-get-shape-as-list0/64766913#64766913', 'QuestionTitle': 'what does &quot;fc1 = tf.reshape(conv2, [-1, weights[&#39;wd1&#39;].get_shape().as_list()[0]])&quot; doing?', 'Answer': ""<p>For better understanding of <code>get.shape().as_list()</code>, here i am providing simple example, hope this help you.</p>\n<pre><code>import tensorflow as tf\nc = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nShape = c.get_shape().as_list()\nprint(Shape)  \n</code></pre>\n<p>Output:</p>\n<pre><code>[2, 3]\n</code></pre>\n<p>It means<code>c</code> has <code>2 rows and 3 columns</code>.\nWhen we print <code>Shape = c.get_shape().as_list()[0]</code> it return <code>0th</code> element of <code>c</code> in list format <em>(generally it return shape in tuple as (2,3))</em></p>\n<pre><code>Shape = c.get_shape().as_list()[0] \n</code></pre>\n<p>Output:</p>\n<pre><code>2\n</code></pre>\n<p>When we use <code>tf.reshape</code> it returns a new tensor that has the same values as old tensor in the same order, except with a new shape specified by shape.</p>\n<p>When we pass <code>shape of [-1]</code>, it flatten into 1D.</p>\n<pre><code>tf.reshape(c, [-1])\n</code></pre>\n<p>Output:</p>\n<pre><code>&lt;tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 2., 3., 4., 5., 6.], dtype=float32)&gt;\n</code></pre>\n<blockquote>\n<p>fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])</p>\n</blockquote>\n<p>To summarize, here we are flattening <em>(i.e into 1D)</em> weights of <code>conv2</code> layer <em>(i.e 2D)</em> and extracting <code>0th</code> element of <code>weights['wd1']</code>.</p>\n"", 'IsAccepted': False, 'CreationDate': 1605003274}, {'QuestionId': 51443898, 'AnswerId': 57627930, 'URL': 'https://stackoverflow.com/questions/51443898/how-do-i-use-tf-reshape/57627930#57627930', 'QuestionTitle': 'How do I use tf.reshape()?', 'Answer': ""<p><strong>Conceptually</strong>\nYou get error here because when you are using <code>sess.run(x, feed_dict{x:x1})</code>. This is trying to feed and reshape same variable. This creates a problem in run time. Thus you cannot do this using a single variable. </p>\n\n<pre><code>import tensorflow as tf\nimport random\nimport numpy as np\n\nx = tf.placeholder('float')\ny = tf.reshape(x, [-1,28,28,1])\n\nwith tf.Session() as sess:\n    x1 = np.asarray([random.uniform(0,1) for i in range(784)])\n    result = sess.run(y, feed_dict={x: x1})\n    print(result)\n</code></pre>\n\n<p>In tensorflow, variables are place holders. So, x will hold floating point values, and another variable say <code>y</code> will hold values in shape <code>[-1,28,28,1]</code>.</p>\n\n<p>If same variable name is used then it has to act as placeholder for two things. This is not possible.</p>\n"", 'IsAccepted': False, 'CreationDate': 1566569024}, {'QuestionId': 51443898, 'AnswerId': 51443997, 'URL': 'https://stackoverflow.com/questions/51443898/how-do-i-use-tf-reshape/51443997#51443997', 'QuestionTitle': 'How do I use tf.reshape()?', 'Answer': ""<p>After you reassign, x is a tensor with shape <code>[-1,28,28,1]</code> and as error says, you cannot shape <code>(784,)</code> to <code>(?, 28, 28, 1)</code>. You can use a different variable name:</p>\n\n<pre><code>import tensorflow as tf\nimport random\nimport numpy as np\n\nx = tf.placeholder('float')\ny = tf.reshape(x, [-1,28,28,1])\n\nwith tf.Session() as sess:\n    x1 = np.asarray([random.uniform(0,1) for i in range(784)])\n    result = sess.run(y, feed_dict={x: x1})\n    print(result)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1532094411}, {'QuestionId': 57236922, 'AnswerId': 57237613, 'URL': 'https://stackoverflow.com/questions/57236922/what-does-reshape-on-an-image-actually-do/57237613#57237613', 'QuestionTitle': 'What does reshape on an image actually do?', 'Answer': '<p>You are reshaping your dataset in a multidimensional array with shapes <code>(60000,28,28,1)</code> which indicates:</p>\n\n<ol>\n<li>You have 60000 samples of images</li>\n<li>The images have the size of 28×28 (Width and height)</li>\n<li>The number 1 indicates the number of channels. In this case you are using grayscale, so you only need one channel. If you need RGB you might use 3 instead. </li>\n</ol>\n\n<p>When you load the MNIST dataset, you have a shape of <code>(60000, 28, 28)</code> which does not include the information of channels. What you did is just add this information to your dataset as another dimension of your array.</p>\n', 'IsAccepted': True, 'CreationDate': 1564282779}, {'QuestionId': 50933697, 'AnswerId': 50933754, 'URL': 'https://stackoverflow.com/questions/50933697/tensorflow-tf-reshape-none-1-is-it-the-same/50933754#50933754', 'QuestionTitle': 'Tensorflow tf.reshape: None / -1, is it the same?', 'Answer': ""<p>No they are not equivalent. </p>\n\n<p>When you use <code>None</code> for placeholder it means the dimension will be defined at run time (usually the batch size).</p>\n\n<p>Whereas <code>-1</code> in reshape means if (total size of data is s_0xs_1xs_2...) -1 will automatically infer s_0 and this I think is same as numpy's behaviour. You can't use None in reshape. </p>\n"", 'IsAccepted': False, 'CreationDate': 1529428674}, {'QuestionId': 46958414, 'AnswerId': 46958574, 'URL': 'https://stackoverflow.com/questions/46958414/tf-reshape-loses-shape-of-the-tensor/46958574#46958574', 'QuestionTitle': 'tf.reshape loses shape of the tensor', 'Answer': ""<p>I think you want to use:</p>\n\n<pre><code>s = c.get_shape().as_list()\n</code></pre>\n\n<p>or</p>\n\n<pre><code>s = c.shape.as_list()\n</code></pre>\n\n<p>I've never really used <code>tf.shape()</code> myself, but when I use the above I receive the proper shape <code>(30000, 579)</code></p>\n"", 'IsAccepted': True, 'CreationDate': 1509032704}, {'QuestionId': 42082913, 'AnswerId': 42086540, 'URL': 'https://stackoverflow.com/questions/42082913/tf-reshape-doesnt-work-as-expected/42086540#42086540', 'QuestionTitle': 'tf.reshape doesn&#39;t work as expected', 'Answer': '<p>Look what happens when you use <code>feed_dict</code> - you reference the variables <code>source</code> and <code>target</code>. However, the python variable no longer refers to the placeholders but rather the reshape ops - hence the op is \'skipped\'. </p>\n\n<p>The easiest fix is renaming the placeholders to something unique. Further down in the network it is OK to reuse the same name (you could just call every layer <code>net</code>), it doesn\'t matter as long as you no longer need to reference them. </p>\n\n<p>Try giving this a go?</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport pickle\n\ndef weight_and_bias(name ,shape):\n    weight = tf.get_variable(""W"" + name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n    bias = tf.get_variable(""B"" + name, shape=shape[-1], initializer=tf.contrib.layers.xavier_initializer())\n    return weight, bias\n\ndef conv2d_2x2(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 5, 5, 1], padding=\'SAME\')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\nsess = tf.InteractiveSession()\n\nsource_placeholder = tf.placeholder(tf.float32, [None, None, 50, 50])\nsource_len = tf.placeholder(tf.int32, [None])\nsource_max_step = tf.shape(source)[1]\n\ntarget_placeholder = tf.placeholder(tf.float32, [None, None, 50, 50])\ntarget_len = tf.placeholder(tf.int32, [None])\ntarget_max_step = tf.shape(target)[1]\n\nW_conv, B_conv = weight_and_bias(\'conv1\', [5, 5, 1, 32])\n\nsource = tf.reshape(source_placeholder, [-1, 50, 50], ""source_reshape"")\nsource_tmp = tf.reshape(source, [-1, 50, 50 ,1])\nsource_conv = tf.nn.relu(conv2d_2x2(source_tmp, W_conv) + B_conv)\nsource_pool = max_pool_2x2(source_conv)\nsource_flat = tf.reshape(source_pool, [-1, 5 * 5 * 32], ""source_pool_reshape"")\nsource = tf.reshape(source_flat, [-1, source_max_step, 5*5*32], ""source_flat_reshape"")\n\nW_conv, B_conv = weight_and_bias(\'conv2\', [5, 5, 1, 32])\n\ntarget = tf.reshape(target_placeholder, [-1, 50, 50], ""target_reshape"")\ntarget_tmp = tf.reshape(target, [-1, 50, 50 ,1])\ntarget_conv = tf.nn.relu(conv2d_2x2(target_tmp, W_conv) + B_conv)\ntarget_pool = max_pool_2x2(target_conv)\ntarget_flat = tf.reshape(target_pool, [-1, 5 * 5 * 32], ""target_pool_reshape"")\ntarget = tf.reshape(target_flat, [-1, target_max_step, 5*5*32], ""target_flat_reshape"")\n\nsource_cell = tf.nn.rnn_cell.LSTMCell(500, initializer=tf.contrib.layers.xavier_initializer())\ntarget_cell = tf.nn.rnn_cell.LSTMCell(500, initializer=tf.contrib.layers.xavier_initializer())\nsource_rnn_output, _ = tf.nn.dynamic_rnn(source_cell, source, source_len, dtype=tf.float32, scope = ""source"")\ntarget_rnn_output, _ = tf.nn.dynamic_rnn(target_cell, target, target_len, dtype=tf.float32, scope = ""target"")\n\nsource_output = tf.transpose(source_rnn_output, [1, 0, 2])\ntarget_output = tf.transpose(target_rnn_output, [1, 0, 2])\n\nsource_final_output = tf.gather(source_output, -1)\ntarget_final_output = tf.gather(target_output, -1)\n\noutput = tf.concat(1, [source_final_output, target_final_output])\n\nW_sf, B_sf = weight_and_bias(\'sf\', [1000, 2])\npredict = tf.nn.softmax(tf.matmul(output, W_sf) + B_sf)\ny = tf.placeholder(tf.float32, [None, 2])\n\ncross_entropy = -tf.reduce_sum(y * tf.log(predict))\ntrain_step = tf.train.RMSPropOptimizer(1e-4).minimize(cross_entropy)\n\ncorrect_prediction = tf.equal(tf.arg_max(predict, 1), tf.arg_max(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nwith open(\'set\', \'rb\') as f:\n    _set = pickle.load(f)\n    training_set = _set[0]\n    training_len = _set[1]\n    training_label = _set[2]\n\nsess.run(tf.global_variables_initializer())\n\nfor i in range(20000):\n\n    if i % 100 == 0:\n        train_accuacy = accuracy.eval(feed_dict = {source_placeholder: training_set[0], target_placeholder: training_set[1], source_len: training_len[0], target_len: training_len[1], y: training_label})\n        print(""step %d, training accuracy %g""%(i, train_accuacy))\n    train_step.run(feed_dict = {source_placeholder: training_set[0], target_placeholder: training_set[1], source_len: training_len[0], target_len: training_len[1], y: training_label})\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1486460882}, {'QuestionId': 40441503, 'AnswerId': 40442606, 'URL': 'https://stackoverflow.com/questions/40441503/tensorflow-tensor-reshape/40442606#40442606', 'QuestionTitle': 'TensorFlow - Tensor Reshape', 'Answer': '<p>This is one quick way to create the first submatrix using TensorFlow API:</p>\n\n<pre><code>U2 = tf.constant([[1,3],[0,1],[1,0]],dtype=tf.float32)\nfirst_col = tf.unpack(U2, axis=1)[0]\nrepeated_cols = tf.tile(first_col, [2])\nrepeated_cols = tf.reshape(repeated_cols, [3,2])\n</code></pre>\n\n<p>Which would create</p>\n\n<pre><code>[[ 1.  1.]\n  [ 0.  0.]\n  [ 1.  1.]]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1478375918}, {'QuestionId': 38837519, 'AnswerId': 38840883, 'URL': 'https://stackoverflow.com/questions/38837519/visualising-np-reshape-for-tensorflow/38840883#38840883', 'QuestionTitle': 'Visualising np.reshape for TensorFlow', 'Answer': '<p>I think you are right that ""np.reshape achieving this"". </p>\n\n<p>-1 means the size of the first dimension will be calculated automatically as total_number_of_elements/16/16/2. </p>\n\n<p>The four dimensions are respectively: batch_size, height, weight, channels (number of feature maps). There is a batch size, because it uses mini-batch gradient descent.</p>\n', 'IsAccepted': False, 'CreationDate': 1470707644}, {'QuestionId': 38426827, 'AnswerId': 38426984, 'URL': 'https://stackoverflow.com/questions/38426827/transposing-dimensions-after-a-reshape-when-it-is-required/38426984#38426984', 'QuestionTitle': 'Transposing dimensions after a reshape: when it is required?', 'Answer': '<p>In memory arrays / tensors are really just 1D objects.  So say you need to store a 10x10 array.  In memory this would just be a 1D array of length 100 where the first 10 elements correspond to the first row.  This is known as row major ordering.  Say you want to reshape it to the shape 20x5.  The underlying memory is not changed by the reshape so the first ten elements now make up rows 1 and 2.  Transpose on the other hand physically reorders the memory to maintain the row major ordering while changing the location of the dimensions.</p>\n\n<p>Also I think you are tiling unessarily.  You should read up on broadcasting.  I think you could do the following.  </p>\n\n<pre><code>some_op = A * tf.reshape(B, [1, X_SIZE, T_SIZE])\n</code></pre>\n\n<p>In this case it will automatically broadcast B along the first dimension of A.</p>\n\n<p>Note.  I am not actually sure if tensorflow is using row major or column major ordering but the same concepts still apply.</p>\n', 'IsAccepted': True, 'CreationDate': 1468799152}]","{51706848, 72918129}","['<p>Tensor before and after <code>tf.reshape</code> have the <strong>same flatten order</strong>.</p>\n\n<p>In tensorflow runtime, a Tensor is consists of raw data(byte array), shape, and dtype, <code>tf.reshape</code> only change shape, with raw data and dtype not changed. <code>-1</code> or <code>None</code> in <code>tf.reshape</code> means that this value can be calculated.</p>\n\n<p>For example,</p>\n\n<pre><code># a tensor with 6 elements, with shape [3,2]\na = tf.constant([[1,2], [3,4], [5,6]]) \n# reshape tensor to [2, 3, 1], 2 is calculated by 6/3/1\nb = tf.reshape(a, [-1, 3, 1])\n</code></pre>\n\n<p>In this example, <code>a</code> and <code>b</code> have the same flatten order, namely <code>[1,2,3,4,5,6]</code>, <code>a</code> has the shape <code>[3,2]</code>, its value is <code>[[1,2], [3,4], [5,6]]</code>, <code>b</code> has the shape <code>[2,3,1]</code>, its value is <code>[[[1],[2],[3]],[[4],[5],[6]]]</code>.</p>\n', 'So <code>-1</code> corresponds to 2nd dimension in the input shape i.e. <code>8</code>. Hence, the target shape for the <code>Reshape</code> layer is <code>( -1 , 1 )</code> or <code>( 8 , 1 )</code>.</p>\n']","{'https://stackoverflow.com/questions/51706848/how-does-tf-reshape-work-internally/51721390#51721390', 'https://stackoverflow.com/questions/72918129/is-it-possible-to-make-the-output-shape-the-same-as-the-input-shape/72918204#72918204'}",,0.2090316667300497,0.10062901480703867
11,63146831,tf.custom_gradient,Documentation Replication on Other Examples,What is the analytic interpretation for Tensorflow custom gradient?,"<p>In the official <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">tf.custom_gradient</a> documentation it shows how to define custom gradients for <code>log(1 + exp(x))</code></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.math.log(1 + e), grad
</code></pre>
<p>When <code>y = log(1 + exp(x))</code>, analytically the derivative comes out to be <code>dy/dx = (1 - 1 / (1 + exp(x)))</code>.</p>
<p>However in the code <code>def grad</code> says its <code>dy * (1 - 1 / (1 + exp(x)))</code>.
<code>dy/dx = dy * (1 - 1 / (1 + exp(x)))</code> is not a valid equation. While <code>dx = dy * (1 - 1 / (1 + exp(x)))</code> is wrong as it should be the reciprocal.</p>
<p>What does the <code>grad</code> function equate to?</p>
","<p>I finally figured it out. The <code>dy</code> should be called <code>upstream_gradient</code> or <code>upstream_dy_dx</code>.</p>
<p>By chain rule we know that</p>
<p><a href=""https://i.stack.imgur.com/7g3aZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7g3aZ.png"" alt=""chain rule"" /></a></p>
<p>where <code>dx[i]/dx[i+1]</code> is the gradient of the current function.</p>
<p>So <code>dy</code> is the product of all the gradients upstream before this function.</p>
<p><a href=""https://i.stack.imgur.com/nu4Z8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nu4Z8.png"" alt=""enter image description here"" /></a></p>
<p>So, if you forget to multiply the <code>dy</code> it is effectively the same as <a href=""https://www.tensorflow.org/api_docs/python/tf/stop_gradient"" rel=""nofollow noreferrer"">tf.stop_gradient</a></p>
<p>Here is a code which demos this. Full notebook <a href=""https://github.com/Ghost---Shadow/differentiable-programming-handbook/blob/master/notebooks/custom-gradient.ipynb"" rel=""nofollow noreferrer"">here</a></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def foo(x):
    tf.debugging.assert_rank(x, 0)

    def grad(dy_dx_upstream):
        dy_dx = 2 * x
        dy_dx_downstream = dy_dx * dy_dx_upstream
        tf.print(f'x={x}\tupstream={dy_dx_upstream}\tcurrent={dy_dx}\t\tdownstream={dy_dx_downstream}')
        return dy_dx_downstream
    
    y = x ** 2
    tf.print(f'x={x}\ty={y}')
    
    return y, grad


x = tf.constant(2.0, dtype=tf.float32)

with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    y = foo(foo(foo(x))) # y = x ** 8

tf.print(f'\nfinal dy/dx={tape.gradient(y, x)}')
</code></pre>
<p>Output</p>
<pre><code>x=2.0   y=4.0
x=4.0   y=16.0
x=16.0  y=256.0
x=16.0  upstream=1.0    current=32.0        downstream=32.0
x=4.0   upstream=32.0   current=8.0     downstream=256.0
x=2.0   upstream=256.0  current=4.0     downstream=1024.0

final dy/dx=1024.0
</code></pre>
","{44982081, 66139974, 46606633, 50030026, 51685934, 60516977, 43934225, 41535347, 50203668, 57392510, 46876063}","[{'QuestionId': 66139974, 'AnswerId': 66140586, 'URL': 'https://stackoverflow.com/questions/66139974/custom-gradients-in-tensor-flow-unable-to-understand-this-example/66140586#66140586', 'QuestionTitle': 'Custom Gradients in Tensor Flow - Unable to understand this example', 'Answer': '<p>When writing a custom gradient, you must define the whole derivative calculation by yourself. Without your custom gradient, we have the following derivative:</p>\n<pre><code>((x**2)**2)dx = (x**4)dx = 4*(x**3) = 32 when x=2\n</code></pre>\n<p>When you override your gradient calculation, you only have</p>\n<pre><code>(x**2)dx = 2x = 4 when x=2\n</code></pre>\n<p>You need to calculate the derivative in your function, i.e:</p>\n<pre><code>@tf.custom_gradient\ndef clip_gradients2(y):\n    def backward(dy):\n        dy = dy * (2*y)\n        return tf.clip_by_norm(dy, 20000000000000000000000000)\n    return y**2, backward\n</code></pre>\n<p>To get the desired behavior.</p>\n', 'IsAccepted': False, 'CreationDate': 1612973422}, {'QuestionId': 46606633, 'AnswerId': 61248011, 'URL': 'https://stackoverflow.com/questions/46606633/tensorflow-gradients-for-a-custom-loss-function/61248011#61248011', 'QuestionTitle': 'tensorflow: gradients for a custom loss function', 'Answer': '<p>I believe the psuedo code would look something like this:</p>\n\n<pre><code>@tf.custom_gradient\ndef loss_function(y_true, y_pred, peak_value=3, weight=2)\n    ## your code\n    def grad(dy):\n        return dy * partial_derivative\n    return loss, grad\n</code></pre>\n\n<p>Where <code>partial_derivative</code> is the analytically evaluated partial derivative with respect to your loss function. If your loss function is a function of more than one variable, it will require a partial derivative respect to each variable, I believe.</p>\n\n<p>If you need more information, the documentation is good: <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/custom_gradient</a></p>\n\n<p>And I\'ve yet to find an example of this functionality embedded in a model that\'s not a toy.</p>\n', 'IsAccepted': False, 'CreationDate': 1587032894}, {'QuestionId': 60516977, 'AnswerId': 60518214, 'URL': 'https://stackoverflow.com/questions/60516977/difficulties-in-understanding-higher-order-derivatives-for-tf-custom-gradient/60518214#60518214', 'QuestionTitle': 'Difficulties in understanding higher order derivatives for tf.custom_gradient()', 'Answer': '<ol>\n<li><blockquote>\n  <p>There is a lack of details on why second_order_and_transpose(ddy) returns two objects.</p>\n</blockquote></li>\n</ol>\n\n<p>Based on what I played with some examples, I believe you are correct. The official doc is somehow ambiguous (or incorrect). The <code>second_order_and_transpose(ddy)</code> should only return the one object, which is the calculated second-order gradient.</p>\n\n<ol start=""2"">\n<li><blockquote>\n  <p>It is also not even clear why did they name it unused_x.</p>\n</blockquote></li>\n</ol>\n\n<p>That is the tricky part. The <code>unused_x</code> explains why they name it (because you never going to use it...). The goal here is to wrap your second-order calculation function in a function called <code>first_order_custom</code>. You calculate the gradient of x from <code>fused_op</code>, and use that as a return value, instead of <code>unused_x</code>.</p>\n\n<p>To make this more clear, I passed an example extended from the official document to define a second-order gradient of the <code>log1pexp</code>:</p>\n\n<p><strong>NOTE:</strong> The second-order gradient is not numerically stable, so let\'s use (1 - tf.exp(x)) to represent it, just to make our life easier.</p>\n\n<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient\ndef log1pexp2(x):\n    e = tf.exp(x)\n    y = tf.math.log(1 + e)\n    x_grad = 1 - 1 / (1 + e)\n    def first_order_gradient(dy):\n        @tf.custom_gradient\n        def first_order_custom(unused_x):\n            def second_order_gradient(ddy):\n                # Let\'s define the second-order graidne to be (1 - e)\n                return ddy * (1 - e) \n            return x_grad, second_order_gradient\n        return dy * first_order_custom(x)\n    return y, first_order_gradient\n\n</code></pre>\n\n<p>To test the script, simply run:</p>\n\n<pre><code>import tensorflow as tf\n\n@tf.custom_gradient\ndef log1pexp2(x):\n    e = tf.exp(x)\n    y = tf.math.log(1 + e)\n    x_grad = 1 - 1 / (1 + e)\n    def first_order_gradient(dy):\n        @tf.custom_gradient\n        def first_order_custom(unused_x):\n            def second_order_gradient(ddy):\n                # Let\'s define the second-order graidne to be (1 - e)\n                return ddy * (1 - e) \n            return x_grad, second_order_gradient\n        return dy * first_order_custom(x)\n    return y, first_order_gradient\n\nx1 = tf.constant(1.)\ny1 = log1pexp2(x1)\ndy1 = tf.gradients(y1, x1)\nddy1 = tf.gradients(dy1, x1)\n\nx2 = tf.constant(100.)\ny2 = log1pexp2(x2)\ndy2 = tf.gradients(y2, x2)\nddy2 = tf.gradients(dy2, x2)\n\nwith tf.Session() as sess:\n    print(\'x=1, dy1:\', dy1[0].eval(session=sess))\n    print(\'x=1, ddy1:\', ddy1[0].eval(session=sess))\n    print(\'x=100, dy2:\', dy2[0].eval(session=sess))\n    print(\'x=100, ddy2:\', ddy2[0].eval(session=sess))\n\n</code></pre>\n\n<p>Result:</p>\n\n<pre><code>x=1, dy1: 0.7310586\nx=1, ddy1: -1.7182817\nx=100, dy2: 1.0\nx=100, ddy2: -inf\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1583288477}, {'QuestionId': 57392510, 'AnswerId': 57392590, 'URL': 'https://stackoverflow.com/questions/57392510/tensorflow-simple-example-help-custom-gradient/57392590#57392590', 'QuestionTitle': 'TensorFlow simple example help - custom gradient', 'Answer': '<p><code>apply_gradients</code> returns an operation that you can use to apply the gradients. In other words, you just do <code>train = optimizer.apply_gradients(grads_and_vars)</code> and the rest will work as in the first snippet. I,e.:</p>\n\n<pre><code>optimizer = tf.train.GradientDescentOptimizer(0.55)\ngrads_and_vars = calc_grad(x,y)\ntrain = optimizer.apply_gradients(grads_and_vars)\n\ninit = tf.global_variables_initializer()\n\ndef optimize():\n  with tf.Session() as session:\n    session.run(init)\n    print(""starting at"", ""x:"", session.run(x), ""y:"", session.run(y), ""z:"", session.run(z))\n    for step in range(10):  \n      session.run(train)\n      print(""step"", step, ""x:"", session.run(x), ""y:"", session.run(y), ""z:"", session.run(z))\n\n\noptimize()\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1565174681}, {'QuestionId': 51685934, 'AnswerId': 51686034, 'URL': 'https://stackoverflow.com/questions/51685934/calculating-tensorflow-gradients/51686034#51686034', 'QuestionTitle': 'Calculating tensorflow gradients', 'Answer': '<p>For comparison, consider the real-valued function <em>f</em> : <strong>R</strong> → <strong>R</strong> of one real variable, given by <em>f</em>(<em>x</em>) = 10 <em>x</em>. Here, <em>f</em>\'(<em>x</em>) = 10, regardless of the value of <em>x</em>, so in particular <em>f</em>\'(0) = 10.</p>\n\n<p>Similarly, as explained in the tutorial, more or less by definition, the <a href=""https://en.wikipedia.org/wiki/Total_derivative"" rel=""nofollow noreferrer"">total derivative</a> of (<em>a</em>, <em>b</em>) ↦ <em>a</em> + <em>b</em> for <em>b</em>(<em>a</em>) = 2 <em>a</em> is (3, 1), which is independent of <em>a</em>.</p>\n\n<p>For a less trivial example, let us consider</p>\n\n<pre><code>a = tf.constant(5.)\nb = 2 * a\ng = tf.gradients(a**3 + 2*b**2, [a, b])\n\nwith tf.Session() as sess:\n    print(sess.run(g))\n</code></pre>\n\n<p>Here, the total derivative with respect to <em>a</em> is the derivative of <em>a</em> ↦ <em>a</em>³ + 2(2 <em>a</em>)² = <em>a</em>³ + 8 <em>a</em>² which becomes <em>a</em> ↦ 3 <em>a</em>² + 16 <em>a</em>, while the derivative with respect to <em>b</em> is <em>a</em> ↦  4 <em>b</em>(<em>a</em>) = 8 <em>a</em>. Thus, at <em>a</em> = 5, we expect the result to be (3 · 5² + 16 · 5, 8 · 5) = (155, 40), and running the code that\'s exactly what you get.</p>\n', 'IsAccepted': True, 'CreationDate': 1533388943}, {'QuestionId': 51685934, 'AnswerId': 51686110, 'URL': 'https://stackoverflow.com/questions/51685934/calculating-tensorflow-gradients/51686110#51686110', 'QuestionTitle': 'Calculating tensorflow gradients', 'Answer': '<p>The document describes that the notation you wrote computes the total derivative. You have 2 variables to compute the derivatives: <code>a</code> and <code>b</code>. So for the derivative for <code>a</code>, the derivative of function <code>a+b = a + 2a = 3a</code> w.r.t <code>a</code> which is 3. For <code>b</code>, the derivative of <code>a+b</code> w.r.t <code>b</code> which is 1.  <code>tf.constant(0.)</code> means you declared <code>a</code> as a constant value. What you may be thinking will happen if you declared <code>a</code> as a <code>tf.variable</code> instead.</p>\n', 'IsAccepted': False, 'CreationDate': 1533389521}, {'QuestionId': 50203668, 'AnswerId': 50209761, 'URL': 'https://stackoverflow.com/questions/50203668/using-tf-custom-gradient-in-tensorflow-r1-8/50209761#50209761', 'QuestionTitle': 'Using tf.custom_gradient in tensorflow r1.8', 'Answer': '<p>If you just want to test the code in the documentation, here is the way.</p>\n\n<p>The following code will give the instable <code>[nan]</code> result:</p>\n\n<pre><code>import tensorflow as tf\n\ndef log1pexp(x):\n    return tf.log(1 + tf.exp(x))\n\nx = tf.constant(100.)\ny = log1pexp(x)\ndy = tf.gradients(y, x)\n\nwith tf.Session() as sess:\n    print(sess.run(dy))\n</code></pre>\n\n<p>And the following code will give the correct result <code>[1.0]</code>:</p>\n\n<pre><code>import tensorflow as tf\n\n@tf.custom_gradient\ndef log1pexp(x):\n    e = tf.exp(x)\n    def grad(dy):\n        return dy * (1 - 1 / (1 + e))\n    return tf.log(1 + e), grad\n\nx = tf.constant(100.)\ny = log1pexp(x)\ndy = tf.gradients(y, x)\n\nwith tf.Session() as sess:\n    print(sess.run(dy))\n</code></pre>\n\n<h1>Details:</h1>\n\n<p>The main problem here is that you are trying to decorate <code>log1pexp</code> twice in your code: once with <code>@tf.custom_gradient</code> and once with <code>f = tf.custom_gradient(log1pexp)</code>. In <a href=""https://docs.python.org/3/glossary.html#term-decorator"" rel=""nofollow noreferrer"">python</a>, <code>@tf.custom_gradient</code> here is equivalent to <code>log1pexp = tf.custom_gradient(log1pexp)</code>. You should do this only once, especially here for the following reason.</p>\n\n<p><code>tf.custom_gradient</code> needs to call the function being pass to it to get both the function output and the gradient, i.e. expecting two returns. During decoration, everything works as expected because <code>log1pexp</code> returns <code>tf.log(1 + e)</code> and <code>grad</code>. After decorating <code>log1pexp</code>, <code>log1pexp</code> (returned by <code>tf.custom_gradient</code>) becomes a new function which returns only one tensor <code>tf.log(1 + e)</code>. When you do <code>f = tf.custom_gradient(log1pexp)</code> after decorating <code>log1pexp</code>, <code>tf.custom_gradient</code> can only get one return which is the single tensor <code>tf.log(1 + e)</code>. It will try to split this tensor into two by iterating this returned tensor. But it is wrong and is not allowed as the error message stated:</p>\n\n<blockquote>\n  <p>Tensor objects are not iterable when eager execution is not enabled.</p>\n</blockquote>\n\n<p>You should not decorate <code>log1pexp</code> twice anyway. But this is why you got this error. One more thing to mention, your code will trigger another error for the same reason even if you removed <code>@tf.custom_gradient</code>. After removing <code>@tf.custom_gradient</code>, the line <code>f = tf.custom_gradient(log1pexp)</code> should work as expected. But <code>f</code> is a function returning only one tensor. <code>y, dy = f(x)</code> is wrong and will not work.</p>\n', 'IsAccepted': True, 'CreationDate': 1525680770}, {'QuestionId': 50030026, 'AnswerId': 50031716, 'URL': 'https://stackoverflow.com/questions/50030026/how-to-provide-custom-gradient-in-tensorflow/50031716#50031716', 'QuestionTitle': 'How to provide custom gradient in TensorFlow', 'Answer': '<p>Since your function <code>f2()</code> has two inputs, you have to provide a gradient to flow back to each of them. The error you see:</p>\n\n<blockquote>\n  <p>Num gradients 2 generated for op name: ""IdentityN"" [...]  do not match num inputs 3</p>\n</blockquote>\n\n<p>is admittedly quite cryptic, though. Supposing you never want to calculate d<strong><em>y</em></strong>/d<strong><em>A</em></strong>, you can just return None, dzByDx. The code below (tested):</p>\n\n<pre><code>import tensorflow as tf\n\n#I want to write custom gradient for this function f1\ndef f1(A,x):\n    y=tf.matmul(A,x,name=\'y\')\n    return y\n\n#for y= Ax, the derivative is: dy/dx= transpose(A)\n@tf.custom_gradient\ndef f2(A,x):\n    y=f1(A,x)\n    def grad(dzByDy): # dz/dy = 2y reaches here correctly.\n        dzByDx=tf.matmul(A,dzByDy,transpose_a=True) \n        return None, dzByDx\n    return y,grad\n\nx= tf.constant([[1.],[0.]],name=\'x\')\nA= tf.constant([ [1., 2.], [3., 4.]],name=\'A\')\n\n#y=f1(A,x) # This works as desired\ny=f2(A,x) #This line gives Error\n\nz=tf.reduce_sum(y*y,name=\'z\')\n\ng=tf.gradients(ys=z,xs=x)\n\nwith tf.Session() as sess:\n    print sess.run( g )\n</code></pre>\n\n<p>outputs:</p>\n\n<blockquote>\n  <p>[array([[20.],\n         [28.]], dtype=float32)]</p>\n</blockquote>\n\n<p>as desired.</p>\n', 'IsAccepted': True, 'CreationDate': 1524692303}, {'QuestionId': 46876063, 'AnswerId': 46880761, 'URL': 'https://stackoverflow.com/questions/46876063/tensorflow-custom-gradients/46880761#46880761', 'QuestionTitle': 'TensorFlow custom gradients', 'Answer': '<p>In short, the correct version of _custom_square_grad should be:</p>\n\n<pre><code>@tf.RegisterGradient(""CustomSquare"")                                             \ndef _custom_square_grad(op, grad):                                               \n    x = op.inputs[0]                                                            \n    return 2.0 * (grad * 2.0 * x)\n</code></pre>\n\n<p>In order to understand the code, you need to know how <code>gradient</code> works. When you define <code>tf.RegisterGradient</code>, it is supposed to BACK-PROPAGATE the gradients from outputs to inputs. For <code>tf.squre</code>, the default gradient function is like this:</p>\n\n<pre><code># Given y = tf.square(x) =&gt; y\' = 2x\ngrad_x = grad_y * 2.0 * x\n</code></pre>\n\n<p>Since you want to double the gradient in your customized gradient   function, you can simply change it to <code>grad_x = 2.0 * (grad_y * 2.0 * x)</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1508721735}, {'QuestionId': 44982081, 'AnswerId': 44986053, 'URL': 'https://stackoverflow.com/questions/44982081/what-does-compute-gradients-return-in-tensorflow/44986053#44986053', 'QuestionTitle': 'What does compute_gradients return in tensorflow', 'Answer': '<p>compute_gradients(a,b) returns d[ sum a ]/db. So in your case this returns d mean_sq / d theta, where theta is set of all variables. There is no ""dx"" in this equation, you are not computing gradients wrt. inputs. So what happens with batch dimension? You remove it yourself in the definition of mean_sq:</p>\n\n<pre><code>mean_sqr = tf.reduce_mean(tf.pow(y_ - y, 2))\n</code></pre>\n\n<p>thus (I am assuming y is 1D for simplicity)</p>\n\n<pre><code>d[ mean_sqr ] / d theta = d[ 1/M SUM_i=1^M (pred(x_i), y_i)^2 ] / d theta\n                        = 1/M SUM_i=1^M d[ (pred(x_i), y_i)^2 ] / d theta\n</code></pre>\n\n<p>so you are in control of whether it sums over batch, takes the mean or does something different, if you would define mean_sqr to use reduce_sum instead of a reduce_mean, gradients would be the sum over the batch and so on.</p>\n\n<p>On the other hand apply_gradients simply ""applies the gradients"", the exact rule for application is optimiser dependent, for GradientDescentOptimizer it would be</p>\n\n<pre><code>theta &lt;- theta - learning_rate * gradients(theta)\n</code></pre>\n\n<p>For Adam that you are using the equation is more complex of course.</p>\n\n<p><strong>Note</strong> however that tf.gradients is more like ""backprop"" than true gradient in mathematical sense - meaning that it depends on the graph dependencies and does not recognise dependences which are in ""opposite"" direction.</p>\n', 'IsAccepted': True, 'CreationDate': 1499515548}, {'QuestionId': 43934225, 'AnswerId': 43934761, 'URL': 'https://stackoverflow.com/questions/43934225/how-do-tf-gradients-work/43934761#43934761', 'QuestionTitle': 'How do tf.gradients() work?', 'Answer': '<p>TensorFlow uses <a href=""https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation"" rel=""noreferrer"">reverse accumulation</a> which is based on the chain rule, to compute the gradient value at point. In order to compute gradient of function with respect to a variable you have to define both. Also you have to specify value at which you want to compute the gradient. In this example you compute gradient of <code>y=x**2+x+1</code> with respect to <code>x</code> at <code>2</code>:</p>\n\n<pre><code>#!/usr/bin/env python3\nimport tensorflow as tf\n\nx = tf.Variable(2.0)\ny = x**2 + x - 1\n\ngrad = tf.gradients(y, x)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    grad_value = sess.run(grad)\n    print(grad_value)\n\n# output: [5.0]\n</code></pre>\n\n<p>It is also possible to compute a gradient in case your variable is a matrix. In such case the gradient will be also a matrix. Here we use a simple case when the function depends on the sum of all matrix elements:</p>\n\n<pre><code>#!/usr/bin/env python3\nimport tensorflow as tf\n\nX = tf.Variable(tf.random_normal([3, 3]))\nX_sum = tf.reduce_sum(X)\ny = X_sum**2 + X_sum - 1\n\ngrad = tf.gradients(y, X)\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    grad_value = sess.run(grad)\n    print(grad_value)\n\n# output: [array([[ 9.6220665,  9.6220665,  9.6220665],\n#   [ 9.6220665,  9.6220665,  9.6220665],\n#   [ 9.6220665,  9.6220665,  9.6220665]], dtype=float32)]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1494582950}, {'QuestionId': 41535347, 'AnswerId': 41535880, 'URL': 'https://stackoverflow.com/questions/41535347/how-gradient-passed-by-tf-py-func/41535880#41535880', 'QuestionTitle': 'How Gradient passed by tf.py_func', 'Answer': '<p>Gradient of <code>py_func</code> is <code>None</code> (just check <code>ops.get_gradient_function(y2.op)</code>). There\'s this <a href=""https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342"" rel=""noreferrer"">gist</a> by @harpone which shows how to use gradient override map for py_func.</p>\n\n<p>Here\'s your example modified to use that recipe</p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\n\ndef addone(x):\n    # print(type(x)\n    return x + 1\n\ndef addone_grad(op, grad):\n    x = op.inputs[0]\n    return x\n\nfrom tensorflow.python.framework import ops\nimport numpy as np\n\n# Define custom py_func which takes also a grad op as argument:\ndef py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n\n    # Need to generate a unique name to avoid duplicates:\n    rnd_name = \'PyFuncGrad\' + str(np.random.randint(0, 1E+8))\n\n    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n    g = tf.get_default_graph()\n    with g.gradient_override_map({""PyFunc"": rnd_name}):\n        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n\ndef pyfunc_test():\n\n    # create data\n    x_data = tf.placeholder(dtype=tf.float32, shape=[None])\n    y_data = tf.placeholder(dtype=tf.float32, shape=[None])\n\n    w = tf.Variable(tf.constant([0.5]))\n    b = tf.Variable(tf.zeros([1]))\n\n    y1 = tf.mul(w, x_data, name=\'y1\')\n    y2 = py_func(addone, [y1], [tf.float32], grad=addone_grad)[0]\n    y = tf.add(y2, b)\n\n    loss = tf.reduce_mean(tf.square(y - y_data))\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    train = optimizer.minimize(loss)\n\n    print(""Pyfunc grad"", ops.get_gradient_function(y2.op))\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        for step in range(10):\n            #            ran = np.random.rand(115).astype(np.float32)\n            ran = np.ones((115)).astype(np.float32)\n            ans = ran * 1.5 + 3\n            dic = {x_data: ran, y_data: ans}\n            tt, yy, yy1= sess.run([train, y1, y2], feed_dict=dic)\n            if step % 1 == 0:\n                print(\'step {}\'.format(step))\n                print(\'{}, {}\'.format(w.eval(), b.eval()))\n\n        test = sess.run(y, feed_dict={x_data:[1]})\n        print(\'test = {}\'.format(test))\n\n\nif __name__ == \'__main__\':\n    pyfunc_test()\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1483898187}]",{46876063},"['<p>In short, the correct version of _custom_square_grad should be:</p>\n\n<pre><code>@tf.RegisterGradient(""CustomSquare"")                                             \ndef _custom_square_grad(op, grad):                                               \n    x = op.inputs[0]                                                            \n    return 2.0 * (grad * 2.0 * x)\n</code></pre>\n\n<p>In order to understand the code, you need to know how <code>gradient</code> works. When you define <code>tf.RegisterGradient</code>, it is supposed to BACK-PROPAGATE the gradients from outputs to inputs.', '<p>In short, the correct version of _custom_square_grad should be:</p>\n\n<pre><code>@tf.RegisterGradient(""CustomSquare"")                                             \ndef _custom_square_grad(op, grad):                                               \n    x = op.inputs[0]                                                            \n    return 2.0 * (grad * 2.0 * x)\n</code></pre>\n\n<p>In order to understand the code, you need to know how <code>gradient</code> works. When you define <code>tf.RegisterGradient</code>, it is supposed to BACK-PROPAGATE the gradients from outputs to inputs. For <code>tf.squre</code>, the default gradient function is like this:</p>\n\n<pre><code># Given y = tf.square(x) =&gt; y\' = 2x\ngrad_x = grad_y * 2.0 * x\n</code></pre>\n\n<p>Since you want to double the gradient in your customized gradient   function, you can simply change it to <code>grad_x = 2.0 * (grad_y * 2.0 * x)</code>.</p>\n']",{'https://stackoverflow.com/questions/46876063/tensorflow-custom-gradients/46880761#46880761'},{46876063},0.18138833055391146,0.20395094178563383
11,71335830,tf.keras.layers.Flatten,Documentation Ambiguity,What is the difference between tf.keras.layers.Input() and tf.keras.layers.Flatten(),"<p>I have seen multiple uses of both <code>tf.keras.layers.Flatten()</code> (ex. <a href=""https://www.tensorflow.org/tutorials/generative/autoencoder#first_example_basic_autoencoder"" rel=""nofollow noreferrer"">here</a>) and <code>tf.keras.layers.Input()</code> (ex. <a href=""https://www.tensorflow.org/tutorials/generative/autoencoder#define_a_convolutional_autoencoder"" rel=""nofollow noreferrer"">here</a>). After reading the documentation, it is not clear to me</p>
<ol>
<li>whether either of them uses the other</li>
<li>whether both can be used interchangeably when introducing to a model an input layer (let's say with dimensions <code>(64, 64)</code>)</li>
</ol>
","<p>I think the confusion comes from using a <code>tf.keras.Sequential</code> model, which does not need an explicit <code>Input</code> layer. Consider the following two models, which are equivalent:</p>
<pre><code>import tensorflow as tf

model1 = tf.keras.Sequential([
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(5, activation='relu'),
    ])

model1.build((1, 28, 28, 1))
</code></pre>
<pre><code>model2 = tf.keras.Sequential([
      tf.keras.layers.Input((28, 28, 1)),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(5, activation='relu'),
    ])
</code></pre>
<p>The difference is that I explicitly set the input shape of <code>model2</code> using an <code>Input</code> layer. In <code>model1</code>, the input shape will be inferred when you pass real data to it or call <code>model.build</code>.</p>
<p>Now regarding the <code>Flatten</code> layer, this layer simply converts a n-dimensional tensor (for example <code>(28, 28, 1)</code>) into a 1D tensor <code>(28 x 28 x 1)</code>. The <code>Flatten</code> layer and <code>Input</code> layer can coexist in a <code>Sequential</code> model but do not depend on each other.</p>
","{66789376, 55109696, 51965094, 44499755, 56278769, 52897874, 46920307, 44176982, 50259290, 49406654}","[{'QuestionId': 66789376, 'AnswerId': 67052621, 'URL': 'https://stackoverflow.com/questions/66789376/difference-between-keras-layers-inputlayer-and-keras-input/67052621#67052621', 'QuestionTitle': 'Difference between keras.layers.InputLayer and keras.Input', 'Answer': '<p>For the benefit of community providing solution here</p>\n<p>It is generally recommend to use the functional layer API via <code>Input</code>, (which creates an <code>InputLayer</code>) without directly using <code>InputLayer</code>. When using <code>InputLayer</code> with Keras Sequential model, it can be skipped by moving the <code>input_shape</code> parameter to the first layer after the <code>InputLayer</code>. For more details you can refer <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer"" rel=""nofollow noreferrer"">this</a> and <a href=""https://stackoverflow.com/questions/46147019/keras-difference-of-inputlayer-and-input"">this</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1618201181}, {'QuestionId': 44499755, 'AnswerId': 64801570, 'URL': 'https://stackoverflow.com/questions/44499755/keras-what-is-the-difference-between-layers-input-and-layers-inputlayer/64801570#64801570', 'QuestionTitle': 'Keras: What is the difference between layers.Input and layers.InputLayer?', 'Answer': ""<p><code>InputLayer</code> is a callable, just like other keras layers, while <code>Input</code> is not callable, it is simply a Tensor object.</p>\n<p>You can use <code>InputLayer</code> when you need to connect it like layers to the following layers:</p>\n<pre><code>inp = keras.layers.InputLayer(input_shape=(32,))(prev_layer)\n</code></pre>\n<p>and following is the usage of <code>Input</code> layer:</p>\n<pre><code>x = Input(shape=(32,))\ny = Dense(16, activation='softmax')(x)\nmodel = Model(x, y)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1605174585}, {'QuestionId': 49406654, 'AnswerId': 49412445, 'URL': 'https://stackoverflow.com/questions/49406654/tf-reshape-vs-tf-contrib-layers-flatten/49412445#49412445', 'QuestionTitle': 'tf.reshape vs tf.contrib.layers.flatten', 'Answer': '<p>All 3 options reshape identically:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\np3 = tf.placeholder(tf.float32, [None, 1, 2, 4])\n\np3_shape = p3.get_shape().as_list()\n\np_a = tf.contrib.layers.flatten(p3)                                  # &lt;-----Option A\n\np_b = tf.reshape(p3, [-1, p3_shape[1] * p3_shape[2] * p3_shape[3]])  # &lt;---- Option B\n\np_c = tf.reshape(p3, [tf.shape(p3)[0], -1])                          # &lt;---- Option C\n\nprint(p_a.get_shape())\nprint(p_b.get_shape())\nprint(p_c.get_shape())\n\nwith tf.Session() as sess:\n    i_p3 = np.arange(16, dtype=np.float32).reshape([2, 1, 2, 4])\n    print(""a"", sess.run(p_a, feed_dict={p3: i_p3}))\n    print(""b"", sess.run(p_b, feed_dict={p3: i_p3}))\n    print(""c"", sess.run(p_c, feed_dict={p3: i_p3}))\n</code></pre>\n\n<p>This code yields the same result 3 times. Your different results are caused by something else and not by the reshaping.</p>\n', 'IsAccepted': True, 'CreationDate': 1521651695}, {'QuestionId': 56278769, 'AnswerId': 56278851, 'URL': 'https://stackoverflow.com/questions/56278769/what-is-difference-between-flatten-and-dense-layers-in-convolutional-neural/56278851#56278851', 'QuestionTitle': 'What is difference between Flatten() and Dense() layers in Convolutional Neural Network?', 'Answer': '<p>Flatten as the name implies, converts your multidimensional matrices (Batch.Size x Img.W x Img.H x Kernel.Size) to a nice single 2-dimensional matrix: (Batch.Size x (Img.W x Img.H x Kernel.Size)). During backpropagation it also converts back your delta of size (Batch.Size x (Img.W x Img.H x Kernel.Size)) to the original (Batch.Size x Img.W x Img.H x Kernel.Size).</p>\n\n<p>Dense layer is of course the standard fully connected layer.</p>\n', 'IsAccepted': False, 'CreationDate': 1558626409}, {'QuestionId': 55109696, 'AnswerId': 56086070, 'URL': 'https://stackoverflow.com/questions/55109696/tensorflow-difference-between-tf-keras-layers-layer-vs-tf-keras-model/56086070#56086070', 'QuestionTitle': 'TensorFlow - Difference between tf.keras.layers.Layer vs tf.keras.Model', 'Answer': '<p>In the documentation:</p>\n\n<blockquote>\n  <p>The Model class has the same API as Layer, with the following\n  differences: - It exposes built-in training, evaluation, and\n  prediction loops (model.fit(), model.evaluate(), model.predict()). -\n  It exposes the list of its inner layers, via the model.layers\n  property. - It exposes saving and serialization APIs.</p>\n  \n  <p>Effectively, the ""Layer"" class corresponds to what we refer to in the\n  literature as a ""layer"" (as in ""convolution layer"" or ""recurrent\n  layer"") or as a ""block"" (as in ""ResNet block"" or ""Inception block"").</p>\n  \n  <p>Meanwhile, the ""Model"" class corresponds to what is referred to in the\n  literature as a ""model"" (as in ""deep learning model"") or as a\n  ""network"" (as in ""deep neural network"").</p>\n</blockquote>\n\n<p>So if you want to be able to call <code>.fit()</code>, <code>.evaluate()</code>, or <code>.predict()</code> on those blocks or you want to be able to save and load those blocks separately or something you should use the Model class. The Layer class is leaner so you won\'t bloat the layers with unnecessary functionality...but I would guess that that generally wouldn\'t be a big problem.  </p>\n', 'IsAccepted': True, 'CreationDate': 1557532025}, {'QuestionId': 44176982, 'AnswerId': 54190741, 'URL': 'https://stackoverflow.com/questions/44176982/how-does-the-flatten-layer-work-in-keras/54190741#54190741', 'QuestionTitle': 'How does the Flatten layer work in Keras?', 'Answer': '<p>Flattening a tensor means to remove all of the dimensions except for one.</p>\n\n<p>A Flatten layer in Keras reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. </p>\n\n<p>This is the same thing as making a 1d-array of elements. </p>\n\n<p>For example in the VGG16 model you may find it easy to understand:</p>\n\n<pre><code>&gt;&gt;&gt; model.summary()\nLayer (type)                     Output Shape          Param #\n================================================================\nvgg16 (Model)                    (None, 4, 4, 512)     14714688\n________________________________________________________________\nflatten_1 (Flatten)              (None, 8192)          0\n________________________________________________________________\ndense_1 (Dense)                  (None, 256)           2097408\n________________________________________________________________\ndense_2 (Dense)                  (None, 1)             257\n===============================================================\n</code></pre>\n\n<p>Note how flatten_1 layer shape is (None, 8192), where 8192 is actually 4*4*512.</p>\n\n<hr>\n\n<p>PS, None means <em>any</em> dimension (or dynamic dimension), but you can typically read it as 1. You can find more details in <a href=""https://stackoverflow.com/questions/43237124/role-of-flatten-in-keras/54190658#54190658"">here</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1547508827}, {'QuestionId': 55109696, 'AnswerId': 55109850, 'URL': 'https://stackoverflow.com/questions/55109696/tensorflow-difference-between-tf-keras-layers-layer-vs-tf-keras-model/55109850#55109850', 'QuestionTitle': 'TensorFlow - Difference between tf.keras.layers.Layer vs tf.keras.Model', 'Answer': '<ul>\n<li>A layer takes in a tensor and give out a tensor which is a result of\nsome tensor operations</li>\n<li>A model is a composition of multiple layers. </li>\n</ul>\n\n<p>If you are building a new model architecture using existing keras/tf layers then build a custom model. </p>\n\n<p>If you are implementing your own custom tensor operations with in a layer, then build a custom layer. If you are using non tensor operation inside your custom layer, then you have to code how the layer will forward propagate and backward propagate. </p>\n', 'IsAccepted': False, 'CreationDate': 1552335950}, {'QuestionId': 52897874, 'AnswerId': 52902412, 'URL': 'https://stackoverflow.com/questions/52897874/difference-between-tensorflow-flattening-methods/52902412#52902412', 'QuestionTitle': 'Difference between tensorflow flattening methods', 'Answer': ""<p>The shapes of <code>conv2</code> tensors are the same in both examples?\nI think the second one has reshaped the tensor in advanced.</p>\n\n<p>The shape of <code>conv2</code> in tensorflow example is <code>(batch, y, x, filters)</code>, while the second one would be  <code>(batch, y*x, 1, filters)</code> so the following code can work normally.</p>\n\n<p>And these two approaches have the same output, if the size of <code>conv2</code> is the same as <code>pool_size</code> (2x2 in this case), which will only output one value per filter and this is \ntotally the same idea as <code>reduce_max</code>.</p>\n\n<p>For example: </p>\n\n<pre><code>import tensorflow as tf\n'The same input, but different shape' \nrandom_tensor = np.random.random([16,2,2,64])\nmethod1_input=tf.constant(random_tensor)             # shape = (16,2,2,64)\nmethod2_input=tf.reshape(method1_input,[16,4,1,64])  # shape = (16,4,1,64)\n'method 1 : maxpool'\nmaxpool      = tf.layers.max_pooling2d(inputs=method1_input, pool_size=[2, 2], strides=2)\nmaxpool_flat = tf.reshape(maxpool, [-1,64])\n\n'method 2 : reduce_max and squeeze'\nred_max = tf.reduce_max(method2_input, 1)   # shape = (16,1,64)\npool2   = tf.squeeze(red_max, squeeze_dims=[1])  # shape = (16,64)  ,literally squeeze dim [1]\n\nwith tf.Session() as sess :\n    method1_result=(sess.run(maxpool_flat) )\n    method2_result=(sess.run(pool2) )\n    Is_true = sess.run(tf.equal(method1_result,method2_result)  )\n    print(Is_true)\n    # output\n    #[[ True  True  True ...  True  True  True]\n    # [ True  True  True ...  True  True  True]\n    # [ True  True  True ...  True  True  True]\n    # ...\n    # [ True  True  True ...  True  True  True]\n    # [ True  True  True ...  True  True  True]\n    # [ True  True  True ...  True  True  True]]\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1540009537}, {'QuestionId': 51965094, 'AnswerId': 51965581, 'URL': 'https://stackoverflow.com/questions/51965094/whats-the-difference-between-tf-feature-column-input-layer-and-tf-layers-input/51965581#51965581', 'QuestionTitle': 'What&#39;s the difference between tf.feature_column.input_layer and tf.layers.Input', 'Answer': '<ul>\n<li><p><a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer"" rel=""nofollow noreferrer""><code>tf.feature_column.input_layer</code></a> returns a dense Tensor as an input layer to the Model from an already defined <a href=""https://www.tensorflow.org/guide/feature_columns"" rel=""nofollow noreferrer"">FeatureColumn</a>. The FeatureColumn APIs <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column"" rel=""nofollow noreferrer""><code>tf.feature_column</code></a> describes the attributes from the dataset that will be fed into an Estimator for training and validation.</p></li>\n<li><p>Whereas, <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model"" rel=""nofollow noreferrer""><code>tf.layers.Input</code></a> is analogous to Keras <code>tf.keras.Input()</code> method which is used to instantiate a Keras or in this case TensorFlow Tensor for use with the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model"" rel=""nofollow noreferrer""><code>tf.keras.Model</code></a> function.</p></li>\n</ul>\n', 'IsAccepted': False, 'CreationDate': 1534936305}, {'QuestionId': 50259290, 'AnswerId': 50259839, 'URL': 'https://stackoverflow.com/questions/50259290/tensorflow-flatten-vs-numpy-flatten-function-effect-on-machine-learning-training/50259839#50259839', 'QuestionTitle': 'Tensorflow flatten vs numpy flatten function effect on machine learning training', 'Answer': '<p><strong>Difference</strong></p>\n\n<p>When you use tensorflow flatten, it gets added as an operation (op) in the graph. It can operate only on tensors. Numpy on the other hand works on actual numpy arrays. The usage is completely different.</p>\n\n<p><strong>Usage</strong></p>\n\n<p>You would use tensorflow op if this is an operation in the training process such as resizing before feeding to the next layer.</p>\n\n<p>You would use numpy op when you want to operate on actual value at that time, like reshaping for calculating accuracy at the end of training step.</p>\n\n<p>So if you had a task of</p>\n\n<p><em>tensor A -> reshape -> matrix_mul</em></p>\n\n<p>If you use <strong>tensorflow</strong> for reshape, you can <strong>directly run</strong> the <code>matrix_mul</code> from session. </p>\n\n<p>If you use <strong>numpy</strong> however, you\'d have to run the operation in <strong>two stages</strong> (two session calls). </p>\n\n<ol>\n<li><p>You calculate tensor A</p></li>\n<li><p>You reshape it in numpy.</p></li>\n<li><p>Run the matrix_mul by ""feeding"" in reshaped array.</p></li>\n</ol>\n\n<p><strong>Performance</strong></p>\n\n<p>I haven\'t benchmarked anything but I\'d say for just a reshape operation as standalone, numpy would be faster (ignoring gpu ) , but in a process where reshape is an intermediate op, tensorflow should be faster.</p>\n', 'IsAccepted': False, 'CreationDate': 1525890195}, {'QuestionId': 50259290, 'AnswerId': 50259726, 'URL': 'https://stackoverflow.com/questions/50259290/tensorflow-flatten-vs-numpy-flatten-function-effect-on-machine-learning-training/50259726#50259726', 'QuestionTitle': 'Tensorflow flatten vs numpy flatten function effect on machine learning training', 'Answer': ""<ul>\n<li>Use numpy directly on your data, without participation of a neural network. This is for preprocessing and postprocessing only    </li>\n<li>Use TF or Keras layers inside models if this operation is needed for some reason in the model. This will assure model connectivity and proper backpropagation</li>\n</ul>\n\n<p>Models are symbolic graphs meant to create Neural Networks that can be trained. There will be a proper connection and backpropagation will work properly when you have a graph connected from input to output.    </p>\n\n<p>If you don't intend to create a network, don't use a TF layer. If your goal just to flatten an array, you don't need a neural network. </p>\n\n<p>Now if inside a model you need to change the format of the data without losing connection and backpropagation, then go for the flatten layer.</p>\n"", 'IsAccepted': False, 'CreationDate': 1525889752}, {'QuestionId': 50259290, 'AnswerId': 50259737, 'URL': 'https://stackoverflow.com/questions/50259290/tensorflow-flatten-vs-numpy-flatten-function-effect-on-machine-learning-training/50259737#50259737', 'QuestionTitle': 'Tensorflow flatten vs numpy flatten function effect on machine learning training', 'Answer': '<p>The biggest difference between <code>np.flatten</code> and <code>tf.layers.flatten</code> (or <code>tf.contrib.layers.flatten</code>) is that numpy operations are applicable <strong>only to static nd arrays</strong>, while tensorflow operations can work with <strong>dynamic tensors</strong>. Dynamic in this case means that the exact shape will be known only at runtime (either training or testing).</p>\n\n<p>So my recommendation is pretty simple:</p>\n\n<ul>\n<li>If the input data is static numpy array, e.g. in pre-processing, use <code>np.flatten</code>. This avoids unnecessary overhead and returns numpy array as well.</li>\n<li>If the data is already a tensor, use any of the <code>flatten</code> ops provided by tensorflow. Between those, <code>tf.layers.flatten</code> is better choice since <code>tf.layers</code> API is more stable than <code>tf.contrib.*</code>.</li>\n</ul>\n', 'IsAccepted': True, 'CreationDate': 1525889786}, {'QuestionId': 50259290, 'AnswerId': 50259637, 'URL': 'https://stackoverflow.com/questions/50259290/tensorflow-flatten-vs-numpy-flatten-function-effect-on-machine-learning-training/50259637#50259637', 'QuestionTitle': 'Tensorflow flatten vs numpy flatten function effect on machine learning training', 'Answer': '<p>The flatten function in numpy does a complete array flattening, meaning that you end up with a single axis of data (1 dimension only).\nFor example, </p>\n\n<pre><code>import numpy as np\na = np.arange(20).reshape((5,4))\nprint(a)\n\nprint(a.flatten().shape)\n</code></pre>\n\n<p>In the previous example, you end up with a 1-d array of 20 elements.\nIn tensorflow, the flatten layer (tf.layers.flatten) preserves the batch axis (axis 0). In the previous example, with tensorflow, you would still have a shape of (5,4).</p>\n\n<p>In any case, there is no effect on training if you use flatten in an equivalent way. However, you should avoid using numpy when working with tensorflow, since almost all numpy operations have their tensorflow counterparts. Tensorflow and numpy rely on different runtime libraries and combining both could be runtime inefficient.</p>\n\n<p>Moreover, avoid using contrib package layers, when they already exist in the main package (use tf.layers.flatten instead of tf.contrib.layers.flatten).</p>\n\n<p>For a more general performance comparison between numpy and tensorflow, have a look at this question: <a href=""https://stackoverflow.com/questions/42702586/tensorflow-vs-numpy-performance"">Tensorflow vs. Numpy Performance</a></p>\n', 'IsAccepted': False, 'CreationDate': 1525889402}, {'QuestionId': 46920307, 'AnswerId': 46920585, 'URL': 'https://stackoverflow.com/questions/46920307/is-tf-contrib-layers-flattenx-the-same-as-tf-reshapex-n-1/46920585#46920585', 'QuestionTitle': 'Is tf.contrib.layers.flatten(x) the same as tf.reshape(x, [n, 1])?', 'Answer': '<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/15bd614fedc20d229f3e97a99e0748136ba4852c/tensorflow/python/layers/core.py#L361"" rel=""nofollow noreferrer"">implementation</a> of <a href=""https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/layers/flatten"" rel=""nofollow noreferrer""><code>tf.layers.flatten()</code></a> reveals the answer. It is equivalent to the following:</p>\n\n<pre><code>flattened = tf.reshape(x, [tf.shape(x)[0], -1])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1508881618}, {'QuestionId': 44499755, 'AnswerId': 44501868, 'URL': 'https://stackoverflow.com/questions/44499755/keras-what-is-the-difference-between-layers-input-and-layers-inputlayer/44501868#44501868', 'QuestionTitle': 'Keras: What is the difference between layers.Input and layers.InputLayer?', 'Answer': '<p>I think <code>InputLayer</code> has been deprecated together with the Graph models. I would suggest you use Input, as all the examples on the Keras documentations show.</p>\n', 'IsAccepted': False, 'CreationDate': 1497277539}, {'QuestionId': 44176982, 'AnswerId': 44191138, 'URL': 'https://stackoverflow.com/questions/44176982/how-does-the-flatten-layer-work-in-keras/44191138#44191138', 'QuestionTitle': 'How does the Flatten layer work in Keras?', 'Answer': '<p>The <code>Flatten()</code> operator unrolls the values beginning at the last dimension (at least for Theano, which is ""channels first"", not ""channels last"" like TF.  I can\'t run TensorFlow in my environment).  This is equivalent to <code>numpy.reshape</code> with \'C\' ordering:</p>\n\n<blockquote>\n  <p>‘C’ means to read / write the elements using C-like index order, with\n  the last axis index changing fastest, back to the first axis index\n  changing slowest.</p>\n</blockquote>\n\n<p>Here is a standalone example illustrating  <code>Flatten</code> operator with the Keras Functional API.  You should be able to easily adapt for your environment.  </p>\n\n<pre><code>import numpy as np\nfrom keras.layers import Input, Flatten\nfrom keras.models import Model\ninputs = Input(shape=(3,2,4))\n\n# Define a model consisting only of the Flatten operation\nprediction = Flatten()(inputs)\nmodel = Model(inputs=inputs, outputs=prediction)\n\nX = np.arange(0,24).reshape(1,3,2,4)\nprint(X)\n#[[[[ 0  1  2  3]\n#   [ 4  5  6  7]]\n#\n#  [[ 8  9 10 11]\n#   [12 13 14 15]]\n#\n#  [[16 17 18 19]\n#   [20 21 22 23]]]]\nmodel.predict(X)\n#array([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n#         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n#         22.,  23.]], dtype=float32)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1495753769}, {'QuestionId': 44176982, 'AnswerId': 44188293, 'URL': 'https://stackoverflow.com/questions/44176982/how-does-the-flatten-layer-work-in-keras/44188293#44188293', 'QuestionTitle': 'How does the Flatten layer work in Keras?', 'Answer': '<p>It is sequential like 24*24*32 and reshape it as shown in following code. </p>\n\n<pre><code>def batch_flatten(x):\n    """"""Turn a nD tensor into a 2D tensor with same 0th dimension.\n    In other words, it flattens each data samples of a batch.\n    # Arguments\n        x: A tensor or variable.\n    # Returns\n        A tensor.\n    """"""\n    x = tf.reshape(x, tf.stack([-1, prod(shape(x)[1:])]))\n    return x\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1495740069}]","{51678524, 71335830}","['<p>Keras models expect the first dimension of your data to be the batch dimension. You can have a look at the docs on the <a href=""https://keras.io/layers/core/#input"" rel=""nofollow noreferrer"">Input layers</a> from the functional API. This layer has a <code>shape</code> argument as well as an <code>batch_shape</code> argument. Both work, but the latters allow to explicitly define a batch shape.</p>\n\n<p>To answer your questions:</p>\n\n<ul>\n<li><code>noise shape = (64,128)</code> would give an error because the expected input dim is 64 and you provide 128 (the 64 in the first dimension does not matter here as the batch size can be anything and does not change the model architecture)</li>\n<li><code>(64, 256, 256)</code> and <code>(256, 256, 64)</code> would give you an error because an input with two dimensions (+ 1 batch dimension) is provided, where the model expects 1-dimensional input (+ 1 batch dimension).</li>\n</ul>\n\n<p>Also check out the docs of the <a href=""https://keras.io/getting-started/sequential-model-guide/"" rel=""nofollow noreferrer"">Sequential</a> API:</p>\n\n<blockquote>\n  <p>Pass an input_shape argument to the first layer. This is a shape tuple\n  (a tuple of integers or None entries, where None indicates that any\n  positive integer may be expected). In input_shape, the batch dimension\n  is not included.</p>\n  \n  <p>If you ever need to specify a fixed batch size for your inputs (this\n  is useful for stateful recurrent networks), you can pass a  batch_size\n  argument to a layer. If you pass both batch_size=32 and\n  input_shape=(6, 8) to a layer, it will then expect every batch of\n  inputs to have the batch shape (32, 6, 8).</p>\n</blockquote>\n', 'In <code>model1</code>, the input shape will be inferred when you pass real data to it or call <code>model.build</code>.</p>\n<p>Now regarding the <code>Flatten</code> layer, this layer simply converts a n-dimensional tensor (for example <code>(28, 28, 1)</code>) into a 1D tensor <code>(28 x 28 x 1)</code>. The <code>Flatten</code> layer and <code>Input</code> layer can coexist in a <code>Sequential</code> model but do not depend on each other.</p>\n', ""<p>I think the confusion comes from using a <code>tf.keras.Sequential</code> model, which does not need an explicit <code>Input</code> layer. Consider the following two models, which are equivalent:</p>\n<pre><code>import tensorflow as tf\n\nmodel1 = tf.keras.Sequential([\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(5, activation='relu'),\n    ])\n\nmodel1.build((1, 28, 28, 1))\n</code></pre>\n<pre><code>model2 = tf.keras.Sequential([\n      tf.keras.layers.Input((28, 28, 1)),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(5, activation='relu'),\n    ])\n</code></pre>\n<p>The difference is that I explicitly set the input shape of <code>model2</code> using an <code>Input</code> layer. In <code>model1</code>, the input shape will be inferred when you pass real data to it or call <code>model.build</code>.</p>\n<p>Now regarding the <code>Flatten</code> layer, this layer simply converts a n-dimensional tensor (for example <code>(28, 28, 1)</code>) into a 1D tensor <code>(28 x 28 x 1)</code>. The <code>Flatten</code> layer and <code>Input</code> layer can coexist in a <code>Sequential</code> model but do not depend on each other.</p>\n""]","{'https://stackoverflow.com/questions/51678524/keras-input-shape-of-a-dense-layer/51678611#51678611', 'https://stackoverflow.com/questions/71335830/what-is-the-difference-between-tf-keras-layers-input-and-tf-keras-layers-flatt/71335932#71335932'}",,0.17091301959455188,0.07199495639286387
11,49701918,tf.nn.batch_normalization,Lack of Alternative Solutions/Documentation,tf.layers.batch_normalization parameters,"<p>I am not sure if it is only me who thinks that tensorflow documentation is a bit weak.</p>

<p>I was planing to use the tf.nn.batch_normalization function to implement batch normalization but later recognized the  tf.layers.batch_normalization function which seemingly should be the one to use for its simplicity. But the documentation is really poor if I may say it.</p>

<p>I am trying to understand how to <em>correctly</em> use it but with the information provided on the Web page is it really not easy. I am hoping that maybe some other people have experience and help me (and possibly many others) to understand it.. </p>

<p>Let me share the interface first:</p>

<pre><code>tf.layers.batch_normalization(
    inputs,
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer=tf.zeros_initializer(),
    gamma_initializer=tf.ones_initializer(),
    moving_mean_initializer=tf.zeros_initializer(),
    moving_variance_initializer=tf.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    training=False,
    trainable=True,
    name=None,
    reuse=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=None,
    virtual_batch_size=None,
    adjustment=None
)
</code></pre>

<p>Q1) beta values are initialized to zero and gamma values are initialized to 1. But it does not say why. When batch normalization used, I understand that the ordinary bias parameter of the neural network becomes obsolete and beta parameter in the batch normalization step kind of does the same thing. From that angle, setting beta to zero is understandable. But why are gamma values initialized to 1? Is that really the most efficient way?</p>

<p>Q2) I see a momentum parameter there as well. The documentation just says "" Momentum for the moving average."". I assume that this parameter is used when calculating the ""mean"" value for a certain mini batch in the corresponding hidden layer. With other words, the mean value used in batch normalization is NOT the mean of current mini batch, it is rather primarily the mean of the last 100 mini batches (since momentum = 0.99). But it is very unclear how this parameter affects the execution in testing, or if I am just validating my model on the dev set by calculating cost and accuracy. My <em>assumption</em> is that anytime I deal with test and dev sets, I set the parameter ""training"" to False so that momentum parameter becomes obsolete for that particular execution and the ""mean"" and ""variance"" values that were calculated during the training are used now instead of calculating new mean and variance values. It is how it should be if I am mistaken but I do not see anything in the documentation if it is the case. Could anyone confirm that my understanding correct? If not, I would really appreciate further explanation on this.</p>

<p>Q3) I am having difficulties to give a meaning to the trainable parameter. I assume beta and gamma params are meant here. Why would they not be trainable?</p>

<p>Q4) The ""reuse"" parameter. What is it really?</p>

<p>Q5) adjustment parameter. Another mistery..</p>

<p>Q5) A kind of summary question.. Here is my overall assumption that needs confirmation and feedback.. Important params here are:
- inputs
- axis
- momentum
- center
- scale
- training
And I assume that as long as the training=True when training, we are safe. And as long as training=False when validating dev set or test set or even when using the model in real life, we are safe too.</p>

<p>Any feedback will really be appreciated.</p>

<p>ADDENDUM:</p>

<p>Confusion continues. Help!</p>

<p>I am trying to use this function instead of implementing a batch normalizer manually. I have the following forward propagation function that loops through layers of the NN.</p>

<pre><code>def forward_propagation_with_relu(X, num_units_in_layers, parameters, 
                                  normalize_batch, training, mb_size=7):

    L = len(num_units_in_layers)

    A_temp = tf.transpose(X)

    for i in range (1, L):
        W = parameters.get(""W""+str(i))
        b = parameters.get(""b""+str(i))
        Z_temp = tf.add(tf.matmul(W, A_temp), b)

        if normalize_batch:
            if (i &lt; (L-1)):  
                with tf.variable_scope(""batch_norm_scope"", reuse=tf.AUTO_REUSE):
                    Z_temp = tf.layers.batch_normalization(Z_temp, axis=-1, 
                                                           training=training)

        A_temp = tf.nn.relu(Z_temp)

    return Z_temp   #This is the linear output of last layer
</code></pre>

<p>The tf.layers.batch_normalization(..) function wants to have static dimensions but I do not have it in my case.</p>

<p>Since I apply mini batches rather than training the entire train set each time before I run the optimizer, 1 dimension of the X appears to be unknown.</p>

<p>If I write:</p>

<pre><code>print(X.shape)
</code></pre>

<p>I get:</p>

<pre><code>(?, 5)
</code></pre>

<p>And when this is the case, when I run the whole program I get the following error below.</p>

<p>I saw in some other threads that some people say that they could solve the problem by using tf.reshape function. I try it.. Forward prop goes fine but later on it crashes in the Adam Optimizer..</p>

<p>Here is what I get when I run the code above (without using tf.reshape):</p>

<p>How do I solve this???</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-191-990fb7d7f7f6&gt; in &lt;module&gt;()
     24 parameters = nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs,
     25                       normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers,
---&gt; 26                       lambd, print_progress)
     27 
     28 print(parameters)

&lt;ipython-input-190-59594e979129&gt; in nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs, normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers, lambd, print_progress)
     34         # Forward propagation: Build the forward propagation in the tensorflow graph
     35         ZL = forward_propagation_with_relu(X_mini_batch, num_units_in_layers, 
---&gt; 36                                            parameters, normalize_batch, training)
     37 
     38     with tf.name_scope(""calc_cost""):

&lt;ipython-input-187-8012e2fb6236&gt; in forward_propagation_with_relu(X, num_units_in_layers, parameters, normalize_batch, training, mb_size)
     15                 with tf.variable_scope(""batch_norm_scope"", reuse=tf.AUTO_REUSE):
     16                     Z_temp = tf.layers.batch_normalization(Z_temp, axis=-1, 
---&gt; 17                                                            training=training)
     18 
     19         A_temp = tf.nn.relu(Z_temp)

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py in batch_normalization(inputs, axis, momentum, epsilon, center, scale, beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer, beta_regularizer, gamma_regularizer, beta_constraint, gamma_constraint, training, trainable, name, reuse, renorm, renorm_clipping, renorm_momentum, fused, virtual_batch_size, adjustment)
    775       _reuse=reuse,
    776       _scope=name)
--&gt; 777   return layer.apply(inputs, training=training)
    778 
    779 

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py in apply(self, inputs, *args, **kwargs)
    805       Output tensor(s).
    806     """"""
--&gt; 807     return self.__call__(inputs, *args, **kwargs)
    808 
    809   def _add_inbound_node(self,

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    676           self._defer_regularizers = True
    677           with ops.init_scope():
--&gt; 678             self.build(input_shapes)
    679           # Create any regularizers added by `build`.
    680           self._maybe_create_variable_regularizers()

~/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py in build(self, input_shape)
    251       if axis_to_dim[x] is None:
    252         raise ValueError('Input has undefined `axis` dimension. Input shape: ',
--&gt; 253                          input_shape)
    254     self.input_spec = base.InputSpec(ndim=ndims, axes=axis_to_dim)
    255 

ValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(6), Dimension(None)]))
</code></pre>

<p>This is so hopeless.. </p>

<p>ADDENDUM(2)</p>

<p>I am adding more information:</p>

<p>The following simply means that there are 5 units in input layer, 6 units in each hidden layer, and 2 units in output layer.</p>

<pre><code>num_units_in_layers = [5,6,6,2] 
</code></pre>

<p>Here is the updated version of forward prop function with tf.reshape</p>

<pre><code>def forward_propagation_with_relu(X, num_units_in_layers, parameters, 
                                  normalize_batch, training, mb_size=7):

    L = len(num_units_in_layers)
    print(""X.shape before reshape: "", X.shape)             # ADDED LINE 1
    X = tf.reshape(X, [mb_size, num_units_in_layers[0]])   # ADDED LINE 2
    print(""X.shape after reshape: "", X.shape)              # ADDED LINE 3
    A_temp = tf.transpose(X)

    for i in range (1, L):
        W = parameters.get(""W""+str(i))
        b = parameters.get(""b""+str(i))
        Z_temp = tf.add(tf.matmul(W, A_temp), b)

        if normalize_batch:
            if (i &lt; (L-1)):  
                with tf.variable_scope(""batch_norm_scope"", reuse=tf.AUTO_REUSE):
                    Z_temp = tf.layers.batch_normalization(Z_temp, axis=-1, 
                                                           training=training)

        A_temp = tf.nn.relu(Z_temp)

    return Z_temp   #This is the linear output of last layer
</code></pre>

<p>When I do this, I can run the forward prop function. But it seems to be crashing in later execution. Here is the error that I get. (Note that I print out the shape of input X before and after reshaping in the forward prop function).</p>

<pre><code>X.shape before reshape:  (?, 5)
X.shape after reshape:  (7, 5)

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1349     try:
-&gt; 1350       return fn(*args)
   1351     except errors.OpError as e:

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1328                                    feed_dict, fetch_list, target_list,
-&gt; 1329                                    status, run_metadata)
   1330 

~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    515             compat.as_text(c_api.TF_Message(self.status.status)),
--&gt; 516             c_api.TF_GetCode(self.status.status))
    517     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: Incompatible shapes: [7] vs. [2]
     [[Node: forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub = Sub[T=DT_FLOAT, _class=[""loc:@batch_norm_scope/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward_prop/batch_norm_scope/batch_normalization/cond_2/Switch_1:1, forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1:1)]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-222-990fb7d7f7f6&gt; in &lt;module&gt;()
     24 parameters = nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs,
     25                       normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers,
---&gt; 26                       lambd, print_progress)
     27 
     28 print(parameters)

&lt;ipython-input-221-59594e979129&gt; in nn_model(train_input_paths, dev_input_paths, test_input_paths, learning_rate, num_train_epochs, normalize_batch, epoch_period_to_save_cost, minibatch_size, num_units_in_layers, lambd, print_progress)
     88                                                                         cost_mini_batch,
     89                                                                         accuracy_mini_batch],
---&gt; 90                                                                         feed_dict={training: True})
     91                       nr_of_minibatches += 1
     92                       sum_minibatch_costs += minibatch_cost

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    893     try:
    894       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 895                          run_metadata_ptr)
    896       if run_metadata:
    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1126     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1127       results = self._do_run(handle, final_targets, final_fetches,
-&gt; 1128                              feed_dict_tensor, options, run_metadata)
   1129     else:
   1130       results = []

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1342     if handle is None:
   1343       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-&gt; 1344                            options, run_metadata)
   1345     else:
   1346       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1361         except KeyError:
   1362           pass
-&gt; 1363       raise type(e)(node_def, op, message)
   1364 
   1365   def _extend_graph(self):

InvalidArgumentError: Incompatible shapes: [7] vs. [2]
     [[Node: forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub = Sub[T=DT_FLOAT, _class=[""loc:@batch_norm_scope/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward_prop/batch_norm_scope/batch_normalization/cond_2/Switch_1:1, forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1:1)]]

Caused by op 'forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub', defined at:
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py"", line 16, in &lt;module&gt;
    app.launch_new_instance()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelapp.py"", line 478, in start
    self.io_loop.start()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 233, in dispatch_shell
    handler(stream, idents, msg)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/ipkernel.py"", line 208, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/zmqshell.py"", line 537, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2728, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2850, in run_ast_nodes
    if self.run_code(code, result):
  File ""/home/cesncn/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py"", line 2910, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-222-990fb7d7f7f6&gt;"", line 26, in &lt;module&gt;
    lambd, print_progress)
  File ""&lt;ipython-input-221-59594e979129&gt;"", line 36, in nn_model
    parameters, normalize_batch, training)
  File ""&lt;ipython-input-218-62e4c6126c2c&gt;"", line 19, in forward_propagation_with_relu
    training=training)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 777, in batch_normalization
    return layer.apply(inputs, training=training)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 807, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 697, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 602, in call
    lambda: self.moving_mean)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/utils.py"", line 211, in smart_cond
    return control_flow_ops.cond(pred, true_fn=fn1, false_fn=fn2, name=name)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1985, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1839, in BuildCondBranch
    original_result = fn()
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 601, in &lt;lambda&gt;
    lambda: _do_update(self.moving_mean, new_mean),
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/layers/normalization.py"", line 597, in _do_update
    var, value, self.momentum, zero_debias=False)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/training/moving_averages.py"", line 87, in assign_moving_average
    update_delta = (variable - value) * decay
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 778, in _run_op
    return getattr(ops.Tensor, operator)(a._AsTensor(), *args)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 934, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 4819, in _sub
    ""Sub"", x=x, y=y, name=name)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 3267, in create_op
    op_def=op_def)
  File ""/home/cesncn/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1650, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Incompatible shapes: [7] vs. [2]
     [[Node: forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub = Sub[T=DT_FLOAT, _class=[""loc:@batch_norm_scope/batch_normalization/moving_mean""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](forward_prop/batch_norm_scope/batch_normalization/cond_2/Switch_1:1, forward_prop/batch_norm_scope/batch_normalization/cond_2/AssignMovingAvg/sub/Switch_1:1)]]
</code></pre>

<p>Regarding the question why the shape of X is not static.. I don't know...
HEre is how I setup the dataset.</p>

<pre><code>with tf.name_scope(""next_train_batch""):
    filenames = tf.placeholder(tf.string, shape=[None])
    dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv))
    dataset = dataset.shuffle(buffer_size=1000)
    dataset = dataset.batch(minibatch_size)
    iterator = dataset.make_initializable_iterator()
    X_mini_batch, Y_mini_batch = iterator.get_next()
</code></pre>

<p>I have 2 csv files that include the train data.</p>

<pre><code>train_path1 = ""train1.csv""
train_path2 = ""train2.csv""
train_input_paths = [train_path1, train_path2]
</code></pre>

<p>And I use the initializable iterator as following:</p>

<pre><code>sess.run(iterator.initializer, 
         feed_dict={filenames: train_input_paths})
</code></pre>

<p>During the training, I keep getting mini batches from the train set. Everything works fine when I disable batch normalization. If I enable batch norm, it requires static shape of the input X (mini batch). I reshape it but this time it crashes later in the execution as seen above. </p>

<p>ADDENDUM(3)</p>

<p>I guess I figured out where it crashes. It probably crashes when I run the optimizer after calculating the cost.</p>

<p>First the sequence of commands:
First forward prop, then compute cost, then run optimizer. First 2 seems to be working but not the optimizer.</p>

<p>HEre is how I define the optimizer:</p>

<pre><code>with tf.name_scope(""train""):
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):        
        # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.
        optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost_mini_batch)
</code></pre>

<p>I have the update_ops there to be able to update the moving averages. If I interpret it right, it is just crashing when it tries to update moving averages. I might be misinterpreting the error msg as well.. </p>

<p>ADDENDUM(4)</p>

<p>I tried to normalize based on the known dimension and it worked! But that's not the dimension I would like to normalize, which is now confusing. Let me elaborate:</p>

<p>nr of units in input layer: 5
nr of units in layer 1 (first hidden layer): 6
so weight1 is (6, 5) matrix
Assume that mini batch size is 7.
Shape of A[0] (or X_mini_batch) in my case is: (7, 5), where 7 is the # training samples in mini batch, and 5 is the # units in input layer.</p>

<p>When calculating Z[1]...
Z[1] = weight1 * A[0].transpose
... then shape of Z[1] is (6, 7) matrix, where each column gives 6 features for each train sample.</p>

<p>The question is then which column do we want to normalize in Z[1]? What makes sense to me is that you normalize each feature from all given train samples. This means that I need to normalize each row bcz I have different feature values for different train examples in each row. And since Z[1] has the shape (6, 7), if I set axis=0, it should refer to normalization in each row. And 7 is the unknown number in my case so it doesn't hurt. Based on this logic, it works! But I am totally puzzled if axis=0 really refers to each row here... Let me show another example about this axis issue, which has bothered me for a long time now..</p>

<p>The <em>irrelevant from this topic</em> code example:</p>

<pre><code>cc = tf.constant([[1.,2.,3.], 
                  [4.,5.,6.]])

with tf.Session() as sess:
    print(sess.run(tf.reduce_mean(cc, axis=0)))
    print(sess.run(tf.reduce_mean(cc, axis=1)))  
</code></pre>

<p>This gives the following output:</p>

<pre><code>[2.5 3.5 4.5]
[2. 5.]
</code></pre>

<p>When I set axis to 0, it is giving the average of each column. And if axis=1, it is giving the average of each row.</p>

<p>(Note that cc.shape gives (2,3))</p>

<p>Now the million dollar question: In a 2 dimensional matrix, is axis 0 or 1 when I want to address each row?</p>

<p>ADDENDUM(5)
I guess I get it now correctly. Let me summarize my axis understanding here. Hopefully I am getting it right now...</p>

<p>Here is the Z[1] matrix representation with the shape (6,7):</p>

<p>t_ex :   train example
f:       feature</p>

<pre><code>t_ex1   t_ex2   t_ex3   t_ex4   t_ex5   t_ex6   t_ex7
  f1      f1      f1      f1      f1      f1      f1
  f2      f2      f2      f2      f2      f2      f2
  f3      f3      f3      f3      f3      f3      f3
  f4      f4      f4      f4      f4      f4      f4
  f5      f5      f5      f5      f5      f5      f5
  f6      f6      f6      f6      f6      f6      f6
</code></pre>

<p>In this mini batch above, there are 7 train examples and each train ex has 6 features (since there are 6 units in layer 1). When we say ""tf.layers.batch_normalization(..,axis=0)"", we mean that the normalization has to be done per row for each feature to eliminate the high variance between - say - f1 values in the first row.</p>

<p>With other words, we do NOT normalize f1,f2,f3,f4,f5,f6 with each other. We normalize f1:s with each other, and f2:s with each other, and so on..</p>
","<p>Q1) Initializing gamma as 1, beta as 0 means directly using the normalized inputs. Since there is no prior information about what the variance of a layer output should be, it is fair enough to assume standard Gaussian.</p>

<p>Q2) During training phase (<code>training=True</code>), the batch is normalized with their own mean and var, assuming that training data are randomly sampled. During test (<code>training=False</code>), since the test data could be arbitrarily sampled, we cannot use their mean and var. Thus, we use, as you said, the moving averaging estimations from the last ""100"" training iterations.</p>

<p>Q3) Yes, trainable refers to <code>beta</code> and <code>gamma</code>. There are cases to set <code>trainable=False</code>, e.g. if a novel method is used to update the parameters, or if the batch_norm layer is pre-trained and needs to be frozen. </p>

<p>Q4) You may have noticed <code>reuse</code> parameters in other <code>tf.layers</code> functions as well. In general, if you wanna call a layer more than once (e.g. training and validation) and you do not wanna TensorFlow to think that you are creating a new layer, you set <code>reuse=True</code>. I prefer <code>with tf.variable_scope(..., reuse=tf.AUTO_REUSE):</code> to achieve the same purpose.</p>

<p>Q5) I am not sure about this one. I guess it is for users who want to design new tricks to adjust the scale and bias.</p>

<p>Q6) Yes, you are right.</p>
","{38321409, 59651396, 53282439, 47953242, 40879967, 45219736, 33949786, 40166236, 33992029, 48001759}","[{'QuestionId': 59651396, 'AnswerId': 59654494, 'URL': 'https://stackoverflow.com/questions/59651396/how-to-calculate-batch-normalization-with-python/59654494#59654494', 'QuestionTitle': 'How to calculate batch normalization with python?', 'Answer': '<p>You should calculate mean and std across all pixels in the images of the batch. So use axis=(0, 2, 3) parameters.\nIf the channels have roughly same distributions - you may calculate mean and std across channels as well. so just use mean() and std() without axes parameter.</p>\n\n<p>The figure in the article is correct - it takes mean and std across H and W (image dimensions) for each batch. Obviously, channel is not shown in the 3d cube.</p>\n', 'IsAccepted': False, 'CreationDate': 1578521315}, {'QuestionId': 53282439, 'AnswerId': 53283709, 'URL': 'https://stackoverflow.com/questions/53282439/in-tensorflow-how-can-i-look-at-the-batch-normalization-parameters/53283709#53283709', 'QuestionTitle': 'In TensorFlow, how can I look at the batch normalization parameters?', 'Answer': ""<p>You could get all the variables inside the scope of the batch normalization layer and print them. Example:</p>\n\n<pre><code>import tensorflow as tf\n\ntf.reset_default_graph()\nx = tf.constant(3.0, shape=(3,))\nx = tf.layers.batch_normalization(x)\n\nprint(x.name) # batch_normalization/batchnorm/add_1:0\n\nvariables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                              scope='batch_normalization')\nprint(variables)\n\n#[&lt;tf.Variable 'batch_normalization/gamma:0' shape=(3,) dtype=float32_ref&gt;,\n# &lt;tf.Variable 'batch_normalization/beta:0' shape=(3,) dtype=float32_ref&gt;,\n# &lt;tf.Variable 'batch_normalization/moving_mean:0' shape=(3,) dtype=float32_ref&gt;,\n#  &lt;tf.Variable 'batch_normalization/moving_variance:0' shape=(3,) dtype=float32_ref&gt;]\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    gamma = sess.run(variables[0])\n    print(gamma) # [1. 1. 1.]\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1542120837}, {'QuestionId': 33949786, 'AnswerId': 36511416, 'URL': 'https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/36511416#36511416', 'QuestionTitle': 'How could I use batch normalization in TensorFlow?', 'Answer': '<p>Since someone recently edited this, I\'d like to clarify that this is no longer an issue.</p>\n\n<p><a href=""https://stackoverflow.com/a/34634291/3924118"">This answer</a> does not seem correct  When <code>phase_train</code> is set to false, it still updates the ema mean and variance. This can be verified with the following code snippet.</p>\n\n<pre><code>x = tf.placeholder(tf.float32, [None, 20, 20, 10], name=\'input\')\nphase_train = tf.placeholder(tf.bool, name=\'phase_train\')\n\n# generate random noise to pass into batch norm\nx_gen = tf.random_normal([50,20,20,10])\npt_false = tf.Variable(tf.constant(True))\n\n#generate a constant variable to pass into batch norm\ny = x_gen.eval()\n\n[bn, bn_vars] = batch_norm(x, 10, phase_train)\n\ntf.initialize_all_variables().run()\ntrain_step = lambda: bn.eval({x:x_gen.eval(), phase_train:True})\ntest_step = lambda: bn.eval({x:y, phase_train:False})\ntest_step_c = lambda: bn.eval({x:y, phase_train:True})\n\n# Verify that this is different as expected, two different x\'s have different norms\nprint(train_step()[0][0][0])\nprint(train_step()[0][0][0])\n\n# Verify that this is same as expected, same x\'s (y) have same norm\nprint(train_step_c()[0][0][0])\nprint(train_step_c()[0][0][0])\n\n# THIS IS DIFFERENT but should be they same, should only be reading from the ema.\nprint(test_step()[0][0][0])\nprint(test_step()[0][0][0])\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1460162161}, {'QuestionId': 33949786, 'AnswerId': 38320613, 'URL': 'https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/38320613#38320613', 'QuestionTitle': 'How could I use batch normalization in TensorFlow?', 'Answer': '<p>There is also an <a href=""https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100"" rel=""nofollow noreferrer"">""official"" batch normalization layer</a> coded by the developers. They don\'t have very good docs on how to use it but here is how to use it (according to me):</p>\n\n<pre><code>from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n\ndef batch_norm_layer(x,train_phase,scope_bn):\n    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=True,\n    reuse=None, # is this right?\n    trainable=True,\n    scope=scope_bn)\n    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=False,\n    reuse=True, # is this right?\n    trainable=True,\n    scope=scope_bn)\n    z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n    return z\n</code></pre>\n\n<p>to actually use it you need to create a placeholder for <code>train_phase</code> that indicates if you are in training or inference phase (as in <code>train_phase = tf.placeholder(tf.bool, name=\'phase_train\')</code>). Its value can be filled during inference or training with a <code>tf.session</code> as in:</p>\n\n<pre><code>test_error = sess.run(fetches=cross_entropy, feed_dict={x: batch_xtest, y_:batch_ytest, train_phase: False})\n</code></pre>\n\n<p>or during training:</p>\n\n<pre><code>sess.run(fetches=train_step, feed_dict={x: batch_xs, y_:batch_ys, train_phase: True})\n</code></pre>\n\n<p>I\'m pretty sure this is correct according to the discussion in <a href=""https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-231917736"" rel=""nofollow noreferrer"">github</a>.</p>\n\n<hr>\n\n<p>Seems there is another useful link:</p>\n\n<p><a href=""http://r2rt.com/implementing-batch-normalization-in-tensorflow.html"" rel=""nofollow noreferrer"">http://r2rt.com/implementing-batch-normalization-in-tensorflow.html</a></p>\n', 'IsAccepted': False, 'CreationDate': 1468301036}, {'QuestionId': 33949786, 'AnswerId': 43285333, 'URL': 'https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/43285333#43285333', 'QuestionTitle': 'How could I use batch normalization in TensorFlow?', 'Answer': '<p>As of TensorFlow 1.0 (February 2017) there\'s also the high-level <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""noreferrer"" title=""tf.layers.batch_normalization""><code>tf.layers.batch_normalization</code></a> API included in TensorFlow itself.</p>\n\n<p>It\'s super simple to use:</p>\n\n<pre class=""lang-py prettyprint-override""><code># Set this to True for training and False for testing\ntraining = tf.placeholder(tf.bool)\n\nx = tf.layers.dense(input_x, units=100)\nx = tf.layers.batch_normalization(x, training=training)\nx = tf.nn.relu(x)\n</code></pre>\n\n<p>...except that it adds extra ops to the graph (for updating its mean and variance variables) in such a way that they won\'t be dependencies of your training op. You can either just run the ops separately:</p>\n\n<pre class=""lang-py prettyprint-override""><code>extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nsess.run([train_op, extra_update_ops], ...)\n</code></pre>\n\n<p>or add the update ops as dependencies of your training op manually, then just run your training op as normal:</p>\n\n<pre class=""lang-py prettyprint-override""><code>extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(extra_update_ops):\n    train_op = optimizer.minimize(loss)\n...\nsess.run([train_op], ...)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1491591617}, {'QuestionId': 47953242, 'AnswerId': 48034744, 'URL': 'https://stackoverflow.com/questions/47953242/tensorflow-batch-normalization-tf-contrib-layers-batch-norm/48034744#48034744', 'QuestionTitle': 'Tensorflow Batch Normalization: tf.contrib.layers.batch_norm', 'Answer': '<p>Unstable <code>decay</code> rate (default 0.999) for moving averages might be the reason for reasonably good training performance but poor validation/test performance. Try a slightly lower <code>decay</code> rate (0.99 or 0.9).  Also, try <code>zero_debias_moving_mean=True</code> for improved stability.</p>\n\n<p>You can also try different batch sizes and see if validation performance increases. Large batch size can break validation performance when batch normalization is used. See <a href=""http://forums.fast.ai/t/batch-normalization-with-a-large-batch-size-breaks-validation-accuracy/7940"" rel=""nofollow noreferrer"">this</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1514643345}, {'QuestionId': 47953242, 'AnswerId': 48034845, 'URL': 'https://stackoverflow.com/questions/47953242/tensorflow-batch-normalization-tf-contrib-layers-batch-norm/48034845#48034845', 'QuestionTitle': 'Tensorflow Batch Normalization: tf.contrib.layers.batch_norm', 'Answer': '<p>Is your phase variable, a tensorflow boolean or a Python boolean?</p>\n', 'IsAccepted': False, 'CreationDate': 1514644107}, {'QuestionId': 40166236, 'AnswerId': 48033589, 'URL': 'https://stackoverflow.com/questions/40166236/tensorflow-batch-normalization/48033589#48033589', 'QuestionTitle': 'Tensorflow batch normalization', 'Answer': ""<pre><code>    if mode == 0:\n        batch_mean, batch_var = tf.nn.moments(x,[0, 1, 2])\n        train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n        train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n        with tf.control_dependencies([train_mean, train_var]):\n            bn = tf.nn.batch_normalization(x, batch_mean, batch_var, beta, scale, epsilon, name='bn')\n    else:\n        bn = tf.nn.batch_normalization(x, pop_mean, pop_var, beta, scale, epsilon, name='bn')\n</code></pre>\n\n<p>It seems that the <code>if</code> prediction here always evaluate to be <code>False</code>. I guess what you want to do is using <code>mode</code> via <code>feed_dict</code> to control your batch normalization. So you should use <code>tf.cond</code> in TensorFlow instead of <code>if</code> in Python.</p>\n"", 'IsAccepted': False, 'CreationDate': 1514634043}, {'QuestionId': 48001759, 'AnswerId': 48006315, 'URL': 'https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow/48006315#48006315', 'QuestionTitle': 'What is right batch normalization function in Tensorflow?', 'Answer': '<p>Just to add to the list, there\'re several more ways to do batch-norm in tensorflow:</p>\n\n<ul>\n<li><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"" rel=""noreferrer""><code>tf.nn.batch_normalization</code></a> is a low-level op. The caller is responsible to handle <code>mean</code> and <code>variance</code> tensors themselves.</li>\n<li><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/fused_batch_norm"" rel=""noreferrer""><code>tf.nn.fused_batch_norm</code></a> is another low-level op, similar to the previous one. The difference is that it\'s optimized for 4D input tensors, which is the usual case in convolutional neural networks. <code>tf.nn.batch_normalization</code> accepts tensors of any rank greater than 1.</li>\n<li><a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""noreferrer""><code>tf.layers.batch_normalization</code></a> is a high-level wrapper over the previous ops. The biggest difference is that it takes care of creating and managing the running mean and variance tensors, and calls a fast fused op when possible. Usually, this should be the <strong>default choice</strong> for you.</li>\n<li><a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm"" rel=""noreferrer""><code>tf.contrib.layers.batch_norm</code></a> is the early implementation of batch norm, before it\'s graduated to the core API (i.e., <code>tf.layers</code>). The use of it is not recommended because it may be dropped in the future releases.</li>\n<li><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/batch_norm_with_global_normalization"" rel=""noreferrer""><code>tf.nn.batch_norm_with_global_normalization</code></a> is another deprecated op. Currently, delegates the call to <code>tf.nn.batch_normalization</code>, but likely to be dropped in the future.</li>\n<li>Finally, there\'s also Keras layer <a href=""https://keras.io/layers/normalization/#batchnormalization"" rel=""noreferrer""><code>keras.layers.BatchNormalization</code></a>, which in case of tensorflow backend invokes <code>tf.nn.batch_normalization</code>.</li>\n</ul>\n', 'IsAccepted': True, 'CreationDate': 1514459047}, {'QuestionId': 48001759, 'AnswerId': 48003210, 'URL': 'https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow/48003210#48003210', 'QuestionTitle': 'What is right batch normalization function in Tensorflow?', 'Answer': '<p>As show in <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib"" rel=""noreferrer"">doc</a>, <code>tf.contrib</code> is a contribution module containing volatile or experimental code. When <code>function</code> is complete, it will be removed from this module. Now there are two, in order to be compatible with the historical version. </p>\n\n<p>So, the former <code>tf.layers.batch_normalization</code> is recommended.</p>\n', 'IsAccepted': False, 'CreationDate': 1514445168}, {'QuestionId': 38321409, 'AnswerId': 47930311, 'URL': 'https://stackoverflow.com/questions/38321409/how-does-one-use-the-official-batch-normalization-layer-in-tensorflow/47930311#47930311', 'QuestionTitle': 'How does one use the official Batch Normalization layer in TensorFlow?', 'Answer': ""<pre><code>training =tf.placeholder(tf.bool, name = 'training')\n\nlr_holder = tf.placeholder(tf.float32, [], name='learning_rate')\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n        optimizer =  tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)\n</code></pre>\n\n<p>when defining the layers, you need to use the placeholder 'training'</p>\n\n<pre><code>batchNormal_layer = tf.layers.batch_normalization(pre_batchNormal_layer, training=training)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1513879006}, {'QuestionId': 40879967, 'AnswerId': 40881347, 'URL': 'https://stackoverflow.com/questions/40879967/how-to-use-batch-normalization-correctly-in-tensorflow/40881347#40881347', 'QuestionTitle': 'How to use Batch Normalization correctly in tensorflow?', 'Answer': '<p>I have tested that the following simplified implementation of batch normalization gives the same result as <code>tf.contrib.layers.batch_norm</code> as long as the setting is the same.</p>\n\n<pre><code>def initialize_batch_norm(scope, depth):\n    with tf.variable_scope(scope) as bnscope:\n         gamma = tf.get_variable(""gamma"", shape[-1], initializer=tf.constant_initializer(1.0))\n         beta = tf.get_variable(""beta"", shape[-1], initializer=tf.constant_initializer(0.0))\n         moving_avg = tf.get_variable(""moving_avg"", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\n         moving_var = tf.get_variable(""moving_var"", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\n         bnscope.reuse_variables()\n\n\ndef BatchNorm_layer(x, scope, train, epsilon=0.001, decay=.99):\n    # Perform a batch normalization after a conv layer or a fc layer\n    # gamma: a scale factor\n    # beta: an offset\n    # epsilon: the variance epsilon - a small float number to avoid dividing by 0\n    with tf.variable_scope(scope, reuse=True):\n        with tf.variable_scope(\'BatchNorm\', reuse=True) as bnscope:\n            gamma, beta = tf.get_variable(""gamma""), tf.get_variable(""beta"")\n            moving_avg, moving_var = tf.get_variable(""moving_avg""), tf.get_variable(""moving_var"")\n            shape = x.get_shape().as_list()\n            control_inputs = []\n            if train:\n                avg, var = tf.nn.moments(x, range(len(shape)-1))\n                update_moving_avg = moving_averages.assign_moving_average(moving_avg, avg, decay)\n                update_moving_var = moving_averages.assign_moving_average(moving_var, var, decay)\n                control_inputs = [update_moving_avg, update_moving_var]\n            else:\n                avg = moving_avg\n                var = moving_var\n            with tf.control_dependencies(control_inputs):\n                output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\n    return output\n</code></pre>\n\n<p>The main tips with using the official implementation of batch normalization in <code>tf.contrib.layers.batch_norm</code> are: (1) set <code>is_training=True</code> for training time and <code>is_training=False</code> for validation and testing time; (2) set <code>updates_collections=None</code> to make sure that <code>moving_variance</code> and <code>moving_mean</code> are updated in place; (3) be aware and careful with the scope setting; (4) set <code>decay</code> to be a smaller value (<code>decay=0.9</code> or <code>decay=0.99</code>) than default value (default is 0.999) if your dataset is small or your total training updates/steps are not that large.</p>\n', 'IsAccepted': False, 'CreationDate': 1480485613}, {'QuestionId': 45219736, 'AnswerId': 45219860, 'URL': 'https://stackoverflow.com/questions/45219736/tensorflow-batch-normalizaiton-layers/45219860#45219860', 'QuestionTitle': 'Tensorflow - batch_normalizaiton layers', 'Answer': ""<p>In a line you're defining <code>x</code> as a placeholder</p>\n\n<pre><code>x = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth), name='x')\n</code></pre>\n\n<p>some line next, you override the <code>x</code> variable with the result of the <code>batch_normalization</code> function call</p>\n\n<pre><code>x = batch_normalization(x, training=phase)\n</code></pre>\n\n<p><code>x</code> is no longer a <code>tf.placeholder</code>, thus when you use it within the <code>feed_dict</code> you're not overriding a <code>tf.placeholder</code> value, but you're overriding the <code>tf.Tensor' generated by the</code>batch_normalization` op.</p>\n\n<p>To solve, you change the line</p>\n\n<pre><code>x = batch_normalization(x, training=phase)\n</code></pre>\n\n<p>with</p>\n\n<pre><code>x_bn = batch_normalization(x, training=phase)\n</code></pre>\n\n<p>and in the lines that follow replace <code>x</code> with <code>x_bn</code>.</p>\n\n<p>In that way the placeholder variable <code>x</code> wont'  be overridden and your code should run fine.</p>\n"", 'IsAccepted': True, 'CreationDate': 1500566517}, {'QuestionId': 40879967, 'AnswerId': 44020133, 'URL': 'https://stackoverflow.com/questions/40879967/how-to-use-batch-normalization-correctly-in-tensorflow/44020133#44020133', 'QuestionTitle': 'How to use Batch Normalization correctly in tensorflow?', 'Answer': '<p>I found the Zhongyu Kuang\'s code really useful, but I stuck on how to dynamically switch between train and test ops, i.e. how to move from a python boolean is_training to a tensorflow boolean placeholder is_training. I need this functionality to be able to test the network on the validation set during the training.</p>\n\n<p>Starting from his code and inspired by <a href=""https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-232535426"" rel=""nofollow noreferrer"">this</a>, I wrote the following code:</p>\n\n<pre><code>def batch_norm(x, scope, is_training, epsilon=0.001, decay=0.99):\n    """"""\n    Returns a batch normalization layer that automatically switch between train and test phases based on the \n    tensor is_training\n\n    Args:\n        x: input tensor\n        scope: scope name\n        is_training: boolean tensor or variable\n        epsilon: epsilon parameter - see batch_norm_layer\n        decay: epsilon parameter - see batch_norm_layer\n\n    Returns:\n        The correct batch normalization layer based on the value of is_training\n    """"""\n    assert isinstance(is_training, (ops.Tensor, variables.Variable)) and is_training.dtype == tf.bool\n\n    return tf.cond(\n        is_training,\n        lambda: batch_norm_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=True, reuse=None),\n        lambda: batch_norm_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=False, reuse=True),\n    )\n\n\ndef batch_norm_layer(x, scope, is_training, epsilon=0.001, decay=0.99, reuse=None):\n    """"""\n    Performs a batch normalization layer\n\n    Args:\n        x: input tensor\n        scope: scope name\n        is_training: python boolean value\n        epsilon: the variance epsilon - a small float number to avoid dividing by 0\n        decay: the moving average decay\n\n    Returns:\n        The ops of a batch normalization layer\n    """"""\n    with tf.variable_scope(scope, reuse=reuse):\n        shape = x.get_shape().as_list()\n        # gamma: a trainable scale factor\n        gamma = tf.get_variable(""gamma"", shape[-1], initializer=tf.constant_initializer(1.0), trainable=True)\n        # beta: a trainable shift value\n        beta = tf.get_variable(""beta"", shape[-1], initializer=tf.constant_initializer(0.0), trainable=True)\n        moving_avg = tf.get_variable(""moving_avg"", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\n        moving_var = tf.get_variable(""moving_var"", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\n        if is_training:\n            # tf.nn.moments == Calculate the mean and the variance of the tensor x\n            avg, var = tf.nn.moments(x, range(len(shape)-1))\n            update_moving_avg = moving_averages.assign_moving_average(moving_avg, avg, decay)\n            update_moving_var = moving_averages.assign_moving_average(moving_var, var, decay)\n            control_inputs = [update_moving_avg, update_moving_var]\n        else:\n            avg = moving_avg\n            var = moving_var\n            control_inputs = []\n        with tf.control_dependencies(control_inputs):\n            output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\n\n    return output\n</code></pre>\n\n<p>Then I use the batch_norm layer in this way:</p>\n\n<pre><code>fc1_weights = tf.Variable(...)\nfc1 = tf.matmul(x, fc1_weights)\nfc1 = batch_norm(fc1, \'fc1_bn\', is_training=is_training)\nfc1 = tf.nn.relu(fc1)\n</code></pre>\n\n<p>Where is_training is a boolean placeholder. Note that the bias addition is not needed because is replaced by the beta parameter as explained in the <a href=""https://arxiv.org/abs/1502.03167"" rel=""nofollow noreferrer"">Batch Normalization paper</a>.</p>\n\n<p>During execution:</p>\n\n<pre><code># Training phase\nsess.run(loss, feed_dict={x: bx, y: by, is_training: True})\n\n# Testing phase\nsess.run(loss, feed_dict={x: bx, y: by, is_training: False})\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1495011558}, {'QuestionId': 38321409, 'AnswerId': 43782711, 'URL': 'https://stackoverflow.com/questions/38321409/how-does-one-use-the-official-batch-normalization-layer-in-tensorflow/43782711#43782711', 'QuestionTitle': 'How does one use the official Batch Normalization layer in TensorFlow?', 'Answer': '<p>I am not sure if this will solve your problem, the documentation for BatchNorm is not quite easy-to-use/informative, so here is a short recap on how to use simple BatchNorm:</p>\n\n<p>First of all, you define your BatchNorm layer. If you want to use it after an affine/fully-connected layer, you do this (just an example, order can be different/as you desire):</p>\n\n<pre><code>...\ninputs = tf.matmul(inputs, W) + b\ninputs = tf.layers.batch_normalization(inputs, training=is_training)\ninputs = tf.nn.relu(inputs)\n...\n</code></pre>\n\n<p>The function <code>tf.layers.batch_normalization</code> calls variable-initializers. These are internal-variables and need a special scope to be called, which is in the <code>tf.GraphKeys.UPDATE_OPS</code>. As such, you must call your optimizer function as follows (after all layers have been defined!):</p>\n\n<pre><code>...\nextra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(extra_update_ops):\n    trainer = tf.train.AdamOptimizer() \n    updateModel = trainer.minimize(loss, global_step=global_step)\n...\n</code></pre>\n\n<p>You can read more about it <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/batch_normalization"" rel=""noreferrer"">here</a>. I know it\'s a little late to answer your question, but it might help other people coming across BatchNorm problems in tensorflow! :)</p>\n', 'IsAccepted': False, 'CreationDate': 1493900302}, {'QuestionId': 40166236, 'AnswerId': 40295580, 'URL': 'https://stackoverflow.com/questions/40166236/tensorflow-batch-normalization/40295580#40295580', 'QuestionTitle': 'Tensorflow batch normalization', 'Answer': '<p>In your conv_net function, you didn\'t set the ""reuse"" parameter for the <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/state_ops.html#variable_scope"" rel=""nofollow"">tf.variable_scope()</a>. The default setting for ""reuse"" is ""None"". Every time conv2d function is called, ""bn_pop_mean"" and ""bn_pop_var"" are re-initalized. </p>\n', 'IsAccepted': False, 'CreationDate': 1477609813}, {'QuestionId': 33949786, 'AnswerId': 39128711, 'URL': 'https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/39128711#39128711', 'QuestionTitle': 'How could I use batch normalization in TensorFlow?', 'Answer': '<p>You can simply use the build-in batch_norm layer:</p>\n\n<pre><code>batch_norm = tf.cond(is_train, \n    lambda: tf.contrib.layers.batch_norm(prev, activation_fn=tf.nn.relu, is_training=True, reuse=None),\n    lambda: tf.contrib.layers.batch_norm(prev, activation_fn =tf.nn.relu, is_training=False, reuse=True))\n</code></pre>\n\n<p>where prev is the output of your previous layer (can be both fully-connected or a convolutional layer) and is_train is a boolean placeholder. Just use batch_norm as the input to the next layer, then.</p>\n', 'IsAccepted': False, 'CreationDate': 1472056512}, {'QuestionId': 33949786, 'AnswerId': 33950177, 'URL': 'https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#33950177', 'QuestionTitle': 'How could I use batch normalization in TensorFlow?', 'Answer': '<p><strong>Update July 2016</strong>  The easiest way to use batch normalization in TensorFlow is through the higher-level interfaces provided in either <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py"" rel=""noreferrer"">contrib/layers</a>, <a href=""http://tflearn.org/layers/normalization/"" rel=""noreferrer"">tflearn</a>, or <a href=""https://github.com/tensorflow/models/blob/master/inception/inception/slim/ops.py"" rel=""noreferrer"">slim</a>.</p>\n\n<p><strong>Previous answer if you want to DIY</strong>:\nThe documentation string for this has improved since the release - see the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L65"" rel=""noreferrer"">docs comment in the master branch</a> instead of the one you found.  It clarifies, in particular, that it\'s the output from <code>tf.nn.moments</code>.</p>\n\n<p>You can see a very simple example of its use in the <a href=""https://github.com/tensorflow/tensorflow/blob/3972c791b9f4d9a61b9ad6399b481df396f359ff/tensorflow/python/ops/nn_test.py#L518"" rel=""noreferrer"">batch_norm test code</a>.  For a more real-world use example, I\'ve included below the helper class and use notes that I scribbled up for my own use (no warranty provided!):</p>\n\n<pre class=""lang-py prettyprint-override""><code>""""""A helper class for managing batch normalization state.                   \n\nThis class is designed to simplify adding batch normalization               \n(http://arxiv.org/pdf/1502.03167v3.pdf) to your model by                    \nmanaging the state variables associated with it.                            \n\nImportant use note:  The function get_assigner() returns                    \nan op that must be executed to save the updated state.                      \nA suggested way to do this is to make execution of the                      \nmodel optimizer force it, e.g., by:                                         \n\n  update_assignments = tf.group(bn1.get_assigner(),                         \n                                bn2.get_assigner())                         \n  with tf.control_dependencies([optimizer]):                                \n    optimizer = tf.group(update_assignments)                                \n\n""""""\n\nimport tensorflow as tf\n\n\nclass ConvolutionalBatchNormalizer(object):\n  """"""Helper class that groups the normalization logic and variables.        \n\n  Use:                                                                      \n      ewma = tf.train.ExponentialMovingAverage(decay=0.99)                  \n      bn = ConvolutionalBatchNormalizer(depth, 0.001, ewma, True)           \n      update_assignments = bn.get_assigner()                                \n      x = bn.normalize(y, train=training?)                                  \n      (the output x will be batch-normalized).                              \n  """"""\n\n  def __init__(self, depth, epsilon, ewma_trainer, scale_after_norm):\n    self.mean = tf.Variable(tf.constant(0.0, shape=[depth]),\n                            trainable=False)\n    self.variance = tf.Variable(tf.constant(1.0, shape=[depth]),\n                                trainable=False)\n    self.beta = tf.Variable(tf.constant(0.0, shape=[depth]))\n    self.gamma = tf.Variable(tf.constant(1.0, shape=[depth]))\n    self.ewma_trainer = ewma_trainer\n    self.epsilon = epsilon\n    self.scale_after_norm = scale_after_norm\n\n  def get_assigner(self):\n    """"""Returns an EWMA apply op that must be invoked after optimization.""""""\n    return self.ewma_trainer.apply([self.mean, self.variance])\n\n  def normalize(self, x, train=True):\n    """"""Returns a batch-normalized version of x.""""""\n    if train:\n      mean, variance = tf.nn.moments(x, [0, 1, 2])\n      assign_mean = self.mean.assign(mean)\n      assign_variance = self.variance.assign(variance)\n      with tf.control_dependencies([assign_mean, assign_variance]):\n        return tf.nn.batch_norm_with_global_normalization(\n            x, mean, variance, self.beta, self.gamma,\n            self.epsilon, self.scale_after_norm)\n    else:\n      mean = self.ewma_trainer.average(self.mean)\n      variance = self.ewma_trainer.average(self.variance)\n      local_beta = tf.identity(self.beta)\n      local_gamma = tf.identity(self.gamma)\n      return tf.nn.batch_norm_with_global_normalization(\n          x, mean, variance, local_beta, local_gamma,\n          self.epsilon, self.scale_after_norm)\n</code></pre>\n\n<p>Note that I called it a <code>ConvolutionalBatchNormalizer</code> because it pins the use of <code>tf.nn.moments</code> to sum across axes 0, 1, and 2, whereas for non-convolutional use you might only want axis 0.</p>\n\n<p>Feedback appreciated if you use it.</p>\n', 'IsAccepted': True, 'CreationDate': 1448597771}, {'QuestionId': 33949786, 'AnswerId': 38325288, 'URL': 'https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/38325288#38325288', 'QuestionTitle': 'How could I use batch normalization in TensorFlow?', 'Answer': '<p>Using TensorFlow built-in batch_norm layer, below is the code to load data, build a network with one hidden ReLU layer and L2 normalization and introduce batch normalization for both hidden and out layer. This runs fine and trains fine. Just FYI this example is mostly built upon the data and code from Udacity DeepLearning course.\nP.S. Yes, parts of it were discussed one way or another in answers earlier but I decided to gather in one code snippet everything so that you have example of whole network training process with Batch Normalization and its evaluation</p>\n\n<pre><code># These are all the modules we\'ll be using later. Make sure you can import them\n# before proceeding further.\nfrom __future__ import print_function\nimport numpy as np\nimport tensorflow as tf\nfrom six.moves import cPickle as pickle\n\npickle_file = \'/home/maxkhk/Documents/Udacity/DeepLearningCourse/SourceCode/tensorflow/examples/udacity/notMNIST.pickle\'\n\nwith open(pickle_file, \'rb\') as f:\n  save = pickle.load(f)\n  train_dataset = save[\'train_dataset\']\n  train_labels = save[\'train_labels\']\n  valid_dataset = save[\'valid_dataset\']\n  valid_labels = save[\'valid_labels\']\n  test_dataset = save[\'test_dataset\']\n  test_labels = save[\'test_labels\']\n  del save  # hint to help gc free up memory\n  print(\'Training set\', train_dataset.shape, train_labels.shape)\n  print(\'Validation set\', valid_dataset.shape, valid_labels.shape)\n  print(\'Test set\', test_dataset.shape, test_labels.shape)\n\nimage_size = 28\nnum_labels = 10\n\ndef reformat(dataset, labels):\n  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n  return dataset, labels\ntrain_dataset, train_labels = reformat(train_dataset, train_labels)\nvalid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\ntest_dataset, test_labels = reformat(test_dataset, test_labels)\nprint(\'Training set\', train_dataset.shape, train_labels.shape)\nprint(\'Validation set\', valid_dataset.shape, valid_labels.shape)\nprint(\'Test set\', test_dataset.shape, test_labels.shape)\n\n\ndef accuracy(predictions, labels):\n  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n          / predictions.shape[0])\n\n\n#for NeuralNetwork model code is below\n#We will use SGD for training to save our time. Code is from Assignment 2\n#beta is the new parameter - controls level of regularization.\n#Feel free to play with it - the best one I found is 0.001\n#notice, we introduce L2 for both biases and weights of all layers\n\nbatch_size = 128\nbeta = 0.001\n\n#building tensorflow graph\ngraph = tf.Graph()\nwith graph.as_default():\n      # Input data. For the training data, we use a placeholder that will be fed\n  # at run time with a training minibatch.\n  tf_train_dataset = tf.placeholder(tf.float32,\n                                    shape=(batch_size, image_size * image_size))\n  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n  tf_valid_dataset = tf.constant(valid_dataset)\n  tf_test_dataset = tf.constant(test_dataset)\n\n  #introduce batchnorm\n  tf_train_dataset_bn = tf.contrib.layers.batch_norm(tf_train_dataset)\n\n\n  #now let\'s build our new hidden layer\n  #that\'s how many hidden neurons we want\n  num_hidden_neurons = 1024\n  #its weights\n  hidden_weights = tf.Variable(\n    tf.truncated_normal([image_size * image_size, num_hidden_neurons]))\n  hidden_biases = tf.Variable(tf.zeros([num_hidden_neurons]))\n\n  #now the layer itself. It multiplies data by weights, adds biases\n  #and takes ReLU over result\n  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset_bn, hidden_weights) + hidden_biases)\n\n  #adding the batch normalization layerhi()\n  hidden_layer_bn = tf.contrib.layers.batch_norm(hidden_layer)\n\n  #time to go for output linear layer\n  #out weights connect hidden neurons to output labels\n  #biases are added to output labels  \n  out_weights = tf.Variable(\n    tf.truncated_normal([num_hidden_neurons, num_labels]))  \n\n  out_biases = tf.Variable(tf.zeros([num_labels]))  \n\n  #compute output  \n  out_layer = tf.matmul(hidden_layer_bn,out_weights) + out_biases\n  #our real output is a softmax of prior result\n  #and we also compute its cross-entropy to get our loss\n  #Notice - we introduce our L2 here\n  loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    out_layer, tf_train_labels) +\n    beta*tf.nn.l2_loss(hidden_weights) +\n    beta*tf.nn.l2_loss(hidden_biases) +\n    beta*tf.nn.l2_loss(out_weights) +\n    beta*tf.nn.l2_loss(out_biases)))\n\n  #now we just minimize this loss to actually train the network\n  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n  #nice, now let\'s calculate the predictions on each dataset for evaluating the\n  #performance so far\n  # Predictions for the training, validation, and test data.\n  train_prediction = tf.nn.softmax(out_layer)\n  valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n  valid_prediction = tf.nn.softmax( tf.matmul(valid_relu, out_weights) + out_biases) \n\n  test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)\n  test_prediction = tf.nn.softmax(tf.matmul(test_relu, out_weights) + out_biases)\n\n\n\n#now is the actual training on the ANN we built\n#we will run it for some number of steps and evaluate the progress after \n#every 500 steps\n\n#number of steps we will train our ANN\nnum_steps = 3001\n\n#actual training\nwith tf.Session(graph=graph) as session:\n  tf.initialize_all_variables().run()\n  print(""Initialized"")\n  for step in range(num_steps):\n    # Pick an offset within the training data, which has been randomized.\n    # Note: we could use better randomization across epochs.\n    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n    # Generate a minibatch.\n    batch_data = train_dataset[offset:(offset + batch_size), :]\n    batch_labels = train_labels[offset:(offset + batch_size), :]\n    # Prepare a dictionary telling the session where to feed the minibatch.\n    # The key of the dictionary is the placeholder node of the graph to be fed,\n    # and the value is the numpy array to feed to it.\n    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n    _, l, predictions = session.run(\n      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n    if (step % 500 == 0):\n      print(""Minibatch loss at step %d: %f"" % (step, l))\n      print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))\n      print(""Validation accuracy: %.1f%%"" % accuracy(\n        valid_prediction.eval(), valid_labels))\n      print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1468316706}, {'QuestionId': 33949786, 'AnswerId': 34634291, 'URL': 'https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/34634291#34634291', 'QuestionTitle': 'How could I use batch normalization in TensorFlow?', 'Answer': '<p>The following works fine for me, it does not require invoking EMA-apply outside.</p>\n\n<pre class=""lang-python prettyprint-override""><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python import control_flow_ops\n\ndef batch_norm(x, n_out, phase_train, scope=\'bn\'):\n    """"""\n    Batch normalization on convolutional maps.\n    Args:\n        x:           Tensor, 4D BHWD input maps\n        n_out:       integer, depth of input maps\n        phase_train: boolean tf.Varialbe, true indicates training phase\n        scope:       string, variable scope\n    Return:\n        normed:      batch-normalized maps\n    """"""\n    with tf.variable_scope(scope):\n        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n                                     name=\'beta\', trainable=True)\n        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n                                      name=\'gamma\', trainable=True)\n        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(phase_train,\n                            mean_var_with_update,\n                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n    return normed\n</code></pre>\n\n<p>Example:</p>\n\n<pre class=""lang-python prettyprint-override""><code>import math\n\nn_in, n_out = 3, 16\nksize = 3\nstride = 1\nphase_train = tf.placeholder(tf.bool, name=\'phase_train\')\ninput_image = tf.placeholder(tf.float32, name=\'input_image\')\nkernel = tf.Variable(tf.truncated_normal([ksize, ksize, n_in, n_out],\n                                   stddev=math.sqrt(2.0/(ksize*ksize*n_out))),\n                                   name=\'kernel\')\nconv = tf.nn.conv2d(input_image, kernel, [1,stride,stride,1], padding=\'SAME\')\nconv_bn = batch_norm(conv, n_out, phase_train)\nrelu = tf.nn.relu(conv_bn)\n\nwith tf.Session() as session:\n    session.run(tf.initialize_all_variables())\n    for i in range(20):\n        test_image = np.random.rand(4,32,32,3)\n        sess_outputs = session.run([relu],\n          {input_image.name: test_image, phase_train.name: True})\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1452086801}, {'QuestionId': 33949786, 'AnswerId': 36525884, 'URL': 'https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/36525884#36525884', 'QuestionTitle': 'How could I use batch normalization in TensorFlow?', 'Answer': ""<p>So a simple example of the use of this batchnorm class:</p>\n\n<pre><code>from bn_class import *\n\nwith tf.name_scope('Batch_norm_conv1') as scope:\n    ewma = tf.train.ExponentialMovingAverage(decay=0.99)                  \n    bn_conv1 = ConvolutionalBatchNormalizer(num_filt_1, 0.001, ewma, True)           \n    update_assignments = bn_conv1.get_assigner() \n    a_conv1 = bn_conv1.normalize(a_conv1, train=bn_train) \n    h_conv1 = tf.nn.relu(a_conv1)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1460263427}, {'QuestionId': 33992029, 'AnswerId': 34006997, 'URL': 'https://stackoverflow.com/questions/33992029/implementing-batch-normalization-with-tensorflow/34006997#34006997', 'QuestionTitle': 'Implementing batch normalization with tensorflow', 'Answer': '<p>Rafal\'s comment gets at the core of the problem:  You\'re not running the assign nodes.  You might try using the batchnorm helper I posted in another answer - <a href=""https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177?noredirect=1#comment55758348_33950177"">How could I use Batch Normalization in TensorFlow?</a> - or you can force the assign to happen by adding with_dependencies, as he suggests.</p>\n\n<p>The general principle is that you should only count on a node being run if data or control dependencies flow ""through"" it.  <code>with_dependencies</code> ensures that before the output op is used, the specified dependencies will have completed.</p>\n', 'IsAccepted': True, 'CreationDate': 1448913906}]","{49701918, 50209310}","[""<p><strong>The batch norm has two phases:</strong></p>\n\n<pre><code>1. Training:\n   -  Normalize layer activations using `moving_avg`, `moving_var`, `beta` and `gamma` \n     (`training`* should be `True`.)\n   -  update the `moving_avg` and `moving_var` statistics. \n     (`trainable` should be `True`)\n2. Inference:\n   -  Normalize layer activations using `beta` and `gamma`.\n      (`training` should be `False`)\n</code></pre>\n\n<p><strong>Example code to illustrate few cases:</strong></p>\n\n<pre><code>#random image\nimg = np.random.randint(0,10,(2,2,4)).astype(np.float32)\n\n# batch norm params initialized\nbeta = np.ones((4)).astype(np.float32)*1 # all ones \ngamma = np.ones((4)).astype(np.float32)*2 # all twos\nmoving_mean = np.zeros((4)).astype(np.float32) # all zeros\nmoving_var = np.ones((4)).astype(np.float32) # all ones\n\n#Placeholders for input image\n_input = tf.placeholder(tf.float32, shape=(1,2,2,4), name='input')\n\n#batch Norm\nout = tf.layers.batch_normalization(\n       _input,\n       beta_initializer=tf.constant_initializer(beta),\n       gamma_initializer=tf.constant_initializer(gamma),\n       moving_mean_initializer=tf.constant_initializer(moving_mean),\n       moving_variance_initializer=tf.constant_initializer(moving_var),\n       training=False, trainable=False)\n\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\ninit_op = tf.global_variables_initializer()\n\n ## 2. Run the graph in a session \n\n with tf.Session() as sess:\n\n    # init the variables   \n    sess.run(init_op)\n\n    for i in range(2):\n        ops, o = sess.run([update_ops, out], feed_dict={_input: np.expand_dims(img, 0)})\n        print('beta', sess.run('batch_normalization/beta:0'))\n        print('gamma', sess.run('batch_normalization/gamma:0'))\n        print('moving_avg',sess.run('batch_normalization/moving_mean:0'))\n        print('moving_variance',sess.run('batch_normalization/moving_variance:0'))\n        print('out', np.round(o))\n        print('')\n</code></pre>\n\n<p><em>When <code>training=False</code> and <code>trainable=False</code>:</em></p>\n\n<pre><code>  img = [[[4., 5., 9., 0.]...\n  out = [[ 9. 11. 19.  1.]... \n  The activation is scaled/shifted using gamma and beta.\n</code></pre>\n\n<p><em>When <code>training=True</code> and <code>trainable=False</code>:</em></p>\n\n<pre><code>  out = [[ 2.  2.  3. -1.] ...\n  The activation is normalized using `moving_avg`, `moving_var`, `gamma` and `beta`. \n  The averages are not updated.\n</code></pre>\n\n<p><em>When <code>traning=True</code> and <code>trainable=True</code>:</em></p>\n\n<pre><code>  The out is same as above, but the `moving_avg` and `moving_var` gets updated to new values.\n\n  moving_avg [0.03249997 0.03499997 0.06499994 0.02749997]\n  moving_variance [1.0791667 1.1266665 1.0999999 1.0925]\n</code></pre>\n"", '<p>Q1) Initializing gamma as 1, beta as 0 means directly using the normalized inputs. Since there is no prior information about what the variance of a layer output should be, it is fair enough to assume standard Gaussian.</p>\n\n<p>Q2) During training phase (<code>training=True</code>), the batch is normalized with their own mean and var, assuming that training data are randomly sampled. During test (<code>training=False</code>), since the test data could be arbitrarily sampled, we cannot use their mean and var. Thus, we use, as you said, the moving averaging estimations from the last ""100"" training iterations.</p>\n\n<p>Q3) Yes, trainable refers to <code>beta</code> and <code>gamma</code>. There are cases to set <code>trainable=False</code>, e.g. if a novel method is used to update the parameters, or if the batch_norm layer is pre-trained and needs to be frozen. </p>\n\n<p>Q4) You may have noticed <code>reuse</code> parameters in other <code>tf.layers</code> functions as well. In general, if you wanna call a layer more than once (e.g. training and validation) and you do not wanna TensorFlow to think that you are creating a new layer, you set <code>reuse=True</code>. I prefer <code>with tf.variable_scope(..., reuse=tf.AUTO_REUSE):</code> to achieve the same purpose.</p>\n\n<p>Q5) I am not sure about this one. I guess it is for users who want to design new tricks to adjust the scale and bias.</p>\n\n<p>Q6) Yes, you are right.</p>\n', 'if a novel method is used to update the parameters, or if the batch_norm layer is pre-trained and needs to be frozen. </p>\n\n<p>Q4) You may have noticed <code>reuse</code> parameters in other <code>tf.layers</code> functions as well. In general, if you wanna call a layer more than once (e.g. training and validation) and you do not wanna TensorFlow to think that you are creating a new layer, you set <code>reuse=True</code>. I prefer <code>with tf.variable_scope(..., reuse=tf.AUTO_REUSE):</code> to achieve the same purpose.</p>\n\n<p>Q5) I am not sure about this one. I guess it is for users who want to design new tricks to adjust the scale and bias.</p>\n\n<p>Q6) Yes, you are right.</p>\n']","{'https://stackoverflow.com/questions/49701918/tf-layers-batch-normalization-parameters/49704170#49704170', 'https://stackoverflow.com/questions/50209310/significance-of-trainable-and-training-flag-in-tf-layers-batch-normalization/50224517#50224517'}",,0.20186045235259006,0.07467775452775816
11,59998335,tf.cond,Documentation Replication on Other Examples,Constantly update tf.cond based on bool value,"<p>I am using <code>tf.cond</code> for controlling the flow of the Tensorflow graph. I went through the documentation and was able to implement <code>tf.cond</code> based branching successfully. But my concern is that while the graph is being loaded the value of the <code>bool</code> variable is checked and the branching decision is made at the initialization step itself. Any further changes in the <code>bool</code> is not tracked. Following is the MWE that better describes the problem:</p>

<pre class=""lang-py prettyprint-override""><code>def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    global foo
    if i &gt; 10:
        foo = False
    print(sess.run(x))    
</code></pre>

<p>This prints only <code>32</code>s. </p>

<p>I tried with <code>eager_execution</code> too with the following code:</p>

<pre class=""lang-py prettyprint-override""><code>tf.enable_eager_execution()
def funa():
    return tf.constant(32)

def funb():
    return tf.constant(21)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(x)
</code></pre>

<p>Still the same result.</p>

<p>So my question is how can I write code such that one part of the graph is chosen dynamically, based on the updates to the <code>bool</code> variable (if possible)? Thanks. I am using Tensorflow v1.14.</p>
","<p>You can make a placeholder for <code>foo</code> and feed it's value while running the session. Modified code:</p>

<pre><code>import tensorflow as tf

def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
foo_p = tf.placeholder(tf.bool)

sess = tf.Session()

x = tf.cond(foo_p, lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(sess.run(x, {foo_p:foo}))
</code></pre>
","{67690144, 65449671, 42159946, 55927951, 42066512, 66980404, 38697045, 59511572, 53213372, 56184095}","[{'QuestionId': 67690144, 'AnswerId': 67691106, 'URL': 'https://stackoverflow.com/questions/67690144/using-tf-cond-with-a-condition-comparing-two-tensors/67691106#67691106', 'QuestionTitle': 'Using tf.cond with a condition comparing two tensors', 'Answer': '<p>What you want is not <code>tf.cond</code>, but <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/where"" rel=""nofollow noreferrer""><code>tf.where(condition, x, y)</code></a>, that returns the elements, either from <code>x</code> or <code>y</code>, depending on the <code>condition</code> exactly as in <a href=""/questions/tagged/numpy"" class=""post-tag"" title=""show questions tagged &#39;numpy&#39;"" rel=""tag"">numpy</a>. (The broadcasting rules of <code>tf.where</code> in TF 1.x are a bit different from numpy, so you might prefer <code>tf.where_v2</code> if it\'s available in your tensorflow 1.x version).</p>\n<pre><code>res = tf.where(input_tensor&gt;tf.pow(delta_vec,3),\n               tf.pow(input_tensor,1.0/3.0),\n               tf.add(res2,res1))\n</code></pre>\n<p>Running inside a session with your input_tensor:</p>\n<pre><code>&gt;&gt;&gt; sess.run(res)\narray([0.58480355, 1.28057916, 1.65096362, 1.88520363, 2.00829885,\n       2.15443469])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1621956532}, {'QuestionId': 66980404, 'AnswerId': 66987188, 'URL': 'https://stackoverflow.com/questions/66980404/tensorflow-2-how-to-conditionally-update-values-directly-in-tf-variable/66987188#66987188', 'QuestionTitle': 'Tensorflow 2 - How to conditionally update values directly in tf.Variable', 'Answer': '<p>Here are a few more cases. The main takeaway is that to update the content in <code>tf.Variable</code>, we use <code>assign</code>. But note that it will assign the value eagerly.</p>\n<pre><code>x = tf.Variable(range(5), shape=(5,), name=&quot;a&quot;)\nx.numpy()\narray([0, 1, 2, 3, 4], dtype=int32)\n\nx.assign(tf.where(x &gt; 2, x, -1)).numpy()\narray([-1, -1, -1,  3,  4], dtype=int32)\n\nx.assign(range(5,10)).numpy()\narray([5, 6, 7, 8, 9], dtype=int32)\n\nx.assign(tf.where(x &lt; 8, tf.range(5), tf.where(x &gt; 8 , x, -1))).numpy()\narray([ 0,  1,  2, -1,  9], dtype=int32)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1617802350}, {'QuestionId': 66980404, 'AnswerId': 66985933, 'URL': 'https://stackoverflow.com/questions/66980404/tensorflow-2-how-to-conditionally-update-values-directly-in-tf-variable/66985933#66985933', 'QuestionTitle': 'Tensorflow 2 - How to conditionally update values directly in tf.Variable', 'Answer': '<p>To update a <code>tf.Variable</code>, you need to call <code>assign</code>. Read more in the <a href=""https://www.tensorflow.org/guide/variable"" rel=""nofollow noreferrer"">Introduction to Variables</a> guide.</p>\n<pre><code>&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.random.set_seed(0)\n&gt;&gt;&gt; x = tf.Variable(tf.random.uniform((3,4),-1,1), dtype=tf.float32)\n&gt;&gt;&gt; x\n&lt;tf.Variable \'Variable:0\' shape=(3, 4) dtype=float32, numpy=\narray([[-0.41604972, -0.5868671 ,  0.07078147,  0.12251496],\n       [-0.16665101,  0.6156559 , -0.0135498 ,  0.9962585 ],\n       [ 0.3934703 , -0.7492528 ,  0.4196334 ,  0.32483125]],\n      dtype=float32)&gt;\n&gt;&gt;&gt; x.assign(tf.where(x&gt;0,x,0))\n&lt;tf.Variable \'UnreadVariable\' shape=(3, 4) dtype=float32, numpy=\narray([[0.        , 0.        , 0.07078147, 0.12251496],\n       [0.        , 0.6156559 , 0.        , 0.9962585 ],\n       [0.3934703 , 0.        , 0.4196334 , 0.32483125]], dtype=float32)&gt;\n&gt;&gt;&gt; x\n&lt;tf.Variable \'Variable:0\' shape=(3, 4) dtype=float32, numpy=\narray([[0.        , 0.        , 0.07078147, 0.12251496],\n       [0.        , 0.6156559 , 0.        , 0.9962585 ],\n       [0.3934703 , 0.        , 0.4196334 , 0.32483125]], dtype=float32)&gt;\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1617797964}, {'QuestionId': 65449671, 'AnswerId': 65449787, 'URL': 'https://stackoverflow.com/questions/65449671/conditional-assignment-of-tf-variable-in-tensorflow-2/65449787#65449787', 'QuestionTitle': 'conditional assignment of tf.variable in Tensorflow 2', 'Answer': '<p>Sure, you can use <a href=""https://www.tensorflow.org/api_docs/python/tf/where"" rel=""nofollow noreferrer""><code>tf.where</code></a> to conditionally set values:</p>\n<pre><code>b = tf.Variable(a)\ntf.where(b &gt;= 3, 199, b)\n# &lt;tf.Tensor: shape=(6,), dtype=int64, numpy=array([  1,   2, 199, 199, 199, 199])&gt;\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1608913330}, {'QuestionId': 59511572, 'AnswerId': 59633397, 'URL': 'https://stackoverflow.com/questions/59511572/if-else-in-tf-function/59633397#59633397', 'QuestionTitle': 'If-else in @tf.function', 'Answer': ""<p>The error message is garbled by the markdown formatter, but it seems that the <code>__call__</code> function itself was not processed by AutoGraph. In the error message, converted functions are marked with an asterisk. This is a bug in the Adam optimizer. Anyway, you can annotate it directly with tf.function it will be picked up:</p>\n\n<pre><code>    @tf.function\n    def __call__(self, step):\n</code></pre>\n\n<p>That said, there are a few things in the code that AutoGraph doesn't like: <code>zip</code>, returning from a loop, chained inequalities - it's safer to use basic constructs when possible. Sadly the errors are still you get are quite a bit confusing. Rewriting it like this should work:</p>\n\n<pre><code>    @tf.function\n    def __call__(self, step):\n        if step &lt; self.endpoints[0][0]:\n            return self.endpoints[0][1]\n        else:\n            # Can't return from a loop\n            lr = self.end_learning_rate\n            # Since it needs to break based on the value of a tensor, loop\n            # needs to be a tf.while_loop\n            for pair in tf.stack([self.endpoints[:-1], self.endpoints[1:]], axis=1):\n                left, right = tf.unstack(pair)\n                l_t, l = tf.unstack(left)\n                r_t, r = tf.unstack(right)\n                # Chained inequalities not supported yet\n                if l_t &lt;= step and step &lt; r_t:\n                    alpha = float(step - l_t) / (r_t - l_t)\n                    lr = linear_interpolation(l, r, alpha)\n                    break\n\n        return lr\n</code></pre>\n\n<p>There is one last issue - <code>tf.function</code> doesn't like it when things create variables, so you need to move the creation of the layer and the optimizer outside:</p>\n\n<pre><code>lr = TFPiecewiseSchedule([[10, 1e-3], [20, 1e-4]])\nl = layers.Dense(10)\nopt = tf.keras.optimizers.Adam(lr)\n@tf.function\ndef f(x):\n  ...\n</code></pre>\n\n<p>I hope this helps!</p>\n"", 'IsAccepted': True, 'CreationDate': 1578418534}, {'QuestionId': 56184095, 'AnswerId': 56185855, 'URL': 'https://stackoverflow.com/questions/56184095/tensorflow-training-on-condition/56185855#56185855', 'QuestionTitle': 'Tensorflow - Training on condition', 'Answer': '<p>Answering the specific question first. It\'s certainly <em>possible</em> to only perform your training step based on the <code>tf.cond</code> outcome. Note that the 2nd and 3rd params are lambdas though so more something like:</p>\n\n<pre><code>train_step_new = tf.cond(my_cond, lambda: train_step, lambda: train_step_fake)\neval_losses_new = tf.cond(my_cond, lambda: eval_losses, lambda: eval_losses_fake)\n</code></pre>\n\n<p>Your instinct that this may not be the right thing to do is correct though.</p>\n\n<p>It\'s <strong>much</strong> more preferable (both in terms of efficiency and in terms of reading and reasoning about your code) to filter out the data you want to ignore before it gets to your model in the first place.</p>\n\n<p>This is something you could achieve using the <a href=""https://www.tensorflow.org/guide/datasets"" rel=""nofollow noreferrer""><code>Dataset</code> API</a>. which has a really useful <code>filter()</code> method you could use. If you are using the dataset api to read your TFRecords right now then this should be as simple as adding something along the lines of:</p>\n\n<pre><code>dataset = dataset.filter(lambda x: {whatever op you were going to use in tf.cond})\n</code></pre>\n\n<p>If you are not yet using the dataset API, now is probably the time to have a little read up on it and consider it rather than butchering the model with that <code>tf.cond()</code> to act as a filter.</p>\n', 'IsAccepted': True, 'CreationDate': 1558093900}, {'QuestionId': 55927951, 'AnswerId': 55929224, 'URL': 'https://stackoverflow.com/questions/55927951/change-values-of-a-tensorflow-tensor-based-on-condition/55929224#55929224', 'QuestionTitle': 'Change values of a tensorflow tensor based on condition', 'Answer': '<p>It is basically more or less the same in tensorflow. Showing with smaller shaped data for convenience:</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\nvalue_to_assign = tf.constant([[1., 0., 1., 0., 1.]])\nrate = tf.constant(.5)\ndummy = tf.random_normal(shape=(4, 1, 1, 5))\n# random_floats = tf.random_normal(shape=(tf.shape(dummy)[0], ))\nrandom_floats = tf.constant([0.4, 0.6, .7, .2]) # &lt;--using const values to illustrate\n\ninit_val = tf.ones((tf.shape(dummy)[0], 1, 1, tf.shape(dummy)[-1]))\nmask = tf.Variable(init_val,\n                   trainable=False)\n\nindices = tf.where(tf.equal(True, rate &gt; random_floats))\ntiled = tf.tile(value_to_assign,\n                multiples=[tf.shape(indices)[0], 1])[:, tf.newaxis, tf.newaxis, :]\nmask = tf.scatter_nd_update(mask,\n                            indices=indices,\n                            updates=tiled)\nres = mask * dummy\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(\'MASK\')\n    print(sess.run(mask))\n    print(\'DUMMY\')\n    print(sess.run(dummy))\n    print(\'RESULT\')\n    print(sess.run(res))\n</code></pre>\n\n<pre><code>MASK\n[[[[1. 0. 1. 0. 1.]]]\n\n\n [[[1. 1. 1. 1. 1.]]]\n\n\n [[[1. 1. 1. 1. 1.]]]\n\n\n [[[1. 0. 1. 0. 1.]]]]\nDUMMY\n[[[[-1.2031308  -1.6657363  -1.5552464   0.8540495   0.37618718]]]\n\n\n [[[-0.4468031   0.46417323 -0.3764856   1.1906835  -1.4670093 ]]]\n\n\n [[[ 1.2066191  -1.4767337  -0.9487017  -0.49180242 -0.33098853]]]\n\n\n [[[-0.1621628   0.61168176  0.10006899  0.7585997  -0.23903783]]]]\nRESULT\n[[[[ 1.7753109   0.         -0.5451439  -0.         -0.53782284]]]\n\n\n [[[ 0.08024058 -1.8178499   1.183356    1.0895957  -0.9272436 ]]]\n\n\n [[[-0.5266396  -2.0316153  -1.0043124  -1.1657876   0.6106227 ]]]\n\n\n [[[-0.46503183  0.          0.01983969 -0.          0.58563703]]]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1556661901}, {'QuestionId': 53213372, 'AnswerId': 53224701, 'URL': 'https://stackoverflow.com/questions/53213372/tensorflow-change-tensor-values-given-a-condition/53224701#53224701', 'QuestionTitle': 'Tensorflow, change Tensor values given a condition', 'Answer': '<p>If you just wanted to make 0 all values below <code>obj_threshold</code> you could just do:</p>\n\n<pre><code>netout = tf.where(netout &gt; obj_threshold, netout, tf.zeros_like(netout))\n</code></pre>\n\n<p>Or:</p>\n\n<pre><code>netout = netout * tf.cast(netout &gt; obj_threshold, netout.dtype)\n</code></pre>\n\n<p>However, your case is a bit more complicated, because you only want the change to affect part of the tensor. So one thing you can do is make a boolean mask that is <code>True</code> for values over <code>obj_threshold</code> or values where the last index is below 5.</p>\n\n<pre><code>mask = (netout &gt; obj_threshold) | (tf.range(tf.shape(netout)[-1]) &lt; 5)\n</code></pre>\n\n<p>Then you can use that with any of the previous methods:</p>\n\n<pre><code>netout = tf.where(mask, netout, tf.zeros_like(netout))\n\nnetout = netout * tf.cast(mask, netout.dtype)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1541762189}, {'QuestionId': 42159946, 'AnswerId': 42164494, 'URL': 'https://stackoverflow.com/questions/42159946/how-to-use-tf-cond-for-batch-processing/42164494#42164494', 'QuestionTitle': 'How to use tf.cond for batch processing', 'Answer': '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/where"" rel=""noreferrer"">tf.where</a> sounds like what you want: a vectorized selection between Tensors.</p>\n\n<p><code>tf.cond</code> is a control flow modifier: it determines which ops are <em>executed</em>, and so it\'s difficult to think of useful batch semantics.</p>\n\n<p>We can also put together a mixture of these operations: an operation which slices based on a condition and passes those slices to two branches.</p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.util import nest\n\ndef slicing_where(condition, full_input, true_branch, false_branch):\n  """"""Split `full_input` between `true_branch` and `false_branch` on `condition`.\n\n  Args:\n    condition: A boolean Tensor with shape [B_1, ..., B_N].\n    full_input: A Tensor or nested tuple of Tensors of any dtype, each with\n      shape [B_1, ..., B_N, ...], to be split between `true_branch` and\n      `false_branch` based on `condition`.\n    true_branch: A function taking a single argument, that argument having the\n      same structure and number of batch dimensions as `full_input`. Receives\n      slices of `full_input` corresponding to the True entries of\n      `condition`. Returns a Tensor or nested tuple of Tensors, each with batch\n      dimensions matching its inputs.\n    false_branch: Like `true_branch`, but receives inputs corresponding to the\n      false elements of `condition`. Returns a Tensor or nested tuple of Tensors\n      (with the same structure as the return value of `true_branch`), but with\n      batch dimensions matching its inputs.\n  Returns:\n    Interleaved outputs from `true_branch` and `false_branch`, each Tensor\n    having shape [B_1, ..., B_N, ...].\n  """"""\n  full_input_flat = nest.flatten(full_input)\n  true_indices = tf.where(condition)\n  false_indices = tf.where(tf.logical_not(condition))\n  true_branch_inputs = nest.pack_sequence_as(\n      structure=full_input,\n      flat_sequence=[tf.gather_nd(params=input_tensor, indices=true_indices)\n                     for input_tensor in full_input_flat])\n  false_branch_inputs = nest.pack_sequence_as(\n      structure=full_input,\n      flat_sequence=[tf.gather_nd(params=input_tensor, indices=false_indices)\n                     for input_tensor in full_input_flat])\n  true_outputs = true_branch(true_branch_inputs)\n  false_outputs = false_branch(false_branch_inputs)\n  nest.assert_same_structure(true_outputs, false_outputs)\n  def scatter_outputs(true_output, false_output):\n    batch_shape = tf.shape(condition)\n    scattered_shape = tf.concat(\n        [batch_shape, tf.shape(true_output)[tf.rank(batch_shape):]],\n        0)\n    true_scatter = tf.scatter_nd(\n        indices=tf.cast(true_indices, tf.int32),\n        updates=true_output,\n        shape=scattered_shape)\n    false_scatter = tf.scatter_nd(\n        indices=tf.cast(false_indices, tf.int32),\n        updates=false_output,\n        shape=scattered_shape)\n    return true_scatter + false_scatter\n  result = nest.pack_sequence_as(\n      structure=true_outputs,\n      flat_sequence=[\n          scatter_outputs(true_single_output, false_single_output)\n          for true_single_output, false_single_output\n          in zip(nest.flatten(true_outputs), nest.flatten(false_outputs))])\n  return result\n</code></pre>\n\n<p>Some examples:</p>\n\n<pre><code>vector_test = slicing_where(\n    condition=tf.equal(tf.range(10) % 2, 0),\n    full_input=tf.range(10, dtype=tf.float32),\n    true_branch=lambda x: 0.2 + x,\n    false_branch=lambda x: 0.1 + x)\n\ncross_range = (tf.range(10, dtype=tf.float32)[:, None]\n               * tf.range(10, dtype=tf.float32)[None, :])\nmatrix_test = slicing_where(\n    condition=tf.equal(tf.range(10) % 3, 0),\n    full_input=cross_range,\n    true_branch=lambda x: -x,\n    false_branch=lambda x: x + 0.1)\n\nwith tf.Session():\n  print(vector_test.eval())\n  print(matrix_test.eval())\n</code></pre>\n\n<p>Prints:</p>\n\n<pre><code>[ 0.2         1.10000002  2.20000005  3.0999999   4.19999981  5.0999999\n  6.19999981  7.0999999   8.19999981  9.10000038]\n[[  0.           0.           0.           0.           0.           0.\n    0.           0.           0.           0.        ]\n [  0.1          1.10000002   2.0999999    3.0999999    4.0999999\n    5.0999999    6.0999999    7.0999999    8.10000038   9.10000038]\n [  0.1          2.0999999    4.0999999    6.0999999    8.10000038\n   10.10000038  12.10000038  14.10000038  16.10000038  18.10000038]\n [  0.          -3.          -6.          -9.         -12.         -15.\n  -18.         -21.         -24.         -27.        ]\n [  0.1          4.0999999    8.10000038  12.10000038  16.10000038\n   20.10000038  24.10000038  28.10000038  32.09999847  36.09999847]\n [  0.1          5.0999999   10.10000038  15.10000038  20.10000038\n   25.10000038  30.10000038  35.09999847  40.09999847  45.09999847]\n [  0.          -6.         -12.         -18.         -24.         -30.\n  -36.         -42.         -48.         -54.        ]\n [  0.1          7.0999999   14.10000038  21.10000038  28.10000038\n   35.09999847  42.09999847  49.09999847  56.09999847  63.09999847]\n [  0.1          8.10000038  16.10000038  24.10000038  32.09999847\n   40.09999847  48.09999847  56.09999847  64.09999847  72.09999847]\n [  0.          -9.         -18.         -27.         -36.         -45.\n  -54.         -63.         -72.         -81.        ]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1486745092}, {'QuestionId': 42066512, 'AnswerId': 45131254, 'URL': 'https://stackoverflow.com/questions/42066512/why-does-tf-cond-recognize-tf-bool-as-python-bool-not-as-tf-bool/45131254#45131254', 'QuestionTitle': 'Why does tf.cond() recognize tf.bool as python bool not as tf.bool?', 'Answer': '<p>Try <code>tf.cast(is_training, tf.bool)</code>. That should work out.\nSo you code should become: </p>\n\n<pre><code>mean , var = tf.cond(tf.cast(is_training, tf.bool) , mean_var_update , lambda :tf.identity(shadow_mean), tf.identity(shadow_var))\n</code></pre>\n\n<p>Please let me if it works. </p>\n', 'IsAccepted': False, 'CreationDate': 1500224787}, {'QuestionId': 42066512, 'AnswerId': 42072258, 'URL': 'https://stackoverflow.com/questions/42066512/why-does-tf-cond-recognize-tf-bool-as-python-bool-not-as-tf-bool/42072258#42072258', 'QuestionTitle': 'Why does tf.cond() recognize tf.bool as python bool not as tf.bool?', 'Answer': '<p>This line looks incorrect to me:</p>\n\n<pre><code> mean , var = tf.cond(is_training , mean_var_update , lambda :tf.identity(shadow_mean), tf.identity(shadow_var))\n</code></pre>\n\n<p>In particular, <code>tf.identity(shadow_var)</code> is being treated as a fourth argument to <a href=""https://www.tensorflow.org/api_docs/python/control_flow_ops/control_flow_operations#cond"" rel=""nofollow noreferrer""><code>tf.cond()</code></a> (i.e. the <code>name</code> argument) rather than the second return value from the <code>lambda</code> (as I think you intend). Adding parentheses to create a tuple of the two <code>tf.identity()</code> tensors should remedy this problem:</p>\n\n<pre><code> mean, var = tf.cond(is_training,\n                     mean_var_update,\n                     lambda: (tf.identity(shadow_mean), tf.identity(shadow_var)))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1486397364}, {'QuestionId': 38697045, 'AnswerId': 39573566, 'URL': 'https://stackoverflow.com/questions/38697045/how-to-pass-parmeters-to-functions-inside-tf-cond-in-tensorflow/39573566#39573566', 'QuestionTitle': 'How to pass parmeters to functions inside tf.cond in Tensorflow?', 'Answer': '<p>You can pass parameters to the functions using <strong>lambda</strong> and the code is as bellows.</p>\n\n<pre><code>x = tf.placeholder(tf.float32)\ny = tf.placeholder(tf.float32)\nz = tf.placeholder(tf.float32)\n\ndef fn1(a, b):\n  return tf.mul(a, b)\n\ndef fn2(a, b):\n  return tf.add(a, b)\n\npred = tf.placeholder(tf.bool)\nresult = tf.cond(pred, lambda: fn1(x, y), lambda: fn2(y, z))\n</code></pre>\n\n<p>Then you can call it as bellowing:</p>\n\n<pre><code>with tf.Session() as sess:\n  print sess.run(result, feed_dict={x: 1, y: 2, z: 3, pred: True})\n  # The result is 2.0\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1474289181}, {'QuestionId': 38697045, 'AnswerId': 38700367, 'URL': 'https://stackoverflow.com/questions/38697045/how-to-pass-parmeters-to-functions-inside-tf-cond-in-tensorflow/38700367#38700367', 'QuestionTitle': 'How to pass parmeters to functions inside tf.cond in Tensorflow?', 'Answer': '<p>The easiest would be to define your functions in the call:</p>\n\n<pre><code>result = tf.cond(pred, lambda: tf.mul(a, b), lambda: tf.add(a, b))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1470058706}]","{37063952, 45517940}","['<p><code>tf.cond</code> is evaluated at the runtime, whereas <code>if-else</code> is evaluated at the graph construction time. </p>\n\n<p>If you want to evaluate your condition depending on the value of the tensor at the runtime, <code>tf.cond</code> is the best option.</p>\n', 'Because execution in a TensorFlow graph flows forward through the graph, all operations that you refer to in <strong>either</strong> branch must execute before the conditional is evaluated. This means that both the true and the false branches receive a control dependency on the <code>tf.assign()</code> op, and so <code>y</code> always gets set to <code>2</code>, even if pred is <code>False</code>.</p>\n<p>The solution is to create the <code>tf.assign()</code> op inside the function that defines the true branch. For example, you could structure your code as follows:</p>\n<pre class=""lang-py prettyprint-override""><code>pred = tf.placeholder(tf.bool, shape=[])\nx = tf.Variable([1])\ndef update_x_2():\n  with tf.control_dependencies([tf.assign(x, [2])]):\n    return tf.identity(x)\ny = tf.cond(pred, update_x_2, lambda: tf.identity(x))\nwith tf.Session() as session:\n  session.run(tf.initialize_all_variables())\n  print(y.eval(feed_dict={pred: False}))  # ==&gt; [1]\n  print(y.eval(feed_dict={pred: True}))   # ==&gt; [2]\n</code></pre>\n', '<p><strong>TL;DR:</strong> If you want <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/control_flow_ops.html#cond"" rel=""nofollow noreferrer""><code>tf.cond()</code></a> to perform a side effect (like an assignment) in one of the branches, you must create the op that performs the side effect <strong>inside</strong> the function that you pass to <code>tf.cond()</code>.</p>\n<p>The behavior of <code>tf.cond()</code> is a little unintuitive. Because execution in a TensorFlow graph flows forward through the graph, all operations that you refer to in <strong>either</strong> branch must execute before the conditional is evaluated. This means that both the true and the false branches receive a control dependency on the <code>tf.assign()</code> op, and so <code>y</code> always gets set to <code>2</code>, even if pred is <code>False</code>.</p>\n<p>The solution is to create the <code>tf.assign()</code> op inside the function that defines the true branch. For example, you could structure your code as follows:</p>\n<pre class=""lang-py prettyprint-override""><code>pred = tf.placeholder(tf.bool, shape=[])\nx = tf.Variable([1])\ndef update_x_2():\n  with tf.control_dependencies([tf.assign(x, [2])]):\n    return tf.identity(x)\ny = tf.cond(pred, update_x_2, lambda: tf.identity(x))\nwith tf.Session() as session:\n  session.run(tf.initialize_all_variables())\n  print(y.eval(feed_dict={pred: False}))  # ==&gt; [1]\n  print(y.eval(feed_dict={pred: True}))   # ==&gt; [2]\n</code></pre>\n']","{'https://stackoverflow.com/questions/45517940/whats-the-difference-between-tf-cond-and-if-else/45519975#45519975', 'https://stackoverflow.com/questions/37063952/confused-by-the-behavior-of-tf-cond/37064128#37064128'}",,0.19251256414958606,0.16740458810375147
12,70747499,tf.map_fn,Documentation Replication on Other Examples,Using tf.map_fn when the function has multiple outputs,"<p>I can easily use tf.map_fn when the function has one output:</p>
<pre><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    return x[0]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p><strong>output:</strong></p>
<pre><code>tf.Tensor([1. 4.], shape=(2,), dtype=float32)
</code></pre>
<p>But, when the function has two outputs:</p>
<pre><code>def my_fun(x):
    return [x[0],x[1]]

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<p>I get an error. Not sure what is going on. I read the information about tf.map_fn in here <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/map_fn</a>, but not sure how to fix this:</p>
<p>map_fn also supports functions with multi-arity inputs and outputs:</p>
<p><em>If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems).
If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures.</em></p>
<p><strong>Output:</strong></p>
<pre><code>~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    317     _pywrap_tensorflow.AssertSameStructure(nest1, nest2, check_types,
--&gt; 318                                            expand_composites)
    319   except (ValueError, TypeError) as e:

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
&lt;ipython-input-36-5b11c7fef461&gt; in &lt;module&gt;
      5     return [x[0],x[1]]
      6 
----&gt; 7 print(tf.map_fn(my_fun,tensaki))

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)
    266         back_prop=back_prop,
    267         swap_memory=swap_memory,
--&gt; 268         maximum_iterations=n)
    269     results_flat = [r.stack() for r in r_a]
    270 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
   2712                                               list(loop_vars))
   2713       while cond(*loop_vars):
-&gt; 2714         loop_vars = body(*loop_vars)
   2715         if try_to_pack and not isinstance(loop_vars, (list, _basetuple)):
   2716           packed = True

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\control_flow_ops.py in &lt;lambda&gt;(i, lv)
   2703         cond = lambda i, lv: (  # pylint: disable=g-long-lambda
   2704             math_ops.logical_and(i &lt; maximum_iterations, orig_cond(*lv)))
-&gt; 2705         body = lambda i, lv: (i + 1, orig_body(*lv))
   2706       try_to_pack = False
   2707 

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\ops\map_fn.py in compute(i, tas)
    256       packed_values = input_pack([elem_ta.read(i) for elem_ta in elems_ta])
    257       packed_fn_values = fn(packed_values)
--&gt; 258       nest.assert_same_structure(dtype or elems, packed_fn_values)
    259       flat_fn_values = output_flatten(packed_fn_values)
    260       tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_fn_values)]

~Users\user2\AppData\Roaming\Python\Python37\site-packages\tensorflow_core\python\util\nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)
    323                   &quot;Entire first structure:\n%s\n&quot;
    324                   &quot;Entire second structure:\n%s&quot;
--&gt; 325                   % (str(e), str1, str2))
    326 
    327 

ValueError: The two structures don't have the same nested structure.

First structure: type=DType str=&lt;dtype: 'float32'&gt;

Second structure: type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]

More specifically: Substructure &quot;type=list str=[&lt;tf.Tensor: id=203, shape=(), dtype=float32, numpy=1.0&gt;, &lt;tf.Tensor: id=207, shape=(), dtype=float32, numpy=2.0&gt;]&quot; is a sequence, while substructure &quot;type=DType str=&lt;dtype: 'float32'&gt;&quot; is not
Entire first structure:
.
Entire second structure:
[., .]```
</code></pre>
","<p>You should make sure you are returning a tensor. Maybe concatenate or stack the list of values:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
tensaki=tf.constant([[1., 2., 3.], [4., 5., 6.]])

def my_fun(x):
    x = tf.stack([x[0], x[1]], axis=0)
    return x

print(tf.map_fn(my_fun,tensaki))
</code></pre>
<pre><code>tf.Tensor(
[[1. 2.]
 [4. 5.]], shape=(2, 2), dtype=float32)
</code></pre>
<p>Of course, it all depends on the output you are expecting.</p>
","{45905601, 56663912, 64508203, 47984876, 42752060, 40101656, 42892347, 55024604, 45789822, 46096767}","[{'QuestionId': 42892347, 'AnswerId': 77527623, 'URL': 'https://stackoverflow.com/questions/42892347/can-i-apply-tf-map-fn-to-multiple-inputs-outputs/77527623#77527623', 'QuestionTitle': 'Can I apply tf.map_fn(...) to multiple inputs/outputs?', 'Answer': '<p>Updated answer for TFv2</p>\n<p>Given</p>\n<pre class=""lang-py prettyprint-override""><code>a = tf.constant([[1,2,3],[4,5,6]], dtype=tf.int32)\nb = tf.constant([True, False], dtype=tf.bool)\n</code></pre>\n<p>The <code>fn</code> should only take 1 arg but you can pass a tuple of <code>tf.Tensor</code> to your function and then split them inside the function like</p>\n<pre class=""lang-py prettyprint-override""><code>def fn(a_and_b):\n    a, b = a_and_b\n    # then do more stuff on a and b\n    return (a, b)\n</code></pre>\n<p>Then call <code>tf.map_fn</code> by</p>\n<pre class=""lang-py prettyprint-override""><code>c = tf.map_fn(\n    fn,\n    elems=(a, b),\n    fn_output_signature=(\n        tf.TensorSpec(shape=[3], dtype=tf.int32), \n        tf.TensorSpec(shape=[], dtype=tf.bool), \n    ),\n)\n</code></pre>\n<p>This gives</p>\n<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; c\n(&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n array([[1, 2, 3],\n        [4, 5, 6]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])&gt;)\n</code></pre>\n<p>Note <code>dtype</code> argument is deprecated for <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer""><code>tf.map_fn</code></a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1700633009}, {'QuestionId': 42892347, 'AnswerId': 75549093, 'URL': 'https://stackoverflow.com/questions/42892347/can-i-apply-tf-map-fn-to-multiple-inputs-outputs/75549093#75549093', 'QuestionTitle': 'Can I apply tf.map_fn(...) to multiple inputs/outputs?', 'Answer': '<p>Both of these are a little messy, you can define the stack &quot;type&quot; explicitly with a custom <a href=""https://www.tensorflow.org/guide/extension_type"" rel=""nofollow noreferrer"">Extension Type</a>. Here\'s your example in a hopefully more readable form:</p>\n<pre class=""lang-py prettyprint-override""><code>class BoolAndVector(tf.experimental.BatchableExtensionType):\n    &quot;&quot;&quot;A collection bools and vectors.&quot;&quot;&quot;\n    bool: tf.Tensor\n    vector: tf.Tensor\n\n@tf.function\ndef fn(bool_and_vector):\n   return BoolAndVector(bool=bool_and_vector.bool,\n                         vector=[bool_and_vector.vector[0]])\n\nc = tf.map_fn(fn, BoolAndVector(bool=b, vector=a))\n</code></pre>\n<p>You might have to explicitly state the signature type, but the errors will guide you to what TF wants.</p>\n<p>Note, you\'ll need to use <a href=""https://www.tensorflow.org/api_docs/python/tf/experimental/BatchableExtensionType"" rel=""nofollow noreferrer"">BatchableExtensionType</a> to make this usable with map.</p>\n', 'IsAccepted': False, 'CreationDate': 1677178064}, {'QuestionId': 42892347, 'AnswerId': 73764425, 'URL': 'https://stackoverflow.com/questions/42892347/can-i-apply-tf-map-fn-to-multiple-inputs-outputs/73764425#73764425', 'QuestionTitle': 'Can I apply tf.map_fn(...) to multiple inputs/outputs?', 'Answer': '<p>Do what is said below, but tuple unpack the values inside the <code>tf.function</code> without the lambda, as computationally expensive and lambdas do not work well with TensorFlow as tf functions.</p>\n<pre><code>@tf.function\ndef x(x):\n  x,y = tuple(x) \n  return \n\nc = tf.map_fn(, (a,b), dtype=(tf.int32, tf.bool))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1663517915}, {'QuestionId': 64508203, 'AnswerId': 64508315, 'URL': 'https://stackoverflow.com/questions/64508203/i-dont-understand-map-fn-with-two-inputs/64508315#64508315', 'QuestionTitle': 'I don&#39;t understand map_fn with two inputs', 'Answer': '<p>From the <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn#specifying_fns_output_signature"" rel=""nofollow noreferrer"">docs</a>:</p>\n<blockquote>\n<p>If fn\'s input and output signatures are different, then the output signature must be specified using fn_output_signature.</p>\n</blockquote>\n<p>Your function takes a tuple of two tensors and returns a single tensor, so you need to specify <code>fn_output_signature</code>:</p>\n<pre><code>y = tf.map_fn(func,(a,b), fn_output_signature=tf.int32)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1603494437}, {'QuestionId': 56663912, 'AnswerId': 56664161, 'URL': 'https://stackoverflow.com/questions/56663912/how-can-i-apply-tf-map-fn-using-multiple-inputs-in-tensorflow/56664161#56664161', 'QuestionTitle': 'How can I apply tf.map_fn using multiple inputs in Tensorflow', 'Answer': '<p>When you give multiple tensors to <code>tf.map_fn</code>, their elements are not passed as independent arguments to the given function, but as a tuple instead. Do this:</p>\n\n<pre><code>def testzz(inputs):\n    idxCut, data = inputs\n    v1 = tf.where(data[idxCut] &gt; data[idxCut - 1], 1, 0)\n    v2 = tf.where(data[idxCut] &gt; data[idxCut + 1], 1, 0)\n    return tf.where(v1 + v2 &gt; 1, 1, 0)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1560935825}, {'QuestionId': 55024604, 'AnswerId': 55025749, 'URL': 'https://stackoverflow.com/questions/55024604/how-to-combine-tf-map-fn-and-tf-split/55025749#55025749', 'QuestionTitle': 'How to combine tf.map_fn and tf.split', 'Answer': '<p>You can use <a href=""https://www.tensorflow.org/api_docs/python/tf/usntack"" rel=""nofollow noreferrer""><code>tf.unstack</code></a> on <code>outputs</code> to get a list of ""subtensors"", then use <a href=""https://www.tensorflow.org/api_docs/python/tf/split"" rel=""nofollow noreferrer""><code>tf.split</code></a> on each of those:</p>\n\n<pre><code>splitted_outputs = [tf.split(output, rate, axis=0) for output in tf.unstack(outputs, axis=0)]\n</code></pre>\n\n<p>Note that <a href=""https://www.tensorflow.org/api_docs/python/tf/usntack"" rel=""nofollow noreferrer""><code>tf.unstack</code></a> can only be used like that when the size of the given <code>axis</code> is known, or otherwise you would need to provide a <code>num</code> parameter.</p>\n', 'IsAccepted': True, 'CreationDate': 1551883433}, {'QuestionId': 47984876, 'AnswerId': 50680270, 'URL': 'https://stackoverflow.com/questions/47984876/tensorflow-tf-map-fn-parameters/50680270#50680270', 'QuestionTitle': 'Tensorflow tf.map_fn parameters', 'Answer': '<p>A probably prettier solution is to specify the <code>dtype</code> argument (see <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer"">documentation</a>) of <code>map_fn</code>, e.g.:</p>\n\n<pre><code>tf.map_fn(lambda x: fn(*x), elements, dtype=tf.float32)\n</code></pre>\n\n<p>if <code>fn</code> returns only one float32 value.</p>\n', 'IsAccepted': False, 'CreationDate': 1528114142}, {'QuestionId': 47984876, 'AnswerId': 48010033, 'URL': 'https://stackoverflow.com/questions/47984876/tensorflow-tf-map-fn-parameters/48010033#48010033', 'QuestionTitle': 'Tensorflow tf.map_fn parameters', 'Answer': ""<p>Turns out tf.map_fn's error messages are horribly misleading; the documentation does not mention this in detail but you need the exact number of returns on your function as arguments if you pass a tuple/list of tensors. Easiest way to do this is to return junk and then only grab the first return value.</p>\n\n<pre><code>print(a.shape) #[batch, 784, 2]\nprint(b.shape) #[batch, 28, 28]\nlambdaData = (a, b)\ntestFunc = lambda x: return &lt;somethingUseful&gt;, 0\nreturnValues, _ = tf.map_fn(testFunc, lambdaData)\n</code></pre>\n\n<p>works as expected.</p>\n"", 'IsAccepted': False, 'CreationDate': 1514474650}, {'QuestionId': 45905601, 'AnswerId': 47935405, 'URL': 'https://stackoverflow.com/questions/45905601/how-does-tf-map-fn-work/47935405#47935405', 'QuestionTitle': 'How does tf.map_fn work?', 'Answer': ""<blockquote>\n  <p>For the second: I think the result is [2, -1], because the first time\n  x=np.array([1, 2, 3]) and return 1*2, the second time x=np.array([-1,\n  1, -1]) and return 1*(-1)</p>\n</blockquote>\n\n<pre><code>In [26]: a = np.array([[1, 2, 3], [2, 4, 1], [5, 1, 7]])\nIn [27]: b = np.array([[1, -1, -1], [1, 1, 1], [-1, 1, -1]])\nIn [28]: elems = (a, b)    \nIn [29]: alternate = map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\nIn [30]: alternate.eval()\nOut[30]: \narray([[ 1, -2, -3],\n       [ 2,  4,  1],\n       [-5,  1, -7]])\n</code></pre>\n\n<p>You will see that it is the tensor in 0 dimension of each element in the elems that is applied to the function. </p>\n\n<blockquote>\n  <p>For the third: I think the shape of result is (3,2), because the first\n  time x=1 and return (1,-1), the second time x=2 and return (2,-2), the\n  third time x=3 and return (3,-3).</p>\n</blockquote>\n\n<pre><code>In [36]: elems = np.array([[1, 2, 3], [4, 5, 1], [1, 6, 1]])\nIn [37]: alternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))\nIn [38]: alternates\nOut[38]: \n(&lt;tf.Tensor 'map_6/TensorArrayStack/TensorArrayGatherV3:0' shape=(3, 3) dtype=int64&gt;,\n &lt;tf.Tensor 'map_6/TensorArrayStack_1/TensorArrayGatherV3:0' shape=(3, 3) dtype=int64&gt;)\nIn [39]: alternates[0].eval()\nOut[39]: \narray([[1, 2, 3],\n       [4, 5, 1],\n       [1, 6, 1]])\nIn [40]: alternates[1].eval()\nOut[40]: \narray([[-1, -2, -3],\n       [-4, -5, -1],\n       [-1, -6, -1]])\n</code></pre>\n\n<p>To get the results you expected: </p>\n\n<pre><code>In [8]: elems = np.array([[1], [2], [3]])                                                          \nIn [9]: alternates = map_fn(lambda x: (x, -x), elems, dtype=(tf.int64, tf.int64))\nIn [10]: sess = tf.InteractiveSession()                                                            \nIn [11]: alternates[0].eval()\nOut[11]: \narray([[1],\n       [2],\n       [3]])\n\nIn [12]: alternates[1].eval()                                                                      \nOut[12]: \narray([[-1],\n       [-2],\n       [-3]])\n</code></pre>\n\n<p>May this can help you in understanding the map_fn better. </p>\n"", 'IsAccepted': False, 'CreationDate': 1513912429}, {'QuestionId': 45905601, 'AnswerId': 46154051, 'URL': 'https://stackoverflow.com/questions/45905601/how-does-tf-map-fn-work/46154051#46154051', 'QuestionTitle': 'How does tf.map_fn work?', 'Answer': '<p>Tensorflow map_fn, from the docs, </p>\n\n<blockquote>\n  <p>map on the list of tensors unpacked from elems on dimension 0.</p>\n</blockquote>\n\n<p>in this case, the only axis of the input tensor [1,2,3], or [-1,1,-1]. Operations are thus 1*-1,2*1 and 3*-1, and the results are repacked giving you the tensor shape. </p>\n', 'IsAccepted': False, 'CreationDate': 1505127447}, {'QuestionId': 46096767, 'AnswerId': 46097146, 'URL': 'https://stackoverflow.com/questions/46096767/how-to-explain-the-result-of-tf-map-fn/46097146#46097146', 'QuestionTitle': 'How to explain the result of tf.map_fn?', 'Answer': '<p>\nFirst,</p>\n\n<pre class=""lang-py prettyprint-override""><code>elems = tf.ones([1,2,3],dtype=tf.int64)\n</code></pre>\n\n<p><code>elems</code> is a 3-dimensional tensor with shape 1x2x3 full of ones, that is:</p>\n\n<pre class=""lang-py prettyprint-override""><code>[[[1, 1, 1],\n  [1, 1, 1]]]\n</code></pre>\n\n<p>Then,</p>\n\n<pre class=""lang-py prettyprint-override""><code>alternates = tf.map_fn(lambda x: (x, x, x), elems, dtype=(tf.int64, tf.int64, tf.int64))\n</code></pre>\n\n<p><code>alternates</code> is a tuple of three tensors with the same shape as <code>elems</code>, each of which is built according to the given function. Since the function simply returns a tuple repeating its input three times, that means that the three tensors will be the same as <code>elems</code>. If the function were <code>lambda x: (x, 2 * x, -x)</code> then the first output tensor would be the same as <code>elems</code>, the second would be the double of <code>elems</code> and the third one the opposite.</p>\n\n<p>In all these cases it is preferable to use regular operations instead of <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""noreferrer""><code>tf.map_fn</code></a>; however, there may be cases where you have a function accepting tensors with <em>N</em> dimensions and you have a tensor with <em>N</em> + 1 that you want to have it applied to.</p>\n\n<p><em>UPDATE:</em></p>\n\n<p>I think you are thinking of <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""noreferrer""><code>tf.map_fn</code></a> ""the other way around"", so to say. There is not a one-to-one correspondence between the number of elements or rows in the tensor and the number of outputs in the function; in fact, you could pass a function returning a tuple with as many elements as you want.</p>\n\n<p>Taking your last example:</p>\n\n<pre class=""lang-py prettyprint-override""><code>elems = tf.constant([1,2,3],dtype=tf.int64)\nalternates = tf.map_fn(lambda x: (x, 2*x, -x), elems, dtype=(tf.int64, tf.int64, tf.int64))\n</code></pre>\n\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""noreferrer""><code>tf.map_fn</code></a> first split <code>elems</code> in the first axis, that is into <code>1</code>, <code>2</code> and <code>3</code>, and applies the function to each of them, getting:</p>\n\n<pre class=""lang-py prettyprint-override""><code>(1, 2, -1)\n(2, 4, -2)\n(3, 6, -3)\n</code></pre>\n\n<p>Note that, as I said, each of these tuples could have as many elements as you wanted. Now, the final output is produced concatenating the results <strong>in the same position</strong>; so you get:</p>\n\n<pre class=""lang-py prettyprint-override""><code>[1, 2, 3]\n[2, 4, 6]\n[-1, -2, -3]\n</code></pre>\n\n<p>Again, if the function produced tuples with more elements you would get more output tensors.</p>\n\n<p><em>UPDATE 2:</em></p>\n\n<p>About your new example:</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\nimport numpy as np\n\nelems = (tf.constant([1,2,3],dtype=tf.int64),tf.constant([1,2,3],dtype=tf.int64))\nalternates = tf.map_fn(lambda x: x, elems, dtype=(tf.int64, tf.int64))\nwith tf.Session() as sess:\n    print(sess.run(alternates))\n</code></pre>\n\n<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""noreferrer"">documentation</a> says:</p>\n\n<blockquote>\n  <p>This method also allows multi-arity elems and output of fn. If elems is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of fn may match the structure of elems. That is, if elems is (t1, [t2, t3, [t4, t5]]), then an appropriate signature for fn is: fn = lambda (t1, [t2, t3, [t4, t5]]):.</p>\n</blockquote>\n\n<p>Here <code>elems</code> is a tuple of two tensors with the same size in the first dimension, as needed. <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""noreferrer""><code>tf.map_fn</code></a> takes one element of each input tensor at a time (so a tuple of two elements) and applies the given function to it, which should return the same structure that you passed in <code>dtypes</code> (a tuple of two elements, too); if you don\'t give a <code>dtypes</code>, then the expected output is the same as the input (again, a tuple of two elements, so in your case <code>dtypes</code> is optional). Anyway, it goes like this:</p>\n\n<pre class=""lang-py prettyprint-override""><code>f((1, 1)) -&gt; (1, 1)\nf((2, 2)) -&gt; (2, 2)\nf((3, 3)) -&gt; (3, 3)\n</code></pre>\n\n<p>These results are combined, concatenating all the corresponding elements in the  structure; in this case, all the numbers in the first position produce the first output and all the numbers in the second positions produce the second output. The result is, finally, the requested structure (the two-element tuple) filled with these concatenations:</p>\n\n<pre class=""lang-py prettyprint-override""><code>([1, 2, 3], [1, 2, 3])\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1504789430}, {'QuestionId': 46096767, 'AnswerId': 46097149, 'URL': 'https://stackoverflow.com/questions/46096767/how-to-explain-the-result-of-tf-map-fn/46097149#46097149', 'QuestionTitle': 'How to explain the result of tf.map_fn?', 'Answer': '\n\n<p>Your input <code>elems</code> have <strong>shape</strong> <code>(1,2,3)</code> and look like this:</p>\n\n<pre class=""lang-py prettyprint-override""><code>[[[1, 1, 1],\n [1, 1, 1]]]\n</code></pre>\n\n<p>It\'s <strong>not</strong> a matrix containing values <code>1,2,3</code>, because you create it with <code>tf.ones()</code> that makes a tensor filled with <code>1</code> with the shape you pass as parameter</p>\n\n<h2>Replying to the Update:</h2>\n\n<p><code>map_fn</code> is applied to <code>elems</code> itself.\nAccording to <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer""><code>tf.map_fn</code>\'s documentation</a>:</p>\n\n<blockquote>\n  <p>elems: A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be applied to fn. </p>\n</blockquote>\n\n<p>From what I understand there, the function expects a tensor or a <strong>list of tensors</strong> and supposedly slices it and applies the function to each element. However, from the results it seems that if you pass in a tensor that\'s the element it applies the function to directly, so <code>x</code> has shape <code>(1,2,3)</code> when the lambda function is called.\nThe function then creates a tuple with 3 copies of your <code>(1,2,3)</code> matrix (which is the <code>array(...)</code> in your output)</p>\n\n<p>Restructuring the output line and adding indent to make it more clear, the output looks as follows:</p>\n\n<pre class=""lang-py prettyprint-override""><code>( \n   array( # first copy of `x`\n       [\n           [\n               [1, 1, 1],\n               [1, 1, 1]\n           ]\n       ], dtype=int64\n   ), \n   array( # second copy of `x`\n       [\n           [\n               [1, 1, 1],\n               [1, 1, 1]\n           ]\n       ], dtype=int64\n   ), \n   array( # third copy of `x`\n       [\n           [\n               [1, 1, 1],\n               [1, 1, 1]\n           ]\n       ], dtype=int64\n   ), \n) # end of the tuple\n</code></pre>\n\n<h2>Update 2:</h2>\n\n<p><s>\nMy suspicion is that you ran into a bug. If you define elems as a list, you have the error, but if you define it as a <code>tuple</code> with <code>elems = (tf.constant([1,2,3],dtype=tf.int64))</code>, the code works as expected. Different handling of tuples and lists is very suspicious... which is why I believe it\'s a bug.</s>\nAs @mrry pointed out, in my example with the tuple I missed a comma (and thus elems was the tensor itself and not a tuple containing the tensor).</p>\n', 'IsAccepted': False, 'CreationDate': 1504789438}, {'QuestionId': 45789822, 'AnswerId': 45867685, 'URL': 'https://stackoverflow.com/questions/45789822/tensorflow-creating-variables-in-fn-of-tf-map-fn-returns-value-error/45867685#45867685', 'QuestionTitle': 'tensorflow: creating variables in fn of tf.map_fn returns value error', 'Answer': '<p>TensorFlow provides two main ways of initializing variables:</p>\n\n<ol>\n<li>""lambda"" initializers: callables that return the value of initialization. TF provides many nicely packaged <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py#L549"" rel=""noreferrer"">ones</a>.</li>\n<li>Initialization by tensor values: This is what you are using currently.</li>\n</ol>\n\n<p>The error message is stating that you need to use the first type of initializer when using variables from within a <code>while_loop</code> (which <code>map_fn</code> calls internally). (In general lambda initializers seem more robust to me.)</p>\n\n<p>Additionally in the past, <a href=""https://github.com/tensorflow/tensorflow/issues/8604"" rel=""noreferrer"">tf.get_variable seems to be preferred over tf.Variable when used from within control flow</a>.</p>\n\n<p>So, I suspect you can resolve your issue by fixing your <code>weight_bias</code> function to something like this:</p>\n\n<pre><code>def weight_bias(W_shape, b_shape, bias_init=0.1):\n  """"""Fully connected highway layer adopted from \n     https://github.com/fomorians/highway-fcn/blob/master/main.py\n  """"""\n  W = tf.get_variable(""weight"", shape=W_shape,\n          initializer=tf.truncated_normal_initializer(stddev=0.1))\n  b = tf.get_variable(""bias"", shape=b_shape,\n          initializer=tf.constant_inititializer(bias_init))\n  return W, b\n</code></pre>\n\n<p>Hope that helps!</p>\n', 'IsAccepted': True, 'CreationDate': 1503595732}, {'QuestionId': 40101656, 'AnswerId': 41661818, 'URL': 'https://stackoverflow.com/questions/40101656/tensorflow-constructing-the-params-tensor-for-tf-map-fn/41661818#41661818', 'QuestionTitle': 'Tensorflow: constructing the params tensor for tf.map_fn', 'Answer': '<p>A similar question has been asked <a href=""https://stackoverflow.com/questions/37086098/does-tensorflow-map-fn-support-taking-more-than-one-tensor"">here</a>. The reason you are getting this error is because the function called by <a href=""https://www.tensorflow.org/api_docs/python/functional_ops/higher_order_operators#map_fn"" rel=""nofollow noreferrer""><code>map_fn</code></a> - <code>lineeqn</code> in your case - is required to take exactly one tensor argument. </p>\n\n<p>Rather than a list of <em>arguments</em> to the function, the parameter <code>elems</code> is expected to be a list of <em>items</em>, where the mapped function is called for each item contained in the list.\nSo in order to take multiple arguments to your function, you would have to unpack them yourself from each item, e.g.</p>\n\n<pre><code>def lineeqn(item):\n    slope, intercept, y, x = tf.unstack(item, num=4)\n    return np.sign(y - (slope * x) - intercept)\n</code></pre>\n\n<p>and call it as</p>\n\n<pre><code>training_y = tf.map_fn(lineeqn, list_of_parameter_tensors)\n</code></pre>\n\n<p>Here, you call the line equation for each tensor in the <code>list_of_parameter_tensors</code>, where each tensor would describe a tuple <code>(slope, intercept, y, x)</code> of packed arguments.\n(Note that depending on the shape of the actual argument tensors, it might also be that instead of <a href=""https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#concat"" rel=""nofollow noreferrer""><code>tf.concat</code></a> you could have to use <a href=""https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining#pack"" rel=""nofollow noreferrer""><code>tf.pack</code></a>.)</p>\n', 'IsAccepted': False, 'CreationDate': 1484488089}, {'QuestionId': 42892347, 'AnswerId': 42892532, 'URL': 'https://stackoverflow.com/questions/42892347/can-i-apply-tf-map-fn-to-multiple-inputs-outputs/42892532#42892532', 'QuestionTitle': 'Can I apply tf.map_fn(...) to multiple inputs/outputs?', 'Answer': '<p>Figured it out. You have to define the data types for each tensor in <code>dtype</code> for each of the different tensors, then you can pass the tensors as a tuple, your map function receives a tuple of inputs, and <code>map_fn</code> returns back back a tuple.</p>\n\n<p>Example that works:</p>\n\n<pre><code>a = tf.constant([[1,2,3],[4,5,6]])\nb = tf.constant([True, False], dtype=tf.bool)\n\nc = tf.map_fn(lambda x: (x[0], x[1]), (a,b), dtype=(tf.int32, tf.bool))\n\nc[0].eval()\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)\nc[1].eval()\narray([ True, False], dtype=bool)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1489959070}, {'QuestionId': 42752060, 'AnswerId': 42815846, 'URL': 'https://stackoverflow.com/questions/42752060/tf-map-fn-and-defun-clash/42815846#42815846', 'QuestionTitle': 'tf.map_fn and Defun clash', 'Answer': '<p>Just to close the loop on this question, the <a href=""https://github.com/tensorflow/tensorflow/issues/8332"" rel=""nofollow noreferrer"">discussion on GitHub</a> revealed that the problem exists in an older version of TensorFlow (0.12 or earlier). Upgrading to TensorFlow 1.0 fixes the problem.</p>\n', 'IsAccepted': True, 'CreationDate': 1489596222}]","{72856571, 69643838}","['<p>The problem might be in the dimensions of your input. From <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn#mapping_functions_with_multi-arity_inputs_and_outputs"" rel=""nofollow noreferrer""><code>tf.map_fn</code> documentation</a>:</p>\n<blockquote>\n<p>If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems).</p>\n</blockquote>\n<p>So in your case if <code>tf_rois</code> and <code>tf_anchors</code> have different dimension (like 3 and 10), <code>fn_map</code> will fail because there are not enough elements in the smaller tensor to pair with the bigger tensor elements.</p>\n<p>If you need to run your functions against all permutations of the two tensors, you need to find those combinations and feed two equal-sized tensors as <code>elems</code> in <code>fn_map</code>.</p>\n<p>EDIT: more detail on the combinations.</p>\n<p>Instead of feeding the two raw tensors to <code>tf.map_fn</code>, you can find all the pairwise combinations, so if your tensors look like this:</p>\n<pre class=""lang-py prettyprint-override""><code>tensor_1 = [1, 2, 3]\ntensor_2 = [100, 200, 300]\n</code></pre>\n<p>The tensor of pairwise combinations will look like this:</p>\n<pre class=""lang-py prettyprint-override""><code>pairwise_combinations = [\n    [1, 100],\n    [1, 200],\n    [1, 300], \n    [2, 100],\n    [2, 200],\n    [2, 300],\n    [3, 100],\n    [3, 200],\n    [3, 300]\n]\n</code></pre>\n<p>Now you can feed columns of this tensor as equal-sized <code>elems</code> to the <code>map_fn</code>.</p>\n<p>About the actual way you can find those combinations, here is a <a href=""https://stackoverflow.com/questions/43534057/evaluate-all-pair-combinations-of-rows-of-two-tensors-in-tensorflow"">link</a> with a tensorflow example and <a href=""https://stackoverflow.com/questions/12935194/permutations-between-two-lists-of-unequal-length"">here</a> is how it can be done in python standard library. Hope it helps!</p>\n', 'From <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn#mapping_functions_with_multi-arity_inputs_and_outputs"" rel=""nofollow noreferrer""><code>tf.map_fn</code> documentation</a>:</p>\n<blockquote>\n<p>If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems).</p>\n</blockquote>\n<p>So in your case if <code>tf_rois</code> and <code>tf_anchors</code> have different dimension (like 3 and 10), <code>fn_map</code> will fail because there are not enough elements in the smaller tensor to pair with the bigger tensor elements.</p>\n<p>If you need to run your functions against all permutations of the two tensors, you need to find those combinations and feed two equal-sized tensors as <code>elems</code> in <code>fn_map</code>.</p>\n<p>EDIT: more detail on the combinations.</p>\n<p>Instead of feeding the two raw tensors to <code>tf.map_fn</code>, you can find all the pairwise combinations, so if your tensors look like this:</p>\n<pre class=""lang-py prettyprint-override""><code>tensor_1 = [1, 2, 3]\ntensor_2 = [100, 200, 300]\n</code></pre>\n<p>The tensor of pairwise combinations will look like this:</p>\n<pre class=""lang-py prettyprint-override""><code>pairwise_combinations = [\n    [1, 100],\n    [1, 200],\n    [1, 300], \n    [2, 100],\n    [2, 200],\n    [2, 300],\n    [3, 100],\n    [3, 200],\n    [3, 300]\n]\n</code></pre>\n<p>Now you can feed columns of this tensor as equal-sized <code>elems</code> to the <code>map_fn</code>.</p>\n<p>About the actual way you can find those combinations, here is a <a href=""https://stackoverflow.com/questions/43534057/evaluate-all-pair-combinations-of-rows-of-two-tensors-in-tensorflow"">link</a> with a tensorflow example and <a href=""https://stackoverflow.com/questions/12935194/permutations-between-two-lists-of-unequal-length"">here</a> is how it can be done in python standard library. Hope it helps!</p>\n', 'Defines the shape of the output tensor.</p>\n</blockquote>\n<p>So, simple scalars will not work in this case. The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer"">map function</a> of a tf.data.Dataset is always executed in Graph mode:</p>\n<blockquote>\n<p>Note that irrespective of the context in which map_func is defined\n(eager vs. graph), tf.data traces the function and executes it as a\ngraph.</p>\n</blockquote>\n']","{'https://stackoverflow.com/questions/69643838/custom-layer-with-keras-and-tensorflow/69660214#69660214', 'https://stackoverflow.com/questions/72856571/tf-reshapetensor-1-vs-tf-reshapetensor-1/72856600#72856600'}",,0.20706870871771982,0.22470839746895144
13,57449484,tf.compat.v1.layers.batch_normalization,Documentation Replicability,What is trainable parameter in tensorflow?,"<p>tf.compat.v1.layers.batch_normalization takes <code>trainable</code> as an input. The documentation says:</p>

<blockquote>
  <p>Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).</p>
</blockquote>

<p>I think only scaling factor (gamma) and offset (beta) should be added to trainable variables and I am skeptical if even moving averages will get added to GraphKeys.TRAINABLE_VARIABLES. Can somebody tell me how trainable input is influencing the behavior of batch_normalization</p>
","<p>First of all, this function is <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">deprecated</a> and should not be used.</p>

<p><code>trainable</code> arguments means that scaling factor (gamma) and offset (beta) will be trainable and it's true by default.</p>

<p>When it comes to moving averages, those <strong>are not trainable</strong>, they are only updated after each batch pass, those are not parameters (<code>tf.Variable</code> objects).</p>

<p>Please notice, you can set <code>trainable</code> to false, in such case, if <code>beta</code> and <code>gamma</code> are set to defaults (zero and one respectively), they won't affect the moving averages. You can turn them off by issuing <code>center</code> (for <code>beta</code>) or <code>scale</code> (for <code>gamma</code>).</p>
","{49325527, 40164583, 72212524, 44272114, 37350131, 47310132, 55182326, 57738199, 47312219, 44599197}","[{'QuestionId': 72212524, 'AnswerId': 72213337, 'URL': 'https://stackoverflow.com/questions/72212524/what-is-the-difference-between-the-trainable-and-training-parameters-in-tensorf/72213337#72213337', 'QuestionTitle': 'What is the difference between the .trainable and training parameters in Tensorflow', 'Answer': '<p>That is possible between the execution <em>call</em> the model or <em>predict</em> they are doing this because to flagged states to the model when training and <em>call</em>.</p>\n<p>You may read the contents they prepared and explained about weights parameters, layers, and that callable method including learning transferred. <a href=""https://www.tensorflow.org/guide/keras/transfer_learning/"" rel=""nofollow noreferrer"">Learning transfer</a></p>\n<p><strong>[ Sample ]:</strong></p>\n<pre><code>import os\nfrom os.path import exists\n\nimport tensorflow as tf\nimport tensorflow_io as tfio\n\nimport matplotlib.pyplot as plt\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n[PhysicalDevice(name=\'/physical_device:GPU:0\', device_type=\'GPU\')]\nNone\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nphysical_devices = tf.config.experimental.list_physical_devices(\'GPU\')\nassert len(physical_devices) &gt; 0, &quot;Not enough GPU hardware devices available&quot;\nconfig = tf.config.experimental.set_memory_growth(physical_devices[0], True)\nprint(physical_devices)\nprint(config)\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nVariables\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nPATH = os.path.join(\'F:\\\\datasets\\\\downloads\\\\Actors\\\\train\\\\Pikaploy\', \'*.tif\')\nPATH_2 = os.path.join(\'F:\\\\datasets\\\\downloads\\\\Actors\\\\train\\\\Candidt Kibt\', \'*.tif\')\nfiles = tf.data.Dataset.list_files(PATH)\nfiles_2 = tf.data.Dataset.list_files(PATH_2)\n\nlist_file = []\nlist_file_actual = []\nlist_label = []\nlist_label_actual = [ \'Pikaploy\', \'Pikaploy\', \'Pikaploy\', \'Pikaploy\', \'Pikaploy\', \'Candidt Kibt\', \'Candidt Kibt\', \'Candidt Kibt\', \'Candidt Kibt\', \'Candidt Kibt\' ]\nfor file in files.take(5):\n    image = tf.io.read_file( file )\n    image = tfio.experimental.image.decode_tiff(image, index=0)\n    list_file_actual.append(image)\n    image = tf.image.resize(image, [32,32], method=\'nearest\')\n    list_file.append(image)\n    list_label.append(1)\n    \nfor file in files_2.take(5):\n    image = tf.io.read_file( file )\n    image = tfio.experimental.image.decode_tiff(image, index=0)\n    list_file_actual.append(image)\n    image = tf.image.resize(image, [32,32], method=\'nearest\')\n    list_file.append(image)\n    list_label.append(9)\n\ncheckpoint_path = &quot;F:\\\\models\\\\checkpoint\\\\&quot; + os.path.basename(__file__).split(\'.\')[0] + &quot;\\\\TF_DataSets_01.h5&quot;\ncheckpoint_dir = os.path.dirname(checkpoint_path)\nloggings = &quot;F:\\\\models\\\\checkpoint\\\\&quot; + os.path.basename(__file__).split(\'.\')[0] + &quot;\\\\loggings.log&quot;\n\nif not exists(checkpoint_dir) : \n    os.mkdir(checkpoint_dir)\n    print(&quot;Create directory: &quot; + checkpoint_dir)\n    \nlog_dir = checkpoint_dir\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nDataSet\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\ndataset = tf.data.Dataset.from_tensor_slices((tf.constant(tf.cast(list_file, dtype=tf.int64), shape=(10, 1, 32, 32, 4), dtype=tf.int64),tf.constant(list_label, shape=(10, 1, 1), dtype=tf.int64)))\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n: Callback\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nclass custom_callback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if( logs[\'accuracy\'] &gt;= 0.97 ):\n            self.model.stop_training = True\n    \ncustom_callback = custom_callback()\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n: Model Initialize\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.InputLayer(input_shape=( 32, 32, 4 )),\n    tf.keras.layers.Normalization(mean=3., variance=2.),\n    tf.keras.layers.Normalization(mean=4., variance=6.),\n    # tf.keras.layers.Conv2D(32, (3, 3), activation=\'relu\'),\n    # tf.keras.layers.MaxPooling2D((2, 2)),\n    # tf.keras.layers.Dense(128, activation=\'relu\'),\n    tf.keras.layers.Reshape((128, 32)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(96, return_sequences=True, return_state=False)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(96)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(192, activation=\'relu\'),\n    tf.keras.layers.Dense(10),\n])\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n: Optimizer\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\noptimizer = tf.keras.optimizers.Nadam( learning_rate=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\'Nadam\' )\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n: Loss Fn\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;                               \n# lossfn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.AUTO, name=\'mean_squared_logarithmic_error\')\nlossfn = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=False,\n    reduction=tf.keras.losses.Reduction.AUTO,\n    name=\'sparse_categorical_crossentropy\'\n)\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n: Model Summary\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nmodel.compile(optimizer=optimizer, loss=lossfn, metrics=[\'accuracy\'])\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n: Training\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nhistory = model.fit( dataset, batch_size=100, epochs=10000, callbacks=[custom_callback] )\nmodel.save_weights(checkpoint_path)\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n: Transfer learning\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nfor layer in model.layers[:-1]:\n    layer.trainable = False\n\nmodel_transferred = tf.keras.models.Sequential([\n    model,\n    tf.keras.layers.Dense(128),\n    tf.keras.layers.Dense(10),\n])\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n: Model Summary ( 2 )\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nmodel_transferred.compile(optimizer=optimizer, loss=lossfn, metrics=[\'accuracy\'])\n\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\n: Training ( 2 )\n&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\nhistory = model_transferred.fit( dataset, batch_size=100, epochs=10000, callbacks=[custom_callback] )\n\nplt.figure(figsize=(6, 6))\nplt.title(&quot;Actors recognitions&quot;)\nfor i in range(len(list_file)):\n    img = tf.keras.preprocessing.image.array_to_img(\n        list_file[i],\n        data_format=None,\n        scale=True\n    )\n    img_array = tf.keras.preprocessing.image.img_to_array(img)\n    img_array = tf.expand_dims(img_array, 0)\n    predictions = model_transferred.predict(img_array)\n    score = tf.nn.softmax(predictions[0])\n    plt.subplot(6, 6, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(list_file_actual[i])\n    plt.xlabel(str(round(score[tf.math.argmax(score).numpy()].numpy(), 2)) + &quot;:&quot; +  str(list_label_actual[tf.math.argmax(score)]))\n    \nplt.show()\n\ninput(\'*.*\')\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1652348580}, {'QuestionId': 72212524, 'AnswerId': 72213036, 'URL': 'https://stackoverflow.com/questions/72212524/what-is-the-difference-between-the-trainable-and-training-parameters-in-tensorf/72213036#72213036', 'QuestionTitle': 'What is the difference between the .trainable and training parameters in Tensorflow', 'Answer': '<p><code>trainable</code> is property of a tensor and indicates whether this tensor can be updated by your optimizer during training.</p>\n<p><code>training</code> is a <em>flag</em> to notify the layer/model being called that the forward call is made during training. It is necessary because some layers behave differently during training and inferencing, and this flag is used for some switching logic within their <code>__call__()</code> method. A notable example is <a href=""https://keras.io/api/layers/normalization_layers/batch_normalization/"" rel=""noreferrer"">batch normalization</a>  layer.</p>\n<p>You can totally have a layer with non-<code>trainable</code> weights, yet behaves differently depending on whether it is called during <code>training</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1652347277}, {'QuestionId': 57738199, 'AnswerId': 57738629, 'URL': 'https://stackoverflow.com/questions/57738199/what-does-the-model-parameters-include/57738629#57738629', 'QuestionTitle': 'What does the `model.parameters()` include?', 'Answer': '<p>Like you wrote there, <code>model.parameters()</code> stores the weight and bias (if set to true) values of the model.\nIt is given as an argument to an optimizer to update the weight and bias values of the model with one line of code <code>optimizer.step()</code>, which you then use when next you go over your dataset.</p>\n', 'IsAccepted': False, 'CreationDate': 1567259776}, {'QuestionId': 55182326, 'AnswerId': 55185688, 'URL': 'https://stackoverflow.com/questions/55182326/understanding-resourcevariables-in-tensorflow/55185688#55185688', 'QuestionTitle': 'Understanding ResourceVariables in tensorflow', 'Answer': '<p>Two remarks:</p>\n\n<ol>\n<li><p>The order in which you write variables/ops in the first argument of <code>sess.run</code> does not mean that is the order of execution.</p></li>\n<li><p>If something worked in one step it does not mean it will work if you add loads of parallelism.</p></li>\n</ol>\n\n<p>The answer to the question:</p>\n\n<p>The key in the definition is <code>depends on</code> : <code>a read_value operation are guaranteed to see all modifications on which the read_value depends on</code>. If you look at the graph below, the add operation actually contains a <code>ReadVariableOp</code> operation for <code>b</code>, and then <code>ReadVariableOp</code> also depends on <code>AssignVariableOp</code>. Hence, <code>c</code> should take into account all modifications to <code>b</code>. </p>\n\n<p>Unless I am mixing something, but I sound convincing to myself. :)\n<a href=""https://i.stack.imgur.com/zvsou.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zvsou.png"" alt=""enter image description here""></a></p>\n\n<p>If you want to see [10.0, 5.0, 0.0] you have to add <code>tf.control_dependency</code> like below</p>\n\n<pre><code>tf.reset_default_graph()\na = tf.placeholder(dtype=tf.float32,shape=(), name=\'a\')\nd = tf.placeholder(dtype=tf.float32,shape=(), name=\'d\')\nb = tf.get_variable(name=\'b\', initializer=tf.zeros_like(d), use_resource=True)\nc=a+b\nwith tf.control_dependencies([c]):\n  b_init = tf.assign(b, d)\n\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())   \n    print(sess.run([b_init,c,b], feed_dict={a:5.,d:10.})) \n</code></pre>\n\n<p>Then the graph will change a bit\n<a href=""https://i.stack.imgur.com/z858K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z858K.png"" alt=""enter image description here""></a></p>\n', 'IsAccepted': True, 'CreationDate': 1552663029}, {'QuestionId': 47312219, 'AnswerId': 47313039, 'URL': 'https://stackoverflow.com/questions/47312219/what-is-the-definition-of-a-non-trainable-parameter/47313039#47313039', 'QuestionTitle': 'What is the definition of a non-trainable parameter?', 'Answer': '<p>In keras, <strong>non-trainable</strong> parameters (as shown in <code>model.summary()</code>) means the <strong>number of weights</strong> that are not updated during training with backpropagation.</p>\n\n<p>There are mainly two types of non-trainable weights:</p>\n\n<ul>\n<li>The ones that you have chosen to keep constant when training. This means that keras won\'t update these weights during training at all.    </li>\n<li>The ones that work like statistics in BatchNormalization layers. They\'re updated with mean and variance, but they\'re not ""trained with backpropagation"".    </li>\n</ul>\n\n<p>Weights are the values inside the network that perform the operations and can be adjusted to result in what we want. The backpropagation algorithm changes the weights towards a lower error at the end. </p>\n\n<p>By default, all weights in a keras model are trainable. </p>\n\n<p>When you create layers, internally it creates its own weights and they\'re trainable. (The backpropagation algorithm will update these weights)   </p>\n\n<p>When you make them untrainable, the algorithm will not update these weights anymore. This is useful, for instance, when you want a convolutional layer with a specific filter, like a Sobel filter, for instance. You don\'t want the training to change this operation, so these weights/filters should be kept constant.   </p>\n\n<p>There is a lot of other reasons why you might want to make weights untrainable. </p>\n\n<hr>\n\n<p><strong>Changing parameters:</strong></p>\n\n<p>For deciding whether weights are trainable or not, you take layers from the model and set <code>trainable</code>:</p>\n\n<pre><code>model.get_layer(layerName).trainable = False #or True\n</code></pre>\n\n<p>This must be done before compilation. </p>\n', 'IsAccepted': False, 'CreationDate': 1510764671}, {'QuestionId': 47312219, 'AnswerId': 52894480, 'URL': 'https://stackoverflow.com/questions/47312219/what-is-the-definition-of-a-non-trainable-parameter/52894480#52894480', 'QuestionTitle': 'What is the definition of a non-trainable parameter?', 'Answer': '<p>There are some details that other answers do not cover.</p>\n\n<p>In Keras, non-trainable parameters are the ones that <strong>are not trained using gradient descent</strong>. This is also controlled by the <code>trainable</code> parameter in each layer, for example:</p>\n\n<pre><code>from keras.layers import *\nfrom keras.models import *\nmodel = Sequential()\nmodel.add(Dense(10, trainable=False, input_shape=(100,)))\nmodel.summary()\n</code></pre>\n\n<p>This prints zero trainable parameters, and 1010 non-trainable parameters.</p>\n\n<pre><code>_________________________________________________________________    \nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 1,010\nTrainable params: 0\nNon-trainable params: 1,010\n_________________________________________________________________\n</code></pre>\n\n<p>Now if you set the layer as trainable with <code>model.layers[0].trainable = True\n</code> then it prints:</p>\n\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 1,010\nTrainable params: 1,010\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>\n\n<p>Now all parameters are trainable and there are zero non-trainable parameters. But there are also layers that have both trainable and non-trainable parameters, one example is the <code>BatchNormalization</code> layer, where the mean and standard deviation of the activations is stored for use while test time. One example:</p>\n\n<pre><code>model.add(BatchNormalization())\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 10)                1010      \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 10)                40        \n=================================================================\nTotal params: 1,050\nTrainable params: 1,030\nNon-trainable params: 20\n_________________________________________________________________\n</code></pre>\n\n<p>This specific case of BatchNormalization has 40 parameters in total, 20 trainable, and 20 non-trainable. The 20 non-trainable parameters correspond to the computed mean and standard deviation of the activations that is used during test time, and these parameters will never be trainable using gradient descent, and are not affected by the <code>trainable</code> flag.</p>\n', 'IsAccepted': False, 'CreationDate': 1539959405}, {'QuestionId': 47310132, 'AnswerId': 52808815, 'URL': 'https://stackoverflow.com/questions/47310132/number-of-cnn-learnable-parameters-python-tensorflow/52808815#52808815', 'QuestionTitle': 'Number of CNN learnable parameters - Python / TensorFlow', 'Answer': ""<p>You can do this with a simple one-liner:</p>\n\n<pre><code>np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n</code></pre>\n\n<p>If you need a little bit more details, here is a helper function I use to see all the trainable parameters:</p>\n\n<pre><code>def show_params():\n  total = 0\n  for v in tf.trainable_variables():\n    dims = v.get_shape().as_list()\n    num  = int(np.prod(dims))\n    total += num\n    print('  %s \\t\\t Num: %d \\t\\t Shape %s ' % (v.name, num, dims))\n  print('\\nTotal number of params: %d' % total)\n</code></pre>\n\n<p>It prints you information like this:</p>\n\n<pre><code>  params/weights/W1:0        Num: 34992      Shape [3, 3, 18, 216] \n  params/weights/W2:0        Num: 839808     Shape [3, 3, 216, 432] \n  params/weights/W3:0        Num: 839808     Shape [3, 3, 432, 216] \n  params/weights/W4:0        Num: 57856      Shape [226, 256] \n  params/weights/W5:0        Num: 32768      Shape [256, 128] \n  params/weights/W6:0        Num: 8192       Shape [128, 64] \n  params/weights/W7:0        Num: 64         Shape [64, 1] \n  params/biases/b1:0         Num: 216        Shape [216] \n  params/biases/b2:0         Num: 432        Shape [432] \n  params/biases/b3:0         Num: 216        Shape [216] \n  params/biases/b4:0         Num: 256        Shape [256] \n  params/biases/b5:0         Num: 128        Shape [128] \n  params/biases/b6:0         Num: 64         Shape [64] \n  params/biases/b7:0         Num: 1          Shape [1]\n\nTotal number of params: 1814801\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1539568859}, {'QuestionId': 47312219, 'AnswerId': 51729624, 'URL': 'https://stackoverflow.com/questions/47312219/what-is-the-definition-of-a-non-trainable-parameter/51729624#51729624', 'QuestionTitle': 'What is the definition of a non-trainable parameter?', 'Answer': '<p>It is clear that if you freeze any layer of the network. all params on that frozen layer turn to non-trainable. On the other hand if you design your network from the scratch, it might have some non-trainable parameters too. For instance batchnormalization layer has 4 parameter which are;</p>\n\n<p><code>[gamma weights, beta weights, moving_mean, moving_variance]</code>   </p>\n\n<p>The first two of them are trainable but last two are not. So the batch normalization layer is highly probably the reason that your custom network has non-trainable paramteres. </p>\n', 'IsAccepted': False, 'CreationDate': 1533653213}, {'QuestionId': 49325527, 'AnswerId': 49325731, 'URL': 'https://stackoverflow.com/questions/49325527/what-is-a-callable-in-tensorflow/49325731#49325731', 'QuestionTitle': 'What is a callable in Tensorflow?', 'Answer': '<p><code>tf.less</code> is an <code>Operation</code> object. To make it callable, just use a <code>lambda</code>:</p>\n\n<pre><code>tensor = tf.while_loop(lambda tensor: tf.less(tf.rank(tensor), ndims), # cond\n                       lambda tensor: tf.append(tensor, axis=axis),    # body\n                       loop_vars = [tensor])                           # loop_vars\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1521218595}, {'QuestionId': 49325527, 'AnswerId': 49325698, 'URL': 'https://stackoverflow.com/questions/49325527/what-is-a-callable-in-tensorflow/49325698#49325698', 'QuestionTitle': 'What is a callable in Tensorflow?', 'Answer': '<p>A callable is anything that can be called. <a href=""https://stackoverflow.com/questions/111234/what-is-a-callable-in-python"">See here</a>.</p>\n\n<p>The cond should be a function. You can use <code>lambda</code> (<a href=""https://stackoverflow.com/questions/890128/why-are-python-lambdas-useful"">See here</a>) to make your condition <code>callable</code>.</p>\n\n<p><a href=""https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer"">Here</a> there is a minimal example of how to use <code>tf.while_loop</code>:</p>\n\n<pre><code>i = tf.constant(0)\nc = lambda i: tf.less(i, 10)\nb = lambda i: tf.add(i, 1)\nr = tf.while_loop(c, b, [i])\n</code></pre>\n\n<p>And in the end, not a bad idea to post a minimal code that actually runs and generates your error. </p>\n', 'IsAccepted': True, 'CreationDate': 1521218466}, {'QuestionId': 37350131, 'AnswerId': 37350251, 'URL': 'https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow/37350251#37350251', 'QuestionTitle': 'What is the default variable initializer in Tensorflow?', 'Answer': '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/get_variable"" rel=""noreferrer"">From the documentation</a>:</p>\n\n<blockquote>\n  <p>If initializer is <code>None</code> (the default), the default initializer passed in the variable scope will be used. If that one is <code>None</code> too, a <code>glorot_uniform_initializer</code> will be used.</p>\n</blockquote>\n\n<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/6bfbcf31dce9a59acfcad51d905894b082989012/tensorflow/python/ops/init_ops.py#L527"" rel=""noreferrer""><code>glorot_uniform_initializer</code></a> function initializes values from a uniform distribution. </p>\n\n<p>This function is <a href=""https://www.tensorflow.org/api_docs/python/tf/glorot_uniform_initializer"" rel=""noreferrer"">documented</a> as:</p>\n\n<blockquote>\n  <p>The Glorot uniform initializer, also called Xavier uniform initializer.</p>\n  \n  <p>It draws samples from a uniform distribution within [-limit, limit],<br>\n  where <code>limit</code> is <code>sqrt(6 / (fan_in + fan_out))</code><br>\n  where <code>fan_in</code> is the number of input units in the weight tensor<br>\n  and <code>fan_out</code> is the number of output units in the weight tensor.</p>\n  \n  <p>Reference: <a href=""http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf"" rel=""noreferrer"">http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a></p>\n</blockquote>\n', 'IsAccepted': True, 'CreationDate': 1463757785}, {'QuestionId': 47312219, 'AnswerId': 47312861, 'URL': 'https://stackoverflow.com/questions/47312219/what-is-the-definition-of-a-non-trainable-parameter/47312861#47312861', 'QuestionTitle': 'What is the definition of a non-trainable parameter?', 'Answer': '<p>Non-trainable parameters are quite a broad subject. A straightforward example is to consider the case of any specific NN model and its architecture.</p>\n\n<p>Say we have already setup your network definition in Keras, and your architecture is something like <code>256-&gt;500-&gt;500-&gt;1</code>. Based on this definition, we seem to have a Regression Model (one output) with two hidden layers (500 nodes each) and an input of 256.</p>\n\n<p>One non-trainable parameters of your model is, for example, <strong>the number of hidden layers itself</strong> (2). Other could be the <strong>nodes on each hidden layer</strong> (500 in this case), or even the nodes on each individual layer, giving you one parameter per layer plus the number of layers itself.</p>\n\n<p>These parameters are ""non-trainable"" because <em>you can\'t optimize its value with your training data.</em> Training algorithms (like back-propagation) will optimize and update the <em>weights</em> of your network, which are the actual trainable parameters here (usually several thousands, depending on your connections). Your training data as it is can\'t help you determine those non-trainable parameters.</p>\n\n<p><strong>However,</strong> this does not mean that <code>numberHiddenLayers</code> is not trainable at all, it only means that in <em>this model</em> and its implementation we are unable to do so. <strong>We could make <code>numberHiddenLayers</code> trainable</strong>; the easiest way would be to define <em>another ML algorithm</em> that takes this model as input and trains it with several values of <code>numberHiddenLayers</code>. The best value is obtained with the model that outperformed the others, thus optimizing the <code>numberHiddenLayers</code> variable.</p>\n\n<p>In other words, non-trainable parameters of a model are those that you will not be updating and optimized during training, and that have to be defined <em>a priori</em>, or passed as inputs. </p>\n', 'IsAccepted': True, 'CreationDate': 1510764130}, {'QuestionId': 47310132, 'AnswerId': 47310307, 'URL': 'https://stackoverflow.com/questions/47310132/number-of-cnn-learnable-parameters-python-tensorflow/47310307#47310307', 'QuestionTitle': 'Number of CNN learnable parameters - Python / TensorFlow', 'Answer': '<p>No function I am aware of, but you can still count yourself using a for loop on the <code>tf.trainable_variables():</code></p>\n\n<pre><code>total_parameters = 0\nfor variable in tf.trainable_variables():\n    variable_parameters = 1\n    for dim in variable.get_shape():\n        variable_parameters *= dim.value\n    total_parameters += variable_parameters\n\nprint(""Total number of trainable parameters: %d"" % total_parameters)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1510757007}, {'QuestionId': 44599197, 'AnswerId': 44837644, 'URL': 'https://stackoverflow.com/questions/44599197/parameters-value-in-tensorflow/44837644#44837644', 'QuestionTitle': 'parameters value in tensorflow', 'Answer': ""<ol>\n<li>The cause of the error is that you cannot reshape a tensor with 44856 values into a tensor with 32*32*3 (=3072) values. Reshaping is an operation that simply changes the shape of the tensor without adding or removing any values. In your case, tf.strided_slice somehow produced a large tensor (with 44856 values) and you can't reshape it into a 32*32*3 tensor. I don't see how that can happen without complete code and example file.</li>\n<li>This question is beyond StackOverflow and is probably too general to have a reasonable answer.</li>\n</ol>\n\n<p>Also, I noticed that you are trying to extract the label from the first byte of your record_bytes. This seems wrong since record_bytes in your case is a decoded png, not a special cifar record that encodes the label for the image in its first byte.</p>\n"", 'IsAccepted': True, 'CreationDate': 1498789331}, {'QuestionId': 44272114, 'AnswerId': 44272469, 'URL': 'https://stackoverflow.com/questions/44272114/how-are-variables-added-to-tf-trainable-variables/44272469#44272469', 'QuestionTitle': 'How are variables added to tf.trainable_variables?', 'Answer': '<p>The <code>trainable</code> argument is passed to the <code>Variable</code> constructor, and implicitly defaults to true. Set it to false in your code to avoid training on some variables.</p>\n', 'IsAccepted': True, 'CreationDate': 1496180699}, {'QuestionId': 40164583, 'AnswerId': 40165610, 'URL': 'https://stackoverflow.com/questions/40164583/tensorflows-tensorflow-variable-scope-values-parameter-meaning/40165610#40165610', 'QuestionTitle': 'Tensorflow&#39;s tensorflow variable_scope values parameter meaning', 'Answer': ""<p>The variable_scope parameter helps ensure uniqueness of variables and reuse of variables where desired.</p>\n\n<p>Yes if you create two or more different computation graphs then they wouldn't necessarily share the same variable scope; however, there are ways to get them to be shared across graphs so the option is there.</p>\n\n<p>Primary use cases for variable scope are for RNN's where many of the weights are tied and reused.  That's one reason someone would need it.  The other main reason it's there is to ensure that you are reusing the same variables when you explicitly mean to and not by accident. (For distributed settings this can become a concern.)</p>\n"", 'IsAccepted': True, 'CreationDate': 1477002950}]","{57449484, 72212524}","['<p>First of all, this function is <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">deprecated</a> and should not be used.</p>\n\n<p><code>trainable</code> arguments means that scaling factor (gamma) and offset (beta) will be trainable and it\'s true by default.</p>\n\n<p>When it comes to moving averages, those <strong>are not trainable</strong>, they are only updated after each batch pass, those are not parameters (<code>tf.Variable</code> objects).</p>\n\n<p>Please notice, you can set <code>trainable</code> to false, in such case, if <code>beta</code> and <code>gamma</code> are set to defaults (zero and one respectively), they won\'t affect the moving averages. You can turn them off by issuing <code>center</code> (for <code>beta</code>) or <code>scale</code> (for <code>gamma</code>).</p>\n', '<p><code>trainable</code> is property of a tensor and indicates whether this tensor can be updated by your optimizer during training.</p>\n<p><code>training</code> is a <em>flag</em> to notify the layer/model being called that the forward call is made during training. It is necessary because some layers behave differently during training and inferencing, and this flag is used for some switching logic within their <code>__call__()</code> method. A notable example is <a href=""https://keras.io/api/layers/normalization_layers/batch_normalization/"" rel=""noreferrer"">batch normalization</a>  layer.</p>\n<p>You can totally have a layer with non-<code>trainable</code> weights, yet behaves differently depending on whether it is called during <code>training</code>.</p>\n']","{'https://stackoverflow.com/questions/72212524/what-is-the-difference-between-the-trainable-and-training-parameters-in-tensorf/72213036#72213036', 'https://stackoverflow.com/questions/57449484/what-is-trainable-parameter-in-tensorflow/57449660#57449660'}",{72212524},0.23533801466833226,0.08441756182087218
14,66879748,tf.keras.Model,Documentation Ambiguity,What is the difference between tf.keras.model and tf.keras.sequential?,"<p>In some <code>tf. keras</code> tutorials, I've seen them instantiated their model class like this:</p>
<p><code>model = tf.keras.Sequential()</code></p>
<p>While in some places, they use something like this:</p>
<p><code>model = tf.keras.Model(inputs=input, outputs=output)</code></p>
<p>But seeing here in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model"" rel=""noreferrer"">docs</a>, they do seem the same, but I am not sure nor is it explicitly mentioned. What are the differences between the two?</p>
","<p>There are two class API to define a model in <code>tf. keras</code>. According to the doc</p>
<ul>
<li><p><a href=""https://keras.io/api/models/sequential/"" rel=""noreferrer""><code>Sequential class</code></a>: Sequential groups a linear stack of layers into a <code>tf. keras.Model</code>.</p>
</li>
<li><p><a href=""https://keras.io/api/models/model/"" rel=""noreferrer""><code>Model class</code></a>: <code>Model</code> group's layers into an object with training and inference features.</p>
</li>
</ul>
<hr />
<p>An <code>Sequential</code> model is the simplest type of model, a linear stack of layers. But there are some flaws in using the <code>sequential</code> model API, it's limited in certain points. We can't build complex networks such as multi-input or multi-output networks using this API.</p>
<p>But using <a href=""https://keras.io/api/models/model/"" rel=""noreferrer""><code>Model class</code></a>, we can instantiate a Model with the <strong>Functional API</strong> (and also with <strong>Subclassing the Model class</strong>) that allows us to create arbitrary graphs of layers. From this, we can get more flexibility and easily define models where each layer can connect not just with the previous and next layers but also share feature information with other layers in the model, for example, model-like <code>ResNet</code>, <code>EfficientNet</code>.</p>
<p>In fact, most of the SOTA model that you can get from <code>tf.keras.applications</code> is basically implemented using the <strong>Functional API</strong>. However, in subclassing API, we define our layers in <code>__init__</code> and we implement the model's forward pass in the <code>call</code> method.</p>
<p>Generally speaking, all the model definitions using Sequential API, can be achieved in Functional API or Model Subclassing API. And in Functional API or Model Subclassing API, we can create complex layers that not possible to achieve in Sequential API. If you wondering which one to choose, the answer is, it totally depends on your need. However, check out the following blog post where we have discussed the various model strategies in <code>tf. keras</code> with more examples. <a href=""https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e"" rel=""noreferrer"">Model Sub-Classing and Custom Training Loop from Scratch in TensorFlow 2</a></p>
","{55109696, 73264482, 51972647, 62682505, 58602285, 67841486, 43268403, 55178230, 62620694, 57751417}","[{'QuestionId': 55178230, 'AnswerId': 76235995, 'URL': 'https://stackoverflow.com/questions/55178230/what-is-the-difference-between-keras-and-tf-keras/76235995#76235995', 'QuestionTitle': 'What is the difference between keras and tf.keras?', 'Answer': '<p>I run the following in my install:</p>\n<pre><code>&gt;&gt;&gt; from tensorflow.python.keras import __version__ as tf_p_k_version\n&gt;&gt;&gt; from tensorflow.keras import __version__ as tf_k_version\n&gt;&gt;&gt; from keras import __version__ as k_version\n&gt;&gt;&gt; print(tf_p_k_version, tf_k_version, k_version)\n2.6.0 2.11.0 2.11.0\n</code></pre>\n<p>Also I grepped for the docstring of the Layer class, and the one for 2.11 is not installed in tensorflow but in keras package.</p>\n<p>My guess is that tensorflow.keras actually imports keras if present, and possibly defaults to tensorflow.python.keras if it is not.</p>\n', 'IsAccepted': False, 'CreationDate': 1683892201}, {'QuestionId': 73264482, 'AnswerId': 73265008, 'URL': 'https://stackoverflow.com/questions/73264482/what-is-the-difference-between-sequential-and-modelinput-output-in-tensorf/73265008#73265008', 'QuestionTitle': 'what is the difference between Sequential and Model([input],[output]) in TensorFlow?', 'Answer': '<p>The <code>Sequence</code> version uses the <a href=""https://keras.io/guides/sequential_model/"" rel=""nofollow noreferrer"">Sequencial model</a> while the <code>Model([inputs], [outputs])</code> uses the <a href=""https://keras.io/guides/functional_api/"" rel=""nofollow noreferrer"">Functional API</a>.</p>\n<p>The first is easier to use, but only works for single-input single-output feed forward models (in the sense of Keras layers).</p>\n<p>The second is more complex but get rid of those constraints, allowing to create many more models.</p>\n<p>So, your main point is right: any sequencial model can be re-written as a functional model. You can double check this by comparing the architectures with the usage of <a href=""https://keras.io/api/models/model/"" rel=""nofollow noreferrer""><code>summary</code> function</a> and <a href=""https://keras.io/api/utils/model_plotting_utils/"" rel=""nofollow noreferrer"">plotting the models</a>.</p>\n<p>However, this only shows that architectures are the same, but not the weights!</p>\n<p>Assuming you are fitting both models with same data and same <code>compile</code> and <code>fit</code> params (by the way, include those in your question), there is lots of randomness in the training process which may lead to different results. So, try the following <strong>to compare them</strong> better:</p>\n<ul>\n<li>remove as much randomness as possible by setting seeds, in your code and for each layer instantiation.</li>\n<li>avoid using data augmentation if using it.</li>\n<li>use the same validation/train split for both models: to be sure, you can split the dataset yourself.</li>\n<li>do not use shuffling in data generators nor during the training.</li>\n</ul>\n<p><a href=""https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development"" rel=""nofollow noreferrer"">Here</a> you can read more about producing reproducible results in keras.</p>\n<p>Even after following those tips, your results may not be deterministic, and hence not the same, so finally, and maybe more important: do not compare single run: train and eval each model several times (for instance, 20) and then compare the average MAE with it\'s standard deviation.</p>\n<p>If after all this your results are still so different, please, update your question with them.</p>\n', 'IsAccepted': True, 'CreationDate': 1659848352}, {'QuestionId': 67841486, 'AnswerId': 67841638, 'URL': 'https://stackoverflow.com/questions/67841486/difference-between-implementation-of-network-by-sequential-and-directly/67841638#67841638', 'QuestionTitle': 'Difference between implementation of network by Sequential() and directly?', 'Answer': '<p>Based on the docs <a href=""https://keras.io/api/models/"" rel=""nofollow noreferrer"">here</a>:</p>\n<p>You have 3 ways to define a model:</p>\n<ul>\n<li><p>Sequential: (Your First approach) The Sequential model, which is very straightforward (a simple list of layers), <strong>but is limited to single-input, single-output stacks of layers</strong> (as the name gives away).</p>\n</li>\n<li><p>Functional API: (Your second approach) The Functional API, which is an easy-to-use, fully-featured API that <strong>supports arbitrary model architectures</strong>. For most people and most use cases, this is what you should be using.</p>\n</li>\n<li><p>Model subclassing: Where you implement everything from scratch on your own. Use this if you have complex, out-of-the-box research use cases.</p>\n</li>\n</ul>\n<p>So, because <strong>Inception Model</strong> has branches, <em>you cannot use Sequential model to implement</em>. You should either use <strong>Functional API</strong> or create a subclass from model and implement your model from scratch.</p>\n<p>Notice that even Functional API has some limitations. It is only suited to models that are directed acyclic graphs of layers e.g. MobileNet, Inception, etc. (models that have no cycle or loops). For more exotic architectures e.g. dynamic and recursive networks or architectures that may be changed on the fly you have to use Model Subclassing.</p>\n', 'IsAccepted': True, 'CreationDate': 1622827792}, {'QuestionId': 57751417, 'AnswerId': 67156874, 'URL': 'https://stackoverflow.com/questions/57751417/what-is-meant-by-sequential-model-in-keras/67156874#67156874', 'QuestionTitle': 'What is meant by sequential model in Keras', 'Answer': ""<p>As others have already mentioned that &quot;<em>The Sequential model is a linear stack of layers.</em>&quot;</p>\n<p>The Sequential model API is a way of creating deep learning models where an instance of the Sequential class is created and model layers are created and added to it.</p>\n<p>The most common method to add layers is <strong>Piecewise</strong></p>\n<pre><code>import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n#initialising the classifier\n#defining sequential i.e sequense of layers\n\n\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6,activation = 'relu'))\n#units = 6 as no. of column in X_train = 11 and y_train =1 --&gt; 11+1/2\n\n#Adding the second hidden lyer\nclassifier.add(Dense(units = 6, activation='relu'))\n\n#adding the output layer\nclassifier.add(Dense(units = 1, activation = 'sigmoid))\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1618813337}, {'QuestionId': 62620694, 'AnswerId': 62626249, 'URL': 'https://stackoverflow.com/questions/62620694/what-is-difference-between-tf-keras-models-sequential-vs-tf-keras-sequential/62626249#62626249', 'QuestionTitle': 'What is difference between tf.keras.models.sequential vs tf.keras.sequential?', 'Answer': '<pre><code>tf.keras.models.Sequential\n</code></pre>\n<p>and</p>\n<pre><code>tf.keras.Sequential\n</code></pre>\n<p>Do the same thing but they are from different versions of tensorflow. By the documentation (TensorFlow 2.0), <code>tf.keras.Sequential</code> is the most recent way of called this function.</p>\n', 'IsAccepted': True, 'CreationDate': 1593366662}, {'QuestionId': 62682505, 'AnswerId': 64285078, 'URL': 'https://stackoverflow.com/questions/62682505/tf-keras-models-model-vs-tf-keras-model/64285078#64285078', 'QuestionTitle': 'tf.keras.models.model vs tf.keras.model', 'Answer': '<p>This might be a little more intuitive and easier to read (though admittedly it depends on the accuracy of the documentation). Using the <a href=""http://This%20is%20with%20**TensorFlow%202.3.0**%20but%20should%20be%20similar%20for%20other%20prior%202.x%20versions."" rel=""nofollow noreferrer"">TensorFlow documentation</a>, you can click on &quot;view aliases&quot; as I\'ve done here in the below screenshot:</p>\n<p><a href=""https://i.stack.imgur.com/aOs8F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aOs8F.png"" alt=""enter image description here"" /></a></p>\n<p>this will show that <code>tf.keras.Model</code> has <code>tf.keras.models.Model</code> as an alias. Therefore, they point to the same thing.</p>\n<p>This is with <strong>TensorFlow 2.3.0</strong> but should be similar for other prior 2.x versions.</p>\n', 'IsAccepted': True, 'CreationDate': 1602266397}, {'QuestionId': 62620694, 'AnswerId': 63740060, 'URL': 'https://stackoverflow.com/questions/62620694/what-is-difference-between-tf-keras-models-sequential-vs-tf-keras-sequential/63740060#63740060', 'QuestionTitle': 'What is difference between tf.keras.models.sequential vs tf.keras.sequential?', 'Answer': '<pre><code>&gt;&gt;&gt; tf.keras.models.Sequential==tf.keras.Sequential\nTrue\n</code></pre>\n<p>Both are same as of TFv2. You could use the later.</p>\n<p>Added in this <a href=""https://github.com/tensorflow/tensorflow/commit/36326acf6dc625014142ad9cbef665dfaf7ecbd9#"" rel=""noreferrer"">commit</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1599217945}, {'QuestionId': 55178230, 'AnswerId': 63517100, 'URL': 'https://stackoverflow.com/questions/55178230/what-is-the-difference-between-keras-and-tf-keras/63517100#63517100', 'QuestionTitle': 'What is the difference between keras and tf.keras?', 'Answer': '<p>The history of Keras Vs tf.keras is long and twisted.</p>\n<p><strong>Keras</strong>: Keras is a high-level (easy to use) API, built by Google AI Developer/Researcher, Francois Chollet. Written in Python and capable of running on top of backend engines like TensorFlow, CNTK, or Theano.</p>\n<p><strong>TensorFlow</strong>: A library, also developed by Google, for the Deep Learning developer Community, for making deep learning applications accessible and usable to public. Open Sourced and available on GitHub.</p>\n<p>With the release of Keras v1.1.0, Tensorflow was made default backend engine. That meant: if you installed Keras on your system, you were also installing TensorFlow.</p>\n<p>Later, with TensorFlow v1.10.0, for the first time tf.keras submodule was introduced in Tensorflow. <strong>The first step in integrating Keras within TensorFlow</strong></p>\n<p>With the release of Keras 2.3.0,</p>\n<ul>\n<li>first release of Keras in sync with tf.keras</li>\n<li>Last major release to support other multi-backend engines</li>\n<li><strong>And most importantly, going forward, recommend switching the code from keras to Tensorflow2.0 and tf.keras packages.</strong></li>\n</ul>\n<p>Refer <a href=""https://twitter.com/fchollet/status/1174018651449544704?s=19"" rel=""noreferrer"">this</a> tweet from François Chollet to use tf.keras.</p>\n<p>That means,\nChange Everywhere</p>\n<p>From</p>\n<pre><code>from keras.models import Sequential\nfrom keras.models import load_model\n</code></pre>\n<p>To</p>\n<pre><code>from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import load_model\n</code></pre>\n<p>And In requirements.txt,</p>\n<pre><code>tensorflow==2.3.0\n</code></pre>\n<p>*Disclaimer: it might give conflicts if you were using an older version of Keras. Do <code>pip uninstall keras</code> in that case.</p>\n', 'IsAccepted': False, 'CreationDate': 1597988492}, {'QuestionId': 62682505, 'AnswerId': 63350695, 'URL': 'https://stackoverflow.com/questions/62682505/tf-keras-models-model-vs-tf-keras-model/63350695#63350695', 'QuestionTitle': 'tf.keras.models.model vs tf.keras.model', 'Answer': '<p>they are the same thing. all is  <code> from tensorflow.python.keras.engine.training import Model</code>\ncheckout:\n<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/__init__.py"" rel=""nofollow noreferrer"">keras.<strong>init</strong>.py</a> and\n<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/models.py%3Ekeras.models.py"" rel=""nofollow noreferrer"">keras.models.py</a></p>\n', 'IsAccepted': False, 'CreationDate': 1597113379}, {'QuestionId': 62620694, 'AnswerId': 62621688, 'URL': 'https://stackoverflow.com/questions/62620694/what-is-difference-between-tf-keras-models-sequential-vs-tf-keras-sequential/62621688#62621688', 'QuestionTitle': 'What is difference between tf.keras.models.sequential vs tf.keras.sequential?', 'Answer': '<p>Keras (keras.io) is a library which is available on its own. It specifies the high-level api.\ntf.keras (<a href=""https://www.tensorflow.org/guide/keras"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras</a>) implements the Keras API specification within TensorFlow.</p>\n<p>If you intend to stick to the Tensorflow implementation I would stick to tf.keras. Otherwise you have the advantage to be backend agnostic.</p>\n<p>=====</p>\n<p>update for updated question.</p>\n<p>The renaming of the package for <code>tf.keras.models.Sequential</code> to <code>tf.keras.Sequential</code> must have happened from <code>1.15</code> to <code>2.x</code> you can either downgrade your tensor flow version or update the code. I\'d go for the latter</p>\n', 'IsAccepted': False, 'CreationDate': 1593342652}, {'QuestionId': 58602285, 'AnswerId': 58604776, 'URL': 'https://stackoverflow.com/questions/58602285/what-is-the-difference-between-from-keras-models-import-sequential-and-from-t/58604776#58604776', 'QuestionTitle': 'What is the difference between &quot;from keras.models import Sequential&quot; and &quot;from tensorflow.python.keras.models import Sequential&quot;?', 'Answer': '<p>Difference between tf.keras and keras.</p>\n\n<ul>\n<li><p><strong>Keras:</strong> Is a high level neural network API for training neural networks. It\'s independent of <code>tensorflow</code> and can run on top of multiple backends such as <code>tensorflow, Theano and CNTK</code>. Documentation <a href=""https://keras.io/"" rel=""nofollow noreferrer"">here</a></p></li>\n<li><p><strong>tf.keras:</strong> <code>tf.keras</code> is a specific high level implementation of the <code>keras</code> API in tensorflow with added support for certain <code>tensorflow</code> features. </p></li>\n</ul>\n', 'IsAccepted': False, 'CreationDate': 1572342069}, {'QuestionId': 57751417, 'AnswerId': 57752374, 'URL': 'https://stackoverflow.com/questions/57751417/what-is-meant-by-sequential-model-in-keras/57752374#57752374', 'QuestionTitle': 'What is meant by sequential model in Keras', 'Answer': '<p>There are two ways to build Keras models: sequential and functional.</p>\n\n<p>The sequential API allows you to create models layer-by-layer for most problems. It is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs.</p>\n\n<p>Alternatively, the functional API allows you to create models that have a lot more flexibility as you can easily define models where layers connect to more than just the previous and next layers. In fact, you can connect layers to (literally) any other layer. As a result, creating complex networks such as siamese networks and residual networks become possible.</p>\n\n<p>for more details visit : <a href=""https://machinelearningmastery.com/keras-functional-api-deep-learning/"" rel=""noreferrer"">https://machinelearningmastery.com/keras-functional-api-deep-learning/</a></p>\n', 'IsAccepted': False, 'CreationDate': 1567406157}, {'QuestionId': 57751417, 'AnswerId': 57751512, 'URL': 'https://stackoverflow.com/questions/57751417/what-is-meant-by-sequential-model-in-keras/57751512#57751512', 'QuestionTitle': 'What is meant by sequential model in Keras', 'Answer': '<blockquote>\n  <p>The <a href=""https://keras.io/getting-started/sequential-model-guide/"" rel=""nofollow noreferrer""><code>Sequential</code></a> model is a linear stack of layers.</p>\n</blockquote>\n\n<p>The common architecture of ConvNets is a sequential architecture. However, some architectures are not linear stacks. For example, siamese networks are two parallel neural networks with some shared layers. <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""nofollow noreferrer"">More examples here</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1567399315}, {'QuestionId': 57751417, 'AnswerId': 57751447, 'URL': 'https://stackoverflow.com/questions/57751417/what-is-meant-by-sequential-model-in-keras/57751447#57751447', 'QuestionTitle': 'What is meant by sequential model in Keras', 'Answer': '<p>From the definition of <strong>Keras</strong> documentation the Sequential model is a <strong>linear stack of layers</strong>.You can create a Sequential model by passing a list of layer instances to the constructor:</p>\n\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential([\n    Dense(32, input_shape=(784,)),\n    Activation(\'relu\'),\n    Dense(10),\n    Activation(\'softmax\'),\n])\n</code></pre>\n\n<p>You can also simply add layers via the .add() method:</p>\n\n<pre><code>model = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation(\'relu\'))\n</code></pre>\n\n<p>For More details click <a href=""https://keras.io/getting-started/sequential-model-guide/"" rel=""noreferrer"">here</a> </p>\n', 'IsAccepted': False, 'CreationDate': 1567398655}, {'QuestionId': 55109696, 'AnswerId': 56086070, 'URL': 'https://stackoverflow.com/questions/55109696/tensorflow-difference-between-tf-keras-layers-layer-vs-tf-keras-model/56086070#56086070', 'QuestionTitle': 'TensorFlow - Difference between tf.keras.layers.Layer vs tf.keras.Model', 'Answer': '<p>In the documentation:</p>\n\n<blockquote>\n  <p>The Model class has the same API as Layer, with the following\n  differences: - It exposes built-in training, evaluation, and\n  prediction loops (model.fit(), model.evaluate(), model.predict()). -\n  It exposes the list of its inner layers, via the model.layers\n  property. - It exposes saving and serialization APIs.</p>\n  \n  <p>Effectively, the ""Layer"" class corresponds to what we refer to in the\n  literature as a ""layer"" (as in ""convolution layer"" or ""recurrent\n  layer"") or as a ""block"" (as in ""ResNet block"" or ""Inception block"").</p>\n  \n  <p>Meanwhile, the ""Model"" class corresponds to what is referred to in the\n  literature as a ""model"" (as in ""deep learning model"") or as a\n  ""network"" (as in ""deep neural network"").</p>\n</blockquote>\n\n<p>So if you want to be able to call <code>.fit()</code>, <code>.evaluate()</code>, or <code>.predict()</code> on those blocks or you want to be able to save and load those blocks separately or something you should use the Model class. The Layer class is leaner so you won\'t bloat the layers with unnecessary functionality...but I would guess that that generally wouldn\'t be a big problem.  </p>\n', 'IsAccepted': True, 'CreationDate': 1557532025}, {'QuestionId': 55178230, 'AnswerId': 55178699, 'URL': 'https://stackoverflow.com/questions/55178230/what-is-the-difference-between-keras-and-tf-keras/55178699#55178699', 'QuestionTitle': 'What is the difference between keras and tf.keras?', 'Answer': '<p>The difference between tf.keras and keras is the Tensorflow specific enhancement to the framework.</p>\n\n<p><code>keras</code> is an API specification that describes how a Deep Learning framework should implement certain part, related to the model definition and training.\nIs framework agnostic and supports different backends (Theano, Tensorflow, ...)</p>\n\n<p><code>tf.keras</code> is the Tensorflow specific implementation of the Keras API specification. It adds the framework the support for many Tensorflow specific features like: perfect support for <code>tf.data.Dataset</code> as input objects, support for eager execution, ...</p>\n\n<p>In Tensorflow 2.0 <code>tf.keras</code> will be the default and I highly recommend to start working using <code>tf.keras</code></p>\n', 'IsAccepted': False, 'CreationDate': 1552639733}, {'QuestionId': 55178230, 'AnswerId': 55178567, 'URL': 'https://stackoverflow.com/questions/55178230/what-is-the-difference-between-keras-and-tf-keras/55178567#55178567', 'QuestionTitle': 'What is the difference between keras and tf.keras?', 'Answer': '<p>At this point tensorflow has pretty much entirely adopted the keras API and for a good reason - it\'s simple, easy to use and easy to learn, whereas ""pure"" tensorflow comes with a lot of boilerplate code. And yes, you can use tf.keras without any issues, though you might have to re-work your imports in the code. For instance</p>\n\n<pre><code>from keras.layers.pooling import MaxPooling2D\n</code></pre>\n\n<p>Would turn into:</p>\n\n<pre><code>from tensorflow.keras.layers import MaxPooling2D\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1552639109}, {'QuestionId': 55109696, 'AnswerId': 55109850, 'URL': 'https://stackoverflow.com/questions/55109696/tensorflow-difference-between-tf-keras-layers-layer-vs-tf-keras-model/55109850#55109850', 'QuestionTitle': 'TensorFlow - Difference between tf.keras.layers.Layer vs tf.keras.Model', 'Answer': '<ul>\n<li>A layer takes in a tensor and give out a tensor which is a result of\nsome tensor operations</li>\n<li>A model is a composition of multiple layers. </li>\n</ul>\n\n<p>If you are building a new model architecture using existing keras/tf layers then build a custom model. </p>\n\n<p>If you are implementing your own custom tensor operations with in a layer, then build a custom layer. If you are using non tensor operation inside your custom layer, then you have to code how the layer will forward propagate and backward propagate. </p>\n', 'IsAccepted': False, 'CreationDate': 1552335950}, {'QuestionId': 51972647, 'AnswerId': 51973498, 'URL': 'https://stackoverflow.com/questions/51972647/what-are-the-differences-between-building-a-model-with-and-without-using-sequent/51973498#51973498', 'QuestionTitle': 'What are the differences between building a model with and without using Sequential() in Keras?', 'Answer': '<p>Actually, there is no difference between the models created using the functional API (i.e. <code>build_model01</code>) and the same model created as a Sequential model (i.e. <code>build_model02</code>). You can further confirm this by checking the <code>Sequential</code> class <a href=""https://github.com/keras-team/keras/blob/1788a0a901efccfffcead3d64a5b8e5876998d32/keras/engine/sequential.py#L24"" rel=""nofollow noreferrer"">source code</a>; as you can see, it is a subclass of <code>Model</code> class. Of course, Keras functional API <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""nofollow noreferrer"">gives you more flexibility</a> and it lets you create models with complex architectures (e.g. models with multiple inputs/outputs or multiple branches).</p>\n', 'IsAccepted': True, 'CreationDate': 1534964469}, {'QuestionId': 43268403, 'AnswerId': 43268696, 'URL': 'https://stackoverflow.com/questions/43268403/whats-difference-between-concatenated-and-sequential-models-in-keras/43268696#43268696', 'QuestionTitle': 'What&#39;s difference between concatenated and sequential models in keras?', 'Answer': ""<p>the second model is more complex than the first model, maybe you should train for more steps.</p>\n\n<p>here is my code, the acc is simple_acc: 0.7923, complex_acc: 0.7244. you can fine-tune it yourself.</p>\n\n<pre><code>#coding: utf-8\nimport numpy as np\nfrom keras.layers import Input,Dense,merge\nfrom keras.models import Sequential,Model\nfrom keras.optimizers import Adagrad\n\ndef simple_model():\n    result = Sequential()\n    result.add(Dense(2, input_shape=(2,), activation='sigmoid'))\n    result.add(Dense(1, input_shape=(2,), activation='sigmoid'))\n    ada_grad = Adagrad(lr=0.001, epsilon=1e-08, decay=0.0)\n    result.compile(optimizer=ada_grad, loss='hinge')\n    return result\n\ndef complex_model():\n    first_input = Input(shape=(1,), name='x1')\n    input_dense = Dense(1, activation='sigmoid', )(first_input)\n    second_input = Input(shape=(1,), name='x2')\n    second_dense = Dense(1, activation='sigmoid', )(second_input)\n\n    merge_one = merge([input_dense, second_dense],mode='concat',concat_axis=1)\n    merge_one_dense2 = Dense(2, activation='sigmoid', )(merge_one)\n    merge_one_dense3 = Dense(1, activation='sigmoid', )(merge_one_dense2)\n\n    result = Model(inputs=[first_input, second_input], outputs=merge_one_dense3)\n    ada_grad = Adagrad(lr=0.001, epsilon=1e-08, decay=0.0)\n    result.compile(optimizer=ada_grad, loss='hinge')\n    return result\n\ndef simple_data():\n    X = np.array([ [1, 1], [1, 0], [0, 1], [0, 0] ])\n    Y = np.array([ [0], [1], [1], [0] ])\n    return X,Y\n\ndef complex_data():\n    X1 = np.array([ [1], [0], [1], [0] ])\n    X2 = np.array([ [1], [1], [0], [0] ])\n    Y = np.array([ [0], [1], [1], [0] ])\n    return [X1,X2],Y\n\ndef test_simple_model():\n    model = simple_model()\n    X,Y = simple_data()\n    model.summary()\n    model.fit(X,Y,batch_size=4,epochs=1000,verbose=1,validation_data=(X,Y))\n    score = model.evaluate(X,Y,verbose=0)\n    return score\n\ndef test_complex_model():\n    model = complex_model()\n    X,Y = complex_data()\n    model.summary()\n    model.fit(X,Y,batch_size=4,epochs=1000,verbose=1,validation_data=(X,Y))\n    score = model.evaluate(X,Y,verbose=0)\n    return score\n\ndef main():\n    simple_acc = test_simple_model()\n    complex_acc = test_complex_model()\n    print 'simple_acc: %.4f' % simple_acc\n    print 'complex_acc: %.4f' % complex_acc\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1491532118}]","{66879748, 71335830}","['<p>There are two class API to define a model in <code>tf. keras</code>. According to the doc</p>\n<ul>\n<li><p><a href=""https://keras.io/api/models/sequential/"" rel=""noreferrer""><code>Sequential class</code></a>: Sequential groups a linear stack of layers into a <code>tf. keras.Model</code>.</p>\n</li>\n<li><p><a href=""https://keras.io/api/models/model/"" rel=""noreferrer""><code>Model class</code></a>: <code>Model</code> group\'s layers into an object with training and inference features.</p>\n</li>\n</ul>\n<hr />\n<p>An <code>Sequential</code> model is the simplest type of model, a linear stack of layers. But there are some flaws in using the <code>sequential</code> model API, it\'s limited in certain points. We can\'t build complex networks such as multi-input or multi-output networks using this API.</p>\n<p>But using <a href=""https://keras.io/api/models/model/"" rel=""noreferrer""><code>Model class</code></a>, we can instantiate a Model with the <strong>Functional API</strong> (and also with <strong>Subclassing the Model class</strong>) that allows us to create arbitrary graphs of layers. From this, we can get more flexibility and easily define models where each layer can connect not just with the previous and next layers but also share feature information with other layers in the model, for example, model-like <code>ResNet</code>, <code>EfficientNet</code>.</p>\n<p>In fact, most of the SOTA model that you can get from <code>tf.keras.applications</code> is basically implemented using the <strong>Functional API</strong>. However, in subclassing API, we define our layers in <code>__init__</code> and we implement the model\'s forward pass in the <code>call</code> method.</p>\n<p>Generally speaking, all the model definitions using Sequential API, can be achieved in Functional API or Model Subclassing API. And in Functional API or Model Subclassing API, we can create complex layers that not possible to achieve in Sequential API. If you wondering which one to choose, the answer is, it totally depends on your need. However, check out the following blog post where we have discussed the various model strategies in <code>tf. keras</code> with more examples. <a href=""https://towardsdatascience.com/model-sub-classing-and-custom-training-loop-from-scratch-in-tensorflow-2-cc1d4f10fb4e"" rel=""noreferrer"">Model Sub-Classing and Custom Training Loop from Scratch in TensorFlow 2</a></p>\n', '<p>There are two class API to define a model in <code>tf. keras</code>. According to the doc</p>\n<ul>\n<li><p><a href=""https://keras.io/api/models/sequential/"" rel=""noreferrer""><code>Sequential class</code></a>: Sequential groups a linear stack of layers into a <code>tf. keras.Model</code>.</p>\n</li>\n<li><p><a href=""https://keras.io/api/models/model/"" rel=""noreferrer""><code>Model class</code></a>: <code>Model</code> group\'s layers into an object with training and inference features.</p>\n</li>\n</ul>\n<hr />\n<p>An <code>Sequential</code> model is the simplest type of model, a linear stack of layers. But there are some flaws in using the <code>sequential</code> model API, it\'s limited in certain points. We can\'t build complex networks such as multi-input or multi-output networks using this API.</p>\n<p>But using <a href=""https://keras.io/api/models/model/"" rel=""noreferrer""><code>Model class</code></a>, we can instantiate a Model with the <strong>Functional API</strong> (and also with <strong>Subclassing the Model class</strong>) that allows us to create arbitrary graphs of layers. From this, we can get more flexibility and easily define models where each layer can connect not just with the previous and next layers but also share feature information with other layers in the model, for example, model-like <code>ResNet</code>, <code>EfficientNet</code>.</p>\n<p>In fact, most of the SOTA model that you can get from <code>tf.keras.applications</code> is basically implemented using the <strong>Functional API</strong>. However, in subclassing API, we define our layers in <code>__init__</code> and we implement the model\'s forward pass in the <code>call</code> method.</p>\n<p>Generally speaking, all the model definitions using Sequential API, can be achieved in Functional API or Model Subclassing API. And in Functional API or Model Subclassing API, we can create complex layers that not possible to achieve in Sequential API. If you wondering which one to choose, the answer is, it totally depends on your need.', ""<p>I think the confusion comes from using a <code>tf.keras.Sequential</code> model, which does not need an explicit <code>Input</code> layer. Consider the following two models, which are equivalent:</p>\n<pre><code>import tensorflow as tf\n\nmodel1 = tf.keras.Sequential([\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(5, activation='relu'),\n    ])\n\nmodel1.build((1, 28, 28, 1))\n</code></pre>\n<pre><code>model2 = tf.keras.Sequential([\n      tf.keras.layers.Input((28, 28, 1)),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(5, activation='relu'),\n    ])\n</code></pre>\n<p>The difference is that I explicitly set the input shape of <code>model2</code> using an <code>Input</code> layer. In <code>model1</code>, the input shape will be inferred when you pass real data to it or call <code>model.build</code>.</p>\n<p>Now regarding the <code>Flatten</code> layer, this layer simply converts a n-dimensional tensor (for example <code>(28, 28, 1)</code>) into a 1D tensor <code>(28 x 28 x 1)</code>. The <code>Flatten</code> layer and <code>Input</code> layer can coexist in a <code>Sequential</code> model but do not depend on each other.</p>\n""]","{'https://stackoverflow.com/questions/66879748/what-is-the-difference-between-tf-keras-model-and-tf-keras-sequential/66880334#66880334', 'https://stackoverflow.com/questions/71335830/what-is-the-difference-between-tf-keras-layers-input-and-tf-keras-layers-flatt/71335932#71335932'}",,0.14810630124282703,0.05315820198086
14,63004540,tf.pad,Documentation Replicability,How to pad 1 dimensinal vector in tensorflow? Getting InvalidArgumentError: paddings must be a matrix with 2 columns with tf.pad,"<p>I am trying to use tf.pad. Here is my attempt to pad the tensor to length 20, with values 10.</p>
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), paddings=20, constant_values=10)
</code></pre>
<p>I get this error message</p>
<pre><code>InvalidArgumentError: paddings must be a matrix with 2 columns: [2,1] [Op:PadV2]
</code></pre>
<p>I am looking at the documentation</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/pad</a></p>
<blockquote>
<p>paddings is an integer tensor with shape [n, 2], where n is the rank of tensor. For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension</p>
</blockquote>
<p>But I am unable to figure out how to shape the pad value</p>
","<p>You have to specify the padding at the beginning and the padding at the end of your vector by matrix of shape (1,2) :</p>
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), [[ 0 , 20]], constant_values=10)
</code></pre>
<p>if you have three-dimensional tensor (rank = 3 e.g : (225,225,3) ) the padding matrix has to be of shape (3, 2 ) where &quot;3&quot; is the rank, and &quot;2&quot; to specify the padding at the beginning and end of each dimension.</p>
<p>For example, a padding matrix  = [ [0,2], [5,5], [2,0] ], means that we want to pad the first dimension by 0 at the beginning (=no padding) and 2 at the end .padding the second dimension by 5 at beginning and 5 at the end.</p>
","{53740898, 73420262, 36973544, 61729033, 50174962, 34092850, 66305493, 42334646, 54481467, 62112123}","[{'QuestionId': 42334646, 'AnswerId': 48535322, 'URL': 'https://stackoverflow.com/questions/42334646/tensorflow-pad-unknown-size-tensor-to-a-specific-size/48535322#48535322', 'QuestionTitle': 'TensorFlow - Pad unknown size tensor to a specific size?', 'Answer': '<p>Yes. There is. Provided you do not need to change the rank of the tensor, it\'s very simple.</p>\n<p><a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer""><code>tf.pad()</code></a> accepts regular python lists with tensors. The format of the padding is a list of pairs of how much to pad on each side of that dimension.</p>\n<p>e.g.</p>\n<pre><code>t = tf.constant([[1, 2], [3, 4]])\npaddings = [[0, 0], [0, 4-tf.shape(t)[0]]]\nout = tf.pad(t, paddings, \'CONSTANT\', constant_values=-1)\nsess.run(out)\n# gives: \n# array([[ 1,  2, -1, -1],\n#       [ 3,  4, -1, -1]], dtype=int32)\n</code></pre>\n<hr />\n<p>If you want to generalise this to a useful function, you could do something like:</p>\n<pre class=""lang-py prettyprint-override""><code>def pad_up_to(t, max_in_dims, constant_values):\n    diff = max_in_dims - tf.shape(t)\n    paddings = tf.pad(diff[:, None], [[0, 0], [1, 0]])\n    return tf.pad(t, paddings, \'CONSTANT\', constant_values=constant_values)\n# (note: see edits for the solution referred to by other answers on this question)\n</code></pre>\n<p>where <code>max_in_dims</code> is essentially the desired shape of the output. <em>Note:</em> this function will fail if you provide a shape that is strictly smaller than <code>t</code> in any dimension.</p>\n<p>You can use it like:</p>\n<pre><code>t = tf.constant([[1, 2], [3, 4]]) # shape = [2, 2]\nt_padded = pad_up_to(t, [2, 4], -1) # shape = [2, 4], padded with -1s\n</code></pre>\n<p>or</p>\n<pre><code>t = tf.placeholder(tf.float32, [None, None]) # shape = [?, ?]\nt_padded = pad_up_to(t, [5,5], -1) # shape = [5, 5], padded with -1s\nt_np = np.random.uniform(0, 1, [3,4]) # shape = [3,4], no padding\nt_padded_out = sess.run(t_padded, {t: t_np})\nt_np2 = np.random.uniform(0, 1, [2,1]) # shape = [2,1], no padding\nt_padded_out2 = sess.run(t_padded, {t: t_np2})\n</code></pre>\n<p>Although the dimension sizes are calculated dynamically, the number of dimensions is not, so make sure that <code>max_in_dims</code> has the same number of elements as t.shape.</p>\n', 'IsAccepted': True, 'CreationDate': 1517377896}, {'QuestionId': 73420262, 'AnswerId': 73420641, 'URL': 'https://stackoverflow.com/questions/73420262/how-to-pad-a-3d-numpy-array/73420641#73420641', 'QuestionTitle': 'How to pad a 3d numpy array', 'Answer': '<p><a href=""https://stackoverflow.com/a/48690064/16975978"">https://stackoverflow.com/a/48690064/16975978</a></p>\n<p>I made like in shared link.</p>\n<pre><code>z = np.zeros((2, 5, 5))\no = np.ones((2, 3, 3))\nz[:, :3, :3] = o\nprint(z)\n</code></pre>\n<p>Check the result</p>\n', 'IsAccepted': False, 'CreationDate': 1660932047}, {'QuestionId': 73420262, 'AnswerId': 73420457, 'URL': 'https://stackoverflow.com/questions/73420262/how-to-pad-a-3d-numpy-array/73420457#73420457', 'QuestionTitle': 'How to pad a 3d numpy array', 'Answer': '<p>You need to add one more rank to the padding tensor.</p>\n<pre><code>import tensorflow as tf\ninp_tns = tf.random.uniform((2, 128, 128))\npad_tns = tf.constant([ [0, 0] ,     [17, 17],    [17, 17]   ])\n# -----------padding: ^first_dim^\n# ------------------------padding: ^second_dim^\n# ---------------------------------------padding: ^third_dim^\n\n# Printing the Input\nprint(&quot;Input: &quot;, inp_tns)\nprint(&quot;Padding: &quot;, pad_tns)\n\n# Generating padded Tensor\nres = tf.pad(inp_tns, pad_tns, mode =\'CONSTANT\', constant_values=0)\nprint(res.shape)\n</code></pre>\n<hr />\n<pre class=""lang-none prettyprint-override""><code>(2, 162, 162)\n</code></pre>\n<p><em><strong>NB.</strong> Reference : <a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer""><code>tf.pad</code></a></em></p>\n', 'IsAccepted': True, 'CreationDate': 1660930960}, {'QuestionId': 66305493, 'AnswerId': 66305988, 'URL': 'https://stackoverflow.com/questions/66305493/tf-pad-returns-array-filled-with-zero-mnist-dataset-padding-from-28-28-image/66305988#66305988', 'QuestionTitle': 'tf.pad returns array filled with zero, MNIST dataset padding from (28,28) image size to (32,32)', 'Answer': '<p>There is no mistake.</p>\n<p>The padding is working correctly.</p>\n<p>You are visually inspecting it and drawing conclusions that it is incorrect, instead of using your computer to simply test it.</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\n# get data\n(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n\n# pad\nX_train_pad = tf.pad(X_train, [[0, 0], [2, 2], [2, 2]], mode=&quot;CONSTANT&quot;)\n\n# extract the original image by manually removing the\n# padding from the borders\nextract_original_image = X_train_pad[:, 2:-2, 2:-2]\n\nprint(extract_original_image.shape)\n# TensorShape([60000, 28, 28])\n\n# check padded image with padding removed equals original\nnp.testing.assert_equal(extract_original_image.numpy(), X_train)\n\n# check padding is all zeros\nassert tf.math.reduce_sum(X_train_pad[:, :2, :2]).numpy() == 0\nassert tf.math.reduce_sum(X_train_pad[:, -2:, -2:]).numpy() == 0\n\n# check the sums are the same\nassert tf.math.reduce_sum(X_train_pad) == tf.math.reduce_sum(X_train)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1613934151}, {'QuestionId': 62112123, 'AnswerId': 62112921, 'URL': 'https://stackoverflow.com/questions/62112123/how-to-give-padding-6-in-tensorflow-2-0/62112921#62112921', 'QuestionTitle': 'how to give padding=6 in tensorflow 2.0?', 'Answer': '<p>According to the <code>tf.keras.layers.Conv2D</code> documentation, <code>padding</code> can only be either <code>\'same\'</code> or <code>\'valid\'</code>.</p>\n\n<p><code>\'same\'</code> ensures that the spatial dimensions are preserved and <code>\'valid\'</code> adds the minimum amount of padding required to ensure that the filter ""fits"" your spatial dimensions (which can be an issue if <code>stride</code> is anything other than one).</p>\n\n<p>You shouldn\'t need to specify an custom padding size as in most cases it would be unnecessary upsampling. But if you must, you can have a padding layer before the conv layer.</p>\n\n<pre class=""lang-py prettyprint-override""><code>tf.keras.layers.ZeroPadding2D(padding=(6, 6)) # put this before your conv layer\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1590909934}, {'QuestionId': 61729033, 'AnswerId': 61729246, 'URL': 'https://stackoverflow.com/questions/61729033/how-to-use-tf-pad-without-batch-size-axis/61729246#61729246', 'QuestionTitle': 'How to use tf.pad without batch size axis?', 'Answer': '<p>In the <code>tf.pad</code> paddings is an integer tensor with shape <code>[n, 2]</code>, where n is the rank of tensor. <a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/pad</a></p>\n\n<p>So you need to change the padding tensor like following:</p>\n\n<pre><code>img = tf.random.uniform((1,28,28))\nprint(img.shape)\n\n&gt;&gt;&gt; (1, 28, 28)\n\npaddings = tf.constant([[0, 0,], [1, 1], [1, 1]])\nimg_padded = tf.pad(img, paddings)\n\nprint(img_padded.shape)\n\n&gt;&gt;&gt; (1, 30, 30)\n</code></pre>\n\n<p>I tested the code on tf 2.x but the same should also work with tf 1.x .</p>\n', 'IsAccepted': False, 'CreationDate': 1589198570}, {'QuestionId': 54481467, 'AnswerId': 54481884, 'URL': 'https://stackoverflow.com/questions/54481467/how-to-pad-a-tf-tensor-with-zero-columns-given-a-list-of-columns/54481884#54481884', 'QuestionTitle': 'How to pad a TF Tensor with zero columns, given a *list* of columns', 'Answer': ""<p>(OP here) I managed to find a solution using <code>tf.scatter_nd</code>. The trick was to align the dimensions of a, the columns and the output shape.</p>\n\n<pre><code>a_np = np.array([[1, 2],\n                 [3, 4], \n                 [5, 6]])\n\n# Note the Transpose on every line below\na = tf.constant(a_np.T) \ncolumns = tf.constant(np.array([[1, 5]]).T.astype('int32'))\nshape = tf.constant((7, 3))\na_padded = tf.transpose(tf.scatter_nd(columns, a, shape))\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1549032754}, {'QuestionId': 54481467, 'AnswerId': 54481975, 'URL': 'https://stackoverflow.com/questions/54481467/how-to-pad-a-tf-tensor-with-zero-columns-given-a-list-of-columns/54481975#54481975', 'QuestionTitle': 'How to pad a TF Tensor with zero columns, given a *list* of columns', 'Answer': ""<p>Here is a solution:</p>\n\n<pre><code>tf.reset_default_graph()\na = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.int32)\ncolumns = tf.constant([1, 5], dtype=tf.int32)\na_padded = tf.Variable(tf.zeros((3, 7), dtype=tf.int32))\nindices = tf.stack(tf.meshgrid(tf.range(tf.shape(a_padded)[0]), columns, indexing='ij'), axis=-1)\nupdate_cols = tf.scatter_nd_update(a_padded, indices, a)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\nprint(sess.run(update_cols))\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1549033105}, {'QuestionId': 50174962, 'AnswerId': 54249762, 'URL': 'https://stackoverflow.com/questions/50174962/tensorflow-how-to-pad-tensor-with-the-edge-values/54249762#54249762', 'QuestionTitle': 'TensorFlow how to pad tensor with the edge values', 'Answer': '<p>As  a complement, if you want to pad image with replicate mode like opencv, the following can do it, dst_image is the image to pad. And pad_h_up, pad_h_down, pad_w_left,pad_w_right, is the four argument:</p>\n\n<pre><code>def pad_replica(image_pad, up,down, left, right):\n    paddings_up = tf.constant([[1, 0],[0,0],[0,0]])\n    paddings_down = tf.constant([[0, 1],[0,0],[0,0]])\n    paddings_left = tf.constant([[0, 0],[1,0],[0,0]])\n    paddings_right = tf.constant([[0, 0],[0, 1],[0 ,0]])\n    i = tf.constant(0)\n    c = lambda i,pad_len,pad_mode, image: tf.less(i, pad_len)\n    def body(i,pad_len,pad_mode,image):\n        i = i+1\n        image = tf.pad(image, pad_mode,""SYMMETRIC"")\n        return [i, pad_len,pad_mode, image]\n    [_, _, _, image_pad_up] = tf.while_loop(c, body, \\\n                                          [i, up, paddings_up, image_pad])\n    i = tf.constant(0)\n    [_, _, _, image_pad_down] = tf.while_loop(c, body, [i, down,paddings_down, image_pad_up])\n    i = tf.constant(0)\n    [_, _, _, image_pad_left] = tf.while_loop(c, body, [i, left, paddings_left, image_pad_down])\n    i = tf.constant(0)\n    [_, _, _, image_pad_right] = tf.while_loop(c, body, [i, right,paddings_right, image_pad_left])\n    i = tf.constant(0)\n    return image_pad_right\ndst_image.set_shape([None, None, None])\ndst_image = pad_replica(dst_image,\\\n             tf.cast(pad_h_up, tf.int32),\\\n            tf.cast(pad_h_down,tf.int32),\\\n            tf.cast(pad_w_left, tf.int32),\\\n            tf.cast(pad_w_right,tf.int32)\n            )\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1547798216}, {'QuestionId': 53740898, 'AnswerId': 53746114, 'URL': 'https://stackoverflow.com/questions/53740898/pad-data-using-tf-data-dataset/53746114#53746114', 'QuestionTitle': 'Pad data using tf.data.Dataset', 'Answer': '<p>Finally got the answer. The issue was for the second padded shapes instead of Tensorshape([None]), we should provide [], because the second item returned  by the generator is a scalar. If using Tensorshape([None]),, make sure we are returning a vector</p>\n\n<pre><code>    import pandas as pd\n    import numpy as np\n    import tensorflow as tf\n    import functools\n\n    total_data_size = 10000\n    embedding_dimension = 25\n    max_len = 17\n\n\n    varying_length = np.random.randint(max_len, size=(10000)) # varying length data\n    X = np.array([np.random.randint(1000, size=(value)).tolist()for index, value in enumerate(varying_length)]) # data of arying length\n    Y = np.random.randint(2, size=(total_data_size)).astype(np.int32) # target binary\n    embedding = np.random.uniform(-1,1,(1000, embedding_dimension))   # word embedding\n\n\n\n    def gen():\n        for index in range(len(X)):\n            yield X[index] , Y[index]\n\n    dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32), (tf.TensorShape([None]), []))\n    padded_shapes = (tf.TensorShape([None]), [])  # sentence of unknown size\n    dataset = (dataset\n               .padded_batch(25, padded_shapes=padded_shapes, padding_values=(-111, 0))\n               )\n\n    iter2 = dataset.make_initializable_iterator()\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer())\n    sess.run(iter2.initializer)\n\n    sess.run(iter2.get_next())\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1544627917}, {'QuestionId': 53740898, 'AnswerId': 53741364, 'URL': 'https://stackoverflow.com/questions/53740898/pad-data-using-tf-data-dataset/53741364#53741364', 'QuestionTitle': 'Pad data using tf.data.Dataset', 'Answer': '<p>I believe that since your generator yields two outputs, your <code>padded_shapes</code> and <code>padded_values</code> tuples must have a length of two. For me, this works:</p>\n\n<pre><code>dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32))\ndataset = dataset.batch(batch_size=25)\npadded_shapes = (tf.TensorShape([None]), tf.TensorShape([None]))  # sentence of unknown size\npadding_values = (tf.constant(-111), tf.constant(-111))  # the value with which pad index needs to be filled \n\ndataset = (dataset\n           .padded_batch(25, padded_shapes=padded_shapes, padding_values=padding_values)\n           )\n\niter2 = dataset.make_initializable_iterator()\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\nsess.run(iter2.initializer)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1544611839}, {'QuestionId': 42334646, 'AnswerId': 51936821, 'URL': 'https://stackoverflow.com/questions/42334646/tensorflow-pad-unknown-size-tensor-to-a-specific-size/51936821#51936821', 'QuestionTitle': 'TensorFlow - Pad unknown size tensor to a specific size?', 'Answer': '<p>An extension of <a href=""https://stackoverflow.com/a/48535322/4055338"">Multihunter\'s solution</a> so that padding is only performed when necessary and does not yield an error for longer inputs:</p>\n\n<p>Suppose we have a sequential input called <code>inp_seq</code>, which is a tensor of rank 4 and should be padded in order to have a minimum length of <code>filter_size</code> in dimension 1.</p>\n\n<pre><code>def dynamic_padding(inp, min_size):\n\n    pad_size = min_size - tf.shape(inp)[1]\n    paddings = [[0, 0], [0, pad_size], [0, 0], [0, 0]] # assign here, during graph execution\n    return tf.pad(inp, paddings)\n\n# Pad only if necessary\npadded = tf.cond(tf.less(tf.shape(inp_seq)[1], filter_size), true_fn=lambda: dynamic_padding(inp_seq, filter_size), false_fn=lambda: inp_seq)  \n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1534790833}, {'QuestionId': 42334646, 'AnswerId': 52085537, 'URL': 'https://stackoverflow.com/questions/42334646/tensorflow-pad-unknown-size-tensor-to-a-specific-size/52085537#52085537', 'QuestionTitle': 'TensorFlow - Pad unknown size tensor to a specific size?', 'Answer': '<p><a href=""https://stackoverflow.com/a/48535322/4068846"">The accepted answer</a> didn\'t work for me either and I am reluctant to do <a href=""https://stackoverflow.com/a/45721424/4068846"">assignments in the graph</a>. Here, I adjusted Multihunter\'s answer in such a way that it should work with variable sizes. A variation on this worked for me. Specifically, I am consuming data with <code>tf.data.TFREcordDataset</code> and wanted to apply padding on load instead of writing out the data pre-padded.</p>\n\n<pre><code>MIN_SIZE = 100\n\n# v shape is undefined on the second dimension. \nv = tf.get_variable(shape=(2, None), dtype=tf.int32)\n\n# Note: this will blow up if `padding_len` &lt; 0\npadding_len = MIN_SIZE - tf.shape(v)[-1]\n\n# paddings = [ [0, 0], [0, padding_len] ]\npaddings = tf.concat( ([[0, 0]], [[0, padding_len ]]), axis=0)\n\n# padded.shape = (2, 100)\npadded = tf.pad(t, paddings, \'CONSTANT\', constant_values=-1)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1535574206}, {'QuestionId': 50174962, 'AnswerId': 50175380, 'URL': 'https://stackoverflow.com/questions/50174962/tensorflow-how-to-pad-tensor-with-the-edge-values/50175380#50175380', 'QuestionTitle': 'TensorFlow how to pad tensor with the edge values', 'Answer': '<p>Use <a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer""><code>tf.pad()</code></a> and mode ""SYMMETRIC"" - it would <em>reflect</em> the values on the edge, but if you do only 1 depth padding, it\'s equivalent to repeating the edge value. If you need more padding, you have to repeat the operation, but you can go exponentially (1 first, then 2, then 4, etc.). This code (tested):</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\na = tf.reshape( tf.constant( range( 1, 10 ) ), ( 3, 3 ) )\nb = tf.pad( a, [ [ 1, 1 ], [ 1, 1 ] ], ""SYMMETRIC"" )\n\nwith tf.Session() as sess:\n    print( sess.run( b ) )\n</code></pre>\n\n<p>Outputs:</p>\n\n<blockquote>\n  <p>[[1 1 2 3 3]<br>\n   [1 1 2 3 3]<br>\n   [4 4 5 6 6]<br>\n   [7 7 8 9 9]<br>\n   [7 7 8 9 9]]  </p>\n</blockquote>\n\n<p>as desired.</p>\n', 'IsAccepted': True, 'CreationDate': 1525438181}, {'QuestionId': 42334646, 'AnswerId': 45721424, 'URL': 'https://stackoverflow.com/questions/42334646/tensorflow-pad-unknown-size-tensor-to-a-specific-size/45721424#45721424', 'QuestionTitle': 'TensorFlow - Pad unknown size tensor to a specific size?', 'Answer': '<p>I ran into something similar.  Not fully general, but you can do something like:</p>\n\n<pre><code>test_a = tf.constant([[1, 2], [3, 4]])\ntest_b = tf.constant([[1, 2, 3], [4, 5, 6]])\n\ndef pad_second_dim(input, desired_size):\n    padding = tf.tile([[0]], tf.stack([tf.shape(input)[0], desired_size - tf.shape(input)[1]], 0))\n    return tf.concat([input, padding], 1)\n\nwith tf.Session() as sess:\n    print sess.run(pad_second_dim(test_a, 4))\n    # [[1 2 0 0] [3 4 0 0]]\n    print sess.run(pad_second_dim(test_b, 4))\n    # [[1 2 3 0] [4 5 6 0]]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1502911277}, {'QuestionId': 36973544, 'AnswerId': 36974824, 'URL': 'https://stackoverflow.com/questions/36973544/tensorflow-valueerror-dimensions-are-not-compatible/36974824#36974824', 'QuestionTitle': 'TensorFlow ValueError Dimensions are not compatible', 'Answer': ""<p>First, it's not clear how many labels you have (3 or 13), and what is the size of input (X) vector (113 or 13)? I assume you have 13 labels, and 118 X vectors based on:</p>\n\n<pre><code>W = tf.Variable(tf.zeros([118, 13]))\ny_ = tf.placeholder(tf.float32, [None, 13])\n</code></pre>\n\n<p>Then, you may change your code something like this:</p>\n\n<pre><code>x = tf.placeholder(tf.float32, [None, 118])\n\nW = tf.Variable(tf.zeros([118, 13]))\nb = tf.Variable(tf.zeros([13]))\n\ny = tf.nn.softmax(tf.matmul(x, W) + b)\ny_ = tf.placeholder(tf.float32, [None, 13])\n</code></pre>\n\n<p>Let me know it addresses your issue. </p>\n"", 'IsAccepted': False, 'CreationDate': 1462161901}, {'QuestionId': 34092850, 'AnswerId': 34186154, 'URL': 'https://stackoverflow.com/questions/34092850/how-do-i-fix-a-dimension-error-in-tensorflow/34186154#34186154', 'QuestionTitle': 'How do I fix a dimension error in TensorFlow?', 'Answer': ""<p>You have to shape the input so it is compatible with both the training tensor and the output. If you input is length 1, your output should be length 1 (length is substituted for dimension).</p>\n\n<p>When you're dealing with-</p>\n\n<pre><code>def conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],\n                    strides=[1, 1, 1, 1], padding='SAME')\n</code></pre>\n\n<p>Notice how I changed the strides and the ksize to <code>[1, 1, 1, 1]</code>. This will match an output to a 1 dimensional input and prevent errors down the road.</p>\n\n<p>When you're defining your weight variable (see code below)-</p>\n\n<pre><code>def weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n</code></pre>\n\n<p>you're going to have to make the first 2 numbers conform to the feature tensor that you are using to train your model, the last two numbers will be the dimension of the predicted output (same as the dimension of the input).</p>\n\n<pre><code>W_conv1 = weight_variable([1, 10, 1, 1])\nb_conv1 = bias_variable([1])\n</code></pre>\n\n<p>Notice the <code>[1, 10,</code> in the beginning which signifies that the feature tensor is going to be a 1x10 feature tensor; the last two numbers <code>1, 1]</code> correspond to the dimensions of the input and output tensors/predictors.</p>\n\n<p>When you reshape your x_foo tensor (I call it x_ [x prime]), you, for whatever reason, have to define it like so-</p>\n\n<pre><code>x_ = tf.reshape(x, [-1,1,10,1])\n</code></pre>\n\n<p>Notice the 1 and 10 in the middle- <code>...1,10,...</code>. Once again, these numbers correspond to the dimension of your feature tensor.</p>\n\n<p>For every bias variable, you choose the final number of the previously defined variable. For example, if <code>W_conv1 = weight_variable([1, 10, 1, 1])</code> appears like so, you take the final number and put that into your bias variable so it can match the dimensions of the input. This is done like so- <code>b_conv1 = bias_variable([1])</code>.</p>\n\n<p>If you need any more explanation please comment below.</p>\n"", 'IsAccepted': True, 'CreationDate': 1449685083}, {'QuestionId': 34092850, 'AnswerId': 34094552, 'URL': 'https://stackoverflow.com/questions/34092850/how-do-i-fix-a-dimension-error-in-tensorflow/34094552#34094552', 'QuestionTitle': 'How do I fix a dimension error in TensorFlow?', 'Answer': ""<p>The dimensions you are using for the filter are not matching the output of the hidden layer.</p>\n\n<p>Let me see if I understood you: your input is composed of 8 features, and you want to reshape it into a 2x4 matrix, right?</p>\n\n<p>The weights you created with <code>weight_variable([1, 8, 1, 4])</code> expect a 1x8 input, in one channel, and produce a 1x8 output in 4 channels (or hidden units). The filter you are using sweeps the input in 2x2 squares. However, since the result of the weights is 1x8, they won't match.</p>\n\n<p>You should reshape the input as</p>\n\n<pre><code>x_image = tf.reshape(tf_in, [-1,2,4,1])\n</code></pre>\n\n<p>Now, your input is actually 2x4 instead of 1x8. Then you need to change the weight shape to <code>(2, 4, 1, hidden_units)</code> to deal with a 2x4 output. It will also produce a 2x4 output, and the 2x2 filter now can be applied.</p>\n\n<p>After that, the filter will match the output of the weights. Also note that you will have to change the shape of your second weight matrix to <code>weight_variable([2, 4, hidden_units, hidden2_units])</code></p>\n"", 'IsAccepted': False, 'CreationDate': 1449251963}]","{37659538, 46839237}","['<p>You can use <code>tf.pad()</code> (see the <a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""noreferrer"">doc</a>) to pad the Tensor before applying <code>tf.nn.conv2d(..., padding=""VALID"")</code> (valid padding means no padding).</p>\n\n<hr>\n\n<p>For instance, if you want to pad the image with 2 pixels in height, and 1 pixel in width, and then apply a convolution with a 5x5 kernel:</p>\n\n<pre class=""lang-py prettyprint-override""><code>input = tf.placeholder(tf.float32, [None, 28, 28, 3])\npadded_input = tf.pad(input, [[0, 0], [2, 2], [1, 1], [0, 0]], ""CONSTANT"")\n\nfilter = tf.placeholder(tf.float32, [5, 5, 3, 16])\noutput = tf.nn.conv2d(padded_input, filter, strides=[1, 1, 1, 1], padding=""VALID"")\n</code></pre>\n\n<p><code>output</code> will have shape <code>[None, 28, 26, 16]</code>, because you have only a padding of 1 in width.</p>\n', '<p>It depends on how do you plan to pad it. On example is like this:</p>\n\n<pre><code># Assuming input is a tensor with shape (?, 28, 28, 1)\noutput = tf.pad(input, [[0, 0], [2,2], [2,2], [0,0]])\n# print(tf.shape(output)) should be (?, 32, 32, 1)\n</code></pre>\n']","{'https://stackoverflow.com/questions/37659538/custom-padding-for-convolutions-in-tensorflow/37660000#37660000', 'https://stackoverflow.com/questions/46839237/padding-mnist-dataset-with-tf-pad/46840090#46840090'}",,0.18650342706060152,0.13431652612363076
14,62752605,tf.nn.sampled_softmax_loss,Documentation Replicability,Loss function in tf.nn.sampled_softmax_loss,"<p>I have a question regarding Tensorflow:</p>
<p>Which loss function is used in <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer""><code>tf.nn.sampled_softmax_loss</code></a>?</p>
<p>I believe it's <em><strong>cross-entropy</strong></em>, but it is not written on the official website. Can anyone confirm my guess?</p>
","<p>Based on <a href=""https://stackoverflow.com/questions/35241251/in-tensorflow-what-is-the-difference-between-sampled-softmax-loss-and-softmax-c"">this other question</a>, it looks like it is cross entropy.</p>
<p>Besides, the main difference between <code>sampled_softmax_loss</code> and <code>softmax_cross_entropy_with_logits</code> (the standard cross_entropy loss in TF) is that the first only takes into account a subset V of your vocabulary to calculate your loss, while the second takes into account your entire vocabulary.</p>
","{59907296, 51835810, 40401187, 43273677, 50783854, 62916592, 62010739, 43810195, 42509878, 40562268, 47122591}","[{'QuestionId': 62916592, 'AnswerId': 62917321, 'URL': 'https://stackoverflow.com/questions/62916592/loss-function-for-sequences-in-tensorflow-2-0/62917321#62917321', 'QuestionTitle': 'Loss function for sequences (in Tensorflow 2.0)', 'Answer': ""<p><code>mask</code> in <code>mask = tf.math.logical_not(tf.math.equal(real, 0))</code> is taking care of the <code>PADDING</code>.</p>\n<p>So, in your batch you would have sentences of different length and you do <code>0</code> padding to make all of them of equal length (think about <code>I have an apple</code> v/s <code>It's a good day to play football in the sun</code>)</p>\n<p>But, it doesn't make sense to include the <code>0</code> padded section in the loss calculation - hence, it's first looking into indices where you have a <code>0</code> and using multiplication later on to make their loss contribution 0.</p>\n"", 'IsAccepted': True, 'CreationDate': 1594823521}, {'QuestionId': 62010739, 'AnswerId': 62010982, 'URL': 'https://stackoverflow.com/questions/62010739/problem-wih-softmax-function-in-the-output-layer/62010982#62010982', 'QuestionTitle': 'Problem wih softmax function in the output layer', 'Answer': '<p>Your input array (or tensor) has shape <code>(5, 1)</code>, and by default, <code>tf.nn.softmax</code> operates on the last dimension. You might see what goes wrong now, because the last dimension is an individual element, which is then normalized with softmax to 1.0</p>\n\n<p>You have then two options:</p>\n\n<ul>\n<li>Specify <code>axis=0</code> to <code>tf.nn.softmax</code> so the operation is performed on the first and not the last dimension.</li>\n<li>Reshape the array to shape <code>(1, 5)</code> which would work with a default call to <code>tf.nn.softmax</code></li>\n</ul>\n', 'IsAccepted': True, 'CreationDate': 1590441723}, {'QuestionId': 59907296, 'AnswerId': 61823235, 'URL': 'https://stackoverflow.com/questions/59907296/how-to-use-tf-nn-sampled-softmax-loss-with-tensorflow-keras/61823235#61823235', 'QuestionTitle': 'How to use tf.nn.sampled_softmax_loss with Tensorflow Keras?', 'Answer': '<p>sampled_softmax_loss() computes and returns the sampled softmax training loss.</p>\n\n<p>This is a faster way to train a softmax classifier over a huge number of classes.</p>\n\n<p>This operation is for training only. It is generally an underestimate of the full softmax loss.</p>\n\n<p>A common use case is to use this method for training, and calculate the full softmax loss for evaluation or inference. In this case, you must set <code>partition_strategy=""div""</code> for the two losses to be consistent, as in the following example:</p>\n\n<pre><code>if mode == ""train"":\n  loss = tf.nn.sampled_softmax_loss(\n      weights=weights,\n      biases=biases,\n      labels=labels,\n      inputs=inputs,\n      ...,\n      partition_strategy=""div"")\nelif mode == ""eval"":\n  logits = tf.matmul(inputs, tf.transpose(weights))\n  logits = tf.nn.bias_add(logits, biases)\n  labels_one_hot = tf.one_hot(labels, n_classes)\n  loss = tf.nn.softmax_cross_entropy_with_logits(\n      labels=labels_one_hot,\n      logits=logits)  \n</code></pre>\n\n<p>Where regular loss functions like CategoricalCrossentropy() uses it\'s default values, even if you don\'t pass any arguments it will calculate the loss based on its default values.  </p>\n\n<p>The key point for sampled_softmax_loss is to pass right shape of <code>weight</code>, <code>bias</code>, <code>input</code> and <code>label</code>.<br>\n The shape of weight passed to sampled_softmax is not the the same with the general situation.<br>\n For example, <code>logits = xw + b</code>, call sampled_softmax like this: </p>\n\n<p><code>sampled_softmax(weight=tf.transpose(w), bias=b, inputs=x)</code>,<br>\nNOT <code>sampled_softmax(weight=w, bias=b, inputs=logits)</code>!!  </p>\n\n<p>Besides, label is not one-hot representation. if your labels are one-hot represented, pass <code>labels=tf.reshape(tf.argmax(labels_one_hot, 1), [-1,1])</code>  </p>\n', 'IsAccepted': True, 'CreationDate': 1589557586}, {'QuestionId': 51835810, 'AnswerId': 51863130, 'URL': 'https://stackoverflow.com/questions/51835810/loss-function-in-keras-tensorflow/51863130#51863130', 'QuestionTitle': 'Loss function in Keras/TensorFlow', 'Answer': '<p>To me, it seems like the authors are using a vanilla binary cross-entropy loss for multi-label classification. They also name it as such, but their definition is a bit odd compared to how you would implement it in Keras. </p>\n\n<p>Basically, you could use <code>binary_crossentropy</code> as a loss function and supply the labels as arrays of shape <code>(400, 200, 1)</code> where a 0 denotes the first class and a 1 denotes the second class. The output of your network would then be of the same shape, with <code>sigmoid</code> activation functions at each output node. This is how semantic segmentation models are usually implemented in Keras. See <a href=""https://github.com/zhixuhao/unet/blob/master/model.py"" rel=""nofollow noreferrer"">this repo</a> for an example: </p>\n\n<pre><code># final layer, sigmoid activations\nconv10 = Conv2D(1, 1, activation = \'sigmoid\')(conv9)\nmodel = Model(input = inputs, output = conv10)\n# binary_crossentropy loss for multi-label classification\nmodel.compile(optimizer = Adam(lr = 1e-4), loss = \'binary_crossentropy\', metrics = [\'accuracy\'])\n</code></pre>\n\n<p>This should give exactly the same result as with the implementation defined in the paper (they did probably not use Keras). </p>\n', 'IsAccepted': True, 'CreationDate': 1534352179}, {'QuestionId': 50783854, 'AnswerId': 50806915, 'URL': 'https://stackoverflow.com/questions/50783854/tensorflow-loss-matrix-function/50806915#50806915', 'QuestionTitle': 'TensorFlow : Loss matrix function', 'Answer': ""<p>One option is to use a lookup table for the different combinations. This would let you then weight whichever loss you're using (e.g. cross entropy) based on the discretized prediction (since this lookup operation is not differentiable).</p>\n\n<pre><code>import tensorflow as tf\n\n# This example is using eager execution, but the same code will work with graph\n# building if you run it in a Session.\ntf.enable_eager_execution()\n\npenalties = tf.constant([\n    # Predicted 0\n    0.,    # Label 0\n    100.,  # Label 1\n    # Predicted 1\n    1.,    # Label 0\n    0.     # Label 1\n])\n\ndef compute_loss_weight(predicted, label):\n  sparse_predicted = tf.argmax(predicted, axis=-1)\n  sparse_label = tf.argmax(label, axis=-1)\n  offset = sparse_predicted * tf.shape(label, out_type=tf.int64)[-1] + sparse_label\n  return tf.gather(params=penalties, indices=offset)\n\nprint(compute_loss_weight(predicted=[1, 0], label=[0, 1]))  # 100.\nprint(compute_loss_weight(predicted=[0, 1], label=[1, 0]))  # 1.\n# Also works on batches\nprint(compute_loss_weight(predicted=[[1, 0], [1, 0], [0, 1], [0, 1]],\n                          label=    [[0, 1], [1, 0], [0, 1], [1, 0]]))\n# Prints                             [100.   0.       0.      1.]\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1528756119}, {'QuestionId': 42509878, 'AnswerId': 50572847, 'URL': 'https://stackoverflow.com/questions/42509878/what-is-the-difference-between-sampled-softmax-loss-and-nce-loss-in-tensorflow/50572847#50572847', 'QuestionTitle': 'what is the difference between sampled_softmax_loss and nce_loss in tensorflow?', 'Answer': '<p>Check out this documentation from TensorFlow <a href=""https://www.tensorflow.org/extras/candidate_sampling.pdf"" rel=""nofollow noreferrer"">https://www.tensorflow.org/extras/candidate_sampling.pdf</a></p>\n\n<p>They seem pretty similar, but sampled softmax is only applicable for a single label while NCE extends to the case where your labels are a multiset. NCE can then model the expected counts rather than presence/absence of a label.  I\'m not clear on an exact example of when to use the sampled_softmax.</p>\n', 'IsAccepted': False, 'CreationDate': 1527537838}, {'QuestionId': 47122591, 'AnswerId': 47123538, 'URL': 'https://stackoverflow.com/questions/47122591/batch-loss-and-total-loss-tf-get-total-loss-in-tensorflow/47123538#47123538', 'QuestionTitle': 'batch_loss and total_loss=tf.get_total_loss() in tensorflow', 'Answer': '<p><em>The summary of discussion in the comments</em>:</p>\n\n<p>The training loss is computed in the forward pass over the mini-batch. But the actual loss values <strong>aren\'t needed</strong> to begin the backprop. The backprop is started with the error signal, which equals to the loss function derivative evaluated at the values from the forward pass. So the loss value doesn\'t affect the parameters update and is reported simply to monitor the training process. For example, if the loss does not decrease, it\'s a sign to double check the neural network model and hyperparameters. So it\'s not a big deal to smooth the reported loss through averaging just to make a chart look nicer.</p>\n\n<p>See <a href=""https://medium.com/@erikhallstrm/backpropagation-from-the-beginning-77356edf427d"" rel=""nofollow noreferrer"">this post</a> for more details.</p>\n', 'IsAccepted': True, 'CreationDate': 1509897167}, {'QuestionId': 43810195, 'AnswerId': 45232868, 'URL': 'https://stackoverflow.com/questions/43810195/tensorflow-sampled-softmax-loss-correct-usage/45232868#45232868', 'QuestionTitle': 'Tensorflow Sampled Softmax Loss Correct Usage', 'Answer': '<p>The key point is to pass right shape of weight, bias, input and label. The shape of weight passed to sampled_softmax is not the the same with the general situation.\nFor example, <code>logits = xw + b</code>, call sampled_softmax like this:\n<code>sampled_softmax(weight=tf.transpose(w), bias=b, inputs=x)</code>, NOT <code>sampled_softmax(weight=w, bias=b, inputs=logits)</code>!!\nBesides, label is not one-hot representation. if your labels are one-hot represented, pass <code>labels=tf.reshape(tf.argmax(labels_one_hot, 1), [-1,1])</code></p>\n', 'IsAccepted': False, 'CreationDate': 1500625992}, {'QuestionId': 42509878, 'AnswerId': 42511026, 'URL': 'https://stackoverflow.com/questions/42509878/what-is-the-difference-between-sampled-softmax-loss-and-nce-loss-in-tensorflow/42511026#42511026', 'QuestionTitle': 'what is the difference between sampled_softmax_loss and nce_loss in tensorflow?', 'Answer': '<p>Sampled softmax tries to normalise over all samples in your output. Having a non-normal distribution (logarithmic over your labels) this is not an optimal loss function. Note that although they have the same parameters, they way you use the function is different. Take a look at the documentation here: <a href=""https://github.com/calebchoo/Tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.nn.nce_loss.md"" rel=""nofollow noreferrer"">https://github.com/calebchoo/Tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.nn.nce_loss.md</a> and read this line: </p>\n\n<blockquote>\n  <p>By default this uses a log-uniform (Zipfian) distribution for sampling, so your labels must be sorted in order of decreasing frequency to achieve good results. For more details, see log_uniform_candidate_sampler.</p>\n</blockquote>\n\n<p>Take a look at this paper where they explain why they use it for word embeddings: <a href=""http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf"" rel=""nofollow noreferrer"">http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf</a></p>\n\n<p>Hope this helps!</p>\n', 'IsAccepted': False, 'CreationDate': 1488291177}, {'QuestionId': 43810195, 'AnswerId': 43813720, 'URL': 'https://stackoverflow.com/questions/43810195/tensorflow-sampled-softmax-loss-correct-usage/43813720#43813720', 'QuestionTitle': 'Tensorflow Sampled Softmax Loss Correct Usage', 'Answer': '<p>In your softmax layer you are multiplying your network predictions, which have dimension <code>(num_classes,)</code> by your <code>w</code> matrix which has dimension <code>(num_classes, num_hidden_1)</code>, so you end up trying to compare your target labels of size <code>(num_classes,)</code> to something that is now size <code>(num_hidden_1,)</code>. Change your tiny perceptron to output <code>layer_1</code> instead, then change the definition of your cost. The code below might do the trick.</p>\n\n<pre class=""lang-py prettyprint-override""><code>def tiny_perceptron(x, weights, biases):\n    layer_1 = tf.add(tf.matmul(x, weights[\'h1\']), biases[\'b1\'])\n    layer_1 = tf.nn.relu(layer_1)\n    return layer_1\n\nlayer_1 = tiny_perceptron(x, weights, biases)\nloss_function = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n                     weights=weights[\'h1\'],\n                     biases=biases[\'b1\'],\n                     labels=labels,\n                     inputs=layer_1,\n                     num_sampled=num_sampled,\n                     num_true=num_true,\n                     num_classes=num_classes))\n</code></pre>\n\n<p>When you train your network with some optimizer, you will tell it to minimize <code>loss_function</code>, which should mean that it will adjust both sets of weights and biases.</p>\n', 'IsAccepted': True, 'CreationDate': 1494018301}, {'QuestionId': 40401187, 'AnswerId': 43332118, 'URL': 'https://stackoverflow.com/questions/40401187/tensorflow-sampled-softmax-loss-example/43332118#43332118', 'QuestionTitle': 'Tensorflow sampled_softmax_loss example', 'Answer': '<p>Sample softmax is used when you have <strong>high number of output classes</strong>. \nThe main reason is if you use normal softmax loss for high number of output classes , lets say 5000 , <em>it\'s very inefficient and heave for our computer to calculate. So sample softmax is something that will take care only k number of classes from total number of classes when calculating the softmax loss.</em> </p>\n\n<p><strong><em>One example this is used is sequence to sequence models in tensorflow.</em></strong> </p>\n\n<p>These modules can predict things that occur in sequential manner.Let\'s say given a sentence predict the next word. So here in order to predict the word you have many output classes. <strong>In this case it is equal to vocabulary size</strong>. So sample softmax is very handy in this.\n<a href=""https://www.google.lk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiq0LW02JrTAhURUI8KHWgPBEwQFggbMAA&amp;url=https%3A%2F%2Fwww.tensorflow.org%2Ftutorials%2Fseq2seq&amp;usg=AFQjCNHXMthJ7Bm4FEOVvHJdmEiFvucapQ&amp;bvm=bv.152174688,d.c2I"" rel=""nofollow noreferrer"">Link to the tensorflow seq2seq models</a></p>\n', 'IsAccepted': False, 'CreationDate': 1491854915}, {'QuestionId': 42509878, 'AnswerId': 43320139, 'URL': 'https://stackoverflow.com/questions/42509878/what-is-the-difference-between-sampled-softmax-loss-and-nce-loss-in-tensorflow/43320139#43320139', 'QuestionTitle': 'what is the difference between sampled_softmax_loss and nce_loss in tensorflow?', 'Answer': '<p>Sample softmax is all about selecting a sample of the given number and try to get the softmax loss. Here the main objective is to make the result of the sampled softmax equal to our true softmax. So algorithm basically concentrate lot on selecting the those samples from the given distribution.\nOn other hand NCE loss is more of selecting noise samples and try to mimic the true softmax. It will take only one true class and a K noise classes.  </p>\n', 'IsAccepted': False, 'CreationDate': 1491818238}, {'QuestionId': 43273677, 'AnswerId': 43273852, 'URL': 'https://stackoverflow.com/questions/43273677/defined-loss-function-in-tensorflow/43273852#43273852', 'QuestionTitle': 'defined loss function in tensorflow?', 'Answer': '<p>Let <code>losses</code> to be a vector (rank-1 tensor) of loss values for the examples in your batch. And let <code>y</code> be the the vector of corresponding labels. You could then achieve the result you want by</p>\n\n<pre><code>weights = w_pos*y + w_neg*(1.0-y)\nloss = tf.reduce_mean(weights*losses)\n</code></pre>\n\n<p>Here, <code>w_pos</code> and <code>w_neg</code> are constant scalar values (<code>w_pos=100.0</code> and <code>w_neg=1.0</code> in your example). The vector <code>weights</code> then has a value of <code>w_pos</code> for examples where the label equals 1 and <code>w_neg</code> where it equals 0. You then multiply <code>weights</code> element-wise with <code>losses</code> to weigh the values in the <code>losses</code> according to the corresponding labels and then take the mean.</p>\n', 'IsAccepted': False, 'CreationDate': 1491555316}, {'QuestionId': 40562268, 'AnswerId': 40562388, 'URL': 'https://stackoverflow.com/questions/40562268/how-to-use-sampled-softmax-loss-in-tensorflow/40562388#40562388', 'QuestionTitle': 'How to use sampled_softmax_loss in Tensorflow', 'Answer': '<p>This particular error is about passing <code>outputs</code> which is a list, when <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sampled_softmax_loss"" rel=""nofollow noreferrer"">tf.nn.sampled_softmax_loss</a> expects a single tensor.</p>\n\n<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L153"" rel=""nofollow noreferrer"">seq2seq.basic_rnn_seq2seq</a> function returns a list of tensors of size <code>[batch_size x output_size]</code> as the first output. Assuming each of your outputs is one-dimensional, you want to concatenate the output list using <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#concat"" rel=""nofollow noreferrer"">tf.concat</a> (creating a tensor of size <code>[seq_len x batch_size x 1]</code>), <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#squeeze"" rel=""nofollow noreferrer"">tf.squeeze</a> the last dimension (resulting <code>[seq_len x batch_size]</code>) and <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#transpose"" rel=""nofollow noreferrer"">tf.transpose</a> to make <code>output</code> have size <code>[batch_size x seq_len]</code>, same as <code>self.output_data</code>.</p>\n\n<p>To debug the problem, print the tensor sizes using <code>print(output.get_shape())</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1478950096}]","{43394152, 62752605}","['<p>Well, usually <code>p(x)</code> in cross-entropy equation is true distribution, while <code>q(x)</code> is the distribution obtained from softmax. So, if <code>p(x)</code> is one-hot (and this is so, otherwise sparse cross-entropy could not be applied), cross entropy is just negative log for probability of true category.</p>\n\n<p>In your example, <code>softmax(logits)</code> is a vector with values <code>[0.09003057,  0.24472847,  0.66524096]</code>, so the loss is <code>-log(0.24472847) = 1.4076059</code> which is exactly what you got as output.</p>\n', '<p>Based on <a href=""https://stackoverflow.com/questions/35241251/in-tensorflow-what-is-the-difference-between-sampled-softmax-loss-and-softmax-c"">this other question</a>, it looks like it is cross entropy.</p>\n<p>Besides, the main difference between <code>sampled_softmax_loss</code> and <code>softmax_cross_entropy_with_logits</code> (the standard cross_entropy loss in TF) is that the first only takes into account a subset V of your vocabulary to calculate your loss, while the second takes into account your entire vocabulary.</p>\n']","{'https://stackoverflow.com/questions/43394152/tensorflow-what-exact-formula-is-applied-in-tf-nn-sparse-softmax-cross-entropy/43395986#43395986', 'https://stackoverflow.com/questions/62752605/loss-function-in-tf-nn-sampled-softmax-loss/62754154#62754154'}",,0.1975107121422883,0.07466577444051198
15,66874943,tf.data.Dataset,Documentation Replicability,Why iterations over the same tf.data.Dataset give different data each iteration?,"<p>I'm trying to understand how <strong>tf.data.Dataset</strong> works.</p>
<p>It says on the documentation that <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""nofollow noreferrer"">take</a> returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch):</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds

# Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=True)

# Build your input pipeline
ds = ds.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)
# ...
</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([2 0 6 6 8 8 6 0 3 4 8 7 5 2 5 7 8 7 1 1 1 8 6 4 0 4 3 2 4 2 1 9], shape=(32,), dtype=int64)
</code></pre>
<p>However, iterating over it again, gives different labels: (continuation of last code)</p>
<pre class=""lang-py prettyprint-override""><code>for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([7 3 5 6 3 1 7 9 6 1 9 3 9 8 6 7 7 1 9 7 5 2 0 7 8 1 7 8 7 0 5 0], shape=(32,), dtype=int64)
tf.Tensor([1 3 6 1 8 8 0 4 1 3 2 9 5 3 8 7 4 2 1 8 1 0 8 5 4 5 6 7 3 4 4 1], shape=(32,), dtype=int64)
</code></pre>
<p>Shouldn't the labels be the same, given that the dataset is the same?</p>
","<p>This is because the data files are shuffled and the dataset is shuffled with <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=ru#shuffle"" rel=""nofollow noreferrer""><code>dataset.shuffle()</code></a>.</p>
<p>With <code>dataset.shuffle()</code>, the data will be shuffled in a different way on each iteration by default.</p>
<p>One can remove <code>shuffle_files=True</code> and set the argument <code>reshuffle_each_iteration=False</code> to prevent reshuffling on different iterations.</p>
<p>The <code>.take()</code> function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them.</p>
<pre class=""lang-py prettyprint-override""><code># Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=False)

# Build your input pipeline
ds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
    
for example in single_batch_dataset:
    image, label = example[&quot;image&quot;], example[&quot;label&quot;]
    print(label)
</code></pre>
<p>Output:</p>
<pre class=""lang-py prettyprint-override""><code>tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)
tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)
</code></pre>
","{48126690, 52925767, 61754089, 68612779, 50330993, 57725172, 72045334, 60250039, 64821948, 56426839}","[{'QuestionId': 48126690, 'AnswerId': 73372469, 'URL': 'https://stackoverflow.com/questions/48126690/how-to-make-tf-data-dataset-return-all-of-the-elements-in-one-call/73372469#73372469', 'QuestionTitle': 'How to make tf.data.Dataset return all of the elements in one call?', 'Answer': '<p>The following example will batch all the elements in the dataset as a single item, and extract them as an array.</p>\n<pre><code>data = data.batch(len(data))\ndata = data.get_single_element()\n</code></pre>\n<p>This will add an outer dimension to the data equal to the length of the batch. For example, if you start with a dataset containing <code>456</code> elements of dimension <code>(32, 100)</code>, you will receive an array of shape <code>(456, 32, 100)</code>.</p>\n', 'IsAccepted': False, 'CreationDate': 1660645473}, {'QuestionId': 48126690, 'AnswerId': 72528619, 'URL': 'https://stackoverflow.com/questions/48126690/how-to-make-tf-data-dataset-return-all-of-the-elements-in-one-call/72528619#72528619', 'QuestionTitle': 'How to make tf.data.Dataset return all of the elements in one call?', 'Answer': ""<p>You can get all the elements of the dataset with</p>\n<pre><code>`dataset.take(lenth_of_the_dataset)`\n</code></pre>\n<blockquote>\n<p>Args:   count: A tf.int64 scalar tf.Tensor, representing the number of\nelements of this dataset that should be taken to form the new dataset. If count is -1, or if count is greater than the size of this\ndataset, the new dataset will contain all elements of this dataset.<br />\nname: (Optional.) A name for the tf.data operation.</p>\n<p>Returns:   Dataset: A Dataset.</p>\n<p>from tensorflow.org</p>\n</blockquote>\n<p>Lets suppose that my dataset is as follows</p>\n<p><code>&lt;BatchDataset element_spec=TensorSpec(shape=(None, 400, 32, 32, 3), dtype=tf.float32, name=None)&gt;</code></p>\n<p>I'm going to iterate over the dataset and get each datapoint as a numpy object.</p>\n<pre><code>for img in data.unbatch().take(10): # because the length is 10\n   print(img.numpy().shape)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1654592726}, {'QuestionId': 72045334, 'AnswerId': 72045471, 'URL': 'https://stackoverflow.com/questions/72045334/unexpected-behavior-in-tf-data-dataset-map-function/72045471#72045471', 'QuestionTitle': 'Unexpected behavior in tf.data.Dataset map function', 'Answer': '<p>I think you have to set a random seed to get deterministic behavior, because when zipping the two datasets, they will be called <a href=""https://github.com/tensorflow/tensorflow/blob/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/data/ops/dataset_ops.py#L4606"" rel=""nofollow noreferrer"">internally</a> (similar to the python <code>zip</code> function) triggering the map function:</p>\n<pre><code>import tensorflow as tf\ntf.random.set_seed(111)\ndataset = tf.data.Dataset.from_tensor_slices([1, 1, 1, 1, 1, 1]) \ndataset = dataset.map(lambda x: x + tf.random.uniform([], minval=0, maxval=9, dtype=tf.dtypes.int32))\n\nds = dataset.zip((dataset, dataset))\nprint(list(ds.as_numpy_iterator()))\n# [(7, 7), (8, 8), (8, 8), (2, 2), (8, 8), (6, 6)]\n</code></pre>\n<p><strong>Update 1</strong></p>\n<p>Maybe something like this:</p>\n<pre><code>import tensorflow as tf\n\nds = tf.data.Dataset.range(1, 10)\ndef my_random_operation(x):\n  # Or with stateless uniform\n  return x + tf.random.uniform([], minval=0, maxval=9, dtype=tf.dtypes.int64, seed=1)\n\ndef another_random_operation(x):\n  # Or with stateless uniform\n  return x + tf.random.uniform([], minval=0, maxval=9, dtype=tf.dtypes.int64, seed=2)\n  \ny = ds.map(lambda x: my_random_operation(x))\nx = ds.map(lambda x: another_random_operation(x))\ndataset = tf.data.Dataset.zip((x, y))\n\nprint(list(dataset.as_numpy_iterator()))\n</code></pre>\n<p>What could also work is something like <code>np.random.randint</code> which would also be executed only once, since it won\'t be traced in a <code>tf.Graph</code>:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nds = tf.data.Dataset.range(1, 10)\ndef my_random_operation(x):\n  return x + np.random.randint(0, 9)\n\ndef another_random_operation(x):\n  return x + np.random.randint(0, 9)\n  \ny = ds.map(lambda x: my_random_operation(x))\nx = ds.map(lambda x: another_random_operation(x))\n</code></pre>\n<p><strong>Update 2</strong></p>\n<p>Regarding the reason behind this behavior, notice that the variable <code>num_traces</code> is called 18 times altogether, which corresponds to your 9 tuples:</p>\n<pre><code>import tensorflow as tf\n\nds = tf.data.Dataset.range(1, 10)\n\nnum_traces = tf.Variable(0)\n\ndef my_random_operation(x):\n  global num_traces\n  num_traces.assign_add(1)\n  return x + tf.random.uniform([], minval=0, maxval=9, dtype=tf.dtypes.int64)\n\nds = ds.map(lambda x: my_random_operation(x))\ndataset = tf.data.Dataset.zip((ds, ds))\n\nprint(list(dataset.as_numpy_iterator()))\nprint(num_traces)\n# [(2, 3), (9, 8), (7, 7), (11, 11), (8, 9), (14, 11), (13, 11), (14, 16), (12, 16)]\n# &lt;tf.Variable \'Variable:0\' shape=() dtype=int32, numpy=18&gt;\n</code></pre>\n<p>This behavior is in accordance with the information in the <a href=""https://www.tensorflow.org/guide/random_numbers#interaction_with_tffunction"" rel=""nofollow noreferrer"">docs</a>:</p>\n<blockquote>\n<p>When used as an argument to a tf.function, different generator objects\nwill cause retracing of the tf.function.</p>\n</blockquote>\n<p>So one assumption is that retracing is triggered by trying to zip the datasets after the first execution of <code>dataset.map</code>. The datasets, as mentioned previously, are iterated internally and therefore trigger the map function again because the values have changed since the first <code>map</code> call. Check this <a href=""https://github.com/tensorflow/tensorflow/tree/3f878cff5b698b82eea85db2b60d65a2e320850e/tensorflow/python/ops"" rel=""nofollow noreferrer"">thread</a> also.</p>\n', 'IsAccepted': False, 'CreationDate': 1651156034}, {'QuestionId': 72045334, 'AnswerId': 72045755, 'URL': 'https://stackoverflow.com/questions/72045334/unexpected-behavior-in-tf-data-dataset-map-function/72045755#72045755', 'QuestionTitle': 'Unexpected behavior in tf.data.Dataset map function', 'Answer': '<p>I think it makes a lot more sense to create separate datasets, zip them, and <em>then</em> perform a common operation.</p>\n<pre><code>import tensorflow as tf\n\nds1 = tf.data.Dataset.range(1, 4)\nds2 = tf.data.Dataset.range(4, 8)\n\nds = tf.data.Dataset.zip((ds1, ds2))\n\n# [(1, 4), (2, 5), (3, 6)]\n\n\ndef add_random_number(a, b):\n    random_number = tf.random.uniform([], minval=0, maxval=9, dtype=tf.dtypes.int64)\n    return a + random_number, b + random_number\n\n\nds = ds.map(add_random_number)\n\nprint(list(ds.as_numpy_iterator()))\n\n# [(7, 10), (3, 6), (10, 13)] +6, +1, +7\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1651157043}, {'QuestionId': 48126690, 'AnswerId': 68851721, 'URL': 'https://stackoverflow.com/questions/48126690/how-to-make-tf-data-dataset-return-all-of-the-elements-in-one-call/68851721#68851721', 'QuestionTitle': 'How to make tf.data.Dataset return all of the elements in one call?', 'Answer': '<p>TensorFlow\'s <code>get_single_element()</code> is finally <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#get_single_element"" rel=""nofollow noreferrer"">around</a> which does exactly this - return all of the elements in one call.</p>\n<h4>This avoids the need of generating and using an iterator using <code>.map()</code> or <code>iter()</code> (which could be costly for big datasets).</h4>\n<p><code>get_single_element()</code> returns a tensor (or a tuple or dict of tensors) encapsulating all the members of the dataset. We need to pass all the members of the dataset batched into a single element.</p>\n<p>This can be used to get <strong>features as a tensor-array, or features and labels</strong> as a tuple or dictionary (of tensor-arrays) depending upon how the original dataset was created.</p>\n<p>Check this <a href=""https://stackoverflow.com/a/68851420/8582902"">answer</a> on SO for an example that unpacks features and labels into a tuple of tensor-arrays.</p>\n', 'IsAccepted': False, 'CreationDate': 1629391343}, {'QuestionId': 68612779, 'AnswerId': 68613019, 'URL': 'https://stackoverflow.com/questions/68612779/efficient-way-to-iterate-over-tf-data-dataset/68613019#68613019', 'QuestionTitle': 'Efficient way to iterate over tf.data.Dataset', 'Answer': '<p>You can use <code>.map(map_func)</code> function which is an efficient way to apply some preprocessing on each sample in your dataset. It runs the <code>map_func</code> on each sample of your dataset in parallel. You can even set number of parallel calls by <code>num_parallel_calls</code> argument. <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""noreferrer"">[Reference]</a></p>\n<p>Here is an example from tensorflow website:</p>\n<pre><code>dataset = tf.data.Dataset.range(1, 6)  # ==&gt; [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1) # instead of adding 1 to each sample in a for loop\nlist(dataset.as_numpy_iterator())      # ==&gt; [ 2, 3, 4, 5, 6 ]\n</code></pre>\n<p>You can pass a function as well:</p>\n<pre><code>def my_map(x): # if dataset has y, it should be like &quot;def my_map(x,y)&quot; and &quot;return x,y&quot;\n  return x+1  \n                                                  \ndataset = tf.data.Dataset.range(1, 6)  # ==&gt; [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(my_map)          # instead of adding 1 to each sample in a for loop\nlist(dataset.as_numpy_iterator())      # ==&gt; [ 2, 3, 4, 5, 6 ]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1627840349}, {'QuestionId': 48126690, 'AnswerId': 68037681, 'URL': 'https://stackoverflow.com/questions/48126690/how-to-make-tf-data-dataset-return-all-of-the-elements-in-one-call/68037681#68037681', 'QuestionTitle': 'How to make tf.data.Dataset return all of the elements in one call?', 'Answer': ""<p>Adding on John's answer:</p>\n<pre><code>total = []\nfor element in val_ds.as_numpy_iterator(): \n  total.append(element[1])\n\nall_total = np.concatenate(total)\nprint(all_total)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1624029511}, {'QuestionId': 64821948, 'AnswerId': 64848443, 'URL': 'https://stackoverflow.com/questions/64821948/iterating-on-tensorfow-dataset-returns-always-a-differently-sorted-array/64848443#64848443', 'QuestionTitle': 'Iterating on Tensorfow Dataset returns always a differently sorted array', 'Answer': ""<p>In addition to Lescurel's answer, another working solution seems to be this piece of code from Kaggle that uses <code>sklearn</code>:</p>\n<pre><code>from sklearn.model_selection import train_test_split\n# Extract target values from the vanilla training dataset.\n# Indices are generated along with the target values, which are used to filter dataset.\ny_targets = np.array([ target.numpy() for _, target in iter(d_shuffled) ])\nX_indices = np.arange(len(y_targets))\n\ny_targets = y_targets.reshape((-1,))\ny_targets.shape\n\n#stratify array-like, default=None If not None, data is split in a stratified fashion, using this as the class labels.\nX_train_indices, X_val_indices, y_train_targets, y_val_targets = train_test_split(\n    X_indices, y_targets, test_size=0.15, stratify=None, random_state=53)\n\nX_test_indices, X_val_indices, y_test_targets, y_val_targets = train_test_split(\n    X_val_indices, y_val_targets, test_size=0.5, stratify=None, random_state=53)\n\ndef get_selected_dataset(ds, X_indices_np):\n    # Make a tensor of type tf.int64 to match the one by Dataset.enumerate(). \n    X_indices_ts = tf.constant(X_indices_np, dtype=tf.int64)\n    \n    def is_index_in(index, rest):\n        # Returns True if the specified index value is included in X_indices_ts.\n        #\n        # '==' compares the specified index value with each values in X_indices_ts.\n        # The result is a boolean tensor, looks like [ False, True, ..., False ].\n        # reduce_any() returns Ture if True is included in the specified tensor.\n        return tf.math.reduce_any(index == X_indices_ts)\n    \n    def drop_index(index, rest):\n        return rest\n\n    # Dataset.enumerate() is similter to Python's enumerate().\n    # The method adds indices to each elements. Then, the elements are filtered\n    # by using the specified indices. Finally unnecessary indices are dropped.\n    selected_ds = ds \\\n        .enumerate() \\\n        .filter(is_index_in) \\\n        .map(drop_index)\n    return selected_ds\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1605466179}, {'QuestionId': 64821948, 'AnswerId': 64822500, 'URL': 'https://stackoverflow.com/questions/64821948/iterating-on-tensorfow-dataset-returns-always-a-differently-sorted-array/64822500#64822500', 'QuestionTitle': 'Iterating on Tensorfow Dataset returns always a differently sorted array', 'Answer': '<h2>Your problem :</h2>\n<p>by calling <code>shuffle</code> on the <strong>whole</strong> dataset before splitting it, you actually reshuffle the dataset after each exhaustion of the dataset. Here is what is happening:</p>\n<ul>\n<li>The first call of <code>y = np.concatenate([y for x, y in test_dataset], axis=0)</code> will exhaust the test dataset</li>\n<li>The second call of <code>y = np.concatenate([y for x, y in test_dataset], axis=0)</code> will see that test_dataset is exhausted, and will trigger:\n<ol>\n<li>A reshuffle of the <strong>whole</strong> dataset</li>\n<li>The call to skip to get a dataset of the right size</li>\n</ol>\n</li>\n</ul>\n<p>You end up with potentially samples of your train dataset of the first exhaustion in the test dataset of the second round.</p>\n<h2>The solution</h2>\n<p>If we look at the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"" rel=""nofollow noreferrer""><code>tf.data.Dataset.suffle</code></a> :</p>\n<blockquote>\n<p>reshuffle_each_iteration (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to <strong>True</strong>.)</p>\n</blockquote>\n<p><strong>Set it to false</strong> to have a deterministic shuffle. If you still want to shuffle your training set each epoch, you need to call shuffle on the train set.</p>\n<h2>A dummy example :</h2>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\ntf.random.set_seed(0) # reproducibility\na = tf.range(10)\nds = tf.data.Dataset.from_tensor_slices(a)\nds_shuffled = ds.shuffle(10,reshuffle_each_iteration=False)\nds_train = ds_shuffled.take(7)\nds_train = ds_train.shuffle(7)\nds_test = ds_shuffled.skip(7)\n</code></pre>\n<p>Running it :</p>\n<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; [x.numpy() for x in ds_test]\n[5, 8, 4]\n&gt;&gt;&gt; [x.numpy() for x in ds_test]\n[5, 8, 4]\n&gt;&gt;&gt; [x.numpy() for x in ds_train]\n[1, 3, 7, 2, 6, 9, 0]\n&gt;&gt;&gt; [x.numpy() for x in ds_train]\n[3, 9, 6, 7, 2, 1, 0]\n</code></pre>\n<p><em>Try running it with <code>reshuffle_each_iteration=True</code> to see what happened in your own code</em></p>\n', 'IsAccepted': True, 'CreationDate': 1605276999}, {'QuestionId': 61754089, 'AnswerId': 61873614, 'URL': 'https://stackoverflow.com/questions/61754089/how-to-repeat-a-tf-data-dataset/61873614#61873614', 'QuestionTitle': 'How to repeat a tf.data.Dataset', 'Answer': '<p>We can do it as below, </p>\n\n<ol>\n<li>Each element is repeated using <code>tf.repeat</code> in <code>map</code> function.</li>\n<li>Flatten the result using <code>flat_map</code>.</li>\n</ol>\n\n<p><strong>Code -</strong></p>\n\n<pre><code>%tensorflow_version 2.x\nimport tensorflow as tf\n\ndataset = tf.data.Dataset.range(2).map(lambda x : tf.repeat(x,3)).flat_map(lambda y: tf.data.Dataset.from_tensor_slices(y))\n\nlist(dataset.as_numpy_iterator())\n</code></pre>\n\n<p><strong>Output -</strong></p>\n\n<pre><code>[0, 0, 0, 1, 1, 1]\n</code></pre>\n\n<p>Hope this answers your question. Happy Learning.</p>\n', 'IsAccepted': False, 'CreationDate': 1589817007}, {'QuestionId': 48126690, 'AnswerId': 61333590, 'URL': 'https://stackoverflow.com/questions/48126690/how-to-make-tf-data-dataset-return-all-of-the-elements-in-one-call/61333590#61333590', 'QuestionTitle': 'How to make tf.data.Dataset return all of the elements in one call?', 'Answer': '<p>In <strong>Tensorflow 2.0</strong></p>\n\n<p>You can enumerate the dataset using <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#enumerate"" rel=""noreferrer"">as_numpy_iterator</a></p>\n\n<pre><code>for element in Xtrain.as_numpy_iterator(): \n  print(element) \n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1587424486}, {'QuestionId': 60250039, 'AnswerId': 60264672, 'URL': 'https://stackoverflow.com/questions/60250039/cant-convert-a-tf-data-dataset-object-to-a-numpy-iterator/60264672#60264672', 'QuestionTitle': 'Can&#39;t convert a tf.data.Dataset object to a numpy iterator', 'Answer': ""<p>As @szymon mentioned, tensorflow-1.14 does not support the <code>as_numpy_iterator</code>. You should move your code to <code>tf&gt;=2.0</code> <br /></p>\n\n<p>A handy tip which I frequently use is firing up a REPL python shell in one of the bash shells and use <code>dir(tf.data.Dataset)</code> to list all the attributes &amp; methods that can be called from that object. You can further use the <code>help(tf.data.Dataset.xxx)</code> for parameters and return values of that method. </p>\n\n<pre><code>&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; dir(tf.data.Dataset) \n... &lt;output&gt;\n\n&gt;&gt;&gt; help(tf.data.Dataset.from_tensor_slices)\n... and so on\n</code></pre>\n\n<p>If you do the same, you'll find that <code>as_numpy_iterator</code> won't be present in the <code>dir(tf.data.Dataset)</code> list output, hence the error.</p>\n"", 'IsAccepted': False, 'CreationDate': 1581949927}, {'QuestionId': 60250039, 'AnswerId': 60251457, 'URL': 'https://stackoverflow.com/questions/60250039/cant-convert-a-tf-data-dataset-object-to-a-numpy-iterator/60251457#60251457', 'QuestionTitle': 'Can&#39;t convert a tf.data.Dataset object to a numpy iterator', 'Answer': '<p>You are using wrong <code>tensorflow</code> and <code>tensorflow_datasets</code> versions. </p>\n\n<p><strong>Please use <code>2.x</code> unless you need <code>1.x</code> for some very specific reasons.</strong></p>\n\n<p>This code works if you use <code>tensorflow 2.1.0</code> and <code>tensorflow_datasets 2.0.0</code>. Proper documentation for <code>1.x</code> of <code>tf.data.Dataset</code> can be found <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">here</a> and it has no such method indeed.</p>\n', 'IsAccepted': True, 'CreationDate': 1581875798}, {'QuestionId': 57725172, 'AnswerId': 58158402, 'URL': 'https://stackoverflow.com/questions/57725172/iterating-over-a-dataset-tf-2-0-with-for-loop/58158402#58158402', 'QuestionTitle': 'Iterating over a Dataset TF 2.0 with for loop', 'Answer': ""<p>It is not very clear what you want to get at your output.\nIf you want to get the values of the dataset output you should execute eagerly.\nExample:</p>\n\n<pre><code>tf.compat.v1.enable_eager_execution()\n\ndef read_dataset_new(filename, target='delay'):\n    ds = tf.data.TFRecordDataset(filename)\n    ds = ds.map(lambda buf: parse(buf, target=target))\n    ds = ds.batch(1)\n    return ds\n# This should return your key values for each example.\nfor features, labels in read_dataset_new(self.tf_rcrds_fl_nm):\n    features_keys = features.keys()\n# This should return your tensor values if they supposed to be numeric.\nfor features, labels in read_dataset_new(self.tf_rcrds_fl_nm):\n    features_array = numpy.array(features)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1569784400}, {'QuestionId': 56426839, 'AnswerId': 56429668, 'URL': 'https://stackoverflow.com/questions/56426839/why-does-tf-data-dataset-map-run-only-once/56429668#56429668', 'QuestionTitle': 'Why does `tf.data.Dataset.map` run only once?', 'Answer': '<p>You need to use <code>tf.random</code> module, because native python will only generate numbers\nonce </p>\n\n<pre class=""lang-py prettyprint-override""><code>dataset1 = tf.data.Dataset.from_tensor_slices(([1]*20))\ndataset1 = (dataset1\n            .batch(4)\n            .map(lambda x: x+tf.random.uniform((), 0, 20, tf.int32)))\n\nfor batch in iter(dataset1):\n    print(batch)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1559572952}, {'QuestionId': 56426839, 'AnswerId': 56427351, 'URL': 'https://stackoverflow.com/questions/56426839/why-does-tf-data-dataset-map-run-only-once/56427351#56427351', 'QuestionTitle': 'Why does `tf.data.Dataset.map` run only once?', 'Answer': '<p>Any tensorflow declaration is a declaration of an execution graph, what must be realy run extra via <code>sess = tf.Session() , sess.run(object) , sess.run(dataset1</code> ) in your case </p>\n', 'IsAccepted': False, 'CreationDate': 1559564742}, {'QuestionId': 48126690, 'AnswerId': 53454114, 'URL': 'https://stackoverflow.com/questions/48126690/how-to-make-tf-data-dataset-return-all-of-the-elements-in-one-call/53454114#53454114', 'QuestionTitle': 'How to make tf.data.Dataset return all of the elements in one call?', 'Answer': '<p><code>tf.data</code> API creates a tensor called <code>\'tensors/component\'</code> with the appropriate prefix/suffix if applicable). after you create the instance. You can evaluate the tensor by name and use it as a batch size.</p>\n\n<pre><code>#Ignore the warnings\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\'figure.figsize\'] = (8,7)\n%matplotlib inline\n\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(""MNIST_data/"")\n\nXtrain = mnist.train.images[mnist.train.labels &lt; 2]\nytrain = mnist.train.labels[mnist.train.labels &lt; 2]\n\nprint(Xtrain.shape)\n#(11623, 784)\nprint(ytrain.shape)\n#(11623,)  \n\n#Data parameters\nnum_inputs = 28\nnum_classes = 2\nnum_steps=28\n\n# create the training dataset\nXtrain = tf.data.Dataset.from_tensor_slices(Xtrain).map(lambda x: tf.reshape(x,(num_steps, num_inputs)))\n# apply a one-hot transformation to each label for use in the neural network\nytrain = tf.data.Dataset.from_tensor_slices(ytrain).map(lambda z: tf.one_hot(z, num_classes))\n# zip the x and y training data together and batch and Prefetch data for faster consumption\ntrain_dataset = tf.data.Dataset.zip((Xtrain, ytrain)).batch(128).prefetch(128)\n\niterator = tf.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\nX, y = iterator.get_next()\n\ntraining_init_op = iterator.make_initializer(train_dataset)\n\ndef get_tensors(graph=tf.get_default_graph()):\n    return [t for op in graph.get_operations() for t in op.values()]\n\nget_tensors()\n#&lt;tf.Tensor \'tensors_1/component_0:0\' shape=(11623,) dtype=uint8&gt;,\n#&lt;tf.Tensor \'batch_size:0\' shape=() dtype=int64&gt;,\n#&lt;tf.Tensor \'drop_remainder:0\' shape=() dtype=bool&gt;,\n#&lt;tf.Tensor \'buffer_size:0\' shape=() dtype=int64&gt;,\n#&lt;tf.Tensor \'IteratorV2:0\' shape=() dtype=resource&gt;,\n#&lt;tf.Tensor \'IteratorToStringHandle:0\' shape=() dtype=string&gt;,\n#&lt;tf.Tensor \'IteratorGetNext:0\' shape=(?, 28, 28) dtype=float32&gt;,\n#&lt;tf.Tensor \'IteratorGetNext:1\' shape=(?, 2) dtype=float32&gt;,\n#&lt;tf.Tensor \'TensorSliceDataset:0\' shape=() dtype=variant&gt;,\n#&lt;tf.Tensor \'MapDataset:0\' shape=() dtype=variant&gt;,\n#&lt;tf.Tensor \'TensorSliceDataset_1:0\' shape=() dtype=variant&gt;,\n#&lt;tf.Tensor \'MapDataset_1:0\' shape=() dtype=variant&gt;,\n#&lt;tf.Tensor \'ZipDataset:0\' shape=() dtype=variant&gt;,\n#&lt;tf.Tensor \'BatchDatasetV2:0\' shape=() dtype=variant&gt;,\n#&lt;tf.Tensor \'PrefetchDataset:0\' shape=() dtype=variant&gt;]\n\nsess = tf.InteractiveSession()\nprint(\'Size of Xtrain: %d\' % tf.get_default_graph().get_tensor_by_name(\'tensors/component_0:0\').eval().shape[0])\n#Size of Xtrain: 11623\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1543018338}, {'QuestionId': 52925767, 'AnswerId': 52931411, 'URL': 'https://stackoverflow.com/questions/52925767/cannot-iterate-over-tf-data-dataset/52931411#52931411', 'QuestionTitle': 'Cannot iterate over tf.data.Dataset', 'Answer': ""<p>I found the solution and I'm sharing in case somebody will run into this problem. The thing is that, as I'm defining a new dataset after the session has been initialised it, it doesn't have the new operation I'm adding for the new dataset (In this case I'm using a new filter everytime I create a new dataset) and that's why the session can't find the operation. To overcome the problem I defined all the datasets I needed to use before the session is initialised and I used a filter that takes as input a placeholder so that I always use the same filter feeded everytime at iterator init time with the right value.</p>\n"", 'IsAccepted': True, 'CreationDate': 1540217476}, {'QuestionId': 48126690, 'AnswerId': 50704517, 'URL': 'https://stackoverflow.com/questions/48126690/how-to-make-tf-data-dataset-return-all-of-the-elements-in-one-call/50704517#50704517', 'QuestionTitle': 'How to make tf.data.Dataset return all of the elements in one call?', 'Answer': ""<p>Not sure if this still works in latest versions of TensorFlow but if this is absolutely needed a hacky solution is to create a batch that's bigger than the dataset size. You don't need to know how big the dataset is, just request a batch size that's larger.</p>\n"", 'IsAccepted': False, 'CreationDate': 1528214680}, {'QuestionId': 50330993, 'AnswerId': 50334392, 'URL': 'https://stackoverflow.com/questions/50330993/how-to-use-tf-datasets-with-iterator-in-tensorflow/50334392#50334392', 'QuestionTitle': 'How to use tf.datasets with iterator in Tensorflow', 'Answer': '<p>These two lines seem to be the source of the problem:</p>\n\n<pre><code>ds_file = tf.data.TextLineDataset(file)\n\nds = ds_file.flat_map(lambda file: (tf.data.TextLineDataset(file).skip(1))) #remove CSV headers\n</code></pre>\n\n<p>The first line creates a dataset from the lines of the file (or files) in named in <code>file</code>. The second line then creates a dataset for each element  in <code>ds_file</code> that treats each element (which is a line of text from <code>file</code>) as another filename. The <code>NotFoundError</code> you are seeing is raised when the first line of <code>file</code>, which appears to be a CSV header, is treated as a filename.</p>\n\n<p>The fix is relatively simple, fortunately, as you can use <code>Dataset.list_files()</code> to create a dataset of files matching your glob, and then the <code>Dataset.flat_map()</code> will operate on filenames:</p>\n\n<pre><code># Create a dataset of filenames.\nds_file = tf.data.Dataset.list_files(""./NSEOIL.csv"")\n\n# For each filename in `ds_file`, read the lines from that file (skipping the\n# header).\nds = ds_file.flat_map(lambda file: (tf.data.TextLineDataset(file).skip(1)))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1526313040}, {'QuestionId': 48126690, 'AnswerId': 48127601, 'URL': 'https://stackoverflow.com/questions/48126690/how-to-make-tf-data-dataset-return-all-of-the-elements-in-one-call/48127601#48127601', 'QuestionTitle': 'How to make tf.data.Dataset return all of the elements in one call?', 'Answer': '<p>In short, there is not a good way to get the size/length; <code>tf.data.Dataset</code> is built for pipelines of data, so has an iterator structure (in my understanding and according to my read of <a href=""https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/data/ops/dataset_ops.py"" rel=""nofollow noreferrer"">the Dataset ops code</a>.  From the <a href=""https://www.tensorflow.org/versions/master/programmers_guide/datasets"" rel=""nofollow noreferrer"">programmer\'s guide</a>: </p>\n\n<blockquote>\n  <p>A <code>tf.data.Iterator</code> provides the main way to extract elements from a dataset. The operation returned by <code>Iterator.get_next()</code> yields the next element of a Dataset when executed, and typically acts as the interface between input pipeline code and your model.</p>\n</blockquote>\n\n<p>And, by their nature, iterators do not have a convenient notion of size/length; see here: <a href=""https://stackoverflow.com/questions/3345785/getting-number-of-elements-in-an-iterator-in-python"">Getting number of elements in an iterator in Python</a></p>\n\n<p>More generally though, why does this problem arise?  If you are calling <code>batch</code>, you are also getting a <code>tf.data.Dataset</code>, so whatever you are running on a batch you should be able to run on the whole dataset; it will iterate through all the elements and calculate validation accuracy. Put differently, I don\'t think you actually need the size/length to do what you want to do.</p>\n', 'IsAccepted': False, 'CreationDate': 1515243647}]","{52925767, 66874943}","['<p>This is because the data files are shuffled and the dataset is shuffled with <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=ru#shuffle"" rel=""nofollow noreferrer""><code>dataset.shuffle()</code></a>.</p>\n<p>With <code>dataset.shuffle()</code>, the data will be shuffled in a different way on each iteration by default.</p>\n<p>One can remove <code>shuffle_files=True</code> and set the argument <code>reshuffle_each_iteration=False</code> to prevent reshuffling on different iterations.</p>\n<p>The <code>.take()</code> function does not imply determinism. It will just take N items from the dataset in whichever order the dataset gives them.</p>\n<pre class=""lang-py prettyprint-override""><code># Construct a tf.data.Dataset\nds = tfds.load(\'mnist\', split=\'train\', shuffle_files=False)\n\n# Build your input pipeline\nds = ds.shuffle(1024, reshuffle_each_iteration=False).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n\nsingle_batch_dataset = ds.take(1)\n\nfor example in single_batch_dataset:\n    image, label = example[&quot;image&quot;], example[&quot;label&quot;]\n    print(label)\n    \nfor example in single_batch_dataset:\n    image, label = example[&quot;image&quot;], example[&quot;label&quot;]\n    print(label)\n</code></pre>\n<p>Output:</p>\n<pre class=""lang-py prettyprint-override""><code>tf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)\ntf.Tensor([4 6 8 5 1 4 5 8 1 4 6 6 8 6 6 9 4 2 3 0 5 9 2 1 3 1 8 6 4 4 7 1], shape=(32,), dtype=int64)\n</code></pre>\n', ""The thing is that, as I'm defining a new dataset after the session has been initialised it, it doesn't have the new operation I'm adding for the new dataset (In this case I'm using a new filter everytime I create a new dataset) and that's why the session can't find the operation. To overcome the problem I defined all the datasets I needed to use before the session is initialised and I used a filter that takes as input a placeholder so that I always use the same filter feeded everytime at iterator init time with the right value.</p>\n""]","{'https://stackoverflow.com/questions/52925767/cannot-iterate-over-tf-data-dataset/52931411#52931411', 'https://stackoverflow.com/questions/66874943/why-iterations-over-the-same-tf-data-dataset-give-different-data-each-iteration/66874975#66874975'}",{52925767},0.203321784060924,0.13381378683979323
16,55176818,tf.keras.layers.Layer,Lack of Alternative Solutions/Documentation,How to support masking in custom tf.keras.layers.Layer,"<p>I'm implementing a custom <code>tf.keras.layers.Layer</code> that needs to support masking.</p>

<p>Consider the following scenario</p>

<pre class=""lang-py prettyprint-override""><code>embedded = tf.keras.layer.Embedding(input_dim=vocab_size + 1, 
                                    output_dim=n_dims, 
                                    mask_zero=True)
x = MyCustomKerasLayers(embedded)
</code></pre>

<p>Now per the documentation</p>

<blockquote>
  <p><code>mask_zero</code>: Whether or not the input value 0 is a special ""padding"" value that should be masked out. This is useful when using recurrent layers which may take variable length input. <strong>If this is True then all subsequent layers in the model need to support masking or an exception will be raised</strong>. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).</p>
</blockquote>

<p>I wonder, what does that mean? Looking through <a href=""https://www.tensorflow.org/tutorials/eager/custom_layers"" rel=""noreferrer"">TensorFlow's custom layers guide</a> and the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer"" rel=""noreferrer"">tf.keras.layer.Layer</a> documentation it is not clear what should be done to support masking</p>

<ol>
<li><p>How do I support masking?</p></li>
<li><p>How do I access the mask from the past layer? </p></li>
<li><p>Assuming input of <code>(batch, time, channels)</code> or `(batch, time) would the masks look different? What will be their shapes?</p></li>
<li><p>How do I pass it on to the next layer? </p></li>
</ol>
","<ol>
<li><p>To support masking one should implement the <code>compute_mask</code> method inside the custom layer</p></li>
<li><p>To access the mask, simply add as the second positional argument in the <code>call</code> method the argument <code>mask</code>, and it will be accessible (ex. <code>call(self, inputs, mask=None)</code>)</p></li>
<li><p>This cannot be guessed, it is the layer's before responsible to calculate the mask </p></li>
<li><p>Once you implemented the <code>compute_mask</code> passing the mask to the next layer happens automatically - excluding the case of model subclassing, which in this case it is up to you to calculate masks and pass them on.</p></li>
</ol>

<p>Example:</p>

<pre class=""lang-py prettyprint-override""><code>class MyCustomKerasLayers(tf.keras.layers.Layer):
    def __init__(self, .......):
        ...

    def compute_mask(self, inputs, mask=None):
        # Just pass the received mask from previous layer, to the next layer or 
        # manipulate it if this layer changes the shape of the input
        return mask

    def call(self, input, mask=None):
        # using 'mask' you can access the mask passed from the previous layer
</code></pre>

<p>Notice that this example just passes on the mask, if the layer will output a shape different than the one received, you should change the mask accordingly in <code>compute_mask</code> to pass on the correct one</p>

<h3>EDIT</h3>

<p>Now explanation is also included in the <a href=""https://www.tensorflow.org/beta/guide/keras/masking_and_padding"" rel=""nofollow noreferrer""><code>tf.keras</code> masking and padding documentation</a>.</p>
","{56508993, 69590274, 55669699, 58669730, 68711877, 59590085, 71426471, 54093203, 57455350, 65577783}","[{'QuestionId': 71426471, 'AnswerId': 71511263, 'URL': 'https://stackoverflow.com/questions/71426471/computing-masking-layer-and-using-a-few-layers-later-in-keras/71511263#71511263', 'QuestionTitle': 'Computing Masking layer and using a few layers later in Keras', 'Answer': ""<p>What I do is using a custom function to get the mask :</p>\n<pre><code>def get_mask_from_lengths(lengths, max_len=None):\n    if max_len is None:\n        max_len = tf.reduce_max(lengths)\n    ids = tf.range(0, max_len)\n    mask = ids &lt; lengths\n    return mask\n</code></pre>\n<p>Then I define the model:</p>\n<pre><code>sequenceLength = 5\ninputFeatures = 1\n\ninputs = tf.keras.layers.Input(shape=(sequenceLength, inputFeatures))\nlengths = tf.keras.layers.Input(shape=(1,))  # vector containing the length of each element of the batch\n\nx = tf.keras.layers.Dense(units=3)(inputs)  # some other layer\nmask = get_mask_from_lengths(lengths=lengths)\noutput = tf.keras.layers.GRU(units=2)(x, mask=mask)\nmodel = tf.keras.Model(inputs=[inputs, lengths], outputs=output)\n\nmodel.compile(loss='mse', optimizer='adam')\nmodel.summary()\n</code></pre>\n<p>An example:</p>\n<pre><code>x1 = tf.reshape(tf.convert_to_tensor([10, 3, 5, 3, 5]), (1, -1))\nx2 = tf.reshape(tf.convert_to_tensor([11, 9, 120, 255, 255]), (1, -1))\ninput_tensor = tf.concat([x1, x2], axis=0)\nlength_tensor = tf.reshape([5, 3], (-1, 1))  # first sequence x1 is full and x2 has three elements not equal to the masking value 255 (should create a function to get this tensor from input_tensor)\nout_tensor = tf.random.uniform(shape=(2, 2))\n\nmodel.fit([input_tensor, length_tensor], out_tensor, epochs=2)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1647515204}, {'QuestionId': 65577783, 'AnswerId': 66270858, 'URL': 'https://stackoverflow.com/questions/65577783/seeking-masking-support-for-dense-layer-in-keras/66270858#66270858', 'QuestionTitle': 'Seeking Masking support for Dense Layer in Keras', 'Answer': '<p>Not sure if you ever received an answer, but in case you haven\'t, you might look at this: <a href=""https://keras.io/guides/understanding_masking_and_padding/"" rel=""nofollow noreferrer"">Masking Guide</a></p>\n<p>I was just going to look to see if Dense supports masking also. Here is the relevant quote from the linked guide:</p>\n<blockquote>\n<p>If you have a custom layer that does not modify the time dimension, and if you want it to be able to propagate the current input mask, you should set <code>self.supports_masking = True</code> in the layer constructor. In this case, the default behavior of <code>compute_mask()</code> is to just pass the current mask through.</p>\n</blockquote>\n<p>This to me, says that Dense will propagate the mask.</p>\n', 'IsAccepted': True, 'CreationDate': 1613700807}, {'QuestionId': 54093203, 'AnswerId': 70377448, 'URL': 'https://stackoverflow.com/questions/54093203/define-a-binary-mask-in-keras/70377448#70377448', 'QuestionTitle': 'Define a binary mask in Keras', 'Answer': '<p>Looks like <a href=""https://keras.io/api/layers/merging_layers/multiply/"" rel=""nofollow noreferrer"">https://keras.io/api/layers/merging_layers/multiply/</a> is the answer, I am going to try and post results. Like this:</p>\n<pre><code>&gt;&gt;&gt; tf.keras.layers.Multiply()([np.arange(5).reshape(5, 1),\n...                             np.arange(5, 10).reshape(5, 1)])\n&lt;tf.Tensor: shape=(5, 1), dtype=int64, numpy=\narray([[ 0],\n     [ 6],\n     [14],\n     [24],\n     [36]])&gt;\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1639650665}, {'QuestionId': 69590274, 'AnswerId': 70268806, 'URL': 'https://stackoverflow.com/questions/69590274/warning-custom-mask-layers-require-a-config-and-must-override-when-saving-the/70268806#70268806', 'QuestionTitle': 'Warning : Custom mask layers require a config and must override when saving the model in keras', 'Answer': '<p>@user123 Agree with you that it was an issue with old versions (from TF2.5, TF2.6, and TF2.7).</p>\n<p>This was resolved in recent tf-nightly. <a href=""https://colab.research.google.com/gist/jvishnuvardhan/1cbd250ce696699f846808affb7ec57d/untitled.ipynb"" rel=""noreferrer"">Here</a> is a gist for reference. If you want to use a stable version, then it will be available in the upcoming TF2.8 in near future. Thanks!</p>\n', 'IsAccepted': False, 'CreationDate': 1638926243}, {'QuestionId': 69590274, 'AnswerId': 69663259, 'URL': 'https://stackoverflow.com/questions/69590274/warning-custom-mask-layers-require-a-config-and-must-override-when-saving-the/69663259#69663259', 'QuestionTitle': 'Warning : Custom mask layers require a config and must override when saving the model in keras', 'Answer': '<p>Two other approaches to try:</p>\n<ol>\n<li>Save the model using a directory path instead of with a path that ends in the <code>.h5</code> extension.  Under the hood, <code>save</code> does different things if you send a path that ends with <code>.h5</code>.  If you send a directory, it will use the newer SavedModel format.  You can then directly load the model with:</li>\n</ol>\n<pre><code>from tensorflow.keras.models import load_model\n\n\nnew_model = load_model(\'&lt;path to directory used in save&gt;\')\n</code></pre>\n<p><a href=""https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk</a></p>\n<ol start=""2"">\n<li>Only save the weights.  To load the model, use your current model creation code to instantiate the object, then load the weights into the empty model object.  This used to be the only way!</li>\n</ol>\n<pre class=""lang-py prettyprint-override""><code>model.save_weights(\'my_model_weights.h5\')\n...\nnew_model = &lt;build your model with your model building code&gt;\nnew_model.load_weights(\'my_model_weights.h5\')\n</code></pre>\n<p>Ref: <a href=""https://keras.io/getting_started/faq/"" rel=""nofollow noreferrer"">https://keras.io/getting_started/faq/</a>. Weights only saving</p>\n<p>In the FAQ, there are also several other suggestions for how to handle it, but for your situation these two will probably cover it.</p>\n<hr />\n<p>Here is a good snippet to add after your train code to make sure that the export works and is not the cause of a performance issue at inference:</p>\n<pre><code># From: https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading\n# Train the model.\ntest_input = np.random.random((128, 32))\ntest_target = np.random.random((128, 1))\nmodel.fit(test_input, test_target)\n\n# Calling `save(\'my_model\')` creates a SavedModel folder `my_model`.\nmodel.save(&quot;my_model&quot;)\n\n# It can be used to reconstruct the model identically.\nreconstructed_model = keras.models.load_model(&quot;my_model&quot;)\n\n# Let\'s check:\nnp.testing.assert_allclose(\n    model.predict(test_input), reconstructed_model.predict(test_input)\n)\n\n# The reconstructed model is already compiled and has retained the optimizer\n# state, so training can resume:\nreconstructed_model.fit(test_input, test_target)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1634824395}, {'QuestionId': 68711877, 'AnswerId': 68713830, 'URL': 'https://stackoverflow.com/questions/68711877/custom-layer-in-keras/68713830#68713830', 'QuestionTitle': 'Custom layer in Keras', 'Answer': ""<p>As default, shape of inputs is [batch_size,width,height,channels], and, when you create your model, batch_size is set to None.</p>\n<pre><code>\nimport os\n# os.environ['KERAS_BACKEND'] = 'theano'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # suppress Tensorflow messages\nimport tensorflow as tf\nfrom keras.layers import *\nfrom keras.models import *\n\n\nclass CustomLinear(Layer):\n  def __init__(self, batch_size,units=32, input_dim=32):\n      super(CustomLinear, self).__init__()\n      self.batch_size = batch_size\n      w_init = tf.random_normal_initializer()\n      self.w = tf.Variable(\n          initial_value=w_init(shape=(input_dim, units), dtype=&quot;float32&quot;),\n          trainable=True,\n      )\n      b_init = tf.zeros_initializer()\n      self.b = tf.Variable(\n          initial_value=b_init(shape=(units,), dtype=&quot;float32&quot;), trainable=True\n      )\n\n  def call(self, inputs):\n    print('inputs', inputs.shape)\n    # for index in range(self.batch_size):\n        # print(index)\n    return tf.matmul(inputs, self.w) + self.b\n\n\nbatch_size = 10\n\nmodel = Sequential()\nmodel.add(Input(shape=(2,32)))\nmodel.add(CustomLinear(batch_size = batch_size)) # inputs (None, 2, 32)\n\nx = tf.random.normal((batch_size,2,32)) # dummy data\n\nmodel(x) # inputs (10, 2, 32)\n</code></pre>\n<p>Mostly, batch_size is not required for the calculations within the layer. But, if you still need it, you can add an argument (e.g. batch_size) to your <code>CustomLinear</code>, define your batch_size beforehand, and access to it inside <code>__call__</code> function.</p>\n"", 'IsAccepted': True, 'CreationDate': 1628519347}, {'QuestionId': 55669699, 'AnswerId': 59691966, 'URL': 'https://stackoverflow.com/questions/55669699/keras-masking-output-layer/59691966#59691966', 'QuestionTitle': 'Keras Masking Output Layer', 'Answer': ""<p>Making this work with tensorflow 2.0 and tf.keras.</p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.keras.layers import Multiply, Conv2D, ConvLSTM2D, Input\n\nmain_input = Input(shape=(None, 2, 100, 100), dtype='float32', name='input')\n\nmask=Input(shape=(1, 100, 100), dtype='float32', name='mask')\n\nhidden = ConvLSTM2D(filters=16, \n                    kernel_size=(5, 5),  \n                    padding='same', \n                    return_sequences=False, \n                    data_format='channels_first')(main_input)\n\noutput = Conv2D(filters=1, \n                kernel_size=(1, 1), \n                padding='same',\n                activation='sigmoid',\n                kernel_initializer='glorot_uniform',\n                data_format='channels_first',\n                name='output')(hidden)\noutput_with_mask=Multiply()([output, mask])\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1578721066}, {'QuestionId': 59590085, 'AnswerId': 59590150, 'URL': 'https://stackoverflow.com/questions/59590085/how-do-i-mask-the-input-of-lstm-in-tf-keras/59590150#59590150', 'QuestionTitle': 'how do I mask the input of lstm in tf.keras', 'Answer': '<p>Have you tried checking the Docs for <code>Tensorflow</code>? Go to this <a href=""https://www.tensorflow.org/guide/keras/masking_and_padding"" rel=""nofollow noreferrer"">link</a> I think it will help you.<br>\nIn the above example, they add <code>mask_zero=True</code></p>\n\n<pre><code>embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\nmasked_output = embedding(padded_inputs)\n\nprint(masked_output._keras_mask)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1578136555}, {'QuestionId': 58669730, 'AnswerId': 58670292, 'URL': 'https://stackoverflow.com/questions/58669730/tensorflow-mask-tensor-element-with-condition/58670292#58670292', 'QuestionTitle': 'Tensorflow - mask tensor element with condition', 'Answer': '<p>I prefer to use <code>tf.tile()</code> operation to expand the mask:</p>\n\n<pre><code>data = tf.constant([[[0, 0], [1, 1], [2, 2]], [[3, 3], [4, 4], [5, 5]]], dtype=tf.float32)\nmask = tf.constant([[True, True, False], [False, True, True]])\n\nmask_expand = tf.tile(tf.expand_dims(mask, axis=-1), multiples=[1,1, tf.shape(data)[-1]])\n\nminus_ones = tf.fill(tf.shape(data), tf.constant(-1, dtype=data.dtype))\ndata = tf.where(mask_expand, data, minus_ones)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1572691179}, {'QuestionId': 58669730, 'AnswerId': 58669973, 'URL': 'https://stackoverflow.com/questions/58669730/tensorflow-mask-tensor-element-with-condition/58669973#58669973', 'QuestionTitle': 'Tensorflow - mask tensor element with condition', 'Answer': ""<p>I do a simple workaround (change the shape of mask), maybe there is a better method, but I can't figure out now.</p>\n\n<pre><code># reshape mask to the same shape with data\nbatch_size, total_timestep, feature_dimension = tf.shape(data)\n\n# mask = [[[True], [True], [False]], [[False], [True], [True]]]\nmask = tf.reshape(mask, [batch_size, total_timestep, 1]) # shape=(2, 3, 1)\n# mask = [[[True, True], [True, True], [False, False]], [[False, False], [True, True], [True, True]]]\nmask = tf.broadcast_to(mask, [batch_size, total_timestep, feature_dimension]) # shape=(2, 3, 2)\n\n# adapt mask\ndata = tf.where(mask, data, tf.constant(-1, dtype=data.dtype) )\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1572688454}, {'QuestionId': 57455350, 'AnswerId': 57476741, 'URL': 'https://stackoverflow.com/questions/57455350/custom-layer-cause-tensorflow-python-framework-errors-impl-invalidargumenterror/57476741#57476741', 'QuestionTitle': 'custom layer cause &quot;tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [128] vs. [128,256,256]&quot;', 'Answer': '<p>Because of the attn_mask\'s shape that doesn\'t match the mul in the ""scale_dot_product"" method. So I make some changes:\nFirst, add parameter keep_dim in ""inp_mask"":<code>inp_mask = Lambda(lambda t: K.any(K.not_equal(t, 0), axis=-1, keep_dim=True), name=""Input_mask"")(input)</code>. But it still doesn\'t work.\nSecond, comment the line <code>attn_mask = K.repeat_elements(attn_mask, self.n_head, 0)</code> and do a new method called ""reshape_mask""</p>\n\n<pre><code>def reshape_mask(mask, head_num):\n            if mask is None:\n                return mask\n            seq_len = K.shape(mask)[1]\n            mask = K.expand_dims(mask, axis=1)\n            mask = K.tile(mask, [1, head_num, 1])\n            return K.reshape(mask, (-1, seq_len))\n</code></pre>\n\n<p>Third, rewrite the method scale_dot_product.</p>\n\n<pre><code>def scale_dot_product(query: tf.Tensor,\n                      key: tf.Tensor,\n                      value: tf.Tensor,\n                      attn_mask=None):\n    feature_dim = K.shape(query)[-1]\n    e = K.batch_dot(query, key, axes=2) / K.sqrt(K.cast(feature_dim, dtype=K.floatx()))\n    e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n    if attn_mask is not None:\n        e *= K.cast(K.expand_dims(attn_mask, axis=-2), K.floatx())\n    a = e / (K.sum(e, axis=-1, keepdims=True) + K.epsilon())\n    v = K.batch_dot(a, value)\n    return v\n</code></pre>\n\n<p>Cheers! Cheers! Cheers! Cheers! Cheers! The problem has been solved!</p>\n', 'IsAccepted': False, 'CreationDate': 1565695391}, {'QuestionId': 56508993, 'AnswerId': 56605191, 'URL': 'https://stackoverflow.com/questions/56508993/tesorflow-custom-layer-in-high-level-api-throws-object-has-no-attribute-expec/56605191#56605191', 'QuestionTitle': 'Tesorflow Custom Layer in High level API: throws object has no attribute &#39;_expects_mask_arg&#39; error', 'Answer': ""<p>I just had the same error and it was due to me forgetting to call <code>.__init__()</code> after <code>super()</code>. You did it, but this make me think that this error is due to wrong initialization of the base layer you are deriving from.\nI notice that in the doc example it's not necessary to call <code>build()</code> on the base layer, and it works for me if you remove that function (as it does nothing related to your layer).</p>\n"", 'IsAccepted': False, 'CreationDate': 1560546843}, {'QuestionId': 55669699, 'AnswerId': 55674721, 'URL': 'https://stackoverflow.com/questions/55669699/keras-masking-output-layer/55674721#55674721', 'QuestionTitle': 'Keras Masking Output Layer', 'Answer': ""<p>I think you should input the mask of each sample to the model at the same time.</p>\n\n<p>Here is the suggested code:</p>\n\n<pre><code>from keras.layers import Multiply\n\nmain_input = Input(shape=(None, 2, 100, 100), dtype='float32', name='input')\n\nmask=Input(shape=(1, 100, 100), dtype='float32', name='mask')\n\nhidden = ConvLSTM2D(filters=16, \n                    kernel_size=(5, 5),  \n                    padding='same', \n                    return_sequences=False, \n                    data_format='channels_first')(main_input)\n\noutput = Conv2D(filters=1, \n                kernel_size=(1, 1), \n                padding='same',\n                activation='sigmoid',\n                kernel_initializer='glorot_uniform',\n                data_format='channels_first',\n                name='output')(hidden)\noutput_with_mask=Multiply()([output, mask])\n\nmodel=Model([main_input, mask], output_with_mask)\n</code></pre>\n\n<p>The summary is as follow:</p>\n\n<pre><code>    __________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput (InputLayer)              (None, None, 2, 100, 0                                            \n__________________________________________________________________________________________________\nconv_lst_m2d_7 (ConvLSTM2D)     (None, 16, 100, 100) 28864       input[0][0]                      \n__________________________________________________________________________________________________\noutput (Conv2D)                 (None, 1, 100, 100)  17          conv_lst_m2d_7[0][0]             \n__________________________________________________________________________________________________\nmask (InputLayer)               (None, 1, 100, 100)  0                                            \n__________________________________________________________________________________________________\nmultiply_7 (Multiply)           (None, 1, 100, 100)  0           output[0][0]                     \n                                                                 mask[0][0]                       \n==================================================================================================\nTotal params: 28,881\nTrainable params: 28,881\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1555241014}, {'QuestionId': 55669699, 'AnswerId': 55669770, 'URL': 'https://stackoverflow.com/questions/55669699/keras-masking-output-layer/55669770#55669770', 'QuestionTitle': 'Keras Masking Output Layer', 'Answer': '<p>Creating an new output and use your old output as second hiden layer. </p>\n\n<p>You want to make an second convolution (with an spécial mask) on your ""old output"" to get your new output</p>\n\n<p>Hope it will help you</p>\n', 'IsAccepted': False, 'CreationDate': 1555188684}, {'QuestionId': 54093203, 'AnswerId': 54094377, 'URL': 'https://stackoverflow.com/questions/54093203/define-a-binary-mask-in-keras/54094377#54094377', 'QuestionTitle': 'Define a binary mask in Keras', 'Answer': '<p>You can do the whole thing with a <a href=""https://keras.io/layers/core/#lambda"" rel=""nofollow noreferrer""><code>Lambda</code></a> layer implementing a custom function:</p>\n\n<pre><code>from keras.models import Model\nfrom keras.layers import Input, Lambda\nfrom keras import backend as K\nimport numpy as np\n\n# Masking function factory\ndef mask_img(x_size, y_size=None):\n    if y_size is None:\n        y_size = x_size\n    # Masking function\n    def mask_func(tensors):\n        img, xy = tensors\n        img_shape = K.shape(img)\n        # Make indexing arrays\n        xx = K.arange(img_shape[1])\n        yy = K.arange(img_shape[2])\n        # Get coordinates\n        xy = K.cast(xy, img_shape.dtype)\n        x = xy[:, 0:1]\n        y = xy[:, 1:2]\n        # Make X and Y masks\n        mask_x = (xx &gt;= x) &amp; (xx &lt; x + x_size)\n        mask_y = (yy &gt;= y) &amp; (yy &lt; y + y_size)\n        # Make full mask\n        mask = K.expand_dims(mask_x, 2) &amp; K.expand_dims(mask_y, 1)\n        # Add channels dimension\n        mask = K.expand_dims(mask, -1)\n        # Multiply image and mask\n        mask = K.cast(mask, img.dtype)\n        return img * mask\n    return mask_func\n\n# Model\nimg = Input(shape=(10, 10, 3))  # Small size for test\nxy = Input(shape=(2,))\noutput = Lambda(mask_img(3))([img, xy])\nmodel = Model(inputs=[img, xy], outputs=output)\n\n# Test\nimg_test = np.arange(100).reshape((1, 10, 10, 1)).repeat(3, axis=-1)\nxy_test = np.array([[2, 4]])\noutput_test = model.predict(x=[img_test, xy_test])\nprint(output_test[0, :, :, 0])\n</code></pre>\n\n<p>Output:</p>\n\n<pre class=""lang-none prettyprint-override""><code>[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0. 24. 25. 26.  0.  0.  0.]\n [ 0.  0.  0.  0. 34. 35. 36.  0.  0.  0.]\n [ 0.  0.  0.  0. 44. 45. 46.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1546959317}]","{55176818, 65577783}","['<p>Not sure if you ever received an answer, but in case you haven\'t, you might look at this: <a href=""https://keras.io/guides/understanding_masking_and_padding/"" rel=""nofollow noreferrer"">Masking Guide</a></p>\n<p>I was just going to look to see if Dense supports masking also. Here is the relevant quote from the linked guide:</p>\n<blockquote>\n<p>If you have a custom layer that does not modify the time dimension, and if you want it to be able to propagate the current input mask, you should set <code>self.supports_masking = True</code> in the layer constructor. In this case, the default behavior of <code>compute_mask()</code> is to just pass the current mask through.</p>\n</blockquote>\n<p>This to me, says that Dense will propagate the mask.</p>\n', '<ol>\n<li><p>To support masking one should implement the <code>compute_mask</code> method inside the custom layer</p></li>\n<li><p>To access the mask, simply add as the second positional argument in the <code>call</code> method the argument <code>mask</code>, and it will be accessible (ex. <code>call(self, inputs, mask=None)</code>)</p></li>\n<li><p>This cannot be guessed, it is the layer\'s before responsible to calculate the mask </p></li>\n<li><p>Once you implemented the <code>compute_mask</code> passing the mask to the next layer happens automatically - excluding the case of model subclassing, which in this case it is up to you to calculate masks and pass them on.</p></li>\n</ol>\n\n<p>Example:</p>\n\n<pre class=""lang-py prettyprint-override""><code>class MyCustomKerasLayers(tf.keras.layers.Layer):\n    def __init__(self, .......):\n        ...\n\n    def compute_mask(self, inputs, mask=None):\n        # Just pass the received mask from previous layer, to the next layer or \n        # manipulate it if this layer changes the shape of the input\n        return mask\n\n    def call(self, input, mask=None):\n        # using \'mask\' you can access the mask passed from the previous layer\n</code></pre>\n\n<p>Notice that this example just passes on the mask, if the layer will output a shape different than the one received, you should change the mask accordingly in <code>compute_mask</code> to pass on the correct one</p>\n\n<h3>EDIT</h3>\n\n<p>Now explanation is also included in the <a href=""https://www.tensorflow.org/beta/guide/keras/masking_and_padding"" rel=""nofollow noreferrer""><code>tf.keras</code> masking and padding documentation</a>.</p>\n']","{'https://stackoverflow.com/questions/55176818/how-to-support-masking-in-custom-tf-keras-layers-layer/55201406#55201406', 'https://stackoverflow.com/questions/65577783/seeking-masking-support-for-dense-layer-in-keras/66270858#66270858'}",{65577783},0.22776872710412596,0.08302558708281815
19,61743921,tf.keras,Lack of Alternative Solutions/Documentation,can we build object detection model using Tensorflow or it is only possible with the help f tf.keras,"<p>Is there any way to build object detection model using Tensorflow without any help of tf.keras module?</p>

<p>From Tensorflow documentation I'm  not able to find any example which helps to create model without Keras.</p>
","<p>Keras is a high level API. But if you want to use only Tensorflow then you have to implement the architecture using low level API. You can certainly implement but you have to code it yourself to build all the convolutional layers and dense layer by yourself. </p>
","{44973184, 42955746, 57255942, 45407210, 66798899, 56630132, 52695829, 67228856, 43495836, 45750364, 55049695}","[{'QuestionId': 52695829, 'AnswerId': 76177673, 'URL': 'https://stackoverflow.com/questions/52695829/object-detection-using-tflearn-or-tensorflow-object-detection-api/76177673#76177673', 'QuestionTitle': 'Object Detection using TFLearn or Tensorflow object detection API', 'Answer': '<p>Tensorflow has an object detection api that is capable of using tensorflow-2.0 and has GPU support. You can also play around with the configurations for model inference.</p>\n<p>You can find it here: <a href=""https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/</a>\nI\'d also recommend trying out a few open source models like Faster-RCNN or Yolo models.</p>\n<p>API: <a href=""https://www.analyticsvidhya.com/blog/2020/04/build-your-own-object-detection-model-using-tensorflow-api/"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2020/04/build-your-own-object-detection-model-using-tensorflow-api/</a></p>\n<p>Yolo: <a href=""https://github.com/AlexeyAB/darknet"" rel=""nofollow noreferrer"">https://github.com/AlexeyAB/darknet</a></p>\n', 'IsAccepted': False, 'CreationDate': 1683238155}, {'QuestionId': 52695829, 'AnswerId': 73635250, 'URL': 'https://stackoverflow.com/questions/52695829/object-detection-using-tflearn-or-tensorflow-object-detection-api/73635250#73635250', 'QuestionTitle': 'Object Detection using TFLearn or Tensorflow object detection API', 'Answer': '<p>There is an API to do this very quickly for free/cheap.it is built on Resnet-20</p>\n<p><a href=""https://rapidapi.com/mantis-object-detection-mantis-object-detection-default/api/mantis-object-detection/"" rel=""nofollow noreferrer"">https://rapidapi.com/mantis-object-detection-mantis-object-detection-default/api/mantis-object-detection/</a></p>\n', 'IsAccepted': False, 'CreationDate': 1662552474}, {'QuestionId': 66798899, 'AnswerId': 71938362, 'URL': 'https://stackoverflow.com/questions/66798899/tensorflow-and-keras-for-easy-object-detection/71938362#71938362', 'QuestionTitle': 'Tensorflow and Keras for easy object detection', 'Answer': '<p>Hope this helps others trying to do <code>Transfer learning using tensorflow object detection api</code></p>\n<p>I found this <a href=""https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub"" rel=""nofollow noreferrer"">Transfer learning with TensorFlow Hub</a>, this link is about <code>classification</code> changing the code for <code>object detection</code> should be a nice learning curve for how every tries it out.</p>\n<p>It basically has 3 steps</p>\n<p>This tutorial demonstrates how to:</p>\n<blockquote>\n<ol>\n<li>Use models from TensorFlow Hub with tf.keras.</li>\n<li>Use an image classification model from TensorFlow Hub.</li>\n<li>Do simple transfer learning to fine-tune a model for your own image classes.</li>\n</ol>\n</blockquote>\n<p>You can have a look at the <a href=""https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub#download_the_classifier"" rel=""nofollow noreferrer"">Downloading the model</a> section which has a classifier as a example</p>\n<p>You can check out pre trained object detection model that the <a href=""https://tfhub.dev/tensorflow/collections/object_detection/1"" rel=""nofollow noreferrer"">Tensorflow Hub</a> currently support</p>\n<p>Here are a few good once\'s</p>\n<div class=""s-table-container"">\n<table class=""s-table"">\n<thead>\n<tr>\n<th>Model name</th>\n<th>Speed (ms)</th>\n<th>COCO mAP</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=""https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1"" rel=""nofollow noreferrer"">CenterNet HourGlass104 512x512</a></td>\n<td>70</td>\n<td>41.9</td>\n</tr>\n<tr>\n<td><a href=""https://tfhub.dev/tensorflow/efficientdet/d2/1"" rel=""nofollow noreferrer"">EfficientDet D2 768x768</a></td>\n<td>67</td>\n<td>41.8</td>\n</tr>\n<tr>\n<td><a href=""https://tfhub.dev/tensorflow/efficientdet/d3/1"" rel=""nofollow noreferrer"">EfficientDet D3 896x896</a></td>\n<td>95</td>\n<td>45.4</td>\n</tr>\n<tr>\n<td><a href=""https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1"" rel=""nofollow noreferrer"">SSD ResNet101 V1 FPN 640x640 (RetinaNet101)</a></td>\n<td>57</td>\n<td>35.6</td>\n</tr>\n<tr>\n<td><a href=""https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1"" rel=""nofollow noreferrer"">CenterNet Resnet101 V1 FPN 512x512</a></td>\n<td>34</td>\n<td>34.2</td>\n</tr>\n<tr>\n<td><a href=""https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1"" rel=""nofollow noreferrer"">CenterNet Resnet50 V1 FPN 512x512</a></td>\n<td>27</td>\n<td>31.2</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><code>Step 1</code> and <code>Step 2</code> can be completed if you follow the previous section properly.</p>\n<p>You can use this section <a href=""https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub#simple_transfer_learning"" rel=""nofollow noreferrer"">simple_transfer_learning</a></p>\n<p>You have to got through the entire <a href=""https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub"" rel=""nofollow noreferrer"">Transfer learning with TensorFlow Hub</a> to understand more</p>\n<p>You have colab code readymade for you to test it out as well</p>\n<p>It is better you raise any issues you have at <a href=""https://github.com/tensorflow/models/issues"" rel=""nofollow noreferrer"">tensorflow/models</a> github page issues</p>\n<p><strong>PS: There seem to be no option to freeze the weight inside <code>pipeline.config</code> of any of tensorflow hub models for both <code>feature extraction</code> layers and <code>final</code> layer, no official support from the tensorflow guys on github also. Training without freezing has been shown to give degrading results for one user according to github tensorflow comment. I would suggest using <code>pytorch</code> where we use <code>requires_grad=False</code> to freeze weights</strong></p>\n', 'IsAccepted': False, 'CreationDate': 1650451787}, {'QuestionId': 67228856, 'AnswerId': 67229228, 'URL': 'https://stackoverflow.com/questions/67228856/can-i-use-my-model-h5-with-object-detection-algorithms/67229228#67229228', 'QuestionTitle': 'can I use my model h5 with object detection algorithms', 'Answer': ""<p>It's not clear how you want to use your model in conjunction with YOLO/Faster R-CNN, so I'll have to make some assumptions. It is also not clear from your question what is in your CSV file - is it actually pixel data or optical flow data? You possibly need to look up some easy YOLO tutorials first to see how it works.</p>\n<p>YOLO is an object detection model. It takes an image as input and draws named bounding boxes around the objects it recognises in that image. It is fast enough to work real-time on some video. The input is pixels: an image (or video frame). The output is a series of labels and the co-ordinates of the bounding box (and most tutorials will draw the bounding boxes and labels on to the image and output that). You could use it to detect a car (using a pre-trained YOLO model file that identifies cars) and estimate co-ordinates to crop the video (spatially and temporally) so that you have just the right amount of information to run your model on. You would then need to get the pixels from your cropped video into the right format for your model (whatever that is). You can load a model from an hd5 model in keras using <code>load_model(&quot;mymodelfilename.hd5&quot;)</code></p>\n"", 'IsAccepted': False, 'CreationDate': 1619177627}, {'QuestionId': 66798899, 'AnswerId': 66832234, 'URL': 'https://stackoverflow.com/questions/66798899/tensorflow-and-keras-for-easy-object-detection/66832234#66832234', 'QuestionTitle': 'Tensorflow and Keras for easy object detection', 'Answer': '<p>Generally, I think object detection is a bit harder to setup. But there is this keras tutorial I quite like for its simplicity while it also dives in a bit deeper if you\'re interested. If you combine the <a href=""https://keras.io/examples/vision/retinanet/#initializing-and-compiling-model"" rel=""nofollow noreferrer"">initialization</a> of the model with the <a href=""https://keras.io/examples/vision/retinanet/#training-the-model"" rel=""nofollow noreferrer"">training</a> parts and add the needed functions, the overall complexity isn\'t that bad. But to understand every step, it requires way more effort.</p>\n<pre><code>resnet50_backbone = get_backbone()\nloss_fn = RetinaNetLoss(num_classes)\nmodel = RetinaNet(num_classes, resnet50_backbone)\n\noptimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\nmodel.compile(loss=loss_fn, optimizer=optimizer)\n\nepochs = 1\nmodel.fit(\n    train_dataset.take(100),\n    validation_data=val_dataset.take(50),\n    epochs=epochs,\n    callbacks=callbacks_list,\n    verbose=1,\n)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1616854179}, {'QuestionId': 66798899, 'AnswerId': 66805232, 'URL': 'https://stackoverflow.com/questions/66798899/tensorflow-and-keras-for-easy-object-detection/66805232#66805232', 'QuestionTitle': 'Tensorflow and Keras for easy object detection', 'Answer': '<p>Of course, object detection is not easy compared to image classification. It\'s a harder problem to solve and requires more work in terms of gathering and labeling data. Maybe you didn\'t find good documentation or a source for helping you out? You can check <a href=""https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">this</a> detailed explanation of setting up the object detection for training/testing on your custom dataset</p>\n', 'IsAccepted': False, 'CreationDate': 1616695565}, {'QuestionId': 52695829, 'AnswerId': 62805118, 'URL': 'https://stackoverflow.com/questions/52695829/object-detection-using-tflearn-or-tensorflow-object-detection-api/62805118#62805118', 'QuestionTitle': 'Object Detection using TFLearn or Tensorflow object detection API', 'Answer': '<p>I suggest you start with Tensorflow object detection Api, as you can quickly train your model from an existant raw model with ready config pipelines, you have many choices on your model architecture and backbone.\nIt is way easier and more sure, it is very popular and a huge community using it,\nor you can use pytoch which i recommand, check <a href=""https://github.com/facebookresearch/detectron2"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/detectron2</a></p>\n', 'IsAccepted': False, 'CreationDate': 1594250915}, {'QuestionId': 57255942, 'AnswerId': 57257550, 'URL': 'https://stackoverflow.com/questions/57255942/object-detection-using-tensorflow-by-own-classifier/57257550#57257550', 'QuestionTitle': 'object detection using tensorflow by own classifier', 'Answer': '<p>I\'ve managed to solve this problem in my system (windows 10). The solution is not very straight forward but:</p>\n\n<p>1) First u need to clone Tensorflow Object Detection API repository <a href=""https://github.com/tensorflow/models"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models</a>.</p>\n\n<p>2) Follow the installation provided in <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md</a></p>\n\n<p>3) In the step 2, you are required to compile protobuf library, so download the protobuf compiler at <a href=""https://github.com/google/protobuf/releases/latest"" rel=""nofollow noreferrer"">https://github.com/google/protobuf/releases/latest</a> (at the time of this writing (3.5.1), there\'s a bug in protoc which may or may not related to the Windows environment, my solution is use the protoc v 3.4.0)\n4) Append the PYTHONPATH environment variable with the directory of /research/ and /research/slim (dont forget to add the PYTHONPATH to Path if you haven\'t done so.</p>\n\n<p>5) No more ModuleNotFoundError: No module named \'object_detection\'</p>\n', 'IsAccepted': True, 'CreationDate': 1564416156}, {'QuestionId': 56630132, 'AnswerId': 56630742, 'URL': 'https://stackoverflow.com/questions/56630132/can-we-use-a-model-trained-with-image-classification-to-help-in-object-detection/56630742#56630742', 'QuestionTitle': 'Can we use a model trained with image classification to help in object detection in tensorflow?', 'Answer': '<p>You can not directly use the <code>.pb</code> model produced by image classification to perform object detection. You will have to obtain an object detection model, train it and then use it to detect. There are pretrained object detection models at Tensorflow obejct detection model <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"" rel=""nofollow noreferrer"">zoo</a>.</p>\n\n<p><strong>detailed answer below:</strong></p>\n\n<p>Image classification and object detection are two different but very closely related tasks. In fact, Ross Girshick asked a similar question on the famous paper <a href=""https://arxiv.org/abs/1311.2524"" rel=""nofollow noreferrer"">R-CNN</a></p>\n\n<blockquote>\n  <p>To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?</p>\n</blockquote>\n\n<p>This question basically means that image classification model can be used to help object detection but there are some more steps needed. So you cannot just directly use a classification network to do object detection task. (But the error you gave was something different, you can find the correct tensor name and fix the error, but it just does not make sense to directly use classification network to do object detection that way.)</p>\n\n<p>There is naive solution to combine the two, you could just use a sliding window of various sizes passing through the image and perform classification, this can perform object detection.</p>\n\n<p>Another solution is integrated. To give an example, Faster R-CNN is an object detection network which used VGG as the feature extractor (In the original paper). Here you can see that VGG is an image classification network and it is pretrained on some image classification task. </p>\n\n<p><a href=""https://i.stack.imgur.com/rTgYF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rTgYF.png"" alt=""enter image description here""></a></p>\n\n<p><a href=""https://courses.cs.washington.edu/courses/csep576/18sp/lectures/8_1_detection.pdf"" rel=""nofollow noreferrer"">image source</a></p>\n', 'IsAccepted': False, 'CreationDate': 1560772290}, {'QuestionId': 55049695, 'AnswerId': 55060096, 'URL': 'https://stackoverflow.com/questions/55049695/use-tensorflow-model-for-object-detection-after-training/55060096#55060096', 'QuestionTitle': 'Use TensorFlow model for object detection after training', 'Answer': '<p>the easiest way to learn this, is by going through the Jupyter Notebook tutorial, included in Tensorflow.</p>\n\n<p>You find it in Tensorflow -> Models -> Research -> Object Detection -> object_detection_tutorial.ipynb, or through the link <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb"" rel=""nofollow noreferrer"">here</a>.</p>\n\n<p>Then you need to revise the Notebook a bit. Here you will need to change:</p>\n\n<ul>\n<li><p>Remove the ""Download Model"" section.</p></li>\n<li><p>Change the path to your Frozen Inference Graph from your retrained model. </p></li>\n<li><p>Change the label_map.pbtxt to your retrained labels.</p></li>\n<li><p>Mark your path to your test images and adjust according to their file extensions. They follow the pattern ""image1.jpg"", ""image2.jpg"" etc. by default</p></li>\n</ul>\n\n<p>Voila, you\'re done. You can test it on a pre-trained model from the Tensorflow Detection Zoo first if you want, by using the default script.</p>\n', 'IsAccepted': True, 'CreationDate': 1552036848}, {'QuestionId': 52695829, 'AnswerId': 54467079, 'URL': 'https://stackoverflow.com/questions/52695829/object-detection-using-tflearn-or-tensorflow-object-detection-api/54467079#54467079', 'QuestionTitle': 'Object Detection using TFLearn or Tensorflow object detection API', 'Answer': ""<p>Both are fine. Object detection API is a standard builtin library you can quickly clone and use. It's good to learn it since it's popular across the industry because of the following reasons.</p>\n\n<ol>\n<li>It's less error prone.</li>\n<li>It saves your time</li>\n<li>You can easily use tensorboard with it, to analyze the training stats.</li>\n<li>It has inbuilt architectures like faster RCNN and SSD. You just have to understand using them. Understanding various hyperparameters in the config file and knowing to tune them is also a skill.</li>\n</ol>\n\n<p>On the other hand if you want to make your own custom made models, you can go for learning tensorflow. Actually these both are not alternatives and should be learnt separately. In fact I would also suggest reading the research papers for the object detection algos available in the API. </p>\n\n<p>Besides object detection tensorflow will also help you in long run, whether you are working on segmentation algos or autoencoders or anything else in DNNs.</p>\n"", 'IsAccepted': False, 'CreationDate': 1548959324}, {'QuestionId': 45750364, 'AnswerId': 50473412, 'URL': 'https://stackoverflow.com/questions/45750364/object-detection-using-tensorflow/50473412#50473412', 'QuestionTitle': 'Object Detection using Tensorflow', 'Answer': ""<p>if you are using Tensorflow API, go to the folder <em>models/research</em>, open there a console. </p>\n\n<p>In the <strong>research</strong> folder run command <code>protoc object_detection/protos/*.proto --python_out=.</code>  and then <code>export PYTHONPATH=$PYTHONPATH:</code>pwd<code>:</code>pwd<code>/slim</code>. </p>\n\n<p>Then run <code>cd object_detection</code> to change folder in the console and open jupyter notebook in current folder. </p>\n\n<p>In jupyter notebook's home find the file <code>object_detection_tutorial.ipynb</code>, modify it so that it suits your purposes.</p>\n"", 'IsAccepted': False, 'CreationDate': 1527010062}, {'QuestionId': 45407210, 'AnswerId': 47816642, 'URL': 'https://stackoverflow.com/questions/45407210/tensorflow-object-detection-with-custom-model/47816642#47816642', 'QuestionTitle': 'Tensorflow Object Detection With Custom Model', 'Answer': '<p>I\'ve followed <a href=""https://pythonprogramming.net/custom-objects-tracking-tensorflow-object-detection-api-tutorial/?completed=/video-tensorflow-object-detection-api-tutorial/"" rel=""nofollow noreferrer"">this guide</a>, is awesome and practically easy, he shows how to train a Mac and cheese detector, hope it helps!\n<a href=""https://i.stack.imgur.com/ybwAo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ybwAo.png"" alt=""enter image description here""></a></p>\n', 'IsAccepted': False, 'CreationDate': 1513264480}, {'QuestionId': 44973184, 'AnswerId': 44973203, 'URL': 'https://stackoverflow.com/questions/44973184/train-tensorflow-object-detection-on-own-dataset/44973203#44973203', 'QuestionTitle': 'Train Tensorflow Object Detection on own dataset', 'Answer': '<p>This assumes the module is already installed. Please refer to their <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md"" rel=""noreferrer"">documentation</a> if not.</p>\n\n<p><strong>Disclaimer</strong></p>\n\n<p>This answer is not meant to be the <em>right</em> or <em>only</em> way of training the object detection module. This is simply I sharing my experience and what has worked for me. I\'m open to suggestions and learning more about this as I am still new to ML in general.</p>\n\n<p><strong>TL;DR</strong></p>\n\n<ol>\n<li>Create your own PASCAL VOC format dataset</li>\n<li>Generate TFRecords from it</li>\n<li>Configure a pipeline</li>\n<li>Visualize</li>\n</ol>\n\n<p>Each section of this answer consists of a corresponding Edit (see below). After reading each section, please read its Edit as well for clarifications. Corrections and tips were added for each section.</p>\n\n<p><strong>Tools used</strong></p>\n\n<p><a href=""https://github.com/tzutalin/labelImg"" rel=""noreferrer"">LabelImg</a>: A tool for creating PASCAL VOC format annotations.</p>\n\n<p><strong>1. Create your own PASCAL VOC dataset</strong></p>\n\n<p><strong>PS:</strong> <em>For simplicity, the folder naming convention of my answer follows that of Pascal VOC 2012</em></p>\n\n<p>A peek into the <a href=""http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"" rel=""noreferrer"">May 2012 dataset</a>, you\'ll notice the folder as having the following structure</p>\n\n<p><code>\n+VOCdevkit\n  +VOC2012\n    +Annotations\n    +ImageSets\n      +Action\n      +Layout\n      +Main\n      +Segmentation\n    +JPEGImages\n    +SegmentationClass\n    +SegmentationObject\n</code></p>\n\n<p>For the time being, amendments were made to the following folders:</p>\n\n<p><strong><em>Annotations</em></strong>: This is were all the images\' corresponding XML files will be placed in. Use the suggested tool above to create the annotations. Do not worry about <code>&lt;truncated&gt;</code> and <code>&lt;difficulty&gt;</code> tags as they will be ignored by the training and eval binaries.</p>\n\n<p><strong><em>JPEGImages</em></strong>: Location of your actual images. Make sure they are of type JPEG because that\'s what is currently supported in order to create TFRecords using their provided script.</p>\n\n<p><strong><em>ImageSets->Main</em></strong>: This simply consists of text files. For each class, there exists a corresponding <em>train.txt</em>, <em>trainval.txt</em> and <em>val.txt</em>. Below is a sample of the contents of the <em>aeroplane_train.txt</em> in the VOC 2012 folder</p>\n\n<pre><code>2008_000008 -1\n2008_000015 -1\n2008_000019 -1\n2008_000023 -1\n2008_000028 -1\n2008_000033  1\n</code></pre>\n\n<p>The structure is basically image name followed by a boolean saying whether the corresponding object exists in that image or not. Take for example image <em>2008_000008</em> does not consist of an aeroplane hence marked with a <em>-1</em> but image <em>2008_000033</em> does.</p>\n\n<p>I wrote a small Python script to generate these text files. Simply iterate through the image names and assign a 1 or -1 next to them for object existence. I added some randomness among my text files by shuffling the image names.</p>\n\n<p>The <em>{classname}_val.txt</em> files consist of the <strike>testing</strike> validation datasets. Think of this as the test data during training. You want to divide your dataset into training and validation. More info can be found <a href=""https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set"">here</a>. The format of these files is similar to that of training.</p>\n\n<p>At this point, your folder structure should be</p>\n\n<p><code>\n+VOCdevkit\n  +VOC2012\n    +Annotations\n     --(for each image, generated annotation)\n    +ImageSets\n      +Main\n        --(for each class, generated *classname*_train.txt and *classname*_val.txt)\n    +JPEGImages\n     --(a bunch of JPEG images)\n</code></p>\n\n<hr>\n\n<p><strong>1.1 Generating label map</strong></p>\n\n<p>With the dataset prepared, we need to create the corresponding label maps.\nNavigate to <em>models/object_detection/data</em> and open <em>pascal_label_map.pbtxt</em>.</p>\n\n<p>This file consists of a JSON that assigns an ID and name to each item. Make amendments to this file to reflect your desired objects.</p>\n\n<hr>\n\n<p><strong>2. Generate TFRecords</strong></p>\n\n<p>If you look into their code especially this <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/create_pascal_tf_record.py#L162"" rel=""noreferrer"">line</a>, they explicitly grab the <em>aeroplane_train.txt</em> only. For curios minds, <a href=""https://stackoverflow.com/questions/44891732/create-pascal-voc-for-tensorflow-object-detection-api/44937455#44937455"">here\'s why</a>. Change this file name to any of your class train text file.</p>\n\n<p>Make sure <em>VOCdevkit</em> is inside <em>models/object_detection</em> then you can go ahead and <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/preparing_inputs.md#generating-the-pascal-voc-tfrecord-files"" rel=""noreferrer"">generate the TFRecords</a>.</p>\n\n<p>Please go through their code first should you run into any problems. It is self explanatory and well documented.</p>\n\n<hr>\n\n<p><strong>3. Pipeline Configuration</strong></p>\n\n<p>The <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md"" rel=""noreferrer"">instructions</a> should be self explanatory to cover this segment. Sample configs can be found in <a href=""https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs"" rel=""noreferrer""><em>object_detection/samples/configs</em></a>.</p>\n\n<p>For those looking to train from scratch as I did, just make sure to remove the <code>fine_tune_checkpoint</code> and <code>from_detection_checkpoint</code> nodes. <a href=""https://pastebin.com/z7a2G2Dq"" rel=""noreferrer"">Here\'s</a> what my config file looked like for reference.</p>\n\n<p>From here on you can continue with the <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_locally.md#running-the-training-job"" rel=""noreferrer"">tutorial</a> and run the training process.</p>\n\n<hr>\n\n<p><strong>4. Visualize</strong></p>\n\n<p>Be sure to run the eval in parallel to the training in order to be able to visualize the learning process. To quote <a href=""https://github.com/tensorflow/models/issues/1877"" rel=""noreferrer"">Jonathan Huang</a></p>\n\n<blockquote>\n  <p>the best way is to just run the eval.py binary. We typically run this\n  binary in parallel to training, pointing it at the directory holding\n  the checkpoint that is being trained. The eval.py binary will write\n  logs to an <code>eval_dir</code> that you specify which you can then point to\n  with Tensorboard.</p>\n  \n  <p>You want to see that the mAP has ""lifted off"" in the first few hours,\n  and then you want to see when it converges. It\'s hard to tell without\n  looking at these plots how many steps you need.</p>\n</blockquote>\n\n<hr>\n\n<p><strong>EDIT I (28 July \'17):</strong></p>\n\n<p>I never expected my response to get this much attention so I decided to come back and review it.</p>\n\n<p><strong>Tools</strong></p>\n\n<p>For my fellow Apple users, you could actually use <a href=""https://itunes.apple.com/us/app/rectlabel-labeling-images-for-object-detection/id1210181730?mt=12"" rel=""noreferrer"">RectLabel</a> for annotations.</p>\n\n<p><strong>Pascal VOC</strong></p>\n\n<p>After digging around, I finally realized that <em>trainval.txt</em> is actually the union of training and validation datasets.</p>\n\n<p>Please look at their <a href=""https://pjreddie.com/media/files/VOC2012_doc.pdf"" rel=""noreferrer"">official development kit</a> to understand the format even better.</p>\n\n<p><strong>Label Map Generation</strong></p>\n\n<p>At the time of my writing, ID 0 represents <code>none_of_the_above</code>. It is recommended that your IDs start from 1.</p>\n\n<p><strong>Visualize</strong></p>\n\n<p>After running your evaluation and directed tensorboard to your Eval directory, it\'ll show you the mAP of each category along with each category\'s performance. This is good but I like seeing my training data as well in parallel with Eval.</p>\n\n<p>To do this, run tensorboard on a different port and point it to your train directory</p>\n\n<pre><code>tensorboard --logdir=${PATH_TO_TRAIN} --port=${DESIRED_NUMBER}\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1499437414}, {'QuestionId': 45750364, 'AnswerId': 45855270, 'URL': 'https://stackoverflow.com/questions/45750364/object-detection-using-tensorflow/45855270#45855270', 'QuestionTitle': 'Object Detection using Tensorflow', 'Answer': ""<p>What are the outputs of boxes, scores and classes? Can you print them? If you get numbers from them, maybe you just need to change a few lines in your code to properly visualize the results.</p>\n\n<p>For test, you can use:</p>\n\n<pre><code>        vis_util.save_image_array_as_png(image,'./outputImg.png')\n        #print(image.shape)\n        print('image saved')\n        img=mpimg.imread('./outputImg.png')\n        imgplot = plt.imshow(img)\n        plt.show()\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1503559059}, {'QuestionId': 45407210, 'AnswerId': 45407451, 'URL': 'https://stackoverflow.com/questions/45407210/tensorflow-object-detection-with-custom-model/45407451#45407451', 'QuestionTitle': 'Tensorflow Object Detection With Custom Model', 'Answer': '<p>Sure!  You can definitely train locally using the command lines here:\n<a href=""https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_locally.md"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_locally.md</a> </p>\n\n<p>I would recommend that you use a GPU if you train locally.  Someone also wrote up a nice guide about how to train on a custom dataset here: <a href=""https://medium.com/towards-data-science/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9"" rel=""nofollow noreferrer"">https://medium.com/towards-data-science/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9</a> \nIn this case he used Cloud, but many of the tips will still be relevant even if training locally.</p>\n', 'IsAccepted': True, 'CreationDate': 1501475210}, {'QuestionId': 44973184, 'AnswerId': 45369144, 'URL': 'https://stackoverflow.com/questions/44973184/train-tensorflow-object-detection-on-own-dataset/45369144#45369144', 'QuestionTitle': 'Train Tensorflow Object Detection on own dataset', 'Answer': '<p>I wrote a <a href=""https://medium.com/@datitran/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9"" rel=""noreferrer"">blog post</a> on Medium about my experience as well on how I trained an object detector (in particular, it\'s a Raccoon detector) with Tensorflow on my own dataset. This might also be useful for others and is complimentary to eshirima\'s answer.</p>\n', 'IsAccepted': False, 'CreationDate': 1501232334}, {'QuestionId': 43495836, 'AnswerId': 43544276, 'URL': 'https://stackoverflow.com/questions/43495836/how-to-make-an-object-detector-from-an-image-classifier/43544276#43544276', 'QuestionTitle': 'how to make an object detector from an image classifier?', 'Answer': '<p>If you really need to use your pretrained network, then you can detect potential boxes of interest then apply your network on each. These boxes can be determined with an ""objectness"" method, such as <a href=""https://www.microsoft.com/en-us/research/publication/edge-boxes-locating-object-proposals-from-edges/"" rel=""nofollow noreferrer"">EdgeBox</a>.</p>\n\n<p>However, on nowadays, object detection is usually obtained by a more integrated way, such those obtained with <a href=""https://github.com/endernewton/tf-faster-rcnn"" rel=""nofollow noreferrer"">faster RCNN</a>. Such an approach integrates a layer named Region Proposal Network (RPN), that determine the region of interest, jointly with the recognition of the classes.</p>\n\n<p>to the best of my knowledge, one of the best recent approaches is <a href=""https://pjreddie.com/darknet/yolo/"" rel=""nofollow noreferrer"">Yolo</a>, but it is natively based on Darknet.</p>\n', 'IsAccepted': False, 'CreationDate': 1492782020}, {'QuestionId': 42955746, 'AnswerId': 42960635, 'URL': 'https://stackoverflow.com/questions/42955746/process-to-build-our-own-model-for-image-detection/42960635#42960635', 'QuestionTitle': 'Process to build our own model for image detection', 'Answer': '<p>There are a couple of papers addressing this issue. For example in <a href=""http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf"" rel=""nofollow noreferrer"">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf</a> some general principles are mentioned, like preserving information by not having too rapid changes in any cut of the graph seperating the output from the input.</p>\n\n<p>Another paper is <a href=""https://arxiv.org/pdf/1606.02228.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1606.02228.pdf</a> where specific hyperparameter combinations are tried. </p>\n\n<p>The remainder are just what you observe in practice and depends on your dataset and on your requirement. Maybe you have performance requirements because you want to deploy to mobile or you need more than 90 % accuracy. Then you will have to choose your model accordingly.</p>\n', 'IsAccepted': True, 'CreationDate': 1490209755}]","{72255992, 61304977}","['<p>I found out that there is no way to \'add\' a class to the previously trained classes but with providing a little amount of data of that class you can have your model detect it. The reason is that the last layer of the model changes when transfer learning is applied. In my case I labeled around 3k frames containing about 12k objects because my frames would be complicated. But for simpler tasks as I saw in tutorials 200-300 annotated images would be enough.</p>\n\n<p>And for the part that the model did not detect anything it has something to do with the convert command that I used. I should have used <code>tflite_convert</code> instead of <code>toco</code>. I explained more <a href=""https://stackoverflow.com/questions/61749548/how-to-convert-tflite-graph-pb-to-detect-tflite-properly/61851433#61851433"">here</a>.</p>\n', ""<p>About the first error:\nYou probably using the following code from the blog-post you mentioned:</p>\n<pre><code>def load_model(model_name):\n  base_url = 'http://download.tensorflow.org/models/object_detection/'\n  model_file = model_name + '.tar.gz'\n  model_dir = tf.keras.utils.get_file(\n    fname=model_name, \n    origin=base_url + model_file,\n    untar=True)\n \n  model_dir = pathlib.Path(model_dir)/&quot;saved_model&quot;\n \n  model = tf.saved_model.load(str(model_dir))\n \n  return model\n</code></pre>\n<p>use a debugger to check if model is being downloaded from the <code>url</code>.</p>\n<p>About the second error:</p>\n<p><code>output_dict = run_inference_for_single_image(model, image_np)</code></p>\n<p>seems to return a dictionary with tensors as values.</p>\n<p>Try:</p>\n<p><code>num_detections = int(output_dict.pop('num_detections').numpy())</code></p>\n""]","{'https://stackoverflow.com/questions/61304977/improving-a-pre-trained-tensorflow-object-detection-model/61851592#61851592', 'https://stackoverflow.com/questions/72255992/error-in-implemeting-tensorflow-object-detection-model/72256244#72256244'}",,0.2269940362494941,0.25945765028306556
19,60398554,tf.keras.Model,Documentation Replication on Other Examples,"Should we apply repeat, batch shuffle to tf.data.Dataset when passing it to fit function?","<p>I still don't after having read documentation about <code>tf.keras.Model.fit</code> and <code>tf.data.Dataset</code>, when passing <code>tf.data.Dataset</code> to fit function, should I call <code>repeat</code> and <code>batch</code> on the dataset object or should I provide the <code>batch_size</code> and <code>epochs</code> arguments to fit instead? or both? Should I apply the same treatment to the validation set?</p>

<p>And while I'm here, can I <code>shuffle</code> the dataset before the <code>fit</code>? (seems like it's an obvious yes)
If so, before, after calling <code>Dataset.batch</code> and <code>Dataset.repeat</code> (if calling them)?</p>

<p><strong>Edit:</strong> When using <code>batch_size</code> argument, and without having called <code>Dataset.batch(batch_size)</code> previously, I am getting the following error:</p>

<pre><code>ValueError: The `batch_size` argument must not be specified for the given input type.
Received input: &lt;MapDataset shapes: ((&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;), (&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;)), 
types: ((tf.float32, tf.float32, tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32))&gt;, 
batch_size: 1
</code></pre>

<p>Thanks</p>
","<p>There's different ways to do what you want here, but the one I always use is: </p>

<pre><code>batch_size = 32
ds = tf.Dataset()
ds = ds.shuffle(len_ds)
train_ds = ds.take(0.8*len_ds)
train_ds = train_ds.repeat().batch(batch_size)
validation_ds = ds.skip(0.8*len_ds)
validation_ds = train_ds.repeat().batch(batch_size)
model.fit(train_ds,
          steps_per_epoch = len_train_ds // batch_size,
          validation_data = validation_ds,
          validation_steps = len_validation_ds // batch_size,
          epochs = 5)
</code></pre>

<p>This way you have access to all the variables after model fitting as well, for example if you want to visualize the validation set, you can. This is not really possible with <code>validation_split</code>. If you remove <code>.batch(batch_size)</code>, you should remove the <code>// batch_size</code>s, but I would leave them, as it clearer what is happening now.</p>

<p>You always have to provide epochs.</p>

<p>Calculating the length of your train/validation sets requires you to loop over them: </p>

<pre><code>len_train_ds = 0
for i in train_ds:
  len_train_ds += 1
</code></pre>

<p>if in <code>tf.Dataset</code> form.</p>
","{68611882, 51909997, 54595256, 50437234, 47650132, 73869781, 66227637, 56944856, 48517244, 63533821}","[{'QuestionId': 73869781, 'AnswerId': 73870800, 'URL': 'https://stackoverflow.com/questions/73869781/dataset-shuffled-with-keras/73870800#73870800', 'QuestionTitle': 'Dataset shuffled with Keras', 'Answer': '<p>The function <code>image_dataset_from_directory</code> uses the <code>tf.data.Dataset</code> API. The default behaviour of <code>tf.data.Dataset.shuffle</code>, is to reshuffle the dataset at each iteration. From the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"" rel=""nofollow noreferrer"">documentation of <code>shuffle</code></a>:</p>\n<pre><code>dataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())\n# [1, 0, 2]\nlist(dataset.as_numpy_iterator())\n# [1, 2, 0]\n</code></pre>\n<p>If you want to shuffle your dataset, but have the same exact order at each iteration over the dataset, you will need to shuffle the dataset after creating it and specify <code>reshuffle_each_iteration=False</code>.</p>\n<pre><code>ds = tf.keras.utils.image_dataset_from_directory(\n    data_dir,\n    color_mode=\'grayscale\',\n    image_size=(img_height, img_width),\n    seed=42,\n    batch_size=batch_size,\n    label_mode=\'binary\',\n    shuffle=False\n)\n# default buffer in image_dataset_from_directory is 8*batch_size\nds = ds.shuffle(buffer_size=8*batch_size, seed=42, reshuffle_each_iteration=False)\n</code></pre>\n<p>Regarding the seed, it makes the dataset being shuffled predictably <strong>at each execution of the program</strong>, not at each iteration through the dataset. You can read more about seeds and tensorflow\'s random behaviour in the <a href=""https://www.tensorflow.org/api_docs/python/tf/random/set_seed"" rel=""nofollow noreferrer"">documentation of <code>tf.random.set_seed</code></a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1664295673}, {'QuestionId': 68611882, 'AnswerId': 69308619, 'URL': 'https://stackoverflow.com/questions/68611882/why-tensorflow-dataset-neet-to-be-batched-before-fit/69308619#69308619', 'QuestionTitle': 'Why tensorflow dataset neet to be batched before fit?', 'Answer': '<p>Datasets are sliced or batched for following reasons.</p>\n<ol>\n<li><p>To avoid high memory usage if entire Dataset is used as one batch ( which might create Out of Memory problems)</p>\n</li>\n<li><p>Computation is faster when Training is done in Batches</p>\n</li>\n<li><p>Weight and Bias update is possible with respect to labels when training is done is batches.</p>\n</li>\n</ol>\n<p>Reference:\n<a href=""https://medium.com/@elimu.michael9/understanding-epochs-and-batches-23120a04b3cb"" rel=""nofollow noreferrer"">https://medium.com/@elimu.michael9/understanding-epochs-and-batches-23120a04b3cb</a></p>\n', 'IsAccepted': False, 'CreationDate': 1632448094}, {'QuestionId': 66227637, 'AnswerId': 66227809, 'URL': 'https://stackoverflow.com/questions/66227637/tensorflow-shuffling-data-twice-during-preprocessing/66227809#66227809', 'QuestionTitle': 'Tensorflow Shuffling Data Twice During Preprocessing', 'Answer': ""<p>They shuffle twice for different reasons:</p>\n<ol>\n<li>The first shuffle is to get a shuffled and consistent trough epochs train/validation split.</li>\n<li>The second shuffle is to shuffle the train dataset at each epoch.</li>\n</ol>\n<p>Explaination:</p>\n<ol>\n<li><p>The <code>shuffle</code> method has a specific parameter <code>reshuffle_each_iteration</code>, that defaults to<code>True</code>. It means that whenever the dataset is exhausted, the whole dataset is reshuffled. If one is splitting one dataset into two after a call to <code>shuffle</code>, (to get an randomized split), when the dataset will be exhausted, the whole dataset will be reshuffled before the split. The train set and validation set then are mixed together.  So, to get an randomized split, but consistent between epochs, shuffle with <code>reshuffle_each_iteration=False</code></p>\n</li>\n<li><p>Then, for the training set, it's better to feed the model with data in a different order at each epoch, hence the call to shuffle a second time.</p>\n</li>\n</ol>\n<hr />\n<p>For your 2nd question, <code>tf.data.AUTOTUNE</code> is simply not compatible with <code>shuffle</code>.</p>\n"", 'IsAccepted': True, 'CreationDate': 1613490982}, {'QuestionId': 63533821, 'AnswerId': 63802795, 'URL': 'https://stackoverflow.com/questions/63533821/batching-in-tf-data-dataset-in-time-series-analysis/63802795#63802795', 'QuestionTitle': 'Batching in tf.data.dataset in time-series analysis', 'Answer': '<p>Here is my solution when dealing with time series data.</p>\n<pre><code>dataset = tf.data.Dataset.from_tensor_slices(series)\ndataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\ndataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\ndataset = dataset.batch(batch_size).prefetch(1)\n</code></pre>\n<p>Following line is important to split the window into xs and ys.</p>\n<pre><code>dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n</code></pre>\n<p>Though it is not important to use shuffle, you can only use the map function to split the window in to xs and ys.</p>\n', 'IsAccepted': False, 'CreationDate': 1599607460}, {'QuestionId': 63533821, 'AnswerId': 63699653, 'URL': 'https://stackoverflow.com/questions/63533821/batching-in-tf-data-dataset-in-time-series-analysis/63699653#63699653', 'QuestionTitle': 'Batching in tf.data.dataset in time-series analysis', 'Answer': '<p>The solution was to window the two datasets separately, <code>.zip()</code> them together, then <code>.concat()</code> the elements to include the label.</p>\n<pre><code>ds = tf.data.Dataset.from_tensor_slices(series1)\nds = ds.window(window_size + 1, shift=1, drop_remainder=True)\nds = ds.flat_map(lambda window: window.batch(window_size + 1))\nds = ds.map(lambda window: (window[:-1], window[-1]))\n\nds2 = tf.data.Dataset.from_tensor_slices(series2)\nds2 = ds2.window(window_size, shift=1, drop_remainder=True)\nds2 = ds2.flat_map(lambda window: window.batch(window_size))\n\nds = tf.data.Dataset.zip((ds, ds2))\nds = ds.map(lambda i, j: (tf.concat([i[0], j], axis=0), i[-1]))\n</code></pre>\n<h2>Returns:</h2>\n<pre><code>(&lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([  1,   2,   3, 100, 200, 300])&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=4&gt;)\n(&lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([  2,   3,   4, 200, 300, 400])&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=5&gt;)\n(&lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([  3,   4,   5, 300, 400, 500])&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=6&gt;)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1599027153}, {'QuestionId': 63533821, 'AnswerId': 63534204, 'URL': 'https://stackoverflow.com/questions/63533821/batching-in-tf-data-dataset-in-time-series-analysis/63534204#63534204', 'QuestionTitle': 'Batching in tf.data.dataset in time-series analysis', 'Answer': ""<p>I think this is the line you're missing:</p>\n<pre><code>ds = ds.batch(2).map(lambda x, y: (tf.concat([x, y], axis=0)))\n</code></pre>\n<p>Full example:</p>\n<pre><code>import tensorflow as tf\n\nseries1 = tf.range(1, 16)\nseries2 = tf.range(100, 1600, 100)\n\nds = tf.data.Dataset.from_tensor_slices((series1, series2))\n\nds = ds.batch(2).map(lambda x, y: (tf.concat([x, y], axis=0)))\n\nfor row in ds:\n    print(row)\n</code></pre>\n<pre><code>tf.Tensor([  1   2 100 200], shape=(4,), dtype=int32)\ntf.Tensor([  3   4 300 400], shape=(4,), dtype=int32)\ntf.Tensor([  5   6 500 600], shape=(4,), dtype=int32)\ntf.Tensor([  7   8 700 800], shape=(4,), dtype=int32)\ntf.Tensor([   9   10  900 1000], shape=(4,), dtype=int32)\ntf.Tensor([  11   12 1100 1200], shape=(4,), dtype=int32)\ntf.Tensor([  13   14 1300 1400], shape=(4,), dtype=int32)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1598085587}, {'QuestionId': 56944856, 'AnswerId': 56951558, 'URL': 'https://stackoverflow.com/questions/56944856/tensorflow-dataset-questions-about-shuffle-batch-and-repeat/56951558#56951558', 'QuestionTitle': 'Tensorflow dataset questions about .shuffle, .batch and .repeat', 'Answer': '<h3>First Question:</h3>\n\n<p>That\'s correct - if you feed a dataset you no longer need to catch the <code>OutOfRangeError</code>. </p>\n\n<p><code>repeat()</code> takes an optional argument for the number of times it should repeat. This means <code>repeat(10)</code> will iterate over the entire dataset 10 times. If you choose to omit the argument then it will repeat indefinately</p>\n\n<h3>Second Question</h3>\n\n<p><code>Shuffle()</code> (if used) should be called before <code>batch()</code> - we want to shuffle records not batches. </p>\n\n<p>The buffer is first filled by adding your records in order then, once full, a random one is selected and emitted and a new record read from the original source. </p>\n\n<p>If you have something like</p>\n\n<pre class=""lang-py prettyprint-override""><code>ds.shuffle(1000).batch(100)\n</code></pre>\n\n<p>then in order to return a single batch, this last step is repeated 100 times (maintaining the buffer at 1000). Batching is a separate operation.</p>\n\n<h3>Third question</h3>\n\n<p>Generally we don\'t shuffle a <strong>test</strong> set at all - only the training set (We evaluate using the entire test set anyway, right? So why shuffle?).</p>\n\n<blockquote>\n  <p>So, if I wanted to just test on the whole test dataset I wouldn\'t use\n  <code>.batch</code></p>\n</blockquote>\n\n<p>Hmm - not so (at least not always). You would certainly need to use batch if your whole test dataset didnt fit into memory - a common occurrence. You would want to test the whole dataset but to run the numbers in manageable bites!</p>\n', 'IsAccepted': True, 'CreationDate': 1562671737}, {'QuestionId': 50437234, 'AnswerId': 55802845, 'URL': 'https://stackoverflow.com/questions/50437234/tensorflow-dataset-shuffle-then-batch-or-batch-then-shuffle/55802845#55802845', 'QuestionTitle': 'tensorflow dataset shuffle then batch or batch then shuffle', 'Answer': ""<p>Fully agree to @mrry, but there exists one case where you might want to do batching <strong>before</strong> shuffling. Suppose you're processing some text data which will be feed into an RNN. Here each sentence is treated as one sequence, and one batch will contain multiple sequences. Since the length of sentences is variable, we need to <em>pad</em> the sentences in a batch to a uniform length. An efficient way to do this is to <strong>group sentences of similar length together</strong> through batching, and then do shuffling. Otherwise, we may end up batches which are full with the <code>&lt;pad&gt;</code> token.</p>\n"", 'IsAccepted': False, 'CreationDate': 1555980613}, {'QuestionId': 54595256, 'AnswerId': 54607341, 'URL': 'https://stackoverflow.com/questions/54595256/randomshufflequeue-functionality-with-tf-data-dataset/54607341#54607341', 'QuestionTitle': 'RandomShuffleQueue functionality with tf.data.Dataset', 'Answer': '<p>If I understood you correctly, it sounds like a perfect match for <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer""><code>Dataset.from_generator()</code></a>. You can add <code>Dataset.shuffle()</code> afterwards, if you would like to make a buffer and take randomly elements out of it.</p>\n', 'IsAccepted': True, 'CreationDate': 1549723715}, {'QuestionId': 51909997, 'AnswerId': 52418702, 'URL': 'https://stackoverflow.com/questions/51909997/tensorflow-dataset-shuffle-before-map-map-and-batch/52418702#52418702', 'QuestionTitle': 'TensorFlow Dataset: shuffle before map (map_and_batch)?', 'Answer': '<p>This might be a case-by-case problem. Several days ago I met a problem that <a href=""https://stackoverflow.com/questions/52325149/tensorflow-when-use-dataset-shuffle180000-computer-will-freeze-what-is-a-b/"">my computer will freeze when I call dataset.shuffle(180000)</a>. It turned out that, if I map() before shuffle(), it will freeze; but if I map() after shuffle(), it will not.</p>\n\n<p>It feels like (at least according to my experience) that when shuffling, tensorflow is probably shuffling the actual tensors in the memory, instead of their reference (or ""pointer""). In my case, my files are images with 112x112 pixels and 3 color channels. If I map() before shuffle(), <code>shuffle(180000)</code> will shuffle 180000 tensors with 112x112x3 numbers ; but if I shuffle() before map(), <code>shuffle(180000)</code> will just shuffle 180000 tensors that each only contains a short string (filename like ""abc-001.jpeg""). </p>\n\n<p>According to my experience above, without any parallel computation, <strong>if the data you are going to map() is much larger in size than their filename (which is usually the case), then shuffle() before map() should be faster than map() before shuffle().</strong></p>\n\n<p><strong>In the stock example, I think it is because the stock data is relatively small in size</strong>. Unlike 112x112x3 numbers in one tensor in my case, each stock datapoint should only have 4 prices(open,close,high,low) and 1 time(year-month-day-hour:min:sec). In this case shuffle() the data won\'t become an issue.</p>\n\n<p><strong>To conclude, I think in most cases (where file size > filename size), shuffle() before map() is better.</strong></p>\n', 'IsAccepted': False, 'CreationDate': 1537423983}, {'QuestionId': 50437234, 'AnswerId': 50453698, 'URL': 'https://stackoverflow.com/questions/50437234/tensorflow-dataset-shuffle-then-batch-or-batch-then-shuffle/50453698#50453698', 'QuestionTitle': 'tensorflow dataset shuffle then batch or batch then shuffle', 'Answer': '<p><strong>TL;DR:</strong> Yes, there is a difference. Almost always, you will want to call <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"" rel=""noreferrer""><code>Dataset.shuffle()</code></a> <strong>before</strong> <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch"" rel=""noreferrer""><code>Dataset.batch()</code></a>. There is no <code>shuffle_batch()</code> method on the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer""><code>tf.data.Dataset</code></a> class, and you must call the two methods separately to shuffle and batch a dataset.</p>\n\n<hr>\n\n<p>The transformations of a <code>tf.data.Dataset</code> are applied in the same sequence that they are called. <code>Dataset.batch()</code> combines consecutive elements of its input into a single, batched element in the output.\nWe can see the effect of the order of operations by considering the following two datasets:</p>\n\n<pre><code>tf.enable_eager_execution()  # To simplify the example code.\n\n# Batch before shuffle.\ndataset = tf.data.Dataset.from_tensor_slices([0, 0, 0, 1, 1, 1, 2, 2, 2])\ndataset = dataset.batch(3)\ndataset = dataset.shuffle(9)\n\nfor elem in dataset:\n  print(elem)\n\n# Prints:\n# tf.Tensor([1 1 1], shape=(3,), dtype=int32)\n# tf.Tensor([2 2 2], shape=(3,), dtype=int32)\n# tf.Tensor([0 0 0], shape=(3,), dtype=int32)\n\n# Shuffle before batch.\ndataset = tf.data.Dataset.from_tensor_slices([0, 0, 0, 1, 1, 1, 2, 2, 2])\ndataset = dataset.shuffle(9)\ndataset = dataset.batch(3)\n\nfor elem in dataset:\n  print(elem)\n\n# Prints:\n# tf.Tensor([2 0 2], shape=(3,), dtype=int32)\n# tf.Tensor([2 1 0], shape=(3,), dtype=int32)\n# tf.Tensor([0 1 1], shape=(3,), dtype=int32)\n</code></pre>\n\n<p>In the first version (batch before shuffle), the elements of each batch are 3 consecutive elements from the input; whereas in the  second version (shuffle before batch), they are randomly sampled from the input. Typically, when training by (some variant of) mini-batch <a href=""https://en.wikipedia.org/wiki/Stochastic_gradient_descent"" rel=""noreferrer"">stochastic gradient descent</a>, the elements of each batch should be sampled as uniformly as possible from the total input. Otherwise, it is possible that the network will overfit to whatever structure was in the input data, and the resulting network will not achieve as high an accuracy.</p>\n', 'IsAccepted': True, 'CreationDate': 1526922993}, {'QuestionId': 47650132, 'AnswerId': 47650238, 'URL': 'https://stackoverflow.com/questions/47650132/shuffling-input-files-with-tensorflow-datasets/47650238#47650238', 'QuestionTitle': 'Shuffling input files with tensorflow Datasets', 'Answer': '<p>Start reading them in order, <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"" rel=""noreferrer"">shuffle</a> right after:</p>\n\n<pre><code>BUFFER_SIZE = 1000 # arbitrary number\n# define filenames somewhere, e.g. via glob\ndataset = tf.data.TFRecordDataset(filenames).shuffle(BUFFER_SIZE)\n</code></pre>\n\n<h3>EDIT:</h3>\n\n<p>The input pipeline of <a href=""https://stackoverflow.com/q/48679622/3214872"">this question</a> gave me an idea on how to implement filenames shuffling with the Dataset API:</p>\n\n<pre><code>dataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.shuffle(BUFFER_SIZE) # doesn\'t need to be big\ndataset = dataset.flat_map(tf.data.TFRecordDataset)\ndataset = dataset.map(decode_example, num_parallel_calls=5) # add your decoding logic here\n# further processing of the dataset\n</code></pre>\n\n<p>This will put all the data of one file before the one of the next and so on. Files are shuffled, but the data inside them will be produced in the same order.\nYou can alternatively replace <code>dataset.flat_map</code> with <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/Dataset#interleave"" rel=""noreferrer""><code>interleave</code></a> to process multiple files at the same time and return samples from each:</p>\n\n<pre><code>dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=4)\n</code></pre>\n\n<p><strong>Note:</strong> <code>interleave</code> does not actually run in multiple threads, it\'s a round-robin operation. For true parallel processing see <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/parallel_interleave"" rel=""noreferrer""><code>parallel_interleave</code></a></p>\n', 'IsAccepted': False, 'CreationDate': 1512466494}, {'QuestionId': 47650132, 'AnswerId': 48559842, 'URL': 'https://stackoverflow.com/questions/47650132/shuffling-input-files-with-tensorflow-datasets/48559842#48559842', 'QuestionTitle': 'Shuffling input files with tensorflow Datasets', 'Answer': '<p>The current Tensorflow version (v1.5 in 02/2018) does not seem to support filename shuffling natively in the Dataset API. Here is a simple work around using numpy:</p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nmyShuffledFileList = np.random.choice(myInputFileList, size=len(myInputFileList), replace=False).tolist()\n\ndataset = tf.data.TFRecordDataset(myShuffledFileList)\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1517478956}, {'QuestionId': 48517244, 'AnswerId': 48518861, 'URL': 'https://stackoverflow.com/questions/48517244/tf-data-mixing-batch-sizes/48518861#48518861', 'QuestionTitle': 'tf.data: &quot;mixing&quot; batch sizes?', 'Answer': ""<p>Ok, here's my tentative solution. Please note that I'm assuming your data is somehow ordered, so that when you build batches of <code>x</code> the next <code>context_x</code> you read is always the one related to the current batch.</p>\n\n<pre><code>import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1' # running on CPU\nimport tensorflow as tf\nimport numpy as np\n\n# Small data input\nx = np.arange(100)\ny = np.arange(100)\n\n# Large context array for both x  and y\ncontext_x = np.random.rand(1, 10000, 10)\ncontext_y = np.random.rand(1, 10000, 10)\n\n# Create datasets\ndataset_x = tf.data.Dataset.from_tensor_slices(x).batch(32)\ndataset_y = tf.data.Dataset.from_tensor_slices(y).batch(32)\n\n# same context should be repeated for every data item\ndataset_context_x = tf.data.Dataset.from_tensor_slices(context_x)\ndataset_context_x = dataset_context_x.repeat() # here just for demonstration purposes. Ideally you'll have enough context data to match the batches\ndataset_context_y = tf.data.Dataset.from_tensor_slices(context_y)\ndataset_context_y = dataset_context_y.repeat() # here just for demonstration purposes. Ideally you'll have enough context data to match the batches\n\ndataset = tf.data.Dataset.zip((dataset_x, dataset_context_x))\ndataset = dataset.concatenate( tf.data.Dataset.zip((dataset_y, dataset_context_y)) ) # This stacks all 'x' samples on top of all 'y' samples. Is this really what you wanted?\n\niterator = dataset.make_initializable_iterator()\n(x_iter, context_iter) = iterator.get_next()\nwith tf.Session() as sess:\n    sess.run(iterator.initializer)\n    while True:\n        try:\n            xi, ci = sess.run([x_iter, context_iter])\n            print(xi.shape, ci.shape)\n        except tf.errors.OutOfRangeError:\n            break\n</code></pre>\n\n<p>In your implementation, remove the <code>dataset_context_* = dataset_context_*.repeat()</code> lines.</p>\n\n<p>The key difference with your pipeline is that I'm batching <code>x</code> <em>before</em> zipping it with the context, so that the context doesn't get replicated. This, however, requires you to be careful in handling the data loading (hence my assumption above).</p>\n"", 'IsAccepted': True, 'CreationDate': 1517307216}]","{56944856, 60398554, 63140320}","['<p>I understand that you are concerned about having your complete dataset in the memory.</p>\n<p>Do not worry, the <code>tf.data.Dataset</code> API is very efficient and it does not load your complete dataset in the memory.</p>\n<p>Internally it just creates a sequence of functions and when called with <code>model.fit()</code> it will load only the batch in the memory and not the complete dataset.</p>\n<p>You can read more in this <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">link</a>, I am pasting the important part from the documentation.</p>\n<blockquote>\n<p>The tf.data.Dataset API supports writing descriptive and efficient\ninput pipelines. Dataset usage follows a common pattern:</p>\n<p>Create a source dataset from your input data.', '<h3>First Question:</h3>\n\n<p>That\'s correct - if you feed a dataset you no longer need to catch the <code>OutOfRangeError</code>. </p>\n\n<p><code>repeat()</code> takes an optional argument for the number of times it should repeat. This means <code>repeat(10)</code> will iterate over the entire dataset 10 times. If you choose to omit the argument then it will repeat indefinately</p>\n\n<h3>Second Question</h3>\n\n<p><code>Shuffle()</code> (if used) should be called before <code>batch()</code> - we want to shuffle records not batches. </p>\n\n<p>The buffer is first filled by adding your records in order then, once full, a random one is selected and emitted and a new record read from the original source. </p>\n\n<p>If you have something like</p>\n\n<pre class=""lang-py prettyprint-override""><code>ds.shuffle(1000).batch(100)\n</code></pre>\n\n<p>then in order to return a single batch, this last step is repeated 100 times (maintaining the buffer at 1000). Batching is a separate operation.</p>\n\n<h3>Third question</h3>\n\n<p>Generally we don\'t shuffle a <strong>test</strong> set at all - only the training set (We evaluate using the entire test set anyway, right? So why shuffle?).</p>\n\n<blockquote>\n  <p>So, if I wanted to just test on the whole test dataset I wouldn\'t use\n  <code>.batch</code></p>\n</blockquote>\n\n<p>Hmm - not so (at least not always). You would certainly need to use batch if your whole test dataset didnt fit into memory - a common occurrence. You would want to test the whole dataset but to run the numbers in manageable bites!</p>\n', ""<p>There's different ways to do what you want here, but the one I always use is: </p>\n\n<pre><code>batch_size = 32\nds = tf.Dataset()\nds = ds.shuffle(len_ds)\ntrain_ds = ds.take(0.8*len_ds)\ntrain_ds = train_ds.repeat().batch(batch_size)\nvalidation_ds = ds.skip(0.8*len_ds)\nvalidation_ds = train_ds.repeat().batch(batch_size)\nmodel.fit(train_ds,\n          steps_per_epoch = len_train_ds // batch_size,\n          validation_data = validation_ds,\n          validation_steps = len_validation_ds // batch_size,\n          epochs = 5)\n</code></pre>\n\n<p>This way you have access to all the variables after model fitting as well, for example if you want to visualize the validation set, you can. This is not really possible with <code>validation_split</code>. If you remove <code>.batch(batch_size)</code>, you should remove the <code>// batch_size</code>s, but I would leave them, as it clearer what is happening now.</p>\n\n<p>You always have to provide epochs.</p>\n\n<p>Calculating the length of your train/validation sets requires you to loop over them: </p>\n\n<pre><code>len_train_ds = 0\nfor i in train_ds:\n  len_train_ds += 1\n</code></pre>\n\n<p>if in <code>tf.Dataset</code> form.</p>\n""]","{'https://stackoverflow.com/questions/56944856/tensorflow-dataset-questions-about-shuffle-batch-and-repeat/56951558#56951558', 'https://stackoverflow.com/questions/60398554/should-we-apply-repeat-batch-shuffle-to-tf-data-dataset-when-passing-it-to-fit/60411616#60411616', 'https://stackoverflow.com/questions/63140320/how-to-use-sequence-generator-on-tf-data-dataset-object-to-fit-partial-data-into/63142173#63142173'}",{56944856},0.1932886602600185,0.12203353316609604
21,55909188,tf.nn.conv2d,Documentation Replication on Other Examples,How can I apply a TensorFlow 2D Convolution (tf.nn.conv2d) to a single (non-batch) 2D image?,"<p>I would like to use the function <code>tf.nn.conv2d()</code> on a <strong>single</strong> image example, but the TensorFlow documentation seems to only mention applying this transformation to a <strong>batch</strong> of images. </p>

<p>The docs mention that the input image must be of shape <code>[batch, in_height, in_width, in_channels]</code> and the kernel must be of shape <code>[filter_height, filter_width, in_channels, out_channels]</code>. However, what is the most straightforward way to achieve 2D convolution with input shape <code>[in_height, in_width, in_channels]</code>?</p>

<p>Here is an example of the current approach, where <code>img</code> has shape (height, width, channels):</p>

<pre><code>img = tf.random_uniform((10,10,3))  # a single image
img = tf.nn.conv2d([img], kernel)[0] # creating a batch of 1, then indexing the single example
</code></pre>

<p>I am reshaping the input as follows:</p>

<p><code>[in_height, in_width, in_channels]-&gt;[1, in_height, in_width, in_channels]-&gt;[in_height, in_width, in_channels]</code> </p>

<p>This feels like an unnecessary and costly operation when I am only interested in transforming one example.</p>

<p>Is there a simple/standard way to do this that doesn't involve reshaping?</p>
","<p>AFAIK there is no way around it. It seems (<a href=""https://stackoverflow.com/questions/50779869/does-tensorflow-tf-slice-incur-allocation-and-or-memory-copy"">here</a> and <a href=""https://stackoverflow.com/questions/53398721/tensorflow-can-reshape-create-a-copy"">here</a>) that the first operation creates a copy (someone correct me if I'm wrong). You may use <a href=""https://www.tensorflow.org/api_docs/python/tf/expand_dims"" rel=""nofollow noreferrer""><code>tf.expand_dims</code></a> instead though, it's IMO more readable because of it's verbosity.</p>

<p>On the other hand, taking <code>0</code> element from the tensor should not perform a copy in this case and is almost free.</p>

<p><strong>Most importantly</strong>, except for a little inconvenience with syntax (e.g. <code>[0]</code>) those operations definitely <strong>are not costly</strong>, especially in the context of performing convolution.</p>

<p>BTW. Other ready alternative layers like the ones in <code>tf.keras</code>, require batch as first dimension as well.</p>
","{65542469, 70008005, 50066313, 55015018, 48190601, 63814193, 52923062, 68446616, 54194233, 50453981, 42743199}","[{'QuestionId': 70008005, 'AnswerId': 70018139, 'URL': 'https://stackoverflow.com/questions/70008005/feeding-a-2d-image-to-a-tensorflow-cnn-for-image-classification/70018139#70018139', 'QuestionTitle': 'Feeding a 2D image to a TensorFlow CNN for image classification', 'Answer': ""<p>When the loss goes to nan the easiest explanation is that you are having either exploding or vanishing gradients. Try using gradient clipping (either by value or by norm) on your optimizer, that basically does not allow the gradients to go above or beyond a certain value.</p>\n<p>This also can affect the performance of the network but is always better than a loss of nan.</p>\n<p>With your current code, the implementation would look like this:</p>\n<pre><code>model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=0.5),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\n</code></pre>\n<p>The value is just an example, if it works, try tweaking it a bit.</p>\n"", 'IsAccepted': False, 'CreationDate': 1637231063}, {'QuestionId': 70008005, 'AnswerId': 70008102, 'URL': 'https://stackoverflow.com/questions/70008005/feeding-a-2d-image-to-a-tensorflow-cnn-for-image-classification/70008102#70008102', 'QuestionTitle': 'Feeding a 2D image to a TensorFlow CNN for image classification', 'Answer': '<p>I think you need to change a few things: the input shape to your model does not need the <code>batch_size</code>, it will be inferred during training. Change it to <code>(400, 400, 3)</code>. Second, if you are working with binary labels, you need to change your loss function to <code>tf.keras.losses.BinaryCrossentropy</code> and your metric to <code>tf.keras.metrics.BinaryAccuracy</code> or simply <code>accuracy</code>. Furthermore, your output layer should have one output node instead of two: <code>tf.keras.layers.Dense(1)</code></p>\n<p>Here is a running example based on your code:</p>\n<pre class=""lang-py prettyprint-override""><code>import numpy as np\nimport tensorflow as tf\n\nno_of_samples = 250\nBATCH_SIZE = 16\nSHUFFLE_BUFFER_SIZE = 50\n\ndata, labels = np.random.random((no_of_samples, 400, 400, 3)), np.random.randint(2, size=no_of_samples)\n\ndataset = tf.data.Dataset.from_tensor_slices((data, labels)).shuffle(SHUFFLE_BUFFER_SIZE)\ntest_dataset = dataset.take(50).batch(BATCH_SIZE)\ntrain_dataset = dataset.skip(50).batch(BATCH_SIZE)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(200, 5, strides=3, activation=\'relu\', input_shape=(400, 400, 3)),\n    tf.keras.layers.Conv2D(100, 5, strides=2, activation=&quot;relu&quot;),\n    tf.keras.layers.Conv2D(50, 5, activation=&quot;relu&quot;),\n    tf.keras.layers.Conv2D(25, 3, activation=&quot;relu&quot;),\n    tf.keras.layers.MaxPooling2D(3),\n    tf.keras.layers.Conv2D(50, 3, activation=&quot;relu&quot;),\n    tf.keras.layers.Conv2D(25, 3, activation=&quot;relu&quot;),\n    tf.keras.layers.MaxPooling2D(3),\n    tf.keras.layers.Conv2D(50, 2, activation=&quot;relu&quot;),\n    tf.keras.layers.Conv2D(25, 2, activation=&quot;relu&quot;),\n    \n    tf.keras.layers.GlobalMaxPooling2D(),\n\n    # Finally, we add a classification layer.\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=[\'accuracy\'])\n\nmodel.fit(train_dataset, epochs=10, validation_data=test_dataset)\n</code></pre>\n<pre><code>print(\'Labels shape --&gt;\',labels.shape)\nprint(\'Labels --&gt;\', labels)\nLabels shape --&gt; (250,)\nLabels --&gt; [1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0\n 0 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 0 0\n 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1\n 1 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0\n 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1\n 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1\n 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1637166393}, {'QuestionId': 68446616, 'AnswerId': 68450913, 'URL': 'https://stackoverflow.com/questions/68446616/how-do-i-run-an-iterative-2d-convolution-for-each-slice-of-a-tensor/68450913#68450913', 'QuestionTitle': 'How do I run an iterative 2D convolution for each slice of a tensor?', 'Answer': '<p>If you have a tensor with shape <code>(500,100,100)</code> and want to feed some subset of this tensor, to separate <code>conv2d</code> layers at the same time, you may do this by defining <code>conv2d</code> layers in the same level. You should first define <code>Lambda</code> layers to split input, then feed their output to <code>Conv2D</code> layers, then <code>concatenate</code> them.</p>\n<p>Let\'s take a tensor with shape <code>(100,28,28,1)</code> as an example, that we want to split it into 2 subset tensor and apply <code>conv2d</code> layers on each subset separately:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, concatenate, Lambda\nfrom tensorflow.keras.models import Model\n\n# define a sample dataset\nx = tf.random.uniform((100, 28, 28, 1))\ny = tf.random.uniform((100, 1), dtype=tf.int32, minval=0, maxval=9)\nds = tf.data.Dataset.from_tensor_slices((x, y))\nds = ds.batch(16)\n\ndef create_nn_model():\n    input = Input(shape=(28,28,1))\n    b1 = Lambda(lambda a: a[:,:14,:,:], name=&quot;first_slice&quot;)(input)\n    b2 = Lambda(lambda a: a[:,14:,:,:], name=&quot;second_slice&quot;)(input)\n    d1 = Conv2D(64, 2, padding=\'same\', activation=\'relu\', name=&quot;conv1_first_slice&quot;)(b1)\n    d2 = Conv2D(64, 2, padding=\'same\', activation=\'relu\', name=&quot;conv2_second_slice&quot;)(b2)\n    x =  concatenate([d1,d2], axis=1)\n    x = Flatten()(x)\n    x = Dense(64, activation=\'relu\')(x)\n    out = Dense(10, activation=\'softmax\')(x)\n    model = Model(input, out)\n    model.compile(loss=\'sparse_categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n    return model\n\nmodel = create_nn_model()\ntf.keras.utils.plot_model(model, show_shapes=True) \n</code></pre>\n<p>Here is the plotted model architecture:</p>\n<p><a href=""https://i.stack.imgur.com/fU1jo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fU1jo.png"" alt=""plotted_model"" /></a></p>\n', 'IsAccepted': True, 'CreationDate': 1626765263}, {'QuestionId': 50453981, 'AnswerId': 50506681, 'URL': 'https://stackoverflow.com/questions/50453981/implement-2d-convolution-using-fft/50506681#50506681', 'QuestionTitle': 'Implement 2D convolution using FFT', 'Answer': '<p>A linear discrete convolution of the form <code>x * y</code> can be computed using convolution theorem and the discrete time Fourier transform (DTFT). If <code>x * y</code> is a circular discrete convolution than it can be computed with the discrete Fourier transform (DFT).</p>\n<p>The convolution theorem states <code>x * y</code> can be computed using the Fourier transform as</p>\n<p><a href=""https://i.stack.imgur.com/B1bAt.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B1bAt.gif"" alt=""Convolution theorem"" /></a></p>\n<p>where <a href=""https://i.stack.imgur.com/1hwbX.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1hwbX.gif"" alt=""Fourier transform"" /></a> denotes the Fourier transform and <a href=""https://i.stack.imgur.com/2pphA.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2pphA.gif"" alt=""Inverse Fourier transform"" /></a> the inverse Fourier transform. When <code>x</code> and <code>y</code> are discrete and their convolution is a linear convolution this is computed using the DTFT as</p>\n<p><a href=""https://i.stack.imgur.com/sYDUg.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sYDUg.gif"" alt=""Discrete convolution theorem"" /></a></p>\n<p>If <code>x</code> and <code>y</code> are discrete and their convolution is a circular convolution the DTFT above is replaced by the DFT. <strong>Note:</strong> linear convolution problems can be embedded in circular convolution problems.</p>\n<hr />\n<p>I\'m more familiar with MATLAB but from reading the TensorFlow documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/signal/fft2d"" rel=""nofollow noreferrer""><code>tf.signal.fft2d</code></a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/signal/ifft2d"" rel=""nofollow noreferrer""><code>tf.signal.ifft2d</code></a> the solution below should be easily convertible to TensorFlow by replacing the MATLAB functions <code>fft2</code> and <code>ifft2</code>.</p>\n<p>In MATLAB (and TensorFlow) <code>fft2</code> (and <code>tf.signal.fft2d</code>) computes the DFT using the fast Fourier transform algorithm. If the convolution of <code>x</code> and <code>y</code> is circular this can be computed by</p>\n<pre><code>ifft2(fft2(x).*fft2(y))\n</code></pre>\n<p>where <code>.*</code> represents element by element multiplication in MATLAB. However, if it is linear then we zero pad the data to length <code>2N-1</code> where <code>N</code> is the length of one dimension (1024 in the question). In MATLAB this can be computed one of two ways. Firstly, by</p>\n<pre><code>h = ifft2(fft2(x, 2*N-1, 2*N-1).*fft2(y, 2*N-1, 2*N-1));\n</code></pre>\n<p>where MATLAB computes the <code>2*N-1</code>-point 2D Fourier transform of <code>x</code> and <code>y</code> by zero padding and then the <code>2*N-1</code>-point 2D inverse Fourier transform. This method can\'t be used in TensorFlow (from my understanding of the documentation) so the next is the only option. In MATLAB and TensorFlow the convolution can be computed by first extending <code>x</code> and <code>y</code> to size <code>2*N-1</code> x <code>2*N-1</code> and then computing the <code>2*N-1</code>-point 2D Fourier transform and inverse Fourier transform</p>\n<pre><code>x_extended = x;\nx_extended(2*N-1, 2*N-1) = 0;\n\ny_extended = y;\ny_extended(2*N-1, 2*N-1) = 0;\n\nh_extended = ifft2(fft2(x_extended).*fft2(y_extended));\n</code></pre>\n<p>In MATLAB, <code>h</code> and <code>h_extended</code> are exactly equal. The convolution of <code>x</code> and <code>y</code> can be computed without the Fourier transform with</p>\n<pre><code>hC = conv2(x, y);\n</code></pre>\n<p>in MATLAB.</p>\n<hr />\n<p>In MATLAB on my laptop <code>conv2(x, y)</code> takes 55 seconds whereas the Fourier transform approach takes less than 0.4 seconds.</p>\n', 'IsAccepted': False, 'CreationDate': 1527156814}, {'QuestionId': 65542469, 'AnswerId': 65542470, 'URL': 'https://stackoverflow.com/questions/65542469/how-do-i-use-tensorflow-tf-nn-conv2-to-make-a-convolutional-layer/65542470#65542470', 'QuestionTitle': 'How do I use Tensorflow tf.nn.conv2 to make a convolutional layer?', 'Answer': '<p>When you subclass a <a href=""https://www.tensorflow.org/tutorials/customization/custom_layers"" rel=""nofollow noreferrer""><code>tf.keras.layers.Layer</code></a>, the model will track all <a href=""https://www.tensorflow.org/api_docs/python/tf/Variable"" rel=""nofollow noreferrer""><code>tf.Variable</code></a> inside as trainable variables. What you then need to do is create a <code>tf.Variable</code> with the shape of the convolutional filter, and these will adjust to the task (i.e. learn) during training. The filters need this input shape:</p>\n<pre><code>(filter_height, filter_width, in_channels, out_channels)\n</code></pre>\n<p>This <code>tf.keras.layers.Layer</code> object will behave exactly like a Keras convolutional layer in a CNN:</p>\n<pre><code>class CustomLayer(tf.keras.layers.Layer):\n    def __init__(self, filters, kernel_size, padding, strides, activation,\n                 kernel_initializer, bias_initializer, use_bias):\n        super(CustomLayer, self).__init__()\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.activation = activation\n        self.padding = padding\n        self.kernel_initializer = kernel_initializer\n        self.bias_initializer = bias_initializer\n        self.strides = strides\n        self.use_bias = use_bias\n        self.w = None\n        self.b = None\n\n    def build(self, input_shape):\n        *_, n_channels = input_shape\n        self.w = tf.Variable(\n            initial_value=self.kernel_initializer(shape=(*self.kernel_size,\n                                                         n_channels,\n                                                         self.filters),\n                                 dtype=\'float32\'), trainable=True)\n        if self.use_bias:\n            self.b = tf.Variable(\n                initial_value=self.bias_initializer(shape=(self.filters,), dtype=\'float32\'),\n                trainable=True)\n\n    def call(self, inputs, training=None):\n        x =  tf.nn.conv2d(inputs, filters=self.w, strides=self.strides, padding=self.padding)\n        if self.use_bias:\n            x = x + self.b\n        x = self.activation(x)\n        return x\n</code></pre>\n<p>You can see that the weights are the filters of the <code>tf.nn.conv2d</code> operation, which are <code>tf.Variable</code>, and so they are weights that will be updated by model training.</p>\n<p>If you run this entire script, you will see that it performs the exact same task as the Keras convolutional layers.</p>\n<pre><code>import tensorflow as tf\ntf.random.set_seed(42)\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n\nrescale = lambda x, y: (tf.divide(tf.expand_dims(x, axis=-1), 255), y)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.map(rescale).\\\n    shuffle(128, reshuffle_each_iteration=False, seed=11).\\\n    batch(8).\\\n    prefetch(AUTOTUNE)\ntest_ds = test_ds.map(rescale).\\\n    shuffle(128, reshuffle_each_iteration=False, seed=11).\\\n    batch(8).\\\n    prefetch(AUTOTUNE)\n\n\nclass CustomLayer(tf.keras.layers.Layer):\n    def __init__(self, filters, kernel_size, padding, strides, activation,\n                 kernel_initializer, bias_initializer, use_bias):\n        super(CustomLayer, self).__init__()\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.activation = activation\n        self.padding = padding\n        self.kernel_initializer = kernel_initializer\n        self.bias_initializer = bias_initializer\n        self.strides = strides\n        self.use_bias = use_bias\n        self.w = None\n        self.b = None\n\n    def build(self, input_shape):\n        *_, n_channels = input_shape\n        self.w = tf.Variable(\n            initial_value=self.kernel_initializer(shape=(*self.kernel_size,\n                                                         n_channels,\n                                                         self.filters),\n                                 dtype=\'float32\'), trainable=True)\n        if self.use_bias:\n            self.b = tf.Variable(\n                initial_value=self.bias_initializer(shape=(self.filters,), \n                                                    dtype=\'float32\'),\n                trainable=True)\n\n    def call(self, inputs, training=None):\n        x =  tf.nn.conv2d(inputs, filters=self.w, strides=self.strides, \n                          padding=self.padding)\n        if self.use_bias:\n            x = x + self.b\n        x = self.activation(x)\n        return x\n\n\nclass ModelWithCustomConvLayer(tf.keras.Model):\n    def __init__(self, conv_layer):\n        super(ModelWithCustomConvLayer, self).__init__()\n        self.conv1 = conv_layer(filters=16,\n                                kernel_size=(3, 3),\n                                strides=(1, 1),\n                                activation=tf.nn.relu,\n                                padding=\'VALID\',\n                                kernel_initializer=tf.initializers.GlorotUniform(seed=42),\n                                bias_initializer=tf.initializers.Zeros(),\n                                use_bias=True)\n        self.maxp = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n        self.conv2 = conv_layer(filters=32,\n                                kernel_size=(3, 3),\n                                strides=(1, 1),\n                                activation=tf.nn.relu,\n                                padding=\'VALID\',\n                                kernel_initializer=tf.initializers.GlorotUniform(seed=42),\n                                bias_initializer=tf.initializers.Zeros(),\n                                use_bias=True)\n        self.flat = tf.keras.layers.Flatten()\n        self.dense1 = tf.keras.layers.Dense(32, activation=\'relu\',\n                            kernel_initializer=tf.initializers.GlorotUniform(seed=42))\n        self.dense2 = tf.keras.layers.Dense(10, activation=\'softmax\',\n                            kernel_initializer=tf.initializers.GlorotUniform(seed=42))\n\n    def call(self, inputs, training=None, mask=None):\n        x = self.conv1(inputs)\n        x = self.maxp(x)\n        x = self.conv2(x)\n        x = self.maxp(x)\n        x = self.flat(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        return x\n\n\ncustom = ModelWithCustomConvLayer(CustomLayer)\ncustom.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=\'adam\',\n               metrics=tf.metrics.SparseCategoricalAccuracy())\ncustom.build(input_shape=next(iter(train_ds))[0].shape)\ncustom.summary()\n\nnormal = ModelWithCustomConvLayer(tf.keras.layers.Conv2D)\nnormal.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=\'adam\',\n               metrics=tf.metrics.SparseCategoricalAccuracy())\nnormal.build(input_shape=next(iter(train_ds))[0].shape)\nnormal.summary()\n\nhistory_custom = custom.fit(train_ds, validation_data=test_ds, epochs=25,\n                            steps_per_epoch=10, verbose=0)\nhistory_normal = normal.fit(train_ds, validation_data=test_ds, epochs=25,\n                            steps_per_epoch=10, verbose=0)\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history_custom.history[\'loss\'], color=\'red\', alpha=.5, lw=4)\nplt.plot(history_custom.history[\'sparse_categorical_accuracy\'],\n         color=\'blue\', alpha=.5, lw=4)\nplt.plot(history_custom.history[\'val_loss\'], color=\'green\', alpha=.5, lw=4)\nplt.plot(history_custom.history[\'val_sparse_categorical_accuracy\'],\n         color=\'orange\', alpha=.5, lw=4)\n\nplt.plot(history_normal.history[\'loss\'], ls=\':\', color=\'red\')\nplt.plot(history_normal.history[\'sparse_categorical_accuracy\'], ls=\':\',\n         color=\'blue\')\nplt.plot(history_normal.history[\'val_loss\'], ls=\':\', color=\'green\')\nplt.plot(history_normal.history[\'val_sparse_categorical_accuracy\'], ls=\':\',\n         color=\'orange\')\nplt.legend(list(map(lambda x: \'custom_\' + x, list(history_custom.history.keys()))) +\n           list(map(lambda x: \'keras_\' + x, list(history_normal.history.keys()))))\nplt.title(\'Custom Conv Layer vs Keras Conv Layer\')\nplt.show()\n</code></pre>\n<p>The dotted lines represent the model performance when the Keras layer is used, and the full line is when my custom layer using <code>tf.nn.conv2d</code> is used. They are <strong>the exact same thing</strong> when the seed is set.</p>\n<p><a href=""https://i.stack.imgur.com/rt6Fl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rt6Fl.png"" alt=""enter image description here"" /></a></p>\n', 'IsAccepted': False, 'CreationDate': 1609611017}, {'QuestionId': 63814193, 'AnswerId': 63814858, 'URL': 'https://stackoverflow.com/questions/63814193/how-to-use-conv2d-in-multiple-images-input/63814858#63814858', 'QuestionTitle': 'How to use Conv2D in multiple images input?', 'Answer': ""<p><code>Conv2D</code> expects input in 4D, you can't change that. I'm not exactly sure what you're trying to accomplish but you could use <code>Conv3D</code> instead:</p>\n<pre><code>from tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nimport tensorflow as tf\n\nmodel = Sequential([\n    Input(shape=(None, 128, 128, 1)),\n    Conv3D(32, kernel_size=(1, 3, 3)),\n    Flatten()\n])\n\nmultiple_images = tf.random.uniform((10, 10, 128, 128, 1), dtype=tf.float32)\n\nmodel(multiple_images)\n</code></pre>\n<pre><code>&lt;tf.Tensor: shape=(10, 5080320), dtype=float32, numpy=\narray([[-0.26742983, -0.09689523, -0.12120364, ..., -0.02987139,\n         0.05515741,  0.12026916],\n       [-0.18898709,  0.12448274, -0.17439063, ...,  0.23424357,\n        -0.06001307, -0.13852882],\n       [-0.14464797,  0.26356792, -0.34748033, ...,  0.07819699,\n        -0.11639086,  0.10701762],\n       ...,\n       [-0.1536693 ,  0.13642962, -0.18564   , ...,  0.07165999,\n        -0.0173855 , -0.04348694],\n       [-0.32320747,  0.09207243, -0.22274591, ...,  0.11940736,\n        -0.02635285, -0.1140241 ],\n       [-0.21126074, -0.00094431, -0.10933039, ...,  0.06002581,\n        -0.09649743,  0.09335127]], dtype=float32)&gt;\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1599665299}, {'QuestionId': 50453981, 'AnswerId': 59164025, 'URL': 'https://stackoverflow.com/questions/50453981/implement-2d-convolution-using-fft/59164025#59164025', 'QuestionTitle': 'Implement 2D convolution using FFT', 'Answer': ""<p>This can be done in a way similar to which for instance <code>scipy.signal.fftconvolve</code> is implemented.</p>\n\n<p>Here is an example, assume we have an image (2 dimensions, if you have also multiple channels you can use the 3d instead of 2 functions) (im), and a filter (e.g. gaussian).</p>\n\n<p>First, take the Fourier transform of the image and define the <code>fft_lenghts</code> (useful if the filter is of a different shape, in which case it will get zero padded.)</p>\n\n<pre><code>fft_lenght1 = tf.shape(im)[0]\nfft_lenght2 = tf.shape(im)[1]\nim_fft = tf.signal.rfft2d(im, fft_length=[fft_lenght1, fft_lenght2])\n</code></pre>\n\n<p>Next, take the FFT of the filter (note, for instance for a 2d gaussian filter make sure the center is in the top left corner, i.e. use only a 'quarter')</p>\n\n<pre><code>kernel_fft = tf.signal.rfft2d(kernel, fft_length=[fft_lenght1, fft_lenght2])\n</code></pre>\n\n<p>Finally, take the inverse transform back to get the convolved image</p>\n\n<pre><code>im_blurred = tf.signal.irfft2d(im_fft * kernel_fft, [fft_lenght1, fft_lenght2])\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1575400967}, {'QuestionId': 55015018, 'AnswerId': 55019189, 'URL': 'https://stackoverflow.com/questions/55015018/using-conv2d-in-tensorflow/55019189#55019189', 'QuestionTitle': 'Using conv2d in tensorflow', 'Answer': '<p><strong>TL;DR answer.</strong> </p>\n\n<p>Use padding=""SAME"":</p>\n\n<pre><code>conv = tf.nn.conv2d(\n    cnn_inputs,\n    w,\n    strides=[1, 1, 1, 1],\n    padding=""SAME"") # old value is padding=""VALID""\n</code></pre>\n\n<p><strong>Detailed answer.</strong></p>\n\n<p>According to TF documentation input tensor (<code>cnn_inputs</code>) should be of shape <code>[batch, in_height, in_width, in_channels]</code> and kernel tensor (<code>w</code> in your example) should be of shape <code>[filter_height, filter_width, in_channels, out_channels]</code></p>\n\n<p>In your sample:</p>\n\n<ul>\n<li><code>cnn_input.shape</code> is <code>[7168, 1, 300, 1]</code> therefore <code>in_height == 1</code> and <code>in_width = 300</code></li>\n<li><code>w.shape</code> is <code>[5, 300, 1, 3]</code>, therefore <code>filter_height == 5</code> and <code>filter_width == 300</code> </li>\n</ul>\n\n<p>If <code>padding=""VALID""</code> and <code>stride=[1, 1, 1, 1]</code> then conv2d operation will ""shrink"" input tensor in spatial dimension by substracting <code>filter_size</code> in spatial dimension. For example if  <code>in_height == 20</code>, and <code>filter_height == 4</code> then output tensor height would probably be 20 - 4 = 16. In your sample with <code>in_height == 1</code> and <code>filter_height == 5</code> the shape of output tensor along the height dimension is approximately <code>in_height - filter_height = 1 - 5 = -4</code>, i.e. you recieve tensor with negative height which is not possible and that causes the error.</p>\n\n<p>With <code>padding=""SAME""</code> the conv2d operation tries to preserve spatial dimensions by adding zero values (the process is called ""zero padding""). Therefore the height of output tensor stays the same with <code>in_height</code></p>\n\n<p>You can find more detailed explanation for <code>padding</code> here: <a href=""https://stackoverflow.com/questions/37674306/what-is-the-difference-between-same-and-valid-padding-in-tf-nn-max-pool-of-t"">What is the difference between &#39;SAME&#39; and &#39;VALID&#39; padding in tf.nn.max_pool of tensorflow?</a></p>\n', 'IsAccepted': True, 'CreationDate': 1551863017}, {'QuestionId': 54194233, 'AnswerId': 54277252, 'URL': 'https://stackoverflow.com/questions/54194233/convert-a-2d-convolution-into-a-1d-convolution-affine-transformation-in-tensor/54277252#54277252', 'QuestionTitle': 'Convert a 2D Convolution into a 1D Convolution + Affine Transformation in TensorFlow?', 'Answer': '<p>You may be able to use the <a href=""https://en.wikipedia.org/wiki/Multidimensional_discrete_convolution#The_Helix_Transform"" rel=""nofollow noreferrer"">Helix Transform</a> in order to create 1D convolution which is equivalence of multi dimensional convolution.</p>\n', 'IsAccepted': True, 'CreationDate': 1547993147}, {'QuestionId': 52923062, 'AnswerId': 52924158, 'URL': 'https://stackoverflow.com/questions/52923062/tensorflow-compute-tf-nn-conv2d/52924158#52924158', 'QuestionTitle': 'tensorflow compute tf.nn.conv2d', 'Answer': '<p>To get identical results as in your excel example, you need to make the following changes: </p>\n\n<ol>\n<li>create two seperate weights</li>\n<li>calculate convolutions for each weight separately </li>\n</ol>\n\n<p>Code example: </p>\n\n<pre><code>x_raw = np.array([\n    [2,5,3],\n    [3,4,2],\n    [4,1,1]\n])\n#created two seperate weights \nweight1 = np.array(\n[[\n    [2,1],\n    [3,4]\n]])\n\nweight2 = np.array(\n[[\n    [4,1],\n    [1,2]\n]]\n)\nweight1 = tf.constant(weight1, dtype=tf.float32)\nweight2 = tf.constant(weight2, dtype=tf.float32)\nx = tf.constant(x_raw, dtype=tf.float32)\n\n#change out_channels to 1 \nfilter1 = tf.reshape(weight1, [2, 2, 1, 1])\nfilter2 = tf.reshape(weight2, [2, 2, 1, 1])\nimage = tf.reshape(x, [1, 3, 3, 1])\n\nwith tf.Session() as sess:\n  print(tf.nn.conv2d(image, filter1, [1, 1, 1, 1], ""VALID"").eval())\n  print(tf.nn.conv2d(image, filter2, [1, 1, 1, 1], ""VALID"").eval())\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1540192848}, {'QuestionId': 42743199, 'AnswerId': 52771066, 'URL': 'https://stackoverflow.com/questions/42743199/tensorflow-convolution-of-2d-array/52771066#52771066', 'QuestionTitle': 'TensorFlow convolution of 2D array', 'Answer': ""<p>In addition to kaufmanu's answer, you can use <code>tf.squeeze</code> if you need a 2D result. I.e.,</p>\n\n<pre><code>c=tf.squeeze(tf.nn.conv2d(a_tensor, k_weight,padding='VALID',strides=[1, 1, 1, 1]))\n</code></pre>\n\n<p>to perform the convolution.</p>\n"", 'IsAccepted': False, 'CreationDate': 1539308553}, {'QuestionId': 50066313, 'AnswerId': 50066616, 'URL': 'https://stackoverflow.com/questions/50066313/tensorflow-2d-convolution-with-mutliple-channels/50066616#50066616', 'QuestionTitle': 'Tensorflow - 2D convolution with mutliple channels', 'Answer': '<p>You just need to use <a href=""https://www.tensorflow.org/api_docs/python/tf/transpose"" rel=""nofollow noreferrer""><code>tf.transpose</code></a> before and after the computation:</p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nk = np.array([[\n    [1, 0, 1],\n    [2, 1, 0],\n    [0, 0, 1]\n],[\n    [1, 0, 1],\n    [2, 1, 0],\n    [0, 0, 1]\n]\n], dtype=np.float32)\ni = np.array([\n    [4, 3, 1, 0],\n    [2, 1, 0, 1],\n    [1, 2, 4, 1],\n    [3, 1, 0, 2]\n], dtype=np.float32)\n\nwith tf.Graph().as_default(), tf.Session() as sess:\n    kernel = tf.expand_dims(tf.transpose(k, (1, 2, 0)), 2, name=\'kernel\')\n    image  = tf.reshape(i, [1, 4, 4, 1], name=\'image\')\n    res = tf.squeeze(tf.nn.conv2d(image, kernel, [1, 1, 1, 1], ""VALID""))\n    res = tf.transpose(res, (2, 0, 1))\n    print sess.run(res)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[[[ 14.   6.]\n  [  6.  12.]]\n\n [[ 14.   6.]\n  [  6.  12.]]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1524846174}, {'QuestionId': 48190601, 'AnswerId': 48190976, 'URL': 'https://stackoverflow.com/questions/48190601/conv2d-on-non-rectangular-image-in-tensorflow/48190976#48190976', 'QuestionTitle': 'conv2d on non-rectangular image in Tensorflow', 'Answer': ""<p>If the black translates to 0 then you don't need to do anything. The convolution will multiply the 0 by whatever weight it has so it's not going to contribute to the result. If it's not you can multiply the data with a binary mask to make them 0.</p>\n\n<p>For all black pixels you will still get any bias term if you have any.\nYou could multiply the result with a binary mask to 0 out the areas you don't want populated. This way you can also decide to drop results that have too many black cells, like around the diagonal.</p>\n\n<p>You can also write your own custom operation that does what you want. I would recommend against it because you only get a speedup of at most 2 (the other operations will lower it). You probably get more performance by running on a GPU. </p>\n"", 'IsAccepted': True, 'CreationDate': 1515598658}, {'QuestionId': 42743199, 'AnswerId': 42746143, 'URL': 'https://stackoverflow.com/questions/42743199/tensorflow-convolution-of-2d-array/42746143#42746143', 'QuestionTitle': 'TensorFlow convolution of 2D array', 'Answer': '<p>The answers posted so far all miss one important point: Tensorflow does <em>not</em> compute a convolution, but a cross-correlation as is stated in the <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/convolution"" rel=""nofollow noreferrer"">doc</a>:</p>\n\n<blockquote>\n  <p>Note that although these ops are called ""convolution"", they are\n  strictly speaking ""cross-correlation"" since the filter is combined\n  with an input window without reversing the filter.</p>\n</blockquote>\n\n<p>If you really want to compute a convolution, you will have to reverse the kernel before passing it into <code>conv2d</code>, i.e. flip it once on the horizontal then on the vertical axis. Using Miriam\'s answer, this could look like this:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\na = np.random.randint(2, size=(10,10))\nk = np.array([[1,1,1],[1,1,1],[1,1,1]],dtype=np.float32)\nflip = [slice(None, None, -1), slice(None, None, -1)]\nk = k[flip]\n\na=a.astype(np.float32)\na_tensor = tf.reshape(a, [1, 10, 10, 1])\nk_weight = tf.reshape(np.array(k), [3,3,1,1])\nc=tf.nn.conv2d(a_tensor, k_weight,padding=\'VALID\',strides=[1, 1, 1, 1])\nsess=tf.Session()\nc.eval(session=sess)\n</code></pre>\n\n<p>Note that in this specific example flipping the kernel is technically in vain, because for symmetric kernels convolution and cross-correlation is the same thing. However, as soon as you have a non-symmetric kernels, you must flip it if you want Tensorflow to actually compute a convolution.</p>\n', 'IsAccepted': False, 'CreationDate': 1489313630}, {'QuestionId': 42743199, 'AnswerId': 42743379, 'URL': 'https://stackoverflow.com/questions/42743199/tensorflow-convolution-of-2d-array/42743379#42743379', 'QuestionTitle': 'TensorFlow convolution of 2D array', 'Answer': '<pre><code>import tensorflow as tf\nimport numpy as np\n\na = np.random.randint(2, size=(10,10))\nk = [[1,1,1],[1,1,1],[1,1,1]]\n\ntensor_a = tf.constant(a, tf.float32)\ntensor_k = tf.constant(k, tf.float32)\n\ntensor_res = tf.nn.convolution(tf.reshape(tensor_a, [1, 10, 10, 1]), tf.reshape(tensor_k, [3, 3, 1, 1]), padding=\'VALID\')\n\nsess = tf.Session()\nprint(sess.run(tensor_res))\n</code></pre>\n\n<p>The Computational Graph tutorial is <a href=""https://www.tensorflow.org/get_started/get_started"" rel=""nofollow noreferrer"">here</a></p>\n\n<p>Convolution helper <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/convolution"" rel=""nofollow noreferrer"">doc</a></p>\n', 'IsAccepted': False, 'CreationDate': 1489289178}]","{54413499, 61924406, 66828047}","['<p>In all Ccnv layers in Keras there is one dimension defined for the number of channels. For example you can have an image which has 2 Dimensions but Conv2D needs 3 dimension (without batch). The reason is simply because the image can have one channel (gray scale) or 3 for example (colored). the same is true for a 1D signal which can be any signal with any number of channels. you can simply add one dimension to you data. if you have an numpy array:\n<code>data = data[:, np.newaxis, :]</code> and setting channels_first <code>keras.layers.Conv1D(filters=64, kernel_size=3, activation=\'relu\', name=""conv_1, data_format=""channels_first"")</code>. you can do the same through adding extra dimension at the end and setting `data_format=""channels_last""</p>\n', ""<p>You have mentioned <code>padding='same'</code> that's why you are getting output having the same shape as the input image <strong>5x5x1x1</strong> it is in the format <code>Height X Width X Channel X No_of_filters</code>. <code>padding='same'</code> parameter add additional padding values to the image to get the desired output volume shape. you can use <code>padding='valid'</code> to avoid padding.</p>\n<p>You have missed batch dimension as well while providing an image to the model:</p>\n<pre><code>iX = iY = 9\nkX = kY = 3\nno_of_filter = 2\nimage = np.ones((iY,iX)).reshape((1,iY,iX,1))\nkernel = np.ones((kY,kX,no_of_filter)).reshape((kY,kX,1,no_of_filter))\nbias = np.array([0.0 for _ in range(no_of_filter)])\ni = layers.Input(shape=(iY,iX,1))#l_input\nx = layers.Conv2D(no_of_filter, (kY,kX), strides=1, padding=&quot;valid&quot;, activation='linear',weights=[kernel,bias])(i) #l_conv2d(i)\nmodel = keras.Model(i, x)\nmodel.summary()\nmodel(image).numpy().shape\n</code></pre>\n"", ""<p>You can directly load the Image data as you would normally do, the Image being binary will have no effect other that the input channel width becoming 1 for the input.</p>\n\n<p>Whenever you put an Image through a convnet, each output filter generally learns features for all the channels, so in case of a binary image, there is a separate kernel defined for each input channel / output channel combination (Since Only 1 input channel) in the first layer.</p>\n\n<p>Each channel is defined by it's <code>number of filters</code> and there exists a 2D kernel for each input channel which averages over all filters, so you will have weights/parameters equal to <code>input_channels * number_of_filters * filter_dims</code>, here for the first layer <code>input_channels</code> becomes one.</p>\n\n<p>Since you asked for some sample code. Let your image be in a tensor X, simply use</p>\n\n<p><code>X_out = tf.nn.conv2d(X, filters = 6, kernel_size = [height,width])</code></p>\n\n<p>After that you can apply an activation, this will make your output image have 6 channels.""]","{'https://stackoverflow.com/questions/61924406/1d-convolution-on-flat-one-dimensional-data-i-e-no-timeseries/61925188#61925188', 'https://stackoverflow.com/questions/54413499/how-to-use-the-black-white-image-as-the-input-to-tensorflow/54413655#54413655', 'https://stackoverflow.com/questions/66828047/why-does-conv2d-seems-to-only-performing-in-1d/66828230#66828230'}",,0.24856635946328606,0.2543643852868652
22,63919438,tf.keras.Model,Inadequate Examples,TensorFlow keras model fit() parameters steps_per_epoch and epochs behavior on train set,"<p>I'm using a tf.data dataset containing my training data consisting of (lets say) 100k images.
I'm also using a tf.data dataset containing my validation set.
Since an epoch of all 100k images takes quite long (in my case approximately one hour) before I get any feedback on performance on the validation set, I set the <code>steps_per_epoch</code> parameter in tf.keras.Model <code>fit()</code> to <code>10000</code>.
Using a batch size of 1 this results into having 10 validation scores when reaching 100k of images.
In order to complete one epoch of 100k images of my entire training dataset, I set the <code>epochs</code> parameter to <code>10</code></p>
<p>However, I'm not sure if using <code>steps_per_epoch</code> and <code>epochs</code> this way has any other consequences. Is it correct to use these parameters in order to get more frequent feedback on performance?
And also a more specific question, does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'?
I already dug into the <a href=""https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">TensorFlow docs</a> and read several different stack overflow questions, but I couldn't find anything conclusive to answer my own question. Hope you can help!</p>
<p>Tensorflow version I'm using is 2.2.0.</p>
","<blockquote>
<p>Is it correct to use these parameters in order to get more frequent
feedback on performance?</p>
</blockquote>
<p>Yes, it is correct to use these parameters. Here is the code that i used to fit the model.</p>
<pre><code>model.fit(
train_data,
steps_per_epoch = train_samples//batch_size,
epochs = epochs,
validation_data = test_data,
verbose = 1,
validation_steps = test_samples//batch_size)
</code></pre>
<blockquote>
<p>does it use all 100k images or does it use the same first 10k images of my
training set at every 'epoch'?</p>
</blockquote>
<p>It use all images in your training data.</p>
<p>For better understanding <code>Epoch</code> is the number times the learning algorithm will work through the entire training data set.</p>
<p>Where as <code>steps_per_epoch</code> is the total number of samples in your training data set divided by the batch size.</p>
<p>For example, if you have 100000 training samples and use a batch size of 100, one epoch will be equivalent to 1000 steps_per_epoch.</p>
<p><em>Note: We generally observe batch size to be the power of 2, this is because of the effective work of optimized matrix operation libraries.</em></p>
","{46820609, 58922916, 59535527, 64258121, 51885739, 61258124, 56562445, 48604149, 63429078, 71595702}","[{'QuestionId': 71595702, 'AnswerId': 71656820, 'URL': 'https://stackoverflow.com/questions/71595702/behavior-of-steps-per-epoch-and-validation-steps-in-keras-model/71656820#71656820', 'QuestionTitle': 'Behavior of steps_per_epoch and validation_steps in Keras Model', 'Answer': '<p>Since the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#fit"" rel=""nofollow noreferrer"">docs</a> state that using the parameter <code>shuffle</code> in <code>model.fit(...)</code> has no effect when using a generator and when <code>steps_per_epoch</code> is <em>not</em> <code>None</code>, it is essentially up to your data generator to shuffle the rows everytime it is called otherwise you will always get the same results. Check for example, how the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator?version=nightly#flow_from_directory"" rel=""nofollow noreferrer"">ImageDataGenerator</a> works:</p>\n<pre><code>import tensorflow as tf\n\nBATCH_SIZE = 2\n\nflowers = tf.keras.utils.get_file(\n    \'flower_photos\',\n    \'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\',\n    untar=True)\n\nimg_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)\n\nds = img_gen.flow_from_directory(flowers, batch_size=BATCH_SIZE, shuffle=True)\n\nfor x, y in ds:\n  print(x.shape, y)\n  break\n</code></pre>\n<p>Using <code>shuffle=True</code> results in different values everytime, whereas <code>shuffle=False</code> always returns the same values.</p>\n', 'IsAccepted': True, 'CreationDate': 1648531164}, {'QuestionId': 61258124, 'AnswerId': 71574143, 'URL': 'https://stackoverflow.com/questions/61258124/python-model-fit-generator-gets-stuck-on-first-epoch-and-tries-to-compute-unkno/71574143#71574143', 'QuestionTitle': 'Python model.fit_generator gets stuck on first epoch and tries to compute &quot;unknown&quot; number of steps', 'Answer': '<p>It could be an issue related to multiprocessing. You can also try setting <code>workers=1</code> and <code>use_multiprocessing=False</code>. It worked for me.</p>\n', 'IsAccepted': False, 'CreationDate': 1647960517}, {'QuestionId': 51885739, 'AnswerId': 68115010, 'URL': 'https://stackoverflow.com/questions/51885739/how-to-properly-set-steps-per-epoch-and-validation-steps-in-keras/68115010#68115010', 'QuestionTitle': 'How to properly set steps_per_epoch and validation_steps in Keras?', 'Answer': '<p>In addition to Greeser\'s answer,\nTo avoid losing some training samples, you could calculate your steps with this function:</p>\n<pre class=""lang-py prettyprint-override""><code>def cal_steps(num_images, batch_size):\n   # calculates steps for generator\n   steps = num_images // batch_size\n\n   # adds 1 to the generator steps if the steps multiplied by\n   # the batch size is less than the total training samples\n   return steps + 1 if (steps * batch_size) &lt; num_images else steps\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1624534670}, {'QuestionId': 64258121, 'AnswerId': 64258721, 'URL': 'https://stackoverflow.com/questions/64258121/is-the-final-history-object-of-sequential-model-fit-calls-accurate/64258721#64258721', 'QuestionTitle': 'Is the final history object of sequential model.fit() calls accurate?', 'Answer': ""<p>By default <code>history</code> is initialized as a callback every time anew when you call fit. This is unless you provide some alternative. One way to do so is to pass the model's <code>history</code> from one <code>fit()</code> call to the next <code>fit()</code> as a callback:</p>\n<pre><code>model.fit(x, y, batch_size, epochs, callbacks=[model.history])\n</code></pre>\n<p>this way the new values will be appended to the previously accumulated values, so you'd get statistics over multiple runs of <code>fit()</code>.</p>\n<p>If you need something more special - save and process <code>history</code> objects  from each fit or write a custom callback with memory.</p>\n"", 'IsAccepted': True, 'CreationDate': 1602145947}, {'QuestionId': 63429078, 'AnswerId': 63430962, 'URL': 'https://stackoverflow.com/questions/63429078/what-is-steps-per-epoch-in-model-fit-generator-actually-doing/63430962#63430962', 'QuestionTitle': 'What is steps_per_epoch in model.fit_generator actually doing?', 'Answer': '<p>Did not go through your code but your original interpertation is correct. Actually per the documentation located <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model"" rel=""nofollow noreferrer"">here</a> you can omit steps per epoch and the model.fit will divide the length of your data set (N) by the batch size to determine the steps. I did copy and run your code. Guess what it printed the index as 5. Only thing I can think of that might be different are the imports.</p>\n', 'IsAccepted': True, 'CreationDate': 1597526854}, {'QuestionId': 61258124, 'AnswerId': 61259499, 'URL': 'https://stackoverflow.com/questions/61258124/python-model-fit-generator-gets-stuck-on-first-epoch-and-tries-to-compute-unkno/61259499#61259499', 'QuestionTitle': 'Python model.fit_generator gets stuck on first epoch and tries to compute &quot;unknown&quot; number of steps', 'Answer': '<p>you should pass <code>steps_per_epoch</code> and <code>validation_steps</code> parameters to your <code>fit_generator</code> function to let the model know how many batches there are for training and validation sets.</p>\n\n<p>The values for those parameters are usually number of examples divided by the batch size. In this case:</p>\n\n<pre><code>steps_per_epoch = 1602//64\nvalidation_steps = 395//64\n</code></pre>\n\n<p>Then:</p>\n\n<pre><code>model.fit_generator(train_generator, \n                epochs=epochs,                 \n                validation_data=val_generator,steps_per_epoch=1602//64, validation_steps=395//64)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1587068879}, {'QuestionId': 59535527, 'AnswerId': 59537277, 'URL': 'https://stackoverflow.com/questions/59535527/keras-fit-generator-takes-more-time-for-epoch/59537277#59537277', 'QuestionTitle': 'Keras.fit_generator takes more time for epoch', 'Answer': '<p>You are setting <code>steps_per_epoch</code> to the wrong value (this is why it takes longer than necessary): it is not set to the number of data points. <code>steps_per_epoch</code> should be set to the size of the dataset divided by the batch size, which should be 8000/32 = 250 for your training set, and 63 for your validation set.</p>\n', 'IsAccepted': False, 'CreationDate': 1577742980}, {'QuestionId': 59535527, 'AnswerId': 59535814, 'URL': 'https://stackoverflow.com/questions/59535527/keras-fit-generator-takes-more-time-for-epoch/59535814#59535814', 'QuestionTitle': 'Keras.fit_generator takes more time for epoch', 'Answer': '<p><strong>Update:</strong></p>\n\n<p>As Matias in his answer pointed out, your <code>steps_per_epoch</code> parameter setting in your <code>fit</code> method led for the huge slowing down per epoch.\nFrom the <a href=""https://keras.io/models/sequential/#fit_generator"" rel=""nofollow noreferrer"">fit_generator documentation</a>:</p>\n\n<blockquote>\n  <p><strong>steps_per_epoch:</strong> \n  <br>Integer. Total number of steps (batches of samples)\n  to yield from generator before declaring one epoch finished and\n  starting the next epoch. It should typically be equal to\n  ceil(num_samples / batch_size) Optional for Sequence: if unspecified,\n  will use the len(generator) as a number of steps.\n  <br><br><strong>validation_steps:</strong> <br>Only relevant if validation_data is a generator.\n  Total number of steps (batches of samples) to yield from\n  validation_data generator before stopping at the end of every epoch.\n  It should typically be equal to the number of samples of your\n  validation dataset divided by the batch size. Optional for Sequence:\n  if unspecified, will use the len(validation_data) as a number of\n  steps.</p>\n</blockquote>\n\n<p>Actually Keras has an inconsistency at handling the two parameters, as <code>fit</code> method raises an <code>Valuerror</code> if you uses a simple <strong>dataset</strong> instead of <strong>datagenerator</strong> and set the parameters like <code>batch_size=batch_size, steps_per_epoch=num_samples</code>:</p>\n\n<pre><code>ValueError: Number of samples 60000 is less than samples required for specified batch_size 200 and steps 60000\n</code></pre>\n\n<p>But when data comes from <strong>datagenerator</strong> it doesn\'t handle the same problem letting you to have an issue like the current one.</p>\n\n<p>I made a little example code to check these up.</p>\n\n<p>The <code>fit</code> method with <code>steps_per_epoch=num_samples</code>:</p>\n\n<pre><code>Number of samples: 60000\nNumber of samples per batch: 200\nTrain for 60000 steps, validate for 50 steps\nEpoch 1/5\n263/60000 [..............................] - ETA: 4:07:09 - loss: 0.2882 - accuracy: 0.9116\n</code></pre>\n\n<p>with <strong>ETA</strong> (estimated time): <strong>4:07:09</strong>,</p>\n\n<p>as this is for 60000 steps, each of 200 samples per batch.</p>\n\n<p><br>The same <code>fit</code> with <code>steps_per_epoch=num_samples // batch_size</code>:</p>\n\n<pre><code>Number of samples: 60000\nNumber of samples per batch: 200\nTrain for 300 steps, validate for 50 steps\nEpoch 1/5\n28/300 [=&gt;............................] - ETA: 1:15 - loss: 1.0946 - accuracy: 0.6446\n</code></pre>\n\n<p>with <strong>ETA</strong>: <strong>1:15</strong></p>\n\n<p><br>\n<strong>Solution:</strong></p>\n\n<pre><code>steps_per_epoch=(training_set.shape[0] // batch_size)\nvalidation_steps=(validation_set.shape[0] // batch_size)\n</code></pre>\n\n<p><br>\n<strong>Further possible issues regarding performance:</strong></p>\n\n<p>As @SajanGohil  wrote in his comment <code>train_datagen.flow_from_director</code> make some tasks like <strong>file operations</strong>, <strong>preprocessings</strong> before actual traning process which sometimes takes <strong>more time as the traning</strong> itself.</p>\n\n<p>So to avoid these extratime, you can do the preprocessing task before the whole traning process <strong>separately only once</strong>. Then you can use these preprocessed data at traning time.</p>\n\n<p>Anyway CNNs with vast images are rather time and resource consuming tasks, which assumes GPU usage for this reason.</p>\n', 'IsAccepted': False, 'CreationDate': 1577733394}, {'QuestionId': 58922916, 'AnswerId': 58982928, 'URL': 'https://stackoverflow.com/questions/58922916/on-epoch-end-not-call-after-all-the-steps-per-epoch-in-fit-generator/58982928#58982928', 'QuestionTitle': 'on_epoch_end not call after all the steps_per_epoch in fit_generator', 'Answer': ""<p>My total volume wasn't divisible by my batch size.  So it didn't trigger the on_epoch_end call.  Had an odd number of volume divide by 2</p>\n"", 'IsAccepted': False, 'CreationDate': 1574366691}, {'QuestionId': 51885739, 'AnswerId': 56739003, 'URL': 'https://stackoverflow.com/questions/51885739/how-to-properly-set-steps-per-epoch-and-validation-steps-in-keras/56739003#56739003', 'QuestionTitle': 'How to properly set steps_per_epoch and validation_steps in Keras?', 'Answer': '<p>Since Keras data generator is meant to loop infinitely, <code>steps_per_epoch</code> indicates how many times you will fetch a new batch from generator during single epoch. Therefore, if you simply take <code>steps_per_epoch = int(number_of_train_samples / batch_size)</code>, your last batch would have less than <code>batch_size</code> items and would be discarded. However, in your case, it\'s not a big deal to lose 1 image per training epoch. The same is for validation step. To sum up: your models are trained [almost :) ] correctly, because the quantity of lost elements is minor.</p>\n\n<p>Corresponding to implementation <code>ImageDataGenerator</code> <a href=""https://keras.io/preprocessing/image/#imagedatagenerator-class"" rel=""nofollow noreferrer"">https://keras.io/preprocessing/image/#imagedatagenerator-class</a> if your number of steps would be larger than expected, after reaching the maximum number of samples you will receive new batches from the beginning, because your data is looped over. In your case, if <code>steps_per_epoch = np.ceil(number_of_train_samples / batch_size)</code> you would receive one additional batch per each epoch which would contains repeated image.</p>\n', 'IsAccepted': False, 'CreationDate': 1561387408}, {'QuestionId': 56562445, 'AnswerId': 56562754, 'URL': 'https://stackoverflow.com/questions/56562445/keras-sequence-fit-generator-and-steps-per-epoch/56562754#56562754', 'QuestionTitle': 'Keras Sequence, fit_generator and steps_per_epoch', 'Answer': '<p>When using <code>Sequence</code>, you do not need to pass <code>steps_per_epoch</code>, as this information can be inferred from the <code>__len__</code> method of your <code>Sequence.</code></p>\n\n<p>If you pass <code>steps_per_epoch</code> while using <code>Sequence</code>, this will override any use of the <code>__len__</code> method and it will effectively only use <code>steps_per_epoch</code> samples from your sequence (from 0 to <code>steps_per_epoch - 1</code>), and it will reset back to zero at the end of the epoch. You can check this behavior in the <a href=""https://github.com/keras-team/keras/blob/master/keras/engine/training_generator.py#L178"" rel=""nofollow noreferrer"">keras source code</a>.</p>\n', 'IsAccepted': True, 'CreationDate': 1560344087}, {'QuestionId': 46820609, 'AnswerId': 46821197, 'URL': 'https://stackoverflow.com/questions/46820609/how-the-keras-steps-per-epoch-in-fit-generator-works/46821197#46821197', 'QuestionTitle': 'How the Keras steps_per_epoch in fit_generator works', 'Answer': '<p>The <code>steps_per_epoch</code> parameter is the number of batches of samples it will take to complete one full epoch. This is dependent on your batch size. The batch size is set where you initialize your training data. For example, if you\'re doing this with <code>ImageDataGenerator.flow()</code> or <code>ImageDataGenerator.flow_from_directory()</code>, the batch size is specified with the <code>batch_size</code> parameter in each of these.</p>\n\n<p>You said you have 3000 samples. </p>\n\n<ul>\n<li>If your batch size was 100, then <code>steps_per_epoch</code> would be 30.</li>\n<li>If your batch size was 10, then <code>steps_per_epoch</code> would be 300. </li>\n<li>If your batch size was 1, then <code>steps_per_epoch</code> would be 3000. </li>\n</ul>\n\n<p>This is because <code>steps_per_epoch</code> should be equivalent to the total number of samples divided by the batch size. The process of implementing this in Keras is available in the two videos below.</p>\n\n<p>The reason why you have to set <code>steps_per_epoch</code> is that the generator is designed to run indefinitely (See the <a href=""https://keras.io/models/model/#fit_generator"" rel=""noreferrer"">docs</a>: </p>\n\n<blockquote>\n  <p>""The generator is expected to loop over its data indefinitely.""</p>\n</blockquote>\n\n<p>). You implemented this by setting <code>while 1</code>.\nSince <code>fit_generator()</code> is supposed to run <code>epochs=x</code> times, the method must know when the next epoch begins within this indefinitely loop (and, hence, the data has to be drawn from the beginning again).</p>\n\n<ul>\n<li><a href=""https://youtu.be/LhEMXbjGV_4"" rel=""noreferrer"">Image preparation for CNN training with Keras</a></li>\n<li><a href=""https://youtu.be/daovGOlMbT4"" rel=""noreferrer"">Create and train a CNN in Keras</a></li>\n</ul>\n', 'IsAccepted': False, 'CreationDate': 1508372873}, {'QuestionId': 48604149, 'AnswerId': 48604620, 'URL': 'https://stackoverflow.com/questions/48604149/keras-fit-generator-and-steps-per-epoch/48604620#48604620', 'QuestionTitle': 'Keras fit_generator and steps_per_epoch', 'Answer': '<p>Yes, you will be assigned only 31 steps here. What you need to do is to do is this: </p>\n\n<pre><code>numpy.ceil(total_samples//batch_size)\n</code></pre>\n\n<p>Since, the model can take any number of inputs so even the last batch is lesser than 32 it will work. </p>\n', 'IsAccepted': True, 'CreationDate': 1517717540}]","{54499885, 63919438}","[""<blockquote>\n<p>Is it correct to use these parameters in order to get more frequent\nfeedback on performance?</p>\n</blockquote>\n<p>Yes, it is correct to use these parameters. Here is the code that i used to fit the model.</p>\n<pre><code>model.fit(\ntrain_data,\nsteps_per_epoch = train_samples//batch_size,\nepochs = epochs,\nvalidation_data = test_data,\nverbose = 1,\nvalidation_steps = test_samples//batch_size)\n</code></pre>\n<blockquote>\n<p>does it use all 100k images or does it use the same first 10k images of my\ntraining set at every 'epoch'?</p>\n</blockquote>\n<p>It use all images in your training data.</p>\n<p>For better understanding <code>Epoch</code> is the number times the learning algorithm will work through the entire training data set.</p>\n<p>Where as <code>steps_per_epoch</code> is the total number of samples in your training data set divided by the batch size.</p>\n<p>For example, if you have 100000 training samples and use a batch size of 100, one epoch will be equivalent to 1000 steps_per_epoch.</p>\n<p><em>Note: We generally observe batch size to be the power of 2, this is because of the effective work of optimized matrix operation libraries.</em></p>\n"", '<p>steps_per_epoch should be roughly equal to number of training examples divided by batch size (default is 32). Similarly validation_steps should be roughly equal to number of validation examples divided by batch size. You can find the documentation <a href=""https://keras.io/models/model/"" rel=""nofollow noreferrer"">here</a>.</p>\n<blockquote>\n<p>steps_per_epoch: Integer or None. Total number of steps (batches of\nsamples) before declaring one epoch finished and starting the next\nepoch. When training with input tensors such as TensorFlow data\ntensors, the default None is equal to the number of samples in your\ndataset divided by the batch size, or 1 if that cannot be determined.</p>\n<p>validation_steps: Only relevant if steps_per_epoch is specified. Total\nnumber of steps (batches of samples) to validate before stopping.</p>\n</blockquote>\n<p>In your case they should be</p>\n<pre><code>steps_per_epoch = len(train_X) / batch_size\nvalidation_steps = len(dev_X) / batch_size\n</code></pre>\n']","{'https://stackoverflow.com/questions/54499885/simple-tensorflow-keras-model-using-tf-tensor-objects-of-structured-data/54500655#54500655', 'https://stackoverflow.com/questions/63919438/tensorflow-keras-model-fit-parameters-steps-per-epoch-and-epochs-behavior-on-t/63935993#63935993'}",,0.19133916111747815,0.07680621869843746
23,68431633,tf.image.stateless_random_crop,Documentation Ambiguity,tf.image.stateless_random_crop VS. tf.image.random_crop. Shouldn't these be the same thing?,"<p>In tf 2.5, there are two functions for cropping an image: <code>tf.image.stateless_random_crop</code>, and <code>tf.image.random_crop</code>. The documentation states that <code>stateless_random_crop</code> is deterministic (always returns the same crop given one seed). However, <code>random_crop</code> has a seed parameter and is also deterministic, one would think. What is the actual difference between these two functions? I cannot find information about statelessness in Tensorflow anywhere.</p>
<p>The differences between <code>tf.image.stateless_random_crop</code>, and <code>tf.image.random_crop</code> are one line where stateless_random_uniform is used instead of a random_uniform:
stateless_random_crop: <a href=""https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465</a>
random_crop: <a href=""https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412</a></p>
<p>I always thought that <code>random_crop</code> would always return the same crop given a seed, but it looks like maybe that wasn't always true? Any enlightenment about statelessness in Tensorflow is greatly appreciated!</p>
","<p><code>random_crop</code> always return the same sequence of crops only when <strong>both</strong> global seed <strong>and</strong> operation seed are set.</p>
<ol>
<li>global seed is set using <code>tf.random.set_seed(global_seed)</code></li>
<li>operation seed is set by passing the seed argument into the operation, i.e., <code>tf.image.random_crop(value, size, seed=ops_seed)</code></li>
</ol>
<p>whereas what <code>stateless_random_crop</code> returns is totally determined by the seed you pass into it when the device and tensorflow version are unchanged.</p>
<p>And you are correct that the functions look redundant and duplicate but actually <code>tf.image.random_crop</code> is from the old RNGs API and it may be buggy in graph mode. The new RNGs API is <code>tf.random.Generator</code> and the stateless RNGs. For more information, see <a href=""https://www.tensorflow.org/guide/random_numbers"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/random_numbers</a></p>
<p>Using <code>tf.random.Generator</code> in combination with <code>stateless_random_crop</code>:</p>
<pre><code>class new_RNGs_random_crop:
  def __init__(self,seed,size):
    self.rand_generator=tf.random.Generator.from_seed(seed)
    self.size=size
  def random_crop(self,x):
    return tf.image.stateless_random_crop(x,self.size,
           seed=self.rand_generator.uniform_full_int([2],dtype=tf.int32))

dummy_dataset=tf.data.Dataset.from_tensor_slices(np.arange(2*3*3).reshape((2,3,3))).batch(1)
cropper=new_RNGs_random_crop(88883,(1,2,2))
dummy_dataset=dummy_dataset.map(cropper.random_crop)

for image in dummy_dataset:
  print(image)
</code></pre>
<p>Example outputs:</p>
<pre><code>tf.Tensor(
[[[3 4]
  [6 7]]], shape=(1, 2, 2), dtype=int64)
tf.Tensor(
[[[ 9 10]
  [12 13]]], shape=(1, 2, 2), dtype=int64)
</code></pre>
","{47738016, 41704484, 58130281, 50733227, 65196460, 38397521, 41559348, 51843509, 45917464, 43411738}","[{'QuestionId': 51843509, 'AnswerId': 74527773, 'URL': 'https://stackoverflow.com/questions/51843509/about-use-tf-image-crop-and-resize/74527773#74527773', 'QuestionTitle': 'About use tf.image.crop_and_resize', 'Answer': ""<p>Below is my working code, also output image is not black, this can be of help to someone</p>\n<pre><code>    for idx in range(len(bboxes)):\n    if bscores[idx] &gt;= Threshold:\n      #Region of Interest\n      y_min = int(bboxes[idx][0] * im_height)\n      x_min = int(bboxes[idx][1] * im_width)\n      y_max = int(bboxes[idx][2] * im_height)\n      x_max = int(bboxes[idx][3] * im_width)\n\n      class_label = category_index[int(bclasses[idx])]['name']\n      class_labels.append(class_label)\n      bbox.append([x_min, y_min, x_max, y_max, class_label, float(bscores[idx])])\n\n      #Crop Image - Working Code\n      cropped_image = tf.image.crop_to_bounding_box(image, y_min, x_min, y_max - y_min, x_max - x_min).numpy().astype(np.int32)\n\n      # encode_jpeg encodes a tensor of type uint8 to string\n      output_image = tf.image.encode_jpeg(cropped_image)\n      # decode_jpeg decodes the string tensor to a tensor of type uint8\n      #output_image = tf.image.decode_jpeg(output_image)\n\n      score = bscores[idx] * 100\n\n      file_name = tf.constant(OUTPUT_PATH+image_name[:-4]+'_'+str(idx)+'_'+class_label+'_'+str(round(score))+'%'+'_'+os.path.splitext(image_name)[1])\n\n      writefile = tf.io.write_file(file_name, output_image)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1669093980}, {'QuestionId': 41704484, 'AnswerId': 44585250, 'URL': 'https://stackoverflow.com/questions/41704484/what-is-difference-between-tf-truncated-normal-and-tf-random-normal/44585250#44585250', 'QuestionTitle': 'What is difference between tf.truncated_normal and tf.random_normal?', 'Answer': '<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal"" rel=""nofollow noreferrer"">documentation</a> says it all:\nFor truncated normal distribution:</p>\n<blockquote>\n<p>The values are drawn from a normal distribution with specified mean and standard deviation, discarding and re-drawing any samples that are more than two standard deviations from the mean.</p>\n</blockquote>\n<p>Most probably it is easy to understand the difference by plotting the graph for yourself (%magic is because I use jupyter notebook):</p>\n<pre><code>import tensorflow as tf\nimport matplotlib.pyplot as plt\n\n%matplotlib inline  \n\nn = 500000\nA = tf.truncated_normal((n,))\nB = tf.random_normal((n,))\nwith tf.Session() as sess:\n    a, b = sess.run([A, B])\n</code></pre>\n<p>And now</p>\n<pre><code>plt.hist(a, 100, (-4.2, 4.2));\nplt.hist(b, 100, (-4.2, 4.2));\n</code></pre>\n<p><a href=""https://i.stack.imgur.com/fXqXs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fXqXs.png"" alt=""enter image description here"" /></a></p>\n<hr />\n<p>The point for using truncated normal is to overcome saturation of tome functions like sigmoid (where if the value is too big/small, the neuron stops learning).</p>\n', 'IsAccepted': True, 'CreationDate': 1497604115}, {'QuestionId': 65196460, 'AnswerId': 65201332, 'URL': 'https://stackoverflow.com/questions/65196460/how-to-random-crop-an-unlabeled-tensorflow-dataset-valueerror-dimensions-must/65201332#65201332', 'QuestionTitle': 'How to random_crop an unlabeled tensorflow Dataset? ValueError: Dimensions must be equal, but are 4 and 3', 'Answer': ""<p>Try using a batch size of 1:</p>\n<pre><code>tensor = tf.image.random_crop(value=tensor, size=(1,256, 256, 3))\n</code></pre>\n<p>But I don't think you should mix high-level data loaders with a lower level <code>tf.data.Dataset</code>. Try using only the latter.</p>\n<pre><code>import tensorflow as tf\n\nimage_dir = r'C:\\Users\\user\\Pictures'\n\nfiles = tf.data.Dataset.list_files(image_dir + '\\\\*jpg')\n\ndef load(filepath):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_image(image)\n    return image\n\nds = files.map(load)\n\ndef augment(tensor):\n    tensor = tf.cast(x=tensor, dtype=tf.float32)\n    tensor = tf.divide(x=tensor, y=tf.constant(255.))\n    tensor = tf.image.random_crop(value=tensor, size=(100, 100, 3))\n    random_target = tf.random.uniform((1,), dtype=tf.int32, maxval=2)\n    return tensor, random_target\n\ntrain_set_raw = ds.map(augment).batch(32)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\n\nhistory = model.fit(train_set_raw)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1607439620}, {'QuestionId': 58130281, 'AnswerId': 58130407, 'URL': 'https://stackoverflow.com/questions/58130281/tensorflow-tf-set-random-seed-same-code-but-got-different-results/58130407#58130407', 'QuestionTitle': 'tensorflow: tf.set_random_seed() same code, but got different results', 'Answer': '<p>Can you set the seed in <code>tf.image.random_flip_left_right(image, seed = 0)</code> and test?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>okay, you said you are not using numpy in the code but TF uses it for some internal tasks. So, can you again try and this time fixing the numpy seed too.</p>\n\n<pre><code>import numpy as np\nfrom numpy.random import seed\nseed(0)\n</code></pre>\n\n<p>If this doesn\'t also work then maybe the problem is with <code>Horovod</code> as talked about it <a href=""https://github.com/horovod/horovod/issues/173#issuecomment-369247966"" rel=""nofollow noreferrer"">here</a>.</p>\n', 'IsAccepted': False, 'CreationDate': 1569571767}, {'QuestionId': 51843509, 'AnswerId': 58010746, 'URL': 'https://stackoverflow.com/questions/51843509/about-use-tf-image-crop-and-resize/58010746#58010746', 'QuestionTitle': 'About use tf.image.crop_and_resize', 'Answer': ""<p>Below is a concrete implementation of the <code>tf.image.crop_and_resize</code> API. tf version 1.14</p>\n\n<pre><code>import tensorflow as tf\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntf.enable_eager_execution()\n\ndef single_data_2(img_path):\n    img = tf.read_file(img_path)\n    img = tf.image.decode_bmp(img,channels=1)\n    img_4d = tf.expand_dims(img, axis=0)\n    processed_img = tf.image.crop_and_resize(img_4d,boxes= \n                   [[0.4529,0.72,0.4664,0.7358]],crop_size=[64,64],box_ind=[0])\n    processed_img_2 = tf.squeeze(processed_img,0)\n    raw_img_3 = tf.squeeze(img_4d,0)\n    return raw_img_3, processed_img_2\n\ndef plot_two_image(raw,processed):\n    fig=plt.figure(figsize=(35,35))\n    raw_ = fig.add_subplot(1,2,1)\n    raw_.set_title('Raw Image')\n    raw_.imshow(raw,cmap='gray')\n    processed_ = fig.add_subplot(1,2,2)\n    processed_.set_title('Processed Image')\n    processed_.imshow(processed,cmap='gray')\n\nimg_path = 'D:/samples/your_bmp_image.bmp'\n\nraw_img, process_img  = single_data_2(img_path)\nprint(raw_img.dtype,process_img.dtype)\nprint(raw_img.shape,process_img.shape)\nraw_img=tf.squeeze(raw_img,-1)\nprocess_img=tf.squeeze(process_img,-1)\nprint(raw_img.dtype,process_img.dtype)\nprint(raw_img.shape,process_img.shape)\nplot_two_image(raw_img,process_img)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1568895104}, {'QuestionId': 51843509, 'AnswerId': 56877577, 'URL': 'https://stackoverflow.com/questions/51843509/about-use-tf-image-crop-and-resize/56877577#56877577', 'QuestionTitle': 'About use tf.image.crop_and_resize', 'Answer': '<p>Yet another variant is to use <a href=""https://www.tensorflow.org/api_docs/python/tf/image/central_crop"" rel=""nofollow noreferrer"">tf.central_crop</a> function.</p>\n', 'IsAccepted': False, 'CreationDate': 1562186036}, {'QuestionId': 41704484, 'AnswerId': 43726140, 'URL': 'https://stackoverflow.com/questions/41704484/what-is-difference-between-tf-truncated-normal-and-tf-random-normal/43726140#43726140', 'QuestionTitle': 'What is difference between tf.truncated_normal and tf.random_normal?', 'Answer': ""<p><code>tf.truncated_normal()</code> selects random numbers from a normal distribution whose mean is close to 0 and values are close to 0. For example, from -0.1 to 0.1. It's called truncated because your cutting off the tails from a normal distribution.</p>\n\n<p><code>tf.random_normal()</code> selects random numbers from a normal distribution whose mean is close to 0, but values can be a bit further apart. For example, from -2 to 2.</p>\n\n<p>In machine learning, in practice, you usually want your weights to be close to 0.</p>\n"", 'IsAccepted': False, 'CreationDate': 1493670990}, {'QuestionId': 45917464, 'AnswerId': 53686652, 'URL': 'https://stackoverflow.com/questions/45917464/tensorflow-whats-the-difference-between-tf-nn-dropout-and-tf-contrib-rnn-dropo/53686652#53686652', 'QuestionTitle': 'Tensorflow: What&#39;s the difference between tf.nn.dropout and tf.contrib.rnn.DropoutWrapper?', 'Answer': '<p>I think dropout can only mask one end, like what you did with rnn_inputs.\nDropoutWrapper can mask multi end, like a lstm cell.</p>\n', 'IsAccepted': False, 'CreationDate': 1544300790}, {'QuestionId': 51843509, 'AnswerId': 51844778, 'URL': 'https://stackoverflow.com/questions/51843509/about-use-tf-image-crop-and-resize/51844778#51844778', 'QuestionTitle': 'About use tf.image.crop_and_resize', 'Answer': '<p>Actually, there\'s no problem with Tensorflow here.</p>\n\n<p>From the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/crop_and_resize"" rel=""noreferrer"">doc</a> of <code>tf.image.crop_and_resize</code> (emphasis is mine) : </p>\n\n<blockquote>\n  <p>boxes: A Tensor of type float32. A 2-D tensor of shape [num_boxes, 4].\n  The i-th row of the tensor specifies the coordinates of a box in the\n  box_ind[i] image and is specified in <strong>normalized coordinates</strong> [y1, x1,\n  y2, x2]. A normalized coordinate value of y is mapped to the image\n  coordinate at y * (image_height - 1), so as the [0, 1] interval of\n  normalized image height is mapped to [0, image_height - 1] in image\n  height coordinates. We do allow y1 > y2, in which case the sampled\n  crop is an up-down flipped version of the original image. The width\n  dimension is treated similarly. <strong>Normalized coordinates outside the [0,\n  1] range are allowed, in which case we use extrapolation_value to\n  extrapolate the input image values.</strong></p>\n</blockquote>\n\n<p>The boxes argument needs normalized coordinates. That\'s why you get a black box with your first set of coordinates <code>[100,100,300,300]</code>  (not normalized, and no extrapolation value provided), and not with your second set <code>[0.5,0.1,0.9,0.5]</code>.</p>\n\n<p>However, as that why matplotlib show you gibberish on your second attempt, it\'s just because you\'re using the wrong datatype.\nQuoting the matplotlib <a href=""https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html"" rel=""noreferrer"">documentation</a> of <code>plt.imshow</code> (emphasis is mine): </p>\n\n<blockquote>\n  <p>All values should be in the range [0 .. 1] for floats or [0 .. 255]\n  for integers. <strong>Out-of-range values will be clipped to these bounds</strong>.</p>\n</blockquote>\n\n<p>As you\'re using float outside the <code>[0,1]</code> range, matplotlib is bounding your values to <code>1</code>. That\'s why you get those colored pixels (either solid red, solid green  or solid blue, or a mixing of these). Cast your array to <code>uint_8</code> to get an image that make sense. </p>\n\n<pre><code>plt.imshow( a[1].astype(np.uint8))\n</code></pre>\n\n<hr>\n\n<p><strong>Edit :</strong>\nAs requested, I will dive a bit more into\n<code>tf.image.crop_and_resize</code>.</p>\n\n<p><em>[When providing non normalized coordinates and no extrapolation values], why I just get a blank result?</em></p>\n\n<p>Quoting the doc :</p>\n\n<blockquote>\n  <p>Normalized coordinates outside the [0, 1] range are allowed, in which\n  case we use extrapolation_value to extrapolate the input image values.</p>\n</blockquote>\n\n<p>So, normalized coordinates outside [0,1] are allowed. But they still need to be normalized ! \nWith your example, <code>[100,100,300,300]</code>, the coordinates you provide makes the red square. Your original image is the little green dot in the upper left corner! The default value of the argument <code>extrapolation_value</code> is <code>0</code>, so the values outside the frame of the original image are inferred as <code>[0,0,0]</code> hence the black.<br>\n<a href=""https://i.stack.imgur.com/E6g7h.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/E6g7h.png"" alt=""crop_and_resize""></a></p>\n\n<p>But if your usecase needs another value, you can provide it. The pixels will take a RGB value of <code>extrapolation_value%256</code> on each channel. This option is useful if the zone you need to crop is not fully included in you original images. (A possible usecase would be sliding windows for example).</p>\n', 'IsAccepted': True, 'CreationDate': 1534259993}, {'QuestionId': 51843509, 'AnswerId': 51844604, 'URL': 'https://stackoverflow.com/questions/51843509/about-use-tf-image-crop-and-resize/51844604#51844604', 'QuestionTitle': 'About use tf.image.crop_and_resize', 'Answer': '<p>It seems that <code>tf.image.crop_and_resize</code> expects pixel values in the range [0,1]. </p>\n\n<p>Changing your code to</p>\n\n<pre><code>test = tf.image.crop_and_resize(image=image_np_expanded/255., ...)\n</code></pre>\n\n<p>solved the problem for me.</p>\n', 'IsAccepted': False, 'CreationDate': 1534259345}, {'QuestionId': 41704484, 'AnswerId': 41704789, 'URL': 'https://stackoverflow.com/questions/41704484/what-is-difference-between-tf-truncated-normal-and-tf-random-normal/41704789#41704789', 'QuestionTitle': 'What is difference between tf.truncated_normal and tf.random_normal?', 'Answer': '<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/truncated_normal"" rel=""nofollow noreferrer"">API documentation for tf.truncated_normal()</a> describes the function as:</p>\n<blockquote>\n<p>Outputs random values from a truncated normal distribution.</p>\n<p>The generated values follow a normal distribution with specified mean and standard deviation, except that values whose magnitude is\nmore than 2 standard deviations from the mean are dropped and\nre-picked.</p>\n</blockquote>\n', 'IsAccepted': False, 'CreationDate': 1484679185}, {'QuestionId': 50733227, 'AnswerId': 51110331, 'URL': 'https://stackoverflow.com/questions/50733227/tf-random-crop-cannot-be-used-with-wildcards/51110331#51110331', 'QuestionTitle': 'tf.random_crop cannot be used with wildcards?', 'Answer': '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/random_crop"" rel=""nofollow noreferrer""><code>random_crop</code></a> doesn\'t work with wildcards. Instead of using -1 as the unknown dimension, you can do:</p>\n\n<p><code>AB_cropped = tf.random_crop(AB, [tf.shape(AB)[0], 480, 480, 4])</code></p>\n\n<p>Note that <code>tf.shape</code> gives the dynamic shape, which is fully known at runtime.</p>\n', 'IsAccepted': True, 'CreationDate': 1530313223}, {'QuestionId': 47738016, 'AnswerId': 47739680, 'URL': 'https://stackoverflow.com/questions/47738016/tensorflow-typeerror-in-random-crop/47739680#47739680', 'QuestionTitle': 'tensorflow: TypeError in random_crop', 'Answer': '<p>The problem comes from <code>batch_size</code>.\nIn <code>batch_size, _, _, _ = X.get_shape().as_list()</code>, <code>batch_size</code> is <strong>not an integer type</strong>.</p>\n\n<p>Use <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer""><code>map_fn()</code></a> instead to avoid compute <code>batch_size</code> in image related operation:\n</p>\n\n<pre><code>tf.map_fn(lambda img: tf.random_crop(img, [24, 24, 3]), X)\n</code></pre>\n\n<hr>\n\n<p>Reference:</p>\n\n<ol>\n<li><a href=""https://stackoverflow.com/questions/38920240/tensorflow-image-operations-for-batches/38922451#38922451"">TensorFlow image operations for batches</a></li>\n</ol>\n', 'IsAccepted': False, 'CreationDate': 1512915053}, {'QuestionId': 45917464, 'AnswerId': 45919372, 'URL': 'https://stackoverflow.com/questions/45917464/tensorflow-whats-the-difference-between-tf-nn-dropout-and-tf-contrib-rnn-dropo/45919372#45919372', 'QuestionTitle': 'Tensorflow: What&#39;s the difference between tf.nn.dropout and tf.contrib.rnn.DropoutWrapper?', 'Answer': '<p>The idea behind both is the same and it is dropout: the network ""drops"" (i.e does not use) some of its nodes in the prediction. This means reducing during training the capacity of the model to prevent overfitting. Thanks to dropout, the network learns not to rely exclusively on particular nodes for its prediction.</p>\n\n<p>The difference between the two methods is that:</p>\n\n<ul>\n<li><p><code>tf.nn.droput</code>is a generic function to perform droput to a given input tensor. Looking at the documentation:</p>\n\n<blockquote>\n  <p>Computes dropout.</p>\n  \n  <p>With probability <code>keep_prob</code>, outputs the input element scaled up by 1 /\n  <code>keep_prob</code>, otherwise outputs 0. The scaling is so that the expected\n  sum is unchanged.</p>\n</blockquote></li>\n<li><p><code>tf.contrib.rnn.DropoutWrapper</code> or <code>tf.nn.rnn_cell.DropoutWrapper</code> is a specific class to define Recurrent Neural Network cells with dropout applied both at the input and the output of the cell. Looking at the documentation: </p>\n\n<blockquote>\n  <p>Operator adding dropout to inputs and outputs of the given cell.</p>\n</blockquote>\n\n<p>In particular, it <a href=""https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/ops/rnn_cell_impl.py#L770"" rel=""nofollow noreferrer"">uses</a> <code>tf.nn.droput</code> to mask the input to the cell, the state and the output.</p></li>\n</ul>\n\n<p>The difference between your two pieces of code is that when you are using <code>tf.nn.dropout</code> you are masking the inputs of the first layer only. In the wrapper case, layer per layer, you are masking the outputs of the cells (since you are providing only the output probabilities )</p>\n', 'IsAccepted': True, 'CreationDate': 1503924226}, {'QuestionId': 43411738, 'AnswerId': 43416019, 'URL': 'https://stackoverflow.com/questions/43411738/tf-image-pad-to-bounding-box-vs-tf-pad-and-tf-image-crop-to-bounding-box-vs-tf-s/43416019#43416019', 'QuestionTitle': 'tf.image.pad_to_bounding_box VS tf.pad and tf.image.crop_to_bounding_box VS tf.slice', 'Answer': ""<p>Mostly you use them <code>tf.image.*</code> for easiness of use.</p>\n\n<p>Both <code>crop_to_bounding_box</code> and <code>pad_to_bounding_box</code> use <code>slice</code> and <code>pad</code>underneath, but also add checkings and constraints to make sure you don't spend hours trying to debug your slice/pad indices and offsets.</p>\n"", 'IsAccepted': True, 'CreationDate': 1492189664}, {'QuestionId': 41559348, 'AnswerId': 41559455, 'URL': 'https://stackoverflow.com/questions/41559348/whats-the-difference-between-those-two-commands-in-tensorflow/41559455#41559455', 'QuestionTitle': 'What&#39;s the difference between those two commands in TensorFlow', 'Answer': '<p>No, they are not equivalent. Statement 1 also adds <a href=""https://github.com/tensorflow/tensorflow/blob/27711108b5fce2e1692f9440631a183b3808fa01/tensorflow/contrib/layers/python/layers/layers.py#L1329"" rel=""nofollow noreferrer"">an activation function</a>.</p>\n\n<p>If you set <code>activation=None</code>, then they are equal.</p>\n', 'IsAccepted': True, 'CreationDate': 1484009123}, {'QuestionId': 38397521, 'AnswerId': 38401761, 'URL': 'https://stackoverflow.com/questions/38397521/tensorflow-what-is-random-crop-doing-in-cifar10-example/38401761#38401761', 'QuestionTitle': 'TensorFlow - What is random_crop doing in Cifar10 example?', 'Answer': '<p>In the example, <code>IMAGE_SIZE</code> is set to <code>24</code>. So basically what this code does is select a randomly chosen offset and extracts a <code>24 X 24</code> patch. It probably ensures that the offset is chosen in a way that the patch can be extracted without any wrap around or other weird boundary condition or maybe it pads it (should be easy to check).</p>\n\n<p>I guess <code>IMAGE_SIZE</code> could be better named as <code>PATCH_SIZE</code> or something. Note the original CIFAR 10 input image is <code>32 x 32</code></p>\n', 'IsAccepted': True, 'CreationDate': 1468602834}]",{68431633},"['<p><code>random_crop</code> always return the same sequence of crops only when <strong>both</strong> global seed <strong>and</strong> operation seed are set.</p>\n<ol>\n<li>global seed is set using <code>tf.random.set_seed(global_seed)</code></li>\n<li>operation seed is set by passing the seed argument into the operation, i.e., <code>tf.image.random_crop(value, size, seed=ops_seed)</code></li>\n</ol>\n<p>whereas what <code>stateless_random_crop</code> returns is totally determined by the seed you pass into it when the device and tensorflow version are unchanged.</p>\n<p>And you are correct that the functions look redundant and duplicate but actually <code>tf.image.random_crop</code> is from the old RNGs API and it may be buggy in graph mode. The new RNGs API is <code>tf.random.Generator</code> and the stateless RNGs.', '<p><code>random_crop</code> always return the same sequence of crops only when <strong>both</strong> global seed <strong>and</strong> operation seed are set.</p>\n<ol>\n<li>global seed is set using <code>tf.random.set_seed(global_seed)</code></li>\n<li>operation seed is set by passing the seed argument into the operation, i.e., <code>tf.image.random_crop(value, size, seed=ops_seed)</code></li>\n</ol>\n<p>whereas what <code>stateless_random_crop</code> returns is totally determined by the seed you pass into it when the device and tensorflow version are unchanged.</p>\n<p>And you are correct that the functions look redundant and duplicate but actually <code>tf.image.random_crop</code> is from the old RNGs API and it may be buggy in graph mode. The new RNGs API is <code>tf.random.Generator</code> and the stateless RNGs. For more information, see <a href=""https://www.tensorflow.org/guide/random_numbers"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/random_numbers</a></p>\n<p>Using <code>tf.random.Generator</code> in combination with <code>stateless_random_crop</code>:</p>\n<pre><code>class new_RNGs_random_crop:\n  def __init__(self,seed,size):\n    self.rand_generator=tf.random.Generator.from_seed(seed)\n    self.size=size\n  def random_crop(self,x):\n    return tf.image.stateless_random_crop(x,self.size,\n           seed=self.rand_generator.uniform_full_int([2],dtype=tf.int32))\n\ndummy_dataset=tf.data.Dataset.from_tensor_slices(np.arange(2*3*3).reshape((2,3,3))).batch(1)\ncropper=new_RNGs_random_crop(88883,(1,2,2))\ndummy_dataset=dummy_dataset.map(cropper.random_crop)\n\nfor image in dummy_dataset:\n  print(image)\n</code></pre>\n<p>Example outputs:</p>\n<pre><code>tf.Tensor(\n[[[3 4]\n  [6 7]]], shape=(1, 2, 2), dtype=int64)\ntf.Tensor(\n[[[ 9 10]\n  [12 13]]], shape=(1, 2, 2), dtype=int64)\n</code></pre>\n', 'For more information, see <a href=""https://www.tensorflow.org/guide/random_numbers"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/random_numbers</a></p>\n<p>Using <code>tf.random.Generator</code> in combination with <code>stateless_random_crop</code>:</p>\n<pre><code>class new_RNGs_random_crop:\n  def __init__(self,seed,size):\n    self.rand_generator=tf.random.Generator.from_seed(seed)\n    self.size=size\n  def random_crop(self,x):\n    return tf.image.stateless_random_crop(x,self.size,\n           seed=self.rand_generator.uniform_full_int([2],dtype=tf.int32))\n\ndummy_dataset=tf.data.Dataset.from_tensor_slices(np.arange(2*3*3).reshape((2,3,3))).batch(1)\ncropper=new_RNGs_random_crop(88883,(1,2,2))\ndummy_dataset=dummy_dataset.map(cropper.random_crop)\n\nfor image in dummy_dataset:\n  print(image)\n</code></pre>\n<p>Example outputs:</p>\n<pre><code>tf.Tensor(\n[[[3 4]\n  [6 7]]], shape=(1, 2, 2), dtype=int64)\ntf.Tensor(\n[[[ 9 10]\n  [12 13]]], shape=(1, 2, 2), dtype=int64)\n</code></pre>\n']",{'https://stackoverflow.com/questions/68431633/tf-image-stateless-random-crop-vs-tf-image-random-crop-shouldnt-these-be-the/68474421#68474421'},,0.23049153987714244,0.03380573741713244
24,64081367,tf.gather,Documentation Ambiguity,Slicing a tensor with a tensor of indices and tf.gather,"<p>I am trying to slice a tensor with a indices tensor. For this purpose I am trying to use <code>tf.gather</code>.
However, I am having a hard time understanding the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer"">documentation</a> and don't get it to work as I would expect it to:</p>
<p>I have two tensors. An <code>activations</code> tensor with a shape of <code>[1,240,4]</code> and an <code>ids</code> tensor with the shape <code>[1,1,120]</code>. I want to slice the second dimension of the <code>activations</code> tensor with the indices provided in the third dimension of the <code>ids</code> tensor:</p>
<pre><code>downsampled_activations = tf.gather(activations, ids, axis=1)
</code></pre>
<p>I have given it the <code>axis=1</code> option since that is the axis in the <code>activations</code> tensor I want to slice.</p>
<p>However, this does not render the expected result and only gives me the following error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0,0,1] = 1 is not in [0, 1)
</code></pre>
<p>I have tried various combinations of the <code>axis</code> and <code>batch_dims</code> options, but to no avail so far and the documentation doesn't really help me on my path. Anybody care to explain the parameters in more detail or on the example above would be very helpful!</p>
<p><strong>Edit:</strong>
The IDs are precomputed before runtime and come in through an input pipeline as such:</p>
<pre><code>features = tf.io.parse_single_example(
            serialized_example,
            features={ 'featureIDs': tf.io.FixedLenFeature([], tf.string)}
</code></pre>
<p>They are then reshaped into the previous format:</p>
<pre><code>feature_ids_raw = tf.decode_raw(features['featureIDs'], tf.int32)
feature_ids_shape = tf.stack([batch_size, (num_neighbours * 4)])
feature_ids = tf.reshape(feature_ids_raw, feature_ids_shape)
feature_ids = tf.expand_dims(feature_ids, 0)
</code></pre>
<p>Afterwards they have the previously mentioned shape (<code>batch_size = 1</code> and <code>num_neighbours = 30</code> -&gt; <code>[1,1,120]</code>) and I want to use them to slice the <code>activations</code> tensor.</p>
<p><strong>Edit2:</strong> I would like the output to be <code>[1,120,4]</code>. (So I would like to gather the entries along the second dimension of the <code>activations</code> tensor in accordance with the IDs stored in my <code>ids</code> tensor.)</p>
","<blockquote>
<p><code>tf.gather</code> Gather slices from <code>params</code> axis <code>axis</code> according to indices.</p>
</blockquote>
<p>Granted that the documentation is not the most expressive, and the emphasis should be placed on the <strong>slices</strong> (since you index slices from the <code>axis</code> and not elements, which is what I suppose you mistakenly took it for).</p>
<p><strong>Let's take a much smaller example:</strong></p>
<pre class=""lang-py prettyprint-override""><code>activations_small = tf.convert_to_tensor([[[1, 2, 3, 4], [11, 22, 33, 44]]])
print(activations_small.shape) # [1, 2, 4]
</code></pre>
<p>Let's picture this tensor:</p>
<pre><code>    XX 4  XX 44 XX XX
  XX  3 XX  33 X  XX
XXX 2 XX   22XX  XX
X-----X-----+X  XX
|  1  |  11 | XX
+-----+-----+X
</code></pre>
<p><code>tf.gather(activations1, [0, 0], axis=1)</code> will return</p>
<pre><code>&lt;tf.Tensor: shape=(1, 2, 4), dtype=int32, numpy=
array([[[1, 2, 3, 4],
        [1, 2, 3, 4]]], dtype=int32)&gt;
</code></pre>
<p>What <code>tf.gather</code> did was to <em>look from</em> axis 1, and picks up index 0 (ofc, two times i.e. <code>[0, 0]</code>). If you were to run <code>tf.gather(activations1, [0, 0, 0, 0, 0], axis=1).shape</code>, you'd get <code>TensorShape([1, 5, 4])</code>.</p>
<p><strong>Your Error</strong>
Now let's try to trigger the error that you're getting.</p>
<p><code>tf.gather(activations1, [0, 2], axis=1)</code></p>
<blockquote>
<p>InvalidArgumentError: indices[1] = 2 is not in [0, 2) [Op:GatherV2]</p>
</blockquote>
<p>What happened here was that when <code>tf.gather</code> looks from axis 1 perspective, there's no item (column if you will) with index = 2.</p>
<p>I guess this is what the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather#args"" rel=""nofollow noreferrer"">documentation</a> is hinting at by</p>
<blockquote>
<p><code>param:&lt;indices&gt;</code> The index Tensor. Must be one of the following types: int32, int64. Must be in range [0, params.shape[axis]).</p>
</blockquote>
<p><strong>Your (potential) solution</strong></p>
<p>From the dimensions of <code>indices</code>, and that of the expected result from your question, I am not sure if the above was very obvious to you.</p>
<p><code>tf.gather(activations, indices=[0, 1, 2, 3], axis=2)</code> or anything with indices within the range of indices in <code>[0, activations.shape[2])</code> i.e. <code>[0, 4)</code> would work. Anything else would give you the error that you're getting.</p>
<p>There's a verbatim answer below in case that's your expected result.</p>
","{70424291, 65580068, 55280388, 42597520, 70584240, 55359922, 47570804, 38851318, 53675834, 63172891}","[{'QuestionId': 42597520, 'AnswerId': 74282439, 'URL': 'https://stackoverflow.com/questions/42597520/slicing-a-tensor-by-an-index-tensor-in-tensorflow/74282439#74282439', 'QuestionTitle': 'Slicing a tensor by an index tensor in Tensorflow', 'Answer': '<p>I know I\'m late, but I recently had to do something similar, and was able to to do it using <a href=""https://www.tensorflow.org/api_docs/python/tf/RaggedTensor"" rel=""nofollow noreferrer"">Ragged Tensors</a>:</p>\n<pre><code>output =  tf.gather(params, tf.RaggedTensor.from_tensor(indices), batch_dims=-1, axis=-1)\n</code></pre>\n<p>Hope it helps</p>\n', 'IsAccepted': False, 'CreationDate': 1667341475}, {'QuestionId': 70584240, 'AnswerId': 70584776, 'URL': 'https://stackoverflow.com/questions/70584240/how-to-use-tf-gather-nd-for-multi-dimensional-tensor/70584776#70584776', 'QuestionTitle': 'How to use tf.gather_nd for multi-dimensional tensor', 'Answer': '<p>You will have to use <code>tf.meshgrid</code>, which will create a rectangular grid of two one-dimensional arrays representing the tensor indexing of the first and second dimension, since <code>tf.gather_nd</code> needs to know exactly where to extract values across the dimensions. Here is a simplified example:</p>\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\nimage = tf.random.normal((1, 4, 4, 3))\nimage = tf.squeeze(image, axis=0)\nidx = tf.argmin(image, axis=-1)\n\nij = tf.stack(tf.meshgrid(\n    tf.range(image.shape[0], dtype=tf.int64), \n    tf.range(image.shape[1], dtype=tf.int64),\n                              indexing=\'ij\'), axis=-1)\n\ngather_indices = tf.concat([ij, tf.expand_dims(idx, axis=-1)], axis=-1)\nresult = tf.gather_nd(image, gather_indices)\n\nprint(\'First option --&gt;\', tf.reduce_min(image, axis=-1))\nprint(\'Second option --&gt;\', result)\n</code></pre>\n<pre><code>First option --&gt; tf.Tensor(\n[[-0.53245485 -0.29117298 -0.64434254 -0.8209638 ]\n [-0.9386176  -0.5993224  -0.597746   -1.5392851 ]\n [-0.5478666  -1.5280861  -1.0344954  -1.920418  ]\n [-0.5580688  -1.425873   -1.9276617  -1.0668412 ]], shape=(4, 4), dtype=float32)\nSecond option --&gt; tf.Tensor(\n[[-0.53245485 -0.29117298 -0.64434254 -0.8209638 ]\n [-0.9386176  -0.5993224  -0.597746   -1.5392851 ]\n [-0.5478666  -1.5280861  -1.0344954  -1.920418  ]\n [-0.5580688  -1.425873   -1.9276617  -1.0668412 ]], shape=(4, 4), dtype=float32)\n</code></pre>\n<p>Or with your example:</p>\n<pre class=""lang-py prettyprint-override""><code>from skimage import data\nimport tensorflow as tf\nimport numpy as np\n\nimage = data.astronaut()\nimage = tf.cast(image, tf.float32)\nimage = tf.expand_dims(image, axis=0)\n\nmin_along_channels_1 = tf.reduce_min(image, axis=-1)\n\nimage = tf.squeeze(image, axis=0)\nidx = tf.argmin(image, axis=-1)\n\nij = tf.stack(tf.meshgrid(\n    tf.range(image.shape[0], dtype=tf.int64), \n    tf.range(image.shape[1], dtype=tf.int64),\n                              indexing=\'ij\'), axis=-1)\n\ngather_indices = tf.concat([ij, tf.expand_dims(idx, axis=-1)], axis=-1)\nmin_along_channels_2 = tf.gather_nd(image, gather_indices)\n\nprint(tf.equal(min_along_channels_1, min_along_channels_2))\n</code></pre>\n<pre><code>tf.Tensor(\n[[[ True  True  True ...  True  True  True]\n  [ True  True  True ...  True  True  True]\n  [ True  True  True ...  True  True  True]\n  ...\n  [ True  True  True ...  True  True  True]\n  [ True  True  True ...  True  True  True]\n  [ True  True  True ...  True  True  True]]], shape=(1, 512, 512), dtype=bool)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1641328766}, {'QuestionId': 70424291, 'AnswerId': 70424516, 'URL': 'https://stackoverflow.com/questions/70424291/how-to-use-tf-gather-similar-to-the-numpy-slicing/70424516#70424516', 'QuestionTitle': 'How to use tf.gather similar to the numpy slicing', 'Answer': '<p>Try <code>tf.stack</code> with <code>tf.gather_nd</code>:</p>\n<pre class=""lang-py prettyprint-override""><code>z = tf.gather_nd(t_arr, tf.stack([t_r, t_c], axis=1))\n</code></pre>\n<pre><code>tf.Tensor([ 517  517 1876 1876 2138 2138 3103 3103 3482 3482 1802 1802], shape=(12,), dtype=int32)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1640015809}, {'QuestionId': 65580068, 'AnswerId': 65616062, 'URL': 'https://stackoverflow.com/questions/65580068/how-to-use-slice-a-tensor-by-indexlist-and-compose-a-new-tensor/65616062#65616062', 'QuestionTitle': 'How to use slice a Tensor by indexlist and compose a new Tensor', 'Answer': '<p>Use <code>tf.gather()</code>:</p>\n<pre><code>text_embeding =tf.constant(\n                    #index 0       index 1      index 2\n                [[[0.1,0.2,0.3],[0.4,0.5,0.6],[0.1,0.2,0.3]], \n                [[0.1,0.2,0.3],[0.4,0.5,0.6],[0.1,0.2,0.3]],\n                [[0.1,0.2,0.3],[0.4,0.5,0.6],[0.1,0.2,0.3]]\n                ] \n            )\nindex_list = tf.constant([[0,0],[1,1],[2,2],[0,1],[1,2],[0,2]])\noutput = tf.gather(text_embeding, index_list, axis=2)\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1610036197}, {'QuestionId': 63172891, 'AnswerId': 63173585, 'URL': 'https://stackoverflow.com/questions/63172891/slicing-with-tf-gather-or-tf-gather-nd/63173585#63173585', 'QuestionTitle': 'slicing with tf.gather or tf.gather_nd', 'Answer': '<p>Try <code>tf.gather(x_test, actions, batch_dims=1)</code></p>\n', 'IsAccepted': True, 'CreationDate': 1596113524}, {'QuestionId': 55359922, 'AnswerId': 55360242, 'URL': 'https://stackoverflow.com/questions/55359922/slice-a-tensor-using-tensor-indices/55360242#55360242', 'QuestionTitle': 'Slice a tensor using tensor indices', 'Answer': '<p>You can use <a href=""https://www.tensorflow.org/api_docs/python/tf/Tensor#__getitem__"" rel=""nofollow noreferrer""><code>tf.Tensor.__getitem__</code></a> pretty much like with NumPy indexing:</p>\n\n<pre><code>img[:, h_start:h_end, w_start:w_end]\n</code></pre>\n\n<p>Alternatively, use <a href=""https://www.tensorflow.org/api_docs/python/tf/slice"" rel=""nofollow noreferrer""><code>tf.slice</code></a>:</p>\n\n<pre><code>sliced_img = tf.slice(img, [0, h_start, w_start], [-1, h_end - h_start, w_end-w_start]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1553612345}, {'QuestionId': 55280388, 'AnswerId': 55281589, 'URL': 'https://stackoverflow.com/questions/55280388/slicing-a-tensor-along-a-dimension-with-given-index/55281589#55281589', 'QuestionTitle': 'slicing a tensor along a dimension with given index', 'Answer': '<pre><code>tensor = tf.constant(\n  [[[0.05340263, 0.27248233, 0.49127685, 0.07926575, 0.96054204],\n    [0.50013988, 0.05903472, 0.43025479, 0.41379231, 0.86508251],\n    [0.02033722, 0.11996034, 0.57675261, 0.12049974, 0.65760677],\n    [0.71859089, 0.22825203, 0.64064407, 0.47443116, 0.64108334]],\n\n   [[0.18813498, 0.29462021, 0.09433628, 0.97393446, 0.33451445],\n    [0.01657461, 0.28126666, 0.64016929, 0.48365073, 0.26672697],\n    [0.9379696 , 0.44648103, 0.39463243, 0.51797975, 0.4173626 ],\n    [0.89788558, 0.31063058, 0.05492096, 0.86904097, 0.21696292]],\n\n   [[0.07279436, 0.94773635, 0.34173115, 0.7228713 , 0.46553334],\n    [0.61199848, 0.88508141, 0.97019517, 0.61465985, 0.48971128],\n    [0.53037002, 0.70782324, 0.32158754, 0.2793538 , 0.62661128],\n    [0.52787814, 0.17085317, 0.83711126, 0.40567032, 0.71386498]]])\n\n\nwith tf.Session() as sess :\n  sess.run( tf.global_variables_initializer() )\n  print(sess.run( tf.concat( [ tensor[0:1,2:3], tensor[1:2,1:2], tensor[2:3,3:4] ] , 1 ) ))\n</code></pre>\n\n<p>This will print the values like this.</p>\n\n<pre><code>[[[0.02033722 0.11996034 0.5767526  0.12049974 0.6576068 ]\n  [0.01657461 0.28126666 0.64016926 0.48365074 0.26672697]\n  [0.52787817 0.17085317 0.83711123 0.40567032 0.713865  ]]]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1553175095}, {'QuestionId': 55280388, 'AnswerId': 55281358, 'URL': 'https://stackoverflow.com/questions/55280388/slicing-a-tensor-along-a-dimension-with-given-index/55281358#55281358', 'QuestionTitle': 'slicing a tensor along a dimension with given index', 'Answer': '<p>You can use <code>tf.one_hot()</code> to mask <code>index_tensor</code>.</p>\n\n<pre><code>index = tf.one_hot(index_tensor,tensor.shape[1])\n\n[[0. 0. 1. 0.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]]\n</code></pre>\n\n<p>Then get your result by <code>tf.boolean_mask()</code>.</p>\n\n<pre><code>result = tf.boolean_mask(tensor,index)\n\n[[0.02033722 0.11996034 0.57675261 0.12049974 0.65760677]\n [0.01657461 0.28126666 0.64016929 0.48365073 0.26672697]\n [0.52787814 0.17085317 0.83711126 0.40567032 0.71386498]]\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1553174335}, {'QuestionId': 53675834, 'AnswerId': 53686871, 'URL': 'https://stackoverflow.com/questions/53675834/how-to-use-tf-gather-nd-to-slice-a-tensor-in-tensorflow/53686871#53686871', 'QuestionTitle': 'How to use tf.gather_nd to slice a tensor in tensorflow?', 'Answer': ""<p>Tensorflow can be quite ugly when it comes to things that are very simple and Pythonic in NumPy. Here is how I used tf.gather_nd to recreate your problem in TensorFlow. There is probably a much better way to do it though.</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nwith tf.Session() as sess:\n    # Define 'a'\n    a = tf.reshape(tf.range(15),(3,5))\n    # Define both index tensors \n    idx_1 = tf.reshape(tf.range(a.get_shape().as_list()[0]),(-1,1)).eval()\n    idx_2 = tf.constant([[1,2,3,4],[0,2,3,4],[0,2,3,4]]).eval()\n    # get indices for use with gather_nd\n    gather_idx = tf.constant([(x[0],y) for (i,x) in enumerate(idx_1) for y in idx_2[i]])\n    # extract elements and reshape to desired dimensions\n    b = tf.gather_nd(a, gather_idx)\n    b = tf.reshape(b,(idx_1.shape[0], idx_2.shape[1]))\n    print(sess.run(b))\n\n[[ 1  2  3  4]\n[ 5  7  8  9]\n[10 12 13 14]]\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1544302361}, {'QuestionId': 47570804, 'AnswerId': 47571192, 'URL': 'https://stackoverflow.com/questions/47570804/tensorflow-cross-index-slicing-of-a-tensor/47571192#47571192', 'QuestionTitle': 'Tensorflow: cross index slicing of a tensor', 'Answer': '<p>You can use <a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer""><code>tf.map_fn</code></a>:</p>\n\n<pre><code> tensor3 = tf.map_fn(lambda u: tf.gather(u[0],u[1],axis=1),[tensor1,tensor2],dtype=tensor1.dtype)\n</code></pre>\n\n<p>You can think of this line as a loop that runs over the first dimensions of <code>tensor1</code> and <code>tensor2</code>, and for each index <code>i</code> in the their first dimension it applies <code>tf.gather</code> on <code>tensor1[i,:,:]</code> and <code>tensor2[i,:]</code>.</p>\n', 'IsAccepted': True, 'CreationDate': 1512037788}, {'QuestionId': 42597520, 'AnswerId': 42606537, 'URL': 'https://stackoverflow.com/questions/42597520/slicing-a-tensor-by-an-index-tensor-in-tensorflow/42606537#42606537', 'QuestionTitle': 'Slicing a tensor by an index tensor in Tensorflow', 'Answer': '<p>You can get exactly what you want using <code>tf.gather_nd</code>.  The final expression is:</p>\n\n<pre><code>tf.gather_nd(params, tf.stack([tf.tile(tf.expand_dims(tf.range(tf.shape(indices)[0]), 1), [1, tf.shape(indices)[1]]), tf.transpose(tf.tile(tf.expand_dims(tf.range(tf.shape(indices)[1]), 1), [1, tf.shape(indices)[0]])), indices], 2))\n</code></pre>\n\n<hr>\n\n<p>This expression has the following explanation:</p>\n\n<ul>\n<li><code>tf.gather_nd</code> does what you expected and uses the indices to gather the output from the params</li>\n<li><p><code>tf.stack</code> combines three separate tensors, the last of which is the indices.  The first two tensors specify the ordering of the first two dimensions (axis 0 and axis 1 of params/indices)</p>\n\n<ul>\n<li>For the example provided, this ordering  is simply 0, 1, 2, ..., 63 for axis 0, and 0, 1, 2, ... 783 for axis 1.  These sequences are obtained with <code>tf.range(tf.shape(indices)[0])</code> and <code>tf.range(tf.shape(indices)[1])</code>, respectively.</li>\n<li><p>For the example provided, indices has shape (64, 784).  The other two tensors from the last point above need to have this same shape in order to be combined with <code>tf.stack</code></p>\n\n<ul>\n<li>First, an additional dimension/axis is added to each of the two sequences using <code>tf.expand_dims</code>.</li>\n<li><p>The use of <code>tf.tile</code> and <code>tf.transpose</code> can be shown by example: Assume the first two axes of params and index have shape (5,3).  We want the first tensor to be:</p>\n\n<pre><code>[[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]]\n</code></pre>\n\n<p>We want the second tensor to be:</p>\n\n<pre><code>[[0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2], [0, 1, 2]]\n</code></pre>\n\n<p>These two tensors almost function like specifying the coordinates in a grid for the associated indices.</p></li>\n</ul></li>\n</ul></li>\n<li><p>The final part of <code>tf.stack</code> combines the three tensors on a new third axis, so that the result has the same 3 axes as params.</p></li>\n</ul>\n\n<hr>\n\n<p>Keep in mind if you have more or less axes than in the question, you need to modify the number of coordinate-specifying tensors in <code>tf.stack</code> accordingly.</p>\n', 'IsAccepted': True, 'CreationDate': 1488705396}, {'QuestionId': 42597520, 'AnswerId': 42603183, 'URL': 'https://stackoverflow.com/questions/42597520/slicing-a-tensor-by-an-index-tensor-in-tensorflow/42603183#42603183', 'QuestionTitle': 'Slicing a tensor by an index tensor in Tensorflow', 'Answer': '<p>What you want is like a custom reduction function. If you are keeping something like index of maximum value at <code>indices</code> then I would suggest using <code>tf.reduce_max</code>:</p>\n\n<pre><code>max_params = tf.reduce_max(params_tensor, reduction_indices=[2])\n</code></pre>\n\n<p>Otherwise, here is one way to get what you want (Tensor objects are not assignable so we create a 2d list of tensors and pack it using <code>tf.pack</code>):</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\nwith tf.Graph().as_default():\n\n    params_tensor = tf.pack(np.random.randint(1,256, [5,5,10]).astype(np.int32))\n\n    indices = tf.pack(np.random.randint(1,10,[5,5]).astype(np.int32))\n\n    output = [ [None for j in range(params_tensor.get_shape()[1])] for i in range(params_tensor.get_shape()[0])]  \n    for i in range(params_tensor.get_shape()[0]):\n        for j in range(params_tensor.get_shape()[1]):\n            output[i][j] = params_tensor[i,j,indices[i,j]]\n    output = tf.pack(output)\n\n    with tf.Session() as sess:\n        params_tensor,indices,output = sess.run([params_tensor,indices,output])\n\n        print params_tensor\n        print indices\n        print output\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1488675280}, {'QuestionId': 38851318, 'AnswerId': 38854589, 'URL': 'https://stackoverflow.com/questions/38851318/slicing-by-tensor-with-indices/38854589#38854589', 'QuestionTitle': 'Slicing by tensor with indices', 'Answer': '<p>Update 10-12-2016: </p>\n\n<p>As of tensorflow version 0.11 and up one can index into tensors in the same way as numpy.</p>\n\n<pre><code>a = tf.Variable([9,10,11])\nb = tf.constant([[1,2,3,4],[5,6,7,8]])\na = b[0,1:]\n</code></pre>\n\n<p>Gradients are also supported on the indexing.</p>\n\n<hr>\n\n<p>What did you try already?</p>\n\n<p>It seems like there\'s a bug with tf.gather_nd that I reported. \nHere\'s the response </p>\n\n<blockquote>\n  <p>Support for partial indices in gather_nd (fewer indices than dimensions) was added quite recently. You are you using a version of TensorFlow where each index tensor must have exactly the number of tensor dimensions. The code should work at HEAD.</p>\n</blockquote>\n\n<p>so by version 0.10 or above gather_nd should work like you want.</p>\n\n<p>However this below works</p>\n\n<pre><code>import tensorflow as tf\nx = tf.constant([[1,1,1,1],[1,2,3,4]],shape=(2,4))\nindices = [[0,0],[0,1]]\ny = tf.gather_nd(x,indices)\n</code></pre>\n\n<p>so it seems like you need the full index description at the moment, not just slice 0.  You also try tf.pack.</p>\n\n<p>You can also track the progress of indexing tensors in tensorflow here:\n<a href=""https://github.com/tensorflow/tensorflow/issues/206"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/issues/206</a></p>\n', 'IsAccepted': False, 'CreationDate': 1470755787}]","{56802840, 64081367}","['<blockquote>\n<p><code>tf.gather</code> Gather slices from <code>params</code> axis <code>axis</code> according to indices.</p>\n</blockquote>\n<p>Granted that the documentation is not the most expressive, and the emphasis should be placed on the <strong>slices</strong> (since you index slices from the <code>axis</code> and not elements, which is what I suppose you mistakenly took it for).</p>\n<p><strong>Let\'s take a much smaller example:</strong></p>\n<pre class=""lang-py prettyprint-override""><code>activations_small = tf.convert_to_tensor([[[1, 2, 3, 4], [11, 22, 33, 44]]])\nprint(activations_small.shape) # [1, 2, 4]\n</code></pre>\n<p>Let\'s picture this tensor:</p>\n<pre><code>    XX 4  XX 44 XX XX\n  XX  3 XX  33 X  XX\nXXX 2 XX   22XX  XX\nX-----X-----+X  XX\n|  1  |  11 | XX\n+-----+-----+X\n</code></pre>\n<p><code>tf.gather(activations1, [0, 0], axis=1)</code> will return</p>\n<pre><code>&lt;tf.Tensor: shape=(1, 2, 4), dtype=int32, numpy=\narray([[[1, 2, 3, 4],\n        [1, 2, 3, 4]]], dtype=int32)&gt;\n</code></pre>\n<p>What <code>tf.gather</code> did was to <em>look from</em> axis 1, and picks up index 0 (ofc, two times i.e. <code>[0, 0]</code>). If you were to run <code>tf.gather(activations1, [0, 0, 0, 0, 0], axis=1).shape</code>, you\'d get <code>TensorShape([1, 5, 4])</code>.</p>\n<p><strong>Your Error</strong>\nNow let\'s try to trigger the error that you\'re getting.</p>\n<p><code>tf.gather(activations1, [0, 2], axis=1)</code></p>\n<blockquote>\n<p>InvalidArgumentError: indices[1] = 2 is not in [0, 2) [Op:GatherV2]</p>\n</blockquote>\n<p>What happened here was that when <code>tf.gather</code> looks from axis 1 perspective, there\'s no item (column if you will) with index = 2.</p>\n<p>I guess this is what the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather#args"" rel=""nofollow noreferrer"">documentation</a> is hinting at by</p>\n<blockquote>\n<p><code>param:&lt;indices&gt;</code> The index Tensor. Must be one of the following types: int32, int64. Must be in range [0, params.shape[axis]).</p>\n</blockquote>\n<p><strong>Your (potential) solution</strong></p>\n<p>From the dimensions of <code>indices</code>, and that of the expected result from your question, I am not sure if the above was very obvious to you.</p>\n<p><code>tf.gather(activations, indices=[0, 1, 2, 3], axis=2)</code> or anything with indices within the range of indices in <code>[0, activations.shape[2])</code> i.e. <code>[0, 4)</code> would work.', '<p>tf.gather is a function to index an array. You gather the elements which you specify by the index argument. This is not natively posible for tensorflow tensors. </p>\n\n<p>tf.gather(y_pred, tf.range(0, batch_size, 3)) is equivalent in numpy to y_pred[0:batch_size:3], which means that you return every third element starting from the first one. </p>\n', '<blockquote>\n<p><code>tf.gather</code> Gather slices from <code>params</code> axis <code>axis</code> according to indices.</p>\n</blockquote>\n<p>Granted that the documentation is not the most expressive, and the emphasis should be placed on the <strong>slices</strong> (since you index slices from the <code>axis</code> and not elements, which is what I suppose you mistakenly took it for).</p>\n<p><strong>Let\'s take a much smaller example:</strong></p>\n<pre class=""lang-py prettyprint-override""><code>activations_small = tf.convert_to_tensor([[[1, 2, 3, 4], [11, 22, 33, 44]]])\nprint(activations_small.shape) # [1, 2, 4]\n</code></pre>\n<p>Let\'s picture this tensor:</p>\n<pre><code>    XX 4  XX 44 XX XX\n  XX  3 XX  33 X  XX\nXXX 2 XX   22XX  XX\nX-----X-----+X  XX\n|  1  |  11 | XX\n+-----+-----+X\n</code></pre>\n<p><code>tf.gather(activations1, [0, 0], axis=1)</code> will return</p>\n<pre><code>&lt;tf.Tensor: shape=(1, 2, 4), dtype=int32, numpy=\narray([[[1, 2, 3, 4],\n        [1, 2, 3, 4]]], dtype=int32)&gt;\n</code></pre>\n<p>What <code>tf.gather</code> did was to <em>look from</em> axis 1, and picks up index 0 (ofc, two times i.e. <code>[0, 0]</code>). If you were to run <code>tf.gather(activations1, [0, 0, 0, 0, 0], axis=1).shape</code>, you\'d get <code>TensorShape([1, 5, 4])</code>.</p>\n<p><strong>Your Error</strong>\nNow let\'s try to trigger the error that you\'re getting.</p>\n<p><code>tf.gather(activations1, [0, 2], axis=1)</code></p>\n<blockquote>\n<p>InvalidArgumentError: indices[1] = 2 is not in [0, 2) [Op:GatherV2]</p>\n</blockquote>\n<p>What happened here was that when <code>tf.gather</code> looks from axis 1 perspective, there\'s no item (column if you will) with index = 2.</p>\n<p>I guess this is what the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather#args"" rel=""nofollow noreferrer"">documentation</a> is hinting at by</p>\n<blockquote>\n<p><code>param:&lt;indices&gt;</code> The index Tensor. Must be one of the following types: int32, int64. Must be in range [0, params.shape[axis]).</p>\n</blockquote>\n<p><strong>Your (potential) solution</strong></p>\n<p>From the dimensions of <code>indices</code>, and that of the expected result from your question, I am not sure if the above was very obvious to you.</p>\n<p><code>tf.gather(activations, indices=[0, 1, 2, 3], axis=2)</code> or anything with indices within the range of indices in <code>[0, activations.shape[2])</code> i.e. <code>[0, 4)</code> would work. Anything else would give you the error that you\'re getting.</p>\n<p>There\'s a verbatim answer below in case that\'s your expected result.</p>\n']","{'https://stackoverflow.com/questions/56802840/what-exactly-tensorflow-gather-does/56802999#56802999', 'https://stackoverflow.com/questions/64081367/slicing-a-tensor-with-a-tensor-of-indices-and-tf-gather/64162041#64162041'}",,0.19406608691781002,0.06403394149548432
27,53032922,tf.while_loop,Documentation Replication on Other Examples,TensorFlow while loop with condition dependent on body,"<p>I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop()</code></a>.</p>

<p>My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by</p>

<pre><code>import numpy as np
import tensorflow as tf
IMAGE_SHAPE = [960, 720]
CROP_SHAPE = [320, 240]
max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
</code></pre>

<p>and the condition is</p>

<pre><code>cond = tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)
</code></pre>

<p>Going over the documentation and examples of <code>tf.while_loop(cond, body, loop_vars, ...)</code>, what I understand is that both <code>cond</code> and <code>body</code> should take the same arguments given in <code>loop_vars</code>.
I don't see how I can have <code>cond</code> depend on <code>img_crop</code> which would be calculated inside <code>body</code>, and isn't provided in <code>loop_vars</code>.</p>

<p>I could equivalently compute <code>cond</code> using <code>crop_begin_index</code> without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem.</p>

<p>Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use <code>tf.while_loop()</code>?</p>
","<p>The arguments that are passed on to the <code>condition</code> function are the arguments returned from your <code>body</code> function. So you just have to return that value that you want to base your condition on in the <code>body</code> function, then carry out the condition on that value in your <code>cond</code> function. Something like, </p>

<pre><code>def body(image_shape, crop_shape, img_crop):
    max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
    crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
    img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
    return (image_shape, crop_shape, img_crop)

def cond(image_shape, crop_shape, img_crop):
    return tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)

image_shape, crop_shape, img_crop = tf.while_loop(cond=cond, body=body, loop_vars=([960, 720], [320, 240], img_crop))
</code></pre>

<p>Don't have access to an interpreter right now, so there might be some syntax problems there, but something like that. </p>

<p>Also, if I recall correctly, the body and the condition need to be pure functions, you cannot alter the outer state from within the functions.</p>

<p>Also note, you'll need to specify some initial value for <code>img_crop</code> in the loop vars.</p>

<p>Moreover, by default, <code>tf.while_loop</code> expects the shapes of all the <code>loop_vars</code> to remain the same across all loop runs. You can modify this through the <code>shape_invariants</code> argument. </p>
","{50486241, 42645165, 60311184, 53483153, 51359731, 38994037, 41233462, 50235287, 45595419, 43701631}","[{'QuestionId': 50235287, 'AnswerId': 64550070, 'URL': 'https://stackoverflow.com/questions/50235287/nested-while-loop-in-tensorflow/64550070#64550070', 'QuestionTitle': 'Nested while loop in tensorflow', 'Answer': '<p>Here is one more example of a nested loop in TensorFlow using <code>tf.while_loop</code>.\nIn this one, ith element of the tensor x are iteratively concatenated by the number of times given in the ith element of the tensor v.</p>\n<pre><code>import tensorflow as tf\nx = tf.Variable([[1,1],[2,2],[3,3]])\nv = tf.constant([1,2,3])\ni = tf.constant(0)\na_combined = tf.zeros([0, 2], dtype=tf.int32)\n\n\ndef body(x,v,i,a_combined):\n    x_slice = tf.slice(x,[i,0], [1, x.shape[1]])\n    v_slice = tf.slice(v,[i],[1])\n    j = tf.constant(0)\n    b_combined = tf.zeros([0, 2], dtype=tf.int32)\n    \n    print(&quot;i: &quot;, i)\n    \n    def body_supp(x_slice,v_slice,j, b_combined):\n        \n        print(&quot;j: &quot;, j)\n        \n        j = tf.add(j,1)\n        b_combined = tf.concat([b_combined,x_slice],0)\n        return x_slice, v_slice, j, b_combined \n    \n    while_condition_supp = lambda x_slice, v_slice, j, b_combined: tf.less(j, v_slice)\n    \n    x_slice, v_slice, j, b_combined = tf.while_loop(while_condition_supp, body_supp, [x_slice, v_slice, j, b_combined])\n    \n    i = tf.add(i,1)\n\n    a_combined = tf.concat([a_combined,b_combined],0)\n    return x, v, i, a_combined\n\nwhile_condition = lambda x, v, i, a_combined: i &lt; v.shape[0]  \n\nx, v, i, a_combined = tf.while_loop(while_condition, body, [x, v, i, a_combined])\n\na_combined \n</code></pre>\n<p>Output would look like this:</p>\n<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">\r\n<div class=""snippet-code"">\r\n<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;tf.Tensor: shape=(6, 2), dtype=int32, numpy=\narray([[1, 1],\n       [2, 2],\n       [2, 2],\n       [3, 3],\n       [3, 3],\n       [3, 3]], dtype=int32)&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n', 'IsAccepted': False, 'CreationDate': 1603784810}, {'QuestionId': 50235287, 'AnswerId': 50235892, 'URL': 'https://stackoverflow.com/questions/50235287/nested-while-loop-in-tensorflow/50235892#50235892', 'QuestionTitle': 'Nested while loop in tensorflow', 'Answer': '<p><a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop</code></a> can be tricky to use, make sure to read the documentation carefully. The return value of the body must have the same structure than the loop variables, and the return value of the <code>tf.while_loop</code> operation is the final value of the variables. In order to make a computation, you should pass an additional loop variable to store the partial results. You could do something like this:</p>\n\n<pre class=""lang-py prettyprint-override""><code>def body1(i, result):\n    global data\n    N = len(data) * positive_samples\n    j = tf.constant(0)\n    condition2 = lambda j, i, result: tf.less(j, N)\n    result = 0\n\n    def body2(j, i, result):\n        global similarity_matrix, U, V\n        result_j = (tf.transpose(U[:, i]) * V[:, j])\n        return j + 1, i, result + result_j\n\n    j, i, result = tf.while_loop(condition2, body2, loop_vars=[j, i, result])\n    return i + 1, result\n\ndef loss_function(x):\n    global data\n    N = len(data)*positive_samples\n    i = tf.constant(0)\n    result = tf.constant(0, dtype=tf.float32)\n    condition1 = lambda i, result: tf.less(i, N)\n    i, result = tf.while_loop(condition1, body1, [i, result])\n    return result\n</code></pre>\n\n<p>It is not clear from your code where <code>x</code> is to be used. In this case, though, the result of the operation should be equal to simply:</p>\n\n<pre class=""lang-py prettyprint-override""><code>result = tf.reduce_sum(tf.linalg.matmul(U, V, transpose_a=True))\n</code></pre>\n\n<p>Which will also be much faster.</p>\n', 'IsAccepted': True, 'CreationDate': 1525789167}, {'QuestionId': 60311184, 'AnswerId': 60324574, 'URL': 'https://stackoverflow.com/questions/60311184/how-to-loop-over-a-tensor-object-until-a-condition-met/60324574#60324574', 'QuestionTitle': 'how to loop over a tensor object until a condition met', 'Answer': '<p>I think I get an idea of what you want, more or less, but I\'m not sure I see the need for the boolean array. If you want to do some iterative process where you compute or retrieve some values until they meet some condition, you can do that without additional arrays. See for example this loop to sample some random values until all of them meet a condition:</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\n# Draw five random numbers until all are &gt; 0.5\nwith tf.Graph().as_default(), tf.Session() as sess:\n    tf.random.set_random_seed(0)\n    # Initial values, here simply initialized to zero\n    tensor1 = tf.zeros([5], dtype=tf.float32)\n    # Loop\n    tensor1 = tf.while_loop(\n        # Loop condition (negated goal condition)\n        lambda tensor1: ~tf.math.reduce_all(tensor1 &gt; 0.5),\n        # Loop body\n        lambda tensor1: tf.random.uniform(tf.shape(tensor1), dtype=tensor1.dtype),\n        # Loop variables\n        [tensor1])\n    # Returned loop value\n    print(tensor1.eval())\n    # [0.7778928  0.9396961  0.572209   0.6187117  0.89615726]\n</code></pre>\n\n<p>See if this helps and leave a comment if you are still not sure how to apply this to your particular case.</p>\n\n<hr>\n\n<p>EDIT: Seeing again your question, your <code>uniqueness</code> function computed both <code>tensor1</code> and the mask, so maybe a more similar analogous code would be this:</p>\n\n<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf\n\ndef sample_numbers(shape, dtype):\n    tensor1 = tf.random.uniform(shape, dtype=dtype)\n    mask = tensor1 &gt; 0.5\n    return tensor1, mask\n\n# Draw five random numbers until all are &gt; 0.5\nwith tf.Graph().as_default(), tf.Session() as sess:\n    tf.random.set_random_seed(0)\n    # Initial values, here simply initialized to zero\n    tensor1 = tf.zeros([5], dtype=tf.float32)\n    mask = tf.zeros(tf.shape(tensor1), dtype=tf.bool)\n    # Loop\n    tensor1, _ = tf.while_loop(\n        # Loop condition (negated goal condition)\n        lambda tensor1, mask: ~tf.math.reduce_all(mask),\n        # Loop body\n        lambda tensor1, mask: sample_numbers(tf.shape(tensor1), tensor1.dtype),\n        # Loop variables\n        [tensor1, mask])\n    # Returned loop value\n    print(tensor1.eval())\n    # [0.95553064 0.5170193  0.69573617 0.9501506  0.99776053]\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1582216301}, {'QuestionId': 41233462, 'AnswerId': 53822325, 'URL': 'https://stackoverflow.com/questions/41233462/tensorflow-while-loop-dealing-with-lists/53822325#53822325', 'QuestionTitle': 'Tensorflow while loop : dealing with lists', 'Answer': '<p>TF offers a <a href=""https://www.tensorflow.org/api_docs/python/tf/TensorArray"" rel=""noreferrer"">TensorArray</a> to deal with such cases. From the doc,</p>\n<blockquote>\n<p>Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p>\n<p>This class is meant to be used with dynamic iteration primitives such as <code>while_loop</code> and <code>map_fn</code>. It supports gradient back-propagation via special &quot;flow&quot; control flow dependencies.</p>\n</blockquote>\n<p>Here is an example,</p>\n<pre><code>import tensorflow as tf\n\narray = tf.Variable(tf.random_normal([10]))\nstep = tf.constant(0)\noutput = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n\ndef cond(step, output):\n    return step &lt; 10\n\ndef body(step, output):\n    output = output.write(step, tf.gather(array, step))\n    return step + 1, output\n\n_, final_output = tf.while_loop(cond, body, loop_vars=[step, output])\n\nfinal_output = final_output.stack()\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(final_output))\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1545077253}, {'QuestionId': 53483153, 'AnswerId': 53483547, 'URL': 'https://stackoverflow.com/questions/53483153/tensorflow-while-loop-on-tensor/53483547#53483547', 'QuestionTitle': 'Tensorflow: while loop on tensor', 'Answer': '<p>The first argument of the  <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer"">tf.while_loop()</a> should return scalar (the tensor of rank 0 is, actually, a scalar - that\'s what the error message is about). In your example you probably want to make the condition return <code>true</code> in case if all the numbers in the <code>a1</code> tensor are less than <code>6.14</code>. This can be achieved by <a href=""https://www.tensorflow.org/api_docs/python/tf/math/reduce_all"" rel=""nofollow noreferrer"">tf.reduce_all()</a> (logical AND) and <a href=""https://www.tensorflow.org/api_docs/python/tf/math/reduce_any"" rel=""nofollow noreferrer"">tf.reduce_any()</a> (logical OR).</p>\n\n<p>That snippet has worked for me:</p>\n\n<pre><code>tf.reset_default_graph()\n\na = np.random.random_integers(3, size=(3,2))\nprint(a)\n# [[1 1]\n#  [2 3]\n#  [1 1]]\n\na1 = tf.constant(a)\ni = 6\n\n# condition returns True till any number in `x` is less than 6\ncondition = lambda x : tf.reduce_any(tf.less(x, i))\nbody      = lambda x : tf.add(x, 1)\nloop = tf.while_loop(\n    condition,\n    body,\n    [a1],\n)\n\nwith tf.Session() as sess:\n    result = sess.run(loop)\n    print(result)\n    # [[6 6]\n    #  [7 8]\n    #  [6 6]]\n    # All numbers now are greater than 6\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1543243461}, {'QuestionId': 51359731, 'AnswerId': 51361824, 'URL': 'https://stackoverflow.com/questions/51359731/tensorflow-while-loop-with-variable-creation/51361824#51361824', 'QuestionTitle': 'Tensorflow While loop with Variable Creation', 'Answer': ""<p>Can you feed it like this ?</p>\n\n<pre><code>l = tf.placeholder(tf.float32, shape=[None, ])\nc = tf.placeholder(tf.float32, shape=[None, ])\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nx = tf.multiply(l,c)   #Assume a formula\n\nfor i in range(50) :\n    arr = np.random.random_sample((i,))\n    print (arr)\n    sess.run(x,feed_dict={l :arr,c :arr})\n</code></pre>\n\n<p>Based on your comment I've only attempted. This isn't the exact answer.</p>\n\n<pre><code>results = tf.TensorArray(dtype=tf.float32, size=100)\n\nlvalues = tf.TensorArray(dtype=tf.float32, size=50)\ncvalues = tf.TensorArray(dtype=tf.float32, size=50)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\ndef my_func(x):\n    a = np.random.random_sample((x,))\n    return a\n\ninput = tf.placeholder(tf.int32)\ny = tf.py_func(my_func, [input], tf.float64)\nfor i in range(50) :\n    r = sess.run(y,feed_dict={input : i})\n    lvalues.write(i,y)\n    cvalues.write(i,y)\n    print(y)\n    print(r)\n</code></pre>\n"", 'IsAccepted': False, 'CreationDate': 1531744013}, {'QuestionId': 50486241, 'AnswerId': 50488872, 'URL': 'https://stackoverflow.com/questions/50486241/tensorflow-tf-while-loop-with-vector-condition/50488872#50488872', 'QuestionTitle': 'Tensorflow: tf.while_loop() with vector condition', 'Answer': '<p>The solution is to use tf.reduce_all():</p>\n\n<pre><code>i = tf.constant(0)\nc = lambda i: tf.reduce_all(tf.greater([10,10],[i,i]))\nb = lambda i: tf.add(i, 1)\nr = tf.while_loop(c, b, [i])\n</code></pre>\n', 'IsAccepted': False, 'CreationDate': 1527079751}, {'QuestionId': 41233462, 'AnswerId': 41240808, 'URL': 'https://stackoverflow.com/questions/41233462/tensorflow-while-loop-dealing-with-lists/41240808#41240808', 'QuestionTitle': 'Tensorflow while loop : dealing with lists', 'Answer': ""<p>Citing the documentation:</p>\n\n<blockquote>\n  <p><code>loop_vars</code> is a (possibly nested) tuple, namedtuple or list\n  of <strong>tensors</strong> that is passed to both <code>cond</code> and <code>body</code></p>\n</blockquote>\n\n<p>You cannot pass regular python array as a tensor. What you can do, is:</p>\n\n<pre><code>i = tf.constant(0)\nl = tf.Variable([])\n\ndef body(i, l):                                               \n    temp = tf.gather(array,i)\n    l = tf.concat([l, [temp]], 0)\n    return i+1, l\n\nindex, list_vals = tf.while_loop(cond, body, [i, l],\n                                 shape_invariants=[i.get_shape(),\n                                                   tf.TensorShape([None])])\n</code></pre>\n\n<p>The shape invariants are there, because normally <code>tf.while_loop</code> expects the shapes of tensors inside while loop won't change.</p>\n\n<pre><code>sess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(list_vals)\nOut: array([-0.38367489, -1.76104736,  0.26266089, -2.74720812,  1.48196387,\n            -0.23357525, -1.07429159, -1.79547787, -0.74316853,  0.15982138], \n           dtype=float32)\n</code></pre>\n"", 'IsAccepted': True, 'CreationDate': 1482231817}, {'QuestionId': 43701631, 'AnswerId': 43702478, 'URL': 'https://stackoverflow.com/questions/43701631/tensorflow-while-loop-with-tensorarray/43702478#43702478', 'QuestionTitle': 'TensorFlow while-loop with TensorArray', 'Answer': '<p>The solution is to change</p>\n\n<pre><code>input_ta.unstack(xs)\n</code></pre>\n\n<p>to</p>\n\n<pre><code>input_ta = input_ta.unstack(xs)\n</code></pre>\n\n<p>and similarly change</p>\n\n<pre><code>output_ta_t.write(time, new_output)\n</code></pre>\n\n<p>to</p>\n\n<pre><code>output_ta_t = output_ta_t.write(time, new_output)\n</code></pre>\n\n<p>With these two changes the code runs as expected.</p>\n', 'IsAccepted': False, 'CreationDate': 1493520952}, {'QuestionId': 38994037, 'AnswerId': 38999135, 'URL': 'https://stackoverflow.com/questions/38994037/tensorflow-while-loop-for-training/38999135#38999135', 'QuestionTitle': 'Tensorflow while_loop for training', 'Answer': '<p>The reason the model does not appear to train is because the input reading, gradient calculation, and the <code>minimize()</code> call are all defined <em>outside</em> (and hence, in dataflow terms, <em>before</em>) the body of the <code>tf.while_loop()</code>. This means that all of these parts of the model run only once, before the loop executes, and the loop itself has no effect.</p>\n\n<p>A slight refactoring&mdash;to move the <code>dequeue()</code> operations, gradient calculation, and <code>minimize()</code> call inside the loop&mdash;fixes the problem and allows your program to train:</p>\n\n<pre class=""lang-python prettyprint-override""><code>optimizer = tf.train.GradientDescentOptimizer(0.05)\n\ndef cond(i):\n    return i &lt; 10\n\ndef body(i):\n    # Dequeue a new example each iteration.\n    x = q_x.dequeue()\n    y = q_y.dequeue()\n\n    # Compute the loss and gradient update based on the current example.\n    loss = (tf.add(tf.mul(x, w), b) - y)**2\n    train_op = optimizer.minimize(loss, global_step=gs)\n\n    # Ensure that the update is applied before continuing.\n    return tf.tuple([tf.add(i, 1)], control_inputs=[train_op])\n\nloop = tf.while_loop(cond, body, [i])\n</code></pre>\n\n<hr>\n\n<p><strong>UPDATE:</strong> Here\'s a complete program the executes the while loop, based on the code in your question:</p>\n\n<pre class=""lang-python prettyprint-override""><code>import tensorflow as tf\n\n# Define a single queue with two components to store the input data.\nq_data = tf.FIFOQueue(100000, [tf.float32, tf.float32])\n\n# We will use these placeholders to enqueue input data.\nplaceholder_x = tf.placeholder(tf.float32, shape=[None])\nplaceholder_y = tf.placeholder(tf.float32, shape=[None])\nenqueue_data_op = q_data.enqueue_many([placeholder_x, placeholder_y])\n\ngs = tf.Variable(0)\nw = tf.Variable(0.)\nb = tf.Variable(0.)\noptimizer = tf.train.GradientDescentOptimizer(0.05)\n\n# Construct the while loop.\ndef cond(i):\n    return i &lt; 10\n\ndef body(i):\n    # Dequeue a single new example each iteration.\n    x, y = q_data.dequeue()\n    # Compute the loss and gradient update based on the current example.\n    loss = (tf.add(tf.multiply(x, w), b) - y) ** 2\n    train_op = optimizer.minimize(loss, global_step=gs)\n    # Ensure that the update is applied before continuing.\n    with tf.control_dependencies([train_op]):\n        return i + 1\n\nloop = tf.while_loop(cond, body, [tf.constant(0)])\n\ndata = [k * 1. for k in range(10)]\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for _ in range(1):\n        # NOTE: Constructing the enqueue op ahead of time avoids adding\n        # (potentially many) copies of `data` to the graph.\n        sess.run(enqueue_data_op,\n                 feed_dict={placeholder_x: data, placeholder_y: data})\n    print (sess.run([gs, w, b]))  # Prints before-loop values.\n    sess.run(loop)\n    print (sess.run([gs, w, b]))  # Prints after-loop values.\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1471443202}, {'QuestionId': 45595419, 'AnswerId': 45596727, 'URL': 'https://stackoverflow.com/questions/45595419/is-it-possible-to-have-multiple-conditions-defined-in-tf-while-loop/45596727#45596727', 'QuestionTitle': 'Is it possible to have multiple conditions defined in tf.while_loop', 'Answer': '<p><code>tf.while_loop</code> accepts a generic callable (python functions defined with <code>def</code>) or lambdas) that must return a boolean tensor.</p>\n\n<p>You can, therefore, chain multiple conditions within the body of the condition using the <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/logical_operators"" rel=""nofollow noreferrer"">logical operators</a>, like <code>tf.logical_and</code>, <code>tf.logical_or</code>, ...</p>\n\n<p>Even <code>body</code> is a general python callable, thus you\'re not limited to lambdas and single statement functions.</p>\n\n<p>Something like that is perfectly acceptable and works well:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n\ndef body(x):\n    a = tf.random_uniform(shape=[2, 2], dtype=tf.int32, maxval=100)\n    b = tf.constant(np.array([[1, 2], [3, 4]]), dtype=tf.int32)\n    c = a + b\n    return tf.nn.relu(x + c)\n\n\ndef condition(x):\n    x = tf.Print(x, [x])\n    return tf.logical_or(tf.less(tf.reduce_sum(x), 1000), tf.equal(x[0, 0], 15))\n\n\nx = tf.Variable(tf.constant(0, shape=[2, 2]))\nresult = tf.while_loop(condition, body, [x])\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    print(sess.run(result))\n</code></pre>\n', 'IsAccepted': True, 'CreationDate': 1502298319}, {'QuestionId': 42645165, 'AnswerId': 42713053, 'URL': 'https://stackoverflow.com/questions/42645165/reevaluate-dependencies-of-a-while-loop/42713053#42713053', 'QuestionTitle': 'Reevaluate dependencies of a while loop', 'Answer': ""<p>With some help from the other answer I managed to get this working. Posting the complete code here as a second answer:</p>\n\n<pre><code>x = tf.constant(4, dtype=tf.float32)\ny = tf.multiply(x,x)\n\ngrad = tf.gradients(y, x)\n\ndef loop_grad(x_loop):\n    y2 = tf.multiply(x_loop, x_loop)\n    return tf.gradients(y2, x_loop)[0]\n\niterations = tf.placeholder(tf.int32)\ni = tf.constant(0, dtype=tf.int32)\nc = lambda i_loop, x_loop: i_loop &lt; iterations\nb = lambda i_loop, x_loop: [i_loop+1, x_loop - 0.1*loop_grad(x_loop)]\nl = tf.while_loop(c, b, [i, x], back_prop=False, parallel_iterations=1)\n\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n    sess.run(tf.global_variables_initializer())\n    res_g = sess.run(grad)\n    res_l = sess.run(l, feed_dict={iterations: 100000})\n    res_x = sess.run(x)\n\n\nprint(res_g)\nprint(res_l)\nprint(res_x)\n</code></pre>\n\n<p>changing the learning rate from the code in the question and increasing the number of iterations gives the output:</p>\n\n<pre><code>[8.0]\n[100000, 5.1315068e-38]\n4.0\n</code></pre>\n\n<p>Which seems to be working. It runs reasonably fast even with high iteration count, so there does not seem to be something really horrible going on with updating the graph in each iteration of the while loop, a fear of which probably was one reason why I didn't opt for this approach from the start.</p>\n"", 'IsAccepted': False, 'CreationDate': 1489132354}, {'QuestionId': 42645165, 'AnswerId': 42659635, 'URL': 'https://stackoverflow.com/questions/42645165/reevaluate-dependencies-of-a-while-loop/42659635#42659635', 'QuestionTitle': 'Reevaluate dependencies of a while loop', 'Answer': '<p>Having <code>tf.Variable</code> objects as loop variables for while loops is not supported, and will behave in weird nondeterministic ways. Always use <code>tf.assign</code> and friends to update the value of a <code>tf.Variable</code>.</p>\n', 'IsAccepted': False, 'CreationDate': 1488926063}]","{53032922, 41233462}","[""<p>The arguments that are passed on to the <code>condition</code> function are the arguments returned from your <code>body</code> function. So you just have to return that value that you want to base your condition on in the <code>body</code> function, then carry out the condition on that value in your <code>cond</code> function. Something like, </p>\n\n<pre><code>def body(image_shape, crop_shape, img_crop):\n    max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)\n    crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)\n    img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])\n    return (image_shape, crop_shape, img_crop)\n\ndef cond(image_shape, crop_shape, img_crop):\n    return tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)\n\nimage_shape, crop_shape, img_crop = tf.while_loop(cond=cond, body=body, loop_vars=([960, 720], [320, 240], img_crop))\n</code></pre>\n\n<p>Don't have access to an interpreter right now, so there might be some syntax problems there, but something like that. </p>\n\n<p>Also, if I recall correctly, the body and the condition need to be pure functions, you cannot alter the outer state from within the functions.</p>\n\n<p>Also note, you'll need to specify some initial value for <code>img_crop</code> in the loop vars.</p>\n\n<p>Moreover, by default, <code>tf.while_loop</code> expects the shapes of all the <code>loop_vars</code> to remain the same across all loop runs. You can modify this through the <code>shape_invariants</code> argument. </p>\n"", ""<p>Citing the documentation:</p>\n\n<blockquote>\n  <p><code>loop_vars</code> is a (possibly nested) tuple, namedtuple or list\n  of <strong>tensors</strong> that is passed to both <code>cond</code> and <code>body</code></p>\n</blockquote>\n\n<p>You cannot pass regular python array as a tensor. What you can do, is:</p>\n\n<pre><code>i = tf.constant(0)\nl = tf.Variable([])\n\ndef body(i, l):                                               \n    temp = tf.gather(array,i)\n    l = tf.concat([l, [temp]], 0)\n    return i+1, l\n\nindex, list_vals = tf.while_loop(cond, body, [i, l],\n                                 shape_invariants=[i.get_shape(),\n                                                   tf.TensorShape([None])])\n</code></pre>\n\n<p>The shape invariants are there, because normally <code>tf.while_loop</code> expects the shapes of tensors inside while loop won't change.</p>\n\n<pre><code>sess = tf.Session()\nsess.run(tf.global_variables_initializer())\nsess.run(list_vals)\nOut: array([-0.38367489, -1.76104736,  0.26266089, -2.74720812,  1.48196387,\n            -0.23357525, -1.07429159, -1.79547787, -0.74316853,  0.15982138], \n           dtype=float32)\n</code></pre>\n"", ""<p>The arguments that are passed on to the <code>condition</code> function are the arguments returned from your <code>body</code> function. So you just have to return that value that you want to base your condition on in the <code>body</code> function, then carry out the condition on that value in your <code>cond</code> function. Something like, </p>\n\n<pre><code>def body(image_shape, crop_shape, img_crop):\n    max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)\n    crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)\n    img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])\n    return (image_shape, crop_shape, img_crop)\n\ndef cond(image_shape, crop_shape, img_crop):\n    return tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)\n\nimage_shape, crop_shape, img_crop = tf.while_loop(cond=cond, body=body, loop_vars=([960, 720], [320, 240], img_crop))\n</code></pre>\n\n<p>Don't have access to an interpreter right now, so there might be some syntax problems there, but something like that. </p>\n\n<p>Also, if I recall correctly, the body and the condition need to be pure functions, you cannot alter the outer state from within the functions.</p>\n\n<p>Also note, you'll need to specify some initial value for <code>img_crop</code> in the loop vars.</p>\n\n<p>Moreover, by default, <code>tf.while_loop</code> expects the shapes of all the <code>loop_vars</code> to remain the same across all loop runs. You can modify this through the <code>shape_invariants</code> argument.""]","{'https://stackoverflow.com/questions/53032922/tensorflow-while-loop-with-condition-dependent-on-body/53033052#53033052', 'https://stackoverflow.com/questions/41233462/tensorflow-while-loop-dealing-with-lists/41240808#41240808'}",{41233462},0.20125980471013835,0.05376167718391792
